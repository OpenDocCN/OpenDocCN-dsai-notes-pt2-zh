- en: P13：13.Mar_23_Lecture - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P13：13.Mar_23_Lecture - Tesra-AI不错哟 - BV1aJ411y7p7
- en: Alright guys， let's get started。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，大家，我们开始吧。
- en: '![](img/ad312fe98e99c01ed03298543603234f_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad312fe98e99c01ed03298543603234f_1.png)'
- en: No protection for you guys。 I hope you had a good spring break。 Today is a full
    lecture on boosting。 We're going to start with "Ada Boost" which we started talking
    about two weeks ago but。 we'll review what we covered because it's been some time。
    And we're going to start by presenting "Ada Boost" as just an algorithm which
    is kind of how。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不为你们提供保护。我希望你们度过了一个愉快的春假。今天的讲座将会是关于提升方法的全面介绍。我们将从两周前开始讲的“Ada Boost”讲起，但我们会回顾一下我们所讲的内容，因为已经有一段时间了。我们将首先把“Ada
    Boost”作为一种算法来介绍，这就是我们开始讲的方式。
- en: perception was presented。 It was an iterative method and it was like a recipe
    for how to compute this prediction。 function。 So we'll start presenting "Ada Boost"
    that way and we're going to kind of get a feel。 for it， see how it's working，
    kind of make sure it at least intuitively makes sense， to us。 And then we're going
    to present kind of a new approach to modeling called "Forward Stage。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机方法被提出时，是一种迭代方法，类似于如何计算这个预测函数的食谱。我们将以这种方式开始介绍“Ada Boost”，并且我们将逐步了解它，看看它是如何工作的，确保至少从直观上来说，我们能理解它。然后我们将介绍一种新的建模方法，叫做“Forward
    Stage”。
- en: Wise Additive Modeling" and what we'll find is that "Ada Boost" is exactly an
    instance。 of this new framework and the new framework is something more familiar
    to us and that we。 can understand in terms of loss functions in terms of minimization，
    iterative minimization。 algorithms。 So that's going to be nice to reframe "Ada
    Boost" in a more familiar setting。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: “Wise Additive Modeling”，我们会发现，“Ada Boost”正是这个新框架的一个实例，而这个新框架对我们来说更为熟悉，我们可以从损失函数的角度来理解它，谈论最小化，迭代最小化算法。因此，以这种更熟悉的方式重新框架“Ada
    Boost”将会很有意义。
- en: And then we're going to generalize a bit further to something called "Greedian"
    which will allow。 us to move away from move to more general loss functions than
    the one that the "Ada Boost"。 is minimizing。 So let's go on today。 So let's do
    a little bit of a review。 We are talking about ensemble methods where we have
    multiple models that we combine together。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进一步推广到一种叫做“Greedian”的方法，它将允许我们从“Ada Boost”正在最小化的损失函数，转向更一般的损失函数。今天我们继续讲这个话题。所以我们先来做一个小回顾。我们在讲集成方法，其中我们有多个模型将它们组合在一起。
- en: We spoke about parallel ensembles which are like the random forests and the
    bagging but。 today we're talking about sequential ensembles。 And the idea there
    is that you build a model。 you see what data points it does well on and， where
    it does poorly and the ones that it does poorly on you increase the importance。
    of and then you make a new model that tries to do well where the previous model
    did poorly。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讲过并行集成方法，比如随机森林和袋装法，但今天我们讲的是顺序集成方法。其思路是你构建一个模型，查看它在哪些数据点上表现良好，在哪些地方表现不佳，然后增加那些表现不佳的点的重要性，再构建一个新模型，试图在前一个模型做得不好的地方做好。
- en: And then you look at the combination of those two models and then you see， well
    where is。 it still doing poorly and you try to repair， in each step you try to
    repair what's still。 not doing well from the models you have so far。 Okay， we'll
    make that a little bit more clear。 Alright， so we started with this idea of a
    weak classifier which is kind of a classifier。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你查看这两个模型的组合，看看它在哪些地方表现不佳，你尝试修正每一步中还做得不好的部分。好吧，我们会稍微澄清一下这个问题。好吧，我们从一个弱分类器的概念开始，这是一种分类器。
- en: which we hope can do at least better than 50 percent， at least better than random
    guessing。 on a classification problem。 Alright， I have some examples and so in
    adaboost the setting is strictly binary classification。 Our weak hypothesis space
    or a weak learner produces hard classifications， negative one， and one。 So we're
    not in the setting of scores at this point where our base classifiers are predicting。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望至少能做得比50%好，至少比随机猜测好，针对一个分类问题。好吧，我有一些例子，在Adaboost中，设定是严格的二分类。我们的弱假设空间或弱学习器会生成硬分类，-1
    和 1。所以我们现在不处于基分类器预测得分的情境中。
- en: hard classes。 Alright。 So the question is， given this base hypothesis space
    and we have an algorithm for choosing。 an F from this hypothesis space that does
    at least better than random on some training， step。 can we combine F's chosen
    from this hypothesis space in some way that can do very。 well on the training
    set， not just a little bit better than random。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 困难的类别。好的。那么问题是，给定这个基本假设空间，我们有一个算法可以从这个假设空间中选择一个至少比随机选择更好的 F，来进行某些训练步骤。那么我们能否以某种方式将从这个假设空间中选择的
    F 结合起来，使其在训练集上表现非常好，而不仅仅是比随机选择稍微好一点？
- en: So an adaboost typical weak learner's typical base hypothesis space would be
    like decision。 trees or even decision stumps which are trees of depth one。 Sometimes
    linear decision functions。 Traditionally we do boosting with rather simple hypothesis
    spaces is not required but this。 is where it seems to give the most benefit。 Okay。
    So one thing that we'll need is this notion of a weighted training set。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，adaboost 的典型弱学习器的基本假设空间通常是像决策树，甚至是深度为一的决策桩。有时也会是线性决策函数。传统上，我们使用相对简单的假设空间来做提升（boosting），虽然这不是必需的，但似乎这样做能带来最多的好处。好的。那么，我们需要的一个概念是加权训练集。
- en: This is where every item of the training set has a， let's call it a non-negative
    real value。 associated with it and we look to minimize a weighted empirical risk，
    a weighted， some。 of the weighted loss functions。 We talked about this last time。
    So rather than just the average of the loss on the data points we have a weighted
    average。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，每个训练集的项都与一个非负实值相关联，我们试图最小化加权经验风险，一个加权的，损失函数的总和。我们上次讲过这个内容。所以，我们不只是计算数据点的损失平均值，而是计算加权平均值。
- en: and we're going to need a， so we're going to either a need an algorithm that
    can choose。 a function from our hypothesis space that minimizes the weighted empirical
    risk or there's。 another possibility。 Do you guys remember the other approach
    to use？
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个，所以我们要么需要一个可以选择一个从我们假设空间中最小化加权经验风险的函数的算法，或者还有另一种可能性。你们还记得另外一种方法吗？
- en: Suppose we don't have an algorithm that knows how to minimize weighted empirical
    risk but。 it's a black box algorithm and we know it can minimize empirical risk。
    So what was。 we talked about this last time， do you guys remember what we can
    do？
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们没有一个可以最小化加权经验风险的算法，但它是一个黑盒算法，我们知道它可以最小化经验风险。那么，我们上次讨论过这个，大家还记得我们可以怎么做吗？
- en: We'll try to figure that again because last time someone figured it out。 Okay
    duplicate the training examples and what proportions？ Yeah， according to the weights。
    So the weights are non-negative。 We have a weight for every training example。
    If we normalize the weights so they sum to one， what does that turn into？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再想想这个，因为上次有人想到了。好的，复制训练样本，按照什么比例？对，根据权重。所以权重是非负的。我们对每个训练样本都有一个权重。如果我们将权重归一化，使它们的总和为
    1，那会变成什么？
- en: This is like a probability distribution on the examples。 So if we draw our example。
    so we have a training set and then now we have a probability distribution， on
    the training set。 Now imagine drawing a new training set according to this distribution。
    So and suppose you made it very large， so large that almost every data point appeared。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是对样本的概率分布。所以如果我们抽取我们的样本，那么我们就有了一个训练集，现在我们有了一个基于训练集的概率分布。现在想象按照这个分布抽取一个新的训练集。如果你把它做得非常大，足够大，以至于几乎每个数据点都出现。
- en: multiple times。 So the number of times these data point appears is going to
    be proportional to this weight。 if you draw enough examples。 So if you just add
    up the examples drawn up from that distribution。 the normal empirical， risk on
    that resampled distribution will be very close to this weighted empirical risk。
    and you can feed those sampled examples into your black box empirical risk minimizer
    that。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多次出现。所以这些数据点出现的次数将与这个权重成正比，如果你抽取足够的样本。如果你把从该分布中抽取的样本加起来，基于该重新抽样分布的正常经验风险将非常接近这个加权经验风险，然后你可以将这些抽样的例子输入到你的黑盒经验风险最小化器中。
- en: doesn't know anything about weights。 Is that clear？ All right。 it's for a little
    slow coming off spring break but I saw some not。 That's a good start。 That's a
    good start。 So we have， so now we have a way to fit a weighted empirical risk
    from a function in our base。 hypothesis space and that's going to be something
    we will need to have。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它对权重一无所知。明白了吗？好吧，春假后有些慢，但我看到了一些不错的反应。这是个好的开始。这是个好的开始。那么，现在我们有了从我们基本假设空间中拟合加权经验风险的方式，这是我们需要的东西。
- en: So here's the rough sketch of out of boost。 We have our training set。 we start
    with equal weights on all the training points and now， we proceed in different
    rounds。 So in each round we use our algorithm and fit our， a week classifier，
    something from a。 hypothesis space to the weighted training points。 In the first
    round they're weighted equally so nothing special。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Out-of-Boost的粗略框架。我们有我们的训练集，我们从给所有训练点赋予相等的权重开始，然后我们进入不同的回合。在每一轮中，我们使用我们的算法，拟合一个弱分类器，从假设空间中选择一些东西，应用于加权训练点。在第一轮中，它们的权重是相等的，所以没什么特别的。
- en: So we get our week classifier and then we see what this GMX misclassified and
    we increase。 the weight on the examples that were misclassified and then we repeat。
    So it's going to happen is if a point is misclassified repeatedly its weight keeps
    getting larger。 and larger and every time we choose a new classifier it's going
    to be working harder。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得到一个弱分类器，然后我们看到这个分类器在什么地方错分了，并且增加那些被错分样本的权重，然后我们重复这个过程。发生的情况是，如果某个点被反复错分，它的权重就会不断增大。而每次我们选择一个新的分类器，它就会在那个点上做得更努力。
- en: and harder to fit that one point or those points that have been misclassified
    before。 because their weights keep getting driven up higher。 Yeah？ Yeah。 so the
    question is it's great you're doing all this work to correct the things that。
    were wrong but are you not messing up the things that you had before？ This is。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 而且更难拟合那些曾经被错分过的点，因为它们的权重不断增大。是吧？是的。所以问题是，你在做这么多工作来纠正错误的地方，但你是不是没有搞砸之前做得对的那些地方呢？这就是。
- en: yeah it's a balancing act。 So the new classifier that you add may only get a
    little bit right than the one it was。 working really hard to get and it made you
    terribly on the other ones it's true but we。 actually combine it with all the
    ones we had before。 So hopefully the ones we had before are still strong enough
    on the rest of the data points。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这就像是一个平衡的过程。所以你新增的分类器可能只比前一个做得稍微好一点，它在其他点上可能做得很差，确实是这样，但我们实际上会将它与之前所有的分类器结合起来。所以希望我们之前的分类器在其余数据点上的效果仍然足够强大。
- en: that the new thing doesn't mess it up。 So this is the。 this is kind of the balancing
    act that we'll see happening when we get into， the details。 All right。 Okay， so
    kind of schematically we start the original training sample we get the first class。ifier
    in the usual way。 Then we have a reweighted example based on what things were
    wrong。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这样新的东西就不会搞砸了。所以这是一个平衡的过程，我们将在进入细节时看到它发生。好的。好的，简单地说，我们从原始训练样本开始，按照通常的方式得到第一个分类器。然后，我们有一个基于错误情况重新加权的样本。
- en: Those things that were wrong have a higher weight。 We get a new classifier based
    on that example。 Then we take the same sample and reweight it again， et cetera
    and then we get M classifiers。 Remember these are classifiers that output negative
    one on one， they're hard classifiers。 And they've all been trained on different
    weightings of the same data set。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那些做错的样本的权重会更高。我们根据这些样本得到一个新的分类器。然后我们再对同样的样本进行加权，依此类推，最终我们得到M个分类器。记住，这些分类器输出的是负一或一，它们是硬分类器。并且它们都在不同的加权数据集上训练过。
- en: And then what do we do at the end？ We take a linear combination of them and
    the specific way we do that we'll talk about shortly。 And then we'll kind of value
    this linear combination going to be。 Is it going to be negative one one？ No。 It's
    going to be a linear combination of negative ones and ones which are not negative
    one on。 one。 It could be a real number， any real number so far。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，最后我们该怎么做呢？我们取它们的线性组合，至于我们如何具体做这件事，我们稍后会讨论。然后我们会评估这个线性组合的值。它会是负一还是一吗？不，它将是负一和一的线性组合，但不会是负一和一。它可能是一个实数，至今为止，它可以是任何实数。
- en: And then so we take the sign of it to convert it back to a classification。 Okay。
    That's the big picture。 Okay。 So what are these weights？ What are the weights？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取它的符号，将其转换回分类。好的。这就是大致的框架。好的。那么，这些权重是什么？这些权重是怎么回事？
- en: We combine the classifiers from each stage。 So the weights are going to be not
    negative。 And as someone asked a few weeks ago， do we want to weigh the classifiers
    that did well。 higher than the ones that didn't do well？ And yes， to some extent，
    yes。 So we're going to see that the alpha M's are going to be larger when this
    classifier fits。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个阶段的分类器结合起来。所以权重不会是负数。正如几周前有人问的那样，是否我们需要将做得好的分类器的权重加大，给它们比做得不好的分类器更高的权重？是的，在一定程度上，确实如此。所以我们会看到，当分类器适配得更好时，alpha
    M会更大。
- en: the weighted data well， the weighted data that it received and smaller otherwise。
    Okay。 All right。 So let's talk about round M， an arbitrary round M。 We get this
    classifier GM and it gets a certain weighted error。 So this is where we count
    the misclassification， this is kind of the zero one misclassification。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 加权数据好，接收到的加权数据比较小。好的。好吧。现在我们来谈谈 M 轮，一个任意的 M 轮。我们得到这个分类器 GM，它得到一个特定的加权误差。所以这里我们计算错误分类，这是一种0-1的误分类。
- en: rate。 Instead of just adding up or getting the percentage of errors。 we're going
    to weight those errors， by the weight of the individual example in that round。
    So if we were really trying hard to get example I correct， the weight was really
    high。 Why would it be high？ Because we kept getting it wrong in the earlier rounds。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: rate。我们不会仅仅计算错误的总和或百分比，而是根据每个例子在该轮中的权重来加权这些错误。所以如果我们在这一轮真的很努力地想要把例子 I 做对，那么它的权重就会很高。为什么会很高？因为我们在之前的轮次中一直做错了它。
- en: So if we were really trying to get the i'th example correct and nevertheless
    we got it， wrong。 we're going to count that higher in our error。 So weighted error。
    Good。 So ERM， error。 we'll call it the error of the Mth round， error sub M， that's
    going to。 characterize how well our Mth classifier did on this weighted training
    set。 All right。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们真的很努力地想要把第 i 个例子做对，然而我们还是做错了，我们就会在我们的错误中计数更高。所以加权错误。好。所以 ERM，错误。我们称之为
    M 轮的错误，错误子 M，这将会表征我们第 M 个分类器在这个加权训练集上的表现。好的。
- en: And you notice it's between zero and one。 Is that clear？ Why is this between
    zero and one？
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到它在0和1之间。这清楚了吗？为什么它在0和1之间？
- en: It's clear。 If you look at W I divided by capital W is the sum of the W I。 So
    W I over W。 that's like a probability。 If you add them up in sums to one， they're
    all non-negative。 So this is like taking a convex combination of things that are
    zero and one。 So that's certainly between zero and one。 Yeah。 So the implementation
    of the fields is going to take one of the errors in the same。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显。如果你看W I 除以大写 W，它就是所有 W I 的总和。所以 W I 除以 W，类似于一个概率。如果你把它们加起来，结果是1，它们都是非负的。所以这就像是从0和1之间做一个凸组合。所以它一定是在0和1之间。对。所以字段的实现将采用相同的错误。
- en: Correct。 And we initialize all the W's to be the same。 That's right。 Yes。 How
    do I decide the order of the classifiers？ [INAUDIBLE]， [INAUDIBLE]， [INAUDIBLE]，
    [INAUDIBLE]。 [INAUDIBLE]， [INAUDIBLE]， [INAUDIBLE]， [INAUDIBLE]， [INAUDIBLE]，
    Let me come back to you。 I'll come back。 Yes？ [INAUDIBLE]， Say again？ [INAUDIBLE]，
    Outlowers in the data， yeah。 [INAUDIBLE]。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对。我们把所有的 W 初始化为相同的。没错。是的。我怎么决定分类器的顺序？ [INAUDIBLE]，[INAUDIBLE]，[INAUDIBLE]，[INAUDIBLE]。[INAUDIBLE]，[INAUDIBLE]，[INAUDIBLE]，[INAUDIBLE]，[INAUDIBLE]。让我再想想，我会再回来回答你。好的？
    [INAUDIBLE]，再说一遍？ [INAUDIBLE]，数据中的离群值，嗯。[INAUDIBLE]。
- en: It could be an issue。 So the question is what about outliers in the data？ Can
    we overfit？ Yeah。 Out of boost is known to have some overfitting issues。 We'll
    come back to that。 Good question。 What？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是个问题。所以问题是数据中的离群值怎么办？我们会过拟合吗？嗯。Boosting 已知有一些过拟合问题。我们稍后会讨论这个问题。好问题。什么？
- en: I don't call it overfitting so much as performance not being great in the case
    of outliers。 Overfitting I don't know。 Well， maybe it's overfitting， yeah。 [INAUDIBLE]，
    That is ridiculous。 I didn't understand your question。 I'm not going to say that。
    I'm not going to say that。 I'm not going to say that。 I'm not going to say that。
    Now we have a weighted training set。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我不太把它叫做过拟合，而是说在离群值的情况下，性能不太好。过拟合我不确定。好吧，也许是过拟合，嗯。 [INAUDIBLE]，那太荒谬了。我没明白你的问题。我不打算说那句话。我不打算说那句话。我不打算说那句话。我不打算说那句话。现在我们有了一个加权训练集。
- en: Now we're here， weight set number two。 Now we fit to minimize the weighted loss。
    weighted empirical risk。 You got it。 We just needed a recap。 Then we get classifier
    number two， G2。 Then we repeat G3。 Then at the end we take a linear combination。
    Now at this point we're saying a non-negative combination of these classifiers。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们到了第二个权重集。现在我们拟合以最小化加权损失，加权经验风险。你明白了。我们只需要回顾一下。然后我们得到第二个分类器，G2。然后我们重复 G3。最后我们取一个线性组合。现在我们说的是这些分类器的非负组合。
- en: That's going to be predicting real value numbers， scores。 Then we take the sign
    to get a classifier。 We have a weighted training set。 It's just two weights because
    we have two。 It's a plus-one minus one。 Are there only two weights that will be
    the sign of the term？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那将是预测真实数值，得分。然后我们取符号来得到分类器。我们有一个加权训练集。只有两个权重，因为我们有两个。它是加一减一。只有两个权重，是否就是项的符号？
- en: In the first round you are correct。 In the very first round there's a symmetry。
    Every point has either been classified correctly once or incorrectly once。 If
    we reweight depending on just on whether it was correct or not， then you're correct。
    Everything in the second round will have one of two weights， but not so in the
    third round。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一轮，你是正确的。在第一轮中存在对称性。每个点要么被正确分类一次，要么被错误分类一次。如果我们根据是否正确分类来重新加权，那么你是对的。第二轮中的每个点会有两种权重，但在第三轮中就不一样了。
- en: There are things that were correctly classified once or twice。 It turns out
    it's not just how many times it was classified。 but it's also related to how well
    the classifier did in that round。 How do you do it？
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有些东西被正确分类了一次或两次。事实证明，这不仅仅是它被分类了多少次，还与分类器在该轮次中的表现有关。你是怎么做到的？
- en: That depends entirely on the question。 The question was how do you minimize
    the rate of zero and error？
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 那完全取决于问题。问题是，你如何最小化零错误的比率？
- en: I gave -- so version one。 We reduce it to the case of minimizing regular zero-one
    error by this re-sampling of data proportional to the weights。 I don't know if
    you were here for that part。 One version is by re-sampling proportional to the
    weights you reduce it to minimizing zero and error。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我给出了——所以是第一版。我们通过对数据进行加权重新采样，将其简化为最小化常规零一错误的情况。我不知道你是否在那部分在场。通过按权重比例重新采样的方式，第一版将其简化为最小化零错误。
- en: You could say we don't know how to minimize zero and error， and that is true。
    You don't have to minimize -- you aim to minimize zero and error or the weight
    of classification error。 but you really just need to do better than random guessing。
    Adjust the weight class bar？
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说我们不知道如何最小化零错误，这个说法是对的。你不必最小化——你可以目标是最小化零错误或分类错误的权重。但你真的只需要做得比随机猜测更好。调整分类错误的权重吧？
- en: Just a week class -- yes。 Just a week classifier， right？
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 只是一个弱分类器——是的。只是一个弱分类器，对吧？
- en: The exact amount that we'll be weighted by -- in every round。 we'll see that
    everything that was incorrectly classified gets scaled up by the same factor depending
    on the round。 The weight -- the fact that it's multiplied by the same amount。
    Why don't we look at the next coming slides where we get into that？ Yeah。 Yeah。
    Sure does。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按每一轮的情况查看具体的加权量。我们会看到所有被错误分类的数据都会按相同的因子进行缩放，具体缩放的因子依赖于轮次。权重——也就是它被乘以相同的量。我们为什么不看一下接下来的幻灯片，看看我们是如何处理这个问题的？是的。没错，确实如此。
- en: It looks at every data point unless -- well， yes。 You never get zero weight，
    so yes。 It looks at every data point in every round。 That's right。 It may almost
    ignore certain data points when the weights get very， very small， which happens。
    Yes？
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它会查看每一个数据点，除非——嗯，是的。你永远不会得到零权重，所以是的。它在每一轮中都会查看每个数据点。没错。当权重变得非常非常小的时候，它可能几乎忽略某些数据点，这种情况是会发生的。是吗？
- en: Would it be possible to have data points get zero weight in order to speed up
    your error power of those？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能让数据点获得零权重，以加速这些点的错误修正能力？
- en: So， okay， ignore it。 That's a great question。 I haven't heard of anyone doing
    that。 but I like the idea。 So， maybe you could have a threshold。 Like， if the
    weight gets really small。 you just set it to zero and you pray that it never stops
    classifying that correctly， subsequently。 It's worth the try。 I don't know。 Okay。
    It's interesting。 But it's the right idea， I think。 Okay。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，忽略它吧。这个问题很好。我没听说过有人这么做过，但我喜欢这个想法。所以，或许你可以设置一个阈值。如果权重变得非常小，你就把它设置为零，然后祈祷它以后再也不会停止正确分类。值得一试。我不知道。好的，这很有趣。但我认为这是个正确的思路。好的。
- en: All right。 So， let's -- you guys are hungry for the actual specifics， so let's
    just get there。 Okay。 All right。 So， this was， let's say， answer your question。
    So。 we want to treat the weak learner as a black box。 We can use any method we
    want。 but we want to at least have kind of weighted error less than a half。 Okay？
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以，来吧——你们都渴望了解实际的细节，那我们就直接进入吧。好吧。好的。那这个问题就假设已解答。那么。我们想把弱学习器当作一个黑盒。我们可以使用任何我们想要的方法。但至少我们希望有一个加权的错误率小于一半。好的？
- en: That's sufficient for what we're going for。 Okay。 All right。 Now， the weight
    of the classifier。 how do we do the reweighting？ Here it is。 So， we reweight by
    this factor alpha。 Here's a formula。 log of 1 minus the error over there。 I don't
    know exactly -- doesn't mean anything to me directly。 but I plotted it so we can
    just look at it。 So， on the x-axis， we have the error。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这就足够我们所需的。好的。现在，分类器的权重，我们怎么做加权调整？就是这样。我们通过这个因子 alpha 来重新加权。这里有一个公式。1 减去错误的对数。我不完全知道——对我来说直接的意义不大，但我绘制了它，这样我们可以直接看它。所以，x
    轴上是错误率。
- en: This is the overall error -- the overall weighted error of the classifier， right？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是总体错误——分类器的总体加权错误，对吧？
- en: The weighted error。 And if we have the no weighted error， alpha is very high。
    And as the weight error gets closer to a half， alpha m goes towards zero。 What's
    alpha m？ Yeah。 Alpha m is the weight of the classifier when we -- so remember
    at the end。 we're going to add all these classifiers together。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 加权错误。如果没有加权错误，alpha 会非常高。而当加权错误接近一半时，alpha m 会趋近于零。什么是 alpha m？对，alpha m 是分类器的权重，记住，到最后，我们会把所有这些分类器加在一起。
- en: And the weight that classifier enters the non-negative combination is alpha
    m。 So。 alpha m big means it gets a lot of weight。 Alpha m swam means it gets very
    little weight。 So。 if the classifier has really bad weighted error， then it's
    going to have very little weight in the final。 Okay。 All right。 That kind of makes
    sense。 The shape at least is in the right direction。 So。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器进入非负组合的权重是 alpha m。所以，alpha m 大意味着它获得了很大的权重。Alpha m 小意味着它获得的权重非常小。所以，如果分类器的加权错误真的很差，它在最终结果中的权重就会非常小。好的，明白了。至少形状上是朝着正确的方向发展的。所以。
- en: now that's the weighting of the entire classifier。 And now the other thing we
    need to know is how we reweight the individual examples， right？
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是整个分类器的加权。现在我们需要知道的另一个问题是，如何重新加权单独的例子，对吧？
- en: In each round。 Okay。 So， let's take a look。 So， let's suppose alpha m is the
    weight of gm。 So。 it turns out we're going to use alpha m also to help us reweight
    the individual examples。 So。 suppose w i is the weight of example i before the
    training。 And if gm classifies xi correctly。 the weight is unchanged。 All right。
    So， in every round any example that was correctly classified。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每一轮。好的，那么我们来看看。假设 alpha m 是 gm 的权重。所以，结果是我们还会使用 alpha m 来帮助我们重新加权每个单独的例子。所以，假设
    w i 是训练前第 i 个例子的权重。如果 gm 正确分类了 xi，那么权重就不变。好的。那么，在每一轮中，任何被正确分类的例子。
- en: we don't touch it。 Otherwise， we adjust it。 We increase it by a factor。 All
    of the weights of all the incorrectly classified examples are increased by e to
    the alpha m。 That's our factor。 All right。 Well， e to the alpha m that counts
    as the log。 but we can look at that too。 So， this is the adjustment to the weights
    for the incorrectly classified ones。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不动它。否则，我们就调整它。通过一个因子增加它。所有被错误分类的例子的权重都会增加 e 的 alpha m 次方。这就是我们的因子。好的。那么，e
    的 alpha m 次方是对数，但我们也可以看看这个。所以，这是对错误分类的例子权重的调整。
- en: All right。 So， the shape of the say， and the only difference is that now it's
    on a log scale on the x-axis。 All right。 So， interpretation。 Or let's do a little
    bit of a recap。 So。 every example that's misclassified in a particular round has
    its weight rescaled。 Is it increased or decreased？ Increased。 Good。 And everything
    is rescaled by the same amount。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，形状是这样，唯一的区别是现在它在 x 轴上是对数刻度。好的。那么，解释一下。或者我们做个小总结。所以，每个在某一轮中被错误分类的例子，其权重都会被重新调整。是增加还是减少？增加。很好。所有的权重都会按照相同的比例重新调整。
- en: They may， they still end up at different values because they started at different
    values。 But the factor you increase them all by is the same。 And the factor is，
    yes。 I do。 We start everything in round one with the same weight。 Say one for
    everything。 Everything that's misclassified is rescaled the same amount。 So， if
    it's a doubling。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可能，最终仍然会达到不同的值，因为它们起始时的值不同。但你增加它们的因子是相同的。这个因子是，没错。我知道。我们从第一轮开始，所有的权重都是相同的。假设都是
    1。所有错误分类的都会按照相同的比例重新加权。所以，如果是翻倍的话。
- en: all the ones that are wrong are rescaled by the same amount。 Yes。 Only ones
    that are wrong are rescaled in each round。 That's right。 So。 if we started all
    at once and there's a particular example that's always classified correctly in
    every round。 it stays at one。 Yeah。 So， if it's a variable， we don't want to take
    into account how much it was。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有错误的都按相同的比例重新缩放。是的。只有那些错误的在每一轮都会重新缩放。没错。所以，如果我们一次性开始，有一个特定的例子在每一轮都被正确分类，它就保持在一。是的。所以，如果它是一个变量，我们不想考虑它有多大。
- en: No， that's correct。 There's no， so the， the question was。 suppose we had a base
    classifier that didn't just output negative one on one。 This is highly restricted
    to hard classifiers。 So。 there is no score given by the functions in these hypothesis
    spaces。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不，没错。没有，所以，问题是。假设我们有一个基础分类器，它不仅仅输出负一和一。这个问题高度限制于硬分类器。所以，假设空间中的函数没有给出分数。
- en: It's just a plus one or a minus one。 You could ask。 suppose our hypothesis space
    gave classifiers of confidence。 Can we leverage that somehow？
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是加一或减一。你可以问，假设我们的假设空间给出了具有置信度的分类器。我们可以利用这一点吗？
- en: Is the question。 I think we can， but not out of boost。 It's not out of boost。
    Yeah。 Yeah。 [ Inaudible ]， We'll rescaling increase the error。 I'm not sure it
    can be a little more clear。 So。 you're rescaling the weight on a data point。 So，
    the error of what？
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 是这个问题。我认为我们可以，但不能仅仅从提升方法（boosting）中得出结论。它不是来自 boosting。是的。是的。[听不清]，我们的重缩放会增加误差。我不确定它是否能更清楚一点。所以，你是在重缩放数据点的权重。那么，是什么的误差呢？
- en: Our goal is to come up with a function that performs， at this point。 our goal
    is to find a function that minimizes the overall training error。 That's right。
    I'm using data rescaling as a piece of the process to get to minimizing the training
    data。 That's correct。 So， that the question I think is basically like， does this
    thing actually work？
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是提出一个函数，在这一点上，我们的目标是找到一个最小化整体训练误差的函数。没错。我使用数据重缩放作为过程的一部分来最小化训练数据的误差。没错。所以，我认为这个问题基本上是，这个方法到底有效吗？
- en: '[ Laughter ]， politely asked。 We''ll come back to that。 All right。 Okay。 [
    Inaudible ]。 Did I say there''s a lower error for higher weight？ Okay。 So， if
    the -- okay。 So， lower error。 higher weight， give me on what？ So， error is the
    weighted error for a particular classifier around M。 say？ Okay。 I thought you
    were rereading the training。 We -- yes。'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[笑声]，礼貌地提问。我们稍后再回到这个问题。好吧。好。[听不清]。我说过高权重对应低误差吗？好吧。所以，如果——好吧。所以，低误差，高权重，给我解释一下是什么？所以，误差是指某个特定分类器在第
    M 轮的加权误差。是吗？好吧。我以为你在重新审视训练数据。我们——是的。'
- en: We weigh the classifiers by our FEM， and that's the amount that it gets multiplied
    by before it's added into the sum of classifiers。 So， there's a weighing of the
    individual classifiers。 That's one piece of it。 And then there's also a re-waying
    of the examples in each round。 And what's interesting is the factor by which we
    re-way the examples is related to the factor that we used to multiply the classifier
    by。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 FEM 来加权分类器，这就是它在加到分类器的总和之前被乘上的权重。所以，存在对单个分类器的加权。这是其中的一部分。然后，在每一轮中，我们还会重新加权样本。而有趣的是，我们重新加权样本的系数与我们用来乘以分类器的系数相关。
- en: We're using alpha M。 Alpha M is involved in both cases。 Yeah？ [ Inaudible ]，
    Okay。 Can we use alpha M as a confidence score of the classifier？
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Alpha M。Alpha M 涉及到两种情况。是吗？[听不清]，好吧。我们能不能将 Alpha M 用作分类器的置信度分数？
- en: Alpha M is related to how well it does on the weighted error。 So。 in -- it's
    a -- it's some measure of performance。 It may be only vaguely related to how helpful
    it is to the final classifier。 But --， [ Inaudible ]， That's correct。 [ Inaudible
    ]， It sounds like a "why does this work？
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Alpha M 与它在加权误差上的表现相关。所以，在——它是——某种性能度量。它可能只与最终分类器的帮助程度有些模糊关系。但是——，[听不清]，没错。[听不清]，这听起来像是“为什么这个方法有效”？
- en: '" question。 We''ll come back to it。 Let me get through some more slides。 and
    then we''ll come back to these questions。 All right。 So， here''s kind of the overall
    algorithm。 Everything we talked about already。 So， I don''t think we need to dwell
    on this。 Here''s a little bit of an illustration。 So， we start with -- our input
    space is the -- a box of size one by one。'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '"问题。我们稍后再回到这个问题。让我再讲几个幻灯片，然后我们再回到这些问题。好吧。所以，这里是整体算法的概览。我们之前讨论的所有内容。所以，我认为我们不需要在这上面停留太久。这里有一个小插图。所以，我们从——我们的输入空间是——一个大小为一乘一的框。'
- en: And we have two classes， red and blue。 And we have -- so this is our first classifier。
    After one round， what do you think our base classifier -- our set our base hypothesis。
    base is if this is -- what？ Huh？ Threshold function。 Yeah。 Okay。 Decision stumps。
    This looks like decision stump。 A stump is a tree where you only have one split。
    So。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个类别，红色和蓝色。我们有——这是我们的第一个分类器。经过一轮后，你认为我们的基本分类器——我们的基本假设是什么？什么？阈值函数。是的。好的。决策树桩。这看起来像是决策树桩。树桩是一个树，只在一个地方分裂。
- en: here we've split on the x-axis at whatever point -- negative point one or something。
    And to the left we're predicting red and to the right we're predicting blue。 The
    color being black and white represents how big our score is for each color。 All
    right。 Let's do another round。 It'll be more clear。 All right。 So， what's going
    on here？
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在 x 轴上分割在任意点——负点一或其他什么地方。左边我们预测红色，右边我们预测蓝色。颜色的黑白代表了我们对每种颜色的分数有多大。好的，让我们再来一轮。会更清楚一些。好的。那么，这里发生了什么？
- en: Now the -- all the points are of different sizes。 What do the sizes represent？
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在——所有的点大小都不一样。大小代表了什么？
- en: There are the weights at this particular round。 All right。 So， after three rounds。
    the weight of this red thing right in the middle of the blue zone。 is very large
    because it keeps getting misclassified。 So， after three rounds。 this point has
    a very large weight。 All right。 These points deepen in red zone and correctly
    classified are quite small。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这一轮的权重。好的。所以，经过三轮后，这个位于蓝色区域正中央的红色点权重大，因为它一直被错误分类。所以，经过三轮后，这个点的权重很大。好的，这些在红色区域的点且被正确分类的权重则相对较小。
- en: They're like spots。 So， okay。 We've been saying that the weights stay the same
    or get bigger。 So。 this is rescaled so that they don't get too ginormous。 These
    are the ones that are the smallest。 So。 in our version， that would be like --
    they would still be at weight one。 But here we've kind of rescaled to keep them
    proportional。 Okay。 So， this is the most confident red。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它们就像小点。好的。我们一直说权重保持不变或变大。那么，这里重新缩放了它们，以免它们变得过于庞大。这些是最小的。在我们的版本中，这就像——它们仍然会保持在权重一。但在这里我们进行了重新缩放，以保持它们的比例。好的。那么，这是最自信的红色。
- en: The score is the maximum and then the spray is a middle score。 And then this
    white is -- score at the other extreme。 And the final thresholding is given by
    this yellow line。 Okay。 Any questions on this picture？ So。 let's look at one more
    round or after 120 rounds you see this -- the decision value can get quite complicated。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 分数是最大的，然后喷射的是中等分数。然后这个白色是——另一端的分数。最终的阈值由这条黄色线给出。好的。关于这张图有什么问题吗？那么，让我们再看一轮，或者说在
    120 轮后，你会看到——决策值变得相当复杂。
- en: This is far more complicated than a single decision -- all right。 So。 this is
    one decision stump by basically linear combination of those。 We get this more
    complicated boundary。 And here we get something that's quite complicated。 Okay。
    And then you see the blue。 Now the blue ones are very large because they've been
    -- now these blue guys are deep in red territory and they keep getting misclassified。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这比单一的决策要复杂得多——好的。所以，这是一个决策树桩，通过这些的线性组合，我们得到了一个更复杂的边界。在这里，我们得到了一个相当复杂的情况。好的。然后你会看到蓝色。现在蓝色的点非常大，因为它们已经——现在这些蓝色的点深入到了红色区域，并且它们一直被错误分类。
- en: All right。 All right。 How's this picture settling with you guys？ Good。 Let's
    clarify some things。 Okay。 [ Inaudible ]， Wow。 That's a great question。 So。 I
    guess it hasn't -- I guess it's had enough times that it wasn't correctly class。
    but -- that's a good question。 Yeah。 [ Inaudible ]， Yes。 [ Inaudible ]， Yeah。
    So。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。怎么样，大家对这个图像的理解如何？好。让我们澄清一些事情。好的。[听不清]，哇，那个问题很好。那么，我猜它没有——我猜它已经有足够的次数没被正确分类了。但——那是个好问题。对。[听不清]，是的。[听不清]，是的。那么。
- en: I think what you're -- I think what you're saying is correct is that -- let's
    say if you can match it up exactly。 I don't know if I have a line that pictures
    well enough。 I wasn't expecting this close examination。 but let's suppose that，
    indeed， see this little yellow thing， that's changing the class there。 So。 this
    is -- I don't know。 Would you say this to me looks like overfitting？ All right。
    Okay。 So。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得你说的没错——我觉得你说的对——假设你能准确匹配它。我不知道我是否有一条足够好的线来表示这一点。我没料到会有如此深入的检查。但假设确实如此，看看这个小黄色的东西，那就是在改变类别。所以，这是——我不知道。你会说我看起来像是过拟合吗？好的。好的。那么。
- en: I have other ways to work on overfitting issue。 But do you have any ideas right
    now of how you might prevent overfitting with this type of algorithm？
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我有其他方法可以解决过拟合问题。但是你们现在有任何想法，如何用这种类型的算法防止过拟合吗？
- en: Limit -- oh， limit your number of iterations of steps of rounds。 Yeah。 we could
    stop running out of this at a certain point。 Maybe use validation error to say，
    all right。 that's enough。 We're getting worse。 Okay。 All right。 All right。 So。
    now we've come to the question that you guys had。 But so -- so， so far， just overall
    in the class。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 限制——哦，限制迭代次数或轮次的步数。是的，我们可以在某个点停止运行这个过程。也许可以用验证误差来判断，行了，够了，我们变差了。好。好。好。那么，现在我们来到了你们的问题。所以——到目前为止，整个课堂的内容。
- en: we've kind of seen -- the main type of thing we've done is set up things as
    convex optimization problems。 Like we had L1 regularization， L2 regularization，
    SVM， kernelized versions of these things。 And。 you know， there are convex。 You
    kind of do any reasonable optimization algorithm and you're going to get to the
    right place。 In the end。 And those were -- those were quite nice。 And then we
    had trees。 In trees。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了——我们做的主要事情是将问题设置为凸优化问题。比如我们有 L1 正则化、L2 正则化、SVM 以及这些方法的核化版本。而且，你知道，这些都是凸优化问题。你只要用任何合理的优化算法，就会找到正确的答案。最终。这些方法都非常好。然后我们有了树。在树的领域。
- en: we know what we wanted to do。 We just didn't know algorithmically how to do
    it exactly。 So we described this kind of heuristic process of building a tree
    out and then trimming it back。 which people find work pretty well。 You're choosing
    something from the hypothesis space of trees。 There's no proof。 In fact， we don't
    know how to actually find the best tree -- the best hypothesis space。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们想做什么，只是不知道在算法上到底如何实现。所以我们描述了这样一个启发式过程：先构建一棵树，再将其修剪回去，结果人们发现这样做效果很好。你在从树的假设空间中选择某个东西。没有证明。事实上，我们并不知道如何找到最好的树——最好的假设空间。
- en: So -- but anyway， we have an algorithm for optimizing the trees。 I was making
    a different point in that bullet， but that's okay。 So， ada-boost is something
    new。 It's just an algorithm， so I mentioned earlier。 So it's kind of like the
    perceptron algorithm。 So we have to ask the question， like， will this minimize
    the training error？ At this point。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以——不过无论如何，我们有一个优化树的算法。我在这一点上想表达的是其他内容，但没关系。所以，AdaBoost 是一种新的方法。就像我之前提到的那样。它有点像感知机算法。所以我们必须问一个问题，像这样，它会最小化训练误差吗？此时。
- en: that's the goal。 Eventually， we want it to do well on test error。 But for starters。
    we need to have it be doing something reasonable on the training error。 So there's
    a theorem。 It's not too hard， but it's too long for lecture。 And may put it in
    the homework as an optional problem。 It's not too hard with some guidance。 So
    it is true under pretty minimal assumption。 All right。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是目标。最终，我们希望它在测试误差上表现得很好。但一开始，我们需要它在训练误差上做得合理。所以有一个定理。它并不难，但讲解起来太长了，可能会把它作为一个可选的作业问题。只要有一些指导，其实不难。所以它在相当小的假设下是成立的。好。
- en: So here's what we need。 We need it to be an actual weak classifier。 So we need
    it to have weighted error less than a half。 So we define this term called the
    edge。 The edge of a classifier is describing how much better than random it is。
    So it just takes a half minus there。 So now we want a big edge。 Big edge is good。
    Big error is bad。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们需要的。我们需要它成为一个真正的弱分类器。所以我们需要它的加权误差小于一半。于是我们定义了一个叫做边缘的术语。分类器的边缘描述了它比随机表现更好的程度。所以它就等于一半减去这个值。所以现在我们想要一个大边缘。大边缘是好的。大误差是坏的。
- en: Big edge is good。 So it's measuring something like how much better than random
    GM is performing。 So here's a theorem， which we'll kind of try to figure out what
    it means in a minute。 The theorem is the empirical 0， 1 risk。 This is the original
    error， not the weighted error。 This is the thing that we actually want to minimize
    in training。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 大边缘是好的。所以它衡量的是像 GM 这种比随机表现更好的程度。那么这里有一个定理，我们稍后会试着弄明白它的意思。这个定理是经验性的 0, 1 风险。这是原始的误差，而不是加权误差。这正是我们在训练中想要最小化的东西。
- en: The empirical 0 and risk of out of whose classifier GX。 What is GX？
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 经验性的 0 风险，来自于哪个分类器 GX。GX 是什么？
- en: It's that non-negative combination of the individual classifier。 the weighted
    classifiers at each round。 So it's like alpha 1 G1 plus alpha 2 G2 up to however
    many rounds we ran。 So what's on the left here？ This is the risk on the training
    data。 the average number of the percentage of errors basically。 It's bounded by
    this product。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个非负的单个分类器的组合。每轮的加权分类器。所以就像是 alpha 1 G1 加上 alpha 2 G2，直到我们运行的轮数为止。那么这里左边是什么？这是训练数据上的风险，基本上是错误百分比的平均值。它被这个积所限制。
- en: So what is it？ It's in terms of gamma m。 Gamma m is the edge of the individual
    classifiers。 So would you think big edge should be better， right？ Big edge is
    far from random。 So this thing is big， but if this thing is big， this thing gets
    small， and we're multiplying。 a bunch of small things together， so it's small。
    So this， when gamma is big， this product is small。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么它是什么？它是关于 gamma m 的。Gamma m 是单个分类器的优势。那么你认为大优势应该更好，对吧？大优势远离随机。所以这个值很大，但如果这个值很大，这个值会变小，而且我们在乘一堆小的值在一起，所以它很小。所以当
    gamma 很大时，这个积会很小。
- en: and that's good because we want， that's the error。 It's bounding the error on
    the left。 Let's look at some examples。 Alright， suppose the weighted error is
    less than 0。4 for all rounds。 just to see what it looks like。 Then the edge is
    0。1， because how much better than 0。5 we are。 And then we get that the error rate
    is bounded by 0。98 to the m， bounded by 0。98 to the m。 So great。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，因为我们希望，那个是误差。它限制了左边的误差。我们来看一些例子。好的，假设加权误差在所有轮次中都小于 0.4。只是看看它长什么样子。那么优势是
    0.1，因为我们比 0.5 好多少。然后我们得到误差率被限制为 0.98 的 m 次方，限制为 0.98 的 m 次方。所以很好。
- en: so if m， the larger m is， the more rounds we run， that thing goes down exponentially，
    fast。 0。98 is bad。 If we only run for one round， that's not a good bounded off。
    But if we multiply 0。98 against itself many times， that thing goes to 0。 So if
    we run for 100 rounds， the bound is 0。133。 200 rounds， 0。018。 Okay， so the upper
    bound decreases exponentially quickly。 Let's go。 Yeah。 Okay。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果 m，m 越大，我们运行的轮次越多，那个值会呈指数级下降，下降得非常快。0.98 很差。如果我们只运行一轮，那就不是一个好的界限。但如果我们将
    0.98 自己乘很多次，那就会变成 0。所以如果我们运行 100 轮，界限是 0.133。200 轮，0.018。好的，所以上界呈指数级快速下降。我们继续。好，继续。
- en: so I made one big assumption to make this picture， to make these numbers。 What
    was my assumption that I made？ I assume that the error is always less than or
    equal to 0。4。 So if I could do that， this is the result。 So you might ask， well，
    so your question was。 does this work for all hypothesis spaces， right？ Okay。 Okay，
    no。 Suppose we have hypothesis space。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我做了一个大的假设来画出这个图，来得出这些数字。我做的假设是什么？我假设误差总是小于或等于 0.4。所以如果我能做到这一点，这是结果。那么你可能会问，嗯，你的问题是，这适用于所有的假设空间，对吧？好吧，好吧，不是。假设我们有一个假设空间，
- en: for which we're not able to get any edge at all。 We can't do better than random
    guessing。 Well。 then we won't have this error bounded away from a half， and we
    won't be able to get。 exponential decrease， perhaps。 So another question would
    be， well， suppose， all right。 test question。 Suppose the edge is， or suppose the
    error is bounded by 0。4。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个假设空间，我们根本无法获得任何优势。我们做得比随机猜测还要差。嗯，那么我们就不能有这个误差从 0.5 远离的界限，也许我们也无法获得指数级下降。所以另一个问题是，好，假设，好的，测试问题。假设优势是，或者假设误差被
    0.4 限制。
- en: Will we always be able to get 0 error on the training set？ So， yes。 Is the proposal，
    and I agree。 The bound goes down to， as long as the， if the edge is always， if
    the error is always bounded。 away from a half， then we're always going to have
    exponential decrease on the upper bound。 and eventually the error is going to
    go to 0。 And it will be exactly 0， right？
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否总是能够在训练集上得到 0 的误差？所以，是的。这是提议，我同意。只要如果优势总是，如果误差总是远离 0.5 的界限，那么我们总是会在上界上看到指数级下降。最终误差会降到
    0。并且会准确地为 0，对吧？
- en: Because on the left-hand side， this will get arbitrarily close to 0， and the
    smallest non-zero。 thing， the thing on the left could be is what？ What's the smallest
    number？
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在左侧，这个值将无限接近 0，最小的非零值，左边的值最小可以是多少？最小的数字是多少？
- en: The thing on the left can be that's bigger than 0。 1 over n， right？ So on the
    right-hand side。 we'll eventually be less than 1 over n， so that drives the left-hand，
    side to 0。 So if we can always guarantee the error is less than a half， strictly
    less than a half， then。 the right-hand side will always go to 0。 So this should
    tell you that if you have a training set without liars。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的东西可以是大于 0 的 1/n，对吧？所以在右边。最终会小于 1/n，这会把左边驱动到 0。所以如果我们总是能保证误差小于一半，严格小于一半，那么，右边的值会始终趋近于
    0。所以这应该告诉你，如果你有一个没有骗子的训练集……
- en: with points that， aren't， you know， where the red and the blue are not separable
    by some kind of linear combination。 of your base hypothesis space， so if your
    hypothesis space can't do it， if you're taking linear。 combinations of things
    in your hypothesis space is not able to separate your data， then you。 can't possibly
    get zero training error， which means you can't possibly have the error always。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些，无法通过某种线性组合来分隔的红色和蓝色的点，所以如果你的假设空间做不到，如果你在假设空间中进行的线性组合无法分开数据，那么你就不可能得到零训练误差，这意味着你不可能总是没有误差。
- en: less than a half， the weighted error of a given round always less than a half。
    Any questions on this？ Yeah。 [ Inaudible ]， Yeah， so out of exactly。 So the question
    is what we classifier can we use？ Yeah。 out of boost basically takes a black box
    classifier， preferably one that can handle。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 小于一半，给定回合的加权误差总是小于一半。对此有任何问题吗？是的。[听不清]，是的，所以问题是我们能使用什么分类器？是的。Boosting 基本上会使用一个黑箱分类器，最好是能够处理……
- en: weighted training data， but if not， you have to do your sampling trick to make
    use of it。 But right。 algorithmically， you can use any classifier you want。 To
    get this type of theorem to work， yeah。 question？ [ Inaudible ]， So the question
    is， is there any reason that people use these simple classifiers instead。 of something
    more sophisticated？ Okay， I think the empirical example reason is that they tend
    to work well。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 加权训练数据，但如果没有，你就需要使用采样技巧来利用它。但没错，从算法上讲，你可以使用任何你想要的分类器。要使这个定理起作用，没错。问题？[听不清]，所以问题是，为什么人们会使用这些简单的分类器，而不是一些更复杂的？好吧，我认为经验性的原因是它们通常能很好地工作。
- en: People have run experiments， playing people run experiments where you fancy
    your classifiers。 like SVMs and stuff， and you can get some benefit from boosting
    SVMs， but not so exciting。 What types of classifiers do we have in our tool chest
    that we might want to use？ Right。 we have these linear classifiers。 So that's
    kind of not as exciting to take linear combinations of linear kind of prediction。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 人们做过实验，像 SVM 这样的分类器，你可以从增强 SVM 中获得一些好处，但没什么特别激动人心的。我们工具箱中有哪些分类器是我们可能想要使用的？对，我们有这些线性分类器。所以这就不是那么令人兴奋，只是线性组合的线性预测。
- en: functions。 Trees are nice because they have these nice nonlinear properties。
    [ Inaudible ]， Yeah。 [ Inaudible ]， Yeah， so the point is that if there's a notion
    of generalization error。 which is how well you're， going to do on your test data
    compared to how you did on your training data。 and the more， complicated your
    base classes， you're exposed to having bigger generalization errors。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 函数。决策树很不错，因为它们具有这些很好的非线性特性。[听不清]，是的。[听不清]，是的，重点是，如果存在泛化误差的概念，也就是你在测试数据上表现得如何，相较于你在训练数据上的表现，且你的基础类越复杂，那么，你更有可能面临较大的泛化误差。
- en: so， bigger， you know， overfitting essentially。 So yes。 that's a good point that
    as you increase the complexity of your hypothesis space， then。 you're potentially
    overfitting， but really we， in any case， we're going to do early stopping。 or
    something， so we're not really going to overfit， right？ It's just a different
    tradeoff。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，复杂性更大，你知道，过拟合本质上。所以是的，这是一个很好的观点，随着你增加假设空间的复杂性，那么，你有可能会过拟合，但实际上，不管怎样，我们会进行早停。或者其他方法，所以我们其实不会真的过拟合，对吧？这只是一个不同的权衡。
- en: It's just --， [ Inaudible ]， Yeah。 Yes。 [ Inaudible ]， The pictures？ Sure。 [
    Inaudible ]， Sure。 [ Inaudible ]， [ Inaudible ]， [ Inaudible ]， The reason we
    decrease errors is because we have more --。 [ Inaudible ]， So the question is
    why are things --， Okay。 So why are we doing better after three rounds than we
    did after one round？
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——，[听不清]，是的。是的。[听不清]，图片吗？当然。[听不清]，当然。[听不清]，[听不清]，[听不清]，我们减少错误的原因是因为我们有更多——。[听不清]，所以问题是，为什么——好吧，为什么我们在三轮之后做得比一轮之后更好？
- en: It's one of the questions。 Okay。 So one reason that we can do better after three
    rounds than after one round is because by taking。 these linear combinations of
    decision trees， of decision stumps， we get a more complicated。 boundary that cannot
    be represented by a decision stump。 So our effective hypothesis space after three
    rounds is bigger than after one round。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是其中一个问题。好吧。我们能在三轮之后比在一轮之后做得更好的原因之一是，因为通过取这些决策树、决策桩的线性组合，我们得到一个更复杂的边界，而这个边界是无法通过决策桩表示的。因此，三轮之后我们的有效假设空间比一轮之后要大。
- en: So that's -- it's just a more expressive set of hypotheses by the third round。
    Yeah。 [ Inaudible ]。 So the question is， is this -- will we get the same boundary
    if we run a decision tree with three splits？
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是——到了第三轮，它只是一个更具表现力的假设集合。是的。[听不清]。所以问题是，运行三次分裂的决策树时，我们会得到相同的边界吗？
- en: '[ Inaudible ]， No， no， not necessarily。 Because you may say， is it the same
    -- one question is。 is it the same hypothesis space that you''re， selecting from？
    You can say。 is every decision boundary that we can get from three rounds of boosting
    on。 stumps the same as the set of all decision trees with， I guess， three splits
    or three nodes？'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，不，不，不一定。因为你可能会问，是否是同一个——其中一个问题是，是否是你选择的假设空间相同？你可以说，三轮提升后，所有可能得到的决策边界，是否和所有拥有三次分裂或三节点的决策树集合相同？'
- en: '[ Inaudible ]， Maybe。 I''m not sure。 But --， [ Inaudible ]， Well。 but none
    of the minimization is exact， right？ For trees。 we have a method that''s an approximate
    method to figure out the right tree。 Adebust is another way to end up with the
    hypothesis in that set。'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，也许吧。我不确定。但——[听不清]，嗯。但没有哪种最小化方法是精确的，对吧？对于树，我们有一种近似方法来找出正确的树。Adebust是另一种方式来得到这个集合中的假设。'
- en: Even if the hypothesis spaces are the same， the way we choose them is not the
    same。 So it's an interesting question whether the hypothesis spaces are the same。
    But in any case。 Adebust and just choosing a decision tree directly of that size
    are two entirely different trees。 you're going to end up with。 Yeah。 [ Inaudible
    ]， You're definitely not going to end up with the same tree。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 即使假设空间相同，我们选择它们的方式也不同。所以，假设空间是否相同是一个有趣的问题。但无论如何，Adebust和直接选择一个那个大小的决策树是两棵完全不同的树。你最终会得到——是的。[听不清]，你肯定不会得到相同的树。
- en: The question is， is the hypothesis space -- the hypothesis space of after three
    rounds of boosting。 with decision stumps the same as a certain type of decision
    tree that you can easily describe？
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，三轮提升之后的假设空间——使用决策桩的假设空间，是否与某种类型的决策树相同，而这个决策树是你可以轻松描述的？
- en: Maybe。 Cool。 Okay。 All right。 I'll show you a few quick questions。 Okay。 So
    it's a curious property of boosting， which is that people often -- this was kind
    of a mystery in。 I don't know， ten years ago or something。 So as we run Adebust
    through many rounds。 what you would expect to happen is that as the number of
    rounds increased。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 也许吧。酷。好的。好吧。我会给你们展示一些快速的问题。好的。提升的一个奇特性质是，人们常常——大约十年前，这曾是一个谜。所以，当我们通过多轮运行Adebust时，你会期待的情况是，当轮数增加时。
- en: your training error would go down， down， down， maybe reach zero in this case。
    The test error would reach a minimum and then go back up。 So what people often
    realize found though in practice is that it didn't actually look like that。 What
    they found was that the training error may go down， but the test error kept going
    down。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你的训练误差会下降，下降，再下降，也许在这种情况下达到零。测试误差会达到最小值，然后再上升。所以人们在实践中经常发现，实际上情况并不是这样。他们发现，训练误差可能会下降，但测试误差继续下降。
- en: Just kind of interesting。 So they have some explanations of this。 They kind
    of go beyond the scope of this course， but this is kind of the topic for like
    a fundamental machine learning class。 I don't know if they cover the specific
    topic， but this is an interesting thing in the lore of machine learning that。
    boosting -- Adebust have this unusual property。 But one thing that tells you is
    that -- and we're going to refer back to this -- is that there's something special
    going on in the Adebust algorithm。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这挺有趣的。所以他们有一些关于这个的解释，虽然它们超出了这门课程的范围，但这是像基础机器学习课程那样的主题。我不知道他们是否会涉及这个具体的主题，但这是机器学习中的一个有趣话题。提升——Adebust有这种不寻常的性质。但其中一件事告诉你的是——我们将会回顾这个——在Adebust算法中发生了一些特别的事情。
- en: Remember， Adebust is like a recipe。 What's coming up next in the second part
    of this lecture is we're going to show that Adebust is actually minimizing in
    some sense a particular loss function in a particular way。 And what we know is
    that if you just minimize a loss function for hypothesis space。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Adebust就像一个配方。接下来在这节课的第二部分，我们将展示Adebust实际上是以某种方式最小化一个特定损失函数。我们知道的是，如果你只是最小化一个假设空间的损失函数。
- en: this is not typical behavior。 So there's something special about how Adebust
    is minimizing this loss function that gives us this interesting behavior。 All
    right， let's take a break then。 See you in 10。 [sigh]， Yep。 [sigh]， Can you use
    anything？
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是典型的行为。所以Adebust在最小化这个损失函数时有一些特殊之处，这也给我们带来了这种有趣的行为。好了，我们休息一下，十分钟后见。[叹气]，好。[叹气]，你能用些什么吗？
- en: People usually use trees。 Yeah， you have to decide。 [sigh]， Not often。 [sigh]，
    [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常使用树。是的，你必须决定。[叹气]，不常见。[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。
    [叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。
- en: '[sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。'
- en: '[sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。'
- en: '[sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]。'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。'
- en: '[sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]。 [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]， [sigh]，
    [sigh]， [sigh]。'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]，[叹气]。'
- en: '![](img/ad312fe98e99c01ed03298543603234f_3.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad312fe98e99c01ed03298543603234f_3.png)'
- en: '[sigh]。'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[叹气]。'
- en: '![](img/ad312fe98e99c01ed03298543603234f_5.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad312fe98e99c01ed03298543603234f_5.png)'
- en: '![](img/ad312fe98e99c01ed03298543603234f_6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad312fe98e99c01ed03298543603234f_6.png)'
