- en: P7：p7 Let's build GPT： from scratch, in code, spelled out. - 加加zero - BV11yHXeuE9d
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P7：p7 让我们从头开始构建GPT：用代码拼写出来。- 加加zero - BV11yHXeuE9d
- en: Hi everyone。 So by now you have probably heard of chat GPT。 It has taken the
    world and AI community by storm。 And it is a system that allows you to interact
    with an AI and give it text based tasks。 So for example， we can ask chat GPT to
    write us a small haiku about how important it is that people understand AI。 And
    then they can use it to improve the world and make it more prosperous。 So when
    we run this。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。所以到现在你们可能听说过聊天GPT。它已经席卷了世界和AI社区。这是一个允许你与AI互动并给予文本任务的系统。例如，我们可以要求聊天GPT为我们写一首小的俳句，关于人们理解AI的重要性。然后，他们可以利用它来改善世界，使其更加繁荣。因此，当我们运行这个时。
- en: AI knowledge brings prosperity for all to see embraces power。 Okay， not bad。
    And so you can see that chat GPT went from left to right and generated all these
    words sequentially。 Now I asked it already the exact same prompt a little bit
    earlier and it generated a slightly different outcome。 AI's power to grow， ignoring
    sources back， learn， prosperity， weights。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: AI知识带来了可见的繁荣，拥抱了权力。好吧，不错。所以你可以看到，聊天GPT是从左到右生成了所有这些单词。现在我稍早前已经问过它完全相同的提示，它生成了略有不同的结果。AI的成长能力，忽视来源，学习，繁荣，权重。
- en: So pretty good in both cases and slightly different。 So you can see that chat
    GPT is a probabilistic system and for any one prompt it can give us multiple answers
    sort of replying to it。 Now this is just one example of a prompt。 People have
    come up with many many examples and there are entire websites that index interactions
    with chat GPT。 And so many of them are quite humorous。 Explain HTML to me like
    I'm a dog。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在两种情况下都相当不错，而且略有不同。所以你可以看到聊天GPT是一个概率系统，对于任何一个提示，它可以给我们多个回答，像是在回应它。这只是一个提示的例子。人们想出了许多许多例子，还有整整的网站索引与聊天GPT的互动。其中很多都相当幽默。解释HTML就像我是一只狗一样。
- en: Write release notes for chess to write a note about Elon Musk buying a Twitter
    and so on。 So as an example， please write a breaking news article about a leaf
    falling from a tree。 In a shocking turn of events a leaf has fallen from a tree
    in the local park。 Witnesses report that the leaf which was previously attached
    to a branch of a tree detached itself and fell to the ground。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 编写国际象棋的发布说明，写一条关于埃隆·马斯克购买Twitter的消息等等。因此，作为一个例子，请写一篇关于树叶从树上掉下来的突发新闻文章。在令人震惊的事件中，一片树叶从当地公园的一棵树上掉落。目击者报告称，这片之前附着在树枝上的树叶自行脱落并掉落到地面上。
- en: Very dramatic。 So you can see that this is a pretty remarkable system and it
    is what we call a language model。 Because it models the sequence of words or characters
    or tokens more generally and it knows how words follow each other in English language。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 非常戏剧化。所以你可以看到这是一个相当显著的系统，我们称之为语言模型。因为它对单词、字符或更一般的标记序列进行了建模，并且它知道单词在英语中是如何相互关联的。
- en: And so from its perspective what it is doing is it is completing the sequence。
    So I give it the start of a sequence and it completes the sequence with the outcome。
    And so it's a language model in that sense。 Now I would like to focus on the under
    the hood of under the hood components of what makes chat GPT work。 So what is
    the neural network under the hood that models the sequence of these words。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从它的角度来看，它正在完成序列。所以我给它一个序列的开头，它用结果完成了这个序列。因此，在这个意义上，它是一个语言模型。现在我想关注聊天GPT工作背后的组成部分。那么，是什么神经网络在幕后建模这些单词的序列呢？
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_1.png)'
- en: And that comes from this paper called Attention is All You Need in 2017 a landmark
    paper a landmark paper and AI that produced and proposed the transformer architecture。
    So GPT is short for generally generatively pre-trained transformer。 So transformer
    is the neural net that actually does all the heavy lifting under the hood。 It
    comes from this paper in 2017。 Now if you read this paper this reads like a pretty
    random machine translation paper。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这源于2017年发表的论文《注意力机制就是一切》，这是一篇具有里程碑意义的AI论文，提出了变换器架构。所以GPT是“通用生成预训练变换器”的缩写。因此，变换器是实际在幕后完成所有繁重工作的神经网络。它来自于2017年的这篇论文。如果你读这篇论文，它看起来就像一篇相当随机的机器翻译论文。
- en: And that's because I think the authors didn't fully anticipate the impact that
    the transformer would have on the field。 And this architecture that they produced
    in the context of machine translation in their case actually ended up taking over
    the rest of the data。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我认为作者们没有充分预见到变换器对这个领域的影响。而他们在机器翻译背景下产生的这种架构，最终实际上接管了剩下的数据。
- en: And so this architecture with minor changes was copy pasted into a huge amount
    of applications in AI in more recent years。 And that includes at the core of chat
    GPT。 Now we are not going to what I'd like to do now is I'd like to build out
    something like chat GPT。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这种架构经过小幅修改后被复制粘贴到了近年来大量的人工智能应用中。这其中也包括了聊天GPT的核心部分。现在我想做的就是构建类似聊天GPT的东西。
- en: But we're not going to be able to of course reproduce chat GPT。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们当然无法重现聊天GPT。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_3.png)'
- en: This is a very serious production grade system。 It is trained on a good chunk
    of internet。 And then there's a lot of pre-training and fine-tuning stages to
    it。 And so it's very complicated。 What I'd like to focus on is just to train a
    transformer based language model。 And in our case it's going to be a character
    level language model。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常严肃的生产级系统。它经过了大量的互联网数据训练，还有很多预训练和微调阶段。因此，它非常复杂。我想专注的是训练一个基于变换器的语言模型。在我们的例子中，这将是一个字符级语言模型。
- en: I still think that is a very educational with respect to how these systems work。
    So I don't want to train on the chunk of internet。 We need a smaller data。 In
    this case I propose that we work with my favorite toy data set。 It's called tiny
    Shakespeare。 And what it is is basically it's a concatenation of all of the works
    of Shakespeare in my understanding。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我仍然认为这对于了解这些系统的工作原理非常有教育意义。因此，我不想在互联网数据上训练。我们需要一个较小的数据集。在这种情况下，我提议使用我最喜欢的玩具数据集。它叫做tiny
    Shakespeare。基本上，它是所有莎士比亚作品的连接。
- en: And so this is all of Shakespeare in a single file。 This file is about one megabyte。
    And it's just all of Shakespeare。 And what we are going to do now is we're going
    to basically model how these characters follow each other。 So for example given
    a chunk of these characters like this。 given some context of characters in the
    past， the transformer neural network will look at the characters that I've highlighted。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是莎士比亚的全部内容，保存在一个文件中。这个文件大约有一兆字节，里面全是莎士比亚的作品。现在我们要做的是建模这些角色如何彼此跟随。例如，给定一段这样的字符，结合过去的字符上下文，变换器神经网络将查看我高亮的字符。
- en: And it's going to predict that G is likely to come next in the sequence。 And
    it's going to do that because we're going to train that transformer on Shakespeare。
    And it's just going to try to produce character sequences that look like this。
    And in that process is going to model all the patterns inside this data。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 它将预测下一个序列中可能出现的G。之所以能够做到这一点，是因为我们将用莎士比亚的作品来训练这个变换器。它只是尝试生成看起来像这样的字符序列。在这个过程中，它将建模这些数据中的所有模式。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_5.png)'
- en: So once we've trained the system， I'd just like to give you a preview。 we can
    generate infinite Shakespeare。 And of course it's a fake thing that looks kind
    of like Shakespeare。 Apologies for there's some jank that I'm not able to resolve
    in here。 You can see how this is going character by character。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练好系统，我想给你一个预览。我们可以生成无限的莎士比亚作品。当然，这是一种看起来有点像莎士比亚的伪作品。抱歉，有一些问题我无法解决。你可以看到这是一个字符接一个字符地进行。
- en: And it's kind of like predicting Shakespeare-like language。 So verily my lord。
    the sights have left the， again， the king coming with my curses with precious
    pale。 And then Tranios says something else， etc。 And this is just coming out of
    the transformer in a very similar manner as it would come out in Chachi P。T。 In
    our case， character by character in Chachi P。T。 it's coming out on the token by
    token level。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点像预测莎士比亚式的语言。所以，确实，我的主啊。景象离去，再次，国王带着我可贵的诅咒而来。然后，特拉尼奥说了一些别的，等等。这正是通过变换器以类似的方式生成的，像在聊天GPT中一样。在我们的例子中，字符是逐个输出的。
- en: And tokens are these sort of like little sub word pieces。 So they're not word
    level。 they're kind of like word chunk level。 And now I've already written this
    entire code to train these transformers。 And it is in a GitHub repository that
    you can find。 And it's called NanoGPT。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌就像是这些小的子词片段。所以它们不是按词级别的，而是更像是词块级别的。我已经编写了整个代码来训练这些变压器。它在一个您可以找到的 GitHub 仓库中，名为
    NanoGPT。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_7.png)'
- en: So NanoGPT is a repository that you can find on my GitHub。 And it's a repository
    for training transformers on any given text。 And what I think is interesting about
    it， because there's many ways to train transformers。 But this is a very simple
    implementation。 So it's just two files of 300 lines of code each。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: NanoGPT 是一个您可以在我的 GitHub 上找到的仓库。它是一个用于训练变压器模型的任何文本的仓库。我认为这很有趣，因为有很多方法可以训练变压器，但这是一个非常简单的实现。所以它只是两个各有
    300 行代码的文件。
- en: One file defiles the Chachi P。T。 model， the transformer。 and one file trains
    it on some given text dataset。 And here I'm showing that if you train it on an
    open web text dataset。 which is a fairly large dataset of web pages， then I reproduce
    the performance of GPT-2。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个文件定义了 Chachi P.T. 模型，变压器；一个文件在给定文本数据集上训练它。在这里，我展示了如果您在开放网页文本数据集上训练它，这是一组相当大的网页数据集，那么我就能复现
    GPT-2 的性能。
- en: So GPT-2 is an early version of OpenAI's GPT from 2017， if I recall correctly。
    And I've only so far reproduced the smallest one， 24 million parameter model。
    But basically this is just proving that the code base is correctly arranged。 And
    I'm able to load the neural network weights that OpenAI has released later。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 是 OpenAI 于 2017 年推出的早期版本，如果我没记错的话。我到目前为止只复现了最小的 2400 万参数模型。但基本上这只是证明代码库的安排是正确的。我能够加载
    OpenAI 后来发布的神经网络权重。
- en: So you can take a look at the finished code here in NanoGPT。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以您可以在 NanoGPT 中查看完成的代码。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_9.png)'
- en: What I would like to do in this lecture is I would like to basically write this
    repository from scratch。 So we're going to begin with an empty file and we're
    going to define a transformer piece by piece。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在这次讲座中基本上从零开始编写这个仓库。所以我们将从一个空文件开始，逐步定义变压器。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_11.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_11.png)'
- en: We're going to train it on the tiny Shakespeare dataset。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对微型莎士比亚数据集进行训练。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_13.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_13.png)'
- en: And we'll see how we can then generate infinite Shakespeare。 And of course this
    can copy paste to any arbitrary text dataset that you like。 But my goal really
    here is to just make you understand and appreciate how under the chat GPT works。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将看到如何生成无尽的莎士比亚文本。当然，这可以复制粘贴到您喜欢的任何任意文本数据集中。但我的目标实际上是让您理解并欣赏聊天 GPT 是如何工作的。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_15.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_15.png)'
- en: And really all that's required is a proficiency in Python and some basic understanding
    of calculus and statistics。 And it would help if you also see my previous videos
    on the same YouTube channel in particular。 my Make More series where I define
    smaller and simpler neural network language models。 So multi-layer perceptrons
    and so on。 It really introduces the language model and framework。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其实只需要具备 Python 的熟练程度，以及一些基本的微积分和统计学知识。如果您还看过我在同一 YouTube 频道上的早期视频，特别是我的“制作更多”系列，其中定义了更小更简单的神经网络语言模型，那会更有帮助。所以多层感知机等。这真的介绍了语言模型和框架。
- en: And then here in this video we're going to focus on the transformer neural network
    itself。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这个视频中，我们将专注于变压器神经网络本身。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_17.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_17.png)'
- en: Okay， so I created a new Google collab Jupyter notebook here。 And this will
    allow me to later easily share this code that we're going to develop together
    with you so you can follow along。 So this will be in a video description later。
    Now here I've just done some preliminarys。 I downloaded the dataset the tiny Shakespeare
    dataset at this URL and you can see that it's about a 1 megabyte file。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我在这里创建了一个新的 Google collab Jupyter notebook。这将让我方便地与您分享我们将共同开发的代码，以便您可以跟着一起学习。这将在稍后的视频描述中提供。现在我已经做了一些初步工作。我在这个网址下载了微型莎士比亚数据集，您可以看到它大约是一个
    1 兆字节的文件。
- en: Then here I open the input。txt file and just reading all the text at a string。
    And we see that we are working with 1 million characters roughly。 And the first
    1000 characters if we just print them out are basically what you would expect。
    This is the first 1000 characters of the tiny Shakespeare dataset roughly up to
    here。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我打开`input.txt`文件，读取所有文本为一个字符串。我们看到大约处理了100万个字符。如果我们打印出前1000个字符，基本上是你所期望的。这是微型莎士比亚数据集的大约前1000个字符。
- en: So far so good。 Next we're going to take this text and the text is a sequence
    of characters in Python。 So when I call the set constructor on it， I'm just going
    to get the set of all the characters that occur in this text。 And then I call
    list on that to create a list of those characters instead of just a set so that
    I have an ordering。 an arbitrary ordering。 And then I sort that。 So basically
    we get just all the characters that occur in the entire dataset and they're sorted。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止都很好。接下来，我们将这段文本视为Python中的字符序列。当我在其上调用`set`构造函数时，我将获得文本中出现的所有字符的集合。然后我调用`list`来创建这些字符的列表，而不是仅仅得到一个集合，这样我就有了一个排序，任意排序。然后我进行排序。因此，我们得到了整个数据集中出现的所有字符，并且它们是有序的。
- en: Now the number of them is going to be our vocabulary size。 These are the possible
    elements of our sequences。 And we see that when I print here the characters。 there's
    65 of them in total。 There's a space character and then all kinds of special characters
    and then capitals and lowercase letters。 So that's our vocabulary and that's the
    sort of like possible characters that the model can see or emit。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它们的数量将是我们的词汇量。这些是我们序列中可能的元素。当我在这里打印字符时，总共有65个字符，包括一个空格字符和各种特殊字符，以及大写和小写字母。这就是我们的词汇量，也就是模型可以看到或输出的可能字符。
- en: Okay so next we would like to develop some strategy to tokenize the input text。
    Now when people say tokenize they mean convert the raw text as a string to some
    sequence of integers according to some vocabulary of possible elements。 So as
    an example here we are going to be building a character level language model so
    we're simply going to be translating individual characters into integers。 So let
    me show you a chunk of code that sort of does that for us。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来我们想开发一些策略来分词输入文本。当人们说分词时，他们的意思是将原始文本作为字符串转换为某些可能元素的整数序列。因此，在这里作为示例，我们将构建一个字符级语言模型，所以我们简单地将单个字符翻译为整数。让我给你展示一段代码，能为我们做到这一点。
- en: So we're building both the encoder and the decoder and let me just talk through
    what's happening here。 When we encode an arbitrary text like hi there we're going
    to receive a list of integers that represents that string。 So for example 46 47
    etc。 And then we also have the reverse mapping so we can take this list and decode
    it to get back the exact same string。 So it's really just like a translation to
    integers and back for arbitrary string and for us it is done on a character level。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建编码器和解码器，让我来讲讲这里发生的事情。当我们编码一个任意文本（比如“hi there”）时，我们将收到一个表示该字符串的整数列表。例如46、47等。然后我们还有反向映射，因此我们可以将这个列表解码以获取完全相同的字符串。所以这实际上就是将任意字符串转换为整数然后再转换回来的过程，而对于我们来说，是在字符级别进行的。
- en: Now the way this was achieved is we just iterate over all the characters here
    and create a lookup table from the character to the integer and vice versa。 And
    then to encode some string we simply translate all the characters individually
    and to decode it back we use the reverse mapping and concatenate all of it。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在实现这一点的方法是我们遍历所有字符，并创建一个从字符到整数的查找表，反之亦然。然后，为了编码某个字符串，我们简单地逐个翻译所有字符，解码时使用反向映射并将其连接起来。
- en: Now this is only one of many possible encodings or many possible sort of tokenizers
    and it's a very simple one。 But there's many other schemas that people have come
    up with in practice。 So for example Google uses a sentence piece so sentence piece
    will also encode text into integers but in a different schema and using a different
    vocabulary。 And sentence piece is a subword sort of tokenizer and what that means
    is that you're not encoding entire words but you're not also encoding individual
    characters。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这只是许多可能编码或许多可能分词器中的一种，且它非常简单。但在实践中，人们提出了许多其他方案。例如，谷歌使用的是句子片段（sentence piece），它也将文本编码为整数，但使用不同的方案和不同的词汇量。句子片段是一种子词分词器，这意味着你不是编码整个单词，也不是单独编码每个字符。
- en: It's a subword unit level and that's usually what's adopted in practice。 For
    example also OpenAI has this library called tick token that uses a byte pair encoding
    tokenizer and that's what GPT uses。 And you can also just encode words into like
    hell world into lists of integers。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个子词单元级别，通常在实践中被采用。例如，OpenAI也有一个名为tick token的库，使用字节对编码分词器，而这就是GPT所使用的。你也可以将单词编码为像“地狱世界”这样的整数列表。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_19.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_19.png)'
- en: So as an example I'm using the tick token library here。 I'm getting the encoding
    from GPT 2 or that was used for GPT 2。 Instead of just having 65 possible characters
    or tokens they have 50，000 tokens。 And so when they encode the exact same string
    high there we only get a list of three integers。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我在这里使用tick token库。我从GPT 2获取编码，或者说是用于GPT 2的编码。它们不是只有65个可能的字符或标记，而是有50,000个标记。因此，当它们对同一个字符串“高”进行编码时，我们仅得到三个整数的列表。
- en: But those integers are not between 0 and 64。 They are between 0 and 5，000 50，000
    256。 So basically you can trade off the codebook size and the sequence lengths。
    So you can have very long sequences of integers with very small vocabularies or
    you can have short sequences of integers with very large vocabularies。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这些整数并不在0到64之间。它们在0到50,000之间，256。因此，基本上，你可以权衡代码本的大小和序列长度。你可以拥有非常小的词汇量的非常长的整数序列，或者拥有非常大的词汇量的短整数序列。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_21.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_21.png)'
- en: And so typically people use in practice these subword encodings but I'd like
    to keep our tokenizer very simple。 So we're using character level tokenizer and
    that means that we have very small codebooks。 We have very simple encode and decode
    functions but we do get very long sequences as a result。 But that's the level
    at which we're going to stick with this lecture because it's the simplest thing。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常人们在实践中使用这些子词编码，但我想保持我们的分词器非常简单。因此，我们使用字符级分词器，这意味着我们有非常小的代码本。我们有非常简单的编码和解码函数，但因此我们确实得到非常长的序列。但这是我们在本讲座中坚持的级别，因为这是最简单的做法。
- en: So now that we have an encoder and a decoder effectively tokenizer we can tokenize
    the entire training set of Shakespeare。 So here's a chunk of code that does that。
    And I'm going to start to use the PyTorch library and specifically the Torque。Tensor
    from the PyTorch library。 So we're going to take all of the text in tiny Shakespeare。
    encode it and then wrap it into a Torque。Tensor to get the data tensor。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有效地拥有一个编码器和解码器，也就是分词器，可以对整个莎士比亚训练集进行分词。因此，这里有一段代码可以做到这一点。我将开始使用PyTorch库，特别是PyTorch库中的Torque.Tensor。我们将把所有文本编码，并将其包装成Torque.Tensor，以获取数据张量。
- en: So here's what the data tensor looks like when I look at just the first 1000
    characters or the 1000 elements of it。 So we see that we have a massive sequence
    of integers and this sequence of integers here is basically an identical translation
    of the first 1000 characters here。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我查看前1000个字符或1000个元素时，数据张量看起来是这样的。因此，我们看到有一个庞大的整数序列，这个整数序列基本上是前1000个字符的完全对应翻译。
- en: So I believe for example that zero is a new line character and maybe one is
    a space， not 100% sure。 But from now on the entire data set of text is re-represented
    as just stretched out as a single very large sequence of integers。 Let me do one
    more thing before we move on here。 I'd like to separate out our data set into
    a train and a validation split。 So in particular we're going to take the first
    90% of the data set and consider that to be the training data for the transformer。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信，例如，零是一个换行符，而一可能是一个空格，虽然不100%确定。但从现在开始，整个文本数据集将重新表示为一个拉伸成非常长的整数序列。在继续之前，让我再做一件事。我想将数据集分为训练和验证集。具体来说，我们将取前90%的数据集，将其视为变换器的训练数据。
- en: And we're going to withhold the last 10% at the end of it to be the validation
    data。 And this will help us understand to what extent our model is overfitting。
    So we're going to basically hide and keep the validation data on the side because
    we don't want just a perfect memorization of this exact Shakespeare。 We want a
    neural network that sort of creates Shakespeare-like text。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留最后10%的数据作为验证数据。这将帮助我们理解模型的过拟合程度。因此，我们基本上会隐藏并保留验证数据，因为我们不想仅仅完美记忆这段莎士比亚文本。我们希望神经网络能够生成莎士比亚风格的文本。
- en: And so it should be fairly likely for it to produce the actual like stowed away
    true Shakespeare text。 And so we're going to use this to get a sense of the overfitting。
    Okay。 so now we would like to start plugging these text sequences or integer sequences
    into the transformer so that it can train and learn those patterns。 Now the important
    thing to realize is we're never going to actually feed entire text into your transformer
    all at once。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它产生真实的、被隐藏的真正的莎士比亚文本的可能性应该相当大。因此我们将用这个来了解过拟合。好的，现在我们想开始将这些文本序列或整数序列插入变压器，以便它可以训练并学习这些模式。现在重要的是要意识到，我们永远不会一次性将整个文本输入变压器。
- en: That would be computationally very expensive and prohibitive。 So when we actually
    train a transformer on a lot of these data sets。 we only work with chunks of the
    data set。 And when we train the transformer。 we basically sample random little
    chunks out of the training set and train them just chunks at a time。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这在计算上非常昂贵且不可行。因此，当我们实际上在许多这些数据集上训练变压器时，我们只处理数据集的一小部分。当我们训练变压器时，我们基本上是从训练集中随机抽样小块，并一次只训练这些块。
- en: And these chunks have basically some kind of a length and some maximum length。
    Now the maximum length typically， at least in the code I usually write， is called
    block size。 You can find it on the different names like context length or something
    like that。 Let's start with the block size of just eight。 And let me look at the
    first train data characters。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块基本上有某种长度和最大长度。通常在我写的代码中，最大长度被称为块大小。你可以在不同的名称中找到，比如上下文长度之类的。我们先从块大小为八开始。让我看一下第一个训练数据字符。
- en: The first block size plus one characters。 I'll explain why plus one in a second。
    So this is the first nine characters in the sequence in the training set。 Now
    what I'd like to point out is that when you sample a chunk of data like this。
    so say if these nine characters out of the training set。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个块的大小加一个字符。稍后我会解释为什么加一。所以这是训练集中序列中的前九个字符。我要指出的是，当你像这样抽样一段数据时，比如说这九个字符来自训练集。
- en: this actually has multiple examples packed into it。 And that's because all of
    these characters follow each other。 And so what this thing is going to say when
    we plug it into a transformer。 is we're going to actually simultaneously train
    it to make prediction at every one of these positions。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上包含多个例子。因为这些字符彼此相连。所以当我们将其插入变压器时，它将同时训练以在每个位置进行预测。
- en: Now in a chunk of nine characters， there's actually eight individual examples
    packed in there。 So there's the example that when 18， in the context of 18， 47
    luckily comes next。 In the context of 18 and 47， 56 comes next。 And so on。 So
    that's the eight individual examples。 Let me actually spell it out with code。
    So here's a chunk of code to illustrate。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在一段九个字符中，实际上有八个单独的例子被打包在其中。所以在18的上下文中，47幸运地接下来。在18和47的上下文中，56接下来。依此类推。这就是这八个单独的例子。让我用代码来具体说明。
- en: X are the inputs to the transformer。 It will just be the first block size characters。
    Y will be the next block size characters。 So it's offset by one。 And that's because
    Y are the targets for each position in the input。 And then here I'm iterating
    over all the blocks as a weight。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: X是变压器的输入。它将只是第一个块大小的字符。Y将是下一个块大小的字符。因此它偏移了一位。这是因为Y是输入中每个位置的目标。然后在这里，我正在遍历所有的块作为权重。
- en: And the context is always all the characters in X up to T and including T。 And
    the target is always the teeth character， but in the targets array Y。 So let me
    just run this。 And basically it spells out what I said in words。 These are the
    eight examples hidden in a chunk of nine characters that we sampled from the two
    characters。 And we sampled from the training set。 I want to mention one more thing。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文始终是X中的所有字符直到T并包括T。目标始终是目标数组Y中的目标字符。所以让我运行这个。基本上，它说明了我用文字所说的内容。这些是我们从两个字符中抽样得到的九个字符块中隐藏的八个例子。我要提到一件事。
- en: We train on all the eight examples here with context between one all the way
    up to context of block size。 And we train on that not just for computational reasons
    because we happen to have the sequence already or something like that。 It's not
    just for efficiency。 It's also done to make the transformer network be used to
    seeing contexts all the way from as little as one all the way to block size。 And
    we'd like the transformer to be used to seeing everything in between。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里对所有八个示例进行训练，上下文从一个到块大小之间。我们进行这样的训练不仅是出于计算原因，因为我们恰好已经有了序列或类似的情况。这不仅仅是为了效率，也是为了让变换器网络习惯于看到从一个到块大小的上下文，并希望变换器能习惯于看到之间的所有内容。
- en: And that's going to be useful later during inference because while we're sampling。
    we can start to sample in generation with as little as one character of context。
    And then transformer knows how to predict the next character with all the way
    up to just context of one。 And so then it can predict everything up to block size。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这在推断时会很有用，因为在我们采样的同时，我们可以在生成时仅用一个字符的上下文开始采样。然后变换器知道如何预测下一个字符，最多可以使用一个字符的上下文。因此，它可以预测到块大小之前的所有内容。
- en: And after block size we have to start truncating because the transformer will
    never receive more than block size inputs when it's predicting the next character。
    Okay， so we've looked at the time dimension of the tensors that are going to be
    feeding into the transformer。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在块大小之后，我们必须开始截断，因为变换器在预测下一个字符时永远不会接收到超过块大小的输入。好的，我们已经看过了将馈送到变换器的张量的时间维度。
- en: There's one more dimension to care about and that is the batch dimension。 And
    so as we're sampling these chunks of text， we're going to be actually。 every time
    we're going to feed them into a transformer。 we're going to have many batches
    of multiple chunks of text that are always like stacked up in a single tensor。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个需要关注的维度，那就是批次维度。因此，当我们对这些文本块进行采样时，我们实际上会在每次将它们输入到变换器时，有许多批次的多个文本块始终堆叠在一个张量中。
- en: And that's just done for efficiency just so that we can keep the GPU busy because
    they are very good at parallel processing of data。 And so we just want to process
    multiple chunks all at the same time。 But those chunks are processed completely
    independently。 They don't talk to each other and so on。 So let me basically just
    generalize this and introduce a batch dimension。 Here's a chunk of code。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要是出于效率考虑，以便我们能让GPU保持忙碌，因为它们在数据的并行处理方面非常出色。因此，我们希望同时处理多个块。这些块是完全独立处理的，它们之间不进行交流。所以让我基本上概括一下并引入一个批次维度。这是一段代码。
- en: Let me just run it and then I'm going to explain what it does。 So here because
    we're going to start sampling random locations in the data sets to pull chunks
    from。 we're going to be setting the seed so that in the random number generator。
    so that the numbers I see here are going to be the same numbers you see later
    if you try to reproduce this。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我运行一下，然后我将解释它的功能。因为我们将开始在数据集中的随机位置进行采样以提取块，所以我们会设置种子，以便在随机数生成器中，这里看到的数字和你稍后尝试重现时看到的数字是相同的。
- en: Now the batch size here is how many independent sequences we are processing
    every forward。 backward pass of the transformer。 The block size as I explained
    is the maximum context length to make those predictions。 So let's say by size
    four， block size eight， and then here's how we get batch for any arbitrary split。
    If the split is a training split， then we're going to look at train data， otherwise
    a value。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这里的批次大小是我们每次前向和反向传播变换器时处理的独立序列的数量。正如我所解释的，块大小是进行这些预测的最大上下文长度。假设批次大小为四，块大小为八，下面是我们如何获取任何任意划分的批次。如果划分是训练划分，那么我们将查看训练数据，否则是一个值。
- en: That gives us the data array。 And then when I generate random positions to grab
    a chunk out of。 I actually grab， I actually generate batch size number of random
    offsets。 So because this is for。 we are， I X is going to be a four numbers that
    are randomly generated between zero and len of data minus block size。 So it's
    just random offsets into the training set。 And then X is， as I explained。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了数据数组。然后，当我生成随机位置以抓取一个块时，实际上我会生成与批次大小相同数量的随机偏移量。因此，因为这是为了，我们将生成四个在零到数据长度减去块大小之间的随机数字。所以这只是训练集中的随机偏移量。然后X正如我所解释的。
- en: are the first block size characters starting at I。 The Y's are the offset by
    one of that。 So just add plus one。 And then we're going to get those chunks for
    every one of integers I in I X and use a torch。stack to take all those one dimensional
    tensors as we saw here。 And we're going to stack them up at rows。 And so they
    all become a row in a four by eight tensor。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 是从 I 开始的前块大小字符。Y 是其偏移量加一。所以只需加一。然后我们将获取每一个整数 I 在 I X 中的那些块，并使用 torch.stack 将这些一维张量叠加，就像我们在这里看到的。然后我们将它们堆叠成行。因此，它们都成为四乘八张量中的一行。
- en: So here's where I'm printing them。 When I sample a batch XB and YB。 the inputs
    that transformer now are the input X is the four by eight tensor for rows of eight
    columns。 And each one of these is a chunk of the training set。 And then the targets
    here are in the associated array Y and they will come in to the transformer all
    the way at the end to create the loss function。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我打印它们的地方。当我抽样一个批次 XB 和 YB 时，输入变换器现在是输入 X，一个四乘八的张量，表示八列的行。每个张量都是训练集的一部分。目标则在关联数组
    Y 中，它们会在最后传入变换器，以创建损失函数。
- en: So they will give us the correct answer for every single position inside X。
    And then these are the four independent rows。 So spelled out as we did before。
    this four by eight array contains a total of 32 examples。 And they're completely
    independent as far as the transformer is concerned。 So when the input is 24。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它们会给我们每个位置 X 中的正确答案。这四个是独立的行。就像我们之前所说的，这个四乘八的数组总共有 32 个示例。就变换器而言，它们是完全独立的。因此，当输入是
    24 时，
- en: the target is 43 or rather 43 here in the Y array。 When the input is 2443， the
    target is 58。 Or like when it is 52， 58， one， the target is 58。 So you can sort
    of see this spelled out。 These are the 32 independent examples packed in to a
    single batch of the input X。 And then the desired targets are in Y。 And so now
    this integer tensor of X is going to feed into the transformer。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是 43，或者说在 Y 数组中的确是 43。当输入是 2443 时，目标是 58。或者像当它是 52，58，1 时，目标是 58。你可以看到这些是明确的。这
    32 个独立的示例被打包成输入 X 的一个批次。然后所需的目标在 Y 中。因此，现在这个整数张量 X 将传入变换器。
- en: And that transformer is going to simultaneously process all these examples and
    then look up the correct integers to predict in every one of these positions in
    the tensor Y。 Okay， so now that we have our batch of input that we'd like to feed
    into a transformer。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变换器将同时处理所有这些示例，然后查找在张量 Y 的每一个位置中预测的正确整数。好的，现在我们有了想要输入到变换器中的输入批次。
- en: let's start basically feeding this into neural networks。 Now we're going to
    start off with the simplest possible neural network。 which in the case of language
    modeling in my opinion is the Bagram language model。 And we've covered the Bagram
    language model in my make more series in a lot of depth。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基本上开始将这些输入到神经网络中。现在我们将从最简单的神经网络开始。在我看来，语言建模的情况下，最简单的就是 Bagram 语言模型。我们在我的《深入学习》系列中已经详细讨论了
    Bagram 语言模型。
- en: And so here I'm going to sort of go faster and let's just implement PyTorch
    module directly that implements the PyGram language model。 So I'm importing the
    PyTorch and N module for reproducibility。 And then here I'm constructing a Bagram
    language model， which is a subclass of an N module。 And then I'm calling it and
    I'm passing in the inputs and the targets。 And I'm just printing。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我将加快速度，让我们直接实现一个 PyTorch 模块来实现 PyGram 语言模型。我正在导入 PyTorch 和 N 模块以确保可重复性。然后在这里我构建一个
    Bagram 语言模型，这是 N 模块的一个子类。然后我调用它，传入输入和目标。我只是在打印。
- en: Now when the inputs and targets come here， you see that I'm just taking the
    index。 the inputs X here， which I renamed to IDX。 And I'm just passing them into
    this token embedding table。 So what's going on here is that here in the constructor，
    we are creating a token embedding table。 And it is of size vocab size by vocab
    size。 And we're using an N dot embedding。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当输入和目标到达时，你会看到我只是获取索引。输入 X 在这里，我重新命名为 IDX。我只是将它们传入这个令牌嵌入表中。那么这里发生的事情是，在构造函数中，我们正在创建一个令牌嵌入表。它的大小是词汇表大小乘以词汇表大小。我们正在使用
    N.dot.embedding。
- en: which is a very thin wrapper around basically a tensor of shape vocab size by
    vocab size。 And what's happening here is that when we pass IDX here。 every single
    integer in our input is going to refer to this embedding table。 And it's going
    to pluck out a row of that embedding table corresponding to its index。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个非常简单的包装，基本上是一个形状为词汇大小乘词汇大小的张量。当我们在这里传递IDX时，我们输入的每个整数将引用这个嵌入表。它将提取出与其索引对应的嵌入表的一行。
- en: So 24 here will go to the embedding table and will pluck out the 24th row。 And
    then 43 will go here and pluck out the 43rd row， et cetera。 And then PyTorch is
    going to arrange all of this into a batch by time by channel tensor。 In this case
    batch is four， time is eight， and C， which is the channels is vocab size or 65。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的24将会到嵌入表中，提取出第24行。然后43会到这里，提取出第43行，等等。然后PyTorch将把所有这些安排成一个按批次、时间和通道的张量。在这个例子中，批次是四，时间是八，C，也就是通道，是词汇大小或65。
- en: And so we're just going to pluck out all those rows， arrange them in a B by
    T by C。 And now we're going to interpret this as the logits。 which are basically
    the scores for the next character in a sequence。 And so what's happening here
    is we are predicting what comes next based on just the individual identity of
    a single token。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将提取所有这些行，并将它们排列成一个B乘T乘C的格式。现在我们将把这些解释为logits，这基本上是序列中下一个字符的分数。因此，这里发生的事情是我们根据单个token的个体身份来预测接下来会发生什么。
- en: And you can do that because， I mean， currently the tokens are not talking to
    each other。 and they're not seeing any context， except for their just， seeing
    themselves。 So I'm a token number five。 And then I can actually make pretty decent
    predictions about what comes next just by knowing that I'm token five。 because
    some characters no， follow other characters in technical scenarios。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样做，因为目前这些tokens彼此之间没有交流。它们没有看到任何上下文，除了它们自己。所以我是token编号五。然后我实际上可以仅凭知道自己是token五，就做出相当不错的预测，因为某些字符在技术场景中确实会跟随其他字符。
- en: So we saw a lot of this in a lot more depth in the Make More series。 And here
    if I just run this。 then we currently get the predictions， the scores。 the logits
    for every one of the four by eight positions。 Now that we've made predictions
    about what comes next， we'd like to evaluate the loss function。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在“Make More”系列中看到了这一点的更深入的内容。如果我现在运行这个，我们就可以得到预测、分数以及每一个四乘八位置的logits。现在我们已经对接下来会发生什么做出了预测，我们希望评估损失函数。
- en: And so in Make More series， we saw that a good way to measure a loss or like
    a quality of the predictions is to use the negative log likelihood loss。 which
    is also implemented in PyTorch under the name cross entropy。 So what we'd like
    to do here is loss is the cross entropy on the predictions and the targets。 And
    so this measures the quality of the logits with respect to the targets。 In other
    words。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Make More”系列中，我们看到测量损失或预测质量的一个好方法是使用负对数似然损失，它在PyTorch中也以交叉熵的名义实现。因此我们在这里想要做的是，损失是对预测和目标的交叉熵。这测量了logits与目标的质量。换句话说。
- en: we have the identity of the next character。 So how well are we predicting the
    next character based on the logits？
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有下一个字符的身份。那么基于logits，我们预测下一个字符的效果如何？
- en: And intuitively， the correct the dimension of logits， depending on whatever
    the target is。 should have a very high number。 And all the other dimensions should
    be very low number。 Now。 the issue is that this won't actually， this is what we
    want。 We want to basically output the logits and the loss。 This is what we want，
    but unfortunately。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，logits的正确维度，取决于目标，应该有一个很高的数字。而所有其他维度应该是很低的数字。现在，问题是这实际上不会运行，这是我们想要的。我们基本上想输出logits和损失。这就是我们想要的，但不幸的是。
- en: this won't actually run。 We get an error message， but intuitively we want to
    measure this。 Now when we go to the PyTorch cross entropy documentation here。
    we're trying to call the cross entropy in its functional form。 So that means we
    don't have to create like a module for it。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上不会运行。我们会收到错误信息，但直观上我们想要测量这个。现在当我们查看PyTorch的交叉熵文档时，我们试图以功能形式调用交叉熵。这意味着我们不必为此创建一个模块。
- en: But here when we go to the documentation， you have to look into the details
    of how PyTorch expects these inputs。 And basically the issue here is PyTorch expects
    if you have multi-dimensional input， which we do。 because we have a B by T by
    C tensor， then it actually really wants the channels to be the second dimension
    here。 So basically it wants a B by C by T instead of a B by T by C。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但在查看文档时，你需要关注 PyTorch 如何期望这些输入。基本上，这里存在的问题是，PyTorch 期望如果你有多维输入（我们确实有），因为我们有
    B 乘以 T 乘以 C 的张量，它实际上希望通道是这里的第二维度。因此，它想要的是 B 乘以 C 乘以 T，而不是 B 乘以 T 乘以 C。
- en: And so it's just the details of how PyTorch treats these kinds of inputs。 And
    so we don't actually want to deal with that。 So what we're going to do instead
    is we need to basically reshape our logits。 So here's what I like to do。 I like
    to take basically give names to the dimensions。 So logis。shape is B by T by C
    and unpack those numbers。 And then let's say that logits equals logis。view。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是 PyTorch 如何处理这些输入的细节。因此，我们实际上不想处理这个问题。我们要做的是基本上重新调整我们的 logits。因此这是我喜欢做的。我喜欢给维度命名。所以
    logits.shape 是 B 乘以 T 乘以 C，并解包这些数字。然后我们可以说 logits 等于 logits.view。
- en: And we want it to be B times C。 B times T by C。 So just a two-dimensional array。
    So we're going to take all of these positions here。 And we're going to stretch
    them out in a one-dimensional sequence and preserve the channel dimension as the
    second dimension。 So we're just kind of like stretching out the array so it's
    two-dimensional。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望它是 B 乘以 C。 B 乘以 T 由 C 计算。所以仅仅是一个二维数组。我们将把所有这些位置展开成一维序列，并将通道维度作为第二维度保留。所以我们就像是将数组拉伸成二维的。
- en: And in that case it's going to better conform to what PyTorch sort of expects
    in its dimensions。 Now we have to do the same two targets。 Because currently targets
    are of shape B by T and we want it to be just B times T。 So one-dimensional。 Now
    alternatively you could always still just do minus one because PyTorch will guess
    what this should be if you want to lay it out。 But let me just be explicit and
    say pew times T。 Once we reshape this it will match the cross entropy case。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，它将更好地符合 PyTorch 对其维度的期望。现在我们必须对目标做同样的事情。因为当前目标的形状是 B 乘以 T，而我们希望它仅为 B
    乘以 T。所以是一维的。或者你也可以一直使用 -1，因为如果你想布局，PyTorch 会猜测这应该是什么。但让我明确地说 pew 乘以 T。一旦我们重新调整，这将与交叉熵情况匹配。
- en: And then we should be able to evaluate our loss。 Okay so at that right now and
    we can do loss。 And so currently we see that the loss is 4。87。 Now because our
    we have 65 possible vocabulary elements we can actually guess at what the loss
    should be。 And in particular we covered negative log likelihood in a lot of detail。
    We are expecting log or long of one over 65 and negative of that。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应该能够评估我们的损失。好的，现在可以进行损失评估。现在我们看到损失是 4.87。由于我们有 65 个可能的词汇元素，我们实际上可以猜测损失应该是多少。特别是我们已经详细讨论了负对数似然。我们期望的是对
    1 除以 65 取对数，并对此取负。
- en: So we are expecting the loss to be about 4。17。 But we are getting 4。87。 And
    so that's telling us that the initial predictions are not super diffuse。 They've
    got a little bit of entropy and so we are guessing wrong。 So yes but actually
    we are able to evaluate the loss。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们期望损失大约为 4.17。但我们得到的是 4.87。这告诉我们初始预测并不是非常分散。它们有一点熵，因此我们猜测错误。所以是的，但实际上我们能够评估损失。
- en: Okay so now that we can evaluate the quality of the model on some data。 We would
    like to also be able to generate from the model。 So let's do the generation。 Now
    I am going to go again a little bit faster here because I covered all of this
    already in previous videos。 So here is a generate function for the model。 So we
    take the same kind of input idx here。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们可以在一些数据上评估模型的质量。我们还希望能够从模型生成内容。所以让我们进行生成。现在我将稍微快一点，因为我在之前的视频中已经讲过了这些内容。这里是模型的生成函数。因此我们在这里使用相同类型的输入
    idx。
- en: And basically this is the current context of some characters in a batch in some
    batch。 So it's also b by t and the job of generate is to basically take this b
    by t and extend it to b by t plus 1 plus 2 plus 3。 And so it's just basically
    it continues the generation in all the batch dimensions in the time dimension。
    So that's its job。 And we will do that for max new tokens。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这是某些字符在批次中的当前上下文。因此，它也是 B 乘以 T，生成的任务就是将这个 B 乘以 T 扩展到 B 乘以 T 加 1 加 2 加 3。所以它基本上是在时间维度中继续生成所有批次维度。因此这是它的工作。我们将为最大新标记这样做。
- en: So you can see here on the bottom there is going to be some stuff here。 But
    on the bottom whatever is predicted is concatenated on top of the previous idx。
    Along the first dimension which is the time dimension to create a b by t plus
    1。 So that becomes a new idx。 So the job of generate is to take a b by t and make
    it a b by t plus 1 plus 2 plus 3。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到底部会有一些东西。但是在底部，无论预测是什么，都会与之前的idx连接。在时间维度的第一维上创建一个b by t加1。这就成了一个新的idx。因此，generate的工作就是将b
    by t变为b by t加1加2加3。
- en: As many as we want max new tokens。 So this is the generation from the model。
    Now inside the generation what are we doing？ We're taking the current indices。
    We're getting the predictions。 So we get those are in the logits。 And then the
    loss here is going to be ignored because we're not using that。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想要的最大新令牌数量。这是模型的生成过程。现在在生成过程中我们在做什么？我们正在获取当前索引，得到预测。这些在logits中。然后这里的损失将被忽略，因为我们不使用它。
- en: And we have no targets that are sort of ground truth targets that we're going
    to be comparing with。 Then once we get the logits we are only focusing on the
    last step。 So instead of a b by t by c we're going to pluck out the negative one
    the last element in the time dimension。 Because those are the predictions for
    what comes next。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有可以作为对比的真实目标。一旦我们得到了logits，我们只关注最后一步。所以我们将从时间维度中提取最后一个元素，而不是b by t by c。因为这些是对下一个内容的预测。
- en: So that gives us the logits which we then cover to probabilities via softmax。
    And then we use towards that multinomial to sample from those probabilities。 And
    we ask PyTorch to give us one sample。 And so idx next will become a b by one。
    Because in each one of the batch dimensions we're going to have a single prediction
    for what comes next。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了logits，我们通过softmax将其转换为概率。然后我们使用多项分布从这些概率中进行采样。我们请求PyTorch给我们一个样本。因此idx
    next将变成b by 1。因为在每个批次维度中，我们将对下一个内容有一个单一的预测。
- en: So this non-samples equals one will make this be a one。 And then we're going
    to take those integers that come from the sampling process according to the probability
    distribution given here。 And those integers got just concatenated on top of the
    current sort of like running stream of integers。 And this gives us a b by t plus
    one。 And then we can return that。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个non-samples等于1将使这个变为1。然后我们将根据给定的概率分布，从采样过程中得到的整数取出，并将这些整数连接在当前的整数流之上。这给我们提供了b
    by t加1。然后我们可以返回这个结果。
- en: Now one thing here is you see how I'm calling self of idx which will end up
    going to the forward function。 I'm not providing any targets。 So currently this
    would give an error because targets is sort of not given。 So targets has to be
    optional。 So targets is none by default。 And then if targets is none then there's
    no loss to create。 So it's just loss is none。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点，你看到我如何调用self的idx，这将最终进入forward函数。我没有提供任何目标。因此，目前这将产生一个错误，因为目标没有提供。所以目标必须是可选的。默认情况下，目标是none。如果目标是none，那么就没有损失可创建。所以损失就是none。
- en: But else all of this happens and we can create a loss。 So this will make it
    so if we have the targets we provide them and get a loss。 If we have no targets
    we'll just get the logits。 So this here will generate from the model。 And let's
    take that for a ride now。 Oops。 So I have another coach on here which will generate
    for the model from the model。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但其他所有事情发生后，我们可以创建一个损失。所以如果我们有目标，我们提供它们并得到一个损失。如果没有目标，我们只会得到logits。因此，这里将从模型生成。现在让我们试试这个。哦，我这里还有另一个教练，它将从模型生成。
- en: And okay this is kind of crazy so maybe let me let you break this down。 So these
    are the idx right。 I'm creating a batch will be just one time will be just one。
    So I'm creating a little one by one tensor and it's holding a zero。 And the dtype
    the data type is integer。 So zero is going to be how we kick off the generation。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这有点疯狂，或许让我来帮你理清一下。这些就是idx，对吧？我创建的批次将只有一次，即1。所以我创建了一个1 by 1的张量，并且它保存了一个零。数据类型是整数。所以零将是我们启动生成的方式。
- en: And remember that zero is the element standing for a new line character。 So
    it's kind of like a reasonable thing to feed in as the very first character in
    a sequence to be the new line。 So it's going to be idx which we're going to feed
    in here。 Then we're going to ask for 100 tokens。 And then end that generate will
    continue that。 Now because generate works on the level of batches we then have
    to index into the zero throw to basically unplug the single batch dimension that
    exists。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，零是表示换行符的元素。因此，将其作为序列中的第一个字符输入是合理的。所以它将是我们将在这里输入的idx。然后我们将请求100个令牌。然后生成将继续这一过程。现在，由于生成是在批量级别上工作的，我们必须索引到零维度，以基本上拔掉唯一存在的批量维度。
- en: And then that gives us a time steps is just a one dimensional array of all the
    indices which we will convert to simple Python list。 So we're going to go to the
    root function。 And then we're going to go to the root function。 And then we're
    going to go to the root function。 And then we're going to go to the root function。
    And then we're going to go to the root function。 And then we're going to go to
    the root function。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这给我们时间步长，实际上是一个一维数组，包含所有索引，我们将其转换为简单的Python列表。所以我们要去根函数。然后我们要去根函数。然后我们要去根函数。然后我们要去根函数。然后我们要去根函数。然后我们要去根函数。
- en: And then we're going to go to the root function。 And then we're going to go
    to the root function。 And then we're going to go to the root function。 This function
    is written to be general but it's kind of like ridiculous right now because we're
    feeding in all this we're building out this context and we're concatenating it
    all。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要去根函数。然后我们要去根函数。然后我们要去根函数。这个函数被编写得比较通用，但现在有点荒谬，因为我们输入了所有这些，我们正在构建这个上下文并将它们连接在一起。
- en: And we're always feeding it all into the model。 But that's kind of ridiculous
    because this is just a simple by-ground model。 So to make for example this prediction
    about K we only needed this W。 But actually what we fed into the model is we fed
    the entire sequence。 And then we only looked at the very last piece and predicted
    K。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是将它全部输入模型。但这有点荒谬，因为这只是一个简单的背景模型。所以例如，要对K进行预测，我们只需要这个W。但实际上我们输入到模型中的是整个序列。然后我们只查看最后一部分并预测K。
- en: So the only reason I'm writing it in this way is because right now this is a
    by-ground model。 But I'd like to keep this function fixed and I'd like it to work
    later when our characters actually basically look further in the history。 And
    so that's why we want to do it this way。 So just a quick comment on that。 So now
    we see that this is random。 So let's train the model so it becomes a bit less
    random。 Okay。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以以这种方式编写，是因为现在这是一个背景模型。但我希望保持这个函数固定，并希望在我们的字符实际上更深入历史时它也能正常工作。因此，我们希望以这种方式进行处理。这是一个简单的评论。所以现在我们看到这是随机的。那么让我们训练模型，使其变得不那么随机。好的。
- en: let's now train the model。 So first what I'm going to do is I'm going to create
    a PyTorch optimization object。 So here we are using the optimizer addmw。 Now in
    a make more series we've only ever used the castigrade in the sent。 The simplest
    possible optimizer which you can get using the SGD instead。 But I want to use
    addmw which is a much more advanced and popular optimizer and it works extremely
    well。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练模型。首先我要做的是创建一个PyTorch优化对象。所以我们在这里使用的是优化器addmw。在这个系列中，我们只使用了简单的castigrade。你可以使用SGD获得的最简单的优化器。但我想使用addmw，这是一种更高级、更流行的优化器，效果非常好。
- en: For typical good setting for the learning rate is roughly 3 in negative 4。 But
    for very very small networks like the case here you can get away with much much
    higher learning rates。 1 in negative 3 or even higher probably。 But let me create
    the optimizer object which will basically take the gradients and update the parameters
    using the gradients。 And then here our batch size up above was only 4。 So let
    me actually use something bigger let's say 32。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于学习率的典型良好设置，大约是负4的3。但对于像这里这样非常小的网络，你可以使用更高的学习率，可能是负3的1甚至更高。让我创建一个优化器对象，它将基本上接受梯度并使用梯度更新参数。然后，我们的批量大小上面只有4。让我们实际使用一个更大的，比如说32。
- en: And then for some number of steps we are sampling a new batch of data。 We're
    evaluating the loss。 We're zeroing out all the gradients from the previous step。
    Getting the gradients for all the parameters and then using those gradients to
    update our parameters。 So typical training loop as we saw in the make more series。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在若干步骤中，我们正在采样一批新的数据。我们在评估损失。我们将上一步的所有梯度清零。获取所有参数的梯度，然后使用这些梯度更新我们的参数。这是我们在“制作更多”系列中看到的典型训练循环。
- en: So let me now run this for say 100 iterations and let's see what kind of losses
    we're going to get。 So we started around 4。7 and now we're going to down to like
    4。6， 4。5， etc。 So the optimization is definitely happening but let's sort of try
    to increase number of iterations and only print at the end。 Because we probably
    want to train for a hundred。 Okay so we're down to 3。6 roughly。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我现在运行这个大约100次迭代，看看我们会得到什么样的损失。我们从大约4.7开始，现在降到大约4.6、4.5等等。因此，优化确实在发生，但我们试着增加迭代次数，只在最后打印结果。因为我们可能想训练一百次。好的，所以我们降到了大约3.6。
- en: Roughly down to 3。 This is the most janky optimization。 Okay it's working。 Let's
    just do 10，000。 And then from here we want to copy this and hopefully we're going
    to get something reasonable。 And of course it's not going to be Shakespeare from
    a Bagram model but at least we see that the loss is improving。 And hopefully we're
    expecting something a bit more reasonable。 Okay so we're down at about 2。5 ish。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大约降到3。这是最不稳定的优化。好的，它在工作。我们就做10,000。然后从这里我们想复制这个，希望能得到一些合理的东西。当然这不会是来自巴格拉姆模型的莎士比亚，但至少我们看到损失在改善。希望我们能期待一些更合理的结果。好的，我们降到了大约2.5左右。
- en: Let's see what we get。 Okay dramatic improvement certainly on what we had here。
    So let me just increase the number of tokens。 Okay so we see that we're starting
    to get something at least like reasonable ish。 Certainly not Shakespeare but the
    model is making progress。 So that is the simplest possible model。 So now what
    I'd like to do is obviously this is a very simple model because the tokens are
    not talking to each other。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么。好的，显然在这里有了戏剧性的改善。那么我就增加一下令牌的数量。好的，所以我们开始得到一些至少像合理的东西。当然不是莎士比亚，但模型正在进步。这是最简单的模型。所以我现在想做的是，显然这是一个非常简单的模型，因为令牌之间并没有相互交流。
- en: So given the previous context of whatever was generated we're only looking at
    the very last character to make the predictions about what comes next。 So now
    these tokens have to start talking to each other and figuring out what is in the
    context so that they can make better predictions for what comes next。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所以鉴于之前生成的上下文，我们只看最后一个字符来预测接下来会发生什么。因此，这些令牌必须开始相互交流，并弄清楚上下文中有什么，以便能够更好地预测接下来会发生什么。
- en: And this is how we're going to kick off the transformer。 Okay so next I took
    the code that we developed in this Jupyter notebook and I converted it to be a
    script。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将启动变压器的方式。好的，接下来我把我们在这个Jupyter notebook中开发的代码转换为脚本。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_23.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_23.png)'
- en: And I'm doing this because I just want to simplify our intermediate work which
    is just the final project that we have at this point。 So in the top here I put
    all the parameters that we defined。 I introduced a few and I'm going to speak
    to that in a little bit。 Otherwise a lot of this should be recognizable。 Reproducibility，
    read data。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我这样做是因为我只是想简化我们的中间工作，这只是我们目前的最终项目。因此在顶部我放置了我们定义的所有参数。我引入了一些参数，稍后会对此进行说明。否则，这些内容应该是可识别的。可重复性，读取数据。
- en: get the encoder and the decoder， create the train into splits。 use the kind
    of like data loader that gets a batch of the inputs and targets。 This is new and
    I'll talk about it in a second。 Now this is the background language model that
    we developed and it can forward and give us a logit and loss and it can generate。
    And then here we are creating the optimizer and this is the training loop。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 获取编码器和解码器，创建训练数据的分割。使用那种像数据加载器的东西，它可以获取输入和目标的一个批次。这是新的，我稍后会谈到。现在这是我们开发的背景语言模型，它可以前向传播并给我们一个logit和损失，同时也可以生成。接着我们创建优化器，这是训练循环。
- en: So everything here should look pretty familiar。 Now some of the small things
    that I added。 number one， I added the ability to run on a GPU if you have it。
    So if you have a GPU then you can。 this will use CUDA instead of just CPU and
    everything will be a lot more faster。 Now when device becomes CUDA then we need
    to make sure that when we load the data we move it to device。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的一切看起来应该非常熟悉。现在我添加的一些小东西。第一，我添加了在你有GPU的情况下运行的能力。如果你有GPU，那么你可以使用CUDA，而不是仅仅使用CPU，一切都会快很多。当设备变为CUDA时，我们需要确保在加载数据时将其移动到设备上。
- en: When we create the model we want to move the model parameters to device。 So
    as an example here we have the in embedding table and it's got a dot weight inside
    it which stores the sort of lookup table。 So that would be moved to the GPU so
    that all the calculations here happen on the GPU and they can be a lot faster。
    And then finally here when I'm creating the context that feeds it to generate
    I have to make sure that I create on the device。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建模型时，我们想要将模型参数移动到设备上。作为一个例子，这里我们有嵌入表，它里面有一个权重存储了查找表。因此，将其移动到GPU，以便这里的所有计算都在GPU上进行，可以更快。最后，当我创建上下文以生成时，我必须确保在设备上创建。
- en: Number two what I introduced is the fact that here in the training loop。 here
    I was just printing the loss dot item inside the training loop。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我引入了在训练循环中这个事实。这里我只是在训练循环中打印损失项。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_25.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_25.png)'
- en: But this is a very noisy measurement of the current loss because every batch
    will be more or less lucky。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一种非常嘈杂的当前损失测量，因为每个批次都会或多或少地运气好坏。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_27.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_27.png)'
- en: And so what I want to do usually is I have an estimate loss function and the
    estimate loss basically then goes up here。 And it averages up the loss over multiple
    batches。 So in particular we're going to iterate eval iter times and we're going
    to basically get our loss and then we're going to get the average loss for both
    splits。 And so this will be a lot less noisy。 So here what we call the estimate
    loss we're going to report the pre accurate train and validation loss。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我通常想要做的是有一个估计损失函数，估计损失基本上上升到这里。它平均多个批次的损失。因此，特别是我们将迭代eval iter次数，并基本上获取我们的损失，然后获取两个分割的平均损失。因此，这将减少很多噪声。因此，在这里我们称之为估计损失，我们将报告更准确的训练和验证损失。
- en: Now when we come back up you'll notice a few things here。 I'm setting the model
    to a valuation phase and down here I'm resetting it back to training phase。 Now
    right now for our model as is this doesn't actually do anything because the only
    thing inside this model is this and then dot embedding。 And this network would
    behave both would behave the same in both evaluation mode and training mode。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们回到上面时，你会注意到这里有几个事情。我将模型设置为评估阶段，而在这里我将其重置为训练阶段。现在对于我们当前的模型，这实际上并不做任何事情，因为这个模型里面只有这个和点嵌入。这个网络在评估模式和训练模式下的行为是相同的。
- en: We have no dropout layers we have no bathroom layers etc。 But it is a good practice
    to think through what mode your neural network is in because some layers will
    have different behavior at inference time or training time。 And there's also this
    context manager Torched up no grad and this is just telling PyTorch that everything
    that happens inside this function。 we will not call dot backward on。 And so PyTorch
    can be a lot more efficient with its memory use because it doesn't have to store
    all the intermediate variables because we're never going to call backward。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有丢弃层，没有批归一化层等。但考虑神经网络处于什么模式是个好习惯，因为某些层在推理时间或训练时间的行为会有所不同。此外，还有这个上下文管理器torch.no_grad，这只是告诉PyTorch在这个函数内部发生的所有事情，我们不会调用反向传播。因此，PyTorch在内存使用上可以更加高效，因为它不需要存储所有中间变量，因为我们从不调用反向传播。
- en: And so it can be a lot more memory efficient in that way。 So also a good practice
    to tell PyTorch when we don't intend to do back propagation。 So right now this
    script is about 120 lines of code of and that's kind of our starter code。 I'm
    calling it by gram。py and I'm going to release it later。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这样会更节省内存。因此，告诉PyTorch我们不打算进行反向传播也是一个好的实践。所以现在这个脚本大约有120行代码，这就是我们的启动代码。我将其命名为gram.py，并将稍后发布。
- en: Now running this script gives us output in the terminal and it looks something
    like this。 It basically as I ran this code it was giving me the train loss and
    the val loss and we see that we convert to somewhere around 2。5 with the by-gram
    model。 And then here's the sample that we produced at the end。 And so we have
    everything packaged up in the script and we're in a good position now to iterate
    on this。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行这个脚本在终端中给我们输出，结果大致如下。当我运行这段代码时，它给了我训练损失和验证损失，我们看到它转变为大约2.5，使用的是字组模型。然后这是我们在最后产生的样本。所以我们将所有内容都打包在脚本中，现在我们可以很好地进行迭代。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_29.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_29.png)'
- en: Okay， so we are almost ready to start writing our very first self-attention
    block for processing these tokens。 Now before we actually get there， I want to
    get you used to a mathematical trick that is used in the self-attention inside
    a transformer。 And it's really just like at the heart of an efficient implementation
    of self-attention。 And so I want to work with this toy example to just get used
    to this operation and then it's going to make it much more clear once we actually
    get to it。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们几乎准备好开始编写我们的第一个自注意力模块来处理这些令牌。现在在我们真正到达那里之前，我想让你熟悉一个在变压器的自注意力中使用的数学技巧。这实际上是高效实现自注意力的核心。我想用这个玩具示例来熟悉这个操作，一旦我们真正到达那里，这会让事情更加清晰。
- en: To it in the script again。 So let's create a B by T by C where B， T and C are
    just 4。 8 and 2 in this toy example。 And these are basically channels and we have
    batches and we have the time component and we have some information at each point
    in the sequence。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次在脚本中。因此，让我们创建一个B乘T乘C的矩阵，其中B、T和C在这个玩具示例中分别为4、8和2。这基本上是通道，我们有批次，还有时间分量，以及序列中每个点的一些信息。
- en: So C。 Now what we would like to do is we would like these tokens。 We have up
    to 8 tokens here in a batch and these 8 tokens are currently not talking to each
    other and we would like them to talk to each other。 We'd like to couple them。
    And in particular we want to couple them in a very specific way。 So the token
    for example at the fifth location。 It should not communicate with tokens in the
    sixth。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所以C。现在我们想要这些令牌。我们在一个批次中有最多8个令牌，而这8个令牌目前并没有彼此交流，我们希望它们能够互相沟通。我们希望将它们耦合起来。特别是，我们想要以非常特定的方式进行耦合。例如，第五个位置上的令牌不应与第六个位置的令牌进行交流。
- en: seventh and eighth location because those are future tokens in the sequence。
    The token on the fifth location should only talk to the one in the fourth， third，
    second and first。 So it's only so information only flows from previous context
    to the current time step。 And we cannot get any information from the future because
    we are about to try to predict the future。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第七和第八个位置是因为它们在序列中是未来的令牌。第五个位置上的令牌只应与第四、第三、第二和第一个位置上的令牌进行交流。因此，信息仅从之前的上下文流向当前时间步。我们无法从未来获得任何信息，因为我们正要尝试预测未来。
- en: So what is the easiest way from tokens to communicate？
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 那么令牌之间沟通的最简单方式是什么？
- en: The easiest way I would say is if we're up to if we're a fifth token and I'd
    like to communicate with my past。 The simplest way we can do that is to just do
    an average of all the preceding elements。 So for example if I'm the fifth token
    I would like to take the channels that make up that are information at my step。
    But then also the channels from the fourth step， third step， second step and the
    first step。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为最简单的方式是，如果我们是第五个令牌，我想与我的过去进行交流。我们可以做的最简单的方法就是对所有前面的元素进行平均。例如，如果我是第五个令牌，我会想要获取构成我步骤的信息通道，同时也获取来自第四步、第三步、第二步和第一步的通道。
- en: I'd like to average those up and then that would become sort of like a feature
    vector that summarizes me in the context of my history。 Now of course just doing
    a sum or like an average is an extremely weak form of interaction。 Like this communication
    is extremely lossy。 We've lost a ton of information about spatial arrangements
    of all those tokens。 But that's okay for now。 We'll see how we can bring that
    information back later。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我想将这些值进行平均，这将成为总结我历史的特征向量。当然，仅仅求和或平均是极其弱的交互方式。这种交流是极其损失的。我们已经丢失了很多关于所有这些令牌的空间排列的信息。但这没关系。我们稍后将看到如何找回这些信息。
- en: For now what we would like to do is for every single batch element independently。
    For every teeth token in that sequence we'd like to now calculate the average
    of all the vectors in all the previous tokens。 And also at this token。 So let's
    write that out。 I have a small snippet here and instead of just fumbling around
    let me just copy paste it and talk to it。 So in other words we're going to create
    X and the BOW is short for bag of words。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们希望为每一个批次元素独立地进行操作。对于序列中的每个标记，我们希望现在计算所有先前标记中所有向量的平均值，以及这个标记。所以让我们写出来。我这里有一个小片段，而不是只是随意翻弄，让我复制粘贴并进行讲解。换句话说，我们将创建X，BOW是词袋的缩写。
- en: Because bag of words is kind of like a term that people use when you are just
    averaging up things。 So it's just a bag of words。 Basically there's a word stored
    on every one of these eight locations and we're doing a bag of words。 So just
    averaging。 So in the beginning we're going to say that it's just initialized at
    zero。 And then I'm doing a for loop here so we're not being efficient yet。 That's
    coming。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因为词袋是人们在平均事物时使用的术语之一。所以这只是一个词袋。基本上在这八个位置上的每一个都有一个单词存储，我们正在进行一个词袋的操作。所以仅仅是平均。因此，在开始时我们将其初始化为零。然后我在这里做一个for循环，所以我们还没有高效。这是后来的事情。
- en: But for now we're just iterating over all the batch dimensions independently。
    Iterating over time。 And then the previous tokens are at this batch of dimension。
    And then everything up to and including the teeth token。 So when we slice out
    X in this way。 X-preve becomes of shape how many elements they were in the past。
    And then of course C。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 但目前我们只是独立地迭代所有的批次维度。迭代时间。然后之前的标记位于这个批次维度。然后所有内容包括第十个标记。因此，当我们以这种方式切割X时，X-preve的形状将是过去有多少个元素。当然还有C。
- en: So all the two dimensional information from these little tokens。 So that's the
    previous sort of chunk of tokens from my current sequence。 And then I'm just doing
    the average or the mean over the zero dimension。 So I'm averaging out the time
    here。 And I'm just going to get a little C one dimensional vector which I'm going
    to store in X bag of words。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以从这些小标记中提取的所有二维信息。这是我当前序列中之前的一组标记。然后我只是对零维进行平均或求均值。因此，我在这里平均时间。最终我将得到一个一维的C向量，我将把它存储在X的词袋中。
- en: So I can run this and this is not going to be very informative because。 Let's
    see。 So this is X of zero。 So this is the zero with batch element and then X bow
    at zero。 Now you see how the at the first location here you see that the two are
    equal。 And that's because it's we're just doing an average of this one token。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我可以运行这个，这不会非常有信息性，因为。让我们看看。这是X的零。所以这是零批次元素，然后是X在零的地方。现在你看到这里的第一个位置，这两个是相等的。这是因为我们只是对这个一个标记进行平均。
- en: But here this one is now an average of these two。 And now this one is an average
    of these three。 And so on。 So and this last one is the average of all of these
    elements。 So this is the typical average just averaging up all the tokens now
    gives this outcome here。 So this is all well and good， but this is very inefficient。
    Now the trick is that we can be very。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里的这个现在是这两个的平均值。而现在这个是这三个的平均值，依此类推。因此最后一个是所有这些元素的平均值。这是典型的平均值，所有标记的平均现在得出了这个结果。所以这一切都很好，但这非常低效。现在的窍门是我们可以非常。
- en: very efficient about doing this using matrix multiplication。 So that's the mathematical
    trick。 And let me show you what I mean。 Let's work with the toy example here。
    Let me run it and I'll explain。 I have a simple matrix here that is three by three
    of all ones。 A matrix B of just random numbers and it's a three by two。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 非常高效地使用矩阵乘法来做到这一点。这就是数学上的技巧。让我给你看看我的意思。让我们用一个玩具示例来做。让我运行它，我会解释。我这里有一个简单的三乘三的全1矩阵。一个三乘二的随机数矩阵B。
- en: And a matrix C which will be three by three multiply three by two。 which will
    give out a three by two。 So here we're just using matrix multiplication。 So A
    multiply B gives us C。 Okay。 So how are these numbers in C achieved？ Right。 This
    number in the top left is the first row of A dot product with the first column
    of B。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个矩阵C，它将是三乘三乘以三乘二，这将得到一个三乘二的结果。所以这里我们只是使用矩阵乘法。A乘以B得到C。好的。那么C中的这些数字是如何得出的呢？对了，左上角的这个数字是A的第一行与B的第一列的点积。
- en: And since all the row of A right now is all just once。 then the dot product
    here with this column of B is just going to do a sum of these of this column。
    So two plus six plus six is fourteen。 The element here in the upper of C is also
    the first column here。 The first row of A multiplied now with the second column
    of B。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于A的所有行现在都是一次，因此与B的这一列的点积将只是对这一列的和。所以2加6加6等于十四。C中上面的元素也是这里的第一列。A的第一行现在乘以B的第二列。
- en: So seven plus four plus five is sixteen。 Now you see that there's repeating
    elements here。 So this fourteen again is because this row is again all once and
    it's multiplying the first column of B。 So we get fourteen。 And this one is and
    so on。 So this last number here is the last row dot product last column。 Now the
    trick here is the following。 This is just a boring number of。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以七加四加五等于十六。现在你看到这里有重复的元素。这个十四再次出现是因为这一行再次是全部为一，并且它在乘以B的第一列。所以我们得到了十四。而这个也是如此。所以这里的最后一个数字是最后一行与最后一列的点积。这里的诀窍如下。这只是一个无聊的数字。
- en: it's just a boring array of all ones。 But torch has this function called the
    trill。 which is short for a triangular， something like that。 And you can wrap
    it in torch that once and it will just return the lower triangular portion of
    this。 Okay。 So now it will basically zero out these guys here。 So we just get
    the lower triangular part。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个全是1的无聊数组。但是torch有一个叫做tril的函数，短语是三角形的，类似的东西。你可以将其包装在torch中，它将只返回这个的下三角部分。好的。所以现在它将基本上将这些数值归零。因此我们只得到下三角部分。
- en: Well what happens if we do that？ So now we'll have A like this and B like this。
    And now what are we getting here in C？ Well what is this number？
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如果我们这样做会发生什么呢？所以现在我们将得到这样的A和这样的B。那么我们在C中得到的是什么？这个数字是什么？
- en: Well this is the first row times the first column。 And because this is zeros
    these elements here are now ignored。 So we just get a two。 And then this number
    here is the first row times the second column。 And because these are zeros they
    get ignored and it's just seven。 This is seven multiplies this one。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这是第一行乘以第一列。因为这些是零，所以这些元素现在被忽略。因此我们只得到2。然后这里的数字是第一行乘以第二列。因为这些是零，它们被忽略，所以只有7。这是7乘以这个1。
- en: But look what happened here because this is one and then zeros we。 what ended
    up happening is we're just plucking out the row of this row of B and that's what
    we got。 Now here we have one one zero。 So here one one zero dot product with these
    two columns will now give us two plus six which is eight and seven plus four which
    is eleven。 And because this is one one one we ended up with the addition of all
    of them。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但看看这里发生了什么，因为这是1而这些是零。我们最终发生的事情是我们只是抽取了B的这一行，这就是我们得到的。现在这里我们有1、1、0。因此这里1、1、0与这两列的点积将给我们2加6，结果是8，7加4，结果是11。因为这是1、1、1，我们最终得到了它们的和。
- en: And so basically depending on how many ones and zeros we have here we are basically
    doing a sum currently of the variable number of these rows and that gets deposited
    into C。 So currently we're doing sums because these are ones but we can also do
    average。 Right？
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，取决于我们这里有多少个1和0，我们基本上正在对这些行的变量数量进行求和，然后将其存储到C中。因此目前我们在进行求和，因为这些是1，但我们也可以进行平均，对吗？
- en: And you can start to see how we could do average of the rows of B sort of in
    incremental fashion。 Because we don't have to， we can basically normalize these
    rows so that they sum to one and then we're going to get an average。 So if we
    took A and then we did A equals A divide a torch dot sum in the of A in the one
    dimension and then let's keep them as true。 So therefore the broadcasting will
    work out。 So if I read from this you see now that these rows now sum to one。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以开始看到我们如何以增量方式对B的行进行平均。因为我们不需要，我们基本上可以对这些行进行归一化，使它们的和为1，然后我们会得到一个平均值。所以如果我们取A，然后我们让A等于A除以torch.dot.sum(A,
    1维) 然后我们就保持它们为真。因此广播将会有效。所以如果我从中读取，你会看到这些行现在的和为1。
- en: So this row is one this row is 0。5。50 and here we get one thirds。 And now when
    we do A multiply B what are we getting？
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这一行是1，这一行是0.5。这里我们得到三分之一。那么当我们进行A与B的乘法时，我们得到什么呢？
- en: Here we are just getting the first row first row。 Here now we are getting the
    average of the first two rows。 Okay so two and six average is four and four and
    seven averages five and five。 And on the bottom here we're now getting the average
    of these three rows。 So the average of all of elements of B are now deposited
    here。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们只获取第一行。这里现在我们获取前两行的平均值。好的，所以二和六的平均是四，四和七的平均是五和五。在底部这里我们现在获取这三行的平均值。所以 B
    的所有元素的平均现在被存储在这里。
- en: And so you can see that by manipulating these elements of this multiplying matrix
    and then multiplying it with any given matrix。 We can do these averages in this
    incremental fashion because we just get and we can manipulate that based on the
    elements of A。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，通过操作这个乘法矩阵的这些元素，然后将其与任何给定矩阵相乘。我们可以以这种增量的方式进行这些平均值，因为我们只获取并可以根据 A 的元素进行操作。
- en: Okay so that's very convenient so let's swing back up here and see how we can
    vectorize this and make it much more efficient using what we've learned。 So in
    particular we are going to produce an array A but here I'm going to call it way
    short for weights。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这非常方便，所以我们回到这里，看看如何对其进行向量化，并利用我们所学的内容使其更加高效。因此，特别地，我们将生成一个数组 A，但在这里我会将其称为权重的简称。
- en: But this is our A and this is how much of every row we want to average up and
    it's going to be an average because you can see that these rows sum to one。 So
    this is our A and then our B in this example of course is X。 So what's going to
    happen here now is that we are going to have an X bow two。 And this X bow two
    is going to be way multiplying our X。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这是我们的 A，这就是我们希望对每一行进行多少平均处理，并且它将是一个平均值，因为你可以看到这些行的和为一。所以这是我们的 A，然后在这个例子中，当然我们的
    B 是 X。那么现在将会发生什么呢，我们将有一个 X bow two。这个 X bow two 将会乘以我们的 X。
- en: So let's think this through way is T by T and this is matrix multiplying in
    PyTorch a B by T by C。 And it's giving us what shape so PyTorch will come here
    and we'll see that these shapes are not the same。 So it will create a batch dimension
    here and this is a batch matrix multiply。 And so it will apply this matrix multiplication
    in all the batch elements in parallel and individually。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们想一下，这个方式是 T 乘 T，而这是在 PyTorch 中进行 B 乘 T 乘 C 的矩阵乘法。它给我们的是什么形状，所以 PyTorch 会来这里，我们会看到这些形状并不相同。因此，它将在这里创建一个批次维度，这是一个批量矩阵乘法。因此，它将在所有批次元素中并行且单独地应用这个矩阵乘法。
- en: And then for each batch element there will be a T by T multiplying T by C exactly
    as we had below。 So this will now create B by T by C and X bow two will now become
    identical to X bow。 So we can see that Torch dot all close of X bow and X bow
    two should be true。 Now so this kind of like commoses us that these are in fact
    the same。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于每个批次元素，将有一个 T 乘 T 乘 T 乘 C，正如我们下面所做的。因此，这将现在生成 B 乘 T 乘 C，X bow two 现在将变得与
    X bow 相同。所以我们可以看到 Torch.dot.allclose(X bow 和 X bow two 应该为真。现在，这种方式就像是告诉我们，这实际上是相同的。
- en: So X bow and X bow two if I just print them。 Okay。 we're not going to be able
    to just stare it down but let me try X bow basically just at the zero element
    and X bow two at the zero element。 So just the first batch and we should see that
    this and that should be identical which they are。 Right， so what happened here
    the trick is we were able to use batch matrix multiply to do this aggregation
    really。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 X bow 和 X bow two 如果我只打印它们。好的。我们不能仅仅盯着看，但让我尝试 X bow 基本上在零元素处，X bow two 在零元素处。因此只看第一批，我们应该看到这个和那个应该是相同的，它们确实是。对，所以这里发生了什么，关键是我们能够使用批量矩阵乘法来进行这个聚合。
- en: And it's a weighted aggregation and the weights are specified in this T by T
    array。 And we're basically doing weighted sums and these weighted sums are according
    to the weights inside here that take on sort of this triangular form。 And so that
    means that a token at the teeth dimension will only get sort of information from
    the tokens perceiving it。 So that's exactly what we want。 And finally I would
    like to rewrite it in one more way and we're going to see why that's useful。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个加权聚合，权重在这个 T 乘 T 的数组中指定。我们基本上在进行加权和，这些加权和根据这里的权重，呈现出这种三角形的形式。因此，这意味着在第 T
    个维度的一个标记只会获取来自其周围标记的信息。所以这正是我们想要的。最后，我想再用一种方式重写它，我们将看到这为何有用。
- en: So this is the third version and it's also identical to the first and second。
    But let me talk through it。 It uses softmax。 So trill here is this matrix lower
    triangular ones。 Way begins as all zero。 Okay， so if I just print way in the beginning
    it's all zero。 Then I use masked fill。 So what this is doing is weight that mask
    fill it's all zeros and I'm saying for all the elements where trill is equal to
    zero make them be negative infinity。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第三个版本，它与第一和第二个版本相同。但让我详细说明一下。它使用 softmax。所以这里的 trill 是这个下三角矩阵。权重一开始全是零。好的，如果我刚开始打印权重，它都是零。然后我使用
    masked fill。所以这在做的事情是将所有元素中 trill 等于零的设置为负无穷。
- en: So all the elements where trill is zero will become negative infinity now。 So
    this is what we get。 And then the final line here is softmax。 So if I take a softmax
    along every single so dim is negative one so long every single row if I do a softmax
    what is that going to do？
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 trill 为零的元素现在将变为负无穷。因此我们得到了这个结果。然后这里的最后一行是 softmax。如果我在每一行上进行 softmax，维度是负一，那么如果我对每一行做
    softmax，这会有什么效果呢？
- en: Well softmax is also like a normalization operation。 And so spoiler alert you
    get the exact same matrix。 Let me bring back the softmax。 And recall that in softmax
    we're going to exponentiate every single one of these and then we're going to
    divide by the sum。 And so if we exponentiate every single element here we're going
    to get a one and here we're going to get basically zero zero zero zero zero zero
    everywhere else。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，softmax 也像是一种归一化操作。所以剧透一下，你会得到完全相同的矩阵。让我再回到 softmax。回想一下，在 softmax 中，我们将对每一个进行指数运算，然后除以总和。因此，如果我们对这里的每个元素进行指数运算，我们将得到一个，而其他地方基本上都是零零零零零零。
- en: And then when we normalize we just get one。 Here we're going to get one one
    and then zeros and then softmax will again divide and this will give us 0。5 and
    so on。 And so this is also the same way to produce this mask。 Now the reason that
    this is a bit more interesting and the reason we're going to end up using it in
    self attention is that these weights here begin with zero。 And you can think of
    this as like an interaction strength or like an affinity。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们归一化时，我们只会得到一个。在这里我们将得到一，一，然后是零，接着 softmax 将再次进行除法，这将给我们 0.5，依此类推。这也是生成这个掩码的相同方式。现在，这个过程更有趣的原因，以及我们最终将在自注意力中使用它的原因是，这里的权重最初是零。你可以将其视为一种交互强度或亲和力。
- en: So basically it's telling us how much of each token from the past do we want
    to aggregate and average up。 And then this line is saying tokens from the past
    cannot communicate by setting them to negative infinity。 We're saying that we
    will not aggregate anything from those tokens。 And so basically this then goes
    through softmax and through the weighted and this is the aggregation through matrix
    multiplication。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这告诉我们我们希望从过去的每个令牌中聚合和平均多少信息。然后这一行表示过去的令牌不能通过将它们设为负无穷来进行通信。我们在说我们不会从那些令牌中聚合任何信息。因此，基本上这个过程经过
    softmax 和加权，而这就是通过矩阵乘法进行的聚合。
- en: And so what this is now is you can think of these as these zeros are currently
    just set by us to be zero。 But a quick preview is that these affinities between
    the tokens are not going to be just constant at zero。 They're going to be data
    dependent。 These tokens are going to start looking at each other and some tokens
    will find other tokens more or less interesting。 And depending on what their values
    are they're going to find each other interesting to different amounts and I'm
    going to call those affinities I think。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以把这些零看作是我们目前设定的。但是简单预览一下，这些令牌之间的亲和力不会始终保持为零。它们将依赖于数据。这些令牌将开始互相观察，有些令牌会发现其他令牌或多或少地有趣。根据它们的值，它们会以不同的程度发现彼此有趣，我将称之为这些亲和力。
- en: And then here we are saying the future cannot communicate with the past。 We're
    going to clamp them。 And then when we normalize and some we're going to aggregate
    sort of their values depending on how interestingly they find each other。 And
    so that's the preview for self-attention。 And basically long story short from
    this entire section is that you can do weighted aggregations of your past elements
    by using matrix multiplication of a lower triangular fashion。 And then the elements
    here in the lower triangular part are telling you how much of each element fuses
    into this position。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们表示未来不能与过去沟通。我们将对它们进行限制。然后，当我们进行归一化和求和时，我们将根据彼此的有趣程度来汇聚它们的值。这就是自注意力的预览。总的来说，从这一整节内容来看，你可以通过使用下三角形式的矩阵乘法来对过去的元素进行加权聚合。下三角部分的元素告诉你每个元素在这个位置上融合的程度。
- en: So we're going to use this trick now to develop the self-attention block。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将利用这个技巧来开发自注意力块。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_31.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_31.png)'
- en: So first let's get some quick preliminaries out of the way。 First the thing
    I'm kind of bothered by is that you see how we're passing and vocab size into
    the constructor。 There's no need to do that because vocab size is already defined
    up top as a global variable。 So there's no need to pass this stuff around。 Next
    what I want to do is I don't want to actually create。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们先快速处理一些预备知识。我感到有点困扰的是，你会看到我们如何将vocab size传递给构造函数。实际上没有必要这样做，因为vocab size已经在上面定义为全局变量。因此没有必要将这些东西传来传去。接下来，我想要做的是，我不想实际上创建。
- en: I want to create like a level of interaction here where we don't directly go
    to the embedding for the logits。 But instead we go through this intermediate phase
    because we're going to start making that bigger。 So let me introduce a new variable
    and embed。 It's short for number of embedding dimensions。 So an embed here will
    be say 32。 That was a suggestion from GitHub Copiled by the way。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在这里创造一种互动层次，我们不会直接使用嵌入来处理logits，而是通过这个中间阶段，因为我们将开始扩展它。所以让我引入一个新变量并进行嵌入。这是“嵌入维度数量”的简称。因此，在这里的嵌入可以设为32。这是来自GitHub
    Copilot的建议。
- en: It also suggests 32 which is a good number。 So this is an embedding table and
    only 32 dimensional embeddings。 So then here this is not going to give us logits
    directly。 Instead this is going to give us token embeddings。 That's what I'm going
    to call it。 And then to go from the token embeddings to the logits we're going
    to need a linear layer。 So self。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 它也建议使用32，这是一个不错的数字。所以这是一个嵌入表，并且只有32维的嵌入。因此，在这里这不会直接给我们logits。相反，这将给我们token嵌入。这就是我将要称之为的。然后，从token嵌入到logits，我们需要一个线性层。因此，自身。
- en: lmhead， let's call it short for language modeling head。 is an in linear from
    an embed up to vocab size。 And then when we swing over here we're actually going
    to get the logits by exactly what the copilot says。 Now we have to be careful
    here because this C and this C are not equal。 This is an embed C and this is vocab
    size。 So let's just say that an embed is equal to C。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: lmhead，我们称其为语言建模头，是一个从嵌入到vocab size的线性层。然后，当我们切换到这里时，我们实际上将根据Copilot所说的内容获得logits。现在我们必须小心，因为这个C和这个C是不相等的。这是一个嵌入C，而这是vocab
    size。所以我们可以说嵌入等于C。
- en: And then this just creates one spurious layer of interaction through a linear
    layer but this should basically run。 So we see that this runs and this currently
    looks kind of spurious but we're going to build on top of this。 Now next up。 So
    far we've taken these indices and we've encoded them based on the identity of
    the tokens inside IDX。 The next thing that people very often do is that we're
    not just encoding the identity of these tokens but also their position。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这只是通过一个线性层创建了一个虚假的交互层，但这基本上应该能运行。因此我们看到这可以运行，目前看起来有些虚假，但我们将基于此继续构建。接下来，到目前为止，我们已经基于IDX中token的身份对这些索引进行了编码。接下来，人们通常会做的是，不仅仅编码这些token的身份，还包括它们的位置。
- en: So we're going to have a second position embedding table here。 So self。 that
    position embedding table is an embedding of block size by an embed。 And so each
    position from zero to block size minus one will also get its own embedding vector。
    And then here first let me decode a b by t from IDX。shape。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将有一个第二个位置嵌入表。在这里，自身的这个位置嵌入表是一个以块大小和嵌入为维度的嵌入。因此，从零到块大小减一的每个位置也将拥有自己的嵌入向量。接下来，让我从IDX.shape解码一个b
    by t。
- en: And then here we're also going to have a pause embedding which is the positional
    embedding and these are this is tortoise arrange。 So this will be basically just
    integers from zero to t minus one。 And all of those integers from zero to t minus
    one get embedded through the table to create a t by c。 And then here this gets
    renamed to just say x and x will be the addition of the token embedding with the
    positional embedding。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里我们还会有一个暂停嵌入，即位置嵌入，这些是海龟排列。因此，这基本上就是从零到t减一的整数。所有这些从零到t减一的整数通过表格嵌入，以创建一个t乘c的矩阵。然后这里被重命名为x，x将是令牌嵌入与位置嵌入的相加。
- en: And here the broadcasting note will work out so b by t by c plus t by c。 This
    gets right aligned a new dimension of one gets added and it gets broadcasted across
    batch。 So at this point x holds not just the token identities but the positions
    at which these tokens occur。 And this is currently not that useful because of
    course we just had a simple binary model so it doesn't matter if you're on the
    fifth position the second position or wherever。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，广播操作将发挥作用，因此b乘t乘c加上t乘c。这右对齐，增加了一个新的维度，并在批处理间进行广播。因此此时，x不仅包含令牌的身份，还包含这些令牌出现的位置。由于我们只是有一个简单的二元模型，因此这目前并不是很有用，所以无论你处于第五个位置、第二个位置还是其他位置都无关紧要。
- en: It's all translation invariant at this stage so this information currently wouldn't
    help but as we work on the self attention block we'll see that this starts to
    matter。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，所有内容都是平移不变的，因此目前这些信息不会有帮助，但随着我们对自注意力模块的深入研究，我们会看到这开始变得重要。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_33.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_33.png)'
- en: Okay so now we get the crux of self attention so this is probably the most important
    part of this video to understand。 We're going to implement a small self attention
    for a single individual head as they're called。 So we start off with where we
    were so all of this code is familiar。 So right now I'm working with an example
    where I change the number of channels from two to 32 so we have a four by eight
    arrangement of tokens。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们进入自注意力的核心内容，因此这是理解此视频最重要的部分。我们将为一个单独的头实现一个小的自注意力。我们从我们之前的位置开始，所以所有这些代码都是熟悉的。现在我正在处理一个示例，将通道的数量从两个更改为32，因此我们有一个四乘八的令牌排列。
- en: And each and the information each token is currently 32 dimensional but we just
    are working with random numbers。 Now we saw here that the code as we had it before
    does a simple weight simple average of all the past tokens and the current token。
    So it's just the previous information and current information is just being mixed
    together in an average。 And that's what this code currently achieves and it does
    so by creating this lower triangular structure which allows us to mask out this
    way matrix that we create。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 每个令牌的信息目前是32维的，但我们只是在使用随机数。现在我们看到之前的代码对所有过去的令牌和当前令牌做了简单的权重平均。所以只是之前的信息和当前的信息在平均中混合在一起。这就是当前代码所实现的，并通过创建这个下三角结构来实现，使我们能够掩盖创建的这个权重矩阵。
- en: So we mask it out and then we normalize it and currently when we initialize
    the affinities between all the different sort of tokens or nodes。 I'm going to
    use those terms interchangeably。 So when we initialize the affinities between
    all the different tokens to be zero。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将其掩盖，然后进行归一化，目前当我们初始化所有不同种类的令牌或节点之间的亲和力时，我将这些术语交替使用。因此，当我们初始化所有不同令牌之间的亲和力为零。
- en: Then we see that way gives us this structure where every single row has these
    uniform numbers。 And so that's what that's what then in this matrix multiply makes
    it so that we're doing a simple average。 Now we don't actually want this to be
    all uniform because different tokens will find different other tokens more or
    less interesting and we want that to be data dependent。 So for example if I'm
    a vowel then maybe I'm looking for consonants in my past and maybe I want to know
    what those consonants are and I want that information to flow to me。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到这个权重给了我们这样一个结构，每一行都有这些均匀的数字。因此，在这个矩阵乘法中，这使得我们做了一个简单的平均。现在我们实际上不希望这都是均匀的，因为不同的令牌会发现其他令牌的趣味程度不同，我们希望这取决于数据。例如，如果我是一个元音，那么也许我会在过去寻找辅音，并且我想知道那些辅音是什么，我希望这些信息流向我。
- en: And so I want to now gather information from the past but I want to do it in
    a data dependent way。 And this is the problem that self-attention solves。 Now
    the way self-attention solves this is the following。 Every single node or every
    single token at each position will emit two vectors。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我现在想从过去收集信息，但我想以一种数据依赖的方式进行。这就是自注意力所解决的问题。自注意力解决这个问题的方式如下。每个位置的每个节点或每个标记将发出两个向量。
- en: It will emit a query and it will emit a key。 Now the query vector roughly speaking
    is what am I looking for？
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 它将发出一个查询，并发出一个键。查询向量大致上是我在寻找什么？
- en: And the key vector roughly speaking is what do I contain？
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 键向量大致上是我包含了什么？
- en: And then the way we get the affinities between these tokens now in a sequence
    is we basically just do a dot product between the keys and the queries。 So my
    query dot products with all the keys of all the other tokens and that dot product
    now becomes way。
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在序列中获得这些标记之间亲和度的方式是我们基本上只需在键和查询之间进行点积。因此，我的查询与所有其他标记的所有键进行点积，这个点积现在变成了方式。
- en: And so if the key and the query are sort of aligned they will interact to a
    very high amount and then I will get to learn more about that specific token as
    opposed to any other token in the sequence。 So let's implement this now。 We're
    going to implement a single what's called head of self-attention。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果键和查询在某种程度上是对齐的，它们会相互作用得非常强烈，这样我就能更多地了解特定的标记，而不是序列中的其他标记。现在让我们来实现这一点。我们将实现一个称为自注意力的单头。
- en: So this is just one head。 There's a hyper parameter involved these heads which
    is the head size。 And then here I'm initializing linear modules and I'm using
    bias equals false so these are just going to apply matrix multiply with some fixed
    weights。 And now let me produce a key and Q k and Q by forwarding these modules
    on X。 So the size of this will now become B by T by 16 because that is the head
    size and the same here B by T by 16。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个头。与这些头相关的超参数是头的大小。现在我在这里初始化线性模块，并且我使用偏置等于假，因此这些只会用一些固定权重进行矩阵乘法。现在让我通过对
    X 进行前向传递来生成键和 Q k 以及 Q。因此，大小将变为 B × T × 16，因为这就是头的大小，后面也是 B × T × 16。
- en: So this being that size。 So you see here that when I forward this linear on
    top of my X all the tokens in all the positions in the B by T arrangement all
    of them in parallel and independently produce a key and a query。 So no communication
    has happened yet。 But the communication comes now all the queries will dart product
    with all the keys。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这样大小的矩阵。所以你在这里看到，当我在我的 X 上前向传递这个线性时，所有 B × T 排列中的所有位置的所有标记都是并行独立地产生一个键和一个查询。因此，尚未发生任何通信。但通信现在发生，所有查询将与所有键进行点积。
- en: So basically what we want is we want way now or the affinities between these
    to be query multiplying key。 But we have to be careful with we can't make us multiply
    this we actually need to transpose K but we have to be also careful because these
    are when you have the batch dimension。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上我们想要的是我们想要方式，现在或这些之间的亲和度是查询乘以键。但我们必须小心，因为我们不能直接相乘，实际上我们需要转置 K，但我们也必须小心，因为这些是带有批量维度的情况。
- en: So in particular we want to transpose the last two dimensions dimension negative
    one and dimension negative two。 So negative two negative one。 And so this matrix
    multiply now will basically do the following B by T by 16。 Matrix multiplies B
    by 16 by T to give us B by T by T。 Right。 So for every row of B we're not going
    to have a T square matrix given us the affinities and these are now the way。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是我们想要转置最后两个维度，维度 -1 和维度 -2。所以是 -2，-1。因此，这个矩阵乘法现在基本上会做 B × T × 16。矩阵乘法 B ×
    16 × T 会给我们 B × T × T。对每一行 B，我们将会得到一个 T 平方矩阵，给我们亲和度，这些就是方式。
- en: So they're not zeros。 They are now coming from this dart product between the
    keys and the queries。 So this can now run I can I can run this and the weighted
    aggregation now is a function in a data band and manner between the keys and queries
    of these nodes。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们不是零。它们现在来自键和查询之间的点积。因此，这现在可以运行，我可以运行这个，带权重的聚合现在是这些节点的键和查询之间以数据驱动的方式进行的函数。
- en: So just inspecting what happened here。 The way takes on this form and you see
    that before way was just a constant so it was applied in the same way to all the
    batch elements。 But now every single batch elements will have different sort of
    way because every single batch element contains different tokens at different
    positions。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 所以只是在检查这里发生了什么。权重呈现这种形式，你会发现之前的权重只是一个常数，所以它以相同的方式应用于所有批次元素。但现在每个批次元素都会有不同的权重，因为每个批次元素包含不同位置的不同标记。
- en: And so this is not a data dependent。 So when we look at just the zero row for
    example in the input。 These are the weights that came out。 And so you can see
    now that they're not just exactly uniform。 And in particular as an example here
    for the last row。 This was the eighth token and the eighth token knows what content
    it has and it knows at what position it's in。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是数据依赖的。所以当我们查看输入中的零行时。这些是产生的权重。你可以看到现在它们并不是完全均匀的。特别是作为示例，这一行是第八个标记，第八个标记知道它包含什么内容，并且知道它处于哪个位置。
- en: And now the token based on that creates a query。 Hey， I'm looking for this kind
    of stuff。 I'm a vowel。 I'm on the eight position。 I'm looking for any consonants
    at positions up to four。 And then all the nodes get to emit keys and maybe one
    of the channels could be I am a consonant and I am in the position up to four。
    And that key would have a high number in that specific channel。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在标记基于此创建查询。嘿，我在寻找这种东西。我是元音。我在第八个位置。我在寻找任何位置在四之前的辅音。然后所有节点都会发出键，也许某个通道的内容是我是一位辅音，并且我在四之前的位置。这个键在特定通道中的数值会很高。
- en: And that's how the query and the key when they dark product they can find each
    other and create a high affinity。 And when they have a high affinity like say
    this token was pretty interesting to this eighth token。 When they have a high
    affinity then through the softmax I will end up aggregating a lot of its information
    into my position。 And so I'll get to learn a lot about it。 Now just this we're
    looking at way after this has already happened。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，当查询和键进行点积时，它们可以找到彼此并产生高亲和力。当它们有高亲和力时，比如说这个标记对第八个标记来说非常有趣。当它们有高亲和力时，通过softmax，我最终会将很多信息聚合到我的位置上。所以我会学到很多关于它的信息。现在我们只是在看这一切发生后的情况。
- en: Let me erase this operation as well。 So let me erase the masking and the softmax
    just to show you the under the hood internals and how that works。 So without the
    masking and the softmax way it comes out like this right。 This is the outputs
    of the top products。 And these are the raw outputs and they take on values from
    negative to to positive to etc。 So that's the raw interactions and raw affinities
    between all the nodes。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我把这个操作也擦掉。所以让我擦掉遮罩和softmax，只是为了向你展示内部机制以及它是如何工作的。所以在没有遮罩和softmax的情况下，它的输出是这样的。这是顶层产品的输出。这些是原始输出，取值范围从负到正等等。这就是所有节点之间的原始交互和亲和力。
- en: But now if I'm a fifth node I will not want to aggregate anything from the sixth
    node。 seventh node and the eighth node。 So actually we use the upper triangular
    masking。 So those are not allowed to communicate。 And now we actually want to
    have a nice distribution。 So we don't want to aggregate negative point one one
    of this node that's crazy。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在如果我是第五个节点，我不想从第六个、第七个和第八个节点聚合任何信息。所以我们实际上使用上三角遮罩。因此，这些节点不能进行通信。现在我们实际上想要有一个合理的分布。所以我们不想聚合负的0.1，这太疯狂了。
- en: So instead we exponentiate and normalize。 And now we get a nice distribution
    that seems to want。 And this is telling us now in the data dependent manner how
    much of information to aggregate from any of these tokens in the past。 So that's
    way and it's not zeros anymore but it's calculated in this way。 Now there's one
    more part to a single self-attention head。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们改为指数化并归一化。现在我们得到一个看似合理的分布。这告诉我们以数据依赖的方式，从过去的任何这些标记中聚合多少信息。因此，方法是这样的，现在不再是零，而是以这种方式计算的。现在单个自注意力头还有一个部分。
- en: And that is that when we do the aggregation we don't actually aggregate the
    tokens exactly。 We aggregate we produce one more value here and we call that the
    value。 So in the same way that we produce P and query we're also going to create
    a value。 And then here we don't aggregate x。 We calculate a v which is just achieved
    by propagating this linear on top of x again。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 而当我们进行聚合时，我们并不是完全聚合令牌。我们聚合并产生一个额外的值，我们称之为值。因此，以产生P和查询的方式，我们也将创建一个值。然后在这里我们不聚合x。我们计算v，这只是通过在x上再次传播这个线性来实现的。
- en: And then we output way multiplied by v。 So v is the elements that we aggregate
    or the vectors that we aggregate instead of the raw x。 And now of course this
    will make it so that the output here of the single head will be 16 dimensional
    because that is the head size。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们输出通过v乘以的权重。因此，v是我们聚合的元素或我们聚合的向量，而不是原始的x。当然，这将使得单一头的输出是16维的，因为这是头的大小。
- en: So you can think of x as kind of like private information to this token if you
    think about it that way。 So x is kind of private to this token。 So I'm a fifth
    token and I have some identity and my information is kept in vector x。 And now
    for the purposes of the single head here's what I'm interested in。 Here's what
    I have and if you find me interesting here's what I will communicate to you。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以将x视为这个令牌的私有信息，如果你这么想的话。x对这个令牌是私有的。我是第五个令牌，我有一些身份，我的信息保存在向量x中。现在对于单一头，我感兴趣的是我拥有的内容，如果你觉得我有趣，我将与你沟通的内容。
- en: And that's stored in v。 And so v is the thing that gets aggregated for the purposes
    of this single head between the different nodes。 And that's basically the self-attention
    mechanism。 This is what it does。 There are a few notes that I would like to make
    about attention。 Number one。 attention is a communication mechanism。 You can really
    think about it as a communication mechanism where you have a number of nodes in
    a directed graph where basically you have edges pointed between nodes like this。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这存储在v中。因此，v是为了这个单一头而在不同节点之间聚合的东西。这基本上就是自注意力机制。这就是它的作用。我想对注意力做几个说明。第一，注意力是一种通信机制。你可以真的把它看作是一个通信机制，其中有多个节点在一个有向图中，节点之间基本上有这样的边指向。
- en: And what happens is every node has some vector of information and it gets to
    aggregate information via a weighted sum from all nodes that point to it。 And
    this is done in a data dependent manner。 So depending on whatever data is actually
    stored at each node at any point in time。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都有一些信息向量，并通过指向它的所有节点的加权和来聚合信息。这是以数据依赖的方式进行的。因此，取决于每个节点在任何时刻实际存储的数据。
- en: Now our graph doesn't look like this。 Our graph has a different structure。 We
    have eight nodes because the block size is eight and there's always eight top
    tokens。 And the first node is only pointed to by itself。 The second node is pointed
    to by the first node and itself all the way up to the eighth node which is pointed
    to by all the previous nodes and itself。 And so that's the structure that our
    directed graph has or happens to have in other aggressive sort of scenario like
    language modeling。
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的图看起来不是这样的。我们的图有不同的结构。我们有八个节点，因为块大小是八，并且总是有八个顶级令牌。第一个节点只被它自己指向。第二个节点被第一个节点和它自己指向，一直到第八个节点，它被所有之前的节点和它自己指向。这就是我们的有向图所具有的结构，或者在其他激进的场景中，比如语言建模。
- en: But in principle attention can be applied to any arbitrary directed graph and
    it's just a communication mechanism between the nodes。 The second note is that
    notice that there's no notion of space。 So attention simply acts over like a set
    of vectors in this graph。 And so by default these nodes have no idea where they
    are positioned in the space。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 但原则上，注意力可以应用于任何任意的有向图，它只是节点之间的通信机制。第二点是要注意没有空间的概念。因此，注意力简单地作用于这个图中的一组向量。因此，默认情况下，这些节点不知道它们在空间中的位置。
- en: And that's why we need to encode them positionally and sort of give them some
    information that is anchored to a specific position so that they sort of know
    where they are。 And this is different than for example from convolution because
    if you run for example a convolution operation over some input。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们需要对它们进行位置编码，并给它们一些锚定于特定位置的信息，以便它们知道自己所在的位置。这与卷积是不同的，因为如果你在某些输入上运行卷积操作。
- en: There is a very specific sort of layout of the information in space and the
    convolutional filters sort of act in space。 And so it's not like an attention。
    An attention is just a set of vectors out there in space they communicate and
    if you want them to have a notion of space you need to specifically add it。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 信息在空间中有非常具体的布局，而卷积滤波器在空间中起作用。因此，这与注意力不同。注意力仅仅是一组在空间中交流的向量，如果你希望它们有空间的概念，你需要特别添加它。
- en: Which is what we've done when we calculated the relative the positional encodings
    and added that information to the vectors。 The next thing that I hope is very
    clear is that the elements across the batch dimension which are independent examples
    never talk to each other。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在计算相对位置编码并将该信息添加到向量时所做的。我希望很清楚的是，跨批次维度的元素（独立示例）永远不会彼此交流。
- en: They're always processed independently and this is a bashed matrix multiply
    that applies basically a matrix multiplication kind of imperilable across the
    batch dimension。 So maybe it would be more accurate to say that in this analogy
    of a directed graph we really have because the backside is four。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 它们始终是独立处理的，这是一个批量矩阵乘法，基本上是对批次维度进行的矩阵乘法。因此，也许更准确地说，在这个有向图的类比中，我们实际上是因为背面有四个。
- en: We really have four separate pools of eight nodes and those eight nodes only
    talk to each other but in total there's like 32 nodes that are being processed。
    But there's sort of four separate pools of eight you can look at that way。 The
    next note is that here in the case of language modeling we have this specific
    structure of directed graph where the future tokens will not communicate to the
    past tokens。 But this doesn't necessarily have to be the constraint in the general
    case。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上有四个独立的八个节点池，这八个节点只彼此交流，但总共有大约32个节点在被处理。不过，你可以这样看，四个独立的八个节点池。下一个要点是，在语言建模的情况下，我们有一个特定的有向图结构，未来的标记不会与过去的标记进行通信。但在一般情况下，这并不一定是限制。
- en: And in fact in many cases you may want to have all of the nodes talk to each
    other fully。 So as an example if you're doing sentiment analysis or something
    like that with a transformer you might have a number of tokens and you may want
    to have them all talk to each other fully。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在许多情况下，你可能希望所有节点完全相互交流。例如，如果你正在进行情感分析或类似的任务，你可能会有多个标记，并希望它们完全交流。
- en: Because later you are predicting for example the sentiment of the sentence。
    And so it's okay for these nodes to talk to each other。 And so in those cases
    you will use an encoder block of self-attention。 And all it means that it's an
    encoder block is that you will delete this line of code allowing all the nodes
    to completely talk to each other。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因为后来你要预测句子的情感。例如，这些节点之间可以相互交流。因此，在这些情况下，你会使用自注意力的编码器块。编码器块意味着你会删除这行代码，使所有节点能够完全相互交流。
- en: What we're implementing here is sometimes called a decoder block。 And it's called
    a decoder because it is sort of like decoding language and it's got this autoregressive
    format where you have to mask with the trindula matrix so that no code is not。
    And this is a case of a decoder block。 And so basically in encoder blocks you
    would delete this。 allow all the nodes to talk。 In decoder blocks this will always
    be present so that you have this trindula structure。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里实现的有时被称为解码器块。之所以称为解码器，是因为它有点像解码语言，并且它具有自回归格式，你必须使用三角矩阵进行掩蔽，以确保没有代码。这是解码器块的一个案例。因此，基本上在编码器块中你会删除这个，允许所有节点交流。在解码器块中，这将始终存在，以保持这种三角结构。
- en: But both are allowed and attention doesn't care。 Attention supports arbitrary
    connectivity between nodes。 The next thing I wanted to comment on is you keep
    me， you keep hearing me say attention。 self-attention， etc。 There's actually also
    something called cross-attention。 What is the difference？
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 但两者都是允许的，注意力并不在意。注意力支持节点之间的任意连接。接下来我想评论的是你让我不断提到注意力。自注意力等。实际上，还有一种叫做交叉注意力的东西。它们有什么区别？
- en: So basically the reason this attention is self-attention is because the keys。
    queries and the values are all coming from the same source from x。 So the same
    source x produces key queries and values。 So these nodes are self-attending。 But
    in principle attention is much more general than that。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这种注意力被称为自注意力的原因是因为键、查询和值都来自同一个源，即 x。因此，同一源 x 生成键、查询和值。因此，这些节点是自我关注的。但原则上，注意力的范围远比这更广泛。
- en: So for example in encoder decoder transformers you can have a case where the
    query is not the same。 So for example in encoder decoder transformers you can
    have a case where the queries are produced from x。 But the keys and the values
    come from a whole separate external source and sometimes from the encoder blocks
    that encode some context that we'd like to condition on。 And so the keys and the
    values will actually come from a whole separate source。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，在编码器-解码器变换器中，你可以遇到一个查询不相同的情况。比如，在编码器-解码器变换器中，你可以有一种情况，查询是从 x 生成的。但键和值来自一个完全独立的外部源，有时来自编码器块，这些块编码了一些我们想要条件的上下文。因此，键和值实际上将来自一个完全独立的源。
- en: Those are nodes on the side and here we're just producing queries and we're
    reading off information from the side。 So cross-attention is used when there's
    a separate source of nodes we'd like to pull information from into our nodes。
    And it's self-attention if we just have nodes that we'd like to look at each other
    and talk to each other。 So this attention here happens to be self-attention。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 那些是在旁边的节点，这里我们仅在生成查询，并从旁边读取信息。因此，当我们希望从独立的节点源中提取信息到我们的节点时，使用的是交叉注意力。如果我们仅有想要彼此查看和交流的节点，那么就是自注意力。因此，这里的注意力恰好是自注意力。
- en: But in principle attention is a lot more general。 Okay and the last note at
    this stage is if we come to the attention is only need paper here。 We've already
    implemented attention so given query key and value we've multiplied the query
    on the key。 We've soft-maxed it and then we are aggregating the values。 There's
    one more thing that we're missing here which is the dividing by one over square
    root of the head size。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 但原则上，注意力要广泛得多。好的，最后一点是，如果我们回到注意力仅需论文这里。我们已经实现了注意力，所以给定查询、键和值，我们已经对查询和键进行了乘法操作。我们进行了
    softmax，然后聚合了值。这里缺少的还有一个步骤，即除以头部大小平方根的倒数。
- en: The decay here is the head size。 Why aren't they doing this one？ It's important。
    So they call it a scaled attention。 And it's kind of like an important normalization
    to basically have。 The problem is if you have unit Gaussian inputs so zero mean
    unit variance。 K and Q are unit Gaussian then if you just do way naively then
    you see that your way actually will be the variance will be on the order of head
    size which in our case is 16。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的衰减是头部大小。为什么他们不这样做呢？这很重要。所以他们称之为缩放注意力。这基本上是一个重要的归一化过程。问题是，如果你有单位高斯输入，即零均值单位方差。K
    和 Q 是单位高斯，那么如果你只是简单地处理，就会发现你的方式的方差实际上将是头部大小的数量级，在我们的情况下是 16。
- en: But if you multiply by one over head size square root so this is square root
    and this is one over then the variance of way will be one so it will be preserved。
    Now why is this important？ You'll notice that way here will feed into softmax。
    And so it's really important especially at initialization that way be fairly diffuse。
    So in our case here we sort of locked out here and way at a fairly diffuse numbers
    here。
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你将其乘以头部大小的平方根的倒数，那么方差将会是 1，所以它会被保留。那么，为什么这很重要呢？你会注意到，这里的方式会输入到 softmax 中。因此，特别是在初始化时，方式相对分散是非常重要的。在我们的情况下，这里我们算是锁定了，方式处于相对分散的状态。
- en: So like this。 Now the problem is that because of softmax if weight takes on
    very positive and very negative numbers inside it。 softmax will actually converge
    towards one hot vectors。 And so I can illustrate that here。 Say we are applying
    softmax to a tensor of values that are very close to zero then we're going to
    get a diffuse thing out of softmax。 But the moment I take the exact same thing
    and I start sharpening it making it bigger by multiplying these numbers by eight
    for example。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。现在问题是，由于 softmax，如果权重取非常正和非常负的值，softmax 实际上会收敛到独热向量。因此我可以在这里说明这一点。假设我们将
    softmax 应用于一个非常接近零的值的张量，那么我们将得到一个相对分散的结果。但是一旦我将相同的内容开始锐化，通过将这些数字乘以 8 例如。
- en: You'll see that the softmax will start to sharpen and in fact it will sharpen
    towards the max。 So it will sharpen towards whatever number here is the highest。
    And so basically we don't want these values to be too extreme especially at initialization
    otherwise softmax will be way too peaky。 And you're basically aggregating information
    from like a single node。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到softmax会开始变得更加尖锐，实际上它会向最大值尖锐化。因此，它会朝着这里的最大值变尖锐。因此，基本上我们不希望这些值在初始化时过于极端，否则softmax将变得过于峰值。而你基本上是在从单个节点聚合信息。
- en: Every node just aggregates information from a single other node。 That's not
    what we want especially at initialization。 And so the scaling is used just to
    control the variance at initialization。 Okay so having said all that let's now
    take our self-attention knowledge and let's take it for a spin。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点只从单个其他节点聚合信息。这并不是我们所希望的，特别是在初始化时。因此，缩放仅用于控制初始化时的方差。好了，讲完这些，我们现在来利用我们的自注意力知识试试吧。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_35.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_35.png)'
- en: So here in the code I created this head module and implements a single head
    of self-attention。 So you give it a head size and then here it creates the key
    query and the value linear layers。 Typically people don't use biases in these。
    So those are the linear projections that we're going to apply to all of our nodes。
    Now here I'm creating this trill variable。 Trill is not a parameter of the module。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的代码中，我创建了这个头模块，并实现了一个自注意力的单头。你给它一个头的大小，然后它创建了键、查询和值的线性层。通常人们在这些层中不使用偏置。因此，这些是我们将应用于所有节点的线性投影。现在我在这里创建这个trill变量。trill不是模块的参数。
- en: So in sort of pytorch naming conventions this is called a buffer。 It's not a
    parameter and you have to assign it to the module using a register buffer。 So
    that creates the trill。 The lower triangular matrix。 And when we're given the
    input X this should look very familiar now。 We calculate the keys。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch命名约定中，这被称为缓冲区。它不是参数，你必须通过注册缓冲区将其分配给模块。这样就创建了trill。下三角矩阵。当我们输入X时，这现在应该看起来非常熟悉。我们计算键。
- en: the queries， we calculate the attention scores in segue。 We normalize it so
    we're using scaled attention here。 Then we make sure that sure doesn't communicate
    with the past。 So this makes it a decoder block。 And then softmax and then aggregate
    the value and output。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 查询，我们计算在segue中的注意力得分。我们对其进行归一化，因此我们在这里使用缩放的注意力。然后我们确保它不与过去的内容进行交互。因此，这使它成为一个解码器块。然后进行softmax，然后聚合值并输出。
- en: Then here in the language model I'm creating a head in the constructor and I'm
    calling it self-attention head。 And the head size I'm going to keep as the same
    and embed。 Just for now。 And then here once we've encoded the information with
    the token embeddings and the position embeddings。 We're simply going to feed it
    into the self-attention head。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在语言模型中，我在构造函数中创建一个头，称为自注意力头。头的大小我将保持不变并嵌入。就目前而言。然后在这里，一旦我们通过令牌嵌入和位置嵌入对信息进行了编码。我们将简单地将其输入自注意力头。
- en: And then the output of that is going to go into the decoder language modeling
    head and create the logits。 So this is sort of the simplest way to plug in a self-attention
    component into our network right now。 I have to make one more change which is
    that here in the generate we have to make sure that our。 IDX that we feed into
    the model。 Because now we're using positional embeddings we can never have more
    than block size coming in。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后该输出将进入解码器语言建模头，并创建logits。因此，这是将自注意力组件插入到我们网络中的最简单方式。我要再做一个更改，就是在生成时，我们必须确保输入模型的IDX。因为现在我们正在使用位置嵌入，我们永远不能传入超过块大小的内容。
- en: Because if IDX is more than block size then our position embedding table is
    going to run out of scope。 Because it only has embeddings for up to block size。
    And so therefore I added some code here to crop the context that we're going to
    feed into self。 So that we never pass in more than block size elements。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果IDX超过块大小，那么我们的位置信息嵌入表将超出范围。因为它只有到块大小的嵌入。因此我在这里添加了一些代码，以裁剪我们将要输入自注意力的上下文。这样我们就永远不会传入超过块大小的元素。
- en: So those are the changes and let's now train the network。 Okay so I also came
    up to the script here and I decreased the learning rate because the self-attention
    can't tolerate very very high learning rates。 And then I also increased the number
    of iterations because the learning rate is lower。 And then I trained it and previously
    we were only able to get to up to 2。5 and now we are down to 2。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是更改，现在让我们训练网络。好的，我也修改了脚本，降低了学习率，因为自注意力无法容忍非常高的学习率。然后我还增加了迭代次数，因为学习率较低。然后我进行了训练，之前我们只能达到2。5，现在降到了2。
- en: 4。 So we definitely see a little bit of improvement from 2。5 to 2。4 roughly
    but the text is still not amazing。 So clearly the self-attention has been doing
    some useful communication but we still have a long way to go。
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 4。我们确实看到从2。5到2。4有一点点改善，但文本仍然不够出色。所以显然自注意力在某种程度上进行了有用的交流，但我们仍然任重道远。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_37.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_37.png)'
- en: Okay so now we've implemented the scale。product。attention。 Now next up in the
    attention is all you need paper。 There's something called multi-head attention。
    And what is multi-head attention？ It's just applying multiple attentions in parallel
    and concatenating the results。 So they have a little bit of diagram here。 I don't
    know if this is super clear。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们实现了缩放点积注意力。在《注意力机制是你所需要的一切》论文中，有一种叫做多头注意力的东西。那么多头注意力是什么呢？它只是将多个注意力并行应用并拼接结果。所以这里有一个小图示。我不知道这是否非常清楚。
- en: It's really just multiple attentions in parallel。
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上只是多个注意力并行存在。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_39.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_39.png)'
- en: So let's implement that fairly straightforward。 If we want a multi-head attention
    then we want multiple heads of self-attention running in parallel。 So in PyTorch
    we can do this by simply creating multiple heads。 So however many heads you want
    and then what is the head size of each。 And then we run all of them in parallel
    into a list and simply concatenate all of the outputs。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们实现这个过程相对简单。如果我们想要多头注意力，那么我们需要并行运行多个自注意力头。因此在PyTorch中，我们只需创建多个头，想要多少头，每个头的大小是多少。然后我们将它们并行运行并简单地拼接所有输出。
- en: And we're concatenating over the channel dimension。 So the way this looks now
    is we don't have just a single attention that has a head size of 32。 Because remember
    N-Embed is 32。 Instead of having one communication channel we now have four communication
    channels in parallel。 And each one of these communication channels typically will
    be smaller correspondingly。
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在通道维度上进行拼接。所以现在的样子是，我们不再只有一个头大小为32的单一注意力。因为请记住N-Embed是32。我们现在有四个并行的通信通道，而每个通信通道通常会相应地更小。
- en: So because we have four communication channels we want eight dimensional self-attention。
    And so from each communication channel we're going to gather eight dimensional
    vectors。 And then we have four of them and that concatenates to give us 32 which
    is the original N-Embed。 And so this is kind of similar to if you're familiar
    with convolutions this is kind of like a group convolution。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有四个通信通道，所以我们想要八维自注意力。因此从每个通信通道我们将收集八维向量。然后我们有四个，这些拼接在一起给我们32，即原始的N-Embed。这种方式类似于如果你熟悉卷积，这就有点像组卷积。
- en: Because basically instead of having one large convolution we do convolution
    in groups。 And that's multi-headed self-attention。 And so then here we just use
    S/A heads self-attention heads instead。 Now I actually ran it and scrolling down。
    I ran the same thing and then we now get this down to 2。28 roughly。 And the operation
    is still not amazing but clearly the validation loss is improving because we were
    at 2。
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们不是进行一次大型卷积，而是分组进行卷积。这就是多头自注意力。因此在这里我们使用S/A头的自注意力头。我实际上运行了它，向下滚动。我运行了相同的操作，现在我们大约降到了2。28。操作仍然不算出色，但显然验证损失在改善，因为我们之前是2。
- en: 4 just now。 And so it helps to have multiple communication channels because
    obviously these tokens have a lot to talk about。 They want to find the consonants，
    the vowels， they want to find the vowels just from certain positions。 They want
    to find any kinds of different things。 And so it helps to create multiple independent
    channels of communication。 gather lots of different types of data and then decode
    the output。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚是 4。 所以拥有多个通信通道是有帮助的，因为显然这些标记有很多要谈论的内容。 他们想找到辅音，元音，他们想仅从特定位置找到元音。 他们想找到任何不同的东西。
    所以创建多个独立的通信通道是有帮助的。 收集各种不同类型的数据，然后解码输出。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_41.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_41.png)'
- en: Now going back to the paper for a second。 Of course I didn't explain this figure
    in full detail but we are starting to see some components of what we've already
    implemented。 We have the positional encodings， token encodings that add。 we have
    the masked multi-headed attention implemented。 Now here's another multi-headed
    attention which is a cross-attention to an encoder which we haven't。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到论文上。 当然我没有详细解释这个图，但我们开始看到我们已经实现的一些组件。 我们有位置编码，添加的标记编码。 我们已经实现了掩蔽的多头注意力。
    现在这里是另一个多头注意力，这是一个跨编码器的注意力，我们还没有实现。
- en: we're not going to implement in this case。 I'm going to come back to that later。
    But I want you to notice that there's a feed-forward part here and then this is
    grouped into a block that gets repeated again and again。 Now the feed-forward
    part here is just a simple multi-layer projection。 So the multi-headed。 so here
    position wise feed-forward networks is just a simple little MLP。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这种情况下不打算实现。 我稍后会回来讨论这个。 但我想让你注意到这里有一个前馈部分，然后这被分组到一个块中，反复出现。 现在这里的前馈部分只是一个简单的多层投影。
    所以多头注意力。 因此这里的按位置前馈网络只是一个简单的小型 MLP。
- en: So I want to start basically in a similar fashion also adding computation into
    the network。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想基本上也以类似的方式开始，将计算添加到网络中。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_43.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_43.png)'
- en: And this computation is on a per node level。 So I've already implemented it
    and you can see the diff highlighted on the left here when I've added or changed
    things。 Now before we had the multi-headed self-attention that did the communication
    but we went way too fast to calculate the logits。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算是按节点级别进行的。 所以我已经实现了它，您可以在左侧看到我添加或更改的差异高亮。 现在在之前我们有多头自注意力进行通信，但我们计算 logits
    的速度太快了。
- en: So the tokens looked at each other but didn't really have a lot of time to think
    on what they found from the other tokens。 And so what I've implemented here is
    a little feed-forward single layer and this little layer is just a linear followed
    by a row nonlinearity and that's it。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些标记互相看着，但实际上没有太多时间去思考他们从其他标记那里发现的内容。 我在这里实现的是一个小的前馈单层，这个小层只是一个线性层后跟一个非线性层，就这样。
- en: So it's just a little layer and then I call it feed-forward and embed。 And then
    this feed-forward is just called sequentially right after the self-attention。
    So we self-attend then we feed-forward and you'll notice that the feed-forward
    here when it's applying linear。 this is on a per token level。 All the tokens do
    this independently。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是一个小层，我称之为前馈和嵌入。 然后这个前馈是在自注意力之后顺序调用的。 所以我们进行自注意力，然后进行前馈，您会注意到这里的前馈在应用线性时，这是在每个标记级别上。
    所有标记都是独立执行的。
- en: So the self-attention is the communication and then once they've gathered all
    the data。 now they need to think on that data individually。 And so that's what
    feed-forward is doing and that's why I've added it here。 Now when I train this，
    the validation loss actually continues to go down now to 2。24 which is down from
    2。28。 The outputs still look kind of terrible but at least we've improved the
    situation。
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所以自注意力是通信，一旦他们收集到所有数据。 现在他们需要对这些数据进行独立思考。 这就是前馈所做的，因此我在这里添加了它。 现在当我训练这个时，验证损失实际上继续下降，现在降到
    2.24，从 2.28 降下来了。 输出仍然看起来有点糟糕，但至少我们改善了情况。
- en: And so as I preview， we're going to now start to interspers the communication
    with the computation。 And that's also what the transformer does when it has blocks
    that communicate and then compute and it groups them and replicates them。
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如我所预览的，我们现在开始将通信与计算交错。 这也是变换器在具有通信和计算的块时所做的，它将它们分组并复制。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_45.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_45.png)'
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_46.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_46.png)'
- en: Okay， so let me show you what we'd like to do。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我给你展示一下我们想要做的事情。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_48.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_48.png)'
- en: We'd like to do something like this。 We have a block and this block is basically
    this part here except for the cross-attention。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想做一些这样的事情。我们有一个块，这个块基本上就是这里的这一部分，只是缺少交叉注意力。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_50.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_50.png)'
- en: Now the block basically intersperses communication and the computation。 The
    computation is done using multi-headed self-attention and then the computation
    is done using a feed-forward network on all the tokens independently。 Now what
    I've added here also is you'll notice this takes the number of embeddings in the
    embedding dimension and the number of heads that we would like which is kind of
    like group sizing group convolution。 And I'm saying that number of heads we'd
    like is 4 and so because this is 32 we calculate that because this 32 the number
    of heads should be 4。
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个块基本上交错了通信和计算。计算是通过多头自注意力机制完成的，然后在所有标记上独立地使用前馈网络进行计算。现在我在这里添加的内容是，你会注意到这涉及到嵌入维度中的嵌入数量和我们想要的头数，这有点像分组大小的分组卷积。我说我们想要的头数是4，因此因为这是32，所以我们计算得出，32的头数应该是4。
- en: The head size should be 8 so that everything sort of works out channel-wise。
    So this is how the transformer structures sort of the sizes typically。 So the
    head size will become 8 and then this is how we want to interspers them。 And then
    here I'm trying to create blocks which is just a sequential application of block
    block block so that we're interspersing communication feed-forward many many times
    and then finally we decode。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 头大小应该是8，以便所有通道的工作效果。因此，这就是变换器结构通常的大小。头大小将变为8，然后这是我们希望交错它们的方式。接着，我试图创建块，这只是块的顺序应用，以便我们进行许多次的通信前馈，最后我们进行解码。
- en: Now actually try to run this and the problem is this doesn't actually give a
    very good answer。 A very good result。 And the reason for that is we're starting
    to actually get like a pretty deep neural net and deep neural nets suffer from
    optimization issues and I think that's where we're kind of like slightly starting
    to run into。
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在实际上尝试运行这个，问题是这实际上并没有给出一个很好的答案。结果不是很好。原因在于我们开始真正得到一个相当深的神经网络，而深度神经网络会遭受优化问题，我认为这就是我们开始遇到的问题所在。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_52.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_52.png)'
- en: So we need one more idea that we can borrow from the transformer paper to resolve
    those difficulties。 Now there are two optimizations that dramatically help with
    the depth of these networks and make sure that the networks remain optimizable。
    Let's talk about the first one。 The first one in this diagram is you see this
    arrow here and then this arrow and this arrow。 Those are skip connections or sometimes
    called residual connections。
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要从变换器论文中借用一个更多的想法来解决这些困难。现在有两个优化措施可以显著帮助这些网络的深度，并确保网络保持可优化性。让我们谈谈第一个。这个图中的第一个是你看到的这个箭头，然后是这个箭头和这个箭头。那些是跳过连接，或有时称为残差连接。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_54.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_54.png)'
- en: They come from this paper， the procedural learning from a direct mission from
    about 2015 that introduced the concept。
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 它们来自这篇论文，关于2015年直接任务的程序学习，介绍了这个概念。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_56.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_56.png)'
- en: Now these are basically what it means is you transform data but then you have
    a skip connection with addition from the previous features。 Now the way I like
    to visualize it that I prefer is the following。 Here the computation happens from
    the top to bottom and basically you have this residual pathway and you are free
    to fork off from the residual pathway。 perform some computation and then project
    back to the residual pathway via addition。
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些基本上意味着你转换数据，但随后你有一个与之前特征的加法跳过连接。现在我喜欢的可视化方式是这样的。这里的计算是从上到下进行的，基本上你有这个残差路径，并且你可以自由地从残差路径分叉。执行一些计算，然后通过加法再投影回残差路径。
- en: And so you go from the inputs to the targets only the plus and plus and plus。
    And the reason this is useful is because during that propagation remember from
    our micrograt video earlier addition distributes gradients equally to both of
    its branches that fed us the input。
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你从输入到目标仅仅是加加加。这样做的原因是，在传播过程中，请记住，我们之前的微图视频中提到的，加法将梯度均等地分配到输入的两个分支。
- en: And so the supervision or the gradients from the loss basically hop through
    every addition node all the way to the input and then also fork off into the residual
    blocks。 But basically have this gradient super highway that goes directly from
    the supervision all the way to the input unimpeded。
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所以来自损失的监督或梯度基本上通过每个加法节点跳跃到输入，然后也分叉到残差块。但基本上有这个梯度超级高速公路，从监督直接到输入没有阻碍。
- en: And then these visual blocks are usually initialized in the beginning so they
    contribute very very little if anything to the residual pathway。 They are initialized
    that way。 So in the beginning they are almost kind of like not there。 But then
    during the optimization they come online over time and they start to contribute
    but at least at the initialization you can go from directly supervision to the
    input。 Gradient is unimpeded and just close and then the blocks over time kick
    in。
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 而这些视觉块通常在开始时被初始化，因此它们对残差路径几乎没有贡献。它们就是这样初始化的。因此在开始时，它们几乎是不存在的。但在优化过程中，它们会随着时间的推移而上线并开始贡献，但至少在初始化时，你可以直接从监督到输入。梯度没有阻碍，并且就近，然后这些块会随着时间的推移生效。
- en: And so that dramatically helps with the optimization。
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这大大有助于优化。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_58.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_58.png)'
- en: So let's implement this。 So coming back to our block here basically what we
    want to do is we want to do x equals x plus self attention and x equals x plus
    self that feed forward。 So this is x and then we fork off and do some communication
    and come back。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们来实现这个。回到我们的块，这基本上是我们想要做的，即让x等于x加自注意力，x等于x加自前馈。因此这是x，然后我们分叉并进行一些通信再返回。
- en: And we fork off and we do some computation and come back。 So those are residual
    connections。 And then swinging back up here we also have to introduce this projection。
    So and then that linear。 And this is going to be from after we concatenate this
    this is the size and embed。 So this is the output of the self attention itself。
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们分叉，进行一些计算再返回。所以这些就是残差连接。接着回到这里，我们还需要引入这个投影。因此这是线性的。这将是我们在连接之后得到的大小和嵌入。因此这是自注意力的输出。
- en: But then we actually want the to apply the projection and that's the result。
    So the projection is just a linear transformation of the outcome of this layer。
    So that's the projection back into the residual pathway。 And then here in a feed
    forward it's going to be the same thing。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们实际上想要应用投影，这就是结果。因此，投影只是这一层输出的线性变换。这就是投影回到残差路径的过程。而在前馈中，这将是相同的。
- en: I could have a self that projection here as well but let me just simplify it
    and let me couple it inside the same sequential container。 And so this is the
    projection layer going back into the residual pathway。 And so that's it。
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我也可以在这里有一个自投影，但让我简化一下，让我把它放在同一个顺序容器内。因此这是回到残差路径的投影层。就是这样。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_60.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_60.png)'
- en: So now we can train this。 So I'm going to do one more small change。 When you
    look into the paper again you see that the dimensionality of input and output
    is 512 for them。 And they're saying that the inner layer here in the feed forward
    has dimensionality of 2048。 So there's a multiplier of four。 And so the inner
    layer of the feed forward network should be multiplied by four in terms of channel
    sizes。
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们可以训练这个。因此我要再做一个小更改。当你再次查看论文时，你会看到输入和输出的维度对他们来说是512。而他们表示这里的前馈内部层的维度是2048。因此有一个四倍的乘数。因此前馈网络的内部层应该在通道大小上乘以四。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_62.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_62.png)'
- en: So I came here and I multiplied four times embed here for the feed forward。
    And then from four times and embed coming back down to an embed when we go back
    to the projection。 So adding a bit of computation here and growing that layer
    that is in the residual block on the side of the residual pathway。 And then I
    train this and we actually get down all the way to 2。08 validation loss。
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我来了，我在前馈中将嵌入乘以四倍。然后从四倍嵌入回到一个嵌入，当我们回到投影时。因此在这里添加了一些计算，并在残差路径的旁边增长了那个层。我训练了这个，实际验证损失降到了2.08。
- en: And we also see that network is starting to get big enough that our train loss
    is getting ahead of validation loss。 So we start to see like a little bit of overfitting。
    And our generations here are still not amazing but at least you see that we can
    see like is here this now grief sink。 Like this starts to almost look like English。
    So yeah we're starting to really get there。
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到网络开始变得足够大，以至于我们的训练损失领先于验证损失。因此，我们开始看到一些过拟合。我们的生成结果仍然不是很好，但至少你可以看到，这里现在有点像英语的东西。所以是的，我们开始真正接近了。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_64.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_64.png)'
- en: Okay and the second innovation that is very helpful for optimizing very deep
    neural networks is right here。 So we have this addition now that's the residual
    part。 But this norm is referring to something called layer norm。 So layer norm
    is implemented in PyTorch。 It's a paper that came out a while back here。 And layer
    norm is very very similar to Bachelorm。
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，第二个创新对于优化非常深的神经网络非常有帮助，就在这里。所以我们现在有这个加法，这是残差部分。但这个归一化是指层归一化。层归一化在PyTorch中实现。这是之前发布的一篇论文。层归一化与Bachelorm非常相似。
- en: So remember back to our Make More Series part 3。 We implemented Bachelormalization。
    And Bachelormalization basically just made sure that across the batch dimension
    any individual neuron had unit Gaussian distribution。 So with zero mean and unit
    standard deviation one standard deviation upward。 So what I did here is I'm copy
    pasting the Bachelorm 1D that we developed in our Make More Series。
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 所以回想一下我们《Make More Series》第三部分。我们实现了Bachelormalization。Bachelormalization基本上确保在批次维度上，任何单个神经元都具有单位高斯分布。均值为零，标准差为一，向上一个标准差。所以我在这里做的是复制粘贴我们在《Make
    More Series》中开发的Bachelorm 1D。
- en: And see here we can initialize for example this module and we can have a batch
    of 32 100 dimensional vectors feeding through the Bachelorm layer。 So what this
    does is it guarantees that when we look at just the zero column it's a zero mean
    one standard deviation。
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以初始化，例如这个模块，我们可以有一个批次的32个100维向量通过Bachelorm层。这保证了当我们仅查看零列时，它的均值为零，标准差为一。
- en: So it's normalizing every single column of this input。 Now the rows are not
    going to be normalized by default because we're just normalizing columns。 So let's
    not implement layer norm。 It's very complicated。 Look。 we come here we change
    this from zero to one。 So we don't normalize the columns we normalize the rows。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它正在归一化这个输入的每一列。现在行默认情况下不会被归一化，因为我们只是对列进行归一化。所以让我们不实现层归一化。这非常复杂。看。我们在这里把这个从零改成一。因此我们不归一化列，而是归一化行。
- en: And now we've implemented layer norm。 So now the columns are not going to be
    normalized。 But the rows are going to be normalized。 For every individual example
    it's 100 dimensional vector is normalized in this way。 And because our computation
    now does not span across examples we can delete all of this buffers stuff。 Because
    we can always apply this operation and don't need to maintain any running buffers。
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了层归一化。现在列不会被归一化。但行将被归一化。对于每个个体示例，其100维向量以这种方式进行归一化。由于我们的计算现在不跨越示例，我们可以删除所有这些缓冲区的东西。因为我们可以始终应用此操作，而无需维护任何运行缓冲区。
- en: So we don't need the buffers。 We don't。 There's no distinction between training
    and test time。 And we don't need these running buffers。 We do keep gamma and beta。
    We don't need the momentum。 We don't care if it's training or not。 And this is
    now a layer norm。 And it normalizes the rows instead of the columns。
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们不需要缓冲区。我们不需要。训练和测试时间之间没有区别。我们不需要这些运行缓冲区。我们保留伽马和贝塔。我们不需要动量。我们不在乎是否是训练。现在这是一个层归一化。它对行进行归一化，而不是对列进行归一化。
- en: And this here is identical to basically this here。 So let's now implement layer
    norm in our transformer。 Before I incorporate the layer norm I just wanted to
    note that as I said very few details about the transformer have changed in the
    last five years。 But this is actually something that's likely the parts from the
    original paper。
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这里与基本上是相同的。所以现在让我们在变换器中实现层归一化。在我整合层归一化之前，我想提到的是，在过去五年中，关于变换器的细节实际上变化不大。但这实际上是可能来自原论文的部分内容。
- en: You see that the add and norm is applied after the transformation。 But now it
    is a bit more basically common to apply the layer norm before the transformation。
    So there's a reshuffling of the layer norms。 So this is called the pre-norm formulation
    and that the one that we're going to implement as well。 So select deviation from
    the original paper。
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到加法和归一化在变换之后应用。但是现在通常会在变换之前应用层归一化。因此，层归一化的顺序发生了重新排列。这被称为预归一化公式，我们也将实现这一点。所以与原论文有所偏离。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_66.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_66.png)'
- en: Basically we need to in the layer norms layer norm one is an end dot layer norm。
    And we tell it how many words the embedding dimension。 And we need the second
    layer norm。 And then here the layer norms are applied immediately on X。 So self
    dot layer norm one in applied on X。 And self dot layer norm two applied on X。
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们需要在层归一化中，层归一化一是结束的点层归一化。我们告诉它嵌入维度中的单词数。然后需要第二个层归一化。这里层归一化直接应用于X。因此，自点层归一化一应用于X，自点层归一化二也应用于X。
- en: Before it goes into self attention and feed forward。 And the size of the layer
    norm here is an embeds of 32。 So when the layer norm is normalizing our features。
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入自注意力和前馈之前。这里层归一化的大小是32的嵌入。因此，当层归一化规范化我们的特征时。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_68.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_68.png)'
- en: It is the normalization here。 Happens。 The mean and the variance are taken over
    32 numbers。 So the batch and the time act as batch dimensions both of them。
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是归一化。均值和方差是基于32个数字计算的。因此，批次和时间都作为批次维度。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_70.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_70.png)'
- en: So this is kind of like a per token transformation that just normalizes the
    features and makes them unit mean。 unit Gaussian at initialization。
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是逐个标记的变换，仅仅规范化特征，使其在初始化时具有单位均值和单位高斯分布。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_72.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_72.png)'
- en: But of course because these layer norms inside it have these gamma and beta
    trainable parameters。 The layer normal eventually create outputs that might not
    be unit Gaussian。
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 但当然，因为这些层归一化内部有可训练的参数gamma和beta，层归一化最终产生的输出可能不是单位高斯分布。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_74.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_74.png)'
- en: But the optimization will determine that。 So for now this is the this is incorporating
    the layer norms and let's train them up。 Okay， so I let it run and we see that
    we get down to 2。06， which is better than the previous 2。08。 So a slight improvement
    by adding the layer norms。 And I'd expect that they help even more if we have
    bigger and deeper network。
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 但优化会决定这一点。因此，目前这就是将层归一化纳入其中，让我们训练它们。好吧，我让它运行，我们看到损失降到2.06，比之前的2.08要好。通过添加层归一化稍微有所改善。如果我们有更大更深的网络，我预计它们会帮助更多。
- en: One more thing I forgot to add is that there should be a layer norm here also
    typically as at the end of the transformer and right before the final linear layer
    that decodes into vocabulary。 So I added that as well。 So at this stage we actually
    have a pretty complete transformer according to the original paper and it's a
    decoder only transformer。
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我忘了补充的是，这里也应该有一个层归一化，通常在变换器的最后，并在最终的线性层之前解码到词汇表。所以我也添加了这一点。到目前为止，我们实际上有了一个相当完整的变换器，根据原论文，它是一个仅解码的变换器。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_76.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_76.png)'
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_77.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_77.png)'
- en: I'll talk about that in a second。 But at this stage the major pieces are in
    place so we can try to scale this up and see how well we can push this number。
    Now in order to scale up the model I had to perform some cosmetic changes here
    to make it nicer。
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍后会谈到这一点。但在这个阶段，主要部分已经到位，因此我们可以尝试扩展这个模型，看看我们能多大程度地推动这个数字。为了扩大模型，我在这里进行了一些外观上的更改以使其更好看。
- en: So I introduced this variable called n layer which just specifies how many layers
    of the blocks we're going to have。 I create a bunch of blocks and we have a new
    variable number of heads as well。 I pulled out the layer norm here and so this
    is identical。 Now one thing that I did briefly change is I added a dropout。
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我引入了一个叫做 n layer 的变量，它只是指定我们将拥有多少层块。我创建了一堆块，并且我们也有一个新的变量头的数量。我在这里抽出了层归一化，所以这是相同的。现在我简要改变的一件事是我添加了一个
    dropout。
- en: So dropout is something that you can add right before the residual connection
    back。 right before the connection back into the residual pathway。 So we can drop
    out that as the last layer here。 We can dropout here at the end of the multi-headed
    retention as well。 And we can also dropout here when we calculate the basically
    the affinities and after the softmax we can dropout some of those。
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 dropout 是你可以在残差连接回来的前面添加的东西。就在回到残差路径的连接之前。所以我们可以在这里作为最后一层进行 dropout。在多头注意力的末尾也可以进行
    dropout。当我们计算基本的相似性时，在 softmax 之后我们也可以对其中一些进行 dropout。
- en: So we can randomly prevent some of the nodes from communicating。
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以随机防止一些节点之间的通信。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_79.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_79.png)'
- en: And so dropout comes from this paper from 2014 or so and basically it takes
    your neural mat。 And it randomly， every forward backward pass， shuts off some
    subset of neurons。 So randomly drops them to zero and trains without them。 And
    what this does effectively is because the mask of what's being dropped out has
    changed every single forward backward pass。
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 dropout 来自 2014 年左右的这篇论文，基本上它取了你的神经矩阵。每次前向和反向传递时，随机关闭一些神经元的子集。所以随机将它们置为零并在没有它们的情况下进行训练。这样做的效果是，因为每次前向和反向传递时被丢弃的掩码都改变了。
- en: it ends up kind of training an ensemble of subnetworks。 And then at test time
    everything is fully enabled and kind of all of those subnetworks are merged into
    a single ensemble。 If you want to think about it that way。 So I would read the
    paper to get the full detail。 For now we're just going to stay on the level of
    this is a regularization technique。
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最终它训练了一个子网络的集合。在测试时，一切都完全启用，所有这些子网络合并成一个单一的集合。如果你想这样想的话。所以我建议你阅读论文以获取完整的细节。目前我们只会停留在这是一种正则化技术的层面上。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_81.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_81.png)'
- en: And I added it because I'm about to scale up the model quite a bit and I was
    concerned about overfitting。 So now when we scroll up to the top we'll see that
    I changed a number of hyperparameters here about our neural mat。 So I made the
    batch size be much larger now 64。 I changed the block size to be 256。 So previously
    it was just eight characters of context。
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了它，因为我准备将模型扩展得相当大，并且我担心过拟合。所以现在当我们向上滚动到顶部时，我们会看到我在神经矩阵的超参数上进行了很多更改。所以我将批量大小增大到现在的
    64。我将块大小更改为 256。之前的上下文只有八个字符。
- en: Now it is 256 characters of context to predict the 257th。 I brought down the
    learning rate a little bit because the neural mat is now much bigger。 So I brought
    down the learning rate。 The embedding dimension is not 384 and there are six heads。
    So 384 divide six means that every head is 64 dimensional as it as a standard。
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是 256 个字符的上下文来预测第 257 个。我把学习率稍微调低了一点，因为神经矩阵现在大得多。所以我降低了学习率。嵌入维度不是 384，并且有六个头。所以
    384 除以六意味着每个头是 64 维的，这是标准的。
- en: And then there are going to be six layers of that。 And the dropout will be at
    0。2。 So every forward backward pass 20% of all these intermediate calculations
    are disabled and dropped to zero。 And then I already trained this and I ran it。
    So drumroll how does it perform？
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 然后会有六层这样做。dropout 将设置为 0.2。所以每次前向和反向传递，20% 的所有这些中间计算都被禁用并丢弃为零。我已经训练过这个并运行过它。那么，隆重介绍它的表现如何？
- en: So let me just scroll up here。 We get a validation loss of 1。48 which is actually
    quite a bit of an improvement on what we had before which I think was 2。07。 So
    we went from 2。07 all the way down to 1。48 just by scaling up this neural mat
    with the code that we have。 And this of course ran for a lot longer。 This may
    be trained for I want to say about 15 minutes on my A100 GPU。
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我向上滚动一下。我们得到了 1.48 的验证损失，这实际上比之前的 2.07 有了相当大的改善。所以我们从 2.07 一路降到 1.48，仅仅通过使用我们代码扩展这个神经矩阵。当然，这个过程持续了更长时间。我想说这是在我的
    A100 GPU 上训练了大约 15 分钟。
- en: So that's a pretty good GPU。 And if you don't have a GPU you're not going to
    be able to reproduce this。 On a CPU this would be I would not run this on a CPU
    or a MacBook or something like that。 You'll have to break down the number of layers
    and the embedding dimension and so on。 But in about 15 minutes we can get this
    kind of a result and I'm printing some of the Shakespeare here。
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一块相当不错的GPU。如果你没有GPU，就无法复现这个。在CPU上，我不会在CPU或MacBook等设备上运行这个。你必须分解层数和嵌入维度等。但大约15分钟后，我们可以得到这样的结果，我在这里打印了一些莎士比亚的内容。
- en: But what I did also is I printed 10，000 characters so a lot more and I wrote
    them to a file。 And so here we see some of the outputs。 So it's a lot more recognizable
    as the input text file。 So the input text file just for reference looked like
    this。 So there's always like someone speaking in this matter。 And our predictions
    now take on that form。
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 但我还打印了10,000个字符，所以更多，我将它们写入了一个文件。所以我们看到了一些输出。因此，它看起来更像输入文本文件。因此，输入文本文件的样子是这样的。总是有人以这种方式说话。我们的预测现在采用了那种形式。
- en: Except of course they're nonsensical when you actually read them。 So it is every
    crimp to be house。 Oh those preplation。 We give heed。 You know。 Oh oh sent me
    you mighty lord。 Anyway so you can read through this。 It's nonsensical of course
    but this is just a transformer trained on the character level for one million
    characters that come from Shakespeare。 So there's sort of like blabbers on in
    Shakespeare like manner but it doesn't of course make sense at this scale。
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当你真正阅读它们时，它们是无意义的。所以每个房屋都有其特定的要求。哦，那些准备工作。我们要注意。你知道。哦，哦，伟大的主，差我过去。无论如何，你可以浏览这个。显然是无意义的，但这只是一个在字符级别上训练了百万个来自莎士比亚字符的变换器。所以它在莎士比亚的风格上像是在喋喋不休，但在这个规模上当然没有意义。
- en: But I think I think still a pretty good demonstration of what's possible。 So
    now I think that kind of like concludes the programming section of this video。
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为这仍然是一个不错的演示，展示了可能的结果。所以现在我认为这大概结束了这个视频的编程部分。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_83.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_83.png)'
- en: We basically kind of did a pretty good job in implementing this transformer
    but the picture doesn't exactly match up to what we've done。 So what's going on
    with all these additional parts here。 So let me finish explaining this architecture
    and why it looks so funky。 Basically what's happening here is what we implemented
    here is a decoder only transformer。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上在实现这个变换器方面做得相当不错，但图像并不完全匹配我们所做的。那么这些额外的部分到底发生了什么呢？让我完成对这个架构的解释，以及为什么它看起来如此奇怪。基本上，这里实现的是一个仅解码的变换器。
- en: So there's no component here。 This part is called the encoder and there's no
    cross attention block here。 Our block only has a self attention and the feed forward。
    So it is missing this third in between piece here。 This piece does cross attention。
    So we don't have it and we don't have the encoder。
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里没有组件。这个部分叫做编码器，这里没有交叉注意块。我们的块只有自注意和前馈。因此，它缺少这个中间的第三部分。这个部分负责交叉注意。所以我们没有它，也没有编码器。
- en: We just have the decoder and the reason we have a decoder only is because we
    are just generating text and it's unconditioned on anything。 We're just blabbering
    on according to a given data set。 What makes it a decoder is that we are using
    the triangular mask in our transformer。 So it has this autoregressive property
    where we can just go and sample from it。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有解码器，之所以只有解码器，是因为我们只是在生成文本，而不依赖于任何条件。我们只是根据给定的数据集在喋喋不休。使其成为解码器的原因是我们在变换器中使用了三角形掩码。所以它具有自回归特性，我们可以从中进行采样。
- en: So the fact that it's using the triangular mask to mask out the attention makes
    it a decoder and it can be used for language modeling。 Now the reason that the
    original paper had an encoder decoder architecture is because it is a machine
    translation paper。
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它使用三角形掩码来屏蔽注意力，使其成为解码器，并可用于语言建模。原始论文采用编码器-解码器架构的原因是因为这是一个机器翻译论文。
- en: So it is concerned with a different setting in particular。 It expects some tokens
    that encode say for example French and then it is expected to decode the translation
    in English。 So typically these here are special tokens。 So you are expected to
    read in this and condition on it。 And then you start off the generation with a
    special token called start。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它关注的是一个不同的设置，特别是。它预期一些标记，比如说法语的编码，然后预期解码成英语翻译。所以通常这些是特殊标记。您需要读取这些并根据它进行条件化。然后您用一个名为“开始”的特殊标记开始生成。
- en: So this is a special new token that you introduce and always place in the beginning。
    And then the network is expected to output neural networks are awesome and then
    a special end token to finish the generation。 So this part here will be decoded
    exactly as we have done it。 Neural networks are awesome。 will be identical to
    what we did。 But unlike what we did they want to condition the generation on some
    additional information。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个特殊的新标记，您需要在开头引入并始终放置。然后网络预期输出“神经网络太棒了”，再加上一个特殊的结束标记来完成生成。所以这部分将与我们之前做的一样被解码。“神经网络太棒了”将与我们所做的完全相同。但与我们所做的不同，他们希望根据一些额外信息来条件化生成。
- en: And in that case this additional information is the French sentence that they
    should be translating。 And what they do now is they bring the encoder。 Now the
    encoder reads this part here。 So we are only going to take the part of French
    and we are going to create tokens from it exactly as we have seen in our video。
    And we are going to put a transformer on it。 But there is going to be no triangular
    mask and so all the tokens are allowed to talk to each other as much as they want。
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这个额外信息是他们应该翻译的法语句子。他们现在所做的是引入编码器。现在编码器读取这一部分。所以我们将仅提取法语部分，并将其创建为标记，正如我们在视频中看到的那样。我们将其放入一个变换器中。但不会有三角形掩码，因此所有标记都可以尽可能地相互交流。
- en: And they are just encoding whatever the content of this French sentence。 Once
    they've encoded it they basically come out in the top here。 And then what happens
    here is in our decoder which does the language modeling。 There is an additional
    connection here to the outputs of the encoder。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 他们只是在编码这个法语句子的内容。一旦他们编码完成，基本上就会在顶部输出。然后在我们的解码器中进行语言建模时，会有一个额外的连接到编码器的输出。
- en: And that is brought in through cross-attention。 So the queries are still generated
    from X but now the keys and the values are coming from the side。 The keys and
    the values are coming from the top generated by the nodes that came outside of
    the encoder。
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 通过交叉注意力引入。所以查询仍然从X生成，但现在键和值来自侧面。这些键和值来自于来自编码器外部的节点生成的顶部。
- en: And those tops， the keys and the values there， the top of it feed in on the
    side into every single block of the decoder。 And so that is why there is an additional
    cross-attention。 And really what it is doing is it is conditioning the decoding
    not just on the past of this current decoding but also on having seen the fully
    encoded French sentence。 And so it is an encoded decoder model which is why we
    have those two transformers and additional blocks and so on。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这些顶部，键和值，顶部的内容从侧面输入到解码器的每一个块中。这就是为什么会有额外的交叉注意力。实际上，它是将解码不仅仅基于当前解码的过去，还基于已完全编码的法语句子进行条件化的。因此，这是一个编码解码器模型，这就是我们有这两个变换器和额外块等的原因。
- en: So we did not do this because we have nothing to encode。 There is no conditioning。
    We just have a text file and we just want to imitate it。 And that is why we are
    using a decoder only transformer exactly as done in GPT。 Okay so now I wanted
    to do a very brief walkthrough of nano GPT which you can find on my GitHub。
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们这样做并不是因为我们需要编码什么。没有条件。我们只有一个文本文件，只想模仿它。这就是我们使用与GPT完全相同的解码器变换器的原因。好的，现在我想简要介绍一下可以在我的GitHub上找到的nano
    GPT。
- en: And nano GPT is basically two files of interest。 There is trained。py and modeled。py。
    trained。py is all the boilerplate code for training the network。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: nano GPT基本上是两个重要文件。一个是trained.py，另一个是modeled.py。trained.py是训练网络的所有样板代码。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_85.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_85.png)'
- en: It is basically all the stuff that we had here is the training loop。 It is just
    that it is a lot more complicated because we are saving and loading checkpoints
    and pre-trained weights and we are decaying the learning rate and compiling the
    model and using distributed training across multiple nodes or GPUs。
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们这里的所有内容都是训练循环。只是这要复杂得多，因为我们在保存和加载检查点及预训练权重，并且我们在衰减学习率，编译模型，并使用分布式训练跨多个节点或GPU。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_87.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_87.png)'
- en: So the training。py gets a little bit more hairy， complicated。 There is more
    options etc。 But the model。py should look very very similar to what we have done
    here。 In fact the model is almost identical。 So first here we have the causal
    self-attention block and all of this should look very very recognizable to you。
    We are producing queries， keys， values。 We are doing dot products。 We are masking，
    applying softmax。
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 所以training.py变得有点复杂。选项更多等等。但model.py应该看起来非常类似于我们在这里所做的。实际上模型几乎是相同的。所以这里我们有因果自注意力块，所有这些对你来说应该是非常熟悉的。我们在生成查询、键、值。我们在进行点积。我们在掩蔽，应用softmax。
- en: optionally dropping out。 And here we are pulling the values。 What is different
    here is that in our code I have separated out the multi-headed attention into
    just a single individual head。
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的丢弃。这里我们在提取值。不同之处在于，在我们的代码中，我将多头注意力分离为单个头。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_89.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_89.png)'
- en: And then here I have multiple heads and I explicitly concatenate them。
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里我有多个头，我明确地将它们连接起来。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_91.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_91.png)'
- en: Whereas here all of it is implemented in a batched manner inside a single causal
    self-attention。 And so we don't just have a B and a T and a C dimension。 We also
    end up with a fourth dimension which is the heads。 And so it just gets a lot more
    sort of hairy because we have four dimensional array tensors now。
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 而这里则是将所有内容批量处理，放在一个单一的因果自注意力内部。因此我们不仅有B、T和C维度。我们还会有第四个维度，也就是头。这样就变得更加复杂，因为我们现在有四维数组张量。
- en: But it is equivalent mathematically。
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 但在数学上这是等价的。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_93.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_93.png)'
- en: So the exact same thing is happening as what we have。 It's just a bit more efficient
    because all the heads are now treated as a batch dimension as well。
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 所以发生的事情和我们所做的完全相同。只是因为现在所有头也作为一个批处理维度处理，所以效率稍微高一些。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_95.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_95.png)'
- en: Then we have the multiple-way perceptron。 It's using the Galoon nonlinearity
    which is defined here instead of RALU。 And this is done just because OpenAI used
    it and I want to be able to load their checkpoints。 The blocks of the transformer
    are identical。 The communicate and the compute phase as we saw。 And then the GPT
    will be identical。 We have the position encodings， token encodings， the blocks。
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有多路感知器。它使用的是这里定义的Galoon非线性，而不是RALU。这么做只是因为OpenAI使用了它，我希望能够加载他们的检查点。变压器的块是相同的。我们看到的通信和计算阶段。然后GPT将是相同的。我们有位置编码、标记编码和这些块。
- en: the layer norm at the end， the final linear layer。 And this should look all
    very recognizable。 And there's a bit more here because I'm loading checkpoints
    and stuff like that。 I'm separating out the parameters into those that should
    be weight-decade and those that shouldn't。 But the generate function should also
    be very very similar。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的层归一化，最终的线性层。这些都应该看起来非常熟悉。这里还有一些额外的内容，因为我在加载检查点和其他东西。我将参数分离为那些应该衰减权重和那些不应该的。但是生成函数也应该非常类似。
- en: So a few details are different but you should definitely be able to look at
    this file and be able to understand a lot of the pieces now。 So let's now bring
    things back to Chacupt。 What would it look like if we wanted to train Chacupt
    ourselves and how does it relate to what we learned today？
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有一些细节不同，但你绝对可以查看这个文件，并能够理解很多部分。现在让我们把事情带回Chacupt。如果我们想自己训练Chacupt，它会是什么样子，如何与我们今天学到的内容相关？
- en: Well to train in Chacupt there are roughly two stages。 First is the pre-training
    stage and then the fine-tuning stage。 In the pre-training stage we are training
    on a large chunk of internet and just trying to get a first decoder-only transformer。
    To babble text。 So it's very very similar to what we've done ourselves。
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在Chacupt训练大约有两个阶段。第一个是预训练阶段，第二个是微调阶段。在预训练阶段，我们在大量的互联网数据上进行训练，只是试图获得一个首个仅解码器的变换器。以便喋喋不休文本。所以这非常非常类似于我们自己所做的。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_97.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_97.png)'
- en: Except we've done like a tiny little baby pre-training step。 And so in our case
    this is how you print a number of parameters。 I printed it and it's about 10 million。
    So this transformer that I created here to create little Shakespeare transformer
    was about 10 million parameters。 Our dataset is roughly 1 million characters。
    So roughly 1 million tokens。
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们做了一个小小的预训练步骤。因此在我们的情况下，这就是你打印参数的数量。我打印了，约为1000万个。因此，我在这里创建的这个小莎士比亚变换器大约是1000万个参数。我们的数据集大约是100万个字符。所以大约是100万个标记。
- en: But you have to remember that opening eyes is different vocabulary。 They're
    not on the character level。 They use these sub-word chunks of words。 And so they
    have a vocabulary of 50，000 roughly elements。 And so their sequences are a bit
    more condensed。 So our dataset。
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 但你必须记住，开放眼的词汇是不同的。他们不是在字符级别上。他们使用这些单词的子词块。因此，他们的词汇量大约有50,000个元素。因此，他们的序列相对更加紧凑。所以我们的数据集。
- en: the Shakespeare dataset would be probably around 300。000 tokens in the opening
    eye vocabulary roughly。 So we trained about 10 million parameter model on roughly
    300。000 tokens。
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 威廉·莎士比亚数据集的词汇量大约在300,000个标记左右。因此，我们在大约300,000个标记上训练了一个约10百万参数的模型。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_99.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_99.png)'
- en: Now when you go to the GPT-3 paper and you look at the transformers that they
    trained。 they trained a number of transformers of different sizes。 But the biggest
    transformer here has 175 billion parameters。 So ours is again 10 million。 They
    used this number of layers in a transformer。 This is the end in bed。
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当你查看GPT-3论文，看看他们训练的变换器时，他们训练了多种不同大小的变换器。但这里最大的变换器有1750亿个参数。因此我们的模型再次是1000万个。他们在一个变换器中使用了这个层数。这是结束的嵌入。
- en: This is the number of heads。 And this is the head size。 And then this is the
    batch size。 So ours was 65。 And the learning rate is similar。 Now when they train
    this transformer。 they trained on 300 billion tokens。 So again， remember ours
    is about 300，000。 So this is about a million fold increase。 And this number would
    not be even that large by today's standards。
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这是头的数量。这是头的大小。这是批量大小。所以我们的批量大小是65。学习率相似。现在，当他们训练这个变换器时，他们训练了3000亿个标记。因此，请记住，我们大约是300,000。所以这是大约百万倍的增加。按照今天的标准，这个数字甚至不会那么大。
- en: You'd be going up 1 trillion above。 So they are training a significantly larger
    model on a good chunk of the internet。 And that is the pre-training stage。 But
    otherwise these hyperparameters should be fairly recognizable to you。 And the
    architecture is actually like nearly identical to what we implemented ourselves。
    But of course it's a massive infrastructure challenge to train this。
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你将超过1万亿。所以他们在互联网上训练了一个显著更大的模型。这就是预训练阶段。否则，这些超参数对你来说应该是相当熟悉的。而且架构实际上与我们自己实施的几乎相同。但是，当然，训练这个是一个巨大的基础设施挑战。
- en: You're talking about typically thousands of GPUs having to talk to each other
    to train models of this size。 So that's just the pre-training stage。 Now after
    you complete the pre-training stage。 you don't get something that responds to
    your questions with answers。 And it's not helpful and etc。 You get a document
    complete。
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 你谈论的是通常需要数千个GPU互相交流以训练这种规模的模型。所以这只是预训练阶段。现在在完成预训练阶段后，你不会得到一个能够回答你问题的响应。而且这并没有帮助，等等。你得到的是一个完整的文档。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_101.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_101.png)'
- en: So it babbles， but it doesn't babbles Shakespeare。 It babbles internet。
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它喋喋不休，但它不喋喋不休莎士比亚。它喋喋不休的是互联网。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_103.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_103.png)'
- en: It will create arbitrary news articles and documents。 And it will try to complete
    documents because that's what it's trained for。 It's trying to complete the sequence。
    So when you give it a question。 it would just potentially just give you more questions。
    It would follow with more questions。
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 它会创建任意的新闻文章和文档，并尝试完成文档，因为那是它所训练的内容。它试图完成这个序列。所以当你给它一个问题时，它可能只会给你更多的问题。它会继续提出更多问题。
- en: It will do whatever it looks like some close document would do in the training
    data on the internet。 And so who knows， you're getting kind of like undefined
    behavior。 It might basically answer to questions with other questions。 It might
    ignore your question。 It might just try to complete some news article。 It's totally
    unaligned as we say。
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 它将会执行看起来像互联网训练数据中某个接近文档的内容。因此，谁知道，你可能会得到一些不确定的行为。它可能会用其他问题回答你的问题，可能会忽略你的问题，或者只是尝试完成一些新闻文章。正如我们所说，它是完全未对齐的。
- en: So the second fine-tuning stage is to actually align it to be an assistant。
    And this is the second stage。
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第二个微调阶段是将其实际对齐为助手。这是第二个阶段。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_105.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_105.png)'
- en: And so this chat GPT blog post from Upaniya talks a little bit about how the
    stage is achieved。
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Upaniya的这篇聊天GPT博客文章稍微谈了一下如何实现这个阶段。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_107.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_107.png)'
- en: We basically， there's roughly three steps to this stage。 So what they do here
    is they start to collect training data that looks specifically like what an assistant
    would do。 So they have documents that have the format where the question is on
    top and then an answer is below。 And they have a large number of these， but probably
    not on the order of the internet。
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个阶段大致分为三个步骤。所以他们在这里所做的是开始收集看起来特别像助手会做的训练数据。因此，他们有格式为问题在上，答案在下的文档。他们有大量这样的文档，但可能没有互联网那么多。
- en: This is probably on the order of maybe thousands of examples。 And so they then
    fine-tuned the model to basically only focus on documents that look like that。
    And so you're starting to slowly align it。 So it's going to expect a question
    at the top and it's going to expect to complete the answer。 And these very， very
    large models are very sample efficient during their fine-tuning。
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能大约有数千个示例。因此，他们对模型进行了微调，基本上只关注看起来像那样的文档。于是你开始慢慢地进行对齐。因此，它会期待顶部有一个问题，并期待完成答案。这些非常，非常大的模型在微调过程中非常有效。
- en: So this actually somehow works。 But that's just step one。 That's just fine-tuning。
    So then they actually have more steps where， okay。 the second step is you let
    the model respond and then different graders look at the different responses and
    rank them for their preferences to which one is better than the other。 They use
    that to train the reward model。 So they can predict basically using a different
    network how much of any candidate response would be desirable。
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这实际上以某种方式有效。但这只是第一步，仅仅是微调。因此，他们实际上还有更多步骤，第二步是让模型响应，然后不同的评分者查看不同的响应，并根据他们的偏好对其进行排名，以决定哪个更好。他们用这个来训练奖励模型，以便能够基本上使用不同的网络预测任何候选响应的吸引力。
- en: And then once they have a reward model， they run PPO。 which is a form of policy
    gradient reinforcement learning optimizer to fine-tune this sampling policy so
    that the answers that the GPT chat GPT now generates are expected to score a high
    reward according to the reward model。
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦他们有了奖励模型，他们就会运行PPO，这是一种策略梯度强化学习优化器，用来微调这个采样策略，以便GPT聊天现在生成的答案能够根据奖励模型获得高分。
- en: And so basically there's a whole aligning stage here or fine-tuning stage。 It's
    got multiple steps in between there as well。 And it takes the model from being
    a document completer to a question-answer。 And that's like a whole separate stage。
    A lot of this data is not available publicly。 It is internal to open AI。 And it's
    much harder to replicate this stage。
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上这里有一个完整的对齐阶段或微调阶段。中间还有多个步骤。它将模型从文档补全器转变为问答助手。这是一个完全不同的阶段。很多数据并未公开，它是开放AI的内部数据，复制这个阶段要困难得多。
- en: And so that's roughly what would give you a chat GPT。 And nano GPT focuses on
    the pre-training stage。 Okay。 and that's everything that I wanted to cover today。
    So we trained to summarize a decoder-only transformer following this famous paper
    "Attention is All You Need" from 2017。
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这大致上就是会给你一个聊天GPT的东西。nano GPT专注于预训练阶段。好的，这就是我今天想要覆盖的所有内容。因此，我们训练了一个仅解码器的变换器来总结，遵循2017年那篇著名论文《Attention
    is All You Need》。
- en: And so that's basically a GPT。 We trained it on a tiny Shakespeare and got sensible
    results。
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这基本上就是一个GPT。我们在一小部分莎士比亚文本上进行了训练，并得到了合理的结果。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_109.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_109.png)'
- en: All of the training code is roughly 200 lines of code。 I will be releasing this
    code base。 So also it comes with all the Git log commits along the way as we built
    it up。
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的训练代码大约有200行。我将发布这个代码库。因此，它也包含我们构建过程中的所有Git日志提交。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_111.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_111.png)'
- en: In addition to this code， I'm going to release the notebook， of course， the
    Google Colab。 And I hope that gives you a sense for how you can train these models
    like， say， GPT-3。 That will be architecturally basically identical to what we
    have。 But they are somewhere between 10。000 and 1 million times bigger depending
    on how you count。 And so that's all I have for now。
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些代码，我还将发布笔记本，当然还有Google Colab。我希望这能让你了解如何训练这些模型，比如说GPT-3。它在架构上基本上与我们拥有的相同。但它们的大小在10,000到100万倍之间，具体取决于你如何计算。所以这就是我现在要说的全部内容。
- en: We did not talk about any of the fine-tuning stages that would typically go
    on top of this。 So if you're interested in something that's not just language
    modeling， but you actually want to。 say， perform tasks， or you want them to be
    aligned in a specific way。 or you want to detect sentiment or anything like that。
    Basically。
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有讨论通常会在此基础上进行的微调阶段。如果你对的不仅仅是语言建模感兴趣，而是想要执行任务，或者希望它们以特定方式对齐，或者想检测情感等内容，基本上。
- en: anytime you don't want something that's just a document-compleiter。 you have
    to complete further stages of fine-tuning， which we did not cover。 And that could
    be simple supervised fine-tuning， or it can be something more fancy like we see
    in ChaiChaPT。 where we actually train a reward model and then do rounds of PPO
    to align it with respect to the reward model。
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你不想要仅仅是一个文档补全器时，你必须完成进一步的微调阶段，而我们没有讨论。这可能是简单的监督微调，或者像我们在ChaiChaPT中看到的更高级的内容，在那里我们实际上训练一个奖励模型，然后进行PPO的轮次，以便与奖励模型进行对齐。
- en: So there's a lot more that can be done on top of it。 I think for now we're starting
    to get to about two hours mark。 So I'm going to kind of finish here。 I hope you
    enjoyed the lecture。 And yeah， go forth and transform。 See you later。
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，还有很多事情可以在此基础上完成。我认为现在我们开始接近两个小时的时限。所以我将结束这里。希望你喜欢这次讲座。好的，去吧，转变吧。再见。
- en: '![](img/d489e9697e4ec525091637b9ac0b6163_113.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d489e9697e4ec525091637b9ac0b6163_113.png)'
