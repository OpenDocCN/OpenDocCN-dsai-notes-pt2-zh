- en: P15：15. L3_4 Automatic Differentiation - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P15：15. L3_4 自动微分 - Python小能 - BV1CB4y1U7P6
- en: So a lot less storage， some basic metric calculus。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以存储需求更少，一些基本的度量微积分。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_1.png)'
- en: So how many of you guys knows metric calculus before？ OK， so good， like 20%。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 那么有多少人之前学过度量微积分？好的，差不多20%的人。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_3.png)'
- en: But I guess everybody should know scalar derivatives。 So here， y is a scalar。
    x is scalar。 So given y is a function of x， so we're going to compute， dy over
    dx。 Now if a is not function of x。 so what's the results？ 0。 0， good。 So if x
    to the power of m， what's the results？ [INAUDIBLE]， Good。 So that's--， everybody
    should know that exp is dx， exp， log x is 1 over x， and sine is cosine。 Also。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但我想每个人应该都知道标量导数。所以在这里，y是标量，x也是标量。所以给定y是x的函数，我们要计算dy对dx。现在如果a不是x的函数，那结果是什么？0。0，好的。如果x的m次方，结果是什么？[听不清]，好。那么——每个人应该都知道exp的导数是dx，exp，log
    x的导数是1/x，sin的导数是cos。还有。
- en: we have if it's u plus v， we're， going to have du over dx plus du over dx。 If
    the u times v。 we're going to have du over dx times v， plus dv over dx times u。
    Similar for general。 if y is a function of u， u is a function of x， we can write
    it down as dy over du， times du over dx。 So that's it for scalars。 It's pretty
    simple。 The one important concept here is that the derivative。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是u加v，我们将有du对dx加du对dx。如果是u乘v，我们将有du对dx乘以v，加上dv对dx乘以u。对于一般情况，如果y是u的函数，u是x的函数，我们可以写成dy对du，乘以du对dx。所以这是标量的情况。其实很简单。这里一个重要的概念是导数。
- en: is a slope of the tangent line。 So here I draw a figure of the square of x。
    So there is a yellow line。 And at the point x equals to 1， we， know the derivative
    is 2x is 2。 so that the slope of the tangent， line is 2。 So that's an important
    concept。 We're going to use for gradient descent letter。 The other concept here
    is that not every function's。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 是切线的斜率。这里我画了一个x的平方图。这里有一条黄色的线。在x等于1的点，我们知道导数是2x，结果是2。所以切线的斜率是2。这个概念很重要，我们将在梯度下降中使用。另一个概念是并不是每个函数的。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_5.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_5.png)'
- en: defrachable。 Similarly， at the first class， we talk about the L1 norm。 It's
    actually a simple version。 It's the super value of x。 So at x equals to 0， at
    this point。 it's not defrachable。 So every nigh， we can draw every nigh under
    the curve。 And the slope can be the sub-defrachative。 So this is generalized，
    the derivative concept。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不可微。同样地，在第一节课中，我们讨论了L1范数。它实际上是一个简单版本，它是x的绝对值。所以在x等于0时，这一点不可微。因此，每个夜晚，我们可以在曲线下画出每个夜晚。斜率可以是子导数。所以这是一个广义的导数概念。
- en: And it's pretty useful， because we have a lot of functions， in deep learning。
    Actually。 it's not defrachable。 So here， in this particular case， the sub-derivative。
    of absolute value of x equals to 1， if it's f larger than 0， minus 1， x more than
    0。 At x equal to 0， it can be any value between minus 1 and 1。 In practice。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有用，因为我们在深度学习中有很多函数。实际上，它是不可微的。所以在这个特定的例子中，绝对值的子导数在x的绝对值上等于1，如果f大于0，x大于0时是-1。在x等于0时，它可以是-1到1之间的任何值。实际中。
- en: we kind of choose a particular value。 It can be either minus 1， 1， or 0。 Another
    example is the max operator。 It's also an important operator。 We can use for max
    pooling method。 So given max x to 0， if x larger than 0 is 1。 if x more than 0
    is 0。 And at the point x equal to 0， we can choose any value， between 0 and 1。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择一个特定的值。它可以是-1、1或0。另一个例子是最大值运算符。它也是一个重要的运算符。我们可以用于最大池化方法。所以给定最大值x为0，如果x大于0则为1。如果x大于0则为0。并且在x等于0的点，我们可以选择任何值，介于0和1之间。
- en: Usually， we pick up either 1 or 0。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们选择1或0。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_7.png)'
- en: So now， we kind of generalize the scalars to vectors。 So that's a key thing。
    because all the neural networks， machine learning， we cannot talk about vectors。
    and matrix and tensor。 So let's first look at how we can make x and a y into vectors。
    So if both x and a y are scalars， we know the gradients， is a scalar。 So gradients，
    actually。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将标量推广到向量。这是一个关键点，因为所有的神经网络、机器学习中，我们不能不谈向量、矩阵和张量。所以让我们首先看看如何将x和y转化为向量。如果x和y都是标量，我们知道梯度是标量。所以梯度实际上。
- en: if it's a vector， we call the gradients。 So now， if x is a vector， it's a column
    vector。 And y is a scalar， which is the first row。 We cannot get partial y over
    partial x， can still vector。 But now it's a row vector。 Similarly， if x is scalar，
    but a y is a vector。 so the gradients will be a vector， has a same shape as y。
    In the more general case。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是向量，我们称之为梯度。所以现在，如果x是一个向量，它是一个列向量，而y是一个标量，它是第一行。我们无法得到关于x的偏导数，但它仍然是一个向量。但现在它是一个行向量。同样，如果x是标量，而y是向量，那么梯度将是一个向量，具有与y相同的形状。更一般的情况。
- en: both x and y are vectors。 Now， we get the matrix of the gradients。 Now。 we cannot
    dive deep into how they're calculated。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: x和y都是向量。现在，我们得到梯度的矩阵。现在，我们不能深入探讨它们是如何计算的。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_9.png)'
- en: Now， in the first case， x is a n dimensional vector。 It's from x1 to xn。 You
    have n elements here。 Then， y is a scalar。 Then， partial y over partial x is a
    row vector。 And the i's element is a derivative of y， with respect to xi。 So here，
    we should partial notation。 This is a generalized D-rotation to vectors。 So let's
    look at the example here。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在第一个例子中，x是n维向量。它从x1到xn。这里有n个元素。然后，y是一个标量。然后，y关于x的偏导数是一个行向量。第i个元素是y关于xi的导数。所以这里我们应该使用偏导符号。这是向量的广义D-旋转。让我们看一个例子。
- en: If y equals to x1 squared plus 2 times x2 squared， then the first element of
    the gradient。 is with respect to x1 is 2 times x1。 The second element is a respect
    to x2 is 4 times x2。 Now。 the most important thing here， is that the gradient
    is a direction that you change the value， most。 So in particular， at this point，
    x1 equals to 1， x2 equals to 1， we have a gradient is 2 and 4。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果y等于x1的平方加上2倍x2的平方，那么梯度的第一个元素是关于x1的偏导数是2倍x1。第二个元素是关于x2的偏导数是4倍x2。现在，最重要的是，梯度是你改变值最多的方向。所以特别是在这个点，当x1等于1，x2等于1时，梯度是2和4。
- en: So that is a point of direction perpendicular， to the counter line。 which if
    we move the point along， these gradients， we can increase this value。 We can increase
    the value of y。 So if we can decrease this value。 we just move along the opposite
    direction of the gradients。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个垂直于反向线的方向点。如果我们沿着这些梯度移动该点，我们可以增加这个值。我们可以增加y的值。所以，如果我们减少这个值，我们只需沿着梯度的反方向移动。
- en: So that concept is the fundamental thing， for the gradient descent for how we
    actually。 solve the new anorems。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个概念是梯度下降法的基础，决定了我们如何解决新的问题。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_11.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_11.png)'
- en: Then， look some examples here。 Why the scalar？ And now x is the undimension
    vector。 A is a non-function of x， so what's the result here？ You do element by
    element 0。 It's a row vector of 0， which 0 is， a undimension of an nth vector，
    but it's a row vector。 So how about 8 times u use a function of x？ So it's actually
    8 times the gradient of u。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，看看这里的一些例子。为什么是标量？现在x是无维向量。A是x的非函数，那么结果是什么？你按元素做0。它是一个0的行向量，0是一个n维向量的无维形式，但它是一个行向量。那么8倍u是x的函数呢？所以它实际上是8倍u的梯度。
- en: with respect to x。 Another one， if we can assign element of x to get a scalar。
    then we get the all-one vectors。 It's a row vector again。 And the last one is
    the square of l2 long。 Then we get 2 times the transpose of x。 We can still have
    all these rules here。 u plus v， u times v。 and similar to the scalar options。
    The only thing here is， like， in the product of u and v。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于x的。另一个，如果我们可以分配x的元素来得到一个标量。然后我们得到全1向量。它再次是一个行向量。最后一个是l2范数的平方。然后我们得到2倍的x的转置。我们仍然可以有这些规则。u加v，u乘v，类似于标量操作。这里唯一不同的是，在u和v的乘积中。
- en: we get u transpose times partial v over partial x， plus the transport of v times
    partial u。 partial over partial x。 So we know that u is a row vector， and then
    partial v and partial x。 that's a matrix。 A row vector times a matrix， you still
    get a row vector。 So as a result。 we still get the row vector， that's graded。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到u转置乘以对v关于x的偏导数，再加上v的传递乘以u关于x的偏导数。所以我们知道u是一个行向量，偏导数v和偏导数x是一个矩阵。行向量乘矩阵，仍然得到一个行向量。所以结果是我们仍然得到一个行向量，这是梯度。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_13.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_13.png)'
- en: OK， so now if we can switch--。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在如果我们可以切换--。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_15.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_15.png)'
- en: question？ [INAUDIBLE]， Yes， so we cannot talk about--。 is that actually on the--
    if both are vectors。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 问题？[无法听清]，是的，所以我们不能谈论--。如果它们实际上是--如果两个都是向量。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_17.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_17.png)'
- en: we call the Jacobi matrix。 So now we switch。 Why is the vector？ X is scalar。
    So in this case。 we cannot get the column vector here。 So the i's element of the
    vector is the yi， partial yi。 over partial x。 So y-- we changed the order。 So
    remember that if x is a vector。 we get the row vector。 If a y is vector， we get
    a column vector。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为雅可比矩阵。所以现在我们交换。为什么是向量？x是标量。所以在这种情况下，我们不能得到列向量。所以向量的第i个元素是yi，相对于x的偏导数。所以y——我们改变了顺序。所以记住，如果x是向量，我们得到行向量。如果y是向量，我们得到列向量。
- en: So this called the numerator layout。 We can transport as well。 We can let--
    if x is vector。 we get the column vector。 And if y is a vector， we get the row
    vector。 We can just switch。 then we could denominator layout。 So that's a homework
    we're going to have in front。 But actually。 we can change this to--， we are going
    to make a little bit trouble later。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这叫做分子布局。我们也可以做转置。如果x是向量，我们得到列向量。如果y是向量，我们得到行向量。我们可以互换。然后我们可以得到分母布局。所以这就是我们将要做的作业。实际上，我们稍后会改变这个——我们稍后会稍微麻烦一点。
- en: but we have a homework for that。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们有一个作业要做。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_19.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_19.png)'
- en: In the last case， both y and x is a vector。 So the matrix we call the Jacobi
    matrix。 So we can look into that。 We know that if y is vector， we get our column
    vector。 So this means we have--， each row is we are being the partial yi over
    x。 And partial yi over vector x is going to row matrix。 Sorry， it's a row vector。
    So at the end。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的情况中，y和x都是向量。所以我们称之为雅可比矩阵。我们可以深入了解。我们知道如果y是向量，我们得到列向量。所以这意味着我们有——每一行是我们求得偏导yi相对于x。而偏导yi相对于向量x将会是行矩阵。抱歉，是行向量。所以最后。
- en: we got the matrix here。 So we can give some examples， having more clear understanding。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这个矩阵。所以我们可以举一些例子，帮助更清楚地理解。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_21.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_21.png)'
- en: of that。 So now we have both vectors y and x。 Assume a is a constant and a vector
    of constant。 So what's the result of right now？ OK， all zeros。 So it's a matrix。
    It's all zeros。 The second question is like， if y equals to x， what， is partial
    x over partial x？ OK。 identity matrix。 The reason is because-- let me write down
    a bit。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那个。所以现在我们有了y和x这两个向量。假设a是常数，a是常向量。那么现在的结果是什么？好的，全是零。所以它是一个矩阵，所有元素都是零。第二个问题是，如果y等于x，那么偏导x相对于x是什么？好的，单位矩阵。原因是——让我写一点。
- en: '![](img/5b9a9c5539229510f04744e47ad8a89c_23.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b9a9c5539229510f04744e47ad8a89c_23.png)'
- en: So we know that the i's row and the j's column will be xi。 So it equals to 1
    if i equals to j and 0 if y， is not equal to j。 So we got identity matrix here。
    The second question is like， how to do a， which is a matrix， a times x with respect
    to x。 So let me do that again here。 If a x-- so now we know this is a vector，
    partial。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们知道第i行和第j列将是xi。所以如果i等于j，它等于1，如果y不等于j，它等于0。所以我们得到了单位矩阵。第二个问题是，如何做a，它是一个矩阵，a乘以x相对于x。让我再做一次。如果a
    x——现在我们知道这是一个向量，偏导。
- en: So we know that it's equals to partial a x。 So i's element with respect to x。
    So if we can write a to be a 0， a 1， it's the i's row of a。 And we stack vertically，
    we get a n。 Then here， a x， the i's element will， be the inner product of a i
    times x。 So which means here is the inner product of a i times x， equals to a
    i。 We talked about previously。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们知道它等于偏导a x。所以第i个元素相对于x。所以如果我们能把a写成a 0，a 1，它就是a的第i行。然后我们垂直堆叠，得到a n。然后这里，a
    x，第i个元素将是a i与x的内积。所以这意味着这里是a i与x的内积，等于a i。我们之前讨论过。
- en: So which means is actually， this is equal to a i。 And we stack a i together。
    We call a。 So the answer is a。 If we do a transpose， we can actually transpose--，
    we can do transpose。 That's also pretty simple。 Just one。 Because for vector，
    you can transport the vector。 It's a row vector。 Column vector doesn't matter。
    So actually。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这意味着，实际上，这等于a i。我们将a i堆叠在一起。我们称之为a。所以答案是a。如果我们做转置，我们实际上可以做转置——我们可以做转置。这个也很简单。只有一个。因为对于向量，你可以转置向量。它是行向量。列向量无关紧要。所以实际上。
- en: it's equals to-- we transport y equals to a， prime x。 And so we get the transport
    a。 That's it。 The other thing， we have a lot of similar thing for matrix， and
    vectors here。 A times u is-- if a is a constant， it's a scalar。 We have a times
    partial u over partial x。 If it's matrix， we put the matrix out。 And if the u
    plus v similes before。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它等于——我们转置y等于a，转置x。所以我们得到转置a。就是这样。其他方面，我们有很多类似的矩阵和向量的东西。a乘以u是——如果a是常数，它是标量。我们有a乘以偏导u相对于x。如果是矩阵，我们把矩阵提出来。如果是u加v在之前的类似情况。
- en: So now we can make more generalized case。 If x is a matrix， we cannot dive deeper
    into the details。 how to calculate them。 But we can't give you the shapes， which
    is you can infer。 what is actually calculated。 Let's look at the first step， though，
    when y is a scalar。 So if x is scalar， we get a scalar。 x is vector， we get a
    row vector。 If x is matrix。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们可以做一个更广泛的情况。如果 x 是矩阵，我们无法深入探讨细节。如何计算它们。但我们可以给你形状，你可以推断出实际计算的内容。让我们先看第一步，当
    y 是标量时。如果 x 是标量，我们得到一个标量。如果 x 是向量，我们得到一个行向量。如果 x 是矩阵。
- en: we get a matrix。 But look at the shape。 X is u by k matrix。 But now。 the gradient
    is like k by m matrix。 We reverse the shape here。 We do a transpose here。 Look
    at the first row of column。 If x is scalar， if y is scalar， you get a scalar。
    y is a vector。 you get a vector。 Is that has the same shape？ If y is a matrix。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个矩阵。但看看形状。X 是 u 行 k 列的矩阵。但现在，梯度就像 k 行 m 列的矩阵。我们在这里反转形状。我们在这里做转置。看第一列的第一行。如果
    x 是标量，如果 y 是标量，你得到一个标量。如果 y 是向量，你得到一个向量。这是否有相同的形状？如果 y 是矩阵。
- en: you get a matrix with the same shape。 So now， look at the second row。 When y
    is a vector。 the interesting case， is that if x is matrix， we got the shape is
    a 3D tensor。 The tensor shape。 the first is shape m， from the vector of y。 y is
    the length m， n's vector。 The second k at n。 shape from matrix A， but reverse
    the matrix here。 So it's m by k by n。 The last column。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到一个具有相同形状的矩阵。那么现在，看看第二行。当 y 是向量时。一个有趣的情况是，如果 x 是矩阵，我们得到的形状是一个三维张量。张量形状，第一个是
    m，来自 y 的向量。y 的长度是 m，n 维向量。第二个是 k 和 n。形状来自矩阵 A，但这里反转了矩阵。所以它是 m × k × n。最后一列。
- en: in the last row， if y is a matrix， x is a vector。 We still get-- we copy the
    matrix shape。 and then got the n's of the x。 In the most complicated case， both
    x and y are matrix。 We get a 3D tensor， a 4D tensor。 The first two shape， m and
    l， is a copy of the front y。 And the third and the fourth shape， can't earn its
    copy for x， but reverse it。 So in general。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一行，如果 y 是矩阵，x 是向量。我们仍然得到——我们复制矩阵的形状。然后得到 x 的 n。最复杂的情况是，x 和 y 都是矩阵。我们得到一个三维张量，一个四维张量。前两个形状，m
    和 l，是 y 的副本。而第三和第四个形状，不能复制 x，但要反转它。所以一般来说。
- en: you can get any arbitrary y， and arbitrary shape x。 We first put the y shape
    before and reverse the shape of x， and the pen at n。 So that is how we can do
    tensor calculus， actually。 Okay。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以得到任何任意的 y 和任意形状的 x。我们首先将 y 的形状放在前面，然后反转 x 的形状，最后是 n。所以这就是我们如何做张量计算的方式，实际上。好的。
