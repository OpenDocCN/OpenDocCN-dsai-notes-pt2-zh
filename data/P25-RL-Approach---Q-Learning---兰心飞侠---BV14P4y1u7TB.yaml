- en: P25：RL Approach - Q-Learning - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P25：RL方法 - Q学习 - 兰心飞侠 - BV14P4y1u7TB
- en: Now， after we talked about stochastic approximation。 it's time to talk about
    the celebrated Q-learning。 one of the most famous algorithms of reinforcement
    learning。 This algorithm was suggested in 1989 in a PhD thesis of Watkins。 and
    since then his paper was cited more than a thousand times。 Q-learning， at its
    extensions。 are used in many interesting applications of reinforcement learning。
    In particular。 Google's DeepMind became very famous before they became a part
    of Google。 when they published a paper where they showed how to use Q-learning
    at scale。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们讨论了随机逼近法之后，到了讨论著名的Q学习的时候了，它是强化学习中最著名的算法之一。这个算法是Watkins在1989年在他的博士论文中提出的，从那时起，他的论文被引用了超过一千次。Q学习及其扩展在许多有趣的强化学习应用中得到了使用。特别是，Google的DeepMind在成为Google的一部分之前非常著名，当时他们发表了一篇论文，展示了如何大规模使用Q学习。
- en: to teach reinforcement learning agents to play Atari video games。 Now， in its
    original form。 as suggested by Watkins， Q-learning works only in the setting of
    discrete states and discrete actions。 In this case， instead of a continuous value
    Q-function， it's represented as a discrete table。 with one value of the Q-function
    per each combination of states and action。 In this case。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 用来教授强化学习代理玩Atari电子游戏。现在，在Watkins建议的原始形式中，Q学习仅在离散状态和离散动作的情况下有效。在这种情况下，Q函数不是以连续值表示，而是以离散表格表示，每个状态和动作的组合都有一个Q函数值。在这种情况下。
- en: we can say that the Q-function is given in a tabulated form。 Now。 the main statement
    of Watkins Q-learning is that given enough data。 the algorithm that takes all
    data point sequentially。 like in the robins-mond-raw stochastic approximation。
    converges to the true Q-function asymptotically， when they have lots of data。
    More specifically。 the convergence proof assumes that each possible combination
    is encountered in data and infinite number of times。 Of course， this is never
    true in practice， where all data are finite。 Therefore， in practice。 the question
    of numerical convergence is always something that needs to be checked。 So。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，Q函数是以表格的形式给出的。现在，Watkins Q学习的主要观点是，给定足够的数据，按照顺序处理所有数据点的算法，像Robins-Monro随机逼近法一样，随着数据量的增大，逐渐收敛到真实的Q函数。当数据量足够大时。更具体地说，收敛证明假设每个可能的组合都会在数据中遇到并且遇到无限次。当然，在实践中，这种情况永远不会成立，因为所有数据都是有限的。因此，在实践中，数值收敛性总是需要检查的问题。所以。
- en: what does Q-learning do？ It simply takes the next observed transition and updates
    the value of a state action peer that was observed in this transition。 Let me
    illustrate its working on some popular examples used for testing and teaching
    Q-learning。 One such example could be an application of Q-learning to solve a
    maze problem。 Here you can see an example of a simple maze with an obstacle， shown
    here as a black square。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是做什么的？它简单地采取下一个观察到的转移，并更新在此转移中观察到的状态-动作对的值。让我通过一些常用的例子来说明它的工作原理，这些例子常用于测试和教授Q学习。一个这样的例子是应用Q学习来解决迷宫问题。这里你可以看到一个简单的迷宫示例，带有一个障碍物，这里用黑色方块表示。
- en: and an exit in the top right corner。 The problem of an agent is to learn an
    optimal policy that would prescribe the direction of the move given the cell location
    of the agent。 In each cell， the agent can move up， down， left， and right。 So，
    we have four degrees of freedom。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在右上角有一个出口。代理人的问题是学习一个最优策略，根据代理人的位置来决定移动的方向。在每个单元格中，代理人可以向上、向下、向左和向右移动。所以，我们有四个自由度。
- en: but some moves cannot be done。 For example， a left move is forbidden if there
    is a wall to the left。 and so on。 So， in this problem， we have eleven possible
    cell locations and up to four possible actions that can be taken in each cell。
    And this gives us forty-four possible combinations of a cell and action。 which
    is still quite manageable a number to keep an index column in a lookup table that
    stores the value of the Q function for all such possible combinations。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有些动作是不能执行的。例如，如果左边有墙，则禁止向左移动，依此类推。所以，在这个问题中，我们有十一种可能的单元格位置，并且在每个单元格中最多可以执行四个可能的动作。这给我们带来了四十四种可能的单元格和动作的组合，这个数量仍然相对可管理，可以在查找表中保持一个索引列，存储所有这些可能组合的Q函数值。
- en: The learning in such simple environment can be done using simulations。 Each
    time a particular data point is absorbed as produced by such simulation。 it's
    taken to update the value of the Q function at the node that corresponds to a
    particular combination of this data and action that were absorbed in this round
    of simulation。 Now， after we spoke about what Q-learning does， let's talk about
    how it does it。 In essence。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个简单的环境中，学习可以通过模拟来完成。每次一个特定的数据点被模拟生成并吸收后，就用它来更新与该数据和在本轮模拟中吸收的动作相对应的 Q 函数值。现在，在我们讨论了
    Q 学习的作用之后，接下来讨论它是如何做到的。本质上。
- en: Q-learning is just the application of the Robins-Monroe stochastic approximation。
    but this time to estimate the unknown expectation that arises in the right-hand
    side of the Bellman-Optimality equation。 Let's compare the Bellman-Optimality
    equation with the Robins-Monroe update。 In the Bellman equation， the optimal Q
    function is given by the expectation of the right-hand side。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习实际上是 Robins-Monroe 随机逼近的应用，但这次是用来估计在贝尔曼最优性方程右侧出现的未知期望。让我们将贝尔曼最优性方程与 Robins-Monroe
    更新进行比较。在贝尔曼方程中，最优 Q 函数由右侧的期望值给出。
- en: On the other hand， the Robins-Monroe formula just shows how to update a running
    estimation of the mean。 Therefore， we can take an individual sum of the current
    observed reward R_t and the marks of the next step optimal Q function as the current
    observation x_t in the Robins-Monroe formula。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Robins-Monroe 公式仅展示了如何更新均值的运行估计。因此，我们可以将当前观察值 x_t 中的当前观察奖励 R_t 和下一步最优 Q
    函数的标记作为 Robins-Monroe 公式中的当前观测值。
- en: The current estimate of the mean x_t will be played by our current estimate
    of the optimal Q function。 If we specify some scheduling scheme for the learning
    rate alpha_k。 these substitutions will produce the famous Q update rule of Q-learning
    that is shown here at the bottom of the slide。 In words， it says that the new
    update for the value of Q function at the node x_t and at is given by a previous
    estimation of its values scaled by the factor 1-alpha_k plus another term equal
    to alpha_k times the new observed value of the quantity that appears in the right-hand
    side of the Bellman-Optimality equation。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的均值估计 x_t 将由我们对最优 Q 函数的当前估计代替。如果我们为学习率 alpha_k 指定一些调度方案，这些替代将生成著名的 Q 更新规则，Q
    学习的规则如底部幻灯片所示。用文字描述，它表示在节点 x_t 和 at 处的 Q 函数值的新更新由其先前的估计值通过因子 1-alpha_k 缩放后加上另一个项，等于
    alpha_k 乘以在贝尔曼最优性方程右侧出现的量的最新观察值。
- en: Now， the Q iteration rule shows two very important things about Q-learning。
    namely that it's both a model-free and off-policy algorithm。 This means that it
    does not make any assumptions on a true data-generating process that produces
    the current observation。 It simply takes it as given and updates the action value
    function for a given state action node x_t and at。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Q 迭代规则展示了 Q 学习的两个非常重要的特性，即它既是无模型的也是脱离策略的算法。这意味着它不会对产生当前观察的数据生成过程做出任何假设。它只是将其视为给定，并更新给定状态-动作节点
    x_t 和 at 的动作值函数。
- en: It's also an off-policy algorithm because the only thing that matters for Q-learning
    to work is that all action state pairs will be visited many times。 But why they
    were visited does not matter。 In the limit when all state action pairs are encountered
    in infinite number of times and data。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它也是一个脱离策略的算法，因为 Q 学习要奏效，唯一重要的是所有状态-动作对会被访问多次。但它们被访问的原因并不重要。在所有状态-动作对被无限次地遇到并且获得数据的极限情况下。
- en: Q iteration is guaranteed to a syntotically converge to true optimal value function
    for all such pairs。 You can read more on conversion properties of Q-learning algorithms
    in a weekly reading for this week。 Now， given what we discussed in the previous
    lesson on dynamic programming solution to the model。 it should be evident that
    even though the classical online Q-learning algorithm is guaranteed to a syntotically
    converge。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Q 迭代保证对于所有这样的状态-动作对，它会渐近收敛到真正的最优值函数。你可以在本周的每周阅读中了解更多关于 Q 学习算法的收敛性质。现在，考虑到我们在上一课中讨论的关于模型的动态规划解法，应该很明显，尽管经典的在线
    Q 学习算法保证会渐近收敛。
- en: it might just take it too long for practical purposes。 And the reason for this
    is that optimal hedges are obtained using cross-sectional information across all
    Monte Carlo paths。 But such cross-sectional information would be masked by any
    online method of updating Q-values and optimal hedge ratios。 However， the way
    out in these cases is also clear。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于实际应用可能需要花费太长时间。其原因在于，最优对冲是通过所有蒙特卡罗路径的横截面信息获得的。然而，这种横截面信息会被任何在线更新Q值和最优对冲比率的方法所掩盖。不过，在这种情况下的解决办法也是明确的。
- en: What we have to do is simply to update Q-values and a value of a star at all
    Monte Carlo paths simultaneously。 As we did when we computed optimal hedges in
    the previous lesson。 Because we are now in the setting of batch mode reinforcement
    learning， the data are already there。 And there is no need actually to take data
    points one by one as is done in the classical Q-learning。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是同时更新所有蒙特卡罗路径上的Q值和星级值。就像我们在上一节课计算最优对冲时所做的那样。因为我们现在处于批处理强化学习的设置中，数据已经准备好。实际上不需要像传统Q学习中那样逐一获取数据点。
- en: In the next video， we will see how the classical Q-learning can be adjusted
    for such batch mode learning。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节视频中，我们将看到如何调整经典Q学习以适应批处理模式的学习。
- en: '![](img/366c69fa21211a8696877b658967c6ed_1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/366c69fa21211a8696877b658967c6ed_1.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
- en: '![](img/366c69fa21211a8696877b658967c6ed_3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/366c69fa21211a8696877b658967c6ed_3.png)'
