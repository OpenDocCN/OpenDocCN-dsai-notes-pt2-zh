- en: P99：99. L19_2 Recurrent Neural Networks in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P99：99. L19_2 Python中的递归神经网络 - Python小能 - BV1CB4y1U7P6
- en: '[INAUDIBLE]。'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[无法听清]。'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_1.png)'
- en: So--， [SIDE CONVERSATION]。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以——，[旁白]。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_3.png)'
- en: OK。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_5.png)'
- en: So this is--， OK， is it recording？ Yeah。 OK。 So this is really just right now
    going through a very。 very， simple example。 And the first thing we need to do
    is we need to load the， data set。 So we're loading the time machine。 And--， [SIDE
    CONVERSATION]， All right。 So--， there we go。 This is the entire time machine loaded。
    All right。 So I'm importing， you know。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在就是——好的，录制了吗？是的。好的。所以现在其实只是通过一个非常，嗯，非常简单的例子。首先我们需要做的事情是加载数据集。所以我们正在加载时间机器。然后——，[旁白]，好吧。好了。我们已经加载了整个时间机器。好吧。所以我正在导入，你知道的。
- en: the usual convenience libraries， autograd， loss， and so on。 And this loads the
    time machine data set。 We'll look at that in a little detail later。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的便利库，自动微分，损失函数等等。这将加载时间机器数据集。稍后我们会详细看看。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_7.png)'
- en: OK。 And so the first thing I'm going to do is I'm going to use a， one-hot encoding。
    So， I mean。 remember what I did is I split things up on a， pro-character basis。
    So every character of the time machine is its own token。 That's not so particularly
    intelligent if I use English。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以我要做的第一件事是使用独热编码。所以，记住我做的就是按字符级别划分。时间机器的每个字符都是它自己的标记。如果我用英语的话，这其实不太聪明。
- en: Can somebody tell me a language where this would be a great idea？ Guys， you
    really should know that。 Yes， Chinese would work really well。 Japanese would work
    really well。 Korean， not so much。 Right？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有人能告诉我一个这种编码非常适合的语言吗？伙计们，你们真的应该知道。是的，中文会非常适合。日语也非常适合。韩语，没那么适合。对吧？
- en: Because they ditched the Chinese characters at some point。 So it would work
    really well for a language where you have a。 nontrivial number of symbols that
    are quite distinctive。 But， OK。 so since I don't want to have to deal with a lot
    of， complicated pre-processing and so on。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们在某个时候抛弃了汉字。所以它对于一个符号数量非平凡且非常独特的语言会非常有效。但是，好的。由于我不想处理很多复杂的预处理等等。
- en: I'm just going to use， one-hot encoding here。 And so， vocabulary size， I think
    it's around 43。 And so if I have the following array， you know， of， you know，
    2/2/30。 So then， OK， so 43。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我只会在这里使用独热编码。所以，词汇表大小，我认为大约是43。所以如果我有以下数组，你知道的，2/2/30。那么，好的，43。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_9.png)'
- en: So you can see that these are the three characters that I get。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到这是我得到的三个字符。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_11.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_11.png)'
- en: So the first character is encoded as such。 The second character is encoded as
    such， right？
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第一个字符被编码成这样。第二个字符被编码成这样，对吧？
- en: So that's position two。 And， well， actually the third position， that's why it's
    two。 Zero position is there。 And for the third one， OK， that would be position，
    yeah， 31 here， probably。 Right， so this is， I mean， a hideously inefficient encoding，
    but， OK， you need to start somewhere。 OK， let's move on。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是位置二。嗯，实际上是第三个位置，这就是为什么是二。零位置在那。对于第三个，好的，那应该是位置31，可能是这个。对，所以这是，我的意思，这是一种非常低效的编码方式，但，好的，得从某个地方开始。好的，继续吧。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_13.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_13.png)'
- en: Now， if I want to encode an entire mini batch to one-hot encoded， data， then
    here's what I would do。 Right， so I need to define a mapping to one-hot。 And what
    it does is basically for all x's in the mini batch。 it converts it into this one-hot
    encoding。 Right， so if I， let's now assume my mini batch， you know。 it's so， you
    know， 2/5。 And I have， you know， these as my inputs， then let's actually， look
    at。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我想将整个小批量编码为独热编码数据，那么我会这么做。对，所以我需要定义一个到独热编码的映射。它的作用基本上是对小批量中的所有x进行处理，将其转换为这个独热编码。对，所以如果我，现在假设我的小批量，你知道的。是2/5。那么我有这些作为我的输入，那么我们就实际看一下。
- en: you know， how large they are。 So it's， you know， 5， 2/43 objects。 43 because，
    well。 that's the vocabulary size， 2 because， that's exactly the two here from
    the， you know。 mini batch size。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道的，看看它们有多大。所以，它是5，2/43个对象。43是词汇表大小，2是因为正好是小批量大小中的两个。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_15.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_15.png)'
- en: I have five of those guys。 Of course， not all of them plot here。 So if you were
    to zoom out， oops。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我有五个这样的单元。当然，并不是所有的单元都会在这里展示。所以如果你把视图缩小，哎呀。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_17.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_17.png)'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_18.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_18.png)'
- en: If you were to zoom out， then you'd see the rest。 1， 2， 3， 4， 5， right？ Yep，
    and， well， that's just。 you know， the first part of the text。 Okay， any questions
    so far？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把视图缩小，你就能看到其余的。1，2，3，4，5，对吧？是的，嗯，这只是，嗯，文本的第一部分。好的，到目前为止有任何问题吗？
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_20.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_20.png)'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_21.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_21.png)'
- en: Good。 Now， if I want to， you know， build， you know， a model。 the first thing
    I have to do is I have to initialize the parameters。 Okay。 So let's actually set
    a couple of parameters。 First thing is the number of inputs。 Well。 that's the
    vocabulary size。 Okay， because we have the one-hot encoding。 So if I have， you
    know。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。现在，如果我想，嗯，你知道，构建，嗯，一个模型。首先我必须做的事情是初始化参数。好的。那么我们实际设置一些参数。第一件事是输入的数量。嗯，那就是词汇表的大小。好的，因为我们使用的是独热编码。所以如果我有，你知道的。
- en: 43 symbols， then I need， you know， canonical vectors from， you know， E0 to E42。
    Then I'm assuming that I have 512 hidden units。 All right？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 43个符号，那么我需要，从，嗯，E0到E42的标准向量。然后我假设我有512个隐藏单元。好的？
- en: That's the number of dimensions for the hidden state。 The number of outputs。
    since I'm producing characters again， is again， you know， 43。 And the last thing
    is I want to find out whether I have a GPU， and， okay， in this case， well。 clearly
    I don't have one。 I could just print that。 And then， yeah。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是隐藏状态的维度数量。输出的数量。因为我再次生成字符，结果，嗯，依然是43。最后一件事是我想知道是否有GPU，嗯，好的，在这种情况下，嗯，很明显我没有。我可以直接打印出来。然后，嗯。
- en: So you can see it's running on the CPU。 Now I need to define all the parameters。
    So remember before that we had this model where we basically had a， you know。
    linear model in hidden and observed data for the hidden layer。 These parameters。
    and so one just gives me a normal distributed， initialization， right？ Let's get
    params。 And， well。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到它正在CPU上运行。现在我需要定义所有的参数。所以记住之前我们有这个模型，其中基本上有一个，嗯，线性模型，用于隐藏层的隐藏数据和观察数据。这些参数。所以其中一个会给我一个正态分布的初始化，对吧？让我们获取参数。然后，嗯。
- en: the bias is just set to zero。 For the output layer， I just need one-weight matrix
    and one bias。 And then the last thing is I need to attach gradients。 So these
    are the parameters here。 And then for all the parameters that I have in my parameter
    dictionary， sorry， parameter list。 I need to attach gradients。 And then you return
    the gradients。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置就设置为零。对于输出层，我只需要一个权重矩阵和一个偏置。然后最后一件事是我需要附加梯度。所以这些就是这里的参数。然后，对于我在参数字典中所有的参数，对不起，是参数列表。我需要附加梯度。然后你返回这些梯度。
- en: So this is fairly standard except that now the parameters have funny names。
    Any questions so far？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是相当标准的，除了现在参数有些奇怪的名字。到目前为止有任何问题吗？
- en: This is really just setting up the variables that we are going to use for。 our
    hidden unit recurrent neural network。 Okay。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上只是设置我们将要使用的变量。用于我们的隐藏单元递归神经网络。好的。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_23.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_23.png)'
- en: Good。 So then let's get to the RNN itself。 First thing is I need to initialize
    my state。 So remember if you have this RNN， you know， it starts from some initial
    state。 And then it keeps on updating things and it moves somewhere。 And I'm going
    to make my life easy here。 I'm just going to use as my initial hidden state all
    zeros。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。那么接下来让我们来看一下RNN本身。第一件事是我需要初始化我的状态。所以记住，如果你有这个RNN，你知道，它是从某个初始状态开始的。然后它会不断更新，并且移动到某个地方。为了让我的工作更轻松，我这里就直接把我的初始隐藏状态设置为全零。
- en: I might decide to pick something else in all zeros for my initial hidden state。
    So I could then maybe make that another parameter and then learn that。 So that
    might not be such a terrible idea。 But okay， here I don't。 Now you might wonder
    why on earth I'm actually returning a list with only one， element， right？
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能会决定为我的初始隐藏状态选择其他的全零状态。然后我可能把它做成另一个参数，并学习它。所以那可能不是一个糟糕的主意。但是，好吧，这里我没有这么做。你可能会好奇，为什么我实际上返回的是一个只有一个元素的列表，对吧？
- en: So somebody have an idea why on earth I'm doing something as weird as that。
    Yes？
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有人知道为什么我会做这么奇怪的事情吗？是的？
- en: '>> Because if you turn a list and you can add in everything or elements。 >>
    I could add more elements。 That''s a good idea。 So I might have some models which
    have a hidden state that has variable size。 It''s actually a really nice idea。
    It''s probably worth while exploring through paper。 It''s actually a really nice
    idea。 The alternative is you and the reason why we do it here is because we want
    the same call。'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 因为如果你转一个列表，你可以加入任何东西或者元素。 >> 我可以加入更多元素。这个主意很好。所以我可能有一些模型，它们的隐藏状态大小是可变的。这个主意其实很不错，可能值得通过论文进一步探索。实际上这个主意真的很不错。另一种方式是你...我们之所以这样做，是因为我们想要相同的调用。'
- en: signature to work also for models that have more than one hidden state。 So later
    on when we'll do long short term memory， we'll see that we'll have a hidden。 state
    which has a consists of a memory cell that is not readable from the rest of the。
    world and then the actual hidden state that is accessible for the purpose of，
    generating the output。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个签名也适用于具有多个隐藏状态的模型。所以稍后当我们做长短期记忆（LSTM）时，我们会看到会有一个隐藏状态，它由一个不可从其他地方读取的记忆单元和一个实际的隐藏状态组成，后者可以用于生成输出。
- en: So in that case you just have two objects that you're going to return。 Okay？
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，你将返回两个对象。好吗？
- en: And then the next thing is we actually need to define our RNN cell。 And so this
    RNN cell takes inputs， state and parameters， and does something， interesting with
    them。 In particular what it outputs is， well， the corresponding outputs and the
    hidden， state。 how it changed。 So let's actually go over that。 So the first thing
    is I just pull out the parameters。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后接下来我们实际上需要定义我们的 RNN 单元。所以这个 RNN 单元接受输入、状态和参数，并对它们做一些有趣的事情。特别是它输出的内容是，对，相应的输出和隐藏状态，它是如何变化的。那么让我们来过一遍。首先，我只是提取这些参数。
- en: So nothing really magic happened。 I'm just taking the parameters that we had
    previously and I pull them out。 right？
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以没有什么魔法发生。我只是拿出了我们之前的参数，并把它们提取出来，对吧？
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_25.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_25.png)'
- en: So here we initialize all those parameters， WXH， WH， BH， WH， Q， BQ。 Right？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们初始化了所有这些参数，WXH，WH，BH，WH，Q，BQ，对吧？
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_27.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_27.png)'
- en: And now all I'm doing is I'm just pulling them out of this parameter vector
    again。 That's really elegant in Python otherwise this might be five lines of code。
    And then here I'm pulling the hidden variables out of the state。 Right？ So that's
    why I'm writing H。 because I'm basically just pulling the first element， out and
    I don't care about the rest。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我所做的就是再次从这个参数向量中提取它们。这在 Python 中非常优雅，否则这可能需要五行代码。然后在这里，我从状态中提取隐藏变量，对吧？所以我写了
    H，因为我基本上只提取第一个元素，其余的我不关心。
- en: The outputs initially are just empty because I don't have any yet。 And then
    what I do is I go over my mini batch。 So for all the X's in the inputs。 here's
    the actual math for my hidden， for my RNN cell， right？
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出一开始是空的，因为我还没有任何输出。然后我做的是遍历我的小批量。所以对于输入中的所有 X，这是我的隐藏状态的实际数学公式，对于我的 RNN 单元，对吧？
- en: So this is just using in this case Tange。 I could have picked some other function。
    but Tange is a good thing， because it will keep the values within some range and
    prevent them from diverging。 which would make training really hard。 And then Y
    is just， you know。 an inner product between the hidden state and some， parameters
    plus bias。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是使用了这个例子中的 Tange。我本可以选择其他函数，但 Tange 是一个很好的选择，因为它可以将值保持在某个范围内，防止它们发散，这样训练就不会变得非常困难。然后
    Y 就是，您知道的，隐藏状态与一些参数加上偏置的内积。
- en: It's just some linear function。 And now in the end I just append Y to that list
    of outputs and that's it。 And so I just keep on iterating over the input data，
    produce a list of output， data。 and then in the end， while I return， you know，
    the new hidden state。 that I get after ingesting the entire sequence and I return
    the sequence of outputs。 Okay。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是一个线性函数。最后，我只是把 Y 添加到输出列表中，仅此而已。然后我继续遍历输入数据，生成一个输出数据的列表，最后，在处理完整个序列后，我返回新的隐藏状态，然后返回输出序列。好吗？
- en: Any questions so far？ I know this is a little bit abstract。 but the point is
    that this is really the， guts of， you know， how your hidden units work。 Once you
    understand this， you can， you know， as we'll see， you can basically， design。 considering
    more complex units。 Okay。 Good。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止有什么问题吗？我知道这有点抽象，但重点是，这是你知道，隐藏单元如何工作的核心。一旦你理解了这一点，你就可以，正如我们将看到的，你基本上可以设计，更复杂的单元。好，明白了。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_29.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_29.png)'
- en: And so now that we have all of this， let's see whether that actually works。
    So， yes。 and I probably didn't define X。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们有了这一切，让我们看看这是否有效。是的，而且我可能没有定义X。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_31.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_31.png)'
- en: Okay， let me see。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我看看。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_33.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_33.png)'
- en: Did I generate the data？
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我生成了数据吗？
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_35.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_35.png)'
- en: Let me see。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我看看。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_37.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_37.png)'
- en: Let me see。 It's not just work fine。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我看看。它不仅仅是正常工作的。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_39.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_39.png)'
- en: Okay， now it works。 So， not quite sure what happened before。 It's the data should
    still persist。 But in any case， what I'm doing is I'm getting， you know， an initial
    state， and that depends on。 you know， the input shape for the X's， number of hidden
    units， and device context。 Then I'm encoding my inputs for X。 You know， I get
    my parameters。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在有效了。所以，不太确定之前发生了什么。数据应该仍然存在。但无论如何，我所做的是获得一个初始状态，这取决于你知道，X的输入形状，隐藏单元的数量，以及设备上下文。然后我对X的输入进行编码。你知道，我得到我的参数。
- en: And now what I'm doing is I'm running the RNN on all the inputs， the initial
    state。 and the parameters， and I get some new state out of it。 And so after that。
    you can just compare what happened between inputs and outputs。 And very unsurprisingly。
    at least if the code is correct， in this case it looks like it is。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我做的是在所有输入上运行RNN，使用初始状态和参数，然后得到一些新的状态。所以之后，你可以比较输入和输出之间发生了什么。毫不意外，至少如果代码正确的话，在这种情况下看起来是正确的。
- en: number of inputs and number of outputs is the same。 That's what you would expect。
    All right。 that's this five here。 The number， the shape of the inputs and outputs
    is also the same。 It doesn't have to be that way， but that's what happens because
    we engineered it that way。 That we have basically the same number of output dimensions。
    And then lastly， the state。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的数量和输出的数量是相同的。这是你预期的结果。好吧，这就是这里的五个。输入和输出的形状也是相同的。虽然不一定非得如此，但因为我们是这样设计的，所以它们基本上具有相同的输出维度。最后是状态。
- en: the old and the new state look the same。 Okay。 Question， why did I write state
    zero？
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 旧状态和新状态看起来一样。好吧，问题是，为什么我写了状态零？
- en: So why did I take the first picture of the state？ Okay。 Any ideas？ Or rather。
    what would happen if I picked something else？ Okay， well。 let's have a quick look
    at what happens if you print state。 Okay。 So state new is this vector of two by
    512。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我要拍摄状态的第一张图片？好吧，有什么想法吗？或者说，如果我选择其他东西，会发生什么呢？好吧，我们快速看一下如果你打印状态会发生什么。好的，所以新的状态是一个2×512的向量。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_41.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_41.png)'
- en: So remember we had 512 hidden units， but why do we have two times 512 now？ Any
    ideas？ Okay。 Well。 who thinks it's the mini bash size？ Okay， well， okay， so I'm
    seeing two hands go up。 Who thinks it's the dimensionality of the hidden state
    overall， two by 512？ Okay。 Who thinks it's something else？ Okay， so right now
    the number of hands doesn't quite add up to the number of people in the room。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以记住，我们有512个隐藏单元，为什么现在有2倍512呢？有什么想法吗？好吧，谁认为是迷你批次大小？好吧，好的，我看到两只手举起来了。谁认为这是隐藏状态的维度，总共是2×512？好吧，谁认为是其他原因？好的，现在手的数量似乎和房间里的人数不完全吻合。
- en: So， okay。 So it's actually， it's the mini bash size， the two。 The 512 is the
    number of hidden units。 But since every sequence in my mini batch， you know， that
    can be very different sequences。 needs its own state sequence， you know， or its
    own state， that's why I need to carry one。 for every observation in the mini batch
    around with me。 Okay。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，实际上，它是迷你批次大小，2。512是隐藏单元的数量。但由于我在迷你批次中的每个序列，您知道，这些序列可以是非常不同的，需要它们自己的状态序列，或者它们自己的状态，所以我需要为迷你批次中的每个观察都携带一个状态。好吧。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_43.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_43.png)'
- en: And so then， the last thing I need is I need， you know， some prediction function。
    So predict RNN。 Well， it takes actually a lot of arguments into this。 It takes
    some string prefix。 number of characters that it should generate。 The， you know，
    the RNN itself， number of parameters。 And yeah， we need that and you will probably
    need that also for your homework。 Stuff like this。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以接下来，我需要的最后一个东西是需要，你知道，一个预测函数。那就是预测RNN。嗯，它实际上接受很多参数。它接受一个字符串前缀。它应该生成的字符数。你知道，RNN本身，参数数量。是的，我们需要这些，你可能也会在作业中用到类似的东西。
- en: right？ So， it's a good idea to pay attention because you will find it very useful
    in homework。 Anyway， you need an initial RNN state。 Number of hidden units， vocabulary
    size， device context。 and then the mappings from， character to index and back。
    Okay。 So the first thing we do in the predict function is， well， we neutralize
    the state。 Right？ So。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？所以，注意这个点是个好主意，因为你会在作业中发现它非常有用。无论如何，你需要一个初始的RNN状态。隐藏单元的数量，词汇表大小，设备上下文。然后是从字符到索引以及反向的映射。好的。所以我们在预测函数中做的第一件事是，嗯，我们初始化状态。对吧？所以。
- en: and then， well， we output， you know， we basically precede the output。 So the
    output in our cases。 you know， the first few characters， that's basically， prefix
    of zero。 And now， what we do is。 we go over that entire sequence。 That's made
    a number of characters plus the length of the prefix minus one。 Minus one because
    I just copied， you know， the first character of the prefix into， output。 I go
    and。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，嗯，我们输出，你知道，我们基本上是继续输出。所以在我们的例子中，输出的前几个字符，基本上就是零的前缀。现在，我们所做的是遍历整个序列。它由多个字符和前缀的长度减去一组成。减去一是因为我只是将前缀的第一个字符复制到输出。我去做。
- en: well， encode to one-hot encoding the， well， the end of the output because that's。
    the next symbol to use to predict the next one。 So， I take one step forward in
    the RNN。 Okay。 And then， well， I need to decide if I actually， if I've already
    seen the， characters， right。 then all I do is I just copy them from the input
    to the， output， right？ If I've not seen them。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，将其编码为独热编码，然后，嗯，输出的末尾就是下一个符号，用来预测下一个符号。所以，我在RNN中向前迈了一步。好的。然后，嗯，我需要决定是否已经看过这些字符，对吧？如果是的话，我只需要将它们从输入复制到输出，对吧？如果没见过的话。
- en: then I perform a maximum likelihood decoding。 So I pick basically the one character
    that's the most likely。 And then， in the end， because I have a list， well， I just
    join it and with no。 separating character in between。 So that's this thing here。
    And I perform a decoding from index ID to characters here for all the items in
    the， output。 Okay。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我执行最大似然解码。所以我基本上选择最有可能的一个字符。最后，因为我有一个列表，嗯，我只是将它们连接起来，中间不加分隔符。所以就是这样。然后我对所有输出项执行从索引ID到字符的解码。好的。
- en: That's really what predict RNN does。 Okay。 Any questions so far？ This is quite
    complex。 Everybody's cool with that？ Good。 Good。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是预测RNN的工作原理。好的，到目前为止有问题吗？这个过程相当复杂。大家都明白了吗？好。好。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_45.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_45.png)'
- en: Let's see how it works in practice。 And if I started with traveler。 because
    right now my weight makes us my， parameters are all garbled。 it just produces
    some garbage sequence of， characters。 Right。 That's to be expected because there's
    nothing useful in there。 Okay。 But at least， you know。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在实践中是如何工作的。如果我从“traveler”开始。因为现在我的权重使得我的参数都是混乱的，它只会生成一些垃圾字符序列。对吧。可以预料到，因为里面没有任何有用的信息。好的。但至少，你知道。
- en: it produces a string。 So the next thing that we need to worry about is gradient
    clipping。 So gradient clipping， okay。 Does somebody know why I need gradient clipping？
    We did this before。 right？ And we looked at overall gradients。 Why do I need gradient
    clipping？ Okay。 Exactly。 So if my gradient is too large， if I optimize， right，
    so let's draw the， situation here。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成一个字符串。那么接下来我们需要担心的是梯度裁剪。梯度裁剪，好的。有人知道为什么我需要梯度裁剪吗？我们以前做过这个，对吧？我们看过整体梯度。为什么我需要梯度裁剪？好的。正是如此。如果我的梯度过大，如果我进行优化，对吧，让我们在这里画一下情况。
- en: I'm sitting here。 My gradient is really large。 Then it might send me， you know，
    essentially。 you know， somewhere over， here。 And it might， you know。 in the best
    case it'll see exactly like so to the， optimum。 In the worst case， it'll go like
    so。 It'll just， you know， oscillate and get bigger and just take me really。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我坐在这里。我的梯度非常大。那么它可能会将我送到，你知道，基本上。你知道，某个地方。最好的情况是，它会正好看到这个最优解。最坏的情况是，它会像这样走。它会只是，你知道，振荡并变大，把我带到很远。
- en: far away from where I would have wanted to be。 And one convenient way of gradient
    clipping is I just multiply it with。 something that's either just one or one or
    theta over the length of the， gradient。 If I have that。 then， well， basically
    I know that the largest， length of the gradient is going to be theta。 If the first
    term kicks in and the length is g， if the second term。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 离我想要的位置还远。一个方便的梯度裁剪方法是，我只需将其乘以一个值，这个值要么是 1，要么是θ除以梯度的长度。如果我有这个，那么，基本上我知道梯度的最大长度将是θ。如果第一个项生效并且长度是
    g，第二项则。
- en: kicks in and the length is theta， right？ So let's have a look at how you go
    and compute it。 Well。 you basically， you know， create an array。 And right now
    this is still a little bit silly because you basically。 create a zero-dimensional
    array scalar。 And now for all the parameters in the parameter list。 go and add
    the， square norm， so the square to that norm。 And then in the end。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 触发并且长度是θ，对吧？那么我们来看看你是如何计算它的。嗯，你基本上，知道吧，创建一个数组。现在，这个还是有点傻，因为你基本上是创建了一个零维数组标量。现在，对于参数列表中的所有参数，去添加平方范数，换句话说，对这个范数进行平方。然后最终。
- en: take a square root and convert it back to， scalar。 Now if this norm is longer
    than theta。 then go and rescale it。 And otherwise do nothing。 Test this equation
    up there， encode。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 取平方根并将其转换回标量。如果这个范数大于θ，那么就重新缩放它。否则不做任何操作。测试上面的这个方程，进行编码。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_47.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_47.png)'
- en: Okay。 Now comes the training loop。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。接下来是训练循环。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_49.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_49.png)'
- en: And this one is a little bit complex。 Let's actually do this directly。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有点复杂。我们实际上直接做一下。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_51.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_51.png)'
- en: In Jupyter。 Because， yeah， this is a little bit long。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 中。因为，是的，这有点长。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_53.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_53.png)'
- en: Unfortunately we'll have to get through that。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们必须通过这个。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_55.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_55.png)'
- en: You know， reading code in public is awkward。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，在公共场合读代码挺尴尬的。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_57.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_57.png)'
- en: But this function will see many， many times over。 Okay。 so the training predict
    RNN takes a lot of arguments， we'll get to them as we go through。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这个函数会被调用很多次。好的。所以训练预测 RNN 需要很多参数，我们会在进行时逐个处理。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_59.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_59.png)'
- en: The first thing is it takes as one of the arguments whether we iterate。 randomly
    through the data or not。 So， it's random iter。 If it is， then， well。 invoke the
    random iterator， otherwise invoke the consecutive， iterator。 So。 let's kind of
    straightforward。 All right。 Params， while it invokes the get parameters routine。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是它作为一个参数，决定我们是否通过数据进行随机迭代。所以，它是随机迭代。如果是，那么，嗯，调用随机迭代器，否则调用连续迭代器。所以，基本上就是直接的。好，参数，在调用获取参数的例程时。
- en: which is appropriately， implemented。 And right now we just care about， you know。
    as the loss function， the log， likelihood of， you know， for the next character
    respectively。 So。 that's our self-max cross entropy loss。 Okay。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是适当实现的。现在我们关心的是，作为损失函数，就是下一个字符的对数似然性。嗯，这就是我们的自我最大化交叉熵损失。好的。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_61.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_61.png)'
- en: Okay， so then， let's look at the training loop itself。 So， the first thing is
    very common。 Well。 just go with the data as many times as you have epochs。 All
    right。 Now。 if we have a random iterator， then， well， we just initialize the state
    with， zero。 Otherwise。 you know， so， you need to initialize it like so。 All right。
    So， it's basically just --， Okay。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么，我们来看一下训练循环本身。首先是非常常见的。嗯，按照你有多少个纪元就遍历数据多少次。好了。如果我们有一个随机迭代器，那么，我们就初始化状态为零。否则，嗯，你知道的，所以，你需要像这样初始化它。好吧。所以，基本上就是——好的。
- en: actually -- that doesn't make sense。 It should be actually if it's random iterator。
    Okay。 Let's see。 Next thing is we need to just get some utility functions done。
    Oh， yeah， because if it's -- oh。 sorry。 So， here's the thing。 If it's random --
    if it's not random iterator。 then you initialize it at the， beginning of each
    epoch。 Right？ If it's a random iterator。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上——这没有意义。如果是随机迭代器的话，应该是这样。好的。接下来我们需要完成一些实用函数。哦，对了，因为如果——哦，抱歉。问题是这样的。如果不是随机迭代器。那么你在每个纪元的开始时初始化它。对吧？如果是随机迭代器的话。
- en: then the initial light is at the beginning of， every mini batch。 Okay。 That's
    why we have this distinction here。 Because here， we just iterate over the data
    iterate。 That's just set up before that， whatever comes above。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始值是在每个迷你批次的开始时设置的。好的。这就是我们在这里做区分的原因。因为在这里，我们只是迭代数据。就像在之前的设置中一样，数据会在此之前准备好。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_63.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_63.png)'
- en: Okay。 So， if it's a random iterator， nuke all the parameters at the beginning
    of the， mini batch。 If not， we want to make sure that our backprop doesn't trust
    the。 boundaries between two mini batches。 Right？ And unfortunately， unless we
    tell gluon not to do this。 it will， actually just keep on pushing the gradients
    all the way through that， chain。 Right？ So。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以，如果这是一个随机迭代器，我们需要在每个迷你批次的开始时清除所有参数。如果不是，我们需要确保我们的反向传播不会信任两个迷你批次之间的边界。对吧？不幸的是，除非我们告诉
    gluon 不这么做，否则它实际上会一直将梯度传递到整个链条中。对吧？所以。
- en: that's why we need to detach them。 And I'm going to talk about that in a fair
    amount of more detail later。 on。 So， don't worry。 So， then we get to the actual
    heart of the backprop part。 So， with autograph。record。 So， we encode the inputs。
    We run our RNN for the input state and parameters。 We get the outputs。 Right？
    There we go。 And then we basically compute the loss between the outputs and what
    we。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们需要将它们断开。我稍后会详细讲解这一点，所以不用担心。那么，接下来我们进入反向传播的核心部分。通过 `autograph.record`，我们对输入进行编码，运行我们的
    RNN 来处理输入的状态和参数，然后得到输出。对吧？就这样。然后我们基本上计算输出和我们预期结果之间的损失。
- en: really should have seen。 So， this is very much the same as we would have in
    any classification。 regression scenario just that now we have a sequence。 Okay。
    So， then comes the backprop。 I clip the gradients。 And then run STD。 And that's
    about it。 Right。 So。 now the last thing that I need is I need to actually log
    a little bit， how well I'm doing。 So。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 真的应该看到了。所以，这和我们在任何分类、回归场景中做的非常相似，只不过现在我们处理的是序列。好吧。那么，接下来是反向传播。我对梯度进行了裁剪，然后运行
    STD。就是这样。对吧。那么，最后我需要做的事情是，我需要记录一下我做得有多好。所以。
- en: in other words， if， you know， it's， you know， number of prediction， periods
    and multiple of that。 I go and， you know， print out the， perplexity。 And I'm going
    to explain what perplexity is。 Namely。 what I'm doing is I'm -- and the last thing
    that I do is I。 have the RNA and actually predict the next few characters。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果你知道它是预测期数的倍数，我就去打印困惑度（perplexity）。我将解释困惑度是什么。也就是说，我正在做的是——最后我做的事情是，我让
    RNN 实际预测下一个几个字符。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_65.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_65.png)'
- en: Okay。 So， this is kind of complex。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以，这有点复杂。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_67.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_67.png)'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_68.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_68.png)'
- en: And now I can just train this。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我可以直接训练这个了。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_70.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_70.png)'
- en: And this is odd because it should actually run just fine。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这很奇怪，因为它实际上应该正常运行。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_72.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_72.png)'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_73.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_73.png)'
- en: And -- oh， yeah。 Last thing we need is we need to set some parameters。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，对了。我们需要做的最后一件事是设置一些参数。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_75.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_75.png)'
- en: Namely， you know how many times we go over the data。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，你知道我们遍历数据的次数。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_77.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_77.png)'
- en: And then it should just run。 It just takes a while。 But， okay， while we do this。
    let's just quickly digest some。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它应该就能运行了。只是需要一点时间。不过，好的，在我们做这个的同时，快速消化一下这些内容。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_79.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_79.png)'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_80.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_80.png)'
- en: parameters。 I don't know whether that's there。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 参数。我不确定是否已经设置。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_82.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_82.png)'
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_83.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_83.png)'
- en: Okay。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_85.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_85.png)'
- en: Yeah， so there you go。 So this is just setting all the parameters -- 500 epochs，
    64， steps。 next character prediction， mini batch size is 32， learning rate is
    100。 So that's very aggressive。 And I clip， you know， the gradient to be no more
    than， you know， 0。01。 So I have a large learning rate， but I clip my gradient
    to prevent， it from going to a batch。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，就这样。这只是设置所有参数——500个周期，64步。下一个字符预测，最小批量大小为32，学习率为100。所以这非常激进。我将梯度裁剪到不超过0.01。所以我有一个很大的学习率，但我会裁剪梯度，以防止它变得过大。
- en: Overall， learning rate times clipping is， you know， in the， order of 1。 And
    then in the end。 I have it， you know， predict a few things。 And I wanted to start
    a travel and time travel。 Okay。 so now what we can see is initially the model
    doesn't， really do anything particularly intelligent。 right？ So it just produces
    this and this and this。 And these are， you know， the most common words。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，学习率乘以裁剪值大约是1。然后在最后，我让它预测了一些东西。我想开始一个旅行和时间旅行的任务。好吧。所以现在我们看到的是，最开始模型并没有做出什么特别智能的东西。对吧？它只是生成了这些东西，这些东西。这些是最常见的单词。
- en: So it's not very surprising。 As the perplexity decreases， it starts producing
    text that looks。 a little bit less， you know， garbled and， you know， you could。
    think of this as being not totally stupid。 Okay。 And if you were to wait for a
    little bit longer。 you would， see that this， you know， keeps on getting better。
    Okay。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其实并不令人惊讶。随着困惑度的下降，它开始生成的文本看起来不那么混乱，你可以认为它不是完全愚蠢的。好吧。如果你再等一会儿，你会看到它会变得越来越好。好吧。
- en: So we are going to get back to this in a while， but right now， this takes about，
    you know。 half a minute for every epoch， so。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会回到这个话题，但现在，每个周期大约需要半分钟时间。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_87.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_87.png)'
- en: it's a little bit boring。 What I'm going to do though is I'm going to show you
    what。 happens if you use sequential sampling。 And the giveaway is that for sequential
    sampling， so it's。 exactly the same algorithm， but just different preparation
    of， the data。 where I don't reset the hidden state between steps。 Well， it works
    better。 And， you know。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点无聊。不过我接下来会展示一下，如果你使用顺序采样，会发生什么。诀窍是，顺序采样使用的算法是完全相同的，只是数据的准备方式不同，我在每一步之间不重置隐藏状态。结果是，它的表现更好。你知道的。
- en: you can see that by， you know， epoch 450 or 500， it actually produces something
    that， you know。 at least for a， few characters could fool a human。 So in a way
    it passes the Turing test for the first ten， characters， right？
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，到了第450或500个周期，它实际上能生成一些文本，至少对于几个字符来说，可以欺骗一个人。所以从某种意义上说，它通过了图灵测试，至少对前十个字符来说，对吧？
- en: Which I think is actually a much more interesting way of， doing the Turing test。
    Not can you fool a human or not， but how long can you fool a， human， right？
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这实际上是一种更有趣的图灵测试方式。不是看你能不能欺骗一个人，而是看你能欺骗一个人多久，对吧？
- en: This way you get a much better measure of， you know， how close， you are。 All
    right。 So for instance。 if you have a phon answering system that only， interacts
    with a human for one minute。 then it only needs to， fool a human for more than
    one minute to get away。 All right。 In any case。 the giveaway is that sequential
    sampling works， a lot better than， you know， random sampling。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，你能更好地衡量你离目标有多近。好吧。例如，如果你有一个电话应答系统，它只与人类互动一分钟，那么它只需要在一分钟内欺骗一个人就可以逃脱。好吧。无论如何，诀窍是顺序采样比随机采样效果要好得多。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_89.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_89.png)'
- en: Let's see where we are。 So epoch 350， it gets a perplexity of 1。53。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看现在的情况。所以在第350个周期，它的困惑度是1.53。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_91.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_91.png)'
- en: Here at 358 it has a perplexity of 1。25。 Does somebody have an idea why sequential
    sampling works better？
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在358周期时，它的困惑度是1.25。有人知道为什么顺序采样表现更好吗？
- en: Okay。 Let's just draw the picture for a moment。 So in sequential sampling， or
    random sampling。 what I do is， see if this is my sequence。 No， actually I've got
    a couple of sequences because I have a。 mini batch。 Maybe it's a mini batch of
    size three。 All right。 So in sequential sampling。 I chop it up into， you know，
    mini batches of the size。 So I have basically， thanks。 So I have。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。让我们先画个图。假设这是我的序列。其实我有几个序列，因为我有一个**迷你批次**。可能这是一个大小为三的迷你批次。好了。所以在顺序采样中，我会将其切分为，嗯，大小为的迷你批次。基本上，谢谢。就是这样，我有。
- en: you know， three of those here。 Actually， let me break it down a little bit more。
    So I have six of them so we don't confuse mini batch size and， number of segments。
    And so if I'm。 let's say here and I make some prediction and I， get some state
    here， some hidden state here。 then here these， hidden states are the same at the
    transition。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，这里有三个。其实让我再分解一下。所以我有六个，这样我们就不会混淆迷你批次大小和段数。如果我是。假设在这里我做了一个预测，然后，我得到了一个状态，这里是一些隐藏状态。然后在这里，这些隐藏状态在过渡时是一样的。
- en: Whereas if I were to do random sampling， I would pick another， mini， you know。
    another segment that's maybe here。 And as a result， the states， well， wouldn't
    be the same。 As a matter of fact， I pretty much would initialize it as zero or，
    something else by default。 That's why random sampling doesn't work so well， we
    see， that forgets all the context of the state。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 而如果我做随机采样，我会选择另一个，迷你，嗯，另一个段，可能是在这里。因此，结果是，这些状态，嗯，应该不会是一样的。实际上，我几乎会将其初始化为零或默认的其他值。这就是为什么随机采样效果不太好，我们看到它忘记了状态的所有上下文。
- en: Okay。 And this is our simple RNN。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这是我们简单的RNN。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_93.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_93.png)'
- en: Okay。 Any questions so far？ Yeah， and you can see it's not as nice。 It gets
    down to a place of 1。37。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，到目前为止有问题吗？是的，你可以看到它不如之前那么好。它下降到1.37的位置。
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_95.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_95.png)'
- en: And here we are down at 1。17。 If you were to train it a little bit longer， it
    would get， well。 that's pretty much great stalls。 Okay。 Any questions？ Yes？ >>
    So percussive is lowest on edge 1。6。 >> Yeah。 >> Yeah， so why did the perplexity
    go back up again？ Did we see this thing before？
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的困惑度是1.17。如果你再训练长一点，它会变得，嗯，那基本上就是停滞了。好了。有问题吗？是吗？ >> 所以在1.6的边缘困惑度最低。 >> 是的。
    >> 是的，那么为什么困惑度又上升了？我们之前看过这个现象吗？
- en: Where if you keep on training and at some point things get worse。 Okay。 so there
    could be two things that caused this， right？ Remember， this is training set perplexity。
    right？ If it was tested perplexity， I would say that's overfitting。 As a matter
    of fact。 part of the homework is going to be to deal with overfitting。 So why
    would the perplexity。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续训练，某个时刻，情况可能会变得更糟。好吧。可能有两件事导致了这个，对吧？记住，这是训练集的困惑度。对吧？如果是测试集的困惑度，我会说那是过拟合。事实上，作业的一部分是要处理过拟合。那么为什么困惑度会。
- en: even during training， go back up again？ After all， we're trying to minimize
    the objective function。 right？ Why does it get worse？ Any ideas？ Hey， guys， next
    time I'll bring some coffee along， right？
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在训练过程中，困惑度也会再次上升吗？毕竟，我们是想要最小化目标函数。对吧？为什么它变得更糟？有什么想法吗？嘿，大家，下次我带些咖啡来，好吗？
- en: Okay， so any ideas why the training error might increase？ Well， remember the
    learning rate thing。 right， where I said， well， if you pick a very aggressive
    learning rate， it might diverge， right？
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，有没有什么想法为什么训练误差可能会增加？嗯，记得学习率的事。对吧，我曾说过，如果你选择一个非常激进的学习率，它可能会发散，对吧？
- en: That's exactly what's happening here。 So probably at the next step to learn，
    you know。 the perplexity will go down again， and it might oscillate a little bit。
    But that's just that my learning rate was probably chosen a little bit enthusiastically。
    Yes？
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是这里发生的情况。所以在下一步学习时，困惑度可能会再次下降，并且可能会稍微振荡。但那只是因为我的学习率可能选得太积极了。是的？
- en: So why did you provide a learning rate to the refactor in the T12s training
    framework network？
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么你在T12s训练框架网络的重构中提供了学习率？
- en: So what I could do is I could have a learning rate scheduler。 Yes， and as a
    matter of fact。 for any production quality implementation， you would have a learning
    rate scheduler。 So we'll talk a little bit about that in the context of object
    recognition。 where he probably talked a bit about cosine schedules and other things。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我可以做的是使用一个学习率调度器。是的，事实上，对于任何生产级的实现，你都会有一个学习率调度器。所以我们将在目标识别的背景下稍微谈谈这个，可能还会谈到余弦调度和其他一些内容。
- en: We'll talk more about those things when we get to the optimization in more detail。
    So that's made me defer a lot of the optimization math until later。 because right
    now you can just use it as a hammer。 And we'll unpack what exactly is inside that
    tool。 We'll be later on。 [BLANK_AUDIO]。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入讨论优化时，我们会更多地谈论这些内容。所以这也让我把很多优化数学推迟到了后面，因为现在你可以先把它当作一个简单的工具来用。我们稍后会解析这个工具里面到底有什么。[BLANK_AUDIO]
- en: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_97.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b7ed10f3bdd3b04e7ec5f22cca723_97.png)'
