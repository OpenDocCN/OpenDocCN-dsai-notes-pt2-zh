- en: P89：89. L16_4 Region-based CNNs - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P89：89. L16_4 基于区域的CNN - Python小能 - BV1CB4y1U7P6
- en: So， R3N is pretty the oldest detection family。 It's maybe the first success
    for deep learning based detection。 algorithm we have。 You have multiple models。
    We're going to do a quick overview here。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，R3N几乎是最早的检测家族。它可能是基于深度学习的检测算法中的第一次成功。你有多个模型。我们在这里快速概述一下。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_1.png)'
- en: The first they call it R， CN。 It's a region， CN。 The idea is pretty simple。
    So， given image。 given the dog and cat， we paid that image here。 What we do here。
    we first generate a bunch of anchor boxes。 The original one， RCL， used a heuristic
    algorithm。 It's pretty complex。 It's based on the original non-deplearning， algorithm's
    families。 So。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们称之为R，CN。它是一个区域，CN。这个思想非常简单。所以，给定图片，给定狗和猫，我们将这张图片放在这里。我们在这里做的事情是，我们首先生成一堆锚框。最初的RCL使用了一种启发式算法。它相当复杂，基于原始的非深度学习算法家族。所以。
- en: a lot of heuristics， how to look at the images， look at a lot of small things。
    So。 a lot of details there。 But you can see that you can just think of it like
    it's a black， box。 Give image， it can output a lot of anchor box candidate。 Then，
    for each anchor box。 what they do here， we just crop the image and make it a training
    example。 So， then for this image。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 很多启发式方法，用来观察图片，观察很多小细节。所以，很多细节在里面。但你可以把它看作是一个黑箱。给定一张图片，它可以输出很多锚框候选框。然后，对于每个锚框，它们在这里做的事情是，我们只需裁剪图片并将其作为训练样本。所以，接下来对于这张图片。
- en: what we do， we just generate two few images， two new images。 The first one can
    have the dog hat。 the second one can nest the cat。 And then， for each region or
    each anchor box。 we just first use a pre-trained network。 At that date， I think
    it's pre-trained。 the VGG or anything else， pre-trained， or image that， to get
    the features for each anchor box。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的事情是，我们只生成两张新图片，第一张可能有狗帽，第二张可能有猫。然后，对于每个区域或每个锚框，我们首先使用一个预训练的网络。在那个时候，我认为是使用预训练的VGG或其他任何东西，来获取每个锚框的特征。
- en: Once we get the feature， then we train SVN to classify the objects， in the image。
    Then。 we train a linear regression to predict the bond box offset。 So， that's
    all。 So， what we do here。 give an image， select some anchor boxes， and use it
    in the CRN to get the features for each anchor box。 So， for maybe each anchor
    box to get one thousand dimension， features。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到特征，我们就训练SVN来分类图片中的物体。然后，我们训练线性回归来预测边框偏移量。就是这些。所以，我们在这里做的事情是，给定一张图片，选择一些锚框，并在CRN中使用它们来获取每个锚框的特征。所以，可能每个锚框得到一千维的特征。
- en: so that's the -- we didn't train yet。 We just did a pre-processing。 Then。 we
    train SVN to get objects， trying a linear regression， to get a bond box。 The training
    is on the last stage。 Question。 >> Which faster is it to use an SPC classifier。
    that is to use a multilec sub-product in stage three？ >> Well。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是——我们还没有训练。我们只是做了预处理。然后，我们训练SVN来获取物体，训练线性回归来获得边框。训练是在最后阶段。问题。>> 使用SPC分类器更快吗？比在第三阶段使用多级子产品更快吗？>>
    嗯。
- en: it's -- I don't remember the color is VN all linear， but the key thing is on
    the feature structure。 So， if -- think about each image， if I can select one，
    thousand images for you。 you're going to run a thousand image， feature structures。
    Which means。 like if I have -- if my data set have one thousand -- not， one thousand。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 它是——我不记得颜色是否是VN线性，但关键在于特征结构。所以，如果——想象每张图片，如果我能为你挑选一千张图片，你将运行一千张图片的特征结构。这意味着，假设我有——如果我的数据集有一千张——不，一千张。
- en: one hundred thousand images， and each image we generate， maybe just the one
    hundred anchor boxes。 that we end up with ten， million images。 We need to run
    the fourth path of these ten million images。 That's pretty expensive。 Which means
    you need to pass an image net by ten times。 That's -- that's -- that's -- that's
    -- that's -- that's。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一百万张图片，每张图片我们生成，可能只有一百个锚框。最终我们得到一千万张图片。我们需要运行这些一千万张图片的四个路径。这是非常昂贵的。这意味着你需要通过十次图像网进行处理。那——那——那——那——那——那——
- en: the algorithm maybe six or at least six years ago， which means。 you have a small
    GPU to take forever for you to do that thing。 So， what do people do here？
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法可能已经有六年，或者至少六年了，这意味着你需要一个小的GPU，花费很长时间才能完成这件事。那么，人们在这里做了什么呢？
- en: I -- I give them -- give them data set。 I do -- I -- I get the features and
    save into disk。 and I can use， those features later。 So， it's -- even saving disk
    is much easier -- much faster。 than trying to generate regenerative -- regenerative
    on the， fly。 Okay？ Other questions？ Okay。 So。 this original family， put this low。
    And then we have a -- something called fast-ass EN。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我——我给它们——给它们数据集。我做——我——我获取特征并将其保存到磁盘。然后我可以稍后使用这些特征。所以，甚至保存到磁盘要比尝试实时生成要容易得多——快得多。好吗？其他问题？好的。所以，这个原始家庭，放低一点。然后我们有一个——叫做快速EN的东西。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_3.png)'
- en: Before we talk about a faster version， let's -- here's one， concept here called
    I/O/I pulling。 region of interest pulling。 The idea here -- so， given a feature
    map and a given， bond box。 given anchor box。 So， this is the -- that box we have
    is color of the anchor box。 So。 if we're going to run a two-by-two I/O/I pulling，
    which， means for each anchor box。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论更快的版本之前，先看看这里有一个概念叫做I/O/I池化。感兴趣区域池化。这里的想法是——给定一个特征图和一个给定的框，给定锚框。所以，这就是我们有的那个框，是锚框的颜色。所以，如果我们要执行一个二乘二的I/O/I池化，意味着对于每个锚框。
- en: we're going to cut into two-by-two， regions， even in Kotlin。 But this is three。
    cannot divide by two。 So， the first region -- the first region we have two-by-two。
    and then the second one is just one。 Okay？ Then for each region， we show in different
    colors here。 I， cannot make the max value of it。 So， at that -- so， then for each
    anchor box， we cannot get。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其切分为二乘二的区域，甚至在Kotlin中也是如此。但是这是三，不能除以二。所以，第一个区域——我们有二乘二。然后第二个区域就是一个。好吗？然后对于每个区域，我们用不同的颜色展示。在这里，我，无法使它的最大值。所以，在那时——对于每个锚框，我们不能得到。
- en: the two-by-two output。 So， in general， if I have N by M， I/O/I pulling for each，
    anchor box。 we cannot get N by M elements from this anchor box。 So， what a good
    thing about that。 The good thing here， no matter the anchor box shape， I can always。
    get a fixed length of the feature presentation of the anchor box。 So。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 二乘二的输出。所以，通常，如果我对每个锚框执行N乘M的I/O/I池化，我们不能从这个锚框中得到N乘M的元素。那么，这有什么好处呢？好的地方在于，无论锚框的形状如何，我都可以始终得到锚框的固定长度特征表示。所以。
- en: I can have different -- different size of anchor box， fit into the -- I/O/I
    pulling。 and then for each one， I get， a light -- fix the shape of the features
    of each box。 Okay？ So。 that's the key thing。 Think about how we did that before。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以有不同——不同大小的锚框，适配到——I/O/I池化中。然后对于每一个，我得到——固定形状的每个框的特征。好吗？所以。这是关键点。想想我们以前是怎么做的。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_5.png)'
- en: Here， each anchor box had different shape， but I fit into， CNN， resize to the
    same shape。 fit into CNN， get the same size of， thing。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，每个锚框的形状不同，但我将其输入到CNN中，调整为相同的形状。输入到CNN中，得到相同大小的结果。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_7.png)'
- en: I can do the other classifications。 So， here， I can realize the ROI pulling
    to get the same size of。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以做其他分类。所以，在这里，我可以实现ROI池化，得到相同大小的结果。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_9.png)'
- en: each anchor box。 Okay？ So， here's the first RCN。 Well。 it only changes the thing
    I just highlighted in the， yellow box here。 First。 I just put it into CNN to get
    the features。 I don't put every anchor box into separate to CNN。 I just put the
    original image into CNN here。 Then， still。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个锚框。好吗？所以，这里是第一个RCN。嗯。它只改变我刚刚在黄色框中标出的内容。首先，我把它输入到CNN中以获取特征。我不会把每个锚框都单独输入到CNN中。我只是将原始图像输入到CNN中。然后，仍然。
- en: like I select anchor boxes from the original image， I propose multiple ones。
    Then。 I apply the anchor boxes to the output of the CNN here。 So。 here you do
    feature instructions and apply the anchor box。 Here。 you need to resize a little
    bit because anchor boxes on the， original image is easier and larger。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我从原始图像中选择锚框，我提出多个锚框。然后，我将锚框应用到这里CNN的输出。所以，在这里，你做特征提取并应用锚框。这里，你需要稍微调整一下，因为原始图像中的锚框更大，容易处理。
- en: The output of the CNN may be a little bit smaller， but you can still。 resize
    the anchor boxes accordingly and to fit into ROI pulling to get output。 So。 at
    the end of the ROI pulling， you get， for each anchor box， you get the fixed length
    of features。 Then， we can fit into dense layer and do like a prediction and。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的输出可能会稍小，但你仍然可以相应地调整锚框的大小，并将其适配到ROI池化中以获得输出。所以，在ROI池化的最后，你会为每个锚框得到固定长度的特征。然后，我们可以将其输入到全连接层进行预测。
- en: the bound of box here as similar as before。 But first。 RCN changed for SVN to
    change SVN to softmax linear， regression。 So。 that is easier to join together。
    But the major thing， why we reduced the complexity because we only。 run the CNN
    once。 We didn't run by a number of anchor boxes and ties。 So。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的边界框和之前类似。但首先，RCN从SVN变为软最大线性回归。所以，那样更容易合并在一起。但最重要的是，我们减少了复杂度，因为我们只运行一次CNN。我们没有根据锚框的数量和约束运行。
- en: this maybe give you 10 times faster here。 Okay， any questions here？ Okay， so，
    well。 that's kind of one year after RCN is faster。 It's faster， but still not
    faster enough， actually。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能让你快10倍。好么，有问题吗？好的，所以，嗯，这就是RCN一年后的进展。它更快了，但实际上还是不够快。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_11.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_11.png)'
- en: Then， we have something called a faster RCN。 Well。 the reason is because the
    selective search select some。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有一个叫做更快的RCN的东西。嗯，原因是因为选择性搜索选择了一些。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_13.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_13.png)'
- en: anchor boxes not good enough。 Like， if you get a good coverage， you need to
    select a lot of anchor。 boxes。 Then， also， the algorithm itself is pretty slow。
    I want to improve the anchor box quality。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框不够好。如果你想得到好的覆盖率，你需要选择很多锚框。而且，算法本身也相当慢。我想提高锚框的质量。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_15.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_15.png)'
- en: So， here， I'm going to try a small network called the regional， proposal network，
    RPN。 to select the anchor boxes。 So， what we do here is actually small -- you
    run a small。 detection algorithm here， but you don't -- you less care about， accuracy。
    So， what do you do here？
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这里，我将尝试一个小型网络，叫做区域提议网络（RPN），用来选择锚框。所以，我们在这里做的其实是运行一个小的检测算法，但你不——你较少关心准确度。那么，你在这里做什么？
- en: I just get the feature instructions fitting the， conclusion neural network and
    propose some cheaper -- a lot of。 random anchor boxes。 And then， do you have a
    small classifier to classify if it。 contains an object or not？ If it doesn't，
    I just throw it away。 If yes， I just predict the offset。 And so， then， I'm using
    the anchor box with objects plus the。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我只需将特征指令适配到结论神经网络，并提出一些更便宜的——大量的随机锚框。然后，你有一个小型分类器来分类它是否包含一个对象？如果没有，我就丢弃它。如果有，我就预测偏移量。所以，然后，我就使用包含对象的锚框加上。
- en: offset and the RMS to remove the duplications and fit into the， training。 Okay，
    so basically。 we run a small detection algorithm here to， generate high -- not
    of bound box predictions and use them as。 high quality anchor boxes here。 And
    why we can make it faster？ Because we can run a small。 very small convolutional
    neural， here to on a small scale， the input。 So， we can make it faster。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移量和RMS用于去除重复项，并适应训练。好吧，总体来说，我们在这里运行一个小型检测算法，生成高——不是边界框预测，并将其用作高质量的锚框。在这里。为什么我们可以加速？因为我们可以在一个小的输入尺度上运行一个非常小的卷积神经网络。所以，我们可以让它更快。
- en: And at the end -- so， the backbone here， we can get less， anchor boxes and also
    the quality is high。 which means we， also improve the accuracy a lot。 Okay？ Questions？
    Question？
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后——所以，这里的主干，我们可以得到更少的锚框，而且质量也更高。这意味着我们也大大提高了准确性。好么？有问题吗？问题？
- en: '>> Can you mention the performance of the region for the， work of the -- >>
    Well。 it''s also have the label as well。 It''s joined together。 It''s joined。
    So， this one。 where you propose anchor boxes， so you also， have the label in you
    do training。 You join -- join。 join， telly。 Okay？ The rest of the work of the
    RCM family is called the master。'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 你能提一下区域性能在这个工作中的表现吗？ >> 嗯，它也有标签。它被合在一起。所以，这个地方，你提出锚框时，也有标签一起用于训练。你将它们合并——合并。合并，告诉我。好吗？RCM系列的其他工作叫做主控。'
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_17.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_17.png)'
- en: RCM。 Well， nothing has been changed except that if you talk about， the code
    dataset。 we not just have a bound boxes。 We do have pixel level in labeling information。
    So， here。 you can see that I can label each pixel if it's a， background or map
    to a real object。 So。 the bundling is much clearer。 So， we have much more information
    here。 Well。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: RCM。嗯，除了如果你谈论代码数据集，我们不仅有边界框外，没什么变化。我们确实有像素级的标注信息。所以，在这里，你可以看到我可以标注每个像素它是背景还是映射到一个真实对象。所以，绑定变得更加清晰。所以，我们有更多的信息。嗯。
- en: then what do you do here？ That if I do have the mask， it's called the mask，
    like， it's a。 mask placed on image， if we have this pixel level in， label information。
    I can add a separate loss into my network to， help to predict the bound box。 So。
    the yellow one is called fully convolutional neural network。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那你在这里做什么？如果我确实有掩码，它叫做掩码，比如说，它是放置在图像上的掩码，如果我们有像素级的标签信息。我可以在我的网络中加入一个单独的损失函数，来帮助预测边框。所以，黄色的那个叫做全卷积神经网络。
- en: is actually used in the by-circamentations。 And you can just add in here as
    a separate loss。 So。 if you have additional features information， I use another，
    loss to help to this detection。 So。 master CACN is actually going to help to improve
    the， bound box accuracy。 So， that's all。 They actually get the best paper for
    CEPR。 All these RCM family get a lot of best papers from CEPR。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上在循环神经网络中被使用。你可以在这里加入一个单独的损失函数。所以，如果你有额外的特征信息，我会使用另一个损失函数来帮助进行检测。所以，主CACN实际上会帮助提高边框的准确度。所以，以上就是全部。实际上它们获得了CEPR的最佳论文奖。所有这些RCM家族的模型在CEPR上获得了很多最佳论文。
- en: necessary。 Any questions？ Question？ >> I want to take a train test for RCM。
    >> Well。 faster RCM is pretty slow， and I'll take a few days on， cocoa。 You should
    take maybe a week on cocoa。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 必要的。有什么问题吗？有问题吗？>> 我想做一个RCM的训练测试。 >> 好吧。更快的RCM其实是相当慢的，我可能需要几天时间来做这个测试。你应该可能花上一周时间来做测试。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_19.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_19.png)'
- en: So， at the end， so what I'll show the model to here， the x， axis is the number
    of images per second。 It's a frame first。 So， it's a log scale。 It's like it's
    pretty -- how to see by this small scale here。 The y is M-A-P。 You can think about
    the accuracy。 You can see we do have three models here。 We're going to talk about
    three models。 So， we talk about the first RCM here。 The blue noise。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最后，我将展示给你们的模型，x轴是每秒图像的数量。它是以帧为单位的。所以，它是对数刻度。就像是，如何通过这个小刻度来看待。y轴是M-A-P。你可以理解为准确度。你可以看到我们这里有三个模型。我们会讨论这三个模型。我们先说第一个RCM，这里是蓝色噪声。
- en: The yellow one is called the SSD。 The green one is called the yellow V3。 You
    can see that first RCM。 even faster， is still like the， slowest one in the network。
    The network cannot talk about。 And it's log scale， which means comparing to the
    second the， slowest one。 comparing the slowest yellow or SSD， faster， ACM is kind
    of 8 to 10 times slower。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 黄色的那个叫做SSD。绿色的那个叫做黄色V3。你可以看到第一个RCM，即使更快，仍然是网络中最慢的。网络无法进行讨论。而且它是对数刻度，这意味着与第二个最慢的模型相比，最慢的黄色或者SSD，快速的ACM大约是慢速模型的8到10倍。
- en: But it's usually have high accuracy。 Okay？ But it's maybe not so true for if
    we actually improve the。 battery。 So， it's the slowest one but have the highest
    accuracy。 So。 this is kind of several driving car companies use a lot， of fast
    RCMs。 So。 because having some pace and actual money to buy the。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常它具有高准确度。好吧？但如果我们实际上提高电池，可能就不完全如此。所以，虽然它是最慢的，但却拥有最高的准确度。所以，这就是一些自动驾驶公司大量使用的情况，使用了很多快速RCM。所以，拥有一定的资金来购买它们。
- en: GP box to choose an expensive algorithm is much better than， a heater， actually
    a human。 So。 that's the cause I wear to pay。 But in practice， fast RCM， well，
    it's too small。 If you really care about the accuracy， you can choose fast， RCM，
    even more scale。 if you have additional labels。 But in practice， if you less care
    about that， you care about。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个昂贵的算法比选择一个加热器，实际上比选择一个人要好。所以，这就是为什么我需要支付的原因。但在实际应用中，快速RCM，嗯，太小了。如果你真的在意准确度，你可以选择更大的快速RCM。如果你有额外的标签。但在实践中，如果你不太在意，你更关心的是……
- en: the cost， you should pick up maybe yellow V3 at this stage。 And also like SSD
    also。 SSD may be not choice anymore。 But that yellow V3 is a good choice。 You
    can balance those things。 But the thing changed a lot。 This is detection algorithm
    is pretty hard topic。 Every year we have new accuracy coming out， maybe， comparing
    to the， for example， comparing to。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 成本方面，在这个阶段你应该选择可能是黄色V3。而且像SSD也是如此。SSD可能不再是选择了。但黄色V3是一个不错的选择。你可以在这些方面做平衡。但事情变化很大。这个检测算法是一个非常难的主题。每年我们都有新的准确度发布，可能，比如说，和……相比。
- en: you can check。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以检查。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_21.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_21.png)'
- en: good on CV。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中表现不错。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_23.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_23.png)'
- en: Complying to classifications， there are so many models in the。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 按照分类来说，这里有很多模型。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_25.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_25.png)'
- en: classifications， and now this is the classifications of， hundreds of models。
    like 10 different new networks we talk， about at least eight。 But detection only
    have much smaller。 So only have three families here。 But research is pretty popular
    this day。 The reason we have less models because much harder to， train is invent
    a new one is much harder。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分类，现在这是数百个模型的分类。我们谈论的至少有十种不同的新网络，至少有八种。但检测只有更少。所以这里只有三大家族。但研究这几天非常热门。我们拥有更少模型的原因是因为训练难度更大，发明一个新的模型更难。
- en: Imagine I take a few hours to train。 I take a few days to train。 That's why
    research community has less models here， but still。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我需要几个小时来训练。我需要几天来训练。这就是为什么研究社区这里的模型较少，但依然如此。
- en: '![](img/ed0474d2ec779a35b477ca3a8d84513f_27.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0474d2ec779a35b477ca3a8d84513f_27.png)'
- en: I can change a lot of these days。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天我可以改变很多东西。
