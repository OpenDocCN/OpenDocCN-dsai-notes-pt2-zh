- en: P12：12.Mar_9_Lecture - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P12：12.Mar_9_Lecture - Tesra-AI不错哟 - BV1aJ411y7p7
- en: Okay， so tomorrow is going to be the first meeting with project advisors。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，明天将是第一次与项目顾问的会议。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_1.png)'
- en: '![](img/7e658e45466cec419a4c446a21134cc6_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_2.png)'
- en: You guys should all know who your project advisors are now， either through a
    Piazza posting。 or an email。 Brian D'Ausandro， he's actually at a town。 so he's
    going to be making individual appointments， via Skype or video chat or something。
    So hopefully there's some communication going on between those groups and Brian。
    Of course。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你们现在应该都知道自己的项目顾问是谁了，要么通过Piazza发布，要么通过邮件。Brian D'Ausandro他实际上不在城里，所以他会通过Skype或视频聊天安排单独的预约。希望你们小组与Brian之间有一些沟通。自然。
- en: Chin is not in the country， so you'll be doing video chat with him。 But the
    other three advisors are here and you'll meet with him tomorrow。 It'll be like
    a five minute pitch of your idea。 You'll get immediate feedback。 If you have to
    leave at eight， kind of let people know in case there's any overage， so。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Chin不在国内，所以你将与他进行视频聊天。但其他三位顾问都在这里，明天你将与他们见面。这将是一个五分钟的想法陈述。你会得到即时反馈。如果你必须在八点离开，请提前告知大家，以防有延迟。
- en: you can go first。 Alright， so that's all to say about the projects tomorrow。
    And so let's talk about what we're talking about today， which is ensemble methods。
    The main highlights for today will be random forests and we might start in to
    some out， of boost。 a boosting technique。 But along the way we're going to talk
    about some statistical technique called the bootstrap。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以先来。好的，关于明天的项目就说这么多。现在让我们讨论一下今天的话题——集成方法。今天的重点将是随机森林，我们可能会稍微谈到一些boosting技术。但在这个过程中，我们会讨论一些叫做bootstrap的统计技术。
- en: which motivates some of the motivates bagging and in fact random forests that
    we'll talk， about。 Okay， so we'll start with a review of bias and variance。 First
    of all， what's a parameter？
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这就激发了bagging方法，实际上我们会讲到的随机森林方法。好了，我们从回顾偏差和方差开始。首先，什么是参数？
- en: We're at our statistics framework now， statistics vocabulary。 And a parameter
    is a function of a distribution。 So we have a probability distribution P。 We would
    like to estimate some characteristic of the distribution。 What are characteristics
    of distributions that we know？ Yeah， mean， variance， median， pertosis。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入统计框架和统计词汇了。参数是分布的一个函数。所以我们有一个概率分布P。我们希望估计这个分布的一些特征。我们知道分布的哪些特征？是的，均值、方差、中位数、偏度。
- en: quantiles。 The entire distribution itself could be a parameter。 It's still a
    function of the distribution。 Okay。 So these things we call parameters。 we can
    denote them as mu equals mu of P to show that。 mu is a function that operates
    on the distribution itself。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数。整个分布本身也可以是一个参数。它仍然是分布的一个函数。好的。我们把这些称为参数。我们可以用μ = μ(P)来表示，表明μ是作用于分布本身的一个函数。
- en: And a lot of statistics is about estimating parameters。 All right， so what's
    the statistic？ Well。 as you expect in statistics， we always have this data set。
    Write that D。 So we have a sample， say。 IID from the distribution P。 A statistic
    is just a function of the data。 So statistic S is any function of our data sample。
    All right。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学的许多内容都是关于估计参数的。好了，那么什么是统计量？如你所料，在统计学中，我们总是有这个数据集，记作D。所以我们有一个样本，比如说，独立同分布（IID）来自分布P。统计量就是数据的一个函数。所以统计量S是我们数据样本的任何函数。好的。
- en: A statistic gets a special name if its purpose is to estimate a parameter。 And
    we say statistics are point estimator， a statistic mu hat， say， which is a function。
    of the data。 It is a point estimator of mu， which is a function of the distribution，
    generating the。 data。 Mu hat is approximately equal to mu。 But more or less。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个统计量的目的是估计一个参数，那么它就有了一个特别的名字。我们说统计量是点估计量，例如μ帽（mu hat），它是数据的一个函数。它是μ的点估计量，而μ是生成数据的分布的一个函数。μ帽大约等于μ，但可能略有不同。
- en: if we intend mu hat as an estimator of mu， then we say it's a point， estimator
    of mu。 Just a little bit of very general terminology for you guys。 Okay。 So suppose
    we have a real value parameter。 So most of the things we mentioned are real value。
    The mean， standard deviation， these are all numbers， real values。 They don't have
    to be。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把μ帽（mu hat）当作μ的估计量，那么我们说它是μ的点估计量。给大家普及一点非常基本的术语。好的，假设我们有一个实值参数。我们提到的大多数内容都是实值的。均值、标准差，这些都是数字，实值。它们不一定非得是实值的。
- en: a parameter could be more than just a real value。 It could be a function itself。
    It could be a pair of numbers。 Anyway， but in this case， real value parameter。
    a mu hat is a real value estimator of mu。 And when we have real value estimators
    and parameters。 we could do things like talk about， bias and variance。 So we define
    the bias of mu hat。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个参数不仅仅是一个实数值。它本身可以是一个函数。它可以是一个数字对。无论如何，但在这种情况下，实值参数。mu hat 是 mu 的实值估计量。当我们有实值估计量和参数时，我们可以做一些事情，比如讨论偏差和方差。所以我们定义
    mu hat 的偏差。
- en: this you guys should know， to be the difference between。 the expected value
    of the estimator and the true value of the parameter。 Does this make some sense？
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你们应该知道，这个是估计量的期望值与参数的真实值之间的差异。这有意义吗？
- en: So the expectation of mu hat somehow depends on mu in some way， right？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 mu hat 的期望在某种程度上依赖于 mu，对吧？
- en: Mu is part of the probability distribution that we're using when we compute
    that expectation。 So these things， the expectation is where mu comes in and expectation
    of mu hat equals， mu。 then we're in a situation called unbiased。 So zero bias。
    All right。 And the variance of mu hat is just the variance of the estimator， definition
    of variance。 Okay。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Mu 是我们在计算期望时使用的概率分布的一部分。所以这些东西，期望就是 mu 的作用所在，mu hat 的期望等于 mu。然后我们就处于一个叫做无偏的情况。所以零偏差。好吧。mu
    hat 的方差就是估计量的方差，方差的定义。好的。
- en: Unbiased is if bias is zero。 So here's a point I wanted to make。 It's that neither
    the bias nor the variance depends on a specific data sample。 It's not like we
    sample end point ID from probability distribution and we say， "Oh， what's。 the
    bias on this data set？ What's the variance？"， No。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无偏是指偏差为零。所以这是我想要说明的一点。就是偏差和方差都不依赖于特定的数据样本。并不是说我们从概率分布中抽取了一个数据集，然后我们说：“哦，这个数据集的偏差是多少？方差是多少？”不。
- en: The bias and the variance depend on what the function is， mu hat， the statistic，
    what's。 the actual function for the point estimator and what's the distribution，
    P。 So。 and that comes through because in the bias and the variance where there's
    no direct。 reference to DN， we're taking the expectation over the data set。 All
    right。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差依赖于函数是什么，mu hat，统计量是什么。点估计量的实际函数是什么，分布是什么，P。所以，这一点之所以会体现出来，是因为在偏差和方差中，没有直接引用
    DN，我们是在对数据集进行期望运算。好的。
- en: So the expectation is random， rather than the data sets random， or we're taking
    the expectation。 over the data set， which is kind of as though we'll see what
    that's like in the next slide。 when we talk about how we estimate it。 All right。
    So how do we estimate the variance of mu hat？
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以期望是随机的，而不是数据集是随机的，或者我们是在对数据集进行期望运算，就好像我们将在下一张幻灯片中看到的那样，当我们讨论如何估计它时。好的。那么我们如何估计
    mu hat 的方差呢？
- en: You see it has this expectation of mu hat and expectation of mu hat squared
    in it。 So let's get estimates of those things and we just subtract them and then
    we get the variance。 All right。 Okay。 So what do we usually do？ What do we do
    back in risk when we want to estimate the empirical risk？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到它包含了 mu hat 的期望和 mu hat 的平方期望。所以我们来估计这些东西，然后我们只需要相减就能得到方差。好的。好的。那么我们通常怎么做呢？当我们想要估计经验风险时，我们该怎么做？
- en: It was the expected loss， right？ So we had this expectation and we need to compute
    it with respect to some probability。 distribution， but we didn't know the probability
    distribution。 So we had a trick， right？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 它是预期损失，对吧？所以我们有这个期望，并且需要根据某种概率分布来计算它，但我们并不知道概率分布。所以我们有一个技巧，对吧？
- en: It's not a trick。 It's just what happens in statistics。 Yeah。 Okay。 So we'll
    have our numbers。 So we had a， we didn't have the distribution， but we have a
    sample from the distribution。 So we were calling it D。 So here DN sample size
    N。 Now let's just take the empirical average over a bunch of things and say that
    we'll estimate。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是技巧。这就是统计学中发生的事情。是的。好的。所以我们会有我们的数字。我们没有分布，但我们有从分布中抽取的样本。所以我们称之为 D。所以这里 DN
    是样本量 N。现在我们就来对一堆数据取经验平均值，并说我们将进行估计。
- en: the expectation。 Okay。 So let's see how to apply that reasoning here。 So instead
    of a single sample DN， let's suppose we actually have B independent samples， of
    size N。 So we have probably a generating distribution。 We draw sample of size
    N and call that D1 sub N。 Another one call that D2。 Let's grab B of them。 Okay。
    And what we would want is。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值。好的。让我们看看如何在这里应用这个推理。所以，代替一个单一的样本**D sub N**，假设我们实际上有B个独立的样本，每个样本大小为N。所以我们可能有一个生成分布。我们从这个分布中抽取一个大小为N的样本，称之为**D1
    sub N**。再来一个，称之为**D2**。我们取B个这样的样本。好的。那么我们想要的是。
- en: so to expectation of mu hat is the expectation of what would。 happen from mu
    hat for randomly chosen D， but we'll estimate it by taking B randomly chosen。
    data sets and averaging。 All right。 So what's that look like？
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以**μ hat**的期望值是从**μ hat**中得到的期望值，假设是从随机选择的D中得到的，但我们通过随机选择B个数据集并取平均来估计。好的。那么这看起来是什么样的？
- en: Exertation of mu hat is approximately equal to the average over B data sets
    of mu hat。 of mu hat on that data set。 Is that clear？ So what kind of。 how many
    elements are in D sub N superscript I？ What is that？ Yeah。 This DI sub N。 that
    itself is a sample of size N from the distribution。 And we have B of those samples。
    All right。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**μ hat**的期望值大约等于B个数据集上**μ hat**的平均值。明白了吗？那么，**D sub N superscript I**中有多少个元素？那是什么？对吗？这个**DI
    sub N**本身是从分布中抽取的大小为N的样本。我们有B个这样的样本。好的。'
- en: Okay。 Good。 So that's how we can estimate， in some hypothetical world where
    we got B repeated samples from。 this distribution that we can estimate expectation
    of mu hat。 And similarly。 we could do mu hat squared。 Fine。 And we get this formula
    to estimate the variance of mu hat。 So what good is estimating the variance of
    our estimator？ Why is this interesting？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，明白了。所以这是我们如何在一个假设的世界中估计**μ hat**的期望值的方法，这个世界里我们从这个分布中获得了B个重复样本。我们可以估计**μ
    hat**的期望值。类似地，我们可以做**μ hat**的平方。好吧。然后我们得到了这个公式来估计**μ hat**的方差。那么估计我们估计量的方差有什么用呢？为什么这有意思？
- en: To statisticians or anybody？ So there's some mu that we want to estimate。 We
    have this point estimator， this formula for point estimator which says， you give
    me。 some data and I'll tell you how to come up with an estimate of mu。 And that's
    mu hat of the data。 So why are we interested in this variance of mu hat？ Yeah。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对统计学家或任何人来说呢？所以我们有一个想要估计的**μ**。我们有这个点估计量，这个点估计量的公式说的是，给我一些数据，我会告诉你如何得出**μ**的估计值。这就是数据的**μ
    hat**。那么我们为什么关心**μ hat**的方差呢？对吗？
- en: If we want to just look like the confidence interval。 So it's not just， yes。
    mu hat is a point estimator。 But sometimes we want what's called an interval estimator
    or confidence interval where we don't。 just say mu hat， we give some range like，
    yeah， it's mu hat plus or minus 5%， plus or。 minus whatever。 So it's the plus
    or minus that's giving the range to get some handle on that。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只是想看置信区间。那么它不仅仅是，**μ hat**是一个点估计量。但是有时候我们需要的是所谓的区间估计量或置信区间，在这种情况下，我们不仅仅说**μ
    hat**，而是给出一个范围，比如说，它是**μ hat**加或减5%，加或减任何值。所以正是这个加或减给出了范围，让我们可以有一个把握。
- en: That's why we're interested in the variance。 So what would the plus or minus
    be roughly in terms of variance of mu hat？
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们为什么关心方差。所以，**μ hat**的加或减大概是多少，涉及到**μ hat**的方差？
- en: So it should be mu hat plus or minus。 Okay。 So 1。96 or something times what？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以应该是**μ hat**加或减。好的。那么1.96或什么的，乘以什么？
- en: Standard deviation of mu hat which is what？ In terms of， there's a square root
    of the variance。 Okay。 So the variance of mu hat isn't quite the units that we're
    looking at。 We need the standard deviation of mu hat which is square root of variance。
    Okay。 All right。 Like that。 All right。 Okay。 So we have this estimator mu hat
    and we talked a little bit about how we might be able to。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**μ hat**的标准差是什么？就是说，涉及到方差的平方根。好的。那么**μ hat**的方差并不是我们需要的单位。我们需要的是**μ hat**的标准差，也就是方差的平方根。好的，明白了。好的。那么我们有了这个估计量**μ
    hat**，我们稍微谈了一下我们可能如何处理它。'
- en: come up with the variance of mu hat in some way。 What if we want to understand
    the whole distribution of some estimator mu hat？
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以某种方式得出**μ hat**的方差。如果我们想要理解某个估计量**μ hat**的整个分布，该怎么办？
- en: Okay。 So how might we do that？ All right。 So suppose we grabbed in this picture
    what's happening is we're grabbing 1000 data sets of。 size 100。 All right。 So
    you get a data set of size 100 that's one of them。 And we do that a thousand times。
    And for every data set we estimate this point estimator now alpha hat。 Okay。 It's
    alpha hat now。 And alpha hat has a value for that particular data set。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么我们该怎么做呢？好吧。假设我们在这张图中看到的是我们抓取了1000个数据集，每个数据集的大小是100。好吧。所以你得到一个大小为100的数据集，这就是其中之一。我们这样做了1000次。对于每个数据集，我们估算这个点估计量，现在是alpha
    hat。好吧。现在是alpha hat。对于那个特定的数据集，alpha hat有一个值。
- en: The data sets are all generated by the same probability distribution。 All right。
    So alpha that we're trying to estimate alpha of P the parameter we're trying to
    estimate。 for the probability distribution。 That's the same every time。 And because
    the data set we sample is random alpha hat will be a little bit different every。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的数据集都是由相同的概率分布生成的。好吧。所以alpha hat我们试图估算的是alpha of P，即我们要估算的概率分布的参数。这个每次都是相同的。由于我们抽取的数据集是随机的，alpha
    hat会每次稍有不同。
- en: time and we can plot a histogram of all the alpha hats we get from all the different
    samples。 So this is what this histogram is showing。 So pink that line in the middle
    purple is that's the true value of alpha。 And then we see that out that has some
    spread around the true value。 Okay。 So how could we if we had this can we estimate
    the standard deviation？ Yeah。 It's about that。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以绘制从所有不同样本中得到的所有alpha hats的直方图。所以这就是这个直方图所展示的内容。所以粉红色的那条中间紫色的线，就是alpha的真实值。然后我们看到，它围绕真实值有一些分布。好的。那么如果我们有了这个，我们能否估算标准差？是的。大致是这样的。
- en: But the point is that this has more information than just the standard deviation
    of the variance。 This is an estimate of the entire distribution of alpha hat。
    Okay。 All right。 So this was all well and good。 We are going along this path of
    having repeated samples of size N。 But in real life we usually， get one sample
    of size M。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但关键是，这比仅仅使用标准差或方差提供了更多的信息。这是alpha hat整个分布的估算。好吧。好的。那么一切都很好。我们沿着这个路径走，使用大小为N的重复样本。但在现实生活中，我们通常只得到一个大小为M的样本。
- en: So you know and if we if we want to get variance estimate we could for example
    take our sample。 of size N and divide it up into groups of into B groups。 And
    then kind of get an estimate on each of the groups and then get the variance。
    But but the problem with that is what's the problem with that if we break this
    data set， and into。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你知道，如果我们想获得方差估算，我们可以例如将样本大小为N的数据集分成B组。然后对每个小组进行估算，再得到方差。但问题是，如果我们把这个数据集分成...
- en: Well if we if we take the sample size and then we break it up into groups that
    they it。 should be independent that's fine。 The issue is that the individual groups
    are small and we would get a better estimate if。 we could use the whole thing
    of size N all at once。 So we use it all at once we don't have an obvious way to
    get variance。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们拿样本大小，然后将其分成小组，确保它们是独立的，那是没问题的。问题是，单独的小组很小，如果我们能一次性使用整个大小为N的数据集，我们会得到更好的估算。所以我们一次性使用它，但我们没有显而易见的方法来得到方差。
- en: If we break it into small groups we can get the estimate of variance but the
    estimate is。 based on a very small group。 So that's that doesn't sound so great。
    So the question is can we get the best of both worlds somehow。 You get an estimate
    from the entire data set of size N and have some estimate of its variance。 Yeah。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将它分成小组，我们可以得到方差的估算，但这个估算是基于非常小的组。所以听起来并不太好。那么问题是，我们能否以某种方式得到两全其美的结果？你可以从整个大小为N的数据集中获得一个估算，并且还能够有一些关于它的方差的估算。是的。
- en: Okay。 Okay。 So the bootstrap that's why that's the motivation for the bootstrap。
    Sure。 Yeah。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。好的。所以bootstrap方法，这就是bootstrap的动机。没错。是的。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_4.png)'
- en: Okay。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_6.png)'
- en: Your question。 So what happens so all these every point underline is histogram
    was the value of alpha。 hat based on a sample of size 100。 So question is what
    happens if that sample were of size of 10。000。 I like that question a lot。 What
    happens to this what does this histogram look like in that case？
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你的问题。那么会发生什么呢？所有这些每个点下划线的直方图是基于大小为100的样本的alpha hat值。那么问题是，如果这个样本的大小是10,000，会发生什么？我非常喜欢这个问题。那么在这种情况下，这个直方图会是什么样子？
- en: Okay。 So a lot of it gets narrower。 It gets select a variant the standard deviation
    of this distribution gets narrower。 Yes。 Right。 Why？ What's happening there？ Think
    the better estimate。 Yeah。 Estimates are getting better。 The estimates are converging
    to the value that you into their expectation。 Hopefully。 That's consistency。 Yeah。
    Any other questions？ In this case。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以很多东西会变得更窄。它会选择一个变异，这个分布的标准差变得更窄。是的。对。为什么？那是什么原因？想一想，估计更好了。是的，估计变得更好了。估计趋向于你期望的值。希望如此。这就是一致性。是的。还有其他问题吗？在这种情况下。
- en: how can we say that what is the standard deviation for the bootstrap？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怎么说自助法的标准差是多少？
- en: So if you found the number of data， if everyone is different， they're not as
    fast as that。 Okay。 Yeah。 So this is every sample is of size 100。 If you want
    it， if you。 your original data set was of size 100， you have your estimate。 you
    have your point estimate for that data set， alpha hat of 100。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你找到了数据的数量，如果每个都不同，它们不会像那个一样快。好的。是的。所以这是每个样本的大小是100。如果你想要它，如果你的原始数据集大小是100，你就有了你的估计。你对该数据集有了你的点估计，alpha
    hat为100。
- en: And now you want to know what's the variance of this thing？ Okay。 Now， to do
    that。 you imagine that you take a lot more samples of size 100 and then you。 look
    at the variance of those。 Is that answer your question or？ I mean。 like if I have
    the most size， 10，000 and then I do use that， I have two terms that。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你想知道这个的方差是多少？好的。现在，为了做到这一点。你可以想象你取了更多大小为100的样本，然后你查看这些样本的方差。这个回答了你的问题吗？我的意思是。如果我有最大小，10,000，然后我使用它，我有两个项。
- en: is that data set is of size 100。 No。 Each， you said 10，000。 No， no， I think
    10。000 random data sets of size。 Okay。 So bootstrap of size 10，000。 No。 Yeah。
    10，000。 B equals 10，000。 Yes， B equals 10。 Okay。 So like， I use 10，000 and he
    used like 100，000。 Yeah。 Okay。 No。 What happens？ So what， this thing gets narrower
    when n gets larger。 So this 100 is replaced by 10。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集大小是100。不是。你说10,000。不是，不，我认为是10,000个随机数据集，每个大小为100。好的。所以自助法大小为10,000。不，没错。10,000。B等于10,000。是的，B等于10。好的。所以像我用10,000，他用了100,000。是的。好的。不。那么会发生什么呢？当n变大时，这个东西会变得更窄。所以这个100被替换为10。
- en: 000。 What happens when B gets larger？ We have more bootstrap， bootstrap， resamples。
    What do you guys think happens then to this histogram？ Say again？ Yes。 Yes。 Same
    but more smooth。 So the distribution of alpha hat based on a sample of size 100，
    that has nothing to do。 that's just a distribution。 We're estimating that distribution
    by taking lots of data sets of the same size。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 000。当B变大时会发生什么？我们有更多的自助重抽样。你们认为这时这个直方图会发生什么？再说一遍？是的。是的，还是一样，但更平滑。所以基于100大小样本的alpha
    hat的分布，那与什么都没关系。那只是一个分布。我们通过取很多相同大小的数据集来估计这个分布。
- en: The more data sets of the same size that we take， the better our estimate of
    that distribution。 becomes。 So yeah， if we had a lot larger B， we could probably
    make these bins a lot smaller and。 it would be nice and smooth。 That's what happens。
    Okay。 Good question。 Okay。 So the bootstrap。 So first let me define a bootstrap
    sample。 A bootstrap sample from a dataset， DN。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取更多相同大小的数据集时，我们对该分布的估计会变得更好。所以，如果我们有一个更大的B，我们可能会把这些箱子做得更小，它会显得平滑。这就是发生的情况。好的，问题很好。好了，接下来是自助法。首先让我定义一个自助样本。一个来自数据集DN的自助样本。
- en: is a sample of the same size， size N， drawn from， DN with replacement。 So picture
    a bucket of all the samples that you got in your initial dataset， in your initial。
    sample of size N。 And then you reach in and you grab one and you record it and
    then you put it back。 And you reach in again and you grab random one and record
    it and put it back。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个大小为N的样本，从DN中带放回地抽取。所以想象一下一个装有你在原始数据集中所有样本的桶，最初样本的大小为N。然后你伸手进去，抓一个并记录下来，然后把它放回去。然后你再次伸手进去，随机抓一个，记录下来，然后放回去。
- en: And you do that N times。 And that gives you what's called a bootstrap sample
    from the original sample。 So some interesting things are going to happen with
    a bootstrap sample because they're sampling。 with their placement。 So we're going
    to have repetitions， right？
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你做N次。这将给你从原始样本中得到的所谓的自助样本。所以使用自助样本会发生一些有趣的事情，因为它们是带放回的抽样。所以我们会有重复，是吧？
- en: We're also going to have items in the original sample that never get pulled
    out in our resample。 We can calculate the probability of that。 So let's do that。
    So every item in the original sample。 every time we reach in and grab it， there's
    a probability， that we don't get it。 One minus one over N。 Yeah， right。 And every
    time we reach in and grab it。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也会有原始样本中的项，在我们的重抽样中从未被抽取出来。我们可以计算这种概率。让我们来做吧。所以原始样本中的每一项，每次我们伸手去抓它时，都有一个概率我们没有抓到它。一减去1除以N。对，没错。每次我们伸手去抓它。
- en: it's an independent event。 So probably that we never grab it and N tries is
    one minus one over N raised to the nth power。 Okay。 And what is that？ Yeah， close。
    Yeah， exactly。 That's right。 As N goes to infinity。 this converges to one over
    E， which is roughly a third。 Exactly。 So back at the end。 So the probability of
    rather the expected fraction of balls that are left behind is about。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个独立事件。所以我们永远不抓到它的概率，在N次尝试中是1减去1除以N的N次方。好吧，那是什么？对，差不多。是的，完全正确。没错。随着N趋向于无穷大，这个值会收敛到1除以E，约等于三分之一。完全正确。回到最后。所以，未被抓到的球的预期比例大约是。
- en: a third in every bootstrap sample。 So there's definitely some diversity in these
    bootstrap samples。 Every time we go for it， we're getting maybe about 70% overlap
    or something， but not complete。 overlap。 All right。 So that's a bootstrap。 This
    is a quick picture of it。 although I think it's pretty clear。 If the original
    dataset is on the left， dataset is size three。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个自助样本中三分之一。所以这些自助样本肯定是有一些多样性的。每次我们去抽时，可能大约有70%的重叠，但不是完全的重叠。好了，这就是自助法。这是它的一个简短示意图，尽管我认为已经很清晰。如果原始数据集在左侧，数据集大小为三。
- en: three observations of X and， Y。 The first bootstrap sample has the third observation
    repeated twice。 The second one's missing altogether。 The second bootstrap sample
    has all of them。 So that's exactly the same。 And the last one has two repeated
    threes missing。 So that's B equals three。 Three bootstrap samples， yes。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 三个关于X和Y的观察值。第一个自助样本重复了第三个观察值两次。第二个样本完全缺失。第二个自助样本包含了所有的观察值。所以这完全相同。最后一个样本则有两个重复的三缺失。所以B等于三。三个自助样本，是的。
- en: So why are we choosing a sample size the same as the original data？ Well。 I
    think the motivation is pretty clear in the previous discussion because here we。
    had a fixed dataset size。 It was a size of 100。 And we wanted to know what's the
    performance of alpha hat on a dataset of size 100。 And so what we did was we drew
    a bunch of datasets of size 100 for the original distribution。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么我们要选择与原始数据相同大小的样本呢？好吧，我认为之前的讨论中动机已经很明确，因为在这里，我们有一个固定的数据集大小。它的大小为100。我们想知道，在大小为100的数据集上，alpha
    hat的表现如何。因此，我们所做的是从原始分布中抽取多个大小为100的数据集。
- en: Basically the fact that they're the same size follows from this whole discussion。
    Whereas now instead of here we are saying that we're drawing from the datasets
    from the。 original distribution。 Instead of that we're saying， well we can't actually
    do that。 So what we're going to do is take bootstrap samples from the sample distribution。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，它们大小相同这一事实是从整个讨论中得出的。而现在，我们不再说这里，而是说我们是从原始分布的数据集中抽取样本。我们并不是说这样做，而是说，我们实际上做不到这一点。于是我们决定从样本分布中抽取自助样本。
- en: Anybody else puzzled by this？ Why is it equivalent？
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有别人对此感到困惑吗？为什么它是等价的？
- en: Why make sense to be talking about bootstrap samples that are the same size
    as the original， sample？
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么讨论自助样本大小与原始样本相同有意义？
- en: Yeah。 Are you basically maintaining the probability distribution by doing it
    with replacement but。 still taking a sample of size n？ We want to have back to
    that and still have the same probability of drawing back or something。 else。 Is
    that like the point of putting it back but still taking it in size n？ Okay。 So
    first why do we want it of size n？ First let's assume we wanted it to be of size
    n。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对。你基本上通过有放回抽样来保持概率分布，但仍然抽取大小为n的样本？我们希望回到这个，仍然保持相同的抽取概率，或者其他什么的。这就是放回去抽取大小为n的意义吗？好的。那么，首先，为什么我们要选择大小为n？首先假设我们希望样本大小为n。
- en: You need to put it back or you're going to get the exact same thing every time。
    If you don't replace it you're just going to get the whole， it's not a different
    sample。 it's the same as the original dataset。 No I get that。 Okay so then the
    question is why is it of size n？ I guess。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要放回去，否则你每次都会得到完全相同的东西。如果你不放回去，你得到的就是整个数据集，不是一个不同的样本。它和原始数据集是一样的。明白了。好的，那么问题是，为什么样本大小是n呢？我猜。
- en: Oh I'm saying it's like that's why you're kind of combining replacement and
    size n to。 try to get like the right distribution。 We want to size n but we want
    a little bit of diversity because we want to pretend that。 the bootstrap sample
    is kind of like taking a fresh sample from the probability distribution。 even
    though it's really a bootstrap sample from the sample。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，我是在说，这就是为什么你会把替换和样本大小n结合起来。尝试得到正确的分布。我们希望样本大小是n，但也希望有一点多样性，因为我们希望假装自助样本就像是从概率分布中抽取的一个新样本。虽然它实际上是从样本中提取的自助样本。
- en: So yes it's kind of like simulating we would like to imagine that the bootstrap
    sample is。 in rough approximation to a fresh sample from the original distribution
    of size n。 That's the objective which seems not entirely convincing that that
    would work。 Maybe plausible yeah。 You're saying is it more efficient than to generate
    a sample from the original distribution？
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的，这就像是在模拟，我们希望假设自助样本大致相当于从原始分布中抽取的一个新样本，大小为n。这是目标，虽然这个想法看起来不完全令人信服，但也许是合理的。你是在说，生成一个样本从原始分布中抽取会更有效吗？
- en: Oh okay that's interesting。 So that's actually I understand correctly you're
    saying you take your sample and then you。 have a model and you fit your model
    to the sample and then you draw data from the model。 Yeah that has a name that's
    called a parametric bootstrap。 That's interesting。 Okay but that's different that's
    something different。 Yeah but that's a real thing。 Okay all right。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，明白了，这很有意思。所以如果我理解正确的话，你是说，你拿到样本，然后建立一个模型，将模型拟合到样本上，然后再从模型中生成数据。对，这个方法有个名字，叫做参数自助法。很有意思。好的，不过那是不同的，是另一回事。对的，但那确实是一个实际的方法。好吧，明白了。
- en: So bootstrap method I kind of alluded to it。 A bootstrap method is when you
    simulate having B independent samples from the distribution。 P by using B bootstrap
    samples from the sample Dn。 Okay so the entire first section of this lecture was
    if only we had B independent samples。 from the probability distribution we could
    do this cool stuff like estimate the variance。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以自助法我之前提到过。自助法是指你通过从样本Dn中使用B个自助样本，来模拟从分布P中得到B个独立样本的情况。好的，这一讲的整个第一部分就是如果我们能从概率分布中得到B个独立样本，我们就能做一些很酷的事情，比如估算方差。
- en: of our estimators。 And then we said but we don't really have B independent samples
    and a bootstrap method。 is saying that might be okay why don't you just take B
    bootstrap samples from your original。 distribution and treat them as though they
    were independent samples from the original， distribution。 All right so we have
    our original data Dn we have we make B bootstrap samples I've written。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前说过，我们并没有真正的B个独立样本，而自助法的意思是说，这可能没问题，为什么不直接从原始分布中抽取B个自助样本，然后把它们当作是从原始分布中独立抽取的样本来处理。好了，我们有了原始数据Dn，我们制作了B个自助样本，我已经写出来了。
- en: not script D here is regular D to kind of say bootstrap。 And so then for each
    bootstrap sample we compute some functions some it should be a statistic。 And
    we work as with those as though they were drawn IID from P。 So the amazing thing
    is that this usually comes this often comes pretty close to what。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的D不是脚本D，而是常规的D，用来表示自助法。然后对于每个自助样本，我们计算一些函数，应该是统计量。我们就把它们当作是从P中独立同分布地抽取的样本来处理。所以惊人的地方在于，这通常能非常接近从原始数据分布中得到的新样本的结果。
- en: you would get with actual fresh samples of size P from the original data jetting
    distribution。 That this works is I find it surprising。 The justification is both
    empirical by running experiments and theoretical with some fairly。 heavy mathematics
    which we're not going to get into。 But it's as a practical tool it's quite effective。
    So here's a illustration。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这一方法之所以有效，我觉得很令人惊讶。它的理论依据既有通过实验的经验验证，也有通过相当复杂的数学理论支撑，虽然我们不会深入讨论这些。不过作为一个实用工具，它非常有效。接下来是一个插图。
- en: So I'm going to repeat what we had before we had the histogram before the histogram。
    So here's the histogram on the left is the histogram we had originally that's
    where we。 had we had 1000 independent samples of size 100 drawn from the original
    distribution that's。 this guy。 The middle histogram is instead of 1000 real independent
    samples we're taking 1000 boot。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我要重复一下我们之前讲过的内容，在我们有直方图之前的内容。所以左边的直方图是我们最初的直方图，就是我们从原始分布中抽取了1000个大小为100的独立样本。那就是这个。中间的直方图则是与其不同的，我们不是用1000个真实的独立样本，而是用了1000个自助样本。
- en: sample bootstrap samples from our original sample of size 100 and we histogram
    those estimators。 up and we get this histogram in blue which some eye looks pretty
    close pretty close。 On the right we have another view of the data。 The box is
    that called box stem and leaf stem anyway。 The box plot。 Alright so this is this
    is our illustration of the effectiveness of bootstrap。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们原始大小为100的样本中进行自助法抽样，然后我们绘制这些估计量的直方图。我们得到一个蓝色的直方图，眼看上去挺接近的。在右边我们有数据的另一种视图。那个框子是所谓的盒式图，反正就是箱型图。好吧，这就是我们展示自助法有效性的示例。
- en: And you can take some statistics if you want to go deeper yes。 You can ask the
    question based on the real data set。 You can calculate the ask the measure based
    on the real data set。 Or based on the size of the real data set is n right？ Yes
    the size of the real data set is n。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解的话，你可以进行一些统计分析。你可以根据真实数据集提出问题。你可以根据真实数据集计算度量。或者根据真实数据集的大小是n，对吧？是的，真实数据集的大小是n。
- en: And you do the bootstrap nothing and you ask the base of the data set with size
    of n for， e-homes。 Yes we do we take the bootstrap samples of size n yes。 So why
    is it because they both calculate based on the size of the data set？
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你做自助法没有任何改变，并且你要求基于大小为n的数据集来计算e-homes。是的，我们确实是从大小为n的自助法样本中进行计算。那为什么呢？是因为它们都是基于数据集的大小来计算的吗？
- en: Yes everything is calculated based on a data set of size n that's right。 And
    if you calculate only one state on the real data set and it calculates the e-homes。
    of the real data set。 That's right we can calculate once on the original data
    set and we do the same thing。 B more times on each of the bootstrap。 That's right。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，一切都是基于大小为n的数据集来计算的，没错。如果你只对真实数据集进行一次计算，它会计算出该数据集的e-homes。没错，我们可以在原始数据集上计算一次，然后对每个自助法数据集做B次相同的计算。没错。
- en: So the question is you already got say mu hat on the original data set。 Why
    did I just calculate it B more times on the bootstrap data sets？ Anybody have
    an answer？
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题是，你已经在原始数据集上得到了μ帽了。那为什么还要对自助法数据集进行B次计算呢？有人能回答吗？
- en: That was a lot of answers。 Say again？ Yes so for example the first motivation
    was we have this estimator mu hat based on the。 data set but we want to put error
    bars on it。 We want to know how much variability there is。 Maybe we got another
    data set and it's a different number and we want to be able to。 say it's very
    common to when you give an estimate to say oh this is the temperature。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那是很多答案。再说一遍？是的，例如，第一个动机是我们有这个估计量μ帽，它基于数据集，但我们想要给它加上误差条。我们想知道它有多少变动性。也许我们拿到另一个数据集，它的值不同，我们希望能够。你知道的，当你给出估计时，通常会说：“哦，这是温度。”
- en: today is 80 degrees plus or minus half a degree。 Not a great example but when
    you do a poll or sample or something you have an error。 range on your estimate。
    But we don't have a way to get an error range。 We don't have an obvious way to
    get an error range if you just take mu hat of your， data set。 What else can you
    do？ You can look at the actual spread of those estimates to get an estimate of
    the standard。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的温度是80华氏度，误差范围是正负半度。虽然这个例子不太好，但当你做民意调查或抽样时，你的估计也有误差范围。但如果你仅仅取μ帽作为你的数据集估计，你就无法得到一个误差范围。你还能做什么呢？你可以查看这些估计值的实际分布，以估计标准误差。
- en: deviation of your original estimate。 So I'm not sure。 You said I trained B。
    I got B separate estimates and so it's better。 What would make it better？
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你原始估计的偏差。所以我不确定。你说我做了B次计算，得到了B个独立的估计，那会更好吗？那是什么使得它更好呢？
- en: Maybe if you average them together that might make it better。 The reason we're
    doing B separate ones is because we want to get an estimate for how much。 things
    vary from data set to data set。 That's why。 It's not about making it better yet。
    It's about estimating how variable it is。 So we're going to have one half n。 One
    half n。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 也许如果你将它们平均在一起，可能会更好。我们进行B个单独的计算，是因为我们想估计不同数据集之间的变化程度。就是这个原因。这还不是在提高结果，而是在估计它的变动性。所以我们会有一个n的一半，一个n的一半。
- en: Like if you want to have each sample generation one half n and the number n
    is same。 That's right。 The reason is that we're going to estimate variance and
    variance might change in the， other hand。 What's the one？ Yes。 Well， yes。 Okay。
    All right。 Suppose we have a probability distribution of mean mu。 We draw a state
    has standard deviation one。 All right。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，如果你想让每个样本的生成量为一半n，并且n的数量相同。没错。原因是我们要估计方差，而方差可能在另一方面发生变化。是什么呢？是的。好的。好了。假设我们有一个均值为mu的概率分布。我们抽取一个标准差为1的状态。好的。
- en: So we draw a sample size n from normal mu one。 We want to estimate mu。 We don't
    know what it is。 Okay。 So what's a good estimator？ All right。 New hat over data。
    Okay。 Okay。 What's the variance of mu hat？ All right。 All right。 All right。 All
    right。 You guys go one over n squared variance of a constant times a variable
    is the square of。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们从正态分布mu 1中抽取一个样本，样本大小是n。我们想要估计mu。我们不知道它是什么。好的。那么一个好的估计器是什么呢？好的。mu hat除以数据。好的。好的。那么mu
    hat的方差是多少呢？好的。好的。好的。好的。你们可以得到1/n平方的方差，常数乘以变量的方差是该变量的平方。
- en: the constant times the variance of the variable。 Great。 Now we get to the summation。
    What happens with the variance with the sum？ We can move the variance inside the
    sum when？
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 常数乘以变量的方差。太好了。现在我们进入求和部分。求和时方差会发生什么变化？我们什么时候可以将方差移到求和里面？
- en: X i's are independent。 Great。 Uncorrelated is sufficient。 So we can write one
    over n squared sum variance of x i。 What's the variance of x i？ Great。 All right。
    So this is one。 We're summing one to n of one。 What's that？ And the final thing
    is。 All right。 Good。 So the variance of mu hat is one over n。 So that's the dependence
    on n。 Great。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: X i 是独立的。太好了。无关性就足够了。所以我们可以写出每个x i的方差之和除以n平方。x i的方差是多少？太好了。好了。所以这是1。我们对1到n求和，结果是多少？最后的结果是。好了。好的。所以mu
    hat的方差是1/n。这就是n的依赖性。太好了。
- en: So now --， [ Background Sounds ]， So by mathematics。 we just figured out that
    the variance of mu hat is one over n。 The problem is a lot of times you come across
    an estimator that we cannot do that， calculation。 We do not know how to figure
    out what the variance is。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在——，[背景声音]，所以通过数学推导。我们刚刚得出mu hat的方差是1/n。问题是，很多时候你会遇到一个估计器，我们无法进行这个计算。我们不知道如何计算它的方差。
- en: We don't know how to do the math to calculate it in closed form with the， variances。
    So the boost trap is a way to figure out what that variance is。 But the reason
    I wrote that out here is because， yes。 So the distribution of mu hat --。 Now what
    do I mean by the distribution of mu hat now？ What I mean is every time you take
    a sample。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道如何做数学计算以封闭形式计算它的方差。所以boost trap是一个计算方差的方式。但我在这里写出来的原因是因为，是的。所以mu hat的分布——现在我所说的mu
    hat的分布是什么意思？我的意思是，每次你抽取一个样本。
- en: you get a mu hat。 And then you take another random sample of size n and you
    get another mu hat。 And then you histogram those up。 Okay。 And then standard deviation
    would be what， roughly。 for this histogram？ One on squared of n， yeah。 Okay。 So
    if you figure this thing gets really small。 And so， yeah， for -- you know， this
    might be n equals 100。 And n equals 1，000 might look --， Okay。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到一个mu hat。然后你再抽取另一个样本，大小为n，你得到另一个mu hat。然后你把这些mu hat做成直方图。好的。那么这个直方图的标准差大概是多少呢？是1除以n的平方根，对吧？好的。所以如果你算出这个值，它会变得非常小。于是，嗯，比如说，n等于100。n等于1,000可能看起来——好的。
- en: So as we change n， the shape of the distribution we're estimating changes。 So
    we want to get -- we want to -- we're estimating this distribution by getting，
    sample points。 And we have to be drawing the sample points from the same distribution。
    which means they have to be estimators using data sets at the same size。 That's
    what's going on。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们改变n时，我们估计的分布形状会发生变化。所以我们想要得到——我们想要——我们通过获取样本点来估计这个分布。并且我们必须从相同的分布中抽取样本点。这意味着它们必须使用相同大小的数据集作为估计器。就是这样。
- en: Okay。 Does that help？ Yeah。 Oh， great。 So basically， if you're in a strat。 it's
    a method for calculating or， estimating the distribution of the statistic based
    on one sample。 I like that characterization。 So this strat is a method of estimating
    the distribution of a statistic based on a single sample。 Sounds good to me。 Okay。
    Any question， or is -- I don't know much about it。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这有帮助吗？是的。哦，太好了。所以基本上，如果你处于一个策略中，这是一个基于单个样本计算或估计统计量分布的方法。我喜欢这个表述。所以这个策略是一个基于单个样本估计统计量分布的方法。我觉得挺好的。好的。有问题吗，还是——我不太了解。
- en: but there's something called Jacknite。 Is that sort of the same problem？ And
    another question is。 is this the best method for this kind of estimation？ This
    is for this strat。 And the third question is， what if our samples are dependent
    somehow？
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一种叫Jacknite的技术。这是不是同样的问题？另一个问题是，这种估计方法是最适合这种情况的吗？这是针对这个策略的。第三个问题是，如果我们的样本之间有某种依赖关系呢？
- en: Is there a variant of bootstrap which would solve that problem？ Okay。 In reverse
    order。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有一种变种的自助法（bootstrap）可以解决这个问题？好的，逆序来说。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_8.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_8.png)'
- en: Yeah， there are variants of bootstrapped， but I think they're very specific
    to the dependence。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，的确有自助法的变种，但我认为它们很具体地依赖于数据的依赖性。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_10.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_10.png)'
- en: Next previous question， is this the best method for doing what we're describing？
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的问题是，这个方法是做我们正在描述的事情的最佳方法吗？
- en: It's the best method that I know of。 It's the best method that -- yeah。 So --
    it's the best method that I'm here to tell you about。 Yeah。 bootstrap was a big
    deal invention。 Prior to this。 Jacknite was indeed another technique to the same
    thing。 But -- I remember what that was about。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我知道的最佳方法。这是最好的方法——对，就是这样。所以——这是我今天来告诉你们的最好的方法。是的，自助法是一项重大的发明。在此之前，Jacknite的确是用来做类似事情的另一种技术。但——我记得那是怎么回事。
- en: Anyway， but the bootstrap is kind of basically supersede the Jacknite。 No one
    really uses Jacknite anymore。 It shows up on problem sets now and then just because
    you can。 But -- was there another question also？ No。 Okay。 [ Inaudible ]， Okay。
    So。 what if you drew -- you're saying you drew a sample size to N？ And what would
    the idea be again？
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，自助法基本上取代了Jacknite。现在几乎没人再使用Jacknite了。它偶尔会出现在习题集里，因为可以用它。但——还有其他问题吗？没有。好的。[听不清]，好的。那么，如果你画了——你是说你抽取了一个样本大小为N？那么它的思路是什么呢？
- en: Well， so for just following this， then you could have a smaller -- a more narrow
    distribution。 But you're still just using the same data that you already have。
    Okay。 All right。 So。 I think what you're getting at is -- well， for -- so， if
    we want to pretend that this。 is a big deal， we're trying to get a real good estimator。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果跟着这个做，你可以得到一个更小、更窄的分布。但你仍然只是使用你已经拥有的相同数据。好的，明白了。那么，我想你想表达的是——好吧，对于——如果我们想假装这是一件大事，我们在试图得到一个真正好的估计器。
- en: Why not just take a sample of size 2N or 10N and then -- well， we just have
    a better， estimate？
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不直接取一个大小为2N或者10N的样本，然后——嗯，直接得到一个更好的估计值呢？
- en: In that case， you run a sigma by xi in seconds because --， Because？ Because？
    Xi will be 40。 I mean --， Okay。 So， but we're -- the assumption -- the leap of
    faith here is that we are kind of treating。 a Buschap sample as though it's a
    real IID sample from the original distribution。 I'm using that -- I'm saying that
    acting in that way gives you results that make。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要每秒运行sigma和xi，因为——为什么？为什么？Xi会是40。我是说——好的。那么——我们做的假设——这里的信任跳跃是，我们将Buschap样本当作来自原始分布的真实独立同分布（IID）样本来处理。我这么说，是因为以这种方式处理会得到合理的结果。
- en: sense for things like variance estimating。 But the same stuff doesn't go through
    for that kind of thing。 So -- which is good because that would be crazy。 All right。
    Any more？ Yeah？ Sure。 [ Inaudible ]。 Okay。 So， the question is if you knew your
    data was noisy， which I'm going to take to mean。 high variance or --， Outlives
    or something。 Okay。 And then the idea is are you hoping to get some robustness
    by taking a smaller sample？
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像方差估计这样的事情，能理解你的意思。但对于这种情况，基本原理并不适用。所以——这很好，因为如果适用的话，那就太疯狂了。好的，还有其他问题吗？是的？当然。[听不清]。好的。那么，问题是如果你知道你的数据是嘈杂的，我理解为高方差，或者——有异常值之类的情况。好的。那么，问题是你是否希望通过抽取一个较小的样本来获得一些鲁棒性？
- en: '[ Inaudible ]， Okay。 So， that''s interesting。 So， first of all， that is on
    the same。 That is along the lines of let''s just make a new estimator， whereas
    the motivation for。 the Buschap is let''s figure out the variance of a given estimator。
    So。 the estimator is I''m going to compute this function on a data set of size
    n。'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，好的。那么，这很有趣。首先，这种方法是沿着“让我们做一个新的估计器”的思路走的，而Buschap的动机是“让我们找出给定估计器的方差”。所以，这个估计器是“我将计算这个函数，应用于大小为n的数据集”。'
- en: And now I want to use Buschap to figure out the variance of that estimator。
    And now you're suggesting let me suggest a new way to estimate something by。 sub-sampling
    something smaller than n and computing my thing on smaller than n。 because maybe
    then I'll get luckier and avoid the outliers or something。 Okay。 So。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想使用 Buschap 来计算该估计量的方差。你现在建议让我用一种新的方式来估计，通过从小于 n 的样本中进行子抽样，然后在小于 n 的数据上计算我的值，因为也许这样我会更幸运，避开离群值什么的。好的。那么。
- en: I don't know the limit of that approach is like just take something of size
    really， small。 So。 there's obviously a trade-off right between using all your
    data， which might。 have some noisy things in there versus smaller subs of data，
    which is just。 giving you less to work with。 So， I don't know if that's -- I don't
    think that would help。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道这种方法的极限是什么，可能就是只取非常小的样本。显然，在使用所有数据之间有一个权衡，数据中可能有一些噪音，或者使用较小的子集，这样你能用的东西就更少。所以，我不知道那是否——我不认为那会有所帮助。
- en: but maybe there are some， strange distributions out there where it actually
    -- I mean， there are。 distributions which is crazy。 There are distributions where
    if you take a bunch of samples -- if you take a。 sample from the distribution
    and you average those points， your estimate of the mean。 of the expected value
    distribution is actually worse than if you took a single。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，也许外面有一些奇怪的分布，实际上——我是说，确实有。分布是疯狂的。存在一些分布，如果你从分布中取一堆样本——如果你从分布中取样，并平均这些点，你对期望值的估计其实比你只取一个点作为期望值的估计还要差。
- en: point as your estimate of the expected value of the distribution。 So。 there
    is an -- this crazy distribution where what you're saying is actually the， correct
    solution。 Like， don't take n points。 The more you have， the more you're going
    to get messed up by outliers。 Just take one。 But that is a very unusual scenario。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，有一个——这种疯狂的分布，在这种情况下，你所说的实际上是正确的解决方案。就像，别取 n 个点。你拿的越多，越容易被离群值弄乱。只取一个。但这是一个非常不寻常的场景。
- en: It has to do with the fact that the distribution has very， very heavy， tails。
    So。 very likely to get a very large number。 Like， they -- they're distributions
    that don't have variances and they're。 distributions that don't have expectations。
    So， the quotient distribution is -- so it has a lot of these strange properties。
    Okay。 Yeah？ [ Inaudible ]， No。 No。 This is -- no distribution -- no assumptions
    on the distribution。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这与分布的尾部非常非常重有关。所以，非常可能会得到一个非常大的数值。例如，有些分布没有方差，也有些分布没有期望。所以，商数分布是——它有很多这种奇怪的性质。好的。是吗？[听不清]，不。没有。没有假设这个分布——
- en: Yeah。 [ Inaudible ]， That's a nice question。 Not that I'm aware of。 So。 I think
    the -- I can get back to you on that， though。 I could look into that。 Nothing
    enough that stands out。 So， maybe the theory will have some --， Okay。 All right。
    So。 so far we've talked about bootstrap。 Now we're going to talk about why you
    might want to do something like。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。[听不清]，这是一个好问题。到目前为止，我并不清楚。所以，我想——我可以再回来给你答复，我可以查一下。没有什么特别突出的地方。所以，也许理论上会有一些——好的。好吧。所以，到目前为止，我们讨论了
    bootstrap。接下来我们将讨论为什么你可能会想做这样的事情。
- en: averaging your bootstrap samples。 Okay。 But let's -- let's size that into that。
    All right。 So。 let's start with a scenario and do what we're calling a lousy or
    a bad， estimator。 Suppose we have -- we're starting over now。 Okay。 We're having
    -- we have z's one through n。 their iid， from some， distribution。 They have expected
    value mu and then our dvn is variance sigma。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对你的 bootstrap 样本进行平均。好的。那我们——我们把它放进去。好吧。那么，让我们从一个场景开始，做一个我们称之为劣质或坏的估计量。假设我们有——我们现在重新开始。好的。我们有
    z 的 1 到 n。它们是独立同分布，来自某个分布。它们的期望值是 mu，然后我们的 dvn 是方差 sigma。
- en: squared。 Okay。 We want to estimate mu。 So， one particularly poor estimator of
    mu would be z one。 What happens with z one？ What's the expectation of z one？ It
    is？ The expectation of z one is mu。 So。 it's an unbiased estimator of mu。 That's
    a good start。 What's the variance of z one？ Sigma squared。 The variance of z one
    is sigma squared。 Okay。 So， we could use z one to estimate mu。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: squared。好的。我们想估计 mu。那么，一个特别差的 mu 估计量是 z one。z one 会发生什么？z one 的期望是什么？它是？z one
    的期望是 mu。所以，它是一个无偏的 mu 估计量。这是一个好的开始。z one 的方差是多少？Sigma squared。z one 的方差是 sigma
    squared。好的。所以，我们可以用 z one 来估计 mu。
- en: We're leaving a lot of data on the table。 But it's an estimator。 And so。 the
    variance is sigma squared。 But the point is we could do better than sigma squared
    by averaging。 So， the average -- if we take the average of z one through z n。
    it actually is a characteristic we just did over there。 It has the same mean，
    mu， good。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们留下了很多数据在桌子上。但它是一个估计器。所以，方差是sigma平方。但关键是，通过平均，我们可以比sigma平方做得更好。所以，平均值——如果我们取z
    one到z n的平均，它实际上就是我们刚才做的那个特征。它有相同的均值，mu，很好。
- en: But now the variance has gone down by a factor of n。 So。 standard deviation
    has gone down by a factor of square root of n。 So。 this is in our nutshell why
    we take averages。 Because the variance goes down by one over square root of n。
    So， the question is， can we use this phenomenon to help us with， you know， machine
    learning？ So。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在方差已经下降了n倍。所以，标准差下降了n的平方根倍。所以，这就是我们为何取平均值的原因。因为方差下降了1除以n的平方根。那么，问题是，我们能否利用这一现象来帮助我们做机器学习？
- en: this is very simple parameter point estimator。 Can we use this to help us with
    machine learning where we have。 prediction functions， masseter of thing？ So， let's
    go back to our original idea。 Like， well。 suppose we had B independent training
    sets now， right， from the same distribution。 And we have a learning algorithm
    that gives us B decision functions。 So。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的参数点估计器。我们可以用这个来帮助我们做机器学习吗？我们有预测函数，马赛特等东西？那么，让我们回到最初的想法。假设我们现在有B个独立的训练集，对吧？它们来自相同的分布。我们有一个学习算法，给我们B个决策函数。那么。
- en: now we have F one hat all with F B hat。 Is it clear？
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有F one hat和F B hat。清楚吗？
- en: We have one decision function based on training our algorithm on each， of B
    different training sets。 sample from the same distribution。 Great。 So， then what
    would we want -- suppose this is regression。 What would you want to do with the
    F hats？ Yeah， I would want to average these F hats。 So。 the average prediction
    function in the regression setting。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个决策函数，基于在每一个B个不同的训练集上训练我们的算法，样本来自同一分布。很好。那么，我们接下来想做什么——假设这是回归问题。你会想如何处理这些F
    hats？是的，我会想把这些F hats进行平均。所以，在回归问题中，平均预测函数就是这样。
- en: would be just take the average of the F hats that we got from each， of these
    individual sets。 And then you'd expect the variance of F hat to be less than the。
    variance of a single prediction function。 All right， just as a concept check，
    what's random here？
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 只需对我们从每个独立集得到的F hats取平均。然后你会预计F hat的方差会比单个预测函数的方差小。好吧，作为一个概念检查，这里什么是随机的？
- en: What's random in this description？ Training sets are random， exactly。 That X
    there。 this is just to indicate that we have functions。 It's not random。 It could
    be。 but in this setting we're not saying X is random。 All right。 So。 the average
    -- so we're going to fix some X。 The average prediction function is defined like
    that。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个描述中有什么是随机的？训练集是随机的，没错。那个X，这只是为了表示我们有函数。它不是随机的。它可能是随机的，但在这个情境下我们并没有说X是随机的。好吧。那么，平均值——我们将固定某个X。平均预测函数是这样定义的。
- en: And then because X is fixed and we can think of each of these F， average of
    X， that's a number。 right？ The function is not being the real。 So， this is a random
    variable， right？
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后因为X是固定的，我们可以认为每一个F，X的平均值，那是一个数字，对吧？函数不再是实际的。所以，这就是一个随机变量，对吧？
- en: It's random because it's random。 It's a random training set。 So， if a random
    variable left。 then we have this average of， random variables on the right。 So。
    this is pretty much exactly the scenario we were in in the。 first set of slides
    where we had these estimators， mu hat。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它是随机的，因为它是随机的。它是一个随机训练集。所以，如果一个随机变量被移除，那么我们就有右边这些随机变量的平均值。所以，这几乎正是我们在第一组幻灯片中遇到的情境，我们有这些估计器，mu
    hat。
- en: and we calculated them on independent data sets。 We had independent random variables。
    So。 now we have， again， independent random variables， which is the prediction
    of the decision functions on a fixed X。 We would expect that the variance of F
    hat average for particular， X to go down also， like what？
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在独立的数据集上计算了它们。我们有独立的随机变量。所以，现在我们再次拥有独立的随机变量，这就是决策函数在固定的X上的预测。我们预计，对于特定的X，F
    hat平均值的方差也会下降，像什么呢？
- en: 1 over square root of B in this case。 Okay。 Good。 Okay。 Let's take a few more
    minutes before we break。 All right。 So， it seems great。 We have this。 we can take
    a bunch of data sets and average them， and we decrease the variance。 and the expected
    value is the same， just like before。 So， it seems like a win。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下是B的平方根的倒数。好的，明白了。好了，我们在休息前再花几分钟时间。好了，看起来很不错。我们有这个方法。我们可以拿一堆数据集并求平均，减少方差，而期望值是相同的，就像之前一样。所以，看起来这是一个胜利的策略。
- en: but of course we don't actually， we're back to the same show off we discussed
    before。 or we have this really， you have one training set， and what do you do
    if you want the variance？
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 但当然我们实际上，回到了我们之前讨论的同一个问题。或者我们只有一个训练集，那如果你想降低方差该怎么办呢？
- en: Do you break it up in pieces and average them， but then each of your pieces
    is much smaller。 So。 we have a possible answer。 Yeah， exactly。 So， we can。 instead
    of hypothesizing that we have B independent， training sets。 let's give a try to
    take and B bootstrap training。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你将数据拆成若干部分并求平均，但每部分的数据量更小。那么，我们有可能的解答。是的，正是如此。所以，我们可以。不是假设我们有B个独立的训练集，而是试试用B个自助法训练集。
- en: sets and training on each of these and then averaging those。 Okay。 Let's take
    a break。 Let's see you in 10。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练集上训练然后求平均。好的，我们休息一下，10分钟后见。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_12.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_12.png)'
- en: What's going on？
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？
- en: '![](img/7e658e45466cec419a4c446a21134cc6_14.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_14.png)'
- en: It's awesome。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_16.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_16.png)'
- en: All right。 Roll it？ Okay。 So， the idea that we just talked about was， boy。 would
    it be nice if we had B independent training sets。 we computed our decision function
    on each of them， we combine them。 say by averaging in the regression case， you'd
    have lower variance， same expected value。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，开始吧？好的。那么我们刚才讨论的想法是，哇，如果我们有B个独立的训练集会有多好。我们在每个训练集上计算我们的决策函数，然后将它们结合起来。比如在回归问题中，通过求平均，你将获得较低的方差，相同的期望值。
- en: That sounds great， but we don't have these replicated data sets。 What the heck？
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来不错，但我们没有这些复制数据集。怎么回事？
- en: Let's try bootstrap replicates and train on each of those and then， average
    those。 So。 that method has a name。 It's called bagging。 So， let's break that down
    more precisely。 So。 let's start with B bootstrap replicates from the original，
    data set D。 All of the same size。 I left off the subscript and let F1 hat through
    F hat B be the。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试试自助法复制，并在每个复制集上训练，然后求平均。所以，这种方法有个名字，叫做袋装法。我们再更精确地分析一下。首先，从原始数据集D中获取B个自助法复制集，大小相同。我省略了下标，让F1^hat到FB^hat表示这些复制集的。
- en: actual decision functions based on training on each of those， training sets。
    And then。 we have something called the bag decision function。 which is basically
    take all these things and combine them， somehow into something we'll call the。
    this should be bag not average。 It's something we'll call the bag estimator。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 基于在每个训练集上的训练，得到的实际决策函数。然后，我们有一个叫做袋装决策函数的东西。它基本上是将所有这些东西结合起来，某种方式组合成我们称之为袋估计器的东西。
- en: But let's think about how we can combine these things。 So， here are some cases。
    How would we plausibly combine them for regression？ Average。 Yes。 Great。 What
    about for binary class predictions？ So， say each F hat gives a zero or a one。
    What might you do then？ Majority certainly makes sense to me。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们想想如何将这些东西结合起来。这里有一些情况。我们如何在回归问题中合理地结合它们？平均。对，太好了。那对于二分类预测呢？比如每个F^hat给出0或1。那你会怎么做？多数原则对我来说是显而易见的。
- en: A certain type of average called the mode。 We'll do it。 Alright。 what if the
    predictions are probabilities？ Average seems plausible。 Somewhere numerically
    combined。 Have a multi-class hard predictions。 Yeah， plurality。 You know， whichever
    has the most votes。 Yeah。 Yeah。 What do you do able to average the weights that
    we check out from， there？ Okay。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一种叫做众数的平均方法。我们可以这样做。好了，如果预测值是概率呢？平均似乎是合理的。数值上结合一下。对于多类硬预测呢？对，最多票数的类别。你知道的，哪个类别票数最多就选哪个。对的。那我们该怎么做，能否将我们从那里检查出来的权重平均呢？好的。
- en: so the question is， do you want to， so I'm suggesting。 combine the predictions
    and you're suggesting， well， what about， combining the weights？
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题是，你想要，嗯，我建议将预测结果结合起来，而你在建议，那权重怎么结合呢？
- en: The actual internals or the prediction functions themselves。 Should we combine
    those？
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，预测函数本身。我们是否应该将它们结合起来？
- en: That's interesting。 So， something like that sometimes happens when you do parallel，
    computations。 parallel learning。 But that would be a totally different approach。
    Yeah。 Well。 at least conceptually it's a totally different approach。 In certain
    situations it might turn out to be the same。 Yeah， for linear methods。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有意思。所以，类似的事情有时会在你做并行计算或并行学习时发生。但那会是完全不同的方法。是的，至少从概念上讲，它确实是完全不同的方法。在某些情况下，它可能最终是相同的。是的，对于线性方法来说。
- en: Seems like it for linear regression。 Alright， good。 So you have some ideas on
    how to combine。 all these things， sound good。 This is a method from a '96 Leo
    Brimann， a famous statistician。 who did a lot of machine learning。 Did a lot of
    the bridging the gap between statistics and machine。 learning。 This is Leo Brimann。
    He also invented random forests。 Pretty much owns this lecture today。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来是这样，适用于线性回归。 好的，没问题。 你对如何组合这些内容有一些想法，听起来不错。 这是1996年Leo Brimann提出的一种方法，他是著名的统计学家，做了很多机器学习的研究，弥合了统计学和机器学习之间的差距。这就是Leo
    Brimann。他还发明了随机森林。今天的讲座几乎可以说是他的。
- en: He didn't invent the bootstrap。 That was a guy's Stanford。 Yeah。 In this situation。
    is there any way to distinguish between the， functions and do a weighted average
    based on how well they。 predict？ Well， that's a really interesting question。 What's
    that not really work here？ No。 so the question is， would it make sense to do some
    kind of。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 他并没有发明自助法。那是斯坦福的一个人发明的。是的。在这种情况下，是否有办法区分这些函数，并根据它们的预测表现做一个加权平均？嗯，这是一个非常有趣的问题。为什么不行呢？不行。所以问题是，做某种形式的加权平均是否有意义？
- en: a weighted average based on some estimate of how well the F， hats did？ Right？
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 基于F hats表现如何的某种估计，做一个加权平均？对吧？
- en: So I like the intuition。 Do you have an idea on how you might estimate how the
    F hats are， doing？
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这种直觉。你有没有什么方法来估算F hats的表现？
- en: That's what I was like kind of wondering if I'm really like， is like more theoretical，
    so yeah。 I'm really able to move， that back。 Yeah， well， so you might， okay。 so
    one way that could work is if， you had a ton of data。 you could estimate each
    F hat's performance， on some holdout data。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直在想，如果我真的是从理论上来看，是否能这样做。所以，是的，我真的能将其转回去。好吧，一个可行的方式是，如果你有大量数据，你可以估算每个F hat在某些保留数据上的表现。
- en: But you would never really have that much。 You would never use the training
    performance。 at least not， and obviously because， I mean， you might， it depends
    on the， situation。 but often F hats are going to be maybe over fit to， the training
    data。 So how they do on the training data may or may not be a very。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 但你通常不会有这么多数据。你不会使用训练性能，至少不会使用，显然，因为，我是说，这取决于情况，但通常F hats可能会对训练数据过拟合。所以它们在训练数据上的表现可能好也可能不好。
- en: good indication of how they do on new data。 Can you combine cross-validation
    with the bootstrapping technique？
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好地指示，能够表明它们在新数据上的表现。你能将交叉验证与自助法技术结合起来吗？
- en: And the goal being to find out weights for the F hats？ Yeah。 That's interesting。
    I think the closest that we're going to come to that is the， next topic， which
    is boosting。 out of boost， which does definitely， have a flavor of this combining
    by how good the prediction functions。 are。 Okay。 So let's just talk about regression，
    bagging for regression， for a minute。 So here。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到F hats的权重吗？是的，这很有意思。我认为我们接近这个的内容是下一个主题——提升方法。提升方法确实有这种根据预测函数表现来组合的味道。好的。那么我们就谈谈回归的集成学习方法，先讲一分钟。
- en: and I got the subscript right， so here the bagging is， yes。 the average of the
    predictions of the regression， functions。 And so if the bootstraps samples are
    actually independent draws， so not boost up。 but like the real thing， independent
    draws， in original distribution。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我把下标写对了，所以这里的集成学习是对回归函数预测结果的平均。如果自助法样本实际上是独立抽取的样本，而不是提升法样本，而是像真正的独立抽样一样，从原始分布中抽取的样本。
- en: then F hat bag would have the same， expectation of any individual decision function，
    great， and it。 would have smaller variance， which is very nice。 So empirically。
    one often gets a similar effect for bagging， but， sometimes there's no effect
    and sometimes there's a huge benefit。 of bagging。 So， and why？ Well， bootstraps
    samples aren't independent samples from the。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 那么F hat的集成方法将具有任何单一决策函数相同的期望值，太好了，它还会有更小的方差，这非常好。所以从经验上看，集成学习通常会得到类似的效果，但有时没有效果，有时集成学习会带来巨大的好处。那么，为什么呢？因为自助法样本并不是独立的样本。
- en: real distribution。 They are the bootstraps samples。 So there's no guarantee
    it's going to help。 but sometimes it， does help。 And we'll all give a little bit
    more intuition and a slide on。 when it might or when people say it might。 There's
    one cool thing you can do with bagging。 So remember every time we took a bag bootstraps
    sample， something。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 真实分布。它们是自助法样本。所以没有保证它一定会有效，但有时它确实有效。我们将在接下来的幻灯片中提供更多直觉，讨论它可能有效的情况，或者人们说它可能有效的情况。使用袋装法有一个很酷的地方。记住每次我们进行袋装自助法抽样时，某些事情会发生。
- en: like 37% of the data set is not included in the bootstraps sample。 Right？ Okay。
    So if we train on the 63% that we're left with， we could predict。 on the 37% and
    that would be an out-of-sample prediction for， that particular decision function。
    So there's kind of a built-in test set for any given decision。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 比如数据集的37%没有包含在自助法样本中。对吗？好。所以如果我们在剩下的63%上训练，我们可以对那37%进行预测，这将是该特定决策函数的外部样本预测。所以每个决策都有一个内建的测试集。
- en: function that we train on a bootstraps sample。 Is that clear？ Right。 whatever
    is still in the bucket after we draw our bootstraps， sample。 well we didn't train
    on that for this particular decision， function。 so we could use that to get an
    estimate on a performance。 But so let's extend that idea a bit。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在自助法样本上训练的函数。清楚吗？对。无论自助法抽样后，剩下的是什么，我们就没在这次决策函数上训练过。所以我们可以利用这些剩余的数据来估计一个性能。但是我们再扩展一下这个想法。
- en: So let's now look at each of the end training points。 So we're going to let
    S of i be a set referring to the i'th， training point out of n。 And we're going
    to let b be the indices of all the bootstraps。 samples that do not contain the
    i'th training point。 All right。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下每个训练点。所以我们将让 S of i 代表第i个训练点所在的集合，出自 n。我们将让 b 代表所有不包含第i个训练点的自助法样本的索引。好吧。
- en: so we got capital B bootstrap training sets。 And roughly 37% of them are not
    going to have the i'th data。 point。 Let's collect those， let's collect the prediction
    functions。 F hat i。 Let's collect the prediction functions corresponding to the。
    data sets that did not train on that data point。 And let's evaluate the prediction
    performance by how it does。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得到了大写字母B的自助法训练集。大约37%的数据点将不会出现在第i个数据点的自助法样本中。我们来收集这些，收集预测函数 F hat i。我们来收集那些没有在该数据点上训练的样本对应的预测函数。然后通过其表现来评估预测性能。
- en: Next slide。 All right， so we call it the out of bag error estimator or the。
    out of bag prediction function。 Basically， we're going to predict on xi， the i'th
    training point。 to get an estimate of how well we would do an out of sample， sense
    on xi。 We look at all of the f hat b's that were not trained on the， i'th data
    point。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 下一张幻灯片。好吧，我们称之为外部袋错误估计器或外部袋预测函数。基本上，我们将在 xi，第i个训练点上进行预测，以估算我们在 xi 上进行外部样本预测时的表现。我们查看所有没有在第i个数据点上训练的
    f hat b。
- en: And we average the predictions of those functions。 So f hat o would be we can
    look at how it predicts on xi。 And the performance on that data point is somehow
    legitimate， out of sample performance。 Any questions on that？ Is that clear at
    all？ Some people found it clear。 Say again。 Yeah。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这些函数的预测结果进行平均。所以 f hat o 就是我们可以查看它在 xi 上的预测情况。而在该数据点上的表现某种程度上是合法的，属于外部样本的表现。对此有任何问题吗？这是否清楚？有些人觉得很清楚。再说一遍。好吧。
- en: I guess I could。 That's interesting。 Let's think about that。 Yeah。 All right。
    Does anyone have a question to ask on this？ All right。 All right。 So this is a
    pretty good way to estimate test error。 It's kind of a just a side benefit of
    using this type of， method。 In practice， it's -- you know。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我可以。那很有趣。我们想想这个。好吧。好了，有人有问题要问吗？好吧。好吧。所以这是一种估算测试误差的非常好的方法。它几乎是使用这种方法的附带好处。实际上，它是——你懂的。
- en: if you have the data， it's better to， use a validation set。 It's more accurate，
    typically。 Okay。 All right。 So here's an illustration of what happens when you
    bag trees。 How can we -- how are we going to bag classification trees？ So we discussed
    two methods already。 It's a classification so you can either do a majority or。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有数据，最好使用验证集。通常来说，它更准确。好吧。好了，这是一个袋装树发生时的示意图。我们如何袋装分类树呢？我们已经讨论了两种方法。这是一个分类问题，所以你可以选择多数投票或者。
- en: plurality build of the classes or you could average the， probabilities that
    are predicted。 All right。 So here's five different trees based -- so we have a
    sample， of size 30。 And then we built a tree on that and we get this original
    tree on， the top left。 And now we took five new chap samples， also of size 30，
    and built decision trees on each of this。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 类别的多数构建，或者你可以对预测的概率进行平均。好的。这里有五棵不同的树——我们有一个大小为30的样本。然后我们在这个样本上构建了一棵树，得到了左上角的这棵原始树。现在我们又拿了五个新的章节样本，大小同样是30，并在每个样本上构建了决策树。
- en: And we got five more decision trees。 And what's interesting is that the decision
    trees are often。 quite different。 So the structures are different。 Can you guys
    see these variables？
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了五棵新的决策树。很有趣的是，决策树通常是非常不同的。所以它们的结构不同。你们能看到这些变量吗？
- en: The first variable here splits on x1， x2， x3， x4。 They're splitting on a different
    variable in the first -- in the。 root node。 The trees are quite different in how
    they're doing the。 classification based on the different bootstrap samples。 And
    this kind of instability to changes in the data set -- some。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一个变量在x1、x2、x3、x4上进行划分。在第一个——根节点中，它们在不同的变量上进行划分。树的分类方式有很大的不同，基于不同的自助采样。这种数据集变化带来的不稳定性——一些。
- en: mild change in the data set -- that is often referred to as， kind of high variance。
    It's a high variance method that a small change in the training。 set can lead
    to large changes in the final prediction function， or at least the structure of
    it。 And what we'll point out later is that people often say methods。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的轻微变化——通常被称为高方差。它是一种高方差方法，训练集中的一个小变化可能会导致最终预测函数的重大变化，或者至少是它的结构变化。我们稍后会指出，人们通常说方法。
- en: like bagging work best when you have this kind of -- any given， tree has a lot
    of variance to it。 a lot of variability。 And bagging helps reduce that variability。
    All right。 So here's a picture of bagging trees using two different methods。 So
    every tree， of course。 you have very easily either， probability prediction or
    a hard class prediction based on the。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，当你有这样的——任何给定的树都有很大的方差，很多变异时，bagging方法效果最好。bagging有助于减少这种变异性。好了，下面是使用两种不同方法的bagging树的图示。所以每一棵树，当然，你可以很容易地得到基于概率的预测，或者基于共识方法的硬分类预测。
- en: relative proportion of classes in your leaf nodes。 And so on the x-axis we have
    the number of bootstrap samples that。 we're using that we're averaging together
    to get our bagging， estimator。 So all the way on the left we have one。 So we have
    a single sample from the original。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 叶节点中类别的相对比例。x轴上是我们使用的自助采样的数量——我们把这些样本平均在一起，得到我们的bagging估计器。所以最左边我们有一个。就是来自原始数据的单个样本。
- en: And then as we take more， the test error -- so this is the。 performance of the
    average of bead bootstrap samples -- it。 decreases very nicely initially and then
    it kind of levels off。 Which kind of makes sense。 There's only so much error that's
    due to the fact that the。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，随着我们使用更多的样本，测试误差——这是基于所有自助采样的平均表现——最初下降得非常好，然后逐渐趋于平稳。这有点道理。因为总有一些误差是由于以下原因造成的。
- en: data set -- there's some randomness in your data set。 More of the error is the
    fact that you have a small data set。 I would say。 All right。 So eventually you
    kind of max out how much benefit you can get， from this bootstrap resampling。
    And kind of can see there's not too much difference between the。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集——你的数据集里有一些随机性。更多的误差来自于你有一个小数据集。我会这么说。好了，最终你会发现，从这个自助重采样中获得的好处会达到一个极限。可以看到，样本之间的差异不大。
- en: consensus method which is the yellow and the probability method， which is green。
    Any questions on this or the previous slide？ [ Inaudible ]， Yeah。 this picture
    maybe seems that probably is a little bit， more robust。 It varies from situation
    to situation。 So it's probably best to try them both and do some cross。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是黄色的共识方法和绿色的概率方法。对此或者上一张幻灯片有问题吗？[听不清]，对。这个图看起来可能更稳健一些。它因情况而异。所以最好尝试两者，并做一些交叉验证。
- en: validation or something。 Yeah。 Sorry。 What is consensus？
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 验证或者其他什么的。对不起。什么是共识？
- en: That's another word for like majority of plurality or -- whichever， has the
    highest number of votes。 Consensus really means everyone agrees so that's not
    really， consensus。 But -- because the rest of the story is the one who were the，
    plurality。 plurality that convinced the rest of the people， to agree with them
    and then it was consensus。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“多数”或“多数派”的另一个说法——即哪个有最多的票数。共识真正的意思是每个人都同意，所以那不算是真正的共识。但——因为接下来的故事是少数派说服了大多数人，使得大家达成一致，最终形成了共识。
- en: All right。 So a lot of times people -- so bias and variance have a。 technical
    -- there's one definition that everyone agrees on。 which is the one I've given
    you which is the case of you're。 predicting a real value and you have your expectation
    and your。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以很多时候人们——偏差和方差有一个技术上的定义——是每个人都同意的。这就是我给你讲的定义，就是你预测一个真实值时，你有一个期望值。
- en: variance and that whole story that's not disputed。 But people would like to
    use the terms bias and variance in。 broader scenarios where you're talking about
    classification and。 not regression or -- so I want to give you a flavor of what。
    people have in mind when they're saying bias and variance in， those scenarios。
    And it's kind of related to estimation error and， approximation error but I'll
    treat them separately here。 So -- all right。 So we have a hypothesis space。 We
    always have a hypothesis space。 Let's call it F。 So just introducing the hypothesis
    space will use the。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 方差以及整个故事没有争议。但人们希望在更广泛的场景中使用偏差和方差的术语，比如在讨论分类而不是回归时——所以我想给你一些人们在那些场景中谈论偏差和方差时的思路。这与估计误差和近似误差有关，但我会在这里将它们分开讨论。所以——好吧。我们有一个假设空间。我们总是有一个假设空间。我们称之为
    F。所以仅仅引入假设空间我们就会使用它。
- en: term it biases the fit which is to say it's a bias on the fit。 you end up with
    because what has to come from that hypothesis， space？ It's a restriction。 A bias
    in this scenario is like a restriction of this prediction， of functions you can
    come up with。 You know， someone comes in and says I want to use a different， hypothesis
    space。 Okay。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 术语偏差适用于拟合，即它是对拟合的偏差。你最终得到的拟合必须来自于这个假设空间吗？这就是一种限制。在这种情况下，偏差就像是你可以得出的预测函数的限制。你知道，有人进来说我想使用一个不同的假设空间。好的。
- en: You've just biased things differently。 When you have a big hypothesis space
    that's one level of bias and。 if you make it strictly smaller that would be maybe
    more biased。 Okay。 So you just want to get you comfortable with this usage of
    the， term bias。 You can say you bias towards a simple model。 Yeah。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你只是偏向了不同的东西。当你有一个大的假设空间时，这就是一种偏差的层次；如果你让它变得更小，那可能是更偏的。好的。你只需要习惯这种偏差术语的用法。你可以说你偏向简单模型。是的。
- en: You can bias also in some sense is pulling you away from the best。 possible
    fit of the training data。 Right？ That's kind of the idea of preventing overfitting。
    You could perhaps fit the data perfectly if an unbiased。 estimator but you're
    waiting overfitting so you pull that， estimate away。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在某种程度上偏离最佳的训练数据拟合。对吧？这就是防止过拟合的基本思想。你可能能够完美拟合数据，如果使用无偏估计器，但这会导致过拟合，所以你需要拉回估计值。
- en: You bias it away towards something that you think are， pure is more likely to
    be good。 Like something with small weights or small L2， no， or small L1， or a
    tree that's not too deep。 These are different biases。 All right。 That's kind of
    clear。 All right。 So full unpruned decision trees where you build them all the
    way。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你把它偏离某个方向，朝着你认为可能更好的方向偏移。比如有小权重或小L2范数，或者小L1范数，或者一棵不太深的树。这些都是不同的偏差。好的。这个应该清楚了。好的。那么完全未修剪的决策树，你将它们完全构建出来。
- en: out until every leaf node has like one element in it。 That would people generally
    would say that's pretty unbiased。 What would the bias left be in the prediction
    functions that come。 out of decision trees that are built as far as deeply as
    you， can build them？ Yeah。 Well。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 直到每个叶节点只有一个元素为止。那一般人会说那几乎是无偏的。那么如果你构建得尽可能深，最终出来的决策树的预测函数会有什么偏差呢？对吧？嗯。
- en: what an advanced is like memorize the data。 So it would be like not biased。
    Trees certainly memorize the data in terms of the prediction， function that results。
    It's going to go through every data data point unless there's， overlap。 Like two
    exactly the same inputs have different outputs， but let's， get rid of that case。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 高级算法就像是记忆数据。所以这将意味着没有偏差。树确实在预测函数的结果上记忆数据。除非有重叠，否则它会遍历每个数据点。例如，如果两个完全相同的输入有不同的输出，但让我们忽略这种情况。
- en: So if every input has different output then it should hit， everything。 Yeah。
    It's not from a continuous distribution。 So those are going to be like jagged。
    Great。 Great。 So the bias that's left in decision tree is that the final。 prediction
    function is piecewise constant。 In every leaf node it's predicting a constant
    number and that's it。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果每个输入都有不同的输出，那么它应该覆盖所有数据点。是的。它不是来自一个连续的分布。所以这些输出会像锯齿一样不平滑。很好。决策树中剩下的偏差是，最终的预测函数是分段常数的。在每个叶节点，它预测一个常数值，仅此而已。
- en: That's more or less the characterization of maybe exactly。 where the boundaries
    of these rectangles are based on the， algorithm。 That would have some amount of
    bias。 But that's the flavor of the bias。 We're biasing towards functions that
    are piecewise constant as。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这或多或少就是对这些矩形边界的表述，基于算法的设计。这会有一定的偏差。但这就是偏差的味道。我们偏向于选择那些分段常数的函数。
- en: opposed to maybe if we did a kernel method we could fit things， exactly but
    it'd be smooth。 It'd be sums of gouging kernel functions or something。 Okay。 So
    when prune and decision tree were introducing bias the bias is。 you can't have
    too much complexity。 You can only go to a certain depth。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们采用核方法，我们可能能够精确拟合数据，但它会更平滑。它会是一些高斯核函数的和之类的。好的。那么当剪枝决策树时，我们引入了偏差，偏差是：你不能有太多的复杂性。你只能到达一定的深度。
- en: You can only have a certain number of leaf nodes or something。 All right。 So
    now variance。 What do people mean when they're saying variance in this more， sense？
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能有有限数量的叶节点之类的东西。好了。那么方差。人们在更深层次上说方差时是什么意思？
- en: So it could describe how much the fit changes as you take different， random
    training sets。 That would be one way to characterize variance。 So the trees we
    were looking at those have a lot of variance in。 this sense because a different
    bootstrap sample can change the。 tree quite a bit whereas linear regression doesn't
    change much， when you do a bootstrap sample。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它可以描述在使用不同的随机训练集时，拟合的变化程度。这是表征方差的一种方式。所以我们观察的这些树在这方面有很大的方差，因为不同的自助采样可能会显著改变树，而线性回归在进行自助采样时变化不大。
- en: So bootstrapping linear regression。 So bagging linear regression doesn't really
    do too much。 Okay。 All right。 So conventional wisdom about when bagging helps。
    And I've gone to conventional wisdom here because I cannot find， any hard theorems
    on this subject。 So if anyone comes across a theory I'd love to hear about it。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 所以自助采样线性回归。集成学习（bagging）对线性回归并不会起太大作用。好的。好了。那么关于集成学习有帮助的传统观点。之所以提到传统观点，是因为我找不到任何关于这个主题的确凿定理。如果有人遇到相关的理论，我很乐意听听。
- en: But the general sentiment is that so bagging does nothing to， eliminate bias。
    That's fine。 The whole point of bagging is to reduce the variance。 So bagging
    helps most when we have relatively unbiased base， predictions。 So something that
    could treat you to some bias linear regression， would be very biased。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 但一般的观点是，集成学习对消除偏差没有作用。没关系，集成学习的重点是减少方差。所以当我们拥有相对无偏的基础预测时，集成学习帮助最大。所以能够带来一些偏差的线性回归，会有很大的偏差。
- en: And high variance when things are high variance they have a lot of。 variability
    that stands to be helped the most by bagging which， reduces the variability。 So
    is this true or not？ I don't know but I think what people are getting this from
    is。 that bagging works well for trees。 So trees have high variance and low bias。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 高方差的情况意味着事物具有很大的变异性，这种情况最能通过集成学习（bagging）得到改善，因为集成学习可以减少变异性。那么这是否正确呢？我不知道，但我认为人们之所以会这样理解，是因为集成学习对于树模型效果很好。所以树模型具有高方差和低偏差。
- en: So that would be at least one data point to support this general。 sentiment
    that you see printed a lot。 So try it out your mileage can vary。 Yeah。 So thinking
    that you want to choose B so that you're touching every。 data point like we can
    have bull tries and you have limited。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这至少是一个数据点，支持这一普遍观点，这个观点经常被提到。那么尝试一下，你的结果可能会有所不同。是的。所以考虑你想选择 B，这样你就能触及每个数据点，就像我们有多个尝试一样，而且你有一定的限制。
- en: amount of computation but is there other considerations like， choosing B？
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 计算量的问题，但是否还有其他考虑因素，比如选择B？
- en: It's not clear to me that it's important to choose B so large。 that you are
    very likely to hit every data point。 I think there are rules of thumb on B。 B
    is 100 is a common rule of thumb。 Some people say 50 and then others say 100 is
    better。 There。 That's the rule of thumb。 It's very empirically based。 So any questions
    on bagging？
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，选择B这么大似乎不太重要，导致你很可能覆盖每一个数据点。我认为关于B有一些经验法则。B为100是一个常见的经验法则。有些人说50，而另一些人认为100更好。就是这样，这是经验法则，完全是基于经验的。那么关于bagging有任何问题吗？
- en: Now we're going to talk about random forest which is actually a。 pretty minor
    variation but it's an important variation。 It has excellent performance and practice。
    So let's recall the motivating principle of bagging。 It came from pretending we
    had these IID samples。 These IID training sets from the original data distribution。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来谈谈随机森林，它实际上是一个相当小的变体，但它是一个重要的变体。它在实践中具有出色的表现。那么让我们回顾一下bagging的动机原则。它源于假设我们有这些IID样本。来自原始数据分布的这些IID训练集。
- en: And of course bootstraps are not independent。 Bootstraps samples are not independent。
    There may be some issue there。 So the -- it may limit the amount of reduction
    in variance。 that we get。 So for example suppose all the bootstraps samples were
    exactly， the same。 This won't happen in practice but imagine you just repeat the，
    exact same data set every time。
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，bootstrap样本不是独立的。bootstrap样本不是独立的，可能会有一些问题。所以，它可能限制了我们能够获得的方差减少量。例如，假设所有的bootstrap样本都是完全相同的。这在实践中不会发生，但假设你每次都重复使用完全相同的数据集。
- en: So all the F hats are exactly the same。 So averaging them does nothing。 So that's
    the limiting case of identical bootstraps data sets。 Bagging gives you nothing。
    Okay。 good。 So maybe there's something we could do to reduce the variance to。
    rather to reduce the correlation， the correlation， the。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所以所有的F帽子完全相同，平均它们没有任何效果。所以这就是完全相同的bootstrap数据集的极限情况。Bagging对你没有任何帮助。好的，明白了。那么，也许我们可以做些什么来减少方差，或者说减少相关性，相关性，那个。
- en: dependence between the different bootstraps samples。 So that's kind of what
    we're trying to do with random forest。 Let's first reverse this math result to
    make it concrete。 So we have suppose you have NID random variables。 We've looked
    at this several times。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 不同bootstrap样本之间的依赖性。所以这正是我们在做随机森林时试图解决的问题。让我们首先反转这个数学结果，使其变得具体。所以我们假设你有N个独立同分布的随机变量。我们已经多次看过这个了。
- en: The variance goes down like one over N。 Variance of the average。 What happens
    if we say the ZIs are correlated pair less？ Okay。 So you can actually have a similar
    result。 So what if the Zs are correlated？
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 方差以1/N的比例下降。平均值的方差。如果我们说Z<sub>i</sub>之间是成对相关的，会发生什么呢？好吧，你实际上可以得到类似的结果。那么，如果Z<sub>i</sub>是相关的呢？
- en: So suppose for all pairs of ZIs and Zj， their correlation is row。 Then the variance
    of the average is this expression。 Suppose row is one。 They're perfectly correlated。
    What happens on the right？ Row is one。 This second thing goes to zero。 So the
    variance is sigma squared。 That's back where it were when you only had one example。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有的Z<sub>i</sub>和Z<sub>j</sub>的相关性为row。那么平均值的方差是这个表达式。假设row为1，它们是完全相关的。右边会发生什么呢？row为1时，第二项变为零。所以方差是sigma平方。这就回到了当你只有一个例子时的情况。
- en: What's the other extreme？ No correlation。 Row is zero。 Okay。 This thing drops
    and then we get one over N sigma squared， which。 is what we're hoping for for
    independence。 That's what we got from independence。 So as the correlation changes，
    you get this sliding between -- this。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种极端情况是什么？没有相关性。row为零。好的，这个项就会消失，然后我们得到1/N * sigma平方，这正是我们期望的独立性结果。这就是我们从独立性中得到的。所以随着相关性的变化，你会看到这种滑动变化——这一点。
- en: trade-off between sigma squared and sigma squared over N。 So this is our math
    motivation to try to de-correlate to try to get。 each of the FIs to be as uncorrelated
    as possible。 All right。 Okay。 So here's to the random varistice。 Random varist
    is you take this bagged decision trees。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是sigma平方和sigma平方除以N之间的权衡。所以这是我们数学上的动机，试图去去相关化，尽量让每个FI尽可能不相关。好的，明白了。那么这是随机变量的情况。随机变量就是你采取这些袋装决策树。
- en: You're using decision trees and you're doing your bootstrap， replicates and
    you're bagging them。 So combining them in some way。 But you're going to modify
    the way we grow the trees。 All right。 So we're actually going inside this tree
    algorithm and we're， changing it a little bit。 So the key step in random varist
    is that when we're building。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你在使用决策树，进行自助法（bootstrap）复制，并且你在对它们进行集成（bagging）。所以以某种方式将它们结合起来。但你要修改我们构建树的方式。好吧。我们实际上是进入这个树的算法，并且对它稍作修改。所以随机森林的关键步骤是，在构建时。
- en: the tree and we are at a node and we need to choose a split， variable in the
    split point。 the choice of splitting variable is， restricted by first choosing
    a random subset of variables。 random subset of features of a certain fixed size，
    but not the， whole set。 So maybe a typical amount would be if you have D features
    at each。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 树，我们处在一个节点，需要选择一个分裂点的变量。分裂变量的选择，首先受到选择一个随机子集变量的限制。这个随机特征子集大小固定，但不是整个集合。所以，典型的做法是，如果你有D个特征，每次。
- en: node we choose a random subset of square root D features。 So if there's like
    100 features。 then at any given node we pick ten， of them randomly and then we
    choose the best variable from。 those ten。 And then when we go to left we do another
    different random subset。 of size ten from the hundred features and we split on
    one of， those。 And the idea。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 节点我们选择一个随机子集的平方根D特征。如果有100个特征，那么在任何给定节点上，我们随机挑选其中的10个，然后从这10个中选择最好的变量。然后当我们走向左边时，我们从这100个特征中再做一个不同的随机子集，大小为10，然后我们在其中一个上进行分裂。这个想法是。
- en: everything else is unchanged。 So the idea is that now we're introducing a fair
    bit of， you， know。 diversity in our trees because it's， you know， first of， all
    the， even the very first root node。 which is very important， in the eventual tree
    structure， it's pretty unlikely to overlap。 with the square root of D feature
    sample to choose from。 All right。 So that is。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其他部分保持不变。所以这个想法是，现在我们在树中引入了相当多的多样性，因为，首先，即使是非常重要的第一个根节点，它在最终的树结构中，几乎不可能与要选择的平方根D特征样本重叠。好的。那就是。
- en: that's pretty much all there is to say about the， random forest algorithm。 We
    can talk about it a bit。 But that defines it。 So square root of P。 if P is number
    of features is typical， but it's worth trying other things like P over two。 You
    could be more aggressive， be less aggressive on this。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎就是关于随机森林算法的所有内容了。我们可以再多谈一点。但这就定义了它。如果P是特征数量，平方根P是典型的，但值得尝试其他方法，比如P除以2。你可以更激进，也可以更保守。
- en: And then you could choose what fraction you want to use based， on cross-vortition
    as usual。 something like that。 Yeah。 >> You would struggle some first。 >> So again。
    >> You would struggle some first。 What are you only changing？ >> Yes。 You would
    struggle sample as well。 So let's act it out from the top。 Here's the full data
    set。 Okay。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以根据交叉验证选择你想使用的特征比例。大致是这样。嗯。>> 你一开始可能会有些困难。>> 所以再来一次。>> 一开始你可能会有些困难。你只改变了什么？>>
    是的。你也会进行随机抽样。所以让我们从头开始演示。这里是完整的数据集。好的。
- en: So we're going to call our first bootstrap sample。 Now we build a tree。 Choose
    square root of P features to decide on the root node。 And then， and so on。 And
    then we build this whole tree。 Another thing I didn't mention is in random forest
    you typically。 build the tree is very deep。 So until every node has maybe five
    data points in it or something。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将称之为第一次自助法样本。现在我们构建一棵树。选择平方根P的特征来决定根节点。然后，依此类推。然后我们构建整棵树。还有一件我没有提到的事情是，在随机森林中，通常。构建的树是非常深的。直到每个节点可能只有五个数据点之类的。
- en: a deep tree。 What happens with a deep tree in terms of bias and variance？ Very
    low bias。 That's right。 Yeah。 We're getting even more variability than usual in
    our trees because。 we're doing this random feature selection。 So we've just bumped
    up the variance a lot。 And by building deep trees， we're doing low bias。 Okay。
    So we build this whole tree out。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一棵深树。深树在偏差和方差方面会发生什么变化？偏差非常低。没错。是的。由于我们进行了随机特征选择，我们的树比通常情况下的波动性更大。所以我们大大提高了方差。通过构建深树，我们保持了低偏差。好的。我们就这样构建了整棵树。
- en: A deep tree。 Done with that tree。 Now we take a new bootstrap sample and repeat
    the process。 Yeah。 And then we --， >> And then you bump。 >> And then you get a
    whole bunch of trees。 A thousand would not be unusual trees。 And then you combine
    them in some way。 Yeah。 >> You had said here it says to restrict the set of variables
    when， you're choosing a node。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一棵深度树。完成了那棵树。现在我们获取一个新的自助抽样样本，并重复这个过程。是的。然后我们——， >> 然后你再碰一下。 >> 然后你就会得到一大堆树。千棵树也不算什么。然后你以某种方式将它们组合起来。是的。
    >> 你之前说过，在选择节点时，要限制变量的集合。
- en: Does it make a difference in practice if you were to just restrict。 the set
    of variables for each bootstrap sample？ >> So the question --， >> All of those
    on the tree。 >> Yeah。 So can we use a variant of this algorithm where we --。 we
    fix at the beginning of each tree a subset， say of a certain size 10。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仅仅限制每个自助抽样样本的变量集，实践中会有什么不同吗？ >> 所以问题是——， >> 所有的都在树上。 >> 是的。那么我们能否使用这种算法的一个变种，其中我们——。在每棵树开始时固定一个子集，比如说大小为10的子集？
- en: And then we have to build a whole tree using just those 10， features and not
    the 400。 So that's a new variant。 So， okay。 You invent to the new algorithm。 So
    the question is does it work？
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们必须只使用这10个特征而不是400个特征来构建一整棵树。这是一个新的变种。那么，好的。你发明了一个新的算法。那么问题是它是否有效？
- en: Does it have intuition？ Do we have intuition that it would be better at worst？
    I'm not sure。 You could try it。 What are the implications？ Any given tree will
    not be very powerful。 Any given tree only has access to 10 features in this case
    when， there's really 100。 So you're limiting -- so in terms of bias and variance，
    what happens with this method？
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 它有直觉吗？我们是否有直觉认为它至少在最坏的情况下会更好？我不确定。你可以试试。那有什么含义吗？任何一棵树都不会很强大。在这种情况下，每棵树只能访问10个特征，而实际上应该有100个。所以你是在限制——那么就偏差和方差而言，这种方法会发生什么？
- en: '>> Increase the bias。 >> Increase the bias because we''re saying whatever function
    you。 come up with has to depend on only 10 features。 So in some sense that''s
    biasing towards functions that have。 limited dependence on the feature set。 So
    I wouldn''t put my money on it。 but it''s -- I''m not saying it''s --， >> It could
    be worth a try。 >> Yeah。'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 增加偏差。 >> 增加偏差，因为我们说你提出的任何函数必须仅依赖于10个特征。所以从某种意义上讲，这就是对依赖于特征集有限的函数的一种偏向。所以我不会把所有的钱都押在这个上，但——我并不是说——，
    >> 这可能值得一试。 >> 是的。'
- en: '>> How exactly do you combine the different trees？ >> Well。 trees are -- the
    trees are talking about are classifiers。 It''s not a matter that the trees are
    not。 It''s a matter of what they produce。 Because we combine what they produce
    as functions as prediction。 functions and not their internal structure。 So how
    would you combine classes？ >> A chart。'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 你是如何将不同的树组合在一起的？ >> 好吧。这里所说的树是——这些树是分类器。问题不在于树本身，而在于它们所产生的结果。因为我们组合的是它们作为函数所产生的预测结果，而不是它们的内部结构。那么你会如何组合这些类呢？
    >> 一张图表。'
- en: '>> Majority。 Yeah。 How would you combine probabilities？ >> Average。 >> Yeah。
    What about SVMs？'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 多数。 >> 是的。你如何组合概率？ >> 平均。 >> 是的。那支持向量机（SVM）呢？'
- en: What do SVMs produce？ There's a hard decision， but there's also a score， a number。
    Yeah。 Okay。 So that is the question。 You guys can think about that。 Yeah。 >> Okay。
    So --， >> [inaudible]。 >> Okay。 It's hard to know。 And this is my question。 >>
    [inaudible]， >> Okay。 So --， >> [inaudible]。 >> Okay。 It's hard to know。 And this
    is mostly intuitive。 It's --。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）产生什么？有一个硬决策，但也有一个得分，一个数字。是的。好吧。这就是问题所在。你们可以思考一下。是的。 >> 好的。 >> [听不清]。
    >> 好的。这很难知道。这就是我的问题。 >> [听不清]， >> 好的。 >> 所以——， >> [听不清]。 >> 好的。这很难知道。这大多数是直觉。它——。
- en: It's just asking because of all the argument we've done about trees。 being unstable
    and being variable， are in fact -- is there。 really any issue with correlation
    for trees？ Let's just say that it works well for trees。 So in some sense that
    it can't be such a big issue。 Yeah。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 只是因为我们之前讨论过树的不稳定性和变量性，事实上——树的相关性真的是个问题吗？我们就假设它在树上能很好工作。所以从某种意义上讲，这不应该是个大问题。是的。
- en: I mean this -- there's no theorem here anywhere。 So in some sense this is all
    intuition and try and see how it。 works。 And the reason you're hearing about this
    today is because this， works。 Yeah。 >> [inaudible]。 >> Is there any situation
    where you would just do bag trees instead， of random forest？
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是——这里没有任何定理。所以从某种意义上讲，这一切都是直觉，你可以试试看它是怎么工作的。而你今天听到这些的原因是因为它——有效。是的。 >> [听不清]。
    >> 有没有什么情况，你会选择使用bagging树而不是随机森林？
- en: Is there any situation where you would just do bag trees instead， of random
    forest？
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有什么情况，你会选择做bag树，而不是随机森林？
- en: I am in practice I would say no。 This is not a theorem， but this is a practical
    empirical。 observation that you should use random forest。 Yeah。 Yeah。 Maybe you
    don't -- yeah， okay。 So there may be practical reasons to do bag trees。 But as
    far as performance， I'm not sure。 Yeah。 >> Is there any other than just observation
    reason why， square root of p is the good choice？
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我会说不是。这不是定理，而是一个实践中的经验观察，你应该使用随机森林。是的，没错。也许你不...好的。所以可能有实际原因使用bag树。但就性能而言，我不确定。是的。>>
    除了观察外，是否有其他原因说明为什么p的平方根是一个好的选择？
- en: Is there any reason behind that？ >> I don't have an intuition。 I would say that
    it's -- now to me it's a category of a lot less， than a number of features。 So
    as opposed to like p over 2， which would be another category。 And it seems that
    square root of p works better。 I don't -- I'm not saying there isn't a better
    intuition。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有什么原因在背后呢？ >> 我没有直觉。我会说现在对我来说，它是一种比特征数量少得多的类别。所以不像p除以2那样，另一个类别。看起来p的平方根效果更好。我不是说没有更好的直觉。
- en: I don't have it。 It's a good question。 All right。 So here's some examples。 So
    empirical results。 right？ So we're looking at three different sub-sample rates。
    We have p。 So with m equals p。 is this just bag trees？ If our sample size is the
    same， if the number of pieces we're。 considering at every node is the full set，
    is this just bag， trees？ Yes。 Okay。 It is。 So p。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有这个答案。这个问题很好。好了，这里有一些例子。看这些经验结果，对吧？我们看三个不同的子样本率。我们有p。所以当m等于p时，这只是bag树吗？如果我们的样本量相同，如果每个节点考虑的特征集是全集，这只是bag树吗？是的。好的，是的。
- en: p over 2， and square root of p。 And then the performance curves based on the
    number of trees。 that we're averaging。 First of all， let's just look at the general
    trend。 Just like we saw with bagging， there's a big decrease initially。 and then
    it kind of levels off in terms of the number of trees。 What else？
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: p除以2，以及p的平方根。然后是基于树木数量的性能曲线。我们正在进行平均。首先，让我们看看整体趋势。就像我们在bagging中看到的那样，最初有一个大幅度的下降。然后随着树木数量的增加，曲线趋于平稳。还有什么？
- en: Square root of p wins in this picture。 See？ Proof。 [laughter]， Okay。 All right。
    That's all I had to say about random fast。 Yeah， the movie。 But it's 500 megabytes
    and it never finished downloading。 Sorry。 [inaudible]。 That's a great question。
    So are we saying that we're actually combining a lot of trees。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，p的平方根表现更好。看到了吗？证据。[笑声]，好吧。好了，这就是我关于随机快速的所有内容。是的，那个电影。但它有500兆，我下载完都没下完。抱歉。[听不清]。这是个好问题。我们是不是在说，我们实际上是在组合很多树？
- en: that can't potentially be overfitting and we're still getting， the results？
    Great observation。 So we're looking at number of trees。 When it goes really small。
    it goes over half for at least some of these。 So first of all。 does this make
    any sense based on how I described random， fast？ [inaudible]。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这有可能是过拟合吗，我们仍然能得到结果吗？很好的观察。那么我们看看树木的数量。当它真的很小的时候，它会超过一半，至少对于某些情况。首先，基于我对随机快速的描述，这有意义吗？[听不清]。
- en: So you'll see that all the way on the left， we have errors that are over 50%。
    which for binary classification seems crazy。 You should never have over 50% error。
    And if you do。 you'll overfit your training。 This is test error。 So if you're
    getting more than 50%。 it sounds like you really overfit your， data。 So there's
    one piece that is not in the official spec of random。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会看到，在最左边，我们的错误率超过50%。对于二分类问题来看，这似乎很疯狂。你不应该有超过50%的错误。如果有的话，你的训练数据就过拟合了。这是测试误差。所以如果你得到超过50%的错误率，那听起来像是你真的过拟合了数据。这里有一部分不在随机森林的官方规范中。
- en: fast， but is how it's implemented generally， which is that you build， very deep
    trees。 which tend to overfit certainly。 So with very few trees。 you're going to
    be averaging things that are， overfit and you're still going to be overfitting。
    But yes， when you average enough of them， these trees that。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 快速，但它通常是如何实现的，也就是构建非常深的树，这种树往往会过拟合。确实如此。所以如果树的数量非常少，你将对过拟合的结果进行平均，而你仍然会过拟合。但是是的，当你足够多地平均这些树时，
- en: individually may be overfitting the data， the combination does quite well。 Yes。
    So the next thing we're going to look at is boosting， where we're combining。 trees
    that are the opposite。 Very simple。 Very small， very few number of leaf nodes。
    But in random fast， we're combining very deep trees， which may， well overfit。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，可能会出现过拟合数据，但组合起来效果却相当好。是的。那么接下来我们要看的是提升方法（boosting），在这里我们是将树进行组合。非常简单，非常小，叶节点数非常少。但在随机森林中，我们组合的是非常深的树，这些树可能会过拟合。
- en: I guess I would probably do the same for bagging of trees。 Yeah？ [INAUDIBLE]。
    A lot of people recommend 100 for bagging。 So random forest may have more fun
    going on as you do。 I usually do more than 100 trees for random forest。 But actually，
    it's not far off here， is it？
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我可能会对树的袋装（bagging）做同样的处理。是吗？[听不清]。很多人建议袋装使用100棵树。所以随机森林在实施过程中可能会更有趣。我通常会用超过100棵树来训练随机森林。但是实际上，这里并没有差别，是吧？
- en: Is that what you're pointing out that it agrees with 100？ Another proof。 Yeah？
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你是在指出它与100一致吗？另一个证明。是吗？
- en: Is the net consistent across the trees？ There's different ways to terminate
    your tree building in random forest。 So sometimes you can do it with every leaf
    node has a certain。 number of elements at most different ways to do it。 You could
    do a certain depth。 But you may not have the same level of depth。 Yeah， you should。
    I mean。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 树之间的网络是否一致？在随机森林中，有不同的方式来终止树的构建。所以有时你可以通过确保每个叶节点最多包含一定数量的元素来终止，也有其他不同的方式。你也可以通过限制树的深度来终止。但你可能不会有相同的深度。是的，你应该这么做。我的意思是。
- en: you'll have the same termination condition for all trees。 Yeah。 which leads
    us to another great point out random forest is that， I don't know if you guys--，
    well。 for the computer scientists here， you should see one thing。 that's great
    about random forests from an implementation， perspective。 Yeah？ [INAUDIBLE]， All
    right。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你将为所有树设置相同的终止条件。是的。这也引出了一个关于随机森林的好问题，我不知道你们——嗯，对于计算机科学家来说，你们应该看到一件事。这是实现方面随机森林的一个优点。是吗？[听不清]，好的。
- en: so there might be some speed up because you're looking， at fewer features each
    time。 OK， maybe。 but that's a very detailed level and there's actually a little
    bit， higher level。 a little bit higher level thing that can go on here。 [INAUDIBLE]，
    Yeah。 embarrassingly parallel in the sense that you can build all 500 trees。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所以可能会有一些加速，因为你每次看的特征更少了。好吧，也许是这样。但那是在非常细节的层面，实际上这里还有一个更高层次的东西可以进行操作。[听不清]，是的。从某种意义上来说，这是一个尴尬的并行化，因为你可以构建所有500棵树。
- en: at the same time if you have 500 processors。 There's no dependence between the
    trees。 So it's very parallelizable。 All right。 So that's a-- yeah？ [INAUDIBLE]，
    Can I？ [INAUDIBLE]， OK。 great question。 How can we regularize the random forest？
    [INAUDIBLE]， So again？ [INAUDIBLE]。 [INAUDIBLE]， [INAUDIBLE]， So one way would
    be to regularize your individual trees by trimming them or。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果你有500个处理器，树之间是相互独立的。所以这是非常适合并行化的。好吧，那么这是一个——是吗？[听不清]，可以吗？[听不清]，好的。很棒的问题。我们如何对随机森林进行正则化？[听不清]，再说一遍？[听不清]，[听不清]，[听不清]，所以一种方式是通过修剪树来对每棵树进行正则化，或者。
- en: you know， other ways to constrain the complexity of a tree。 Sure， that would
    be a way。 [INAUDIBLE]。 Yeah？ [INAUDIBLE]， If you're choosing from end of one particular
    feature。 one is in a set of m is never chosen， and you start grabbing those features。
    [INAUDIBLE]。 So in the unlikely event that a feature is never randomly chosen，
    and then it kind of--。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗，其他约束树复杂度的方法。是的，这也算是一种方式。[听不清]。是吗？[听不清]，如果你从某个特定特征的末尾开始选择，某些特征集合中的特征永远不会被选择，然后你开始抓取这些特征。[听不清]。所以如果某个特征不太可能被随机选择，那么就有可能——。
- en: '[INAUDIBLE]， No， that''s never randomly chosen。 It''s never from the-- after
    being randomly chosen。 it''s never chosen。 Oh。 As to split on。 Yes。 OK， so what
    was the question？ Oh， no。 It was the same。 It would have the array of regular
    logs。 But you''re not controlling that。 That''s just the algorithm not doing it，
    right？ Well。'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，不，那些特征从来不会被随机选择。它不会从——被随机选择后再被选择。哦，作为划分的标准。是的，好的。那么问题是什么？哦，不。是同样的。它会有一系列的正则日志。但是你没有控制它。这只是算法没有做到，是吧？嗯。'
- en: speaking of if that's the way of not having features of a random， feature， it's
    a quick word。 Oh。 OK， sure， maybe saying-- OK。 You're saying as a byproduct of
    the way these things are built。 we may end up not including a feature in any tree。
    and that in some sense the feature is out of there。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 说到如果这是避免特征被随机选择的一种方式，那是一个快速的过程。哦，好的，没问题，可能就是说——好的。你是在说，这些东西的构建方式可能导致我们最终没有在任何树中包含某个特征，从某种意义上来说，这个特征就被排除在外。
- en: and that's-- feature lamination is kind of a regularization。 OK。 Usually the
    regularization is something imposed on top of the method， though。 Yeah， so--，
    Yeah。 right-- no。 I mean， yes， there are methods， but I think--。 what's another--
    the one parameter here is the m， right？ So the regularization for m is clear。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是——特征层叠是一种正则化方法。好的。通常，正则化是对方法上施加的某种限制。对，没错——不，我是说，是的，确实有一些方法，但我认为——。这里的一个参数是m，对吧？所以，m的正则化是明确的。
- en: The more-- the smaller m is， the more restricted in some sense you're--， well，
    there's a trade-off。 isn't there， between-- if you're willing， to wait long enough
    in terms of the number of trees。 no matter how small m is， you can always get
    any tree just as long as you're。 lucky at every single split net。 So there's some--
    there's some trade-off between the number of trees you have and m。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: m越小，从某种意义上说你就越受限制——嗯，之间是有权衡的，不是吗？如果你愿意等得足够长，无论m有多小，只要你足够幸运，每一个分裂点都能得到一棵树。所以，树的数量和m之间有某种权衡。
- en: All right， I think we-- so you can still ask questions on random forest。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我认为我们——所以你们仍然可以问关于随机森林的问题。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_18.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_18.png)'
- en: '![](img/7e658e45466cec419a4c446a21134cc6_19.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_19.png)'
- en: but I think we're-- yeah， we're good。 Question？ Random forest？ Yeah。 Stretch。
    No？ Sure。 OK。 Let's start with boosting， which is pretty neat， and then we'll
    finish it up next time。 [ Inaudible Remark ]， Yeah。 [ Inaudible Remark ]， Can
    you say it again？ [ Inaudible Remark ]， No。 There is some theory for random forest。
    [ Inaudible Remark ]， Yeah。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 但我觉得我们——是的，我们没问题。问题？随机森林？是的。伸展一下。没有？好的。我们从提升方法开始，这个方法很有意思，然后下次我们再继续。 [听不清的发言]，是的。[听不清的发言]，你能再说一遍吗？
    [听不清的发言]，不。对于随机森林是有一些理论的。[听不清的发言]，是的。
- en: the theory is based on kind of a nearest neighbor type stuff。 This is the extent
    of my intuition on it。 There may be theory I'm boosting。 but it's not-- I'm rather
    unbagging， but it's not as-- it's not a flavor of like， oh。 bigger variance bias。
    That sort of stuff。 OK。 It's a good question。
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理论基于某种近邻算法。这是我对它的直觉部分。可能还有提升的理论，但它并不——我更倾向于bagging，但它不像——不是那种偏差方差的风味。那类东西。好的，这是个好问题。
- en: Why do we have deep trees for random forest？ [ Inaudible Remark ]， OK， small
    bias。 So the intuition。 should you choose to believe it， is that， bagging， which
    random forest is an instance of。 works best when you have， individual prediction
    functions that have low bias。 which is you should think like， you know， fits the
    data really well and potentially high variance。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在随机森林中使用深度树？[听不清的发言]，好的，偏差小。那么直觉上，如果你愿意相信的话，就是，bagging（随机森林就是一种bagging方法）在你有低偏差的单独预测函数时表现最好。你应该理解为，模型非常适合数据，且可能具有较高的方差。
- en: It's a lot of everybody。 So the deep trees is the low bias。 Fitting the data
    really well is the low bias。 Deep trees， yeah。 [ Inaudible Remark ]， Yeah。 [ Inaudible
    Remark ]， Especially because of the sub-sampling。 Yes， especially。 [ Inaudible
    Remark ]。 Any more burning questions？ OK。 All right。 So let's get an intro to
    boosting。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大家的集体努力。所以，深度树代表低偏差。非常适合数据就是低偏差。深度树，是的。[听不清的发言]，是的。[听不清的发言]，尤其是因为子采样的原因。是的，尤其是。[听不清的发言]。还有更多紧迫的问题吗？好的。好，接下来我们介绍提升方法。
- en: which is a distinct technique and pretty， interesting。 So it comes into-- so
    I mentioned at the beginning we're talking about ensemble， methods today。 And
    there's two broad classes。 There's parallel and there's sequential ensemble。 Now。
    just the parallel ensemble methods， that's what we're talking about so far， today。
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个独特的技术，非常有趣。所以它涉及到——我在一开始提到过，今天我们讨论的是集成方法。它有两个广泛的类别。一个是并行集成，另一个是顺序集成。现在，仅仅是并行集成方法，这是我们今天讨论的内容。
- en: It's where each model is built independently。 So bagging。 we built B models
    and we combine them somehow。 Random forest， same way。 All right。 The other one
    is sequential。 So in the sequential ensemble， the models are built sequentially。
    We build the first model and then we see how it does。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个模型独立构建的地方。所以bagging，我们构建B个模型，然后以某种方式将它们组合起来。随机森林也是这样做的。好了，另一个是顺序式的。所以在顺序集成方法中，模型是顺序构建的。我们先构建第一个模型，然后看它的表现如何。
- en: And then we try to build a new model that prepares the failures of the first，
    model。 All right。 So we build a model and then we see， well， this model didn't
    do so well over， here and over here。 Let's build a new model that tries to do
    well in exactly in those places。 And then we keep building more models to kind
    of tune things closer to the， ideal。
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们尝试构建一个新的模型，来弥补第一个模型的失败。好的。所以我们构建一个模型，然后我们看到，这个模型在这里和这里表现得不太好。让我们构建一个新模型，试图在这些地方做得更好。接着我们不断地构建更多的模型，以便将结果更接近理想状态。
- en: And that's what boosting is an example of。 So boosting。 there's this classic
    question in statistical learning theory and machine， learning。 It's called the
    boosting question。 And the question was this。 Suppose you have something that's
    called a weak learner， which is a classifier。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是提升的一个例子。所以提升有一个经典问题，出现在统计学习理论和机器学习领域。这个问题叫做提升问题。问题是这样的：假设你有一个叫做弱学习器的东西，它是一个分类器。
- en: on classification case here， that does slightly better than random guessing。
    So if it's binary。 we have a classifier that can beat 50% correct。 It's a pretty
    light requirement。 So there's kind of like rules of thumb。 So you could say， all
    right。 You know。 an email from a friend is probably not spam。 A linear decision
    boundary。
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里是分类问题，它的表现略好于随机猜测。所以如果是二分类问题，我们有一个分类器，它能超过50%的正确率。这个要求相当轻松。所以这就像是经验法则。你可以说，好吧，一个来自朋友的电子邮件可能不是垃圾邮件。一个线性决策边界。
- en: It's a very simple prediction function。 It may not be very accurate， but it's
    a rule of thumb。 It may beat 50% guessing。 So these are examples of weak learners。
    And the question is。 can we combine a set of weak learners to form a， single classifier
    that makes accurate predictions？
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个非常简单的预测函数。它可能不太准确，但它是一个经验法则。它可能超过50%的随机猜测。所以这些就是弱学习器的例子。问题是，我们能否将一组弱学习器结合起来，形成一个能做出准确预测的单一分类器？
- en: All right。 And this is kind of， it's a formal statement。 It's precise。 Even
    a set of classifiers that perform better than 50% accurate。 can you combine them
    to have arbitrarily small error rate？
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这是一个正式的声明，精确的。即使是一组表现超过50%准确率的分类器，你能否将它们组合起来，得到一个误差率极小的分类器？
- en: So it's posed by Michael Curran's and Leslie Valiant in the late '80s。 And the
    problem was solved by Raupe's period in 1990。 And the algorithm that solves it。
    it's called boosting。 The first solution wasn't quite so practical。 But a follow-on
    algorithm。 just a year or two later， is still used today and works great。 It's
    called Adebust。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题由 Michael Curran 和 Leslie Valiant 在80年代末提出。问题在1990年由 Raupe 的研究解决。解决这个问题的算法叫做提升（boosting）。第一个解决方案并不是特别实用。但一年或两年后，后续的算法仍然被今天使用，并且效果很好。它叫做
    Adebust。
- en: That's a question。 Okay。 So at Adebust setting， we're in the classification
    setting。 negative one and one。 And we suppose we have a weak learner， so now--
    whoops。 Sorry。 Suppose we have an algorithm for taking some data and producing
    a classifier that's。 better than 50% performance。 So in practice， what we're typically
    using are things like decision stumps。
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题。好的。在 Adebust 设置下，我们处于分类场景中。类别为负一和一。我们假设有一个弱学习器，现在——哎呀，对不起。假设我们有一个算法，可以从一些数据中生成一个分类器，且性能优于
    50%。所以在实际应用中，我们通常使用的是像决策树桩这样的东西。
- en: A decision stump is a tree with only a root node。 All right。 So it's like a
    split of the data on one feature。 Or maybe a shallow tree would be another typical
    weak learner。 a linear decision boundary， another typical。 So it's a typical weak
    learner。 And so Adebust is based on an notion of re-weighting training examples。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树桩是一棵只有根节点的树。好的。所以它就像是基于一个特征对数据进行划分。或者也许一个浅层树是另一个典型的弱学习器。一个线性决策边界，另一个典型的弱学习器。所以它是一个典型的弱学习器。因此，Adebust
    基于重新加权训练样本的概念。
- en: So remember I said you take your first classifier， you see where it did poorly。
    and then you retrain on where the first one did badly？ So that's where this comes
    in。 Wherever the first prediction failed， it got incorrect results。 we up-weight
    those training examples to make it extra more important that in the second round
    the classifier tried to get that correctly。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 所以记住我说过，你先拿到第一个分类器，看看它做得不好的地方，然后重新训练这些地方？这就是这里的关键所在。无论第一个预测失败的地方，它得到了错误的结果，我们会加大这些训练样本的权重，让它们变得更加重要，以便在第二轮中，分类器尝试正确预测这些样本。
- en: And so on。 So by re-weighting the examples， this is how we sequentially adapt
    to how our prediction function is doing。 So to actually get there， let's define
    what we mean by training with weights on the examples。 So here's a training set。
    We have W1 through WN giving weights to the examples。 Let's say they're not negative。
    So we have the weighted empirical risk。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推。所以通过重新加权这些例子，这是我们如何顺序地调整我们的预测函数表现的方式。为了真正达到这一点，让我们定义一下通过在例子上加权来训练的意思。这里有一个训练集。我们有
    W1 到 WN 给例子加权。假设它们不是负数。这样我们就有了加权的经验风险。
- en: So we have the loss on each example， but we weight it by WI。 We read normalized
    by one over by the sum of the weights。 And what we need to have is a classification
    method that can handle weighted examples that can minimize this empirical risk。
    And most methods we've discussed can do that。 But if you can't， it's okay。
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在每个例子上都有损失，但我们通过 WI 对其加权。我们通过权重的和来进行归一化。我们需要的是一种能够处理加权例子、最小化经验风险的分类方法。我们讨论的大多数方法都能做到这一点。如果不能，也没关系。
- en: Can you think of any trick we could use to suppose I have a black box learning
    algorithm？
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到我们可以用什么技巧吗？假设我有一个黑盒学习算法？
- en: It just takes data sets。 It does not take weighted examples。 I can only feed
    it in a data set D。 Can you think of any way to kind of rig the system to have
    it essentially fit a weighted training example？
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是接受数据集，不接受加权的例子。我只能给它输入一个数据集 D。你能想到什么方法来让它基本上拟合一个加权的训练例子吗？
- en: Say again？ Yes， replicate the records。 That's absolutely the right track。 So
    suppose a particular example has weight two and all the rest have weight one。
    Repeat that example twice。 That's the intuition。 Great。 So that works just fine
    if your weights are integer。 What if they're like fractional？
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 再说一遍？是的，复制记录。这绝对是正确的思路。假设一个特定的例子的权重是 2，其他所有例子的权重是 1。就把这个例子重复两次。这就是直觉。很好。如果你的权重是整数，这就完全没问题。如果是分数呢？
- en: Still there's a way to do it。 Okay。 Some ideas？ Linear combination， right？ Yeah？
    Yeah。 Sample each example with probability proportional to its weight。 Great。
    So we take all the W's。 renormalize them so they sum to one。 Now we have a probability
    distribution on examples。 Draw up a sample according to that probability distribution
    of some size。 Two and or whatever。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一种方法可以做到这一点。好吧，有什么想法吗？线性组合，对吧？对吧？对。按权重的比例对每个例子进行抽样。很好。所以我们拿所有的 W，然后重新归一化，使它们的总和为
    1。现在我们有了一个例子的概率分布。根据这个概率分布抽取一个样本，大小可以是 2 或其他。
- en: Anyway， this is a way to kind of back into the weighted， minimizing the weighted
    training error。 Great。 Okay。 All right。 So I'll give you a rough sketch of out
    of boost and then maybe call it a day。 All right。 So we have a training set of
    n examples。 We'll start with all the weights equal to one。 And we repeat in this
    way。 We have a weak classifier called G sub at the mth step。
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，这是一种反向处理加权、最小化加权训练误差的方式。很好。好的。那么，我会给你大致描述一下提升方法（boosting），然后也许就到此为止。好了，我们有一个包含
    n 个例子的训练集。我们从所有权重都等于 1 开始。然后按照这种方式重复进行。我们有一个弱分类器，叫做 G 在第 m 步的分类器。
- en: gm will be the class that we get。 We fit it to the weighted training points。
    Start off with uniform weights。 And then we're going to increment the weights
    on points that gm x misclassifies。 as I said。 And you repeat m times。 And then
    at the end you have m different classifiers。 One from each round。 So how are we
    going to end up with a single classifier？ You could average。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: gm 将是我们得到的类别。我们将其拟合到加权训练点。首先使用均匀权重。然后我们将在 gm x 错误分类的点上增加权重。如我所说。然后重复 m 次。最后你将得到
    m 个不同的分类器，每一轮一个。那么我们怎么得到一个单一的分类器呢？你可以进行平均。
- en: You could average。 Say again。 Yeah， combine them with some coefficients。 So
    the way we're going to combine them is not necessarily obvious。 It's part of the
    algorithm。 Okay。 So yes， we're going to take this alpha m linear combination of
    the gm。 Plus your gm's output。 They're not scores。 We're strictly dealing with
    classifiers that output negative one or one。 Okay。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进行加权平均。再说一遍。对，结合它们，使用一些系数。所以我们结合它们的方式不一定显而易见。这是算法的一部分。好的。所以是的，我们要使用这个 alpha
    m 对 gm 进行线性组合，加上你的 gm 输出。它们不是得分。我们严格处理输出 -1 或 1 的分类器。
- en: So we're taking linear combinations of negative ones and ones for each x。 And
    then we're taking the sign， the threshold， to get the prediction。 All right。 So
    what's missing is what are these weights？ So first of all， the alpha m's are not
    negative。 And here's a rough， rough， rough with speaking。 Alpha m's are larger
    when gm， so the mth classifier。
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们对于每个 x 都在进行负一和一的线性组合。然后我们取符号，设定阈值来得到预测结果。好了。那么缺失的部分是什么呢？这些权重是什么？首先，alpha
    m 的值不是负数。这里有一个粗略的，粗略的，粗略的说法。 当 gm，即第 m 个分类器的表现较好时，alpha m 的值会较大。
- en: fits its data， it's weighted data well in terms， of having small training errors。
    So this is when you're asking about can we combine them using weights somehow
    related to how well they do。 This is as close as we're going to get today， which
    is let's see how they do on the training data on the way to training data。 So
    it seems a little risky to use training data to figure out how important how good
    this classifier is because you can over fit training data。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 它很好地拟合了数据，并且在训练误差方面具有较小的加权数据。所以当你问是否可以通过某种方式根据它们的表现来结合它们时，这是我们今天能做到的最接近的做法，即看看它们在训练数据上的表现，在训练数据的过程中。所以，用训练数据来评估分类器的好坏似乎有些冒险，因为你可能会过拟合训练数据。
- en: But in boosting， we generally use very weak classifiers。 So overfitting in a
    single gm is not such a big issue。 Okay。 All right。 So let's stop there because
    it just gets more complicated from here and we only have a couple minutes left。
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在提升方法中，我们通常使用非常弱的分类器。所以单个分类器的过拟合问题并不是那么严重。好吧。好了，我们到这里停止，因为接下来会变得更复杂，而我们只剩下几分钟时间。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_21.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_21.png)'
- en: Any questions on the boosting stuff？ Yeah。 Sure。 Good。 Is it all to me？
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 关于提升方法的内容，有什么问题吗？是的，当然。很好。是我一个人讲吗？
- en: Are we just meeting with the advisors？ Tomorrow is just meeting with the advisors。
    So if we have a professor else on your own。 You don't have to come。 If you have
    an advisor who's not here tomorrow， you have to make a separate appointment。 You
    do not have to come tomorrow。 Okay。 Thank you for the lecture。
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是和顾问开会吗？明天只是和顾问开会。所以如果有其他教授在你自己那边，你就不必来了。如果明天你的顾问不在，你需要另行预约。你不必明天来。好的，谢谢您的讲座。
- en: '![](img/7e658e45466cec419a4c446a21134cc6_23.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e658e45466cec419a4c446a21134cc6_23.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[空白音频]。'
