- en: P24：24. L5_8 Softmax Classification in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P24：24. L5_8 Python中的Softmax分类 - Python小能 - BV1CB4y1U7P6
- en: So remember， last time fashion MNIST didn't quite download in the right way。
    Okay， this was fixed。 So let's say we executed here。 And the good thing is， for
    a lot of data sets， we already have。 Preconfigured data loaders。 This is also
    going to give you an idea of how those data loaders are。 supposed to look like
    in practice。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以记住，上次fashion MNIST没有正确下载。好吧，现在已经修复了。假设我们在这里执行了。好消息是，对于许多数据集，我们已经有了预配置的数据加载器。这也能让你了解这些数据加载器在实践中应该是什么样子的。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_1.png)'
- en: And it just looks like standard MNIST。 You know， 60，000 observations for training
    and 10。000 for testing。 And then you can look at the shape and it's 28 by 28 pixels。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来就像标准的MNIST数据集。你知道，训练集有60,000个观测值，测试集有10,000个。然后你可以查看它的形状，它是28x28像素。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_3.png)'
- en: And that's that。 So these are just some convenience routines。 They can be used
    to get the labels。 And we'll be using those routines later on over and over。 And
    so it's a good idea to introduce them once。 And all they do is just。 if you give
    it some label ID， it'll spit out the， corresponding text。 And then for plotting
    things。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样了。所以这些只是一些便捷的例程。它们可以用来获取标签。我们以后会一遍又一遍地使用这些例程。所以一次性介绍它们是个好主意。它们做的就是，如果你给它一些标签ID，它会输出对应的文本。然后用于绘图。
- en: it'll do the nice thing of actually， you， Know， plotting for the various images
    what they look like。 So for instance， if I want to get the first 10 images， these
    are the， ones。 And if I go from 10 to 19， you get some other pretty images about。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 它会做一件很好的事情，那就是实际绘制不同图像的样子。例如，如果我想获取前10张图像，这些就是图像。如果我从第10张到第19张，你会看到一些其他不错的图像。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_5.png)'
- en: T-shirts and bags and so on。 And this is a little bit harder to recognize in
    handwritten digits。 which is why we're going to use it for this class。 Because
    you can actually tell whether your algorithm， but your model is， better。 Because
    if you're just looking at about half a percent error rate。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: T恤、包等物品。而手写数字的识别稍微难一些，这也是我们将在本课程中使用它的原因。因为你实际上可以判断你的算法是否更好。如果你只看大约半个百分点的错误率。
- en: then it's no longer so interesting。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它就不再那么有趣了。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_7.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_7.png)'
- en: And so if I want to load this， it's a very simple data loader。 So all it does
    is it takes it into tensor representation。 So basically it moves it into in the
    array。 And I can do all sorts of pre-processing。 Here those data loaders don't
    actually do anything terribly， interesting。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我想加载这个数据，它是一个非常简单的数据加载器。它做的就是把数据转换成张量表示。所以基本上，它将数据转换为数组。然后我可以做各种预处理。在这里，这些数据加载器实际上没有做任何特别有趣的事情。
- en: It just maps into a tensor。 Later on， we'll look at pre-processing operators
    that actually do。 interesting things with images。 Once we do that， we will need
    GPUs。 Right now。 it's not so important。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是映射到一个张量。稍后我们将看到一些预处理操作符，它们实际上会对图像做一些有趣的事情。一旦我们做到这一点，我们将需要GPU。现在，这还不是特别重要。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_9.png)'
- en: And so if you wanted to run this， it takes about two and， half， well。 almost
    three seconds to iterate to the data once。 Okay。 So that's really straightforward。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你想运行这个，它大约需要两秒半，几乎三秒才能迭代一次数据。好吧，这真的很简单。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_11.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_11.png)'
- en: Now， let's look at how to do regression as a softmax regression。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何做回归，作为一个softmax回归。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_13.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_13.png)'
- en: All right。 Let's just go back。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们回到原点吧。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_15.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_15.png)'
- en: So any questions so far？ Yes？ Oh， because it's the soft version of Maxwell。
    Right？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止有什么问题吗？是吗？哦，因为它是Maxwell的软版本。对吧？
- en: So remember what we did is we mapped the outputs， which are just， in Rd。 Right。
    they are just some scalars。 We want to map them into some numbers that go between
    zero and。 one and that sum up to one。 Right？ So what we did is we therefore mapped
    O going into E to the OI。 divided by some over all the J's， E to the OJ。 Right。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以记住，我们做的是将输出映射，这些输出只是Rd空间中的一些标量。我们想要把它们映射到一些在0到1之间的数字，并且它们的总和为1。对吧？所以我们做的是将O映射到E的OI，然后除以所有J的E的OJ。对吧。
- en: And so this would now be the probability that I have class I， given this O。
    Okay。 Now。 if I take the log of this， negative logarithm of this。 Right。 Actually。
    if I take the logarithm of this， right？ So log of O is， well， actually negative
    log of O。 Okay。 Well， sorry。 Negative log of P of I given O is sum over J， log
    of sum over J， E to the OJ minus OI。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这现在将是给定 O 时，我属于类 I 的概率。好吧。现在，如果我取它的对数，取这个负对数。对吧。实际上，如果我取它的对数，对吧？所以 O 的对数是，嗯，实际上是
    O 的负对数。好吧，抱歉。负对数 P（I 给定 O）是对 J 求和，求 J 上的对数，E 的 OJ 减去 OI。
- en: Right？ Now， this is exactly the softmax operation。 Right。 This will compute
    something that's。 you know， larger than the， largest of those terms， of all of
    those terms， but only slightly， larger。 So let's work that through with two numbers。
    And without loss of generality。 I'm going to pick zero as one of， those two numbers。
    So I have log of E to the x plus E to the zero。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？现在，这正是 softmax 操作。对。这将计算出某个值，它比所有这些项中最大的那个值稍大一些。所以我们可以通过两个数字来演示这个过程。为了不失一般性，我选取零作为其中的一个数字。所以我有
    E 的 x 次方加 E 的零次方的对数。
- en: So that's nothing else in one。 Right？ And I can plot this function。 So， well。
    what happens for x equals zero？ I get log two。 And， okay， what happens for very
    small x's？
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这不就是一吗？对吧？然后我可以绘制这个函数。那么，嗯。对于 x 等于零的情况，结果是对数二。那么，非常小的 x 会怎么样呢？
- en: It goes to zero。 And as part of the homework， you're going to prove that it
    goes。 to zero like E to the minus x。 That's the rate at which it'll go to zero。
    The proof is fairly elementary。 So， lopitase rule will help you。 Now， for large
    x。 what's going to happen？ Exactly。 So this is going to grow linearly because
    then this term here。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它趋近于零。作为作业的一部分，你将证明它是如何趋近于零的，类似于 E 的负 x 次方。那就是它趋近零的速度。证明相当简单。所以，洛必达法则会帮助你。现在，对于大的
    x，接下来会发生什么？没错。所以这将线性增长，因为这个项。
- en: becomes dominant。 This is essentially then negligible。 So I get log of E to
    the x。 which is essentially x。 So it goes like x。 So this is， of course。 an upper
    bound on that function here， which is the max of zero and x。 So this is a softened
    version of the max， which is why it's called， the soft max。 Yeah？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 变得占主导地位。这基本上是可以忽略的。所以我得到的是 E 的 x 次方的对数，实际上就是 x。所以它的变化就像 x 一样。所以这是，当然是。这个函数的上界，就是零和
    x 的最大值。所以这是最大值的一个软化版本，这也是为什么它被称为 soft max。对吧？
- en: Sometimes the English language is very convenient for coming up。 with terms
    that make a lot of sense with the benefit of hindsight。 Yeah。 Good question， though。
    Any other questions？ Okay。 Good。 So then let's try soft max regression from scratch。
    So we go ahead and import some stuff。 We need to get our data。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，英语确实很方便，能够提出一些回顾来看非常有道理的术语。嗯。好的问题，不过。还有其他问题吗？好，没问题。那么让我们从零开始尝试 softmax 回归。所以我们先导入一些内容。我们需要获取我们的数据。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_17.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_17.png)'
- en: Okay。 And now， well， we need to initialize our parameters。 So remember， our
    data is， you know。 784 dimensional because it's， 28 by 28。 And we have 10 output
    categories。 So we need a 784 by 10 dimensional matrix。 And we need a 10 dimensional
    bias。 Okay。 So I'm going to allocate this， and this is going to be WMP， just as
    you would。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在，嗯，我们需要初始化我们的参数。记住，我们的数据是，嗯，784 维的，因为它是 28 乘 28。而且我们有 10 个输出类别。所以我们需要一个
    784 乘 10 的矩阵。还需要一个 10 维的偏置项。好吧。所以我要分配这个，这就是 WMP，就像你会做的那样。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_19.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_19.png)'
- en: And we also need to attach a gradient to this。 Okay。 The reason why I need to
    attach a gradient to this is because， Glue on needs to know， hey。 I might want
    to compute the gradient for anything that， depends on those variables。 And so
    if nothing else， it at least needs to allocate the memory for this。 Well。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要为此添加梯度。好吧。之所以需要为此添加梯度，是因为，Glue on 需要知道，嘿。我可能想要计算依赖于这些变量的任何东西的梯度。所以，如果没有别的，至少它需要为此分配内存。嗯。
- en: actually it needs to start allocating， creating some data structures to， keep
    tracking it。 But yeah。 that's a different story。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，它需要开始分配，创建一些数据结构来跟踪它。但这是另一个话题。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_21.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_21.png)'
- en: So， okay， fine。 So we implement the softmax。 And yeah。 this is not how you want
    to implement the softmax， mind you。 Because that's numerically unstable。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，好的，明白了。所以我们实现 softmax。对。嗯，记住，这不是你应该实现 softmax 的方式，因为它在数值上不稳定。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_23.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_23.png)'
- en: So don't do it。 The way I say。 Don't do it the way I do it。 Do it the way I
    say， right？
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以不要这么做。不要按照我说的做。按照我教你的方式做，明白吗？
- en: But since this is just for illustration purposes and this is a small example。
    things aren't going to go wrong。 But in general， don't do this。 It's the equivalent
    of driving without the seat belt。 So let's actually see what happens， right？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于这只是为了演示目的，并且这是一个小示例，事情不会出错。但一般来说，不要这么做。这相当于开车不系安全带。所以让我们实际看看会发生什么，好吗？
- en: And this is really just， you know， I exponentiate all the terms in the night。
    divided by the corresponding sum。 And the broadcast mechanism ensures that I normalize
    every term separately。 Okay。 So let's， you know， construct some x's and then I
    can compute the softmax。 And then I can check whether everything works out properly。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上就是，你知道的，我将所有的项指数化，然后除以对应的和。广播机制确保我对每一项进行单独的归一化。好的。所以让我们构造一些x，然后我可以计算softmax。然后我可以检查一切是否正常运行。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_25.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_25.png)'
- en: So let's look at that。 And yes， these are probabilities。 And they sum up to
    one。 which is whatever you expect。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们来看一下。是的，这些是概率。它们的总和为1，这正是你所期望的。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_27.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_27.png)'
- en: Okay。 So this is softmax。 The neural network model itself is going to be fairly
    trivial as well。 right？ All I'm doing is I'm returning the softmax of， well， w。x
    plus b。 Okay。 And then I need to define the cross entropy。 And that doesn't do
    anything else but just picks out。 you know， the， corresponding， you know， yi。
    So actually calling it cross entropy is a little bit flaky。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这是softmax。神经网络模型本身也会相当简单，对吧？我做的只是返回softmax，即w.x加b的结果。好的。然后我需要定义交叉熵。它没有做别的，只是选出对应的，嗯，yi。所以实际上叫它交叉熵有点不太准确。
- en: And I'm taking the log。 So this is really just minus log of p of y given o。
    Okay。 Fine。 So we have that。 And then， okay， need a few more things。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我取对数。所以这实际上就是y给定o的概率p的负对数。好的，明白了。我们有了这个。然后，好的，还需要做一些其他的事情。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_29.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_29.png)'
- en: I need to define the accuracy。 So I'm basically doing is I'm looking at the
    number of times where the。 labels that I output， namely to an argmax， and the
    labels that I should have gotten， match。 and I take the average of them。 So I
    get this binary matrix of zero and ones one with the labels match zero if。 they
    don't。 And I just compute that number。 And then， you know， to compute the accuracy，
    I mean。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要定义准确率。其实我做的是查看输出标签，即通过argmax得到的标签，和我应该得到的标签是否匹配。我取它们的平均值。所以我得到了一个由零和一组成的二进制矩阵，标签匹配时是1，不匹配时是0。然后我计算这个数值。然后，你知道的，计算准确率，就是这样。
- en: you need to iterate through the， data， you go and really compute， you know。
    the function for everything and that's it。 Now， initially。 because I used some
    garbage initialization， well， I can't really。 expect anything particularly good
    out of that。 We have 10 classes。 So random guessing is 10%。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要迭代数据，真的去计算所有的函数，就这些。现在，最初，由于我使用了一些糟糕的初始化，嗯，我不能指望得到什么特别好的结果。我们有10个类。所以随机猜测的准确率是10%。
- en: and in this case， I'm able to slightly better than。 random guessing for whatever
    reason I just got lucky。 Okay。 Any questions so far？ So to recap。 we define our
    network。 Right？ That's net。 We defined a loss function。 We defined a measure of
    the accuracy。 Those two things need not be the same。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我比随机猜测稍微好一点，不知道为什么，我可能只是运气好。好的，到目前为止有问题吗？那么总结一下。我们定义了我们的网络，对吧？那就是net。我们定义了损失函数。我们定义了准确率的衡量标准。那两者不一定相同。
- en: So this loss function is the thing that we optimize over。 The accuracy is the
    thing that we actually care about in the end。 But since it may not have any derivatives
    in a meaningful way， I'm much better off。 just taking something that's differentiable
    and a good proxy。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个损失函数是我们优化的目标。准确率是我们最终关心的事情。但由于它可能没有任何有意义的导数，我最好只是采取一些可微分的、好的代理函数。
- en: And there's a lot of really beautiful theory about surrogate losses and so on。
    So if you're interested in that， take a class of Peter Bartlett's。 He'll probably
    cover that in his classes。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 而且关于替代损失等方面有很多非常美丽的理论。如果你对这些感兴趣，可以参加Peter Bartlett的课程。他在课堂上可能会讲到这些内容。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_31.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_31.png)'
- en: He's here in the stats department。 Anyway， so then he is a training function。
    And just because we're going to use that over and over and also in the context
    of。 Glue on and later on for other examples， we just define it as a function。
    So let's pick that function apart。 So this training function takes a couple of
    things as an argument。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 他在统计学系。总之，他是一个训练函数。因为我们会反复使用它，而且还会在后面的例子中使用它，所以我们将其定义为一个函数。那么我们来逐步分析这个函数。这个训练函数接受一些参数。
- en: So this would be like your Keras fit function， if you will。 It takes a network
    as an argument。 The number of iterations for training， since we also want to output
    the test error， okay。 so we'll find。 We also have number of test iterations there。
    You care about a loss function。 Well。 sorry， train iterative and test iterative
    are the iterators over the data。 Sorry， I apologize。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像你的 Keras `fit` 函数。它将网络作为一个参数，训练的迭代次数，因为我们还希望输出测试误差。所以我们找到了那个。我们还设定了测试迭代次数。你关心的是损失函数。抱歉，训练迭代器和测试迭代器是数据的迭代器。抱歉，我道歉。
- en: Number of epochs is the number of times you actually go to the data set。 Batch
    size is the mini batch size， so how many observations you pick in one block。 And
    if you pick a larger mini batch size within reason， the algorithm goes faster。
    but it may not always converge quite as rapidly。 Number of。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数是指你实际访问数据集的次数。批量大小是指小批量的大小，即每次从数据中选取多少个观察值。如果你选择较大的小批量大小，在合理范围内，算法会运行得更快，但可能不会那么迅速收敛。次数。
- en: then there's some additional parameters that you can feed into your， optimization
    algorithm。 Right now， there are none of those。 Because we haven't actually discussed
    learning algorithms as optimization algorithms yet。 We'll do that quite a bit
    later。 Then I need a learning rate and an optimizer。 Right。 So then。 what it does
    is it just goes over the data as many times as you want。 Right。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以为优化算法提供一些附加参数。现在没有这些参数，因为我们还没有讨论学习算法作为优化算法的内容。稍后我们会详细讲解。然后我需要一个学习率和一个优化器。对吧？然后它会根据你指定的次数，遍历数据。
- en: that's number of epochs。 Okay， it initializes all the errors to zero。 And then
    for all the data in the training iterator， what it does is it now starts recording。
    for autograph， it computes the output of the network， computes the loss， and then。
    takes the gradient with regard to the parameters of the loss。 So that's the thing
    here。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迭代次数。好了，它会将所有的错误初始化为零。然后，对于训练迭代器中的所有数据，它会开始记录。对于自动求导，它计算网络的输出，计算损失，然后根据损失的参数计算梯度。所以这就是这里的工作原理。
- en: That's the heart of the algorithm。 If you didn't specify an optimization algorithm。
    we pick one for you。 Right。 If you did， well， we'll use the one that is specified。
    So then。 we check the accuracy。 And we report that。 Right。 Right。 And then we，
    in case it's needed。 we also evaluate the test accuracy。 Then we log this。 So
    it's fairly straightforward code。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是算法的核心。如果你没有指定优化算法，我们会为你选择一个。对吧？如果你指定了，那么我们会使用你指定的那个。那么我们接下来检查准确率，并报告它。对吧？然后，如果需要的话，我们还会评估测试准确率。然后我们记录这些结果。所以这段代码相当直接。
- en: Any questions so far？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止有任何问题吗？
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_33.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_33.png)'
- en: Okay。 Let me zoom in a bit。 Okay。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我放大一下。好了。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_35.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_35.png)'
- en: Good。 So then let's actually train it and see what happens。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么接下来我们就训练一下，看看会发生什么。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_37.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_37.png)'
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_38.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_38.png)'
- en: And now just invoking it with a learning rate of 。1 and 5 epochs。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用学习率为0.1，迭代5次来调用它。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_40.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_40.png)'
- en: And we can see that， you know， the loss is decreasing。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，损失在不断减少。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_42.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_42.png)'
- en: That's good。 Training accuracy。 So that's the loss function that I minimize。
    And the training accuracy keeps on increasing。 And the test accuracy is also increasing。
    And it actually turns out that in this case， here after five iterations， my test
    accuracy。 is ever so slightly higher than the training accuracy。 It's good。 So
    I didn't know of it。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。训练准确率。所以这是我需要最小化的损失函数。训练准确率持续上升，测试准确率也在增加。事实上，在这种情况下，经过五次迭代后，我的测试准确率略微高于训练准确率。这很好。所以我之前并不知道这一点。
- en: We could actually increase that to ten。 Let's see what happens now。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以将这个值增加到十。让我们看看现在会发生什么。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_44.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_44.png)'
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_45.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_45.png)'
- en: By the way， we end up starting at a good value already because we are restarting
    from the。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，我们已经从一个不错的值开始了，因为我们从之前优化过的点重新开始。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_47.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_47.png)'
- en: point where we optimized before。 So otherwise， I would have had to go and re-inertilize
    w and b。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前优化过的点开始。所以否则的话，我就得重新初始化w和b。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_49.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_49.png)'
- en: And yeah， it keeps on getting a little bit better， but there isn't really much
    to do anymore。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，是的，它会不断变得稍微更好一点，但实际上已经没有什么可做的了。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_51.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_51.png)'
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_52.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_52.png)'
- en: And it's pretty much plateaued。 The reason why I'm not overfitting here。 does
    anybody have any idea why I might not be。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上已经达到了平台期。我为什么没有过拟合，大家有什么想法吗？
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_54.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_54.png)'
- en: overfitting here？ Yes？ Is it using a fitting batch？ Not so much。 Not the mini
    batch。 It's just that we're using a ridiculously dumb model。 It's just a linear
    model of 756 times ten dimensions。 So it's basically not very many parameters
    relative to the amount of data that I have。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这里没有过拟合？是的？它是不是用了一个合适的批量？其实不是。不是小批量训练。只是我们使用了一个非常愚蠢的模型。它只是一个 756 乘以 10 维的线性模型。所以相对于我拥有的数据量，它的参数并不多。
- en: And at this point， there isn't really much that's happening anymore。 So actually。
    I ended up overfitting a little bit in the NC。 You can see that the test accuracy
    diminished ever so slightly。 but that's pretty much， within the noise。 So let's
    see how well that works in practice afterwards。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，实际上已经没有什么事情再发生了。所以其实，我在NC中稍微过拟合了一下。你可以看到测试精度稍微下降了一点，但这基本上是噪声范围之内的。所以接下来我们来看看它在实践中的表现如何。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_56.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_56.png)'
- en: So let's actually go and display this。 And if you look at the very bottom。 you
    can see that it mostly gets things right。 So it confuses swatters and shirts and
    dresses and coats。 But at least for the really obvious ones like bags or sandals，
    it gets it right。 So this is softmax regression completely from scratch。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们实际去展示一下。如果你看看最底部，你会发现它大多能正确识别。所以它会把羽毛球拍、衬衫、裙子和外套弄混。但至少对于一些非常明显的物品，比如包包或凉鞋，它能准确识别。所以这就是完全从零开始的Softmax回归。
- en: So all you need is just a linear algebra library and automatic differentiation。
    And that's it。 Now。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你只需要一个线性代数库和自动求导。就这么简单。
- en: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_58.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4552b5a5a8ccd94394a3a3a1aa5c198e_58.png)'
