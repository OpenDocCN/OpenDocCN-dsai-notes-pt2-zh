- en: P98：98. L19_1 Recurrent Neural Networks - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P98：98. L19_1 递归神经网络 - Python小能 - BV1CB4y1U7P6
- en: Okay， so welcome to another course on recurrent neural networks。 So last week。
    we covered kind of the basics a little bit， why you actually need recurrent models。
    What the difference is between latent variable models and， models without latent
    variables。 And what we effectively saw is that latent variable models can be。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，欢迎来到关于递归神经网络的另一个课程。上周，我们讲解了基本的内容，为什么你实际上需要递归模型。潜在变量模型和不带潜在变量的模型之间的区别是什么。我们实际看到的是，潜在变量模型可以是。
- en: a really convenient way of dealing with the fact that you need to。 aggregate
    in some way all the past state of what you've seen so far。 And aggregate that
    in some variable that helps you to make predictions of what。 you're going to see
    next。 And the key distinction really was that you can either just have the past。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理你需要以某种方式聚合你到目前为止看到的所有过去状态的一个非常方便的方法。并将其聚合到一个有助于你做出预测的变量中，预测你接下来将看到什么。关键的区别实际上是，你可以选择仅使用过去的信息。
- en: k terms as an embedding or you can have a latent variable。 And if you have to
    pass k terms as an embedding， this makes training really easy and we actually
    saw what happens。 But then you may not have a lot of fidelity。 On the other hand，
    if you have a latent variable model。 you have a little bit more freedom in terms
    of what you can stuff into that。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: k个术语作为嵌入，或者你可以拥有一个潜在变量。如果你必须将k个术语作为嵌入，这样训练就非常容易，我们实际上也看到了发生了什么。但是你可能没有很高的精度。另一方面，如果你有一个潜在变量模型，你在可插入内容的自由度上会更多。
- en: variable and then it may potentially work better。 So in the ideal case。 this
    will store the sufficient statistics of all the past observations。 So today we're
    going to actually see a little bit how those models behave in， practice。 how we
    engineer some of them。 And let's see how far we get， probably we'll get to at
    least a basic。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 变量，然后它可能会表现得更好。所以在理想情况下，这将存储所有过去观测的充分统计量。所以今天我们将实际看看这些模型在实践中的表现，如何工程化其中的一些模型。让我们看看我们能进行多远，可能至少会涉及到基本的部分。
- en: latent variable model， maybe we'll get to a GRU。 Unlikely that we'll cover the
    STMs。 let's see how we go。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在变量模型，也许我们会涉及到GRU。 不太可能会讲到STM。让我们看看接下来怎么进行。
- en: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_1.png)'
- en: So let's just recap。 Basically， what we assume is that we have some function
    F。 which compresses all the information from X1 up to Xt minus 1 into some state
    ht。 Of course。 if we compute that explicitly and T ranges from 1 to 10，000， this
    is super expensive。 so you don't want that。 And what we do is we basically engineer
    this function F to be 1 that takes。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们简单回顾一下。基本上，我们假设我们有一个函数F，它将从X1到Xt-1的所有信息压缩成某个状态ht。当然，如果我们显式地计算这个，T的范围从1到10,000，这非常昂贵，所以你不希望这样做。我们做的是，我们基本上将这个函数F设计成一个可以接受。
- en: as its inputs the previous statistic， ht minus 1。 And the previous observation
    in order then to allow us to make inference p of ht。 given ht minus 1， xt minus
    1， and also p of xt， given ht， and xt minus 1。 And so we just move forward from
    the left to the right in this grid。 So really the differences between width and，
    without latent variables are kind of subtle。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 它的输入是之前的统计量，ht-1。以及之前的观测值，以便我们可以推断出p(ht)。 给定ht-1，xt-1，以及p(xt)，给定ht，xt-1。因此我们只是从左到右在这个网格中前进。所以，实际上，宽度模型和不带潜在变量的模型之间的差异是相当微妙的。
- en: but the implications are quite profound。 And essentially， maybe about a decade
    ago。 I would have talked to you a lot about， how to do efficient inference in
    such models how to scale this up using graphical。 models tools。 There are really
    a lot of beautiful mathematical constructs for it。 The reason that we are pretty
    much skipping this and going right into deep。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但其含义非常深远。基本上，也许在大约十年前，我会和你们讨论如何在这种模型中做高效推断，如何使用图模型工具扩展它。有很多美丽的数学构造来支持这一点。之所以我们几乎跳过这一部分，直接进入深度学习，是因为。
- en: learning is actually twofold。 First of all， those models that we would be using
    are probably not true anyway。 right？ So I mean， after all， they are models， right？
    In very few cases。 do you know that you really have 10 clusters as a latent state？
    You don't know that。 It's just a modeling assumption。 Secondly， while you then
    may have that。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 学习实际上是双重的。首先，我们将使用的那些模型可能本来也不是真实的，对吧？毕竟，它们是模型，对吧？很少有情况，你知道你确实有10个聚类作为潜在状态？你不知道。这只是一个建模假设。其次，虽然你可能有。
- en: and you can maybe even specify it， you might not be able to do inference efficiently
    anyway to get an exact outcome。 And so the fact that first of all， you shouldn't
    trust your model anyway。 Secondly， even if you do。 you might not get the true
    answer either。 You might as well decide， well。 since I don't know the truth and，
    even if I knew it， I couldn't get it。 Well。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可能会指定它，但即使你做了推理，也可能无法高效地得到精确结果。所以首先，你不应该信任你的模型；其次，即便你信任它，你也可能得不到真正的答案。你可能会决定，嗯，既然我不知道真相，而且即使我知道了，我也无法得到它。嗯。
- en: let's just do the best thing that an engineer would do， namely。 try to engineer
    the best model that we can actually get our hands on。 And that's to some extent
    why deep networks are now quite popular at。 the expense of graphical models， at
    least in some applications。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个工程师会做的最好的事情，也就是说，尝试设计我们能实际得到的最佳模型。这也是深度网络如今在某些应用中相较于图模型非常流行的原因。
- en: There are some cases where graphical models are still superior。 especially if
    you have very specific structure that you can efficiently exploit。 But short of
    that。 it gives you a bit of a background of why you care。 Any questions so far？
    Yes？
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一些情况，图模型仍然优于其他方法，特别是当你有非常具体的结构时，你可以有效地加以利用。但除此之外，这可以给你一点背景，帮助你理解为何它很重要。到目前为止有任何问题吗？是吗？
- en: '>> The graphical models be combined with neural networks in a way that。 you
    have some major ability that you can graph the model while still having。 the expressive
    power of the deep neural number？ >> So the question was。 can you combine both
    and get the best of both worlds？ And indeed you can。 So for instance。'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 图模型与神经网络的结合方式是，你可以绘制模型，同时仍然保持深度神经网络的表现力吗？ >> 所以问题是，你能将二者结合起来，得到两者的最佳效果吗？的确可以。比如说。'
- en: there's this quite famous paper from， Cabbie Murphy about using a conditional
    random field in combination with。 basically lower level image pre-processing to，
    for， instance， perform segmentation。 And this paper is a really cool paper。 It
    has a couple of thousand citations， so。 that's actually rather remarkable， given
    that it's only like maybe two， three years out。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有一篇非常著名的论文，来自 Cabbie Murphy，讲的是如何将条件随机场与基础的图像预处理相结合，举例来说，用于执行图像分割。这篇论文真的是一篇很棒的论文，引用了几千次。所以，考虑到这篇论文可能才只有两三年历史，这真是相当了不起。
- en: It turns out that by now the paper is getting the citations not because。 the
    conditional random field part that sits on top， but。 because of the essentially
    dilating convolutions where you up sample， the feature map。 which is something，
    it's kind of a side remark and， quite trivial in an engineering solution。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，到现在为止，这篇论文获得引用并不是因为位于顶部的条件随机场部分，而是因为本质上是扩展卷积，涉及到上采样特征图。这是一个稍微的旁注，在工程解决方案中其实是很简单的。
- en: And by now it's getting the citations because of that and， nobody really uses
    a CRF anymore。 they just go all the way straight to a deep learning。 So only if
    you really， really。 really must have interoperability， or， if you have a very
    narrow setting where you know that specific graphical。 model is really true， would
    I recommend that you build such a hybrid approach？
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 而现在，它的引用量正是因为这一点，而实际上现在没有人再使用 CRF 了，他们直接跳到深度学习。所以，只有在你真的非常非常必须要实现互操作性，或者你有一个非常狭窄的场景，你知道特定的图模型真的成立时，我才会推荐你构建这样一个混合方法。
- en: But you'll pretty much know that you have this case if you have it。 Yeah。 so
    this is kind of both quite liberating， but also very sad。 Sad because a lot of
    tooling is now no longer so relevant， but。 also liberating because it makes things
    a lot easier。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你遇到这种情况，你大概会知道。是的，所以这既是非常解放的，也让人有点伤感。伤感的是很多工具现在变得不再那么相关了，但也很解放，因为它让事情变得更容易了。
- en: Before that it used to be that you have this fancy model and。 then you basically
    get a PhD student and you have him spend three months on。 working out the math
    both either a sample or a variational inference。 implement it write a paper and
    you do that five times。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 之前你可能会有一个很复杂的模型，然后你基本上找一个博士生，让他花三个月的时间，去解决数学问题，无论是样本方法还是变分推断，实施并写一篇论文，然后你会做五次这样的事。
- en: Papers are guaranteed because the math is not trivial and then you graduate，
    right？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 论文之所以有保证，是因为数学内容并不简单，然后你就毕业了，对吧？
- en: Or maybe it takes two， three years， but， this was a very predictable way of
    graduating。 Unfortunately that angle no longer works so， well because they're
    easier tools。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 或者可能需要两三年的时间，但这是一个非常可预测的毕业方式。不幸的是，这种方法现在已经不再有效了，因为它们是更简便的工具。
- en: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_3.png)'
- en: Anyway， so required models with hidden state。 In the simplest case。 you could
    have something like this。 I have hidden state h t which is just given by some
    function phi of。 I have matrix vector w times h plus matrix vector for， w times
    the pass observations and some bias。 And then for the output， again， I just take
    some nonlinear function of the hidden state。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，所需的模型都有隐藏状态。在最简单的情况下，你可以像这样定义。我的隐藏状态h t 只是由某个函数phi给出。我有一个矩阵向量w乘以h，再加上一个矩阵向量，w乘以通过的观测和一些偏置。然后对于输出，我同样只是取隐藏状态的某个非线性函数。
- en: So this is about as primitive as it goes and， you can use that for a simple
    RNN。 And this is essentially not much more complex than a multi layer perceptron。
    except that you walk through it one step at a time。 And the other thing is I distinguish
    between output and observations。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是最基础的形式，你可以用它来构建一个简单的RNN。而且这本质上并不比一个多层感知机复杂，除了你需要一步一步地进行处理。另一点是，我区分了输出和观测。
- en: In many cases I might want to use the output to generate the next observation
    and， go through it。 but that's not necessarily the case。 For instance。 if I want
    to annotate a text sequence for named entities， my input。 so the observations
    are the words。 The explanation of the hidden state is then whatever that hidden
    state is。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我可能想使用输出生成下一个观测，并继续处理。但这不一定是必须的。比如，如果我要为命名实体标注文本序列，那么我的输入，即观测，就是单词。隐藏状态的解释则是隐藏状态本身。
- en: And the output would then allow me to generate named entity annotations， right？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输出将允许我生成命名实体标注，对吧？
- en: So that's why it makes sense to distinguish those two pieces。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是为什么区分这两个部分是有意义的。
- en: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_5.png)'
- en: Okay， and so now let's actually have a look at this in terms of code because。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么现在让我们从代码的角度来实际看一下这个问题，因为。
- en: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a9d29a230d7e01a1a4d4526a4c14de9_7.png)'
- en: it's really straightforward。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的非常简单。
