- en: P38：RL for Stock Trading -RL and Inverse Reinforcement Learning Solutions -
    兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P38：股票交易的强化学习 - 强化学习与反向强化学习解决方案 - 兰心飞侠 - BV14P4y1u7TB
- en: Now we are ready to put everything together in one backward-occursion scheme
    to compute。 the optimal policy from data using reinforcement learning。 The first
    thing we do is update the optimal policy。 We first do it in terms of the increments。
    increment delta A(t)。 The posterior probability of A(t) is shown in equation 50。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备将所有内容整合成一个反向迭代方案，用于通过数据计算最优策略，使用强化学习。我们首先做的是更新最优策略。我们首先通过增量来实现它，增量delta
    A(t)。A(t)的后验概率如方程50所示。
- en: Here pi0 of delta A(t) is a prior distribution pi0， which is re-expressed in
    terms of delta。 A(t) instead of A(t)。 Now because pi0 of delta A(t) is Gaussian
    and the expression in the exponent is quadratic。 the result is another Gaussian
    distribution， as shown in the second line of equation 50。 The new mean and covariance
    of this distribution are shown in equations 51。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，delta A(t)的pi0是一个先验分布pi0，它以delta A(t)而非A(t)的形式重新表达。现在，由于delta A(t)的pi0是高斯分布，并且指数中的表达式是二次的，因此结果是另一个高斯分布，如方程50的第二行所示。这个分布的新均值和协方差在方程51中给出。
- en: We can view this equation as Bayesian updates of prior values for these quantities。
    Now we are almost there， but we need one more element here， namely a way to find
    the trajectory。 that we used before in our scheme。 Such trajectory is found in
    the model self-consistently。 iterating several times between a forward pass， and
    a backward pass。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个方程看作是对这些量的先验值的贝叶斯更新。现在我们已经接近完成，但这里还需要一个元素，即找到我们在方案中之前使用过的轨迹的方式。这种轨迹是通过自洽地在前向传递和反向传递之间进行多次迭代来找到的。
- en: The method used in this calculation is called iterated quadratic Gaussian regulator，
    and。 it was suggested in 2005 by Todorov and Li。 Here is how the method works。
    We start with an initial trajectory that is built using the mean of the prior
    policy distribution。 At each step we take the current mean of the prior as a deterministic
    action and then compute。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算中使用的方法称为迭代二次高斯调节器，它是由Todorov和Li于2005年提出的。方法是这样工作的：我们从一个初始轨迹开始，这个轨迹是使用先验策略分布的均值构建的。在每一步，我们将先验的当前均值作为一个确定性动作，然后计算。
- en: a new state using the original state equation without the noise term。 Then we
    go backwards and compute the g function by recursion as we did above。 And this
    is the backward pass of the algorithm。 This produces an update for the mean of
    the policy。 And after that in a forward pass we make a new trajectory around this
    mean and the whole。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始状态方程计算新的状态，但不包含噪声项。然后我们反向计算，像上面那样递归计算g函数。这是算法的反向传递。它为策略的均值提供了一个更新。之后，在前向传递中，我们围绕这个均值生成一个新的轨迹，整个过程重新开始。
- en: thing begins again。 This is continued until convergence and produces a self-consistent
    solution for the dynamics。 The end result can be expressed as an optimal data
    implied policy that has the same functional。 form as our prior policy by zero
    but with updated parameters。 You can find the explicit expression for coefficients
    A0 and A1 that are recomputed here。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会持续进行，直到收敛，并产生一个自洽的动力学解。最终结果可以表示为一个最优的数据隐含策略，它具有与我们的先验策略相同的函数形式，但具有更新的参数。你可以找到在这里重新计算的系数A0和A1的显式表达式。
- en: at each step of the trajectory optimization procedure in the original publications。
    The updated covariance matrix sigma m is shown here and the change is clearly
    seen。 Sigma m is a regularized covariance matrix of the prior where a regularization
    term is。 equal to beta times the matrix G aa t。 So now let's summarize and put
    the whole scheme together。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始文献中的每一步轨迹优化过程中，更新后的协方差矩阵sigma m如图所示，并且变化清晰可见。Sigma m是先验的正则化协方差矩阵，其中正则化项等于beta乘以矩阵G
    aa t。那么，现在让我们总结一下并把整个方案整合起来。
- en: This is an algorithm that applies for our model in the conventional reinforcement
    learning。 setting when the words are observable。 The algorithm starts with an
    initial trajectory as a sequence of pairs x bar and u bar for。 all values of t
    from zero to capital T。 And this is the first forward pass。 Then we start iterating
    between the backward and forward passes until convergence。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个适用于我们模型的算法，适用于传统强化学习设定，当词语是可观察时。该算法从初始轨迹开始，作为一系列的对（x bar 和 u bar），涵盖从零到T的所有t值。这是第一次前向传递。然后，我们开始在反向和前向传递之间迭代，直到收敛。
- en: First we perform a backward pass that goes back in time from capital T minus
    1 to zero。 And in this pass for each value of t。 In step one we compute the expected
    value of the F function from the next period as。 we showed above。 Next in step
    two we use this computed value and observed three words and compute the G function。
    at this step。 In step three we compute the F function at the same time step。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们执行一个从T-1时刻回溯到0时刻的后向传递。在这一过程中，对于每个t值，在第一步中，我们计算从下一时期得到的F函数的期望值，正如我们上面所展示的那样。接下来，在第二步中，我们使用计算出的值和观察到的三个词语，计算G函数。在这一步中。第三步，我们计算相同时间步长的F函数。
- en: And after that in step four we recomputed the policy for this time step by updating
    its。 mean and variance。 We repeat step one to four for all time steps until we
    complete the backward pass。 Then we do a forward pass by constructing a new trajectory
    using the new mean and variance。 and the whole procedure repeats。 This summarizes
    the procedure for the regular reinforcement learning with our model when。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，在第四步中，我们通过更新其均值和方差来重新计算此时间步的策略。我们重复步骤一到四，直到所有时间步都完成后向传递。然后，我们通过使用新的均值和方差构建新的轨迹进行前向传递，整个过程重复进行。这总结了在我们的模型下进行常规强化学习的过程。
- en: the words are observable。 But it turns out that even more interesting problems
    can be addressed by this model in。 this setting when the words are not observable。
    I would like to remind you that this setting is called inverse reinforcement learning
    or。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 词语是可观察的。但是事实证明，当词语不可观察时，这个模型可以解决更有趣的问题。我要提醒大家，这种设置被称为逆向强化学习（IRL）。
- en: IRL。 In this setting we observe states and actions but not rewards。 In general
    this setting is both very interesting and more difficult than a direct reinforcement。
    learning case。 We will talk more about the IRL in general in a little bit but
    first let's see how our。 model can work in such setting。 It turns out that this
    case is easy for our model because inverse reinforcement learning。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，我们观察状态和动作，但无法观察奖励。通常，这种设置既非常有趣，又比直接强化学习更加困难。我们稍后会详细讨论逆向强化学习，但首先让我们看看我们的模型在这种设置下如何工作。事实证明，这个案例对我们的模型来说是容易的，因为逆向强化学习。
- en: amounts in this model just to conventional maximum likelihood estimation。 The
    reason for this is that a policy that we obtain in this model is a Gaussian policy。
    whose mean and variance depend on parameters of the word function。 Therefore we
    can use the observed state action data and apply the regular maximum likelihood。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型中的数值仅为常规最大似然估计。之所以这样，是因为我们在该模型中获得的策略是高斯策略，其均值和方差依赖于奖励函数的参数。因此，我们可以利用观察到的状态-动作数据，应用常规的最大似然估计。
- en: method to such Gaussian distribution。 This gives us the negative log likelihood
    of data shown here in equation 54。 Because parameters of this Gaussian likelihood
    depend on the original parameters of the reward。 function we can use the characteristic
    gradient descent or gradient descent method to compute。 these parameters by minimization
    of the negative log likelihood。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 方法对于这种高斯分布。这给出了数据的负对数似然，如方程54所示。由于该高斯似然的参数依赖于奖励函数的原始参数，我们可以使用特征梯度下降法或梯度下降法，通过最小化负对数似然来计算这些参数。
- en: This should be done at each iteration of the above forward backward scheme。
    So for each iteration K we have to substitute here values of A0 and A1 and sigma
    M computed。 the this iteration。 This procedure replaces observed rewards of the
    reinforcement learning setting by estimated。 rewards of inverse reinforcement
    learning setting。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程应该在上述前向-后向方案的每次迭代中进行。所以对于每次迭代K，我们需要在此处替换A0和A1的值，以及在这一迭代中计算出的sigma M。这一过程将强化学习设置中的观察奖励替换为逆向强化学习设置中的估计奖励。
- en: Now we bring this additional element for inverse reinforcement learning and
    present a full scheme。 for the model for inverse reinforcement learning setting。
    The only new element comparing to the reinforcement learning case is a new step
    two where we use。 maximum likelihood to estimate the reward parameters and other
    parameters that entered， the model。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为逆向强化学习引入了这一附加元素，并提出了完整的方案。与强化学习模型相比，唯一的新元素是第二步，在这一步中，我们使用最大似然法来估计奖励参数和其他进入模型的参数。
- en: including in particular the inverse temperature parameter beta。 The next step
    three uses this estimated rewards instead of observed rewards to compute the。
    G function and the rest of the steps is the same as for reinforcement learning
    setting。 This shows that computationally inverse reinforcement learning in this
    model is not much harder than。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是包括逆温度参数beta。接下来的步骤使用这个估计的奖励值，而不是观察到的奖励值，来计算G函数，其余的步骤与强化学习设置中的相同。这表明，在这个模型中，逆向强化学习在计算上并不比
- en: directly enforcement learning。 Now how could we use this model to to interested
    in reinforcement learning things？
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直接强化学习。现在我们如何利用这个模型来关注强化学习的相关内容呢？
- en: Recall that when we introduced the model we said that it can be used either
    as a model。 of a particular trader or as a model for the market portfolio such
    as S&P 500 portfolio。 In the first case we need proprietary trading data from
    that particular trader to learn。 the reward function and optimal policy for that
    trader。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们介绍这个模型时，我们说过它可以作为某个特定交易员的模型，或者作为市场投资组合模型，比如S&P 500投资组合。在第一种情况下，我们需要来自该交易员的专有交易数据，以学习该交易员的奖励函数和最优政策。
- en: If we have such data we can for example compute implied risk aversion parameter
    lambda for。 that trader。 But more often than not we do not have such proprietary
    data because only brokers or traders。 have it and that's why it's called proprietary
    data。 So what else we can do with the model is to use it with open data namely
    with market data。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有这样的数据，我们可以例如计算出该交易员的隐含风险厌恶参数lambda。但往往我们没有这样的专有数据，因为只有经纪人或交易员才拥有它，这就是为什么它被称为专有数据。那么我们还能用这个模型做什么呢？我们可以使用公开数据，也就是市场数据。
- en: We can apply it to the market S&P 500 portfolio in a similar way to the black
    litremon model。 that I mentioned earlier。 The whole scheme of the model can then
    be viewed as a dynamic probabilistic and data-driven。 extension of the black litremon
    model and its inverse optimization interpretation by。 Burt Smith and co-workers。
    In this case the optimal policy that we computed before which I repeat in the
    formula here。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前提到的Black-Litterman模型一样，将其应用于市场S&P 500投资组合。这个模型的整体结构可以视为Black-Litterman模型的动态概率性和数据驱动的扩展，以及Burt
    Smith和他的同事们对其逆优化的解释。在这种情况下，我们之前计算出的最优政策，我在这里重复这个公式，
- en: would be a market implied optimal policy。 On a set of commonly used predictors
    ZT we can directly estimate this market implied optimal。 policy using the method
    we just described。 On the other hand if we have some private signals ZT also called
    private views in the。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将会是市场隐含的最优政策。通过一组常用的预测因子ZT，我们可以直接使用我们刚才描述的方法估计出这个市场隐含的最优政策。另一方面，如果我们有一些私人信号ZT，也叫做私人观点，
- en: black litremon model that are not available to the rest of the market we can
    use them。 to try to beat the market by our improved optimal policy that takes
    those signals into， account。 Such improved policy would be obtained by adding
    these signals to the list of common， predictors。 You can find more details on
    this model in the original publications that you will need。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有Black-Litterman模型中不为市场其他参与者所知的信号，我们可以利用这些信号，通过我们改进的最优政策来尝试超越市场。这种改进的政策是通过将这些信号添加到常见预测因子的列表中得到的。你可以在原始的文献中找到更多关于该模型的细节。
- en: to consult because you will be dealing with this model in your final course
    project for。 this course。 And at this point we are ready to wrap up our week and
    our whole course on the reinforcement。 learning。 We will summarize what we learned
    in our next video。 Thank you。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 咨询，因为你将在课程的最终项目中使用这个模型来完成。本周的内容以及我们关于强化学习的整个课程到此结束。我们将在下一个视频中总结我们所学的内容。谢谢。
- en: '![](img/e0d72f2fd4009a30dbdfa6fae2f18059_1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0d72f2fd4009a30dbdfa6fae2f18059_1.png)'
- en: and we will see you on the next video。 Bye。 [BLANK_AUDIO]。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一个视频中见面。再见。[BLANK_AUDIO]。
- en: '![](img/e0d72f2fd4009a30dbdfa6fae2f18059_3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0d72f2fd4009a30dbdfa6fae2f18059_3.png)'
