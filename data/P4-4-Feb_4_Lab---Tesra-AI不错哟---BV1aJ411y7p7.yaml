- en: P4：4.Feb_4_Lab - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P4：4.Feb_4_Lab - Tesra-AI不错哟 - BV1aJ411y7p7
- en: Alright， so I get started。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我开始了。
- en: '![](img/8074d5ece37ab440fb724f0e80292e6c_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8074d5ece37ab440fb724f0e80292e6c_1.png)'
- en: '![](img/8074d5ece37ab440fb724f0e80292e6c_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8074d5ece37ab440fb724f0e80292e6c_2.png)'
- en: Cool。 So today we're continuing with the LASO and we're going to talk first
    about some methods。 to actually optimize the LASO just like we have gradient descent，
    stochastic gradient descent。 applied directly to the objective function to do
    the ridge regression。 Now we're going to see what can we do for LASO。 Alright，
    can you guys hear me？ Alright back there。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒。那么今天我们继续讨论Lasso，首先我们要讨论一些方法，实际上优化Lasso，就像我们有梯度下降、随机梯度下降直接应用到目标函数做岭回归一样。现在我们要看看如何处理Lasso。好吧，大家能听到我说话吗？好，后面的同学能听到吗？
- en: Alright， great。 Alright， so recall this is the objective function for the last
    two。 We have our average square loss plus an L1 regularization term， which is
    some of the。 squares of those coefficients of the linear model。 And we can start
    by saying alright。 what about gradient descent？ And we get stuck right away because
    this L1 norm is a differentiable。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，太棒了。那么回想一下，这是前两个的目标函数。我们有平均平方损失加上L1正则化项，它是线性模型系数的平方和。我们可以从这里开始讨论，好吧，梯度下降怎么样？我们马上就遇到问题了，因为这个L1范数不可微分。
- en: If you break it out， it's got absolute values in it。 It's not differentiable
    what the kink and the absolute value。 So what can we do？
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把它分开，它里面就有绝对值。**它在折点和绝对值处不可微分**。那么我们该怎么做呢？
- en: So there's a common trick that we use in this sort of situation。 It starts off
    as a trick and eventually you'll get used to it and it's just a tool you'll use。
    And it's called breaking numbers into their positive and negative parts。 So let
    me show you some notation。 So consider any number A， just a number， real number。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，我们有一个常用的技巧。一开始它是一个技巧，最终你会习惯它，它会变成你常用的工具。这个技巧叫做把数字分解成正负部分。让我给你展示一些符号。考虑任意一个数A，只是一个实数。
- en: And we define the positive part of A。 We write it A with a plus and a superscript。
    So basically the either A， if A is positive， or zero if it's negative。 Or zero。
    So the positive part of 10 is 10。 The positive part of negative 10 is zero。 It's
    gotten a positive part。 Similarly with the negative part。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义A的正部分。我们写作A加和一个上标。所以基本上，A如果是正的，就是A，如果是负的，就是零。所以10的正部分是10。负10的正部分是零。它有一个正部分。同样地，对于负部分也是如此。
- en: A minus the negative part of negative 10 is 10。 The negative part of 10 is zero。
    So the negative part of a negative number is the absolute value of the number。
    And the negative part of a positive number is zero。 So this will be nice。 One
    thing that we'll get is that you can write A。 How would you write A in terms of
    A plus and A minus？
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: A减去负的负10是10。负10的负部分是零。所以负数的负部分是该数的绝对值。而正数的负部分是零。所以这样会很方便。我们得到的一件事是，你可以写出A。你怎么用A加和A减来表示A呢？
- en: A generic A。 It could be positive。 It could be negative。 Yeah， okay。 So you
    can decompose A into A plus minus A minus。 Because if it's positive。 A minus is
    zero and A plus is the right thing。 If it's negative， you got zero minus A minus。
    which is the right value of A。 Good。 So I'll just， well it'll show up on the next
    slide。 Similarly。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的A。它可以是正的，也可以是负的。对，好吧。所以你可以把A分解成A加减A减。因为如果A是正的，A减就是零，A加就是正确的。如果是负的，你就得到零减去A减，这就是A的正确值。很好。接下来会在下一页展示出来。类似地。
- en: what do we do for absolute value of A？ This is maybe even easier。 How do we
    write absolute value of A in terms of A plus and A minus？ Yeah， A plus plus A
    minus。 Because A plus and A minus are already not negative。 So we just add them
    together to get the absolute value。 Great。 Alright。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怎么处理A的绝对值？这可能更简单。我们怎么用A加和A减来表示A的绝对值呢？对，A加加上A减。因为A加和A减已经不是负数。所以我们只需要把它们加在一起，就得到了绝对值。好，太棒了。
- en: So we can apply this trick to lasso in a certain way。 So here's the last problem
    again。 Now what if we wrote every component of W in terms of A plus and W plus
    and W minus？
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以以某种方式将这个技巧应用到lasso上。这里又是最后一个问题。那么如果我们把W的每个组成部分写成A加、W加和W减的形式呢？
- en: So we say W i is equal to W i plus minus W i minus。 We basically take our vector
    of W's and make two new vectors。 the positive part of W and the negative part
    of W。 And then we say W equals the difference between those two。 Alright。 That's
    interesting。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们说W i等于W i加减W i减。我们基本上将W的向量分成两个新向量。W的正部分和负部分。然后我们说W等于这两者之间的差。好的。这个有意思。
- en: Let's try that。 Let's try plugging in that decomposition into this objective
    function。 And what about here？ We have this L1 norm of W。 So that's easy to write。
    The L1 norm of W is like the sum of the absolute values。 So you're going to have
    a sum of W plus and W minus。 Let's see how it looks。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来试试吧。我们来尝试将那个分解代入这个目标函数。那这里怎么样？我们有W的L1范数。所以这很容易写出来。W的L1范数就像是绝对值的和。所以你将会有W加和W减的和。让我们看看效果如何。
- en: I'm missing a transpose。 Anyway。 So we have the W part becomes W plus minus
    W minus。 And then the absolute value part。 Can you see what I've done wrong then？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我少了一个转置。无论如何。所以W部分变成了W加减W减。然后是绝对值部分。你能看到我做错了什么吗？
- en: Is this a vector or a number？ Or a scalar？ This is a vector still。 So what do
    I have to do？
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个向量还是一个数字？还是标量？这仍然是一个向量。那么我该怎么做呢？
- en: Say again？ That would give the sum of the squares。 Yeah。 We basically need to
    sum the entries of that。 So one transpose that， since that is a common true。 So
    what's nice now is we've written this in terms of there's no more absolute value。
    Alright。 So now this thing is differentiable。 We got rid of the kinky absolute
    value。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 再说一遍？那样会得到平方和。是的。我们基本上需要对它的条目求和。所以先对其进行转置，因为这是一个常见的操作。所以现在的好处是，我们已经将其写成了没有绝对值的形式。好的。现在这个函数是可微的。我们去掉了那个复杂的绝对值。
- en: The absolute value of the kink。 But we in exchange have to constrain W plus
    and W minus to be non-negative。 So we have a nice and smooth differential objective
    function。 But now we have a constraint on W plus and W minus。 We've also now instead
    of just one vector of size D， we've got two vectors。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对值的折点。但是我们必须对W加和W减施加非负约束。所以我们得到了一个平滑且可微的目标函数。但现在我们对W加和W减施加了约束。我们现在不再只有一个D维的向量，而是有两个向量。
- en: So we have a total of 2D variables。 Alright。 So that's the trade-off。 We have
    2D constraints and 2D variables。 But it's differentiable。 And before it was not
    differentiable， we had no constraints and we had half the number of variables。
    Okay。 But we know how to attack this。 We can do for instance， we'll see on the
    next slide。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们一共有2D个变量。好的。这就是权衡。我们有2D个约束和2D个变量。但它是可微的。而之前它是不可微的，我们没有约束，变量数量只有一半。好的。但我们知道怎么解决这个问题。比如，我们可以在下一张幻灯片上看到。
- en: So there's something called projected stochastic gradient descent。 So in projected
    stochastic gradient descent， it's just like stochastic gradient descent。 You take
    your point， you find your gradient of the objective with respect to that point。
    You go in the opposite direction， a certain step size。 That's the stochastic gradient
    descent piece。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有一种叫做投影随机梯度下降的方法。在投影随机梯度下降中，它和随机梯度下降类似。你先找到你的点，计算这个点关于目标函数的梯度。然后沿着相反的方向走，一定的步长。那就是随机梯度下降的部分。
- en: And then what's the projection part going to look like？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那投影部分会是什么样子的呢？
- en: What's the possible issue with the step I just took？ I might have violated the
    constraints。 I might have stepped out of the set satisfying the constraints。 In
    this particular problem。 that means I might have taken a step in a direction，
    that let W plus or W minus go negative。 And that violates the constraints。 So
    the projected part means we project that point where we ended up back into the
    constraint step。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才采取的步骤可能有什么问题？我可能违反了约束条件。我可能走出了满足约束的集合。在这个特定的问题中，这意味着我可能沿着某个方向走，使得W加或W减变为负值。这违反了约束。所以投影部分意味着我们将最终的点重新投影回满足约束的区域。
- en: And it's very easy in this setting。 How do we do that projection？ Yeah？
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中非常简单。我们怎么做那个投影？是吗？
- en: I will dial into the other part。 Okay。 That's tricky。 There's an even easier
    way to make sure we get back into it。 Yeah？
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我将拨打另一个部分。好的。这有点棘手。其实有一种更简单的方法可以确保我们回到正轨。是吗？
- en: Two of the terms that I'm getting？ Yeah， we can simply。 if we take a step and
    W plus suddenly becomes negative point five， we just bring it back to zero。 So
    we overshot and then we say， okay， back up。 That's the projection。 So for last
    sort of stochastic projective stochastic gradient descent is as easy as stochastic
    gradient descent。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到的两个项？是的，我们可以简单地。如果我们迈出一步，W 加上突然变成负五分之一，我们只需把它恢复到零。所以我们超调了，然后说，好吧，退回去。这就是投影。因此，对于最后一种随机投影随机梯度下降，它和随机梯度下降一样简单。
- en: plus a truncation when we cross zero。 All right。 Very good。 All right。 So that's
    one method。 easy to understand， easy to explain。 Let's try another method。 Coordinate
    descent。 So coordinate descent is a very general method。 It applies to many different
    objective functions。 So I'm going to present it a bit in general and we'll see
    how it applies to Lasso。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当穿越零时加上截断。好吧，非常好。好吧，这是其中一种方法，易于理解，易于解释。让我们尝试另一种方法。坐标下降。因此，坐标下降是一种非常通用的方法。它适用于许多不同的目标函数。所以我将稍微概述一下它，并看看它如何应用于
    Lasso。
- en: So let's suppose we have this function， general now， not just Lasso。 It's a
    function of a D dimensional vector， W1 through WD。 And recall that when you take
    a gradient step or stochastic gradient to step， we're potentially。 updating every
    entry of our parameter vector。 So we go the gradient direction and if that gradient
    has non-negative entries in all of its dimensions。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们现在有这个函数，一般来说，不仅仅是 Lasso。它是一个 D 维向量 W1 到 WD 的函数。回想一下，当你进行梯度步进或随机梯度步进时，我们可能会更新参数向量中的每一个条目。所以我们沿着梯度方向走，如果该梯度在其所有维度中都有非负条目。
- en: when we take a step， we're going to change every entry of that W。 Right？ Okay。
    So coordinate descent does something different。 In coordinate descent， in every
    step。 we only update a single entry， a single one of these， W's that we're optimizing
    over。 So how would that look？ We start， for instance， trying to optimize over
    W1。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们迈出一步时，我们将改变 W 的每一个条目，对吧？好吧。那么坐标下降做了不同的事情。在坐标下降中，每一步，我们只更新一个条目，即我们正在优化的这些
    W 中的一个。那么这看起来怎么样呢？我们首先从尝试优化 W1 开始。
- en: And what that would mean is we look -- this is the W1 direction， the coordinate
    direction。 And we say， all right， I need to minimize dysfunction along this line。
    Somewhere along this line。 I want to find the minimum， holding everything else
    fixed。 So I find the minimum in the W1 direction and that's one step of coordinate
    descent。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们看——这是 W1 方向，坐标方向。我们说，好吧，我需要在这条线上最小化这个函数。在这条线上的某个地方，我想找到最小值，同时固定其他所有东西。所以我找到
    W1 方向的最小值，这就是坐标下降的第一步。
- en: Now I go to my next direction， which is W2， which is this way。 And then I minimize
    our longest line。 So maybe I go all the way to here。 All right？ And now we do
    another coordinate。 So maybe that's W3 direction。 That might be this way。 Okay。
    So we iterate through the coordinate directions， each time freezing all the other
    things that we've seen。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我转到下一个方向，即 W2，也就是这个方向。然后我最小化我们最长的线。所以也许我一直走到这里。好吧？然后我们进行另一个坐标。所以这可能是 W3 方向。可能是这个方向。好吧。所以我们遍历坐标方向，每次都将其他已看到的东西冻结。
- en: and just optimizing over one of the directions。 All right。 So in math。 we write
    -- so the i-th coordinate， the new i-th coordinate that we stepped to。 is the
    minimum over that i-th coordinate of the objective function。 where everything
    except W_i is fixed to whatever it was before， and we optimize over W_i。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 仅对其中一个方向进行优化。好吧。那么在数学中，我们写作——所以第 i 个坐标，我们走到的新的第 i 个坐标，是目标函数在该 i 个坐标上的最小值，其中除了
    W_i 之外的所有变量都保持不变，我们对 W_i 进行优化。
- en: This is coordinate descent math person。 Yeah？ [ Inaudible ]， Okay。 Question。
    When does this actually work？ Great question。 We'll come back in a second。 All
    right。 So in this true， though， that I kind of left out some details。 When I say
    minimize long this direction， how do we do that？ Okay。 So maybe that's another
    iteration。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是坐标下降的数学，明白吗？[听不清]，好的。问题。当它实际上何时有效？很好的问题。我们稍后再回来。好吧。所以在这里，虽然如此，但我有点忽略了一些细节。当我说沿这个方向最小化时，我们是怎么做的呢？好的，也许这是另一轮迭代。
- en: Maybe we have to do steps to do that as well。 And that's -- think of that as
    a subroutine of this optimization method。 And it turns out that this is a good
    one。 This coordinate descent is good。 When that subroutine is easy， when it's
    easy to find this arg min for one coordinate at a time。 If that's an easy problem，
    coordinate descent is a good idea。 Turns out it's very easy for lasso。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们也必须采取一些步骤来实现这一点。这就——把它看作是这个优化方法的一个子程序。结果证明这是一个好方法。这种坐标下降法很好。尤其是在每次只需要找到一个坐标的最小值时，当这个子问题很容易解决时，坐标下降法就很有用。如果这个问题简单，坐标下降法是个好主意。对于lasso问题来说，事实证明它非常简单。
- en: That's why I bring it up。 All right。 So， yeah。 I wrote it here。 So coordinate
    descent is great when it's easy or easier to minimize。 with respect to one coordinate
    at a time。 Then to do the full gradient or the gas gradient or something。 Yeah。
    [ Inaudible ]， So you're asking how do we compute this arg min？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我提到它的原因。好了。所以，嗯，我在这里写了。坐标下降法非常有效，尤其是当相对于某个坐标逐一最小化变得容易或更容易时，而不是做完全梯度下降或加速梯度法之类的。所以你问的是如何计算这个`arg
    min`？
- en: Lots of different ways one can do it。 So -- but I -- no。 Your question specifically
    was。 do we just take a little bit of a step in the direction。 of the minimum along
    the line or do we go all the way to the minimum？ I think it's your question。 No。
    Well， wait。 Let me answer that question。 Which would have been a good question。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多不同的方式可以做到这一点。所以——但是我——不行。你具体的问题是，我们是只是沿着那条线朝着最小值方向迈出一小步，还是要走到最小值？我想这就是你的问题。不是。等等，让我来回答这个问题。这本来是个好问题。
- en: Which is that in the classic coordinate descent， you do the full minimization。
    So you pick a direction and you do your full search。 Maybe you have to go back
    and forth and you find the minimum in that direction。 And you're done。 This is
    the minimum in that one coordinate。 And then you do your minimum -- that's --
    that's the classic version。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的坐标下降法是做完全的最小化。所以你选择一个方向，然后进行完整的搜索。也许你需要来回走动，找到那个方向上的最小值。然后就完成了。这就是该坐标方向上的最小值。然后你继续进行最小化——这就是经典版本。
- en: There's also a version where you just take one step in a coordinate direction。
    So I do maybe one gradient step in this direction。 One step in this -- not a gradient
    step。 It's one step in direction -- so what would the gradient step be in one
    dimension？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种版本，你只是在某个坐标方向上迈出一步。所以我可能在这个方向上做一步梯度下降。不是梯度下降，是在这个方向上迈出一步——那么在一维中梯度下降会是什么样子呢？
- en: Which direction do I move？ What are my options for a gradient step in one direction？
    Yeah。 either go that way or I go this way。 And everything else is steps us。 So
    -- yeah。 Is this about the consistent with gradient descent？ I don't know what
    quiet that means。 If there's only one global minimum and they both find it， then
    they will find the same one。 Okay。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我该往哪个方向移动？在一个方向上，梯度下降的选项是什么？对。要么朝那边走，要么朝这边走。其他的方向是给我们提供的。所以——是的。这与梯度下降法一致吗？我不知道“安静”是什么意思。如果只有一个全局最小值，它们都能找到，那它们会找到相同的最小值。好的。
- en: Okay。 All right。 Okay。 Lots of questions。 Yes？ [ Inaudible ]。 Is this a version
    of batch gradient descent？ [ Inaudible ]。 There's nothing stochastic in this method
    that I presented。 So this is generic。 This is for any objective function you're
    optimizing。 If we applied it to LASO。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。好的。好了。有很多问题。是吗？[听不清]。这是不是一种批量梯度下降的版本？[听不清]。我所展示的方法中没有任何随机性。所以这是通用的。这适用于任何你优化的目标函数。如果我们将它应用于LASSO。
- en: it would be the full objective empirical risk plus regularization。 Yeah？ [ Inaudible
    ]， Yeah。 I mean， but we're not going to just do one path。 We'll cycle through。
    Okay。 All right。 So why do we bring up coordinate descent for LASO？
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它将是完整的目标经验风险加上正则化。对吗？[听不清]，对。我是说，但我们不会只走一条路径。我们会进行循环。好了。那么，为什么我们提到LASSO的坐标下降法？
- en: What's quite surprising is that for the LASO objective function， the minimizer
    in。 a coordinate direction has a closed form， an explicit expression you can just
    solve for。 mathematically。 So no iterative method to find that minimum， or minimum。
    along one coordinate direction。 It's pretty surprising。 I'll show you what the
    form looks like。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，对于LASSO目标函数，坐标方向上的最小化器有一个闭式解，即你可以直接求解的明确表达式。从数学角度来看，找到那个最小值并不需要迭代方法。沿着某个坐标方向的最小值非常简单，实在是令人惊讶。我会展示这个表达式的样子。
- en: and then as an optional homework problem， you can derive it。 We can -- we'll
    walk you through that。 So this is the whole expression。 This is -- so noted， this
    is the minimum over。 single coordinate WJ of the LASO objective function。 All
    right。 And it has this interesting form。 I'll just point out to -- well， I'll
    briefly speak， about this CJ。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后作为一个可选的家庭作业问题，你可以推导出这个结果。我们可以——我们会带你一步步分析。所以这是整个表达式。这是——请注意，这是单个坐标 WJ 对应的套索目标函数的最小值。好吧。它有这个有趣的形式。我将简要指出——嗯，我将简要讲解一下这个
    CJ。
- en: and then you can kind of study it more on your own。 So CJ， if you look in here。
    there's a W minus J transpose times xi minus J。 What is this？ This is saying。
    let's take our parameter vector W。 Let's leave out the jth entry。 Let's take our
    x coordinates and leave out the jth entry。 And let's take their inner product。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以自己进一步研究它。所以 CJ，如果你看这里，有 W 减去 J 的转置乘以 xi 减去 J。这是什么？这就是说，我们取我们的参数向量 W，省略第
    j 个条目。我们取 x 坐标并省略第 j 个条目。然后计算它们的内积。
- en: So that's like making a prediction using all of， our features except the jth
    one。 So this little term is， what's the， prediction leaving out the jth feature？
    And we're predicting on。 in this case， the， i-th data point。 And we're looking
    at the residual when we。 predict on the i-th data point without using the jth
    feature。 All right。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像使用所有特征进行预测，除了第 j 个特征。所以这个小项是，省略第 j 个特征时的预测是什么？我们正在对第 i 个数据点进行预测。我们在对第 i 个数据点进行预测时，查看未使用第
    j 个特征时的残差。好吧。
- en: And then we look at that residual and we -- see， we're taking like an inner
    product。 of it with that jth feature which we had left out of the prediction。
    And so this is roughly like how correlated is that jth feature with the。 residual
    when we predict things without the jth feature。 It's rough。 Okay。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看一下残差，我们——看，我们在计算它与我们在预测时遗漏的第 j 个特征的内积。所以这大致上是说，预测时未使用第 j 个特征，残差与第 j 个特征的相关性如何。大概是这样的。好。
- en: But this is what I think of when I see this。 All right。 In any case， so if that
    cj。 which I'm going to think roughly as how， correlated is the jth feature with
    the residual。 not using it， if that's small， so， it's not very helpful in fixing
    that residual。 then that coefficient gets mapped， to zero。 And otherwise it gets
    mapped to something else。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是我看到这个时的想法。好吧。无论如何，所以如果这个 cj，我将大致认为它是第 j 个特征与残差的相关性，不使用它时，如果它很小，那么它在修正残差时就没有什么帮助。那么这个系数就会被映射到零。否则它会被映射到其他值。
- en: That's all I'm going to say about this。 But you'll have a chance to look at
    it more， carefully。 So the takeaway is lasso seems free promising for。 quarter
    descent because that minimization that we do in each quarter direction has。 an
    explicit form。 We could just code up directly。 So it has a sense that it will
    be， very fast。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我要说的全部内容。但你将有机会更仔细地看它。所以要点是，套索回归似乎对四分之一下降法非常有前景，因为我们在每个四分之一方向上进行的最小化有明确的形式。我们可以直接编写代码实现。所以它具有非常快速的潜力。
- en: Okay。 So the question -- one question was， one does this thing work？ It's a
    great question。 So here's a first kind of the easy sufficient conditions。 Turns
    out it's not enough for lasso though。 So if you're minimizing an f from rd to
    r。 sufficient conditions for this method to work is that f is continuously。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么问题——一个问题是，这个方法到底有效吗？这是一个很好的问题。所以这里是第一个简单的充分条件。结果发现对于套索回归来说，这还不够。所以如果你在从
    R^d 到 R 的函数 f 上进行最小化，确保这个方法有效的充分条件是 f 是连续的。
- en: differentiable and it's strictly convex in each coordinate。 So if you fix all
    the other coordinates and you look at just the -- just as a， function of wj。 for
    instance， it's got to be strictly convex in that。 We'll talk about -- we'll get
    into -- we'll review what convexity is maybe later or next week。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它是可微的，并且在每个坐标上是严格凸的。所以如果你固定其他坐标，只看——仅仅作为 wj 的函数来看，它一定是严格凸的。我们会讨论——我们稍后或下周会复习什么是凸性。
- en: So what's -- what's goes wrong with the lasso objective here that doesn't meet
    these， criteria？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，套索目标函数在这里出了什么问题，未能满足这些标准呢？
- en: I've already mentioned it。 It's very --， obviously， it's differentiability。
    Yes。 it's not differentiable。 So criterion one is， violated。 So this is nice and
    easy to use in sub-situations when it's true， but it's not， for lasso。 So there's
    a more fancy theorem which does apply。 So this is an interesting， theorem。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经提到过了。它很明显，是可微性。是的，它是不可微的。所以第一个标准被违反了。所以在它成立的子情况中，这个方法很好用，但对于套索回归不适用。所以有一个更复杂的定理适用。这是一个有趣的定理。
- en: So it turns out if the objective function f that we're minimizing decomposes。
    in a certain way or in good shape， basically it's that it decomposes into two
    pieces， one。 of which is a function which is differentiable and convex。 So g is
    like the nice part。 differentiable and convex。 So an example would be aeroimpirical
    risk， right？ That's a sum。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以结果是，如果我们要最小化的目标函数f可以分解成某种方式，或者说它有好的形式，基本上就是它分解成两部分，其中一部分是可微且凸的函数。所以g就是那部分，既可微又凸。所以一个例子就是经验风险，
    right？它是一个和。
- en: with the square loss。 The sum of the square losses， that's differentiable and
    it's， convex。 And then this other piece with the sum over hj's acting on one coordinate
    at， a time， all right？
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用平方损失。平方损失的和是可微的并且是凸的。然后这一部分，作用于每个坐标一次的hj的和，好吗？
- en: And each of these hj's only have to be convex。 It doesn't have to be， convex。
    It doesn't have to be differentiable。 So how does this apply to our lasso？ Yeah？
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些hj每个只需要是凸的。它不需要是，凸的。它不需要是可微的。那么这怎么适用于我们的套索回归呢？是吗？
- en: '[ Inaudible Remark ]， Yeah。 This thing is exactly the form of the l1 penalty。
    The l1 penalty is the sum of， the absolute value of each coordinate， all right？'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 听不清楚的备注 ]， 是的。这个东西正是l1惩罚项的形式。l1惩罚项是各个坐标的绝对值之和，好吗？'
- en: So hj is an absolute value function， for the lasso in case。 And yes。 it's only
    working on one coordinate。 So this is almost， perfect for the lasso scenario。
    So this justifies using lasso coordinate descent， for the lasso。 All right。 Any
    questions on that？
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以hj是绝对值函数，适用于套索回归的情况。是的。它只作用于一个坐标。所以这几乎是为套索回归场景量身定做的。所以这就为使用套索坐标下降法提供了合理性。好的。对此有任何问题吗？
- en: Why does this justify the use of the lasso？ Simply that if you recall the lasso，
    we have this piece。 which is differentiable and， convex。 And this piece， if you
    write it out。 it's the sum of the absolute values of， wi。 And that's of this form
    where hj is the absolute value function。 Okay。 All right。 So just， a side note。
    Yes？ Can you repeat it？ Can you repeat it？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这能证明使用套索回归是合理的？简单来说，如果你回忆一下套索回归，我们有这一部分，它是可微的，并且是凸的。还有这一部分，如果你写出来，它是各个wi的绝对值的和。它的形式是这样的，其中hj是绝对值函数。好的。好吧。只是一个附带说明。是吗？你能重复一遍吗？你能重复一遍吗？
- en: '[ Inaudible Remark ]， So the ridge regression。 I don''t know。 That''s a good
    question。 If there''s an advantage， I''m not sure。 It might be worth a try。 That''d
    be interesting。 I think it would depend on the specifics。 What''s the dimension
    of your space， for， example？'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 听不清楚的备注 ]， 所以是脊回归。我不知道。这是个好问题。如果有优势，我不确定。可能值得尝试。那会很有意思。我觉得这可能取决于具体情况。例如，你的空间维度是多少？'
- en: So if your space is very high dimensional， you have to do a lot of -- you。 have
    to go through a lot of coordinates before you finish one pass。 I don't know。 Yeah。
    It's a good question。 All right。 So what about coordinate descent when we don't。
    have this beautiful thing with the explicit closed form expression for the。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你的空间是非常高维的，你必须做很多——你必须遍历很多坐标才能完成一次遍历。我不知道。是的。这个问题很好。好的。那么当我们没有这个漂亮的东西，且没有明确的闭式解时，坐标下降法怎么办？
- en: minimizer in the coordinate direction？ Like what do we do then？ So do we really
    have。 to go all the way to the minimum in the coordinate direction before we stop？
    Can we。 just kind of go a little bit in that direction and then go to the next，
    coordinate？
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标方向上的最小化器？那我们该怎么做呢？所以我们真的需要在坐标方向上走到最小值才能停下来吗？我们能不能在那个方向上稍微走一点，然后再去下一个坐标呢？
- en: So it turns out that for lasso regression， the answer is yes， and， there's some
    paper proving that。 So that's interesting。 We can combine the two， things we've
    been talking about。 So we have -- remember， this is the， differentiable version
    of our lasso objective。 which we -- since it's differentiable， we can go ahead
    and do gradient descent or stochastic gradient descent on it with。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以结果是，对于套索回归，答案是肯定的，并且有一些论文证明了这一点。很有意思。我们可以结合我们所讨论的这两个问题。所以我们记得，这是我们套索目标函数的可微版本。由于它是可微的，我们可以继续在其上执行梯度下降或随机梯度下降。
- en: the constraint。 Or we can try coordinate descent on it， but we can use a modified。
    version where we choose a coordinate and we take the gradient with respect to
    only。 one coordinate at a time and do just one step in that direction and then
    move on。 to the next coordinate。 I just wanted to mention this。 I don't want to
    go into。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个约束。或者我们可以尝试对其进行坐标下降，但我们可以使用一种修改版的方法，在这种方法中，我们选择一个坐标，只针对一个坐标计算梯度，沿该方向执行一步，然后再转到下一个坐标。我只是想提一下这个，不想深入讨论。
- en: too much detail because we have some other stuff to cover， but I just wanted
    to put。 this on the side。 You can take a look later ask questions if you want。
    All right。 So that wraps up the optimization -- the notes I wanted to give on，
    optimization for lasso。 And then you'll have some implementation of that to do
    on， homework two。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要太多细节，因为我们还有其他内容要覆盖，但我只是想把这个内容放在一旁。你可以稍后查看，如果有问题可以问。好了，这就是我想要讲的关于Lasso优化的内容。接下来你将在第二次作业中做一些实现。
- en: So you'll get a chance to actually dig into that。 All right。 Okay。 So now as
    promised。 we're going to go a little bit deeper to， understanding lasso and ridge
    and how they compare understanding their properties。 So let's start with a really
    simple model。 Another kind of like a thought， experiment。 So suppose we have a
    single feature， x1， all right。 And we have a。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你将有机会深入了解这一点。好了，好的。现在，正如我们所承诺的，我们将更深入地理解Lasso和Ridge，以及它们如何比较，理解它们的性质。让我们从一个非常简单的模型开始。又是一个类似于思想实验的例子。假设我们有一个单一特征x1，好吧。然后我们有一个。
- en: response variable and out the variable y， both reals。 And suppose we get some
    data， you know。 xy pairs， it's just two columns。 Very simple。 And we run least
    squares。 regression and we find that the empirical risk minimizer is a -- because
    we were。 restricting linear functions -- is simply f hat x equals four times x1。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 响应变量和输出变量y，都是实数。假设我们得到一些数据，你知道，xy对，只是两列。非常简单。然后我们运行最小二乘回归，发现经验风险最小化器是——因为我们限制了线性函数——就是f
    hat x等于四倍的x1。
- en: That's our empirically best linear predictor of y given x。 All right。 Very，
    simple。 All right。 So that's nice。 And now here's the question I posed。 What happens
    if we introduce a new feature。 a second column， x2， but x2 is， always exactly
    equal to x1。 All right。 So I claim that we have no new， information。 x2 is exactly
    x1。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们根据x预测y的经验最佳线性预测器。好了，非常简单。好了，这很好。现在这是我提出的问题。如果我们引入一个新的特征，一个第二列x2，但x2总是等于x1。好了，我声明我们没有新的信息。x2就是x1。
- en: So the empirical risk minimizer we had before is， still the best。 We're still
    not going to be able to beat four times x1 in our。 prediction because x2 tells
    us nothing。 All right。 Sure。 But what's different is we。 actually have more options
    now。 Now we have two things。 And， you know， 2x1 plus 2x2。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们之前的经验风险最小化器，依然是最好的。我们依然无法在预测中击败四倍的x1，因为x2对我们没有任何帮助。没错，当然。但不同之处在于，现在我们有了更多的选择。现在我们有两个项。你知道，2x1加2x2。
- en: gives the same predictions， right？ Because 2x1 and x2 are equal， so that sums
    to 4x1， same thing。 So there's lots of ways to bring -- to come up with an empirical。
    risk minimizer with the exact same predictions。 All right。 So this is true for。
    straight empirical risk minimization。 But what happens if we introduce L1。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 给出相同的预测，对吧？因为2x1和x2是相等的，所以它们加起来就是4x1，结果是一样的。所以有很多方法可以得出——得出一个经验风险最小化器，预测结果完全相同。好了，这对于直接的经验风险最小化来说是成立的。但如果我们引入L1呢？
- en: regularization or L2 regularization？ How does that play？ All right。 So first
    of all。 anything -- and this is obvious -- anything where W1 and W2， the coefficients
    of x1 and。 x2 sum to 4 is the same function， exactly the same function， when x1
    and x2 are equal。 All right。 So let's look at some different values of coefficients
    we can use for W1， and W2。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化或L2正则化？这又是怎么发挥作用的呢？好了，首先，任何——这很明显——W1和W2，x1和x2的系数和为4的地方，都是相同的函数，完全相同的函数，当x1和x2相等时。好了，接下来我们来看一下可以为W1和W2使用的不同系数值。
- en: And let's look at the corresponding L1 norm and L2 norm of those coefficients
    to get a。 sense for what happens。 So all of these sum to 4。 And check out the
    L1 norm。 And this is just some。 of the absolute values。 So you just matter adding
    4 plus 2s or 2 plus 2s 4。 Mine's 1 plus -- all right。 So here's an interesting
    one。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些系数对应的L1范数和L2范数，以便更好地理解发生了什么。所以这些数值的和为4。再看一下L1范数，这只是一些绝对值的和。所以你只需加上4加2或2加2就等于4。我的就是1加——好了，这里有一个有趣的例子。
- en: So L1 norm is a sum of the absolute values。 So this is 1 plus 5 is 6。 So check
    out that the L1 norm is the same for all of these solutions when they have the
    same。 sign and starts to grow in at different signs。 All right。 So this is --
    this is the characteristic of， the L1 norm for a bunch of solutions where the
    coefficients have the same sum。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以L1范数是绝对值之和。所以这是1加5等于6。所以请注意，当这些解的符号相同且开始以不同符号增长时，L1范数在所有这些解中是相同的。好的。所以这是——这是L1范数的特性，适用于那些系数和相同的解。
- en: All right。 So in words， what I draw -- the conclusion I would draw from this
    is that if we were to use an L1。 penalty on least squares -- in least squares
    regression -- and we had two features that were。 identical， L1， we didn't care
    which one you select as long as they're the same size。 It'll penalize。 when they're
    different。 Same sign。 It'll penalize when they're different sign。 Okay。 So that's
    L1。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么用文字来表达，我画的——从这得出的结论是，如果我们在最小二乘回归中使用L1惩罚——并且我们有两个完全相同的特征——L1时，我们不在乎选择哪个，只要它们的大小相同。它会惩罚。它们不同的情况。当符号相同的时候，它会惩罚它们不同符号的情况。好的。那就是L1。
- en: Now let's go to L2。 So L2， you'll see it's minimized when they're equal， wait，
    2 and 2。 So is that -- is that， obvious why that would be the case？
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看L2。所以L2，你会发现当它们相等时，最小化，例如，2和2。所以这是——这个为什么是这样的，有没有显而易见的原因？
- en: So you could actually show this using math again。 That if you have a bunch of
    numbers that sum to a fixed total and you want to minimize the sum of。 their squares，
    which is the L2 norm， that happens when they're all equal。 So you can show that
    with。 Leung-Rin multipliers。 We can also show it with a picture if you guys want
    to see that。 Okay。 So let's have two coefficients。 Let's call them W1 and W2。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你实际上可以再次用数学证明这一点。如果你有一堆数，它们的和是一个固定的总数，而你想要最小化它们的平方和，也就是L2范数，那么当它们都相等时就会发生这种情况。你可以通过Leung-Rin乘子来证明这一点。如果你们想看的话，我们也可以用图像来展示。好的。那么我们来有两个系数。我们称它们为W1和W2。
- en: And we want the set of all W1 and W2 that， have W1 plus W2 equal 4。 Those are
    all the ERMs。 So the set of ERMs is equal to the set of W1 and W2， such that，
    they sum to 4。 What's that set look like in this plane？ Yeah， I saw someone did
    this。 That's correct。 So this is the set。 All right。 And what's the level set
    of the L2 norm look like？ It's the circle。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要找出所有W1和W2的集合，使得W1加上W2等于4。这些就是所有的ERM。所以ERM的集合等于所有W1和W2的集合，满足它们的和为4。这个集合在这个平面上是什么样子的？是的，我看到有人做了这个。这是正确的。所以这是这个集合。好的。那么L2范数的水平集是什么样子的？它是一个圆。
- en: right？ We talked about the S2。 Okay。 So the key point here is that the empirical
    risk minimizers。 as we -- so this is the minimum loss。 This is the ERM。 And remember。
    just as we did last time with the L1 regularization and the L2 regularization，
    we said， well。 the ERM is here， but it doesn't satisfy this L2 constraint。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对吗？我们讨论了S2。好的。那么这里的关键点是经验风险最小化器。正如我们所说——这是最小损失。这就是ERM。记住——就像我们上次讨论L1正则化和L2正则化时所说的那样，我们说过，ERM在这里，但它不满足这个L2约束。
- en: We need to gradually allow ourselves to have worse loss。 which means moving
    away from this empirical risk。 But it's going to be parallel to this line。 And
    so eventually it'll hit right at that 45 degree point， which is where W1 and W2
    are equal。 All right。 Some people bought that。 Any questions on that？ Yeah。 [
    Inaudible Remark ]， Okay。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要逐步允许自己有更差的损失。这意味着要远离这个经验风险。但它将与这条线平行。所以最终它会正好落在那个45度点上，也就是W1和W2相等的地方。好的。有人理解了吗？有问题吗？是的。[听不清的发言]，好的。
- en: So question。 L2 gives a split between the two， an equal split between -- and
    weight between W1 and W2。 The first entry has all the weight on W1。 Question，
    which one do we prefer？
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 那么问题来了。L2在W1和W2之间进行均等分配。第一个条目将所有权重集中在W1上。问题是，我们更倾向于哪个？
- en: You were suggesting the first one because of sparse or something， right？
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你是因为稀疏性之类的原因建议第一个的吗？
- en: And then -- so last -- yesterday I gave a slide of all the reasons that one
    might prefer sparsity。 What if none of them apply to me？ I don't like sparsity。
    I'm not that into sparsity。 Baby。 But there's a serious reason why we might actually
    prefer the second one。 So we'll come to that in a little while。 All right。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后——所以昨天我给了一个关于为什么有人可能更倾向于稀疏性的幻灯片。如果这些理由都不适用我呢？我不喜欢稀疏性。我对稀疏性不感兴趣。宝贝。但实际上，我们可能更倾向于第二种方法的一个严肃理由。稍后我们会讨论这个问题。好的。
- en: So the takeaway of the summary here is that when you put a L2 penalty and you
    have a lot of features that are exactly equal。 then the L2 penalty will cause
    the weights to spread out exactly equally among the features that are equal。 Now
    one penalty doesn't care how they get distributed as long as they have the same
    sum and they're all the same sign。 All right。 So in practice， features are not
    exactly equal， typically。 But they're correlated。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里总结的要点是，当你添加L2惩罚并且有很多特征完全相等时，L2惩罚会导致权重在这些相等的特征之间平均分配。现在，单一的惩罚并不关心它们如何分配，只要它们的和相同且符号相同就可以。明白了吗？好。实际中，特征通常不完全相等，但它们是相关的。
- en: They can be highly correlated。 That happens。 And the point is that these same
    things that are for sure true when they're exactly equal are also roughly true
    when they're highly correlated and not exactly equal。 Okay。 So in the practical
    case， suppose you have five features that are pretty correlated。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可能高度相关。这是可能发生的。关键是，这些完全相等时肯定成立的规律，在特征高度相关且不完全相等时大体也成立。好的。那么在实际情况下，假设你有五个特征，它们之间有很强的相关性。
- en: not exactly the same。 When you do an L1 regularization。 it may tend to select
    just one or a couple of those variables。 It may put something zero。 It may kind
    of seemingly randomly assign different weights to each as long as they have the
    same sum。 Whereas an L2 regularization on the same set of say five very correlated
    features will very definitively kind of spread that weight pretty evenly across
    five features。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 并不完全相同。当你做L1正则化时，它可能会倾向于选择其中一个或几个变量。它可能会将某些变量的权重设为零。它可能会看似随机地将不同的权重分配给每个特征，只要它们的和相同。而对于一组非常相关的特征（例如五个特征），L2正则化则会非常明确地将权重均匀地分配给这五个特征。
- en: That's an important thing。 I think it's a very important thing to remember about
    L1 and L2。 This is this property。 All right。 So what I'm about to show is some
    experimental data。 what kind of simulated data that shows this in practice。 So
    I'll set up this model for you。 So here。 the true model is that Y is a linear
    combination of Z1 and Z2。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很重要的点。我认为这是关于L1和L2正则化需要记住的一个非常重要的特性。这就是这一特性。好的。接下来我要展示一些实验数据，这些模拟数据展示了这一点在实际中的应用。所以我会为你设置这个模型。这里，真实模型是Y是Z1和Z2的线性组合。
- en: But we don't observe Z1 and Z2 directly。 Instead， we get three noisy observations
    of Z1 and three noisy observations of Z2。 All right。 So the noisy observations
    of Z1 are all very correlated。 And they are near one thing。 And noisy observations
    of Z2 are also correlated and near another value Z2。 Is that clear？
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们并不直接观察Z1和Z2。相反，我们得到了Z1的三次噪声观察和Z2的三次噪声观察。好的。所以，Z1的噪声观察值是高度相关的，并且接近某个值。Z2的噪声观察值也是相关的，并且接近另一个值Z2。明白了吗？
- en: And we're going to observe the noisy observations， not the z's。 but the noisy
    observations of the z's。 And we're going to need to go back and predict Y。 Is
    that setting clear？ All right。 Let's look at that in math。 So let's suppose Z1
    and Z2 are drawn from gouge distributions independently。 And then once we。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将观察到的不是Z，而是Z的噪声观察值。我们需要回到预测Y。这个设置清楚了吗？好。我们来看一下数学表达。假设Z1和Z2是从独立的高斯分布中抽取的。然后一旦我们...
- en: and then we have， let's give ourselves six noise variables。 six independent
    gouge noise variables to play with。 And then we'll say Y is this nice linear combination
    of the Z's plus some noise。 We don't observe the Z's， but if we did， this is the
    exact expression for the Y's。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有六个噪声变量，六个独立的高斯噪声变量可以使用。然后我们说Y是这些Z的线性组合加上一些噪声。我们不能直接观察到Z，但如果我们能观察到，它就是Y的确切表达式。
- en: And then the X's are either each X， we'll say X goes from 1 to 6。 Each X is
    either Z1 plus some noise or Z2 plus some noise。 Picture that？ Okay。 Great。 So
    here's what we did。 We generated 100 samples of X and Y， of X and Y from this
    distribution。 We generated Z's too， but we don't observe those。 And then just
    to give you a sense。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这些X要么是每个X，我们说X从1到6。每个X要么是Z1加上一些噪声，或者是Z2加上一些噪声。理解了吗？好。那我们做了什么呢？我们从这个分布中生成了100个X和Y的样本。我们也生成了Z，但我们并不直接观察它们。然后给你一个概念。
- en: the correlations among the X's in each of these two groups were quite high，
    0。97。 It's a typical correlation。 Alright。 So what we're going to look at is the
    regularization paths as we fit this data with lots of regression with different
    amounts of regularization。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个组中的X之间的相关性相当高，0.97。这是一个典型的相关性。好的。那么我们要看的是，在我们使用不同程度的正则化拟合这些数据时的正则化路径。
- en: Okay。 So here's what we have。 Let's start all the way on the right。 which is
    where we have no regularization at all。 So we have， we should have。 we still have
    some regular。 That's interesting。 Okay。 So we have six variables。 Alright。 And
    their values after lots of regression with no regularization have， I say， no。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，看看我们有什么。我们从最右边开始，这是我们没有任何正则化的地方。所以我们有，应该有。我们仍然有一些常规的。那很有意思。好的，我们有六个变量。好了，它们在经过大量回归后，且没有正则化时的值，我说，没有。
- en: there's most of some。 So I got this from the book and I think there might be
    an error actually。 I emailed the author and I thought it was close enough， but
    now I'm questioning。 But basically。 what's， this is roughly correct。 So the two
    groups are in yellow and in blue。 And they are totally correlated。 So you kind
    of would。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这大致是这样。所以我从书中看到这个，我觉得可能实际上有个错误。我发了邮件给作者，我认为它差不多，但现在我开始质疑了。不过基本上，这是大致正确的。所以这两个组分别是黄色和蓝色。它们是完全相关的。所以你可能会这样做。
- en: you kind of want them to have equal weighting potentially， or， but in this case。
    LASO doesn't do that at all。 So it's， this is showing kind of LASO's indifference
    to where it puts the weight across the groups of variables that are correlated。
    Alright。 So I claim this is not a good outcome。 I claim that in a lot of situations。
    we prefer an even split of the weights across the things that are correlated。
    So particular。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望它们具有相等的权重，或者，在这种情况下，LASO根本没有做到这一点。所以，它显示了LASO对于在相关变量组中分配权重的漠不关心。好了，我声称这是一个不好的结果。我声称，在许多情况下，我们更倾向于在相关的事物之间平均分配权重。所以，特别是。
- en: what I think happened here is that the third variable， which is not even。 we
    don't see two blue variables， right？ So the third one is zero all the way。 Okay。
    So the reason I'm wondering if this is incorrect is because with no regularization。
    it's very unlikely for any of those to actually be exactly zero。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这里发生的情况是，第三个变量并不完全是。我们没有看到两个蓝色变量，对吧？所以第三个变量完全为零。好的。我之所以怀疑这是不正确的，是因为没有正则化的情况下，任何一个变量的权重实际上都不太可能是完全零的。
- en: That's why I think something's a little off here。 So I'm not quite sure。 What's
    going on。 but it's close enough to make the point。 Alright。 So can anyone give
    some thoughts on why we。 why I'm concerned that this variable has， for instance，
    zero weight。 even though it's no different than the other two in terms of what
    they represent informationally。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我认为这里有点问题的原因。所以我不太确定发生了什么，但它足够接近，可以阐明问题。好了，谁能给点意见，为什么我担心这个变量，比如说，权重为零，尽管它在信息表示上与另外两个没有区别。
- en: They all have the same amount of noise of the original variable。 Why should
    we ignore one of them and keep weights for the other two？ Why might this be an
    issue？
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都有与原始变量相同量的噪声。为什么我们要忽略其中一个而保持另外两个的权重呢？为什么这可能是个问题？
- en: Yeah。 Anyone？ As a beginner， trying to avoid overfitting is like in fact what
    you're trying to do here and put all the weight on one variable instead of splitting
    it more evenly。 That might be to a lot of like variance of noise when it's exactly
    what we're trying to avoid。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，有人吗？作为初学者，避免过拟合实际上就是你在这里尝试做的事，把所有的权重都放在一个变量上，而不是更均匀地分配它。这可能会导致很多方差和噪声，而这正是我们要避免的。
- en: So putting all the weight on one variable， that might lead to you saying like
    variance and noise and you said overfitting。 So maybe I think that's in the right
    direction。 Yeah。 [inaudible]， Yes。 Our best estimator of the Z's would average
    all of the elements in each group。 That's right。 That's the main point I have
    in mind when I say I like to spread the weight evenly。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，把所有的权重都放在一个变量上，这可能会导致你说方差和噪声，甚至过拟合。所以也许我认为这就是正确的方向。是的。[听不清]，是的。我们对Z的最佳估计应该是平均每组中的所有元素。没错。这就是我在说我喜欢均匀分配权重时的主要观点。
- en: So let's think about that。 We really want to know Z which is the un-noise version
    of each of these yellow ones。 That would be Z1 and each of these blue ones would
    be Z2。 The best way to get rid of noise when you have three things that are noisy
    observations of the same thing is to average them。 [inaudible]， If you have three
    people taking independent measurements of the same thing and would you rather
    just take one of them or would you rather take off three and average them？
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们来考虑一下这个问题。我们真正想知道的是Z，也就是每个这些黄色变量的无噪声版本。这将是Z1，而这些蓝色的变量则是Z2。当你有三个都是同一事物噪声观测值时，去除噪声的最佳方式是对它们进行平均。[听不清]，如果有三个人独立测量同一事物，你更愿意只取其中一个，还是更愿意取三个并进行平均？
- en: Certainly you want to average them。 It's a much， it's an estimate with a much
    lower variance。 So in the regression setting， spreading your weight evenly across
    all the variables that have essentially the same information gives you a lot more
    robustness。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你希望对它们进行平均。这是一种估计，且方差更低。因此，在回归设置中，将权重均匀分配到所有本质上具有相同信息的变量上，可以提高鲁棒性。
- en: So suppose instead you said， oh， they all have the same information。 Let's be
    sparse and just put all the weight on X1。 Well。 what happens if X1 for some reason
    had a lot of noise one day and your prediction would be totally off。 Whereas if
    you took all three X1， X2 and X3 and gave them equal weight。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，假设你认为它们都有相同的信息。那么我们就可以保持稀疏性，只把所有权重放在X1上。那会发生什么呢？如果某一天X1因为某些原因有很多噪声，而你的预测就会完全偏离。相反，如果你对X1、X2和X3三个变量赋予相等的权重。
- en: which would do the same predictions typically on average， and then if X1 had
    an error。 a particularly large noise， it would kind of be averaged in with the
    other two observations and it would kind of smooth that noise out。 Yeah？ [inaudible]，
    Well， basically the question is does this make it difficult to find the important
    features？
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通常会做出相同的预测，平均来看，如果X1出现错误，特别大的噪声，它就会和其他两个观测值一起被平均掉，噪声也会被平滑掉。对吧？[听不清]，嗯，基本上问题是，这样做是否会让找到重要特征变得困难？
- en: It depends what you mean by important features because basically what this is
    doing is it's。 what I'm proposing would be nice to have done， is that if there's
    kind of a group of features that all show the same thing。 then it's the group
    that's important。 There's no one that's more important than the others。 So I would
    like to select the whole group or leave out the whole group。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你对“重要特征”的定义，因为从根本上说，这个方法的作用是这样的。我所提议的，理想情况下是，假如有一组特征都展示了相同的信息，那么这组特征就是重要的。没有哪个比其他更重要。所以我希望选择整个组，或者排除整个组。
- en: If you have other reasons to want sparsity， like， you know。 you want to reduce
    memory or you want to reduce the time。 you don't want to carry around all these
    features， that's kind of another motivation。 Then you might want to keep only
    one of the batch。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有其他原因需要稀疏性，比如，你想减少内存消耗，或者减少时间开销，或者你不想携带所有这些特征，那么这是另一种动机。那时，你可能只想保留一批特征。
- en: But if your goal is to make the best predictions possible and maintain robustness
    to noise and get the advantage of averaging out over multiple noisy versions of
    the same thing。 then you should keep all of the ones in the same group。 Yeah？
    [ Inaudible ]， Was court set not used？
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你的目标是做出最好的预测，保持对噪声的鲁棒性，并通过对多个噪声版本的平均来获得优势。那么你应该保留同一组中的所有变量。对吧？[听不清]，法院集没有被使用吗？
- en: I don't see the connection。 Maybe we could talk offline。 Okay。 Yeah？ [ Inaudible
    ]， Okay。 So the question， I think what you're saying is it possible that one of
    the variables is maybe less noisy than the others？
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我看不出这之间的联系。也许我们可以私下再聊聊。好的。是吗？[听不清]，好的。那么问题是，我认为你在说的是，是否有可能其中一个变量比其他变量的噪声小？
- en: Yeah。 And that when you combine it with the ones that are more noisy。 then you're
    actually taking a penalty for that。 Yes。 I hear what you're saying。 And yeah。
    in some sense that is also an issue。 So in this pure setting。 they all have the
    same amount of noise by definition。 So this is not an issue。 But in practice，
    yes。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。当你将其与噪声更多的特征结合时，你实际上是在为此付出代价。是的，我听懂了你的意思。是的，在某种程度上，这确实是一个问题。在这个纯粹的设置中，所有变量的噪声量是相同的，因此这不是问题。但在实际中，确实会有这样的情况。
- en: it's possible that some predictors are more noisy than others。 even though they're
    all kind of around the same mean， for instance。 So in that case。 it's a trade-off。
    And that's okay。 We're finding methods that kind of have different properties。
    And then in practice， you try them out and you test them on validation error，
    validation data。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能某些预测变量比其他变量更嘈杂，即使它们的均值差不多。例如，在这种情况下，这是一个权衡。而且这没关系。我们正在寻找具有不同特性的不同方法。然后在实践中，你可以尝试它们，并通过验证误差、验证数据来测试它们。
- en: and you choose which actually performs the best。 Yep。 Good question。 Okay。 All
    right。 So the question is， how can we get the weight spread out more evenly？ So
    here， this is last。 so it doesn't spread the weight out evenly。 What can we do
    to get it spread out more evenly？
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以选择哪种方法表现得最好。对。好问题。好的。那么问题是，我们如何让权重分布得更均匀呢？这里，这是最后的结果。所以它没有均匀分布权重。我们能做什么来让它分布得更均匀呢？
- en: Any thoughts on that？ Yeah。 Okay。 So what if we use L2 regularization in addition
    to L1 regularization？
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对此有什么想法吗？嗯。好的。那么如果我们在 L1 正则化的基础上再加入 L2 正则化会怎样呢？
- en: So L1 doesn't care at all how we divide the weight， right？ But L2 cares。 So
    if you just threw in a dash， a little bit of L2 regularization on top of the L1
    regularization。 if in the case of exactly equal features， that would already break
    the tide。 And it would just put in a little L2 regularization in this setting，
    it would use 2/2 right away。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 L1 完全不关心我们如何划分权重，对吧？但 L2 是关心的。所以如果你在 L1 正则化上再加一点 L2 正则化的话，如果在特征完全相等的情况下，这就会打破平衡。它只会在这种设置中加入一点
    L2 正则化，立刻使用 2/2。
- en: It would use the one where the weight spread equally。 So that's the idea of
    elastic net。 which is where we have an L1 and an L2 penalty。 And the idea here
    is that what you're hoping for is that groups of variables enter kind of together
    all at once。 and have roughly even weights。 And then maybe the next group that's
    the second most useful will enter and become nonzero。 and essentially the same
    time with roughly equal weights。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 它会选择权重分布均匀的方法。所以这就是弹性网的理念，结合了 L1 和 L2 惩罚。这里的想法是，你希望变量的组能一起进入，并且权重大致相等。接着，可能是下一个组，它是第二个最有用的，将会进入并变成非零值，并且基本上同时以大致相等的权重进入。
- en: So let's see what happens on our same data set with some LASO。 There's a theorem
    I'll come back to。 So on the right we have elastic net curves and on the left
    we have pure LASO。 So on the right is a LASO plus a little bit of L2 regularization。
    And what you see is， yeah indeed。 like all these three coefficient values in the
    different groups are quite tightly spaced。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们看看在相同数据集上使用一些 LASSO 会发生什么。这里有一个定理我会再回来讨论。所以右边是弹性网曲线，左边是纯 LASSO。右边是 LASSO
    加上少量的 L2 正则化。你会看到，确实如此，像这些不同组中的三个系数值之间是紧密排列的。
- en: So it did manage to keep them with similar weights on the correlated variables。
    which was kind of the objective we're going for。 So the sense is that this elastic
    net will give you prediction functions that are more robust to little errors。
    because in a particular group， because you're at least kind of using a LASO。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它确实设法保持了相关变量之间的权重相似。这也正是我们追求的目标。所以意思是，这种弹性网方法会给你更加稳健的预测函数，因为在某个特定的组中，至少你在使用
    L1 正则化。
- en: '![](img/8074d5ece37ab440fb724f0e80292e6c_4.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8074d5ece37ab440fb724f0e80292e6c_4.png)'
