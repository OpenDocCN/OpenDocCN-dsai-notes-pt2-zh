- en: P97：97. L18_5 Text Preprocessing - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P97：97. L18_5 文本预处理 - Python小能 - BV1CB4y1U7P6
- en: Okay。 So now that we've seen how basic text statistics look like。 let's look
    at how we actually would want to process text。 And so the first thing we need
    to do is we need to tokenize text。 So by tokenization， I mean， well。 we turn every
    word or every， you know， simple into an idea and so that text turns into a sequence
    of integers。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么现在我们已经看到了基本的文本统计信息的样子。接下来，让我们看看我们实际上该如何处理文本。所以我们首先要做的就是对文本进行分词处理。所谓分词，我的意思是……嗯，我们把每个词或者每个简单的单位转换成一个概念，然后文本就变成了一系列整数。
- en: basically。 So there are a number of ways how you can go about this。 One is I
    can use a character encoding。 And that's very nice because， you know， if I have，
    you know。 maybe 30 to 35 meaningful characters， I get a very， small vocabulary。
    And so it's very。 very easy to， you know， predict one new character at， the time。
    And if I were to do this。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上。所以，有很多方法可以去实现这个目标。一种是我可以使用字符编码。这是非常好的，因为你知道，如果我有，也许是30到35个有意义的字符，我会得到一个非常小的词汇表。所以它非常非常容易，嗯，你知道，每次预测一个新字符。如果我做这个的话。
- en: I would never end up in a situation where I haven't。 seen a particular word
    because it's composed out of those characters。 The problem is if I do this。 then
    quite often the model doesn't work so well。 It doesn't work so well。 I mean that
    the network actually needs to learn how to， spell properly。 Now for some languages。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我永远不会遇到一个我没有见过的特定单词，因为它是由这些字符组成的。问题是，如果我这么做，模型往往并不能很好地工作。它运作得不好。我的意思是，网络实际上需要学习如何正确拼写。对于某些语言来说。
- en: this is actually a good thing。 So if you， for instance。 have a weird language
    like German where you have， you， know， not just the verbs。 but also the nouns
    changing their shapes and forms or， Russian where you have， you know。 six different
    cases and then you have single， line plural。 So。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是件好事。所以如果你有，比如说，像德语那样的奇怪语言，其中不仅动词，名词的形态也会发生变化，或者像俄语那样有六种不同的格，并且还有单数和复数形式。那么。
- en: or if you have languages that attach a lot of stuff at the end of， words。 so
    for weird languages like that， that's quite fine。 It actually works quite well
    for Chinese for very obvious reasons。 Any thoughts？
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你有一些语言，它们在单词的末尾附加了很多东西，那么对于那些奇怪的语言来说，这其实是完全可以接受的。对于中文来说，这种方法实际上非常有效，原因显而易见。有什么想法吗？
- en: Why does it work well for Chinese or for Japanese for that matter？ Exactly。
    you have actually thousands of characters。 So this is actually pretty。 much in
    the Goldilocks zone where you don't have too many， you don't have too few。 So
    you get a representation of the meaningful granularity。 But if you have a small
    alphabet。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这种方式对于中文或日语有效呢？确切地说，你实际上有成千上万个字符。所以这实际上是在一个刚刚好的区域，你既没有太多，也没有太少。所以你得到了有意义的颗粒度表示。但如果你有一个小的字母表。
- en: then it's not so good。 Well， you can go to the other direction， right？ So， well。
    why can't you do the same， thing as what you know you would do with Chinese， also
    for English？ Well。 and let's just pretend that， you know， every word is a token，
    right？
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这样就不太好了。嗯，你可以往相反的方向走，对吧？那么，嗯，为什么你不能做跟中文一样的事情，来处理英语呢？嗯，让我们假装，你知道，每个词就是一个标记，对吧？
- en: The fact that this isn't quite the case is exactly what saves us in Chinese，
    but。 for English and if we didn't have， well， for English it's not that bad yet，
    but。 for pretty much any other language nouns and verbs really change their forms
    quite， drastically。 Well， you get a really huge vocabulary。 And you may not be
    able to generalize， well， otherwise。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这并不是完全如此，但这正是我们在中文中能够得以拯救的原因，然而对于英语来说，如果我们没有……嗯，对于英语来说，情况还不是那么糟糕，但对于几乎所有其他语言，名词和动词的形式变化真的是非常剧烈的。所以，你会拥有一个非常庞大的词汇量，而你可能无法做到普遍化，嗯，不然的话。
- en: for instance， you might have， seen some verb only in one form and not in all
    the other forms。 So it's a huge vocabulary， it's quite costly and slow and the
    GPU vendor will。 happily sell you a bigger GPU because of that， but it's a bad
    idea。 So there's obviously something in the middle and that's called byte pairing，
    coding。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，你可能只在一个形式下见过某个动词，而没有看到它的所有其他形式。所以词汇量庞大，代价高且速度慢，GPU厂商会很高兴因此卖给你一个更大的GPU，但这是个糟糕的主意。所以显然在中间有一个解决方案，那就是字节对编码（byte
    pairing）。
- en: So what people did is they said， well， let's just look for frequent common，
    substrings。 Then you just order them， you aggregate， you know， the most frequent
    common substrings。 and then you aggregate this， further and so you get basically
    something that looks almost like syllables。 but， can be a little bit longer if
    you have some other substrings that are very common。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以人们做的是，他们说，嗯，让我们寻找常见的频繁子字符串。然后你只需要对它们排序，聚合，嗯，最频繁的常见子字符串。然后你将这些进一步聚合，所以你基本上得到的东西几乎看起来像音节。但是，如果你有一些非常常见的其他子字符串，它们可能会稍微长一些。
- en: And so this byte pairing coding gets you pretty much into a Goldilocks zone，
    which， well。 if you were dealing with Japanese or Chinese， you would be pretty
    much， in already from the get-go。 Okay。 The next thing that we need to do is，
    if you。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这种字节对编码几乎可以让你进入一个“恰到好处”的区间，这，嗯，如果你处理的是日语或中文，你基本上一开始就已经进入了。好的，接下来我们需要做的是，如果你。
- en: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_1.png)'
- en: think about it， you need to generate the mini-batch。 As we need to generate
    our training data。 remember， we had this embedding and then， we want to predict
    the next character。 So what you could do is you could， basically take the text
    and you break it up in this case into。 you know， sub-sequences of five。 So you
    have different offsets and you just need to。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想，你需要生成小批量。因为我们需要生成我们的训练数据。记住，我们有这个嵌入，然后我们想预测下一个字符。所以你可以，基本上将文本分割开来，在这个例子中，分成，嗯，五个字符的子序列。这样你就有不同的偏移量，你只需要继续。
- en: count a little bit such that you don't end up going out of bounds， right？
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微计算一下，确保你不会越界，对吧？
- en: So it's very straightforward。 And then， so what you do is， you could， for instance。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这非常直接。然后，接下来你要做的事是，你可以，举个例子。
- en: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_3.png)'
- en: just randomly partition， say pick a random offset。 You distribute the。 sequences
    at random over the mini-batch。 And this gives you sort of kind of。 independent-ish
    samples。 They're not quite independent but under certain mixing。 properties and
    so on。 You can essentially calculate all the problems away。 So， for instance。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随机分区，比如说选择一个随机偏移量。你将序列随机分布到小批量中。这会给你一种相对独立的样本，虽然它们不完全独立，但在某些混合属性下，你基本上可以计算出所有的问题。所以，举个例子。
- en: there's some nice work on beta mixing， Sarah Funder-Gere and。 Don Mayer and
    others have done good work on that。 Trouble is you need to reset that。 hidden
    state if you have a latent variable model， right？ Because every time you。 drive
    a different sequence in a different part of that string， you don't know any。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于β混合有一些不错的工作，Sarah Funder-Gere和Don Mayer等人做了很好的工作。问题是，如果你有一个潜变量模型，你需要重置隐藏状态，对吧？因为每当你在该字符串的不同部分生成一个不同的序列时，你并不知道任何。
- en: of the context where this came from， so you need to reset your hidden state。
    Okay。 We'll get to that in a lot more detail later。 The alternative is you use
    sequential partitioning。 So you basically just， you， know， take a random offset
    and then， you know。 distribute the sequences in， the mini-batch but you keep the
    sequence preserved over mini-batch。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它所来自的上下文，所以你需要重置你的隐藏状态。好的，稍后我们会更详细地讨论这一点。另一种方法是使用顺序分区。所以基本上，你只需，嗯，取一个随机偏移量，然后，嗯，将序列分布到小批量中，但你需要保持序列在小批量中不变。
- en: So basically， you have one mini-batch for this part and the next mini-batch
    is for， here。 next mini-batch is for here and you keep on going through。 This
    way you can carry the hidden state across the mini-batch。 Then， as we'll see。
    works a lot better。 Okay。 And now it's time for code。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，你为这一部分生成一个小批量，接下来的小批量用于这里，下一个小批量用于这里，然后继续下去。这样你可以在小批量之间携带隐藏状态。然后，正如我们将看到的，这样做效果要好得多。好了，现在是代码时间。
- en: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_5.png)'
- en: Okay。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_7.png)'
- en: See？ There we go。 Okay。 So we read our time machine again。 Okay。 So we have
    about， you know， 178。000 characters and that's the text。 And so I'm going to first
    produce an index and so I'm going to use a character。 model here。 So I'm going
    to just， you know， look at all the characters in the， dataset。 So raw dataset
    is， you know， my dataset and set of that conveniently。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 看到了吗？我们开始了。好，我们再读取一次我们的时光机。好了，我们大约有178,000个字符，这就是文本。所以我将首先生成一个索引，我将在这里使用字符模型。所以我将，嗯，查看数据集中所有的字符。原始数据集是，嗯，我的数据集，并且集成了方便的那部分。
- en: in Python just makes this all unique。 So index to char gives me that。 And then
    char to index I need to just， you know， invert that mapping。 And so then I get
    the recovery size and， you know， this is my count， right？
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，这一切都变得独一无二。索引到字符给我返回这个。然后字符到索引，我只需要，你知道，反转那个映射。接着我得到了恢复大小，你知道，这就是我的计数，对吧？
- en: So the first character is a comma， the second one is an S and so on。 These are
    all the characters that actually occur in the dataset。 Okay。 This is exactly why
    Python is so convenient。 Okay。 Now what I can do is I can just look at what I
    get with characters and， the indices。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第一个字符是逗号，第二个字符是 S，依此类推。这些都是数据集中实际出现的字符。好的。这正是 Python 如此方便的原因。好的。现在我可以做的是，我可以查看字符和索引的内容。
- en: So char to index of char。 Well， that's just my mapping for all the characters
    in the raw dataset that gives。 me the set of indices。 That's this thing here。
    So one number per character。 And so sample of the first 20 characters is that。
    And then I can do the opposite。 I just run a join。 And I iterate over all the
    characters in the data in that sample。 And I perform that mapping。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以字符到字符索引。嗯，这就是我对原始数据集中所有字符的映射，生成了索引集。这就是这里的内容。所以每个字符一个数字。然后前 20 个字符的样本是这样的。接下来我可以做反向操作。我只需执行一次连接操作。然后我遍历数据中的所有字符，按照这个映射进行操作。
- en: So again， this is convenient Python because， you're running the full loop inside
    an index。 Okay。 For the first time you code this up， you're not going to do that。
    You're going to take the scenic route and then afterwards if you want to show
    off。 pretty code you'll do that。 Otherwise it's error prone。 Yeah。 Anything surprising
    in there？ Nope。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以再次强调，这是 Python 的便利，因为你在一个索引内运行整个循环。好的。当你第一次编写这个时，你不会这么做。你会走一条更长的路，之后如果你想展示漂亮的代码，你可以这么做。否则，这样容易出错。嗯。里面有意外的东西吗？没有。
- en: Okay。 So then now this is a bit of a， walker of a function。 This will generate
    random sub sequences。 So we have all the indices here， the mini batch size， number
    of steps， the device， context。 The device context makes sure that we can push
    things on to a GPU if we， want to。 So here I compute a random offset。 And then
    I just throw away the first few characters before that offset。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么现在这是一个有点复杂的函数。这个函数将生成随机子序列。所以我们这里有所有的索引，迷你批次大小，步骤数，设备，上下文。设备上下文确保我们可以把东西推送到
    GPU 上（如果我们需要的话）。所以这里我计算一个随机偏移量。然后我就丢掉偏移量之前的几个字符。
- en: Then we can work out how many substrings we can actually generate。 And that's
    basically whatever data that I've got left over， minus one。 And then I need to
    divide this by the number of steps that I have。 All right。 Minus one。 Otherwise
    I would have over counted the last one。 Then I discard everything that's half
    empty because I'm assuming that we have。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以计算出实际上能生成多少个子字符串。这基本上就是剩余数据量减去一。然后我需要把它除以我拥有的步骤数。好的，减去一。否则我会多算一个。然后我丢弃所有半空的部分，因为我假设我们已经有了。
- en: plenty of data so I don't really need to be particularly frugal。 And now I just
    partitioned the data into this。 So number of examples time number of steps。 Number
    of steps。 So this is basically just reshuffling things。 And then， okay， we reshuffle。
    Okay。 So this is all pre-processing。 Now， who's ever coded up an iterator in，
    Python？ Okay。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据非常充足，所以我不需要特别节省。现在我只需将数据划分成这样。也就是说，例子的数量乘以步骤数。步骤数。所以这基本上就是在重新洗牌。然后，好的，我们重新洗牌。好的。这是所有的预处理。现在，谁曾经在
    Python 中写过迭代器代码？好的。
- en: So an iterator in Python is a really， convenient thing。 You can essentially
    use it in a for loop。 And whenever you say， well， give me a new observation， the
    iterator returns a new， thing。 And we code it up like a function。 And rather than
    return， it does this thing called yield。 So the goal is to yield。 Basically， whenever
    I call data iterator random。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Python 中的迭代器是一个非常方便的东西。你实际上可以在 `for` 循环中使用它。每当你说“给我一个新观察值”时，迭代器就返回一个新的东西。我们把它写成一个函数。与其返回，它做了一件叫做
    `yield` 的事情。所以目标是执行 `yield`。基本上，每当我调用 `data iterator random` 时，它都会返回。
- en: it gives me a new substring back。 So it does all of that at startup time。 This
    is just some useful helper function that basically just extracts the data of a。
    certain subsequence。 And I'm just using the underscore to， indicate that， yeah。
    this is an internal function。 You shouldn't touch it。 And now I'm looping over
    the data。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它会返回一个新的子字符串。所以它在启动时就完成了所有这些操作。这只是一个有用的辅助函数，基本上就是提取某个子序列的数据。我使用下划线来表示，这个是内部函数，你不应该去动它。现在我开始遍历数据。
- en: This is basically batch size times number of batches。 That's basically what
    I'm looking at。 So many batch size， many strings at once。 And I'm going all the
    way up to batch size times number of batches。 Okay。 So then batch indices。 That's
    just the starter here。 Today， generate x and y。 So I extract exactly in the current
    and the next substring。 And then， well， we're done。 Okay。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是批次大小乘以批次数。那就是我所看到的。所以很多批次大小，一次处理很多字符串。我会一直进行，直到批次大小乘以批次数。好的。那么批次索引。这里只是一个开始。今天，生成x和y。所以我精确地提取出当前和下一个子字符串。然后，嗯，我们完成了。好的。
- en: So let's see how this goes。 So I'm using a batch size of two and five， the sequences
    of five steps。 Total sequence length is 30。 So in this case， well， I'm just， splitting
    out x and y pairs。 So I basically have， you know， six， seven， eight， nine， ten。
    And the y's are just shifted by one。 So I get， you know， seven to eleven。 Likewise
    here， from 16 to 20， I get 17 to 21。 And that's。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们来看一下这个是怎么进行的。我使用了批次大小为二和五，步长为五的序列。总序列长度是30。所以在这种情况下，我只是将x和y对拆分开来。基本上，我有六、七、八、九、十。而y则只是向前移动了一个位置。所以我得到了七到十一。类似的，这里从16到20，我得到的是17到21。就是这样。
- en: with that particular offset， as much as I could squeeze out。 And then， you know。
    there's another one。 One to five and eleven to fifteen。 I haven't covered this
    either yet。 And the corresponding of y's。 So basically each of those blocks here
    is a mini batch。 Okay。 And if I were to run this again， let me do it again。 I'll
    get something else。 Okay。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用那个特定的偏移量，我尽可能地提取出数据。然后，你知道的。这里还有一个例子。一到五和十一到十五。我还没有覆盖到这个。还有对应的y值。所以基本上，每一个块就是一个小批次。好。如果我再运行一遍，来让我再做一次。那我会得到其他的结果。好的。
- en: So there's another way how to do it。 And that's basically sequential partitioning。
    In that case。 I just take my long sequence。 I break it up into， let's say。 a half
    a mini batch of size three of three sub-segments。 I then basically fold those
    over， right？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以还有另一种方法来做这个。那基本上是顺序划分。在这种情况下，我只需要拿我的长序列。我将它分成，例如，大小为三的小批次，其中有三个子段。然后我基本上会把它们折叠起来，对吧？
- en: And here， another one。 This guy goes over here。 And then I go and trade it accordingly，
    right？
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个例子。这一块过来。然后我会根据需要调整它，是吧？
- en: So this will give me one， two， three， four mini batches。 And I'm going to read
    them in that order。 Okay。 So this code， what you're seeing there， that's exactly
    that。 So again。 I pick around them offset。 Okay。 Workout number of indices。 So
    that's basically just trying to figure out how much data I have。 Then， since I
    have the targets。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将给我一个、二、三、四个小批次。我会按这个顺序读取它们。好的。所以这段代码，你看到的就是那样。再次，我选择它们的偏移量。好的。计算出索引的数量。所以这基本上就是在试图找出我有多少数据。然后，因为我有目标值。
- en: well， I need to basically leave one more term。 And then， yeah， I just have my
    trader again。 except that now it goes in sequence through the data。 Okay。 You
    may need maybe a few minutes at home to read through this。 Let me quickly show
    you what it does。 So， similar situation as before， we had， you know， 30 observations。
    In this case， I assumed， okay。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我基本上需要再留一个术语。然后，是的，我又得到了我的交易者。只是现在它按照顺序遍历数据。好的。你可能需要一些时间在家里阅读这个。让我快速向你展示它的作用。所以，和之前类似的情况，我们有，嗯，30个观察值。在这种情况下，我假设，好的。
- en: well， we have basically sequence length of six， mini batch size of two。 And
    so you can see the corresponding sub-sequences extracted。 So we have， you know，
    three to eight。 it continues with nine to 14。 In the bottom here， we have 16 to
    21， it continues with， well。 22 to 27。 So if I were to run this again， well， in
    that case。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们基本上是六的序列长度，二的小批次大小。你可以看到提取出的对应子序列。所以我们有三到八。然后继续是九到十四。底部这里，我们有十六到二十一，然后继续是，二十二到二十七。如果我再运行一遍，好的，那就是这种情况。
- en: only one fits because I just happen to be quite unlucky。 In terms of my offset，
    yeah。 That's what you get。 Any questions？ Okay。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个适配，因为我只是恰巧非常不幸。在偏移量方面，是的。这就是结果。有什么问题吗？好的。
- en: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fa8609d91cfeb3b3a3b2dbf7d41a715_9.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
