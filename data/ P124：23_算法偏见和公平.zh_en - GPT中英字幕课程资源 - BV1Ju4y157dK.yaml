- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P124：23_算法偏见和公平.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 沃顿商学院《AI用于商业（AI For Business）》（中英字幕） - P124：23_算法偏见和公平.zh_en - GPT中英字幕课程资源 -
    BV1Ju4y157dK
- en: I'm Kevin Werback， a professor of Legal Studies and Business Ethics at Wharton，
    and I work。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我是凯文·沃巴克，沃顿商学院法律研究与商业伦理教授，我在工作。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_1.png)'
- en: on issues of ethics and responsibility around AI and analytics。 Algorithmic
    bias and fairness。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能和分析的伦理与责任问题上。算法偏见和公平性。
- en: You would think that a major benefit of AI is that it overcomes human biases
    and blind， spots。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你会认为人工智能的一个主要好处是它克服了人类的偏见和盲点。
- en: And you'd be right。 Humans are prone to stereotypes and implicit biases and
    even explicit discrimination。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你是对的。人类容易受到刻板印象、隐性偏见甚至明确歧视的影响。
- en: An AI system just looks at the data。 However， it's dangerous and misleading
    to think AI is inherently objective。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人工智能系统只是看数据。然而，认为人工智能本质上是客观的，这种想法是危险且误导的。
- en: It can actually replicate or even reinforce human biases and produce deep unfairness。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上可以复制甚至强化人类偏见，产生深刻的不公平。
- en: Algorithmic bias is ethically wrong。 It's harmful to marginalized populations。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见在伦理上是错误的。它对边缘化群体有害。
- en: It can lead to backlash from employees， customers， and other stakeholders， and
    it may even lead。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会引发员工、客户和其他利益相关者的反感，甚至可能导致。
- en: to legal consequences。 In 2019， a study came out in science about a major academic
    hospital in Boston。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会导致法律后果。2019年，一项关于波士顿一家大型学术医院的研究发表在《科学》杂志上。
- en: Researchers found that a care management algorithm。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现了一个护理管理算法。
- en: which referred patients for additional resources， if they were high risk。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果患者风险较高，算法会推荐他们获得额外资源。
- en: produced systemic racial discrimination。 The average black patient referred
    by this program had nearly double the number of underlying。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序所推荐的平均黑人患者的潜在疾病几乎是双倍。
- en: conditions as the average white patient。 In other words。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与平均白人患者的情况相比。换句话说。
- en: the black patient had to be much sicker in order to receive the equivalent，
    level of care。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 黑人患者必须病得更重才能获得同等水平的护理。
- en: When the research has changed the algorithm， so that equally sick white and
    black patients。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当研究改变了算法，使得同样生病的白人和黑人患者。
- en: made the cut it nearly tripled the percentage of black patients who qualified。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它的应用几乎使符合条件的黑人患者的比例增加了三倍。
- en: This is not a unique problem。 Facial recognition systems have been shown to
    be less accurate for dark skin faces。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个独特的问题。人脸识别系统已被证明对深色皮肤的面孔准确性较低。
- en: Hiring algorithms used by companies like Amazon to predict job performance based
    on resumes。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 像亚马逊这样的公司使用的招聘算法，基于简历预测工作表现。
- en: have been found to disadvantage women。 Text generation and machine translation
    systems impose sexist associations and sometimes racist。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 已被发现对女性不利。文本生成和机器翻译系统施加性别歧视的关联，有时甚至是种族主义的。
- en: associations。 For example， if you translate from Hungarian， which has gender
    neutral pronouns。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 关联。例如，如果你从匈牙利语翻译，匈牙利语有性别中立的代词。
- en: there is， no he or she in Hungarian， but if you translate a Hungarian text into
    English on a major search。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在匈牙利语中没有他或她，但如果你将匈牙利语文本翻译成英语，在主要搜索中会出现。
- en: engine， it will pick he as the pronoun for professor and politician， but she
    for washes。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索引擎中，它会将“他”作为教授和政治家的代词，而将“她”用于洗衣工。
- en: the dishes and assistant。 What is going on here？ First data can embed human
    prejudice。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些菜肴和助手。这里发生了什么？首先，数据可以嵌入人类偏见。
- en: If women traditionally fail to get promoted and enjoy long careers at a company
    because。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果女性传统上在公司内难以晋升并享有长时间的职业生涯，因为。
- en: of rampant sexism， an AI system will find that being female is associated with
    poor outcomes。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在普遍的性别歧视下，人工智能系统会发现女性与不良结果相关联。
- en: In the healthcare example， the problem was that the algorithm used previous
    treatment。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健的例子中，问题在于该算法使用了先前的治疗。
- en: costs as a proxy for how sick a patient was。 The difficulty was that black people
    receive worse care on average。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 成本作为患者病情的代理。问题在于黑人患者的护理通常较差。
- en: so less is spent on， them even when they are equally or more sick than a white
    patient。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使她们同样生病或更严重，花费在她们身上的钱也会更少。
- en: In other cases， the data itself may be biased。 There may simply be fewer examples
    of minority populations in the training data set resulting。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，数据本身可能存在偏见。训练数据集中可能少了少数群体的例子。
- en: in less accurate models that appears to be part of what happened with the facial
    recognition。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在较不准确的模型中，这似乎是面部识别中发生的部分原因。
- en: systems。 And sometimes， even when classifications such as race and gender aren't
    even in the data。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 系统有时即便在数据中没有种族和性别的分类。
- en: set， they influence models through proxies。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素通过代理影响模型。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_3.png)'
- en: A zip code is just an address marker， but it can be strongly correlated with
    race or。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 邮政编码只是一个地址标记，但它可能与种族或。
- en: socioeconomic status， for example。 There are a variety of tools to incorporate
    fairness criteria into the design of algorithmic。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，社会经济地位。有多种工具可以将公平标准纳入算法设计。
- en: systems directly or to assess whether they produce discriminatory results。 However。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 系统直接或评估是否产生歧视性结果。然而。
- en: they aren't foolproof。 For one thing， there isn't a single definition of fairness。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它们并不是万无一失的。首先，公平的定义并不统一。
- en: The North Point Compass System， which was used to make recommendations for parole，
    famously。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 北点指南系统曾用于提供假释建议，颇具名声。
- en: made errors more frequently for black prisoners than white ones。 However。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对黑人的错误率高于对白人的错误率。然而。
- en: given two identical prisoners in every respect but race， it would generally，
    assign the same score。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个在各方面都相同但种族不同的囚犯，通常会分配相同的评分。
- en: In this case， individual and group level fairness， two different ways of measuring
    fairness。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，个体和群体层面的公平是衡量公平的两种不同方式。
- en: were shown to literally be mathematically impossible to achieve simultaneously。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 显示这些目标在数学上是根本不可能同时实现的。
- en: There had to be choices and trade-offs made。 Systems that are more fair may
    also be less accurate。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 必须做出选择和权衡。更公平的系统可能准确性较低。
- en: We have to make choices in the design of these systems。 And in some cases。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在这些系统的设计中做出选择。在某些情况下。
- en: there may not be objective standards of fairness at all。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可能根本没有客观的公平标准。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_5.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_5.png)'
- en: How one decides whether a social media news feed discriminates against conservatives，
    or liberals。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如何决定社交媒体新闻推送是否对保守派或自由派有歧视。
- en: for example， depends on inherently subjective decisions about what should be
    the， baseline。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，依赖于对什么应为基线的主观决定。
- en: what fits into those political categories， and so forth。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 符合这些政治类别的内容，等等。
- en: It's not that we can't ever evaluate such claims。 We just can't write an objective
    specification about what a "neutral" news feed would look。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说我们不能评估这种主张。我们只是无法写出一个关于“中立”新闻推送的客观规范。
- en: like because there's no such thing。 Finally， it's not an accident what data
    gets collected。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不存在，因此最终数据的收集并非偶然。
- en: how data is evaluated， or what， questions get asked in the design of algorithms。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的评估方式或在算法设计中所提出的问题。
- en: The same human factors that lead to marginalized groups being discriminated
    against in other。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 导致边缘化群体在其他方面受到歧视的人为因素是相同的。
- en: contexts still apply here。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种背景在这里同样适用。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_7.png)'
- en: Now there are some legal claims that can be brought against biased or unfair
    algorithms。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以对有偏见或不公平的算法提出一些法律主张。
- en: but their scope is quite limited today。 The major legal category that would
    apply here is what's called disparate impact when。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但它们今天的适用范围相当有限。适用的主要法律类别是所谓的不同影响。
- en: a policy or practice that on its face is neutral。 It doesn't explicitly treat
    minority populations different from other populations。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个表面上中立的政策或做法，并不会明确地对待少数族裔群体与其他群体不同。
- en: It still could have an effect of different treatment。 That's called disparate
    impact。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它仍然可能产生不同的待遇效果，这被称为不同影响。
- en: The problem is that area of the law is typically quite limited。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是该法律领域通常相当有限。
- en: It only applies to a limited set of protected classes， like race and gender
    typically， and。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它只适用于有限的受保护类别，通常是种族和性别，及。
- en: it generally applies only to certain activities which are specified under the
    law。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它通常只适用于法律中规定的某些活动。
- en: In the US that's basically employment and housing。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，这基本上涉及就业和住房。
- en: And then the bigger problem is that disparate impact generally requires a defined
    policy。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的问题是，不同影响通常需要明确的政策。
- en: or practice that's having that differential impact on the protected class。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 或者实践中对保护类群体产生差异影响的行为。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_9.png)'
- en: The United States Supreme Court has said that just showing a statistical disparity，
    just。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 美国最高法院表示，仅仅显示统计差异是不够的。
- en: showing that there is an effect that's worse against a protected class isn't
    enough。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 显示出对保护类群体有更糟影响的证据是不够的。
- en: There has to be some deliberate step that was taken， some policy， and just using
    an algorithm。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 必须采取某些明确的步骤，某项政策，仅仅使用算法是不够的。
- en: itself is not enough。 So there's a disconnect between the way the discrimination
    law is structured and the kinds。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这本身并不足够。因此，歧视法的结构与算法系统出现的问题之间存在脱节。
- en: of issues that come up with algorithmic systems。 In Europe。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与算法系统相关的问题。在欧洲。
- en: the general data protection regulation has an anti-bias provision in the sections。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一般数据保护法规的某些条款中包含反歧视条款。
- en: dealing with what are called fully automated processing。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 处理所谓的完全自动化处理。
- en: But that is limited in terms of its context and it's pretty vague。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但这在其背景上是有限的，而且相当模糊。
- en: It's not entirely clear yet how that could be applied。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目前还不完全清楚如何应用这一点。
- en: There are a variety of proposals and different jurisdictions for new laws。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 各个司法管辖区提出了多种新法律的提案。
- en: The European Union is considering a major new AI law that they've had a discussion
    paper， about。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟正在考虑一项重大的新人工智能法律，他们已经就此发表了讨论文件。
- en: And in the US there have been proposals for what are called the Algorithmic
    Accountability。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 而在美国，关于所谓的算法责任已经提出了一些提案。
- en: Act which would require bias checking techniques。 But these have not yet been
    widely adopted。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该法案将要求进行偏见检查技术。但这些尚未得到广泛采用。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_11.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_11.png)'
- en: So how should organizations respond to these challenges？ Well first of all。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，组织应如何应对这些挑战？首先。
- en: make sure that your training data set is deep and diverse。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的训练数据集深厚且多样。
- en: That it has enough examples along different measures of diversity。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须在不同的多样性度量上有足够的示例。
- en: Think about be aware of proxies where something that facially is neutral might
    actually encode。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑要警惕的代理因素，那些表面上中立的东西实际上可能编码了。
- en: some attributes of a protected class。 Think about what fairness function makes
    the most sense for your application。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 保护类群体的一些特征。思考哪个公平函数对你的应用最有意义。
- en: There are a variety of different ways to mathematically define what a fair algorithm
    is。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种不同的方法可以在数学上定义公平算法是什么。
- en: And different ones might be appropriate in different contexts。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的上下文中，可能适合不同的方案。
- en: Test and evaluate your systems based on these metrics。 Again。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些指标测试和评估你的系统。再一次。
- en: there are now tools out there that will allow you to assess what the impacts
    are。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一些工具可以让你评估其影响。
- en: And then finally be aware of hidden historical biases。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要意识到潜在的历史偏见。
- en: And this is where having diverse teams is critically important。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是多元化团队至关重要的地方。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_13.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_13.png)'
- en: If you've got someone in the process who has experienced discrimination themselves，
    they're。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在过程中有经历过歧视的人，他们会。
- en: just much more likely to be aware and flag something when it comes up in the
    design of。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计时，他们更可能意识到并标记出出现的问题。
- en: an AI system。 [BLANK_AUDIO]。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一种人工智能系统。[BLANK_AUDIO]。
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_15.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8b13040f91efdc4ae10a034a302ebe_15.png)'
