- en: P10：p10 Let's reproduce GPT-2 (124M) - 加加zero - BV11yHXeuE9d
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P10：p10 让我们重现GPT-2 (124M) - 加加zero - BV11yHXeuE9d
- en: Hi everyone。 So today we are going to be continuing our Zero2Hero series and
    in particular today we are going to reproduce the GPT-2 model。 the 124 million
    version of it。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。今天我们将继续我们的Zero2Hero系列，特别是今天我们要重现GPT-2模型，即124百万版本。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_1.png)'
- en: So when opening I released GPT-2 this was 2019 and they released it with this
    blog post。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我发布GPT-2时，那是2019年，他们通过这篇博客发布了它。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_3.png)'
- en: On top of that they released this paper and on top of that they released this
    code on GitHub。 So opening I/GPT-2。 Now when we talk about reproducing GPT-2 we
    have to be careful because in particular in this video we are going to be reproducing
    the 124 million parameter model。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，他们还发布了这篇论文，并在此基础上将代码发布到了GitHub上。所以打开I/GPT-2。现在，当我们谈论重现GPT-2时，我们必须小心，因为在这个视频中，我们将重现124百万参数的模型。
- en: So the thing to realize is that there is always a miniseries when these releases
    are made。 So there are the GPT-2 miniseries made up of models at different sizes
    and usually the biggest model is called the GPT-2。 But basically the reason we
    do that is because you can put the model sizes on the x-axis of plots like this。
    And on the y-axis you put a lot of downstream metrics that you are interested
    in like translation。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以要明白的是，每当发布这些内容时，总会有一个迷你系列。因此，GPT-2迷你系列由不同大小的模型组成，通常最大的模型被称为GPT-2。但基本上，我们这样做的原因是你可以将模型大小放在图的x轴上。在y轴上，你可以放置很多你感兴趣的下游指标，比如翻译。
- en: summarization， question answering and so on。 And you can chart out the scaling
    loss。 So basically as the model size increases you are getting better and better
    at downstream metrics。 And so in particular for GPT-2 if we scroll down in the
    paper there are four models in the GPT-2 miniseries starting at 124 million all
    the way up to 1558 million。 Now the reason my numbers， the way I say them disagree
    with this table is that this table is wrong。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要、问答等等。你可以绘制缩放损失。因此，基本上随着模型大小的增加，你在下游指标上表现得越来越好。因此，特别是对于GPT-2，如果我们在论文中向下滚动，可以看到GPT-2迷你系列中有四个模型，从1.24亿参数到15.58亿参数。现在，我的数字与这个表格不一致的原因是这个表格是错误的。
- en: If you actually go to the GPT-2 github repo they sort of say that there was
    an error in how they added up the parameters。 But basically this is the 124 million
    parameter model， etc。 So the 124 million parameter had 12 layers in the transformer
    and it had 768 channels in the transformer 768 dimensions。 And I'm going to be
    assuming some familiarity with what these terms mean because I covered all of
    this in my previous video。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你实际访问GPT-2的GitHub库，他们会说在计算参数时出现了错误。但基本上这就是124百万参数的模型等等。所以124百万参数的模型在变压器中有12层，并且在变压器中有768个通道，768个维度。我将假设你对这些术语有一定的熟悉度，因为我在之前的视频中已经涵盖了所有这些内容。
- en: let's build GPT-2。 Let's build GPT from scratch。 So I covered that in the previous
    video in this playlist。 Now if we do everything correctly and everything works
    out well。 by the end of this video we're going to see something like this。 Where
    we're looking at the validation loss which basically measures how good we are
    at predicting the next token in a sequence on some validation data that the model
    has not seen during training。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建GPT-2。让我们从头开始构建GPT。我在这个播放列表的前一个视频中覆盖了这个内容。现在，如果我们一切都正确，所有事情都顺利进行，到视频结束时，我们将看到类似这样的结果。我们将查看验证损失，这基本上衡量我们在一些模型在训练期间未见过的验证数据中预测下一个token的能力。
- en: And we see that we go from doing that task not very well because we're initializing
    from scratch all the way to doing that task quite well by the end of the training。
    And hopefully we're going to beat the GPT-2 124M model。 Now previously when they
    were working on this this is already five years ago。 So this was probably a fairly
    complicated optimization at the time and the GPUs and the compute was a lot smaller。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，我们从一开始不太好地完成这个任务，因为我们是从头开始初始化，到训练结束时，这个任务的完成情况相当不错。希望我们能够超越GPT-2 124M模型。现在之前他们在处理这个时，已经是五年前的事了。因此，这可能是当时相当复杂的优化，而当时的GPU和计算能力都比较小。
- en: Today you can reproduce this model in roughly an hour or probably less even
    and it will cost you about ten bucks if you want to do this on the cloud compute
    as a computer that you can all rent。 And if you pay ten dollars for that computer
    you wait about an hour or less。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 今天你可以在大约一个小时或更短的时间内重现这个模型，如果你想在云计算上进行，这大约需要十美元，所有这些计算机都可以租用。如果你为那台计算机支付十美元，你大约等一个小时或更少。
- en: You can actually achieve a model that is as good as this model that opening
    I released。 And one more thing to mention is unlike many other models。 opening
    I did release the weights for GPT-2。 So those weights are all available in this
    repository。 But the GPT-2 paper is not always as good with all of the details
    of training。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以获得一个与OpenAI发布的模型一样好的模型。还有一件事要提到的是，与许多其他模型不同，OpenAI确实发布了GPT-2的权重。这些权重在这个存储库中都可以获得。但GPT-2论文在训练的所有细节上并不总是那么出色。
- en: So in addition to the GPT-2 paper we're going to be referencing the GPT-3 paper
    which is a lot more concrete in a lot of the high parameters and optimization
    settings and so on。 And it's not a huge departure in the architecture from the
    GPT-2 version of the model。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GPT-2论文，我们还将参考GPT-3论文，它在许多高参数和优化设置上更加具体。并且在架构上与GPT-2版本的模型没有太大偏离。
- en: So we're going to be referencing both GPT-2 and GPT-3 as we try to reproduce
    GPT-2， 124M。 So let's go。 So the first thing I would like to do is actually start
    at the end or at the target。 So in other words let's load the GPT-2， 124M model
    as it was released by OpenAI and maybe take it for a spin。 Let's sample some tokens
    from it。 Now the issue with that is when you go to the codebase of GPT-2 and you
    go into the source and you click in on the model that Pi。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将在尝试重现GPT-2，124M时参考GPT-2和GPT-3。让我们开始吧。首先，我想从最后开始，或者说从目标开始。换句话说，让我们加载OpenAI发布的GPT-2，124M模型，并进行一些试验。我们来从中采样一些标记。现在的问题是，当你去到GPT-2的代码库，进入源代码并点击Pi模型时。
- en: you'll realize that actually this is using TensorFlow。 So the original GPT-2
    code here was written in TensorFlow which is， you know， not， let's just say。 not
    used as much anymore。 So we like to use PyTorch because it's a lot friendlier。
    easier and I just personally like it a lot more。 The problem with that is the
    initial code is in TensorFlow。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你会意识到实际上这是在使用TensorFlow。因此，原始的GPT-2代码是用TensorFlow编写的，嗯，不用说，现在用得不多。所以我们喜欢使用PyTorch，因为它更友好，更容易，我个人更喜欢它。问题在于初始代码是在TensorFlow中。
- en: We like to use PyTorch。 So instead to get the target we're going to use the
    hugging phase transformers code which I like a lot more。 So when you go into the
    transformers source transformers models GPT-2 modeling GPT-2。py。 you will see
    that they have the GPT-2 implementation of that transformer here in this file。
    And it's like medium readable but not fully readable。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们喜欢使用PyTorch。因此，为了获得目标，我们将使用hugging face的transformers代码，我个人更喜欢这个。进入transformers源代码中的transformers
    models GPT-2 modeling GPT-2.py文件，你会看到他们在这个文件中实现的GPT-2变换器。这是中等可读的，但并不完全可读。
- en: What it does is it did all the work of converting all those weights from TensorFlow
    to PyTorch friendly and so it's much easier to load and work with。 So in particular
    we can look at the GPT-2 model here and we can load it using hugging phase transformers。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它所做的是将所有这些权重从TensorFlow转换为PyTorch友好的格式，因此加载和使用起来要容易得多。因此，我们可以在这里查看GPT-2模型，并使用hugging
    face transformers加载它。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_5.png)'
- en: So swinging over this is what that looks like。 From transformers import the
    GPT-2 L&M head model and then from pre-trained GPT-2。 Now one awkward thing about
    this is that when you do GPT-2 as the model that we're loading this actually is
    the 124 million parameter model。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的样子。从transformers导入GPT-2 L&M头模型，然后从预训练的GPT-2。现在有一件尴尬的事情是，当你将GPT-2作为我们要加载的模型时，这实际上是124百万参数的模型。
- en: If you want the actual the GPT-2 the 1。5 billion then you actually want to do
    dash-accel。 So this is the 124M R target。 Now what we're doing is when we actually
    get this we're initializing the PyTorch and N module as defined here in this class。
    From it I want to get just the state dict which is just the raw tensors。 So we
    just have the tensors of that file。 And by the way here this is a Jupyter notebook
    but this is Jupyter notebook running inside VS code。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要实际的 GPT-2，即 15 亿参数版本，那么你实际上要使用 dash-accel。这是 124M R 目标。现在我们做的是，当我们实际获取这个时，我们正在初始化
    PyTorch 和在这个类中定义的 N 模块。我们想要获取的就是状态字典，它只是原始张量。因此，我们只有该文件的张量。顺便说一下，这里是一个 Jupyter
    Notebook，但这是在 VS Code 内部运行的 Jupyter Notebook。
- en: So I like to work with it all in a single sort of interface so I like to use
    VS code so this is the Jupyter notebook extension inside VS code。 So we can get
    the state dict this is just a dict so we can print the key and the value which
    is the tensor and let's just look at the shapes。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢在单一的界面中进行操作，因此我喜欢使用 VS Code，这里是 VS Code 中的 Jupyter Notebook 扩展。我们可以获取状态字典，这只是一个字典，因此我们可以打印出键和值，即张量，让我们来看一下形状。
- en: So these are sort of the different parameters inside the GPT-2 model and their
    shape。 So the W weight for token embedding is of size 50257 by 768。 Where this
    is coming from is that we have 50257 tokens in the GPT-2 vocabulary。 And the tokens
    by the way these are exactly the tokens that we spoke about in the previous video
    on my tokenization series。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 GPT-2 模型内部的不同参数及其形状。因此，标记嵌入的 W 权重大小为 50257 x 768。这来自于我们在 GPT-2 词汇中有 50257
    个标记。顺便提一下，这些正是我们在之前的标记化系列视频中提到的标记。
- en: So the previous videos just before this I go into a ton of detail on tokenization。
    GPT-2 tokenizer happens to have this many tokens。 For each token we have a 768
    dimensional embedding that is the distributed representation that stands in for
    that token。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的视频中，我详细讲解了标记化。GPT-2 的标记器恰好有这么多标记。对于每个标记，我们有一个 768 维的嵌入，作为该标记的分布式表示。
- en: So each token is a little string piece and then P768 numbers are the vector
    that represents that token。 And so this is just our lookup table for tokens and
    then here we have the lookup table for the positions。 So because GPT-2 has a maximum
    sequence of 1024 we have up to 1024 positions that each token can be attending
    to in the past。 And every one of those positions in GPT-2 has a fixed vector of
    768 that is learned by optimization。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记都是一个小字符串片段，然后 P768 数字是表示该标记的向量。因此，这只是我们用于标记的查找表，然后这里我们有位置的查找表。由于 GPT-2 的最大序列为
    1024，我们最多可以有 1024 个位置，每个标记可以在过去的这些位置上进行关注。GPT-2 中的每个位置都有一个通过优化学习到的固定 768 维向量。
- en: And so this is the position embedding and the token embedding。 And then everything
    here is just the other weights and biases and everything else of this transformer。
    So when you just take for example the positional embeddings and flatten it out
    and take just 20 elements you can see that these are just parameters。 These are
    weights， floats， just we can take and we can plot them。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是位置嵌入和标记嵌入。这里的所有内容只是 Transformer 的其他权重和偏置等等。因此，当你拿取例如位置嵌入并将其扁平化，然后仅取 20 个元素时，你可以看到这些仅仅是参数。这些是权重，浮点数，我们可以拿取并绘制它们。
- en: So these are the position embeddings。 And we get something like this and you
    can see that this has structure。 And it has structure because what we have here
    really is every row in this visualization is a different position。 a fixed absolute
    position in the range from 0 to 1024。 And each row here is the representation
    of that position。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是位置嵌入。我们得到的结果像这样，你可以看到它有结构。它有结构是因为在这个可视化中，每一行代表一个不同的位置，范围从 0 到 1024 的固定绝对位置。这里的每一行是该位置的表示。
- en: And so it has structure because these positional embeddings end up learning
    these sinusoids and cosines that sort of like represent each of these positions。
    And each row here stands in for that position and is processed by the transformer
    to recover all the relative positions and sort of realize which token is where
    and attend to them depending on their position。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有结构，因为这些位置嵌入最终学习了这些正弦和余弦，类似于表示每个位置。这里的每一行代表该位置，并由 Transformer 处理以恢复所有相对位置，并根据它们的位置来识别和关注相应的标记。
- en: not just their content。 So when we actually just look into an individual column
    inside these and I just grabbed three random columns。 you'll see that for example
    here we are focusing on every single channel。 And we're looking at what that channel
    is doing as a function of position from one from zero to 1023 really。 And we can
    see that some of these channels basically like respond more or less to different
    parts of the position spectrum。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是它们的内容。因此，当我们实际查看这些中的一个单独列时，我随便选了三列。你会看到，例如，我们专注于每个通道。我们在观察从0到1023的位置上这个通道的表现。
- en: So this green channel really likes to fire for everything after 200 up to 800
    but not less but a lot less and has a sharp drop off here near zero。 So who knows
    what these embeddings are doing and why they are the way they are。 You can tell
    for example that because they're a bit more jagged and they're kind of noisy。
    you can tell that this model was not fully trained。 And the more trained this
    model was。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个绿色通道对于200到800之间的所有内容非常敏感，但低于这个范围则响应较少，并且在零附近有一个急剧下降。因此，谁知道这些嵌入在做什么，为什么会是这样的。你可以看出，例如，由于它们有些锯齿状且有些嘈杂，你可以判断这个模型并没有完全训练好。而这个模型训练得越多。
- en: the more you would expect to smooth this out。 And so this is telling you that
    this is a little bit of an under trained model。 But in principle actually these
    curves don't even have to be smooth。 This should just be totally random noise。
    And in fact in the beginning of the optimization it is complete random noise because
    this position embedding table is initialized completely at random。 So in the beginning
    you have jaggedness and the fact that you end up with something smooth is already
    kind of impressive that that just falls out of the optimization because in principle
    you shouldn't even be able to get any single graph out of this that makes sense。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你会更期望将其平滑化。因此，这告诉你这个模型有点训练不足。但原则上，这些曲线甚至不必是平滑的。这应该完全是随机噪声。实际上，在优化的开始阶段，它确实是完全的随机噪声，因为这个位置嵌入表完全是随机初始化的。因此，最开始你会有锯齿状，而你最终得到的光滑结果已经是有些令人印象深刻，因为原则上你甚至不应该从中得到任何有意义的图形。
- en: But we actually get something that looks a little bit noisy but for the most
    part looks sinusoidal like。 In the original transformer paper the attention is
    all you need paper。 The positional embeddings are actually initialized and fixed。
    and sinusoidal like features during the optimization。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不过我们实际上得到的东西看起来有点嘈杂，但大部分看起来是正弦波的。在原始的变换器论文《注意力即所有你需要的》中，位置嵌入实际上是初始化并固定的，并且在优化过程中呈现正弦波特征。
- en: We can also look at any of the other matrices here so here I took the first
    layer of the transformer and looking at like one of its weights and just the first
    block of 300 by 300。 And you see some structure but like again like who knows
    what any of this is。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以查看这里的其他矩阵，所以我这里选取了变换器的第一层，观察其一个权重，大小为300乘300的第一个块。你会看到一些结构，但再次强调，谁知道这些是什么。
- en: If you're into mechanistic interoperability you might get a real kick out of
    trying to figure out like what is going on what is this structure and what does
    this all mean but we're not going to be doing that in this video。 But we definitely
    see that there's some interesting structure and that's kind of cool。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机械互操作性感兴趣，你可能会很高兴尝试弄清楚到底发生了什么，这个结构是什么，这一切意味着什么，但在这个视频中我们不会这样做。不过，我们确实看到一些有趣的结构，这很酷。
- en: But we're most interested in this we've loaded the weights of this model that
    was released by OpenAI and now using the hugging face transformers we cannot just
    get all the raw weights but we can also get the what they call pipeline and sample
    from it。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们最感兴趣的是，我们加载了OpenAI发布的这个模型的权重，现在使用Hugging Face的变换器，我们不仅可以获取所有原始权重，还可以获取他们所称的管道并从中采样。
- en: So this is the prefix hello I'm a language model comma and then we're sampling
    30 tokens and we're getting five sequences and I ran this and this is what it
    produced。 Hello I'm a language model。 But what I'm really doing is making a human
    readable document。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前缀“你好，我是一个语言模型”，然后我们采样30个标记，并生成五个序列。我运行了这个，这是它产生的结果。“你好，我是一个语言模型。”但我真正做的是生成一个人类可读的文档。
- en: There are other languages but those are thought that thought。 So you can read
    through these if you like but basically these are five different completions of
    the same prefix from this duty to 124 M。 Now if I go here I took this example
    from here。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他语言，但这些是我们认为的想法。如果你愿意，可以阅读这些，但基本上这是来自该模型的同一前缀的五个不同补全，范围从2到124M。现在如果我在这里，我从这里取了这个例子。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_7.png)'
- en: And sadly even though we are fixing the seed we are getting different generations
    from the snippet than what they got。 So presumably the code changed but what we
    see though at this stage that's important is that we are getting coherent text。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，即使我们固定了种子，我们从片段中得到的生成结果也与他们的不同。因此可以推测代码发生了变化，但在这个阶段重要的是，我们得到了连贯的文本。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_9.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_9.png)'
- en: So we've loaded the model successfully we can look at all its parameters and
    the keys tell us where in the model these come from。 And we want to actually write
    our own GPT two class so that we have full understanding of what's happening there。
    We don't want to be working with something like the modeling GPT two。py because
    it's just too complicated we want to write this from scratch ourselves。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功加载了模型，可以查看所有参数，键告诉我们这些参数来自模型的哪个部分。我们实际上想要编写自己的GPT-2类，以便完全理解其中发生的事情。我们不想使用像modeling
    GPT-2.py这样的东西，因为太复杂了，我们希望从头开始自己编写。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_11.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_11.png)'
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_12.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_12.png)'
- en: So we're going to be implementing the GPT model here in parallel。 And as our
    first task let's load the GPT two on 24 M into the class that we are going to
    develop here from scratch。 That's going to give us confidence that we can load
    the opening model and therefore there's a setting of weights that exactly is the
    124 model。 But then of course what we're going to do is we're going to initialize
    the model from scratch instead and try to train it ourselves on a bunch of documents
    that we're going to get。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里并行实现GPT模型。作为我们的第一项任务，让我们将24M的GPT-2加载到我们将从头开始开发的类中。这将给我们信心，因为我们可以加载开源模型，因此有一个权重设置正好是124模型。但当然，我们将从头开始初始化模型，尝试在我们将获得的一些文档上自行训练。
- en: And we're going to try to surpass that model。 So we're going to get different
    weights and everything's going to look different hopefully better even。 But we're
    going to have a lot of confidence that because we can load the opening model we
    are in the same model family and model class and we just have to rediscover a
    good setting of the weights but from scratch。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试超越这个模型。因此，我们将获取不同的权重，希望一切看起来有所不同，甚至更好。但我们会对这一点充满信心，因为我们可以加载开源模型，我们处于同一模型系列和类别，只需从头开始重新发现权重的良好设置。
- en: So let's now write the GPT two model and let's load the weights and make sure
    that we can also generate text that looks coherent。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写GPT-2模型，并加载权重，确保我们也能够生成看起来连贯的文本。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_14.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_14.png)'
- en: Okay， so let's now swing over to the attention is on any paper that started
    everything and let's scroll over to the model architecture the original transformer。
    Now remember that GPT two is slightly modified from the original transformer in
    particular we do not have the encoder GPT two is a decoder only transformer as
    we call it。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们转到关注任何开始一切的论文，并浏览原始变压器的模型架构。请记住，GPT-2相较于原始变压器有所修改，特别是我们没有编码器，GPT-2是我们所称的仅解码器变压器。
- en: So this entire encoder here is missing。 And in addition to that this cross attention
    here that was using that encoder is also missing so we delete this entire part。
    Everything else stays almost the same but there are some differences that we're
    going to sort of look at here。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个完整的编码器在这里缺失。此外，使用这个编码器的交叉注意力也缺失，因此我们删除了整个部分。其他一切几乎保持不变，但我们将在这里查看一些差异。
- en: So there are two main differences。 And then we go to the GPT two paper under
    two point three model。 We notice that first there's a reshuffling of the layer
    norms so they change place and second。 and additional layer normalization was
    added here to the final self attention block。 So basically all the layer norms
    here， instead of being after the MLP or after the attention。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以主要有两个区别。接下来我们去看 GPT 二的论文中的二点三模型。我们注意到，首先层归一化的顺序被重新排列了，其次，在最终的自注意力模块中添加了一个额外的层归一化。因此，所有的层归一化现在是在
    MLP 或注意力之前。
- en: they swing before it and an additional layer name gets added here right before
    the final classifier。 So now let's implement some of the first sort of skeleton
    and modules here in our GPT and module。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在之前进行，并且在最终分类器之前添加了一个额外的层名称。所以现在让我们在我们的 GPT 和模块中实现一些初步的框架和模块。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_16.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_16.png)'
- en: And in particular we're going to try to match up to this schema here that is
    used by hugging face transformers because that will make it much easier to load
    these weights from this state。 So we want something that reflects this schema
    here。 So here's what I came up with。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将尝试匹配 hugging face transformers 使用的此架构，因为这样会更容易从这个状态加载这些权重。因此，我们想要一些反映这个架构的东西。这是我想到的。
- en: Basically we see that the main container here that has all the modules is called
    transformer。 So I'm reflecting that within an end module dict。 And this is basically
    a module that allows you to index into the sub modules using keys just like a
    dictionary strings。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们看到这里的主要容器称为 transformer。因此，我在一个 end 模块字典中反映了这一点。这基本上是一个模块，允许你使用键索引子模块，就像字典字符串一样。
- en: Within it we have the weights of the token embeddings WTE and that's an embedding。
    And the weights of the position embeddings which is also just an embedding。 And
    if you remember。 an embedding is really just a fancy little wrapper module around
    just a single array of numbers。 A single block of numbers just like this。 It's
    a single tensor。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，我们有 token embeddings 的权重 WTE，这也是一个嵌入。还有位置嵌入的权重，这也只是一个嵌入。如果你记得，嵌入实际上只是围绕一组数字的华丽小包装模块。就像这样，它是一个单一的张量。
- en: And embedding is a glorified wrapper around a tensor that allows you to access
    its elements by indexing into the rows。 Now in addition to that we see here that
    we have a dot H and then this is indexed using numbers instead of indexed using
    strings。 So there's a dot H dot zero one two etc。 All the way up till dot H dot
    eleven。 And that's because there are twelve layers here in this transformer。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一个华丽的包装器，围绕一个张量，使你可以通过索引行来访问其元素。现在除了这一点，我们看到这里有一个 dot H，并且这个是用数字进行索引，而不是用字符串进行索引。所以有
    dot H dot zero、one、two 等等，一直到 dot H dot eleven。这是因为这个变换器中有十二层。
- en: So to reflect that I'm creating also an H I think that probably stands for hidden。
    And instead of a module dict this is a model list so we can index it using integers
    exactly as we see here。 And we can index it using a dot zero dot one two etc。
    And the module list has n layer blocks and the blocks are yet to be defined and
    then module in a bit。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以为了反映这一点，我也创建了一个 H，我想这可能代表隐藏层。并且这不是一个模块字典，而是一个模型列表，这样我们可以使用整数进行索引，正如我们在这里看到的那样。我们可以使用
    dot zero、dot one、two 等进行索引。模块列表有 n 层块，而这些块尚待定义，稍后将定义模块。
- en: In addition to that following the GPT two paper we need an additional final
    layer norm that we're going to put in there。 And then we have the final classifier
    the language model head which projects from 768 the number of embedding dimensions
    in this GPT。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据 GPT 二的论文，我们需要一个额外的最终层归一化，我们将把它放在那里。然后我们有最终分类器，语言模型头，它从 768 这个 GPT 中嵌入维度的数量进行投影。
- en: All the way to the vocab size which is about 50，257 and GPT two uses no bias
    for this final sort of projection。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一直到词汇大小，大约是 50,257，而 GPT 二在这个最终的投影中不使用偏置。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_18.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_18.png)'
- en: So this is the skeleton and you can see that it reflects this so the WTE is
    the token embeddings。 Here it's called output embedding but it's really the token
    embeddings。 The PE is the positional encolings those two pieces of information
    as we saw previously are going to add and then going to the transformer。 The dot
    H is the older blocks in gray and the LNF is this new layer that gets added here
    by the GPT two model。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是框架，你可以看到它反映了这一点，所以WTE是令牌嵌入。这里称为输出嵌入，但它实际上是令牌嵌入。PE是位置编码，这两部分信息正如我们之前看到的，将被相加，然后进入变压器。dot
    H是旧的灰色块，LNF是GPT二模型在这里添加的新层。
- en: And L and head is this linear part here。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: L和head是这里的线性部分。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_20.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_20.png)'
- en: So that's the skeleton of the GPT two。 We now have to implement the block。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是GPT二的框架。我们现在需要实现这个块。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_22.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_22.png)'
- en: Okay so let's now recurse to the block itself so we want to define the block。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在让我们递归到块本身，我们要定义这个块。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_24.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_24.png)'
- en: So I'll start putting them here。 So the block I like to write it out like this。
    These are some of the initializations and then this is the actual forward passive
    what this block computes。 And notice here that there's a change from the transformer
    again that is mentioned in the GPT two paper。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我开始把它们放在这里。这个块我喜欢这样写。这些是一些初始化，然后这是这个块计算的实际前向过程。注意这里有一个变化，这在GPT二论文中提到的变压器。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_26.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_26.png)'
- en: So here the layer normalizations are after the application of attention or feed
    forward。 In addition to that note that the normalizations are inside the residual
    stream。 You see how feed forward is applied and this arrow goes through and through
    the normalization。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，层归一化是在应用注意力或前馈之后。除此之外，请注意归一化是在残差流内。你可以看到前馈是如何应用的，这个箭头穿过归一化。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_28.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_28.png)'
- en: So that means that your residual pathway has normalizations inside them。 And
    this is not very good or desirable。 You actually prefer to have a single clean
    residual stream all the way from supervision all the way down to the inputs of
    the tokens。 And this is very desirable and nice because the gradients that flow
    from the top if you remember from your micro grad。 addition just distributes gradients
    during the backward stage to both of its branches equally。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你的残差路径内部有归一化。这并不是很好或理想的。你实际上更希望有一条从监督到令牌输入的干净的单一残差流。这是非常理想和好的，因为如果你还记得微分梯度的流动，梯度在反向阶段会均匀分配到它的两个分支。
- en: So addition is a branch in the gradients。 And so that means that the gradients
    from the top flow straight to the inputs。 the tokens， through the residual pathways
    unchanged。 But then in addition to that the gradient also flows through the blocks。
    And the blocks contribute their own contribution over time and kick in and change
    the optimization over time。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以加法是梯度中的一个分支。这意味着来自顶部的梯度直接流向输入。令牌通过残差路径不变地流动。但除此之外，梯度也通过块流动。块随着时间的推移贡献自己的部分，并介入并改变优化。
- en: But basically clean residual pathway is desirable from an optimization perspective。
    And then this is the pre normalization version where you see that our X first
    goes through the layer normalization and then the attention and then goes back
    out to go to the layer normalization number two and the multi-layer perceptron。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但从优化的角度来看，干净的残差路径是理想的。然后这是预归一化版本，你可以看到我们的X首先通过层归一化，然后是注意力，然后返回去进入第二层归一化和多层感知器。
- en: Sometimes also refer to as a feed forward network or an FM and then that goes
    into the residual stream again。 And the one more thing that is kind of interesting
    to note is that recall that attention is a communication operation。 It is where
    all the tokens and there's one thousand twenty four tokens lined up in a sequence。
    And this is where the tokens communicate。 This is where they exchange information。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有时也称为前馈网络或FM，然后它又进入残差流。在这里值得注意的是，回想一下注意力是一种通信操作。这是所有令牌的地方，有一千零二十四个令牌排成一个序列。这是令牌进行交流的地方，这是它们交换信息的地方。
- en: So attention is a aggregation function。 It's a pooling function。 It's a weighted
    sum function。 It is a reduce operation。 Whereas MLP， this MLP here happens at
    every single token individually。 There's no information being collected or exchanged
    between the tokens。 So the attention is the reduce and the MLP is the map。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，注意力是一种聚合函数。它是一个池化函数。它是加权求和函数。它是一个减少操作。而MLP，这里的MLP在每个单独的标记上发生。标记之间没有信息被收集或交换。因此，注意力是减少，而MLP是映射。
- en: And what you end up with is that the transformers just ends up just being a
    repeated application of map reduce。 If you want to think about it that way。 So
    this is where they communicate and this is where they think individually about
    the information that they gathered。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最终你得到的是变换器最终只是重复应用map reduce。如果你想这样考虑。所以这是它们交流的地方，也是它们对收集到的信息进行独立思考的地方。
- en: And every one of these blocks it really refines the representation inside the
    residual stream。 So this is our block。 It's likely modified from this picture。
    Okay， so let's now move on to the MLP。 So the MLP block I implemented as follows。
    It is relatively straightforward。 We basically have two linear projections here
    that are sandwiched in between the GALU nonlinearity。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个块确实细化了残差流中的表示。这是我们的块。它可能是从这张图片中修改而来。好的，现在让我们继续讨论MLP。所以我实现的MLP块如下。它相对简单。我们基本上有两个线性投影，它们夹在GALU非线性之间。
- en: So， and in that GALU approximate is 10H。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在那个GALU近似中是10H。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_30.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_30.png)'
- en: Now when we swing on， swing over to the patch of documentation。 this is in that
    GALU and it has this format。 And it has two versions。 The original version of
    GALU which we'll step into in a bit and the approximate version of GALU which
    we can request using 10H。 So as you can see just as a preview here GALU is a basically
    like a relu。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们切换到文档补丁时。这是GALU，并且有这种格式。它有两个版本。GALU的原始版本我们稍后会深入探讨，还有可以请求的10H近似版本。所以正如你所看到的，GALU基本上就像一个relu。
- en: Except there's no flat exactly flat tail here at exactly zero。 But otherwise
    it looks very much like a slightly smoother relu。 It comes from this paper here。
    Gosh in error linear units。 And you can step through this paper and there's some
    mathematical kind of reasoning that leads to interpretation at least to the specific
    formulation。 It has to do with stochastic real risers and the expectation of a
    modification to it have to drop out。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在零处没有完全平坦的尾部之外，但除此之外，它看起来非常像稍微平滑的relu。它来自这篇论文。天哪，线性误差单位。你可以逐步阅读这篇论文，其中有一些数学推理，至少引导到具体公式的解释。这与随机真实升降机及其修改的期望有关。
- en: So you can read through all of that if you'd like here。 And there's a little
    bit of history as to why there is an approximate version of GALU。 And that comes
    from this issue here as far as I can tell。 And in this issue Daniel Hendrix mentions
    that at the time when they developed this nonlinearity。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，可以浏览这些内容。还有一点历史可以解释为什么会有GALU的近似版本。根据我的了解，这来自于这个问题。在这个问题中，Daniel Hendrix提到在他们开发这种非线性时的情况。
- en: the IRF function which you need to evaluate the exact GALU was very slow and
    tensor flow。 So they ended up basically developing this approximation。 And this
    approximation then ended up being picked up by BERT and by GPT-2 etc。 But today
    there's no real good reason to use the approximate version。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 评估精确GALU的IRF函数在tensor flow中非常慢。因此他们最终开发了这个近似。这个近似随后被BERT和GPT-2等模型采用。但今天没有真正的理由使用近似版本。
- en: You'd prefer to just use the exact version。 Because my expectations that there's
    no big difference anymore。 And this is kind of like a historical kind of quirk。
    But we are trying to reproduce GPT-2 exactly。 GPT-2 used the 10H approximate version。
    So we prefer to stick with that。 Now one other reason to actually just intuitively
    use GALU instead of VARLU is previously in videos in the past。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能更倾向于使用精确版本。因为我认为没有太大差异了。这有点像历史上的一种怪癖。但我们正试图完全再现GPT-2。GPT-2使用的是10H近似版本。所以我们更愿意坚持这一点。还有一个原因是直观上使用GALU而不是VARLU，因为以前的视频中有提到。
- en: We've spoken about the dead relu neuron problem where in this tail of a relu
    if it's exactly flat at zero。 Any activations that fall there will get exactly
    zero gradient。 There's no change。 There's no adaptation。 There's no development
    of the network if any of these activations end in this flat region。 But the GALU
    always contributes a local gradient。 And so there's always going to be a change。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过死 ReLU 神经元问题，在 ReLU 的尾部，如果它在零点是完全平坦的，任何落在那里的激活都会得到完全为零的梯度。这没有变化。这没有适应性。如果任何激活结束于这个平坦区域，网络就不会有发展。但
    GALU 总是会贡献一个局部梯度。因此，始终会有变化。
- en: always going to be an adaptation。 And sort of smoothing it out ends up empirically
    working better in practice as demonstrated in this paper。 And also as demonstrated
    by it being picked up by the BERT paper GPT-2 paper and so on。 So for that reason
    we adopt this nonlinearity here in the 10 in the GPT-2 reproduction。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 始终会有适应性。而且将其平滑化在实践中经验上证明效果更好，正如这篇论文所示。并且它也被 BERT 论文、GPT-2 论文等所采用。因此，出于这个原因，我们在
    GPT-2 的再现中采用了这种非线性。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_32.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_32.png)'
- en: Now in more modern networks also like LAMA3 and so on this nonlinearity also
    further changes to SWIGWOO and other variants like that。 But for GPT-2 they use
    this approximate GALU。 Okay and finally we have the attention operation。 So let
    me paste in my attention。 So I know this is a lot so I'm going to go through this
    a bit quickly。 A bit slowly but not too slowly because we have covered this in
    the previous video and I would just punch you there。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在更现代的网络中，比如 LAMA3 等，这种非线性进一步变化为 SWIGWOO 和其他类似的变体。但对于 GPT-2，它们使用这种近似的 GALU。好吧，最后我们有了注意力操作。让我粘贴我的注意力。所以我知道这很多，我会稍微快一点地讲解，但又不会太慢，因为我们在之前的视频中已经覆盖过这个内容，我只是提醒你一下。
- en: So this is the attention operation。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是注意力操作。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_34.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_34.png)'
- en: Now in the previous video you will remember this is not just attention。 This
    is multi-headed attention。 And so in the previous video we had this multi-headed
    attention module。 And this implementation made it obvious that these heads are
    not actually that complicated。 There's basically in parallel inside every attention
    block。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在之前的视频中，你会记得这不仅仅是注意力。这是多头注意力。在之前的视频中，我们有这个多头注意力模块。这种实现使得这些头实际上并没有那么复杂。它们基本上是并行存在于每个注意力块内。
- en: There's multiple heads and they're all functioning in parallel and their outputs
    are just being concatenated。 And that becomes the output of the multi-headed attention。
    So the heads are just kind of like parallel streams and their outputs get concatenated。
    And so it was very simple and made the head be kind of like fairly straightforward
    in terms of its implementation。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个头，它们都并行工作，输出被串联。然后这成为多头注意力的输出。因此，头就像并行流，它们的输出被串联在一起。因此，这非常简单，使得头的实现相对直接。
- en: What happens here is that instead of having two separate modules and indeed
    many more modules that get concatenated。 all of that is just put into a single
    self-attention module。 And instead I'm being very careful and doing a bunch of
    transpose split tensor gymnastics to make this very efficient impact torch。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的事情是，与其有两个独立的模块，实际上还有更多的模块被串联，所有这些都被放入一个单一的自注意力模块中。相反，我非常小心地进行了大量的转置、分割张量的操作，以使这个在
    PyTorch 中非常高效。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_36.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_36.png)'
- en: But fundamentally and algorithmically nothing is different from the implementation
    we saw before in this give repository。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但从根本上讲，算法上与我们之前在这个给定的仓库中看到的实现没有什么不同。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_38.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_38.png)'
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_39.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_39.png)'
- en: So to remind you very briefly and I don't want to go into this in too many too
    much time。 but we have these tokens lined up in a sequence and there's one thousand
    twenty of them。 And then each token at this stage of the attention emits three
    vectors， the query key and the value。 And first what happens here is that the
    queries and the keys have to multiply each other to get sort of the attention
    amount。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想简单提醒一下，不想花太多时间在这个上面。我们有这些令牌排成一个序列，总共有一千二十个。在这个阶段，每个令牌的注意力会发出三个向量，查询、键和值。首先发生的事情是，查询和键必须相互相乘，以获得注意力的数量。
- en: Like how interesting they find each other。 So they have to interact multiplicatively。
    So we're doing here as we're calculating the qkv， we're splitting it and then
    there's a bunch of gymnastics as I mentioned here。 And the way this works is that
    we're basically making the number of heads， an H。 into a batch dimension。 And
    so it's a batch dimension just like B so that in these operations that follow。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就像他们对彼此的有趣程度。因此他们必须以乘法的方式进行交互。在这里我们在计算 qkv 时进行拆分，然后正如我在这里提到的，有很多技巧。这种工作方式是我们基本上将头数
    H 作为批次维度。所以它是一个批次维度，就像 B 一样，以便在后续的操作中。
- en: PyTorch treats B and H as batches and it applies all the operations on all of
    them in parallel in both the batch and the heads。 And the operations that can
    apply are number one， the queries and the keys interact to give us our attention。
    This is the autoregressive masks that make sure that the tokens only attend to
    tokens before them and never to tokens in the future。 The softmax here normalizes
    the attention so it sums to one always。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 将 B 和 H 视为批次，并在批次和头部上并行应用所有操作。可以应用的操作有，首先，查询和键进行交互，以提供我们的注意力。这是自回归掩码，确保标记仅关注它们之前的标记，而不会关注未来的标记。这里的
    softmax 正常化注意力，使其始终总和为一。
- en: And then recall from the previous video that doing the attention matrix multiplied
    with the values is basically a way to do a weighted sum of the values of the tokens
    that we found in terms of the values。 And then the final transpose contiguous
    and view is just reassembling all of that again。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后回想一下之前的视频，做注意力矩阵与值相乘基本上是一种对我们找到的标记值的加权求和方法。最后的转置连续视图只是将所有内容重新组装在一起。
- en: And this actually performs the concatenation operation。 So you can step through
    this slowly if you'd like。 But it is equivalent mathematically to our previous
    implementation。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 而这实际上执行了连接操作。如果你愿意，可以慢慢查看这一点。但在数学上它等同于我们之前的实现。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_41.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_41.png)'
- en: It's just more efficient in PyTorch。 So that's why I chose this implementation
    instead。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中这更高效。这就是我选择这个实现的原因。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_43.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_43.png)'
- en: Now in addition to that， I'm being careful with how I name my variables。 So
    for example。 CATEN is the same as CATEN。 And so actually our keys should basically
    exactly follow the schema of the hugging face transformers code。 And that will
    make it very easy for us to now port over all the weights from exactly this sort
    of naming conventions。 Because all of our variables are named the same thing。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在除了这一点，我对变量命名非常谨慎。例如，CATEN 与 CATEN 是相同的。因此实际上我们的键应该完全遵循 hugging face transformers
    代码的模式。这将使我们现在轻松地将所有权重移植过来，因为我们所有变量的命名都是相同的。
- en: But at this point we have finished the dgpt2 implementation。 And what that allows
    us to do is we don't have to basically use this file from hugging face。 which
    is fairly long。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这一点上，我们已经完成了 dgpt2 的实现。这使我们无需使用来自 hugging face 的这个文件，而这个文件相当长。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_45.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_45.png)'
- en: This is 2000 lines of code。 Instead we just have less than 100 lines of code。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 2000 行代码。相反，我们只需要不到 100 行代码。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_47.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_47.png)'
- en: And this is the complete gpt2 implementation。 So at this stage we should just
    be able to take over all the weights。 set them， and then do generation。 So let's
    see what that looks like。 Okay。 so here I've also changed the gpt config so that
    the numbers here。 the hyperparameters agree with the gpt2， 124， and model。 So
    the maximum sequence length。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的 gpt2 实现。因此在这个阶段，我们应该能够接管所有权重。设置它们，然后进行生成。那么让我们看看这是什么样子。好的，我在这里也修改了 gpt
    配置，以便这里的数字，超参数与 gpt2、124 和模型一致。所以最大序列长度。
- en: which I call block size here， is 124。 The number of tokens is 5257。 which if
    you watch my token as a video， know that this is 50，000 mergers， Bp mergers。 256
    byte tokens， the leaves of the bp3， and one special end of text token that delimits
    different documents and can start generation as well。 And there are 12 layers，
    there are 12 heads in the attention and the dimension of the transformers was
    768。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我称之为块大小，这里是 124。标记数量是 5257。如果你看过我的标记视频，知道这是 50,000 个合并，Bp 合并。256 字节标记，bp3 的叶子，以及一个特殊的结束文本标记，用于区分不同文档，并可以开始生成。共有
    12 层，注意力中有 12 个头，变换器的维度是 768。
- en: So here's how we can now load the parameters from hugging face to our code here
    and initialize the gpt class with those parameters。 So let me just copy paste
    a bunch of code here。 And I'm not going to go through this code too slow。 too
    quickly， too slowly， because honestly it's not that interesting， it's not that
    exciting。 we're just loading the weights， so it's kind of dry。 But as I mentioned。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将参数从 Hugging Face 加载到我们的代码中，并用这些参数初始化 GPT 类。让我复制粘贴一堆代码。我不会太快或太慢地讲解这段代码，因为说实话，它并不太有趣，不太激动人心。我们只是加载权重，所以这有点乏味。但正如我提到的。
- en: there are four models in this mini series of gpt2。 This is some of the jupyter
    code code that we had here on the right， I'm just porting it over。 These are the
    hyper parameters of the gpt2 models。 We're creating the config object and creating
    our own model。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 GPT2 的迷你系列中有四个模型。这是我们在右侧的 Jupyter 代码，我只是将其移植过来。这些是 GPT2 模型的超参数。我们正在创建配置对象并创建我们自己的模型。
- en: And then what's happening here is we're creating the state dict。 both for our
    model and for the hugging face model。 And then what we're doing here is we're
    going over the hugging face model keys and we're copying over those tensors。 And
    in the process， we are kind of ignoring a few of the buffers。 They're not parameters。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来这里发生的事情是，我们为我们的模型和 Hugging Face 模型创建状态字典。然后我们遍历 Hugging Face 模型的键，并复制那些张量。在这个过程中，我们忽略了一些缓冲区，它们不是参数。
- en: they're buffers。 So for example， attention to bias， that's just used for the
    R aggressive mask。 And so we are ignoring some of those masks。 And that's it。
    And then one additional kind of annoyance is that this comes from the tensor flow
    repo and I'm not sure how this is a little bit annoying。 but some of the weights
    are transposed from what PyTorch would want。 And so manually。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是缓冲区。例如，注意力偏差仅用于 R aggressive mask。因此我们忽略了其中一些掩码。就是这样。还有一个额外的烦恼是这来自 TensorFlow
    仓库，我不太确定这有点烦人，但一些权重的转置与 PyTorch 想要的不同。因此，我们手动处理。
- en: I hard coded the weights that should be transposed and then we transpose them
    if that is so。 And then we return this model。 So the firm pre-trained is a constructor
    or a class method in Python that returns the GPT object if we just give it the
    model type。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我硬编码了应该转置的权重，然后如果需要就转置它们。然后我们返回这个模型。因此，firm pre-trained 是一个构造函数或 Python 中的类方法，如果我们只给定模型类型，它将返回
    GPT 对象。
- en: which in our case is GPT2， the smallest model that we're interested in。 So this
    is the code and this is how you would use it。 And we can pop open a terminal here
    in VS code and we can Python train GPT2。py and fingers crossed。 Okay， so we didn't
    crash。 And so we can load the weights and the biases and everything else into
    our and then module。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中是 GPT2，这是我们感兴趣的最小模型。这是代码，这就是你如何使用它。我们可以在 VS Code 中打开一个终端，运行 Python train
    GPT2.py，希望一切顺利。好的，我们没有崩溃。因此，我们可以加载权重、偏差和其他所有内容到我们的模块中。
- en: But now let's also get additional confidence that this is working and let's
    try to actually generate from this model。 Okay， now before we can actually generate
    from this model， we have to be able to forward it。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在让我们进一步确认这个模型的有效性，并尝试实际生成内容。在我们能够从这个模型生成内容之前，我们必须能够向前传播。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_49.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_49.png)'
- en: We didn't actually write that code yet。 So here's the forward function。 So the
    input to the forward is going to be our indices， our tokens， token indices。 And
    they are always of shape B by T。 And so we have batch dimension of B。 And then
    we have the time dimension of up to T。 And the T can be more than the block size。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上还没有编写那段代码。所以这是向前传播函数。向前传播的输入将是我们的索引、标记和标记索引。它们的形状始终为 B x T。因此，我们有批次维度 B，然后我们有最多
    T 的时间维度。T 可以大于块大小。
- en: The block size is the maximum sequence length。 So B by T indices arranged a
    sort of like a two dimensional layout。 And remember that basically every single
    row of this is of size up to block size。 And this is T tokens that are in a sequence。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小是最大序列长度。因此，B 和 T 的索引排列成一种类似二维布局的形式。请记住，基本上每一行的大小都不超过块大小。这是序列中的 T 个标记。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_51.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_51.png)'
- en: And then we have B independent sequences stacked up in a batch so that this
    is efficient。 Now here we are forwarding the position embeddings and the token
    embeddings。 And this code should be very recognizable from the previous lecture。
    So we basically use a range。 which is kind of like a version of range but for
    PyTorch。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有 B 个独立序列堆叠在一个批次中，这样是高效的。现在这里我们正在前向传播位置嵌入和 token 嵌入。这段代码应该与之前的讲座非常相似。所以我们基本上使用一个范围，它有点像
    PyTorch 的一个版本。
- en: And we're iterating from zero to T and creating this positions sort of indices。
    And then we are making sure that they're unseen devices IDX because we're not
    going to be training on only CPU。 That's going to be too inefficient。 We want
    to be training on GPU and it's going to come in a bit。 Then we have the position
    embeddings and token embeddings and the addition operation of those two。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从零迭代到 T，创建这些位置的索引。然后确保它们是不可见的设备 IDX，因为我们不会仅在 CPU 上训练。这将太低效。我们希望在 GPU 上进行训练，稍后会实现。然后我们有位置嵌入和
    token 嵌入，以及这两者的加法操作。
- en: Now notice that the position embeddings are going to be identical for every
    single row of the input。 And so there's broadcasting hidden inside this plus where
    we have to create an additional dimension here。 And then these two add up because
    the same position embeddings apply to every single row of our examples stacked
    up in a batch。 Then we forward the transformer blocks and finally the last layer
    norm and the LMD。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意，位置嵌入在输入的每一行都是相同的。因此，在这个加法中隐藏着广播，我们必须在这里创建一个额外的维度。然后这两个相加，因为相同的位置嵌入适用于我们在批次中堆叠的每一行示例。接着我们通过
    transformer 块，最后是最后的层归一化和 LMD。
- en: So what comes out after forward is the logits。 And if the input was B by T indices。
    then at every single B by T we will calculate the logits for what token comes
    next in the sequence。 So what is the token B T plus 1， the one on the right of
    this token。 And book half size here is the number of possible tokens。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在前向传播后得到的是 logits。如果输入是 B 乘 T 的索引，那么在每个 B 乘 T 的位置我们都会计算下一个序列中哪个 token 会出现。因此，token
    B T 加 1 是这个 token 右侧的那个。而这里的半个书的大小是可能的 token 数量。
- en: And so therefore this is the tensor that we're going to obtain。 And these logits
    are just a softmax away from coming probabilities。 So this is the forward pass
    of the network and now we can get logits and so we're going to be able to generate
    from the model。 Okay， so now we're going to try to set up the identical thing
    on the left here that matches hugging face on the right。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们将获得的张量。这些 logits 只需通过 softmax 转换即可得出概率。这是网络的前向传播，现在我们可以得到 logits，所以我们能够从模型中生成内容。好的，现在我们将尝试在左侧设置与右侧
    hugging face 匹配的相同内容。
- en: So here we sampled from the pipeline and we sampled five times up to 30 tokens
    with a prefix of hello on the language model。 And these are the completions that
    we achieved。 So we're going to try to replicate that on the left here。 So number
    of term sequences is five max length is 30。 So the first thing we do of course
    is we initialize our model。 Then we put it into evaluation mode。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从管道中抽样，最多抽样五次以获得 30 个 token，并在语言模型中以 hello 作为前缀。这是我们得到的完成内容。所以我们将在左侧尝试复制这一点。因此，序列的数量是五，最大长度是
    30。因此我们首先做的当然是初始化我们的模型。然后我们将其置于评估模式。
- en: Now this is a good practice to put the model into eval when you're not going
    to be training it。 You're just going to be using it。 And I don't actually know
    if this is doing anything right now for the following reason。 Our model up above
    here contains no modules or layers that actually have a different behavior at
    training or evaluation time。 So for example dropout， batch alarm and a bunch of
    other layers have this kind of behavior。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，把模型置于评估模式是一个好习惯，当你不打算训练时，只是要使用它。我实际上不知道这是否在起作用，原因是我们上面提到的模型不包含在训练或评估时行为不同的模块或层。例如，dropout、batch
    norm 和其他一些层具有这种行为。
- en: But all of these layers that we've used here should be identical in both training
    and evaluation time。 So potentially model that eval is nothing but then I'm not
    actually sure if this is the case and maybe pytorch internals do some clever things
    depending on the evaluation mode inside here。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们在这里使用的所有层在训练和评估时应该是相同的。因此，评估模型潜在上并不是什么特殊情况，我不确定是否确实如此，也许 pytorch 内部在这里根据评估模式做了一些聪明的处理。
- en: The next thing we're doing here is we are moving the entire model to CUDA。 So
    we're moving this all of the tensors to GPU。 So I'm SSH'd here to a cloud box
    and I have a bunch of GPUs on this box。 And here I'm moving the entire model and
    all of its members and all of its tensors and everything like that。 Everything
    gets shipped off to basically a whole separate computer that is sitting on the
    GPU。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要做的是将整个模型移动到 CUDA。也就是说，我们将所有张量移动到 GPU。我在这里通过 SSH 连接到一个云服务器，这个服务器上有很多 GPU。在这里，我正在移动整个模型及其所有成员和所有张量，所有这些都被发送到一个基本上独立的计算机，它坐落于
    GPU 上。
- en: And the GPU is connected to the CPU and they can communicate but it's basically
    a whole separate computer with its own computer architecture。 And it's really
    a well catered to parallel processing tasks like those of running neural networks。
    So I'm doing this so that the model lives on the GPU。 a whole separate computer
    and it's just going to make our code a lot more efficient because all of this
    stuff runs a lot more efficiently on the GPUs。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 连接到 CPU，它们可以通信，但基本上是一个独立的计算机，拥有自己的计算机架构。它非常适合并行处理任务，比如运行神经网络。所以我这样做是为了让模型在
    GPU 上运行，整个独立的计算机，这会使我们的代码效率更高，因为所有这些任务在 GPU 上运行得更高效。
- en: So that's the model itself。 Now， the next thing we want to do is we want to
    start with this as the prefix when we do the generation。 So let's actually create
    those prefix tokens。 So here's the code that I've written。 We're going to import
    the tick token library from OpenAI and we're going to get the GPT-2 encoding。
    So that's the tokenizer for GPT-2。 And then we're going to encode this string
    and get a list of integers which are the tokens。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型本身。接下来，我们想要做的是在生成时将这个作为前缀。所以让我们创建这些前缀代币。这是我写的代码。我们将从 OpenAI 导入 tick token
    库，并获取 GPT-2 编码。也就是 GPT-2 的分词器。然后我们将编码这个字符串，获取一个整数列表，这些就是代币。
- en: Now these integers here should actually be fairly straightforward because we
    can just copy paste this string and we can sort of inspect what it is in tick
    tokenizer。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些整数应该相当简单，因为我们可以直接复制粘贴这个字符串，并检查它在 tick tokenizer 中是什么。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_53.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_53.png)'
- en: So just spacing that in， these are the tokens that are going to come out。 So
    this list of integers is what we expect tokens to become。 And as you recall， if
    you saw my video。 of course， all the tokens， they're just little string chunks。
    Right？
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以稍微安排一下，这些是将要输出的代币。这个整数列表就是我们期望代币变成的样子。如你所记，如果你看过我的视频，当然，所有的代币都是小字符串块。对吧？
- en: So these are the chunkation of this string into GPT-2 tokens。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些是将这个字符串切分为 GPT-2 代币。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_55.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_55.png)'
- en: So once we have those tokens， it's a list of integers， we can create a torch
    tensor out of it。 In this case， it's eight tokens。 And then we're going to replicate
    these eight tokens four。 five times to get five rows of eight tokens。 And that
    is our initial input X as I call it here。 And it lives on the GPU as well。 So
    X now is this IDX that we can put into forward to get our logits so that we know
    what comes as the sixth token。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这些代币，它们是一个整数列表，我们可以创建一个 torch 张量。在这个案例中，有八个代币。然后我们将这些八个代币复制五次，以获得五行八个代币。这就是我所称的初始输入
    X。它也运行在 GPU 上。所以现在的 X 是这个 IDX，我们可以将其放入前向传播中，以获取我们的 logits，从而知道第六个代币是什么。
- en: sorry， as the ninth token in every one of these five rows。 Okay， and we are
    now ready to generate。 So let me paste in one more code block here。 So what's
    happening here in this code block is we have these X。 which is a size B by T，
    right？ So batch by time。 And we're going to be in every iteration of this loop。
    we're going to be adding a column of new indices into each one of these rows。
    Right？
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，作为这五行中每一行的第九个代币。好的，现在我们准备生成了。所以让我再粘贴一个代码块。这个代码块中发生的事情是我们有这些 X，大小为 B 乘 T，对吧？所以是批量与时间。在这个循环的每一次迭代中，我们将向每一行添加一列新的索引，对吧？
- en: And so these are the new indices and we're appending them to the sequence as
    we're sampling。 So with each loop iteration， we get one more column into X。 And
    all of the operations happening in the context manager of Torx。no。grad。 this is
    just telling PyTorch that we're not going to be calling that backward on any of
    this。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些是新的索引，我们在采样时将它们附加到序列中。随着每次循环迭代，我们向 X 中添加一列。在 Torx 的上下文管理器中执行所有操作，.no.grad
    这只是告诉 PyTorch，我们不会在任何这些操作上调用反向传播。
- en: So it doesn't have to cache all the intermediate tensors。 It's not going to
    have to prepare in any way for a potential backward later。 And this saves a lot
    of space and also possibly some time。 So we get our logits。 We get the logits
    at only the last location。 We throw away all the other logits。 We don't need them。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它不需要缓存所有的中间张量。也不需要为后续的潜在反向传播做任何准备。这节省了很多空间，也可能节省一些时间。所以我们得到 logits。我们只在最后的位置获取
    logits。我们丢弃所有其他 logits。我们不需要它们。
- en: We only care about the last column's logits。 So this is being wasteful。 but
    this is just kind of like an inefficient implementation of sampling。 So it's correct
    but inefficient。 So we get the last column of logits。 pass it through softmax
    to get our probabilities。 Then here I'm doing top case sampling of 50。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心最后一列的 logits。因此这有些浪费，但这只是一种低效的采样实现。虽然是正确的，但效率不高。所以我们获取最后一列的 logits，通过 softmax
    得到我们的概率。然后在这里我进行 50 的 top K 采样。
- en: And I'm doing that because this is the hugging face default。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我这样做是因为这是 Hugging Face 的默认设置。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_57.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_57.png)'
- en: So just looking at the hugging face dogs here of a pipeline。 There's a bunch
    of quarks that go into hugging face。 And it's kind of a lot honestly。 But I guess
    the important one that I noticed is that they're using top K by default， which
    is 50。 And what that does is that's being used here as well。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以看看这里 Hugging Face 的管道。有一堆小粒子进入 Hugging Face。老实说，这有点多。但我想我注意到的重要一点是，他们默认使用
    top K，数量是 50。这在这里也在使用。
- en: And what that does is basically we want to take our probabilities and we only
    want to keep the top 50 probabilities。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的基本目的是我们想保留我们的概率，并且只想保留前 50 个概率。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_59.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_59.png)'
- en: And anything that is lower than the 50th probability， we just clamped to zero
    and renormalized。 And so that way we are never sampling very rare tokens。 The
    tokens we're going to be sampling are always in the top 50 of most likely tokens。
    And this helps keep the model kind of on track and it doesn't blabber on and it
    doesn't get lost and doesn't go off the rails as easily。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 任何低于第 50 个概率的标记，我们都将其限制为零并重新归一化。这样一来，我们就不会采样非常罕见的标记。我们要采样的标记总是在最可能标记的前 50 个中。这有助于保持模型在轨道上，不会喋喋不休，也不容易迷失方向或偏离主题。
- en: And it kind of like sticks in the vicinity of likely tokens a lot better。 So
    this is the way to do it in PyTorch and you can step through it if you like。 I
    don't think it's super insightful so I'll speed through it。 But roughly speaking
    we get this new column of tokens。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 它在可能的标记附近的表现要好得多。所以这是在 PyTorch 中的做法，如果你愿意，可以逐步了解。我认为这不是特别有见地，所以我会快速过一遍。但大致来说，我们得到了这一列新的标记。
- en: We append them on X and basically the columns of X grow until this while loop
    gets tripped up。 And then finally we have an entire X of size。 Five by 30 in this
    case in this example。 And we can just basically print all those individual rows。
    So I'm getting all the rows。 I'm getting all the tokens that are sampled and I'm
    using the decode function from tick tokness or to get back the string which we
    can print。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们附加到 X 上，基本上 X 的列增长，直到这个 while 循环被触发。最终，我们得到了一个大小为 5x30 的完整 X。在这个例子中。我们基本上可以打印出所有这些单独的行。所以我得到了所有的行。我得到了所有被采样的标记，并使用
    tick tokness 的解码函数将其转换回字符串，以便我们可以打印。
- en: And so terminal， new terminal。 And let me Python train GPT to。 Okay。 so these
    are the generations that we're getting。 Hello， I'm a language model， not a program。
    New line， new line， etc。 Hello， I'm a language model and one of the main things
    that bothers me when they create languages is how easy it becomes to create something
    that。 I mean， so this will just like blabber on right in all these cases。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以终端，新的终端。让我用 Python 训练 GPT。好的。这就是我们得到的生成结果。你好，我是一个语言模型，不是一个程序。新行，新行，等等。你好，我是一个语言模型，当他们创造语言时，令我困扰的主要事情之一是创造某种东西是多么容易。我是说，这样一来，它就会在所有这些情况下喋喋不休。
- en: Now one thing you will notice is that these generations are not the generations
    of fucking face here。 And I can't find the discrepancy to be honest and I didn't
    fully go through all these options but probably there's something else hiding
    in addition to the top P。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，这些生成并不是hugging face的生成。说实话，我找不到这个差异，我没有完全检查所有这些选项，但可能还有其他东西在top P之外隐藏。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_61.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_61.png)'
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_62.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_62.png)'
- en: So I'm not able to match it up but just for correctness down here below in the
    Jupyter notebook and using the hugging face model。 So this is the hugging face
    model here。 I was， I replicated the code and if I do this and I run that。 then
    I'm getting the same results。 So basically the model intervals are not wrong。
    It's just I'm not 100% sure what the pipeline does in hugging face and that's
    why we're not able to match them up。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我无法将其匹配，但为了确保正确性，在Jupyter notebook中使用hugging face模型。所以这里是hugging face模型。我复制了代码，如果我这样做并运行它。那么我得到的结果是相同的。因此，基本上模型的间隔并没有错。只是我不完全确定hugging
    face的管道做了什么，这就是为什么我们无法将它们匹配。
- en: But otherwise the code is correct and we've loaded all the tensors correctly。
    So we're initializing the model correctly and everything here works。 So once very
    short。 we've ported all the weights， we initialize the GPT to。 This is the exact
    opening on GPT to and it can generate sequences and they look sensible。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但除此之外，代码是正确的，我们已经正确加载了所有的张量。所以我们正确地初始化了模型，这里的一切都在正常工作。因此，一切都很简短。我们已经移植了所有权重，初始化了GPT。这里是GPT的确切启动，它能够生成看起来合理的序列。
- en: And now here of course we're initializing with GPT to model weights。 But now
    we want to initialize from scratch from random numbers and we want to actually
    train the model that will give us sequences as good as or better than these ones
    in quality。 And so that's what we turn to next。 So it turns out that using the
    random model is actually fairly straightforward because PyTorch already initializes
    our model randomly and by default。 So when we create the GPT model in the constructor。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这里当然是用GPT的模型权重进行初始化。但现在我们想从随机数开始初始化，实际上训练模型，以生成质量与这些序列一样好或更好的序列。因此，这就是我们接下来要做的。事实证明，使用随机模型实际上相当简单，因为PyTorch已经默认随机初始化我们的模型。因此，当我们在构造函数中创建GPT模型时。
- en: this is all of these layers and modules have random initializers that are there
    by default。 So when these linear layers get created and so on， there's default
    constructors。 for example using the Javier initialization that we saw in the past
    to construct the weights of these layers。 And so creating a random model instead
    of a GPT to model is actually fairly straightforward and we would just come here
    and instead we would create model equals GPT。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些层和模块都有默认的随机初始化器。因此，当这些线性层被创建等时，会有默认构造器。例如，使用我们之前看到的哈维尔初始化来构建这些层的权重。因此，创建一个随机模型而不是GPT模型实际上相当简单，我们只需在这里创建模型等于GPT。
- en: And then we want to use the default config GPT config and the default config
    uses the 124m parameters。 So this is the random model initialization and we can
    run it。 And we should be able to get results。 Now the results here of course are
    total garbage-carbel and that's because it's a random model。 And so we're just
    getting all these random token string pieces chunked up totally at random。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们想要使用默认的GPT配置，默认配置使用124m参数。因此，这是随机模型初始化，我们可以运行它。我们应该能够得到结果。当然，这里的结果完全是垃圾——这是因为它是一个随机模型。所以我们只是随机地得到了这些随机的标记字符串片段。
- en: So that's what we have right now。 Now one more thing I wanted to point out by
    the way is in case you do not have CUDA available because you don't have a GPU。
    you can still follow along with what we're doing here to some extent。 And probably
    not to the very end because by the end we're going to be using multiple GPUs and
    actually doing a serious training run。 But for now you can actually follow along
    decently okay。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们现在拥有的。顺便说一句，我想指出的是，如果你没有CUDA可用，因为你没有GPU。你仍然可以在某种程度上跟随我们正在做的事情。可能到最后不能完全跟上，因为到最后我们将使用多个GPU进行实际的训练。但是现在你实际上可以相当好地跟随。
- en: So one thing that I like to do in PyTorch is I like to auto detect the device
    that is available to you。 So in particular you could do that like this。 So here
    we are trying to detect the device to run on that has the highest compute capability。
    You can think about it that way。 So by default we start with CPU which of course
    is available everywhere because every single computer will have a CPU。 But then
    we can try to detect the heavy GPU if so use a CUDA。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我在PyTorch中喜欢做的一件事是自动检测可用的设备。因此特别是你可以这样做。在这里我们尝试检测可运行的设备，以便具有最高的计算能力。你可以这样考虑。所以默认情况下，我们从CPU开始，当然这在任何地方都是可用的，因为每台计算机都会有CPU。但然后我们可以尝试检测强大的GPU，如果有的话，就使用CUDA。
- en: And then if you don't have a CUDA do you at least have MPS。 MPS is the backend
    for Apple Silicon。 So if you have a MacBook that is fairly new you probably have
    Apple Silicon on the inside。 And then that has a GPU that is actually fairly capable
    depending on which MacBook you have。 And so you can use MPS which will be potentially
    faster than CPU。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有CUDA，至少你有MPS吗？MPS是Apple Silicon的后端。如果你有一台相对较新的MacBook，你可能内部有Apple Silicon。这种设备的GPU实际上相当强大，具体取决于你拥有的MacBook。因此你可以使用MPS，这可能比CPU更快。
- en: And so we can print the device here。 Now once we have the device we can actually
    use it in place of CUDA。 So we just swap it in and notice that here when we call
    model on X if this X here is on CPU instead of GPU then it will work fine because
    here in the forward which is where PyTorch will come。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以在这里打印设备。现在一旦我们有了设备，就可以在CUDA的位置上使用它。因此我们只需替换，并注意当我们在X上调用模型时，如果这里的X是在CPU而不是GPU上，那么它也能正常工作，因为在前向传播中PyTorch会这样处理。
- en: When we created Pause we were careful to use the device of IDX to create this
    tensor as well。 And so there won't be any mismatch where one tensor is on CPU
    and GPU and you can't combine those。 But here we are carefully initializing on
    the correct device as indicated by the input to this model。 So this will auto
    detect device。 For me this will be of course GPU。 So using device CUDA。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建Pause时，我们谨慎地使用IDX设备来创建这个张量。因此不会出现一个张量在CPU上而另一个在GPU上的不匹配，导致无法结合的情况。但在这里，我们小心地根据输入模型的指示在正确的设备上初始化。因此，这将自动检测设备。对我来说，这当然是GPU。所以使用设备CUDA。
- en: But you can also run with as I mentioned another device。 And it's not going
    to be too much slower。 So if I override device here。 If I override device = CPU
    then we'll swap print CUDA of course but now we're actually using CPU。 One， two，
    three， four， five， six。 Okay about six seconds。 And actually we're not using torch
    compile and stuff like that which will speed up everything a lot faster as well。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如我提到的，你也可以使用另一个设备。这样不会慢太多。所以如果我在这里覆盖设备。如果我将设备重写为CPU，那么我们当然会交换打印CUDA，但现在我们实际上是在使用CPU。一个，二，三，四，五，六。好吧，大约六秒。实际上我们没有使用torch
    compile等，这样可以大大加快速度。
- en: But you can't follow along even on a CPU I think to a decent extent。 So that's
    a note on that。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使在CPU上，你也不能完全跟上，我认为这在一定程度上是个问题。所以这是一个说明。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_64.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_64.png)'
- en: Okay so I do want to loop around eventually into what it means to have different
    devices in PyTorch。 And what this exactly that PyTorch does in the background
    for you when you do something like module。to device， or where you take a torch
    tensor and do a 。to device。 And what exactly happens and how that works。 But for
    now I'd like to get to training and I'd like to start training the model。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我确实想最终回到在PyTorch中拥有不同设备意味着什么。当你执行类似module.to device或将torch张量用.to device时，PyTorch在后台为你做了什么，以及这到底发生了什么，如何运作。但现在我想直接进入训练，开始训练模型。
- en: And for now let's just say the device makes code go fast。 And let's go into
    how we can actually train the model。 So to train the model we're going to need
    some data set。 And for me the best debugging simplest data set that I like to
    use is the tiny Shakespeare data set。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们先说这个设备可以让代码运行得很快。接下来我们来讨论如何实际训练模型。为了训练模型，我们需要一些数据集。对我来说，我喜欢使用的最简单的调试数据集是小型的莎士比亚数据集。
- en: And it's available at this URL so you can W get it or you can just search tiny
    Shakespeare data set。 And so I have in my file system as just tell us input。txt。
    So I already downloaded it。 And here I'm reading the data set getting the first
    1000 characters and printing the first 100。 Now remember that GPT2 has roughly
    a compression ratio。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在这个URL获取，所以你可以W获取它，或者直接搜索tiny Shakespeare数据集。因此，我在我的文件系统中将其命名为input.txt。我已经下载好了。在这里我读取数据集，获取前1000个字符并打印前100个。请记住，GPT2大约有一个压缩比。
- en: The tokenizer has a compression ratio of roughly 3 to 1。 So 1000 characters
    is roughly 300 tokens here that will come out of this in the slides that we're
    currently getting。 So this is the first few characters。 And if you want to get
    a few more statistics on this we can do word count on input。txt。 So we can see
    that this is 40，000 lines， about 200，000 words in this data set。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该tokenizer的压缩比大约为3比1。因此，1000个字符大约会产生300个token，这些token将来自我们目前正在获取的幻灯片。这是前几个字符。如果你想获取更多统计信息，我们可以对input.txt进行字数统计。因此，我们可以看到这个数据集有40000行，约200000个单词。
- en: and about 1 million bytes in this file。 And knowing that this file is only ASCII
    characters。 There's no crazy Unicode here as far as I know。 And so every ASCII
    character is encoded with one byte。 And so this is the same number roughly a million
    characters inside this data set。 So this is the data set size by default， very
    small and minimal data set for debugging to get us off the ground。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件大约有1百万字节，并且知道这个文件只有ASCII字符。到目前为止我知道这里没有疯狂的Unicode。因此，每个ASCII字符用一个字节编码。这与这个数据集中的大约一百万字符相同。所以这是默认的数据集大小，一个非常小且最小的数据集，用于调试，以便我们起步。
- en: In order to tokenize this data set we're going to get tiktoken encoding for
    GPT2， encode the data。 the first 1000 characters， and then I'm only going to print
    the first 24 tokens。 So these are the tokens as a list of integers。 And if you
    can read GPT2 tokens you will see that 198 here you'll recognize that as the slashing
    character。 So that is a new line。 And then here for example we have two new lines
    so that's 198 twice here。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这个数据集进行标记化，我们将获取tiktoken编码以供GPT2使用，编码前1000个字符，然后我只会打印前24个token。这些token是整数列表。如果你能阅读GPT2的token，你会看到198，你会认识到这是换行符。因此这里有两个换行符，所以这里198出现了两次。
- en: So this is just the tokenization of the first 24 tokens。 So what we want to
    do now is we want to actually process these token sequences and feed them into
    a transformer。 And in particular we want them， we want to rearrange these tokens
    into this IDX variable that we're going to be feeding into the transformer。 So
    we don't want a single very long one dimensional sequence。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是前24个token的标记化。所以我们现在想要做的是实际上处理这些token序列并将其输入到transformer中。特别是我们希望将这些token重新排列成我们将输入到transformer中的IDX变量。因此，我们不想要一个非常长的一维序列。
- en: We want an entire batch where each sequence is up to。 it's basically t tokens
    and t cannot be larger than the maximum sequence length。 And then we have these
    t long sequence of tokens and we have b independent examples of sequences。 So
    how can we create a b by t tensor that we can feed into the forward out of these
    one dimensional sequences？
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望整个批次中的每个序列都能达到。基本上是t个token，而t不能大于最大序列长度。然后我们有这些t长的token序列，还有b个独立的序列示例。那么我们如何创建一个b乘t的tensor，以便将这些一维序列输入到前向计算中呢？
- en: So here's my favorite way to achieve this。 So if we take torch and then we create
    a tensor object out of this list of integers and just the first 24 tokens。 My
    favorite way to do this is basically you do a dot view of。 for example 4 by 6
    which multiply to 24。 And so it's just a two dimensional rearrangement of these
    tokens。 And you'll notice that when you view this one dimensional sequence as
    two dimensional 4 by 6 here。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我最喜欢的实现方式。如果我们使用torch，然后从这个整数列表创建一个tensor对象，并且只取前24个token。我最喜欢的做法是基本上执行一个点视图，例如4乘6，结果为24。因此，这只是这些token的二维重新排列。当你将这个一维序列视为二维4乘6时，你会注意到。
- en: the first 6 tokens up to here end up being the first row。 the next 6 tokens
    here end up being the second row and so on。 And so basically it's just going to
    stack up every 6 tokens in this case as independent rows and it creates a batch
    of tokens in this case。 And so for example if we are token 25 in the transformer
    when we feed this in and this becomes the IDX。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的前6个标记最终成为第一行。接下来的6个标记成为第二行，依此类推。因此，基本上在这种情况下，每6个标记将堆叠为独立的行，并创建一个标记批次。因此，例如，如果我们在转换器中是标记25，当我们输入时这将成为IDX。
- en: this token is going to see these three tokens and it's going to try to predict
    that 198 comes next。 So in this way we are able to create this two dimensional
    batch that's quite nice。 Now in terms of the label that we're going to need for
    the target to calculate the loss function。 how do we get that？ Well we could write
    some code inside the forward pass because we know that the next token in a sequence
    which is the label is just to the right of us。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标记将看到这三个标记，并将尝试预测接下来是198。因此，我们能够创建这个相当不错的二维批次。至于我们需要的标签以计算损失函数，我们该如何获取呢？我们可以在前向传播中写一些代码，因为我们知道序列中的下一个标记即标签就在我们右侧。
- en: And you'll notice that actually for this token at the very end 13 we don't actually
    have the next correct token because we didn't load it。 So we actually didn't get
    enough information here。 So I'll show you my favorite way of basically getting
    these batches and I like to personally have not just the input to the transformer
    which I like to call X。 But I also like to create the labels tensor which is of
    the exact same size as X but contains the targets at every single position。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，实际上对于这个最后的标记13，我们并没有加载下一个正确的标记，因为我们没有加载它。因此我们实际上没有获取到足够的信息。接下来我会展示我最喜欢的获取这些批次的方式，我个人喜欢的不仅是输入到转换器的部分，我称之为X。同时我也喜欢创建一个与X大小完全相同的标签张量，但在每个位置都包含目标。
- en: And so here's the way that I like to do that。 I like to make sure that I fetch
    +1 token because we need the ground truth for the very last token for 13。 And
    then when we're creating the input we take everything up to the last token not
    including and view it as for my 6。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我喜欢的做法。我会确保获取+1个标记，因为我们需要最后一个标记13的真实值。然后在创建输入时，我们获取所有直到最后一个标记（不包括它），并以6的形式展示。
- en: And when we're creating targets we do the buffer but starting at index 1 not
    index 0 so we're skipping the first element and we view it in the exact same size。
    And then when I print this here's what happens where we see that basically as
    an example for this token 25 its target was 198 and that's now just stored at
    the exact same position in the target tensor which is 198。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建目标时，我们使用缓冲区，但从索引1开始而不是索引0，所以我们跳过第一个元素，并以完全相同的大小展示。当我打印这个时，发生的情况是，比如对于这个标记25，它的目标是198，现在正好存储在目标张量中的相同位置，即198。
- en: And also this last token 13 now has its label which is 198 and that's just because
    we loaded this +1 here。 So basically this is the way I like to do it you take
    long sequences you view them in two dimensional terms so that you get batches
    of time。 And then we make sure to load one additional token so we basically load
    a buffer of tokens of b times t +1 and then we sort of offset things and view
    them。 And then we have two tensors one of them is the input to the transformer
    and the other exactly is the labels。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的标记13现在有它的标签，即198，这仅仅是因为我们在这里加载了+1。因此，基本上这是我喜欢的做法，你可以把长序列以二维的方式展示，这样就能获得时间的批次。然后我们确保加载一个额外的标记，所以我们基本上加载了b乘以t加1的标记缓冲区，然后对其进行偏移和展示。我们有两个张量，一个是输入到转换器的，另一个正好是标签。
- en: And so let's now reorganize this code and create a very simple data loader object
    that tries to basically load these tokens and feed them to the transformer and
    calculate the loss。 Okay so I reshuffled the code here accordingly so as you can
    see here I'm temporarily overriding to run on CPU。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们重新组织这段代码，创建一个非常简单的数据加载器对象，基本上加载这些标记并将它们输入到转换器中以计算损失。好的，我在这里相应地重组了代码，正如你所看到的，我暂时覆盖以在CPU上运行。
- en: And importing to token and all of this should look familiar we're loading a
    thousand characters。 I'm setting bt to just b4 and 32 right now just because we're
    debugging we just want to have a single batch that's very small。 And all of this
    should now look familiar and follows what we did on the right。 And then here we
    get the we create the model and get the logits。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 导入到token，所有这些应该很熟悉，我们正在加载一千个字符。我现在将bt设置为b4和32，仅仅是因为我们在调试，我们只想要一个非常小的单批次。所有这些现在应该看起来很熟悉，并遵循我们在右侧所做的。然后在这里我们创建模型并获取logits。
- en: And so here as you see I already ran this only runs in a few seconds but because
    we have a batch of four by 32。 Our logits are now size four by 32 by 50，257。 So
    those are the logits for what comes next at every position。 And now we have the
    labels which are stored in Y。 So now is the time to calculate the loss and then
    do the backward pass and then the optimization。 So let's first calculate the loss。
    Okay so to calculate the loss we're going to adjust the forward function of this
    and in module in the model。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，如你所见，我已经运行过，这仅仅在几秒钟内完成，但因为我们有一个4乘以32的批次。我们的logits现在的大小是4乘以32乘以50,257。这些是每个位置上接下来会发生的logits。现在我们有存储在Y中的标签。因此，现在是计算损失并进行反向传播和优化的时间。那么我们先计算损失。好的，为了计算损失，我们将调整模型中的这个forward函数。
- en: And in particular we're not just going to be returning logits but also we're
    going to return the loss。 And we're going to not just pass in the input in the
    cease but also the targets in Y。 And now we will print not logits at shape anymore
    but we're actually going to print the loss function。 And then syst。exit of zero
    so that we skip some of the sample in logic。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是我们不仅返回logits，还会返回损失。我们不仅将输入传入cease，还将目标传入Y。现在我们不再打印logits的形状，而是打印损失函数。然后是`syst.exit(0)`，以便跳过一些样本逻辑。
- en: So now let's swing up to the forward function which gets called there。 Because
    now we also have these optional targets。 And when we get the targets we can also
    calculate the loss。 And remember that we want to basically return logit's loss
    and loss by default is none but let's put this here。 If targets is not none then
    we want to calculate the loss。
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到forward函数，它在那里被调用。因为现在我们也有这些可选目标。当我们得到目标时，我们也可以计算损失。记住，我们基本上希望返回logits的损失，而默认情况下损失为none，但让我们把它放在这里。如果目标不是none，那么我们想要计算损失。
- en: And copilot is already getting excited here and calculating what looks to be
    correct loss。 It is using the cross entropy loss as is documented here。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 而助手在这里已经开始兴奋地计算，看起来是正确的损失。它正在使用交叉熵损失，具体文档在这里。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_66.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_66.png)'
- en: So this is a function in PyTorch under the functional。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是PyTorch中功能下的一个函数。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_68.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_68.png)'
- en: Now what is actually happening here because it looks a little bit scary。 Basically
    the F。 that cross entropy does not like multi dimensional inputs。 It can't take
    a B by T by vocab size。 So what's happening here is that we are flattening out
    this three dimensional tensor into just two dimensions。 The first dimension is
    going to be calculated automatically and it's going to be B times T。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这里实际发生了什么，因为看起来有点吓人。基本上，F。这个交叉熵不喜欢多维输入。它不能接受B乘以T乘以词汇大小。因此，这里发生的事情是我们将这个三维张量展平为两个维度。第一个维度将自动计算，并且将是B乘以T。
- en: And then the last dimension is vocab size。 So basically this is flattening out
    this three dimensional tensor of logits to just be two dimensional B times T。
    All individual examples and vocab size in terms of the length of each row。 And
    then it's also flattening out the targets which are also two dimensional at this
    stage。 But we're going to just flatten them out so they're just a single tensor
    of B times T。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后最后一个维度是词汇大小。因此，基本上这将三维的logits张量展平为二维的B乘以T。所有单个示例和词汇大小对应于每一行的长度。然后，它也将目标展平，此时目标也是二维的。但我们将把它们展平为单个B乘以T的张量。
- en: And this can then pass into cross entropy to calculate a loss which we return。
    So this should basically at this point run because it's not too complicated。 So
    let's run it and let's see if we should be printing the loss。 And here we see
    that we printed 11 roughly。 And so。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这可以传入交叉熵以计算返回的损失。因此此时基本上应该运行，因为它并不太复杂。让我们运行它，看看是否应该打印损失。在这里我们看到我们大约打印了11。所以。
- en: and notice that this is the tensor of a single element which is this number
    11。 Now we also want to be able to calculate a reasonable kind of starting point
    for a random linearized network。 So we covered this in previous videos but our
    vocabulary size is 50，257。 At initialization of the network you would hope that
    every vocab element is getting roughly a uniform probability。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这是一个单元素的张量，就是这个数字11。现在我们还希望能够为随机线性网络计算一个合理的起始点。我们在之前的视频中覆盖了这一点，但我们的词汇大小是50,257。在网络初始化时，你希望每个词汇元素得到大致均匀的概率。
- en: So that we're not favoring at initialization any token way too much。 We're not
    confidently wrong at initialization。 So we're hoping that the probability of any
    arbitrary token is roughly one over 50。257。 And now we can sanity check the loss。
    Because remember that the cross entropy loss is just basically the negative log
    likelihood。 So if we now take this probability and we take it through the natural
    logarithm and then we do the negative。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们在初始化时不会过于偏向任何标记。我们在初始化时并不会自信地错误。因此我们希望任何任意标记的概率大致为1/50,257。现在我们可以检查损失。因为记住交叉熵损失基本上就是负对数似然。因此如果我们现在将这个概率取自然对数然后取负。
- en: that is the loss we expect at initialization and we covered this in previous
    videos。 So I would expect something around 10。82 and we're seeing something around
    the level。 So it's not way off。 This is roughly the probability I expect at initialization。
    So that tells me that the initialization or probability distribution is roughly
    diffuse。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在初始化时期望的损失，我们在之前的视频中讨论过这个。所以我预计大约在10.82左右，而我们看到的也是这个水平。所以并没有太大的偏差。这大致是我在初始化时所期望的概率。这告诉我初始化或概率分布大致是分散的。
- en: It's a good starting point and we can now perform the optimization and tell
    the network which elements should follow correctly in what order。 So at this point
    we can do a loss。backward， calculate the gradients and do an optimization。 So
    let's get to that。 Okay， so let's do the optimization now。 So here we have the
    loss。 This is how we get the loss。 But now basically we want a load for loop here。
    So for i in range。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的起始点，我们现在可以进行优化，并告诉网络哪些元素应该按正确顺序跟随。所以在这个时候，我们可以进行loss.backward，计算梯度并进行优化。那么我们来开始吧。好的，现在进行优化。所以这里是损失。这是我们获得损失的方式。但现在基本上我们想要一个加载循环。所以对于i的范围。
- en: let's do 50 steps or something like that。 Let's create an optimizer object in
    PyTorch。 And so here we are using the atom optimizer， which is an alternative
    to stochastic gradient descent optimizer S。G。D。 that we're using。 S。G。T。 is a
    lot simpler。 Atom is a bit more involved。 And actually specifically like the atom
    W variation because in my opinion it kind of just like fixes a bug。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做50步之类的。让我们在PyTorch中创建一个优化器对象。所以我们使用的是atom优化器，这是我们使用的随机梯度下降优化器S.G.D.的替代方案。S.G.T.简单得多。Atom稍微复杂一些。实际上特别是像atom
    W的变体，因为在我看来，它就像是修复了一个bug。
- en: So atom W is a bug fix of atom is what I would say。 When we go to the documentation
    for atom W。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为atom W是atom的一个bug修复。当我们查看atom W的文档时。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_70.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_70.png)'
- en: Oh my gosh。 We see that it takes a bunch of parameters and it's a little bit
    more complicated than the S。G。D。 we were looking at before。 Because in addition
    to basically updating the parameters with the gradient scaled by the learning
    rate。 it keeps these buffers around and it keeps two buffers。 The M and the V。
    which it calls the first and the second moment。 So something that looks a bit
    like momentum is something that looks a bit like RMS prop if you're familiar with
    it。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，我的天啊。我们看到它需要一堆参数，且比之前讨论的S.G.D.要复杂一些。因为除了基本上用学习率缩放的梯度来更新参数外，它还保留了一些缓冲区，并且保留了两个缓冲区，M和V，称之为第一和第二动量。所以这看起来有点像动量，也有点像你熟悉的RMS
    prop。
- en: But you don't have to be。 It's just kind of like a normalization that happens
    on each gradient element individually。 and speeds up the optimization， especially
    for language models。 But I'm not going to go into the detail right here。 We're
    going to treat it as a bit of a black box and it just optimizes the objective
    faster than S。G。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 但你并不一定需要这样做。这只是在每个梯度元素上单独发生的归一化，加快了优化，尤其是对于语言模型。但我不会在这里详细讨论。我们会把它视为一个黑箱，它优化目标的速度比S.G.D.快。
- en: D。 which is what we've seen in the previous lectures。 So let's use it as a black
    box in our case。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: D，这就是我们在前面的讲座中看到的。因此，让我们在这种情况下将其视为一个黑箱。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_72.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_72.png)'
- en: Create the optimizer object and then go through the optimization。 The first
    thing to always make sure the copilot did not forget to zero the gradients。 So
    always remember that you have to start with a zero gradient。 Then when you get
    your loss and you do a dot backward， dot backward adds to gradients。
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 创建优化器对象，然后进行优化。首先始终确保副驾驶没有忘记将梯度归零。因此，请始终记住，你必须从零梯度开始。然后当你得到损失并进行dot backward时，dot
    backward会累加梯度。
- en: So it deposits gradients。 It always does a plus equals on whatever the gradients
    are。 which is why you must set them to zero。 So this accumulates the gradient
    from this loss and then we call the step function on the optimizer to update the
    parameters and to decrease the loss。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它会存储梯度。它总是对梯度进行加法运算，这就是为什么你必须将它们设为零。因此，这会累积来自损失的梯度，然后我们调用优化器上的步骤函数来更新参数并减少损失。
- en: And then we print the step and the loss dot item is used here because loss is
    a tensor with a single element。 Dot item will actually convert that to a single
    float and this float will not will live on the CPU。 So this gets to some of the
    internals again of the devices but loss is a tensor with a single element and
    it lives on GPU for me because I'm using GPUs。 When you call dot item， PyTorch
    behind the scenes will take that one dimensional tensor。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们打印步骤，loss的dot item在这里使用，因为损失是一个包含单个元素的张量。dot item实际上会将其转换为单个浮点数，而这个浮点数将存在于CPU上。因此，这再次涉及到设备的一些内部机制，但损失是一个包含单个元素的张量，并且对我来说它存在于GPU上，因为我正在使用GPU。当你调用dot
    item时，PyTorch在后台将取出这个一维张量。
- en: ship it back to the CPU memory and convert it into a float that we can just
    print。 So this is the optimization and this should probably just work。 Let's see
    what happens。 Actually。 sorry， let me instead of using CPU override， let me delete
    that。 So this is a bit faster for me and it runs on CUDA。 Oh。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 将其发送回CPU内存并转换为我们可以打印的浮点数。所以这是优化，这应该能正常工作。让我们看看会发生什么。实际上，抱歉，让我删除CPU覆盖。这样对我来说快一点，并且在CUDA上运行。哦。
- en: expected all tensors to be on the same device but found at least two devices，
    CUDA zero and CPU。 So CUDA zero is the zero GPU because I actually have a GPU
    on this box。 So the 0th GPU on my box and CPU。 And a model we have moved to device
    but when I was writing this code I actually introduced the bug because buff we
    never moved to device。 And you have to be careful because you can't just do buff
    that to device。 It's not stateful。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 期望所有张量在同一设备上，但发现至少有两个设备：CUDA零和CPU。所以CUDA零是零GPU，因为我实际上在这个设备上有一个GPU。因此，我的箱子的第0个GPU和CPU。我们将模型移动到设备上，但当我编写此代码时，实际上引入了一个错误，因为缓冲区从未移动到设备上。你必须小心，因为你不能只是将缓冲区移动到设备。这不是有状态的。
- en: It doesn't convert it to be a device。 It instead returns pointer to a new memory
    which is on the device。 So you see how we can just do model that to a device but
    it does not apply to tensors。 You have to do buff equals。 Buff that to device
    and then this should work。 So what do we expect to see？ We expect to see a reasonable
    loss in the beginning and then we continue to optimize just a single batch。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 它不会将其转换为设备。相反，它返回指向设备上新内存的指针。因此，你会看到我们如何将模型移动到设备，但这并不适用于张量。你必须做缓冲区等于。将缓冲区移动到设备，然后这应该有效。那么我们期望看到什么？我们期望在开始时看到合理的损失，然后继续优化单个批次。
- en: And so we want to see that we can overfit this single batch。 We can crush this
    little batch and we can perfectly predict it in the C's on just this little batch。
    And in these that is roughly what we're seeing here。 So we started off at roughly
    10。82。 11 in this case。 And then as we continue optimizing on this single batch
    without loading new examples。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望看到我们可以对这个单个批次进行过拟合。我们可以压缩这个小批次，并且我们可以完美预测这个小批次的C。这大致就是我们在这里看到的。因此，我们开始时大约是10.82。
    在这种情况下是11。然后随着我们在这个单个批次上继续优化，而不加载新的示例。
- en: we are making sure that we can overfit a single batch。 And we are getting to
    very， very low loss。 So the transformer is memorizing this single individual batch。
    And one more thing I didn't mention is the learning rate here is 3e negative four。
    which is a pretty good default for most optimizations that you went around at
    a very early debugging stage。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保可以对单个批次进行过拟合。而且我们得到了非常非常低的损失。因此，变压器正在记忆这个单独的批次。我还没提到的另一件事是这里的学习率是3e负四，对于大多数早期调试阶段的优化来说，这是一个相当不错的默认值。
- en: So this is our simple inner loop and we are overfitting a single batch and this
    looks good。 So now what comes next is we don't just want to overfit a single batch。
    We actually want to do an optimization。 So we actually need to iterate these xy
    batches and create a little data loader that makes sure that we're always getting
    a fresh batch and that we're actually optimizing a reasonable objective。 So let's
    do that next。 Okay， so this is where I came up with and I wrote a little data
    loader light。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们简单的内部循环，我们正在对单个批次进行过拟合，这看起来不错。接下来我们想要的不仅仅是对单个批次进行过拟合。我们实际上想要进行优化。因此，我们需要迭代这些xy批次，并创建一个小数据加载器，以确保我们始终获得一个新鲜的批次，并且我们实际上是在优化一个合理的目标。接下来我们来做这个。好的，这就是我想出的，并写了一个小数据加载器。
- en: So what this data loader does is we're importing the token up here。 reading
    the entire text file from this single impa。txt tokenizing it。 and then we're just
    printing the number of tokens in total。 And the number of batches and a single
    epoch of iterating over this dataset。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据加载器的作用是我们在这里导入标记，从这个单一的impa.txt读取整个文本文件，对其进行标记化，然后打印总标记数，以及在遍历此数据集的单个时期中的批次数量。
- en: So how many unique batches do we output before we loop back around the beginning
    of the document and start reading it again。 So we start off at position zero and
    then we simply walk the document in batches of b times t。 So we take chunks of
    b times t and then always advance by b times t。 And it's important to note that
    we're always advancing our position by exactly b times t。
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环回文档开头并再次开始读取之前，我们输出了多少个唯一批次。因此，我们从位置零开始，然后简单地以b乘以t的批次遍历文档。因此，我们取b乘以t的块，然后总是向前推进b乘以t。值得注意的是，我们始终将位置推进恰好是b乘以t。
- en: But when we're fetching the tokens， we're actually fetching from current position
    to b times t plus one。 And we need that plus one because remember we need the
    target token for the last token in the current patch。 And so that way we can do
    the x， y exactly as we did it before。 And if we are to run out of data。 we'll
    just loop back around to zero。 So this is one way to write a very， very simple
    data loader。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们获取标记时，我们实际上是从当前位置提取到b乘以t加一。我们需要这个加一，因为记住我们需要当前批次最后一个标记的目标标记。因此，我们可以像之前那样精确地进行x，y。如果数据用完，我们就会循环回零。这是编写非常简单的数据加载器的一种方式。
- en: That simply just goes through the file in chunks。 And this could not for us
    for current purposes。 And we're going to complexify it later。 And now we'd like
    to come back around here and we'd like to actually use our data loader。 So the
    import tic to can has moved up。 And actually all of this is now useless。 So instead
    we just want a train loader for the training data。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是将文件分块处理。这对于我们当前的目的来说不可行。我们稍后会使其复杂化。现在我们想回到这里，实际上使用我们的数据加载器。所以导入的tic to可以已经上升了。实际上，这一切现在都是无用的。因此，我们只想要一个用于训练数据的训练加载器。
- en: And we want to use the same hyperparameters for four。 So batch size was four
    and time was 32。 And then here we need to get the x， y for the current batch。
    So let's see if CoPALD gets it because this is simple enough。 So we call the next
    batch and then we make sure that we have to move our tensors from CPU to the device。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望为四个使用相同的超参数。所以批量大小是四，时间是32。然后在这里我们需要获取当前批次的x，y。所以让我们看看CoPALD是否能够做到，因为这简单到足以。我们调用下一个批次，然后确保将我们的张量从CPU移动到设备上。
- en: So here when I converted the tokens， notice that I didn't actually move these
    tokens to the GPU。 I left them on the CPU， which is default。 And that's just because
    I'm trying not to waste too much memory on the GPU。 In this case this is a tiny
    data set that it would fit。 But it's fine to just ship it to GPU right now for
    our purposes right now。 So we get the next batch。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，当我转换这些标记时，请注意我并没有将这些标记移动到GPU上。我把它们留在CPU上，这是默认设置。这只是因为我不想在GPU上浪费太多内存。在这种情况下，这是一个微小的数据集，能够适应。但为了我们的目的，现在将其发送到GPU也是可以的。所以我们获取下一个批次。
- en: we keep the data loader simple CPU class。 And then here we actually ship it
    to the GPU and do all the computation。 And let's see if this runs。 So Python train
    dpt to dot pi。 And what do we expect to see before this actually happens？
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保持数据加载器简单为CPU类。然后我们实际上将其传送到GPU上进行所有计算。让我们看看这个是否能运行。所以用Python运行train dpt to
    dot pi。在这实际发生之前，我们期待看到什么？
- en: What we expect to see is now we're actually getting the next batch。 So we expect
    to not overfit a single batch。 And so I expect our loss to come down， but not
    too much。 And that's because I still expected to come down because in the 50，257
    tokens。 many of those tokens never occur in our data set。 So there are some very
    easy gains to be made here in the optimization。
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期待看到的是，现在我们实际上正在获取下一个批次。所以我们希望不对单个批次过拟合。因此，我期望我们的损失会下降，但不会下降太多。这是因为我仍然期望下降，因为在50,257个令牌中，许多令牌从未出现在我们的数据集中。因此，在优化中这里有一些非常简单的增益可以获得。
- en: By for example taking the biases of all the logits that never occur and driving
    them to negative infinity。 And that would basically just it's just that all of
    these crazy unicodes or different languages。 those tokens never occur。 So their
    probability should be very low。 And so the gains that we should be seeing are
    along the lines of basically deleting the usage of tokens that never occur。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过将所有从未出现的logits的偏置推向负无穷大。这基本上意味着所有这些疯狂的unicode或不同语言，这些令牌从未出现。所以它们的概率应该非常低。因此，我们应该看到的增益大致上是删除从未出现的令牌的使用。
- en: That's probably most of the loss gain that we're going to see at this scale
    right now。 But we shouldn't come to zero because we are only doing 50 iterations。
    And I don't think that's not to do an epoch right now。 So let's see what we've
    got。 We have 338。000 tokens， which makes sense with our three to one compression
    ratio because there are one milling characters。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这大概是我们现在在这个规模上看到的大部分损失增益。但我们不应该归零，因为我们只进行了50次迭代。我认为现在不需要进行一个完整的epoch。所以让我们看看我们有什么。我们有338,000个令牌，这与我们的三比一压缩比是合理的，因为有一百万个字符。
- en: So one epoch with the current setting of B and T will take 2，600 batches。 And
    we're only doing 50 batches of optimization in here。 So we start off in a familiar
    territory as expected and then we seem to come down to about 6。6。 So basically
    I think seem to be working okay right now with respect to our expectations。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用当前的B和T设置，一个epoch将需要2600个批次。而我们在这里只进行50个批次的优化。所以我们像预期的那样从熟悉的领域开始，然后似乎降到了大约6.6。因此，基本上我认为在我们期望的范围内，现在的情况似乎工作得还不错。
- en: So that's good。 Okay， next I want to actually fix a bug that we have in our
    code。 It's not a major bug but it is a bug with respect to how GPT-2 training
    should happen。 So the bug is the following。 We were not being careful enough when
    we were loading the weights from Hugging Face and we actually missed a little
    detail。 So if we come here， notice that the shape of these two tensors is the
    same。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这很好。好的，接下来我想修复我们代码中的一个bug。这个bug并不重大，但与GPT-2的训练方式有关。这个bug是这样的。我们在从Hugging Face加载权重时不够小心，实际上错过了一个小细节。所以如果我们来看这里，注意这两个张量的形状是相同的。
- en: So this one here is the token embedding at the bottom of the transformer。 And
    this one here is the language modeling head at the top of the transformer。 And
    both of these are basically two dimensional tensors and their shape is identical。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的这个是变换器底部的令牌嵌入。而这里的这个是变换器顶部的语言建模头。这两个基本上都是二维张量，它们的形状是相同的。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_74.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_74.png)'
- en: So here， the first one is the output embedding， the token embedding。 And the
    second one is this linear layer at the very top of the classifier layer。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里，第一个是输出嵌入，即令牌嵌入。第二个是在分类器层顶部的这个线性层。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_76.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_76.png)'
- en: Both of them are of shape 50，000 to 57 by 768。 This one here is giving us our
    token embeddings at the bottom。 And this one here is taking the 768 channels of
    the transformer and trying to upscale that to 50。257 to get the logis for the
    next token。 So they're both the same shape but more than that actually if you
    look at comparing their elements in PyTorch this is an element twice equality。
    So then we use 。all and we see that every single element is identical。 And more
    than that。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的形状都是 50,000 到 57 乘以 768。这里的这个给出了我们底部的标记嵌入。这里的这个则是将变换器的 768 个通道试图上升到 50,257，以获得下一个标记的逻辑。因此它们的形状相同，但更重要的是，如果你在
    PyTorch 中比较它们的元素，这是元素双重相等性。然后我们使用 .all，并且我们看到每一个元素都是相同的。更重要的是。
- en: we see that if we actually look at the data pointer。 this is what this is a
    way in PyTorch to get the actual pointer to the data and the storage。 We see that
    actually the pointer is identical。 So not only are these two separate tensors
    that happen to have the same shape and elements。 they're actually pointing to
    the identical tensor。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到如果实际查看数据指针，这是在 PyTorch 中获取实际数据和存储的指针。我们看到实际上指针是相同的。因此，这两个是分开的张量，恰好形状和元素相同，但它们实际上指向的是相同的张量。
- en: So what's happening here is that this is a common wait time scheme that actually
    comes from the original from the original attention is all in the paper and actually
    even the reference before it。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里发生的事情是，这是一个常见的权重时间方案，实际上来自于最初的注意力中的所有内容的论文，实际上甚至在它之前的引用。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_78.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_78.png)'
- en: So if we come here embeddings and softmax in the attention is all in the paper。
    they mention that in our model， we share the same wait matrix between the two
    embedding layers and the pre-payments。 And the pre-softmax linear transformation
    similar to 30。 So this is an awkward way to phrase that these two are shared and
    they're tied in their same matrix。
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们查看嵌入和注意力中的 softmax，论文中提到，在我们的模型中，我们在两个嵌入层和预先付款之间共享相同的权重矩阵。预 softmax 线性变换类似于
    30。所以用这种方式来表达这两个是共享的且绑定在同一矩阵中的确有些笨拙。
- en: And the 30 reference is this paper。 So this came out in 2017。 And you can read
    the full paper but basically it argues for this wait time scheme。 And I think
    intuitively the idea for why you might want to do this comes from this paragraph
    here。 And basically you can observe that you actually want these two matrices
    to behave similar in the following sets。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 而 30 的引用是这篇论文。这篇论文于 2017 年发表。你可以阅读完整的论文，但基本上它主张这种权重时间方案。我认为直观上，为什么你可能想这样做的想法来自于这一段。基本上你可以观察到，你实际上希望这两个矩阵在以下集合中表现相似。
- en: If two tokens are very similar semantically， like maybe one of them is all lowercase
    and the other one is all uppercase or it's the same token in the different language
    or something like that。 If you have similarity between two tokens， presumably
    you would expect that they are nearby in the token embedding space。
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个标记在语义上非常相似，比如一个全小写，另一个全大写，或者是同一个标记在不同语言中的表示等。如果你在两个标记之间存在相似性，推测你会期待它们在标记嵌入空间中是相近的。
- en: But in the exact same way， you'd expect that if you have two tokens that are
    similar semantically。 you'd expect them to get the same probabilities at the output
    of a transformer because they are semantically similar。 And so both positions
    in the transformer at the very bottom and at the top have this property that similar
    tokens should have similar embeddings or similar weights。 And so this is what
    motivates their exploration here and they kind of， you know。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 但以同样的方式，你会期待如果有两个在语义上相似的标记，它们在变换器的输出中应该获得相同的概率，因为它们在语义上是相似的。因此，在变换器的底部和顶部的两个位置都有这种特性，即相似的标记应该具有相似的嵌入或相似的权重。这就是促使他们在这里进行探索的原因。
- en: I don't want to go through the entire paper and you can go through it。 But this
    is what they observe。 They also observed that if you look at the output embeddings。
    they also behave like word embeddings。 If you just kind of try to use those weights
    as word embeddings。 So they kind of observe this similarity。 They try to tie them
    and they observe that they can get much better performance in that way。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想逐篇讨论整个论文，你可以去阅读它。但这是他们观察到的。他们还观察到，如果你查看输出嵌入，它们的行为也像词嵌入。如果你尝试将这些权重用作词嵌入，他们发现了这种相似性。他们试图将它们绑定，并观察到这种方式能获得更好的性能。
- en: And so this was adopted and the attention is on the paper。 And then it was used
    again in GPT-2 as well。 So I couldn't find it in the transformers implementation。
    I'm not sure where they tie those embeddings， but I can't find it in the original
    GPT-2 code introduced by OpenAI。 So this is OpenAI GPT-2 source model。 And here
    where they are forwarding this model。
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是被采纳的，注意力在论文上。然后它也在 GPT-2 中再次使用。所以我找不到它在变压器实现中的位置。我不确定他们是如何关联这些嵌入的，但我在 OpenAI
    提出的原始 GPT-2 代码中找不到它。所以这是 OpenAI GPT-2 源模型。在这里他们正在转发这个模型。
- en: and this is a tensor flow， but that's okay。 We see that they get the WTE token
    embeddings。 And then here is the encoder of the token embeddings and the position。
    And then here at the bottom。 they use the WTE again to do the logits。 So when
    they get the logits。 it's a mat model of this output from the transformer and
    the WTE tensor is reused。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个张量流，但没关系。我们看到他们获取了 WTE 令牌嵌入。然后这里是令牌嵌入和位置的编码器。然后在底部，他们再次使用 WTE 来计算 logits。所以当他们获得
    logits 时，它是来自变压器的输出的矩阵模型，而 WTE 张量被重复使用。
- en: And so the WTE tensor basically is used twice on the bottom of the transformer
    and on the top of the transformer。 And in the backward pass， we'll get gradients
    contributions from both branches， right？
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 WTE 张量基本上在变压器的底部和顶部使用了两次。在反向传播中，我们将从两个分支获得梯度贡献，对吧？
- en: And these gradients will add up on the WTE tensor。 So we'll get a contribution
    from the classifier layer。 And then at the very end of the transformer。 we'll
    get a contribution at the bottom of it flowing again into the WTE tensor。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这些梯度将在 WTE 张量上累加。因此我们将获得来自分类器层的贡献。然后在变压器的最底部，我们将再次获得流入 WTE 张量的贡献。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_80.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_80.png)'
- en: So we are currently not sharing WTE in our code， but we want to do that。 So
    wait， sharing， scheme。 And one way to do this， let's see if Quapal gets it。 Oh，
    it does。 Okay。 So this is one way to do it。 Basically， relatively straightforward。
    What we're doing here is we're taking the WTE。wait。 And we're simply redirecting
    it to point to the element。 So this basically copies the data pointer。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们目前在代码中并没有共享 WTE，但我们想这样做。所以稍等，共享，方案。而实现这一点的一种方法是，让我们看看 Quapal 是否能理解。哦，能理解。好的。这是实现这一点的一种方式。基本上，相对简单。我们在这里做的是，我们正在获取
    WTE。稍等。我们只是简单地重定向它以指向该元素。所以这基本上是复制数据指针。
- en: right？ It copies the reference。 And now the WTE。wait becomes orphaned， the old
    value of it。 And pytorch will clean it up。 And so we are only locked with a single
    tensor。 And it's going to be used twice in the forward pass。 And this is， to my
    knowledge。 all that's required。 So we should be able to use this。 And this should
    probably train。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？它复制了引用。现在 WTE。稍等，变得孤立，它的旧值。而 pytorch 会清理它。所以我们只锁定了一个张量。它将在前向传播中使用两次。根据我的了解，这就是所需的全部。因此我们应该能够使用这个。这应该也能训练。
- en: We're just going to basically be using this exact same sensor twice。 And we
    weren't being careful with tracking the likelihoods。 But according to the paper
    and according to the results。 you'd actually expect slightly better results doing
    this。 And in addition to that。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上只会使用这个完全相同的传感器两次。我们没有小心地跟踪可能性。但根据论文和结果，你实际上会期望这样做能获得稍微更好的结果。除此之外。
- en: one other reason that this is very， very nice for us is that this is a ton of
    parameters， right？
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个原因让我们觉得这非常好，那就是这有大量参数，对吧？
- en: What is the size of here？ It's 768 times 50，257。 So this is 40 million parameters。
    And this is a 124 million parameter model。 So 40 divided 124。 So this is like
    30% of the parameters are being saved using this wait time scheme。 And so this
    might be one of the reasons that this is working slightly better。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的大小是多少？是 768 乘以 50,257。所以这是 4000 万个参数。这是一个 1.24 亿参数的模型。所以 40 除以 124。因此大约 30%
    的参数通过这种等待时间方案得以节省。所以这可能是它稍微工作得更好的原因之一。
- en: If you're not training the model long enough， because of the wait time。 you
    don't have to train as many parameters。 And so you become more efficient in terms
    of the training process。 Because you have fewer parameters and you're putting
    in this inductive bias that these two embeddings share similarities between tokens。
    So this is the wait time scheme。 And we've saved a ton of parameters。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有足够长时间地训练模型，因为等待时间，你不需要训练那么多参数。因此，你在训练过程中变得更加高效。因为你有更少的参数，并且你引入了这种归纳偏置，使得这两个嵌入在标记之间共享相似性。所以这是等待时间方案。而且我们节省了大量参数。
- en: And we expect our model to work slightly better because of this scheme。 Okay，
    next。 I would like us to be a bit more careful with the initialization and to
    try to follow the wait GPT-2 initialized their model。 Now， unfortunately， the
    GPT-2 paper and the GPT-3 paper are not very explicit about initialization。 So
    we kind of have to read between the lines。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期待我们的模型因为这个方案而稍微表现得更好。好的，接下来。我希望我们在初始化时更加小心，并尽量遵循GPT-2初始化模型的方式。现在，不幸的是，GPT-2论文和GPT-3论文对初始化并没有非常明确的说明。因此，我们必须略微解读一下。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_82.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_82.png)'
- en: And instead of going to the paper， which is quite vague。 there's a bit of information
    in the code that open the app released。 So when we go to the model。py。 we see
    that when they initialize their weights， they are using the standard deviation
    of 0。02。 And that's how they， so this is a normal distribution for the weights
    and the standard deviation is 0。
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是去查阅论文（它相当模糊），在开放的应用程序发布的代码中有一些信息。因此，当我们查看model.py时，我们会发现他们在初始化权重时使用标准差为0.02。这是权重的正态分布，标准差为0。
- en: 02。 For the bias， they initialize that with zero。 And then when we scroll down
    here。 why is this not scrolling？ The token embeddings are initialized at 0。02
    and position embeddings at 0。01 for some reason。 So those are the initialization
    and we'd like to mirror that in GPT-2 in our module here。
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于偏置，他们将其初始化为零。当我们向下滚动时，为什么不滚动？令牌嵌入初始化为0.02，而位置嵌入初始化为0.01，出于某种原因。这些是初始化，我们希望在这里的GPT-2模块中镜像它。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_84.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_84.png)'
- en: So here's a snippet of code that I sort of came up with very quickly。 So what's
    happening here is at the end of our initializer for the GPT module。 we're calling
    the apply function of an end module。 and that iterates all the sub-modules of
    this module and applies in it weights function on them。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个我很快想到的代码片段。这里发生的事情是在我们GPT模块的初始化器结束时，我们调用一个终止模块的apply函数，它会遍历该模块的所有子模块，并在其上应用权重函数。
- en: And so what's happening here is that we're iterating all the modules here。 And
    if they are an end that linear module， then we're going to make sure to initialize
    the weight using a normal with the standard deviation of 0。02。 So as a bias in
    this layer， we make sure to initialize that to zero。
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里发生的事情是我们在迭代所有模块。如果它们是线性模块，我们会确保使用标准差为0.02的正态分布来初始化权重。因此，在该层的偏置中，我们确保将其初始化为零。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_86.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_86.png)'
- en: Note that the zero initialization for the bias is not actually the pie-torch
    default。 By default。 the bias here is initialized with a uniform。 So that's interesting。
    So we make sure to use zero。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，偏置的零初始化实际上不是pie-torch的默认值。默认情况下，这里的偏置是用均匀分布初始化的。这很有趣，所以我们确保使用零。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_88.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_88.png)'
- en: And for the embedding， we're just going to use 0。02 and keep it the same。 So
    we're not going to change it to 0。01 for positional because it's about the same。
    And then if you look through our model， the only other layer that requires initialization
    and that has parameters is the layer norm。 And the pie-torch default initialization
    sets the scale in the layer norm to be 1 and the offset in the layer norm to be
    0。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌入，我们将使用0.02并保持不变。因此，我们不会将其更改为0.01用于位置嵌入，因为差不多。而且如果你查看我们的模型，唯一需要初始化且具有参数的层是层归一化。pie-torch的默认初始化将层归一化中的比例设置为1，偏移量设置为0。
- en: So that's exactly what we want。 And so we're just going to keep it that way。
    And so this is the default initialization if we are following the。
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这正是我们想要的。因此，我们将保持这种状态。如果我们遵循默认初始化。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_90.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_90.png)'
- en: Where is it？ The GPT-2 source code that they released。
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 它在哪里？他们发布的GPT-2源代码。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_92.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_92.png)'
- en: I would like to point out by the way that typically the standard deviation here
    on this initialization。 if you followed the heavier initialization， would be 1
    over the square root of the number of features that are incoming into this layer。
    But if you'll notice， actually， 0。02 is basically consistent with that because
    the D model sizes inside these transformers from GPT-2 are roughly 768。 1， 600，
    etc。
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便指出，通常在这种初始化下的标准差，如果你遵循更重的初始化，将是进入该层的特征数量的平方根的倒数。但如果你注意到，0.02实际上与此一致，因为这些变压器内部的D模型大小大致为768，1,600等。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_94.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_94.png)'
- en: So 1 over the square root of， for example， 768 gives us 0。03。 If we plug in
    1，600， 1，600， we get 0。02。 If we plug in 3 times that 0。014， etc。 So basically
    0。02 is roughly in the vicinity of reasonable values for these initializations
    anyway。
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，768的平方根的倒数为0.03。如果我们代入1,600，得到0.02。如果我们代入三倍的值，得到0.014，等等。所以基本上，0.02大致上是这些初始化的合理值。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_96.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_96.png)'
- en: So it's not completely crazy to be hard-coding 0。02 here。 but you'd like typically
    something that grows with the model size instead。 but we will keep this because
    that is the GPT-2 initialization per their source code。
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里硬编码0.02并不是完全疯狂，但你通常会希望有一些与模型大小增长的内容。但我们将保留这个，因为这是根据其源代码的GPT-2初始化。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_98.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_98.png)'
- en: But we are not fully done yet on initialization because there's one more caveat
    here。 So here。 a modified initialization which accounts for the accumulation on
    the residual path with model depth is used。 We scaled the weight of residual layers
    initialization by factored 1 over squared event。 where n is the number of residual
    layers。
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 不过我们在初始化上还没有完全完成，因为这里还有一个 caveat。因此，这里使用了修改过的初始化，考虑到模型深度下的残差路径的积累。我们通过因子1除以平方事件缩放了残差层初始化的权重，其中n是残差层的数量。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_100.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_100.png)'
- en: So this is what GPT-2 paper says。 So we have not implemented that yet， and we
    can do so now。 Now。 I'd like to actually kind of motivate a little bit what they
    mean here， I think。 So here's roughly what they mean。 If you start out with 0s
    in your residual stream。 remember that each residual stream is of this form where
    we continue adding to it。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是GPT-2论文所说的。因此，我们还没有实现这一点，现在可以进行实现。现在，我想稍微激励一下他们在这里的意思。我认为大致是这样的。如果你在残差流中从0开始，记住每个残差流都是这种形式，我们会继续向其添加内容。
- en: X is x plus something， some kind of contribution。 So every single block of the
    residual network contributes some amount and it gets added。 And so what ends up
    happening is that the variance of the activations in the residual stream grows。
    So here's a small example。 If we start at 0 and then we for 100 times。 we have
    sort of this residual stream of 768 zeros。 And then 100 times we add random。
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: X是x加上某种贡献。因此，残差网络的每一个块都贡献一定的量并被添加。最终发生的情况是残差流中的激活方差增加。所以这是一个小例子。如果我们从0开始，然后进行100次，我们就有一个768个零的残差流。然后100次我们添加随机数。
- en: which is a normal distribution， 0 mean 1 standard deviation。 If we add to it。
    then by the end the residual stream has grown to have standard deviation of 10。
    And that's just because we're always adding these numbers。 And so this scaling
    factor that they use here exactly compensates for that growth。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个正态分布，均值为0，标准差为1。如果我们向其中添加东西，那么最终残差流的标准差将增长到10。这仅仅是因为我们总是在添加这些数字。因此，他们在这里使用的缩放因子正好补偿了这种增长。
- en: So if we take n and we basically scale down everyone of these contributions
    into the residual stream by 1 over the square root of n。 So 1 over the square
    root of n is n to the negative 0。5， right？ Because n to the 0。5 is the square
    root and then 1 over the square root is n negative 0。5。 If we scale it in this
    way。 then we see that we actually get 1。 So this is a way to control the growth
    of activations inside the residual stream in the forward pass。
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取n，并基本上将每个对残差流的贡献缩小到n平方根的倒数。因此，n平方根的倒数是n的负0.5，对吗？因为n的0.5是平方根，然后平方根的倒数是n的负0.5。如果我们以这种方式缩放，那么我们实际上得到1。这是一种控制前向传递中残差流内激活增长的方法。
- en: And so we'd like to initialize in the same way where these weights that are
    at the end of each block。 so this C-proj layer， the GPT paper proposes to scale
    down those weights by 1 over the square root of the number of residual layers。
    So one crude way to implement this is the following。 I don't know if this is a
    PyTorch sanctioned。 but it works for me。 It is we all do in the initialization。
    See that's not special。
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望以相同的方式初始化，这些在每个块末尾的权重。GPT 论文建议按残差层数的平方根缩小这些权重。因此，有一种粗略的方法实现这一点。我不知道这是否被
    PyTorch 官方认可，但对我来说是有效的。我们在初始化时都这样做。看，这并不特别。
- en: NanogPT scale in it is 1。 So we're setting kind of like a flag for this module。
    There must be a better way in PyTorch， right？ But I don't know。 Okay。 so we're
    basically attaching this flag and trying to make sure that it doesn't conflict
    with anything previously。 And then when we come down here， this STD should be
    0。02 by default。
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: NanogPT 的规模是 1。所以我们为这个模块设置了一个标志。在 PyTorch 中一定有更好的方法，对吧？但我不知道。好吧。所以我们基本上是在附加这个标志，并试图确保它不与之前的任何东西冲突。然后当我们到这里时，这个
    STD 默认应该是 0.02。
- en: But then if it has a true module of this thing， then STD times equals。 。 what？
    。 Copal does not get it correctly。 So we want 1 over the square root of the number
    of layers。 So the number of residual layers here is twice times the saltout conflict
    layers。 And then this times negative 0。5。 So we want to scale down that standard
    deviation。
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果它有这个东西的真实模块，那么 STD 乘以等于……什么？Copal 没有正确处理。所以我们希望是残差层数的平方根的倒数。因此这里的残差层数是 saltout
    冲突层的两倍。然后这个乘以负 0.5。所以我们想缩小那个标准差。
- en: And this should be correct and implement that。 I should clarify by the way that
    the two times number of layers comes from the fact that every single one of our
    layers in the transformer。 actually has two blocks that add to the residual pathway，
    right？
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是正确的并实现了这一点。我应该澄清一下，层数的两倍来自于我们每个 transformer 层实际上有两个块添加到残差路径，对吧？
- en: We have the attention and then the MLP。 So that's where the two times comes
    from。 And the other thing to mention is that what's slightly awkward but we're
    not going to fix it is that。 because we are weight-sharing the WTE and the LMA。
    In this iteration of our old sub-modules。 we're going to actually come around
    to that tensor twice。
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有注意力机制和 MLP。所以这就是两倍的来源。还有一点要提到的是，这有点尴尬，但我们不会去修复，因为我们在 WTE 和 LMA 之间共享权重。在我们旧的子模块的这个迭代中，我们实际上会对那个张量进行两次操作。
- en: So we're going to first initialize it as an embedding with 0。02。 And then we're
    going to come back around it again in the linear and initialize it again using
    0。02。 And it's going to be 0。02 because the LMA is of course not scaled。 So it's
    not going to come here。 It's just it's going to be basically initialized twice
    using the identical same initialization。
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将首先将其初始化为嵌入，值为 0.02。然后我们会在全连接层中再次使用 0.02 初始化它。因为 LMA 当然没有缩放，所以它不会到这里。基本上会使用相同的初始化进行两次初始化。
- en: But that's okay。 And then scrolling over here， I added some code here so that
    we have reproducibility to set the seeds。 And now we should be able to Python
    train GPT2。py and let this running。 And as far as I know。 this is the GPT2 initialization
    in the way we've implemented right now。 So this looks reasonable to me。 Okay，
    so at this point we have the GPT2 model。
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 但这没关系。然后在这里滚动，我添加了一些代码，以便我们可以设置种子，确保可复现性。现在我们应该能够使用 Python 训练 `GPT2.py` 并让它运行。就我所知，这是我们目前实现的
    GPT2 初始化。因此，这看起来合理。好吧，目前我们有了 GPT2 模型。
- en: We have some confidence that it's correctly implemented。 We've initialized it
    properly and we have a data loader that's iterating through data batches and we
    can train。 So now comes the fun part。 I'd like us to speed up the training by
    a lot。 So we're getting our money's worth with respect to the hardware that we
    are using here。
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一定的信心它已正确实现。我们已正确初始化，并且有一个数据加载器正在迭代数据批次，我们可以训练。因此，接下来就是有趣的部分。我希望我们能大大加快训练速度。我们希望充分利用我们在这里使用的硬件。
- en: And we're going to speed up the training by quite a bit。 Now you always want
    to start with what hardware do you have？ What does it offer？
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加快训练速度。现在你总是想先了解你有什么硬件？它提供了什么？
- en: And are you fully utilizing it？ So in my case， if we go to NVIDIA SMI。 we can
    see that I have eight GPUs。 And each one of those GPUs is an 8100 SXM 80 gigabytes。
    So this is the GPU that I have available to me in this box。 Now when I use to
    spin up these kinds of boxes， by the way。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否充分利用它？所以就我而言，如果我们去看NVIDIA SMI。我们可以看到我有八个GPU。每个GPU都是8100 SXM 80GB。这就是我在这个盒子中可用的GPU。顺便说一下，当我用来启动这些类型的盒子时。
- en: my favorite place to go to is Lambda Labs。
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的地方是Lambda Labs。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_102.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_102.png)'
- en: They do sponsor my development and that of my projects。 But this is my favorite
    place to go。 And this is where you can spin up one of these machines and you pay
    per hour。 And it's very。 very simple。 So I like to spin them up and then connect
    the S code to it。 And that's how I develop。 Now when we look at the 8100s that
    are available here。
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 他们确实资助我的开发和我的项目。但这是我最喜欢的地方。而且在这里，你可以启动这些机器，按小时付费。而且这非常简单。所以我喜欢启动它们，然后将S代码连接上去。这就是我的开发方式。现在我们来看看这里可用的8100。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_104.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_104.png)'
- en: 8100 80 gigabyte SXM is the GPU that I have here。 And we have a bunch of numbers
    here for how many calculations you can expect out of this GPU。 So when I come
    over here and I break in right after here， so Python， True， and Jupy。 So I'm breaking
    in right after we calculate the logits and the loss。 And the interesting thing
    I'd like you to note is when I do logits。detive， this prints a torch。
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 8100 80GB SXM是我这里的GPU。我们这里有很多数字，说明你可以从这个GPU上期望得到多少计算。所以当我过来这里并且在这里打断时，所以是Python、True和Jupy。这样我就可以在计算logits和损失之后打断。我要你注意的有趣之处是，当我执行logits.detive时，这会打印一个torch。
- en: float32。 So by default in PyTorch， when you create tensors。 And this is the
    case for all the activations and for the parameters of the network and so on。
    By default， everything is in float32。 That means that every single number。 activation
    or weight and so on， is using a float representation that has 32 bits。
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: float32。因此，在PyTorch中，当你创建张量时，默认情况下，所有的激活和网络参数等都是float32。也就是说，每一个数字、激活或权重等都使用32位的浮点表示。
- en: And that's actually quite a bit of memory。 And it turns out empirically that
    for deep learning as a computational workload。 this is way too much。 And deep
    learning and the training of these networks can tolerate significantly lower precision。
    Not all computational workloads can't tolerate small precision。 So for example。
    if we go back to the data sheet。
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上占用了一些内存。实证结果表明，对于深度学习作为计算工作负载来说，这个是太多了。深度学习和这些网络的训练可以容忍显著较低的精度。并不是所有的计算工作负载都不能容忍小精度。例如，如果我们回到数据表。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_106.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_106.png)'
- en: you'll see that actually these GPUs support up to FP64。 And this is quite useful。
    I understand for a lot of scientific computing applications。 And there they really
    need this。 But we don't need that much precision for deep learning training。 So
    currently we are here， FP32。 And with this code as it is right now， we expect
    to get at most 19。5 teraflabs of performance。
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到这些GPU实际上支持高达FP64。这对于很多科学计算应用是非常有用的。我理解他们确实需要这个。但我们在深度学习训练中并不需要那么高的精度。所以目前我们在这里，FP32。根据目前的代码，我们预计最多能得到19.5
    teraflops的性能。
- en: That means we're doing 19。5 trillion operations， floating point operations。
    So this is floating point multiply at most likely。 And so these are the floating
    point operations。 Now， notice that if we are willing to go down in precision。
    So TF32 is a lower precision format。 We're going to see in a second。 You can actually
    get an 8x improvement here。
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们正在进行19.5万亿次操作，浮点操作。所以这很可能是浮点乘法。这就是浮点操作。现在，注意，如果我们愿意降低精度。TF32是一种较低精度格式。我们稍后会看到。你实际上可以在这里获得8倍的提升。
- en: And if you're willing to go down to float 16 or B float 16。 you can actually
    get times 16x performance all the way to 312 teraflabs。 You see here that NVIDIA
    likes to cite numbers that have an asterisk here。 This asterisk says with sparsity。
    But we are not going to be using sparsity in our code。
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意降低到float 16或B float 16，你实际上可以得到16倍的性能，甚至达到312 teraflops。你会看到NVIDIA喜欢引用带有星号的数字。这个星号表示带稀疏性。但我们不会在代码中使用稀疏性。
- en: And I don't know that this is very widely used in the industry right now。 So
    most people look at this number here without sparsity。 And you'll notice that
    we could have got even more here。 But this is int 8。 And int 8 is used for inference，
    not for training。 Because int 8 has a。
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道目前这在行业中是否被广泛使用。因此，大多数人看到这个数字时没有稀疏性。你会注意到我们本可以获得更多的结果。但这是int 8。而int 8用于推理，而不是训练。因为int
    8有一个。
- en: It basically has uniform spacing。 And we actually require a float so that we
    get a better match to the normal distributions。 that occur during training of
    neural networks， where both activations and weights are distributed。 as a normal
    distribution。 And so floating points are really important to match that representation。
    So we're not typically using int 8 for training， but we are using it for inference。
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上具有均匀的间隔。我们实际上需要浮点数，以便更好地匹配在神经网络训练期间出现的正态分布，其中激活和权重分布为正态分布。因此，浮点数对于匹配这种表示是非常重要的。因此，我们通常不使用int
    8进行训练，但用于推理。
- en: And if we bring down the precision， we can get a lot more teraflabs out of the
    tensor。 course available in the GPUs。 We'll talk about that in a second。 But in
    addition to that。 if all of these numbers have fewer bits of representation。 it's
    going to be much easier to move them around。 And that's where we start to get
    into the memory bandwidth and the memory of the model。
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们降低精度，就可以从张量中获得更多的teraflabs。课程可在GPU中获得。我们稍后会谈到这一点。但除此之外，如果所有这些数字的表示位数更少，那么移动它们将会容易得多。这就是我们开始涉及内存带宽和模型内存的地方。
- en: So not only do we have a finite capacity of the number of bits that our GPU
    can store。 but in addition to that， there's a speed with which you can access
    this memory。 And you have a certain memory bandwidth。 It's a very precious resource。
    And in fact。 many of the deep learning workloads for training are memory bound。
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅有有限的位数容量供我们的GPU存储，而且还有访问这块内存的速度。你有一定的内存带宽。这是一种非常宝贵的资源。实际上，许多深度学习的训练工作负载是受内存限制的。
- en: And what that means is actually that the tensor course that do all these extremely
    fast。 multiplications， most of the time they're waiting around， they're idle。
    Because we can't feed them with data fast enough。 We can't load the data fast
    enough for memory。 So typical utilizations of your hardware， if you're getting
    60% utilization。
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上意味着进行所有这些极快的乘法运算的张量核心，大部分时间都在等待，它们处于空闲状态。因为我们无法快速为它们提供数据。我们无法快速从内存加载数据。因此，如果你的硬件利用率达到60%。
- en: you're actually doing extremely well。 So half of the time in a well-tuned application。
    your tensor course are not doing multiplies， because the data is not available。
    So the memory bandwidth here is extremely important as well。 And if we come down
    in the precision for all the floats， all the numbers， weights and activations。
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上做得非常好。因此，在一个调优良好的应用中，张量核心的一半时间并不在进行乘法运算，因为数据不可用。因此，这里的内存带宽也极其重要。如果我们对所有浮点数、所有数字、权重和激活降低精度。
- en: suddenly require less memory。 So we can store more and we can access it faster。
    So everything speeds up and it's amazing。 And now let's reap the benefits of it。
    And let's first look at the tensor float 32 format。 Okay， so first of all， what
    are tensor cores？
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 突然间需要更少的内存。因此我们可以存储更多，访问更快。一切都加速了，真是惊人。现在让我们享受其带来的好处。首先看一下张量浮点32格式。好吧，首先，张量核心是什么？
- en: Well， tensor core is just an instruction in the A100 architecture， right？
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，张量核心只是A100架构中的一条指令，对吧？
- en: So what it does is it does basically a little 4x4 matrix multiply。 So this is
    just matrix multiplication here of 4x4 matrices。 And there are multiple configurations
    as to what precision any of these matrices are。 So in what precision the internal
    accumulate happens and then what is the output precision。
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 它的作用基本上是执行一个小的4x4矩阵乘法。这只是4x4矩阵的矩阵乘法。关于这些矩阵的精度有多种配置。所以内部累加发生的精度是什么，输出精度又是什么。
- en: and the precision etc。 So there's a few switches but it's basically a 4x4 multiply。
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 还有精度等。这有几个开关，但基本上是一个4x4的乘法。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_108.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_108.png)'
- en: And then any time we have any operations that require matrix multiplication。
    they get broken up into this instruction of 4x4 multiply。
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们有需要矩阵乘法的操作时，它们就会被拆分为这条4x4乘法指令。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_110.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_110.png)'
- en: And so everything gets broken up into this instruction because it's the fastest
    way to multiply matrices。
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有这些都被分解为这条指令，因为这是乘法矩阵的最快方式。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_112.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_112.png)'
- en: And it turns out that most of the computational work that we're doing up above。
    all of it really is matrix multiplication。 Most of the work competitionally happens
    in the linear layers。 Linear， linear， etc。 There's a few things sandwiched in
    between。 There's some additions in residuals， there's some galude nonlinearities，
    there's some layer norms。
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发现我们上面做的大部分计算工作实际上都是矩阵乘法。大多数计算工作发生在线性层中。线性，线性，等等。中间夹杂着一些其他操作。有一些在残差中的加法，一些非线性，和一些层归一化。
- en: etc。 But if you just time them， you'll see that these are nothing。 Like basically
    the intrad transformer is just a bunch of matrix multiplications really。 And especially
    at this small scale， 124 million perimeter model。 actually the biggest matrix
    multiplication by far is the classifier layer at the top。
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。但如果你计时，你会发现这些几乎是没有的。基本上，内部变压器只是大量的矩阵乘法，特别是在这个小规模，1.24亿参数的模型中，实际上最大矩阵乘法无疑是顶部的分类器层。
- en: That is a massive matrix multiply of going from 768 to 50，257。 And that matrix
    multiply dominates anything else that happens in that network roughly speaking。
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个巨大的矩阵乘法，从768到50,257。大致来说，这个矩阵乘法主导了网络中发生的其他所有操作。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_114.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_114.png)'
- en: So it's matrix multiplies that become a lot faster， which are hidden inside
    our linear layers。 And they're accelerated through tensor cores。 Now the best
    reference I would say for tensor cores is basically just go to the。 8100 architecture
    white paper。 And then it's pretty detailed， but I think people。 it's like relatively
    readable mostly if you have to understand what's happening。
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 所以矩阵乘法变得更快，这些操作隐藏在我们的线性层中。它们通过张量核心加速。至于张量核心的最佳参考，我会建议查看8100架构的白皮书。内容相当详细，但我认为如果你想了解发生了什么，阅读起来相对容易。
- en: So figure nine tensor float 32。 So this is the explanation basically for TF32
    and what happens here。 And you see that there's many configuration options here
    available。 So the input operands and what precision are they in？
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 所以图九的张量浮点32。这基本上是TF32的解释，以及这里发生了什么。你会看到这里有许多可用的配置选项。那么输入操作数是什么精度呢？
- en: The accumulator and what basically the internal representation within the instruction
    when you do the accumulate of this matrix multiplication。 So the intermediate
    plus equals of the intermediate little vector multiplies here。 That all happens
    in FV32。 And then this is an 8x improvement as I mentioned to the ops that we
    get。 So TF32 specifically we're looking at this row here。
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 累加器以及当你进行这个矩阵乘法的累加时，指令内部的基本表示。因此，这里中间的小向量乘法的中间加法都发生在FV32中。正如我提到的，这相较于我们获得的操作有8倍的提升。因此我们专门关注TF32这一行。
- en: And the way this works is normally FV32 has 32 bits。 TF32 is the exact same
    bits。 We have one signed bit， we have 8 exponent bits。 Except the mantissa bits
    get cropped in the float。 And so basically we end up with just 19 bits instead
    of 32 bits。 Because the last 13 bits get truncated， they get dropped。
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作原理是，通常FV32有32位。TF32是完全相同的位数。我们有一个符号位，8个指数位。只不过尾数位在浮点数中被裁剪。因此，我们最终只有19位，而不是32位。因为最后的13位被截断，丢弃了。
- en: And all this is internal to the instruction。 So none of it is visible to anything
    in our PyTorch。 None of our PyTorch code will change。 All of the numbers will
    look identical。 It's just that when you call the tensor core instruction internally
    in the hardware。 it will crop out these 13 bits。 And that allows it to calculate
    this little matrix multiply significantly faster。
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是指令内部的内容。因此，在我们的PyTorch中没有任何可见的东西。我们的PyTorch代码不会改变。所有数字看起来都是相同的。只是在硬件内部调用张量核心指令时，它会裁剪出这13位。这使得它能够显著更快地计算这个小矩阵乘法。
- en: 8x faster。 Now of course this speed up comes at a cost。 And the cost is that
    we are reducing the precision。 Our accumulator still in FV32。 Our output is FV32。
    Our inputs are FV32。 But internally things get truncated in the operands to perform
    the operation faster。 And so our results are starting to be a bit more approximate。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 速度提高了8倍。当然，这种加速是有代价的。代价就是我们降低了精度。我们的累加器仍然是FV32。我们的输出是FV32。我们的输入是FV32。但是在内部，为了更快地执行操作，操作数会被截断。因此，我们的结果开始变得更加近似。
- en: But empirically when you actually train with this， you basically can't tell
    the difference。 So the reason I like TF32 is because if you can tolerate a little
    bit of a precision fudge。 then this is pre。 None of your code sees this。 It's
    fully internal to the operation。 And the operation to you just go 8x faster and
    it's a bit more approximate。
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上，当你进行训练时，你基本上无法分辨出差异。所以我喜欢TF32的原因是，如果你能容忍一点精度的妥协，那么这是可以接受的。你的代码并不会看到这个。它完全是操作内部的事情。对于你来说，这个操作就是快8倍，而且结果略微近似。
- en: And so it's a pretty sweet spot I would say in optimization。 And let's see what
    that looks like first。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在优化方面，我会说这是一个相当不错的平衡点。让我们先看看这是什么样子。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_116.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_116.png)'
- en: So I've set up our codes to just time the iterations。 So import time。 I changed
    the hyperparameters so that we have something a bit more that reflects kind of
    workload that we want to run。 Because we want to do a fairly large run at the
    end of this。 So let's use batch size 16。 And let's now use the actual GPT-2 maximum
    sequence length of 1024 tokens。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我设置了我们的代码来计时迭代。导入时间。我改变了超参数，以便我们有一些更能反映我们想要运行的工作负载的东西。因为我们希望在最后进行一次相当大的运行。所以让我们使用批处理大小16。现在使用实际的GPT-2最大序列长度为1024个标记。
- en: So this is the configuration。 And then for 50 iterations， I'm just doing something
    very lazy here。 I'm doing time that time to get the current time。 And then this
    is the optimization loop。 And now I want to time how long this takes。 Now one
    issue with working with GPUs is that as your CPU runs。 it's just scheduling work
    on GPU。 It's ordering some work。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置。在50次迭代中，我在这里做一些非常简单的操作。我在计时，获取当前时间。然后这是优化循环。现在我想计时这需要多长时间。与GPU合作时的一个问题是，当你的CPU运行时，它只是安排GPU上的工作。它在排序一些工作。
- en: And so it sends a request and then it continues running。 And so it can happen
    sometimes that we sort of speed through this。 And we queue up a lot of kernels
    to run on the GPU。 And then the CPU sort of like gets here and takes time at that
    time。 But actually the GPU is still running because it takes a time to actually
    work through the work that was scheduled to run。 And so you're just building up
    a queue for the GPU。 And so actually if you need to。 you want to wait to start
    to go to the synchronize and this will wait for the GPU to finish all the work
    that was scheduled to run up above here。 And then we can actually take the time。
    So basically we're waiting for the GPU to stop this iteration。
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 它发出请求，然后继续运行。有时会发生这种情况，我们快速通过这个过程，并且我们在GPU上排队了很多内核来运行。然后CPU似乎到了这里，并在那个时间花费了时间。但实际上，GPU仍在运行，因为它需要时间来处理计划运行的工作。因此，你只是为GPU建立了一个队列。如果需要，你可能想要等待开始去同步，这将等待GPU完成所有上述计划要运行的工作。然后我们实际上可以计时。所以基本上我们在等待GPU停止这一轮。
- en: take the time and then we're going to just print it。 So here I'm going to run
    the training loop。 And here on the right， I'm watching NVIDIA SMI。 So we start
    off at zero。 We're not using the GPU。 And then my default PyTorch will use GPU
    zero。 So we see that it gets filled up。 And we're using 35 gigabytes out of 80
    gigabytes available。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 记录时间，然后我们将打印出来。所以在这里，我将运行训练循环。在右侧，我在监视NVIDIA SMI。所以我们从零开始。我们不使用GPU。然后我默认的PyTorch将使用GPU零。因此我们看到它逐渐填满。我们使用了80GB可用空间中的35GB。
- en: And then here on the left we see that because we cranked up the batch size。
    Now it's only 20 batches to do a single epoch on our tiny Shakespeare。 And we
    see that we're seeing roughly 1000 milliseconds per iteration here。 Right？
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在左侧，我们看到因为我们提高了批处理大小。现在在我们的小型莎士比亚数据集上，只需20个批次就能完成一个纪元。我们看到每次迭代大约需要1000毫秒。对吗？
- en: So the first iteration sometimes is slower。 And that's because PyTorch might
    be doing a lot of initialization here on the very first iteration。 And so it's
    probably initializing all these tensors and buffers to hold all the gradients。
    And I'm not sure all the work that happens here。 But this could be a slower iteration。
    When you're timing your logic， you always want to be careful with that。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第一次迭代有时会比较慢。这是因为PyTorch可能在第一次迭代时进行大量初始化。因此，它可能正在初始化所有这些张量和缓冲区来保存所有梯度。我不确定这里发生的所有工作。但这可能会导致较慢的迭代。当你在计时你的逻辑时，你总是要对此保持谨慎。
- en: But basically we're seeing 1000 milliseconds per iteration。 And so this will
    run for roughly 50 seconds as we have it right now。 So that's our baseline in
    float 32。 One more thing I wanted to mention is that if this doesn't fit into
    your GPU and you're getting out of memory errors。 then start decreasing your batch
    size until things fit。 So instead of 16。
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 但基本上我们看到每次迭代大约需要1000毫秒。因此，根据现在的设置，这大约会运行50秒。这是我们在浮点32中的基线。我还想提到的是，如果这不适合你的GPU并且出现内存不足错误，那么请开始减少你的批量大小，直到适合为止。因此，而不是16。
- en: try 8 or 4 or whatever you need to fit the batch into your GPU。 And if you have
    a bigger GPU。 you can actually potentially get away with 32 and so on。 By default。
    you want to basically max out the batch size that fits on your GPU。 And you want
    to keep it by numbers。 So use numbers that have lots of powers of two in them。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试8、4或你需要的任何数字，以便将批量适配到你的GPU。如果你有更大的GPU，你实际上可以使用32等。默认情况下，你基本上想要最大化适合你GPU的批量大小。你想保持在合理的数字范围内。因此，使用包含大量2的幂的数字。
- en: So 16 is a good number， 8， 24， 32， 48。 These are nice numbers。 But don't use
    something like 17 because that will run very inefficiently on the GPU。 And we're
    going to see that a bit later as well。 So for now， let's just stick with 16， 1024。
    And the one thing that I added also here and I ran it again is I'm calculating
    tokens per second through it during training。
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 所以16是一个不错的数字，8、24、32、48。这些都是不错的选择。但不要使用像17这样的数字，因为这在GPU上会运行得非常低效。稍后我们也会看到这一点。所以现在，让我们坚持使用16和1024。我还添加了一件事，并再次运行，我是在训练过程中计算每秒的标记数。
- en: Because we might end up changing the batch size around over time。 But tokens
    per second is the objective measure that we actually really care about。 How many
    tokens of data are we training on？ And what is the throughput of tokens that we're
    getting in our optimization？
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 因为随着时间的推移，我们可能会更改批量大小。但每秒的标记数是我们实际关心的客观指标。我们正在训练多少个数据标记？在优化中，我们得到的标记吞吐量是多少？
- en: So right now we're processing and training on 163，000 tokens per second roughly。
    And that's a bit more objective metric。 Okay， so let's now enable TF32。 Now luckily
    PyTorch makes this fairly easy for us。 And to enable TF32。 you just need to do
    a single line。
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们大约以每秒处理163,000个标记的速度进行训练。这是一个更客观的指标。好的，现在我们启用TF32。幸运的是，PyTorch为我们提供了相对简单的方式来启用TF32。你只需要写一行代码。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_118.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_118.png)'
- en: And it's this。 And when we go to the PyTorch documentation here for this function。
    basically this tells PyTorch what kind of kernels to run。 And by default， I believe
    it is highest。 Highest precision for Matmull。 And that means that everything happens
    in Flow32。 just like it did before。 But if we set it too high as we do right now。
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是重点。当我们查看PyTorch文档中的这个函数时，基本上这告诉PyTorch应该运行什么类型的内核。我相信默认情况下是最高的。最高精度用于矩阵乘法。这意味着一切都在Flow32中进行，就像之前一样。但如果我们设置得太高，就像现在这样。
- en: matrix multiplications will now use TensorFlow32 when it's available。
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法现在会在可用时使用TensorFlow32。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_120.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_120.png)'
- en: My GPU is the A100， so it's an ampere series and therefore TF32 is available。
    If you're an older GPU， this might not be available for you。 But for my GPU it's
    available。 And so what I expect PyTorch to do is that every single place where
    we see an end-out linear。 inside there is a matrix multiplication。 And I expect
    that matrix multiplication now to be running on TensorCore。
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我的GPU是A100，所以是安培系列，因此TF32是可用的。如果你使用的是旧GPU，这可能对你来说不可用。但对我来说是可用的。因此，我期望PyTorch在我们看到的每个end-out线性地方，里面都会进行矩阵乘法。我期望这个矩阵乘法现在能够在TensorCore上运行。
- en: utilizing the TF32 precision。 So this is the single line of change that is，
    I believe， necessary。 And let's rerun this。 Now we saw that in terms of the throughput
    that has promised to us。 we're supposed to be getting 8x roughly。 So let's see
    what happens。 And that 8x came from here。 right？
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 利用TF32精度。这是我认为必要的单行代码修改。让我们重新运行这个。现在我们看到，根据承诺的吞吐量，我们应该大约获得8倍的提升。那么让我们看看会发生什么。这8倍来自于这里，对吧？
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_122.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_122.png)'
- en: 8x。
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 8倍。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_124.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_124.png)'
- en: And it also came from looking at it here。 156 T-flops instead of 19。5。 Okay。
    so what actually happened？ So we're seeing that our throughput roughly 3x， not
    8x。 So we are going。 we're from 1000 milliseconds， we're going down to 300 milliseconds。
    and our throughput is now about 50，000 tokens per second。 So we have a roughly
    3x instead of 8x。
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这也来自于这里的观察。156 T-flops，而不是19.5。好的，那么实际上发生了什么？所以我们看到我们的吞吐量大约是3倍，而不是8倍。我们从1000毫秒降至300毫秒。现在我们的吞吐量大约是每秒50,000个令牌。所以我们大约是3倍，而不是8倍。
- en: So what happened？ And basically what's happening here is， again。 a lot of these
    workloads are memory bound。 And so even though the TF32 offers in principle a
    lot faster throughput。 all of these numbers everywhere are still float 32s。 And
    it's float 32 numbers that are being shipped all over the place through the memory
    system。
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 那么发生了什么？基本上这里发生的是，很多工作负载都受限于内存。因此，尽管TF32原则上提供了更快的吞吐量，但所有这些数字仍然是float 32s。并且通过内存系统传输的都是float
    32数字。
- en: And it's just costing us way too much time to shuttle around all those data。
    And so even though we've made the multiply itself much faster， we are memory bound。
    and we're not actually seeing the full benefit that would come from this napkin
    map here。 That said。 we are getting a 3x faster throughput。 And this is free。
    Single line of code in PyTorch。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在移动所有这些数据时耗费了太多时间。因此，尽管我们使乘法本身变得更快，但我们受限于内存。我们并没有真正看到来自这个简化映射的全部好处。也就是说，我们的吞吐量快了3倍。这是免费的。在PyTorch中的一行代码。
- en: All your variables are still float 32 everywhere。 It just runs faster。 It's
    slightly more approximate， but we're not going to notice it basically。
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你所有的变量仍然是float 32。它只是运行得更快。它稍微更近似，但基本上我们不会注意到。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_126.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_126.png)'
- en: So that's TF32。 Okay， so let's now continue。 So we've exercised this row。 and
    we saw that we can crop out some of the precision inside the operation itself。
    But we saw that we're still memory bound。 We're still moving around all these
    floats， right。 otherwise。 And we're paying that cost because of this。
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是TF32。好的，现在我们继续。我们已经对这一行进行了操作，并且看到我们可以在操作内部裁剪出一些精度。但是我们发现我们仍然受限于内存。我们仍然在移动这些浮点数，对吧？否则的话。我们因为这个付出了代价。
- en: So let's now decrease the amount of stuff that we're going to be moving around。
    And we're going to do that by dropping down to Bflow 16。 So we're only going to
    be maintaining 16 bits per float。 And we're going to use the Bflow 16。 and I'll
    explain in a bit at B16 difference， and we're going to be in this row。
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在让我们减少要移动的数据量。我们将通过降至Bflow 16来实现。因此，我们将只保持每个浮点16位。我们将使用Bflow 16，我会稍后解释B16的差异，我们将处于这一行。
- en: So when we go back to the documentation here for the A100。 we see here the precision
    that are available。 And this is the original F3。2。 The TF32 crops out the precision。
    And then here in Bf16， you see that it is very similar to TF32。 but it's even
    more aggressive in cropping off the precision， the mantissa of this float。
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们回到A100的文档时，我们看到可用的精度。这是原始的F3.2。TF32裁剪掉了精度。然后在Bf16中，你会看到它与TF32非常相似，但在裁剪浮点数的尾数方面更为激进。
- en: So the important thing with Bflow 16 is that the exponent bits， and the sign
    bit， of course。 remain unchanged。 So if you're familiar with your float numbers。
    and I think this should probably be an entire video by itself。 the exponent sets
    the range that you can represent of your numbers。
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Bflow 16的重要之处在于指数位和符号位当然保持不变。所以如果你熟悉浮点数，我认为这可能应该是一个完整的视频。指数决定了你可以表示的数字范围。
- en: And the precision is how much precision you have for your numbers。 And so the
    range of numbers is identical， but we have fewer possibilities within that range。
    because we are truncating the mantissa， so we have less precision in that range。
    What that means is that things are actually fairly nice because we have the original
    range of numbers that are representable in float。
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 精度是你对数字的精确度。因此，数字的范围是相同的，但我们在该范围内的可能性更少，因为我们截断了尾数，所以在该范围内的精度较低。这意味着事情实际上相当不错，因为我们拥有可以在浮点数中表示的原始数字范围。
- en: But we just have less precision for it。 And the difference with Fb16 is that
    they actually touch and change the range。 So Fb16 cannot represent the full range
    of Fb32。 It has a reduced range。 And that's where you start to actually run into
    issues because now you need these gradients。 scalars and things like that。 And
    I'm not going to go into the detail of that in this video because that's a whole
    video by itself。
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们只是对它的精度较低。与 Fb16 的不同之处在于，他们实际上触及并改变了范围。因此，Fb16 不能表示 Fb32 的全部范围。它的范围被缩小了。这就是你开始遇到问题的地方，因为现在你需要这些梯度缩放器等等。我不会在这个视频中详细讲解这些，因为那是一个完整的视频。
- en: But Fb16 actually historically came first。 That was available in the Volta series
    before Ampere。 And so Fb16 came first and everyone started to train Fb16。 But
    everyone had to use all these gradient scaling operations which are kind of annoying。
    and it's an additional source of state and complexity。
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 Fb16 实际上历史上是第一个。它在安培架构之前就已经在 Volta 系列中可用。因此，Fb16 首先出现，每个人都开始训练 Fb16。但每个人都必须使用所有这些梯度缩放操作，这些操作有点烦人，并且是额外的状态和复杂性来源。
- en: And the reason for that was because the exponent range was reduced in Fb16。
    So that's the IEEE Fb16 spec。 And then they came out with Bf16 and the Ampere。
    And they made it much simpler because we're just truncating Mntissa。 We have the
    exact same range and we do not need gradient scalars。 So everything is much simpler。
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是因为 Fb16 中的指数范围被缩小了。这是 IEEE Fb16 规范。然后他们推出了 Bf16 和安培架构。他们让事情变得简单得多，因为我们只是截断了尾数。我们拥有完全相同的范围，并且不需要梯度缩放器。所以一切都简单得多。
- en: Now when we do use Bf16 though， we are impacting the numbers that we might be
    seeing in our PyTorch code。 This change is not just local to the operation itself。
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们使用 Bf16 时，我们实际上会影响在 PyTorch 代码中可能看到的数字。这一变化不仅限于操作本身。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_128.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_128.png)'
- en: So let's see how that works。 There's some documentation here that。 so I think
    this is probably the best page to explain how to use mixed precision in PyTorch。
    Because there are many other tutorials and so on， even within PyTorch documentation。
    that are a lot more confusing。 And so I recommend specifically this one because
    there's five other copies that I would not recommend。
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看这是如何工作的。这里有一些文档，我认为这可能是解释如何在 PyTorch 中使用混合精度的最佳页面。因为还有许多其他教程，甚至在 PyTorch
    文档中，这些都更加混乱。因此，我特别推荐这一篇，因为还有五个我不推荐的副本。
- en: And then when we come here， ignore everything about everything。 Ignore everything
    about the gradient scalars and only look at Torx。autocast。 And basically also
    this comes to a single line of code at the end。 So this is the context manager
    that we want。 And we want to use that in our network。
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 然后当我们来到这里时，忽略关于所有事情的一切。忽略关于梯度缩放器的一切，仅查看 Torx.autocast。基本上，这在最后变成了一行代码。因此，这是我们想要的上下文管理器。我们希望在我们的网络中使用它。
- en: When you click into the Torx。autocasting， it has a bit more guideline for you。
    So it's telling you do not call Bf16 on any of your tensors。 Just use autocast
    and only surround the forward pass of the model and the loss calculation。 And
    that's the only two things that you should be surrounding。
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击 Torx.autocasting 时，它会给你更多的指导。它告诉你不要在任何张量上调用 Bf16。只需使用 autocast，并仅围绕模型的前向传播和损失计算。这是你应该围绕的唯一两个部分。
- en: Leave the backward and the off-measure step alone。 So that's the guidance that
    comes from the PyTorch team。 So we're going to follow that guidance。 And for us，
    because the loss calculation is inside of the model forward pass for us。 we are
    going to be doing this。 And then we don't want to be using Torx。
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 不要碰回传和离线测量步骤。因此，这是 PyTorch 团队给出的指导。我们将遵循这一指导。对于我们来说，因为损失计算在模型的前向传播内部，所以我们将这样做。然后我们不想使用
    Torx。
- en: autocasting because if we do that， we need to start using gradient scalars as
    well。 So we are going to be using Bf16。 This is only possible to do an ampere。
    But this means that the changes are extremely minimal。 Well， basically just this
    one line of code。 Let me first break in to here before we actually run this。 So
    right after logits。
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 自动混合精度，因为如果我们这样做，我们需要开始使用梯度缩放器。因此我们将使用Bf16。这只有在Ampere架构上才能实现。但这意味着变化非常微小。基本上就是这一行代码。让我先插入一下，然后再实际运行。在logits之后。
- en: I'd like to show you that different from the TF32 that we saw。 this is actually
    going to impact our tensors。 So this logit tensor。 if we now look at this and
    we look at the D type， we suddenly see that this is now Bf16。 It's not flue 32
    anymore。 So our activations have been changed。 The activations tensor is now Bf16。
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我想向你展示，与我们看到的TF32不同，这实际上会影响我们的张量。因此这个logit张量，如果我们现在看看它的D类型，我们突然发现它现在是Bf16。它不再是flow
    32了。因此我们的激活值已经改变。激活张量现在是Bf16。
- en: But not everything has changed。 Model。transformer。WTE。 This is the weight token
    embedding table。 It has a dot weight inside it。 And the D type of this weight，
    this parameter， is still torchflow 32。 So our parameters seem to still be in flow
    32。 But our activations， the logits are now in Bf16。 So clearly， this is why we
    get the mixed precision。 Some things PyTorch is keeping in flow 32。
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 但并不是所有的东西都改变了。模型。transformer。WTE。这是权重令牌嵌入表。它里面有一个点权重。这个权重的D类型，这个参数，仍然是torchflow
    32。因此我们的参数似乎仍然在flow 32中。但我们的激活值，logits现在是Bf16。因此，这显然是我们获得混合精度的原因。有些东西PyTorch仍然保留在flow
    32中。
- en: Some things PyTorch is converting to lower precision。
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 有些东西PyTorch正在转换为低精度。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_130.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_130.png)'
- en: And what gets converted， at what point， is not super clear。 I remember scrolling
    down。 is it here？
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 什么被转换，在哪个时点，并不是特别清楚。我记得往下滚动。是这里吗？
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_132.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_132.png)'
- en: Okay， I can't find it。 I thought it was here。 Okay， there we go。 So there are
    a few docs on when you're using this AutoCast。 What gets converted to Bf16 and
    when？
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我找不到它。我以为在这里。好吧，有一些文档说明你在使用这个AutoCast时，什么会被转换为Bf16，以及何时转换？
- en: So， for example， only these matrix multiply-like operations get converted to
    Bf16。 But a lot of operations remain in flow 32。 So in particular。 a lot of normalizations
    like layer norms and things like that。 Not all of those layers might be converted。
    So only some layers selectively would be running Bf16。
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，只有这些类似矩阵乘法的操作会被转换为Bf16。但许多操作仍然在flow 32中。因此，特别是很多标准化操作，如层归一化等。并不是所有这些层都可能被转换。因此，只有某些层会选择性地运行Bf16。
- en: But things like softmax， layer norms， log softmax， so loss function calculations。
    A lot of those things might remain in flow 32 because they are more susceptible
    to precision changes。 Major multiplies are fairly robust to precision changes。
    So some parts of the network are impacted more or less by the precision change。
    So basically。
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 但像softmax、层归一化、log softmax以及损失函数计算这样的东西。许多这些东西可能仍然在flow 32中，因为它们更容易受到精度变化的影响。主要的乘法对精度变化相对鲁棒。因此，网络的某些部分受到精度变化的影响程度不同。基本上。
- en: only some parts of the model are running in reduced precision。 Let's take it
    for a spin and let's actually see what kind of improvement we achieved here。 Okay。
    so we used to be 333 milliseconds。 We're now 300。 And we used to be somewhere
    around 50。000 tokens per second。 We're now 55。 So we're definitely running faster，
    but maybe not a lot faster。
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 只有模型的某些部分在减少精度下运行。让我们试试看，实际上看看我们在这里取得了什么样的改进。好的，我们以前是333毫秒。现在是300毫秒。我们以前每秒大约50,000个标记。现在是55,000。因此我们肯定运行得更快，但可能并没有快很多。
- en: And that's because there are still many， many bottlenecks in RGP2。 We're just
    getting started。 But we have dropped down the precision as far as we can with
    my current GPU， which is A100。 We're using PyTorch AutoCast。 Unfortunately， I
    don't actually exactly know what PyTorch AutoCast does。 I don't actually know
    exactly what's in Bflow 16， what's in flow 32。
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这因为RGP2中仍然有许多瓶颈。我们才刚刚开始。但我们已尽可能降低精度，使用的是A100 GPU。我们正在使用PyTorch AutoCast。不幸的是，我实际上并不知道PyTorch
    AutoCast到底做了什么。我也不确切知道Bflow 16中包含了什么，flow 32中又有什么。
- en: We could go in and we could start to scrutinize it。 But these are the kinds
    of rules that PyTorch has internally。 And unfortunately。 they don't document it
    very well。 So we're not going to go into that in too much detail。 But for now，
    we are training in Bflow 16。 We do not need a gradient scaler。
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以深入探讨，但这是 PyTorch 内部的一些规则。不幸的是，他们没有很好地记录它。因此我们不会深入细节。但现在，我们在 Bflow 16 中训练。我们不需要梯度缩放器。
- en: And the reason things are running faster is because we are able to run tensor
    course in Bflow 16 now。 That means we are in this row。 But we are also paying
    in precision for this。 So we expect slightly less accurate results with respect
    to the original IP32。 But empirically。 in many cases， this is a worth it kind
    of trade off。 Because it allows you to run faster。
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 事情运行得更快的原因是我们现在能够在 Bflow 16 中运行张量课程。这意味着我们在这一行中。但我们也因此付出了精度的代价。因此，我们预期与原始 IP32
    相比，结果会略微不准确。但从经验上来看，在很多情况下，这种权衡是值得的。因为它允许你运行得更快。
- en: And you could， for example， train longer and make up for that precision decrease。
    So that Bflow 16 for now。 Okay， so as we can see， we are currently at about 300
    milliseconds per iteration。 And we're now going to reach for some really heavy
    weapons in the PyTorch arsenal。
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，你可以训练更长时间，以弥补精度下降。所以目前的 Bflow 16。好的，正如我们所见，我们当前每次迭代大约需要 300 毫秒。现在我们将使用 PyTorch
    工具箱中的一些强大武器。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_134.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_134.png)'
- en: And in particular， we're going to introduce Torch。compile。 So Torch。compile
    is really quite incredible infrastructure from the PyTorch team。 And it's basically
    a compiler for neural networks。 Like it's almost like GCC for C and C++ code。
    And this is just GCC of neural nets。
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将介绍 `Torch.compile`。所以 `Torch.compile` 实际上是 PyTorch 团队提供的非常棒的基础设施。它基本上是一个神经网络的编译器。就像是
    C 和 C++ 代码的 GCC。这就是神经网络的 GCC。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_136.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_136.png)'
- en: So it came out a while ago and extremely simple to use。 The way to use Torch。compile
    is to do this。 It's a single line of code to compile your model and return it。
    Now this line of code will cost you compilation time。 But as you might guess。
    it's going to make the code a lot faster。 So let's actually run that。
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具不久前推出，使用起来非常简单。使用 `Torch.compile` 的方式就是这样。编译你的模型并返回它只需要一行代码。现在，这行代码会花费你一些编译时间。但正如你可能猜到的，这会使代码变得更快。所以我们实际运行一下。
- en: Because this will take some time to run。 But currently remember we're 300 milliseconds。
    And we'll see what happens。 Now while this is running。 I'd like to explain a little
    bit of what Torch。compile does under the hood。 So feel free to read this page
    of PyTorch。 But basically there's no real good reason for you to not use Torch。
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这会花费一些时间来运行。但目前请记住，我们是 300 毫秒。我们将看看发生了什么。现在在运行时，我想稍微解释一下 `Torch.compile` 在幕后做了什么。所以随时可以阅读这页
    PyTorch 的内容。但基本上没有什么好的理由让你不使用 `Torch`。
- en: compile in your PyTorch。 I kind of feel like you should be using it almost by
    default if you're not。 Unless you're debugging and you want your code to run really
    fast。 And there's one line here in Torch。compile that I found that actually kind
    of like gets to why this is faster。 Speedup comes from reducing Python overhead
    and GPU read writes。 So let me unpack that a little bit。
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的 PyTorch 中使用 `compile`。我觉得如果你不这样做，你几乎应该默认使用它。除非你在调试并希望你的代码运行得非常快。这里有一行在 `Torch.compile`
    中，我发现它实际上有助于解释为什么这更快。速度提升来自于减少 Python 的开销和 GPU 的读写。因此让我详细解释一下。
- en: Okay， here we are。 Okay， so we're going from 300 milliseconds。 We're now running
    at 129 milliseconds。 So this is 300 divided 129， about 2。3x。 improvement from
    a single line of code in PyTorch。 So quite incredible。 So what is happening。 what's
    happening under the hood？ Well when you pass the model to Torch。compile。
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们到了。好的，从 300 毫秒开始。现在我们运行在 129 毫秒。所以这是 300 除以 129，约 2.3 倍。从 PyTorch 中的一行代码获得的改进，真是不可思议。那么到底发生了什么？在幕后发生了什么？当你将模型传递给
    `Torch.compile` 时。
- en: what we have here in this end module， this is really just the algorithmic description
    of what we'd like to happen in our network。 And Torch。compile will analyze the
    entire thing and it will look at what operations you like to use。 And with the
    benefit of knowing exactly what's going to happen。 it doesn't have to run in what's
    called the eager mode。
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个结束模块中，我们所拥有的实际上是我们希望在网络中发生的算法描述。`Torch.compile` 将分析整个过程，并查看你希望使用的操作。凭借确切知道将会发生什么的优势，它不必在所谓的急切模式下运行。
- en: It doesn't have to just kind of like go layer by layer。 like the Python interpreter
    normally would start at the forward。 And the Python interpreter will go。 okay，
    let's do this operation。 And then let's do that operation。 And it kind of materializes
    all the operations as it goes through。
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 它不必逐层进行，就像Python解释器通常从正向开始一样。Python解释器会说，好的，我们先执行这个操作，然后再执行那个操作。它在进行时逐步实现所有操作。
- en: So these calculations are dispatched and run in this order。 And the Python interpreter
    and this code doesn't know what kind of operations are going to happen later。
    But Torch。compile sees your entire code at the same time。 And it's able to know
    what operations you intend to run and it will kind of optimize that process。
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算被调度并按顺序运行。而Python解释器和这段代码不知道后续将会发生什么样的操作。但Torch.compile能够同时查看你的整个代码，并能够知道你打算执行哪些操作，它会对这个过程进行优化。
- en: The first thing we'll do is we'll take out the Python interpreter from the forward
    pass entirely。 And it will kind of compile this entire neural net as a single
    object with no Python interpreter involved。 So it knows exactly what's going to
    run and we'll just run that。 And it's all going to be running in efficient code。
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是完全移除正向传递中的Python解释器。它会将整个神经网络编译为一个单一对象，而不涉及Python解释器。因此它确切知道将要运行的内容，我们将直接运行它。所有内容都将以高效的代码运行。
- en: The second thing that happens is this read/write that they mentioned very briefly。
    So a good example of that I think is the Galou nonlinearity that we've been looking
    at。 So here we use the NN Galou。 Now this here is me basically just breaking up
    the NN Galou which you remember has this formula。
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 第二件事是他们提到的读/写，这里简要提到过。所以我认为一个很好的例子是我们一直在研究的Galou非线性。在这里，我们使用NN Galou。现在这里是我基本上只是拆分NN
    Galou，你还记得这个公式。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_138.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_138.png)'
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_139.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_139.png)'
- en: So this here is the equivalent implementation to what's happening inside Galou。
    Algorithmicly it's identical。 Now by default if we just were using this instead
    of NN that Galou here。 What would happen without Torch。compile？ Well the Python
    interpreter would make its way here。 And then it would be okay well there's an
    input。 Well let me first let me raise this input to the third power。
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是与Galou内部发生的事情相等效的实现。在算法上，它是相同的。现在如果我们只是使用这个而不是NN Galou，会发生什么情况而没有Torch.compile呢？那么Python解释器会来到这里。然后它会说，好的，有一个输入。那么让我先将这个输入提升到三次方。
- en: And it's going to dispatch a kernel that takes your input and raises to the
    third power。 And that kernel will run。 And when this kernel runs what ends up
    happening is this input is stored in the memory of the GPU。
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 它将调度一个内核，该内核接收你的输入并提升到三次方。然后这个内核将运行。当这个内核运行时，输入被存储在GPU的内存中。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_141.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_141.png)'
- en: So here's a helpful example of the layout of what's happening right。 If your
    CPU this is in every single computer there's a few cores in there and you have
    your RAM。 your memory。 And the CPU can talk to the memory and this is all well
    known。 But now we've added the GPU。 And the GPU is a slightly different architecture
    of course they can communicate。
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个有用的示例，展示了正在发生的事情。如果你的CPU，这是每台计算机都有的，有几个核心在里面，你有你的RAM和内存。CPU可以与内存通信，这一切都是众所周知的。但现在我们增加了GPU。GPU的架构稍微不同，当然它们可以通信。
- en: And it's different in that it's got a lot more cores than the CPU。 All of those
    cores are individually a lot simpler too。 But it also has memory right this high
    bandwidth memory。 Sorry if I'm botching it。 HBM I don't even know what that stands
    for。 I'm just realizing now。
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 它的不同之处在于它的核心数量比CPU多得多。所有这些核心在各自的层面上也都简单得多。但它也有内存，对，这种高带宽内存。抱歉，如果我说错了。HBM我甚至不知道那代表什么。我现在才意识到。
- en: But this is the memory and it's very equivalent to RAM basically in the computer。
    And what's happening is that input is living in the memory。 And when you do input
    cubed this has to travel to the GPU to the course and to all the caches and registers
    on the actual chip of this GPU。 And it has to calculate all the elements to the
    third and then it saves the result back to the memory。
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 但这就是内存，基本上等同于计算机中的RAM。发生的事情是输入存储在内存中。当你执行输入的三次方时，这个数据必须传输到GPU的核心，以及这个GPU芯片上的所有缓存和寄存器。它必须将所有元素计算到三次方，然后将结果保存回内存中。
- en: And it's this travel time that actually causes a lot of issues。 So here remember
    this memory bandwidth？ We can communicate about two terabytes per second which
    is a lot。 But also we have to traverse this link and it's very slow。 So here on
    the GPU we're on a chip and everything is super passed within the chip。
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这段旅行时间导致了很多问题。所以这里记住这个内存带宽？我们可以以每秒大约两个太字节的速度进行通信，这非常多。但我们也必须穿越这条链接，而这非常慢。因此在GPU上，我们处于一个芯片上，一切都在芯片内部超级快速。
- en: But going to the memory is extremely expensive takes extremely long amount of
    time。 And so we load the input， do the calculations and load back the output。
    And this round trip takes a lot of time。
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 但是去内存是非常昂贵的，耗时极长。因此我们加载输入，进行计算并加载输出。这次往返需要很多时间。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_143.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_143.png)'
- en: And now right after we do that we multiply by this constant。
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在我们做完之后，我们会乘以这个常量。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_145.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_145.png)'
- en: So what happens then is we dispatch another kernel and then the result travels
    back all the elements get multiplied by constant。 And then the result travel back
    to the memory。
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 所以接下来发生的是，我们调度另一个内核，然后结果回传，所有元素都与常量相乘。然后结果传回内存。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_147.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_147.png)'
- en: And then we take the result and we add back input。
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取结果并加回输入。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_149.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_149.png)'
- en: And so this entire thing again travels to the GPU as the inputs and gets written
    back。 So we're making all these round trips from the memory to actually where
    the computation happens。 Because all the tensor cores and ALUs and everything
    like that is all stored on the chip in the GPU。 So we're doing a ton of round
    trips。 And PyTorch without using Torch Compile doesn't know to optimize this because
    it doesn't know what kind of operations you're running later。
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这一切又作为输入传送到GPU，并写回去。因此我们正在进行许多往返，从内存到实际进行计算的地方。因为所有的张量核心和算术逻辑单元（ALU）等都存储在GPU芯片上。所以我们进行大量的往返。PyTorch在不使用Torch
    Compile的情况下不知道如何优化这一点，因为它不知道你之后要执行的操作类型。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_151.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_151.png)'
- en: You're just telling it raise the power to the third， then do this， then do that。
    And it will just do that in that sequence。 But Torch Compile sees your entire
    code。 It will come here and it will realize wait all of these are element-wise
    operations。
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需告诉它将指数提升到三次，然后进行这个，再进行那个。它会按照顺序执行这些操作。但Torch Compile可以看到你的整个代码。它会来到这里，意识到所有这些都是逐元素操作。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_153.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_153.png)'
- en: And actually what I'm going to do is I'm going to do a single trip of input
    to the GPU。
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 其实我将要做的是，我会将输入单次传送到GPU。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_155.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_155.png)'
- en: Then for every single element I'm going to do all of these operations while
    that memory is on the GPU。
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于每个元素，我会在内存仍在GPU上的时候进行所有这些操作。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_157.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_157.png)'
- en: Or chunks of it rather。 And then I'm going to write back a single time。 So we're
    not going to have these round trips。 That's one example of what's called kernel
    fusion and is a major way in which everything is sped up。 So basically if you
    have your benefit of concept and you know exactly what you're going to compute。
    you can optimize your round trips to the memory。 And you're not going to pay the
    memory bandwidth cost。
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 或者说是它的一部分。然后我将只写回一次。因此我们不会有这些来回。这就是所谓的内核融合的一个例子，是加速的主要方式之一。因此基本上，如果你对概念有了明确的认识，并且知道你要计算的内容，你可以优化与内存的往返次数。你不会为内存带宽成本而付出代价。
- en: And that's fundamentally what makes some of these operations a lot faster。
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是根本上让某些操作变得更快的原因。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_159.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_159.png)'
- en: And what they mean by read writes here。 So let me erase this because we are
    not using it。 And yeah we should be using the torch compile and our code is now
    significantly faster。 And we're doing about 125，000 tokens per second。 But we
    still have a long way to go。
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 那么他们这里所说的读写是什么意思呢？让我把这个擦掉，因为我们不再使用它。是的，我们应该使用Torch Compile，现在我们的代码显著更快了。我们每秒处理大约125,000个标记。但我们仍然有很长的路要走。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_161.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_161.png)'
- en: Before we move on I wanted to supplement the discussion a little bit with a
    few more figures。 Because this is a complicated topic but it's worth understanding
    on the high level what's happening here。 And I could probably spend an entire
    video of like two hours on this but just the preview of that basically。 So this
    chip here that is the GPU。 This chip is where older calculations happen mostly。
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我想用几个数字补充一下讨论。因为这是一个复杂的话题，但值得高层理解这里发生了什么。我可能会用两个小时的整个视频来讨论这个，但只是简单预览一下。因此，这个芯片就是
    GPU。这颗芯片主要进行旧计算。
- en: But this chip also does have some memory in it。 But most of the memory by far
    is here in the high bandwidth memory。 HBM， and is connected。 They're connected。
    But these are two separate chips basically。 Now here this is a zoom in of kind
    of this cartoon diagram of a GPU。 And what we're seeing here is number one you
    see this HBM。
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 但这颗芯片内部确实有一些内存。但绝大多数内存仍在高带宽内存 HBM 中，并且是相互连接的。它们基本上是两个独立的芯片。现在这里是 GPU 卡通图的放大图。我们在这里看到的是，第一，你可以看到这个
    HBM。
- en: I realize it's probably very small for you。 But on the sides here it says HBM。
    And so that's the length of the HBM。 Now the HBM is again off chip。 On the chip
    there are a large number of these streaming multiprocessors。 Every one of these
    is an SM。 There's 120 of them in total。
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我意识到对你来说可能非常小。但这里的侧面标记着 HBM。所以那是 HBM 的长度。现在 HBM 还是在芯片外。在芯片上有大量的流式多处理器。每一个都是一个
    SM，总共有 120 个。
- en: And this is where a lot of the calculations happen。 And this is a zoom in of
    a single individual SM。 It has these four quadrants and C for example tensor core。
    This is where a lot of the matrix multiply stuff happens。 But there's all these
    other units to do all different kinds of calculations for FV64， FV32。
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大量计算发生的地方。这里是一个单个 SM 的放大图。它有这四个象限和 C，比如张量核心。这是矩阵乘法的主要部分。但还有其他单元执行不同类型的计算，比如
    FV64 和 FV32。
- en: and for integers and so on。 Now so we have all this logic here to the calculations。
    But in addition to that on the chip there is memory sprinkled throughout the chip。
    So L2 cache is some amount of memory that lives on the chip。 And then on the SMs
    themselves there's L1 cache。 I realize it's probably very small for you but this
    blue bar is L1。
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 以及整数等等。现在，我们在这里有所有逻辑进行计算。此外，芯片上还有分散的内存。因此，L2 缓存是芯片上的某种内存。而在 SM 本身上有 L1 缓存。我意识到对你来说可能非常小，但这个蓝条就是
    L1。
- en: And there's also registers。 And so there is memory stored here。 But the way
    this memory is stored is very different from the way memory stored in HBM。 This
    is a very different implementation using just in terms of like what the silicon
    looks like。 It's a very different implementation。 So here you would be using transistors
    and capacitors。
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 还有寄存器。因此，这里存储了内存。但这种内存的存储方式与 HBM 中存储内存的方式截然不同。这是一种非常不同的实现，仅在硅的外观上来看。这是一个非常不同的实现。所以这里会使用晶体管和电容器。
- en: And here it's a very different implementation with SRAM and what that looks
    like。 But long story short is there is memory inside the chip but it's not a lot
    of memory。 That's the critical point。 So this is some example diagram of a slightly
    different GPU just like here。 Where it shows that for example typical numbers
    for CPU D or M memory which is this thing here。
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的实现非常不同，使用 SRAM，外观也不同。但长话短说，芯片内部有内存，但并不多。这是关键点。因此，这是一个稍微不同的 GPU 示例图，就像这里一样。它显示了典型的
    CPU D 或 M 内存的典型数值，即这里的东西。
- en: You might have one terabyte of disk but it would be extremely expensive to access。
    Especially for a GPU you have to go through the CPU here。 Now next we have the
    HBM。 So we have tens of gigabytes of HBM memory on a typical GPU here。 But as
    I mentioned very expensive to access。 And then on the chip itself everything is
    extremely fast within the chip。
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有一太字节的磁盘，但访问成本极高。特别是对于 GPU，你必须通过 CPU 来访问。接下来是 HBM。因此，典型的 GPU 上有数十 GB 的 HBM
    内存。但正如我提到的，访问成本非常高。然后在芯片上，内部一切都是极快的。
- en: But we only have a couple of 10 megabytes of memory collectively throughout
    the chip。 And so there's just not enough space because the memory is very expensive
    on the chip。 And so there's not a lot of it but it is lightning fast to access
    in relative terms。 And so basically whenever we have these kernels the more accurate
    picture of what's happening here is that we take these inputs which live by default
    on the global memory。
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们在整个芯片上只有几MB的内存。因此，空间非常有限，因为芯片上的内存非常昂贵。所以内存不多，但在相对条件下访问速度非常快。基本上，每当我们有这些内核时，实际发生的情况是我们获取这些默认存在于全局内存中的输入。
- en: And now we need to perform some calculation。 So we start streaming the data
    from the global memory to the chip。 We perform the calculations on the chip and
    then stream it back and store it back to the global memory。 And so if we don't
    have torch compile we are streaming the data through the chip during the calculations
    and saving to the memory。 And we're doing those round trips many many times。
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要进行一些计算。因此，我们开始将数据从全局内存流向芯片。我们在芯片上进行计算，然后再将其流回并存储到全局内存中。如果没有torch compile，我们将在计算期间通过芯片流动数据并保存到内存。这种往返操作会进行很多次。
- en: But if it's torch compiled then we start streaming the memory as before。 But
    then while we're on the chip we have a chunk of the data that we're trying to
    process。 So that chunk now lives on the chip。 While it's on the chip it's extremely
    fast to operate on。 So if we have kernel fusion we can do all the operations right
    there in an element wise passion。
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果是torch compiled，那么我们会像之前一样开始流动内存。但在芯片上，我们有一部分数据正在处理。因此，这部分数据现在驻留在芯片上。驻留在芯片上时，它的操作速度极快。如果我们有内核融合，我们可以在这里逐元素进行所有操作。
- en: And those are very cheap。 And then we do a single round trip back to the global
    memory。 So operator fusion basically allows you to keep your chunk of data on
    the chip and do lots of calculations on it before you write it back。 And that
    gives huge savings and that's why torch compile ends up being a lot faster or
    that's one of the major reasons。 So again just a very brief intro to the memory
    hierarchy and roughly what torch compile does for you。
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作非常便宜。然后我们只需进行一次往返到全局内存。因此，操作融合基本上允许你将数据块保留在芯片上，并在写回之前对其进行大量计算。这带来了巨大的节省，这也是torch
    compile速度更快的主要原因之一。再次简单介绍一下内存层次结构以及torch compile为你做的大致工作。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_163.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_163.png)'
- en: Now torch compile is amazing but there are operations that torch compile will
    not find。 And an amazing example of that is flash attention to which we turn next。
    So flash attention comes from this paper from Stanford in 2022。 And it's this
    incredible algorithm for performing attention。 So and running it a lot faster。
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，torch compile很惊人，但有些操作torch compile无法找到。一个惊人的例子就是flash attention，我们接下来会讨论它。flash
    attention源自2022年斯坦福大学的一篇论文。这是一个令人难以置信的算法，用于执行注意力机制，并且运行速度更快。
- en: So flash attention will come here and we will take out these four lines。 And
    flash attention implements these four lines really really quickly。
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，flash attention会在这里来，我们将提取这四行。flash attention非常快速地实现这四行。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_165.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_165.png)'
- en: And how does it do that？ Well flash attention is a kernel fusion operation。
    So you see here we have in this diagram they're showing PyTorch and you have these
    four operations。 They're including dropout but we are not using dropout here。
    So we just have these four lines of code here。 And instead of those we are fusing
    them into a single fused kernel of flash attention。
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何做到的呢？flash attention是一个内核融合操作。你可以在这个图中看到PyTorch和这四个操作。它们包括dropout，但我们这里不使用dropout。所以我们只有这四行代码。我们将其融合为一个单一的flash
    attention融合内核。
- en: So it's a kernel fusion algorithm but it's a kernel fusion that torch compile
    cannot find。 And the reason that it cannot find it is that it requires an algorithmic
    rewrite of how attention is actually implemented here in this case。 And what's
    remarkable about it is that flash attention actually if you just count the number
    of flops。 flash attention does more flops than this attention here。
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个内核融合算法，但它是torch compile无法找到的内核融合。它无法找到的原因在于需要对这里实际实现的注意力机制进行算法重写。令人惊讶的是，如果你只计算flops，flash
    attention的flops数量比这里的注意力还要多。
- en: But flash attention is actually significantly faster。 In fact they site 7。6
    times faster potentially。 And that's because it is very mindful of the memory
    hierarchy as I described it just now。 And so it's very mindful about what's in
    High Bandwidth memory， what's in the shared memory。 And it is very careful with
    how it orchestrates to computation such that we have fewer reads and writes to
    the High Bandwidth memory。
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 但flash attention实际上要快得多。事实上，他们的速度可能快7.6倍。这是因为它非常关注内存层次结构，就像我刚才描述的那样。因此，它非常注意高带宽内存中有什么，共享内存中有什么。它在协调计算时非常小心，从而减少对高带宽内存的读取和写入。
- en: And so even though we're doing more flops the expensive part is they're load
    and store into HBM and that's what they avoid。 And so in particular they do not
    ever materialize this end by end attention matrix。 This ADT here。 A flash attention
    is designed such that this matrix never gets materialized at any point and it
    never gets read or written to the HBM。 And this is a very large matrix right so
    because this is where all the queries and keys interact and we're sort of getting
    for each head。
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管我们执行了更多的浮点运算，但昂贵的部分是加载和存储到HBM中，而这正是他们所避免的。尤其是，他们从未实现这个端到端的注意力矩阵。这种ADT设计使得这个矩阵在任何时候都不会被实现，也不会被读写到HBM中。这是一个非常大的矩阵，因为这是所有查询和键交互的地方，我们在为每个头部获取信息。
- en: for each batch element we're getting a T by T matrix of attention which is a
    million numbers even for a single head at a single batch index。 So basically this
    is a ton of memory and this is never materialized。 And the way that this is achieved
    is that basically the fundamental algorithmic rewrite here relies on this online
    softmax trick which was proposed previously and I'll show you the paper in a bit。
    And the online softmax trick coming from a previous paper shows how you can incrementally
    evaluate a softmax without having to sort of realize all of the inputs to the
    softmax of the normalization。
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个批次元素，我们获得一个T乘T的注意力矩阵，即使对于单个头在单个批次索引中也是一百万个数字。因此，这基本上占用了大量内存，而且这个矩阵从未被实现。实现这一点的方法是基本上依赖于之前提出的在线softmax技巧，我稍后会向你展示那篇论文。这个来自前一篇论文的在线softmax技巧展示了如何增量地评估softmax，而不需要实现所有输入到softmax归一化的过程。
- en: And you do that by having these intermediate variables M and L and there's an
    update to them that allows you to evaluate the softmax in an online manner。 Now
    flash attention actually recently flash attention to came out as well so I have
    that paper up here as well that has additional gains to how it calculates flash
    attention。
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有这些中间变量M和L，你可以进行更新，使你能够以在线方式评估softmax。最近flash attention也发布了，所以我在这里也有那篇论文，里面有关于它如何计算flash
    attention的额外收获。
- en: And the original paper that this is based on basically is this online normalization
    calculation for softmax and remarkably it came out of Nvidia and it came out of
    it like really early 2018。 And this is four years before flash attention。 And
    this paper says that we propose a way to compute the classical softmax with fewer
    memory accesses and hypothesize that this reduction in memory accesses should
    improve softmax performance on actual hardware。
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此的原始论文基本上是关于软max的在线归一化计算，值得注意的是它出自Nvidia，并且是在2018年初期提出的。这比flash attention早了四年。这篇论文提出了一种方法，可以减少内存访问次数来计算经典的softmax，并假设这种内存访问的减少应该能提高实际硬件上softmax的性能。
- en: And so they are extremely correct in this hypothesis but it's really fascinating
    to me that they're from Nvidia and that they had this realization but they didn't
    actually take it to the actual flash attention that had to come four years later
    from Stanford。
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，他们在这个假设上是极其正确的，但对我来说很有趣的是他们来自Nvidia，并且他们有这种认识，但实际上并没有将其应用到真正的flash attention上，这要等到四年后才由斯坦福大学提出。
- en: So I don't fully understand the historical how this happened historically but
    they do basically propose this online update to the softmax right here。 And this
    is fundamentally what they reuse here to calculate the softmax in a streaming
    manner。
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不完全理解这在历史上是如何发生的，但他们基本上在这里提出了对softmax的在线更新。这是他们在流式计算中重用的基本内容。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_167.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_167.png)'
- en: And then they realize that they can actually fuse all the other operations with
    the online softmax calculation into a single fused kernel flash attention and
    that's what we are about to use。 So a great example I think of being aware of
    memory hierarchy， the fact that flobs don't matter。
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 然后他们意识到他们实际上可以将所有其他操作与在线softmax计算融合成一个单一的融合内核闪存注意力，这就是我们即将使用的。因此，我认为这是一个很好的例子，意识到内存层次结构，flobs并不重要。
- en: the entire memory access pattern matters， and that torch compile is amazing
    but there are many optimizations that are still available to us that potentially
    torch compile cannot find。 Maybe one day it could but right now it seems like
    a lot to ask。 So here's what we're going to do。
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 整个内存访问模式很重要，torch compile很棒，但仍然有许多优化可供我们使用，这些优化可能torch compile无法找到。也许有一天可以，但现在看来有点要求过高。所以我们要做的就是这样。
- en: we're going to use flash attention and the way to do that basically in PyTorch
    is we are going to comment out these four lines and we're going to replace them
    with a single line。 And here we are calling this compound operation in PyTorch
    called scale。productattention。
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用闪存注意力，基本上在PyTorch中的方法是我们将注释掉这四行，并用一行替换它们。在这里，我们调用这个在PyTorch中的复合操作，称为scale.productattention。
- en: And PyTorch will call flash attention when you use it in this way。 I'm not actually
    100% sure why torch compile doesn't realize that these four lines should just
    call flash attention in this exact way。 We have to do it again for it， which in
    my opinion is a little bit odd。 But here we are。 So you have to use this compound
    up and let's wait for a few moments before torch compile gets around to it。
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 当你以这种方式使用它时，PyTorch将调用闪存注意力。我实际上不太确定为什么torch compile不意识到这四行应该以这种确切方式调用闪存注意力。我们必须再次为此操作，这在我看来有点奇怪。不过我们在这里。所以你必须使用这个复合操作，让我们等几秒钟再等torch
    compile处理它。
- en: And then let's remember that we achieved 6。05661， I have it here。 that's the
    loss we are expecting to see。 And we took 130 milliseconds before this change。
    So we're expecting to see the exact same result by iteration 49 but we expect
    to see faster runtime。 Because flash attention is just an algorithmic rewrite
    and it's a faster kernel but it doesn't actually change any of the computation
    and we should have the exact same optimization。
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们记住，我们达到了6.05661，我在这里有。这是我们期望看到的损失。在这次更改之前，我们花了130毫秒。因此，我们期望在第49次迭代时看到完全相同的结果，但我们预计运行时间会更快。因为闪存注意力只是算法重写，是一个更快的内核，但它实际上并没有改变任何计算，我们应该拥有完全相同的优化。
- en: So okay， so we're a lot faster。 We're at about 95 milliseconds and we achieve
    6。058。 Okay。 so they're basically identical up to a floating point fetch factor。
    So it's the identical computation but it's significantly faster going from 130
    to roughly 96。 And so this is 96 divided 130 ish so this may be 27% improvement。
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 所以好的，我们快得多。我们大约在95毫秒左右，达到了6.058。所以它们基本上在浮点获取因子上是相同的。因此，计算是相同的，但从130毫秒缩短到大约96毫秒，速度显著更快。这是96除以130，因此这可能是27%的提升。
- en: So really interesting and that is flash attention。 Okay。 we are now getting
    to one of my favorite optimizations。 And it is simultaneously the dumbest and
    the most brilliant optimization and it's always a little bit surprising to me。
    Anyway， so basically I mentioned a few minutes ago that there are some numbers
    that are nice and some numbers that are ugly。
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很有趣，那就是闪存注意力。好的，我们现在要谈到我最喜欢的优化之一。这是同时最愚蠢和最聪明的优化，这总是让我有点惊讶。无论如何，基本上我几分钟前提到了一些数字，有些数字很好，有些数字很丑。
- en: So 64 is a beautiful nice number。 128 is even nicer。 256 is beautiful。 What
    makes these numbers beautiful is that there are many powers of two inside them。
    You can divide by two many times。 And examples of ugly numbers are like 13 and
    17 and something like that。 Prime numbers， numbers that are not even and so on。
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 所以64是一个美丽的好数字。128更好。256也很美。使这些数字美丽的原因是其中包含了许多二的幂。你可以多次除以二。丑陋数字的例子是像13和17这样的质数，不是偶数的数字等等。
- en: And so pretty much you always want to use nice numbers in all of your code that
    deals with neural networks or CUDA。 Because everything in CUDA works in sort of
    like powers of two and lots of kernels are written in terms of powers of two。
    And there are lots of blocks of sizes 16 and 64 and so on。 So everything is written
    in those terms and you always have special case handling for all kinds of logic
    that when your inputs are not made of nice numbers。
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你几乎总是想在所有与神经网络或CUDA相关的代码中使用好数字。因为CUDA中的一切都是以2的幂的方式运作，许多内核是以2的幂为基础编写的。还有许多大小为16和64的块等等。因此，一切都是以这些术语编写的，你总是需要处理所有输入不是由好数字组成的各种逻辑的特殊情况。
- en: So let's see what that looks like。 Basically scan your code and look for ugly
    numbers is roughly the heuristic。 So three times is kind of ugly。 I'm not 100%
    sure maybe this can be improved but this is ugly and not ideal。 Four times is
    nice。 So that's nice。 1024 is very nice。 That's a power of two。 12 is a little
    bit suspicious。 Not too many powers of two。 768 is great。 50，000 to 57 is a really。
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看这是什么样子。基本上扫描你的代码，寻找难看的数字是大致的启发式方法。所以三倍有点难看。我不是100%确定，也许这可以改进，但这是难看的，不理想。四倍是好的。所以这很好。1024是非常好的。这是2的幂。12有点可疑。2的幂不多。768很好。50,000到57确实是一个很。
- en: really ugly number。 First of all it's odd。 And there's not too many powers of
    two in there。 So this is a very ugly number and it's highly suspicious。 And then
    when we scroll down all these numbers are nice and then here we have mostly nice
    numbers except for 25。 So in this configuration of GPT-2 XL a number of heads
    is 25。 That's a really ugly number。
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 真的很难看的数字。首先，它是奇数。而且里面的2的幂不多。因此，这是一个非常难看的数字，而且非常可疑。当我们向下滚动时，所有这些数字都很好，然后在这里我们有大多数不错的数字，除了25。因此在这个GPT-2
    XL配置中，头的数量是25。这真是一个非常难看的数字。
- en: That's an odd number and actually this did cause a lot of headaches for us recently
    when we were trying to optimize some kernels to run this fast。 And required a
    bunch of special case handling。 So basically these numbers are。 we have some ugly
    numbers and some of them are easier to fix than others。 And in particular the
    vocab size being 50，000 to 57。 That's a very ugly number。
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个奇怪的数字，实际上当我们最近试图优化一些内核以快速运行时，这确实给我们带来了很多麻烦。这需要处理一堆特殊情况。因此，基本上这些数字是……我们有一些难看的数字，其中一些比其他的更容易修复。特别是词汇量为50,000到57，这是一个非常难看的数字。
- en: very suspicious and we want to fix it。 Now when you fix these things one of
    the easy ways to do that is you basically increase the number until it's the nearest
    power of two that you like。 So here's a much nicer number。 It's 50，000， 304。 And
    why is that？ Because 50，000。
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 非常可疑，我们想要修复它。现在，当你修复这些事情时，简单的方法之一是基本上增加这个数字，直到它成为你喜欢的最接近的2的幂。因此，这里有一个更好的数字。它是50,000，304。为什么呢？因为50,000。
- en: 304 can be divided by 8 or by 16 or by 32， 64。 It can even be divided by 128
    I think。 So it's a very nice number。 So what we're going to do here is this is
    the GPT config and you see that we initialize vocab size to 50。000， 257。 Let's
    override just that element to be 50，304。 Okay。 So everything else stays the same。
    We're just increasing our vocabulary size。 So we're adding。
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 304可以被8、16、32和64整除。我想它甚至可以被128整除。因此，这是一个非常好的数字。那么我们在这里要做的是，这个是GPT配置，你会看到我们将词汇量初始化为50,000，257。让我们把那个元素覆盖为50,304。好的，其他的一切保持不变。我们只是增加我们的词汇量。
- en: it's almost like we're adding fake tokens。 So that vocab size has powers of
    two inside it。 Now actually what I'm doing here by the way is I'm increasing the
    amount of computation that our network will be doing。 If you just count the flops
    on like do the math of how many flops we're doing。 we're going to be doing more
    flops。 And we still have to think through whether this doesn't break anything。
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎就像我们在添加虚假的令牌。因此，词汇量内部有2的幂。顺便说一下，我在这里所做的事情是我在增加我们的网络将要进行的计算量。如果你只是计算一下像是做数学的操作，我们将进行更多的flops。而我们仍然需要考虑这是否会破坏任何东西。
- en: But if I just run this， let's see what we get。 Currently this ran in maybe 96。5
    milliseconds per step。 I'm just kind of like eyeballing it。 And let's see what
    kind of result we're going to get。 While this is compiling。 let's think through
    whether our code actually works。 Okay。
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我只是运行这个，看看我们能得到什么。目前，这大概每步运行在96.5毫秒。我只是大致估算一下。让我们看看我们会得到什么结果。在这个编译的过程中，让我们思考一下我们的代码实际上是否有效。好的。
- en: When we increase the vocab size like this。 Let's look at where vocab size is
    actually used。 So we swing up to the init and we see that it's used inside the
    embedding table， of course。 So all the way at the bottom of the transformer and
    it's used at the classifier layer all the way at the top of the transformer。 So
    in two places。 And let's take a look and we're running at 93。 So 93 milliseconds
    instead of 96。5。
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们以这种方式增加词汇量时。让我们看看词汇量实际上是在哪里使用的。因此，我们转到初始化，当然看到它在嵌入表中使用。所以在变换器的底部，并且它在变换器顶部的分类器层中使用。所以在两个地方。让我们看看，我们运行在
    93。因此是 93 毫秒，而不是 96.5。
- en: So we are seeing a roughly a whole percent improvement here by doing more calculations。
    And the reason for this is we fixed， we made an ugly number into a nice number。
    Let's。 I'm going to come into the explanation for that a little bit again。 But
    for now。 let's just convince ourselves that we're not breaking anything when we
    do this。 So first of all。
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过进行更多计算，我们看到这里有大约 1% 的提升。原因是我们修复了，将一个丑陋的数字变成了一个好数字。让我再稍微解释一下。但现在，让我们首先相信，当我们这样做时不会破坏任何东西。所以首先。
- en: we've made the WTE， the embedding table for the tokens。 We've made it larger。
    It's almost like we introduced more tokens at the bottom。 And these tokens are
    never used because the GPT tokenizer only has tokens up to 50，000 to 56。 And so
    we'll never index into the rows that we've added。
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经制作了 WTE，即用于令牌的嵌入表。我们将其做得更大。几乎就像我们在底部引入了更多的令牌。这些令牌从未被使用，因为 GPT 的分词器仅包含 50,000
    到 56 的令牌。因此，我们永远不会索引到我们添加的行。
- en: So we're wasting a little bit of space here by creating memory that's never
    going to be accessed。 never going to be used， et cetera。 Now that's not fully
    correct because this WTE weight ends up being shared and ends up being used in
    the classifier here at the end。
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过创建永远不会被访问、永远不会被使用等的内存，我们在这里浪费了一些空间。虽然这并不完全正确，因为这个 WTE 权重最终被共享，并在最后的分类器中使用。
- en: So what is that doing to the classifier？ I brought here。 Well。 what that's doing
    is we're predicting additional dimensions of the classifier now。 And we're predicting
    probabilities for tokens that will， of course。 never be present in the training
    set。 And so therefore the network has to learn that these probabilities have to
    be driven to zero。
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这对分类器有什么影响？我带来了。好吧。这的确是，我们现在正在预测分类器的额外维度。我们正在预测那些当然永远不会出现在训练集中的令牌的概率。因此，网络必须学习这些概率必须被驱动到零。
- en: And so the logits that the network produces have to drive those dimensions of
    the output to negative infinity。 But that's no different from all the other tokens
    that are already in our dataset or rather that are not in our dataset。 So Shakespeare
    only probably uses， let's say， 1，000 tokens out of 50，000 to 57 tokens。 So most
    of the tokens are already being driven to zero probability by the optimization。
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络产生的 logits 必须将输出的这些维度驱动到负无穷大。但这与我们数据集中已经存在的其他令牌并无不同，或者说是数据集中不存在的令牌。所以莎士比亚可能只使用了大约
    1,000 个令牌中的 50,000 到 57 个令牌。因此，大多数令牌已经通过优化被驱动到零概率。
- en: We've just introduced a few more tokens now that in a similar manner will never
    be used and have to be driven to zero in probability。 So functionally though nothing
    breaks。 We're using a bit more extra memory。 But otherwise this is a harmless
    operation as far as I can tell。 But。 and we're adding calculation but it's running
    faster。
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚引入了一些更多的令牌，它们以类似的方式将永远不会被使用，并且必须在概率上被驱动到零。因此，从功能上来说，尽管没有什么破坏。我们使用了稍微多一点的内存。但就我所知，这是一项无害的操作。不过，我们增加了计算，但它运行得更快。
- en: And it's running faster because as I mentioned in CUDA so many kernels use block
    tiles。 And these block tiles are usually nice numbers， so powers of two。 So calculations
    are done in like chunks of 64 or chunks of 32。 And when you're desired calculation
    doesn't neatly fit into those block tiles。
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行得更快，因为正如我在 CUDA 中提到的，许多内核使用块大小。这些块大小通常是好数字，即 2 的幂。因此，计算是以 64 或 32 的块进行的。当你想要的计算不能完美地适应这些块大小时。
- en: There are all kinds of boundary kernels that can kick in to like do the last
    part。 So basically in a lot of kernels they will truncate up your input and they
    will do the nice part first。 And then they have a whole second phase where they
    come back to anything that like remains。 And then they process the remaining part。
    And the kernels for that could be very inefficient。
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种边界内核可以启动来处理最后一部分。因此在很多内核中，它们会截断你的输入，并首先处理漂亮的部分。然后它们有一个完整的第二阶段，回来处理剩余的部分。用于处理剩余部分的内核可能非常低效。
- en: And so you're basically spinning up all this extra compute and it's extremely
    inefficient。 So you might as well pad your inputs and make it fit nicely。 And
    usually that empirically it's actually running faster。 So this is another example
    of a 4% improvement that we've added。
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你基本上是在利用额外的计算资源，这样非常低效。因此你不如填充输入，让它看起来更合适。通常从经验上来看，这样的做法实际上运行得更快。这是我们增加的另一个4%改进的例子。
- en: And this is something that also Torch Compile did not find for us。 You would
    hope that Torch Compile at some point could figure an optimization like this out。
    But for now this is it。 And I also have to point out that we're using PyTorch
    nightly。 So that's why we're only seeing 4%。 If you're using PyTorch 2。3。
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是Torch Compile没有为我们找到的东西。你会希望Torch Compile在某个时候能够找出这样的优化。但目前就只有这些。同时我也要指出，我们正在使用PyTorch的夜间版本。所以这就是为什么我们只看到了4%的提升。如果你使用的是PyTorch
    2.3。
- en: 1 or earlier you would actually see something like 30% improvement。 Just from
    this change from 50。000 to 57 to 53。04。 So again one of my favorite examples also
    of having to understand the under the hood and how it all works。 And to know what
    kinds of things to tinker with to push the performance of your code。 Okay so at
    this point we have improved the performance by about 11x。
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是1或更早的时候，你实际上会看到大约30%的提升。仅仅是因为从50,000改到57再到53.04。所以，这也是我最喜欢的例子之一，理解底层是如何运作的，以及知道应该调整哪些内容来推动代码的性能。好的，到目前为止，我们的性能提高了大约11倍。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_169.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_169.png)'
- en: Because we started at about 1000 milliseconds per step and we're now down to
    like 93 milliseconds。 So that's quite good。 And we're doing a much better job
    of utilizing our GPU resources。 So I'm going to now turn to more algorithmic changes
    and improvements to the actual optimization itself。 And what we would like to
    do is we would like to follow the hyperparameters that are mentioned in the GPT2
    or GPT3 paper。
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们最初每一步约为1000毫秒，而现在降到了93毫秒。所以这是相当不错的。我们在利用GPU资源方面做得更好了。现在我将转向更多的算法改进和优化本身。我们希望遵循在GPT2或GPT3论文中提到的超参数。
- en: Now sadly GPT2 doesn't actually say too much。 It's very nice of them that they
    released the model weights and the code。 But the paper itself is extremely vague
    as to the optimization details。 The code itself that they released as well。 The
    code would be looking at this is just the inference code。 So there's no training
    code here and very few hyperparameters。
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 现在遗憾的是，GPT2并没有详细说明太多内容。他们发布模型权重和代码是非常不错的。但论文本身在优化细节方面非常模糊。发布的代码也一样。代码主要关注的是推理部分，并没有训练代码，超参数也非常少。
- en: So this doesn't also tell us too much。 So for that we have to turn to the GPT3
    paper。 And in the appendix of the GPT3 paper they have a lot more hyperparameters
    here for us to use。 And the GPT3 paper in general is a lot more detailed as to
    all the small details that go into the model training。 But GPT3 models were never
    released。 So GPT2 we have the weights but no details。
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 因此这并没有告诉我们太多。因此我们需要参考GPT3的论文。在GPT3论文的附录中，他们为我们提供了更多的超参数。而且GPT3论文在模型训练的所有小细节方面要详细得多。但GPT3模型从未发布。因此我们有GPT2的权重，但没有细节。
- en: In GPT3 we have lots of details but no weights。 But roughly speaking GPT2 and
    GPT3 architectures are very very similar。 And basically there are very few changes。
    The context length was expanded from 1024 to 2048 and that's kind of like the
    major change。
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT3中，我们有很多细节，但没有权重。大致而言，GPT2和GPT3的架构非常相似。基本上变化很少。上下文长度从1024扩展到2048，这大致就是主要的变化。
- en: And some of the hyperparameters around the transformer have changed。 But otherwise
    they're pretty much the same model。 It's just that GPT3 was trained for a lot
    longer on a bigger data set and has a lot more thorough evaluations。 And the GPT3
    model is 175 billion instead of 1。6 billion in the GPT2。 So long story short we're
    going to go to GPT3 paper to follow along some of the hyperparameters。
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的一些超参数发生了变化。但否则它们基本上是同一模型。只是 GPT-3 在更大的数据集上训练了更长时间，并进行了更全面的评估。GPT-3 模型的参数为1750亿，而
    GPT-2 只有16亿。简而言之，我们将查看 GPT-3 的论文，以跟踪一些超参数。
- en: So to train all the versions of GPT3 we use Adam with beta 1 beta 2 of 0。9 and
    0。95。 So let's move over here and make sure that the beta is parameter which you
    can see here defaults to 0。9 and 0。999 is actually set to 0。9 and 0。95。 And then
    the epsilon parameter you can see is the default is 1 -8 and this is also 1 -8。
    Let's just put it in so that we're explicit。 Now next up they say we clip the
    grad global norm of the gradient at 1。
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练所有版本的 GPT-3，我们使用的 Adam 优化器的 beta 1 和 beta 2 分别为0。9和0。95。接下来我们确保 beta 参数，这里默认值为0。9，而0。999实际设置为0。9和0。95。然后可以看到，epsilon
    参数的默认值为1 -8，这也是1 -8。我们明确设置一下。接下来他们说我们将梯度的全局范数裁剪到1。
- en: 0。 So what this is referring to is that once we calculate the gradients right
    after loss。backward。 we basically have the gradients at all the parameter tensors。
    And what people like to do is basically clip them to have some kind of a maximum
    norm。 So in PyTorch this is fairly easy to do。 It's one line of code here that
    we have to insert right after we calculate the gradients。
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 0。这里提到的是，在我们计算完损失的梯度后执行`loss.backward()`，我们基本上就得到了所有参数张量的梯度。人们通常会将这些梯度裁剪到某种最大范数。因此在
    PyTorch 中，这个操作相对简单。我们只需在计算梯度后插入一行代码。
- en: And what this utility function is doing is it's calculating the global norm
    of the primers。 So every single gradient on all the primers you square it and
    you add it all up and you take a big square root of that。 And that's the norm
    of the parameter vector basically。 It's the length of it if you'd like to look
    at it that way。
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具函数的作用是计算参数的全局范数。对所有参数的每一个梯度进行平方，然后将其全部相加，最后取平方根。这基本上就是参数向量的范数。如果你愿意，也可以将其视为长度。
- en: And we are basically making sure that its length is no more than 1。0 and we're
    going to clip it。 And the reason that people like to use this is that sometimes
    you can get unlucky during the optimization。 Maybe it's a bad data batch or something
    like that。 And if you get very unlucky in a batch you might get really high loss
    and really high loss could lead to a really high gradient。
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上确保它的长度不超过1。0，并且我们将对其进行裁剪。人们喜欢使用这个方法的原因是，有时在优化过程中可能会出现不幸情况。也许是一个坏的数据批次或其他问题。如果在某个批次中非常不幸，可能会得到非常高的损失，而高损失会导致非常高的梯度。
- en: And this could basically shock your model and shock the optimization。 So people
    like to use a gradient norm clipping to prevent the model from basically getting
    two bigger shocks in terms of the gradient magnitude and the upper bound in this
    way。 It's a bit of a hacky solution。 It's about like a patch on top of deeper
    issues but people still do it fairly frequently。 Now the clip grad norm returns
    the norm of the gradient which I like to always visualize because it is useful
    information and sometimes you can look at the norm of the gradient。
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会冲击你的模型并冲击优化过程。因此，人们喜欢使用梯度范数裁剪来防止模型在梯度大小和上界方面受到过大的冲击。这有点像一种应急解决方案，类似于在更深层次问题上打补丁，但人们仍然相当频繁地使用它。现在，`clip_grad_norm`返回的是梯度的范数，我总是喜欢将其可视化，因为这是一条有用的信息，有时你可以查看梯度的范数。
- en: And if it's well behaved things are good。 If it's climbing things are bad and
    they're destabilizing during training。 Sometimes you could get a spike in the
    norm and that means there's some kind of an issue or instability。 So the norm
    here will be a norm and let's do a 。4 f or something like that。 And I believe
    this is just a float。 And so we should be able to print that。
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 如果情况良好，那就没问题。如果出现波动，那就糟糕了，并且在训练过程中会不稳定。有时范数会出现尖峰，这意味着存在某种问题或不稳定性。因此这里的范数将是一个范数，我们将设定为0。4或其他值。我相信这只是一个浮点数。因此我们应该能够打印出来。
- en: So that's global gradient clipping。 Now they go into the details of the learning
    rate scheduler。 So they don't just use a fixed learning rate like we do here for
    three negative four but there's actually basically a cosine decay learning rate
    schedule。
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全局梯度裁剪。现在他们开始详细介绍学习率调度器。因此，他们并不像我们在这里使用的三负四那样仅使用固定学习率，而是基本上使用了余弦衰减学习率调度。
- en: It's got a warm up and it's got a cosine decay to 10% over some horizon。 And
    so we're going to implement this in a second。 I just like to see the norm printed
    here。 Okay。 there we go。 So what happened here is the norm is actually really
    high in the beginning 30 or so。 And you see that as we continue training it kind
    of like stabilizes at values below one。
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 它有预热，并且在某个范围内以10%的比例进行余弦衰减。因此我们很快就会实现这个。我只想看到这里打印的范数。好的，来了。所以这里发生的事情是，开始的30个阶段范数实际上非常高。你会看到随着训练的继续，它会在低于1的值附近稳定下来。
- en: And this is not that crazy uncommon for the norm to be high in the very first
    few stages。 Basically what's happening here is the model is completely random。
    And so there's a ton of learning happening very early in the network。 But that
    learning is kind of like it's mostly learning the biases of the output tokens。
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的几个阶段，范数高并不是那么罕见。基本上这里发生的事情是模型完全随机。因此在网络的早期有大量的学习发生。但这种学习主要是学习输出token的偏差。
- en: And so it's a bit of an unstable time but the network usually stabilizes in
    the very few iterations。 So this looks relatively reasonable to me except usually
    I would expect this looks a little bit funky。 We go from 28 to 6 to 2 and then
    to 10。 It's not completely insane but it's just kind of a little bit funky。 Okay，
    so let's now get to the learning rate scheduler。
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这段时间有点不稳定，但网络通常在很少的迭代中会稳定下来。所以这对我来说看起来相对合理，尽管通常我会觉得它有点奇怪。我们从28降到6再到2，然后又升到10。这并不是完全疯狂，但就是有点奇怪。好吧，让我们现在来看学习率调度器。
- en: So the learning rate schedule that's used here in GPT-3 is what's called a cosine
    decay learning schedule with warmup。 And the way this looks is that the learning
    rate is basically starts right at around zero。 linearly ramps up over some amount
    of time and then comes down with this cosine sort of form。 and comes down to some
    kind of a minimum learning rate that's up to you。
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-3中使用的学习率调度被称为余弦衰减学习调度，带有预热。这个过程的方式是，学习率基本上从零开始，线性上升一段时间，然后以余弦的形式下降，最终达到一个最低学习率，这个最低学习率由你来决定。
- en: So here the minimum learning rate is zero but here in the paper they said that
    they use cosine decay。 for learning rate down to 10% of its value over the first
    260 billion tokens and then training continues 10% after。 And there's a linear
    warmup over the first 375 million tokens。 So that's about the learning rate so
    let's now implement this。
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最低学习率是零，但在论文中他们提到使用了余弦衰减。在前2600亿个token中，学习率下降到其值的10%，然后训练继续10%之后。并且在前375百万个token中有线性预热。这就是关于学习率的情况，现在我们来实现这个。
- en: So I already implemented it here and the way this works is let me scroll down
    first here。 I changed our training loop a little bit so this was a 4i in max steps。
    I just changed it to step now so that we have the notion of a step is a single
    optimization step in the 4 loop。 And then here I get the LR for this step of the
    optimization using a new function I call get LR。
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在这里实现了这个功能，首先让我向下滚动一下。我稍微修改了我们的训练循环，这个循环在最大步骤中是4i。我将其改为step，现在我们有了单次优化步骤的概念。
- en: And then in PyTorch to set the learning rate I think this is the way to set
    the learning rate。 It's a little bit gnarly because you have to basically there's
    a notion of different parameter groups that could exist in the optimizer。 and
    we actually have to iterate over them even though we currently have a single parameter
    group only。 And you have to set the LR in this 4 loop kind of style is my impression
    right now。
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在PyTorch中设置学习率，我认为这是设置学习率的方法。这个过程有点复杂，因为优化器中可能存在不同的参数组。尽管我们目前只有一个参数组，但实际上我们需要遍历它们。而且你必须以这种4循环的方式来设置学习率，这是我目前的印象。
- en: So we have this local for LR we set the learning rate and then on the bottom
    also printing it。 So that's all the changes I made to this loop and then of course
    the get LR is my scheduler。 Now it's worth pointing out that PyTorch actually
    has learning rate schedulers and you can use them and I believe there's a cosine
    learning rate schedule in PyTorch。 I just don't really love using that code because
    honestly it's like five lines of code and I fully understand what's happening
    inside these lines。
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有这个本地的学习率，我们设置学习率，然后底部也打印它。这就是我对这个循环所做的所有更改，当然get LR是我的调度器。值得指出的是，PyTorch实际上有学习率调度器，你可以使用它们，我相信PyTorch中有余弦学习率调度。我只是并不特别喜欢使用那段代码，因为老实说，它只有五行代码，我完全理解这些代码内部发生了什么。
- en: So I don't love to use abstractions where they're kind of incurable and then
    I don't know what they're doing。 So personal style。 So the max learning rate here
    is let's say three negative four but we're going to see that in GPT three here。
    They have a table of what the maximum learning rate is for every model size。 So
    for this one basically 12 12 layer 768 GPT three。
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我不喜欢使用那些几乎无法治愈的抽象概念，然后我不知道他们在做什么。这是个人风格。因此这里的最大学习率可以说是三负四，但我们将会在GPT-3中看到。他们有一个表格，列出了每种模型规模的最大学习率。对于这个模型，基本上是12层768的GPT-3。
- en: So the GPT three small is roughly like a GPT two one 24m。 We see that here they
    use a learning rate of six E negative four。 So we could actually go higher。 In
    fact we may want to try to follow that and just set the max LR here at six。 Then
    the maximum learning rate the middle learning rate is 10% of that per description
    in the paper。
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 所以GPT-3小模型大致相当于GPT-2的1.24亿参数。我们在这里看到他们使用的学习率是六E负四。所以我们实际上可以设定更高。事实上，我们可能希望尝试遵循这一点，并将最大学习率设置为六。然后根据论文的描述，最大学习率的中间学习率是其10%。
- en: Some number of steps that we're going to warm up over and then the maximum steps
    of the optimization which I now use also in the for loop down here。 And then you
    can go for this code if you like it's not it's not terribly inside floor interesting。
    I'm just modulating based on the iteration number which learning rate there should
    be。 So this is the warm up region。 This is the region after the optimization and
    then this is the region sort of in between and this is where I calculate the cosine
    learning rate schedule。
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 一些步骤将会在此升温，然后是优化的最大步骤，我现在在下面的for循环中也使用了。然后如果你喜欢，可以查看这段代码，它并不是特别有趣。我只是根据迭代次数调节应该使用的学习率。这是升温区域。这是优化后的区域，然后这是介于两者之间的区域，这是我计算余弦学习率调度的地方。
- en: And you can step through this in detail if you'd like but this is basically
    implementing this curve。 And I ran this already and this is what that looks like。
    So when we now run we start at some very low number。 Now note that we don't start
    exactly at zero because that would be not useful to update with a learning rate
    of zero。
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，可以详细了解这一点，但这基本上是在实现这个曲线。我已经运行过这个，这是它的效果。所以当我们现在运行时，从一个非常低的数字开始。请注意，我们并不是从零开始，因为用学习率为零进行更新是没有用的。
- en: That's why there's an it plus one so that on the zero iteration we are not using
    exactly zero we're using something very very low。 Then we've linear warm up to
    maximum learning rate which in this case was three negative four when I ran it
    but now would be sixteen negative four。
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么有一个加一项，这样在零次迭代时我们并不是使用完全的零，而是使用一个非常非常小的值。然后我们线性地升温到最大学习率，在我的运行中是三负四，但现在是十六负四。
- en: And then it starts to decay all the way down to three negative five which was
    at the time 10% of the original learning rate。 Now one thing we are not following
    exactly is that they mentioned that。 Let me see if I can find it again。 We're
    not exactly following what they did because they mentioned that their training
    horizon is 300 billion tokens。 And they come down to 10% of the initial learning
    rate of at 260 billion and then they train after 260 with 10%。
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它开始衰减，直到三负五，当时是原始学习率的10%。现在我们没有完全遵循他们所提到的。我看看能否再找到一次。他们提到他们的训练范围是3000亿个token。并且在2600亿时降低到初始学习率的10%，然后在2600亿之后以10%进行训练。
- en: So basically their decay time is less than the max steps time where for us they're
    exactly equal。 So it's not exactly faithful but it's an okay。 This is okay for
    us and for our purposes right now and we're just going to use this ourselves。
    I don't think it makes too big of a difference honestly。 I should point out that
    what learning rate schedule you use is totally up to you。
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，他们的衰减时间小于最大步数的时间，而对我们来说它们是完全相等的。所以这并不是完全忠实，但对我们来说还可以。这对我们目前的目的来说是可以接受的，我们就打算这样使用。我老实说不认为这有什么太大的区别。我应该指出，使用什么学习率调度完全取决于你自己。
- en: So we have two different types。 Cosine learning rate has been popularized a
    lot by GPT 200。 GPT 3 but people have come up with all kinds of other learning
    rate schedules。 And this is kind of like an active area of research as to which
    one is the most effective at training these networks。 Okay next up。 The paper
    talks about the gradual batch size increase。
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有两种不同类型。余弦学习率在GPT 200和GPT 3中被广泛宣传，但人们提出了各种各样的其他学习率调度。这实际上是一个活跃的研究领域，研究哪种方法在训练这些网络时最有效。好的，下一个。论文讨论了逐渐增加批量大小。
- en: So there's a ramp on the batch size that is linear and you start with very small
    batch size and you ramp up to a big batch size over time。 We're going to actually
    skip this and we're not going to work with it。 And the reason I don't love to
    use it is that it complicates a lot of the arithmetic because you are changing
    the number of tokens that you're processing at every single step of the optimization。
    And I like to keep that math very very simple。 Also my understanding is that this
    is not like a major improvement。
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 所以批量大小的提升是线性的，你从非常小的批量大小开始，随着时间的推移逐渐增加到较大的批量大小。我们实际上会跳过这个步骤，不会使用它。我不喜欢使用它的原因是，它使许多算术变得复杂，因为你在优化的每一步都在改变处理的令牌数量。我喜欢保持这些数学非常简单。此外，我的理解是，这并不是一个重大的改进。
- en: And also my understanding is that this is not like an algorithmic optimization
    improvement。 It's more of a systems and speed improvement。 And I'm actually speaking
    this is because in the early stages of the optimization。 Again the model is in
    a very atypical setting and mostly what you're learning is that you're mostly
    learning to ignore the tokens that don't come up in your training set very often。
    You're learning very simple biases and that kind of a thing。
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 我同样理解，这并不是一种算法优化改进。它更像是系统和速度的提升。我实际上这么说是因为在优化的早期阶段。再次强调，模型处于一个非常不典型的环境中，你大部分学到的只是学会忽略那些在训练集中出现频率很低的令牌。你学到的是非常简单的偏差和类似的东西。
- en: And so every single example that you put through your network is basically just
    telling you use these tokens and don't use these tokens。 And so the gradients
    from every single example are actually extremely highly correlated。 They all look
    roughly the same in the original parts of the optimization because they're all
    just telling you that these tokens don't appear and these tokens do appear。 And
    so because the gradients are all very similar and they're highly correlated then
    why are you doing bad sizes of like millions when if you do a bad size of 32K
    you're basically getting the exact same gradient early on in the training。
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你放入网络的每个示例基本上只是告诉你使用这些令牌，而不要使用那些令牌。因此，来自每个示例的梯度实际上是高度相关的。它们在优化的最初阶段看起来大致相同，因为它们都在告诉你这些令牌不出现，而这些令牌会出现。所以因为这些梯度非常相似且高度相关，为什么你要使用像几百万这样的批量大小，而如果你使用32K的批量大小，你基本上在训练早期获得的梯度是完全相同的呢？
- en: And then later in the optimization once you've learned all the simple stuff
    that's where the actual work starts and that's where the gradients become more
    de-correlated per examples。 And that's where they actually offer you sort of statistical
    power in some sense。
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在优化的后期，一旦你学会了所有简单的内容，实际的工作才开始，这时每个示例的梯度变得更加去相关。那时候它们在某种意义上确实提供了统计能力。
- en: So we're going to skip this just because it kind of complicates things。 And
    we're going to go to data are sampled without replacement during training。 So
    until an epoch boundary is reached。 So without replacement means that they're
    not sampling from some fixed pool and then take a sequence train on it but then
    also like return to sequence the pool。 They are exhausting a pool。 So when they
    draw a sequence it's gone until the next epoch of training。
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将跳过这一部分，因为它有点复杂。我们将讨论在训练过程中数据是无替换抽样的。直到达到一个训练周期的边界。无替换意味着它们不是从某个固定的池中抽样然后训练一个序列，而是消耗掉这个池。当它们抽取一个序列时，直到下一个训练周期，它就消失了。
- en: So we're already doing that because our data loader iterates over chunks of
    data。 So there's no replacement。 They don't become eligible to be drawn again
    until the next epoch。 So we're basically already doing that。 All models use a
    weight decay of 0。1 to provide a small amount of regularization。 So let's implement
    the weight decay。
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在这样做，因为我们的数据加载器遍历数据块。因此没有替换。它们在下一个周期之前不会再次被抽取。所以我们实际上已经在这样做了。所有模型使用0.1的加权衰减来提供少量正则化。因此让我们实现加权衰减。
- en: And you see here that I've already kind of made the changes。 And in particular
    instead of creating the optimizer right here。 I'm creating a new configure optimizer
    function inside the model and I'm passing in some of the hyper parameters instead。
    So let's look at the configure optimizer switch is supposed to return the optimizer
    object。 Okay。
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我已经进行了更改。特别是，我没有在这里直接创建优化器，而是在模型内部创建一个新的配置优化器函数，并传入一些超参数。那么让我们看看配置优化器开关应该返回优化器对象。好的。
- en: so it looks complicated but it's actually really simple and it's just we're
    just being very careful and there's a few settings here to go through。 The most
    important thing with respect to this line is that you see there's a weight decay
    parameter here and I'm passing that into。
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来复杂，但其实非常简单，我们只是非常小心，并且这里有一些设置需要通过。与这一行相关的最重要的事情是，你会看到这里有一个加权衰减参数，我将其传递进去。
- en: well I'm passing that into something called Optim groups that eventually ends
    up going into the add and W optimizer。 And the weight decay that's by default
    used in add and W here is 0。01。 So it's 10 times lower than what's used in a GPT
    3 paper here。 So the weight decay basically ends up making its way into the add
    and W 3 optimizer groups。
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我将其传递给一个叫做优化组的东西，最终会进入加法和W优化器。这里默认使用的加权衰减是0.01，因此比GPT 3论文中使用的低10倍。因此，加权衰减最终会传递到加法和W
    3优化器组中。
- en: Now what else is going on here in this function？ So the two things that are
    happening here that are important is that I'm splitting up the parameters into
    those that should be weight decay and those that should not be weight decay。 So
    in particular it is common to not weight decay biases and any other sort of one
    dimensional tensors。
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在这个函数中还有什么其他的事情发生？这里发生的两个重要事情是我将参数分为应该进行加权衰减和不应该进行加权衰减的参数。特别是，通常不对偏置和其他类型的一维张量进行加权衰减。
- en: So the one dimensional tensors are in the node decay parameters and these are
    also things like layer norm。 scales and biases。 It doesn't really make sense to
    weight decay those。 You mostly want to weight decay the weights that participate
    in matrix multiplications and you want to potentially weight decay the embeddings。
    And we've covered in previous video why it makes sense to decay the weights because
    you can sort of think of it as a regularization because when you're pulling down
    all the weights you're forcing the optimization。
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 一维张量在节点衰减参数中，这些还包括层归一化、尺度和偏置。对这些进行加权衰减没有太大意义。你主要希望对参与矩阵乘法的权重进行加权衰减，并且可能希望对嵌入进行加权衰减。在之前的视频中，我们已经讨论了为什么进行权重衰减是有意义的，因为你可以将其视为一种正则化，因为当你降低所有权重时，你迫使优化。
- en: to use more of the weights and you're not allowing any one of the weights individually
    to be way too large。 You're forcing the network to kind of distribute the word
    across more channels because there's sort of like a pool of gravity on the weights
    themselves。
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多的权重，而不允许任何一个权重单独变得过大。你迫使网络在多个通道之间分配权重，因为在权重本身上有一种引力的池。
- en: So that's why we are separating it in those ways here。 We're only decaying the
    embeddings and the matmole participating weights。 We're printing the number of
    parameters that we're decaying and not。 Most of the parameters will be decayed。
    And then one more thing that we're doing here is I'm doing another optimization
    here。
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在这些方面进行区分的原因。我们只对嵌入和参与权重进行衰减。我们打印出正在衰减和未衰减的参数数量。大多数参数将会衰减。然后我们在这里做的另一件事情是我正在进行另一个优化。
- en: And previous Adam W did not have this option but later parts of PyTorch introduced
    it。 And that's why I'm guarding it with an inspect that signature which is basically
    checking if this fused quad is present inside Adam W。 And then if it is present
    I'm going to end up using it and passing it in here。 Because some earlier versions
    do not have fused equals。 So here's Adam W fused equals。
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的Adam W没有这个选项，但PyTorch后来的版本引入了它。这就是我用一个检查来保证的原因，这个检查基本上是在确认Adam W中是否存在融合选项。如果存在，我会使用它并在这里传递它。因为一些早期版本没有fused
    equals。所以这里是Adam W的fused equals。
- en: It did not used to exist and it was added later。
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 它以前是不存在的，后来才添加的。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_171.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_171.png)'
- en: And there's some docs here for what's happening。 And basically they say that
    by default they do not use fused because it is relatively new and we want to give
    it sufficient peak time。 So by default they don't use fused but fused is a lot
    faster when it is available and when you're running on CUDA。
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些文档解释发生了什么。基本上，他们表示默认情况下不使用融合，因为它相对较新，我们希望给它足够的高峰时间。因此默认情况下不使用融合，但在可用时融合会快得多，尤其是在CUDA上运行时。
- en: And what that does is instead of iterating in a for loop over all the parameter
    tensors and updating them。 that would launch a lot of kernels。 And so fused just
    means that it's it。 All those kernels are fused into a single kernel。 You get
    rid of a lot of overhead and you a single time on all the primers call a kernel
    that updates them。 And so it's just basically a kernel fusion for the Adam W update
    instead of iterating over all the tensors。
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这段内容的作用是替代在循环中迭代所有参数张量并更新它们。这样会启动很多内核。所以融合的意思是，它将所有这些内核融合成一个单一的内核。你消除了很多开销，只需一次调用内核来更新所有的参数。因此，这基本上是对Adam
    W更新的内核融合，而不是对所有张量进行迭代。
- en: So that's the configure optimizers function that I like to use。 And we can rerun。
    And we're not going to see any major differences from what we saw before but we
    are going to see some prints coming from here。 So let's just take a look at what
    they look like。 So we see that number of decay tensors is 50 and it's most of
    the primers。 A number of non-decade tensors is 98 and these are the biases in
    the layer norm parameters mostly。
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我喜欢使用的配置优化器函数。我们可以重新运行。我们不会看到与之前的显著差异，但我们会看到一些来自这里的打印信息。让我们看看它们的样子。我们看到衰减张量的数量为50，且大部分是参数。非衰减张量的数量为98，主要是层归一化参数中的偏差。
- en: And that's there's only 100，000 of those。 So most of it is decayed。 And we are
    using the fused implementation of Adam W which will be a lot faster。 So if you
    have it available I would advise you to use it。 I'm not actually 100% sure why
    they don't default to it。 It seems fairly benign and harmless。
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数总共有100,000个，所以大部分已经衰减。我们正在使用Adam W的融合实现，这会更快。因此，如果你可以使用它，我建议你使用。我并不完全确定为什么它们不默认为此，似乎相对无害。
- en: And also because we are using the fused implementation I think this is why we
    have dropped。 Notice that the running time is to be 93 milliseconds per step and
    we're now down to 90 milliseconds per step because of using the fused Adam W optimizer。
    So in a single commit here we are introducing fused Adam getting improvements
    on the time and we're adding or changing the weight decay。 But we're only weight
    decaying the two-dimensional parameters。
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 而且因为我们使用了融合实现，我认为这就是我们减少运行时间的原因。注意到每步的运行时间为93毫秒，现在因为使用融合的Adam W优化器降低到了90毫秒。因此，在这里我们引入融合Adam，以提高时间效率，并添加或更改权重衰减。但我们仅对二维参数进行权重衰减。
- en: the embeddings and the matrices that participated in linear。 So that is this
    and we can take this out and yeah that is it for this line。 One more quick note
    before we continue here。 I just want to point out that the relationship between
    weight decay。 learning rate， batch size， the Adam parameters， beta 1， beta 2，
    the epsilon and so on。
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入和参与线性运算的矩阵。这就是这一点，我们可以将其拿出来，是的，这就是这一行的内容。在继续之前，我想快速指出权重衰减、学习率、批量大小、Adam参数、beta
    1、beta 2、epsilon等之间的关系。
- en: These are very complicated mathematical relationships in the optimization literature。
    And for the most part in this video I'm just trying to copy paste the settings
    that OpenAI used。 But this is a complicated topic quite deep and yeah in this
    video I just want to copy the parameters because it's a whole different video
    to really talk about that in detail and give it a proper justice instead of just
    high level intuitions。 Now the next thing that I want to move on to is that this
    paragraph here by the way we're going to turn back around to when we improve our
    data load。
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在优化文献中非常复杂的数学关系。在本视频中，我只是尝试复制OpenAI使用的设置。但这是一个复杂的主题，深入且复杂，因此在本视频中我只是想复制这些参数，因为详细讨论这些内容是完全不同的视频，而不是仅仅提供高层次的直觉。接下来我想讨论的是这段文字，顺便提一下，当我们改进数据加载时，我们将回到这里。
- en: For now I want to swing back around to this table where you will notice that
    for different models we of course have different parameters for the transformer
    that dictate the size of the transformer network。 We also have a different learning
    rate so we're seeing the pattern that the bigger networks are trained by slightly
    lower learning rates。
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我想回到这个表格，您会注意到对于不同的模型，我们当然有不同的变换器参数，决定变换器网络的大小。我们也有不同的学习率，因此我们看到更大网络的训练使用稍低的学习率。
- en: And we also see this batch size where in a small network they use a smaller
    batch size and in the bigger networks they use a bigger batch size。 Now the problem
    with for us is we can't just use 0。5 million batch size because if I just try
    to come in here and I try to set this B， where is my B？
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到这个批量大小，在小型网络中使用较小的批量大小，而在大型网络中使用较大的批量大小。现在对我们来说的问题是，我们不能仅仅使用0.5百万的批量大小，因为如果我试图进入这里，设置这个B，B在哪里？
- en: B equals where do I call it？ Okay B equals 16。 If I try to set。 well we have
    to be careful it's not 0。5 million because this is the batch size in the number
    of tokens。 Every single one of our rows is 1024 tokens so 0。5 e6 1 million divide
    1024。 This would need about a 488 batch size so the problem is I can't come in
    here and set this to 488 because my GPU would explode。
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: B等于我该如何称之？好的，B等于16。如果我尝试设置。我们必须小心，不能是0.5百万，因为这是以标记数为单位的批量大小。我们的每一行是1024个标记，所以0.5
    e6 1百万除以1024。这大约需要488的批量大小，所以问题是我不能将其设置为488，因为我的GPU会崩溃。
- en: This would not fit for sure。 But we still want to use this batch size because
    again as I mentioned the batch size is correlated with all the other optimization
    high parameters and the learning rates and so on。 So we want to have a faithful
    representation of all the hyperparameters and therefore we need to use a batch
    size of 0。
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这肯定无法适应。但是我们仍然想使用这个批量大小，因为正如我提到的，批量大小与所有其他优化高参数和学习率等相关。因此，我们希望准确表示所有超参数，因此需要使用0的批量大小。
- en: 5 million roughly。 But the question is how do we use 0。5 million if we only
    have a small GPU？
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 大约5百万。但是问题是，如果我们只有一个小GPU，如何使用0.5百万？
- en: Well for that we need to use what's called gradient accumulation so we're going
    to turn to that next and it allows us to simulate in a serial way any arbitrary
    batch size that we set。 And so we can do a batch size of 0。5 million which is
    have to run longer and we have to process multiple sequences and basically add
    up all the gradients from them to simulate a batch size of 0。
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要使用所谓的梯度累积，因此我们将转向这一点，它允许我们以串行方式模拟任何任意的批量大小。因此，我们可以使用0.5百万的批量大小，这需要运行更长时间，我们必须处理多个序列，并基本上将它们的所有梯度相加以模拟0的批量大小。
- en: 5 million。 So let's turn to that next。 Okay so I started the implementation
    right here just by adding these lines of code。 And basically what I did is first
    I set the total batch size that we desire。 So this is exactly 0。5 million and
    I used a nice number of power of two because two to the 19 is 52428。 So it's roughly
    0。5 million in some nice number。 Now our micro batch size as we call it now is
    16。
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 500万。所以让我们接着这个话题。好的，我在这里开始实现，正是通过添加这些代码行。基本上我做的是首先设置我们期望的总批量大小。因此，这正好是0.5百万，我使用了一个好的二的幂的数字，因为2的19次方是52428。所以大约是0.5百万的一个漂亮数字。现在我们的微批量大小，如我们所称的，现在是16。
- en: So this is going to be we still have B by T in the cease that go into the transformer
    and do forward backward but we're not going to do an update right we're going
    to do many forward backwards。 We're going to end those gradients are all going
    to plus equals on the parameter gradients they're all going to add up。
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将是，我们仍然有B乘以T在输入到变换器中进行前向反向传播，但我们不会进行更新，对吧？我们将进行多次前向和反向传播。所有这些梯度都会在参数梯度上进行累加。
- en: So we're going to do forward backward grad acoom steps number of times and then
    we're going to do a single update once all that is accumulated。 So in particular
    our micro batch size is just now controlling how many tokens how many rows we're
    processing in a single go over forward backward。
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将进行前向反向传播的梯度累积步骤次数，然后在所有累积完成后进行一次更新。因此，特别是我们的微批量大小现在只是控制我们在一次前向反向传播中处理多少tokens和多少行。
- en: So here we are doing 16 times one 24。 We're doing 16。 384 tokens per forward
    backward and we are supposed to be doing two to the 19。 Oops。 am I doing two to
    the 19 in total。 So the grad acoom will be 32。 So therefore grad acoom here will
    work out to 32 and we have to do 32 forward backward and then a single update。
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在进行16乘以24的计算。我们正在处理16。每次前向和反向传播有384个tokens，而我们应该处理2的19次方。哎呀，我总共是在处理2的19次方吗？所以梯度累积的次数将是32。因此，这里的梯度累积将计算为32，我们需要进行32次前向反向传播，然后进行一次更新。
- en: Now we see that we have about 100 milliseconds for a single forward backward。
    So doing 32 of them will be will make every step roughly three seconds just napkin
    math。 So that's grad acoom steps but now we actually like to implement that。 So
    we're going to swing over to our training loop because now this part here and
    this part here the forward and backward。
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到单次前向反向传播大约需要100毫秒。因此，进行32次将使每个步骤大约需要三秒钟，这只是个粗略估算。所以这是梯度累积步骤，但现在我们确实想要实现这一点。我们将转到我们的训练循环，因为现在这部分和这部分的前向和反向传播。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_173.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_173.png)'
- en: We have to now repeat this 32 times before we do everything else that follows。
    So let's see how we can implement that。 So let's come over here and actually we
    do have to load a new batch every single time。 So let me move that over here and
    now this is where we have the inner loop。 So for micro step in range grad acoom
    steps we do this and remember that loss of backward always deposits gradients。
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要在进行接下来的所有操作之前重复这一过程32次。让我们看看如何实现这个。我们每次确实需要加载一个新的batch。让我把这个移过来，现在这是我们进行内部循环的地方。所以对于微步（micro
    step）在梯度累积步骤范围内，我们进行这个操作，并记住反向传播的损失总是会累积梯度。
- en: So we're doing inside loss of backward。 There's always a plus equals on the
    gradients。 So in every single loss of backward gradients will add up on the grade
    in testers。 So we lost a backward and then we get all the gradients over there
    and then we normalize and everything else should just follow。 So we're very close
    but actually there's like subtle and deep issue here and this is actually incorrect。
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在反向传播的损失中。梯度总是会进行累加。因此，在每一次反向传播的损失中，梯度会在梯度累积器上相加。所以我们进行反向传播，然后获取所有的梯度，然后进行归一化，其他一切都应该跟着进行。所以我们非常接近，但实际上这里存在一个微妙而深刻的问题，这实际上是错误的。
- en: So I invite you to think about why this is not yet sufficient and let me fix
    it then。 Okay so I brought back the Jupyter notebook so we can think about this
    carefully in a simple toy setting and see what's happening。 So let's create a
    very simple neural nut that takes a 16 vector of 16 numbers and returns a single
    number。 And then here I'm creating some random examples X and some targets Y and
    then we are using the mean squared loss here to calculate the loss。
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我邀请你思考一下为什么这还不足够，然后让我来解决这个问题。好的，我带回了Jupyter笔记本，这样我们可以在一个简单的玩具环境中仔细考虑这个问题，看看发生了什么。让我们创建一个非常简单的神经网络，它接受一个16个数字的16维向量，并返回一个单一的数字。然后在这里，我创建一些随机示例X和一些目标Y，然后我们使用均方损失来计算损失。
- en: So basically what this is is four individual examples and we're just doing simple
    regression with the mean squared loss over those four examples。 Now when we calculate
    the loss and we lost that backward and look at the gradient this is the gradient
    that we achieve。
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上这就是四个独立的示例，我们仅对这四个示例执行简单的回归，使用均方损失。现在当我们计算损失并对其反向执行并查看梯度时，这就是我们获得的梯度。
- en: Now the loss objective here notice that an MSC loss the default for the loss
    function is reduction is mean。 So we're calculating the average mean loss the
    mean loss here over the four examples。 So this is the exact loss objective and
    this is the average the one over four because there are four independent examples
    here。 And then we have the four examples and their mean squared air the squared
    air and then this makes it the mean squared air。
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 现在损失目标注意到，MSC损失的默认设置是均值作为损失函数的减少方式。所以我们在计算这四个示例的平均损失。这样就是确切的损失目标，这里是四个示例的平均值，因为这里有四个独立的示例。然后我们有这四个示例及其均方误差，平方误差，这样就得到了均方误差。
- en: So therefore we are we calculate the squared air and then we normalize it to
    make it the mean over the examples and there's four examples here。 So now when
    we come to the gradient accumulation version of it this this here is the gradient
    accumulation version of it where we have grad active steps of four and I reset
    the gradient。
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们计算平方误差，然后将其标准化以使其成为示例的均值，这里有四个示例。所以现在当我们进入梯度累积版本时，这里是它的梯度累积版本，我们有四个活动步骤，并且我重置了梯度。
- en: We've got as come steps of four and now I'm evaluating all the examples individually
    instead and calling the loss backward on them many times。 And then we're looking
    at the gradient that we achieve from that。 So basically now we forward our function，
    calculate the exact same loss。 do it backward and we do that four times。 And when
    we look at the gradient you'll notice that the gradients don't match。
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有四个步骤，现在我在单独评估所有示例，并多次对它们进行反向调用损失。然后我们查看由此获得的梯度。因此基本上现在我们前向计算我们的函数，计算完全相同的损失。反向执行，并且我们这样做了四次。当我们查看梯度时，你会注意到梯度不匹配。
- en: So here we did a single batch of four and here we did four gradient accumulation
    steps of batch size one and the gradients are not the same。 And basically the
    reason that they're not the same is exactly because this mean squared error gets
    lost this one quarter and this loss gets lost because what happens here is the
    loss objective for every one of the loops is just a mean squared error。
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们做了一个四个的单一批次，而这里我们进行了四个批大小为一的梯度累积步骤，梯度并不相同。基本上，它们不相同的原因正是因为这个均方误差在这四分之一中丢失了，而这个损失也丢失了，因为在这里每个循环的损失目标仅是均方误差。
- en: And then one which in this case because there's a single example is just this
    term here。 So that was the loss in the zero iteration， same in the first third
    and so on。 And then when you do the loss that backward we're accumulating gradients。
    And what happens is that accumulation in the gradient is basically equivalent
    to doing a sum in the loss。
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这种情况下，由于只有一个示例，所以就只有这一项。因此这是零次迭代的损失，在第一次、第三次等中都是一样的。然后当你进行反向损失时，我们在积累梯度。而积累的梯度基本上等同于损失的求和。
- en: So our loss actually here is this without the factor of one quarter outside
    of it。 So we're missing the normalizer and therefore our gradients are all。 And
    so the way to fix this or one of them is basically we can actually come here and
    we can say loss equals loss divide four。 And what happens now is that we're introducing
    we're scaling our loss we're introducing one quarter in front of all of these
    places。
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们这里的损失实际上是没有外部四分之一因子的。这意味着我们缺少标准化因子，因此我们的梯度都是。所以解决这个问题的方法，或者其中一种方法是我们可以来到这里并说损失等于损失除以四。现在发生的事情是我们在缩放我们的损失，我们在所有这些地方前面引入了四分之一。
- en: So all the individual losses are not scaled by one quarter。 And then when we
    backward all of these accumulate with a sum but now there's a one quarter inside
    every one of these components。 And now our losses will be equivalent。 So when
    I run this you see that the gradients are now identical。 So long story short with
    this simple example when you step through it you can see that basically the reason
    that this is not correct is because in the same way as here in the MSC loss。
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 所以所有的单个损失没有按四分之一缩放。然后当我们反向传播这些损失时，它们会累积成一个总和，但现在每个组件内部都有一个四分之一。因此我们的损失将是等价的。当我运行这个时，你会看到梯度现在是相同的。长话短说，通过这个简单的例子，当你逐步查看时，你可以看到基本上这不正确的原因是因为与MSC损失在这里的情况相同。
- en: The loss that we're calculating here in the model is using a reduction of mean
    as well。 So where's the loss？ So we have that cross entropy and by default the
    reduction here in cross entropy is also I don't know why they don't show it but
    it's the mean the mean loss at all the B by T elements。
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在模型中计算的损失也使用均值归约。那么，损失在哪里呢？所以我们有交叉熵，默认情况下交叉熵的归约我不知道为什么他们不显示，但它是所有B和T元素的均值损失。
- en: So there's a reduction by mean in there and if we're just doing this gradient
    accumulation here we're missing that。 And so the way to fix this is to simply
    compensate for the number of gradient accumulation steps and we can in the same
    way divide this loss。
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 因此这里有一个通过均值的归约，如果我们只是进行梯度累积，我们就会遗漏这一点。因此，解决这个问题的方法是简单地补偿梯度累积步骤的数量，我们可以同样地将这个损失除以它。
- en: So in particular here the number of steps that we're doing is loss equals loss
    divided by the gradient accumulation steps。 So even a copilot gets the modification。
    But in the same way exactly we are scaling down the loss so that when we do loss
    that backward which basically corresponds to a sum in the objective。
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是这里我们进行的步骤数量是损失等于损失除以梯度累积步骤。因此即使是助手也会得到这个修改。但是同样，我们确实在缩小损失，以便当我们进行反向传播时，这基本上对应于目标中的一个总和。
- en: We are summing up the already normalized loss and therefore when we sum up the
    losses divided by gradients steps we are recovering the additional normalizer。
    And so now these two will be now this will be equivalent to the original sort
    of optimization because the gradient will come out the same。
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在汇总已经标准化的损失，因此当我们将损失除以梯度步骤时，我们恢复了额外的标准化因子。因此现在这两个将等价于原来的优化，因为梯度的结果是相同的。
- en: Okay so I had to do a few more touch ups and I launched the optimization here。
    So in particular one thing we want to do because we want to print things nicely
    is well first of all we need to create like an accumulator over the loss。 We can't
    just print the loss because we'd be printing only the final loss at the final
    micro step。 So instead we have a loss of whom which I initialized at zero and
    then I accumulate a loss into it。
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我需要再进行几次调整，并且我在这里启动了优化。特别是我们想做的一件事是因为我们想要优雅地打印内容，所以首先我们需要创建一个损失的累加器。我们不能只打印损失，因为我们只会在最后一个微步打印最终的损失。因此我们有一个初始为零的损失，然后我将损失累加到它上面。
- en: And I'm using detached so that I'm detaching the tensor from the graph and I'm
    just trying to keep track of the values。 So I'm making these leaf nodes when I
    add them。 So that's loss of whom and then we're printing that here instead of
    loss。 And then in addition to that I have to account for the grad， and so this
    looks pretty good。 Now if you'd like to verify that your optimization and the
    implementation here is correct and you're working on the side。
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用`detached`，这样我就可以将张量从图中分离出来，并且我只是试图跟踪这些值。所以当我添加它们时，我在创建这些叶节点。这就是损失的来源，我们在这里打印，而不是打印损失。此外，我还必须考虑梯度，因此这看起来相当不错。如果你想验证你的优化和这里的实现是正确的，而你在旁边工作。
- en: Well now because we have the total back size and the gradient accumulation steps
    our setting of B is purely a performance optimization kind of setting。 So if you
    have a big GPU you can actually increase this to 32 and you'll probably go a bit
    faster。 If you have a very small GPU you can try eight or four but in any case
    you should be getting the exact same optimization and the same answers up to what
    a floating point error because the gradient accumulation kicks in and can handle
    everything serially as necessary。
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 现在由于我们有总的后备大小和梯度累积步骤，我们的 B 设置纯粹是性能优化的设置。所以如果你有一个大 GPU，你实际上可以将这个值增加到 32，这样可能会快一点。如果你有一个非常小的
    GPU，你可以尝试八或四，但无论如何，你都应该获得完全相同的优化和相同的答案，直到浮点误差，因为梯度累积启动并可以按需处理所有内容。
- en: So that's it for gradient accumulation I think。 Okay so now is the time to bring
    out the heavy weapons。 You've noticed that so far we've only been using a single
    GPU for training but actually I am paying for AGPs here。 And so we should be putting
    all of them to work。 In particular they're all going to collaborate and optimize
    over tokens at the same time and communicate so that they're all kind of collaborating
    on the optimization。 For this we are going to be using the distributed data parallel
    from PyTorch。
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 所以关于梯度累积就这些。我想可以了。好了，现在是时候拿出重武器了。你可能注意到，到目前为止，我们只使用了一个 GPU 进行训练，但实际上我在这里支付了
    AGPs。所以我们应该让它们全部投入工作。特别是它们将同时协作并优化标记，并进行通信，以便它们都在优化上进行协作。为此，我们将使用 PyTorch 的分布式数据并行。
- en: There's also a legacy data parallel which I recommend you not use and that's
    kind of like legacy。 The distributed data parallel works in a very simple way。
    We have AGPs so we're going to launch eight processes and each process is going
    to be assigned a GPU。 And for each process the training loop and everything we've
    worked on so far is going to look pretty much the same。
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个遗留的数据并行，我建议你不要使用，那种有点像遗留技术。分布式数据并行以非常简单的方式工作。我们有 AGPs，因此我们将启动八个进程，每个进程将被分配一个
    GPU。对于每个进程，训练循环和我们到目前为止工作的所有内容看起来几乎都是相同的。
- en: Each GPU as far as it's concerned is just working on exactly what we've built
    so far。 But now secretly there's eight of them and they're all going to be processing
    slightly different parts of the data。 And we're going to add one more part where
    once they all calculate their gradients there's one more part where we do an average
    of those gradients。 And so that's how they're going to be collaborating on the
    computational workload here。
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 GPU 只是在处理我们到目前为止构建的内容。但现在秘密地有八个，它们都会处理数据的略微不同部分。我们将再添加一个部分，一旦它们都计算出梯度，还有一个部分是我们对这些梯度进行平均。因此，这就是它们在这里协作处理计算工作负载的方式。
- en: So to use all eight of them we're not going to be launching our script anymore
    with just PyTorch train GPT2。py。
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了使用所有八个，我们不再仅仅用 PyTorch train GPT2.py 启动我们的脚本。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_175.png)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_175.png)'
- en: We're going to be running it with a special command called Torchrun in PyTorch。
    We'll see them in a bit。 And Torchrun when it runs our Python script will actually
    make sure to run eight of them in parallel。 And it creates these environmental
    variables where each of these processes can look up which basically which one
    of the processes it is。 So for example Torchrun will set rank， local rank， in
    world size environmental variables。
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个名为 Torchrun 的特殊命令在 PyTorch 中运行它。稍后我们会看到它。Torchrun 在运行我们的 Python 脚本时，实际上会确保并行运行八个进程。它创建这些环境变量，使每个进程都可以查找基本上是哪个进程。因此，例如，Torchrun
    将设置排名、局部排名和世界大小的环境变量。
- en: This is a bad way to detect whether DDP is running。 So if we're using Torchrun。
    if DDP is running then we have to make sure that good is available because I don't
    know that you can run this on CPU anymore or that that makes sense to do。 This
    is some setup code here。 The important part is that there's a world size which
    for us will be eight。 That's the total number of processes running。 There's a
    rank which is each process will basically run the exact same code at the exact
    same time roughly。
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种不好的检测 DDP 是否正在运行的方法。所以如果我们使用 Torchrun。如果 DDP 正在运行，那么我们必须确保 good 是可用的，因为我不知道你是否还能在
    CPU 上运行这个，或者这样做是否有意义。这是一些设置代码。重要的部分是有一个世界大小，对我们来说是八。这是正在运行的进程总数。还有一个排名，每个进程基本上会在大致相同的时间运行完全相同的代码。
- en: But all the process， the only difference between these processes is that they
    all have a different DDP rank。 So the GPU zero will have DDP rank of zero， GPU
    one will have rank of one， etc。 So otherwise they're all running the exact same
    script。 It's just that DDP rank will be a slightly different integer and that
    is the way for us to coordinate that they don't for example run on the same data。
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 但是所有进程之间的唯一区别是它们都有不同的DDP等级。因此，GPU零的DDP等级为零，GPU一的等级为一，依此类推。否则，它们都在运行完全相同的脚本。只是在DDP等级上会有一个略微不同的整数，这样我们才能协调它们不在同一数据上运行。
- en: We want them to run on different parts of the data and so on。 Now local rank
    is something that is only used in a multi node setting。 We only have a single
    node with a GPU and so local rank is the rank of the GPU on a single node。 So
    from zero to seven as an example。 But for us we're mostly going to be running
    a single box so the things we care about are rank and world size。
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望它们在不同的数据部分上运行等。现在，local rank仅在多节点设置中使用。我们只有一个带GPU的单节点，因此local rank是单节点上GPU的等级。例如，从零到七。但对我们来说，我们主要会在单个节点上运行，所以我们关心的是等级和世界大小。
- en: This is eight and this will be whatever it is depending on the GPU that this
    particular instantiation of the script runs on。 Now here we make sure that according
    to the local rank we are setting the device to be could a column and column indicates
    which GPU to use if there are more than one GPUs。
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是八个，而这将取决于这个特定脚本实例运行的GPU。现在，在这里我们确保根据local rank设置设备为CUDA列，列指示如果有多个GPU时使用哪个GPU。
- en: So depending on the local rank of this process it's going to use just the appropriate
    GPU so there's no collisions on which GPU is being used by which process。 And
    finally there's a Boolean variable that I like to create which is the DDP rank
    equals equals zero。
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 所以根据这个进程的local rank，它将使用适当的GPU，以避免哪些GPU被哪个进程使用的冲突。最后，我喜欢创建一个布尔变量，即DDP等级等于零。
- en: So the master process is arbitrarily process number zero and it does a lot of
    the printing logging checkpointing etc。 And the other processes are thought of
    mostly as a compute processes that are assisting。 And so master process zero will
    have some additional work to do。 All the other processes will will mostly just
    be doing forward backwards。
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程是任意的进程编号零，它负责大部分打印、日志记录、检查点等。其他进程主要被视为辅助计算进程。因此，主进程零会有一些额外的工作要做。所有其他进程主要只做前向和后向传播。
- en: And if we're not using DDP and none of these variables are set we revert back
    to single GPU training。 So that means that we only have rank zero the world size
    is just one and we are the master process and we try to auto detect the device
    and this is world as normal。
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用DDP，并且这些变量都未设置，我们将退回到单GPU训练。因此，这意味着我们只有rank零，世界大小仅为一，我们是主进程，我们尝试自动检测设备，这一切都是正常的。
- en: So so far all we've done is we've initialized DDP and in the case where we're
    running with Torchron which we'll see in a bit。 There's going to be eight copies
    running in parallel each one of them will have a different rank。 And now we have
    to make sure that everything happens correctly afterwards。 So the tricky thing
    with running multiple processes is you always have to imagine that there's going
    to be eight processes running in parallel。
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的就是初始化DDP，在我们稍后看到的与Torchrun一起运行的情况下，将会有八个副本并行运行，每个副本都有不同的等级。现在我们必须确保之后的一切都正确进行。运行多个进程的棘手之处在于，你总是要想象将有八个进程并行运行。
- en: So as you read the code now you have to imagine there's eight Python interpreters
    running down these lines of code。 And the only difference between them is that
    they have a different DDP rank。 So they all come here they all pick the exact
    same seed。 They all make all of these calculations completely unaware of the other
    copies running roughly speaking。
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当你阅读代码时，你需要想象有八个Python解释器在这些代码行中运行。它们之间唯一的区别是它们有不同的DDP等级。它们都在这里，选择完全相同的种子。它们进行所有这些计算时，完全不知道其他副本的存在。
- en: So they all make the exact same calculations and now we have to adjust these
    calculations to take into account that there's actually like a certain world size
    and certain ranks。 So in particular these micro batches and sequence links these
    are all just per GPU。
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们都进行相同的计算，现在我们必须调整这些计算，以考虑到实际上有一个特定的世界大小和某些等级。因此，特别是这些微批次和序列链接，这些都是针对每个GPU的。
- en: So now there's going to be non processes of them running in parallel。 So we
    have to adjust this right because the gradicum steps now is going to be total
    by size divided by times T times DDP role size。 Because each process will do B
    times T and there's this many of them。 And so in addition to that we want to make
    sure that this fits nicely into total batch size。
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将会有非进程并行运行。所以我们必须调整这个，因为梯度步长现在将是总大小除以时间T乘以DDP角色大小。因为每个进程将执行B乘以T，而它们的数量正是这样。因此，除此之外，我们还希望确保这很好地适配到总批量大小中。
- en: which for us it will be because 16 times 124 times 8 GPUs。 Is 131 K and so 524288。
    This means that our gradicum will be for with the current settings。 Right。 So
    there's going to be 16 times 124 process in each GPU and then there's a GPU so
    we're going to be doing 131。000 tokens in a single forward backward on the HPUs。
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，情况是这样的，因为16乘以124乘以8 GPU。是131 K和524288。这意味着我们的梯度将为当前设置。对。因此，每个GPU中将会有16乘以124个进程，然后有一个GPU，所以我们将会在HPUs上进行131,000个标记的单次前向反向计算。
- en: So we want to make sure that this fits nicely so that we can derive a nice grading
    accumulation steps。 And yeah let's just adjust the comments here。 TDP role size。
    Okay。 So each GPU calculates this。 Now this is where we start to get around to
    issues right so we are each process is going to come by a print and they're all
    going to print。 So we're going to have eight copies of these prints。
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们希望确保这适配得很好，以便我们可以得出一个良好的梯度累积步骤。是的，让我们在这里调整一下注释。TDP角色大小。好的。所以每个GPU计算这个。现在这里就是我们开始遇到问题的地方，每个进程都会打印，它们都会打印。因此，我们将会有这八个打印的副本。
- en: So one way to deal with this is exactly this master process variable that we
    have so if master process then guard this。 And that's just so that we just print
    this a single time because otherwise all the processes when I've computed the
    exact same variables and there's no need to print this eight times。
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一种方法正是我们所拥有的这个主进程变量，因此如果是主进程，就要进行保护。这只是为了我们只打印一次，因为否则所有进程都计算了相同的变量，没有必要打印八次。
- en: Before getting into the data loader and we're going to have to refactor it obviously。
    Maybe at this point is we should do some prints and just take it out for a spin
    and exit at this point so import sys。 And sys。exit in print。 I am GPU DDP rank。
    I am GPU DDP rank and print by。 So now let's try to run this and just see how
    this works。
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入数据加载器之前，我们显然需要重构它。也许在这一点上我们应该做一些打印，简单地运行一下并在此退出，所以导入sys。然后在打印中使用sys.exit。我是GPU
    DDP等级。我是GPU DDP等级，并且按打印。所以现在让我们试着运行一下，看看这怎么运作。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_177.png)'
  id: totrans-582
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_177.png)'
- en: So let's take it for a spin just so we see what it looks like。 So normally we
    used to launch Python。 Change of D2。py like this。 Now we're going to run with
    tort run and this is what it looks like。 So tort run standalone number processes
    for example is eight for us because we have a GPU and then change of D2。py。 So
    this is what the command would look like and tort run again will run eight of
    these。
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们运行一下，看看它看起来是什么样的。因此，通常我们习惯于启动Python。像这样更改D2.py。现在我们将使用tort run运行，这就是它的样子。所以tort
    run独立进程数，例如对我们来说是八，因为我们有一个GPU，然后更改D2.py。所以这就是命令的样子，而tort run将再次运行八个这样的进程。
- en: So let's just see what happens。 So first it gets a little busy so there's a
    lot going on here。 So first of all there's some warnings from distributed and
    I don't actually know that these mean anything。 I think this is just like the
    code is setting up and the processes are coming online and we're seeing some preliminary
    failure to collect while the processes come up。 I'm not 100% sure about that。
    But we start to then get into actual prints。
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们来看看会发生什么。所以首先有点忙，所以这里发生了很多事情。首先，分布式系统有一些警告，我实际上不知道这些是否意味着什么。我认为这只是代码正在设置中，进程正在上线，我们看到在进程启动时初步收集失败。我不确定这一点。但我们开始看到实际的打印。
- en: So all the processes went down and then the first print actually comes from
    process five just by chance。 And then it printed so process five basically got
    here first。 It said I'm process on GPU five by and then this these prints come
    from the master process。 So process five just finished first for whatever reason
    it just depends on how the operating system scheduled the processes to run。
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 所有进程都停止了，然后第一个打印实际上来自进程五，纯粹是偶然。然后它打印了，所以进程五基本上最先到达这里。它说我是 GPU five 的进程，然后这些打印来自主进程。所以进程五无论出于什么原因首先完成，这完全取决于操作系统如何调度进程运行。
- en: Then GPU zero ended then GPU three and two。 And then probably process five or
    something like that has exited。 And DDP really doesn't like that because we didn't
    properly dispose of the multi GPUs setting。 And so process group has not been
    destroyed before we destruct。 So it really doesn't like that and in actual application
    we would want to call the process group。
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 GPU zero 结束，接着是 GPU three 和 two。然后可能是进程五或类似的东西退出了。DDP 实际上不喜欢这样，因为我们没有正确处理多
    GPU 设置。因此，进程组在我们销毁之前并没有被销毁。所以它对此不太满意，在实际应用中我们希望调用进程组。
- en: So that we clean up DDP properly。 And so it doesn't like that too much。 And
    then the last of the GPU is finished。 And that's it。 So basically we can't guarantee
    when these processes are running it's totally arbitrary but they are running in
    parallel。 We don't want that to be printing。 And next up let's erase this。
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以正确清理 DDP。因此，它对此也不太满意。然后最后一个 GPU 完成了。这就结束了。所以基本上我们无法保证这些进程何时运行，这完全是任意的，但它们是并行运行的。我们不希望这打印出来。接下来让我们把这擦掉。
- en: Next up we want to make sure that when we create data a little light we need
    to now make it aware of this a multi process setting。 Because we don't want all
    the processes to be loading the exact same data。 We want every process to get
    its own chunk of data so that they're all working on different parts of the data
    set of course。 So let's adjust that。 So one particularly simple and then the deep
    way to do this is we have to make sure that we pass in the rank and the size to
    the data loader。
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们希望确保在创建数据时，我们需要让它意识到多进程设置。因为我们不希望所有进程都加载完全相同的数据。我们希望每个进程获取自己的一部分数据，以便它们都在不同的数据集部分上工作。因此，让我们进行调整。实现这一点的一种特别简单而又深刻的方法是确保我们将
    rank 和 size 传递给数据加载器。
- en: And then we come up here we see that we now take rank and processes and we save
    them。 Now the current position will not be zero because what we want is we want
    to stride out all the processes。 So one way to do this is we basically take cell
    that B times cell that T and then multiply it by the process rank。 So process
    rank zero will start at zero。 But process rank one now starts at B times D。
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们来到这里，我们看到现在获取了 rank 和进程并保存它们。现在当前的位置不会是零，因为我们想要遍历所有进程。实现这一点的一种方法是基本上取 cell
    that B 乘以 cell that T，然后再乘以进程 rank。所以进程 rank zero 将从零开始，但进程 rank one 现在从 B 乘以
    D 开始。
- en: Process rank two is starts at two times B times D。 So that is the initialization。
    Now we still do this identically but now when we advance we don't advance by B
    times T。 We advance by B times T times number of processes。 So basically the total
    number of tokens that we're consuming is B times T times number processes and
    they all go off to a different rank。
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 进程 rank two 从两个 B 乘以 D 开始。这就是初始化。现在我们仍然以相同的方式进行，但是现在当我们推进时，我们不再按 B 乘以 T 进行推进。我们按
    B 乘以 T 乘以进程数量进行推进。所以我们消耗的总令牌数是 B 乘以 T 乘以进程数量，它们都会发送到不同的 rank。
- en: And the position has to advance by the entire chunk。 And then here at B times
    T times cell that number of processes plus one would be to exceed number of tokens。
    Then we're going to loop。 And when we loop we want to of course loop in the exact
    same way。 So we sort of like reset back。 So this is the simplest change that I
    can find for kind of a very simple distributed data loader like。
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 位置必须按整个块推进。然后在 B 乘以 T 乘以 cell that number of processes 加一处将超过令牌数量。然后我们将循环。当我们循环时，当然希望以完全相同的方式循环。所以我们有点像是重置回去。这是我能找到的最简单的更改，适用于一种非常简单的分布式数据加载器。
- en: And you can notice that if process rank is zero and then processes is one then
    the whole thing will be identical to what we had before。 But now we can have actually
    multiple processes running and this should work fine。 So that's the data loader。
    Okay， so next up once they've all initialized the data loader。 They come here
    and they all create a GPT model。 So we create eight GPT models on eight processes。
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到如果进程等级是零，然后进程是一个，那么整个过程将与我们之前的一模一样。但现在我们可以实际运行多个进程，这应该运行良好。这就是数据加载器。好的，接下来一旦它们都初始化了数据加载器。它们就会来这里，并且都创建一个GPT模型。因此我们在八个进程上创建了八个GPT模型。
- en: But because the seeds are fixed here they all create the same identical model。
    They all move it to the device of their rank and they all compile the model。 And
    because the models are identical there are eight identical compilations happening
    in parallel but that's okay。 Now none of this changes because that is on a per
    step basis and we're currently working kind of within step because we need to
    just all the changes we're making are kind of like a within step changes。
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 但是因为这里的种子是固定的，它们都创建了相同的模型。它们都将模型移动到各自的设备上，并且都编译模型。因为模型是相同的，所以会并行发生八次相同的编译，但这没关系。现在这一切都不会改变，因为这是逐步进行的，我们目前在步骤内工作，因为我们所做的所有更改都像是步骤内的更改。
- en: Now the important thing here is when we construct the model we actually have
    a bit of work to do here。 Get logits is deprecated so create model。 We need to
    actually wrap the model into the distributed data parallel container。 So this
    is how we wrap the model into the DDP container。 And these are the docs for DDP
    and they're quite extensive。
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这里重要的是，当我们构建模型时，我们实际上需要做一些工作。获取logits已经被弃用，因此需要创建模型。我们实际上需要将模型包装到分布式数据并行容器中。这就是我们如何将模型包装到DDP容器中。这些是DDP的文档，而且相当详细。
- en: And there's a lot of caveats and a lot of things to be careful with because
    everything complexifies times 10 when multiple processes are involved。 But roughly
    speaking this device ID I believe has to be passed in。 Now unfortunately the docs
    for what device ID is is extremely unclear。 So when you actually like come here。
    This comment for what device ID is is roughly nonsensical。
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多注意事项和需要小心的地方，因为一旦涉及多个进程，一切都会复杂化十倍。但大致来说，这个设备ID我认为必须传入。现在不幸的是，关于设备ID的文档是极其不明确的。因此，当你实际上来这里时，关于设备ID的注释大致上是无意义的。
- en: But I'm pretty sure it's supposed to be the DDP local rank so not the DDP rank
    the local rank。 So this is what you pass in here。 This wraps the model and in
    particular what DDP does for you is in a forward pass it actually behaves identically。
    So my understanding of it is nothing should be changed in the forward pass。 But
    in the backward pass as you are doing the backward pass in the simplest setting
    once the backward passes over on each independent GPU。
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 但我很确定这里应该是DDP的本地等级，而不是DDP等级，而是本地等级。所以这是你在这里传入的内容。这会包装模型，特别是DDP为你做的就是在前向传播中它实际上表现得一模一样。因此，我的理解是前向传播中不应该有任何更改。但在反向传播中，当你在最简单的设置中进行反向传播时，一旦每个独立GPU的反向传播结束。
- en: Each independent GPU has the gradient for all the primers。 And what DDP does
    for you is once the backward passes over it will call what's called all reduce。
    And it basically does an average across all the ranks of their gradients。 And
    then it will deposit that average on every single rank。
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 每个独立的GPU都有所有引导词的梯度。而DDP为你做的就是，一旦反向传播经过它，就会调用所谓的“全局归约”。它基本上是在所有梯度的等级上进行平均，然后将这个平均值存储到每个等级上。
- en: So every single single rank will end up with the average on it。 And so basically
    that's the communication it just synchronizes and averages the gradients and that's
    what DDP offers you。 Now DDP actually is a little bit more involved in that because
    as you are doing the backward pass through the layers in the transformer it actually
    can dispatch communications for the gradient while the backward passes still happening。
    So there's overlap of the communication of the gradients and the synchronization
    of them and the backward pass。
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 所以每个等级最终会得到一个平均值。因此，基本上这就是通信，它只是同步并平均梯度，这就是DDP为你提供的。现在DDP实际上稍微复杂一点，因为在你进行反向传播时，它实际上可以在反向传播仍在进行时调度梯度的通信。因此，梯度的通信与它们的同步以及反向传播之间有重叠。
- en: And this is just more efficient and to do it that way。 So that's what DDP does
    for you。 Forward is unchanged and backward is mostly unchanged and we're tacking
    on this average as we'll see in a bit。 Okay， so now let's go to the optimization。
    Nothing here changes。 Let's go to the optimization here the inner loop and think
    through the synchronization of these gradients in the DDP。
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做更有效率。所以这就是 DDP 为你做的。前向传播没有改变，反向传播大部分没有改变，而我们将添加这个平均值，稍后会看到。好的，现在让我们来看看优化。这里没有变化。让我们来看看优化的内部循环，并思考在
    DDP 中这些梯度的同步。
- en: So basically by default what happens as I mentioned is when you do lost a backward
    here it will do the backward pass and then it will synchronize the gradients。
    The problem here is because of the gradient accumulation steps loop here。 We don't
    actually want to do the synchronization after every single lost a backward because
    we are just depositing gradients and we're doing that serially。 And we just want
    them adding up and we don't want to synchronize every single time。
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上默认情况下，正如我提到的，当你在这里做反向传播时，它将进行反向传播，然后同步梯度。这里的问题是由于梯度累积步骤的循环。我们实际上不想在每一次反向传播后进行同步，因为我们只是逐步累积梯度。我们希望它们累加，不想每次都同步。
- en: That would be extremely wasteful。 So basically we want to add them up and then
    on the very last step when microstep becomes gradicum steps-1 only at that last
    step that we want to actually do the overdoase to average of gradients。 So to
    do that we come here and the official sanctioned way by the way is to do this
    no sync context manager。
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是极其浪费的。因此，基本上我们希望将它们相加，然后在最后一步当微步骤变为 gradicum steps-1 时，仅在最后一步我们希望实际上进行梯度平均的过度操作。为了做到这一点，我们来到这里，官方推荐的方式是使用这个不进行同步的上下文管理器。
- en: So PyTorch says this is a context manager to disable gradient synchronization
    across the DDP processes。 So within this context gradients will be accumulated
    and basically when you do no sync there will be no communication。 So they are
    telling us to do with DDP no sync do the gradient accumulation。 accumulate grads
    and then they are asking us to do DDP again with another input and that backward。
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 PyTorch 说这是一个上下文管理器，用于禁用 DDP 进程之间的梯度同步。在这个上下文中，梯度将被累积，基本上，当你不进行同步时将没有通信。因此，他们告诉我们在
    DDP 中不进行同步，进行梯度累积。累积梯度，然后他们要求我们再次使用另一个输入进行 DDP 并进行反向传播。
- en: And I just really don't love this。 I just really don't like it。 The fact that
    you have to copy paste your code here and use a context manager and it's just
    super ugly。 So when I went to this source code here you can see that when you
    enter you simply toggle this variable。 This required backward grad sync and this
    is being toggled around and changed。
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的不喜欢这样。我真的不喜欢。你必须在这里复制粘贴你的代码并使用上下文管理器，这太难看了。所以当我查看这个源代码时，你可以看到当你进入时，简单地切换这个变量。这个所需的反向梯度同步正在被切换和更改。
- en: And this is the variable that basically if you step through it is being toggled
    to determine if the gradient is going to be synchronized。
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个变量，基本上如果你逐步执行它，会被切换以确定梯度是否会被同步。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_179.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_179.png)'
- en: So I actually just kind of like to use that directly。 So instead what I like
    to do is the following。 Right here before the last backward if we are using DDP
    then then basically we only want to synchronize。 We only want this variable to
    be true when it is the final iteration and all the other iterations inside the
    microsteps we want to be false。 So I just toggle it like this。 So required backward
    grad sync should only turn on when the microstep is the last step。
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我实际上就是喜欢直接使用它。因此，我喜欢做的是以下操作。在最后的反向传播之前，如果我们正在使用 DDP，那么基本上我们只想同步。我们只希望这个变量在最后一次迭代时为真，而在微步骤内部的所有其他迭代中希望为假。所以我这样切换它。所需的反向梯度同步应仅在微步骤是最后一步时打开。
- en: And so I'm toggling this variable directly and I hope that that impacts lost
    backward。 And this is a naughty thing to do because they could probably change
    the DDP and this variable will go away。 But for now I believe this works and it
    allows me to avoid the use of context managers and code duplication。 I'm just
    toggling the variable and then lost backward will not synchronize most of the
    steps and it will synchronize the very last step。
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我直接切换这个变量，希望这会影响`lost.backward`。这是件麻烦事，因为他们可能会改变DDP，这个变量就会消失。但现在我相信这样做有效，并且可以避免使用上下文管理器和代码重复。我只是切换变量，然后`lost.backward`不会同步大多数步骤，而是同步最后一步。
- en: And so once this is over and we come out every single rank will suddenly magically
    have the average of all the gradients that are stored on all the ranks。 So now
    we have to think through whether that is what we want and also if this suffices
    and whether how it works with the loss and what is lossacoom。
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦结束，每个进程会突然神奇地拥有存储在所有进程上的所有梯度的平均值。现在我们必须考虑这是否是我们想要的，是否足够，以及它如何与损失和`lossacoom`一起工作。
- en: So let's think through them。 And the problem I'm getting at is that we've averaged
    the gradients which is great but the lossacoom has not been impacted yet。 And
    this is outside of the DDP container so that is not being averaged。 And so here
    when we are printing lossacoom well presumably we're only going to be printing
    on the master process rank zero。 And it's just going to be printing the losses
    that it saw on its process。
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下。我的问题是我们已经对梯度进行了平均，这很好，但`lossacoom`尚未受到影响。这在DDP容器外，所以没有被平均。因此在这里打印`lossacoom`时，显然我们只会在主进程（rank
    0）上打印。这只是打印它在该进程中看到的损失。
- en: And instead we wanted to print the loss over all the processes and the average
    of that loss because we did average of gradients so we want the average of loss
    as well。 So simply here after this this is the code that I've used in the past。
    And instead of lossac we want lossacoom。 So if DDP again then this is a PyTorch
    distributed I imported what do I imported。 Oh gosh。 So this file started to get
    out of control。
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要打印所有进程的损失和该损失的平均值，因为我们已经对梯度进行了平均，所以我们也想要损失的平均值。因此在这里，这段代码是我过去使用的。我们需要使用`lossacoom`而不是`lossac`。如果是DDP，那么这是一个PyTorch分布式，我导入了什么呢？哦天哪，这个文件开始变得难以控制。
- en: So if so import torch distributed as this so this dot all reduce。 And we're
    doing the average on lossacoom and so this lossacoom tensor exists on all the
    ranks when we call all reduce of average it creates the average of those numbers
    and it deposits that average on all the ranks。
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果这样导入`torch.distributed`，那么调用`all_reduce`。我们对`lossacoom`进行平均，因此这个`lossacoom`张量在所有进程中都存在，当我们调用`all_reduce`进行平均时，它会创建这些数字的平均值，并将该平均值存储在所有进程中。
- en: So all the ranks after this call will now contain lossacoom averaged up。 And
    so when we print here on the master process the lossacoom is identical in all
    the other ranks as well。 So here if master process we want to print like this。
    Okay and finally we have to be careful because we're not processing even more
    tokens so times DDP world size。
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在此调用后，所有进程现在将包含平均的`lossacoom`。当我们在主进程中打印时，`lossacoom`在所有其他进程中也是相同的。因此在主进程中，我们想要像这样打印。好吧，最后我们必须小心，因为我们还没有处理更多的标记，所以是DDP世界大小的倍数。
- en: That's number of tokens that we've processed up above。 And everything else should
    be fine。 The only other thing to be careful with is as I mentioned you want to
    destroy the process group so that we are nice to nickel and it's not going to
    to to DDP and it's not going to complain to us when we exit here。
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们处理的标记数量。其他一切应该没问题。唯一需要注意的是，如我提到的，你想要销毁进程组，以便我们对`nickel`友好，当我们退出时DDP不会抱怨。
- en: So that should be it。 Let's try to take it for a spin。 Okay so I launched the
    script and it should be printing here imminently。 We're now training with 8 GPUs
    at the same time。 So the gradient accumulation steps is not 32 it is now divide
    8 and it's just 4。
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该就是了。让我们试试吧。好吧，我启动了脚本，它应该会马上打印。我们现在使用8个GPU同时训练。因此梯度累积步骤不是32，而是8的除数，现在是4。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_181.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_181.png)'
- en: So otherwise this is what the optimization looks like。 And wow we're going really
    fast so we're processing 1。5 million tokens per second now。 So these are some
    serious numbers。 And the tiny Shakespeare data set is so tiny that we're just
    doing like so many epochs over it most likely。 But this is roughly what it looks
    like。 One thing that I had to fix by the way is that this was a model that configures
    optimizers which now doesn't work because model now is a DDP model。
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，优化的样子就是这样。哇，我们的速度真快，现在每秒处理150万个token。这是一些严肃的数字。而且小莎士比亚数据集是如此之小，以至于我们可能会进行很多轮次。但这大致就是它的样子。顺便说一下，我需要修复的一件事是，这是一个配置优化器的模型，现在不再有效，因为模型现在是DDP模型。
- en: So instead this has to become raw model that configures optimizers where raw
    model is something I create here。 So right after I wrapped a model into DDP I
    have to create the raw model which in the case of DDP is a model that module is
    where it stores the raw and a module of GPT2 as we have it。
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这必须成为配置优化器的原始模型，而原始模型是在这里创建的东西。因此，在我将模型包装成DDP之后，我必须创建原始模型，在DDP的情况下，它是存储原始模型的模块，以及我们所拥有的GPT2模块。
- en: And that which contains the configure optimizers function that we want to call。
    So that's one thing that I had to fix。 Otherwise this seems to run。 Now one thing
    you'll notice is that when you actually compare this run and the numbers in it
    to just running a single GPU。 You'll notice that this is single GPU run with 32
    radacum。 The numbers won't exactly match up。
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包含我们想要调用的配置优化器函数。因此，这是我需要修复的一件事。否则，这似乎运行正常。现在你会注意到，当你实际比较这个运行及其数字与单个GPU的运行时，你会发现这是一个32
    radacum的单个GPU运行。数字并不会完全匹配。
- en: And it's kind of a boring reason for why that happens。 The reason for that is
    that in the data loader we're basically just iterating through batches in a slightly
    different way。 Because now we're looking for an entire page of data。 And if that
    page for all the GPUs。 if that chunk exceeds the number of tokens we just loop。
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生的原因有些无聊。原因是，在数据加载器中，我们基本上是以略微不同的方式迭代批次。因为现在我们在寻找一整页数据。如果对于所有GPU来说，该块超出了token的数量，我们就会循环。
- en: And so actually the single GPU and the GPU process will end up resetting in
    a slightly different manner。 And so our batches are slightly different and so
    we get slightly different numbers。 But one way to convince yourself that this
    is okay。 It just makes the total batch size much smaller and the B and a T。
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，单个GPU和GPU处理将以略微不同的方式重置。因此，我们的批量略有不同，因此我们得到略微不同的数字。但说服自己这是可以的一个方法是，将总批量大小减小得更小，B和A
    T。
- en: And then so I think I used 4 times 1。24 times 8。 So I used 32。768 as a total
    batch size。 And then so I made sure that the single GPU will do 8 great even simulation
    steps and then the multi GPU。 And then you're reducing the boundary effects of
    the data loader and you'll see that the numbers match up。 So one story short，
    we're now going really really fast。
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我想我使用了4倍的1.24乘以8。所以我使用了32.768作为总批量大小。然后我确保单个GPU可以进行8个出色的仿真步骤，然后是多GPU。这样可以减少数据加载器的边界效应，你会看到数字是匹配的。简而言之，我们现在的速度非常快。
- en: The optimization is mostly consistent with GPT 2 and 3 hybrid parameters。 And
    we have outgrown our tiny Shakespeare file and we want to upgrade it。 So let's
    move to next to that next。 So let's now take a look at what datasets were used
    by GPT 2 and GPT 3。 So GPT 2 used this webtext dataset that was never released。
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 优化大致与GPT 2和3的混合参数一致。我们已经超越了我们的小莎士比亚文件，并希望对其进行升级。那么让我们接下来看看GPT 2和GPT 3使用了什么数据集。GPT
    2使用了这个从未发布的webtext数据集。
- en: There's an attempt at reproducing it called open webtext。 So basically roughly
    speaking what they say here in the paper is that this created all outbound links
    from Reddit。 And then with at least three karma。 And that was kind of like their
    starting point and they collected all the web pages and all the text in them。
    And so this was 45 million links and this ended up being 40 gigabytes of text。
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个试图重现它的项目叫做open webtext。简单来说，他们在论文中提到，这创建了来自Reddit的所有外部链接，并且至少有三次karma。这大致是他们的起点，他们收集了所有的网页及其文本。因此，这有4500万个链接，最终形成了40GB的文本。
- en: So that's roughly what GPT 2 says about its dataset。 So basically outbound links
    from Reddit。 Now when we go over to GPT 3 there's a training dataset section and
    that's where they start to talk about common crawl。 Which is a lot more used。
    Actually I think you can GPT 2 talk about common crawl。 But basically it's not
    a very high quality dataset all by itself because it's extremely noisy。
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 大致上，GPT-2关于其数据集的描述是这样的。基本上是来自Reddit的外部链接。现在当我们转到GPT-3时，有一个训练数据集的部分，这时他们开始讨论常见的网络爬虫数据。实际上，常见网络爬虫使用得更多。其实我认为GPT-2也谈到了常见的网络爬虫。但基本上，这个数据集本身并不是一个很高质量的数据集，因为它非常嘈杂。
- en: This is a completely random subset of the internet and it's much worse than
    you think。 So people go into great lengths to filter common crawl because there's
    good stuff in it。 But most of it is just like ad spam， random tables and numbers
    and stock tickers。 And it's just a total mess。 So that's why people like to train
    on these data mixtures that they curate and are careful with。
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 这是互联网的一个完全随机的子集，情况远比你想象的糟糕。因此，人们非常努力地过滤常见的网络爬虫数据，因为其中有好的内容。但大部分内容就像是广告垃圾、随机表格和数字以及股票行情。整个数据非常混乱。因此，人们喜欢在这些经过精心策划的数据混合物上进行训练。
- en: So a large chunk of these data mixtures typically will be common crawl。 Like
    for example 50% of the tokens will be common crawl。 But then here in GPT 3 they're
    also using web text to from before。 They're not using web text to get outbound
    but they're also adding for example books and they're anything Wikipedia。
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据混合物中的一大部分通常会是常见的网络爬虫数据。例如，50%的标记将来自常见的网络爬虫数据。但在GPT-3中，他们还使用了之前的网页文本。他们没有使用网页文本来获取外部链接，但他们还添加了例如书籍和维基百科的内容。
- en: There's many other things you can decide to add。 Now this dataset for GPT 3
    was also never released。 So today some of the datasets that I'm familiar with
    that are quite good and will be representative of something along these lines。
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以决定添加很多其他的东西。现在这个GPT-3的数据集也从未发布。所以今天我所熟悉的一些相当好的数据集将会在这些方面具有代表性。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_183.png)'
  id: totrans-628
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_183.png)'
- en: Our number one the red pajama dataset or more specifically for example the slim
    pajama subset of the red pajama dataset。
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一数据集是红色睡衣数据集，或者更具体地说是红色睡衣数据集的精简子集。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_185.png)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_185.png)'
- en: Which is a clean and de-duplicated version of it。 And in the other sense again
    it's a bunch of common crawl。 C4 which is also as far as I know more common crawl
    but processed differently。 And then we have github， books， archive， Wikipedia，
    stack is change。 These are the kinds of datasets that would go into these data
    mixtures。
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个干净且去重的版本。在另一种意义上，它又是一堆常见的网络爬虫数据。C4也如我所知，是更多的常见网络爬虫数据，但处理方式不同。然后我们还有GitHub、书籍、档案、维基百科、堆栈变化。这些就是会进入这些数据混合物的数据集类型。
- en: Now specifically the one that I like that came out recently is called fine web
    dataset。 So this is an attempt to basically collect really high quality common
    crawl data and filter it in this case to 15 trilane tokens。 And then in addition
    to that more recently hugging face released this fine web edu subset which is
    1。3 trillion of educational and 5。4 trillion of high educational content。
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我特别喜欢最近发布的一个数据集，叫做精细网络数据集。这是一个基本上收集高质量常见网络爬虫数据并过滤到15万亿标记的尝试。此外，最近Hugging Face发布了这个精细网络教育子集，其中包含1.3万亿的教育内容和5.4万亿的高质量教育内容。
- en: So basically they're trying to filter common crawl to very high quality educational
    subsets。 And this is the one that we will use。 There's a long webpage here on
    fine web and they go into a ton of detail about how they process the data which
    is really fascinating reading by the way。
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，他们试图对常见的网络爬虫数据进行过滤，以获得高质量的教育子集。这就是我们将使用的内容。这里有一篇关于精细网络的长网页，他们详细讲述了如何处理这些数据，这非常值得一读。
- en: And I would definitely recommend if you're interested into data mixtures and
    so on and how data gets processed at these scales。 I'll look at this page。 And
    more specifically we'll be working with the fine web edu I think。 And it's basically
    educational content from the internet。 They show that training on educational
    content in their metrics works really really well。
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对数据混合物以及这些规模下的数据如何处理感兴趣，我绝对推荐你查看这个页面。更具体地说，我们将使用精细网络教育子集，我认为它基本上是来自互联网的教育内容。他们展示了在教育内容上进行训练的效果非常好。
- en: And we're going to use this sample 10 billion tokens subsample of it because
    we're not going to be training on trillions of tokens。 We're just going to train
    on 10 billion sample off the fine web edu because empirically in my previous few
    experiments。
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个样本的10亿个标记子样本，因为我们不会在万亿个标记上进行训练。我们只会在10亿个样本上训练，来自fine web edu，因为在我之前的几个实验中经验表明。
- en: This actually suffices to really get close to GPT two performance and it's simple
    enough to work with。 And so let's work with the sample 10 bt。 So our goal will
    be to download it process it and make sure that our data loader can work with
    it。 So let's get to that。
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上足以让性能接近GPT二，并且操作起来相对简单。所以我们来处理样本的10亿个标记。我们的目标是下载、处理，并确保我们的数据加载器可以使用它。让我们开始吧。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_187.png)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_187.png)'
- en: Okay， so I introduced another file here that will basically download fine web
    edu from hugging face datasets。
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我在这里引入了另一个文件，基本上会从hugging face数据集中下载fine web edu。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_189.png)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_189.png)'
- en: It will pre process and pre tokenize all of the data and it will save data shards
    to a folder on a local disk。 And so while this is running， just wanted to briefly
    mention that you can kind of look through the dataset viewer here just to get
    a sense of what's in here。
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 它会对所有数据进行预处理和预标记化，并将数据片段保存到本地磁盘的一个文件夹中。因此，在此运行时，我想简要提到你可以通过数据集查看器大致了解这里有什么内容。
- en: And it's kind of interesting。 I mean， it basically looks like it's working fairly
    well。 Like it's talking about nuclear energy in France。 It's talking about Mexican
    America， some Mac， Pi。 J's， etc。 So actually it seems like their filters are working
    pretty well。 The filters here。 by the way， were applied automatically using llama
    370 B， I believe。 And so basically。
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 这还挺有趣的。它看起来运行得相当不错。比如它在讨论法国的核能，它在讨论墨西哥美国，还有一些Mac，Pi。J的等等。因此，实际上看起来他们的过滤器运作得相当好。顺便提一下，这里的过滤器是自动应用的，使用的是llama
    370 B，我相信。因此基本上。
- en: LLMs are judging which content is educational and that ends up making it through
    the filter。
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: LLM正在判断哪些内容是教育性的，这最终会通过过滤器。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_191.png)'
  id: totrans-643
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_191.png)'
- en: So that's pretty cool。 Now in terms of the script itself。 I'm not going to go
    through the full script because it's not as interesting and not as LLM centric。
    But when you run this basically， number one， we're going to load the dataset。
    which this is all hugging face code， running this。
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 这真不错。关于脚本本身，我不会详细讲解完整的脚本，因为它没有那么有趣，也不那么以LLM为中心。但是当你运行这个时，首先，我们会加载数据集，这都是hugging
    face的代码。
- en: And then we're going to need to pip install datasets。 So it's downloading the
    dataset。 then it is tokenizing all of the documents inside this dataset。 Now when
    we tokenize the documents。 you'll notice that to tokenize a single document， we
    first start the tokens with the end of text token。 And this is a special token
    in the GPT2 tokenizer as you know， so 50。
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要用`pip install datasets`来安装数据集。所以它正在下载数据集。然后它正在对这个数据集中的所有文档进行标记化。现在，当我们对文档进行标记化时，你会注意到，要标记一个单独的文档，我们首先用文本结束标记开始标记。正如你所知道的，这在GPT2标记器中是一个特殊标记，所以是50。
- en: 256 is the ID of the end of text。 And this is what begins a document。 even though
    it's called end of text。 But this is the first token that begins a document。 Then
    we extend with all of the tokens of that document。 Then we create a NumPy array
    out of that。 We make sure that all the tokens are between。 oh。
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 256是文本结束的ID。这是一个文档开始的标志。尽管它被称为文本结束，但这是开始文档的第一个标记。然后我们扩展所有该文档的标记。接着我们用这些标记创建一个NumPy数组。我们确保所有的标记都在之间。哦。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_193.png)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_193.png)'
- en: Okay， let me debug this。 Okay， so apologies for that。 It just had to do with
    me using a float division in Python and it must be integer division so that this
    is an int and everything is nice。 Okay， but basically the tokenization here is
    relatively straightforward。 Returns tokens in MP。UN16。 We're using UN。16 to save
    a little bit of space because 2 to the 16 minus 1 is 65，000。
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我调试一下。好的，抱歉。这与我在Python中使用浮点除法有关，必须是整数除法，这样这就是一个整数，一切都很好。好的，但基本上这里的标记化相对简单。返回MP.UN16格式的标记。我们使用UN.16来节省一些空间，因为2的16次方减1是65000。
- en: So the GPT2 max token ID is well below that。 And then here there's a bunch of
    multi-processing code and it's honestly not that exciting so I'm not going to
    step through it。 But we're loading the dataset， we're tokenizing it and we're
    saving everything to shards and the shards are NumPy files。
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GPT2 的最大代币 ID 远低于此。而且这里有一堆多进程代码，老实说，这并不是那么令人兴奋，所以我不打算逐步讲解。但是我们在加载数据集，我们在对其进行标记，并将所有内容保存到分片中，分片是
    NumPy 文件。
- en: So just storing a NumPy array which is very very similar to Torx tensors。 And
    the first shard。 0 0 0 is a validation shard and all the other shards are training
    shards。 And as I mentioned they all have 100 million tokens in them exactly。 And
    that just makes it easier to work with us to shard the files because if we just
    have a single massive file sometimes it can be hard to work with on the disk。
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 所以只需存储一个 NumPy 数组，它与 Torx 张量非常相似。第一个分片，0 0 0 是验证分片，所有其他分片都是训练分片。正如我提到的，它们都有
    1 亿个代币，完全一样。这使得与我们合作时处理分片文件更容易，因为如果我们只有一个巨大的文件，有时在磁盘上处理会比较困难。
- en: And so sharding it is just kind of massive from that perspective。 And yeah。
    so we'll just let this run。 This will be probably 30ish minutes or so and then
    we're going to come back to actually train on this data。 And we're going to be
    actually doing some legit pre-training in this case。 This is a good data set。
    we're doing lots of tokens per second。 We have HEPUs。
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，分片是相当庞大的。是的，我们就让它运行吧。这可能需要大约 30 分钟，然后我们将回到实际在这些数据上进行训练。我们实际上将进行一些合法的预训练。这是一个很好的数据集。我们每秒处理大量的代币。我们有
    HEPUs。
- en: the code is ready and so we're actually going to be doing a serious training
    run。 So let's get back in a minute。 Okay， so we're back。 So if we LSE do find
    web。 we see that there's now 100 shards in it。 And that makes sense because each
    shard is 100 million tokens。 so 100 shards of that is 10 billion tokens in total。
    Now swinging over to the main file。
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经准备好了，所以我们实际上将进行一次严肃的训练运行。我们等一会儿回来。好的，我们回来了。所以如果我们 LSE 查找 web，我们会看到现在里面有
    100 个分片。这是合理的，因为每个分片有 1 亿个代币。所以 100 个分片总共有 100 亿个代币。现在切换回主文件。
- en: I made some adjustments to our data loader again。 And that's because we're not
    running with Shakespeare anymore。 we want to use the find web shards。 And so you'll
    see some code here that additionally basically can load these shards。 We load
    the un16 NumPy file， we convert it to a torch。long tensor。 which is what a lot
    of the layers up top expect by default。
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 我再次对我们的数据加载器进行了调整。这是因为我们不再使用莎士比亚。我们想使用查找 web 的分片。所以你会看到一些代码，基本上可以加载这些分片。我们加载
    un16 的 NumPy 文件，将其转换为 torch.long 张量，这正是上面许多层默认期望的格式。
- en: And then here we're just enumerating all the shards。 I also added a split to
    data loader light。 so we can load the split training， but also the split value，
    the zero split。 And then we can load the shards and then here we also have not
    just a current position now。 but also the current shard。 So we have a position
    inside a shard and then when we run out of tokens in a single shard。
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里我们只是列举所有的分片。我还为数据加载器添加了一个拆分功能，因此我们可以加载拆分的训练数据，但也可以加载拆分的验证数据，零拆分。然后我们可以加载分片，这里我们不仅有当前的位置，还有当前的分片。所以我们在一个分片内有一个位置，当我们在单个分片中耗尽代币时。
- en: we first advanced the shard and loop if we need to。 And then we get the tokens
    and readjust the position。 So this data loader will now iterate all the shards
    as well。 So I changed that and then the other thing that I did while the data
    is processing is our train loader now has split train of course。
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先推进分片，并在需要时循环。然后我们获取代币并重新调整位置。所以这个数据加载器现在也会遍历所有分片。所以我做了这些更改，然后在数据处理的同时，我们的训练加载器现在当然有拆分训练。
- en: And down here I set up some numbers。 So we are doing two to the 19 tokens per
    step。 And we want to do roughly 10 billion tokens because that's how many unique
    tokens we have。 So if we did 10 billion tokens， then divide that by two to the
    19， we see that this is 19，073 steps。 So that's where that's from。 And then the
    GPT three paper says that they warm up the learning rate over 375 million tokens。
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我设定了一些数字。所以我们每步处理 2 的 19 次方个代币。我们想大约处理 100 亿个代币，因为这就是我们拥有的唯一代币数量。所以如果我们处理
    100 亿个代币，再除以 2 的 19 次方，我们会发现这是 19,073 步。这就是来源。然后 GPT 三论文提到他们在 3.75 亿个代币上预热学习率。
- en: So I came here and 375 e6 tokens divide two to the 19 is 715 steps。
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我来到这里，375 e6 代币除以 2 的 19 次方是 715 步。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_195.png)'
  id: totrans-658
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_195.png)'
- en: So that's why warm up steps is set to 715。 So this will exactly match the warm
    up schedule that GPT three used。
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是为什么热身步骤设置为 715。所以这将完全匹配 GPT-3 使用的热身计划。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_197.png)'
  id: totrans-660
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_197.png)'
- en: And I think 715 by the way is very mild and this could be made significantly
    more aggressive。 Probably even like a hundred is good enough。 But it's okay。 Let's
    leave it for now so that we have the exact hyper parameters of GPT three。 So I
    fix that and then that's pretty much it。 We can run。
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得 715 顺便说一下是非常温和的，这可以显著变得更具攻击性。可能像 100 就足够了。但没关系。我们就先这样，以便我们有 GPT-3 的确切超参数。所以我修复了这个，然后差不多就这样。我们可以运行。
- en: So we have our script here and we can launch。 And actually sorry。 Let me do
    one more thing。 Excuse me。 For my GPU， I can actually fit more back size and I
    believe I can fit 64 on my GPU as a micro back size。 So let me try that。 I could
    be misremembering。 But that means 64 times one 24 per GPU and then we have a GPU。
    So that means we would not even be doing great in accumulation of this fit because
    this just multiplies out to the full total back size。
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们这里有我们的脚本，我们可以启动。实际上抱歉。让我再做一件事。抱歉。对于我的 GPU，我实际上可以适应更多的批量大小，我相信我可以在我的 GPU
    上适应 64 作为微批量大小。所以让我尝试一下。我可能记错了。但这意味着每个 GPU 64 乘以 24，然后我们有一个 GPU。所以这意味着我们在这个适配的累积中甚至不会表现得很好，因为这只是乘以了完整的总批量大小。
- en: So no greener accumulation。 And that would run pretty quickly if that fits。
    I mean if this works then this is basically a seriously trading run。 We're not
    logging。 We're not evaluating the validation split。 We're not running any evaluations
    yet。 So it's not we haven't crossed our teas and dotted our eyes。
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 所以没有更好的累积。如果适合的话，那将运行得非常快。我是说如果这有效，那么这基本上就是一个严肃的交易运行。我们没有记录。我们没有评估验证集。我们还没有进行任何评估。所以并不是说我们已经划好我们的“t”，点好我们的“i”。
- en: But if we let this run for a while we're going to actually get a pretty good
    model。 And the model that might even be on par with or better than GPT two on
    24 M。 Okay。 So it looks like everything is growing great。 We're processing 1。5
    million tokens per second。 Everything here looks good。 We're doing 330 milliseconds
    per iteration and we have to do a total of。
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们让它运行一段时间，我们实际上会得到一个相当不错的模型。而且这个模型可能与或优于 GPT-2 在 24 M 上。好的。所以看起来一切都在良好增长。我们每秒处理
    150 万个标记。这里一切看起来不错。我们每次迭代花费 330 毫秒，而我们必须做的总数是。
- en: Where are we printing out 1973？ So 19。073 times 0。33 is this many seconds。 This
    many minutes。 So this will run for 1。7 hours。 So one and a half hour run like
    this。 And we don't even have to use great accumulation which is nice。 And you
    might not have that luxury in your GPU。 In that case just start decreasing the
    batch size until things fit。
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在哪里打印 1973？所以 19.073 乘以 0.33 是这么多秒。这么多分钟。因此这将运行 1.7 小时。像这样的一个半小时的运行。而且我们甚至不需要使用较好的累积，这很好。而且你可能没有在你的
    GPU 上享受这种奢侈。在那种情况下，只需开始减小批量大小，直到适合为止。
- en: But keep it to nice numbers。 So that's pretty exciting。 We're currently warming
    up the learning rate so you see that it's still very low。 1 in negative 4。 So
    this will ramp up over the next few steps all the way to 6E negative 4 here。 Very
    cool。 So now what I'd like to do is let's cross the T's and dot our I's。
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 但保持为好的数字。所以这非常令人兴奋。我们目前正在加热学习率，所以你看到它仍然非常低。负 4 的 1。因此这将在接下来的几步中逐渐升高，达到这里的 6E
    负 4。非常酷。所以现在我想做的是让我们划好“t”，点好“i”。
- en: Let's evaluate on the validation split。 And let's try to figure out how we can
    run E valves。 how we can do logging， how we can visualize our losses and all the
    good stuff。 So let's get to that before we actually do the run。 Okay。 So I adjusted
    the code so that we're evaluating on the validation split。
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在验证集上进行评估。让我们尝试弄清楚我们如何运行 E valves。我们如何进行记录，如何可视化我们的损失和所有好的东西。所以在我们实际进行运行之前，让我们先处理这个。好的。所以我调整了代码，以便我们在验证分片上进行评估。
- en: So creating the validator just by passing in split equals val。 That will basically
    create a data loader just for the validation shard。 The other thing I did is in
    the data loader， I introduced a new function reset。 which is called an init and
    it basically reset the data loader。
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 所以通过传入 split 等于 val 来创建验证器。这基本上会为验证分片创建一个数据加载器。我在数据加载器中做的另一件事是引入了一个新函数 reset，它被称为
    init，基本上重置数据加载器。
- en: And that is very useful because when we come to the main training now。 So this
    is the code I've added。 And basically every 100th iteration， including the 0th
    iteration。 we put the model into evaluation mode。 We reset the validator and then
    no gradients involved。 We're going to basically accumulate the gradients over
    say 20 steps and then average it all up and print out the validation loss。
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有用，因为当我们进入主要训练时。这是我添加的代码。基本上每进行100次迭代，包括第0次迭代，我们将模型置于评估模式。我们重置验证器，然后不涉及梯度。我们基本上将在大约20个步骤中累积梯度，然后将其平均并打印出验证损失。
- en: And so that basically is the exact same logic as the training roughly。 but there's
    no loss that backward。 It's only inference。 We're just measuring the loss。 We're
    adding it up。 Everything else otherwise applies and is exactly as we've seen it
    before。 And so this will print the validation loss every 100th iteration。
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这和训练的逻辑是完全一样的，但没有反向传播的损失。这里只是推理。我们只是测量损失。我们将其累加。其他一切都是一样的，和我们之前看到的一样。所以每100次迭代会打印验证损失。
- en: including the very first iteration。 So that's nice。 That will tell us some amount。
    some a little bit about how much we're overfitting。 That said， like we have roughly
    infinity data。 So we're mostly expecting our train and val loss to be about the
    same。 But the other reason I'm kind of interested in this is because we can take
    the GPT 2。
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 包括第一次迭代。这很好，这将告诉我们一些关于我们过拟合程度的信息。不过，考虑到我们大致拥有无限的数据，我们基本上预期训练和验证损失会大致相同。但我对这个感兴趣的另一个原因是，我们可以使用GPT-2。
- en: 124M as OpenAI released it。 And we can initialize from it and we can basically
    see what kind of loss it achieves on the validation loss as well。 And that gives
    us kind of an indication as to how much that model would generalize to 124M。 But
    it's not a sorry to find web EDU validation split。 That said。 it's not a super
    fair comparison to GPT 2 because it was trained on a very different data distribution。
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI发布的124M。我们可以从中初始化，基本上可以看到它在验证损失上达到什么样的损失。这为我们提供了一种指示，说明该模型在124M上的泛化能力。但这不是一个非常公平的比较，因为GPT-2是在完全不同的数据分布上训练的。
- en: But it's still kind of like an interesting data point。 And in any case。 you
    would always want to have a validation split in a training run like this。 So that
    you can make sure that you are not overfitting。 And this is especially a concern
    if we were to make more epochs in our training there。
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 但这仍然算是一个有趣的数据点。无论如何，你总是希望在这样的训练过程中有一个验证集，以确保你没有过拟合。尤其是在我们进行更多迭代时，这点尤其需要关注。
- en: So for example， right now we're just doing a single epoch。 But if we get to
    a point where we want to train on a 10 epochs or something like that。 we would
    be really careful with maybe we are memorizing that data too much。 If we have
    a big enough model and our validation split would be one way to tell whether that
    is happening。
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现在我们只进行一次迭代。但如果我们想训练10次迭代或类似的情况，我们会非常小心，可能我们会过度记忆那些数据。如果我们的模型足够大，而我们的验证集是判断这种情况的一种方式。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_199.png)'
  id: totrans-675
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_199.png)'
- en: Okay， and in addition to that， if you remember at the bottom of our script。
    we had all of this orphaned code for sampling from way back when。 So I deleted
    that code and I moved it up to here。 So once in a while we simply evaluate validation。
    Once in a while we sample， we generate samples。 And then we do that only every
    100 steps and we train on every single step。
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，此外，如果你记得在我们脚本的底部，我们有一些从很久以前留下来的孤立代码进行采样。所以我删除了那段代码并将其移动到这里。因此，我们偶尔评估验证。偶尔我们进行采样，生成样本。然后我们只在每100个步骤中进行一次训练。
- en: So that's how I have a structure right now。 And I've been running this for 1000
    iterations。 So here are some samples on the ration 1000。 Hello。 I'm a language
    model and I'm not able to get more creative。 From a language model and languages
    you're learning about here is or is the beginning of a computer。
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我目前的结构。我已经运行了1000次迭代。这里是关于1000次迭代的一些样本。你好，我是一个语言模型，我无法变得更具创意。从一个语言模型来看，你在这里学习的语言是计算机的开始。
- en: Okay， so this is all like pretty， it's still a garble。 But we're only at a ration
    1000 and we've only just barely reached the maximum learning rate。 So this is
    still a learning。 We're about to get some more samples coming up in 100。 Okay。
    Okay。 this is， you know， the model is still a still a young baby。 Okay。
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这一切都还是有点混乱。但我们现在只有 1000 的比率，我们刚刚达到最大学习率。所以这仍然是一个学习阶段。我们将在 100 次之后获得更多样本。好的。这是，模型仍然还是个小宝宝。好的。
- en: so basically all of this sampling code that I've put here。 everything should
    be familiar with to you and came from before。 The only thing that I did is I created
    a generator object in PyTorch so that I have a direct control over the sampling
    of the random numbers。 Because I don't want to impact the RNG state of the random
    number generator that is the global one used for training。
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我在这里放的所有采样代码应该对你来说都很熟悉，并且来自之前的内容。我所做的唯一事情是我在 PyTorch 中创建了一个生成器对象，这样我就可以直接控制随机数的采样。因为我不想影响用于训练的全局随机数生成器的
    RNG 状态。
- en: I want this to be completely outside of the training loop。 And so I'm using
    a special sampling RNG and then I make sure to seed it that every single rank
    has a different seed。 And then I pass in here where we sort of consume random
    numbers in multinomial where the sampling happens。 I make sure to pass in the
    generator object there。 Otherwise this is identical。
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这完全在训练循环之外。因此，我使用了一个特殊的采样 RNG，并确保为每个单独的 rank 设置了不同的种子。然后我在这里传入我们在多项式中消费随机数的地方，采样在这里发生。我确保在这里传入生成器对象。否则，这与之前是完全相同的。
- en: Now the other thing is you'll notice that we're running a bit slower。 That's
    because I actually had to disable torch。compile to get this to sample。 And so
    we're running a bit slower。 So for some reason it works with no torch compile
    but when I torch compile my model I get a really scary error from PyTorch and
    I have no idea to resolve it right now。 So probably by the time you see this code
    released or something like that， maybe it's fixed。
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你会注意到我们运行得稍微慢一些。这是因为我实际上不得不禁用 torch.compile 才能进行采样。因此我们运行得稍微慢一些。所以出于某种原因，它在没有
    torch compile 的情况下可以工作，但当我使用 torch compile 我的模型时，我收到了 PyTorch 的一个非常可怕的错误，而我现在不知道如何解决。因此，可能等你看到这个代码发布或类似的情况时，也许它已经修复了。
- en: But for now I'm just going to do in false。 And I'm going to bring back torch
    compile and you're not going to get samples。 And I think I'll fix this later。
    By the way I will be releasing all this code。 And actually I've been very careful
    about making Git commits every time we add something。 And so I'm going to release
    the entire repo that starts completely from scratch all the way to now and after
    this as well。
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我只是打算将其设置为 false。我将重新启用 torch compile，你不会得到样本。我想我稍后会修复这个。顺便说一下，我会发布所有这些代码。实际上，我在每次添加内容时都非常小心地进行
    Git 提交。因此，我将发布从零开始到现在的整个代码库，包括之后的部分。
- en: And so everything should be exactly documented in the Git commit history。 And
    so I think that would be nice。 So hopefully by the time you go to GitHub this
    is removed and it's working and I will fix the bug。
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一切都应该在 Git 提交历史中准确记录。因此，我觉得这样很好。希望等你去 GitHub 时，这个问题会被解决，并且它可以正常工作，我会修复这个 bug。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_201.png)'
  id: totrans-684
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_201.png)'
- en: Okay so I have the optimization running here。 And it's stepping and we're on
    step 6。000 or so so we're about 30% through training。 Now while this is training
    I would like to introduce one evaluation that we're going to use to supplement
    the validation set。
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我在这里运行优化。它正在进行，我们大约在第 6,000 步，所以训练大约进行到 30%。现在在训练期间，我想介绍一个评估方法，我们将用它来补充验证集。
- en: And that is the Hellesfag eval。 So Hellesfag comes from this paper back in 2019
    so it's a 5 year old eval now。 And the way Hellesfag works is that it's basically
    a sentence completion dataset。 So it's a multiple choice。 For every one of these
    questions we have basically a shared context like a woman is outside with a bucket
    and a dog。 The dog is running around trying to avoid bath。 She， A。
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Hellesfag 评估。Hellesfag 来源于 2019 年的一篇论文，因此它现在是一个 5 年前的评估。Hellesfag 的工作方式是，它基本上是一个句子完成数据集。它是一个多项选择题。对于这些问题中的每一个，我们都有一个共享的上下文，比如一个女人在外面带着一个桶和一只狗。狗在跑来跑去，试图避免洗澡。她，A。
- en: raises the bucket off with soap and blow dry the dog's head。 B。 uses a hose
    to keep it from getting soapy。 C， gets the dog wet and it runs away again。 Or
    D。 gets into a bathtub with the dog。 And so basically the idea is that these multiple
    choices are constructed so that one of them is a natural continuation of the sentence
    and the others are not。 And the others might not make sense like uses the hose
    to keep it from getting soapy。
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 用肥皂提起水桶并用吹风机吹干狗的头。B。用水管保持它不变得肥皂泡沫。C，弄湿狗，它又跑掉了。或者D。和狗一起进入浴缸。所以基本上这个想法是这些多个选项构造得使得其中一个是句子的自然延续，而其他的则不是。其他选项可能没有意义，比如使用水管保持它不变得肥皂泡沫。
- en: That makes no sense。 And so what happens is that models that are not trained
    very well are not able to tell these apart but models that have a lot of world
    knowledge and can tell which and can tell a lot about the world will be able to
    create these completions。
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 这毫无意义。因此，训练不好的模型无法区分这些，但具有丰富世界知识的模型能够创建这些完成。
- en: And these sentences are sourced from activity net and from Wookieow。 And at
    the bottom of the paper there's kind of like a cool chart of the kinds of domains
    in Wookieow。 So there's a lot of sentences from computers and electronics and
    homes and garden。 And it has kind of a broad coverage of the kinds of things you
    need to know about the world in order to find the most likely completion and the
    identity of that completion。
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子来源于活动网和Wookieow。在论文底部有一个很酷的图表，展示了Wookieow中的各种领域。因此有很多关于计算机、电子、家庭和花园的句子。它涵盖了你需要了解世界的广泛知识，以便找到最可能的完成及其身份。
- en: One more thing that's kind of interesting about the last work is the way it
    was constructed is that the incorrect options are deliberately adversarially sourced。
    So they're not just random sentences。 They're actually sentences generated by
    language models。
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最后一项工作的一个有趣之处是，其构造方式是错误选项是故意来源于对抗性。因此它们不仅仅是随机句子。实际上它们是由语言模型生成的句子。
- en: And they're generated in such a way that language models basically find them
    difficult but humans find them easy。 And so they mentioned that humans have a
    95% accuracy on this set but at the time the state of the art language models
    had only 48%。 And so at the time this was a good benchmark。 Now you can read the
    details of this paper to learn more。 The thing to point out though is that this
    is five years ago and since then what happened to Halaswag is that it's been totally
    just solved。
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它们的生成方式使得语言模型基本上觉得它们很难，但人类觉得很简单。他们提到人类在这个集合上的准确率为95%，但当时最先进的语言模型只有48%。因此那时这是一个很好的基准。现在你可以阅读这篇论文的细节来了解更多。不过要指出的是，这已经是五年前的事了，自那以后Halaswag完全被解决了。
- en: And so now the language models here are 96%， so basically the last 4% is probably
    errors in the data set or the questions are really really hard。 And so basically
    this data set is kind of crushed with respect to language models but back then
    the best of the language models was only at about 50%。
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这里的语言模型准确率为96%，所以最后的4%可能是数据集中的错误，或者问题真的很难。因此这个数据集在语言模型方面有点不足，但当时最好的语言模型只有大约50%。
- en: But this is how far things got。 But still the reason people like Halaswag and
    it's not used by the way in GPT-2 but in GPT-3 there is Halaswag evil。 And lots
    of people use Halaswag。 And so for GPT-3 we have results here that are cited。
    So we know what percent accuracy GPT-3 attains at all these different model check
    points for Halaswag evil。 And the reason people like it is because Halaswag is
    a smooth evil and it is an evil that offers "early signal"。
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 但这就是事情发展的程度。不过，喜欢Halaswag的人喜欢它，而它并未在GPT-2中使用，但在GPT-3中有Halaswag evil。很多人使用Halaswag。因此对于GPT-3，我们这里有被引用的结果。我们知道GPT-3在所有这些不同模型检查点的Halaswag
    evil的准确率。
- en: So "early signal" means that even small language models are going to start at
    the random chance of 25% but they're going to slowly improve and you're going
    to see 25。 26， 27， etc。 And you can see slow improvement even when the models
    are very small and it's very early。
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 所以“早期信号”意味着即使是小型语言模型也会从25%的随机机会开始，但它们会慢慢提高，你会看到25，26，27等。即使模型非常小，而且还是很早的时候，你也可以看到缓慢的改进。
- en: So it's smooth， it has "early signal" and it's been around for a long time。
    So that's why people kind of like this evil。 Now the way that we're going to evaluate
    this is as follows。 As I mentioned we have a shared context and this is kind of
    like a multiple choice task。 But instead of giving the model a multiple choice
    question and asking it for A， B， C or D。
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它是顺畅的，具有“早期信号”，并且存在很长时间了。这就是为什么人们喜欢这种方法。现在我们评估它的方式如下。如我所提到的，我们有一个共享的上下文，这有点像多项选择任务。但不是给模型一个多项选择问题并询问它选择A、B、C或D。
- en: we can't do that because these models when they are so small， as we are seeing
    here。 the models can't actually do multiple choice。 They don't understand the
    concept of associating a label to one of the options of multiple choice。 They
    don't understand that。 So we have to give it to them in a native form and the
    native form is a token completion。 So here we do， we construct a batch of four
    rows and T tokens， whatever that T happens to be。
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法做到这一点，因为这些模型在如此小的情况下，如我们所见。这些模型实际上无法进行多项选择。他们不理解将标签与多项选择中的选项关联的概念。他们不明白这一点。因此，我们必须以原生形式提供给它们，而原生形式是令牌完成。所以在这里，我们构造了一个四行和T个令牌的批次，无论这个T是什么。
- en: Then the shared context that is basically the context for the four choices。
    the tokens of that are shared across all of the rows。 And then we have the four
    options so we kind of like lay them out。 And then only one of the options is correct，
    in this case label three， option three。
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 然后共享的上下文基本上是四个选择的上下文。这些令牌在所有行之间共享。接着我们有四个选项，所以我们把它们摆放在一起。只有一个选项是正确的，在这种情况下是标签三，选项三。
- en: And so this is the correct option and option one two for our concurrent。 Now
    these options might be of different lengths。 So what we do is we sort of like
    take the longest length and that's the size of the batch B by T。 And then some
    of these here are going to be padded dimensions。 So they're going to be unused。
    And so we need the tokens， we need the correct label and we need a mask that tells
    us which tokens are active。
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是正确的选项，选项一和二是我们的并行选项。现在这些选项的长度可能不同。所以我们所做的是取最长的长度，这就是批次B乘以T的大小。然后这些中的一些将是填充维度，因此它们将未使用。因此，我们需要令牌，我们需要正确的标签，以及一个掩码，告诉我们哪些令牌是有效的。
- en: And the mask is then zero for these padded areas。 So that's how we construct
    these batches。 And then in order to get the language model to predict a， b， c
    or d。 the way this works is basically we're just going to look at the tokens。
    They're probabilities。 And we're going to pick the option that gets the lowest
    or the highest average probability for the token set for the tokens。
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码对于这些填充区域为零。这就是我们构造这些批次的方式。然后，为了让语言模型预测a、b、c或d，基本上我们将查看这些令牌。它们的概率。我们将选择令牌集的最低或最高平均概率的选项。
- en: Because that is the most likely completion according to the language model。
    So we're just going to look at the probabilities here and average them up across
    the options and pick the one with the highest probability roughly speaking。 So
    this is how we're going to do hella swag。 And this is I believe also how GPT three
    did it。 This is how GPT three did it as far as I know。 But you should note that
    some of the other emails or you might see hella swag may not do it this way。
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是根据语言模型最有可能的完成方式。所以我们将查看这里的概率，并在选项中进行平均，选择概率最高的一个。大致来说，这就是我们将如何进行“超酷”。我相信这也是GPT-3的工作方式。根据我所知，这就是GPT-3的运作方式。但你应该注意，其他一些可能会看到“超酷”的邮件可能不是这样做的。
- en: They may do it in a multiple choice format where you sort of give the context
    a single time and then the four completions。 And so the model is able to see all
    the four options before it picks the best possible option。 And that's actually
    an easier task for a model because you get to see the other options when you're
    picking your choice。 But unfortunately models at our size can't do that。
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可能采用多项选择的格式，你只需给出一次上下文，然后是四个完成选项。因此，模型能够在选择最佳选项之前看到所有四个选项。这实际上是模型更容易完成的任务，因为在选择时你可以看到其他选项。但不幸的是，我们这种规模的模型无法做到这一点。
- en: Only models at a bigger size are able to do that。 And so our models are actually
    slightly handicapped in this way that they are not going to see the other options。
    They're only going to see one option at a time and they just have to assign probabilities
    and the correct option has to win out in this metric。
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 只有更大规模的模型才能做到这一点。因此，我们的模型在这一点上稍微受限，它们只能一次看到一个选项，必须为每个选项分配概率，正确的选项必须在这个指标中获胜。
- en: All right， so let's now implement this very briefly and incorporate it into
    our script。
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们现在简要实现这一点并将其融入到我们的脚本中。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_203.png)'
  id: totrans-704
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_203.png)'
- en: Okay， so what I've done here is I've introduced a new file called hella swag。py
    that you can take a look into。 And I'm not going to step through all of it because
    this is not exactly like deep code。 Deep code。 It's kind of like a little bit
    tedious， honestly。 because what's happening is I'm downloading hella spark from
    GitHub and I'm rendering all of its examples and there are a total of 10。
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我在这里做的是引入了一个新的文件叫做hella swag.py，你可以查看一下。我不会逐行讲解，因为这并不是完全深入的代码。实际上，这有点乏味，因为发生的事情是我正在从GitHub下载hella
    spark，并渲染它的所有示例，总共有10个。
- en: 000 examples。 I am rendering them into this format。
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 000个示例。我将它们渲染成这种格式。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_205.png)'
  id: totrans-707
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_205.png)'
- en: And so here at the end of this render example function， you can see that I'm
    returning the tokens。 The tokens of this four by T array of tokens， the mask。
    which tells us which parts are the options and everything else is zero。 And the
    label that is the correct label。 And so that allows us to then iterate the examples
    and render them。
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个渲染示例函数的末尾，你可以看到我返回了标记。这个四行T数组的标记，掩码指示哪些部分是选项，其他的都是零。还有标签，即正确标签。这使得我们能够迭代示例并渲染它们。
- en: And I have an evaluate function here， which can load a GPT to from a face and
    it runs the eval here。 And basically just calculates just as I described。 it predicts
    the option that has the lowest or the highest probability。 And the way to do that
    actually is we can basically evaluate the cross entropy loss。
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里有一个评估函数，可以从一个面加载GPT，并在这里运行评估。基本上就是如我所描述的那样，它预测具有最低或最高概率的选项。实际上做到这一点的方法是我们基本上可以评估交叉熵损失。
- en: So we're basically valuing the loss of predicting the next token in the sequence。
    And then we're looking at the row that has the lowest average loss。 And that's
    the option that we pick as the prediction。
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们基本上是在评估预测序列中下一个标记的损失。然后我们查看具有最低平均损失的行。我们选择的预测就是这个选项。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_207.png)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_207.png)'
- en: And then we do some stats and prints and stuff like that。 So that is a way to
    evaluate the loss of now。 If you go up here， I'm showing that for GPT to 124M。
    if you run this script， you're going to see that the loss of gets 29。55%。 So that's
    the performance we get here。 Now remember that random chances 25% so we haven't
    gone too far。
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们做一些统计和打印之类的。这是一种评估当前损失的方法。如果你查看上面，我显示的是GPT到124M。如果你运行这个脚本，你会看到损失为29.55%。所以这是我们在这里获得的表现。现在记住，随机机会是25%，所以我们还没有走得太远。
- en: And GPT to XL， which is the biggest V GPT to gets all the way up to 49% roughly。
    So these are pretty low values considering that today's state of the art is more
    like 95%。 So these are definitely older models by now。 And then there's one more
    thing called a Luther harness。 which is a very common piece of infrastructure
    for running emails for language models。
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: GPT到XL，这个最大的V GPT大约能达到49%的准确率。所以考虑到今天的最先进技术大约是95%，这些数值相对较低。显然，这些模型现在已经过时了。还有一个叫做Luther
    harness的东西，这是运行语言模型电子邮件的非常常见基础设施。
- en: And they get slightly different numbers。 And I'm not 100% sure what the discrepancy
    is for these。 It could be that they actually do the multiple choice instead of
    just the completions。 And that could be the discrepancy。 But I'm not 100% sure
    about that。 I'd have to take a look。 But for now， our script reports 29。55。 And
    so that is the number that we'd like to beat if we were training a GP to 124M
    from scratch and ourselves。
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 它们得到的数值稍有不同。我对这些差异并不完全确定。可能是它们实际上进行了多项选择，而不仅仅是补全。这可能就是差异所在。但我对此并不完全确定。我需要看看。不过现在，我们的脚本报告29.55。所以这是我们希望超越的数字，如果我们从零开始训练一个GP到124M的话。
- en: So now I'm going to go into actually incorporating this eval into our main training
    script。 And basically because we want to evaluate it in a periodic manner so that
    we can track Haloswag and how it evolves over time。 And see when and if we cross
    this 29。55 sort of region。 So let's now walk through some of the changes to train
    GPT to that pipe。
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将实际将这个评估集成到我们的主要训练脚本中。基本上，因为我们想以周期性的方式进行评估，以便跟踪 Haloswag 及其随时间的演变。并查看我们是否跨越了这个
    29.55 的区域。现在让我们逐步了解一下训练 GPT 到管道的一些更改。
- en: The first thing I did here is actually made use compile optional kind of and
    I disabled it by default。 And the problem with that is the problem with compile
    is that unfortunately it does make our code faster。 But it actually breaks the
    evaluation code and the sampling code。 It gives me a very gnarly message and I
    don't know why。
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里做的第一件事是使 compile 成为可选的，并默认禁用。问题是 compile 使我们的代码更快，但它确实破坏了评估代码和采样代码。它给了我一个非常复杂的错误信息，我不知道为什么。
- en: So hopefully by the time you get to the code base when I put it up on GitHub。
    we're going to fix that by then。 But for now I'm running without torch compile
    which is why you see this be a bit slower。 So we're running without torch compile。
    I also created a log directory log where we can place our log。txt which will record
    the train loss， validation loss and the Haloswag accuracies。
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 所以希望当你在我把代码库放到 GitHub 时，你能看到它。到那时我们将修复这个问题。但目前我在没有 torch compile 的情况下运行，这就是你看到运行速度有点慢的原因。因此我们在没有
    torch compile 的情况下运行。我还创建了一个日志目录 log，我们可以在其中放置 log.txt，记录训练损失、验证损失和 Haloswag 准确性。
- en: So a very simple text file and we're going to open for writing so that it sort
    of starts empty and then we're going to append to it。 I created a simple variable
    that helps tell us when we have a last step。 And then basically periodically inside
    this loop every 250th iteration or at the last step we're going to evaluate the
    validation loss。 And then every 250th iteration we are going to evaluate Haloswag
    but only if we are not using compile because compile breaks it。
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将打开一个非常简单的文本文件以进行写入，这样它就会从空开始，然后我们将向其中附加内容。我创建了一个简单的变量，帮助我们判断何时是最后一步。然后基本上在这个循环中，每进行
    250 次迭代或在最后一步时，我们将评估验证损失。每 250 次迭代，我们将评估 Haloswag，但只有在不使用 compile 的情况下，因为 compile
    会导致问题。
- en: So I'm going to come back to this code for evaluating Haloswag in a second。
    And then every 250th iteration as well we're also going to sample from the model。
    And so you should recognize this as our ancient code from way back when we started
    the video and we're just sampling from the model。 And then finally here these
    are if we're not after we validate sample and evaluate Haloswag we actually do
    a training step here。
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 我将稍后再回到用于评估 Haloswag 的代码。然后每进行 250 次迭代，我们也会从模型中采样。所以你应该认出这是我们在视频开始时的古老代码，我们正在从模型中进行采样。最后，这里是，如果我们在验证样本和评估
    Haloswag 后，我们实际上会在这里执行一个训练步骤。
- en: And so this is one step of training and you should be pretty familiar with all
    of what this does。 And at the end here once we get our training loss we write
    it to the file。 So the only thing that changed that I really added is this entire
    section for Haloswag eval。 And the way this works is I'm trying to get all the
    GPUs to collaborate on the Haloswag。
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练的一步，你应该对它的所有功能非常熟悉。在这里，当我们获得训练损失时，我们将其写入文件。所以我真正添加的唯一更改是整个 Haloswag 评估部分。它的工作方式是我试图让所有
    GPU 在 Haloswag 上协作。
- en: And so we're iterating all the examples and then each process only picks the
    examples that assign to it。 So we sort of take i and mod it by the world size
    and we have to make it equal to rank otherwise we continue。 And then we render
    an example， put it on a GPU， we get the logits。 then I created a helper function
    that helps us basically predict the option with the lowest loss。
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在迭代所有示例，每个进程只选择分配给它的示例。因此我们取 i 对世界大小取模，并使其等于 rank，否则我们继续。然后我们渲染一个示例，放在 GPU
    上，获取 logits。接着我创建了一个辅助函数，帮助我们基本上预测损失最低的选项。
- en: So this comes here the prediction and then if it's correct we sort of keep count。
    And then if multiple processes were collaborating on all this then we need to
    synchronize their stats。 And so the way one way to do that is to package up our
    statistics here into tensors。 which we can then call this。alverduson and sum。
    And then here we sort of unwrap them from tensors so that we just have ints。
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是预测，如果正确我们就记录下来。如果多个过程在协同工作，我们需要同步它们的统计数据。一个方法是将我们的统计数据打包到张量中，然后调用这个.alverduson并进行求和。然后在这里我们将它们从张量中解开，只得到整数。
- en: And then here the master process will print and log the Haloswag accuracy。 So
    that's kind of it and that's what I'm running right here。 So you see this optimization
    here and we just had a generation。 And this is step 10。000 out of about 20，000
    right so we are halfway done。
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里主进程将打印并记录Haloswag的准确性。这就是我在这里运行的内容。所以你看到这里的优化，我们刚刚进行了生成。这是从大约20,000步中的第10,000步，所以我们完成了一半。
- en: And these are kinds of samples that we are getting at this stage so let's take
    a look。 Hello I'm a language model so I'd like to use it to generate some kinds
    of output。 I'm a language model and I'm a developer for a lot of companies。 I'm
    a language model。 Let's see if I can find any fun one。 I don't know you can go
    through this yourself but certainly the predictions are getting less and less
    random。
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在这个阶段我们获得的样本，让我们来看看。你好，我是一个语言模型，我想用它生成一些输出。我是一个语言模型，也是许多公司的开发者。我是一个语言模型。让我们看看我能否找到有趣的内容。我不知道你可以自己浏览，但显然预测变得越来越不随机。
- en: It seems like the model is a little bit more self-aware and using language that
    is a bit more specific to it being a language model。 Hello I'm a language model
    and like how the language is used to communicate I'm a language model and are
    going to be speaking English and German。
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 模型似乎变得更自我意识，使用的语言更贴合作为语言模型的特点。你好，我是一个语言模型，语言是用来交流的，我会说英语和德语。
- en: Okay I don't know。 So let's just wait until this optimization finishes and we'll
    see what kind of samples we get。 And we're also going to look at the train， the
    val and the hellosware accuracy and see how we're doing with respect to GPT2。
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我不知道。那么我们就等到这个优化完成，再看看我们得到什么样的样本。我们还将查看训练、验证和hellosware的准确性，看看相对于GPT2的表现如何。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_209.png)'
  id: totrans-727
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_209.png)'
- en: Okay good morning。 So focusing for a moment on the Jupyter Notebook here on
    the right I created a new cell that basically allows us to visualize the train。
    val and hella。 And the helloscore。 And you can step through this it basically
    like parses the log file that we are writing。
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，早上好。所以稍微关注一下右边的Jupyter Notebook，我创建了一个新的单元，基本上允许我们可视化训练、验证和helloscore。你可以逐步查看，这基本上解析了我们正在写的日志文件。
- en: And a lot of this is just like boring matplotlib code but basically this is
    what our optimization looks like。 So we ran for 19，073 steps which is roughly
    10 billion tokens which is whoops oh my gosh。 Which is one epoch of the sample
    10b of find what BDO。 On the left we have the loss and in the blue we have the
    training loss。
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 这大部分就像无聊的matplotlib代码，但基本上这就是我们的优化过程。所以我们运行了19,073步，大约是100亿个标记，哎呀，天哪。这是找到BDO的样本10b的一个周期。在左侧我们有损失，在蓝色部分是训练损失。
- en: In orange we have the validation loss and red as a horizontal line。 We have
    the opening of GPT2。 124M model checkpoint when it's just evaluated on the validation
    set of this fine web BDO。 So you can see that we are surpassing this orange is
    below the red so we're surpassing the validation set of this data set。 And like
    I mentioned the data set distribution is very different from what GPT2 trained
    on。
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 橙色部分是验证损失，红色是水平线。我们有GPT2 124M模型检查点的开头，当它刚在这个微调的BDO验证集上评估时。所以你可以看到，橙色部分低于红色，意味着我们超过了这个数据集的验证集。正如我提到的，数据集的分布与GPT2训练时的非常不同。
- en: So this is not exactly a fair comparison but it's a good cross check to look
    at。 Now we would ideally like something that is withheld and comparable and somewhat
    standard。 And so for us that is helloscoreg。 And so on here we see the helloscoreg
    progress we made from 25% all the way here。 In red we see the opening of GPT2，
    124M model in red。
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完全是公平的比较，但这是一个好的交叉检查。我们理想中希望有一些被保留且可比较的标准。所以对我们来说是helloscoreg。在这里我们看到从25%开始的helloscoreg进展。红色部分是GPT2
    124M模型的开头。
- en: So it achieves this helloscoreg here and the GPT3 model 124M which was trained
    on 300 billion tokens achieves green。 So that's over here。 So you see that we
    basically surpass the GPT2 120 program model right here which is really nice。
    Now interestingly we were able to do so with only training on 10 billion tokens
    while GPT2 was trained on 100 billion tokens。 So for some reason we were able
    to get away with significantly fewer tokens for training。
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它在这里达到了HelloScoreG，而训练了3000亿标记的GPT-3模型124M达到了绿色。所以在这里你可以看到，我们基本上超过了GPT-2的120程序模型，这真的很不错。有趣的是，我们仅用100亿标记进行训练就能做到这一点，而GPT-2是用1000亿标记进行训练的。因此，出于某种原因，我们能够在训练时使用显著更少的标记。
- en: There are many possibilities to us to why we could match or surpass this accuracy
    with only 10 billion training。 So number one it could be that opening of GPT2
    was trained on a much wider data distribution。 So in particular fine web EDU is
    all English it's not multilingual and there's not that much math in code。 And
    so math and code and multilingual could have been stealing capacity from the original
    GPT2 model and basically that could be partially the reason why this is not working
    out。
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多可能性解释为什么我们能够以仅100亿的训练数据匹配或超过这个准确度。首先，可能是因为GPT-2是在更广泛的数据分布上进行训练的。因此，尤其是Fine
    Web EDU是全英文的，并不是多语言的，代码中的数学内容也不多。因此，数学和代码及多语言可能从原始GPT-2模型中抽取了能力，这可能是部分原因为什么这并没有奏效。
- en: There's many other reasons。 So for example the helloscoreg eval is fairly old
    maybe five years or so。 It is possible that aspects of helloscoreg in some way
    or even identically have made it into the training set of fine web。 We don't know
    for sure but if that was the case then we are basically looking at the training
    curve instead of the validation curve。 So long story short this is not a perfect
    eval and there's some caveats here but at least we have some confidence that we're
    not doing something completely wrong。
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他原因。例如，HelloScoreG评估相当旧，可能有五年左右。某些HelloScoreG的方面可能以某种方式，甚至完全相同地，进入了Fine
    Web的训练集。我们不能确定，但如果真是这样，那么我们基本上是在看训练曲线而不是验证曲线。长话短说，这并不是一个完美的评估，有一些警告，但至少我们对自己并没有做错什么有一定信心。
- en: And it's probably the case that when people try to create these data sets they
    try to make sure that test sets that are very common are not part of the training
    set。 For example when hugging face created the fine web EDU they use helloswag
    as an eval so I would hope that they make sure that they deduplicate and that
    there's no helloswag in the training set but we can't be sure。
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，当人们尝试创建这些数据集时，他们可能会确保非常常见的测试集不包含在训练集中。例如，当Hugging Face创建Fine Web EDU时，他们将HelloSwag作为评估数据，所以我希望他们能确保去重，并且训练集中没有HelloSwag，但我们不能确定。
- en: The other thing I wanted to address briefly is look at this LOSCER this looks
    really really wrong here。 I don't actually know 100% what this is and I suspect
    it's because the 10 billion sample of fine web EDU was not properly shuffled。
    And there's some issue here with the data that I don't fully understand yet and
    there's some weird periodicity to it。 And because we are in a very lazy way sort
    of serializing all the tokens and just iterating on them from scratch without
    doing any permutations or any random sampling ourselves。
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 我想简单提一下的是，看看这个LOSCER，这看起来真的很不对。我并不完全知道这是什么，我怀疑是因为Fine Web EDU的100亿样本没有被正确打乱。而且这里的数据存在一些我还不完全理解的问题，还有一些奇怪的周期性。因为我们以非常懒惰的方式对所有标记进行序列化，并且从头开始对它们进行迭代，而没有进行任何置换或随机抽样。
- en: I think we're inheriting some of the ordering that they have in the dataset。
    So this is not ideal but hopefully by the time you get to this repo some of these
    things by the way will hopefully be fixed。
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们正在继承数据集中他们的一些排序。因此这并不理想，但希望到你查看这个仓库时，其中一些问题能得到解决。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_211.png)'
  id: totrans-738
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_211.png)'
- en: And I will release this build nano GPT repo and right now it looks a little
    ugly and preliminary so hopefully by the time you get here it's nicer。 But down
    here I'm going to show Arata and I'm going to talk about some of the things that
    happened after the video and I expect that we will have fixed the small issue。
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 我会发布这个构建Nano GPT的仓库，目前看起来有点丑陋和初步，所以希望你到达这里时它会更好。但在这里我将展示Arata，并且我将谈论视频后发生的一些事情，我预计我们会修复这个小问题。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_213.png)'
  id: totrans-740
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_213.png)'
- en: But for now basically this shows that our training is not completely wrong and
    it shows that we're able to surpass the accuracy with only 10x the token budget。
    And possibly it could be also that the dataset might have improved。 So the original
    GPT-2 dataset was webtext。 It's possible that not a lot of care and attention
    wanted to the dataset。 This was very early in LMS。 Whereas now there's a lot more
    scrutiny on good practices around deduplication。
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 但目前为止，这表明我们的训练并不完全错误，并且显示出我们能够在仅使用10倍的令牌预算的情况下超越准确率。可能数据集也有所改进。原始的GPT-2数据集是webtext，可能在数据集上没有投入太多的关注和精力。这是在LMS早期，而现在对去重的好做法有了更多的审查。
- en: filtering， quality filtering and so on。 And it's possible that the data set
    were training on just a higher quality per token and that could be giving us a
    boost as well。 So a number of caveats to think about but for now we're pretty
    happy with this。 And yeah。
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤、质量过滤等等。而且可能我们正在训练的数据集在每个令牌上的质量更高，这也可能给我们带来提升。因此，有许多需要考虑的注意事项，但目前为止我们对此相当满意。是的。
- en: now the next thing I was interested in is as you see it's a morning now so there
    was an overnight and I wanted to basically see how far I could push the result。
    So to do an overnight run I basically did instead of one epoch which took roughly
    two hours。
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我感兴趣的是，现在是早晨，所以经历了一夜的训练，我基本上想看看我能把结果推得多远。为了进行过夜训练，我基本上做了四个周期，而不是一个周期，大约花了两个小时。
- en: I just did it times four so that that would take eight hours while I was sleeping。
    And so we did four epochs or roughly 40 billion tokens of training and I was trying
    to see how far we could get。 And so this was the only change in I rerun the script
    and when I point and read the log file at the 40B this is what the curve looked
    like。 Okay， so to narrate this number one we are seeing this issue here with the
    periodicity through the different epochs and something really weird with the FindWeb
    EDU dataset and that is to be determined。
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 我把它做了四倍，这样在我睡觉的时候大约会花八个小时。所以我们进行了四个周期的训练，大约40亿个令牌，我在试图看看能达到什么程度。因此，这是唯一的变化，我重新运行了脚本，当我查看40B的日志文件时，这就是曲线的样子。好的，所以来叙述一下，首先我们看到不同周期间存在周期性的问题，以及FindWeb
    EDU数据集的一些奇怪现象，这还需要进一步确定。
- en: But otherwise we are seeing that the heliswag actually went up by a lot and
    we almost made it to the GPT-3-124M accuracy up here。 But not quite so it's too
    bad that I didn't sleep slightly longer and I think if this was a five epoch run
    we may have gotten here。
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 但除此之外，我们看到heliswag的结果实际上大幅上升，我们几乎达到了GPT-3-124M的准确率。可惜的是，我没有多睡一会儿，我认为如果这是五个周期的训练，我们可能会达到这个结果。
- en: Now one thing to point out is that if you're doing multi epoch runs we're not
    actually being very careful in our data loader and we're not。 This data loader
    goes through the data in exactly the same format and exactly the same order and
    this is kind of suboptimal and you would want to look into extensions where you
    actually permute the data randomly。
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要指出的一点是，如果你正在进行多轮周期的训练，我们在数据加载器方面并没有非常仔细。这个数据加载器以完全相同的格式和顺序处理数据，这实际上是次优的，你应该考虑使用扩展功能来随机打乱数据。
- en: And then you permute the documents around in every single shard on every single
    new epoch and potentially even permute the shards。 And that would go a long way
    into decreasing the pre-allicity and it's also better for the optimization so
    that you're not seeing things in the identical format and you're introducing some
    of the randomness in how the documents follow each other。
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你在每个新周期的每个分片中打乱文档，甚至可能还会打乱分片。这将大大减少预先相似性，同时对于优化也是更好的，这样你不会以完全相同的格式看到内容，同时引入一些文档之间的随机性。
- en: Because you have to remember that in every single row these documents follow
    each other and then there's the end of text open and then the next document。 So
    the documents are currently glued together in the exact same identical manner
    but we actually want to break break up the documents and shuffle them around because
    the order of the documents shouldn't matter and they shouldn't。
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你必须记住，在每一行中，这些文档是相互跟随的，然后是文本结束标记，接着是下一个文档。因此，文档目前是以完全相同的方式粘在一起的，但我们实际上想要打乱这些文档，因为文档的顺序不应该重要。
- en: Basically we want to break up that dependence because it's kind of a spurious
    correlation and so our data litter is not currently doing that and that's one
    improvement you could think of making。 The other thing to point out is we're almost
    matching GPT-3 accuracy with only 40 billion tokens GPT-3 trained on 300 billion
    tokens。
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们想打破这种依赖关系，因为它是一种虚假的相关性，因此我们的数据当前并没有做到这一点，这是一项你可以考虑改进的地方。另一个需要指出的地方是，我们几乎用仅40亿个令牌达到了与GPT-3相匹配的准确性，而GPT-3是在300亿个令牌上训练的。
- en: So again we're seeing about a 10x improvement here with respect to learning
    efficiency。 The other thing I wanted to and I don't actually know exactly what
    to attribute this to other than some of the things that I already mentioned previously
    for the previous one。
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们再次看到学习效率在这里大约提高了10倍。我想提到的另一件事是，我实际上并不知道具体该归因于什么，除了我之前提到的一些因素。
- en: The other thing I wanted to briefly mention is the max LR here。 I saw some people
    already play with this a little bit in a previous related repository and it turns
    out that you can actually almost like 3X this so it's possible that the maximum
    learning rate can be a lot higher。
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 我想简要提到的另一件事是这里的最大学习率。我看到一些人已经在之前的相关仓库中对此进行了一些尝试，结果发现你实际上可以几乎将其提高3倍，因此最大学习率可能会高得多。
- en: And for some reason the GPT-3 hyperparameters that we are inheriting are actually
    extremely conservative and you can actually get away with higher learning rate
    and it would train faster。 So a lot of these hyperparameters are quite tunable
    and feel free to play with them and they're probably not set precisely correctly
    and it's possible that you can get away with doing this basically。
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某种原因，我们继承的GPT-3超参数实际上是非常保守的，你实际上可以使用更高的学习率，它会训练得更快。因此，这些超参数都是相当可调的，可以自由尝试，它们可能并没有被精确设置，你可能会发现这样做是可行的。
- en: And if you wanted to exactly be faithful to GPT-3 you would also want to make
    the following difference。 You want to come here and the sequence length of GPT-3
    is 2X。 It's 2048 instead of 1024。 So you would come here changes to 2048 for T
    and then if you want the exact same number of tokens half a million per iteration
    or per step you want to then decrease this to 32。 So they still multiply to half
    an L。 So that would give your model sequence length equal to that GPT-3 and in
    that case basically the models would be roughly identical as far as I'm aware
    because again GPT-2 and GPT-3 are very very similar models。
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想严格遵循GPT-3，你还需要做以下修改。你需要来这里，将GPT-3的序列长度改为2倍，即2048，而不是1024。所以你需要把T改为2048，然后如果你想要每次迭代或每一步的确切令牌数量为50万，你需要将其减少到32。这样仍然乘积到半个L。因此，这将使你的模型序列长度等于GPT-3，在这种情况下，模型在我所知的范围内基本上是相同的，因为GPT-2和GPT-3是非常相似的模型。
- en: Now we can also look at some of the samples here from the model that was trained
    overnight。 So this is the optimization and you see that here we stepped all the
    way to 7600 to 90 also or so and these are the Hellersmag which was 33。24 and
    these are some of the samples from the model。 And you can see that if you read
    through this and pause the video briefly you can see that there are a lot more
    coherent。
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也可以看看这个过夜训练的模型的一些样本。这是优化的结果，你可以看到我们一路走到了7600到90左右，这些是Hellersmag，它是33.24，这些是模型的一些样本。如果你稍微暂停一下视频，你会看到这些样本更加连贯。
- en: So and they're actually addressing the fact that it's a language model almost。
    So hello I'm a language model and I try to be as accurate as possible。 I'm a language
    model not a programming language。 I know how to communicate。 I use Python。 I don't
    know if you pause this and look at it and then compare it to the one to the model
    that was only trained for 10 billion。
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，他们正在解决这是一个语言模型的问题。所以你好，我是一个语言模型，我尽量做到尽可能准确。我是一个语言模型，而不是编程语言。我知道如何进行沟通。我使用Python。如果你暂停一下，看看它，然后将其与仅训练了100亿的模型进行比较。
- en: You will see that these are a lot more coherent and you can play with this yourself。
    One more thing I added to the code by the way is this chunk of code here。 So basically
    right after we evaluate the validation loss if we are the master process in addition
    to logging the validation loss every 5000 steps were also going to save the checkpoint。
    Which is really just the state dictionary of the model。
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现这些更加连贯，你可以自己试试。另外，我在代码中添加的一个东西是这段代码。所以基本上在我们评估验证损失后，如果我们是主进程，除了每5000步记录验证损失，我们还将保存检查点。实际上只是模型的状态字典。
- en: And so checkpointing is nice just because you can save the model and later you
    can use it in some way。 If you wanted to resume the optimization then in addition
    to saving the model we have to also save the optimizer state dict。 Because remember
    that the optimizer has a few additional buffers because of Adam。 So it's got the
    M and B and you need to also resume the optimizer properly。
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点的好处在于你可以保存模型，之后可以以某种方式使用它。如果你想恢复优化，那么除了保存模型，我们还需要保存优化器状态字典。因为要记住，优化器有一些额外的缓冲区，因为Adam算法。所以你需要正确地恢复优化器。
- en: So you need to be careful with the RNG seeds， random number generators and so
    on。 So if you wanted to exactly be able to resume optimization you have to think
    through the state of the training process。 But if you just want to save the model
    this is how you would do it。 And one nice reason why you might want to do this
    is because you may want to evaluate the model a lot more carefully。
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要小心随机数生成器的种子等。如果你想准确地恢复优化过程，就必须考虑训练过程的状态。但如果你只想保存模型，这就是你要做的。而且，你可能想这样做的一个好理由是，你可能想更仔细地评估模型。
- en: So here we are only kind of like winging the L。S。Y。G。 value。 But you may want
    to use something nicer like for example the Luther evaluation hardness。
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们只是随便使用L.S.Y.G.值。不过你可能想用更好的方法，比如说路德评价的难度。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_215.png)'
  id: totrans-760
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_215.png)'
- en: So this is a way to also evaluate language models。 And so it's possible that
    you may want to use a different infrastructure to more thoroughly evaluate the
    models on different evaluations。
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是评估语言模型的一种方式。因此，可能你会想使用不同的基础设施，更全面地评估模型在不同评估上的表现。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_217.png)'
  id: totrans-762
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_217.png)'
- en: and compare it to the opening RGP-T2 model on many other tasks like math code
    or different languages and so on。 So this is a nice functionality to have as well。
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 并将其与许多其他任务的开源RGP-T2模型进行比较，比如数学代码或不同语言等。因此，这也是一个不错的功能。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_219.png)'
  id: totrans-764
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_219.png)'
- en: And then the other thing I wanted to mention is that everything we've built
    here this is only the pre-training step。 So the GPT here is a dreams document。
    It just predicts the next open。 You can't talk to it like you can talk to chat
    GPT。 If you wanted to talk to the model we have to fine tune it into the chat
    format。
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 还有我想提到的另一点是，我们在这里构建的所有内容仅仅是预训练步骤。因此，这里的GPT是一个梦想文档。它只是预测下一个开放项。你无法像与聊天GPT对话那样与它交谈。如果你想与模型对话，我们必须将其微调为聊天格式。
- en: And it's not actually like that complicated。 If you're looking at supervised
    fine tuning or SFT。
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上并没有那么复杂。如果你在看监督微调或SFT。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_221.png)'
  id: totrans-767
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_221.png)'
- en: Really what that means is we're just swapping out a dataset into a dataset that
    is a lot more conversational。 and there's a user-assistant user-assistant kind
    of structure。 And we just fine tune on it and then we basically fill in the user
    tokens and we sample the assistant tokens。 It's not a lot more deeper than that
    but basically we swap out the dataset and continue training。
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 其实这意味着我们只是将一个数据集换成一个更加对话化的数据集，并且有一种用户助手的结构。我们只是在这个基础上进行微调，然后基本上填充用户的标记，采样助手的标记。其实没那么复杂，但基本上我们更换数据集并继续训练。
- en: But for now we're going to stop at pre-training。
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们将停留在预训练阶段。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_223.png)'
  id: totrans-770
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_223.png)'
- en: One more thing that I wanted to briefly show you is that of course what we built
    up today was building towards NANDA GPT。 which is this repository from earlier。
    But also there's actually another NANDA GPT implementation and it's hiding in
    a more recent project that I've been working on called LLAN。
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 我想简要展示的另一件事是，当然我们今天构建的内容是为了NANDA GPT而搭建的，这是之前提到的这个代码库。但实际上还有另一个NANDA GPT实现，它隐藏在我最近正在进行的一个名为LLAN的项目中。
- en: C。 And LLAN。C is a pure C CUDA implementation of GPT2 or GPT3 training。 And
    it just directly uses CUDA and is written as C CUDA。 Now the NANDA GPT here acts
    as reference code in PyTorch to the C implementation。 So we're trying to exactly
    match up the two but we're hoping that the C CUDA is faster。
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: C。LLAN。C是一个纯C CUDA实现的GPT2或GPT3训练。它直接使用CUDA并以C CUDA形式编写。现在这里的NANDA GPT作为PyTorch中的参考代码，针对C实现。因此，我们尝试完全匹配这两者，但我们希望C
    CUDA能更快。
- en: And of course currently that seems to be the case because it is a direct optimized
    implementation。 So traingpt2。py in LLAN。C is basically the NANDA GPT。 And when
    you scroll through this file you'll find a lot of things that very much look like
    things that we've built up in this lecture。 And then when you look at traingpt2。cu
    this is the C CUDA implementation。
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，目前看起来确实如此，因为这是一个直接优化的实现。所以在LLAN。C中的traingpt2。py基本上就是NANDA GPT。当你浏览这个文件时，你会发现很多看起来非常像我们在这次讲座中构建的内容。然后当你查看traingpt2。cu时，这是C
    CUDA实现。
- en: So there's a lot of MPI called GPU CUDA CC++ and you have to be familiar with
    that。 But when this is built up we can actually run the two set by side。
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有很多MPI调用GPU CUDA CC++，你需要对这些有所了解。但是当这个构建完成后，我们实际上可以将两个并排运行。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_225.png)'
  id: totrans-775
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_225.png)'
- en: And they're going to produce the exact same results but LLAN。C actually runs
    faster。 So let's see that。 So on the left I have PyTorch， NANDA GPT looking thing。
    On the right I have the LLAN C call and here I'm going to launch the two。 Both
    of these are going to be running on a single GPU。 And here I'm putting the LLAN。
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 它们将产生完全相同的结果，但LLAN。C实际上运行得更快。让我们看看这一点。所以在左侧我有PyTorch，NANDA GPT看起来的东西。在右侧我有LLAN
    C调用，现在我要启动这两个。它们都会在单个GPU上运行。这里我放置了LLAN。
- en: C on GPU1 and this one will grab GPU0 by default。 And then we can see here that
    LLAN。C compiled and then allocate space and it's stepping。 So basically meanwhile
    PyTorch is still compiling because Torp compile is a bit slower here than the
    LLAN。C and BCCC C CUDA compile。 And so this program has already started running
    and we're still waiting here for Torp compile。
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: C在GPU1上，而这个默认会抓取GPU0。然后我们可以看到这里LLAN。C已编译并分配了空间，而且正在进行中。因此，基本上同时PyTorch仍在编译，因为Torp编译在这里比LLAN。C和BCCC
    C CUDA编译稍慢。因此，这个程序已经开始运行，而我们仍在这里等待Torp编译。
- en: Now of course this is a very specific implementation to GPT 2 and 3。 PyTorch
    is a very general neural network framework so they're not exactly comparable。
    But if you're only interested in training GPT 2 and 3 LLAN。C is very fast。 It
    takes less space。 it's faster to start and it's faster per step。
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个实现是非常特定于GPT 2和3的。PyTorch是一个非常通用的神经网络框架，因此它们并不完全可比。但是如果你只对训练GPT 2和3感兴趣，LLAN。C非常快。它占用的空间更小，启动更快，每步的速度也更快。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_227.png)'
  id: totrans-779
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_227.png)'
- en: And so PyTorch started stepping here and as you can see we're running at about
    223。000 tokens per second here and about 185，000 tokens per second here。 So quite
    a bit slower but I don't have full confidence that I exactly squeezed out all
    the juice from the PyTorch implementation。 But the important thing here is notice
    that if I line up the steps you will see that the losses and the norms that are
    printed between these two are identical。
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 所以PyTorch开始在这里进行步进，如你所见，我们的运行速度约为每秒223,000个标记，而这里大约为每秒185,000个标记。因此，速度确实慢了一些，但我并不完全确定我是否完全挤出了PyTorch实现的所有性能。但这里重要的是，如果我对齐这些步骤，你会看到这两者之间打印的损失和范数是相同的。
- en: So on the left we have the PyTorch and on the right this C CUDA implementation
    and they're the same except this one runs faster。 So that's kind of， I want to
    show you also briefly LLAN。C and this is a parallel implementation and it's also
    something that you may want to play with or look at and it's kind of interesting。
    Okay so at this point I should probably start wrapping up the video because I
    think it's getting way longer than anticipated。
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 所以左边是PyTorch，右边是C CUDA实现，它们是相同的，只是这个运行得更快。所以我也想简要介绍一下LLAN.C，这是一个并行实现，你可能想要尝试一下或者看看，它还挺有趣的。好吧，此时我可能应该开始结束这个视频，因为我觉得它已经比预期的长得多了。
- en: But we did cover a lot of ground and we built everything from scratch。 So as
    a brief summary we were looking at the GPT2 and GPT3 papers。
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们确实涵盖了很多内容，并且一切都是从零开始构建的。所以简单总结一下，我们在研究GPT2和GPT3的论文。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_229.png)'
  id: totrans-783
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_229.png)'
- en: We were looking at how you set up these training runs and all the considerations
    involved。
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在研究如何设置这些训练运行以及涉及的所有考虑事项。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_231.png)'
  id: totrans-785
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_231.png)'
- en: We wrote everything from scratch and then we saw that over the duration of either
    a two hour training run or an overnight run。 We can actually match the 124 million
    parameter checkpoints of GPT2 and GPT3 to a very large extent。 In principle the
    code that we wrote would be able to train even bigger models if you have the patients
    or the computing resources。 And so you could potentially think about training
    some of the bigger checkpoints as well。
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从头开始编写了一切，然后我们发现，在两小时的训练运行或一夜的运行期间，我们实际上能够在很大程度上匹配GPT2和GPT3的1.24亿参数检查点。原则上，我们编写的代码能够训练更大的模型，只要你有耐心或计算资源。因此，你可以考虑训练一些更大的检查点。
- en: There are a few remaining issues to address。 What's happening with the loss
    here which I suspect has to do with the final web EDU data sampling。 Why can't
    we turn on Torch Compile？ It currently breaks generation and hella swag。 What's
    up with that？ In the data loader we should probably be permuting our data when
    we reach epoch boundaries。 So there's a few more issues like that and I expect
    to be documenting some of those over time in the build manage GPT repository here。
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些待解决的问题。这里的损失情况我怀疑与最终的网络EDU数据采样有关。为什么我们不能启用Torch Compile？这目前会破坏生成效果，真是奇怪。数据加载器中，当我们达到时代边界时，可能应该对数据进行随机打乱。因此还有一些类似的问题，我希望能在管理GPT的构建过程中记录这些。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_233.png)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_233.png)'
- en: Which I'm going to be releasing with this video。 If you have any questions or
    like to talk about anything that we covered please go to the discussions tab so
    we can talk here。 Or please go to issues or pull requests depending on what you'd
    like to contribute。
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 我将与这个视频一起发布它。如果你有任何问题或者想讨论我们涵盖的内容，请前往讨论标签，以便我们可以在这里交流。或者请根据你想要贡献的内容去查看问题或拉取请求。
- en: Or also have a look at the zero to hero discord and I'm going to be hanging
    out here on the manage GPT。 Otherwise for now I'm pretty happy about where we
    got and I hope you enjoyed the video and I will see you later。
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 或者看看零到英雄的Discord，我将会在管理GPT上待一会儿。总的来说，我对我们所取得的进展感到很满意，希望你喜欢这个视频，我们稍后再见。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_235.png)'
  id: totrans-791
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_235.png)'
- en: I will see you later。
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后再见。
- en: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_237.png)'
  id: totrans-793
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fffe638cf84a8d1e020ce63d0efeee6e_237.png)'
