- en: P26：26. L6_1 Perceptron Algorithm - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P26：26. L6_1 感知机算法 - Python小能 - BV1CB4y1U7P6
- en: OK， so far we dealt with really boring stuff， so this was essentially state
    of the art， 1950， 1960。 So we need to move slowly to the '70s， and so I hope that
    by the end of today we will。 have reached the '70s。 So in order to do so， let's
    first actually look at a little bit what people in the '50s。 did with neural networks。
    And so for that we need to look at the perceptron and what Mr。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，到目前为止我们处理了一些非常枯燥的内容，实际上这些都是1950年到1960年之间的最前沿技术。所以我们需要慢慢进入70年代，希望到今天结束时，我们能达到70年代。所以为了做到这一点，我们首先需要看看50年代人们是如何利用神经网络的。所以，为此我们需要看看感知机以及Mr.。
- en: Rosenblatt did and all of that。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 罗斯恩布拉特做的这一切。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_1.png)'
- en: And then we'll take care of the multi-layer perceptron。 So this is the Mark
    I perceptron， 1960。 It's a nice space heater。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们会处理多层感知机。所以这是Mark I感知机，1960年。它是一个不错的空间加热器。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_3.png)'
- en: And yeah， so Bitcoin miners are small by comparison。 But anyway， so there's
    the perceptron， right？
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，虽然比特币矿工相对较小。但不管怎样，这就是感知机，对吧？
- en: So we've already seen this a few times， so my output is some non-linear function
    of the。 inner product between， let's say， W and X， and then I have some constant
    and let's say。 it's one if it's positive and zero otherwise。 So this is nothing
    very special。 And mind you。 I mean this is like there are three different ways
    how you can look at this。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过这个好几次了，所以我的输出是内积的某个非线性函数，假设是W和X，然后我有一个常数，假设它是正数时为1，否则为0。所以这没什么特别的。请注意，我的意思是，这就像是有三种不同的方式来看待这个问题。
- en: So binary classification just outputs zero one。 But you could also have a real
    valued scalar term for regression or you could have probabilities。 and I mixed
    up those two lines。 So that'll be fixed in the slides later。 But if you look at
    what kind of decision boundaries you can get with this linear function， well。
    if I have the normal equation for a plane， which this really is， if I require
    that W。X。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以二分类问题的输出只是0和1。但你也可以有一个实际值的标量项用于回归，或者你可以有概率。我把那两条线弄错了。所以稍后会在幻灯片中修正。但是，如果你看看用这个线性函数能得到什么样的决策边界，嗯。如果我有一个平面的法线方程，而这实际上就是平面，如果我要求W⋅X。
- en: plus B equals zero， they get any one of those lines。 And so I could， for instance。
    classify ham from spam if I want to the spam filtering and， this is with a simple
    linear function。 Quick question， out of all those classifiers， which one do you
    like best？
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加上B等于零，他们得到任何一条直线。所以我可以，举个例子，通过一个简单的线性函数来进行垃圾邮件过滤，判断火腿与垃圾邮件。如果我想分类，这就是简单的线性函数。快速提问，在所有这些分类器中，你最喜欢哪一个？
- en: Who likes the red one best？ Hey， it's a nice color， right？ Okay， nobody likes
    red here。 Hey。 how about purple？ Okay。 How about black？ Okay。 How about green？
    Okay。 So why did you all raise your hands for green？ Because I asked with last
    now。 So why do you like green best？ Yes？ It maximizes the margin of separation
    from any of the two classes。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 谁最喜欢红色的那个？嘿，它是一个不错的颜色，对吧？好吧，没人喜欢红色。嘿，紫色怎么样？好吧，黑色呢？好吧，绿色怎么样？好吧。那么你们为什么都举手表示喜欢绿色呢？因为我最后问的嘛。那么，为什么你们喜欢绿色呢？是的？它最大化了两个类别之间的分隔边界。
- en: Now if you had done your PhD in the '90s， this would have led to support vector
    machines。 and all sorts of things。 And basically the way how to make this nonlinear
    would have been to map the entire data into。 some nonlinear representation space。
    But since this is now almost， you know， it's 2019 now。 So you would do things
    by finding a good embedding of the data， which actually is the same thing。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是在90年代做的博士学位研究，那么这将会导致支持向量机以及各种各样的东西。基本上，让这个变成非线性的方式是将整个数据映射到某个非线性表示空间。但因为现在是2019年了，所以你会通过找到一个好的数据嵌入来做，这其实是一样的事情。
- en: as mapping it into this nonlinear space， except that you're learning that in
    time。 So if you want to have a difference between kernels and neural networks，
    in one case， the。 engineer invents the embedding in neural networks， we learn
    it。 In any case。 this is a good way of separating data。 And it works quite well。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就像是将数据映射到这个非线性空间一样，唯一的不同是你是在时间上学习这一点。所以如果你想要了解核方法和神经网络之间的区别，在一种情况下，工程师发明了嵌入，而在神经网络中，我们则是学习这个嵌入。不管怎样，这是一种很好的数据分离方式，效果也相当不错。
- en: Now how would you train such a thing？ And this is an algorithm。 This is probably
    the first deep learning algorithm ever。 All right。 So it's a perceptron algorithm
    of Frank-Rosenblatt。 It goes as follows。 Initialize w and b equals 0。 And then
    iterate to the data。 Whenever you get something wrong。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你如何训练这样的东西呢？这就是一个算法。这可能是有史以来第一个深度学习算法。好了。那么这是一个Frank-Rosenblatt的感知器算法。具体如下。初始化w和b等于0。然后对数据进行迭代。每当你做错时。
- en: so w dot x plus b times yi less equal than 0。 So that means I'm on the wrong
    side of the margin of the separator。 Then add yi times xi to the weight vector
    and add yi to the bias。 You keep on doing this until everything is classified
    correctly。 So this is the same thing as performing stochastic gradient descent
    with batch size 1 with this。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以w·x + b乘以yi小于等于0。这意味着我处于分隔器的错误一侧。然后将yi乘以xi加到权重向量中，并将yi加到偏差中。你一直这样做，直到所有的分类都正确为止。所以这与以批次大小为1执行随机梯度下降是一样的。
- en: loss function。 Let me write for you。 This is the loss function that I'm using。
    Well， actually。 to be correct， this is the loss function。 So here I'm plotting
    yi times f of xi。 And this is the loss。 And so if y times f is greater equal than
    0。 it means that I'm predicting class 1 and， the label is 1， so everything's good。
    If I'm here。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数。让我为你写一下。这是我使用的损失函数。嗯，实际上，准确来说，这是损失函数。所以这里我画的是yi乘以f(xi)。这是损失。所以如果y乘以f大于等于0，这意味着我预测的是类别1，并且标签也是1，那么一切都很好。如果我在这里。
- en: it means that label and prediction don't match。 And then I incur a loss that's
    proportional to how badly I get it wrong。 Now of course， the derivative of that
    is this function here。 In other words。 you get an update by just the observation
    that you have or you get no， update at all。 OK。 everybody's cool with that algorithm。
    So this is shamelessly stolen from Wikipedia。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着标签和预测不匹配。然后我会遭遇一个损失，这个损失与我错得有多严重成正比。当然，这个损失的导数就是这里的这个函数。换句话说，你通过观察到的结果来更新，或者根本不做任何更新。好了，大家对这个算法都没问题吧？这个算法无耻地来自于维基百科。
- en: Let's say I want to separate cats from dogs， I have some separator， I get the
    new observation。 I change the classifier， keep on doing this， and yeah， eventually
    I get that。 So there's actually a theorem。 And that theorem is exactly in terms
    of the margin of separation。 And so I've got the large margin cat and dog separator
    with exactly that width。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我想区分猫和狗，我有一个分隔器，我获得新的观察数据。我改变分类器，继续这样做，最终我就得到了这个。所以实际上有一个定理。这个定理正是关于分隔的间隔。因此，我有一个大间隔的猫狗分隔器，宽度正好是这个。
- en: And basically the theorem says if I can bound my data within some radius r and
    if for a unit。 vector w， so norm of w squared plus b squared less equal than 1。
    So if I can find some weight vector and bias combination that's less equal than
    1 in terms。 of overall length， then I'm guaranteed that the perceptron will converge
    after r squared。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，定理说如果我能把我的数据限制在某个半径r内，并且对于单位向量w，w的平方范数加上b的平方小于等于1。如果我能找到一个权重向量和偏差组合，其长度总和小于等于1，那么我就能保证感知器在r平方之后会收敛。
- en: plus 1 over rho squared update steps。 Not number of observations but update
    steps。 So why is it update steps rather than number of observations？
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 加上1除以ρ平方的更新步骤。不是观察的数量，而是更新的步骤。那么为什么是更新步骤而不是观察的数量呢？
- en: Have you ever been in a terribly boring lecture？ So if you ever were in a terribly
    boring lecture you learned nothing new and so you。 didn't learn anything， you
    didn't update the internal model。 On the other hand。 if you're in an interesting，
    exciting lecture， then you will very often。 have to update you in a model of how
    the world works。 You will update your parameters。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有上过一节非常无聊的讲座？如果你曾经上过一节非常无聊的讲座，你什么都没学到，因此你没有学到任何新东西，也没有更新内部模型。另一方面，如果你在一场有趣且激动人心的讲座中，那么你将很常更新自己对世界如何运作的模型。你会更新你的参数。
- en: So I hope that when I fall into this latter category， so you will converge rapidly。
    But hey。 we're trying。 So now let's actually see how this works in practice。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我希望当我进入这个后者类别时，你将快速收敛。但嘿，我们在尝试。所以现在让我们看看这个在实践中是如何运作的。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_5.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_5.png)'
- en: By the way， just one quick insert before we do this。 So this is game called
    black and white。 You guys probably don't remember it and the game makes you actually
    impost after this。 And so this game， so I'm telling you this because this is one
    of the very first applications。 of machine learning in a computer game。 So the
    thing was you basically played God and you had this avatar and the avatar would。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，在我们做这个之前插入一点内容。这个游戏叫做黑白棋。你们可能不记得它了，而这个游戏实际上让你在玩后被迫做出选择。所以我告诉你这个是因为这是机器学习在计算机游戏中的最早应用之一。事情是这样的，你基本上扮演上帝，拥有一个虚拟人物，这个虚拟人物会。
- en: try to do and mimic what you were doing。 And your goal was to teach the avatar
    to be benevolent if you want to play benevolent。 God or evil if you want to be
    evil。 So you could smash the villagers or you could feed them， right？
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试去做并模仿你们刚才做的事情。你的目标是教会虚拟人物如何做出仁慈的行为，如果你想做一个仁慈的上帝，或者做邪恶的事情，如果你想做一个邪恶的上帝。那么你可以击败村民，或者喂养他们，对吧？
- en: Stuff like this。 So what they did is they used a perceptron algorithm to learn
    based on contextual parameters。 and so on what the avatar should do。 Now this
    would have worked really nicely if humans were actually consistent。 Turns out
    humans aren't always consistent at least not in their representation space。 picked
    by the game designers。 And so what happened is that basically the algorithm which
    should have converged towards。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样。所以他们使用了感知机算法，根据上下文参数学习虚拟人物应该做什么。现在，如果人类行为一致的话，这个方法会非常有效。结果证明，人类并不是总是一致的，至少在游戏设计者选择的表示空间中并不一致。因此，发生的事情是，基本上这个算法本应该收敛到。
- en: a avatar that's very beautifully trained didn't converge to that and the game
    just progressively。 became harder and harder to play。 Which is probably one of
    the reasons why you haven't heard of this game。 So bad machine learning actually
    ruined the entire game studio。 So don't do that。 So now before we go to the XOR
    problem we actually I'll leave you with that for a moment。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经过精美训练的虚拟人物没有收敛到那个目标，游戏变得越来越难玩。这可能就是你们没有听说过这个游戏的原因之一。糟糕的机器学习实际上毁了整个游戏工作室。所以不要那样做。那么现在在我们进入
    XOR 问题之前，我会先给你们留一些思考时间。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_7.png)'
- en: Then we'll look at the examples。 So there was this AI winter around early 70s
    and this was due to those two guys。 Minsky， and Pappard。 And what they proved
    is that if I have a simple XOR problem my perceptron cannot learn。 it。 Basically
    fair line the reds and green dots this way I cannot find in the line that will。
    happily separate those into reds and green separately。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们来看这些例子。所以在70年代初期曾经有过一次 AI 寒冬，主要是由于那两位学者，明斯基和帕帕德。他们证明了，如果我有一个简单的 XOR 问题，我的感知机是无法学习的。基本上，把红点和绿点分开，我无法找到一条能够愉快地将它们分开成红点和绿点的直线。
- en: So that sounds like a pre-fundamental flaw to neural networks and that pretty
    much set。 back machine learning via like 5 to 10 years。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这听起来像是神经网络的一个根本性缺陷，几乎使得机器学习倒退了大约 5 到 10 年。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_9.png)'
- en: And of course there's a nice solution to that because otherwise we wouldn't
    be teaching。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这有一个很好的解决方案，否则我们就无法进行教学了。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_11.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_11.png)'
- en: machine learning now。 But let's actually look at the perceptron how it works。
    So this is really just okay let's import the data just as I would。 And now to
    make things interesting I need to create some separable training problem and。
    I'm just going to create some random data。 So now in order to make this problem
    separable so first of all okay I create a random weight。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在讲的是机器学习。但是让我们实际上看看感知机是如何工作的。其实这就是，好的，让我们导入数据，就像我之前做的一样。为了让事情变得有趣，我需要创建一些可分离的训练问题。我将创建一些随机数据。所以现在为了让这个问题变得可分离，首先好吧，我创建一个随机权重。
- en: vector W fake and B fake and then I make sure I rescale it okay so this is essentially。
    no weight in bias。 And then I go and create random data so they exist in the Y's。
    And I go through all the data and I check whether the data can be accurately classified。
    into let's say 1's or minus 1's according to W and if not then I throw it out
    otherwise， I keep it。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 W 假和 B 假，然后我确保重新缩放它，好吧，这本质上没有权重和偏置。然后我去创建随机数据，这些数据存在于 Y 的集合中。我遍历所有数据，检查这些数据是否可以根据
    W 被准确分类为 1 或 -1，如果不能，我就把它丢掉，否则，我就保留它。
- en: So I basically assign it a label 1 or minus 1 and if it's too close to the margin
    I just。 throw it out。 So that's essentially all I'm doing so that's what this
    epsilon does right。 So this is the margin as scale greater than epsilon。 The margin
    is basically in a product here let's scale。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我基本上给它分配一个标签1或-1，如果它离边界太近，我就把它丢掉。所以这基本上就是我所做的，这就是这个epsilon的作用。这里的边距是一个大于epsilon的尺度。边距基本上是一个内积，我们来做缩放。
- en: So if I cannot separate it confidently I throw it out otherwise I add it to
    my training set。 So that's my data generator generates fake data。 And then well
    what I need I need some plotting routine I need to know I need to get the contour。
    plots because otherwise I can't really tell exactly how well converse I am because
    okay。 so that's that that's kind of vanilla I don't think there's anything particularly
    exciting。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我不能自信地分开它，我就把它丢掉，否则我就把它加入到我的训练集。所以这是我的数据生成器，它生成假数据。然后，嗯，我需要一些绘图例程，我需要得到轮廓图，因为否则我无法准确地判断自己收敛得有多好，因为好吧。嗯，就这些，我觉得没什么特别有趣的地方。
- en: in there we need to cover and so now if I wanted to run this it's not very good
    it basically。 created only data of one class。 So here we have some data read in
    blue dots that are reasonably well separable。 So now let's actually implement
    the perceptron algorithm and this perceptron algorithm does。 exactly what I described。
    If y times inner product between w and x plus b is less equal than zero perform
    an update。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们需要覆盖的内容，如果我现在想要运行这个，结果不好，基本上它只生成了一类的数据。所以这里我们有一些用蓝点表示的数据，这些数据可以被合理地分开。所以现在让我们实际实现感知机算法，这个感知机算法做的正是我刚才描述的。如果y乘以w和x的内积加上b小于等于零，就执行一次更新。
- en: and say that I made a mistake otherwise return zero and do nothing。 Okay so
    I start with w and b set to zero and iterate to the data。 And so basically I just
    perform a perceptron update and if I encounter an error I print。 up my parameters
    and then I just keep on updating things。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后说我犯了一个错误，否则返回零什么都不做。好的，我从w和b都设置为零开始，遍历数据。所以基本上我只执行一次感知机更新，如果遇到错误，我就打印出我的参数，然后继续更新。
- en: Now for that let me quickly zoom back to the regular mode because it will not
    show very。 nicely otherwise。 Okay so let's run this code。 So initially it doesn't
    do very much this is my classifier and this green dot is what。 I misclassified
    so I get the separator。 Of course that's not particularly interesting so let's
    see what happens after I get one more。 observation okay here's another green dot。
    So now this is positive class so now my classifier already starts looking a little
    bit closer。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了做到这一点，让我快速切换回正常模式，因为否则它显示得不好。好的，那么我们来运行这段代码。最开始它并没有做太多，这是我的分类器，这个绿色的点就是我误分类的点，所以我得到了分隔线。当然这并不特别有趣，所以让我们看看在我得到另一个观测数据后会发生什么。好的，这是另一个绿色点。那么现在它是正类，所以我的分类器现在看起来已经有点接近了。
- en: to what I should have gotten。 Okay after the next mistake this one here it starts
    separating the data quite nicely so。 we're pretty much done except that the bias
    is wrong。 Okay so here's another observation okay updates this。 The one updates
    that。 Another one here and okay so I'm done going over the data if I went through
    it another。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该得到的结果。好的，在下一个错误后，这个地方开始很好地分开数据了，所以我们差不多完成了，除了偏置值不对。好的，这里又有一个新的观测值，好的，更新这一次。又一个，好的，我已经完成了遍历数据。如果我再走一遍的话。
- en: time it would be converged。 So this is the perceptron algorithm in action I've
    basically shown you every single error。 that it made after one pass the data。
    Any questions so far？
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它最终会收敛。所以这是感知机算法的实际应用，我基本上展示了每次通过数据时它所犯的所有错误。到目前为止有什么问题吗？
- en: Okay so now one thing that's kind of interesting is to check whether you know
    how difficult I。 make the problem actually has any bearing on how many updates
    I need to make。 So what I'm doing is I basically just pick a range between you
    know 0。25 and 0。025 and 0。025。 has a range of parameters for the margin so large
    margin means easy problem narrow margin。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以现在有一件有趣的事情，就是检查一下我给问题设定的难度，是否对需要多少次更新有影响。所以我做的是，基本上选择一个范围，在0.25和0.025之间，0.025作为边界的参数范围，大的边距意味着容易解决的问题，小的边距意味着困难的问题。
- en: means difficult problem right。 And then I just go and actually generate random
    data do that a few times run the perceptron。 algorithm count how many times it
    takes until it's done and then I plot this。 So I'm not going to run this piece
    of code right now which this would take a while because。 it's very efficiently
    written。 But if you ran it and waited for a while you will get this curve。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着困难问题，对吧？然后我就开始生成随机数据，做几次，运行感知机算法，计算需要多少次才能完成，然后我绘制出这条曲线。所以我现在不会运行这段代码，因为它会花一些时间，因为它写得非常高效。但如果你运行它并等待一段时间，你将得到这条曲线。
- en: So remember where we had the perceptron convergence theorem where we had basically
    no radius of。 a margin right。 This is exactly that bound nine practice。 But of
    course in reality it's a little bit better than the worst case bound but you can。
    still see the same behavior namely you can see that as the margin gets wider the
    number。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以记得我们之前讨论的感知机收敛定理，我们基本上没有一个半径的*边界*，对吧？这实际上就是那个界限，九实践。但当然，现实中它比最坏情况的界限稍微好一些，不过你依然能看到相同的行为，也就是说，你可以看到随着边界的增宽，所需的次数。
- en: of updates decreases so for a very simple problem I might need only maybe four
    or five。 updates in order to converge。 Whereas for a really difficult problem
    I might need you know 20， 30。 40 updates。 Okay。 Any more questions about the perceptron？
    Okay， cool。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 更新次数减少，因此对于一个非常简单的问题，我可能只需要四五次更新就能收敛。而对于一个非常困难的问题，我可能需要20、30、40次更新。好的，还有关于感知机的其他问题吗？好的，没问题。
- en: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_13.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd3d577dbbb5a0a1926f2566a216dd1_13.png)'
