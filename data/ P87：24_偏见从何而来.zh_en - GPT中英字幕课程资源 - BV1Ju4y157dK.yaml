- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P87：24_偏见从何而来.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 沃顿商学院《AI用于商业：AI基础／市场营销+财务／人力／管理》（中英字幕） - P87：24_偏见从何而来.zh_en - GPT中英字幕课程资源 -
    BV1Ju4y157dK
- en: The use of AI in HR systems brings within a number of problems， a number of
    challenges。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在人力资源系统中使用人工智能带来了许多问题和挑战。
- en: '![](img/d593916359e063caa93199ca5ee712db_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_1.png)'
- en: but they're also a number of emerging solutions to deal with some of these challenges。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有许多新兴解决方案来应对一些这些挑战。
- en: One of the biggest problems is the issue of bias， and this is the one you've
    likely， heard of。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的问题之一是偏见问题，这可能是你听说过的。
- en: It's very popular with the media。 It's the idea that when you apply machine
    learning algorithms to data。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这在媒体上非常受欢迎。它的观点是，当你将机器学习算法应用于数据时。
- en: they may produce， predictions or recommendations that are unequal， that are
    unfair to some groups。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可能会产生不平等、不公平的预测或推荐，针对某些群体。
- en: We talked earlier about the idea that machine learning algorithms， what they
    basically。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们早先谈到过机器学习算法的观点，它们基本上。
- en: do is learn the mapping from the examples you give it。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所做的是学习你提供的例子的映射。
- en: But what that implies is that if the examples you have provided contain some
    inherent bias， if they。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这意味着，如果你提供的例子包含某种固有的偏见，如果它们。
- en: for example， contain decisions by humans that were themselves biased， the machine。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，包含人类本身存在偏见的决策，机器。
- en: can then learn to mimic that kind of bias。 An important focus these days in
    machine learning is learning how to identify that bias and。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随后学习模仿这种偏见。当前机器学习的一个重要焦点是学习如何识别这种偏见并。
- en: to extend possible eliminate this type of bias。 But given the way machine learning
    algorithms work。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展可能消除这种偏见。但考虑到机器学习算法的工作方式。
- en: if prior decisions encode historical， bias， algorithms will necessarily learn
    to be biased as well。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以往的决策编码了历史偏见，算法必然也会学习到偏见。
- en: '![](img/d593916359e063caa93199ca5ee712db_3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_3.png)'
- en: So for example， if we're training a model or building a model that recommends
    promotion。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以例如，如果我们正在训练一个模型或建立一个推荐晋升的模型。
- en: '![](img/d593916359e063caa93199ca5ee712db_5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_5.png)'
- en: and that model is basically built using data on prior success， prior examples
    of people。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 而该模型基本上是使用以往成功的数据建立的，以往人们的例子。
- en: within the firm who were promoted to higher positions， and this historical data
    reflects。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在公司内被晋升到更高职位的人，这些历史数据反映了。
- en: historical bias at an employer， and the model we're using， the model we're building
    can。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 雇主的历史偏见，以及我们使用的模型，我们正在构建的模型可以。
- en: learn to pick up that bias as well， which is something that the industry that
    we want。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 学习识别这种偏见，这正是我们希望在行业中解决的问题。
- en: to learn how to identify and eliminate in these machine learning systems。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何识别并消除这些机器学习系统中的偏见。
- en: So that's one way that bias can enter these systems is you're using training
    data， using。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以偏见进入这些系统的一个方式是使用训练数据，使用。
- en: historical examples that contain， that themselves contain bias。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 包含历史例子的偏见，偏见本身就存在于其中。
- en: It's not the only way bias can arise。 There's also something called data adequacy
    bias。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是偏见出现的唯一方式。还有一种叫做数据充分性偏见。
- en: So think about the example of using video data or audio data from an interview
    to make。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以考虑使用视频数据或音频数据的访谈例子来制作。
- en: some predictions about candidate fit。 Well， it turns out that many systems learn
    to work better the more data they are fed。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于候选人适配性的预测。结果显示，许多系统在被喂入更多数据时工作得更好。
- en: and it turns out that a lot of the data sets that we feed these systems， we
    just happen。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，我们喂入这些系统的许多数据集，恰好。
- en: to have a lot more data on some groups than others。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些群体的数据比其他群体多得多。
- en: So if we have differences across demographic groups， across gender， across race，
    some race。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们在不同的人口统计群体、性别和种族之间存在差异。
- en: and gender groups say are not well represented in the data set， we may just
    do a poorer job。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 性别群体在数据集中表现不佳，我们可能只会做得更差。
- en: in terms of accurately predicting outcomes for that group， and that can disadvantage
    that。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在准确预测该群体结果方面，这可能会对该群体造成不利影响。
- en: '![](img/d593916359e063caa93199ca5ee712db_7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_7.png)'
- en: group， which leads to another kind of bias that's not based on historical decision
    making。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致另一种基于历史决策的偏见。
- en: but just based on the quantity and quality of data we have。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但只是基于我们拥有的数据的数量和质量。
- en: '![](img/d593916359e063caa93199ca5ee712db_9.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_9.png)'
- en: Now this kind of bias can emerge all the time， even inadvertently。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这种偏见可能会随时出现，甚至是无意中。
- en: One recent and fairly interesting example is in advertising STEM jobs。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一个相当有趣的例子是在广告 STEM 职位方面。
- en: So science and mathematics jobs。 There has been a recent attention on how these
    jobs of course are the exposure of these。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以科学和数学职位。最近对这些工作的关注当然在于它们的曝光。
- en: jobs to men and to women。 It turns out that if employers use engines that are
    commonly used to share information。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 工作机会面向男性和女性。事实证明，如果雇主使用那些常用来分享信息的引擎。
- en: '![](img/d593916359e063caa93199ca5ee712db_11.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_11.png)'
- en: with people at scale， in other words advertising engines， so think about Facebook
    engines。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模人群中，也就是说广告引擎，比如说 Facebook 引擎。
- en: Google engines， these are engines that are optimized to make information available
    to。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Google 引擎，这些是优化过的信息获取引擎。
- en: people who need to see it。 So employers have had the idea that it may be useful。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 需要看到这些信息的人。雇主们认为这可能是有用的。
- en: it may be valuable to be able to， put job opening information into these engines
    so that more people who might be a good fit。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 能够将职位空缺信息输入这些引擎可能是有价值的，这样更多可能合适的人能够看到。
- en: for the job can see it。 This is a very reasonable idea。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个工作来说，人们可以看到它。这是一个非常合理的想法。
- en: Well it turns out these engines are optimized in a way that their where information
    is routed。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，这些引擎的优化方式影响了信息的传递。
- en: is optimized in a way that this can actually inadvertently route these job openings
    disproportionately。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以这样的方式进行优化，这实际上可能会无意中导致这些职位空缺不成比例地被转发。
- en: to certain groups。 So in this case the employer has no bad intentions。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些群体来说。因此，在这种情况下，雇主没有恶意。
- en: The company that builds the engine has no bad intentions but because of the
    way the algorithm， runs。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 建造这个引擎的公司没有恶意，但由于算法的运行方式。
- en: it shows different information to men and women and because this is an HR or
    a job， context。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它向男性和女性展示不同的信息，因为这是一个人力资源或工作上下文。
- en: it starts to produce outcomes that disadvantage some groups in terms of the
    labor， market。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它开始产生对某些群体在劳动市场上的不利结果。
- en: So this is a pervasive problem that can arise even when people don't have an
    explicit intention。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个普遍存在的问题，即使人们没有明确的意图也会出现。
- en: to impose bias on their decisions and in the next video we'll start to talk
    about why。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对他们的决策施加偏见，接下来的视频我们将开始讨论原因。
- en: this is a difficult problem to manage。 Thank you。 [BLANK_AUDIO]。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个难以管理的问题。谢谢。[BLANK_AUDIO]
- en: '![](img/d593916359e063caa93199ca5ee712db_13.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d593916359e063caa93199ca5ee712db_13.png)'
