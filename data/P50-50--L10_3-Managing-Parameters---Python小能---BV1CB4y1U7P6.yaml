- en: P50：50. L10_3 Managing Parameters - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P50：50. L10_3 管理参数 - Python小能 - BV1CB4y1U7P6
- en: Once you define the network， another important thing， how to access the parameters。
    So the ground block we have both the network definition and also parameters。 So
    we want to access and modify all the parameters。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了网络，另一个重要的事情就是如何访问参数。所以在这个基础块中，我们既有网络定义，也有参数。我们想要访问并修改所有参数。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_1.png)'
- en: So here we define a similar MLPS before， two dense layers， and initialize it。
    and give random X and get the output。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里我们定义了一个类似于之前的MLPS，两个密集层，并初始化它，给定随机X并获取输出。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_3.png)'
- en: Because we have two layers， we are using the un-sequential。 we can indexing
    each layer by zero and one。 So for each layer。 the dot_prance will tell you all
    the parameters we have。 So this is the first dense layer。 The parameter is dictionary，
    which is half a name called dense_zero， and the underscore weight。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有两层，所以我们使用非顺序方式。我们可以用零和一来索引每一层。所以对于每一层，`dot_prance`会告诉你我们拥有的所有参数。这个是第一个密集层。它的参数是字典，其中有一个叫做dense_zero的名字，后面跟着下划线weight。
- en: this is weight， and tells you the shape of the weight。 The input size is 20，
    and up size is 256。 Similarly， we have bias here。 For the second layer， we can
    access by net one。 but prance will still get all this weight and a bias here。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是权重，并告诉你权重的形状。输入大小是20，上层大小是256。类似的，我们这里有偏置。对于第二层，我们可以通过net one来访问，但是`prance`仍然会获取所有这些权重和偏置。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_5.png)'
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_6.png)'
- en: Then， because it's dictionary， we can access the parameters using the string
    names。 Here I tell you the dense_zero， underscore weight， we cannot get the weight。
    which is an instance of the parameter。 And if you call dot data。 actually get
    the under arrays of all the elements of the parameter。 Okay。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，因为它是字典，我们可以通过字符串名称来访问这些参数。这里我告诉你dense_zero，underscore weight，我们无法获取weight，它是参数的一个实例。如果你调用`dot
    data`，实际上可以获取参数中所有元素的数组。好的。
- en: So sometimes it's not so convenient to remember all these names。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有时候记住所有这些名称不是那么方便。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_8.png)'
- en: Actually， you can access the bias using the dot bias。 So the second layer。 the
    second dense_zero_ bias is the parameter， and also the dot data。 you cannot get
    the under arrays。 All zeros。 So in default， we initialize all the bias into zero。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你可以通过`dot bias`来访问偏置。所以第二层，第二个dense_zero_bias是一个参数，并且调用`dot data`后，你不能得到数组。全是零。所以默认情况下，我们将所有偏置初始化为零。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_10.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_10.png)'
- en: Similarly， we can get the weight。 This is the first dense_layer's weight。 and
    the dot grad will give you the gradients matrix。 So this is the gradient matrix
    with respect to the weight。 So it has the same shape as the weight。 and in default
    all zeros。 So because we didn't do a backwater yet。 Okay。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的，我们可以获取权重。这是第一个密集层的权重，`dot grad`会给你梯度矩阵。所以这是相对于权重的梯度矩阵，它的形状与权重相同，默认全是零。所以因为我们还没有进行反向传播。好的。
- en: We can also get all the parameters by connect the primes。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过连接关键字获取所有参数。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_12.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_12.png)'
- en: So this is a network， we connect all the parameters， we have two layers。 Each
    layer have weight and the bias。 Now we get the dictionary of four items。 The dense_zero_weight。
    dense_zero_ bias， dense_one_weight， dense_one_bias。 So because it's dictionary。
    we can access a particular bias using the name。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个网络，我们连接了所有的参数，我们有两层。每一层都有权重和偏置。现在我们得到了一个包含四个项的字典。dense_zero_weight，dense_zero_bias，dense_one_weight，dense_one_bias。所以因为它是字典，我们可以通过名称访问特定的偏置。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_14.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_14.png)'
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_15.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_15.png)'
- en: That's a small mess before。 The useful thing here， we can access patterns。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 之前有一点小混乱。这里有用的是，我们可以访问模式。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_17.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_17.png)'
- en: Which means all the weights for different layers。 So this is dense_zero_weight，
    dense_one_weight。 The dense_zero， this is the first layer， all the primers you
    have。 The weight bias。 so you can get by patterns。 Okay。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着不同层的所有权重。所以这是dense_zero_weight，dense_one_weight。dense_zero，这是第一层，你所有的关键字。权重和偏置，你可以通过模式来获取。好的。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_19.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_19.png)'
- en: Now， not about initialization。 At homework you may be seeing that initialization
    is pretty important。 If you did an initialize well， if you run multiple times
    you get the random。 you get lots of stable results。 So we only showed how to use
    the initialize default initializations。 Now you can do， like， you can use init
    to use a different one。 Using normal， for example。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，谈到初始化。在作业中你可能会看到，初始化非常重要。如果你初始化得好，运行多次后会得到随机的结果，也会得到许多稳定的结果。所以我们只展示了如何使用默认的初始化方式。现在你可以做，比如，你可以使用init来选择不同的初始化方式。例如，使用常规的初始化。
- en: the normal distribution with the sigma， which is standard deviation， equal to
    0。01。 And we use force init because net， we already have initializer before。 We're
    going to call it twice。 Sometimes usually due to bulk， we're going to generate
    warning message。 So you're going to tell me that I'm going to force rate init
    this primers。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正态分布，标准差σ等于0.01。我们使用`force init`，因为网络已经有初始化器了。我们将调用两次。有时由于批量操作，通常会生成警告信息。所以你会告诉我，我将强制初始化这些参数。
- en: Now we can access the weight， the first layer， the weight and the data。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以访问权重、第一层的权重和数据。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_21.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_21.png)'
- en: and the first row of the data。 You can see that， like。 these random variables
    according to this distribution。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的第一行。你可以看到，这些随机变量是根据这个分布生成的。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_23.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_23.png)'
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_24.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_24.png)'
- en: Also we can do constant initialization， which is， it should don't happen。 You
    can do that， okay。 I can use a constant equal to 1。 Also you can do， for one，
    you can just give an DRA。 For example。 I know I grabbed this weight from a different
    network。 I'm going to initialize that weight using the other network。 You just
    grab the DRA here。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以进行常量初始化，也就是说，常量不应该发生变化。你可以这么做，没问题。我可以使用一个常量等于1。你也可以做，比如，给一个DRA。例如，我知道我从不同的网络抓取了这个权重。我要使用另一个网络来初始化这个权重。你只需要抓取这个DRA。
- en: you can also do the constants as well。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用常量。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_26.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_26.png)'
- en: That one that you can specify different initializers for different layers。 In
    default。 we're going to use constant。 But in the second layer， we're going to
    use constant。 And in the first layer， for the weight， we're going to use in the
    X-VIR initializers。 That's kind of useful if you have a network have very different
    parts。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为不同的层指定不同的初始化器。默认情况下，我们将使用常量。但在第二层，我们将使用常量。在第一层，对于权重，我们将使用X-VIR初始化器。如果你的网络有非常不同的部分，这种方法非常有用。
- en: And you can initialize one part with maybe a prime method for another network。
    and the other one with random variables。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以用一个网络的预训练方法初始化一部分，用随机变量初始化另一部分。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_28.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_28.png)'
- en: Also you can do even more fancy things。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以做更复杂的事情。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_30.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_30.png)'
- en: You can write a customized function to do arbitrary things。 For example。 I create
    a class called my_init， which is a subset of， it's a sub-class of init initializer。
    And I define the init with function。 So the name， you can ignore the name， this
    is the layer name。 And the data actually the data。 What it do is like this function
    kind of rewrite the elements on the data。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以编写一个自定义函数来做任意的事情。例如，我创建了一个名为`my_init`的类，它是初始化器`init`的子类。我定义了一个`init_with`函数。所以名称你可以忽略，这个是层的名称。数据实际上就是数据。它的作用是，这个函数会重写数据中的元素。
- en: You can do whatever you want。 For example， here I'm going to create， I'm going
    to initialize data。 and then I'm going to call it into this distribution。 So what
    we can see that I replace data with something here I cannot dive deep。 and then
    also do some defenses things here。 So when I initialize it。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做任何你想做的事情。例如，这里我将创建并初始化数据，然后将其调用到这个分布中。所以我们可以看到，我用某些东西替换了数据，但我不能深入探讨。然后这里还做了一些防御性操作。所以，当我初始化时。
- en: I can pass the instance of my initializers。 And then if I call it， I can show，
    okay。 this is the log message I have。 That's zero's weight， and the initializer's
    dense one's weight。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以传递`my_init`初始化器的实例。然后，如果我调用它，我可以显示出日志信息。比如，这就是零层的权重，和初始化器生成的Dense层的权重。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_32.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_32.png)'
- en: and the causes function to pass the data in。 Okay， so by this way， you can write
    it up。 you can use a non-pilot， you can modify the weight you want。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 而且原因函数是用来传递数据的。好的，通过这种方式，你可以写出来。你可以使用非主控，你可以修改你想要的权重。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_34.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_34.png)'
- en: A more stressful way that you just grab， you first initialize by random。 and
    then grab the data and modify it by yourself。 For example， here I do the dense
    layer。 get the weight， get the data， and all of them plus by one。 And then for
    the first element。 I change it to 42， and then I print the first row， you can
    see that the first element is 42。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更紧张的方式是你先通过随机初始化，然后抓取数据并自己修改它。例如，在这里我做了密集层。获取权重，获取数据，并将它们全部加一。然后对于第一个元素，我将其改为
    42，然后我打印第一行，你可以看到第一个元素是 42。
- en: and all the others like at least is a lot of ones there。 Okay。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他类似的，至少有很多这样的。好的。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_36.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_36.png)'
- en: The last thing here is that we already showed that using unblock。 we can share
    the parameters for different layers。 Here we can do that again。 but using un-sequential。
    We first create the dense layer。 Then using the un-sequential。 we first insert
    another dense layer， the dense layer we created before， and this one。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最后一个是我们已经展示了使用 unblock。我们可以为不同的层共享参数。在这里我们可以再次这样做，但使用 un-sequential。我们首先创建密集层。然后使用
    un-sequential。我们首先插入另一个密集层，之前创建的那个密集层，以及这个。
- en: and then pass the prance equals to the shared prance。 which means I can create
    dense layer with the parameters， from the shared parameters。 which means these
    two layers， are going to share the same parameters。 To compute gradients。 we compute
    gradients over these two layers， and then some data together for the parameter。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将 prance 传递给共享的 prance。这意味着我可以从共享的参数中创建密集层，这意味着这两层将共享相同的参数。为了计算梯度，我们计算这两层上的梯度，然后一些数据一起用于参数。
- en: So that is actually not a copy， just a share。 And here is something you can
    show that。 If I modify the first， the second dense layer's weight。 actually I
    can modify the third layer's weight as well， because they're shared just a point。
    Any questions so far？ Okay。 So these tricks let you to like can do fancy networks。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这实际上不是复制，而是共享。这里有一些你可以展示的内容。如果我修改了第一个和第二个密集层的权重，实际上我也可以修改第三个层的权重，因为它们只是共享一个点。到目前为止有问题吗？好的。这些技巧让你可以做一些花哨的网络。
- en: and do fancy training letters， but in most cases， you don't need it too much。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并且做一些花哨的训练字母，但在大多数情况下，你不太需要这样做。
- en: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_38.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f9f63bc80abff3cd0c5bd3c056ddbc_38.png)'
