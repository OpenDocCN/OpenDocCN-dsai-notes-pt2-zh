- en: P3：3.Feb_3_Lec - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P3：3.Feb_3_Lec - Tesra-AI不错哟 - BV1aJ411y7p7
- en: So just a few logistical comments。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有一些后勤上的小评论。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_1.png)'
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_2.png)'
- en: Have you guys all who have registered for the class gotten onto Piazza？ Yeah。
    it's pretty active conversation already。 I think we've had like 50 posts。 So a
    lot going on。 My only request is that why don't you students help answer questions
    and read the questions。 before you ask other questions。 In case there is a repetition。
    So that's my only request。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 注册了课程的同学都加入 Piazza 吗？是的，讨论已经非常活跃了。我觉得已经有大约 50 条帖子。所以很多事情在进行中。我唯一的请求是，希望你们同学能帮助回答问题并先阅读问题，在提问之前，避免重复问题。所以这是我唯一的请求。
- en: Participate by helping others。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过帮助他人来参与。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_4.png)'
- en: Will make life easier for the instructors。 So but lots of nice and interesting
    questions that I think you could benefit from reading。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这会让讲师的工作更轻松。不过有很多不错且有趣的问题，我觉得你们从阅读中能受益。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_6.png)'
- en: actually。 All right。 Give you one second。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 其实。好吧，给你们一秒钟。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_8.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_8.png)'
- en: Okay。 So today we're going to go a little bit deeper on the kind of abstract
    side of things。 This notion of the risk risk minimization hypothesis spaces。 And
    then we're going to move into some algorithms L1 and L2 regularization。 So the
    range regression problem which you guys have already seen in your homework。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，今天我们将更深入地探讨一些抽象方面的内容。风险最小化假设空间的概念。然后我们将进入一些算法：L1 和 L2 正则化。所以回归问题，你们已经在作业中见过。
- en: Lasso which may be new for some of you。 And we'll see how those fit into hypothesis
    spaces and regularization。 And then we'll talk about some ways to optimize those
    the objective functions you get。 And that'll be about it for today。 Tomorrow will
    be a bit of a continuation where we'll be thinking even more carefully about。
    some of the interesting properties of Lasso and Ridge that are actually important
    in practice。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 可能对你们中的一些人来说是新东西。我们将看到它们如何融入假设空间和正则化中。接着，我们会讨论一些优化你们所得到的目标函数的方法。今天大致就到这里。明天会稍微延续一下，我们会更加仔细地思考
    Lasso 和 Ridge 的一些有趣特性，这些特性在实际应用中实际上非常重要。
- en: And we may begin some discussion of classification losses tomorrow。 And the
    next week just giving you a heads up。 Next week may be our hardest lecture mathematically
    or most intense。 It's a lot of material。 And I just wanted to let you guys know
    in case you had some extra time to prepare。 I posted some notes and some slides
    that we had last year。 A look in them over in advance can help。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能明天开始讨论分类损失。下周预告一下，下周可能是我们数学上最难的一节课，或者是最密集的一节。内容很多。我只是想让你们知道，以防你们有额外时间准备。我上传了去年我们使用过的一些笔记和幻灯片，提前查看它们会有所帮助。
- en: Okay。 All right， so let's get started。 Today's topic in the first half。 Excess
    risk decomposition。 All right。 So let's do a little review。 Is this okay？ Of statistical
    learning theory from last time。 So remember we had our input space or output space
    or action space where the decision functions。 which take an input and produce
    an action。 We have loss functions that once you produce your action and you see
    what the output actually。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，开始吧。今天上半部分的主题是过度风险分解。好的，我们先做一个小复习，行吗？复习一下上次的统计学习理论。所以记得我们有输入空间、输出空间或行动空间，其中决策函数接收输入并产生一个行动。我们有损失函数，一旦你做出行动并看到实际输出是什么。
- en: is， it gives you a score， a loss。 Small is good。 Big is bad。 And then we had
    our gold standard of how we evaluate a particular decision function called。 the
    risk or the expected loss where this is based on the assumption that our input
    and。 output pairs come from a data generating distribution。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它给你一个分数，一个损失值。小的好，大的坏。然后我们有了评估特定决策函数的黄金标准：风险或期望损失，这基于假设我们的输入和输出对来自于一个数据生成分布。
- en: And that's what the expectation is taken with respect to。 And then finally when
    we have this risk we can ask well what function minimizes that， risk。 minimizes
    the expected loss。 Now it's called the base optimal， the base decision function。
    And the risk that that function attains is called the base risk。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值是相对于此进行计算的。最后，当我们有这个风险时，我们可以问，哪个函数最小化这个风险，最小化期望损失。现在它被称为贝叶斯最优决策函数。这个函数所达到的风险被称为贝叶斯风险。
- en: And then of course what we pointed out is we don't know the generating distribution
    except。 in kind of contrived situations that we make up。 And so what can we do
    instead and of course what we do is we assume we have a sample of。 data from that
    data generating distribution called D， D sub M。 And then we introduce the。 empirical
    risk which rather than an expected value is an empirical average， a sample average。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后当然，我们指出的是，我们不知道生成分布，除非在我们设定的某些人为情况中。而我们能做什么呢？当然，我们假设我们有一个来自数据生成分布的样本，记作D，D子M。然后我们引入了经验风险，它不是一个期望值，而是一个经验平均值，一个样本平均值。
- en: with respect to a data set drawn from the distribution。 So we have this average
    loss over the data set as the empirical risk。 And we saw that little demonstration
    last time that shows if you try to minimize the。 empirical risk overall functions
    this can lead to overfitting likely will lead to overfitting。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于从分布中抽取的数据集。因此，我们有这个数据集上的平均损失作为经验风险。我们看到上次的小示范，如果你尝试最小化经验风险，整体函数可能会导致过拟合，可能会导致过拟合。
- en: So what was the answer we gave last time？ We suggested let's constrain our optimization
    to a hypothesis space。 a subset of the space， of all functions rather than all
    functions and optimize over the hypothesis space instead。 And we're saying how
    do you make the hypothesis space where you put those functions into the。 hypothesis
    space that you want to end up with that have the properties that seem reasonable。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们上次给出的答案是什么？我们建议将我们的优化限制在假设空间内，即所有函数的子集，而不是所有函数，并在假设空间内优化。我们在讨论如何构建假设空间，如何将那些函数放入你希望最终得到的假设空间，它们应该具备看似合理的属性。
- en: to you from the beginning。 So those were linear functions， polynomial functions，
    decision trees。 all of these were， possible hypothesis spaces that are subsets
    of all possible decision functions。 Alright。 So once we restrict to hypothesis
    space called F， we again now we get the empirical risk。 minimizer constraint and
    the risk minimizer constraint both constraint to that hypothesis， space F。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始就给你们讲解。所以那些是线性函数、多项式函数、决策树。所有这些都是可能的假设空间，都是所有可能决策函数的子集。好了，一旦我们将假设空间限制为F，我们再次得到经验风险最小化器约束和风险最小化器约束，都限制在假设空间F内。
- en: So remember F hat N， anytime there's a hat that means something is derived from
    data。 And this is a typo。 There should be no hat here。 The risk minimizer has
    nothing to do with data。 So this should be just an F sub the hypothesis space。
    So it'll be cleaned up on the next slide。 Alright。 So here's an important concept
    coming up。 I'm going to draw it on the board。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，F̂ N，每当出现“帽子”符号时，意味着某些东西是从数据中得出的。这是一个打字错误。这里不应该有帽子。风险最小化器与数据无关。所以这里应该只是一个F子集，属于假设空间。所以在下一张幻灯片中会修正。好，接下来有一个重要概念，我将在板上画出来。
- en: I'm going to roughly repeat what's on the slide。 So if you can't see what I'll
    draw。 you can refer to this slide。 But I prefer to draw this on the board。 So
    I'm going to draw a big box。 That's too big。 And this represents all possible
    functions mapping from input space to the action space。 So alright， but F mapping
    X to A action space。 All possible。 Now suppose we have a loss function。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我将大致重复幻灯片上的内容。所以如果你看不清我画的东西，你可以参考这张幻灯片。但我更喜欢在板上画。所以我要画一个大框。这个太大了。这代表了所有可能的函数，将输入空间映射到动作空间。所以好吧，F将X映射到动作空间，所有可能的。现在假设我们有一个损失函数。
- en: we have a data generating distribution， then we can， talk about the Bayes decision
    function。 And it's a function， decision function， so it lives somewhere in this
    space。 So let's write that here。 Hold on。 F star。 Can you guys see， okay with
    that angle？
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个数据生成分布，然后我们可以谈论贝叶斯决策函数。它是一个函数，决策函数，因此它位于这个空间的某个地方。让我们在这里写下它。稍等。F*。你们看得清楚吗，角度可以吗？
- en: Well otherwise it's up there。 Alright。 Now I've been choosing the hypothesis
    space。 The hypothesis space is a subset of this whole space。 So I'll draw it。
    Just a cartoon。 The idea is to show this is not the whole space， it's a subset
    of the space。 Alright。 In this hypothesis space of functions， which we'll call
    F， there's going to be a best function。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，不然它就在那里。好了，现在我已经选择了假设空间。假设空间是整个空间的一个子集。所以我画一下。只是个示意图。意思是要显示这不是整个空间，它是空间的一个子集。好了。在这个假设空间的函数中，我们称之为F，必定会有一个最佳函数。
- en: in the sense of minimizing the risk。 So to show that it's like as close to the
    optimal as possible。 I'm going to draw it， here。 Kind of like almost like the
    projection of this Bayes optimal function onto the hypothesis。 space F， but that's
    informal。 And we'll write this F sub hypothesis space F。 Alright。 Let's write
    down the risk of the Bayes optimal。 So let's write that as R of F star。 Okay。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从最小化风险的角度来看。所以要证明它尽可能接近最优。我将画出它，这里。像是几乎将这个贝叶斯最优函数投影到假设空间F上的感觉，但这不正式。我们将写下这个F子假设空间F。好了。让我们写下贝叶斯最优的风险。所以我们写下它为R（F*）。好的。
- en: And what can we say about the comparison between the risk of the Bayes optimal
    and the risk。 of this constrained optimal function？ Sure。 Great。 So this is less
    than or equal to this。 Great。 Alright。 So now what's the next function we need？
    Any questions？ Alright。 What's the next function we need to introduce to this
    picture？ The empirical risk minimizer， right？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们能说些什么关于贝叶斯最优风险和这个受约束最优函数风险之间的比较呢？当然。太好了。所以这个小于或等于这个。太好了。好了。那么接下来我们需要什么函数？有什么问题吗？好了。那么接下来我们需要引入哪个函数到这个图中？经验风险最小化器，对吧？
- en: Let's write that， let's give us a little more room。 Let's write it here。 So
    it's still an hypothesis space F。 In general it's different from the best in this
    class。 Okay。 This is the empirical risk minimizer。 This is based on minimizing
    the empirical risk。 It's based on data。 How about this one？ F sub F。 Based on
    data？ No， it's based。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写下来，给自己留点空间。让我们写在这里。所以它仍然是一个假设空间F。通常它不同于这个类中的最佳函数。好吧。这个是经验风险最小化器。它是基于最小化经验风险的。它是基于数据的。那么这个呢？F子F。是基于数据的吗？不是，它是基于。
- en: It's an unattainable ideal。 It's the minimum of the risk over this set。 But
    we don't really know the risk。 The risk depends on the data generating distribution。
    It has the expectation over the data generating distribution。 So we can have a
    really fine-ness but we can discuss its existence in breaking its down， down。
    Okay。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个无法实现的理想。它是这个集合中最小的风险。但我们并不知道风险是什么。风险依赖于数据生成分布。它是关于数据生成分布的期望。因此，我们可以有非常精细的划分，但我们可以通过分解它来讨论它的存在。好吧。
- en: So we introduce this thing， this hypothesis space to control overfitting。 But
    there's a penalty for that。 Yeah？ No？ Okay。 There's a penalty for that。 So the
    issue is that the best function isn't in the hypothesis space。 So we lost something
    by constraining this F。 And this gap in how well we perform according to risk。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们引入了这个东西，这个假设空间来控制过拟合。但是这会带来惩罚。是吗？不是吗？好吧。它会带来惩罚。所以问题是，最佳函数不在假设空间中。因此，通过限制这个F，我们失去了一些东西。这就是根据风险我们在表现上所失去的差距。
- en: that's called the approximation error。 The approximation error， we can write
    it out as。 I'm going to just say， APP for approximation， error。 This is on the
    slide。 I could run to it。 Okay。 So the approximation error is the risk of this
    function minus the risk of this function。 All right。 What happens as we make our
    hypothesis space bigger？ What happens to the approximation error？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这叫做逼近误差。逼近误差，我们可以写出来。我就说，APP代表逼近误差。这个在幻灯片上。我要去它那里。好吧。所以逼近误差是这个函数的风险减去这个函数的风险。好吧。当我们增大假设空间时，逼近误差会发生什么？
- en: It's smaller。 Because if the new hypothesis space is strictly bigger。 the best
    in the class can only get， better in terms of the risk。 Great。 Very clear。 Let's
    talk about the empirical risk minimizer， this guy。 Maybe not as obvious this time。
    What happens do you think when we make the hypothesis space bigger？ Is empirical
    risk minimizer？
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它更小了。因为如果新的假设空间严格更大，类中的最佳函数在风险上只能更好。太好了，非常清楚。让我们谈谈经验风险最小化器，这个家伙。这次可能不那么明显。当我们增大假设空间时，你认为会发生什么？经验风险最小化器？
- en: Well let me add one thing first。 You expect the risk of the empirical risk minimizer
    to be better or worse than the best。 in the class？ Almost obvious， right？ By definition。
    So we can talk about the difference of those two things。 The risk of the empirical
    risk minimizer minus the risk of the best in the class。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我先加一件事。你是否期望经验风险最小化器的风险比类中的最佳要好，还是更差？几乎显而易见，对吧？根据定义。所以我们可以讨论这两者之间的差异。经验风险最小化器的风险减去类中最佳的风险。
- en: This is called estimation error。 So the estimation error。 what's the source
    of this estimation error？ The main thing with estimation error is that we don't
    know the data generating distribution。 We're approximating the risk with the empirical
    risk based on the data。 So this gap between the ERM and the best in the class
    is purely due to the fact that we。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这叫做估计误差。那么，估计误差的来源是什么？估计误差的主要原因是我们不知道数据生成的分布。我们基于数据使用经验风险来逼近风险。所以经验风险最小化器和类中最佳之间的差距纯粹是由于我们。
- en: don't know the distribution， we only have a sample from it。 This gap between
    the best in the class and the overall best， that has nothing to do with， sample。
    The issue there is the constraint of that hypothesis space。 So two different sources
    of error。 Now as we make the hypothesis space bigger， yes， approximation error
    decreases。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道分布，我们只有一个样本。这种最佳类和总体最佳之间的差距与样本无关。这里的问题是该假设空间的约束。所以有两种不同的误差来源。现在，当我们扩大假设空间时，是的，近似误差会减少。
- en: But the estimation error is likely to increase。 Why is that？ The idea， anyone
    have an idea？ Okay。 so if we， the idea is that if this， the size of this hypothesis
    space gets smaller。 we have the same amount of data。 It's easier to find something
    close to the best than if it's a very big space。 Big space corresponds to lots
    of functions that can overfit。 Overall space is more constrained。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但估计误差可能会增加。为什么会这样？这个想法，有人有想法吗？好吧。所以如果我们，假设空间的大小变小，而我们有相同数量的数据，那么找到接近最佳的东西就比在非常大的空间中容易。大空间对应着可以过拟合的许多函数。总体空间的约束性更强。
- en: less likely to overfit。 All right？ So an overfitting is a big gap between the
    empirical and the best in the class。 So big estimation error is like overfitting。
    If this hypothesis space is too small and every function in the space is too simple
    for。 people to predict。 That's underfitting。 We don't have the capacity in our
    hypothesis space to fit the。 to find the decision function， that we need。 Okay，
    one more question。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 更不容易过拟合。好吧？所以过拟合是经验风险最小化器和类中最佳之间的巨大差距。因此，大的估计误差就像是过拟合。如果这个假设空间太小，且空间中的每个函数都过于简单，无法做出预测。那就是欠拟合。我们的假设空间没有足够的能力来拟合，去找到我们需要的决策函数。好的，再问一个问题。
- en: So fix the size of that hypothesis space。 We increase the size of our sample。
    More data。 What changes？ Yeah， so as we get more data， the empirical risk minimizer
    may change because we have a。 different data set。 The estimation error， we expect
    to decrease。 Typically。 we expect estimation error to decrease as you get more
    data or fix a hypothesis space， size。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以修正假设空间的大小。我们增加样本的大小。更多的数据。会有什么变化？是的，随着数据的增加，经验风险最小化器可能会发生变化，因为我们有一个不同的数据集。我们期望估计误差会减少。通常来说，随着数据量的增加或假设空间大小的固定，我们期望估计误差会减少。
- en: And this approximation error， unchanged。 Okay， good。 All right。 Any questions？
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个近似误差没有变化。好的，明白了。好吧，有问题吗？
- en: Pretty straightforward。 Great。 There's a notion called excess risk。 The excess
    risk of an decision function is the difference in the risk between that function。
    and the phase risk， the best possible risk。 So if the base of those up star。 you
    can say excess risk of F is how much worse than the， best possible did we do？
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 很直接。很好。有一个叫做过剩风险的概念。决策函数的过剩风险是该函数与基础风险之间的风险差异，基础风险是最佳可能的风险。所以如果那个基础风险是上限，你可以说F的过剩风险就是我们做得比最佳可能结果差多少？
- en: I want to drive that towards zero。 So in particular， the excess risk of the
    ERM。 What's that look like？ Turns out this can be decomposed very nicely in terms
    of approximation error and estimation。 error。 So here's the excess risk of F-head
    M。 Let's add and subtract a term。 The risk of the best in the hypothesis space，
    that got there。 So we've added and subtracted it。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我想把它推向零。那么，特别是，经验风险最小化器的过剩风险是什么样的？事实证明，这可以非常漂亮地分解成近似误差和估计误差。所以这是F-head M的过剩风险。让我们加上并减去一个项。假设空间中最佳的风险，就是它得出来的风险。所以我们加上并减去了它。
- en: This is still equal。 And now you see the first pair of terms， the last the estimation
    error。 That's the risk of the ERM minus the risk of the best in the class F。 Next
    pair of terms。 that's the risk of the best in the class minus the base risk。 That's
    the approximation error。 So we've decomposed the excess risk， the total disappointment，
    the total gap between ERM。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是相等的。现在你可以看到第一对项，最后是估计误差。这是ERM的风险减去类F中最佳模型的风险。下一对项是类F中最佳模型的风险减去基准风险。这就是近似误差。所以我们已经将过度风险、总失望以及ERM之间的总差距分解开来了。
- en: and the base into an approximation error and an estimation error piece。 And
    all right。 so let's drill in a little bit。 Approximation error。 First thing， it's
    a property of the class F。 It's our penalty for restricting the hypothesis， space。
    Great。 Bigger F。 Small approximation error。 Clear。 All right， it's approximation
    error， a random or non-random variable。 Non-random。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 并且将基准分解为近似误差和估计误差部分。好吧，让我们深入一点。近似误差。首先，它是类F的一个属性。它是我们限制假设空间的惩罚。很好。更大的F，较小的近似误差。明白了。好吧，这是近似误差，是随机的还是非随机的？非随机的。
- en: It involves an expectation， yes， but once you-- but the expectation is not random。
    It involves probability theory， but it's not random。 Estimation error。 It's the
    performance hit for choosing F based on a finite sample instead of using the full。
    distribution。 Smaller F。 Smaller estimation error， roughly speaking， and less
    overfitting。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它涉及到期望值，是的，但是一旦你--但期望值并不是随机的。它涉及到概率论，但它并不是随机的。估计误差。就是基于有限样本选择F而不是使用完整分布的性能损失。更小的F，较小的估计误差，粗略来说，也减少了过拟合。
- en: So it's estimation error random or non-random random。 It's random because the
    empirical risk minimizer depends on the data set that you sample。 It's random。
    Yes， great。 All right。 So a bit of a recap。 We get a loss function。 We choose
    a hypothesis space that we think will do the trick。 We do empirical risk minimization。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么估计误差是随机的还是非随机的？是随机的。因为经验风险最小化器依赖于你采样的数据集。它是随机的。是的，太好了。那么，稍微回顾一下。我们得到一个损失函数。我们选择一个我们认为能够解决问题的假设空间。然后我们进行经验风险最小化。
- en: It's going to require some optimization。 And then the data scientist's job is
    basically the choice of F to trade off between approximation。 error and estimation
    error to minimize excess risk as best you can。 That's the job。 If you were to
    get more training data， what do you do？ Yeah。 If you get more training data。 you
    will be justified or it will make sense to enlarge， your hypothesis space。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一些优化。而数据科学家的工作基本上是选择F，在近似误差和估计误差之间做出权衡，以尽可能地最小化过度风险。这就是工作。如果你能得到更多的训练数据，你该怎么办？是的，如果你得到更多的训练数据，那么扩大假设空间是合理的，或者说是有意义的。
- en: which would decrease approximation error。 And hopefully。 the increase in sample
    size will let you maintain your estimation error， at a reasonable level。 So you'll
    notice that we write the ERM。 We're writing the argument over this hypothesis
    space。 which is fine to write。 But in practice， you actually have to find the
    argument。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这会减少近似误差。希望样本量的增加能够让你保持估计误差在合理范围内。所以你会注意到我们写ERM时，实际上是在写这个假设空间上的参数，这样写是没问题的。但实际上，你必须找到这个参数。
- en: This is what we use optimization algorithms for。 So let's think in there a bit。
    because there's something going on there。 So it is true that for nice choices
    of loss functions and reasonable hypothesis spaces。 we can find， computationally，
    the optimum， the minimum， the ERM， as precisely as we want。 if we're going to
    wait for it。 So examples are red regression that you're working on in the homework。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用优化算法的原因。所以我们来思考一下，因为这里面有一些东西在发生。所以，确实，对于一些良好的损失函数选择和合理的假设空间，我们可以计算出最优解，精确到我们想要的程度。如果我们愿意等待的话。所以，举个例子，你在做作业时涉及到的红色回归。
- en: If you're going to wait， you can find the empirical risk minimizer to arbitrary
    precision。 Takes a lot of time。 It's worth going that far。 First question。 First
    point。 Second point。 there are situations like neural networks where in fact，
    we have actually。 no idea how to find the empirical risk minimizer。 Now。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要等的话，你可以将经验风险最小化器找到任意精度。这需要很多时间，值得这么做。第一个问题，第一个点。第二个点，有些情况，比如神经网络，事实上我们并不知道如何找到经验风险最小化器。现在。
- en: it's not a convex function and we don't know how to find the true minimum of
    the objective。 for a neural network， for example。 Is it an issue？ Yeah， question。
    It doesn't matter。 We may be coming back to that later in the semester， but the
    actual form， I'm just pointing。 out that there are hypothesis spaces that we do
    not know how to find the empirical risk。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它不是一个凸函数，我们不知道如何找到目标函数的真实最小值。举个例子，对于神经网络。是一个问题吗？是的，问题。没关系。我们可能会在学期后期再回到这个问题，但实际上，我只是指出，有些假设空间我们不知道如何找到经验风险。
- en: minimizer for。 That's the main point。 Yeah？ How do we compute the base risk？
    Ooh。 How do we compute the base risk？ Yeah。 We cannot。 We can compute the empirical
    risk minimizer。 Correct。 Yes。 We can never compute the risk or the base risk or
    the risk minimizer unless we have the。 data generating distribution unless we
    know it。 In practice， we never really know it。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最小化器。这就是关键点。是吗？我们如何计算基础风险？哦。我们如何计算基础风险？是的。我们无法。我们可以计算经验风险最小化器。对的。是的。除非我们知道数据生成分布，否则我们永远无法计算风险或基础风险或风险最小化器。在实践中，我们通常并不知道它。
- en: We just have a sample from that distribution。 Why not？ It's a equal to zero。
    Say again。 The base risk may not be equal to zero。 That's true。 There may be some
    randomness in the system that's always there， and you can never predict。 better
    than a certain level。 Like for example， if you're flipping a fair coin and you
    want to predict whether it's。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是从那个分布中获得了一个样本。为什么不呢？它等于零。再说一遍。基础风险可能不等于零。没错。系统中可能存在一些随机性，它始终存在，你永远无法预测。比某个特定水平更好。比如说，如果你掷一个公平的硬币，想预测它是否
- en: heads or tails， you can't do better than 50/50。 There's no prediction function
    that will have low risk。 Good questions。 Any other questions？ Yeah。 [ Inaudible
    ]， Okay。 So the question is。 we've run our optimization algorithm。 We found the
    F hat U R M。 Is there any way to -- okay。 We never said we could find it， but
    if someone gave us one， can we check whether it is the。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 抛硬币，无论如何，你做得不比50/50更好。没有哪个预测函数能有较低的风险。好问题。还有其他问题吗？是的。[听不清]，好。问题是，我们已经运行了优化算法。找到了F
    hat U R M。有没有办法 -- 好吧。我们从未说过我们能找到它，但如果有人给了我们一个，我们能否检查它是否是。
- en: actual risk minimizer？ I don't know how unless you know the data generating
    distribution。 I don't know if there's any way to check that now。 Yeah。 [ Inaudible
    ]。 Does larger F necessarily mean smaller approximation error？ Roger F。 well approximation
    error will never increase as F gets bigger。 Right。 And when I say get bigger。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际风险最小化器？我不知道，除非你知道数据生成分布。我不知道现在是否有任何方法可以检查这一点。是的。[听不清]。更大的F是否一定意味着更小的逼近误差？Roger
    F。好吧，逼近误差随着F的增大永远不会增加。对。当我说增大时。
- en: what I'm precisely meeting in this scenario is kind of a nested。 scenario where
    the next hypothesis space completely contains the previous one plus some， more。
    That's what I mean by bigger。 Yeah。 [ Inaudible ]， Okay。 So the question is。 what
    if you -- we don't know the data generating distribution。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这种情况下具体指的是一种嵌套的。场景，其中下一个假设空间完全包含前一个，并且加上更多。这就是我所说的更大。是的。[听不清]，好。所以问题是。如果你
    -- 我们不知道数据生成分布。
- en: What if we directly estimated the data generating distribution first and then
    computed the。 base risk with respect to that distribution？ Yeah。 Okay。 That's
    a method。 I think it's some scenarios that would correspond to the plug-in method
    it's called。 The issue is that it's very difficult to estimate these distributions
    in the very high dimensions。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们首先直接估计数据生成分布，然后计算相对于该分布的基础风险呢？是的。好吧。这是一种方法。我认为在某些场景下，这与插件方法相对应。问题是，在非常高的维度下，估计这些分布非常困难。
- en: that we work with in machine learning。 We don't have the data。 If you want to
    have some slides from last year， first lecture on the cursive dimensionality。
    at a point。 Yeah。 [ Inaudible ]， Yeah。 [ Inaudible ]， Yeah。 [ Inaudible ]， Yeah。
    [ Inaudible ]。 Yeah。 [ Inaudible ]， Yeah。 [ Inaudible ]， Yeah。 [ Inaudible ]，
    Yeah。 [ Inaudible ]， Yeah。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中我们处理的是没有数据。如果你想看看去年的一些幻灯片，第一讲关于曲线维度。某个时刻。是的。[听不清]，是的。[听不清]，是的。[听不清]，是的。[听不清]。是的。[听不清]，是的。[听不清]，是的。[听不清]，是的。[听不清]，是的。
- en: '[ Inaudible ]， Yeah。 [ Inaudible ]， Yeah。 [ Inaudible ]， Yeah。 [ Inaudible
    ]， Yeah。 [ Inaudible ]。 Yeah。 [ Inaudible ]， Yeah。 [ Inaudible ]， Question is
    how do we choose a hypothesis space？'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，是的。[听不清]，是的。[听不清]，是的。[听不清]，是的。[听不清]。是的。[听不清]，是的。[听不清]，问题是我们如何选择一个假设空间？'
- en: A lot of people use linear regression。 Why do we do something different or how
    do we do something different？
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人使用线性回归。我们为什么要做不同的事情，或者我们怎么做不同的事情？
- en: In some sense that's the kind of the main question we're working with in this
    course。 We're going to talk about a lot of different hypothesis spaces and when
    they might be beneficial。 compared to something like linear regression。 So there's
    no simple answer to that。 I mean this question is usually called the model selection
    problem。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上来说，这就是我们在这门课程中要解决的主要问题。我们将讨论许多不同的假设空间以及它们何时可能比线性回归更有利。因此没有简单的答案。我是说，这个问题通常叫做模型选择问题。
- en: And it's a problem that doesn't have a direct answer。 The machine learning course
    is the answer to the model selection problem。 Yeah。 [ Inaudible ]。 F star may
    not be unique but fine。 That is true。 Thank you for asking that。 First simplicity
    this year I changed all my slides to say argument instead of some more carefully
    written way。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个没有直接答案的问题。机器学习课程是模型选择问题的答案。对。[听不清]。F*可能不唯一，但没关系。那是真的。谢谢你提这个问题。今年我首先改变了所有幻灯片，改成说“论点”而不是用一些更精心写的方式。
- en: But there may not be a minimizer。 It may not be unique。 If you see anything
    we do or you think that will cause a problem， give a shout。 I don't think it'll
    be a big issue。 But yeah there are certainly situations that we'll talk about
    in fact tomorrow where the。 empirical risk minimizer is not unique in a way that's
    a bit annoying。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但是可能没有一个最小化器。它可能不是唯一的。如果你看到我们做的任何事，或者你认为那会引发问题，请告诉我。我不认为会有大问题。但对，确实有一些情况，我们明天会谈到，在这些情况下，经验风险最小化器不是唯一的，这有点麻烦。
- en: And we talk about how to fix that。 But for the approximation part I think it
    just is。 I think that's okay。 Sure。 What is that？ What's the effect of the more
    data is the question？ Okay。 No。 More data has no effect on our approximation error。
    More data we expect generally decreases estimation error。 Yes。 Right。 Okay。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会讨论如何修复这个问题。但对于近似部分，我觉得这就这样。我觉得没问题。好的。那是什么？更多数据的影响是什么？好的。没有。更多数据对我们的近似误差没有影响。更多数据通常会减小估计误差。是的，对。好的。
- en: So what is the effect of the data？ The job of the data scientists is to choose
    the hypothesis space and one hopes to bounce the errors。 But I suppose there are
    some use mediums to mean the error。 Yeah well when I said bounce what I really
    mean is minimize the total error however you want to do it。 So we expect that
    when it's time when the estimation error is in the job of the model to grow。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 那么数据的影响是什么？数据科学家的工作是选择假设空间，并且希望能够减少误差。但我猜有些使用媒介可以表示误差。是的，我当时说减少的时候，实际上我是指最小化总误差，无论你怎么做。因此，我们期望在估计误差成为模型的工作时，误差会增加。
- en: As you change， let's discuss after an example coming up。 We can discuss after
    that。 Any more？ Yeah？
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你改变时，我们来讨论接下来要举的一个例子。我们可以在那之后再讨论。还有其他问题吗？是吗？
- en: You don't want to cover these two controls so we can talk about how to fix。
    Yeah。 So what is underfitting？ I mean if I had to put underfitting into this picture
    I would say underfitting is when the hypothesis space is so small that there's
    no issue with estimation error。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你不想覆盖这两个控制项，所以我们可以讨论如何修复。对。那么什么是欠拟合？我的意思是，如果我把欠拟合放入这个图像中，我会说欠拟合是当假设空间太小，以至于估计误差没有问题。
- en: So this gap is very small but this approximation error is big and the lack in
    our fitting is because the hypothesis space is too small or inappropriate。 We
    may be， yeah， leave that。 So given a case like in a given it is one of the。 data
    that we are not fitting or that we can't take some point because there is being
    white。 But it is not a given by any particular point。 So I want to make these
    words a little bit easier。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个差距非常小，但这个近似误差很大，我们的拟合不准确是因为假设空间太小或者不合适。我们可能是，嗯，算了，不说这个了。所以给定一个像这样的案例，在给定的情况下，它是我们没有拟合的，或者我们不能取某个点，因为它是空白的。但它并不是由任何特定的点给定的。所以我想让这些话稍微简单一些。
- en: Okay。 First let me say this。 The question is how did I know in the example from
    last lecture that we are overfitting and not underfitting？
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，首先让我说这个。问题是我怎么知道在上次讲座中的例子里我们是过拟合而不是欠拟合的？
- en: We just say that I have not found a convincing precise definition of overfitting
    and underfitting per se。 Okay。 So let's just， why don't we come back to that question？
    Maybe yeah。 Okay。 So we move on。 Good questions。 Where were we？ Optimization error。
    Right。 So。 I was saying first of all for some scenarios we can optimize to as
    find a precision as we like。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是说我还没有找到一个令人信服的、精确的过拟合和欠拟合的定义。好吧。那么我们就这样吧，为什么我们不回到这个问题？也许是的。好吧。我们继续。好问题。我们在哪里？优化误差。对了。所以。我刚才说的是，在某些情况下，我们可以将精度优化到我们喜欢的程度。
- en: In some scenarios such as neural networks and others we don't even know how
    to find the global minimum。 Nevertheless we take a stab。 We write an algorithm
    that we hope find something like a minimum or at least has some small empirical
    risk。 And we'll call it F tilde sub n。 This is the function that we actually end
    up with after we run our algorithm。 All right。 It's not the empirical risk minimized
    necessarily。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景下，比如神经网络等，我们甚至不知道如何找到全局最小值。尽管如此，我们还是尝试了。我们写了一个算法，希望能找到类似最小值的东西，或者至少有一些小的经验风险。我们称之为F
    tilde sub n。这就是我们在运行算法后最终得到的函数。好吧。这不一定是经验风险最小化的结果。
- en: It's something that actually comes out of our algorithm。 It's also in that box
    of space。 And we're hoping it's good enough。 Let's characterize how good it is。
    There's a notion of optimization error now。 It's the gap in the risk between the
    function that we get from an algorithm and the empirical risk minimizer。 Because
    we're trying to find the empirical risk minimizer we probably aren't going to
    find it exactly。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是我们算法的一个输出。它也在那个空间框架中。我们希望它足够好。让我们来描述它有多好。现在有一个优化误差的概念。它是我们从算法得到的函数与经验风险最小化器之间的风险差距。因为我们正在试图找到经验风险最小化器，所以我们可能不会完全找到它。
- en: Not quite sure how far away we are。 The gap in the risk is called optimization
    error。 Questions？
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不太确定我们离得有多远。风险之间的差距被称为优化误差。有问题吗？
- en: Okay。 Is optimization error greater than or equal to zero？ Yeah？
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。优化误差大于或等于零吗？是吗？
- en: Is it likely a little bit greater than？ Because you probably don't find the
    actual minimum。 If you run my greatest descent or something， you may be fine。
    Like when they're very close to it。 Okay。 We're probably not going to find the
    empirical risk minimizer。 I'll grant that。 We're not going to find it exactly。
    There's always a precision perhaps that we will never reach。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会稍微大一点吗？因为你可能找不到实际的最小值。如果你运行我的梯度下降之类的，可能会找到。就像它们非常接近时。好吧。我们可能找不到经验风险最小化器。我承认这一点。我们不会完全找到它。总是存在某种精度，我们可能永远无法达到。
- en: So that's true。 But with that， that implies something that's a little bit different
    from this。 You know what it would imply？ So what we're minimizing is the empirical
    risk。 So your comment that we're not finding the empirical risk minimizer would
    mean that our hat of F tilde minus our hat of F hat is greater than equal to zero。
    Is that clear？ Because we're not just fine。 It's building on the point。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实如此。但这意味着一些与此稍有不同的事情。你知道这意味着什么吗？我们正在最小化的是经验风险。所以你提到的我们没有找到经验风险最小化器，这意味着我们的F
    tilde的估计值减去我们的F hat的估计值是大于或等于零的。明白了吗？因为我们不仅仅是在找。它是在这个点上继续发展的。
- en: Which is that so in fact yes because。 Yes。 F hat n is the by definition the
    minimizer of our hat。 So yes。 If this works out at F tilde minus our hat F hat
    that would always be greater than equal to zero。 Okay。 [ Inaudible ]， We cannot
    estimate this。 What I was saying is that the whole reason we have F tilde that
    is our algorithm for getting F hat as well as we can。 So we don't necessarily
    know how to find F hat exactly。 We can either get it very。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其实是的，因为。是的。F hat n是根据定义我们估计的最小值。所以是的。如果这算出来是F tilde减去我们的F hat，那一定总是大于或等于零。好吧。[听不清]，我们无法估计这个。我想说的是，正因为如此，我们才有F
    tilde，它是我们通过算法尽可能得到F hat的方法。所以我们不一定知道如何精确地找到F hat。我们可以很接近。
- en: very close or something that we just hope is close。 One way or another that's
    what we're calling F tilde。 All right。 So although F tilde certainly has worse
    empirical risk than F hat than the ERM。 It doesn't actually necessarily have worse
    risk because， whoa， for various reasons。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 非常接近，或者说我们只是希望它接近。不管怎样，这就是我们称之为F tilde的东西。好吧。所以，尽管F tilde的经验风险确实比F hat，即ERM差，但实际上不一定风险更大，因为，呃，出于种种原因。
- en: F hat is not a risk minimizer in any way。 It's just a empirical risk minimizer。
    So just you might get lucky that F tilde might actually be better than F hat in
    terms of actual risk。 [ Inaudible ]， Well， estimation error。 At each step we have
    a goal。 Right？
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: F hat 在任何情况下都不是风险最小化器。它只是经验风险最小化器。所以，可能你会很幸运，F tilde 实际上在实际风险上比 F hat 更好。[听不清]，好吧，估计误差。在每一步我们都有一个目标。对吧？
- en: So our first biggest goal is F star。 We can't get this。 It's true for overfitting
    reasons。 Then we say， okay， let's constrain。 Then the best we can do is F f is
    this best in the class。 We can do that either because we don't have the infinite
    data set we need。 We don't have the true risk。 What can we do？ We can minimize
    the empirical risk。 That's F hat。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们第一个最大的目标是 F star。我们无法得到它。这是因为过拟合的原因。然后我们说，好吧，我们来加以限制。那么我们能做到的最好情况是 F f，这是类别中最好的。我们之所以能做到这一点，是因为我们没有我们需要的无限数据集。我们没有真实的风险。我们能做什么？我们可以最小化经验风险。这就是
    F hat。
- en: That's nice。 That's nice。 But actually to do that we'd have to be able to solve
    an optimization problem which we might not have the computing power to do。 We
    might not have an algorithm to do it。 We may not actually be able to find the
    minimizer than empirical risk either。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 那很好。那很好。但实际上，要做到这一点，我们必须能够解决一个优化问题，而我们可能没有足够的计算能力来完成。我们可能没有相关的算法。我们甚至可能无法找到经验风险的最小化器。
- en: Even though we know what the empirical risk is， computationally finding the
    minimizer F hat may also be too difficult for us。 So when we take another step
    back and we say F tilde。 F tilde is the result of the algorithm we happen to run
    which whose intention is to find F hat。 But it may not actually be exactly F hat。
    This can be negative。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们知道什么是经验风险，计算上找到最小化器 F hat 对我们来说也可能太困难。所以当我们再退一步，假设 F tilde 时，F tilde 是我们执行的算法的结果，这个算法的目的是找到
    F hat。但它可能并不完全等于 F hat。这可能是负面的。
- en: But we usually think it's probably going to be positive。 Great。 So here's this
    full access risk decomposition where we go all the way from the algorithm output
    to the base risk optimization error plus estimation error plus approximation error。
    Is this excess risk？ All right。 So we have some time before the break to go over
    an example where I hope this clears up some questions just by making it a little
    bit more concrete。 So I'm going to give you a toy example where we completely
    control the we know the data generating distribution。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们通常认为它可能会是正的。很好。接下来是这个完整的超额风险分解，我们从算法输出一直推导到基础风险优化误差加估计误差加近似误差。这个是超额风险吗？好的。在休息之前，我们还有一些时间来通过一个例子，帮助解答一些问题，使这些概念更加具体。接下来，我将给出一个玩具例子，其中我们完全控制数据生成分布。
- en: We're defining the data generating distribution so kind of like in the laboratory
    setting we can see what would happen if we actually did know we could find the
    base risk and that sort of thing。 So this is a classification problem。 Classes
    are blue and orange。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在定义数据生成分布，类似于实验室设置下，我们可以看到如果我们确实知道数据生成分布，我们可以找到基础风险等东西。那么这是一个分类问题。类标签是蓝色和橙色。
- en: Our input space is the unit square。 And the input distribution is going to be
    a uniform distribution on the square。 And you can see that this you can see the
    distribution of the class label Y。 Everything below this diagonal line is orange
    and everything above is blue。 All right。 And the base error rate。 Oh， actually，
    I didn't quite say correctly。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入空间是单位正方形。而输入分布将是正方形上的均匀分布。你可以看到类标签 Y 的分布。所有在这条对角线下方的点是橙色的，而在上方的则是蓝色的。好吧。还有基础误差率。哦，实际上，我之前说得不太准确。
- en: So the probability of it being orange below the diagonal line is 0。9。 The probability
    of it being orange above is 0。1。 So I misstated before。 So there is some error
    built into the distribution。 Some thing built into the distribution that we can
    never get rid of。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在对角线下方是橙色的概率是 0.9，在线上方是橙色的概率是 0.1。所以我之前说错了。分布中确实有一些误差。分布中有一些东西是我们永远无法消除的。
- en: So the base error rate is going to be 0。1。 Even if we predict the perfect separation。
    which is the diagonal line， we're always going to have some error。 0。1。 10% error
    rate。 That's the base risk for this problem。 Notice I still haven't talked about
    hypothesis spaces or sample data。 So we've already figured out our base risk。
    OK。 So what hypothesis space show you is I will in one slide introduce a new type
    of hypothesis space。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本错误率将是0.1。即使我们预测出完美的分离，也就是对角线，我们总是会有一些错误，0.1，即10%的错误率。这是这个问题的基本风险。请注意，我仍然没有讨论假设空间或样本数据。所以我们已经算出了基本风险。好的。那么，假设空间展示的内容是，我将在一张幻灯片中介绍一种新的假设空间。
- en: which actually I hope a lot of you have seen before。 Trees。 Binary decision
    tree。 If you don't understand every detail right now， this is fine。 It's just，
    well。 hopefully we can get it。 So decision tree works like this。 And input comes
    in。 In this case。 it's a pair X1， X2。 We start at the top。 This is a tree。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其实我希望你们很多人之前已经看过这个了。树。二叉决策树。如果现在你不理解每个细节，这没关系。只是，嗯，希望我们能理解它。决策树就是这样工作的。输入进来。在这种情况下，是一个对(X1,
    X2)。我们从顶部开始。这是一棵树。
- en: Start at the top of the tree and there is a， there's this predicate， like a
    question。 And it says。 is X1 less than or equal to T1？ If it is， we go to the
    left branch。 If it's not。 we go to the right branch and we follow the tree down。
    Next， we get here。 Suppose it was less than or equal to T1。 Is X2 less than or
    equal to T2？ If yes， we go to the left。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从树的顶部开始，这里有一个谓词，就像一个问题。它问：“X1是否小于或等于T1？”如果是，我们走左支。如果不是，我们走右支，然后继续沿着树向下走。接下来，我们来到这里。假设它小于或等于T1，X2是否小于或等于T2？如果是，我们走左边。
- en: We end up in R1。 Otherwise we go to the right。 We end up in R2。 Each of these
    things at the bottom are called leaf nodes。 And you'll notice that they correspond
    to regions of the input space。 So everything in R1 must have X1 less than or equal
    to T1。 And X2 less than or equal to T2。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终到达R1。否则我们走右边，最终到达R2。底部的每一个节点叫做叶节点。你会注意到，它们对应于输入空间的区域。所以，R1中的每个元素必须满足X1小于或等于T1，且X2小于或等于T2。
- en: So we come over here and this is the region of the input space that corresponds
    to that terminal。 node R1。 And every terminal node， every path down the tree，
    ends up corresponding to a particular。 region of this space。 The region's partition
    of the space。 And we can have a different action taken in every region。 Right？
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们来到这里，这是输入空间的区域，对应于那个终端节点R1。每个终端节点，每条树上的路径，最终都对应于这个空间的一个特定区域。这个区域划分了空间。我们可以在每个区域采取不同的动作，对吧？
- en: So how does this become a decision function？ We assign an action to every terminal
    node。 In a classification problem， we could put， this could be class zero， this
    could be class one。 Each region gets a class。 In a regression problem， each region
    gets a number。 In a generalized regression problem we're predicting distributions，
    each region gets a。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这如何变成一个决策函数呢？我们为每个终端节点分配一个动作。在分类问题中，我们可以将这设为类零，将这设为类一。每个区域得到一个类别。在回归问题中，每个区域得到一个数值。在广义回归问题中，我们预测分布，每个区域得到一个。
- en: distribution。 Each region gets a prediction that we can produce in action。 Any
    questions on binary trees？ Just rough。 The main takeaway is that it partitions
    your space into these blocks that are。 because we， only， one thing I left off
    is that in this type of tree。 the questions we ask only refer， to one coordinate
    at a time。 That's important。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分布。每个区域都会得到一个我们可以采取的动作的预测。关于二叉树有任何问题吗？只是大致的。最重要的结论是，它将你的空间划分为这些块，因为我们，唯一要补充的是，在这种类型的树中，我们提问的问题一次只涉及一个坐标。这一点很重要。
- en: So that gives it this blocky structure where all the lines are vertical or horizontal。
    The depth of the tree， the depth of this tree would be three， because there's
    in the longest。 path from root to leaf nodes。 That's how many decisions are made。
    That's the depth of the tree。 And you can imagine the deeper your tree is the
    more complicated partitions you can get。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就形成了这个块状结构，其中所有的线条都是垂直或水平的。树的深度，树的深度是三，因为从根节点到叶节点的最长路径就是做了多少次决策。那就是树的深度。你可以想象，树越深，你得到的划分就会越复杂。
- en: The more breakouts you can have。 We're going to consider the hypothesis of trees。
    So， solid F。 which is all decision trees on the unit square。 And then to consider
    subsets of that space， F sub D。 which will be， F sub D will be a set， of all decision
    trees of depth D or smaller。 So even smaller。 F D is even smaller than F， and
    we're going to consider a few of them。 F two， F three， F four， F。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以有更多的划分。我们将考虑树的假设空间。所以，固体的F，表示单位正方形上的所有决策树。然后考虑该空间的子集，F子D，F子D将是一个集合，包含所有深度为D或更小的决策树。所以更小的空间。F子D比F还要小，我们将考虑其中的一些。F2，F3，F4，F。
- en: okay。 These are， these are called nested hypothesis spaces， because one is contained
    on the next。 Any questions on this set up so far？ So here's what we're going to
    do。 We're going to look at a few different F， a few different tree hypothesis
    spaces。 And because we know the data generating distribution， we're going to be
    in this unique situation。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这些被称为嵌套的假设空间，因为一个空间包含在另一个空间中。目前对于这个设置，有什么问题吗？所以我们将要做的是，观察几个不同的F，几个不同的树假设空间。因为我们知道数据生成分布，所以我们将处于这种独特的情况。
- en: that we actually can't figure out the best in this class。 So we're going to
    be actually able to find the approximation error， exactly， which is an。 unusual
    thing。 But here we own the data generating distribution so we can。 And then we're
    going to run a algorithm， a tree algorithm， that is going to do its best。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上无法确定这个类中的最佳答案。所以我们实际上能够找到近似误差，准确地说，这是一个*不寻常的事情*。但在这里我们掌握数据生成的分布，因此我们可以做到这一点。然后我们将运行一个算法，一个树算法，来尽最大努力。
- en: to find the empirical risk minimizer。 Yeah？ Yeah， sure。 So I think intuitively
    you would agree that the best possible prediction function would be。 one that
    predicts where you have to predict either an orange or a blue in every spot。 Is
    that you predict orange anywhere below this diagonal line and blue anywhere above。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 找到经验风险最小化器。是吗？好的，当然。所以我认为从直觉上你会同意，最佳的预测函数是那种在每个位置上都需要预测橙色或蓝色的函数。也就是说，你应该在这个对角线以下的地方预测橙色，而在对角线以上的地方预测蓝色。
- en: Would you agree that that's the base optimal decision function？ In other words。
    it has the least chance of getting an error。 Yeah， so the issue is that even in
    this region。 this is only 90% orange and it's 10% below。 That's why I wrote the
    。9 and also written over here。 The probability of orange for x1 greater than x2
    is 。9。 So this will be orange 90% of the time。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你同意这是最基本的最优决策函数吗？换句话说，它是最小化错误概率的函数。是的，问题是即使在这个区域，橙色的概率也只有90%，而10%的区域是蓝色。这就是我写下0.9并且在这里写下的原因。对于x1大于x2，橙色的概率是0.9。所以这会是90%的橙色。
- en: So if we predict orange， we're going to be wrong 10% of the time。 So the base
    error rates 10%。 [ Inaudible ]， You almost can。 What you have to say is the base
    error is equal to the probability of error in your prediction。 So you have to
    also include the probability， so there's two cases。 One is that the correct answer
    is blue and you predict orange。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们预测橙色，我们将有10%的错误概率。所以基本的错误率是10%。[听不清]，你几乎可以做到。你需要说的是，基本的错误率等于你预测错误的概率。所以你还需要考虑这个概率，因此有两种情况。一种情况是正确答案是蓝色，而你预测的是橙色。
- en: But you have to add to that the probability that the correct answer is orange
    and you predict blue。 So if you write that out， that will equal 。1。 [ Inaudible
    ]， All right。 So here we started with F2。 Decision trees of depth two， MOS2。 And
    we've written down the theoretical best。 This is just by math。 Basically， there's
    no computer involved here。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你还需要加上正确答案是橙色的概率，而你预测的是蓝色。所以如果你写出这个公式，它将等于0.1。[听不清]，好的。那么我们从F2开始。深度为二的决策树，MOS2。我们已经写下了理论上的最佳答案。这仅仅是通过数学计算得出的。基本上，这里没有涉及计算机。
- en: We just see that this is the best we can possibly do。 And we compute。 you'll
    see in like different regions of the space， you know。 it's not totally orange
    or totally blue。 And if you predict， we're going to predict orange。 for instance，
    in this block。 But you're going to be right only 83% of the time。 All right。 Yes。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只看到这是我们能够做到的最佳情况。然后我们计算。你会看到，在空间的不同区域里，知道吗？它并不完全是橙色或者完全是蓝色。如果你预测，我们将会预测橙色。例如，在这个块中。但是你将只有83%的机会预测正确。好的，是的。
- en: This is using math offline。 So we just measure and calculate and because everything
    is completely unknown。 we can compute this。 Yeah。 Okay。 So it's kind of like assuming
    you had an infinite training set and an optimal algorithm that could find the
    best possible tree。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用离线数学。所以我们只需测量和计算，因为一切都是完全未知的。我们可以计算这个。是的。好的。所以这就像是假设你有一个无限大的训练集和一个能够找到最佳树的最优算法。
- en: But in fact， we just figured it out。 You can see there's certain symmetry。 This
    is the separating decision boundary that's optimal in this case。 Under the constraint
    that we're using trees of depth two。 All right。 So what happens when we go to
    depth three？ Oh。 So what's the--so the risk of this boundary is 。2。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上，我们刚刚搞明白了。你可以看到有一定的对称性。这是这个情况下最优的分隔决策边界。在我们使用深度为二的树的约束下。好了。那么当我们进入深度三时会发生什么？哦。那么这个边界的风险是
    0.2。
- en: 20% error rate。 So while the best possible was the diagonal line which got us
    10%。 when we do this square line that's an output of it。 So if we get an output
    of the decision tree of depth two， we get 20% error。 So twice is twice the risk。
    So the approximation error is the gap between the risk of this function and the
    best possible。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 20%的误差率。所以最佳可能的结果是对角线，得到了 10%的误差。当我们做这个方形线时，它是一个输出。所以如果我们得到一个深度为二的决策树的输出，我们得到
    20%的误差。所以是两倍的风险。所以近似误差就是这个函数的风险与最佳可能的风险之间的差距。
- en: So that's 。1。 Let's go to trees of depth three。 Again， we can calculate the
    best possible。 We see that the risk of that is 15% 。15。 So it's a little better。
    And F4 gets a little bit more complicated。 The risk is 。125。 You can see the approximation
    error keeps going down to 。1， 。05， 。025。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以是 0.1。我们进入深度为三的树。再次，我们可以计算出最佳可能的结果。我们看到风险是 15%，0.15。所以稍微好一些。F4 变得有些复杂，风险是
    0.125。你可以看到近似误差持续下降，直到 0.1，0.05，0.025。
- en: So these are all theoretical bests。 Now let's see what happens when we actually
    estimate these things from data。 So now I'm taking data。 I'm feeding it into my
    tree algorithm。 I use R。 And seeing what it comes up with， I say only give me
    trees that are at depth in most three。 Give me the best you can find。 And so for
    F3， you can't wait with this separating boundary。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些都是理论上的最佳情况。现在让我们看看当我们实际从数据中估算这些值时会发生什么。现在我拿到数据，将它输入到我的树算法中。我使用 R。看看它得出什么结果，我只要求它给我深度最多为三的树。给我你能找到的最佳树。因此对于
    F3，你不能等待这个分隔边界。
- en: Not so great。 It has， we look at the error。 One thing to note is that we can
    always figure out this。 the true risk of a F tilde by， I mean， we could either
    do it by calculation。 And yes。 or we can just keep sampling more and more points，
    make a test that's， you know。 a million large and estimate the risk empirically。
    So this number should be as accurate as we want。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不是很好。我们看一下误差。需要注意的一点是，我们总是能算出 F 反的真实风险。我是说，我们可以通过计算得出。是的。或者我们可以不断地抽取更多的样本，做一个大的测试，像是有一百万个样本，然后从经验上估算风险。所以这个数字应该是我们想要的任何精度。
- en: But why did I write plus or minus 。04 when I wrote the risk of F tilde？ [ Inaudible
    ]， Okay。 it's yes。 It's like standard deviation。 And why would there be standard
    deviation attached to that number？
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么我在写 F 反的风险时写了加减 0.04？[听不清楚]，好的。是的。它就像标准差。那么为什么这个数字会附带标准差？
- en: '[ Inaudible ]， Yes， exactly。 So to compute， so we''re taking different samples
    each time。 To compute F tilde， we draw a training set。 And then we run our algorithm
    on the training set。 Training sets random。 So F tilde is random， so its risk is
    a random number。 And if we were to repeat multiple times with different training
    sets each time。'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清楚]，是的，完全正确。所以为了计算，我们每次都在取不同的样本。为了计算 F 反， 我们抽取一个训练集。然后我们在训练集上运行我们的算法。训练集是随机的。所以
    F 反是随机的，它的风险是一个随机数。如果我们每次使用不同的训练集重复多次的话。'
- en: we're going to get some variation in the risk of F tilde。 And that's what's
    being captured by that standard error。 Yes， exactly。 Okay。 [ Inaudible ]。 How
    can I compute the optimization？ I cannot。 What we can do -- all right。 so the
    question is how do we compute optimization error？ We can't。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会在 F 反的风险中看到一些波动。这就是那个标准误差所捕捉的内容。是的，完全正确。好的。[听不清楚]。我如何计算优化？我做不到。我们能做的是——好的，问题是我们如何计算优化误差？我们做不到。
- en: What we can compute is the error of F tilde。 Because we get the error of F tilde。
    the risk of F tilde。 But we also know the approximation error。 So what we can
    do is we know estimation error plus optimization error plus approximation error。
    is equal to the excess risk。 And so we can solve the estimation error plus the
    optimization error。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能计算的是F tilde的误差。因为我们得到了F tilde的误差，F tilde的风险。但是我们也知道近似误差。所以我们能做的是，我们知道估计误差加上优化误差加上近似误差，等于过度风险。因此我们可以解决估计误差加上优化误差的问题。
- en: Don't know how to separate the two。 That we can't do。 What would we need to
    separate estimation error from approximation error？ Rather。 estimation error from
    optimization error。 I think -- say again？ Yes。 If we had an algorithm that could
    guarantee arbitrarily -- trees that are arbitrarily close to the。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不知道如何区分这两者。我们做不到。我们需要什么来区分估计误差和近似误差？或者说，估计误差和优化误差？我想 —— 再说一遍？是的。如果我们有一个算法，可以保证得到任意接近
    —— 任意接近的树。
- en: empirical risk minimizer， then we would know we had a -- then we would know
    what the actual。 estimation error is and then what's left is the optimization
    error。 Exactly。 Right。 [ Inaudible ]。 Right。 We cannot calculate the estimation
    error because I do not know how to find the best possible。 tree in the class。
    I don't know how to find the empirical risk minimizer。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 经验风险最小化器，那么我们就知道我们有一个 -- 然后我们会知道实际的估计误差是什么，剩下的就是优化误差。没错。对。[听不清]。对。我们无法计算估计误差，因为我不知道如何找到类中最好的树。我不知道如何找到经验风险最小化器。
- en: It's a computationally difficult problem。 Okay。 So as we get more -- all right。
    So this was for F three with a sample of size thousand， twenty-four。 This was
    F four。 so a more complicated space。 You can see the -- the boundary is getting
    a little more interesting。 And the error is going down。 And here's F six and it
    gets more complicated。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个计算上困难的问题。好的。所以随着我们获得更多 —— 好的。这个是对于F三，样本大小为一千，二十四。这个是F四。所以这是一个更复杂的空间。你可以看到
    —— 边界变得有点更有趣了。误差在下降。这里是F六，它变得更复杂了。
- en: And F eight gets much now starting to get a little bit crazy。 Okay。 So sticking
    with decision trees of depth eight。 Now let's see what happens when we amp up
    the sample size。 Yeah。 Right now there's one thousand， twenty-four。 There's no
    -- how many are there？
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: F八现在变得更加疯狂。好的。所以继续使用深度为八的决策树。现在让我们看看当我们增加样本量时会发生什么。是的。现在是1,024个样本。没有 —— 有多少个？
- en: We define the data generating distribution so we can generate infinitely many。
    We can generate as many as we want。 So ten， twenty-four gives this and now what
    you hope is as we increase the sample size。 the estimation plus approximation
    error goes down。 So we're looking at that number and we're looking at these which
    should look nicer。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义数据生成分布，因此我们可以生成无限多个样本。我们可以生成任意多个。所以，十、二十四给出了这个，现在你希望的是，随着样本量的增加，估计误差加上近似误差会下降。所以我们在看这个数字，并且看这些，应该看起来更好。
- en: So we're looking at the number of data。 So we're looking at the number of data。
    And we're looking at the number of data。 And we're looking at the number of data。
    And we're looking at the number of data。 And we're looking at the number of data。
    And we're looking at the number of data。 And we're looking at the number of data。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在看数据的数量。我们在看数据的数量。我们在看数据的数量。我们在看数据的数量。我们在看数据的数量。我们在看数据的数量。我们在看数据的数量。
- en: And we're looking at the number of data。 And these are fairly typical。 Let's
    break this down a bit。 So x-axis we have tree depth。 As we increase the depth
    of the trees。 things are getting more complicated。 So we have left to right。 The
    Bayes risk is a constant at point one， ten percent。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在看数据的数量。这些是相当典型的。我们稍微分解一下。横轴是树的深度。随着树的深度增加，事情变得更复杂。所以我们从左到右，贝叶斯风险是一个常数，点数是0.1，即10%。
- en: The green line is the minimum risk achievable by trees of the given depth。 So
    you can see as the depth increases we can get closer and closer to the Bayes risk。
    This other line is the empirical risk minimizer。 And this has a very--this is
    kind of a classic shape。 You see this a lot， a U shape， where the empirical risk
    minimizer with a very simple tree depth。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色线是给定深度的树能够实现的最小风险。你可以看到，随着深度的增加，我们可以越来越接近贝叶斯风险。另一条线是经验风险最小化器。这有一个非常 —— 这是一种经典的形状。你会经常看到这种U形，经验风险最小化器使用非常简单的树深度。
- en: very， very simple hypothesis space isn't very good。 Initially。 as the hypothesis
    space gets more complicated and can predict functions with more， you know。 subtleties
    in them， the risk decreases。 But at some point it starts getting worse again when
    the trees get too complicated because we start overfitting。 And that's what's
    kind of being demonstrated here。 So what's increasing here？
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 非常非常简单的假设空间并不好。最初，随着假设空间变得更复杂，能够预测更多含有微妙差异的函数，风险会减少。但在某个时刻，随着树变得过于复杂，它开始变得更糟，因为我们开始过拟合。这就是这里展示的内容。那么这里在增加的是什么？
- en: The estimation error plus the approximation error is growing more quickly than
    the approximation error is shrinking。 So that is some question about the balance
    between the two。 Let's look at that in terms of the error pieces。 So now I've
    drawn--so the excess risk is the green。 The red is the approximation error which
    keeps going down。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 估计误差加上近似误差的增长速度比近似误差的缩小速度要快。所以这是关于两者之间平衡的一些问题。让我们从误差部分来看看这个。现在我画了--过剩风险是绿色的。红色是近似误差，它一直在减少。
- en: And then this blue is the estimation error plus the optimization error which
    kind of keeps going up。 And we talked about why some F-confidence fans and some
    don't。 Yeah， question？
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这个蓝色的是估计误差加上优化误差，这个误差会不断增大。我们已经讨论过为什么一些F置信区间有效，而一些则无效。嗯，问题？
- en: '>> If your independent variable for discrete， then you would be able to define
    the--。 would you be able to find an optimum with the optimization error just by
    writing the writing on the tree？'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 如果你的自变量是离散的，那么你能定义--。你能通过在树上写下优化误差找到最优解吗？'
- en: '>> Sure。 If you had the time to wait， you could find the optimization error。
    You could find the empirical risk minimizer exactly as long as there''s a finite
    set of decision functions to search over。 >> Yeah。 >> Another question before
    we take a break？ Yeah。 [ Inaudible ]。 So the question is if we can''t distinguish
    between estimation error and optimization error。'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 当然。如果你有时间等待，你可以找到优化误差。你可以精确地找到经验风险最小化器，只要决策函数的集合是有限的，进行搜索就可以。>> 是的。>> 休息前的另一个问题？嗯？[听不清]。所以问题是如果我们无法区分估计误差和优化误差。'
- en: why did I even mention them？ And it's not that we never can。 It was a trade-off。
    So for linear regression or rigid regression， you can drive the optimization error
    century to zero。 So there you know。 In this case， we couldn't， but then that hot
    space was interesting and we could figure out the exact minimizer。 One question？
    Yeah？ [ Inaudible ]， We can in this scenario， we can figure out the excess risk。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我为什么提到它们？并不是说我们永远不能做到。那是一个权衡。所以对于线性回归或刚性回归，你可以将优化误差降到零。所以，你知道的。在这种情况下，我们做不到，但那个热空间很有趣，我们可以找出确切的最小化器。有什么问题吗？嗯？[听不清]，在这种情况下，我们可以找出过剩风险。
- en: we can figure out the approximation error because we know the data-gettering
    distribution。 Okay。 In general， yeah？ [ Inaudible ]， Okay。 So the question is
    what can we do with the validation set？
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以找出近似误差，因为我们知道数据获取分布。好的。一般来说，嗯？[听不清]，好的。那么问题是我们可以如何使用验证集？
- en: Does that enter in here， right？ [ Inaudible ]， Okay。 So another fresh data set。
    not the validation set。 How does that help us？ And let's even say it's a huge
    data set。 So what a fresh， huge data set lets us do is get really good approximations
    to the risk。 Because if。 remember what we said last time by law of large numbers。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这是否会影响到这里，对吧？[听不清]，好的。那么另一个新的数据集，不是验证集。那对我们有什么帮助？假设它是一个巨大的数据集。那么一个新的、大的数据集让我们做的事情是得到风险的非常好的近似。因为如果记得我们上次所说的，大数法则。
- en: as the size of our sample increases to infinity， the imputeable risk of a particular
    function converges to the true risk。 So with a big enough extra data set， a big
    enough test set or whatever you want to call it。 we can get very precise estimates
    of the risk of any single function that we want。 However。 that's not what we need
    to find the approximation error。 To find the approximation error。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的样本量增大到无限时，特定函数的可估计风险会收敛于真实风险。所以有了足够大的额外数据集，足够大的测试集，或者你想怎么称呼它，我们可以对我们想要的任何单一函数的风险进行非常精确的估计。然而，这并不是我们找到近似误差所需要的。要找到近似误差。
- en: what we need to do is find the minimizer of the risk。 It's a harder problem。
    So we can get a good approximation to the risk of any particular function。 But
    to get the approximation error， you're going to have to know the true risk of
    minimizer。 which is another story。 Question more？ Alright， let's take a break。
    And let's come back in。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是找到风险的最小化器。这是一个更难的问题。所以我们可以得到某个特定函数的风险的良好近似。但是要得到近似误差，你必须知道最小化器的真实风险。这是另一个话题。还有问题吗？好了，我们休息一下。然后我们再回来。
- en: I say it's in five minutes because we're running a little late。 So around seven，
    ten， seven。 thirteen。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我说是五分钟后，因为我们有点晚了。所以大约是七点十分，七点十三分。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_10.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_10.png)'
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_11.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_11.png)'
- en: '>> A group of questions last round。 Let''s see what we can get through round
    two。 So we''re going to talk about L1 and L2 regularization。'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 上一轮有一组问题。让我们看看第二轮我们能得到什么结果。我们将讨论L1和L2正则化。'
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_13.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_13.png)'
- en: These are really the work courses of data science， the Lasso and the Ridge regression。
    These are probably the --， If we had one set of algorithms to learn， this would
    be Ridge and Lasso。 So， taking off an Ivanov regularization， this is the topic
    of this set。 Hypothesis spaces。 we're talking about -- we've spoken vaguely about
    larger and smaller hypothesis spaces。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些真的是数据科学的核心课程，Lasso和Ridge回归。这可能是——如果我们只需学习一套算法，那就是Ridge和Lasso。所以，接下来的伊凡诺夫正则化就是这一节的主题。假设空间。我们之前曾模糊地谈到过更大和更小的假设空间。
- en: And in practice， it's convenient to work with nested sequences of hypothesis
    spaces like trees of certain depth。 like we worked on before。 And so we're going
    to talk about one particular way to get nested sequences。 It involves defining
    a complexity measure on your decision functions。 So for trees we have one。 right？
    Depth of the tree。 That was a complexity measure。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，处理像树这种某种深度的嵌套假设空间是很方便的，就像我们之前做过的那样。所以我们将讨论一种特定的方式来得到嵌套序列。它涉及在决策函数上定义一个复杂度度量。所以对于树来说，我们有一个度量，对吧？树的深度。这就是一个复杂度度量。
- en: Last time we were talking about linear functions。 And you can kind of think
    how many features you have。 the more features you have， the more complex prediction
    function you can get。 That would be a measure of complexity。 Polynomials。 the
    degree of the polynomial could be a measure of complexity。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我们讨论了线性函数。你可以想象一下，特征越多，预测函数就越复杂。这可以作为复杂性的一个度量。多项式。多项式的次数也可以作为复杂性的一个度量。
- en: Last time I'd say stuff about smoothness。 So here's a precise way to define
    smoothness。 If a thing is twice differentiable， you can integrate the second derivative
    of square。 This is some kind of measure of how quickly it changes。 So all these
    are ways to say more or less complicated for a single function。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我提到了平滑性。所以这里有一种精确的方式来定义平滑性。如果一个函数是二阶可微的，你可以积分它的二阶导数的平方。这是衡量它变化速度的一种方式。所以所有这些都是用来描述一个单一函数更复杂或更简单的方式。
- en: There's no ground truth of like what's really more or less complicated。 We have
    different ways to define it。 So these are some options。 Okay。 So for linear models
    in particular， we're moving towards sparsity with the L1。 So we could also define
    like L0 complexity， which is even more direct measure。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 没有像什么是更复杂或更简单的真实标准。我们有不同的方法来定义它。所以这些是一些选项。好的。那么对于线性模型，我们正朝着用L1趋向稀疏性。我们也可以定义L0复杂度，它是一个更直接的度量。
- en: It's the number of non-zero coefficients。 So straight up how many non-zero coefficients
    are allowed to be a measure of complexity。 The last little complexity that we're
    going to talk about later is if you have a linear prediction function。 the sum
    of the absolute values of the coefficients。 The bigger that is， the more complex。
    the more erratic functions you can produce in a certain sense。 And there's another
    one。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非零系数的数量。所以，直接来说，允许多少个非零系数就是复杂度的一个度量。稍后我们要讨论的最后一个复杂度是，如果你有一个线性预测函数，系数绝对值之和。它越大，越复杂。在某种意义上，你可以产生更不规则的函数。还有另一个度量。
- en: ridge complexity。 This is where you look at the sum of squares of the coefficients。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归复杂度。这是你查看系数的平方和时得到的度量。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_15.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_15.png)'
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_16.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_16.png)'
- en: And the larger that sum of squares is for your linear function， the more， again。
    the more bouncy or erratic types of functions you can produce。 This is what you're
    using right now in your homework from ridge regression。 This is the penalty。 It's
    the L2 penalty。 All right。 So given a complexity measure。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你线性函数的平方和越大，你就能产生越多、更具波动性或不稳定的类型的函数。这就是你现在在作业中使用的岭回归方法。这就是惩罚项，就是 L2 惩罚项。好的。那么，给定一个复杂度度量。
- en: we can define a nested sequence of hypothesis spaces in a pretty simple way。
    So we start with a particular hypothesis space F and a complexity measure that
    maps functions in F to some score。 some number describing how complex they are。
    And then we can define something that's like a ball。 I'm just going to call it
    FR， F sub R。 Is the subset of F containing all decision functions that have complexity
    R or smaller。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以一种非常简单的方式定义一个嵌套的假设空间序列。所以我们从一个特定的假设空间 F 和一个将 F 中的函数映射到某个分数的复杂度度量开始，分数表示它们有多复杂。然后我们可以定义类似球形的东西。我将其称为
    FR，F 子 R。它是 F 的一个子集，包含所有复杂度为 R 或更小的决策函数。
- en: That's what we're going to find is FR。 And if this omega。 this complexity measure
    omega were a norm on this function space。 then this will quite literally be a
    ball of radius R in the space。 When I picture these types of things， I am picturing
    a ball in a function space and a bigger ball in a smaller ball。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们要找到的 FR。如果这个 omega，复杂度度量 omega 是这个函数空间上的一个范数，那么这将字面上是一个半径为 R 的球体。在我想象这些东西时，我会把它们想象成函数空间中的一个球和一个更大的球与更小的球。
- en: Anyway， yes？ [ Inaudible ]， Oh， sorry。 Yeah， I haven't used this yet。 So omega
    maps from F into R greater than zero。 That's my notation for real numbers that
    are greater than or equal to zero。 So we can define some complexity levels。 And
    for each one， there's a corresponding hypothesis space。 And as that complexity
    level increases， the hypothesis spaces grow in a nested fashion。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，是的？[听不清]，哦，抱歉。对了，我还没用过这个。所以 omega 从 F 映射到大于零的 R。那是我表示大于或等于零的实数的符号。这样我们可以定义一些复杂度水平。对于每个水平，都有一个对应的假设空间。而随着复杂度水平的增加，假设空间以嵌套的方式增长。
- en: This is how we can get a nested sequence of hypothesis spaces from a complexity
    measure and a series of points。 All right。 So， let's bring this to our constrained
    empirical risk minimization。 So now。 we have a complexity measure and we can talk
    about minimizing the empirical risk over functions F in our hypothesis space F。
    And then we say on top of that， let's restrict even more and say that the F's
    we're considering。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们如何从一个复杂度度量和一系列点中得到一个嵌套的假设空间序列。好的。那么，让我们把这个带入到有约束的经验风险最小化中。所以现在，我们有了一个复杂度度量，并且我们可以谈论在假设空间
    F 中对函数 F 进行经验风险最小化。然后我们在此基础上说，进一步限制并表示我们正在考虑的 F。
- en: this F's team means such that， so an additional constraint。 On top of that。
    the complexity of F should be bounded by R。 So we started with a big space F。
    First。 there's the whole space of all functions。 Then there's a space F， I have
    about the space。 And then we're restricting the hypothesis space to a smaller
    hypothesis space F sub R of functions in F that have complexity in the most part。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 F 的团队意味着这样的约束，额外的约束。在此基础上，F 的复杂度应该被限制在 R 以内。所以我们从一个大的空间 F 开始。首先，整个所有函数的空间。然后是一个空间
    F，我大致描述了这个空间。接着我们将假设空间限制到一个更小的假设空间 F 子 R，包含复杂度在大多数情况下为 R 的函数。
- en: Cool。 And then we can vary R using validation data or cross validation。 however
    you like to choose R and find the hypothesis space of the right size。 And， okay。
    So that's one form。 This form has a name。 It's called Ivanov regularization。 There's
    going to be another form。 It's called TIKANov regularization。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷。然后我们可以通过验证数据或交叉验证来变化 R，选择你喜欢的方式来确定 R，并找到合适大小的假设空间。好的，这是其中的一种形式。这个形式有个名字，叫做
    Ivanov 正则化。还有另一种形式，叫做 TIKANov 正则化。
- en: There's a little bit different， but it's the same idea。 Here we're minimizing
    again over the whole space F。 But this time。 there's no hard constraint on the
    complexity。 There's no bound the complexity needs to be less than R。 Instead，
    what we do is we add into the objective function itself a penalty for how complex
    it is。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点不同，但基本思路是一样的。在这里，我们再次对整个空间 F 进行最小化。但这一次，复杂度没有硬性约束。没有要求复杂度必须小于 R。相反，我们做的是将复杂度的惩罚项直接加入目标函数中。
- en: So this is what you actually did in your homework。 This is the actual form from
    the homework。 So we have the empirical risk plus lambda， some coefficient， times
    the complexity of F。 And as we send lambda very large， this is going to be imposing
    a very aggressive complexity constraint。 And it's going to be driving the function
    that we choose to have very low complexity。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你们在作业中实际做的事情。这是作业中的实际形式。所以我们有经验风险加上 lambda，某个系数，乘以 F 的复杂度。当我们将 lambda 调得非常大时，这将强制实施一个非常激进的复杂度约束，并且它会驱使我们选择的函数具有非常低的复杂度。
- en: And when lambda goes to zero， this complexity penalty doesn't affect our empirical
    risk minimization at all。 Is that clear？ So for very large lambda， we're penalizing
    the complexity heavily。 which will make the minimizer of the entire objective
    function have relatively lower complexity。 Is that clear？ Yeah？ Why do we need
    to constrain the hypothesis space F additionally？
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当 lambda 趋近于零时，这个复杂度惩罚对我们的经验风险最小化没有任何影响。明白吗？所以，对于非常大的 lambda，我们正在严重惩罚复杂度，这会使得整个目标函数的最小化器具有相对较低的复杂度。明白吗？是吗？为什么我们还需要进一步约束假设空间
    F？
- en: We already constrained it to get to F。 Why are we constraining it more？ Is that
    the question？ No。 I'm being quite confused about the term。 Is that a problem you're
    just a example of？
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将其约束到 F。为什么我们要更多地约束它？这是问题吗？不，我对这个术语感到有些困惑。这是你们仅仅作为一个例子提出来的问题吗？
- en: So in the previous example with the trees， you're asking。 we had hypothesis
    spaces of varying sizes that got bigger。 And it's just a way to。 it's kind of
    a way to quickly describe and try out and trade off between many different hypothesis
    spaces in an organized fashion。 So at least in this， it's even more clear here。
    As we change R。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在之前的树的例子中，你在问，我们有不同大小的假设空间，它们变得更大。这只是一个方法。它有点像一种快速描述和尝试并权衡多个不同假设空间的方式，且以有组织的方式进行。所以至少在这个例子中，更加清楚了。当我们改变
    R 时。
- en: we're essentially getting different hypothesis spaces。 They all happen to be
    subsets at the same space F。 So this is a way to try out multiple different hypothesis
    spaces。 Remember。 one of the jobs of the data scientists is to choose the hypothesis
    space。 Okay。 So， yeah？
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本质上得到了不同的假设空间。它们恰好都是空间 F 的子集。所以，这是一种尝试多个不同假设空间的方法。记住，数据科学家的工作之一是选择假设空间。好，明白吗？
- en: '[ Inaudible question ]， So the question is if we fix lambda， are we， this is
    a little rougher now。 If we， in this scenario， you know， we have a hypothesis
    space F and we have this complexity path。 If we had this complexity penalty， are
    we really， does it correspond to a subset of the hypothesis space F actually？'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清的问题]，所以问题是，如果我们固定 lambda，是否，我们现在的情况有些粗糙。如果我们在这种情况下，你知道，我们有一个假设空间 F 并且我们有这个复杂度路径。如果我们有这个复杂度惩罚，它是否真正对应于假设空间
    F 的一个子集？'
- en: And what we'll claim later and you'll can prove in homework three is that there's
    an exact equivalent under some conditions between these two。 Yeah？ [ Inaudible
    question ]， Yes。 That's right。 So， let me come to that in a second。 All right。
    So。 yes？ [ Inaudible question ]， I don't know what the question was。 Why what？
    [ Inaudible question ]。 Oh， why restrict it more？ Oh。 What， I was， so I was kind
    of saying that this is more a way to give us easy access to multiple hypothesis
    spaces。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将要声明，并且你们可以在第三次作业中证明，在某些条件下，这两者是完全等价的。是吗？[听不清的问题]，是的。没错。好，让我在一会儿解释。好，明白吗？[听不清的问题]，我不知道问题是什么。为什么什么？[听不清的问题]。哦，为什么要更多约束它？哦。我是说，这更像是一种方式，让我们能够轻松访问多个假设空间。
- en: and try out multiple hypothesis spaces that happen to be nested in a convenient
    way。 So， like。 a linear function with a very large feature space may still be
    too complex and may still over fit。 even though it's a great restriction on all
    functions。 So we restrict it more in various ways。 We can constrain the one norm
    of the coefficients or the L2 norm。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 并且尝试多个恰好以方便的方式嵌套在一起的假设空间。所以，比如，一个具有非常大特征空间的线性函数可能仍然过于复杂，可能仍然会过拟合，即使它对所有函数来说是一个很好的限制。因此，我们以各种方式进一步约束它。我们可以约束系数的
    L1 范数或 L2 范数。
- en: We can constrain it very aggressively or we can constrain it lightly。 And each
    of these things corresponds to a different hypothesis space that might have better
    trade-offs between overfitting and underfitting or approximation error and estimation
    error。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以非常强烈地约束它，或者我们可以轻微地约束它。每一个这样的操作都对应着一个不同的假设空间，这些假设空间可能在过拟合与欠拟合，或者近似误差与估计误差之间有更好的权衡。
- en: So， it's all about searching for the right hypothesis space。 It's not like you
    just choose one and you're done。 You have to try multiple ones。 And this is a
    way to get there。 This is a very explicit way to look at trying multiple hypothesis
    spaces。 Every R corresponds to a different one。 And this is a less explicit way。
    Yes？
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一切都在于寻找正确的假设空间。这不像你选择一个就完事了。你必须尝试多个假设空间。这是一种非常明确的方法来尝试多个假设空间。每一个 R 对应一个不同的空间。这是一个不太明确的方法。是吗？
- en: '[ Inaudible question ]， The question is， is inserting more constraints allowing
    us to generalize more？'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清的问题]，问题是，插入更多约束会让我们更好地推广吗？'
- en: '[ Inaudible question ]， So， indeed， the ultimate goal is to generalize。 Generalization
    means basically risk。 Like， how do you perform on new examples？'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清的问题]，所以，的确，最终目标是推广。推广基本上意味着风险。比如，你在新样本上表现如何？'
- en: If you draw a random example from the data generation distribution， how do you
    expect to perform？
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从数据生成分布中随机抽取一个样本，你预期的表现如何？
- en: That's risk。 That's what we mean by generalizing well。 It means having low risk。
    And that is the ultimate goal is to find a function space。 such that when we run
    our optimization algorithm and pull a function from that space。 it has low risk。
    It generalizes well。 [ Inaudible question ]， Go on。 [ Inaudible question ]， Okay。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是风险。这就是我们所说的良好推广。它意味着低风险。而最终的目标是找到一个函数空间，确保当我们运行优化算法并从该空间中提取一个函数时，它具有低风险，能够良好推广。[听不清的问题]，继续。[听不清的问题]，好的。
- en: So， you're saying that if you vary R， you're eventually just going to make R
    really large like the whole space F？
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你的意思是，如果你改变 R，最终你只是把 R 增大到像整个空间 F 那样大吗？
- en: '[ Inaudible question ]， Yes， right。 But if we didn''t have this constraint，
    it would be much --。 [ Inaudible question ]， I think what you''re saying is writing
    less than or equal to instead of equal to is in much difference。 Is that what
    you''re saying？ [ Inaudible question ]， Yeah， afterwards。 I''m not sure。 [ Inaudible
    question ]， All right。 So， what I wanted to say in answer to the question here
    was。'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清的问题]，是的，没错。但如果我们没有这个约束，它会变得更... [听不清的问题]，我认为你说的是写“小于等于”而不是“等于”没什么区别。你是这个意思吗？[听不清的问题]，嗯，之后我不确定。[听不清的问题]，好的。所以，我想回答这里的问题。'
- en: is there a difference between these two forms of regularization。 the Ivanov
    and the Tikunov regularization？ And they're different in form。 but in many general
    settings， they're actually equivalent。 What does that mean？
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种正则化形式之间有区别吗？Ivanov 正则化和 Tikunov 正则化？它们在形式上不同，但在许多常见的设置下，它们实际上是等效的。这意味着什么呢？
- en: It means that any F that you can get from some setting of R here。 you can also
    get from some setting of lambda here。 If you have the same loss and the same complexity
    of penalties and vice versa。 If there's a particular setting of lambda that gave
    you a particular F。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你可以从这里某个 R 设置中得到的任何 F，也可以从这里某个 lambda 设置中得到。如果你有相同的损失和相同的复杂度惩罚，反之亦然。如果有一个特定的
    lambda 设置给你一个特定的 F。
- en: then there's another corresponding setting of R in this framework that would
    give you the same F。 Setting all else stays the same。 And so， I think it's all
    I can say about that now。 but so conceptually， you are free and right to -- if
    you see this。 you can picture a ball in the space of some size， even though I'm
    practiced。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这个框架中，还有另一个相应的 R 设置，它会给你相同的 F。其他设置保持不变。因此，我想现在我能说的就这些。但从概念上讲，你是自由的，正确的——如果你看到这个，你可以想象一个某种大小的球体，尽管我已经习惯了。
- en: This isn't constraining to a ball like the Ivanov form is。 And why might we
    prefer this form？
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这不像 Ivanov 形式那样限制于一个球体。那么我们为什么可能更倾向于这种形式呢？
- en: It's because it's not a constrain's minimization。 We're constraining to F。 but
    F could be a very pleasant space to work with， like all linear functions， for
    instance。 And it's kind of an unconstrained optimization within F。 All right。
    So the proof of the equivalence between the two is actually a consequence of this
    Lagrangian duality theory that we're going to talk about next week。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为它不是约束的最小化。我们是在对 F 进行约束，但 F 可能是一个非常适合工作的空间，比如所有线性函数。例如，这是一个在 F 内部的无约束优化。好的。那么这两者等效的证明实际上是下周我们将讨论的拉格朗日对偶理论的一个结果。
- en: All right。 So now let's get into the last row in the ridge regression。 So we
    have our linear least squares model。 Our function space is linear functions。 parameterized
    by W， the coefficients， W vector。 And we'll start with empirical risk minimization
    where we just minimize this average square loss of the prediction over the data。
    And why isn't this enough？ Because even linear models can overfit sometimes。 So
    for instance。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么现在我们来看看岭回归中的最后一行。所以我们有线性最小二乘模型。我们的函数空间是由W参数化的线性函数，即系数W向量。我们从经验风险最小化开始，在这里我们只是最小化数据上预测的平均平方损失。为什么这还不够？因为即使是线性模型有时也可能会发生过拟合。例如。
- en: if your feature space is much larger than your number of samples。 which happens
    in natural language processing， for instance。 where you could have millions of
    features。 And even with linear prediction functions。 you can overfit。 So we need
    to do something to constrain the optimization more。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的特征空间远大于样本数。例如在自然语言处理领域，就可能发生这种情况。你可能会有数百万个特征，即使是线性预测函数，也可能会发生过拟合。所以我们需要做一些事情来约束优化过程。
- en: So ridge regression is， I'd say it's like the number one workhorse of data science
    when you have just the first thing you can try for most problems。 And let me just
    break down the pieces。 There's this empirical risk piece。 And this new piece is
    the complexity measure times lambda。 The complexity measure here is the L2 norm
    of the coefficients。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所以岭回归，我会说它是数据科学中的头号工具，通常是面对大多数问题时可以尝试的第一个方法。让我先分解一下这些部分。这里有一个经验风险部分。这个新部分是复杂度度量乘以lambda。这里的复杂度度量是系数的L2范数。
- en: which is the sum of the squares of the coefficients。 which again is exactly
    what you've seen from the homework。 And then the corresponding Ivanov form。 which
    is the other way to phrase these types of problems。 would be we minimize the empirical
    risk subject to an additional constraint that the sum of the squares of the coefficients
    is less than or equal to r。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 那么就是系数的平方和，这正是你从作业中看到的内容。然后是相应的伊万诺夫形式，这是描述这类问题的另一种方式。就是说，我们最小化经验风险，同时加上一个额外的约束条件：系数的平方和小于或等于r。
- en: So a hard constraint on the sum of squares instead of kind of a soft penalty
    type of condition。 And we can show that there's equal ones there。 So this is an
    interesting plot that's kind of common for these types of things。 This is called
    a regularization paths。 And let me go first go back here to。 let's talk about
    this bottom expression， the Ivanov form。 Now， when r is very small。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个关于平方和的硬约束，而不是一种软惩罚类型的条件。我们可以证明，在那里存在相等的值。所以这是一个有趣的图，通常在这类问题中都会看到。这个叫做正则化路径。让我先回到这里，谈谈这个底部表达式，伊万诺夫形式。现在，当r非常小的时候。
- en: when we choose a small value far close to 0， what you're going to end up with
    is a parameter vector w where all the coefficients are very close to 0 because
    the sum of squares has to equal something very small。 So in that case， the coefficients
    are going to be very small。 And then the opposite extreme。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们选择一个接近0的小值时，你最终会得到一个参数向量w，其中所有的系数都非常接近0，因为平方和必须等于一个非常小的值。所以在这种情况下，系数会非常小。然后是另一个极端。
- en: if r were infinity， then what is your w going to end up at？ Yeah？ If r is infinity，
    right。 which is the least square solution。 Exactly。 So this is what we're seeing
    here。 On the left side of the graph， I'll tell you what the x-axis is in a moment。
    On the left side of the graph is what happens when we have regularization。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果r是无穷大，那么你的w最终会是什么样子？对吗？如果r是无穷大，对吧。这就是最小二乘解。没错。这就是我们在这里看到的。在图表的左侧，我稍后会告诉你x轴代表什么。在图表的左侧是我们有正则化时发生的情况。
- en: very strong regularization， which is like the equivalent of r being very close
    to 0。 And each of these lines is the value of the coefficient when we solve it
    for varying levels。 varying amounts of regularization。 On the left， we have very
    aggressive regularization。 so strong in fact that all the coefficients are 0。
    And as we regularize less。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 非常强的正则化，就像r非常接近0的情况一样。每一条线代表在不同正则化强度下求解出的系数值。左边是非常激进的正则化，实际上强到所有系数都为0。随着我们减小正则化强度。
- en: the coefficients get larger than 0 and all the way to the right side of the
    graph。 what we have is the ordinary least square solution where we have no regularization，
    no penalty。 So what's the x-axis？ The x-axis is the ratio between the L2 norm
    of the coefficients and of the regularized solution and the L2 norm of the unregularized
    solution。 So the highest value could be is 1， which is when we have no regularization。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 系数增大至0并且一直到图表的右侧。我们得到的是普通最小二乘解，这时我们没有正则化，也没有惩罚。那么，x轴是什么？x轴表示系数的L2范数与正则化解的L2范数和非正则化解的L2范数之间的比率。所以，最大值可能是1，也就是当我们没有正则化时。
- en: And the norm of the coefficients is the same as the norm of the least square
    solutions because they're the same solution。 Whereas at the left， the norm of
    the coefficients。 the sum of the squares of the coefficients is 0 because of regularizing
    so aggressively they all have to be 0 to respect to constraint。 Yeah？ [ Inaudible
    ]， Question， is there any connection to principal component analysis？
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 系数的范数与最小二乘解的范数相同，因为它们是相同的解。而在左侧，系数的范数，也就是系数的平方和为0，因为通过强正则化，所有的系数都必须为0以满足约束。是吗？[听不清]，问题是，这与主成分分析有什么关系吗？
- en: There actually is a subtle connection to principal component analysis。 We can
    talk about that afterwards， but in principal component regression。 which is where
    you run principal component analysis and first keep a certain number of principal
    components and represent your inputs in a space of reduced dimension and then
    run regression is kind of a。 another type of regularization that's related to
    this。 But you can see that this is much smoother。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其实这与主成分分析（PCA）有微妙的联系。我们稍后可以讨论这个，但在主成分回归中，首先运行主成分分析，保留一定数量的主成分，并将输入表示在一个降维后的空间中，然后再进行回归，这其实是另一种与之相关的正则化方法。但你可以看到，这种方法要平滑得多。
- en: We can regularize an arbitrary amount， whereas with PCA regression。 we choose
    a specific number of dimensions to keep。 So you can see there is also a difference。
    All right。 Any questions about these regularization paths？ [ Inaudible ]， I'm
    not sure。 So the question would be， so as the scientists， we're going to have
    to choose our hypothesis base。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行任意量的正则化，而在PCA回归中，我们选择保留一个特定数量的维度。所以你可以看到，两者之间也有差异。好了，关于这些正则化路径有任何问题吗？[听不清]，我不确定。那么问题是，作为科学家，我们必须选择我们的假设基础。
- en: We have to choose one of these sets of coefficients to produce as their decision
    function。 It's a good point。 And so how do we choose each of these x values corresponds
    to a different decision function with different coefficient values true and how
    do we pick which one to keep and not for that we would use a holdout validation
    set or something。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须选择这些系数集中的一个作为决策函数。这个观点很有道理。那么我们如何选择每个x值对应不同的决策函数，系数值也不同，正确吗？我们如何决定选择哪个，并且不选择那个？为此，我们可以使用一个保留验证集或其他方法。
- en: I don't know in this case， which was the best。 Yeah。 [ Inaudible ]， [ Inaudible
    ]， [ Inaudible ]。 So there's， well， the one I'm presenting is there's two steps。
    First you choose your F。 In this case， F shows in linear functions， all linear
    functions。 And that's a space of a certain size。 And then in addition， I'm saying
    let's。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道在这种情况下，哪种方法是最好的。是的。[听不清]，[听不清]，[听不清]。所以，我展示的方法有两个步骤。首先选择你的F。在这种情况下，F是线性函数，所有线性函数。那是一个特定大小的空间。然后，此外，我说我们来约束一下。
- en: we fixed our function of form， yes， true。 And I'm saying let's additionally
    constrain in different ways in terms of the sum of the coefficient。 So there's
    additional constraint。 Yes， there's a different type of restriction in that the
    form is the same。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们固定了我们的函数形式，是的，没错。然后我说我们另外约束系数的和。所以，这里有额外的约束。是的，虽然形式相同，但它是另一种限制。
- en: but it's another way to take a subset， a smaller space。 All right。 So this is
    ridge regression。 Great。 And now we're going to slide show a little bit。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一种将子集缩小的方式，较小的空间。好了，这就是岭回归。太棒了。现在我们要稍微展示一下幻灯片。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_18.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_18.png)'
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_19.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_19.png)'
- en: All right。 All right。 So now we talk about the last step。 You have a question？
    [ Inaudible ]。 So intuitively， so the question is why， what does the magnitude
    of the coefficients have to do with the complexity of the function？
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么现在我们来谈谈最后一步。你有问题吗？[听不清]。直观来说，问题是，系数的大小和函数的复杂性有什么关系？
- en: Okay。 So my short， so there's one answer is like， yes， visually in certain situations。
    they will actually， what？ It depends on the， let me just say this。 The important
    part is that the size of the hypothesis spaces grow and shrink。 Whether or not
    you want to agree that the functions individually are more or less complex。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我的简短回答是：是的，在某些情况下，视觉上它们确实会有什么不同？这取决于， let me just say this。关键部分是假设空间的大小会增大或缩小。无论你是否同意每个函数本身的复杂性更多或更少。
- en: complexity is quite literally in the behind of the holder。 There's different
    ways to define complexity。 So it's just a function that gives a score。 a function
    that gives a score to every function， but it's also a way to describe to define
    this that I pop as a space。 And a bigger hypothesis space has more， we could say
    has more complexity than a small hypothesis space just because it has more functions。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性实际上是函数的背后。在这里有不同的方式来定义复杂性。它只是一个给每个函数打分的函数，但它也是一种定义复杂性的方式。我可以把它定义为空间。更大的假设空间比小的假设空间具有更多复杂性，因为它包含了更多的函数。
- en: Okay。 So， last little regression。 Just swaps out some of squares with some of
    absolute values。 All right。 That's the， that's what's Lasso or L1 regularized
    regression。 And we have two forms。 Just like before， it's taken off form with
    penalty and the Ivan off form with the heart constraint。 And these are the regularization
    paths we get for Lasso。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，最后一个回归。只是用绝对值之和替代平方之和。好的，这就是Lasso或L1正则化回归。我们有两种形式。就像之前一样，它有带惩罚的形式和带硬约束的形式。这些是我们为Lasso得到的正则化路径。
- en: You could see that they are quite different。 And tomorrow we're going to drill
    in deeper to why they're different。 But I think we'll get to that somewhat today。
    What you're， what you're， what you're。 I should see here is that on the left side
    where we're very aggressively regularizing。 Yes。 they're all zero。 But what's
    different is that a lot of them stay zero until the regularization lightens up。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它们是相当不同的。明天我们将深入探讨它们为何不同。但我想今天我们会稍微讨论一下。你们可以看到，在左边，我们非常积极地进行正则化。是的，它们都是零。但不同之处在于，许多系数在正则化减轻之前都保持为零。
- en: So whereas here， here's comparing them side by side。 Whereas， ridge regression。
    as soon as we moved away from all being zero， they all got to be non zero and
    they grew smoothly thereafter。 Whereas the Lasso， the regularization paths are
    quite different。 A lot of them stay zero for longer and then move away from zero。
    And this is。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们并排比较它们。相比之下，岭回归一旦离开了全为零的情况，所有系数都会变为非零，并且其值平滑增长。而Lasso回归，正则化路径则完全不同。许多系数在更长时间内保持为零，之后才远离零。这就是。
- en: kind of related to what we were saying last time is that sparsity is one consequence
    of L1 regularization。 And this is showing the sparsity where for certain choices
    of amount of regularization。 for instance over here， there's only one coefficient
    that's non zero。 When we get to here。 there's only two coefficients that are non
    zero。 At the dotted line。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们上次所说的内容相关，稀疏性是L1正则化的一个结果。这里展示了稀疏性，在某些正则化量的选择下，例如这里，只有一个系数是非零的。当我们走到这里时，只有两个系数是非零的。在虚线处。
- en: which in this example was what they finally chose for predicting out of sample，
    there are one， two。 three non zero coefficients。 So two different ways to regularize
    give kind of two different types of coefficients that we would end up with。 So
    let's talk a little bit about why。 So why do we care about sparsity？
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，他们最终选择了用于预测样本外的方式，那里有一个、两个、三个非零系数。所以，正则化的两种不同方式会产生两种不同类型的系数，这是我们最终得到的结果。那么我们来聊聊为什么会这样。为什么我们关心稀疏性？
- en: I kind of challenged you guys with that last time。 So when a coefficient is
    zero。 we have a solution that's sparse。 We don't need those features。 We don't
    need the features corresponding to those coefficients。 Okay， so so what？ Maybe。
    like what's the advantage of that？ There are some cases that it has advantages。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我上次有点挑战你们。那当一个系数为零时，我们有一个稀疏的解。我们不需要那些特征。我们不需要与那些系数对应的特征。好吧，那又怎么样？也许……这有什么优势？在某些情况下，它确实有优势。
- en: One is a lot of times features are expensive。 You have to compute features。
    Sometimes you even buy features。 Like they're companies that produce features。
    They gather data and generate features for them。 And that has cost to it。 If you
    find that you don't need that feature to do your predictions well。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，特征往往很昂贵。你必须计算特征。有时你甚至购买特征。例如，有些公司会生产特征，它们收集数据并为你生成特征。这是有成本的。如果你发现你不需要那个特征就能做好预测。
- en: you don't have to use that feature。 Save money， save competition。 So there's
    an advantage on a related note。 Sometimes you can have thousands of features。
    And if you want to run a system that's like giving predictions in real time。 you
    have to store all those thousands of features in memory， perhaps。 So if you have，
    you know。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你不一定非得使用这些特征。节省开支，节省计算。所以，相关的一个优势是。有时你可能会有成千上万个特征。如果你想运行一个实时给出预测的系统，你可能需要将这些成千上万的特征存储在内存中。
- en: 50 million users and a thousand features of each user， that's a lot of features
    to store memory。 Well， if you could have a sparse prediction function that only
    needs 15 features to do almost as well。 that's a big advantage because you could
    store much， you could store your feature set in memory。 From a more science perspective，
    maybe you're just interested in what features are the really important ones。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 5000万用户，每个用户有一千个特征，存储这些特征需要很大的内存。好吧，如果你能有一个稀疏的预测函数，只需要 15 个特征就能几乎达到相同的效果，那就是一个很大的优势，因为你可以将特征集存储在内存中。从更科学的角度来看，也许你只关心哪些特征才是最重要的。
- en: So this is the feature selection problem。 Does it do better prediction？
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是特征选择问题。它能做出更好的预测吗？
- en: Sometimes it may actually predict better than say range regression。 I wouldn't
    think that's your first reason to use it though。 And finally。 it's quite good
    as a feature selection step。 The algorithms for running Lasso are quite efficient
    these days。 And what you can do is use it as a way to choose maybe the most promising
    50 features out of a batch of a thousand。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有时它的预测效果可能比范围回归还要好。不过我不会认为这应该是你使用它的主要理由。最后，它作为特征选择步骤相当有效。如今运行 Lasso 的算法非常高效。你可以用它来选择出
    1000 个特征中的 50 个最有前景的特征。
- en: for example。 Then you could take those 50 features and run just those on another
    more complicated nonlinear hypothesis space that requires a lot more computation
    to work with the finding empirical risk minimizer of。 So you could use it as kind
    of a two step process。 First find your features that you like。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 例如。然后你可以将这 50 个特征用于另一个更复杂的非线性假设空间，这需要更多的计算来寻找经验风险最小化器。所以你可以将其作为一种两步法。首先找到你喜欢的特征。
- en: then use those just those features in a more complicated machine learning algorithm。
    In terms of identifying the important features， will Lasso and ACI give us similar
    results？ PCA has。 so PCA regression， oh， important features。 Okay。 Interesting
    question。 PCA doesn't really find you important features very naturally。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在一个更复杂的机器学习算法中使用这些特征。关于识别重要特征，Lasso 和 ACI 会给我们类似的结果吗？PCA 有。那么，PCA 回归，哦，重要特征。好的，挺有意思的问题。PCA
    并不能很自然地找出重要特征。
- en: It finds you important linear combinations of features。 You can then look at
    those linear combinations and say， oh。 this particular feature of the linear combination
    has a very high coefficient。 That must be an important one， but that's kind of
    a less natural thing to do with principal components。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 它会找到重要的特征线性组合。然后你可以查看这些线性组合并说，哦，这个线性组合中的某个特征系数非常高，那一定是重要的，但这对于主成分来说并不是很自然的做法。
- en: Yeah。 Good question。 Oh， yeah。 [inaudible]， Does that mean your kind of feature
    is more nonlinear or your trend or not？
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，好的问题。哦，耶。[听不清]，这是否意味着你的特征是非线性的，或者你的趋势不是线性的？
- en: The first one is， I can't say yes， please。 [inaudible]。 The idea here was that
    you buy a sample of the features， you get all the features。 you see what you like，
    what works， what your Lasso selects。 and then from then on you only use those
    ones。 So that's the idea。
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是，我不能说是的，请。[听不清]。这里的想法是，你购买一组特征样本，得到所有特征。然后你看看你喜欢哪些，哪些有效，Lasso 选择了哪些。从那时起，你只使用那些特征。大致就是这个意思。
- en: It's not that you have to see them at some point to assess them。 [inaudible]，
    All right。 Question。 How do we figure out， this is a model selection problem。
    How do we figure out how much to regularize？ So which regularization coefficient
    or radius。 whatever， is the one we use to actually get the decision function that
    we want to use in prediction。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是说你必须在某个时刻才能评估它们。[听不清]，好的。问题来了。我们如何解决这个问题，这是一个模型选择问题。我们如何确定正则化的程度？也就是说，哪个正则化系数或半径，才是我们用来得到我们在预测中要使用的决策函数的那个值？
- en: So what do you think is an answer to that question？ Anybody？ Second。 Yeah， use
    a validation set。 So for every lambda that you try， you get a decision function。
    you get an estimate of its risk on it using a validation set。 and you see which
    has the best validation performance， and that's what you use。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你认为这个问题的答案是什么？有人吗？第二个答案。是的，使用验证集。对于你尝试的每一个 lambda，都会得到一个决策函数。你可以用验证集估算它的风险，并查看哪个具有最佳的验证性能，然后就使用那个。
- en: This is the standard recipe。 Yes？ [inaudible]， Oh， great advantages to elastic
    net。 but we'll talk about that tomorrow。 Great。 It's going to be an exciting topic。
    Yeah。 [inaudible]。 Can we choose a hypothesis space？ [inaudible]， Yeah。 So the
    question is。 maybe you don't know initially how big to make your hypothesis space。
    Let's make it extra large。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是标准的做法，是吗？[听不清]，哦，弹性网有很大的优势，不过我们明天会讨论这个。太棒了。这将是一个令人兴奋的话题。是的。[听不清]。我们可以选择假设空间吗？[听不清]，是的。所以问题是，也许你最开始并不知道该如何确定你的假设空间的大小。那就设定一个超大的空间吧。
- en: So like we're pretty sure it's bigger than it needs to be。 And then we go back
    and adjust the regularization to figure out the perfect size using lambda。 [inaudible]，
    Yeah， we use regularization as a convenient way to adjust the size of hypothesis
    space essentially。 Yes？ [inaudible]， Is that clear？ Good？ Okay。 Great。 Excellent。
    All right。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们基本可以确定它比实际需要的要大。然后我们返回去调整正则化，使用 lambda 来确定完美的大小。[听不清]，是的，我们使用正则化作为一种方便的方式来调整假设空间的大小，基本上就是这样。对吗？[听不清]，明白了吗？好？好的。太棒了。很棒。
- en: So we talked about why do we want sparsity。 These are all the reasons I can
    come up with。 If you guys have any more reasons， I'd love to hear them and add
    to the slide。 This is your easy interview question slide。 Why do we want sparsity？
    That's all I got for you。 [inaudible]， Yeah。 So if the underlying model is actually
    sparse。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们讨论了为什么我们需要稀疏性。这些是我能想到的所有原因。如果你们有更多的理由，我很想听听并加到幻灯片上。这是你们的简单面试问题幻灯片。我们为什么需要稀疏性？这就是我能提供的全部。[听不清]，是的。所以如果底层模型实际上是稀疏的。
- en: I would guess that you would get better prediction perhaps using L1 in that
    case。 [inaudible]。 So maybe in a science-y view， you want to find what those features
    actually are。 So identify the important features。 [inaudible]， All right。 So the
    little time left。 Let's。 All right。 Here's what I'm hoping to do on the rest of
    the time。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜在这种情况下，使用 L1 可能会得到更好的预测。[听不清]。所以也许从科学的角度来看，你希望找到那些特征到底是什么。于是，识别出重要的特征。[听不清]，好吧。剩下的时间不多了。让我们开始吧。好的。这是我希望在接下来的时间里做的事情。
- en: I'm hoping to get us understanding that picture that I showed in lecture one
    with the circles and the lines and the ellipses。 I want to try to wrap that up。
    Let's see if we can do it。 So let's think for graphical purposes。 Let's take a
    very restricted iPod to space。 So let's take an input space that's R2。 So we can
    draw on the xy plane。 So then our function space is simply based on two parameters。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望让我们理解我在第一讲中展示的那张带有圆圈、线条和椭圆的图。我要尝试总结它。让我们看看是否能做到。为了图形化的目的，我们来考虑一个非常受限的 iPod
    空间。我们设定输入空间为 R2。所以我们可以在 xy 平面上作图。然后我们的函数空间仅仅是基于两个参数。
- en: W1 and W2。 All right。 And so we can represent a function in a space by just
    a point in the plane。 Right？ Bop the two coordinates， W1 and W2。 So what we have
    on the left is a contour of L2。 So these。 I'm using this plane to represent functions
    in the hypothesis space F。 So every point in this plane corresponds to a function。
    How？ This axis is W1 and this axis is W2。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: W1和W2。好吧。所以我们可以通过平面中的一个点来表示空间中的一个函数。对吗？这两个坐标，W1和W2。所以我们左边看到的是L2的等高线。所以这些。我用这个平面来表示假设空间F中的函数。所以这个平面上的每个点都对应于一个函数。怎么理解？这个轴是W1，这个轴是W2。
- en: All right。 So for instance， this point here， that's W1 is equal to 1 and W2
    equals 0。 That clear？
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。例如，这里这个点，W1等于1，W2等于0。清楚了吗？
- en: Correspondent to a friction function。 It's just F of x equals x1。 All right？
    Okay。 So I've convinced you that this plane represents all the functions in our
    hypothesis space F。 And now let's do the Ivanov constraint version。 So within
    that space。 let's look at all the functions that have L2 norm。 Less than or equal
    to R。 All right？
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于一个函数。就是F(x) = x1。好的？明白了吗？好。我已经说服你们，这个平面代表了我们假设空间F中的所有函数。现在让我们来看Ivanov约束版本。在这个空间内。我们来看所有L2范数小于或等于R的函数。好吧？
- en: So what I've drawn is this circle， which I claim is the。 represent the set of
    functions that have L2 norm exactly equal to R。 Why did I draw a circle？
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我画的是这个圆，我声称它代表了L2范数恰好等于R的函数集合。为什么我画了一个圆？
- en: This is the equation of the circle。 W1 square plus W2 square。 That is the norm
    of the L2 norm of W。 And when that equals R， that's a circle of radius squared
    of R。 Okay。 Great。 Great。 Now。 on the left， which are the points that correspond
    to functions that have some sparsity？ On the axis。 great。 So what do I mean by
    sparsity？ Sparsity means。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这是圆的方程式。W1的平方加上W2的平方。这是W的L2范数。当它等于R时，就是半径为R的圆。好，非常好。现在，左边，哪些点对应于具有某种稀疏性的函数？在轴上。很好。那么稀疏性是什么意思呢？稀疏性意味着。
- en: let's just for now say at least one of these coefficients is 0。 And yes。 the
    coefficients are 0 on the axis。 So any point on the axis corresponds to a function
    with some coefficient to be 0。 Fantastic。 What's going on on the right？ We have
    the L1 regularization。 And again。 now the square is all functions that have L1
    norm equal to R。 And again。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就暂时说，至少其中一个系数是0。对，系数在轴上是0。所以轴上的任何一点都对应于一个系数为0的函数。太棒了。那么右边发生了什么呢？我们有L1正则化。同样，现在这个方框是所有L1范数等于R的函数集合。再次。
- en: the points in the axis would be the sparse ones。 So the question we're looking
    for is what is it about L1 regularization that supposedly leads to sparsity？
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 轴上的点将是稀疏的点。那么我们要问的问题是，L1正则化有什么特性能导致稀疏性呢？
- en: So what's that mean？ Supposedly， that means that it leads to finding functions
    that are the coordinates on the coordinate lines。 In the case of this kind of
    constraint that the norm is actually equal to R。 we'd actually expect it to be
    one of these four corners to be the one selected。 and if we're going to get sparsity。
    So let's dig into that a little bit。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这意味着什么呢？据说，这意味着它会找到在坐标线上具有某些坐标的函数。在这种约束条件下，范数恰好等于R时，我们实际上会期望选中这些四个角中的一个。如果我们要得到稀疏性。那么我们来深入研究一下。
- en: See why that would be happening。 So here's the famous picture for the L1 regularization。
    So we've talked about this square boxes。 It's the set of all functions that have
    a norm L1 norm less than or equal to R in this case。 Then we have this black dot
    w hat。 W hat is the empirical risk minimizer of the square error without constraint。
    except being an F。 No norm constraint。 So if we minimize this empirical risk over
    all w in R2。
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 看看为什么会发生这种情况。那么这是著名的L1正则化的图示。我们讨论过这些方框。这是所有L1范数小于或等于R的函数集合。在这种情况下。然后我们有这个黑点w
    hat。w hat是没有约束的平方误差的经验风险最小化器。除了是F之外，没有范数约束。因此，如果我们最小化所有w在R2中的经验风险。
- en: this is what we find。 Now， w hat is where we're here， which means it's L1 norm
    is larger than R。 It's too big for our constraint。 So it's not the constraint
    empirical risk minimizer。 We need to find something that has the smallest possible
    empirical risk， but it's in this blue box。 So what we do is we say， okay， so this
    had risk， I don't know， it's called C。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们发现的情况。现在，w hat就在这里，这意味着它的L1范数大于R。对于我们的约束来说，它太大了。所以它不是约束下的经验风险最小化器。我们需要找到一个在这个蓝色框中具有最小经验风险的点。所以我们这样做，我们说，好吧，这个风险，我不知道，它叫C。
- en: Let's go a little bit bigger than C。 Let's say C plus epsilon。 And then all
    the functions that had risk C plus epsilon are on this red ellipse。 This first
    one。 All right， we're still too big in norm。 We're still not L1 norm less than
    or equal to R。 So it's taking a little bit bigger。 Then we go to the next ellipse。
    And then we keep saying。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微大一点，假设是$C$加上$\epsilon$。然后所有具有$C$加上$\epsilon$风险的函数都在这个红色椭圆上。这个第一个。好的，我们在范数上还是太大了。我们仍然没有L1范数小于等于$R$。所以它变得稍微大一点。然后我们到下一个椭圆。然后我们继续这样做。
- en: all right， let's allow a little bit more risk， a little bit more empirical risk。
    And so finally。 we get to a level of empirical risk， which， and everyone on this
    red outside ellipse has the same empirical risk。 And it just hits the constraint
    for the norm。 And that would be the solution that minimizes this empirical risk
    subject to the L1 norm constraint。 But is that clear that that walk through？ All
    right， so yeah。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们允许一些更大的风险，一些更多的经验风险。最后，我们得到了一个经验风险水平，并且在这个红色外部椭圆上的每个点都有相同的经验风险。它刚好触及到范数的约束。这将是最小化这个经验风险的解，受到L1范数约束。但是，这个过程清楚吗？好的，嗯。
- en: There's absolutely a reason it's represented as an ellipse。 Question number
    one。 Question number two。 Just because in this picture it hits the corner。 why
    would it generically hit the corner？ Is there a question？ No， I agree。 That is
    my intermediate question。 It's my intermediate corner。 Yeah， why would hit the
    corner？
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对有理由将其表示为椭圆。问题一。问题二。只是因为在这张图中它碰到了角落。为什么它通常会碰到角落？有什么问题吗？不，我同意。这是我中间的问题。这是我的中间角落。是的，为什么会碰到角落？
- en: Fantastic。 Okay。 So first the ellipse。 So the answer to that is math。 And I'm
    going to get a little bit of the math。 And the rest of the math is on homework
    two。 So you'll get your chance to really work through the equations。 I'll take
    a start there。 So these are -- so these ellipses are things with constant empirical
    risk。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒。好吧。首先是椭圆。那么它的答案是数学。我会涉及到一些数学，剩下的数学内容在第二次作业里。所以你将有机会真正地处理这些方程。我先从这里开始。这些是——这些椭圆是具有恒定经验风险的事物。
- en: So what's the expression for the empirical risk？ This is another way to write
    it。 This is a vectorized form。 And if we say -- okay， so I want a different。 So
    this empirical risk is minimized by the ordinarily squares minimizer， right？
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 那么经验风险的表达式是什么？这是另一种写法。这是一个向量化形式。如果我们说——好吧，我想要不同的。那么这个经验风险是由普通最小二乘法（OLS）最小化的，对吗？
- en: When it's unconstrained。 And you know this expression。 And the question is what
    does our unhat look like near W hat but away from it。 So we can do this cool manipulation
    of our hat N of W。 This expression。 And we can rewrite it in terms of W hat。 The
    W hat is the best possible solution。 That's the OLS。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有约束时。你知道这个表达式。问题是我们的$\hat{W}$在接近$\hat{W}$时看起来是什么样子的，但又不完全是它。所以我们可以对我们的$\hat{N}(W)$做一个很酷的变换。这个表达式。然后我们可以将它用$\hat{W}$来重新写。$\hat{W}$是最好的解。那就是OLS。
- en: the ordinarily squares solution。 And we can write the empirical risk of W some
    other point。 some other function in the space in terms of the risk of the best
    possible W hat。 the ordinarily squares solution， plus a piece that makes it a
    little worse because we're not at the OLS solution。 So this is an expression which
    you can -- you'll drive in the homework using this technique called Completing
    the Square。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘法的解。我们可以将$W$的经验风险写成某个其他点，某个函数在空间中的风险，表示为最优$\hat{W}$的风险，即普通最小二乘法解，加上一个使得它变得稍微差一点的部分，因为我们不在OLS解上。所以这是一个你可以通过这项技术“完成平方”在作业中推导出的表达式。
- en: which you may remember from algebra in middle school or something。 But this
    is the matrix vector version of it。 So it's a little bit fancier。 I put a little
    document on the website explaining what this Completing the Square looks like
    in vectors and with matrices and vectors。 I used to call it Completing the Quadratic
    Form as a better description but no one else calls it that。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得中学代数中的某些内容。但这是它的矩阵向量版本。所以它稍微复杂一些。我在网站上放了一个文档，解释了这个“完成平方”在向量和矩阵中的样子。我以前称其为“完成二次形式”，作为一个更好的描述，但没有人这样称呼它。
- en: So Completing the Square。 All right。 So do you recognize this expression？
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 所以完成平方。好的。那么你认出这个表达式了吗？
- en: Does that say a ellipse to you？ Or ellipse to it？ All right。 Well。 All right。
    Let's go a little further。 So now let's consider the set of W， the set of points
    in that plane。 that have --， that exceed -- whose empirical risk exceeds the minimal
    possible empirical risk by C。 by certain amounts of C， right？ So that's a set
    of all W where the empirical risk is equal to C plus -- that's what we have。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这对你来说是椭圆吗？还是它的椭圆？好吧。好了，让我们再往前走一点。那么现在让我们考虑W的集合，即在那个平面中的点的集合。它们——超过——其经验风险比最小可能的经验风险多了C。按某些C的量，右吗？所以这是一个所有W的集合，在这些W中，经验风险等于C加——这就是我们所拥有的。
- en: That's a bug， sorry -- plus the risk minimizer。 And it has this form。 And this
    you can -- some of you will recognize as the equation for an ellipseoid。 All right。
    So if that's not familiar， that's another issue and that you can review separately。
    But this is the -- this is the equation of an ellipseoid in Rd。
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个错误，抱歉——加上风险最小化器。它具有这种形式。你们中的一些人会认出这是椭球体的方程。好了，如果这不熟悉，那是另一个问题，你可以单独复习一下。但这是——这是在Rd中的椭球体方程。
- en: And so that's how we know it's an ellipse。 So you can look up ellipseoids and
    see that this is what that is。 All right。 So this is a little bit of matrix algebra
    to show that。 All right。 So the second question was， why should it hit the corner？
    So it's a great question。 And I'm going to give you a picture answer， which is
    --， it is what it is。
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是我们知道它是椭圆的方式。你可以查阅椭球体，看到这就是它的样子。好了。所以这是一些矩阵代数，展示这一点。好了。所以第二个问题是，为什么它应该碰到角呢？这是一个很好的问题。我将给你一个图像回答，也就是——它就是这样。
- en: It's not a proof of anything。 I am not quite -- I mean。 there are -- there are
    ways to show that things converge to sparse solutions。 And for instance。 the situation
    of the true model being sparse， that's called sparse-distancy。 But this is different。
    This is saying -- this is not about convergence。 This is saying we have a fixed
    data set and we're ending up with sparse solutions。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是任何东西的证明。我并不是很——我的意思是，确实有方法可以证明某些东西收敛到稀疏解。例如，真正的模型是稀疏的，这称为稀疏距离。但这是不同的。这里说的是——这与收敛无关。这里的意思是我们有一个固定的数据集，并且最终得到的是稀疏解。
- en: And why？ And I'll draw the picture that I find helpful。 So -- okay。 So here's
    our L1 ball， right？
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么呢？我会画一个我认为有帮助的图。所以——好的。这是我们的L1球，对吧？
- en: All right。 Let me make it a little smarter。 So here's my claim and you can verify
    it yourself。 Let me draw some lines here。 And let's do the coloring。 All right。
    So here's my claim。 So to simplify it， let's suppose instead of ellipses， we had
    circles。 just to make things a little bit easier。 So remember we had a W hat。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。让我把它做得更聪明一点。那么这是我的声明，你可以自己验证它。让我在这里画一些线。然后我们做一些着色。好了。这是我的声明。为了简化它，我们假设不是椭圆，而是圆形。仅仅为了让事情变得稍微简单一点。记住，我们有一个W
    hat。
- en: which was the ordinary least square solution。 So what I'm going to claim is
    that if the W hat。 the OLS solution， is anywhere in the yellow areas， and the
    ellipses --。 and we have a circle instead of ellips， it's going to hit a corner。
    And if it's anywhere in the non-yellow areas， it's not going to hit a corner。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这是普通最小二乘解。我要声明的是，如果W hat，即OLS解，位于黄色区域内，而椭圆——如果我们把椭圆换成圆形，它将会碰到一个角。如果它位于非黄色区域，它就不会碰到角。
- en: So if you grow a circle around this point， it's going to hit the edge。 But anywhere
    in this whole sector， it's going to hit a corner。 So does this convince anybody
    that in some imprecise sense we're going to be more likely to hit a corner than
    an edge？
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你在这个点周围画一个圆，它会碰到边缘。但在这个整个扇区内，它都会碰到角。所以，这能让任何人相信在某种不精确的意义上，我们更有可能碰到角而不是边缘吗？
- en: OK。 So I might try to formalize this when we get to Bayesian statistics。 where
    perhaps we could say like， if the true W is chosen kind of in a way that's symmetric
    and usually symmetric。 So we're as likely to get something in this sector as its
    other sector and somehow how likely we are to get something in each sector is
    proportional to its area。 then perhaps this can be made precise。 Yeah？ [ Inaudible
    ]， Will decrease？ Let's come back to this。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以我可能会在我们讨论贝叶斯统计时尝试形式化这个问题。也许我们可以说，如果真正的W被以某种对称的方式选择，并且通常是对称的。那么我们在这个扇区和另一个扇区中获得某个东西的可能性是一样的，并且我们在每个扇区中获得某个东西的可能性与该扇区的面积成正比，那么或许这可以被精确化。对吗？[听不清]，会减少吗？我们回到这个话题。
- en: Let's do two minutes。 Let me do one more slide。 Let's come back to this picture。
    This is the core of picture from lecture one。 So does this picture explain anything
    about lasso？
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做两分钟。让我再做一张幻灯片。我们回到这张图。这是第一讲中的核心图。那这张图有解释什么关于套索的内容吗？
- en: So when I looked at this recently， I was like， I don't get this。 These are lines。
    We need ellipses。 right？ I mean， I get the idea of this line touching the corner，
    but we're not about lines。 We're about ellipses。 So what's going on？ So can anyone
    think of any way to make sense of this picture in our context of lasso？
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我最近看到这个时，我想，我不明白。这些是直线。我们需要的是椭圆，对吧？我的意思是，我理解这条线接触角落的意思，但我们讨论的不是直线，我们讨论的是椭圆。那么，这到底是怎么回事呢？有没有人能想出什么办法，在我们的套索（lasso）背景下理解这张图？
- en: Supposed someone told you that someone who wrote this wasn't -- maybe the person
    who wrote their core answer didn't exactly understand。 but maybe the person who
    made the original diagram is like a real machine learning expert。 Is there any
    way we could twist this to actually represent something meaningful for lasso？
    Yeah？
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有人告诉你，写这张图的人可能——也许是写核心答案的人并没有完全理解，但也许画这张原始图的人是一个真正的机器学习专家。有没有办法我们能把这张图扭转过来，实际上代表一些有意义的东西，跟套索（lasso）相关？对吧？
- en: '[ Inaudible ]， Maybe。 [ Inaudible ]， So it could be -- not sure。 It could be
    a tangent。 but the tangent is different from the actual level set。 Yeah？ [ Inaudible
    ]， Okay。 So I think another way to say what you''re saying is that if you imagined
    that our W hat。 our least-forged solution， we''re really， really far away。'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，也许。[听不清]，所以它可能是——不确定。它可能是一个切线，但切线和实际的水平集是不同的。对吧？[听不清]，好的。所以我认为另一种说法是，如果你想象我们的W
    hat，我们的最优解，真的很远。'
- en: Then when that ellipse boundary comes all the way back here。 it's going to look
    like a straight line。 All right？ So， okay。 that would be a way to make sense of
    it。 So suppose we were in that situation that the W hat were really far away。
    and so it looks like a straight line。 And what I see when I see a straight line
    is it's even more likely to hit a corner than when it's rounded。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当那个椭圆边界完全回到这里时，它就会看起来像一条直线。对吧？所以，好的，这样就能解释得通了。那么，假设我们处于那个情境中，W hat真的很远，那么它看起来就像一条直线。而当我看到一条直线时，我会觉得它比圆滑的线更可能碰到角落。
- en: Right？ And so this actually kind of makes sense because when W hat is really
    far away。 then when we have this hard-straint where a regularization is changing
    W hat a lot。 it's pulling it in tremendously。 And so to tie it together。 if regularization
    is extremely aggressive， it's having a very large effect on the change in how
    W hat。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？所以这其实是有道理的，因为当W hat真的很远时，当我们有这种硬约束，正则化对W hat的变化影响很大，它把W hat拉得很近。所以，来把它们联系起来，如果正则化非常激进，它对W
    hat变化的影响就非常大。
- en: these squares W hat changes， then we're going to end up with more sparsity。
    or more likely enough with sparsity。 So that's my best way to interpret this picture
    in terms of lasso。 All right。 So I don't want to keep you guys past nine as nine
    now。
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方框W hat的变化，那么我们最终会得到更多的稀疏性，或者更有可能得到稀疏性。所以这是我最好的方式来解释这张图，关于套索（lasso）。好的。现在已经九点了，我不想再让你们待得太久。
- en: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_21.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e43eb5d01ffd7bcdfbbc08b055bcf2a_21.png)'
- en: You're welcome to say but we're also we can end the class and I'll see you guys
    tomorrow。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你们可以说出来，但我们也可以结束今天的课，明天见。
