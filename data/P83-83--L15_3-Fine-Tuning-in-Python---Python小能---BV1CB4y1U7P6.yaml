- en: P83：83. L15_3 Fine Tuning in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P83：83. L15_3 Python中的微调 - Python小能 - BV1CB4y1U7P6
- en: Okay， here we show how do we actually do fine tuning。 That's important because
    the next homework。 not due after spring break， you're going to have fun with the
    break and we're。 going to handle the homework after spring break。 But the honk
    we're about fine tuning。 because that's pretty the key。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这里我们展示了如何实际进行微调。这很重要，因为下一个作业，在春假之后才截止，你将会在假期时玩得开心，作业会在春假后处理。但重点是微调。因为这是一个非常关键的部分。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_1.png)'
- en: scale you're going to have for communication。 Okay， first of all， import the
    library。 and we double small dataset。 It's called Harddog recognition。 Well， you
    can see that it's on the。 it's on right on the web。 What we do here， we just Google
    Harddog。 and Google some other things similar to Harddog and present。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将用于通信的比例。好了，首先，导入库。然后我们准备了一个小数据集。它叫做Harddog识别。嗯，你可以看到它在网上。我们在这里做的，就是用Google搜索Harddog，和Google其他一些类似Harddog的东西，并展示出来。
- en: and just spend the afternoon to get the dataset。 So you can see that we download
    the dataset。 just extract all these images。 You can see the folder contains a
    training folder， a test folder。 the training folder have two subfolders， a harddog，
    not harddog。 Even folder contains a bunch of JPEGs and test datasets similarly。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 只需要花一个下午就可以获得数据集。所以你可以看到我们下载了数据集，提取了所有这些图像。你可以看到文件夹包含一个训练文件夹和一个测试文件夹。训练文件夹有两个子文件夹，一个是harddog，另一个是not
    harddog。测试文件夹也包含一堆JPEG图像和类似的测试数据集。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_3.png)'
- en: Then， well， we can do here。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，嗯，我们可以在这里做什么。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_5.png)'
- en: we can train the image， we already know that image folder dataset through the
    harddog。 train and also test the images that we visualize all the things。 So you
    can see that the first row is harddogs。 The second row is not harddogs， pretty
    similar。 The bananas。 (laughter)， Well， something else， like you also。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以训练图像，我们已经知道了通过harddog的图像文件夹数据集。训练并测试图像，所有的东西都可以可视化。所以你可以看到，第一行是harddogs，第二行是not
    harddogs，十分相似。香蕉。（笑声）嗯，其他一些东西，像你也一样。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_7.png)'
- en: but pretty similar things。 Okay， then， the image augmentation is here。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是相似的事情。好了，接下来是图像增强。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_9.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_9.png)'
- en: We already talked about that before。 Firstly， we normalize the RGB channels。
    So we have the meaning of the RGB channels and the values of the RGB。 So this
    value is from the image net。 Just on the starting from 80 years ago。 people are
    using these values。 We never sought to verify these values， but just a copy here。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经讨论过了。首先，我们标准化RGB通道。所以我们知道RGB通道的含义和RGB的值。这些值来自ImageNet。大约在80年前，人们就开始使用这些值了。我们从未验证过这些值，但这里只是复制过来。
- en: Because the pre-channel models are trained with these values， so you don't want
    to change it。 That the thing。 But I heard this from image net， but I don't know
    actually。 Then， transformations。 what they do， you randomly size， because it's
    larger images to--， 4。2。4。4。 This is the default one we train with image net。
    Also do a lot of leap of flipping and two tensors。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因为预训练的模型是用这些值训练的，所以你不想改变它。这就是问题所在。不过我从ImageNet那里听说过这个，但我并不确切知道。然后，进行变换。它们做的事情是，你随机调整大小，因为它是较大的图像——4.2.4.4。这是我们与ImageNet一起训练的默认设置。还会做很多翻转操作，并且转换为两个张量。
- en: converting to the layers convolutional neural network 1/2， and normalize to
    RGB channels。 For the test dataset， we just resize to a reasonable shape。 256
    is also like from image net。 At the original day， Alex said what Alex is doing
    is like--。 they first resized the image to this shape。 So that we just keep this
    value here。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 转换到卷积神经网络层1/2，并将其标准化为RGB通道。对于测试数据集，我们只是调整到一个合理的形状。256也像来自ImageNet一样。在最初的那一天，Alex说，Alex做的事情是这样的——他们首先将图像调整为这个形状。所以我们在这里就保持这个值。
- en: And do a central crop， which means we are not doing random， crop。 We just crop
    the central area to get the validation。 So the reason is because we don't want
    to have any， randomizations here to get the reliable results。 Then， lastly， two
    tensor and normalize the output。 So， OK， question。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 做一个中心裁剪，这意味着我们不做随机裁剪。我们只是裁剪中心区域来获取验证数据。原因是因为我们不想在这里做任何随机化，以便获得可靠的结果。然后，最后，将图像转换为张量并标准化输出。所以，好的，问题。
- en: What does assuming the hot dogs in the center are they painted？ Yes。 so the
    question that if the hot dog is not centered， is maybe in the corner， what happened？
    Well。 this is not image classification program。 Then this object detection we're
    going to talk about。 in maybe 10 minutes。 So image classification， we assume。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么假设热狗在中心，它们被画出来了吗？是的。那么问题是，如果热狗不在中心，而是在角落，会发生什么？嗯，这就不是一个图像分类问题了。接下来我们要谈的是目标检测，可能会在大约10分钟后讲解。所以图像分类中，我们假设。
- en: damage contains the main projects on the center。 Maybe not exactly on the center，
    but it only。 contains the main objects。 Yeah， I mean for the documentation of
    how you should。 have labeled like states the same。 If the hot dog is in the corner
    you crop， then the。 like boat should be like bolts。 OK， so the reason is because
    the random resized crop is。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像包含了主要物体在中心的位置。可能不完全在中心，但它只包含了主要的物体。是的，我的意思是，在文档中，你应该像这样标注。如果热狗在角落，你裁剪掉它，那么像船只这样的目标应该像螺栓一样处理。好的，原因是，随机缩放裁剪可以。
- en: kind of guaranteed that you don't pick up two small areas。 Even if you're not
    luckily the crop you have doesn't， condense an object。 you think about just a
    random noise。 It give a fake， fake， input training image into a network。 It's
    just a noise。 Usually the neural networks stay way enough to handle all， these
    noises you have。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可以保证你不会选择太小的区域。即使裁剪的区域不是很幸运，裁剪后没有浓缩成一个物体，你可以把它当作随机噪声。它会给网络一个假的训练图像。这只是噪声。通常神经网络足够强大，可以处理你输入的所有噪声。
- en: Even for image net， I don't know image net， what the error rate， because you
    are labeled by human。 There are lots of arrows usually。 Image net is pretty good。
    And the other data side maybe contains a lot of arrows。 The reason is because
    you send images to。 a connector， a turd， and ask the other guy to label for you。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是图像识别（image net），我也不知道图像识别的错误率是多少，因为标签是由人工标注的。通常会有很多错误。图像识别做得相当不错。至于其他数据，可能包含很多错误。原因是因为你将图像发送给一个连接器，一个傻瓜，然后让另一个人帮你标注。
- en: They usually something just close their eyes to label things。 The reason is
    because， well， you are。 be paid for how many images you are labeled。 It's really
    hard to guarantee the quality。 So it's our job。 We care about the application
    to guarantee the quality。 But if we didn't do a good job， the data， said we had
    a lot of noises in that thing。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通常只是闭上眼睛去标注。原因是，因为你是按标注的图像数量付费的。很难保证标注的质量。所以这是我们的工作。我们关心应用的质量保证。如果我们没做得好，数据中就会有很多噪声。
- en: I remember they did benchmark 10% to 5% wrong labeling， in the data set。 That's
    pretty normal。 Then。 well， here's the thing。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得他们做了基准测试，数据集中有10%到5%的错误标注。这是很正常的。那么，事情就是这样。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_11.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_11.png)'
- en: We download the pre-trained model from the model zoo。 So model zoo vision。 ResNet
    is the second version of ResNet， 18。 We specify pre-trained equals 2， 2， which。
    we get the network and download the pre-trained weight， from the web。 So this
    one。 we are contains both the model definition， and all the pre-trained weights。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从模型库下载了预训练模型。所以，模型库视觉。ResNet是ResNet的第二版，18。我们指定预训练等于2，2，这样就可以获得网络并从网上下载预训练的权重。因此，这一部分包含了模型定义和所有预训练的权重。
- en: So you can see that the pre-trained weight， has a dot output， which is the last
    layer。 You also have the features， which is the backbone feature， instructors。
    So what do we do here is that because the hard-out classification。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，预训练的权重有一个输出层，那是最后一层。你还可以看到特征层，它是主干特征提取器。那么我们在这里做的事情是，因为这是一个硬分类问题。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_13.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_13.png)'
- en: we only have two classes。 So we， again， we grab the same model from the model
    zoo。 but without all this pre-trained weight， we specify the number of classes
    equal to 2， which。 is the same feature instructors， but the last layer， only have
    the hinders size equal to 2。 Then what we do here， we copy the weight from the
    features。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有两个类别。所以，我们再次从模型库抓取相同的模型，但不包含所有这些预训练权重，指定类别数为2，这样就保留了相同的特征提取器，只是最后一层的大小变成了2。然后我们所做的是，将特征的权重复制到新的模型中。
- en: which is all these layers except for the last one， to my fine-tuned net。 Then
    for the last classifier， we just， randomly initialize it。 It's a get layer initialize
    with the random initializer。 And one thing you maybe do。 you can skip it if you
    want。 The thing is like for the last layer。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，除了最后一层，所有这些层都应用于我微调的网络。然后对于最后的分类器，我们只是随机初始化它。它是通过随机初始化器初始化的获取层。你可能会做一件事，如果你愿意的话也可以跳过。问题在于，最后一层。
- en: we set the linearity multiplier to 10， so which means if we give linearity 0。0
    to the net walk。 then the last layer is we're going to train， with the larger
    linearity rate by 10 times larger。 which is what？ Do you know the reason why？
    The question is like。 why we will have a larger linear rate， for the last layer？
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将线性度乘数设置为10，这意味着如果我们给网络施加0.0的线性度，那么最后一层将以10倍更大的线性度进行训练。这是什么？你知道为什么吗？问题是，为什么我们要为最后一层设置更大的线性度？
- en: Because the previous layers have been pretty well trained。 You want to move
    larger pairs for the last layer。 It's quickly new。 You want to have a larger linearity，
    able to have a fast purchase。 Yes。 because the feature instructors already， are
    in a good shape。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因为前面的层已经训练得很好。你希望最后一层有更大的步长。它会快速更新。你想要有更大的线性度，能够快速优化。是的，因为特征提取器已经处于良好的状态。
- en: It's already very close to the final position。 But the last layer is randomly
    initialized。 You want this far away。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它已经非常接近最终位置。但最后一层是随机初始化的。你希望它离得更远。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_15.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_15.png)'
- en: You want to converge faster at the end。 OK。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望在最后阶段更快地收敛。好的。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_17.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_17.png)'
- en: Then fine-tuning， nothing， no different compared， to what we had before。 We
    get the data loader。 get the context， trying to use all GPUs。 That's one thing。
    We talk about using all GPUs。 And we said to all the GPUs we have， because we
    copied the parameters initialized。 from the CPUs in default and hybridized make
    it faster， get the loss function， get the trainer。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后微调，与之前没有什么不同。我们获取数据加载器，获取上下文，尝试使用所有GPU。就是这一点。我们讨论使用所有GPU。我们对所有GPU进行了设置，因为我们从CPU初始化了参数，并将其混合以提高速度，获取损失函数，获取训练器。
- en: So that is nothing particular new。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这没有什么特别新的内容。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_19.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_19.png)'
- en: We can specify the linearity here。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里指定线性度。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_21.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_21.png)'
- en: And I showed the results pretty quickly。 For fine-tuning， we start with a very
    small linearity， 0。01。 And you can see that well， converge。 Well， it's hard to
    say fast or slow。 but let's see transform scratch。 So this is scratch net。 We
    just grabbed the network。 Randomized-- I randomly initialized all the parameters，
    and the trend with a larger linearity， 0。1。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我很快展示了结果。对于微调，我们从一个非常小的线性度0.01开始。你可以看到它收敛得很好。嗯，很难说是快还是慢。但让我们看看从零开始的变化。所以这是从零开始的网络。我们只是抓取了网络，所有参数都进行了随机初始化，并且趋势是用较大的线性度0.1。
- en: So you can see that compared the training accuracy， so this is 0。9， 0。9， and
    this is 0。8。 And especially over here， the test accuracy， is much higher， which
    test accuracy even higher。 compared to the training accuracy。 Because training，
    we have a lot of data limitations。 which is decreased accuracy。 And for here，
    it's 0。1 difference here。 Also。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，与训练准确率相比，这是0.9，0.9，而这是0.8。尤其是这里，测试准确率明显更高，测试准确率甚至超过了训练准确率。因为训练时，我们有很多数据限制，这会降低准确度。而在这里，差距是0.1。也有。
- en: even if we are using smaller linearity， it actually converge faster than comparing。
    to the training for scratch。 Because while we are already on the original shape。
    it's much easier to converge。 OK， any questions？ Good。 For the homework， we're
    going to try。 grab ImageNet， to train 100 dots like a subset of ImageNet。 [BLANK_AUDIO]。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们使用较小的线性度，它实际上比从零开始训练更快地收敛。因为我们已经处于原始形状。收敛要容易得多。好的，有什么问题吗？很好。对于作业，我们将尝试。获取ImageNet，训练100个点，类似于ImageNet的一个子集。[BLANK_AUDIO]。
- en: '![](img/1739d641dd4a44aeff1198ab5c59d941_23.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1739d641dd4a44aeff1198ab5c59d941_23.png)'
