- en: P16：16.Mar_31_Lab - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P16：16.Mar_31_Lab - Tesra-AI不错哟 - BV1aJ411y7p7
- en: So we have a little problem with the project。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在项目中遇到了一些问题。
- en: '![](img/092825a2229a7f4b5c258377024d0ca0_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/092825a2229a7f4b5c258377024d0ca0_1.png)'
- en: So I'm going to try to write a few lines from the slides， but the slides will
    be online。 So don't worry about it。 You'll have access to all that we will do。
    So let's talk about generalized linear model。 It's just a little bit of probabilistic
    setup and we'll see one example and we'll see how。 a generalized linear model
    relates to the minimizing the square norm and then we'll move。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我打算尝试写几行幻灯片的内容，但幻灯片会在线上提供。所以不用担心，你将能够访问我们做的所有内容。现在我们来谈谈广义线性模型。它只是一个稍微的概率设定，我们将看到一个例子，看看一个广义线性模型是如何与最小化平方范数相关的，然后我们将继续。
- en: on to the test， reviewing the test。 So this time the hypothesis space of functions
    are the ones that takes the input to a Gaussian。 with a parameterized mean and
    the parameterized variance。 You can imagine a bunch of Gaussians and given input，
    identify which one will come out。 So the goal is obviously to find parameters
    for each x， f of x is a Gaussian density。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行测试，复习测试。所以这次的假设空间是那些将输入映射到高斯分布的函数，具有参数化的均值和方差。你可以想象一堆高斯分布，给定输入，确定哪个会输出。所以目标显然是为每个x找到参数，f(x)是一个高斯密度。
- en: With the standard deviation， sigma squared。 So finding v determines the function。
    So f depends on v。 So f depends on v takes input as x。 We should be able to write
    our prediction function。 x is y。 So we can write the density as the following。
    Probability density that depends on the parameter v of y given x。 where y is our
    prediction。 For x。 So normal density。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准差sigma平方。所以找到v就确定了函数。所以f依赖于v。所以f依赖于v并以x为输入。我们应该能够写出我们的预测函数。x是y。所以我们可以将密度写成以下形式。依赖于参数v的y给定x的概率密度，其中y是我们的预测。对于x。所以是正态密度。
- en: conditional normal density of y given w transpose x， sigma squared。 So we're
    interested in this object here， this conditional density of predictions given
    data。 If you have a bunch of data， we can factor this out using independence。
    So the probability density that we want depends on parameters given a bunch of
    data， some。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 给定w转置x和sigma平方的条件正态密度。所以我们对这个对象感兴趣，这个给定数据的预测的条件密度。如果你有一堆数据，我们可以利用独立性将其分解。所以我们想要的概率密度依赖于给定一堆数据的参数，一些。
- en: family of x， i， y， i's， say i from 1 to n， just the product of all individual
    Gaussians。 We'll write the explicit formal distance。 Hopefully it will be clearer。
    The product goes from 1 to n。 So the product goes from all data。 So I just introduced
    a new model and the new model is just a conditional probability。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: x，i，y，i的族，假设i从1到n，就是所有单个高斯分布的乘积。我们将写出明确的正式距离。希望它会更清晰。乘积从1到n。所以这个乘积遍历所有数据。所以我刚刚引入了一个新模型，这个新模型只是一个条件概率。
- en: So we want to predict y。 We want to predict y given x in the probabilistic setting。
    And the assumption is that data is normal。 x is the input， y is the prediction。
    Conditionally， yes。 So given x， we want to find which conditional family x belongs
    to。 Let's say for now we fixed the standard deviation of those Gaussians。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们想要预测y。在概率设定下，我们想要预测给定x的y。假设数据是正态分布的。x是输入，y是预测。条件上，是的。所以给定x，我们想要找出x属于哪个条件分布族。暂时假设我们固定了那些高斯分布的标准差。
- en: We have a bunch of densities here and here with the fixed sigma。 If I find the
    correct parameter family w here over there up there， then I can find given。 x
    which ones I have。 Yes？ What's s， w of x of y of y？ It's the density of the prediction。
    conditional density of the prediction given data。 The whole problem is parameterized
    by v and it goes to find v。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一堆密度，这里和这里是固定sigma的。如果我在这里找到正确的参数族w，那么我就可以找到给定x时我有哪些结果。是吗？什么是s，w of x of
    y of y？它是预测的密度。给定数据的预测的条件密度。整个问题由v来参数化，并且它会找到v。
- en: Of course， once we have a probabilistic setting， what we do is to maximize the
    probability。 So we want to find， so of course we want to find the v that maximizes
    the probability， density。 So we want to find the w star that's arg max or rho
    possible w's that maximizes this product。 If we have independent observations，
    this line， I could have started with just writing， this line。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一旦我们有了一个概率设定，我们要做的就是最大化概率。所以我们想要找到，当然我们要找到能够最大化概率密度的v。所以我们想找到w*，这是能够最大化这个乘积的arg
    max或rho可能的w。如果我们有独立观测值，这一行，我本来可以从写这一行开始。
- en: The above is just a formal setup。 So what's this？ What's this with the given
    normality assumption？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上面只是一个形式上的设置。那么这是什么？在给定正态性假设的情况下，这是什么意思？
- en: This is just the density of the Gaussian since it's now a single variable。 Then
    over sigma times squared p exponential negative 1 over 2 sigma squared times y
    i minus。 wt xi squared。 Given data， my parametric prediction density for the output
    is the Gaussian density with。 the prescribed mean and standard deviation。 So if
    you're confused what's going on up there。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是高斯分布的密度，因为现在是单一变量。然后是sigma乘以平方根2π，再加上这个表达式的求和。给定数据，我的参数化预测密度对于输出是高斯密度，具有规定的均值和标准差。所以，如果你对上面的内容感到困惑的话。
- en: you can just start with this part。 So this part says， well。 I have some independent
    observations that are normal with some parameterized。 mean and some fixed standard
    deviation。 Then how can I maximize this probability？
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这部分开始。所以这部分的意思是，我有一些独立的观察数据，这些数据服从正态分布，具有一些参数化的均值和固定的标准差。那么我如何最大化这个概率呢？
- en: That's all we are doing here。 This is going a little fast because it's actually
    a recap of the previous semester's class。 Maximum likelihood estimation。 So what
    gives us this arg max？
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在这里做的所有事情。这个讲得有点快，因为其实是在回顾上学期的课程内容——最大似然估计。那么，是什么给了我们这个arg max呢？
- en: It's the answer is maximum likelihood estimation which can be found by， so instead
    of looking。 at the maximum of this thing where you can take the log of it， it
    will be at the same， place。 so we'll just start。 If I call this function， I already
    had an aim for it up there。 It's log of the density。 Yes？ It's what？ It's continuous。
    It's a continuous random variable。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是最大似然估计，可以通过这样找到。所以，代替直接寻找这个东西的最大值，你可以对它取对数，它依然会是在同一个地方。所以我们就从这里开始。如果我把这个函数称作，我之前已经给它命名了。它是密度的对数。对吗？它是什么？它是一个连续的随机变量。
- en: It's true。 But this is not probability。 This is the probability density。 Right？
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。但这不是概率。这是概率密度，对吗？
- en: I'm going to maximize the density itself。 This is not maximizing good question。
    This is not。 because it's a common confusion。 This is not maximizing probability
    of a random variable taking a value。 If I had a single example， the density would
    look like this。 And what I would want to find would be this point here。 The point
    in the domain that maximizes this。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我要最大化的是密度本身。这不是最大化概率的问题。这不是。因为这是一个常见的误解。这不是在最大化随机变量取某个值的概率。如果我只有一个例子，密度会是这样的。我想找到的是这个点，定义域中最大化这个密度的点。
- en: Yes？ Yes， it's missing some， it's just this part。 Oh， okay。 Summation will come
    in the next line。 So if I take log of this thing， let's make it clear that this
    box corresponds to this， box。 not this expression below。 The log of this will
    just separate this part and that part。 It will be again arg marks of log。 I take
    negative here sum sigma square root 2 pi plus sum of this expression。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 是吗？是的，缺少一些内容，就是这一部分。哦，好的。求和将在下一行出现。所以如果我对这个取对数，首先明确一下，这个框对应的是这个框，而不是下面的这个表达式。对它取对数时，会把这部分和那部分分开。它依然是对数的arg
    max。我在这里取负号，求和的sigma乘以根号2π，再加上这个表达式的求和。
- en: Now why can't we take the log because we are looking at the point in the domain
    that maximizes。 this function。 Right？ We are not interested in the value of the
    maximum。 We're interested in the point that maximizes it。 So for the case of the
    original function and for the case of the log。 the point that maximizes， them
    will be the same。 Their values will be different but the point that maximizes
    them。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在为什么不能直接取对数，因为我们关注的是在定义域中最大化这个函数的点，对吗？我们并不关心最大值的大小，而是关心最大值的点。因此，对于原始函数和对数函数，它们最大化的点是相同的。它们的值会不同，但最大化它们的点是一样的。
- en: namely the arg max， the， w star， the v star， will be the same。 So I can take
    log。 And why taking log is useful because instead of having this whole exponential，
    I think I。 get rid of the exponential and the first term I have here is independent
    of v。 This part is independent of the parameters so I can just get rid of it。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即最大值点（arg max）、w 星（w star）和v 星（v star）将是相同的。所以我可以取对数。取对数有用的原因是，因为与其处理整个指数部分，我可以去掉指数部分，剩下的第一项与v无关。这个部分与参数无关，所以我可以把它去掉。
- en: And now I have to maximize a negative function which is the same thing as minimizing
    when。 I ignore this negative sign。 So this whole problem turns to minimizing。
    I can also ignore this constant here。 I don't care。 It's minimizing the sum of
    yi minus w transpose xi squared。 Does this look familiar？ Yeah。 Right？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我必须最大化一个负函数，这和最小化是一样的，当我忽略这个负号时。所以整个问题就变成了最小化。我也可以忽略这里的常数。我不在乎。它是最小化yi减去w转置xi的平方。这看起来熟悉吗？对吧？
- en: So in the probabilistic setting， if you assume normal priors then with the maximum
    likelihood。 method， the parameters you want to estimate corresponds to the minimization
    problem of。 the square loss。 Yes？ Also to what？ What do you mean by its normal
    error？
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在概率设置中，如果你假设正态先验，那么使用最大似然法时，你想要估计的参数对应的是最小化问题。平方损失。是吗？那还对应什么呢？你说的正常误差是什么意思？
- en: Y equals Wtx plus error term？ Probably because you are saying that W minus y。
    so prediction minus Wtx is going to be， I think it's a little different。 So you
    are saying that I am assuming that this is normal something。 This is Gaussian。
    Probably means zero， some standard deviation。 It's a little different。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Y等于Wtx加误差项？可能是因为你说的是W减去y。所以预测值减去Wtx会有些不同。你说我假设这是一个正常的东西。这个是高斯分布。可能意味着零，某个标准差。其实是有点不同的。
- en: You can make it seem close with this a little different。 If you take sigma fixed。
    that's how you get the equivalence with the least squares。 Estimating sigma is
    one step more complicated。 So in this problem， it's sigma is fixed。 What we assume
    that we know we are waiting on。 Yeah。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以让它看起来接近，虽然有点不同。如果你把sigma固定，那就能得到与最小二乘法等价的结果。估计sigma是更复杂的一步。所以在这个问题中，sigma是固定的。我们假设我们知道它，我们在等待它。对吧？
- en: Now this is another way to show that this generalized linear model with the
    Gaussian， assumption。 you can solve that problem using your linear algebra methods
    for the least， square problem。 Just this set up above in the first page。 Okay，
    let's talk about the exam。 I was hoping to project the questions， but since project
    is not working now， I'm going， to ask you。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是另一种方式来展示这个带有高斯假设的广义线性模型。你可以使用线性代数方法解决这个最小二乘问题。就像第一页上方的设定一样。好了，接下来我们来谈谈考试。我原本希望能展示题目，但由于投影仪现在无法工作，我将直接问你们。
- en: do you have your copies of exams？ Do you remember the questions？ Okay。 so I'm
    going to pull up the questions。 Okay， let's start with the multiple choice question。
    Because in the meantime， we have the projector back。 We can probably talk about
    the pictorial stuff on the board。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你们有考试题目的副本吗？你们记得题目吗？好吧，我来调出题目。好了，我们从选择题开始。因为这时候，投影仪已经恢复了，我们可以在黑板上讨论图示内容。
- en: Do you remember the multiple choice question？ Well， briefly。 Okay。 We didn't
    type it。 but I was played with it。 I can post my notes， but it's not typed， so
    it's handwritten。 Okay。 let's start from the first problem。 The first problem
    was separable data。 The part A of the first problem， you probably remember that
    there was a bunch of circles， here。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你记得那个选择题吗？嗯，简短地说。好吧，我们没输入它，但我玩过它。我可以发布我的笔记，但它没有打字，所以是手写的。好了，我们从第一个问题开始。第一个问题是可分数据。第一个问题的A部分，你可能记得这里有一堆圆圈。
- en: a bunch of x is here。 And there was another dot mixed in between。 So this appeared
    as one group。 and this appeared as another group。 And the question asked you to
    find the hyperplane that maximizes the margin。 Half of you got this， and the other
    half probably got this。 And since it's a hard margin problem。 we don't allow any
    slack variables。 So this answer is wrong。 This answer is right。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一堆x。还有另外一个点混合在其中。所以这些看起来像是一组。这些则看起来像是另一组。问题要求你找到最大化间隔的超平面。你们一半做对了，另一半可能做错了。因为这是一个硬间隔问题，我们不允许任何松弛变量。所以这个答案是错的。这个答案是对的。
- en: So you have to consider all the problems。 It has to separate the data completely。
    Even if it means that you need to give up one point to lose a lot of margin or
    vice versa。 to gain a lot of margin。 Questions？ Yes， in a hard margin。 It doesn't
    allow any slack。 If it doesn't say soft margin， you don't allow it， right？ Sorry？
    I don't hear。 I mean， that's clear。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你必须考虑所有问题。它必须完全分隔数据。即使这意味着你需要放弃一个点来失去大量的间隔，或者反过来，获得大量的间隔。有什么问题吗？是的，硬间隔下不允许任何松弛。如果题目没有说明是软间隔，那就不允许，对吗？抱歉？我没听清。我的意思是，这很明确。
- en: right？ It's the same thing。 You don't need to specify it extra。 You want the
    separating hyperplane。 The separating hyperplane always means separating hyperplane。
    You don't need to specify the margin problem either。 Yeah， of course。 I mean，
    look。 I don't tell you anything about SVM。 I don't tell you anything about the
    margin。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？这就是一样的。你不需要额外指定它。你想要的是分隔超平面。分隔超平面永远意味着分隔超平面。你也不需要额外指定边距问题。是的，当然。我的意思是，看，我没有告诉你任何关于SVM的事，我也没有告诉你关于边距的事。
- en: I just give you this， and I tell you to find the separating hyperplane。 What
    would you do？
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是给你这个，然后告诉你找到分隔超平面。你会怎么做？
- en: Separating hyperplane。 By its definition， in English， it must separate all the
    points。 Just the only virtue that the separating hyperplane has is separating
    the points。 There's nothing else。 Okay， the second part of this problem was a
    bunch of points here and a bunch of points。 on this line。 And note that it starts
    from zero here。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔超平面。根据定义，简单来说，它必须将所有的点分开。分隔超平面唯一的作用就是将点分开。没有其他作用。好的，第二部分的问题是这里有一堆点，另一堆点在这条线上。请注意，它从零开始。
- en: so you can't really separate it by just trying， to put a parable in between。
    How do you do that？
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你不能仅仅通过尝试，在中间放一个抛物线来分隔它。你怎么做呢？
- en: If you tried hard， you could find a shifted parabola， but getting the parameter's
    right。 would be hard。 You need to shift it a little up and scale it in a way that
    it's nice and separated。 So what do I mean by， you know， find， add the feature
    that separates the data linearly？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你努力的话，你可以找到一个平移的抛物面，但是把参数调整正确会很难。你需要稍微把它抬高一点，并且按某种方式缩放，使它变得很整齐并且可以分开。那么我说的“找到能线性分隔数据的特征”是什么意思呢？
- en: There are two ways to think about it。 You can either use your features and come
    up with a function that non-linearly separates。 the data。 Or you can think of
    this as you obtain your data and you transform your data into a new。 space。 On
    that space it's linearly separable。 So the example that most of you， you know。
    wrote in the exam was from the slides that， had this problem。 So how do you separate
    this？ You know。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方式来考虑这个问题。你可以使用特征来构造一个非线性分隔数据的函数。或者你可以把数据获取到手后，转换成一个新的空间。在那个空间中，数据是线性可分的。所以你们大部分人，在考试中写的例子，就是幻灯片中那个问题。那么，怎么分隔这个呢？你知道的。
- en: yx squared plus y squared works in this case。 It does what？ Mm-hmm。 If the origin
    is not centered。 I could have shifted my function。 It would have been x1 minus
    the x-coordinate of the center and y1 minus y-coordinate of。 the center。 So what
    it does， when I add this function， what it does is that it takes this。 it embeds，
    this into a three-dimensional space。 It embeds it specifically on a three-dimensional
    parabola。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: yx的平方加上y的平方在这个案例中有效。它做了什么？嗯哼。如果原点没有居中，我可以把函数做一个平移。那就会是x1减去中心的x坐标，y1减去中心的y坐标。那么它做的事情是，当我添加这个函数时，它把这个嵌入到三维空间中。它具体地嵌入到三维抛物面中。
- en: And at the bottom of the parabola I have those values。 And in the middle of
    the parabola I have those values so I can cut it where they are。 separated in
    that three-dimensional plane， right？ I have x1 comma x2 comma something else。
    I could have done it in two-dimensions as well。 I could have considered only，
    you know。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在抛物线的底部，我有那些值。而在抛物线的中间，我有那些值，所以我可以在它们被分隔的三维平面上切割它们，对吧？我有x1，x2，还有其他什么。我也可以在二维中做。也可以只考虑你知道的。
- en: one part of it， one-dimension and then add the， x1 squared plus x2 squared。
    The whole point is to lift it up， lift those points up and while keeping those
    points low。 so you could separate it。 You know， in a similar way， this can be
    separated by a reflection。 Data starts from here。 If you could reflect the points
    onto the either side。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中的一部分，首先是一个维度，然后加上，x1的平方加上x2的平方。整个关键是将这些点抬起来，抬起这些点，同时保持这些点在低的位置。这样你就能将它们分开。你知道，以类似的方式，这可以通过反射来分开。数据从这里开始。如果你能将这些点反射到两侧。
- en: then you would be able to separate this， with the cyber plane。 Does that make
    sense？
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你就能通过“赛博平面”来分隔这些点。这样说通吗？
- en: How do you reflect it？ So if you get this data and you need to transform the
    data into a new plane in which it's separable。 So one way， this v-shape is a hint
    of it， one way to do this is to reflect the data。 You can look at the-- you know，
    this is the x1 axis and this is the x2 axis。 What happens if I take x1， x2 here
    and map it to absolute value of x1 and x2？ What picture do I get？
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么反射它？所以如果你得到这个数据并且需要将数据转换到一个新的平面，在这个平面中它是可分的。一种方式，这个v形状是一个提示，一种方法是反射数据。你可以看看——你知道，这是x1轴，这是x2轴。如果我在这里取x1、x2，并将它映射到x1的绝对值和x2上，会得到什么图像？
- en: Yes？ Yes。 Right， it reflects them。 So all the negative x1 values become positive。
    So this overlaps with the right hand side。 So instead of this， I get a bunch of
    points here。 Instead of this part， I get a bunch of points here。 And this becomes
    linearly separable。 The point of adding new features is actually transforming
    your data into a new plane。 Right？ Yes？
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 是的？是的。对，它反射了它们。所以所有负的x1值变成了正值。所以这与右边重叠。所以我得到了一堆点，而不是这个。我得到了一堆点，而不是这一部分。这样就变得线性可分了。添加新特征的关键实际上是将你的数据转换到一个新的平面上，对吧？是的？
- en: You can do that to x1， x2， comma--， You could do that too。 Yeah。 The only point
    that-- the only thing you need to do is to show that the idea of reflection。 will
    work and you can do reflection by either taking squares or absolute values or
    whatever。 you can。 [INAUDIBLE]， So for-- look， what this does-- suppose you take
    a data point here。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对x1、x2等做这个操作——你也可以这样做。是的。唯一需要做的就是证明反射的概念有效，你可以通过取平方、绝对值或者其他方式进行反射。[听不清]，所以看，假设你在这里取一个数据点。
- en: Whenever you're confused， just take a data point， sample data point。 Let's say
    this is 1。2。 Then you need to find this representation in the new space， which
    is going to be what？
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你感到困惑时，只需取一个数据点，采样数据点。假设这个数据点是1.2。那么你需要在新空间中找到这个表示，它会是什么呢？
- en: Which is going-- well， this is trivial。 It's going to be the-- it's going to
    give the 1。2。1。 So let's take this point。 Minus 1。2 will be mapped to 1。2。1 as
    well。 And if you map all the points。 you'll really realize that in the third dimension，
    you can， actually cut them。 Or not exactly in the third dimension， but if you
    project it from the last two dimensions。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是显而易见的。它将变成1.2.1。所以我们取这个点，-1.2也会映射到1.2.1。如果你映射所有的点，你会真正意识到，在第三维度中，你实际上可以将它们切割开来。或者不完全是在第三维度中，但如果你从最后两个维度投影它。
- en: you'll be able to cut them。 [INAUDIBLE]， Yes， no， that won't work。 Because what
    that does is that if you add a constant C， what does it do？
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你将能够将它们切割开来。[听不清]，是的，不，这样做不行。因为这样做的结果是，如果你加上常数C，它会有什么效果？
- en: It just lifts it in the three-dimensional space。 Everything is in the same place。
    but it just lifts it。 So if you use absolute value of x1， in the three-dimensional
    space。 you just lift them--， so in the third dimension， there will be only positive
    values。 So only positive values here。 And then if you look at it from the side。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是将数据提升到三维空间。一切都保持在原位，但只是被提升了。所以如果你使用x1的绝对值，在三维空间中，你只需提升它们——因此在第三维度中，将只有正值。因此这里只会有正值。然后，如果你从侧面看它。
- en: then you'll be able to cut it。 It just makes it geometrically a little bit more
    complicated。 [INAUDIBLE]， Yes。 Yes。 Yes。 You can just work on absolute value of
    x1， yes。 [INAUDIBLE]， Yes。 Yes。 You can just work on absolute value of x1。 Yes。
    [INAUDIBLE]， You can use x1 squared。 You need to convince me that you were aware
    of the reflection idea。 Just didn't write it。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你将能够将它切割开来。它只是从几何上变得稍微复杂一点。[听不清]，是的。是的。是的。你可以仅使用x1的绝对值，是的。[听不清]，是的。是的。你可以仅使用x1的绝对值。是的。[听不清]，你可以使用x1的平方。你需要让我相信你了解反射的概念，只是没写出来。
- en: '[INAUDIBLE]， x1 squared eventually doesn''t put the parabola。 That''s why it''s
    wrong。 x1 squared is just this。 How do you expect x1 squared to put a parabola
    between the two？ Right？'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[听不清]，x1的平方最终并没有形成抛物线。这就是它错误的原因。x1的平方只是这个。你怎么期望x1的平方能在这两个点之间形成一个抛物线呢？对吧？'
- en: Just try it out， and you will see it。 Take one point。 Take two points， and try
    that if it separates。 It doesn't separate。 OK。 Next question。 Step directions
    in optimization。 Just remind you。 the step directions in optimization， asks you
    the following。 If you have a function。 j that depends on v。 There are three cases。
    In each case， the question asks you。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 试试看，你就会明白。取一个点。取两个点，试试它们是否分开。如果它们不分开。好的，下一个问题。优化中的步长方向。提醒你一下，优化中的步长方向，问你以下问题。如果你有一个函数
    j，依赖于 v。有三种情况。在每种情况下，问题会问你：
- en: to identify whether the following is true。 [INAUDIBLE]， All these are strict。
    So in the first case。 the function j is convex and defrachable。 And we take a
    step in the negative gradient times some eta。 and eta is small enough。 So in the
    first case， the function j is convex and defrachable。 In that case， what will
    happen？ W* is the minimizer。 Which one of them will be true。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 判断以下是否正确。[听不清]，所有这些都是严格的。那么在第一种情况下，函数 j 是凸的且可微的。我们在负梯度方向上迈出一步，乘以某个 η，且 η 足够小。那么在第一种情况下，函数
    j 是凸且可微的。那时会发生什么？W* 是最小值点。哪个是对的？
- en: or none of them， can be true too？ Any ideas？ When the function is convex and
    defrachable。 if you take a step in the negative gradient， in the direction of
    the negative gradient。 when the step sizes small enough， what will happen？ Will
    the value decrease？ Always。 Yes。 Unless it's flat， and it's not flat， because
    it's given that the point we start is not a minimizer。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 或者它们都不对也可以吗？有什么想法吗？当函数是凸的并且可微时，如果你在负梯度方向上迈出一步，当步长足够小，结果会怎样？值会减少吗？一定会。是的，除非它是平的，而它不会平，因为已知我们开始的点不是最小值点。
- en: So this is true。 Will it get us closer to the minimum？ Yes， it's also true。
    The second part of the question， what if we have a convex j， but it's not defrachable？
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是对的。它是否会让我们更接近最小值？是的，这也是对的。问题的第二部分，如果我们有一个凸函数 j，但它不可微呢？
- en: Then we take a direction， we take a step， in the direction of the negative sub
    gradient。 Will it necessarily reduce the value？ No。 There's a conterexample in
    the slides。 Will it necessarily get us closer to the optimum？ Yes， there's a theorem。
    Third part。 We look at the sum this time。 J is a sum of convex functions， convex
    and defrachable functions。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们选择一个方向，沿着负子梯度的方向迈出一步。它是否一定会减少值？不会。幻灯片中有一个反例。它是否一定会让我们更接近最优解？是的，有一个定理。第三部分。我们这次看的是总和，J
    是凸函数的总和，是凸且可微的函数。
- en: But we take a step in the direction of the derivative， of only one of them。
    which is just this stochastic step， just like the first stochastic step。 Will
    it necessarily reduce the value？ No。 Will it get us closer to the optimum？ No，
    right？
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们只在其中一个方向上迈出一步，即沿着它的导数方向，这就像第一次随机步骤一样。这是否一定会减少值？不会。它是否会使我们更接近最优解？不会，对吧？
- en: It can take a step in arbitrary direction。 Overall， though， we know that it
    takes us to the minimum。 [SOUND]， [SOUND]。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以朝任意方向迈步。然而，我们知道它会带我们到最小值。[声音]，[声音]。
- en: '![](img/092825a2229a7f4b5c258377024d0ca0_3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/092825a2229a7f4b5c258377024d0ca0_3.png)'
- en: The third problem starts with the algebraized regression。 Then it asks you to
    justify just the derivative。 So in that question， what you do is just you。 take
    the derivative and you give an expression for the solution。 So derivative-- of
    course。 you need to use the general， and take the derivative of this too。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个问题从代数回归开始。然后它要求你证明只是求导数。所以在那个问题中，你做的就是求导并给出解的表达式。所以求导——当然，你需要使用一般形式，并且也需要对这个进行求导。
- en: So it's x transpose times this times 2 lambda v。 And if you set this to 0 and
    expand this。 you get the condition that the gradient is equal to 0， gives you
    x transpose x plus lambda v。 is equal to x transpose y， which is what's， asked
    to justify in the first part。 And if you take it into v parenthesis and immerse--，
    immerse this。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它是 x 转置乘以这个再乘以 2 λ v。如果你将其设置为 0 并展开，你会得到梯度等于 0 的条件，给出 x 转置 x 加 λ v。等于 x 转置
    y，这就是第一个部分要求证明的内容。如果你将其带入 v 括号并深入——深入这个。
- en: you get x transpose x plus lambda i， inverse times x transpose y。 And how can
    you do this？
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到 x 转置 x 加 λ i 的逆，乘以 x 转置 y。你怎么做这个呢？
- en: You can do this because x transpose x， is positive semi-definite。 So if eigenvalues
    are non-negative， it can be 0。 So maybe it's not invertible。 But since I know
    that lambda is positive， then I know that I shift all the--。 if there is a possible
    0 eigenvalue， I shift it。 So there's no 0 eigenvalue。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样做，因为 x 转置 x 是半正定的。所以如果特征值是非负的，它可以是零。所以可能它不可逆。但由于我知道 lambda 是正的，我知道我可以移位所有--。如果有可能的零特征值，我就移位它。所以没有零特征值。
- en: So that thing is invertible。 Unless lambda is 0， then you need。 to have x transpose
    x being invertible。 That's the extra condition you need to put。 So if you write
    all that， you're good。 Perceptron problem， is it emerginals？ [INAUDIBLE]， 。 。
    。 。 。 and the hypothesis space is that it is just the usual W transpose times
    x。 Is this， a margin loss？
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个东西是可逆的。除非 lambda 为 0，这时你需要。确保 x 转置 x 是可逆的。那是你需要加上的额外条件。所以如果你写下这些，没问题。感知机问题，它是紧急的吗？[听不清]，。。。然后假设空间就是通常的
    W 转置乘以 x。这个是，边距损失吗？
- en: Yes， how？ Marginal loss and SVM just in this set up perceptron loss。 and the
    hinged loss with perceptron。 They are the same thing。 They are the same， functions。
    It's just the margin loss and the SVM set up in the slides。 You know the one？
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，怎么？边距损失和 SVM 在这个设置中就是感知机损失。以及带有感知机的铰链损失。它们是一样的。它们是相同的，函数。只不过是边距损失和幻灯片中的 SVM
    设置。你知道那个吗？
- en: So margin is y times f。 So what margin is label times the prediction， negative
    label。 times the prediction。 And then if it's on one side， you give the loss，
    when you assign。 the hinged loss， what happens？ If it's positive， you keep the
    value。 If it's negative， you assign。 zero。 That's the same thing。 So it's almost
    the topologically it's the same thing。 Yes。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以边距是 y 乘以 f。所以边距是标签乘以预测，负标签。乘以预测。然后如果它在一边，你给出损失，当你分配。铰链损失时，发生什么？如果是正的，你保持这个值。如果是负的，你分配。零。那是一样的。所以从拓扑上讲，几乎是一样的。是的。
- en: and you just write this is the margin， this minimizes the margin， you know，
    online。 That could work。 I don't know how exactly it's created， but I believe
    it could work。 The。 second part of the question asks you to write the condition
    for the hyperplane that separates。 the data。 So obviously the answer should involve
    data。 How do you do that？ Yes， so if I have x， y。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你只需写下这是边距，最小化边距，你知道的，在线。这应该能工作。我不确定它是如何创建的，但我相信它能工作。问题的第二部分要求你写出分离数据的超平面的条件。所以显然，答案应该涉及数据。你怎么做呢？是的，如果我有
    x，y。
- en: i， data pairs， I just want to have y， i times the prediction to， be strictly
    positive。 but it has to be 4 o i。 So it covers all data。 The third part asks。
    so the third part says the following。 If you can find the perfect separator， then
    the， loss is zero。 But is the commerce true？ If you find something that gives
    us zero loss。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: i，数据对，我只想让 y，i 乘以预测值严格为正。但是它必须是 4 o i。所以它覆盖所有数据。第三部分要求。第三部分说如下。如果你能找到完美的分隔符，那么损失为零。但这个命题是真的吗？如果你找到某个能给我们零损失的东西。
- en: does it imply that there is perfect separation？ No， how？ You just give it simple
    count， for。 example。 It can be？ Yeah， it can be on the hyperplane or you can have
    just the zero predictor。 You know， w zero gives you zero loss， but it doesn't
    separate anything。 Or yeah， data， on h。 Regularized perceptron。 So regularized
    perceptron。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 它是否意味着有完美的分离？不，怎么做到？你只需给出简单的计数，例如。它可以是吗？是的，它可以在超平面上，或者你可以仅有零预测。你知道，w 为零给你零损失，但它什么也不分离。或者是的，数据，位于
    h 上。正则化感知机。所以正则化感知机。
- en: the objective function is one-half norm squared plus， I have it here plus c
    divided by n。 sum of roll data， again the same max of zero or negative， yi， w
    transpose xi。 What's this upgrading for this？ So it's differentiable in two parts，
    right？ If this is positive。 strictly positive is differentiable。 If this is strictly
    negative， it's differentiable too。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数是半范数的平方加上，我在这里有它加上 c 除以 n。数据的总和，再次是零或负的最大值，yi，w 转置 xi。这是对这个的升级吗？所以它在两部分上是可微分的，对吧？如果这是正的。严格正的就是可微的。如果这是严格负的，也是可微的。
- en: So the only problematic part is zero and at that point， I can use the。 derivative
    for either one or something in between would work。 So I'm just going to write
    one。 of them is w， just derivative of this plus。 Well， in the problem， there is
    no， it's just， this。 In two point one， it's just this function。 So the sub gradient
    is just going to be negative， c y x。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以唯一有问题的部分是零，在这一点上，我可以使用导数，对于其中一个或者介于两者之间的东西都可以。所以我只是写一个。它是 w，只是这个的导数加上。好吧，在这个问题中，没有，它只是，这个。在
    2.1 中，就是这个函数。所以子梯度就是负的，c y x。
- en: when if this is positive， so the negative of it is negative and else this is
    just going。 to give me zero。 So I end up with the derivative of the first part。
    which is just w if y w transpose， x is greater than or equal to zero， right？ That
    covers the H case。 Second part is Lagrangian。 Yes。 What is the second part instead
    of solving？ This。 This。 Plus。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当如果这个是正的，那么它的负值是负的，否则它就只是给我零。所以我最终得到的是第一部分的导数。那就是 w 如果 y w 转置，x 大于等于零，对吧？这涵盖了
    H 情况。第二部分是拉格朗日函数。是的。第二部分是什么？而不是解这个？这个。这个。加上。
- en: You can do that too。 The second part asks you to write the Lagrangian once you're
    given the constraint。 So minimize， it。 J w with subject to norm squared norm is
    greater than or equal to one。 So if you remember， the standard form， in standard
    form， the inequality constraints。 we like to write it as some function， less or
    equal to zero。 So it's just rewrite it that way。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以这么做。第二部分要求你在给定约束的情况下写出拉格朗日函数。所以最小化它。J w 使得范数的平方大于等于 1。所以如果你记得，标准形式中，标准形式中，不等式约束。我们喜欢将它写成某个函数，小于等于零。所以就按这个方式重写。
- en: This is just one minus norm squared， is less than zero。 Then we can write the
    Lagrangian。 So Lagrangian for this will depend on the， new parameter lambda。 It's
    J of the parameters plus lambda times one minus norm e squared。 Of course for
    the inequality constraints we have lambda positive。 The primal， they are。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是 1 减去范数的平方，小于零。那么我们可以写出拉格朗日函数。所以这个拉格朗日函数将依赖于新的参数 lambda。它是参数的 J 加上 lambda
    乘以 1 减去范数 e 的平方。当然，对于不等式约束，我们有 lambda 正数。原始问题，它们是。
- en: just in the slides。 If you want justifications I can briefly explain them too。
    But the primary。 form， P star will be just infimum。 So we want to eventually find
    infimum over parameters。 so it's infimum or parameters， supremum over lambda positive
    of the Lagrangian。 Next part asks you to find the dual problem。 That's the dual
    problem。 Just swap them。 When。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在幻灯片中。如果你需要解释，我也可以简要说明。但主要的形式，P 星将是下确界。所以我们最终想找到关于参数的下确界。所以它是参数的下确界，lambda
    正数的拉格朗日函数的上确界。下一部分要求你找到对偶问题。那就是对偶问题。只要交换它们。什么时候。
- en: you swap them for the dual， D star is supremum over lambda's。 Infimum of these，
    L V lambda。 What's the dual function？ The dual function is the function that depends
    only on lambda。 which is the inner part here。 You can call it some function G
    that depends on lambda。 Sorry。 you had the dual function。 In the previous part
    it says， explain why this gives。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你将它们交换为对偶形式，D 星是 lambda 的上确界。这些的下确界是 L V lambda。对偶函数是什么？对偶函数是只依赖于 lambda 的函数。它是这里的内部部分。你可以称它为依赖于
    lambda 的某个函数 G。抱歉，你有对偶函数。在前面部分中提到，解释为什么这给出了。
- en: the same optimal value as the original problem。 Let me go back to that and then，
    was it？
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 和原始问题相同的最优值。让我回到这个，然后，是吗？
- en: You were going to ask that。 Okay。 Yeah， the second part of the third question
    is explain。 why this gives the same optimal value。 So why this gives the same
    optimal value？
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你本来要问这个问题。好的。是的，第三个问题的第二部分是解释。为什么这给出了相同的最优值。那么，为什么这给出了相同的最优值？
- en: So supremum of lambda positive over lambda positive of the Lagrangian function，
    what does this do？
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，lambda 正数的上确界对 lambda 正数的拉格朗日函数，这有什么作用？
- en: Let's rewrite the function。 It's supremum over positive lambda's of J。 of W
    plus lambda times 1 minus norm V squared。 So now， you know， what does this supremum
    give， us？
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写这个函数。它是 J 的 W 加上 lambda 乘以 1 减去范数 V 的平方的 lambda 正数的上确界。那么现在，你知道，这个上确界给我们带来了什么？
- en: If I happen to have this part positive， what will happen to the supremum？ It
    will， be infinite。 right？ So it will be infinite if 1 minus norm V squared is
    positive or if， I have this。 So if the constraint is not satisfied， it's going
    to be infinite。 But if the constraint。 is satisfied， what will happen？ It's going
    to give me， the supremum will give me just， this。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我碰巧有这个部分为正，那么上确界会发生什么？它会是无穷大，对吧？所以如果 1 减去范数 V 的平方为正，或者如果我有这个。所以如果约束没有得到满足，它会是无穷大。但如果约束得到了满足，会发生什么？它会给我，上确界会给我，正是这个。
- en: So this if part captures the constraint and this part captures the original
    minimization， problem。 Because after this soup， we take that， that's why you have
    this impsup problem， right？
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个 if 部分捕捉了约束条件，这部分捕捉了原始的最小化问题。因为在这个 soup 之后，我们取这个，这就是为什么你有这个 impsup 问题，对吧？
- en: So you can think of this as the following。 We had one minimization problem and
    one constraint。 We had two operations here and the two operations， the first operation
    is here， minimize over， V。 And the second operation has to be engraved somehow
    in the supremum， right？ And it's actually。 contained in the supremum in this following
    way。 That's the explanation for part three。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以把这个看作是以下内容。我们有一个最小化问题和一个约束条件。这里有两个操作，第一个操作是在这里，最小化关于V的值。第二个操作必须以某种方式刻画在上确界里，对吧？而且它实际上是这样包含在上确界中的。这就是第三部分的解释。
- en: Any questions？ Yes。 Which part？ Mm-hmm。 Yeah。 And then you can do it in that
    form。 Yeah。 you could do it analytically。 But sometimes you can't take the derivative。
    You don't know。 what the derivative looks like。 So from the Lagrangian formalism，
    the way to justify is， this。 Let's finish the class。 I'm running over time。 But
    I can continue answering some。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么问题吗？是的。哪个部分？嗯哼。是的。然后你可以用那个形式来做。是的。你可以进行解析。但是有时候你无法求导。你不知道导数是什么样子的。所以从拉格朗日形式来看，证明的方法是，嗯，这个。让我们结束这节课吧。我已经超时了。但我可以继续回答一些问题。
- en: of the questions related to the exam or other questions if you have。 I have
    a problem。 I just went for， when you were going through this， I figured out in，
    this。 you could watch it and do it in my brain。 Yeah。 Yeah。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何关于考试的问题或其他问题，我有一个问题。我刚刚去过，当你讲解这个的时候，我明白了，嗯，这个。我可以在我的大脑里看并做出来。是的。是的。
- en: '![](img/092825a2229a7f4b5c258377024d0ca0_5.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/092825a2229a7f4b5c258377024d0ca0_5.png)'
- en: Yeah。 Yeah。 Yeah。 Yeah。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。是的。是的。是的。
