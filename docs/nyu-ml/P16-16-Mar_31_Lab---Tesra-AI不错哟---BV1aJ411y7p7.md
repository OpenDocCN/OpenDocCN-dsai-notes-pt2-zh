# P16：16.Mar_31_Lab - Tesra-AI不错哟 - BV1aJ411y7p7

所以我们在项目中遇到了一些问题。

![](img/092825a2229a7f4b5c258377024d0ca0_1.png)

所以我打算尝试写几行幻灯片的内容，但幻灯片会在线上提供。所以不用担心，你将能够访问我们做的所有内容。现在我们来谈谈广义线性模型。它只是一个稍微的概率设定，我们将看到一个例子，看看一个广义线性模型是如何与最小化平方范数相关的，然后我们将继续。

继续进行测试，复习测试。所以这次的假设空间是那些将输入映射到高斯分布的函数，具有参数化的均值和方差。你可以想象一堆高斯分布，给定输入，确定哪个会输出。所以目标显然是为每个x找到参数，f(x)是一个高斯密度。

使用标准差sigma平方。所以找到v就确定了函数。所以f依赖于v。所以f依赖于v并以x为输入。我们应该能够写出我们的预测函数。x是y。所以我们可以将密度写成以下形式。依赖于参数v的y给定x的概率密度，其中y是我们的预测。对于x。所以是正态密度。

给定w转置x和sigma平方的条件正态密度。所以我们对这个对象感兴趣，这个给定数据的预测的条件密度。如果你有一堆数据，我们可以利用独立性将其分解。所以我们想要的概率密度依赖于给定一堆数据的参数，一些。

x，i，y，i的族，假设i从1到n，就是所有单个高斯分布的乘积。我们将写出明确的正式距离。希望它会更清晰。乘积从1到n。所以这个乘积遍历所有数据。所以我刚刚引入了一个新模型，这个新模型只是一个条件概率。

所以我们想要预测y。在概率设定下，我们想要预测给定x的y。假设数据是正态分布的。x是输入，y是预测。条件上，是的。所以给定x，我们想要找出x属于哪个条件分布族。暂时假设我们固定了那些高斯分布的标准差。

我们有一堆密度，这里和这里是固定sigma的。如果我在这里找到正确的参数族w，那么我就可以找到给定x时我有哪些结果。是吗？什么是s，w of x of y of y？它是预测的密度。给定数据的预测的条件密度。整个问题由v来参数化，并且它会找到v。

当然，一旦我们有了一个概率设定，我们要做的就是最大化概率。所以我们想要找到，当然我们要找到能够最大化概率密度的v。所以我们想找到w*，这是能够最大化这个乘积的arg max或rho可能的w。如果我们有独立观测值，这一行，我本来可以从写这一行开始。

上面只是一个形式上的设置。那么这是什么？在给定正态性假设的情况下，这是什么意思？

这只是高斯分布的密度，因为现在是单一变量。然后是sigma乘以平方根2π，再加上这个表达式的求和。给定数据，我的参数化预测密度对于输出是高斯密度，具有规定的均值和标准差。所以，如果你对上面的内容感到困惑的话。

你可以从这部分开始。所以这部分的意思是，我有一些独立的观察数据，这些数据服从正态分布，具有一些参数化的均值和固定的标准差。那么我如何最大化这个概率呢？

这就是我们在这里做的所有事情。这个讲得有点快，因为其实是在回顾上学期的课程内容——最大似然估计。那么，是什么给了我们这个arg max呢？

答案是最大似然估计，可以通过这样找到。所以，代替直接寻找这个东西的最大值，你可以对它取对数，它依然会是在同一个地方。所以我们就从这里开始。如果我把这个函数称作，我之前已经给它命名了。它是密度的对数。对吗？它是什么？它是一个连续的随机变量。

是的。但这不是概率。这是概率密度，对吗？

我要最大化的是密度本身。这不是最大化概率的问题。这不是。因为这是一个常见的误解。这不是在最大化随机变量取某个值的概率。如果我只有一个例子，密度会是这样的。我想找到的是这个点，定义域中最大化这个密度的点。

是吗？是的，缺少一些内容，就是这一部分。哦，好的。求和将在下一行出现。所以如果我对这个取对数，首先明确一下，这个框对应的是这个框，而不是下面的这个表达式。对它取对数时，会把这部分和那部分分开。它依然是对数的arg max。我在这里取负号，求和的sigma乘以根号2π，再加上这个表达式的求和。

现在为什么不能直接取对数，因为我们关注的是在定义域中最大化这个函数的点，对吗？我们并不关心最大值的大小，而是关心最大值的点。因此，对于原始函数和对数函数，它们最大化的点是相同的。它们的值会不同，但最大化它们的点是一样的。

即最大值点（arg max）、w 星（w star）和v 星（v star）将是相同的。所以我可以取对数。取对数有用的原因是，因为与其处理整个指数部分，我可以去掉指数部分，剩下的第一项与v无关。这个部分与参数无关，所以我可以把它去掉。

现在我必须最大化一个负函数，这和最小化是一样的，当我忽略这个负号时。所以整个问题就变成了最小化。我也可以忽略这里的常数。我不在乎。它是最小化yi减去w转置xi的平方。这看起来熟悉吗？对吧？

所以在概率设置中，如果你假设正态先验，那么使用最大似然法时，你想要估计的参数对应的是最小化问题。平方损失。是吗？那还对应什么呢？你说的正常误差是什么意思？

Y等于Wtx加误差项？可能是因为你说的是W减去y。所以预测值减去Wtx会有些不同。你说我假设这是一个正常的东西。这个是高斯分布。可能意味着零，某个标准差。其实是有点不同的。

你可以让它看起来接近，虽然有点不同。如果你把sigma固定，那就能得到与最小二乘法等价的结果。估计sigma是更复杂的一步。所以在这个问题中，sigma是固定的。我们假设我们知道它，我们在等待它。对吧？

现在这是另一种方式来展示这个带有高斯假设的广义线性模型。你可以使用线性代数方法解决这个最小二乘问题。就像第一页上方的设定一样。好了，接下来我们来谈谈考试。我原本希望能展示题目，但由于投影仪现在无法工作，我将直接问你们。

你们有考试题目的副本吗？你们记得题目吗？好吧，我来调出题目。好了，我们从选择题开始。因为这时候，投影仪已经恢复了，我们可以在黑板上讨论图示内容。

你记得那个选择题吗？嗯，简短地说。好吧，我们没输入它，但我玩过它。我可以发布我的笔记，但它没有打字，所以是手写的。好了，我们从第一个问题开始。第一个问题是可分数据。第一个问题的A部分，你可能记得这里有一堆圆圈。

这里有一堆x。还有另外一个点混合在其中。所以这些看起来像是一组。这些则看起来像是另一组。问题要求你找到最大化间隔的超平面。你们一半做对了，另一半可能做错了。因为这是一个硬间隔问题，我们不允许任何松弛变量。所以这个答案是错的。这个答案是对的。

所以你必须考虑所有问题。它必须完全分隔数据。即使这意味着你需要放弃一个点来失去大量的间隔，或者反过来，获得大量的间隔。有什么问题吗？是的，硬间隔下不允许任何松弛。如果题目没有说明是软间隔，那就不允许，对吗？抱歉？我没听清。我的意思是，这很明确。

对吧？这就是一样的。你不需要额外指定它。你想要的是分隔超平面。分隔超平面永远意味着分隔超平面。你也不需要额外指定边距问题。是的，当然。我的意思是，看，我没有告诉你任何关于SVM的事，我也没有告诉你关于边距的事。

我只是给你这个，然后告诉你找到分隔超平面。你会怎么做？

分隔超平面。根据定义，简单来说，它必须将所有的点分开。分隔超平面唯一的作用就是将点分开。没有其他作用。好的，第二部分的问题是这里有一堆点，另一堆点在这条线上。请注意，它从零开始。

所以，你不能仅仅通过尝试，在中间放一个抛物线来分隔它。你怎么做呢？

如果你努力的话，你可以找到一个平移的抛物面，但是把参数调整正确会很难。你需要稍微把它抬高一点，并且按某种方式缩放，使它变得很整齐并且可以分开。那么我说的“找到能线性分隔数据的特征”是什么意思呢？

有两种方式来考虑这个问题。你可以使用特征来构造一个非线性分隔数据的函数。或者你可以把数据获取到手后，转换成一个新的空间。在那个空间中，数据是线性可分的。所以你们大部分人，在考试中写的例子，就是幻灯片中那个问题。那么，怎么分隔这个呢？你知道的。

yx的平方加上y的平方在这个案例中有效。它做了什么？嗯哼。如果原点没有居中，我可以把函数做一个平移。那就会是x1减去中心的x坐标，y1减去中心的y坐标。那么它做的事情是，当我添加这个函数时，它把这个嵌入到三维空间中。它具体地嵌入到三维抛物面中。

在抛物线的底部，我有那些值。而在抛物线的中间，我有那些值，所以我可以在它们被分隔的三维平面上切割它们，对吧？我有x1，x2，还有其他什么。我也可以在二维中做。也可以只考虑你知道的。

这其中的一部分，首先是一个维度，然后加上，x1的平方加上x2的平方。整个关键是将这些点抬起来，抬起这些点，同时保持这些点在低的位置。这样你就能将它们分开。你知道，以类似的方式，这可以通过反射来分开。数据从这里开始。如果你能将这些点反射到两侧。

那么你就能通过“赛博平面”来分隔这些点。这样说通吗？

你怎么反射它？所以如果你得到这个数据并且需要将数据转换到一个新的平面，在这个平面中它是可分的。一种方式，这个v形状是一个提示，一种方法是反射数据。你可以看看——你知道，这是x1轴，这是x2轴。如果我在这里取x1、x2，并将它映射到x1的绝对值和x2上，会得到什么图像？

是的？是的。对，它反射了它们。所以所有负的x1值变成了正值。所以这与右边重叠。所以我得到了一堆点，而不是这个。我得到了一堆点，而不是这一部分。这样就变得线性可分了。添加新特征的关键实际上是将你的数据转换到一个新的平面上，对吧？是的？

你可以对x1、x2等做这个操作——你也可以这样做。是的。唯一需要做的就是证明反射的概念有效，你可以通过取平方、绝对值或者其他方式进行反射。[听不清]，所以看，假设你在这里取一个数据点。

每当你感到困惑时，只需取一个数据点，采样数据点。假设这个数据点是1.2。那么你需要在新空间中找到这个表示，它会是什么呢？

这是显而易见的。它将变成1.2.1。所以我们取这个点，-1.2也会映射到1.2.1。如果你映射所有的点，你会真正意识到，在第三维度中，你实际上可以将它们切割开来。或者不完全是在第三维度中，但如果你从最后两个维度投影它。

你将能够将它们切割开来。[听不清]，是的，不，这样做不行。因为这样做的结果是，如果你加上常数C，它会有什么效果？

它只是将数据提升到三维空间。一切都保持在原位，但只是被提升了。所以如果你使用x1的绝对值，在三维空间中，你只需提升它们——因此在第三维度中，将只有正值。因此这里只会有正值。然后，如果你从侧面看它。

那么你将能够将它切割开来。它只是从几何上变得稍微复杂一点。[听不清]，是的。是的。是的。你可以仅使用x1的绝对值，是的。[听不清]，是的。是的。你可以仅使用x1的绝对值。是的。[听不清]，你可以使用x1的平方。你需要让我相信你了解反射的概念，只是没写出来。

[听不清]，x1的平方最终并没有形成抛物线。这就是它错误的原因。x1的平方只是这个。你怎么期望x1的平方能在这两个点之间形成一个抛物线呢？对吧？

试试看，你就会明白。取一个点。取两个点，试试它们是否分开。如果它们不分开。好的，下一个问题。优化中的步长方向。提醒你一下，优化中的步长方向，问你以下问题。如果你有一个函数 j，依赖于 v。有三种情况。在每种情况下，问题会问你：

判断以下是否正确。[听不清]，所有这些都是严格的。那么在第一种情况下，函数 j 是凸的且可微的。我们在负梯度方向上迈出一步，乘以某个 η，且 η 足够小。那么在第一种情况下，函数 j 是凸且可微的。那时会发生什么？W* 是最小值点。哪个是对的？

或者它们都不对也可以吗？有什么想法吗？当函数是凸的并且可微时，如果你在负梯度方向上迈出一步，当步长足够小，结果会怎样？值会减少吗？一定会。是的，除非它是平的，而它不会平，因为已知我们开始的点不是最小值点。

所以这是对的。它是否会让我们更接近最小值？是的，这也是对的。问题的第二部分，如果我们有一个凸函数 j，但它不可微呢？

然后我们选择一个方向，沿着负子梯度的方向迈出一步。它是否一定会减少值？不会。幻灯片中有一个反例。它是否一定会让我们更接近最优解？是的，有一个定理。第三部分。我们这次看的是总和，J 是凸函数的总和，是凸且可微的函数。

但是我们只在其中一个方向上迈出一步，即沿着它的导数方向，这就像第一次随机步骤一样。这是否一定会减少值？不会。它是否会使我们更接近最优解？不会，对吧？

它可以朝任意方向迈步。然而，我们知道它会带我们到最小值。[声音]，[声音]。

![](img/092825a2229a7f4b5c258377024d0ca0_3.png)

第三个问题从代数回归开始。然后它要求你证明只是求导数。所以在那个问题中，你做的就是求导并给出解的表达式。所以求导——当然，你需要使用一般形式，并且也需要对这个进行求导。

所以它是 x 转置乘以这个再乘以 2 λ v。如果你将其设置为 0 并展开，你会得到梯度等于 0 的条件，给出 x 转置 x 加 λ v。等于 x 转置 y，这就是第一个部分要求证明的内容。如果你将其带入 v 括号并深入——深入这个。

你得到 x 转置 x 加 λ i 的逆，乘以 x 转置 y。你怎么做这个呢？

你可以这样做，因为 x 转置 x 是半正定的。所以如果特征值是非负的，它可以是零。所以可能它不可逆。但由于我知道 lambda 是正的，我知道我可以移位所有--。如果有可能的零特征值，我就移位它。所以没有零特征值。

所以这个东西是可逆的。除非 lambda 为 0，这时你需要。确保 x 转置 x 是可逆的。那是你需要加上的额外条件。所以如果你写下这些，没问题。感知机问题，它是紧急的吗？[听不清]，。。。然后假设空间就是通常的 W 转置乘以 x。这个是，边距损失吗？

是的，怎么？边距损失和 SVM 在这个设置中就是感知机损失。以及带有感知机的铰链损失。它们是一样的。它们是相同的，函数。只不过是边距损失和幻灯片中的 SVM 设置。你知道那个吗？

所以边距是 y 乘以 f。所以边距是标签乘以预测，负标签。乘以预测。然后如果它在一边，你给出损失，当你分配。铰链损失时，发生什么？如果是正的，你保持这个值。如果是负的，你分配。零。那是一样的。所以从拓扑上讲，几乎是一样的。是的。

然后你只需写下这是边距，最小化边距，你知道的，在线。这应该能工作。我不确定它是如何创建的，但我相信它能工作。问题的第二部分要求你写出分离数据的超平面的条件。所以显然，答案应该涉及数据。你怎么做呢？是的，如果我有 x，y。

i，数据对，我只想让 y，i 乘以预测值严格为正。但是它必须是 4 o i。所以它覆盖所有数据。第三部分要求。第三部分说如下。如果你能找到完美的分隔符，那么损失为零。但这个命题是真的吗？如果你找到某个能给我们零损失的东西。

它是否意味着有完美的分离？不，怎么做到？你只需给出简单的计数，例如。它可以是吗？是的，它可以在超平面上，或者你可以仅有零预测。你知道，w 为零给你零损失，但它什么也不分离。或者是的，数据，位于 h 上。正则化感知机。所以正则化感知机。

目标函数是半范数的平方加上，我在这里有它加上 c 除以 n。数据的总和，再次是零或负的最大值，yi，w 转置 xi。这是对这个的升级吗？所以它在两部分上是可微分的，对吧？如果这是正的。严格正的就是可微的。如果这是严格负的，也是可微的。

所以唯一有问题的部分是零，在这一点上，我可以使用导数，对于其中一个或者介于两者之间的东西都可以。所以我只是写一个。它是 w，只是这个的导数加上。好吧，在这个问题中，没有，它只是，这个。在 2.1 中，就是这个函数。所以子梯度就是负的，c y x。

当如果这个是正的，那么它的负值是负的，否则它就只是给我零。所以我最终得到的是第一部分的导数。那就是 w 如果 y w 转置，x 大于等于零，对吧？这涵盖了 H 情况。第二部分是拉格朗日函数。是的。第二部分是什么？而不是解这个？这个。这个。加上。

你也可以这么做。第二部分要求你在给定约束的情况下写出拉格朗日函数。所以最小化它。J w 使得范数的平方大于等于 1。所以如果你记得，标准形式中，标准形式中，不等式约束。我们喜欢将它写成某个函数，小于等于零。所以就按这个方式重写。

这只是 1 减去范数的平方，小于零。那么我们可以写出拉格朗日函数。所以这个拉格朗日函数将依赖于新的参数 lambda。它是参数的 J 加上 lambda 乘以 1 减去范数 e 的平方。当然，对于不等式约束，我们有 lambda 正数。原始问题，它们是。

仅在幻灯片中。如果你需要解释，我也可以简要说明。但主要的形式，P 星将是下确界。所以我们最终想找到关于参数的下确界。所以它是参数的下确界，lambda 正数的拉格朗日函数的上确界。下一部分要求你找到对偶问题。那就是对偶问题。只要交换它们。什么时候。

你将它们交换为对偶形式，D 星是 lambda 的上确界。这些的下确界是 L V lambda。对偶函数是什么？对偶函数是只依赖于 lambda 的函数。它是这里的内部部分。你可以称它为依赖于 lambda 的某个函数 G。抱歉，你有对偶函数。在前面部分中提到，解释为什么这给出了。

和原始问题相同的最优值。让我回到这个，然后，是吗？

你本来要问这个问题。好的。是的，第三个问题的第二部分是解释。为什么这给出了相同的最优值。那么，为什么这给出了相同的最优值？

那么，lambda 正数的上确界对 lambda 正数的拉格朗日函数，这有什么作用？

让我们重写这个函数。它是 J 的 W 加上 lambda 乘以 1 减去范数 V 的平方的 lambda 正数的上确界。那么现在，你知道，这个上确界给我们带来了什么？

如果我碰巧有这个部分为正，那么上确界会发生什么？它会是无穷大，对吧？所以如果 1 减去范数 V 的平方为正，或者如果我有这个。所以如果约束没有得到满足，它会是无穷大。但如果约束得到了满足，会发生什么？它会给我，上确界会给我，正是这个。

所以这个 if 部分捕捉了约束条件，这部分捕捉了原始的最小化问题。因为在这个 soup 之后，我们取这个，这就是为什么你有这个 impsup 问题，对吧？

所以你可以把这个看作是以下内容。我们有一个最小化问题和一个约束条件。这里有两个操作，第一个操作是在这里，最小化关于V的值。第二个操作必须以某种方式刻画在上确界里，对吧？而且它实际上是这样包含在上确界中的。这就是第三部分的解释。

有什么问题吗？是的。哪个部分？嗯哼。是的。然后你可以用那个形式来做。是的。你可以进行解析。但是有时候你无法求导。你不知道导数是什么样子的。所以从拉格朗日形式来看，证明的方法是，嗯，这个。让我们结束这节课吧。我已经超时了。但我可以继续回答一些问题。

如果你有任何关于考试的问题或其他问题，我有一个问题。我刚刚去过，当你讲解这个的时候，我明白了，嗯，这个。我可以在我的大脑里看并做出来。是的。是的。

![](img/092825a2229a7f4b5c258377024d0ca0_5.png)

是的。是的。是的。是的。
