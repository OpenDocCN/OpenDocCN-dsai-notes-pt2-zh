- en: P20：20.Lab_April_21 - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P20：20.Lab_April_21 - Tesra-AI不错哟 - BV1aJ411y7p7
- en: Okay， so how would they exam？ How did it go？ Too easy， too short too long。 But
    it wasn't hard。 right？
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么考试是怎么进行的？怎么样？太简单了吗？太短了吗？太长了吗？但不难，对吧？
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_1.png)'
- en: Okay。 Can you read this？ It's not quite readable， right？ So I'm gonna try to。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。你能读懂这个吗？看起来不是很清晰，对吧？所以我会尝试。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_3.png)'
- en: Okay， so most of the problems were very self-descriptive， so if you were attentive
    to the text。 you would get a hang on to the solution。 That the text would guide
    you to the solution。 It's also a problem。 The first problem tells you that we
    sampled from the data we have。 then look at the averages of our predictions。 Then
    we compare what does the averages of predictions do compared to the。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，大部分问题都非常自描述，如果你注意到文本，你就能掌握解决方案。文本会引导你找到解决方案。这也是一个问题。第一个问题告诉你我们从已有数据中采样，然后查看我们预测的平均值。然后我们比较预测的平均值和实际值有什么不同。
- en: rather than having just one prediction。 So on one hand we have F。 On the other
    hand we have N different samples from D and we'll get the averages of N different
    predictions。 So these are identical samples and they also have the same size。
    So they have the same statistical properties。 All FIs in this question have the
    same statistical properties。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是只有一个预测。所以一方面我们有F，另一方面我们有N个来自D的不同样本，并且我们会得到N个不同预测的平均值。所以这些是相同的样本，并且它们的大小也相同。所以它们具有相同的统计特性。这个问题中的所有FIs具有相同的统计特性。
- en: Then clearly， but expectation of the average of something is just the expectation
    of the individual thing。 In this case the average is F-hat and the individual
    thing is just one of the Fs。 And we get by this equality here， the expected value
    of Z-bar is expected value of Z。 Why？
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么显然，某个东西的平均值的期望值就是单个事物的期望值。在这种情况下，平均值是F-hat，单个事物只是其中一个F。通过这个等式，我们得到Z-bar的期望值等于Z的期望值。为什么？
- en: Because expectation is linear。 In question？ Part two。 What about bias？
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因为期望值是线性的。在问题中？第二部分。那偏差呢？
- en: By us it's just the difference between the expected value of our predictions
    to the true value。 So it seems like it's a linear quantity。 Looking at the bias
    of Z and bias of Z-bar。 you see that they're really cool because of part one。
    They're expected values are really cool and mu is the true value which doesn't
    change depending on the side of the equation you are。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，它只是我们预测的期望值与真实值之间的差异。所以它看起来像是一个线性量。观察Z和Z-bar的偏差，你会发现它们真的很酷，因为第一部分。它们的期望值真的很酷，mu是一个不变的真实值，无论你在方程的哪一边，它都不会改变。
- en: So you get the same value。 Just most questions have simple one-liner answers。
    The third part asks about the variance。 Now in this case it's variance is not
    a linear operator。 That part can be a little tricky。 But all the information that's
    needed is given in the question。 What breaks the narrative is the first equation
    here in the hint that if you multiply around the variable with a constant then
    it's variance case with that constant square。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你得到了相同的值。大多数问题都有简单的一行答案。第三部分问的是方差。现在在这种情况下，方差不是线性算子。那部分可能有点棘手。但问题中给出的所有必要信息都很明确。打破叙述的是提示中的第一个公式：如果你将变量乘以常数，那么它的方差是常数的平方乘以原方差。
- en: Then using that， taking that 1 over m out gives you 1 over m squared times the
    variance of the sum。 Now you can break the sum apart using the second equation
    given in the hint which gives you m times variance of 1 of the random variables。
    I remember in the denominator we had 1 over m squared so 1 of the m's cancel gives
    you variance of Z divided by m。 The fourth part which one would you choose？ The
    one that has a lower variance is a better candidate to choose。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，利用这一点，将1/m提取出来，得到1/m平方乘以和的方差。现在你可以使用提示中给出的第二个公式将和拆开，得到m倍的一个随机变量的方差。我记得在分母中有1/m平方，所以一个m会抵消，得到Z的方差除以m。第四部分你会选择哪个？选择方差较小的那个更合适。
- en: If you are the one who gets to choose。 The one part 6 is just the description
    of random force。 So in part 5， since the general reason is the common sense reasoning
    is that you choose the one that has the lower variance in part 5。 What this equation
    tells you is that if you have correlations you tend to have higher variance so
    you want to use the one that has less correlations。 Because you want to use the
    one that has less variance， smaller variance。 That's enough。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是你来选择的话。第6部分仅仅是随机力的描述。所以在第5部分，由于常识推理的一般原因，你会选择那个方差较小的。在第5部分，这个方程告诉你，如果你有相关性，你通常会有更高的方差，所以你想选择那个相关性较小的。因为你想选择那个方差较小的。方差越小越好。就这样。
- en: And just because you would use the one that has less variance is enough of an
    answer。 Classification trees remember we had this simple tree。 And the thing we
    want so I saw some papers that got confused in this question on what to look at。
    The first part I saw some answers that somehow separated the data that has four
    items into past distance and open space。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为你选择了那个方差较小的就足够回答问题了。分类树记得我们有这个简单的树。而我们想要的是什么，我看到一些论文在这个问题上感到困惑，不知道该看什么。我看到的第一部分是，有些答案把包含四个项的数据分成了过去的距离和开放的空间。
- en: But what is asked is really the main thing is whether the place is desired or
    not。 So it's like a coin toast that you have four coins and the outcome is either
    heads or tails。 In this case outcome is whether the place is desirable or not。
    And then you have certain other properties like the coin is biased or the coin
    is like a dime or a quarter。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但真正被问到的主要问题是该地点是否是期望的。所以这就像是投掷硬币，你有四个硬币，结果要么是正面，要么是反面。在这种情况下，结果是该地点是否是期望的。然后你有一些其他的属性，比如硬币是偏的，或者硬币像一枚镍币或四分之一美元硬币。
- en: You look at other properties。 The main thing is whether it's desired or not。
    So each PI here it should actually refer to the probability that the place is
    desired or not。 So given the sample you have four elements in the sample。 What's
    the probability that a given place is desired？ It's one half。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要看其他属性。最主要的是是否是所期望的。所以这里的每个PI实际上应该指的是该地点是否是期望的概率。所以，给定样本中有四个元素。某个特定地点是期望的吗？它是二分之一的概率。
- en: There are two possible outcomes。 Yes or no？ It's like a binary problem。 So what
    you would have is the sum of two things。 First term is the yes term。 The second
    term is no term。 Then each PI is one half because judging by the data you extract
    that probability information then you get one。 Because log one half in base two
    is one also。 What do you mean when you use the formula entropy to say that it's
    not set for size？
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种可能的结果。是或不是？这就像一个二元问题。所以你得到的是两个事物的总和。第一个项是“是”的项。第二个项是“不是”的项。然后每个PI是二分之一，因为根据你提取的数据，你得到的概率信息就是一。因为二的对数一二分之一也是一。那么，当你使用公式熵时，你想说它不是为大小设置的是什么意思？
- en: Yeah， it is great。 So it's like in the same case， probability of a coin。 A coin
    is heads or tails。 So it's your omega is H or T。 That's what it refers to。 The
    size of omega here is two。 If I was looking at two coin tosses。 Yes。 So it's like
    your event is the coin toss event。 You can flip as many coins as you like。 The
    outcome will be heads or tails。 Your sample space。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，太好了。所以就像在同样的情况下，投硬币的概率。硬币是正面还是反面。所以你的Ω是H或T。这就是它所指的。这里Ω的大小是二。如果我看的是两次投硬币。是的。所以就像你的事件是投硬币的事件。你可以投掷任意数量的硬币。结果将是正面或反面。你的样本空间。
- en: right？ Because you are tossing it once。 You don't do it twice。 So tossing two
    coins in your row would mean buying two apartments。 You don't do that。 And your
    sample space is this and then you draw samples from that。 I think that many of
    us go from here to the analysis period。 You've got size of size。 Sorry。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？因为你只投掷一次。你不会投掷两次。所以在一排中投掷两个硬币就意味着买两套公寓。你不会这么做。你的样本空间就是这样，然后你从中抽取样本。我认为我们很多人都会从这里进入分析阶段。你得到了大小的大小。抱歉。
- en: It's back。 Okay。 I thought that you was coming to the calculations。 Yeah。 but
    in a sense that's testing a basic probability knowledge。 So what if I've given
    you results of five coin tosses？ How would you do？ Your N is five？ No。 it's not
    because your sample space is given。 The size of the sample space is known if you
    have it at hand。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回来了。好吧。我以为你会进入计算的部分。是的，但从某种意义上讲，这是在测试基本的概率知识。那么，如果我给你五次投硬币的结果，你该怎么做？你的N是五吗？不。不是的，因为你的样本空间是已知的。样本空间的大小是已知的，如果你手头有数据。
- en: So you're not coming up with the probability model of the problem。 No， I understand
    that。 But it's just a little confusing how it's written。 Well。 I think the description
    of the problem is pretty clear because it emphasizes that you should focus on
    the desired column。 And the desired column has two elements。 Either yes or no。
    There's nothing else in it。 Yes？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你并没有提出问题的概率模型。不，我理解这个。但是它的书写方式有点让人困惑。嗯，我认为问题的描述还是很清楚的，因为它强调你应该集中关注目标列。目标列只有两个元素。要么是“是”，要么是“否”。里面没有其他的。对吧？
- en: I used to do this because on the homework they define it like N as a number
    of elements that you have in the node。 Yeah， here it's the same thing。 There's
    the yes node and there's no node。 Right？ So yes。 there are two。 So how do you
    do this number of yes？ Number of elements which are yes。 A form of total number
    of elements which are the end of the node。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我以前是这样做的，因为在作业中，他们定义了N作为你在节点中拥有的元素数量。是的，这里也是一样。这里有“是”节点和“否”节点。对吧？所以是的。那里有两个。那么你如何做这个“是”的数量呢？“是”的元素数量，是所有元素总数的一部分，位于节点的末端。
- en: So you will be doing so according to number of two by two， or two by two。 And
    on that side you could be longer and longer。 But it doesn't， it gives you the
    wrong answer。 Not according to the solution。 It doesn't give you the correct interpretation。
    Right？
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你将根据二分之一的方式进行，或者二分之一的方式。在那一边，你可以做得更长。可是它给出的答案是错误的。不符合解答。它没有给出正确的解释。对吧？
- en: Then you have the each version of what this has to be given yet of one by two，
    one by two。 one by two， one by two。 Yeah， but that's confusing two things。 The
    first part actually in this。 you know， I read that there's no separation between
    leaves， right？
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你会得到每个版本需要满足的条件，例如一半一半，一半一半，一半一半。是的，但这会让两件事变得混淆。其实，第一部分是这样的，你知道的，我读到过，叶子节点之间没有分隔，对吧？
- en: You only have a set and you have four samples。 So in this problem before even
    starting to separate the data。 you start your parent node。 In the parent node
    there's no separation。 Then you focus on the parent node and you need to come
    up with the entropy of the parent node。 Now you don't even， you didn't even split
    the data。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你只有一个集合，且有四个样本。所以在这个问题中，即使在开始分隔数据之前，你首先从父节点开始。在父节点中没有分隔。然后你集中精力处理父节点，并且需要计算父节点的熵。现在你甚至没有，甚至没有拆分数据。
- en: And in the parent node you have four samples and from the four samples you gather
    probabilities because you don't know the actual distribution because those four
    are the only ones that you see。 That's why the first part wants you to focus on
    the desiredness because that's your main thing。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在父节点中，你有四个样本，从这四个样本中你获取概率，因为你不知道实际的分布，因为这四个是你唯一看到的。这就是为什么第一部分要求你集中关注目标值，因为那是你要关注的主要内容。
- en: And then looking at the features you'll separate。 Then comes the leaves。 So
    there will be three different ways of separating the data。 Each one looking at
    the different features， right？ The features， so， you know。 the text of the question
    somehow forces you to understand that， you know。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后查看你将用来分隔的特征。接下来是叶子节点。所以将会有三种不同的数据分隔方式。每种方式都会看不同的特征，对吧？这些特征，你知道，问题的文本某种程度上强迫你去理解，你知道的。
- en: desired is not the feature。 The desired is what we are looking at at the end，
    right？
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 目标值不是特征。目标值是我们最终关注的东西，对吧？
- en: The features that we are going to use to separate are pet's distance and open
    space。 So that's why the two parts， you know， it should be a hint that the first
    part is like introduction and the second part is three sub parts within itself
    that。 you know， forces you to deal with the problem itself。 Because even if you
    did the first part the way some of you did。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用来分隔的特征是宠物的距离和开放空间。所以这就是为什么这两部分，你知道，它应该是一个提示，第一部分像是介绍，第二部分则是包含三个子部分的部分。你知道，强迫你去解决这个问题。因为即使你按照某些人的方式做了第一部分。
- en: then how would you continue in the second part？ The second part asks you to
    calculate the parent entropy minus weighted average of the children entropy。 How
    are you going to come up with the weights？ What's the weight then？
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你如何在第二部分继续呢？第二部分要求你计算父节点的熵减去加权平均的子节点熵。你打算如何得出权重呢？那么权重是多少？
- en: If you start the separation by looking at the desired ability。 how are you going
    to calculate the w1 and the w2？ Right， it makes everything unclear。 But it's not
    unclear。 So you could somehow reverse engineer the problem to fix the first one。
    And don't get me wrong， more than half of you got the problem correct。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过观察期望能力开始分离，你如何计算w1和w2？对吧，这样会让一切变得不清楚。但其实并不不清楚。所以你可以以某种方式逆向推导问题，来修正第一部分。别误会，超过一半的人答对了这个问题。
- en: But the ones who didn't get it right， it's。 The focusing on the individual example
    gives you like probability is like 1/4 times log 1/4 and there are four terms。
    But many people also got partial credits for that too， so it's not the big loss。
    But I want you to understand the main issue here。 The third part for part C， in
    two。 you didn't have to do any calculations。 It would be enough。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但那些没有答对的人是这样。聚焦于单个样本，你会得到像1/4乘以log 1/4这样的概率，且有四个项。但很多人也得到了部分分数，所以并不算大损失。但我希望你能理解这里的主要问题。第三部分C中的第二题，你不需要做任何计算。这样就够了。
- en: So some of you also got credits if you did a wrong calculation in the first
    one。 but if the same carries out in the other parts， you get full credit for that
    because you don't want to be punished multiple times for the single mistake。 So
    even if you did part A and B wrong in part C， you said it's somewhere between
    A and B。 you get full credit for that part。 Because the point is that it's going
    to be。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你在第一题做错了计算，一些人也会得到部分分数。但如果在其他部分同样发生错误，你将获得满分，因为你不希望因为一个错误而被多次惩罚。所以即使你在C部分做错了A和B，你也说它介于A和B之间，你仍然会得到满分。因为关键是，它将会是。
- en: it's going to carry less information than one feature and more information than
    the other feature。 So it's information gauge should be somewhere between the two。
    You didn't have to go through the calculation。 And part three is just an obvious
    result from the looking at the three numbers。 Again， you could get the numbers
    wrong but the interpretation correct and you get full credit。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它将携带比某个特征少的信息，同时又比另一个特征携带更多的信息。所以它的信息量应该介于两者之间。你不需要进行计算。而第三部分只是从看三个数字的结果中得出的一个明显结论。同样，你可能会算错数字，但解释正确，仍然能获得满分。
- en: The third problem， the part one is just obvious。 Actually。 then when you got
    here waiting and you see that it is possible that the prediction is a 0。 the right
    way。 No， here it's given you that it's from Rd cross minus 1 to 1。 Which one？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第三题，第一部分是显而易见的。实际上，当你到这一步时，你会看到预测是0的可能性。正确的方法是？不，这里给出了从Rd乘积到-1到1的范围。是哪个？
- en: In the three。 G of X， use a zero value with no reference。 Here。 [inaudible]。
    But G is the return of the average one。 In part one it asks you to。 it asks you
    what the base classifier returns。 Base classifier is just GM， right？ So it's a
    little。 once it's not a presentation， it's a little hard to navigate。 So look。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三题中，G of X，使用零值且没有参考。这里。[听不清]。但G是返回平均值的函数。在第一部分中，它问你什么是基础分类器的返回值。基础分类器就是GM，对吧？所以这有点复杂。一旦不是演示的例子，就有点难以导航。所以看。
- en: the base classifier is this one。 There's a typo here。 So this GM is not GM。
    Of course。 it's indicator function of GM Xi is not equal to Y i。 So it's the zero
    one。 it's in the slides of the same of the corresponding section。 The other was
    just the linear combination of the predictions。 Now。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基础分类器就是这个。这里有一个打字错误。所以这个GM不是GM。当然，它是GM Xi不等于Yi的指示函数。所以它是零一函数。它在相应章节的幻灯片中有提到。另一个是预测的线性组合。现在。
- en: alphas are calculated on the way with the algorithm。 Then at the end you just
    take the sign of it。 Also just the。 the usual thing。 If you did the homework or
    if you read the slides you probably would have remembered it。 I think most of
    you got it right。 The part four。 of course you did it but it's hard to remember。
    So the description is given in the text。 And the key there is the optimization
    part。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔法（alphas）是通过算法的方式计算的。然后到最后，你只需要取它的符号。也就是常规操作。如果你做过作业或者看过幻灯片，你可能会记得它。我认为大多数人都答对了。第四部分，当然你做了，但很难记住。所以描述在文本中给出。关键在于优化部分。
- en: H and V are chosen to minimize the empirical risk。 So you just choose H and
    V。 You just write the loss function and find the argument of the two of them together。
    That was also part of the homework。 One part of the homework， the optional part
    of the homework。 even if you didn't solve it， the text of the homework was telling
    you that the title of the homework was that if you use the exponential function
    in FSAM。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: H和V被选中以最小化经验风险。所以你只需要选择H和V。你只需要写出损失函数，然后找到它们的两个参数。那也是作业的一部分。作业的其中一部分，是作业的可选部分。即使你没解决，作业的文本也在告诉你，作业的标题是，如果你在FSAM中使用指数函数。
- en: it gives you the other boost。 They correspond to each other。 So you just write
    the exponential loss and you're good to go。 Part six is a true or false。 What
    did you think？ Huh？ Yep， I mean now you see it but what do you think in the exam？
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它给你带来了另一种提升。它们相互对应。所以你只需要写出指数损失，你就可以开始了。第六部分是判断题。你怎么看？嗯？是的，我是说现在你看到了，但在考试中你怎么想？
- en: It's false because the hypothesis space is different。 Right one is the hypothesis
    space parameterized by V then it's V times X， V transpose times X。 Then the other
    is just it's combinations of this that gives you that's the larger space。 So it's
    not easy to recover。 The seventh part is just an estimation problem。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是错误的，因为假设空间不同。右边的是由V参数化的假设空间，它是V乘以X，V的转置乘以X。另一个则是它的组合，给你的是一个更大的空间。所以不容易恢复。第七部分只是一个估计问题。
- en: There is a typo that we fixed during the exam。 I hope it wasn't frustrating。
    So the typo carries over here。 It's not fixed here。 The point is not the fact
    that gamma m is less than 0。5。 The whole point is that gamma m is away from zero。
    and strictly away from zero so gamma m has to be strictly away from zero。 So there's
    a gap and why do we need the gap？ First of all， if I have a product。 how can I
    have that product go to zero？ I have to have its terms to be less than 1。 But
    I have to have its terms to be strictly less than 1。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在考试中我们修正了一个拼写错误。我希望这没有让你感到沮丧。所以这个拼写错误延续到了这里，还是没有修正。关键不在于gamma m小于0.5。重点在于gamma
    m远离零，并且严格远离零，所以gamma m必须严格远离零。这样就有了一个间隔，我们为什么需要这个间隔呢？首先，如果我有一个积，我怎么让它趋近于零？我必须让它的各项小于1，但我必须让它的各项严格小于1。
- en: Because if I have this product of am from mountain infinity， then if I know
    that am converges to 1。 I can't guarantee convergence of this product to zero。
    That's why I need gamma to be strictly away from zero。 The first point you needed
    to write was the fact that this square root is less than 1。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果我有这个从山到无穷大的am的积，那么如果我知道am收敛到1，我不能保证这个积会收敛到零。这就是为什么我需要gamma严格远离零。你需要写的第一个点是这个平方根小于1。
- en: but it's strictly less than 1， because it's strictly less than 1 minus 4 times
    gamma squared。 So that it doesn't come first to 1。 The second point is to notice
    that the range of values。 that the function can take is discrete。 It can be zero，
    1 over n， 2 over n。 So as soon as it's less than 1 over n， which will happen because
    it converges。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但它严格小于1，因为它严格小于1减去4倍gamma的平方。所以它不会首先达到1。第二点是要注意函数可以取的值的范围是离散的。它可以是零，1/n，2/n。所以一旦它小于1/n，这将会发生，因为它会收敛。
- en: you just establish the fact that it converges to zero， then it will be zero。
    It will reach zero。 and it will reach zero in finite time。 Yes。 I have to hold
    out if gamma is zero for example。 Yes。 It's less than a half。 Yes。 And you have
    a multiplication of a lot of ones。 Yes。 And never will I be reached zero。 Yes，
    true。 And then it doesn't work。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要证明它收敛于零，那么它就会是零。它会达到零，并且会在有限的时间内达到零。是的。如果gamma是零的话，我得坚持下去。例如。是的。它小于一半。是的。你有很多个1相乘。是的。而且永远不会达到零。是的，没错。然后它就不起作用了。
- en: That's why gamma has to be away from zero。 This is a fixed number。 That's away
    from zero。 Make maybe your confidence is difficult。 This is fixed。 So this first
    line。 suppose we have gamma m less than gamma less than 0。5 is no longer the case。
    The correct version is。 suppose we have zero less than gamma less than gamma m。
    And that less than one half actually。 Yes。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么gamma必须远离零。这是一个固定的数字。它远离零。可能是你的信心很难接受。这是固定的。所以这一行，假设我们有gamma m小于gamma小于0.5，这不再成立。正确的版本是，假设我们有零小于gamma小于gamma
    m。并且gamma小于一半。是的。
- en: Could you explain why you can improve on the test performance after you have
    zero or one on the train？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你能解释为什么在训练后，你能提升测试表现吗？
- en: It's not a problem， but we want to know if I can get it。 It doesn't guarantee，
    right？
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是问题，但我们想知道我是否能得到它。它并不能保证，对吧？
- en: It doesn't guarantee anything about the test。 Does it？ Typically， yes。 That's
    a generalization problem。 Like generalization problem in machine learning in general
    refers to whether finding a minimum in training set gives you a minimum in the
    test set。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它并不能保证测试的任何结果，对吧？通常是的。这就是泛化问题。像机器学习中的泛化问题通常指的是，是否在训练集上找到的最小值能在测试集上也找到最小值。
- en: Sometimes it doesn't。 We know it doesn't because there's a phenomena called
    overfitting。 Sometimes you find lower， lower points。 It gives you a higher error。
    You maybe shouldn't have done that。 Maybe you should have controlled your test
    set or validation set before it starts to increase。 You remember the typical bottom
    of training curve going like this and the test curve going on like that somewhere。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有时不能。我们知道它不能，因为有一种现象叫做过拟合。有时你会找到更低的点，它会给你更高的错误。你也许不该这么做。也许你应该在损失开始增加之前控制你的测试集或验证集。你记得典型的训练曲线底部是这样的，测试曲线则是这样的，某个地方会发生这种情况。
- en: That's the generalization problem。 Like generalization problem has different
    effects on different problems。 At that， I don't want to take any reason。 It just
    increases。 And it's right here。 But it doesn't。 This can go to zero。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是泛化问题。泛化问题对不同问题有不同的影响。在这种情况下，我不想给出任何原因。它只是增加了。而且它就在这里。但它并没有。这可以趋近于零。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_5.png)'
- en: This is the。 So I don't understand the mechanism that allowed you once you got
    zero error。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是。所以我不理解当你得到零误差时，是什么机制允许你继续。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_7.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_7.png)'
- en: But once you get zero error in the train set， you memorize data。 I remember。
    maybe I'm remembering incorrectly， but you thought I remembered that there were
    cases where you could continue improving even though perfectly labeled your training。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但是一旦你在训练集上得到了零误差，你就记住了数据。我记得，也许我记错了，但你曾认为我记得有些情况是，即使你的训练数据标签完全正确，仍然可以继续改进。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_9.png)'
- en: Oh yeah， you can。 You can separate the data perfectly， but depending on the
    loss。 your loss can still decrease。 So that's different because I think you're
    asking。 "Suppose you have this data， this separates the data perfectly。"， But
    depending on your loss。 this function will have a lower loss value。 So the fact
    that you separated the data doesn't mean that you found the lowest possible training
    cost。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，是的，你可以。你可以完美地分开数据，但根据损失，你的损失仍然可能会减少。所以这是不同的，因为我觉得你在问：“假设你有这些数据，完美地分开了数据。”但根据你的损失，这个函数可能会有一个更低的损失值。所以，分开数据并不意味着你找到了最低的训练成本。
- en: Your training cost is still parameterized and depends on where you put the exact
    line。 And it probably has some sort of landscape like this in the parameter domain。
    And its values will be the loss values。 Maybe this separates your data。 but maybe
    finding another local minimum here will give you a lower loss。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你的训练成本仍然是参数化的，取决于你在何处设置精确的边界线。它可能在参数域中呈现出某种类似这样的景观。它的值将是损失值。也许这能分开你的数据。但也许在这里找到另一个局部最小值会给你一个更低的损失。
- en: And that might have better generalization or that might have worse generalization
    depending on your data。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 而这可能会有更好的泛化，也可能会有更差的泛化，具体取决于你的数据。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_11.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_11.png)'
- en: Does that clarify your previous question？ So part eight of it is that of course
    if you have the situation in which you have the same mix but different wise。 then
    what's going to happen is that you won't have this non-convergence to one。 You
    had。 you guaranteed that the factor didn't converge to one， but now it's not the
    case anymore。 So the assumption of the previous part fails， hence the result also
    holds true。 Yeah？
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这样能澄清你之前的问题吗？所以它的第八部分是，当然，如果你遇到的是相同的混合数据，但处理方式不同，那么发生的情况就是，你将不会有这种非收敛到1的现象。你曾经保证这个因素不会收敛到1，但现在不再是这种情况。所以前一部分的假设失败了，因此结果也仍然成立。对吧？
- en: Sorry to ask you， Matt。 And you said in the entire site that you would have
    the u reach zero in finite。 which means that。 Yes。 But。 And has the only thing
    if you're equal to zero， right？ True， right。 So it's infinite， not finite。 Yes。
    Why does it converge？ So many people didn't get it。 It's good to emphasize。 So
    why does it go in finite？
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉问你，Matt。你在整个站点上说过，你会让 u 在有限的时间内达到零。这意味着是吗？是的。但只有当你等于零时才行，对吧？对，没错。所以是无限的，不是有限的。是的。那为什么它会收敛呢？很多人没有明白。强调一下为什么它会趋于有限？
- en: Even if we need M to vote infinity in order for that function to vote zero。
    What's the possible values of that function？ We are looking at the empirical risk，
    right？ Yes。 The risk is discrete。 It can be zero， one over one， two over one，
    all the way up to one。 So I know that since it goes to zero as M goes to infinity，
    I know that there is a finite M。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们需要让 M 趋向于无穷大，才能让那个函数趋向于零。那个函数可能取哪些值呢？我们正在关注经验风险，对吧？是的。风险是离散的。它可以是零，1，1 以上，直到一。所以我知道随着
    M 趋近无穷大，它会趋向于零，我知道有一个有限的 M。
- en: let's say capital M， that's large enough。 I don't know how large。 but it's large
    enough so that at some point it's going to be some positive number between zero
    and one over M。 Okay。 Right。 Yeah。 Once I prove that the empirical loss is less
    than some number that's between zero and one over M。 what can it be？ It has to
    be zero。 It has to be exactly zero because it doesn't take any other values。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 M 很大，我不知道多大，但它足够大，直到某一点，它将是一个介于零和 1/M 之间的正数。好吧，对。是的。一旦我证明经验损失小于一个介于零和 1/M
    之间的数字，那它能是什么呢？它必须是零。它必须是准确的零，因为它不会取任何其他值。
- en: This range is limited。 But the range of values that that function can take。
    the function M tried to bound。 If I show that it's less than this。 then there
    are three possible values。 This is zero， this is one over M。 I'm telling you that
    there's some M， some capital M less high enough that that function will be less
    than some point here。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个范围是有限的。但那个函数可以取的值的范围，M 函数试图进行界定。如果我证明它小于这个值，那么有三个可能的值。这是零，这是 1/M。我告诉你有一个 M，有一个足够大的
    M，使得那个函数小于这里的某一点。
- en: If it's less than some point here， it can be anything in between。 It can be
    that as well。 It can be that point。 It has to be zero。 And that capital M is the
    finite number that you are looking for。 In the multi-class classification， we
    have to form the function C， C is x cross y， x is in R2。 Our samples are drawn
    in R2。 And y is the set that has only three elements， one， two， three。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它小于某个点，它可以是任何介于其中的值。它也可以是那个点。它必须是零。这个大 M 就是你在寻找的有限数。在多类分类中，我们必须构造函数 C，C 是
    x 交 y，x 在 R2 中。我们的样本在 R2 中抽取。而 y 是只有三个元素的集合，1，2，3。
- en: And of course， the D is going to be six in this case。 It's just a simple representation
    of C。 And this F is just rewriting of what's here， so did this part would be enough？
    In the second part。 the delta function is given in the table。 I mean， just have
    to put things in place and do the calculation。 So you can take the i to be one
    here or one to be i。 It doesn't matter。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，D 在这种情况下将是六。它只是 C 的一个简单表示。而这个 F 只是对这里内容的重写，那么这部分内容就足够了吗？在第二部分，delta 函数在表中给出。我的意思是，只需要把东西放到位然后做计算。所以你可以把
    i 取为 1 或者 1 取为 i，这没关系。
- en: It's just the outputs of h is for the sample x we have in hand。 So if you call
    it x1。 then we can calculate this core function because we know delta， we know
    h's， right？
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是 h 对我们手头的样本 x 的输出。所以如果你把它叫做 x1，那么我们可以计算这个核心函数，因为我们知道 delta，我们知道 h，对吧？
- en: We can calculate them。 So once we calculate， it's going to be zero， two， and
    so before we calculate。 will it predict the correct class？ Yes， because the arg
    max of h， maximum of h is two。 which is obtained at the prediction y， prediction
    one。 And it was actually coming from the first class， so it's going to give us
    the correct answer。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算它们。所以一旦我们计算，它会是零、二，那么在我们计算之前，它会预测正确的类别吗？是的，因为 h 的 arg max，h 的最大值是 2。这个最大值是在预测
    y 时获得的，预测为 1。而它实际上来自于第一类，因此它会给我们正确的答案。
- en: And then looking at the table and looking at the values given here， just write
    the function。 Oh。 And the function， the loss function， your loss value will be
    two。 Part three is using the same thing parameterized involved c than average
    over n samples。 And the last part is the sub gradient of this。 So in finding the
    sub gradient。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看一下表格，查看这里给定的值，直接写出函数。哦，损失函数，你的损失值将是二。第三部分使用相同的参数化形式，涉及 c 并对 n 个样本求平均。最后一部分是次梯度的计算。所以在寻找次梯度时，
- en: one issue that we have here is the maximum。 Got to somehow manage around the
    maximum and that is given in the hint。 So how do you get rid of the maximum？ Since
    you know that g in gradient fk implies g in gradient f。 so it's enough if you
    find a sub gradient of fk， right？
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里的一个问题是最大值。必须以某种方式绕过最大值，而最大值在提示中给出。那么，如何去除最大值呢？既然你知道梯度 fk 中的 g 隶属于梯度 f，那么如果你找到
    fk 的一个次梯度就足够了，对吧？
- en: And you do that by just plugging one specific y value， which is given which
    line。 Here， this line。 So just clog this y-head i to get rid of max。 Once you
    get rid of max。 you're able to write the sub gradient。 Since the first part doesn't
    depend on v。 only this part depends on v and gradient of this is just simple linear
    algebra。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过代入一个特定的 y 值来实现，这个值是给定的，位于这一行。就在这里，直接代入这个 y 头 i 来去除最大值。一旦去除了最大值，你就能够写出次梯度。由于第一部分不依赖于
    v，只有这一部分依赖于 v，且该部分的梯度仅仅是简单的线性代数。
- en: Turnalized logistic regression。 Just putting everything together again in the
    same way。 given in the text that you know your functions， your hypothesis space
    is the linear hypothesis space。 the inner product of v and cx。 You need to have
    the square loss。 so you write the norm squared of v of the parameters。 And there's
    a regularization parameter。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化的逻辑回归。将所有内容重新组合在一起，就像文本中给出的那样，你知道你的函数，假设空间是线性假设空间，v 和 cx 的内积。你需要平方损失函数。所以你写出
    v 的参数的范数平方。并且有一个正则化参数。
- en: Just put it in front of it。 Then the rest is of course the un-agularized loss。
    which is the average of the， data of the loss function and loss function is given
    a loss function above。 Just replace n with the， again with what's given here y
    times f of x and f of x f comes from the hypothesis space。 So f of x is just v
    times cx。 Put everything together and write the term。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 就把它放在前面。然后剩下的当然是未正则化的损失函数，它是数据损失函数的平均值，损失函数如上所示。只需将 n 替换为，这里给出的 y 乘以 f(x)，而
    f(x) 来自假设空间。所以 f(x) 就是 v 乘以 cx。将所有内容结合起来，写出这个项。
- en: So we're calling for a mid-1。 Here c by m， we'll see。 If c is constant， that's
    fine。 but if c is a parameter then we don't have it， right？ We don't have that
    parameter here。 But if it's constant， then that's fine。 And now， so we want to
    use the representers theorem in the next part。 We want to have the two forms look
    similar， so what we do is we take r as the square function。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们调用一个 mid-1。这里的 c 除以 m，我们会看到。如果 c 是常数，那没问题，但如果 c 是参数，那就没有了，对吧？我们这里没有那个参数。但如果它是常数，那么没问题。现在，我们想在下一部分使用表示定理。我们希望这两种形式看起来相似，所以我们做的是将
    r 作为平方函数。
- en: It takes the number x， gives x squared times some longer。 And the point is that
    x squared satisfies the assumptions of the representers theorem。 which is the
    fact that it has to be strictly increasing。 Now what do we do？
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 它取 x，给出 x 平方乘以某个长度。重点是 x 平方满足表示定理的假设，即它必须严格递增。那么我们该怎么做呢？
- en: If we know that the assumptions of the theorem satisfy， we can find a solution
    that's given。 that's described in the body of the theorem。 So we just write the
    candidate solution as linear combinations of c's and then plug it in the inner
    product。 Once we plug it in the inner product， we can take the sum out because
    inner product is a bilinear function in each variable。 so we can take the constants
    out and the sum out in the same manner。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道定理的假设成立，我们就能找到给定的解。这在定理的正文中有所描述。所以我们只需要将候选解写成 c 的线性组合，然后代入内积中。一旦代入内积中，我们可以把求和符号拿出来，因为内积是每个变量的双线性函数。所以我们可以以相同的方式将常数和求和符号提取出来。
- en: And that gives us the inner product of c's only inside， so we get rid of the，
    you know。 take the coefficients out。 And what's that inner product？ That inner
    product is the kernel。 It's again given in the body of the problem。 So we just。
    so the law of line is just a notation difference。 This is the kernel of xi x and
    we kernelize that part。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们仅仅包含c的内积，因此我们去掉了，你知道的，把系数提取出来。那个内积是什么？那个内积就是核。它再次在问题的主体部分给出。所以我们只是。线性法则只是一个符号上的区别。这是xi
    x的核，我们把那部分核化。
- en: And we continue in the third part of the question。 So now we want to write all
    the。 remember the J of V was this。 Now we want to write everything we see here
    in terms of the new notation。 Now that， you know， we just reformulate everything
    in terms of x is what's V squared。 norm V squared。 It's， we have norm of x is
    just inner product of x for my x squared root。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续第三部分的问题。现在我们想要把所有的东西都写出来。记得V的J是这个。现在我们想要用新的符号来写出这里看到的所有内容。现在，你知道，我们只是将一切重新公式化为x就是V的平方，V的范数。我们有x的范数就是x和x的内积开方。
- en: You can write the norm in terms of inner product。 That gives you the first term
    of J。 And for the second term of J， what do we need？ We need to write this inner
    product， V times c。 So we plug V in that part， again taking the coefficients out，
    kernelizing is in the second part。 We just get the representation of the loss
    we want in terms of alpha， in terms of the coefficients。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用内积表示范数。这样就得到了J的第一项。对于J的第二项，我们需要什么？我们需要写出这个内积，V乘以c。所以我们将V代入那部分，再次提取系数，核化是在第二部分。我们就得到了我们想要的损失表示，用α表示，用系数表示。
- en: It should be straight forward。 Any questions？ [silence]， [silence]， Okay， so
    the sixth problem。 square loss for classification。 We have the， we have the square
    loss by minus prediction squared。 obviously， but in this probability of accepting，
    we'll look at it given the data so it's given x and we take expectation after
    that。 So one thing you should know in this problem is that， [silence]。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是直截了当的。有什么问题吗？[沉默]，[沉默]，好的，那么第六题。分类的平方损失。我们有平方损失为真实值减去预测值的平方，显然如此，但在这个接受概率中，我们会根据数据来查看它，所以它是给定x之后再求期望。所以你在这个问题中应该知道的一点是，[沉默]。
- en: So what is the thing that minimizes the square loss？
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么东西最小化了平方损失呢？
- en: Do you remember some of you taking the class loss and this and so on？
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你们有些人还记得上过课堂，损失和这些内容吗？
- en: You might remember the fact that the expected value， expected。 so the conditional
    expectation minimizes the square loss。 But。 I mean you didn't need to know that
    for the exam but it's， it's， it's important to remind yourself。 So here， what
    do we have， we have with， so the problem again describes itself。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，期望值，期望。条件期望最小化了平方损失。但是，我是说，你并不需要知道这些内容来考试，但它，重要的是要提醒自己。那么，这里，我们有什么呢，我们有，问题再次自我描述。
- en: We have to form expected value of the square loss given x， write it out。 take
    the derivative with respect to y-head and solve for the parameter you want to
    solve。 And why is it script random variable？ So given x， there are two values
    for y。 it's either 1 or minus 1。 So given x， I can write it， oops。 Given x， if
    there are two cases。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须计算给定x的平方损失的期望值，把它写出来。对y-hat求导并求解你想要的参数。为什么它是一个随机变量？所以，给定x，y有两个值，它可以是1或者-1。所以给定x，我可以写出，哎呀。给定x，如果有两种情况。
- en: y is either 1 or minus 1。 If it's 1， it's 1 with probability p。 Then it's p
    times 1 minus y-head squared， which is the function inside。 Plus。 the other option
    is that y is minus 1， so here is y minus 1 in the second term。 And that happens
    with probability 1 minus p。 And this is the function here。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: y的值要么是1，要么是-1。如果是1，它以概率p为1。那么它是p乘以1减去y-hat的平方，这就是里面的函数。加上。另一个选项是y是-1，这里在第二项中是y减去1。这发生的概率是1减去p。这就是这里的函数。
- en: So the expected value of functions of random variables is taken in the same
    way。 So here。 maybe probabilistic fact which， no， was how to calculate expected
    value of this。 expected value of f of x in terms of the distribution of x。 Once
    you find this expression。 the rest is more straightforward。 You want to find the
    argument， so you just take the derivative。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量的函数的期望值是以相同的方式计算的。所以这里，也许是概率事实，如何计算这个期望值。f(x)的期望值可以通过x的分布来表示。一旦你找到这个表达式，其余的就更直接了。你要找到参数，所以你只需对其求导。
- en: just take the y-head， then set it to 0， solve for y-head。 Y is 2p minus 1。 And
    p here。 in the solution for simplicity， was representing pi of x。 So the actual
    solution you should have written should be f star of x is equal to 2 pi x minus
    1。 Then， of course， the last part is just solve pi for f。 So just a simple algebra。
    Second part。 So。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 只需要取y-head，然后将其设为0，解出y-head。Y是2p减去1。这里的p，在解法中为了简化，表示的是x的pi。所以你实际应该写出的解是f star
    of x等于2 pi x减去1。然后，最后一步就是解pi为f。这个只是简单的代数运算。第二部分。所以。
- en: I showed that the square loss is a margin-based loss。 A margin is y-y-head。
    l-fam is 1 minus m squared， then how do you move forward？
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了平方损失是基于边际的损失。边际是y-y-head。l-fam是1减去m的平方，那么接下来怎么做？
- en: The loss that you had was y minus y-head squared。 If you write it out。 you get
    the terms y times y-head。 So you can group them in terms of y times y-head。 Maybe
    the tricky thing is。 I mean， the hint here tells you that this y squared is always
    1 because it's either 1 or minus 1。 This part is fine。 We don't have to deal with
    this because this is m already。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到的损失是y减去y-head的平方。如果你展开，你得到y乘以y-head的项。所以你可以按y乘y-head的形式分组。可能棘手的地方是，意思是，提示这里告诉你，y的平方始终为1，因为它要么是1，要么是-1。这部分没问题。我们不需要处理这个，因为这已经是m了。
- en: I can write it in terms of m。 And this y-head squared， again， I can write it。
    You know why？
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以用m来表示它。再一次，我可以将y-head的平方写出来。你知道为什么吗？
- en: Since y squared is 1， I can multiply y-head squared with whatever I want。 So
    y squared times y-head squared is。 I can add that factor to write it in terms
    of m。 The whole point is just finding a way to extract m in the equation。 Then
    I get 1 minus m squared。 Conditioned expectations。 You're given the family of
    functions we have。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于y的平方是1，我可以用任何我想要的东西乘以y-head的平方。所以y的平方乘以y-head的平方是。我可以加上这个因子，将其写成m的形式。关键是找到一种方式在方程中提取m。然后我得到1减去m的平方。条件期望。给定我们拥有的函数族。
- en: And you're given a map from W。 Any questions？ From W。 T。 X to lambda。 So given
    X。 my parameter lambda can be characterized using the parameter V as V transpose
    times X。 Then I can just replace the function here， lambda times e to the minus
    lambda y。 by what I have lambda to be when I'm given X， which is again rewriting
    this function here。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 给定从W到lambda的映射。有问题吗？从W到T。X到lambda。所以给定X，我的参数lambda可以用参数V来表征，V转置乘以X。然后我可以替换这里的函数，lambda乘以e的负lambda
    y。通过给定X的情况下lambda的值，即重新写出这个函数。
- en: replacing lambda with exponential W transpose times X。 Second part。 Given data。
    given many variable samples like this， they're independent。 so I can write the
    joint distribution as the product of their individual distribution。 That's the
    independent sampling assumption。 Normally you just have to write。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将lambda替换为指数的W转置乘以X。第二部分。给定数据，给定像这样的多变量样本，它们是独立的。所以我可以将联合分布写成它们各自分布的乘积。这就是独立采样假设。通常你只需写出。
- en: Parameterized by V of y given X。 They're vectors。 But if you know that they're
    in the family。 you can just factor the map and multiply them。 And in the multiplication
    you replace it with the expression you found in the first part。 And again， load
    likelihood is just taking the load。 but in the load likelihood what's important
    is that you can always get rid of the terms that doesn't involve your parameters。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由X给定的V参数化的y。它们是向量。但是如果你知道它们属于这个族，你可以将映射因子化并相乘。在乘法中，你用第一部分找到的表达式替换它。再次强调，似然函数仅仅是求似然，但在似然函数中重要的是你总能去掉那些不涉及你的参数的项。
- en: You remember in the Gaussian case， if you look at the load likelihood you have
    all this product of factors of 1 over square root 2 pi's。 All that doesn't matter
    in your load likelihood because you are not interested in the values of the load
    likelihood。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你记得在高斯情况下，如果你看似然函数，你会看到1除以根号2 pi的各个因子的乘积。所有这些在你的似然函数中并不重要，因为你并不关心似然函数的值。
- en: You are interested in the location in the domain that maximizes the load likelihood。
    That's the whole point of doing maximum likelihood estimation。 Taking log just
    makes things easier to do the differentiation。 And as another sign that the values
    of that logged function is not important。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你关心的是最大化似然函数的域中的位置。这就是做最大似然估计的全部意义。取对数只是让微分变得更容易。作为一个信号，表明该对数函数的值并不重要。
- en: The location of the point in the domain is important。 That's what matters for
    you。 And that's represented in the fact that you are interested in is argmax of
    JFW in RD。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 点在领域中的位置很重要。对你来说这才是关键。而这体现在你所关心的是RD中JFW的argmax。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_13.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_13.png)'
- en: What numerical methods did you use？ The minimum optimization would be good。
    This describes the Schrasse gradient as a sample because we just take one sample
    from the whole sum。 And we write the derivative of that sample and then we take
    a step in the negative of the gradient of that particular example。 multiplied
    by some positive step size。 Just the description of a Schrasse gradient descent。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你用了什么数值方法？最小化优化应该是好的。这描述了Schrasse梯度作为一个示例，因为我们只是从整个总和中取一个样本。然后我们写下那个样本的导数，然后我们朝着该特定示例的梯度的负方向迈出一步。乘以一个正的步长。这就是Schrasse梯度下降的描述。
- en: I could have used the sum that would be an ingredient descent。 And you don't
    have to find the whole algorithm。 It's just the step。 Again， in the same way。
    it's the function approach。 Now it's some function in the exponential。 Just write
    the J。 Again。 take the derivative with respect to F。 It's pretty much the same
    way of taking the derivative with respect to a variable。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我本可以使用那个和成分下降相关的总和。你不必找到整个算法。只是那个步骤。再次地，用相同的方法。这是函数方法。现在它是指数中的某个函数。只需写下J。再次地。对F求导。几乎就是对变量求导的相同方式。
- en: If you parameterize your function， then you could use chain rule to use this
    formula and apply it to chain rule to find the original expression with all the
    parameters you have for the purpose of coding。 Then in the same way， it's just
    the argument over the hypothesis space of your loss function。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对你的函数进行参数化，那么你可以使用链式法则来使用这个公式，并将其应用于链式法则，以找到你为编码目的所拥有的所有参数的原始表达式。然后以同样的方式，就是在你的损失函数的假设空间中取参数。
- en: the gradient of the loss function。 I guess that's the end of the exam。 There
    are no more pages。 Any other questions regarding the exam？ I think there are a
    few typos。 Once they are fixed。 David might post it online。 He might not post
    it online， but if he doesn't， he'll put it on PESA。 So he'll have access to it，
    but I think it's better if exams are somehow not like public access to the classroom。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的梯度。我猜这就是考试的结束。没有更多的页面了。关于考试还有其他问题吗？我认为有一些拼写错误。一旦修正，David可能会在线发布它。他可能不会在线发布，但如果不发布，他会把它放在PESA上。这样他就能访问到它，但我认为最好是考试不要像公开课堂访问那样。
- en: But I don't know if the video is accessible， so maybe we already broke that
    part。 What do you expect from the back？ I mean， if you think， let me think。 Hopefully
    next week in class。 or maybe some other time， then， you know， if you don't bother。
    Sometimes next week， I believe。 [Loud laughter]， you。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但我不知道视频是否可以访问，所以我们可能已经弄坏了那部分。你对后面的部分有什么期待？我的意思是，如果你想的话，让我想想。希望下周在课堂上见，或者也许其他时间，如果你不介意的话。有时候下周，我相信。[大声笑]，你。
- en: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_15.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5f9bafb53682bb9dee42b6fa388c6d6_15.png)'
