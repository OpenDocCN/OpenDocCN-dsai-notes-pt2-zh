- en: P22：22.Lab_April_28 - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P22：22.Lab_April_28 - Tesra-AI不错哟 - BV1aJ411y7p7
- en: All right。 Today we're going to talk about the e and algorithm with specific
    application。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。今天我们将讨论e和算法的具体应用。
- en: '![](img/362c510b7786023274101d45cf5e21b5_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/362c510b7786023274101d45cf5e21b5_1.png)'
- en: to the Gaussian mixture model。 All right。 So remember， a Gaussian mixture model
    we have。 components from which our data points are drawn and each component has
    a probability。 which we designate with pi and then each component has associated
    with a probability。 distribution or density in this case。 We have a different
    Gaussian density associated。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型。好的。记住，高斯混合模型我们有。数据点是从其中抽取的组件，每个组件都有一个概率。我们用π表示，然后每个组件都有一个与之关联的概率分布或密度，在这种情况下。我们有一个不同的高斯密度相关。
- en: with each of three components and each of these components would have a probability。
    of generating a point。 So the generative model description that we're dealing
    with in Gaussian。 mixture models is first we draw a variable z which is the cluster
    assignment or component。 assignment from， have you guys heard of the categorical
    distribution？ It's like the blue。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 其中包含三个部分，每个部分都有生成一个点的概率。所以我们处理的高斯混合模型的生成模型描述是，首先我们从分类分布中绘制一个变量z，表示聚类分配或组件分配，大家听说过分类分布吗？就像是蓝色。
- en: distribution is either zero or one。 Categorical distribution is one of k categories。
    It's。 like picking a number from one to k。 It's related to the multinomial distribution
    but。 the multinomial distribution is if you chose n draws from a categorical distribution
    and。 you count it up how many of each type of number of one to k you got。 Okay，
    so certainly。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 分布是零或一。分类分布是k个类别之一。就像从1到k之间选择一个数字。这与多项分布相关，但是多项分布是指从分类分布中选择n次抽样，并统计你得到的1到k之间每个类别的个数。好的，所以显然。
- en: keep a categorical and multinomial related。 So we choose a cluster assignment
    z from the。 categorical distribution and then we draw a point x from the density
    associated with， cluster z。 Okay， so that's how we generate our data points x。
    Is that clear？ Questions？ Okay， good。 So that's a generative model and we can
    put that together into a joint distribution。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 保持分类分布和多项分布相关。所以我们从分类分布中选择一个聚类分配z，然后从与聚类z相关的密度中绘制一个点x。好的，这就是我们生成数据点x的方式。明白了吗？有问题吗？好的，明白了。这是一个生成模型，我们可以将它们组合成一个联合分布。
- en: for a pair x and z where of course x is observed but z will generally not be
    observed and this。 is simply definition of conditional probability。 But now let's
    plug in our conditional distribution。 of x given z。 That's that Gaussian and then
    pi z。 That's our probability of cluster z。 Okay。 so that's our joint but what
    we really need to work with is the marginal distribution。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一对x和z，当然x是观察到的，但z通常不会被观察到，这。只是条件概率的定义。但是现在让我们代入条件分布。给定z的x分布。那是高斯分布，然后是πz。这是我们聚类z的概率。好的。所以这是我们的联合分布，但我们真正需要处理的是边缘分布。
- en: The marginal distribution for the data that we observe which is x。 We only observe
    x。 We。 don't observe z。 So we can get a density for x， a distribution for x simply。
    What do we， do？
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的数据的边缘分布是x。我们只观察到x。我们。没有观察到z。所以我们可以得到x的密度，x的分布。我们该怎么做？
- en: It's very easy。 Yeah， right， we sum out over z which we don't observe。 That's
    marginalization。 like this。 Right， we take the joint， we sum out over z， we get
    p of x and then of course。 we could just substitute in our expression for the
    joint like that。 So this is our marginal。 distribution for a single x。 Great。
    So one way to estimate the parameters of the Gaussian。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。是的，没错，我们对我们没有观察到的z进行求和。这就是边缘化。像这样。对吧，我们取联合分布，对z进行求和，得到x的概率，当然。我们可以像那样直接代入联合分布的表达式。所以这是我们单一x的边缘分布。太好了。这样一种估计高斯分布参数的方法。
- en: mixture model is with maximum likelihood。 I mentioned yesterday that there's
    a method。 of moments approach also which is turning out to be very nice and you
    can learn about。 that in a second class。 Sontag's class I think。 Okay， but we're
    doing maximum likelihood， here。 So we're going to write down the likelihood of
    the data we observe and figure out which。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型使用最大似然法。我昨天提到过也有矩法方法，这个方法也非常好，你可以在第二门课程中学习到。Sontag的课程，我想。好的，但是我们这里用最大似然法。所以我们将写下我们观察到的数据的似然函数，并找出。
- en: parameters maximize that likelihood。 So here's the expression for the likelihood。
    It's true。 of you now because we have the likelihood for p of x we did on the
    previous slide。 We have。 n data points。 X1 through Xn drawn iid。 So to get the
    likelihood of the whole data set， trivial。 we just multiply the likelihood to
    the individual points like that。 Okay。 All， right。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数最大化似然函数。所以这里是似然函数的表达式。现在这对你们来说是对的，因为我们已经有了 p(x) 的似然函数，上一页我们做过了。我们有 n 个数据点，X1
    到 Xn，独立同分布（iid）。所以要获得整个数据集的似然，十分简单，我们只需要将每个数据点的似然相乘就行了。好的，一切都好。
- en: in the usual way we want to look at the log likelihood。 It's， we usually think。
    it's going to be easier to work with。 So we take the log of pd。 I'm calling this
    is going。 to be our objective function， our log likelihood objective function。
    So now I'm writing it J。 and the parameters of the objective function are the
    parameters of the model that we want。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 按照通常的方法，我们希望查看对数似然。我们通常认为这样做会更容易操作。所以我们对 p(d) 取对数。我称之为我们的目标函数，即对数似然目标函数。所以现在我将它写为
    J，目标函数的参数就是我们想要的模型参数。
- en: to discover that we want to maximize over in maximum likelihood。 So when we
    take the log， of p of d。 the log moves nicely inside。 Can you guys see my hand？
    Is this helpful？ All， right， good。 So we take the log of this product becomes
    the sum of the logs and we get sum， of the logs。 And then we wish we could have
    this log interacting with the Gaussian distribution。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了发现我们要最大化的就是最大似然。所以当我们对 p(d) 取对数时，对数会很好地进入公式。你们能看到我的手吗？这对你们有帮助吗？好的，太好了。所以我们对这个乘积取对数，变成了对数的和，我们得到了对数的和。然后我们希望能够让这个对数项与高斯分布相互作用。
- en: because the Gaussian distribution has this leading exponentiation and the log
    and the。 x cancel nicely。 But the sum blocks the log。 And this is what makes maximizing
    this a little。 bit difficult。 Okay。 So this is what we're going to maximize J
    using the em algorithm。 That's our motivation for em。 Okay。 Any questions on this？
    This is just review。 Very good。 Okay。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因为高斯分布有这个主导的指数项和对数项，`x` 之间能够很好地相互抵消。但求和会阻碍对数项的作用。这就是为什么最大化这个稍微有些困难的原因。好的，我们要用
    EM 算法来最大化 J。这就是我们使用 EM 的动机。好的，大家有问题吗？这只是复习。很好，好的。
- en: So a little more review。 Remember this thing we talked about yesterday， gamma，
    IC。 This is。 we're calling yesterday the responsibility of cluster C for the point
    xi。 I'm。 would prefer to just think of it as for xi。 What's the probability we
    think that it's， in cluster C？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 再复习一下。记住我们昨天讨论过的那个东西，gamma IC。昨天我们称之为簇 C 对点 xi 的责任。我更愿意把它理解为，对于 xi，我们认为它属于簇
    C 的概率是多少？
- en: So that's gamma IC。 The vector of the gammas across all clusters is like a。
    soft cluster assignment for data point。 And then the n sub C was， let's call this
    now。 the expected number of points in cluster C。 So I remember I said the probability
    and。 expectation were kind of interchangeable。 I just thought I'd worked that
    out for you。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 gamma IC。所有簇的 gamma 向量就像是数据点的软簇分配。然后 n 子 C 就是我们现在所说的，簇 C 中的期望点数。所以我记得我说过，概率和期望在某种程度上是可以互换的。我只是想给你们搞清楚这个概念。
- en: since got some uncertain stares last time。 So let's define n sub C as the expected
    number。 of points in cluster C for the data that we see。 So expectation of the
    sum in the。 we can put zi equal C。 So if the， if the cluster for xi， which is
    zi is equal to C， then this。 expression is one， otherwise it's zero。 We're adding
    up across all data points how many。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我看到了一些不确定的目光。让我们定义 n 子 C 为我们看到的簇 C 中的期望点数。所以在这里，我们可以把 zi 设为 C。如果 xi 的簇（即 zi）等于
    C，那么这个表达式就是 1，否则就是 0。我们将所有数据点相加，得到有多少个符合条件的点。
- en: zi's are equal to C。 So if we， this is a random variable， if we knew what it
    was， that would。 be the number of points that are actually from cluster C。 But
    we don't observe this， but we。 can take the expectation of that with this condition
    on the data that we do observe。 And。 then expectation of a sum is， solid the expectation，
    right？ And then expectation of an indicator。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 zi 等于 C。所以如果我们知道这是什么，它将是实际属于簇 C 的点数。但我们无法观察到这个值，但我们可以在我们观察到的数据的条件下对它进行期望估计。而且，和的期望就是期望，对吧？然后期望一个指示函数。
- en: function is the probability of the event in the indicator exactly。 Good。 So
    sum by is。 the probability that zi is equal to C given xi。 And this of course
    is just gamma i C。 So this is how we get n sub C。 All right。 Good。 Good that everyone's
    nodding because you。 know then otherwise I would just do it again next time。 All
    right。 So EM algorithm for GMM。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 函数是事件在指示器中发生的概率。很好。所以，按概率求和。是zi等于C，给定xi的概率。当然，这就是gamma i C。所以这就是我们得到n sub C的方式。好。大家都点头表示理解，因为什么，如果你们不理解，下次我就重新做一遍。好。那我们就来看看GMM的EM算法。
- en: I showed you this yesterday。 It's an alternating algorithm where first we figure
    out the soft。 cluster assignments for each data point。 And then we have these
    parameter estimate step。 So very similar to maximum likelihood estimate for the
    regular Gaussian。 But now， for example。 for the mean， instead of just taking the，
    the mean of the points in cluster C， we don't。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我昨天给你们展示过这个。这是一个交替算法，首先我们为每个数据点计算软聚类分配。然后我们进行参数估计步骤。这和常规高斯分布的最大似然估计非常相似。但现在，比如对于均值，我们不是直接取簇C中点的均值，而不是。
- en: know which points are on cluster C。 So we take a weighted average where the
    weights are。 the probability that this point is in cluster C。 All right。 So this
    should seem plausible。 but you shouldn't necessarily see why this is actually
    the maximum likelihood or helping。 us get the maximum likelihood， but it should
    at least be impossible。 Okay。 All right。 So。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 知道哪些点属于簇C。所以我们取一个加权平均值，其中权重是该点属于簇C的概率。好吧。这看起来是合理的，但你不一定会明白为什么这实际上是最大似然，或者为什么这能帮助我们得到最大似然，但至少它应该是不可或缺的。好的。
- en: here's an illustration。 So we'll start with randomly chosen means and variances。
    This。 doesn't look random。 These variances are close to spherical。 Here are data
    points。 So we've。 initialized our parameters。 And then the next step would be
    to find the soft assignment for。 every point to the two clusters。 Right。 So for
    every point here， we find the probability。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示意图。所以我们从随机选择的均值和方差开始。这个看起来并不随机，这些方差接近球形。这里是数据点。所以我们已经初始化了我们的参数。然后下一步是找到每个点到两个簇的软分配。对，每个点这里，我们找到其概率。
- en: that it belongs to this cluster and the probability belongs to this cluster。
    And here we color。 code based on those probabilities blue is dark blue is one
    dark red is this cluster two。 and things between is some kind of mixture and probabilities。
    Yeah。 So the assignment。 here is based on a probability。 Sure。 That's， that's
    easy。 So basically， we have， we have。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它属于这个簇的概率属于这个簇。这里我们根据这些概率进行颜色编码，蓝色是深蓝色是1，深红色是簇2，而介于其中的是某种混合概率。是的。所以这里的分配是基于概率的。没错，没错，基本上，我们有。
- en: these two， so we had these two， let's go， first let's go back to the， so this
    is where we calculate。 that probability。 So the probability， say we're looking
    at the ith point xi， the probability。 that the ith point is in cluster J is we
    look at the weight of each cluster and we calculate。 its Gaussian likelihood and
    well this is the expression。 So it's， it's， it's， you can。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个，所以我们有这两个，首先让我们回到这个地方，这是我们计算概率的地方。假设我们正在查看第i个点xi，那么第i个点属于簇J的概率是我们查看每个簇的权重，并计算其高斯似然度，嗯，这是表达式。所以，它是，你可以。
- en: explicitly calculate the probabilities。 Yeah。 Yeah， you initialize all the parameters，
    pi's。 mu's and sigma's。 Yeah。 Did you have something specific in mind for another
    way to do the。 soft assignments？ Okay。 Yeah。 So you can use other distributions
    besides Gaussian， you can。 have a mixture of some other distribution。 Okay。 All
    right。 So now we have our soft assignments。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 显式计算概率。是的，是的，你初始化所有的参数，pi、mu和sigma。是的，你有其他方式来做软分配吗？好的。是的，你可以使用高斯之外的其他分布，你可以有其他分布的混合。好的。现在我们有了软分配。
- en: and now we have our parameter fitting which is using that weighted average of
    the mean。 and then a weighted estimate of the covariance and this gives us these
    adjusted means and。 covariance matrices， adjusted class specific densities for
    the two classes， two clusters。 All right。 And so on。 And this is the， yeah， algorithm
    converging to， so what do these， circles represent？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的参数拟合，它使用的是均值的加权平均值。然后是协方差的加权估计，这给我们带来了这些调整后的均值和协方差矩阵，以及两个类别、两个簇的调整后的类别特定密度。好的，等等。这就是算法收敛的过程，那么这些圆圈代表什么呢？
- en: What have I drawn here？ Yeah。 These are like the covariance ellipses。 of each
    of the two Gaussian distributions。 So you could think of it roughly as like maybe。
    a one standard deviation contour of the Gaussian density， something like this。
    Okay。 All right。 Yeah。 So there are some dots that have the colors that are not。
    So the color of that。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我画的是什么？对。这些就像是每个高斯分布的协方差椭圆。你可以大致理解为高斯密度的一个标准差轮廓，大致是这样的。好的。对。是的。所以有一些点的颜色不太一样。那么那种颜色是什么呢？
- en: every dot basically represents the probability that is average。 Well， the probability
    of。 being in blue versus red。 So very blue is high probability of being in blue。
    Very red is。 high probability of being red and purple in the middle is kind of
    split。 Yeah。 The question， is。 well， what is being in red？ What I mean is that
    it was generate in the generative， process。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点基本上代表的是它的平均概率。嗯，表示的是蓝色和红色的概率。所以非常蓝的点表示它属于蓝色的概率很高。非常红的点表示它属于红色的概率很高，而中间的紫色则是两者的概率较为平分。是的。问题是，嗯，什么是属于红色的？我的意思是它是在生成过程中生成的。
- en: It was generated by the distribution。 That's the red color。 So yeah， it's certainly，
    possible。 but rare that from this distribution， we would generate a point that's
    all the way， over here。 So we would not assign that to the red cluster because
    that would be less， likely。 but it's possible to have come。 So there could be
    errors。 Yeah。 Yeah。 Yes。 Yeah。 Yeah。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 它是由分布生成的。那就是红色。是的，确实可能，但很少见，从这个分布中，我们生成的一个点完全在这里。所以我们不会将其分配到红色簇，因为那样的可能性比较小，但有可能出现。所以可能会有错误。是的。是的。是的。是的。
- en: So you have a probability distribution over assignment to the clusters。 So there's，
    no relative。 It's the probability is always sum to one。 The color you could。 Yeah。
    So。 if you can imagine that these colors are generated by， you know， you take，
    if the probability。 of being in blue is 30%， then we took 30% blue and 70% red
    and made the color。 Yeah。 Yeah。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你对簇的分配有一个概率分布。所以没有相对的。概率始终总和为1。颜色你可以。是的。所以，如果你能想象这些颜色是由你知道的生成的，你取，如果蓝色的概率是30%，那么我们取30%的蓝色和70%的红色来生成这个颜色。是的。是的。
- en: What is the expectation？ I mean， if you say you have an expectation step， I
    don't， know。 I didn't say anything my expectation step yet。 I mean， I did， but
    I didn't intend。 for it to be clear here。 No， I don't necessarily say that。 You
    know， if a point lands here。 it really is， it may not have a clear assignment
    to a cluster。 So， no。 So for a point to have。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 期望是什么？我的意思是，如果你说你有一个期望步骤，我不清楚。我还没有说明我的期望步骤。我是说，我确实提到过，但我没有打算在这里解释清楚。不是，我并不一定是这么说的。你知道，如果一个点落在这里，它可能没有明确的分配到某个簇。所以，不。为了让一个点有。
- en: just like all， almost all its probability on just one cluster， you'd have to
    see that。 the likelihood of that point under the other clusters would have to
    be very， very small。 So that's what happened。 We would need these covariance matrices
    to be very， very， very， small。 So if the covariance is a very， very small， then
    yes， you will see more heart assignment。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就像几乎所有的概率都集中在一个簇上，你必须看到。在其他簇下的点的可能性会非常非常小。这就是发生的情况。我们需要这些协方差矩阵非常非常小。所以如果协方差非常非常小，是的，你会看到更多的硬性分配。
- en: of every point， like either the one or the other。 And in fact， that's the lecture。
    Next， slide。 So the example that you give us right now is that actually， so the
    self-market actually。 the result of this self-market algorithm is the same as
    the car parking。 No， I don't。 know about that。 So first of all， this is an interesting
    point that the gadget mixture。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个点，就像是其中的一个或另一个。实际上，这就是讲座的内容。接下来，幻灯片。你现在给出的例子其实是，自我市场。实际上，这个自我市场算法的结果与停车场是一样的。不了解。首先，这是一个有趣的点，关于小工具混合。
- en: model approach doesn't give you in the first presentation a hard classification
    at all。 It gives you soft classifications。 So now you're maybe suggesting you
    threshold it to make。 a hard classification， a hard clustering。 I understand your
    question。 Is it just by coincidence。 that the graph that we see right now in next
    to this slide that we have fewer colors in， both sides？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模型方法在第一次呈现时并没有给出硬性的分类。它给的是软分类。所以现在你可能建议通过阈值化来做硬分类，硬聚类。我明白你的问题。是否只是巧合？我们现在在这张幻灯片旁看到的图像两边颜色较少？
- en: Is that coincidence？ How did it？ Oh， I understand your question。 Is it coincidence？
    I'm not sure。 They are perfectly， the clusters are perfectly separate。 So the
    question is， I guess you're asking。 do the probabilities always converge to like
    zero or one？ Yeah， okay。 All right。 But so。 so EM for gadget mixture model seems
    a little like it means。 Seems like what you are suggesting。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是巧合吗？是怎么回事？哦，我明白你的问题了。是巧合吗？我不确定。簇是完全分开的。所以问题是，我猜你是在问，概率是否总是收敛到零或一？是的，好的。好吧。所以，EM算法在加盖混合模型中的应用似乎有点像K均值。看起来像你所建议的那样。
- en: And there is a precise correspondence。 So first of all， in gadget mixture models。
    we're allowed to have clusters that are shaped in ways that， are not circular。
    So we can have clusters that look like this， for example。 Okay。 K。 means would
    never give clustering like this。 Okay。 So if we make a in the GM in the gadget。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 而且有一个精确的对应关系。所以，首先，在加盖混合模型中，我们允许簇的形状是非圆形的。所以我们可以有像这样的簇。例如。好吧，K均值从来不会给出这种聚类。所以如果我们在加盖混合模型中做出。
- en: mixture model setting， however， if we have if we fix our covariance matrices
    to be spherical。 so not allowing stretched out conveyances。 And we have the variances
    going towards zero。 So really tight gagens。 Then the probabilities of your cluster
    assignments do converge to。 one on one on the closest cluster in， yeah， closest
    cluster in Euclidean sense and zero。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合模型设置中，然而，如果我们将协方差矩阵固定为球形的，不允许伸长的协方差，并且让方差趋向于零。这样会形成非常紧凑的加盖簇。那么，簇的分配概率会收敛到一个点，在欧几里得意义上，最接近的簇会是1，其他簇则为0。
- en: on the others。 And then we do get a key means algorithm pretty much exactly。
    Yeah。 Okay。 And that has to do with how the tails of the gagens decay。 So if you're
    very far from the。 gagens center or equivalently， if the variance is very small，
    then the likelihood for the。 cluster that you're closer to will be in some sense，
    exponentially larger than the one that。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在另外的情况下。然后我们几乎完全得到一个K均值算法。是的。好的。这与加盖分布的尾部衰减有关。所以，如果你远离加盖中心，或者等价地说，如果方差非常小，那么你更靠近的簇的似然会在某种意义上指数级地大于其他簇。
- en: you're further away from the ratio of them will be very large when you're when
    when those。 variances are very small。 It's this type of thing that's very easy
    to try out in Python。 or something。 And you can just see that your likelihood
    under two gagens， two different， gagens。 different means say you're between but
    a little bit closer to one of them。 And。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你越远，似然比率会变得非常大，当这些方差非常小的时候。这种情况在Python中很容易尝试。你可以看到，在两个加盖簇之间，你的似然值会如何变化，如果你稍微靠近其中一个簇。
- en: you look at the ratio of those likelihoods。 And then you send those variances
    to zero。 you'll see that the ratio goes up to gets very large。 Someone check that
    and post a， piazza。 Yes。 Yes。 There's no magic in optimization。 So it will be
    different though。 I mean， it means。 the course now that we have a nice correspondence，
    we can talk about the connection。 So if we。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你查看那些似然的比率。然后你将那些方差送到零。你会看到比率会变得非常大。有人检查一下并在Piazza上发个帖子吗？是的，是的。优化中没有魔法。所以会有所不同。不过，我的意思是，现在我们有了一个很好的对应关系，我们可以谈论它们之间的联系。所以，如果我们。
- en: had a gagener mixture model approach and we restricted our models with this
    type of covariance。 we could still get stuck in local minimum in our attempt to
    find the maximum likelihood。 of the gagener mixture model。 So， okay。 So I don't
    have off the top of my head an example。 of gagener mixture modeling where we get
    stuck in a local minimum。 But it's interesting， question。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个加盖混合模型方法，并且用这种类型的协方差限制了我们的模型。我们仍然可能在尝试找到最大似然时陷入局部最小值。所以，好吧，我现在脑海中没有一个加盖混合建模的例子，能够展示我们是如何陷入局部最小值的。但这个问题很有趣。
- en: So it is true that the likelihood function of for gagener mixture models is，
    not convex。 And so in general， there will be local maxima。 But I haven't thought
    about， it before like this。 So。 let's think about it for 20 seconds。 Anyone have
    an example？
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，确实，加盖混合模型的似然函数不是凸的。因此，一般来说会有局部最大值。但是我以前没有这样想过。所以，让我们考虑一下，20秒钟。有人有例子吗？
- en: Can I think of an example where maximum likelihood gets stuck？ Okay， that's
    enough。 Okay。 Any more。 questions before we get into general EMR-able， and if
    we don't finish， we can continue next， week。 I think it's pretty interesting。
    It's a little bit mathy， but it's not too bad。 I。 hope you guys will follow along。
    Ready？ All right。 First， we have a few。 We have two or。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以想一个最大似然法可能会遇到问题的例子吗？好的，这样就够了。好的，在我们进入一般的 EMR-able 之前，还有更多问题吗？如果我们没完成，下周可以继续。我觉得这很有趣。虽然有点数学化，但不算太难。我希望你们能跟得上。准备好了吗？好的。首先，我们有几个。我们有两个或者。
- en: three little lemmas to review from math。 First， you remember convex functions。
    Good。 Jensen's。 inequality。 Very， very fun。 There aren't very many fundamental
    inequalities that come up。 so often in math。 There's triangle inequality， coach
    of shorts inequality， and he answers。 inequality are like the three top。 And I
    rarely see much beyond that。 There are， of course。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个小引理需要复习，首先，记得凸函数吗？好，詹森不等式。非常，非常有趣。数学中没有多少基本的不等式会如此频繁地出现。像三角不等式、科赫-舒尔茨不等式，还有汉斯不等式，它们是前三大。我很少见到比这更多的不等式。当然，还有。
- en: but Jensen's is one of the top three。 David's top three。 Okay。 So， he answers
    inequality。 If you have a convex function， and x is a random variable， the expected
    value of f of。 x is greater than or equal to f of the expected value of x。 Okay。
    All right。 Wikipedia has。 a nice geometric proof of this。 The way I remember it
    is with this example， I know that。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是詹森不等式是前三名之一。大卫的前三名。好的。所以，汉斯不等式。如果你有一个凸函数，x 是一个随机变量，那么 f(x) 的期望值大于或等于 x 的期望值的
    f 值。好的。好。维基百科上有这个的几何证明。我记住它的方式是通过这个例子，我知道。
- en: e x squared， the expected value of x squared is greater than or equal to the
    square of。 the expected value of x。 How do I know that？ How do I always remember
    that？ Yeah， very， good。 So。 if I subtract e x squared， the expectation of x from
    e x squared， I bring this over here。 that's variance of x。 And I know variances
    are always greater than or equal to zero。 This。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: e x 的平方，x 的平方的期望值大于或等于 x 的期望值的平方。我怎么知道这个？我怎么总是记得这个？是的，非常好。所以，如果我从 e x 的平方中减去
    x 的期望值的平方，我把这个移过来。那是 x 的方差。我知道方差总是大于或等于零的。这。
- en: is how I remember the direction of Jensen's inequality。 Okay。 I see the answer
    is， was， there。 Okay。 So， that's Jensen's inequality。 Now， KL divergence。 Have
    you guys seen KL， divergence before？
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我记住詹森不等式方向的方法。好的。我看到答案在那里。好的。所以，这就是詹森不等式。现在，KL 散度。你们之前见过 KL 散度吗？
- en: Kolbak Live Lord divergence。 It's interesting。 It's a way in some sense。 to
    measure how different two probability distributions are。 Okay。 So， it's not quite。
    a distance metric， but it gives you a measure of how different things are。 So，
    here's its。 definition。 In the discrete case for discrete probability distributions，
    sum over the every。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kolbak Live Lord 散度。这很有趣。从某种意义上说，它是一种衡量两个概率分布差异的方法。好的。所以，它不完全是一个距离度量，但它给出了一个衡量事物差异的标准。这里是它的定义。在离散情况下，对于离散的概率分布，求每个。
- en: element in the support of the distribution， all possible values。 And Px， log
    Px over Qx。 It's a strange thing。 There's ways to understand it， but it's the
    best way to kind of get it。 is to look at information theory。 And I don't present
    it this year， but if you go to last。 year's slides， there's about 10 or 12 slides
    that's a very brief introduction to this field。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的支持中的元素，所有可能的值。然后是 Px，log Px 除以 Qx。这是一个奇怪的东西。虽然有方法可以理解它，但理解它的最好方式就是从信息论入手。我今年没有讲这个，但如果你查去年的幻灯片，会看到大约
    10 到 12 张幻灯片，里面有对这个领域的简短介绍。
- en: called information theory， where things like Kolbak Live Lord divergence and
    entropy and。 mutual information all have very natural interpretations in terms
    of bits and coding。 up numbers and stuff like that。 Okay。 All right。 And the thing
    to， one thing to notice。 about KL divergence is that this is actually an expectation。
    See， we're summing。 It's the。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这叫做信息论，其中像 Kolbak Live Lord 散度、熵和互信息之类的概念，在比特和编码方面都有非常自然的解释。比如数字和其他类似的东西。好的。好。然后，需要注意的一点是，KL
    散度实际上是一个期望值。看看，我们是在求和。它是。
- en: expectation of log of Px over Qx with respect to the distribution P where x
    is distributed， as P。 See how you can write， see how this is just an instance
    of the expectation。 Okay。 All right。 So we have this Kolbak Live Lord divergence
    and there's just two really important。 properties of it that we're going to use。
    And this comes from what's called Gibbs inequality。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对数的 Px 与 Qx 之比的期望值，关于分布 P，其中 x 按 P 分布。看看你是如何写的，看看这只是期望的一个实例。好的。好的。所以我们有这个 Kolbak
    Live Lord 散度，并且有两个非常重要的属性我们将使用。这来自于所谓的 Gibbs 不等式。
- en: And it simply says that the Kolbak Live divergence between any two distributions
    is first of。 all non-negative。 And second of all， it's equal to zero only if the
    distributions are， the same。 So that's at least a very nice property of something
    that we claim to measure the， difference。 the dis， somehow differences between
    distributions。 The Kolbak Live earns between。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 它简单地说，任意两个分布之间的 Kolbak Live 散度首先是非负的。其次，只有当分布相同的时候，它才等于零。所以这至少是我们声称能够衡量分布之间差异的东西的一个非常好的属性。Kolbak
    Live 散度在。
- en: two distributions is zero if and only if it's the same distribution。 And that's
    what we're。 going to need in a few slides。 Okay。 So it's not， don't， don't compute，
    don't think that。 it's actually a distance metric。 It's not。 It's in fact not
    even symmetric。 So the Kolbak。 divergence between P and Q is not in general the
    same as the distance， the difference from， Q to P。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果且仅当两个分布相同时，两个分布的 KL 散度为零。这就是我们在接下来的几张幻灯片中需要的。好的。所以不要，不要计算，不要认为。它实际上是一个距离度量。它不是。事实上，它甚至不是对称的。所以
    Kolbak 散度在 P 和 Q 之间通常不等于 Q 到 P 的差异。
- en: All right。 So the proof I think will not go over it now， but it's worth walking，
    through。 There's only one inequality which comes from Jens' inequality。 Okay。
    Okay。 Yeah。 Yeah。 Can you say it again？ You could take any distribution at all
    two distributions and。 if they differ in mean and variance or anything， then yeah，
    the KL divergence will be greater。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以证明我想我们现在不做了，但值得走一遍。只有一个不等式来自于 Jens 不等式。好的。好的。是的。是的。你能再说一遍吗？你可以拿任意两个分布，如果它们在均值和方差或其他方面有差异，那么是的，KL
    散度会更大。
- en: than zero。 Okay。 All right。 So the E。M。 algorithm， we're going to look at it
    in terms of the。 general latent variable model。 So a general model in which you
    only observe X but not Z。 So we're going to have Z and X now， they can be more
    than just random variables。 They。 can be vectors of random variables， but we'll
    still do not them as X and Z。 Okay。 So we're。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大于零。好的。好的。那么 E.M. 算法，我们将从一般的潜变量模型来看。一个一般模型，在该模型中，你只观察 X 而不观察 Z。所以我们现在有 Z 和 X，它们不仅仅是随机变量。它们可以是随机变量的向量，但我们仍然把它们当作
    X 和 Z。好的。我们。
- en: going to parameterize the model by some unknown parameter of theta。 So in their
    Gaussian。 motion model， theta would have been pi's and mu's and sigma's， but we'll
    just capture。 it all as theta to simplify notation a bit and to generalize it。
    This is just to say that， I'm not。 this is not a Bayesian setting because we're
    conditioning on theta。 This is， this。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一些未知的参数 theta 来参数化模型。所以在他们的高斯运动模型中，theta 可能是 pi、mu 和 sigma，但我们将它简化为 theta，以便简化符号并使其更加通用。这里只是想说明，我并不是在进行贝叶斯设置，因为我们是基于
    theta 条件化的。这就是。
- en: is really just theta is just a parameter and we write it this way because it's
    easier to。 write this way than with the putting theta as a subscript on P or with
    a semicolon which。 would be more traditional ways of writing it。 All right。 So
    X is called our incomplete。 data set because it's what we observe but we imagine
    that there's also this Z out there。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 只是 theta 只是一个参数，我们这样写是因为它比将 theta 作为 P 的下标或用分号书写更容易。这是写它的传统方式。好的。所以 X 被称为我们的不完整数据集，因为它是我们观察到的，但我们假设那里还有
    Z。
- en: which is unobserved。 So X and Z together is called the complete data set。 X
    is called。 the incomplete data set。 So of course after the maximizing the likelihood
    of the incomplete。 data set X because that's what we observe。 All right。 So a
    deme of the data set and， all， right。 We want to find this marginal log likelihood。
    So this is the likelihood of X， right because。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是未观察到的。所以 X 和 Z 一起被称为完整数据集。X 被称为不完整数据集。所以，当然，在最大化不完整数据集 X 的似然性之后，因为这是我们观察到的。好的。那么数据集的一个样本和，好的。我们想找到这个边际对数似然性。所以这是
    X 的似然性，好的，因为。
- en: we're summing out of our Z that gives us the P of X and this is for a single，
    sort of single。 element。 For the data set there should be an extra product in
    here。 All right。 So， so。 the key idea of the EM algorithm is that even though
    this is hard to maximize directly， if。 we have full data， if we only had to maximize
    the likelihood for the complete data X and， Z。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对Z求和，这样我们就得到了X的概率，这适用于单个元素。对于数据集，这里应该有一个额外的乘积。好吧。所以，EM算法的关键思想是，即使直接最大化这一项很难，如果我们有完整数据，或者说如果我们只需要最大化完整数据X和Z的似然。
- en: if that's easy then the EM algorithm works well。 Okay。 So if we can maximize
    the complete。 log likelihood， then we're in good shape for EM algorithm even if
    the marginal log likelihood。 is difficult。 Now for Gaussian mixture model， are
    we in that situation where if you get， X and Z。 is it easy to find the maximum
    likelihood？ Again？ What's the， what would be the issue？ Okay。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这很简单，那么EM算法就能很好地工作。好吧。所以如果我们能最大化完整的对数似然，那么即使边际对数似然很难，我们在EM算法中也能处于一个良好的状态。那么对于高斯混合模型，如果我们得到了X和Z，是否容易找到最大似然？再说一遍，问题是什么呢？好吧。
- en: So the sigma is to marginalize out the Z。 But if we observe Z， we don't have
    to marginalize， it out。 Yeah。 So I think we mentioned this yesterday。 If you observe，
    if you know the。 cluster assignment of every point， it's really easy to get the
    maximum likelihood parameter。 settings， right？ We take all the points that are
    in cluster one， we find their mean， we。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以sigma是对Z进行边际化。但是如果我们观察到了Z，我们就不需要将其边际化。是的。所以我想我们昨天提到过。如果你知道每个点的簇分配，得到最大似然参数设置是非常容易的，对吧？我们取簇1中的所有点，找到它们的均值，我们。
- en: find their covariance。 That is the maximum likelihood estimate for that Gaussian
    distribution。 And so on for the other clusters。 So if we have the full data for
    Gaussian mixture model。 it's very easy to find the maximum likelihood estimates。
    Yeah。 Okay。 So this is the case。 for Gaussian mixture model。 All right。 But of
    course we don't observe Z。 So the EM algorithm。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 找到它们的协方差。这是高斯分布的最大似然估计。其他簇也是如此。所以如果我们有高斯混合模型的完整数据，找到最大似然估计是非常容易的。是的。好吧。这就是高斯混合模型的情况。好吧。但当然我们并不能观察到Z。所以EM算法。
- en: is based on this really interesting idea， which is what if we had a probability
    distribution， on Z。 we came up with a probability distribution on Z。 And then
    we tried to maximize the expected。 complete log likelihood。 All right。 So we post，
    we come up with somehow， we'll come to how。 later on， but somehow we come up with
    a distribution on this unobserved variable Z， clock Q of， Z。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于一个非常有趣的想法，那就是如果我们为Z构建一个概率分布。我们给Z构建了一个概率分布，然后我们尝试最大化期望的完整对数似然。好吧。所以我们提出，我们将稍后探讨如何做到，但我们以某种方式得出了这个未观察到的变量Z的分布，Q(Z)。
- en: All right。 And then we're looking for the expectation of the complete data log
    likelihood。 We can compute the expectation of the complete log likelihood with
    respect to this distribution， Q。 Finding the， okay， is that clear？ So that's what
    this， this expression is finding the。 expectation of the complete log likelihood。
    Here's a complete log likelihood。 We don't。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。然后我们在寻找完整数据对数似然的期望。我们可以根据这个分布Q计算完整对数似然的期望。找到，好的，明白了吗？所以这就是这个表达式的意义，它是在找到完整对数似然的期望。这里是完整的对数似然。我们并不。
- en: know Z's or something overall possible Z's and we're weighing it by the probability
    that。 Q gives to Z。 This is the expectation of complete log likelihood with respect
    to Q。 Questions on。 that？ Okay。 Sure。 How do we come up with a， how do you come
    up with Q？ Yes。 Any guesses？
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 知道Z的值或所有可能的Z，然后我们按Q赋予Z的概率进行加权。这是关于Q的期望完整对数似然。关于这个，有问题吗？好吧，当然。我们怎么得出Q的？是的，有人猜测吗？
- en: It should be distribution on Z that seems like a plausible distribution for
    Z given what。 we've observed about X。 There's a pretty natural one actually。 You
    could think of it。 All right。 You have another question？ This is over on the X's
    for one Z。 I only wrote one X and Z。 This。 is the only one。 Why did I only wrote
    one？ It's because we're collapsing all X one through。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是一个关于Z的分布，给定我们对X的观察，这个分布看起来是一个合理的Z分布。实际上有一个相当自然的选择。你可以考虑一下。好吧。你还有其他问题吗？这是在X上，对于一个Z。我只写了一个X和Z。为什么我只写了一个？因为我们在合并所有的X。
- en: X and into X and Z one through Z and into Z。 That's what's happening here。 That's
    why， there's only。 it looks like one because it is one but it's an entire vector。
    Right。 That's， the simplify。 Okay。 Any more？ Okay。 All right。 So EM also assumes
    that this maximization is， feasible。 All right。 So EM algorithm， we don't know
    exactly how to maximize the marginal likelihood。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: X进入X，Z从Z1到Zn进入Z。这就是这里发生的事情。这就是为什么它看起来只有一个，因为它确实是一个，但它是一个完整的向量。对吧。这就是简化。好的，还有别的吗？好的。那么，EM也假设这个最大化是可行的。好的。所以EM算法，我们并不完全知道如何最大化边际似然。
- en: So what we're going to do instead is maximize， we're going to find a lower bound
    than the。 likelihood and maximize that instead。 So let's， let's examine this marginal
    likelihood。 This。 is the log likelihood。 Expand it out。 This is marginalizing
    out the joint。 Find that's， inequality。 Let's multiply and divide by this Q of
    Z。 So just write it down。 Q of Z。 We haven't。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们接下来要做的是最大化，我们将找到比似然更低的下界，并最大化它。让我们来看看这个边际似然。这是对数似然。展开它。这是对联合的边际化。找到那个不等式。让我们通过Q(Z)来乘以并除以。所以就写下来。Q(Z)。我们还没有。
- en: even said what it is yet。 Except for it's a distribution on Z。 So no change。
    Right。 These， cancel。 But now notice that this is an expectation with respect
    to Q of Z because we're summing。 over all the S of Z with this probability of
    Z and some random expression。 So we have a。 log of an expectation。 So now we can
    use N's as inequality and swap it。 Right。 So which。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至还没说它是什么。除了它是Z的分布。所以没有变化。对吧。它们取消掉了。但现在请注意，这是一个相对于Q(Z)的期望，因为我们对所有的S(Z)进行求和，带有这个Z的概率和某些随机表达式。所以我们有一个期望的对数。现在我们可以使用N的不等式并交换它。对吧。哪个。
- en: way will the inequality go？ It's hard。 So the function is not convex。 So you
    have to switch。 it once and then so。 Okay。 Yes。 Greater than or equal to。 So we
    switch the expectation on。 the log and we get this expression and then we define
    this final expression to be LQ， theta。 We're going to be doing a lot of this LQ
    theta in the remaining slides。 This is。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不等式将如何变化？这很难。所以这个函数不是凸的。所以你必须切换一次，然后继续。好的。是的。大于或等于。所以我们将期望值交换到对数上，然后得到这个表达式，接着我们将这个最终的表达式定义为LQ，θ。接下来的几张幻灯片我们将会经常使用这个LQθ。这是。
- en: something called our variational lower bound on the marginal likelihood。 Okay。
    So if in。 the future you ever study variational methods and you're like， I'm not
    sure what this is。 Come back to this slide because the calculations in this slide
    is essentially the core of variational。 methods。 E algorithm is kind of the simplest
    introduction to variational methods。 All right。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一种叫做边际似然变分下界的方法。好的。所以，如果将来你学习变分方法时，心里想：我不确定这是什么。回到这一张幻灯片，因为这张幻灯片中的计算本质上是变分方法的核心。E算法是变分方法的最简单介绍。好的。
- en: So where we're headed here is the E algorithm。 Our goal is we're going to maximize
    LQ theta。 We're going to search over all probability distributions Q and all possible
    values of。 theta and we're just going to be some method to it but essentially
    we're just going over。 this whole space of distributions Q and parameters that
    is theta trying to get LQ of theta as。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要去的方向是E算法。我们的目标是，我们要最大化LQθ。我们将搜索所有概率分布Q和所有可能的θ值，我们只是会用某种方法，但本质上我们只是在遍历这个分布Q和参数θ的整个空间，试图使LQθ最大化。
- en: big as we can。 All right。 That's the objective of the M algorithm。 At the end
    when we've。 gotten LQ theta as large as we can， we're going to take whatever theta
    it is and we're going。 to say this is our estimate of theta。 That's going to be
    our best guess of the maximum likelihood。 estimate of theta。 All right。 So all
    of our effort is on finding a way to maximize L and。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能大。好的。这是M算法的目标。当我们把LQθ最大化到最大时，我们将取那个θ，然后说这是我们对θ的估计。这将是我们对θ的最大似然估计的最佳猜测。好的。所以我们所有的努力都在于找到一种方法来最大化L。
- en: what we're going to show is that maximizing L does indeed help maximize this
    marginal likelihood。 All right。 And in fact if we find the global maximum of L
    that will also be the global。 maximum of the marginal likelihood。 So that's good。
    That we can prove。 Yeah。 So are you maximizing LQ？ I mean easier than you don't。
    Oh， well， and you'll see。 Okay。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要展示的是，最大化L确实有助于最大化这个边际似然。好的。事实上，如果我们找到L的全局最大值，那也将是边际似然的全局最大值。所以这很好。我们可以证明这一点。是的。那么，你在最大化LQ吗？我意思是比你不做更容易。哦，好吧，你会看到的。
- en: But I mean at the core is the fact that in here we have。 a joint we have the
    complete likelihood which is we know it to be easier than the marginal。 So we're
    not， notice we're not summing out over Z in the middle。 Okay。 Like here。 Okay。
    So we're going to try to maximize LQ theta。 What we can do is take this expression
    and。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但我的意思是，核心问题在于我们这里有一个联合概率，我们有完整的似然函数，而我们知道它比边际似然更容易计算。所以我们没有在中间对Z求和。好了。像这里。好了。所以我们要尝试最大化LQ
    theta。我们可以做的是带入这个表达式。
- en: we split it into two pieces by breaking the log of this ratio into the log of
    the numerator。 minus the log of the denominator。 And what you'll notice， which
    you should notice， is that。 the second term has no theta。 All right。 The first
    term has Q and theta。 Okay。 So that's， key。 So the other thing to notice is that
    this expression is the expected complete data log， like that。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将这个比率的对数分解为分子对数减去分母对数来将其分成两部分。你会注意到，你应该注意到的是。第二项没有theta。好了。第一项有Q和theta。好的。这是关键。所以另外需要注意的是，这个表达式是期望的完整数据对数，像这样。
- en: That's the thing that we assume is easy to maximize in the m algorithm。 All，
    right。 So if we maximize this expression over theta， we only have to look at this
    piece。 If we maximize it over Q， okay， both pieces are present。 All right。 So
    now we're going， to show。 we're going to look a little closer at how L lower bounds
    are marginal likelihood。 So first of all。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是我们假设在m算法中容易最大化的东西。好了。所以如果我们对theta最大化这个表达式，我们只需要看这一部分。如果我们对Q最大化它，好吧，两部分都会出现。好了。所以现在我们将展示。我们将更仔细地看看L下界是如何对应边际似然的。所以首先。
- en: I want you to think of these things。 I want you to think of LQ of theta。 for
    fixed Q for fixed probability distribution。 Think of it as a function of theta。
    All right。 And so for every theta， this bound holds。 In fact， for any Q and every
    theta， this bound， holds。 So we can always write this down no matter what we plug
    in for Q and theta。 So here's， a picture。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你思考这些问题。我希望你思考LQ关于theta的情况，对于固定的Q和固定的概率分布。将其视为theta的函数。好的。所以对于每个theta，这个界限都成立。事实上，对于任何Q和每个theta，这个界限都成立。所以无论我们为Q和theta插入什么，我们总是可以写下这个公式。这里有张图。
- en: The red line is our， all right。 So x axis is theta。 We've， it's a cartoon version。
    where our parameter space is just real。 So x axis is our theta。 The red plot is，
    that's。 the log marginal likelihood of our data for each theta， right？ For every
    theta， we have。 a different likelihood。 So if we knew the red line， the max， the
    maximum likelihood theta。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 红线是我们的，好吧。横轴是theta。我们，它是一个卡通版，其中我们的参数空间仅仅是实数。所以横轴是我们的theta。红色的图是，这是。我们数据的对数边际似然，对于每个theta，对吧？对于每个theta，我们有不同的似然。所以如果我们知道红线的最大值，即最大似然的theta。
- en: is right here。 Can you say because it's under the highest point of the marginal
    likelihood？
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 就在这里。你能说出它是因为它在边际似然的最高点下吗？
- en: So we would like to find this point。 All right。 Yeah。 No。 Okay。 Now。 Yeah。 Well，
    red is the。 marginal likelihood。 Yes。 As I was going to say， this blue or purple
    line is L Q of theta。 for a fixed Q。 So Q is fixed and now it's a function of
    theta。 And this is the plot of。 our lower bound as a function of theta。 And yes，
    you're right。 The green is for a different。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们想找到这个点。好了。是的。不是的。好吧。现在。是的。红色是边际似然。是的。正如我刚才要说的，这条蓝色或紫色的线是L Q of theta，针对固定的Q。所以Q是固定的，现在它是theta的函数。这是我们下界的图像，作为theta的函数。是的，你说得对，绿色是针对不同的。
- en: Q as a function of theta。 So these are two different lower bounds for different
    probability。 distributions Q。 So the method that we're aiming for here is we're
    going to kind of jiggle。 around Q。 And for every Q， we're going to find the maximum
    theta for that Q。 And then。 we're going to kind of go back and forth。 All right。
    All right。 So I gave you this big。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Q作为theta的函数。所以这些是两个不同概率分布Q下的下界。所以我们在这里追求的方法是，我们将对Q进行调整。对于每个Q，我们将找到该Q对应的最大theta。然后，我们将来回调整。好了。好了。所以我给了你这个大。
- en: picture idea already。 We have this inequality for all theta and Q。 And we're
    going to， yes， I said。 we're going to search over theta for maximizing L and then
    take the corresponding。 theta as a estimate。 So this can very well be a local
    maximum。 This will typically， we。 hope to be a local maximum of our likelihood。
    You can get much better with this type of， algorithm。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有了图像的概念。我们对于所有theta和Q都有这个不等式。我们将，嗯，我说了。我们将对theta进行搜索以最大化L，然后取相应的theta作为估计值。所以这很可能是一个局部最大值。这通常，我们希望它是似然的局部最大值。使用这种类型的算法，你可以得到更好的结果。
- en: So if you want to try to find the global， you may need to restart with different。
    initial initialization points。 All right。 So the question left is， all right，
    how exactly。 are we going to go about changing Q and theta to try to find the
    maximum value of LQ theta？
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你想尝试找到全局最优值，你可能需要从不同的初始点重新开始。好了。那么剩下的问题是，好的，我们具体该如何去改变Q和theta，以尝试找到LQ theta的最大值？
- en: All right。 So we're going to use coordinate ascent， which we know about。 Coordinate
    ascent。 remember is where we first take one of our parameters and hold the other
    one fixed and。 maximize our function with respect to that parameter。 Then we fix
    that parameter and maximize。 respect to the other parameter。 And then are we done
    at that point？ No， we have to iterate。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们将使用坐标上升法，这是我们已知的。坐标上升法。记住，它是这样的：我们首先取一个参数，固定其他参数，并且最大化关于该参数的函数。然后我们固定这个参数，再对另一个参数进行最大化。到这个点我们就完成了吗？不，我们需要迭代。
- en: That's not enough to do one。 It's an iterative algorithm you keep repeating。
    Okay。 That's。 called coordinate ascent。 All right。 So here is EM algorithm at
    the high level。 We start。 with an initial random point for theta。 We maximize
    over distributions， which is an interesting。 thing。 But we maximize over all probability
    distributions， LQ theta。 And then we fix that。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不足以完成一个。它是一个迭代算法，你需要不断重复。好了。那就是所谓的坐标上升法。好的。这是EM算法的高级概述。我们从theta的一个初始随机点开始。我们对分布进行最大化，这是一个有趣的过程。但我们对所有概率分布LQ
    theta进行最大化。然后我们固定它。
- en: Q star and maximize over theta and repeat。 So what we'll show is that the theta
    is give。 us monotonically increasing likelihood。 So that's interesting。 That's
    not necessarily。 obvious from here。 So as we get a new theta in every round， certainly
    the lower bound keeps。 increasing。 LQ star theta will keep increasing because
    all we do is keep stepping to a new， point。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Q星开始，并对theta进行最大化，然后重复。所以我们将展示theta会给我们单调递增的似然值。这很有趣。并不一定从这里就能显而易见。所以每一轮我们得到一个新的theta，显然下界会不断增加。LQ星theta会不断增加，因为我们所做的就是不断步进到新的点。
- en: That's the maximum。 So we can only be increasing LQ theta。 What's interesting
    is。 that we also have increasing likelihood of theta。 So the probability of X
    given theta is also。 increasing。 Okay。 So here's an illustration of our algorithm。
    We start at theta old。 Well。 let's let's start with a Q。 And now this okay， start
    with theta old apologies， start with。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 那是最大值。所以我们只能增加LQ theta。有趣的是，我们还会增加theta的似然函数。所以，给定theta的X的概率也在增加。好了，这里是我们算法的一个示意图。我们从theta旧值开始。嗯，让我们从Q开始。现在，好的，从theta旧值开始抱歉，从。
- en: theta old。 Now we find a Q that gives the best lower bound at theta old。 Now
    would be， this。 So the best lower bound at a point is basically any lower bound
    that is equaled。 if we can find one that's equal to the actual function at that
    point， that would be the best。 We can have a lower bound at that's the best at
    this one point that's better than touching。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: theta旧值。现在我们找一个Q，能在theta旧值处给出最佳下界。现在应该是这个。所以在某一点的最佳下界基本上就是任何等于该点的下界。如果我们能找到一个与该点的实际函数值相等的下界，那将是最佳的。我们可以得到一个在这一点最好的下界，甚至比接触点更好。
- en: the graph。 You might find another lower bound that is tangent to the graph and
    is better。 in other places。 But that's not the case here。 All right。 So we find
    our lower bound at theta， old。 Then we maximize this lower bound。 So we go from
    here and maximize the lower bound。 brings us to the peak of the purple curve。
    And that gets us to new theta theta new。 Okay。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图。你可能会找到另一个与图相切并在其他地方更好的下界。但这里不是这种情况。好了。我们找到了theta旧值的下界。然后我们最大化这个下界。所以我们从这里出发，最大化下界。这将我们带到紫色曲线的峰值。然后得到新的theta，theta新值。好的。
- en: Now we find a new Q that gives us a lower bound that's the best possible at
    this point。 And that would be this green curve which is tangent to the red line。
    All right。 And repeat。 Okay。 All right。 We've got six minutes left。 So we have
    some time to make some progress， still。 So this is a very neat thing that happens
    if we examine this lower bound。 So here's a。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们找一个新的Q，给我们一个在这一点上最佳可能的下界。那就是这个绿色曲线，它与红线相切。好了。然后继续。好的。我们还有六分钟。所以我们还有一些时间可以继续进展。这是一个非常有趣的现象，如果我们检查这个下界的话。这里是一个。
- en: definition of the lower bound。 Now we're going to split it in the log a little
    bit differently。 than we did before。 Okay。 So first we're going to split this
    joint distribution into conditional。 and marginal。 Right。 So that's just standard
    base rule or definition of conditional probability。 Now we're going to split it，
    split that log like this。 So we keep these two terms in the。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下界的定义。现在我们将以稍微不同的方式拆分对数。与之前不同。好的。首先，我们将这个联合分布拆分为条件分布和边际分布。对吧。这就是标准的贝叶斯法则或条件概率的定义。现在我们将以这种方式拆分对数。所以我们将这两项保留在第一部分。
- en: first piece and move this to the second piece。 All right。 Now let's look at
    the second piece。 for our second。 Do you see how this simplifies？ Px given theta
    is independent of z。 Right。 So we can move that outside of the sum。 Now do you
    see how it simplifies？ Yeah。 The sum。 over z of Qz is one。 So this term becomes
    log of Px given theta。 What's log of Px given， theta？
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分并将其余部分移到第二部分。好的。现在让我们看看第二部分。对于我们的第二项。你看到它是如何简化的吗？给定theta的Px与z独立。对吧。所以我们可以将它移到求和之外。现在你看到它是如何简化的吗？是的。对z的求和中Qz的和为1。所以这一项变成了log
    Px给定theta。那么，log Px给定theta是什么？
- en: That's what we're trying to maximize。 That is exactly the marginal log likelihood。
    the marginal log likelihood of x。 All right。 So that's interesting。 This is what
    we have。 the lower bound on。 And now it's somehow related to our lower bound by
    inequality。 So let's。 rearrange a bit。 So the marginal likelihood is exactly equal
    to the lower bound plus this。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们要最大化的目标。它正是边际对数似然。x的边际对数似然。好吧。所以这很有趣。这就是我们得到的。下界。而现在它通过不等式与我们的下界有某种关系。让我们稍微调整一下。边际似然恰好等于下界加上这个。
- en: kau'd divergence piece。 So this is an explicit expression telling us the gap
    between a marginal。 likelihood and a lower bound on it。 It's exactly given by
    the kau'd divergence between Q and。 the marginal likelihood。 It's pretty interesting。
    We're looking at it for a minute。 All right。 Okay。 All right。 So we can use this
    to figure out how to find the best Q。 Remember in our。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler散度项。所以这是一个明确的表达式，告诉我们边际似然与其下界之间的差距。它正是由Q和边际似然之间的Kullback-Leibler散度给出的。这很有趣。我们看了一会儿。好吧。好的。那么我们可以利用这个来弄清楚如何找到最优的Q。记住，在我们的。
- en: maximized Q step？ We have maximized Q step and the maximized theta step。 This
    is going。 to help us with the maximized Q step。 So here's our lower bound again
    now with equality。 We。 want to maximize this over a Q。 The second expression has
    no Q in it。 So to maximize we。 can ignore this。 So now we're just maximizing this
    negative Q out divergence。 All right。 Overall。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化的Q步骤？我们已经最大化了Q步骤和最大化的theta步骤。这将帮助我们完成最大化Q步骤。那么这是我们的下界，现在有了等式。我们。想要对Q进行最大化。第二个表达式中没有Q。所以为了最大化，我们可以忽略这一项。所以现在我们只需要最大化这个负的Q散度。好了。总的来说。
- en: possible distribution is Q。 Okay。 So what's the answer to this maximization
    problem then？ Yeah？
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的分布是Q。好的。那么，这个最大化问题的答案是什么呢？嗯？
- en: Yeah？ Yeah。 Right。 Yes。 That's right。 So the maximum， so first of all， let's
    remember。 that the kau'd divergence is greater than equal to zero and equals zero
    only if it's two。 pieces are equal。 The two distributions are equal。 All right。
    This is a negative Q divergence。 which means it's always going to be negative
    or zero。 So if we can make this zero， that's。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯？对吗？是的。没错。那么最大值，首先让我们记住，Kullback-Leibler散度大于等于零，并且只有当它的两个部分相等时，才等于零。两个分布是相等的。好吧。这是负的Q散度，这意味着它总是负数或零。所以如果我们能让它为零，那就。
- en: the maximum。 How can we choose Q to make this zero？ Very easy。 Take Q equal
    to P of Z given。 X and theta old。 This would have been what you could have guessed。
    And it's pretty natural， right？
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值。我们如何选择Q使其为零？非常简单。取Q等于给定的Z的P。X和旧的theta。这是你本来可以猜到的。也挺自然的，对吧？
- en: What's our best guess for Z？ How about the conditional distribution of Z given
    what， we observed？ X。 It's very natural。 And here's the proof that that is the
    maximizer for fixed， theta old。 Okay。 So at that， so now we have this equality。
    If we plug in Q star， then this， is inequality。 So is there something odd about
    this？ It's a lower bound。 And then I'm saying， it's equal。 Like。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对Z的最佳猜测是什么？那么，如何看待给定我们观察到的X条件下的Z的条件分布？这很自然。这里是证明它是固定theta old情况下的最大化器。好的。所以现在我们有了这个等式。如果我们代入Q
    star，那么这个是一个不等式。那么这有什么奇怪的吗？这是一个下界。然后我说它是相等的。就像。
- en: are we done？ What's the gap here？ So have we found， does this mean， we can？
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了吗？这里的差距是多少？那么这是否意味着，我们可以？
- en: So it's just inequality at one particular theta at theta old。 It's not inequality
    everywhere。 It's equality at theta old。 This expression was for a fixed theta。
    Okay。 This equality。 was for a fixed theta。 That's important。 All right。 Okay。
    But we do have always an inequality。 for all theta。 So remember， L is always a
    lower bound for any Q and theta。 So we have。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它在特定的θ_old处是等式。并不是在每个地方都是不等式。在θ_old处是等式。这个表达式是针对固定的θ。好的。这个等式是针对固定的θ。这个很重要。好的。好的。但是我们总是有一个不等式。对于所有θ。记住，L总是任何Q和θ的下界。所以我们有。
- en: this inequality for all theta and an equality for the particular theta old。
    And now this。 explains our picture。 Right？ So we have equality at theta old and
    inequality everywhere else。 Right？
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有θ都成立的不等式，以及对于特定的θ_old成立的等式。现在这个。解释了我们的图像。对吧？所以我们在θ_old处有等式，在其他地方有不等式。对吧？
- en: Or less than or equal to everywhere else。 When the lower bound touches what
    it's。 supposed to be a lower bound for， it's called a tight lower bound。 It's
    tight at this point。 All right？ Cool。 Any questions on this？ All right。 So now
    we have our， we could talk about。 our general EM algorithm。 We alternate between
    maximizing Q and maximizing theta。 The expectation。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在其他地方小于或等于。当下界正好触及它应该是下界的地方时，我们称之为紧下界。在这一点上是紧的。好的？酷。有任何问题吗？好了。那么现在我们有了，我们可以讨论我们的通用EM算法。我们在最大化Q和最大化θ之间交替进行。期望。
- en: step was it's where we find this Q star， which is， we just set Q star equal
    to this conditional。 distribution。 And then we plug that Q star into our lower
    bound L。 So L Q star over theta。 And when we plug it in， this is an expectation。
    Right？ Because it's an expectation with respect。 to Q star。 That's why it's called
    the expectation step。 So defining， finding this function right。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤是我们找到这个Q_star，我们只是将Q_star设为这个条件分布。然后我们将Q_star代入我们的下界L。所以L Q_star/θ。当我们代入时，这是一个期望。对吧？因为它是相对于Q_star的期望。这就是为什么叫做期望步骤。所以定义，找到这个函数对吧。
- en: as a function of theta， which is what we want to maximize。 It's an expectation，
    so kind。 of computing that function， that expectation， that's the expectation
    step。 All right。 So。 what's the next step？ We've maximized over Q。 That was the
    choice of Q star。 And now we。 maximize over theta。 Right。 With fixed Q star。 So
    we want to maximize L Q star theta with。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为θ的函数，这是我们想要最大化的。它是一个期望，所以。计算这个函数，这个期望的过程就是期望步骤。好了。那么，下一步是什么？我们已经对Q进行了最大化。那就是选择Q_star。现在我们。对θ进行最大化。对吧？在固定Q_star的情况下。所以我们要在L
    Q_star θ上进行最大化。
- en: fixed Q star。 And we just call that J theta。 So we just find the maximum of
    J theta over， theta。 That's the maximization step。 All right。 So there you have
    the EM algorithm。 And right， out of time。 But yeah。 That's a great question。 I
    think about that。 I want to say it's because， of the log。 but I don't want to
    even jump to that。 All right guys。 So I'll see you on， Wednesday。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 固定的Q_star。我们把它叫做J(θ)。所以我们只需要在θ上找到J(θ)的最大值。这就是最大化步骤。好的。这样你就得到了EM算法。对，时间差不多了。但这是一个很好的问题。我想了想，我想是因为对数的原因。但我不想马上跳到这个结论。好的，大家。那么星期三见。
- en: I'm flying back from somewhere on Wednesday。 So if anything weird happens with，
    the airplane。 I will try to post the piazza。 But I should be here。 Okay。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我星期三从某个地方飞回来。所以如果飞机发生了什么奇怪的事情，我会尝试在Piazza上发帖。但我应该会在这里。好的。
- en: '![](img/362c510b7786023274101d45cf5e21b5_3.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/362c510b7786023274101d45cf5e21b5_3.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
- en: '![](img/362c510b7786023274101d45cf5e21b5_5.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/362c510b7786023274101d45cf5e21b5_5.png)'
