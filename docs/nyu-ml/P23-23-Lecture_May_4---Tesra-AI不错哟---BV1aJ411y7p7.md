# P23：23.Lecture_May_4 - Tesra-AI不错哟 - BV1aJ411y7p7



![](img/f9864a91004ac8c69f8ecdc814883727_0.png)

好的，这声音太大了还是可以的，所以今天的计划，首先，我们做了很好的调查，然后我们将直接完成EM算法，我们在EM算法上进行了几轮很好的热身，但现在我们要穿过它，谈谈EM的几个扩展。在那里的某个地方可能会有一个休息，然后有一个班级照片的请求，我认为这是个很酷的主意，所以休息后可以是一张班级照片，然后在E之后，M算法我们可以取一些随机的问题，如果你们有什么还在坚持的。或者我们可以简单地介绍一下神经网络，我有一些神经网络的幻灯片，然后我们会提前结束，你们可以拿起你们的测试，明天是和顾问们的会议，现在我要把它包起来，我们很好，好吧，让我们继续谈论好吧。所以我想你们会想复习一下EM算法，所以让我们记住设置，我们有潜在变量模型，其中我们有变量，观察到的一组变量，这些是X的，和一组未被观察到的变量，这些是Z，我们要写x和z，对于Z将是所有未观察到的变量。

称为隐变量或潜在变量，x将是所有观察到的变量，我们要把它们放在一个向量里，我们有一个概率模型，所以给定θ的xz表示x和z上的联合概率分布，我们收到的是这个不完整的数据集x，不知何故。我们需要弄清楚给定θ的概率分布pxe是多少，只是来自X，所以换句话说，给定x，我们想求最大似然参数θ，这是这个问题的最大似然目标函数，D现在是我们观察到的数据，只有X，我们想找到给出观测数据的θ。最大似然，我们想做的另一件事是估计模型，让我们找到西塔，一旦我们有了模型，我们就开始了，我们观察到一些X，那是未被观察到的东西，我们要做的是弄清楚，Z是未被观察到的部分，所以在我们的高斯混合模型中。比如说，x是我们观察到的所有点，每个x对应一个z，这是潜在的变量，哪个是X来自的星系团，所以给了下一个，我们想知道Z上的分布，x来自的星系团上的分布，所以这就是这个分布，这很容易计算，嗯。

所以对于小工具混合模型，至少在给定x的情况下计算z i很容易，i与子溪的关节成正比，这很容易写下来，它是星系团的概率z i乘以可能性，与平均μi和协方差相关的簇子分布，西格玛·泽西，很好。回顾一下关于这东西的任何问题，好的，所以有一个高斯混合模型，问题是很难解决这个最大化问题，最大化这个边际可能性，所以我们上次讲的是找到一个下限，叫做L，它有两个参数，有一个分布q和θ。我们证明了这个L是我们想要最大化的下界，数据的可能性是多少，所以让我们看看，这是我们对这个下界的推导，这种不等式来自延森的不等式，所以左边的东西，这就是我们想要最大化的，很难和它一起工作。所以我们用这个表达式给它下界，如果你看着它，你可以看到这是一个期望，因为z的q，那是Z上的分布，这个表达式是z的函数，所以它是一个随机变量，这整件事是一个对数的期望，我们称之为qθ的l，在其他情况下。

也许你们已经采取了一些为什么不，也许你上过课，你已经做完了，l前的变分推理称为肘部elbo，经常出现的有趣术语，E代表证据，有时这种边际对数似然被称为证据，这是你观察到的数据，所以e和l和b是下界。所以证据下限，我想O是束缚的一部分，波，记住这里的策略是在所有分布上最大化l qθ，q和所有参数值θ，所以我们最大化下限，我们能得到的最大的LQθ，与最大化相对应的任何θ，这就是我们最终要提出的建议。我们认为这是最大的可能性，或者这是我们对最大可能性的最佳猜测，所有的权利，所以另一个有趣的事情是，我们可以为下界写出这个精确的表达式，因为这其中的区别也是一个期望，这是预期的完整数据日志可能性。我为什么说这个pxe，这是完整数据的可能性，所以x是未观察到的变量，z是潜在的变量，我们在Z上求和，我们不观察Z，所以我们对z的分布求和，所以就像我们不观察Z一样，所以我们对某个分布的z的期望值，q。

第二学期没有θ，所以如果我们想在θ上最大化，我们只需要专注于第一学期，好的，所有的权利，这是我们将要看的照片，今天有一堆，红线是我们的你们还记得这一切吗，还是这是一个很好的评论，好的，好评，好的。这是边际可能性的对数，x轴是θ，参数空间，我们想找到θ，使我们观察到的数据的可能性最大化，所以就像这里一样，这很难直接最大化，所以我们从某个随机点θ开始，一些初始化点θ，我们在那一点上找到一个下界。这里首先要注意的是，到目前为止，我们只需要注意下界lqθ，有很多，在这里我们画了两个如此不同的q不同的概率分布，对不起，不同概率分布，q产生不同的下限，所以这里有一个提示给出蓝线，另一个提示给果岭。上次有一个问题是这些下限是否需要，都是凹进去的，对呀，我就像，我不想说那么多，所以实际上答案很简单，因为EM算法中没有任何东西，我们如何知道一个函数是否是凹的，有一个要求是，甚至在我们开始谈论函数之前。

谈论一个函数是否凹，这个函数必须定义在一个凸集上，在我们讨论一个函数是凸还是凹之前，因此，如果在集合上定义了一个函数，看起来像，假设，我们有一个函数，它只定义在这些不相交的区间上，也许在它被定义的地方。它看起来是凸起的，我们甚至不能应用凸性的概念，因为如果你拿这两个点，你看看它们之间的线，我的意思是函数甚至没有定义在那里无论如何，部分，在凸函数的定义中，函数需要定义在凸集上，所以在参数空间θ。它甚至不必是凸的，而且，它甚至不必是连续的，它可能是一个离散参数空间θ，所以凸性或凹性的概念甚至不一定适用，这是这个问题最简单的答案，好的，所有的权利，到目前为止还有什么问题吗？好的，好，所有的权利。所以我们要试着达到最大值，通过迭代方法确定该边际对数似然的最大值，所以lqθ是我们试图最大化的，作为两个参数q和θ，我们将通过坐标上升来做到这一点，在那里我们交替，我们先固定一个θ，然后我们允许q变化。

找到使lqθ最大化的q，我们称之为Q星，然后我们要保持Q星固定，让第二个参数改变theta并在theta上最大化，我们交替，这就是我们的大图像算法，所以这是我们开始一个θ，这里有一个下界。我们跳到下界的最大值，然后我们在这一点上找到一个新的下限，那就是绿线，然后我们跳到这个下界的最大值，我们继续，看起来就是这样，我们稍后再看这张照片，再仔细一点，所以这是一个惊人的等式。我们有边际对数可能性，我们可以把它写为等于下界，加上这个kl，分布Q之间的分歧，和我们未观察到的变量的条件分布，给定观察到的变量，对吗，所以是的，我们已经证明了对于任何Q和θ，lq，θ是px上的下界。给了西塔我们想要的，而且，在这里我们得到了两者之间的精确差距，下界之间的间隙，我们的下界，正是这个分布Q之间的关键分歧，和给定x的z的条件分布，这是我们将在下一张幻灯片中使用的关键等式，所有的权利。

所以让我们把注意力集中在θ被固定的阶段，让我们优化Q，这是我们的下限，我们在西塔老，我们想找到使这个lqθ最大化的q，这是我们对lqθ的确切表达式，你会注意到第二块没有Q和A所以，为了最大化Q。这肯定无关紧要，这是一个负的分歧，那么Q将使这个表达式最大化，是呀，所以要记住，kl散度有这样一个性质，首先，它总是大于或等于零，其次，它只等于零，当KL改道时，当它需要两个相等的分布时。所以kl散度有两个参数，它需要两个分布，这有点像测量一个距离，两个分布有多不同，也就是，如果两个分布完全相同，只有在它们完全相同的情况下，我们得到的散度为零，对呀，所有的权利，所以这是一个负散度。那么最大的价值是什么，负发散可以为零，因为它是如此的KL总是积极的，是呀，所以底片是可以的，然后我们如何使它为零，确定取q等于z的p给定x和θ，好的，所以对于固定θ，最大化q。

所以你认为这是一个很难的问题，因为你知道我们在最大化总体概率分布，在这种情况下是这样发生的，由于KL发散的这种性质，这很容易，好的，所以我们找到了使固定θ的l最大化的队列，好吧，让我们把它插到Q星上。那么第一项为零，剩下的是这个对数px给定θ，这正是我们试图最大化的，对呀，所以在西塔老，我们试图最大化的东西之间是平等的，现在是一个下界，所以当下界正好等于它的下界时，这叫做紧下界，就在那儿。它与曲线相切，我们试图下界的函数，所以让我们把它看成一张照片，所以平等就是为什么我们有，所以在西塔老，我们θ的下界l q，这是所有θ的下界，但尤其是在Theta老，这是平等的权利，这就是平等，它很感人。这就是这种平等，那是从哪里来的，好的，所以在图片中，切线不是巧合，这应该是在那里所有权利，否则就是下限，有什么问题吗？好的，所有的权利，所以一般的EM算法，第一件事叫做期望步y。

第一件事是我们在哪里找到优化的Q星，的，固定θ旧的下限，我们把它插进去，所以现在LQθ星，下一步是最大化我们的θ，所以当我们写出lq星θ时，把它拆开，我们有这个对数表达式的Q星号之和。现在这又是一个期望，因为我们把这个日志表达式求和为z的函数，我们以前见过同样的事情，z的q星是每个z的概率，我们正在计算所有可能的结果，c's，这是关于Q星的对数表达式的期望，我们需要计算这个期望。以Q星θ的函数结束，我们就叫它Jθ，强调不再涉及Q的事实，我们已经我们已经采取的期望，我是说，在这一点上，我们只对这些数据感兴趣，那是杰西塔，所以计算，这是计算一个期望，这就是为什么在EM算法中。这个东西被称为期望步，所以接下来的事情是，对于固定的Q星，我们在θ上最大化，就在这边，我们需要什么才能让这个算法工作，正如它所说的那样，最大化需要是可行的，不仅仅是期望需要是可行的。

这些计算中的每一个都需要相当简单，当你把这个应用到高斯混合模型时，比如说，它是它是非常，很简单，你可以很容易地做到这一点，让我们来谈谈我们对这个东西实际工作的了解，到目前为止，我们已经讨论过LQ了。我们在最大化l qθ，我们能把这和最大化边际可能性联系得多紧密，这就是我们想要的对吧，所以首先，让我们至少看看我们移动到的每一个θ，我们单调地增加可能性，所以我想这很清楚，每次我们迈出一步。我们正移动到我们下界的一个较高的点，下界总是好的，让我们让我们追踪它，所以我们从Theta老开始，关键是向上推，我们的可能性是，我们的下限是紧的，所以我们开始老，然后当我们最大化蓝色的时候。我们还不一定很清楚，是呀，所以红线总是在蓝线之上，对呀，所以当我们从θ移动到θnu时，我们已经知道在这一点上，那个，在θnu处的似然值，一定比我们刚来的地方高，然后我们找到一个新的下界，它与红线相切。

当我们跳到最大值时，这个新的下界，我们再次知道，可能性也一定比我们来自的地方大，如果你不相信我的照片，这里有一些数学，好的，所以对于Q星来说，我们知道在Theta老。我们在下限和边际可能性之间有精确的质量，好了，现在我们选择西塔努，它使给定q星的下界最大化，所以Q星和θu的下界，肯定大于等于Q星和θ老右边，因为qθnu选择最大化这个表达式相对于θ槽。所以根据Theta Nu的定义，这当然是正确的，好吧，接下来呢，现在让我们从边际可能性开始，所以px给定θnu，所以这就是我们最后的地方，让我们看看θu的边际可能性是多少，嗯，我们知道我是一个下界。所以它肯定大于等于l，在q星上，θ，nu，好的，现在我们应用上面的不等式，大于或等于Q星θ老，然后我们也有平等，好吧，所以我们在这里看到，当我们从西塔到西塔努，利润的可能性肯定至少一样大，所有的权利。

那是一回事，所以这很好，至少当我们采取措施增加下限时，我们知道边际可能性增加，真令人欣慰，那很好，让我们多说一点，假设我们能够找到下界的全局最大值，所以我们有lqθ，我们做得很好，我们发现全局最大值。结果会是，这也是边际可能性的全局最大值，太神奇了，因为那是我们想要的，那该怎么做呢？假设Q星θ星是下界的全局最大值，所以它肯定比任何地方的l qθ都大，好的，我们也知道对于给定的θ星，条件概率是最大值。这就是我们所展示的，所以我们知道Q星是什么，所以声称是θ星，它参与了下界的最大值，也是这个边缘对数的最大化器，很有趣，让我们看看它看起来怎么样，所有的权利，所以让我们取任何一个θ质数，所有的权利。所以会有其他可能的θ质数，它可能比我们的θ星更好，我们将证明实际上不是，但让θ是任意的θ质数，是任何其他点，我们知道q质数z是，最佳分布对应于θ素数，在θ质数处的边际似然对数等于这个下界。

加上kl发散度，这一直是正确的，因为我们用的是q素数，哪一个是等于LQ素数的最佳分布，θ质数，现在我们所知道的Q星，西塔星，假设下界的最大值，所以这肯定小于或等于这个，所有权利，现在我们终于有了松紧。所以l立方θ星等于px给定θ星，所有权利，所以如果你把它放在一起，我们看到对于任何其他θ质数，边际对数似然将小于，或等于θ星的边际对数似然，好的，所以现在就是四行，所以现在，我们给了自己很好的动力。试图最大化这个下限，如果我们真的能最大化下限，我们还发现了边际对数似然的最大值，所以这是一个全局最大值，事实上，一般来说，我们不知道如何找到全局最大值，好的，那太糟糕了，嗯，稍加努力。你可以把这个定理推广到局部极大值，这很好，你可能需要有某种功能的连续性，或者一些小的技术条件，但可以扩展到本地最大值，嗯，但至少这给了很好的动力，所以实际上有一个定理，没那么老。

实际上在m和i左的收敛性上，它有一些技术条件，会有点超出水平，这学期到目前为止我们一直讨厌的，但我有一个推荐人给你，但只是为了给你一个定理所说的味道，所以让我们把它设置一下，Theta n。设θn为θ的当前位置，在EM算法的最后一步，在第n步之后，让我们定义一个转移函数m，所以在，如果一开始我们在零点，在第一回合后，我们在第一回合，然后总的来说，让这个函数m从θ1到θ2。或者从Theta n到Theta n，加一个，所以θ加1等于θ的m，这将帮助我们把事情写下来，这清楚吗，所有的权利，所以我们假设这个对数似然函数是可微的，这里有个话题，我想我们还没有完全提到。让我们是静止点的集合，所以函数的驻点就是点的集合，其中梯度零，所以当你寻找最小或最大或鞍点时，你要找的，其中梯度零，所以这些都被称为驻点，事实证明，这将是最好的，我们可以向它展示到一个固定点的收敛。

好的，所以定理是这样的，所以在一定的规律性条件下，你可以在这篇论文中寻找，不管有什么好笑的，这将是另一个规律性条件的幻灯片，嗯，对于任何起点，θ0，我们θ序列的极限，从m算法收敛到θ星，对于某个固定点。那很好，你知道的，到目前为止，我们还没有证明这一点，直到定理，我们不知道我们可能会失去动力，我们总是知道每次我们去一个新的Theta，活的可能性在增加，我们证明了这一点，所以这很好。但我们没有证明它实际上会到达一个固定点，你知道如果它总是在增加，但是越走越慢停了下来，它仍然可以上坡，但它就像它没有完全停止，但它下降得很慢，它实际上并没有在山上取得进展，所以定理说不，那是不会发生的。我们实际上会聚到一个固定点，所以这是一个很好的定理，这是其他几件事，但没那么有趣，好的，所以有一个EM定理的收敛性，关于这个EM算法有什么问题吗，所有的权利，如果你们上一零五课，概率推理或图形模型类。

当然了这样到时候你就可以复习了，让我们谈谈这个算法的几种变体，可能会固化一些概念，所以算法基本上给了我们两个问题，我们要解期望步和最大化步，这是期望步和最大化步，这只是在参数空间上最大化吗。问题是这两个步骤中的任何一个都可能很难，对呀，计算，这个期望可能很难，计算最大值可能很难，所以每种情况下都有不同的扩展，所以有一种叫做广义EM的东西，这真的没什么大不了的，听起来很花哨。但它只是减轻了M算法的假设，所以不是在每一步中找到θ的实际最大值，我们满足于发现一些严格比旧的更好的新的，好的，例如，如果你可以取一个渐变，渐变不为零，事物是可微的，你总是可以迈出一步，也许很小。但你总是可以采取一个步骤，我们有严格的功能增加，所有的权利，所以这将是广义的dm，只要朝着正确的方向前进，即使你不能做到完全最大化，所有的权利，没事的，还有什么困难的，这是这是嗯，当人们谈论变分方法时。

这是很常见的，他们通常会在E步很难的情况下，他们所做的是用Q星代替Q星等于PZ，给定x，这是你能得到的最好的暗示，这就是你的下限和下限之间相等的地方，就在你拿Q星的时候，问题是，然而，对于特定的Q星。这种期望可能很难接受，但这个想法是，可能还有其他Q星，其中期望更容易计算，所以我们引入了概率分布的限制，q um，我马上给你举个例子，然后你找到Q星，这使得这两个东西之间的KL偏差最小化。服从q在此约束中的约束，易于使用的发行版集，所以说，我们知道我们至少得到了最好的分配，在那些我们喜欢计算的东西中，例如，嗯，这有时可以用于图形学习，还记得我们在几节课前讨论过贝叶斯网络吗。记住贝叶斯网络有这些条件独立关系，但也有依赖性，这就是箭头，它们表示随机变量之间的依赖关系，所以在这种情况下，我们考虑的一组标准分布Q被称为，所谓均场近似是我们考虑分布集q的地方。

其中每个随机变量都是独立的，所以即使他们不是独立的，我们将通过选择分布Q来找到一个下界，从一组分布中，每个变量都是独立的，平均场近似非常常见，好的，所以这是两种放松，另一个应用程序，我想我会给你。因为这很容易，我们谈到了地图估计，至少有些家庭作业，所以在这种情况下，我们有一个关于θ的先验，我们想在那里找到，我们仍然要估计θ，但我们要通过找到θ上后验分布的峰值来估计它，对呀。所以我们有标准的贝叶斯设置，我们在参数θ上有一些先验分布，我们观察到一些数据x，现在我们更新我们在θ上的分布，我们得到了θ上的后验分布，所以这是给定x的pθ，给定观测x的θ分布。现在我们可以在θ上最大化，这叫做地图估计最大值，后验估计，所以我们可以在这里做同样的下边界，让我们拿同样的Q星，所以l q星θ取l有相同的定义，它仍然是这个边际对数可能性的下界，所以让我从这里开始。

所以pθ给定x，这就是我们现在试图最大化的后验分布，让我们把它的日志，现在它碎成了一些碎片，第一块是边际对数似然，我们刚刚讨论了如何用m算法最大化这一点，然后我们有这个额外的p theta原木。然后如果我们最大化θ，最后一块没关系，因为它是PX的对数，不涉及θ，所以我们要用我们的下界来处理这一块，所以我们仍然有lq星θ，这就是我们如何选择我们的CU，但是最大化步骤现在有这个部分加上pθ的对数。因为当涉及到θ时，我们需要用这两块来做后部，因此，EM算法非常适合于地图估计，可能值得研究这些细节，作为一个小暑假作业给你，关于EM算法我要说的就这些，实际上这些幻灯片的其余部分你可以看看，是嗯。所以如果我们有另一个家庭作业，家庭作业是什么，将导出高斯混合模型的EM算法，根据这一一般提法，在第十三讲，我想我们谈过，高斯混合模型M算法的特殊形式，看起来很直观，但我们没有派生出来。

下面是EM算法的一般推导，特殊情况将是，如果你解决了，正是我们在上一节课中谈到的，所以这里有一些幻灯片给你一些提示，通常伴随着家庭作业，看一看，所有的权利。

![](img/f9864a91004ac8c69f8ecdc814883727_2.png)

你们想启动神经网络，还是你现在想问几个问题，休息一下，然后启动神经网络，想现在或最后把你的胸部拿回来，在最后，所有的权利，最后想要的请举手，如果你现在想要，她要我寄给你，好的，所有的权利。我们现在就做吧，为什么不呢？嗯，所以现在就做，然后我们让它跑到刹车上，然后我们大约八点十分开始，好的。

![](img/f9864a91004ac8c69f8ecdc814883727_4.png)



![](img/f9864a91004ac8c69f8ecdc814883727_5.png)



![](img/f9864a91004ac8c69f8ecdc814883727_6.png)

你想拍张照片吗，只是为了拿一个，是啊，是啊，是一样的，好的好的，一二三，我要再做一次，哈哈哈哈，这位年轻女士的每一个人，事实上，超过五个人完全失踪了。

![](img/f9864a91004ac8c69f8ecdc814883727_8.png)



![](img/f9864a91004ac8c69f8ecdc814883727_9.png)

你们中有多少人对神经网络有所了解。

![](img/f9864a91004ac8c69f8ecdc814883727_11.png)

让我问一下，首先是这学期同时上神经网络课的人，所以我会给你我最喜欢的幻灯片，这里人太多了，现在不能做了，我将翻阅高光部分，所以说，我假设你们根本不知道什么是神经网络，但我想你们中的一些人可能知道。所以简单的目标是什么是神经网络，至少是最简单的神经网络，它如何适合我们的工具箱，我们什么时候应该考虑使用它们，实际上，到目前为止，到目前为止还没有到目前为止，我们已经做了线性方法和非线性方法，的。线性的或支持向量机和岭回归，套索长线性的，就像任何基于树的随机森林增强，因此，神经网络具有很强的非线性，事物非线性的一面，从高水平上来说，神经网络给你的，基于树的算法在非线性方面给你的是。神经网络的输出是平滑的，最近，它们并不总是可微的，传统上它们是可微的，但是，但现在他们可以踢了，但他们至少是连续的，好的，所以这里有一个神经网络，到目前为止我们所做的所有线性方法的版本，θ1到θ1到3。

这些就像我们现在表示的特征向量，就像这样，这个图不是，贝叶斯网络是另一种图，更像是一个计算图，但是θ1，θx 1到θx 3，这些是作为输入的三个特性，这代表的是这些特征中的每一个都乘以一些权重w。每支箭上都有一个重物，它们都聚集在一起形成一个最后的节点，在这张图中，这意味着它们被加在一起，所以分数等于线性代数符号中的分数，x的w转置phi和x的phi是这三个节点的向量，w表示所有这些权重。所以这将是一种用神经网络表示法表示线性分类器的方法，所以说，当然，只有当我们添加另一层和非线性变换时，它才会变得令人兴奋，所以我们在这里加了一层，称为输入层和分数之间的隐藏层。最终输出和每个节点的输入来自整个未来向量，所以这个特征向量，在本例中，这些条目中的每一个都乘以一个权重v，然后把它们加在一起，这就成为这个节点的输入，然后特征的线性组合的总和通过一些非线性变换，西格玛。

这就是这些神经网络非线性的原因，同样的事情发生在另一个节点，这两个节点的区别在于权重，对于这两个节点中的每一个节点，输入的线性组合都是不同的，最后，这两个节点的输出线性组合成最终的分数。这是一个基本的前馈神经网络的模板，任何关于这一点的问题都很清楚，所以这里是这里，让我写下这张图片的数学含义，所以一个隐藏的节点，让我们打电话说h one或h i，它是隐藏节点的输出。沿着这个箭头向下的等于sigma的sigma，这是一个函数，它被称为激活函数，它一般是非线性的，我们稍后会看一些例子，这就是非线性的来源，你传递给非线性函数的是一个数字，单个标量，它是输入的线性组合。所以总结一下，你得到一个X，你把X的特征表示，那是x的phi，你取V转置的特征向量的线性组合，i，e，那是标量，数字，你把它通过一个非线性函数，这给了你另一个数字，这是Hone的输出。

你在H 2上做同样的事情，你得到两个数字，然后你把这两个数字的线性组合，h 1和h 2，如果你把它，是啊，是啊，好的功能对每个都是一样的，你可以做你想做的事，但是是的，对于所有音符。激活函数通常是相同的，是啊，是啊，嗯是的，您可以有任意数量的隐藏节点，你可以有任意数量的层，这是这个基本模板，所以我会看看，这些是回答你问题的子弹，所以耶，可能会有，你知道，五万个节点，可能会有。嗯是的，传统上你知道，如果你回到十多年前，通常会有一个隐藏的层，但最近他们你现在走了，你回到十年前，他们做了八层，最近他们只有一千层，所以有这个，他们没有做更多层的原因，因为他们发现很难做到。在深度网络中，学习部分的训练很难，但近年来有了很大的进步，关于如何有效地学习所有参数，所以所有的w和v都是从数据中学习的，没什么太花哨的，基本上是随机梯度下降，可能会有一些小的变化和改进，但是嗯。

但他们很难训练他们适应较小的网络，但现在，通过不同的发展来训练他们，即使是网络也像一千层，同样的不，我是说，第一输入层不隐藏，最后一个没有隐藏，其余的是因为你没有直接观察到这些节点。这些不在潜在变量模型中的原因之间的区别，就像我们所说的EM算法，我们实际上想知道这里隐藏节点的值，我们不一定真的关心他们，它们只是让我们得到这个非线性函数的一种方法，所以我想这是。这是一个参数化的非线性函数，参数是所有的W和V，我们必须学会它们，但从高层来看，什么都不跨越，这里没有什么是我们不熟悉的，这是一组从r 3映射到r的函数，在本例中，由w向量和v向量参数化，就这样了。好的，那么这与带有非线性核的线性回归相比如何呢，所以我们就叫它非线性特征图吧，好的，所以这只是回到这个案子，所以我们把非线性特征作为输入，然后这是线性回归，所以这是很新的东西，对吧。

即使我们有输入是非线性的，也是用非线性特征来表示的，即使函数是一个非线性函数，这真的不一样，是啊，是啊，哦耶，为什么非线性，嗯，我想最简单的动机是，很难发明所有的非线性，如果你想没有这个。那么你就必须发明你的非线性特征，人们已经做了很长时间了，但如果你能自动学会，那不是更好吗，事实上，最近，在某些领域性能要好得多，就像谁决定西格玛一样预测，选择西格玛，西格玛不是学过的，在这种情况下。西格玛是由数据科学家选择的，只有几个通常使用的，我一会儿就给你看，是啊，是啊，是啊，是啊，H在这种新功能上，我们能把H作为新功能吗，当然，我希望你把H作为新功能来考虑，事实上，我想让你考虑H作为特征。通过神经网络学习，而不是自己设计，这是我们越能有更多的年龄，更多数据，我们也可以，你说你可以有更多，你可以有更多是的，而不是从H1到H2，可能是H一三额外，一千个是，你也可以有更多的层，这是正确的。

我不做你想做的事，嗯，增加，这是个好问题，那么在什么条件下，所以你你有，你有，网有多宽、网有多深，人们最近发现，更深的网络似乎更有效，如果你能训练他们，所以如果有了所有的交易技巧。能够成功地最小化你的目标函数，实际上得到很好的训练误差，似乎更深的网络往往在以下方面工作得更好，说，比更宽的网络更少过配，说，即使对于相同数量的节点，但是有点，它仍然是非常艺术的，你发现为了有效的表现。人们发现深度网络往往会提供更好的性能，至少在某些领域，如视觉，比如说，我不想给规则，我只是说，粗略地说，人们最近通过使他们的关系网变得更深而取得了成功，所以这是一个很好的方向。所以神经网络的挑战之一是你可以改变太多东西，网络结构，你的激活功能，你有所有常见的问题，你好。优化，你的步数，诸如此类的事情，所以在神经网络中有很多东西在进行，所以看起来一切都很顺利。

你去参加神经网络会议，我刚从一个，所有这些很酷的东西都很有效，你会感觉到他们就像，哦，我就试试看，会成功的，问题是所有这些微小的变化，什么有效根本不起作用，这样你就会和人们说话，他们会说，是啊，是啊。这太棒了，但是当我把我的激活函数换成另一个东西时，它实际上很穷，所以我想如果你要进入神经网络，一个好主意是从你知道有效的东西开始，然后逐渐调整到你想去的方向，除非你有很多时间做很多实验，让它工作。但最近神经网络社区最酷的地方是那个男孩，他们把所有的代码都放在GitHub上，他们真的很喜欢分享，让一切都可以复制，所以从某种意义上说，参与进来再容易不过了，就像，你知道的，最先进的东西就在那里。你可以复制结果，然后你想玩它，你知道你总是可以回到他们工作的设置，所以这是一个很好的，对于喜欢玩东西的人来说，这是一种很好的感觉，然后嗯，这不是一个很好的领域，如果你喜欢有非常严格的理由。

为神经网络设置某些参数设置，因为据我所知，这仍然不存在，所以有很多游戏、调整和实验，结果在很多领域也往往是最先进的，所以说，这是修补匠的好时代，数学家们也许已经有了他们的一天，好的，所有的权利。关于这个基本设置还有什么问题吗，所有的权利，所以激活功能，这是多年来最常见的，叫做乙状结肠，饱和在零和一，有一些弧线切线，基本上是一模一样的形状，刚刚转移了，所以介于负数和有时也用的负数之间。关键是它是B非线性的，这就是神经网络的力量，这是魔法，如果它是线性的，那就一点也不有趣了，这只是一个所有，这些线性变换层将崩溃为单个线性变换，那不是什么新鲜事，作为乙状结肠，这是最近很流行的一种。效果很好，它被称为整流线性函数，现在人们只是把它称为relu给你，就是这个形状，所以说，你们看到这种功能的利弊了吗，你知道的，想象你正在处理它，你写下你的目标，它有这个功能，嗯，你要做新元。

你要采取渐变，你得用它计算，所以这一块在零不变，这一块正好是y=x，是啊，是啊，所以一个问题是，它是不可微的，但是好吧，我们对此没意见，我们做次梯度，没什么大不了的，反正你也没踢过，好的，我们没事。所以另一件事是两件事，计算速度超快，和这里相比，我们有一个指数，是啊，是啊，计算机很快，但指数仍然是，你知道吗，你必须做一堆运算来计算，数的a的指数，但这很容易，就我所知，如果没有更快的计算机计算的话。如果是这样，你分支，如果小于零，你知道，要么返回与输入相同，或者是零，所以这是非常快的计算速度，它的衍生物已经死了，太容易了，不是零就是一，好吧，另一个if语句，所以你一个。你在这里得到的主要是计算速度，那个，为什么这很重要，因为如果你调整你的模型，你想让事情跑得快的很多方式，您可以调整更多常见的主题，你想使用快速的计算机和快速的算法，主要是给自己更多的时间来调整超参数。

因为你的很多表演，不仅仅是神经网络，但对于所有型号，将来自于仔细尝试新事物，您尝试新功能，你尝试新的超参数，你尝试新的型号，所以速度很重要，所以我觉得这很整洁，对神经网络内部的一点观察。所以这里我们要看的是拟合一个函数，y等于，f是x的平方，x的f是x的平方，所以这代表了我们拟合的函数，蓝点是我们的训练步骤，三个隐藏节点的输出，所以想象一下我们有这个网络，除了代替H1和H2，是他一个。氢二和氢三，我们看到的是，跨输入维度的三个隐藏节点的输出为一个，所以我们有一个输入节点，输入维度只是一个实值，我们正在查看这三个隐藏节点的输出，对于输入的各种值，这些是我们作为隐藏节点输出得到的曲线。所以我们看到非线性存在于隐藏层的输出中，然后红线是最后的线性组合正好把这个x平方，因此，如何将这些功能组合起来以适应，我们要找的东西很清楚，这是什么，所以我有同样的图片为其他几个功能，正弦x。

这三个输出加起来是线性的，组合正弦x，那很酷，这里有一个V函数，所以它不是很精确，因为你知道这些，这是为了乙状结肠逻辑激活，非常光滑，所以为了得到一个扭结，你得努力工作。这就是乙状结肠必须迅速改变的原因，好的，这里有一个步骤函数，是啊，是啊，我也觉得很酷，有一个伟大的告诉你们，有一个很棒的JavaScript，一种实验室模拟器。在那里你可以让像点击这样的网络变得非常容易，我会发一个到广场的链接，它是，这是一个很好的方式来了解发生在你身上的事情，得到这样的图片，保持它们在OK输出中的功能相同。所以这里发生的是我们用随机梯度下降训练模型，我还没有谈到损失函数，或者怎么，但它是，没什么好奇怪的，所有的重量都设定好了，然后我们输入不同的x值作为输入，我们查看每个隐变量的输出，所以现在再来一次。

你的问题，我只想玩这些东西，然后就像，当你能写的时候，我们利用了这个功能，我们的意思是我们看起来，它是在，就像我们做的那样，因为这就是，哦是的，所以你使用的激活函数的选择是一个建模选择。所以你试试乙状结肠，好像我不喜欢那个，让我们试试雷卢，让我们试试，人们使用的选择并不多，所以你尝一个，你试试另一个，就是这样，或多或少有一些，最近，有不同的修改，动机，使用一个函数，那是。是什么给了你一个强大的动力，已经计算速度和容易性，但你在问，我觉得，从建模的角度来看，关于relu，你会注意到一件事，那就是它是无界的，而乙状结肠在零到一之间有界，所以发生的事情是。如果你看看你有训练数据的区域之外的预测，行为是可以预测的，这就是Relus倾向于爆炸，像这样，而乙状结肠就不是这样了，所有的权利，所以好吧，还有什么问题吗，我想再给你看几样东西，是啊，是啊。

我派人去找红色的，不知何故，是啊，是啊，就像，这是一个很好的，所以我看到了，不是神经网络，我在SVM环境中见过，嗯，我几乎可以肯定这是在神经网络中完成的，我只是不知道这是怎么回事，我没听到很多人谈论它。所以不知何故，这并不令人惊讶，它成为主流，但是是的，我敢肯定做这件事是可行的，如果你很好，出了点问题，所以如果我没理解错的话，你想把它装进后备箱吗，所以问题是，如果你在树干上经过截断。所以你在水平平坦的部分，那么你就不会得到渐变更新，所以当你做渐变下降，当你在平坦的地方时，你没有变化信号，所以这是一个和是的一样的问题，如此正确，所以如果你太消极了，你有麻烦了，但如果你太积极。这也会让它陷入困境，所以你有点。



![](img/f9864a91004ac8c69f8ecdc814883727_13.png)

这是一张卡通图，展示了你现在得到的东西，当你为视觉做神经网络时，或者你希望得到什么，你真的做得很好，如果原始输入是像素，好的，那是你的意见，你会发现在网络的不同层，您会得到隐藏的节点。这些节点倾向于响应输入中不同类型的模式，所以说，比如说，这里发生了什么，这个特别可爱的块对应于我们的第一层的一个隐藏节点，我们的神经网络，和，解释这一点的一种方法是，如果图像中的像素。这种过滤器在特定区域有很高的内积，那么这个隐藏节点就会有很高的输出，否则我们的产量会很低，所以，这些是输入中每个隐藏节点响应的模式类型，所以你可以看到他们在最低层的反应，这些会像边缘一样。在更高的层次上，很多层，如果这是人脸识别问题，它倾向于学会对面部的某些部位做出反应，所以鼻子和眼睛什么的，或者在顶层，它甚至会像整张脸一样，所以更一般地说，你希望你所学到的特征成为一种。

从非常低级的简单功能转移到高端的更复杂的功能，例如，嗯，在神经科学或假说中有一种理论，你的大脑里会有一个神经元，当你看着你的祖母时，它会有反应，它被称为祖母神经元，当你看到你的祖母时。就会有一个神经元开始放电，所以事实上他们是这样训练神经网络的，他们经常能找到祖母神经元的类似物，所以嗯，对特定的神经元做出反应的神经元，你知道的，这只对猫有反应，这只对狗有反应。这是对你最好的朋友的回应，所以他们发现这种情况有时很有趣，是啊，是啊，的，很好的问题那么我们的神经网络有多复杂，我们为什么要关心这个，因为它和过度装修有关，估计误差与近似误差的权衡。



![](img/f9864a91004ac8c69f8ecdc814883727_15.png)

所以说，让我跳到那个，神经网络，让我们把它看作是一个假设空间，对呀，那么超参数是什么，是呀，是你说的，层数，每个隐藏在层中的节点数，我们选择的激活函数，所以是的。我们的一组函数或一组神经网络是由这些东西参数化的，和那种，假设空间的复杂性会增加或缩小，取决于这些参数，嗯，有一件事让故事变得有点复杂，虽然，就是你知道，当我们谈论，当我们在学期开始的时候谈到。就像1和2，正规化，我们先有了这个，我们谈到了功能空间中的球，对呀，就像一个L 2，特征空间中的球，我们将把自己限制在所有的预测函数上，l比alpha少两个范数，这显然没有那么复杂。比所有具有L 2的函数更不可能过度拟合，参数的范数小于两个alpha右，所以我很干净，然后，但实际上我们是在做另一种形式，我们以阿尔法乘以惩罚，规范权，我们争辩说，实际上你仍然可以想象球的事情。

因为有家庭作业，其中我们证明了这两种正则化形式的等价物，所以如果你有这个惩罚函数形式，它等价于有界范数形式，所以如果你想象像，你的球变大或变小，改变了复杂性，当我们去最小化我们的目标函数时。我们倾向于找到实际的最小值，因为我们几乎总是在谈论凸函数，所以我们要么非常接近或实际的最小化，好的，所以说，神经网络假说，空间，所以我把它放在一边，但我现在会给你小费，那个神经网络，客观，目标函数不凸。这就是为什么人们很长一段时间都避开它们，因为它们很难最小化，从数学方面来看，如果你想证明定理，你不知道要证明什么，因为你想证明，你知道吗，你也许可以证明一些关于全局极小化的东西，但你永远找不到。所以你要证明的是你的算法最终会在哪里，这是一件很难处理的事情，无论如何，所以因为这个东西不是凸的，可能是一个非常复杂的模型，有各种各样的技巧或技巧，人们用来不过度适应，早停。

我们知道当验证错误开始时您将停止，向上，我们知道，但还有一些其他的东西，我会翻阅一下，嗯，所以是的，我们可以做l 2和正则化，现在有一条规定，神经网络的正则化，但人们会想出其他奇怪的事情，比如辍学。这是当你学习你的网络时，您随机选择一些节点归零，不允许它们参与，似乎是一件有趣的事情，但它似乎有助于防止过度适应，嗯，所以其他关于哪里的事情，因为它不是凸的，我们。我们的终点在很大程度上取决于我们的起点，所以你选择一个起点，你可能会在当地的最低限度，但如果你从别的地方开始，你最终会在另一个当地最低限度，好的，所以说，关键是对于神经网络来说，很难得到一个，嗯。一个大网络之间的权衡并不清楚，导致过拟合，这就是我想说的，所以对于大多数其他事情，小是之间有很明显的联系，你知道，我们想要越小越好的东西，它仍然符合数据，但不太清楚，神经网络就是这样，事实上。

人们发现有时更大的网络，In-out-of-sample性能，即使您可以在较小的网络中得到相同的训练错误，这与你通常得到的完全相反，我的建议是，原因是因为训练中有各种各样的技巧，神经网络，嗯。这样你就不会太合身了，因为它不是凸的，这使得它的联系不太清楚，你知道的。

![](img/f9864a91004ac8c69f8ecdc814883727_17.png)

所以这里有一件很酷的事情你可以用神经网络做，也就是说，您可以有多个输出节点，你可以有多个输出，这是一种非常简单的方法来获得所谓的多任务学习。

![](img/f9864a91004ac8c69f8ecdc814883727_19.png)

这是你有一个输入的地方，你想预测多种事情，这里有一个简单的例子，我有两个二进制分类问题，我想应用于相同的输入图像，我想知道，画中有一只猫吗？是或否，我也想知道。

![](img/f9864a91004ac8c69f8ecdc814883727_21.png)

画里有狗吗？是或否，好的，我们可以用两个独立的输出来表示，我是说，我们怎么训练，我们仍然需要一个单一的目标函数，好的，我们就，我们就看看这个输出的损失，将它或其他东西添加到其他输出的损失中。

![](img/f9864a91004ac8c69f8ecdc814883727_23.png)

然后进行相应的训练，所以另一个术语，我想让你们听听，所以目标函数正是你所期望的，你有你的预测功能，给你一个分数，然后你可以写下一个损失函数，在这种情况下，我写下了平方损失和你。最小化这个目标函数的第一种方法是随机梯度下降，或小批量梯度下降，这是当今神经网络中使用的方法，还有一种叫做反向传播的东西，你们听说过反向传播吗，所以反向传播实际上只是，一种非常有效和聪明的梯度下降算法。实际上是为了计算函数的梯度，以及计算你的神经网络的分数，所以这是神经网络的向前传递，其中计算输出，给定输入，和一个向后的传递，在那里你计算关于那个例子的梯度，在那个例子中相对于参数的梯度。所以我想让你把反向传播划分开来，对自己说，好的，反向传播只是梯度下降的一个聪明的算法，对于神经网络，从建模数学的角度，一点都不新鲜，这是一个很酷的算法，值得一看，关于神经网络，我要说的最后一点是。

所以我们已经提到过你可以，你可以把隐藏的层想象成神经网络发现的特征，有一种东西叫过脚，你们中的一些人可能正在使用，你们中的一些人可能正在使用它，所以他们在大量数据上训练了这个神经网络，图像分类数据，嗯。他们有一个非常成功的神经网络，拿走了，你知道吗，大量的计算能力和资金来训练网络，他们所做的是，隐藏节点，您可以将其视为网络学习的特性，您可以下载这些功能给您，这样它们就已经学会了。所以你可以下载程序来计算新图像上的特征，然后将这些特性输入到一个更简单的模型中，就像任何SPM，比如说，那是一种很好的能力，重用某人已经投入的所有计算能力来生成这个网络，也许他们在预测。你知道他们可能在预测，嗯，一组固定的类，所以你不会真的想让他们的整个网络，如果您想预测另一组类，你知道吗，也许他们在预测动物，但你想预测车型，原来这个特征，许多对一种图像问题有用的特性。

经验上倾向于对其他图像问题有用，即使是不同的类或不同的领域，所以这有点，你知道的迁移学习，在那里你把在一个领域完成的一些学习转移到另一个领域，你只要把这些特征，你可以立即在你的新数据集中插入这些功能。训练得很好，你听说过VEC的消息吗，单词到VEC对单词来说是完全相同的想法，所以Word to VEC有一个训练集，在另一堂课上谈论这个，我推荐给大家，他们想出一个非常有用的每个单词的表示。在您可以使用的新域上，而不是一袋文字。

![](img/f9864a91004ac8c69f8ecdc814883727_25.png)

好的，所以这里有一点，嗯，图片，这给了我们一个很好的视角来理解神经网络对我们的意义，似乎，随着我们拥有的数据量急剧增加，我们所说的大多数算法算法的性能，所以神经网络，人们声称他们把性能水平提高了。最终用一定量的数据，而深度学习有如此大的能力，因为你总是可以让这个网络越来越大，它的性能往往会越来越好，当你给它提供更多的数据时，当然，这样做需要大量的计算时间，这就是我对神经网络要说的。我想问任何关于神经网络的问题，因为我只是想在最后给你几个建议。

![](img/f9864a91004ac8c69f8ecdc814883727_27.png)

喜欢其他课程，你们可以拿着，是啊，是啊，深度学习是关于神经网络的，深海更喜欢很多层，没错。

![](img/f9864a91004ac8c69f8ecdc814883727_29.png)

就这么简单，是啊，是啊，是啊，是啊，关于我们所看到的领域，我们的形象，现在接近，是啊，是啊，用于图像，用于机器视觉和语音识别的图像，神经网络绝对是最先进的，所有这些都是最先进的方法，利用神经网络。自然英语处理是非常有趣的，接下来是你的问题不在那里，就像不同的问题，我们应该尝试用神经网络来接近，哦，谢谢，好问题，是呀，这没关系，所以如果你想在视觉上做一个严肃的工作，神经网络，卷积网络。如果你想进入语音识别，神经网络，当然，如果你想，做一些文档分类什么的，去吧，试着用一个词来表示VEC或其他东西，一般来说，如果你想使用神经网络，你你在想什么，你有大量的数据。你有非常强大的计算机来处理这些数据，主要意味着GPU，所以图形处理单元是你现在在神经网络训练方面取得进展所需要的，看起来，当你有更少的数据时，你没有使用它们，我是的，我觉得，是啊，是啊，我会。

我会说是的，但是他们的一个训练，或者任何预测，如果没有，如果没有，预测还是比较慢，因为你有更多的层要计算，但是是的，它不是，嗯，以帮助您在一年后或更典型的，是呀，这个来了，所有的权利，好问题。所以有一个定理说你所需要的只是一个隐藏的层，你可以任意紧密地表示任何函数，所以就近似误差而言，我们可以用一个单独的隐藏层做得有多好是没有限制的，问题是需要指数级的比隐藏层多得多，所以当你有更多的深度。你可能得到的是更有效的函数表示，通过高效，我是说，更少的参数，我是说更少的节点，所以这就是你从深度中获得的，mlp，多层感知器，是呀，那是，我们就谈了这么多，是啊，是啊，所以这些的另一个术语。这种神经网络结构是多层感知器，即使它与感知器无关，是啊，是啊，在这项工作中，我们知道这个很酷，所以说，无人监督的问题，我们不知道，我们不知道，为什么我们可以应用这个，我提问。

我们能否将神经网络应用于无监督学习，所以我们只有x没有is，好的，所以有一个很酷的方法，它被称为自动编码器，自动编码器，喜欢汽车，但是自动编码器，在这里，你所做的是你接受你的输入，你有一个网络。你有一堆层，你有一个薄层叫做瓶颈，然后你的层又变大了，你试着把输入预测为输出，所以你得到一个输入，并通过你的网络提供它，你的损失是，你的输出分数输出分数离你的输入有多远，从您的输入。所以如果你能很好地从输入中重现输出，但你把所有的知识都塞进去了，并通过了一个只有很少节点的瓶颈，这个想法是你做了一些很好的知识压缩，不知何故，网络发现了输入空间的一些良好表示。这是在无人监督的情况下学习的一种方式，还有其他方法可以在无人监督的情况下训练它们，但这和我们以前做的完全不同，是啊，是啊，最后一个问题，因为我们没时间了，教这门课我最喜欢的部分，哦，我喜欢写家庭作业。

否，我最喜欢的部分是你们问问题并参与，有一些互让，这就是有趣的地方，所以在那个音符上，这是一个非常有趣的学期，我想感谢你们是一个很棒的班级。

![](img/f9864a91004ac8c69f8ecdc814883727_31.png)



![](img/f9864a91004ac8c69f8ecdc814883727_32.png)