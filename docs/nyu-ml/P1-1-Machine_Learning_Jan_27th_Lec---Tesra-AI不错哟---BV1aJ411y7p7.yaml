- en: P1：1.Machine_Learning_Jan_27th_Lec - Tesra-AI不错哟 - BV1aJ411y7p7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P1：1.Machine_Learning_Jan_27th_Lec - Tesra-AI不错哟 - BV1aJ411y7p7
- en: Hey guys， welcome to machine learning， DSGA1003。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，大家好，欢迎来到机器学习课程，DSGA1003。
- en: '![](img/97b99a42211704fc3d0686faa377c22a_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_1.png)'
- en: I'm David Rosenberg。 I'll be your instructor this semester。 So we'll start off
    with some slides on logistics， a little bit about the core structure， not。 too
    much。 And then we'll get into some material。 We're going to do some pretty abstract
    stuff and some very concrete stuff。 And maybe that's the tone for the course，
    in fact。 So just out of curiosity by a show of hands。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我是David Rosenberg。这学期将是你们的讲师。我们将首先介绍一些关于后勤的幻灯片，以及核心结构的一些内容，不会太多。然后我们将进入一些教材内容。我们会讨论一些非常抽象的东西，也会涉及一些非常具体的内容。或许这就是本课程的基调，事实上。好奇的话，可以举手看看。
- en: how many people are in the master's program， center for data science？ Okay，
    what is it like？ 50？
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少人是数据科学中心的硕士项目学生？好的，是什么样的情况？50人左右吗？
- en: All right。 And how many people are just here auditing？ Or for a look？ Maybe
    you'll take it。 And on the waiting list？ Okay， not so many。 Okay， good。 So the
    survey is over。 So first。 the class webpage is posted on GitHub。 You should take
    your time to read it。 I'm not going to cover all the information in these slides。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么有多少人是来旁听的？或者是来看看？也许你们会选这门课？在等候名单上吗？好的，不是很多。好的，调查结束了。那么，首先，课程网页已发布在GitHub上。你们应该花时间阅读它。我不会在这些幻灯片中覆盖所有信息。
- en: There's a lot of details for the class that you can find on the website， particularly。
    the syllabus and some important dates， such as the tests。 The semester we're going
    to be using Piazza。 I bet a lot of you guys have already used Piazza。 It's a very
    nice website for posting questions， getting answers from your peers and from the。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 课程的很多细节可以在网站上找到，特别是课程大纲和一些重要的日期，比如考试。这个学期我们将使用Piazza。我敢打赌你们中的许多人已经使用过Piazza了。它是一个非常好的网站，可以用来发布问题，从同学和教师那里得到答案。
- en: instructors， from the graders。 It worked very well last year。 and it's a good
    way to get some help or get some discussion， going pretty quickly。 And so that
    should be your go-to for asking questions。 You'll need to email the graders or
    myself。 just post directly to Piazza and we monitor， that very， very well。 So
    schedule。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从评分员那边得到的反馈。去年效果很好，这是一个很好的方式，可以快速获得帮助或开展讨论。因此，应该把它作为提问的首选方式。你们需要发送邮件给评分员或我，只需直接在Piazza上发帖，我们会非常密切地监控它。接下来是课程安排。
- en: Class times will start， usually things are ready to go， promptly at 7。10 by
    iPhone time。 On Wednesday we go 7。10 to 9。 And Thursday， we call it a Thursday
    hour meeting a lab that's just more for historical reasons。 It's really just an
    extra class。 And so both are required。 So on the lab sessions。 some will be led
    by myself， some will be led by the TA， I'll introduce， in a second。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 课程时间通常会准时在7点10分开始（按iPhone时间）。周三我们上课的时间是7点10分到9点。周四我们称之为“周四时段会议”，这是为了历史原因命名的。实际上它只是一个额外的课程。所以这两个都是必修的。至于实验课，有些会由我主持，有些会由助教主持，我稍后会介绍。
- en: And it'll mostly be lectures， often amplifying what was taught on the Wednesday
    class。 Sometimes it'll be something supplementary， such as a math topic。 Most
    topic will be some mathematical things， differentiation with respect to vectors，
    which。 will be helpful in your calculus。 One of the tests is going to be during
    lab sessions and then we'll all see lab sessions。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主要是讲座，通常会扩展周三课程上讲解的内容。有时会涉及一些补充内容，比如数学主题。大多数主题都会涉及一些数学内容，比如关于向量的微分，这将对你们的微积分学习有帮助。我们的其中一场考试会安排在实验课期间，然后我们会看到实验课的安排。
- en: for meetings with the advisors。 So we have projects in this class and some of
    the advisor meetings happen during lab。 So lab is kind of a mixed bag of types
    of meetings and Wednesday lectures are all standard。 to our lectures。 Okay， so
    the course staff。 So first let me introduce our TA， Levant-Sagoon。 He came highly
    recommended from one of the prerequisite classes， 1002。 Do you want to say a word？
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 用于与导师的会议。所以我们在这门课上有一些项目，而一些导师会议会在实验课期间举行。所以实验课是各种类型会议的混合体，而周三的讲座都是标准的。至于我们的讲座，好的，那么课程工作人员。首先让我介绍一下我们的助教，Levant-Sagoon。他是从先修课程1002中被高度推荐的。你想说几句话吗？
- en: I have a percent in tomorrow。 We have four graders。 You'll see little pictures。
    small blurbs on the website。 And we have some project advisors for the project
    for the class and these are kind of。 real experts in data science work and industry
    and they'll come in and advise usually about。 eight groups each and that's usually
    a very rewarding experience。 So， how is the class graded？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我明天有个评分员。我们有四个评分员，你们会在网站上看到小图片和简短的介绍。我们还为课程的项目安排了一些项目顾问，这些顾问都是数据科学领域的专家，他们通常会为大约每组八个学生提供建议，这通常是一次非常有价值的经历。那么，课程是如何评分的呢？
- en: There's going to be about eight homeworks。 They'll be worth about 40%。 They'll
    be worth 40%。 There will be two tests， a shorter one and a longer one for another
    40% total and then。 the project will be 20%。 And that will be primarily based
    on your final report。 There's a poster session and that adds up to 100。 There's
    also some extra credit opportunities and how this shows up is as optional questions。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大约会有八个作业，它们总共占40%的分数。会有两个考试，一个短的和一个长的，总共占40%，然后项目将占20%。这主要基于你的最终报告。会有一个海报展示环节，所有加起来是100%。也有一些额外的加分机会，表现为可选问题。
- en: on the homework primarily。 So what's kind of unique about this is that the extra
    credit on the homework does not。 help your homework grade。 It's a totally different
    category。 So you definitely to finish the regular problems on the homework first。
    The optional problems are you want to go kind of beyond， cover more material，
    you'll have。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作业上，主要是这样。有什么不同的是，作业上的加分并不会提高你的作业成绩，它是一个完全不同的类别。所以你一定要先完成常规的作业题目。可选问题是想要深入学习更多材料时的选择。
- en: an extra time。 You just want to go really deep into data science。 I want to
    give you all the opportunities to learn as much data science as you can。 So that's
    kind of why there are these optional questions。 Even if you don't do the questions。
    it might be worth reading them because sometimes we。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有额外的时间，想深入钻研数据科学，我希望给你们提供所有机会来学习尽可能多的内容。这也是为什么会有这些可选问题的原因。即使你不做这些问题，阅读它们也可能是值得的，因为有时我们会介绍一些新的主题，这些主题不是必修的，但无论如何可能会引起你的兴趣。
- en: introduce some new topics that aren't required but may be of interest in any
    case。 Also if you're a kind of power contributor to Piazza or class discussions
    that can help。 boost your grade at the end。 But all told the extra credit can
    only increase your final grade by like half a letter grade。 So B plus D minus，
    A minus to A， that sort of thing。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果你在Piazza或者课堂讨论中是一个积极的贡献者，这也有助于提升你的最终成绩。但总的来说，额外的加分最多只能提高你的最终成绩半个等级。例如，B+提高到A-，A-提高到A等等。
- en: Any questions on evaluation or anything else so far？ Okay。 All right。 so the
    first homework assignment will be out tomorrow。 It'll be due in a week。 For submitting
    homework we use NYU classes which I assume you guys are familiar with and if。
    you have any trouble post to Piazza we'll get back to you。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，有关评估或者其他方面的问题吗？好的。那好，第一份作业明天会发布，截止日期是一个星期后。作业提交使用NYU的课堂系统，我假设你们都熟悉。如果遇到任何问题，可以在Piazza上发帖，我们会回复你。
- en: Is there anyone who's registered for the class but is not on Piazza yet or hasn't
    received。 the Piazza invite？ So you should be able to find the website。 You should
    be able to register for this class on Piazza if you have an NYU email address。
    If you don't， just email me directly and we'll work that out。 So here's an important
    point。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有人已经注册了这门课程但还没有加入Piazza或者没有收到Piazza邀请吗？你应该能在网站上找到它。如果你有NYU的邮箱地址，应该能在Piazza上注册这门课。如果没有，直接给我发邮件，我们会解决的。接下来这是一个重要的点。
- en: We want all the homework submitted as PDFs and it'll be clear on the homework
    what needs。 to be included in those PDF files but make sure you know how to compile
    your PDF more。 than five minutes before the homework is due because the late policy
    is pretty simple。 We accept homework late at a 20% penalty。 So let's just try
    to avoid awkward situations like oh I was done but I couldn't put the PDF。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望所有作业都以PDF格式提交，作业上会明确标注需要包含哪些内容。但请确保你在作业截止前至少五分钟知道如何生成PDF文件，因为延迟政策很简单。我们会接受迟交作业，但会扣除20%的分数。所以我们最好避免出现那种“哦，我做完了，但我没法提交PDF”的尴尬情况。
- en: together。 See look it's time stamped and it's just do it in advance so we don't
    have these awkward。 situations。 Collaboration's fine just make sure when you write
    your solution you're doing it kind。 of independently so you can discuss good ideas
    with other people that's fine but make。 sure you write down your solution on your
    own and note who you worked with if it was。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一起做吧。看，时间戳已经标好了，提前完成，这样就不会有这些尴尬的情况。合作是可以的，只是当你写解决方案时，要确保是独立完成的，你可以和别人讨论好的想法，没问题，但确保你自己写下解决方案，并且标明你合作的对象。
- en: a serious collaboration。 So projects aren't for a couple months so I'm not going
    to say too much about them but。 the projects are done in groups of three so you
    can go ahead and start thinking who you。 might want to work with and what you
    might want to do。 It's not really early to start。 Things really pick up around
    spring break。 There'll be a project proposal and meetings with advisors so that's
    kind of in March so。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一次严肃的合作。所以项目不会只做几个月，我不会说太多，但项目是以三人小组进行的，你可以开始考虑一下你可能想与谁合作，以及你可能想做什么。现在开始考虑并不算早。事情会在春假期间真正开始。会有一个项目提案和与导师的会议，大概是在三月左右。
- en: we can circle back to this later in the semester。 Alright so prerequisites for
    the class。 So the designs prerequisites are this course 1001 and 1002。 There's
    some data science there and there's a lot of math there。 If you haven't taken
    those basically the types of things you need to know are multivariate。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在学期后期再回到这个话题。好了，课程的前提要求是这样的：课程 1001 和 1002。这些里面有一些数据科学，也有很多数学内容。如果你没上过这些课，基本上你需要知道的内容包括多变量。
- en: calculus mostly differential calculus like gradients。 Matrix algebra linear
    algebra。 some out of probability theory， some out of statistics。 maybe like a
    semester course in each of those at the undergraduate level would be sufficient。
    And then on the programming side we're all on Python and NumPy。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分主要是微分学，比如梯度。矩阵代数，线性代数。还有一些来自概率论的内容，一些来自统计学的内容。也许每个科目在本科阶段学一个学期就足够了。然后在编程方面，我们都是用
    Python 和 NumPy。
- en: So the support code for the homework comes in NumPy so hopefully that works
    for you guys。 Okay so there's this general idea of learning things for mastery
    versus performance。 And I just want to encourage you guys to keep this in mind
    as you're doing homework or reviewing。 your lectures， lecture notes。 So you should
    ask yourself when you read something。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作业的支持代码是用 NumPy 写的，希望这对你们有帮助。好了，关于掌握学习和表现学习的这个大致概念，我只想鼓励你们在做作业或复习讲座笔记时，牢记这一点。所以，当你读到某个内容时，你应该问自己。
- en: do I really understand this or I just kind， of understand it well enough to
    get this question。 And I want you to try to go that extra step and challenge yourself
    to understand it all， the way。 So my math teacher in college， one of my first
    professor said that almost understanding with。 the enemy of actual understanding。
    And so it's like yeah I pretty much got it。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的是理解了吗，还是我只是稍微理解了一些，足够回答这个问题？我希望你能走得更远，挑战自己真正完全理解。我的大学数学老师之一曾经说过，"差不多理解"是对真正理解的敌人。所以就像是，“我差不多懂了。”
- en: that's a sign that you really don't get it， and you need to keep working until
    you completely got it。 Alright so how many of you guys have heard of L1 and L2
    regularization？ Hence okay。 So about half maybe a little more from 1001？ So why
    do we ever want to use L1 regularization？
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明你其实没完全理解，需要继续努力直到完全理解为止。好了，有多少人听说过 L1 和 L2 正则化？嗯，好像有一半人，可能从课程 1001 来的？那我们为什么要使用
    L1 正则化呢？
- en: What's the answer？ Sparsity， right， that's right。 So when you run linear regression
    with some L1 regularization。 it's called Lasso， you， can get sparse coefficients。
    So if this were an interview question and someone is interviewing you and they're
    smart。 person but just hasn't heard of this L1 regularization， they may say what's
    this L1 regularization。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是什么？稀疏性，对的，没错。所以当你运行带有 L1 正则化的线性回归时，它叫做 Lasso，你可以得到稀疏的系数。所以如果这是一个面试问题，假如面试官是个聪明的人，但没听说过
    L1 正则化，他可能会问，这是什么 L1 正则化。
- en: all about and you're like oh it's good because of sparsity。 And then they may
    say well why do you care about sparsity？ It's a good question。 Any ideas on that？
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 全是关于稀疏性的，哦，它因为稀疏性而好。然后他们可能会问，为什么你在乎稀疏性？这是个好问题。大家有什么想法吗？
- en: Yeah。 It's easier to solve minimally。 It's easier to solve minimally。 So if
    you consider sparse solutions it's easier to。 It's easier to solve minimally。
    Okay。 so you say it's easier to solve minimization problems if you have sparse
    solutions。 Okay and it's easier to solve the minimization problem in like a computational
    sense。 We'll see。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，最小化问题更容易解决。如果考虑稀疏解，它更容易解决。确实，更容易解决最小化问题。好吧，所以你说如果有稀疏解，最小化问题更容易解决。好，我们将在计算的意义上看看，解决最小化问题是不是更简单。
- en: So yeah another idea？ So sparsity reduces the variance of the prediction。 Interesting。
    Maybe some of that。 Yeah please。 Okay， so maybe the underlying model is sparse。
    The thing that actually proves that it is sparse and you want to impose this L1
    regularization。 to recover this sparsity。 Yeah， great， yeah。 Okay， some applications
    need a sparse solution。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，其他的想法呢？稀疏性减少了预测的方差。很有意思，可能是其中的一部分。是的，请讲。好的，可能底层的模型是稀疏的。实际上证明它是稀疏的，并且你想强加L1正则化来恢复这种稀疏性。很好，是的。好的，某些应用需要稀疏解。
- en: So just compress that sensing。 Okay， great。 So what if then the person says
    well I don't have any of these constraints。 Should I be interested in L1 regularization？
    I don't know what the model was。 I don't care。 I just want it to perform well。
    So does L1 do better than L2 in performance？ All right。 we'll investigate that。
    The next question would be can you explain how this L1 regularization gives sparse
    solutions？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以就是压缩感知。好的，太好了。那么如果有人说，我没有这些约束条件。我应该关注L1正则化吗？我不关心模型是什么。我不在乎，只想要它的表现好。那么L1比L2在性能上更好吗？好的，我们会研究这个问题。下一个问题是，你能解释一下L1正则化是如何给出稀疏解的吗？
- en: Did you guys recognize this picture？ It's a standard picture。 So I was looking
    at this picture last night and I wanted to encourage you guys to all。 know how
    to interpret this picture and every piece of it。 So there's a circle what does
    that correspond to？ If there's a line what does that correspond to？
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你们认出这张图了吗？这是一张标准的图。我昨晚在看这张图时，想鼓励你们每个人都知道如何解读这张图以及其中的每一部分。这里有一个圆圈，它对应的是什么？如果有一条线，它对应的又是什么？
- en: And then I realized I didn't actually understand this picture because when I
    think of this picture。 those lines should be ellipses and I realized well this
    is this k from a quora and so I don't。 even know that this picture explains anything。
    So when we cover L1 regularization next week I want to come back to this and we'll
    discuss。 whether this picture makes any sense。 All right， so course topics。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我意识到我实际上并不理解这张图，因为当我看到这张图时，这些线应该是椭圆形的，我意识到这其实是来自一个问题，我甚至不知道这张图是否解释了什么。因此，当我们下周讲解L1正则化时，我想再回到这张图，我们将讨论这张图是否有意义。好吧，接下来是课程内容。
- en: We're going to start with kind of the bulk of the course that's going to cover
    kind of。 standard machine learning frequentist approaches。 We're going to start
    with empirical risk minimization today。 We're going to talk about support vector
    machines in a couple weeks。 We're going to do kernels and kernel methods， ensemble
    methods like boosting， random for， us。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从课程的主要部分开始，涵盖标准的机器学习频率主义方法。今天我们将开始讨论经验风险最小化。几周后我们将讲解支持向量机。我们还会讲解核方法、集成方法（如提升法）和随机森林等。
- en: These are really state of the art methods。 Neural networks could fall into this
    category。 Then we're going to move on to probabilistic models。 So in this setting
    we could for instance not just predict a， if we're doing regression。 you typically
    predict a number。 So in a probabilistic setting you could go beyond that。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是非常先进的方法。神经网络可以归类为这一类。然后我们将进入概率模型的部分。在这种情况下，我们可以做的不仅仅是预测一个数字，在回归中，通常你预测一个数值。而在概率设置中，你可以超越这个。
- en: You could predict an entire probability distribution instead of just a number。
    Gause your mixture models as a method for being clustering。 The EM algorithm is
    a way to kind of learn models when you have missing data。 We're going to cover
    Bayesian approaches to machine learning and statistics which can。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以预测整个概率分布，而不仅仅是一个数字。高斯混合模型可以作为聚类的一种方法。EM算法是当你有缺失数据时学习模型的一种方式。我们将讨论贝叶斯方法在机器学习和统计中的应用。
- en: be very nice to use and practice if you have some notion about what your model
    should look。 like before you begin。 And then finally some miscellaneous topics
    which we'll choose from perhaps dimensionality。 reduction or collaborative filtering。
    This could enter him。 All right。 Let's have some brief overview。 You guys have
    any questions for me before we start with the machine learning itself？
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对你的模型应该是什么样子有一些概念，在开始之前会非常有帮助，这样你可以进行实践。最后我们会选择一些杂项话题，也许是降维或协同过滤。这可能会进入他的讨论。好吧，让我们简单概述一下。在我们开始机器学习之前，你们有任何问题吗？
- en: Okay。 Okay。 So we're going to talk about statistical learning theory today。
    This is kind of the core theoretical foundation for machine learning。 Oops。 All
    right。 So in data science what types of problems are we solving？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，好吧。今天我们要讨论统计学习理论。这是机器学习的核心理论基础。哦，好的。那么在数据科学中，我们解决的是什么类型的问题？
- en: So many types of problems come into data science but the consistent core we
    usually have。 to make a decision， take an action or produce some kind of output。
    That's pretty consistent across data science problems。 And in addition to taking
    some kind of action or producing something we have a formal way。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中有许多类型的问题，但我们通常有一个一致的核心，就是做出决定、采取行动或产生某种输出。这在数据科学问题中是非常一致的。此外，除了采取某种行动或产生某种结果，我们还有一种正式的方式。
- en: to evaluate what we've produced and that's crucial in machine learning and evaluation，
    criteria。 So let's start with the action part。 We can define an action as whatever
    is produced by our system。 So typical examples。 We can have a classification problem。
    In that case we produce a zero-one classification。 That can be called an action。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 评估我们所产生的结果，这在机器学习和评估中至关重要，标准。所以让我们从行动部分开始。我们可以将行动定义为系统产生的任何结果。典型的例子，我们可以有一个分类问题。在这种情况下，我们产生一个零一分类。那可以被称为一个行动。
- en: Classical statistics we have hypothesis testing and the action is to either
    reject the null。 hypothesis or fail to reject the null hypothesis。 Two possible
    actions we could take。 How about speech recognition？ We get an audio signal and
    the action we produce， well。 our action is to produce some written， text that
    corresponds to the audio signal。 It's the action。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 经典统计学中，我们有假设检验，行动是要么拒绝零假设，要么未能拒绝零假设。我们可以采取的两种可能行动。语音识别怎么样？我们得到一个音频信号，而我们产生的行动是，嗯，我们的行动是产生一些与音频信号相对应的文字。就是这个行动。
- en: How about computer vision？ Question。 Does this picture have an animal in it
    yesterday？
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉怎么样？问题是，这张图片昨天有动物吗？
- en: We can produce a probability。 We could say probability point eight that this
    picture contains an animal。 That would be could be an action。 What else？ Suppose
    we're doing storm tracking and we have some data up to the current time period
    and。 we want to predict where the storm will be in three hours。 What might be
    an action we would want to produce to express where the storm will be？ Coordinates。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以产生一个概率。我们可以说有0.8的概率这张图片包含一个动物。那可能是一个行动。还有什么呢？假设我们在做风暴追踪，并且我们有一些当前时间段的数据，我们想预测风暴将在三小时后到达哪里。我们可能想产生什么样的行动来表达风暴将到达的地方？坐标。
- en: It's like latitude， longitude， coordinates on the earth for where the storm
    will be。 Great。 What if we wanted to fix this to one point？ I'm the storm and
    I'm moving around and all the winds and I could end up here in three。 hours or
    maybe the wind can shift and I'll end up over here。 Would there be a way to express
    those two possibilities？ A vector field。 A vector field。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 就像是地球上风暴的纬度、经度坐标。太好了。如果我们想将其固定到一个点怎么办？我是风暴，我在四处移动，所有的风和我可能在三小时后到达这里，或者风可能改变方向，我可能会到达这里。有没有办法表达这两种可能性？向量场。向量场。
- en: A vector field indicates where things would travel from each location and I
    want to know。 where it's going to end up in three hours。 A density。 Great。 So
    you produce a density function on the earth and that will express like a probability。
    distribution on where you think that the storm will be。 Sounds good。 Good。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 向量场表示从每个位置物体将要移动到哪里，我想知道。它将在三小时后到达哪里。一个密度。太好了。所以你在地球上产生一个密度函数，这将像一个概率分布，表示你认为风暴会在哪里。听起来不错。好。
- en: For storm tracking you can predict a probability distribution on the earth。
    Unusual support for distribution but that makes total sense。 How about automated
    driving？
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于风暴追踪，你可以预测地球上的概率分布。虽然这是一种不寻常的分布支持，但它完全有意义。自动驾驶怎么样？
- en: Will it be action for an automatic driving problem？ Yeah。 Apply the break。 Yes。
    Depressed the accelerator by one inch。 Yes。 Steer right 45 degrees。 Yes。 These
    would all be actions in an automatic driving problem。 Great。 I want you guys to
    think very broadly about what the action space could be。 Kind of profound。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶问题会是一个动作吗？是的。踩刹车。是的。加速踏板压了一英寸。是的。右转45度。是的。这些都会是自动驾驶问题中的动作。很好。我希望你们能非常广泛地思考一下动作空间可能是什么样的。非常深刻。
- en: We're not really going to cover such a broad range but it all fits into this
    framework。 of decision theory。 That's why I mention it。 So what is decision theory？
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会覆盖这么广泛的内容，但所有这些都符合决策理论的框架。这就是我提到它的原因。那么什么是决策理论呢？
- en: And we're talking about decision theory first and we restricted the statistical
    learning theory。 in a bit。 So decision theory is all about finding the optimal
    action under various definitions of。 optimality。 Pretty generic。 So it of course
    depends on your evaluation criteria。 So let's talk a little bit about the other
    piece of data science which is the evaluation。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论的是决策理论，然后稍微限制一下统计学习理论。所以决策理论就是在各种优化标准下，找到最优的动作。相当通用。当然，这取决于你的评估标准。所以让我们稍微谈一下数据科学的另一个部分，那就是评估。
- en: So if we're in classification evaluation can be quite clear。 Is the classification
    correct or not？
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们在做分类问题，评估可能是非常明确的。分类正确与否？
- en: Done。 That's the evaluation。 You get a point for correct and zero for not or
    something like that。 For speech recognition does the text transcription of what
    was uttered exactly match what was。 actually said according to a human listening
    or something。 So that could also be binary。 Yes you succeeded。 No you didn't。
    All right but should we give some partial credit to this problem？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。这就是评估。正确的得一分，错误的得零分，或者类似的东西。对于语音识别来说，文本转录的内容是否与实际说出的话完全匹配，是否符合人类听力的判断，或者类似的标准。所以这也可以是二元的。是的，你成功了。不是，你没有成功。那么我们是否应该给这个问题一些部分的评分呢？
- en: Are there degrees of errors on audio transcription？ Probably so。 And so you
    might want to work somewhat on your evaluation criteria for speech transcription。
    It's not necessarily so straightforward how to score a text transcribed that doesn't。
    exactly match but there are various ways one can do that。 How about a probability？
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 音频转录会有误差的程度吗？可能会。所以你可能需要在语音转录的评估标准上做一些工作。要评分一个没有完全匹配的转录文本并不一定那么直接，但有很多方法可以做到。概率呢？
- en: So we predicted that there's an animal in the picture with probability 80%。
    How do we evaluate how good that prediction is？ Let's take a specific example。
    So when it shows you a picture it has an animal in it。 I predict 80% that it has
    an animal。 Well it does have an animal so 80% is kind of more right than wrong
    but is there a way to。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们预测图像中有一只动物，概率为80%。我们怎么评估这个预测的准确性呢？让我们来看一个具体的例子。当它展示给你一张图片，里面有一只动物。我预测80%的可能性它有一只动物。好吧，图片里确实有一只动物，所以80%的预测比错的要更准确，但有没有什么方法可以。
- en: assess that 80%。 So one interpretation is that 80% when I say 80% I want to
    be correct that there's an animal。 in the picture 80% of the time。 That would
    be what's called a well calibrated probability。 So there are various ways to assess
    that。 Okay。 So in a kind of a real life business setting there's some kind of
    problem that's presented。 to you and the first step to formalizing it would be
    to define an action space。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 评估80%。所以一个解释是，当我说80%时，我想表达的是，80%的时间我想正确地预测图像中有一只动物。这就是所谓的良好校准的概率。评估这个有很多方法。好的。在一个现实生活中的商业场景中，通常会有某种问题呈现给你，第一步就是将问题形式化，定义一个动作空间。
- en: What is it that you're going to produce for action？ And the other piece is the
    evaluation criterion。 And what you'll find in practice is a lot of times one of
    the most interesting parts。 of the problem is to make this， is to do this formalization。
    And a lot of times it's iterative right？ So say we're doing the storm prediction
    problem and our first pass of the problem is to predict。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你将采取什么样的行动？另外一个部分是评估标准。而你会发现，实际上很多时候问题中最有趣的部分是做这个形式化的工作。很多时候它是一个迭代过程，对吧？比如说我们在做风暴预测问题，我们的第一次尝试是去预测。
- en: a single point， a lot of too longitude。 Maybe we're predicting the one place
    we think the storm is most likely to be if forced to。 predict one place。 And then
    we work with that for a while and we're like no this just doesn't capture the。
    information we have that sometimes it's really a coin flip and it's either going
    to end up。 over there or it's going to end up over there。 Two different places
    separated and maybe with 50/50。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单一的点，可能有很多经度。也许我们预测的是我们认为风暴最有可能出现的地方，如果非得预测一个地方的话。然后我们在一段时间内进行处理，结果我们发现，这并没有捕捉到我们所拥有的信息，有时它就像是一个硬币抛掷，要么会出现在那边，要么会出现在那边。两个不同的地方，彼此分开，可能各占50/50的概率。
- en: So if we just produce a point for an action that doesn't capture the essence
    of the problem。 we want to be able to produce something that's more structured
    for perhaps a probability， density。 Similarly on the evaluation side， maybe if
    we predict the point the evaluation could， be clear。 Where was the storm actually？
    How far away is it from the point we predicted？
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们只是为一个行动生成一个点，这并不能捕捉到问题的本质。我们希望能够生成一些更具结构性的内容，可能是一个概率密度。类似地，在评估方面，如果我们预测了一个点，评估可能是清晰的。风暴实际上在哪里？它距离我们预测的点有多远？
- en: That would be a very simple evaluation。 But then when we go to a probability
    distribution now it's not so obvious how you evaluate it。 We have a probability
    density on the earth。 The storm actually shows up in a particular place。 How do
    we say how good or bad our prediction was？ So evaluation can be an interesting
    problem。 So the two most important parts are action space and evaluation。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个非常简单的评估。但当我们进入概率分布时，评估变得不那么明显。我们在地球上有一个概率密度。风暴最终出现在某个特定的地方。我们怎么说我们的预测有多好或多坏呢？因此，评估可能是一个有趣的问题。所以最重要的两个部分是行动空间和评估。
- en: That's really what you need to get started。 Most of the problems we deal with
    have two more pieces。 First is an input。 So they're called inputs and machine
    learning often in statistics。 They go by the name Colverius。 In other settings
    they're called side information。 We've already given some examples of inputs。
    Pictures that like a historical location of a storm。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上就是你开始所需的。我们处理的大多数问题都有两个其他要素。第一个是输入。所以它们通常被称为输入，在统计学中的机器学习中也常这样称呼。它们被称为Colverius。在其他环境中，它们被称为侧面信息。我们已经给出了一些输入的例子。例如，一张记录风暴历史位置的图片。
- en: a search query could be an input。 So all inputs。 These are also very common。
    And finally we often have an output or an outcome in the picture as well。 So in
    the example of does this picture have an animal yes or no， the output is what
    we。 call the output is whether there is an animal in that picture actually。 So
    like the truth。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索查询可能是一个输入。所以所有输入也是非常常见的。最后，我们通常在这个图像中也会有一个输出或结果。所以在“这张图片里有动物吗”是与否的例子中，输出就是我们所谓的输出，实际上是图片中是否有动物。所以就像是事实一样。
- en: If it's the storms location the output will be where the storm actually ended
    up。 In some sense the output is it's kind of like what we wish we could produce
    as an action。 but at least in certain situations。 In a search situation if you
    presented some URLs in a search what the person actually。 clicked that would be
    the output or the outcome。 Depending on the scenario sometimes outcomes sounds
    more appropriate。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是风暴的位置，输出将是风暴最终实际发生的位置。在某种意义上，输出就像是我们希望能够产生的行动。但至少在某些情况下是这样。在搜索场景中，如果你展示了一些URL，用户实际点击的内容将是输出或结果。根据场景的不同，有时“结果”这个词更合适。
- en: Sometimes output they are mathematically we're talking about the same thing。
    So here's a typical sequence of events in a machine learning or data science scenario。
    So we first observe an input X。 You could say sometimes people say things like
    nature gives you X。 The world shows you X。 We take X we think about it and then
    we take an action A。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有时输出在数学上我们是在讨论同一个概念。所以这里是机器学习或数据科学场景中的一个典型事件序列。所以我们首先观察输入X。你可以说，有时人们会说大自然给你X，世界展示给你X。我们接受X，思考它，然后采取行动A。
- en: World gives us X we respond with， A and action。 Then the world says okay here's
    the outcome Y。 So there's input X we take an action A then there's an outcome
    Y。 Then we have to assess how well was our action。 How good was our action in
    relation to the actual outcome Y。 This is formalized it's something called a loss
    function which I've written here L A Y。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 世界给我们X，我们回应A并采取行动。然后世界说好，这里是结果Y。所以有输入X，我们采取行动A，然后有结果Y。接下来我们需要评估我们的行动有多好。我们的行动与实际结果Y相比有多好。这是形式化的，它被称为损失函数，我在这里写的是L
    A Y。
- en: where A would represent the action Y would represent the actual outcome and
    then through。 some magic of our evaluation criterion we assess how good that action
    was。 So here's an interesting thing to think about。 So the outcome Y is often
    independent of the action we take A。 Where's the storm going to go？ I make a prediction
    that's my action。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中A代表行动，Y代表实际结果，然后通过我们评估标准的某些神奇方法，我们评估该行动有多好。所以这是一个有趣的思考点。结果Y通常是独立于我们采取的行动A的。风暴将去哪里？我做出预测，这就是我的行动。
- en: The storm doesn't care where I predicted the storm is going to go where it goes
    regardless。 So in that scenario the outcome and the action are independent。 But
    when is this not the case？
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 风暴并不在乎我预测风暴会去哪里，它会无论如何地去往它自己想去的地方。所以在这种情况下，结果和行动是独立的。那么什么时候不是这样呢？
- en: It's some of our examples。 We've had a few right？ Automatic driving。 No when
    you press the brake the outcome is that the car slows down the state of the car，
    change。 Do you have another idea？ Chest。 Chest gain。 Chest gain。 Chest gain。 Chest
    gain。 Sure。 If the outcome is the responding move then yes your action will affect
    the responding move。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的几个例子。我们有几个对吧？自动驾驶。没有，当你踩下刹车时，结果是汽车减速，汽车的状态发生变化。你还有其他想法吗？胸部。胸部增益。胸部增益。胸部增益。胸部增益。好吧。如果结果是响应动作，那么是的，你的行动会影响响应动作。
- en: Another idea？ Classification would be so a picture is presented。 I say know
    that there's no animal in the picture and they may be animals either there。 or
    it's not regardless of what I say。 So independent。 Alright。 Alright so let's start
    to formalize these ideas。 So we capture the space of input that we can receive。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个想法？分类问题，假设展示了一张图片。我说知道图片里没有动物，而它可能有动物，无论我说什么，它的内容都不受影响。所以是独立的。好了，接下来我们开始形式化这些想法。我们将捕捉输入的空间，可能接收到的输入。
- en: The space of actions that we can take and the space of outcomes that can occur
    and we。 write this script x script a and script y。 So let's do some concept check。
    Also to see what you guys remember from introductory data science。 So linear regression。
    What's the input space？ Features。 Great。 We might represent them as a D dimensional
    vector of reals。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能采取的行动空间，以及可能发生的结果空间，然后我们写下这个脚本x、脚本a和脚本y。那么让我们做一些概念检查。还可以看看你们对初级数据科学的记忆。线性回归。输入空间是什么？特征。很好。我们可能将它们表示为一个D维实数向量。
- en: Okay that would be an input space。 R D could be an input space。 And the output
    space？
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那将是一个输入空间。R D可以是输入空间。输出空间呢？
- en: The action space。 So again。 Walk the bodies。 Into your categories。 For linear
    regression。 In linear regression we take some input features and we produce a
    number。 So the action space would be。 A variety。 So again。 A variety。 Oh that's
    interesting。 You're saying we produce parameters of a model。 That's actually the
    outcome of a learning algorithm。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 行动空间。所以再次强调。把这些分到类别里。对于线性回归来说，在线性回归中，我们取一些输入特征，并生成一个数值。所以行动空间将是。一个多样性。哦，这很有意思。你说我们生成的是模型的参数。那实际上是学习算法的结果。
- en: So we'll come back to that。 The action that's actually taken is to predict a
    number。 So regression would be the input of someone's height and I predict someone's
    weight。 So input is some real vector R D vector reals and we'll produce a real
    number。 That's classic regression。 What regression means is that we predict a
    number。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后再讨论这个问题。实际上采取的行动是预测一个数字。所以回归就是输入某人的身高，我预测某人的体重。所以输入是一些实数向量R D的实数向量，我们将生成一个实数。那就是经典回归。回归的意思就是我们预测一个数字。
- en: By definition when we talk about regression we really mean producing something
    from real。 So action space is the real。 And once the outcome space or output space
    in regression。 Also real。 Great。 Alright。 Next concept check。 Logistic regression。
    Input space。 You can call it out。 Same input space。 Yes。 Vector real。 Great。 Action
    space。 What？ Again？ Binary。 Binary。 Could be。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 按定义，当我们谈论回归时，我们真正指的是从实数中生成某些东西。所以行动空间是实数。而回归中的结果空间或输出空间，也是实数。很好。接下来，概念检查。逻辑回归。输入空间。你们可以说出来。一样的输入空间。是的。向量实数。很好。行动空间。是什么？再来一次？二元。二元。可能是。
- en: We can certainly produce a binary。 Zero one。 Probability。 Probability。 Yeah。
    I think the more native output of logistic regression is a probability。 Well probability
    is something it's already interpreted。 What is it as a value？
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以生成一个二进制的。零一。概率。概率。是的。我认为逻辑回归的更常见输出是概率。好吧，概率是已经被解释过的东西。它作为一个值是什么？
- en: What's its type in computer science？ This were like a computer program。 Say
    that？ It's a real。 It's a real and it's bounded between zero and one。 So I would
    say that the action space for logistic regression is the interval zero one。 And
    yes we like to interpret it as a probability。 But maybe we messed up the training
    method and it's not really a good probability at all。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它在计算机科学中的类型是什么？就像计算机程序一样？说什么？它是实数。它是实数，并且被限定在零和一之间。所以我会说，逻辑回归的动作空间是区间零到一。是的，我们喜欢把它解释为一个概率。但也许我们搞砸了训练方法，它实际上并不是一个真正好的概率。
- en: but at least it's the number between zero and one。 The outcome space for logistic
    regression。 Okay。 The actual value of zero or one output。 Yes。 Yes。 The actual
    zero。 That's right。 Right。 So the output space for logistic regression is zero
    one。 Logistic regression is about classification despite its name。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但至少它是一个介于零和一之间的数字。逻辑回归的输出空间。好的。零或一的实际值。是的。是的。实际的零。没错。对。所以逻辑回归的输出空间是零和一。尽管名字是逻辑回归，但它实际上是用于分类的。
- en: And so the outcome space is a binary class。 So a two， an L， a set a class with
    two elements in it。 So it could be zero one plus minus。 It has two elements。 Great。
    So there's already an elementary example where the action space and the output
    space are。 different a little bit。 Support vector machines。 What do you guys remember？
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以结果空间是一个二分类类。一个二元 L，包含两个元素的类别集。所以它可以是 0、1、正负。它有两个元素。太好了。所以已经有一个初步的例子，展示了动作空间和输出空间的不同。支持向量机。你们记得什么？
- en: Have you seen support vector machines？ Okay。 So we have an SVM。 Same thing。
    Great。 Output space。 Okay。 Okay。 So let me ask you to get the forgot what I asked。
    So we have an SVM。 What's the actual space of an SVM？ What do we produce？ Okay。
    So we either produce a class。 a hard classification， you know， one or zero， or
    we predict a score。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你见过支持向量机吗？好吧。我们有一个 SVM。是一样的。太好了。输出空间。好吧。好吧。那让我问一下，忘了我问了什么。我们有一个 SVM。SVM 的实际空间是什么？我们产生什么？好吧。所以我们要么产生一个类别，一个硬分类，知道吗，0
    或 1，要么我们预测一个得分。
- en: where a positive number is kind of plus and a negative number is the other class
    negative。 and the size of the score indicates how confident we are。 That's the
    interpretation。 So the action space for support vector machine is again all reels。
    And the outcome space is classification。 So it's again the two class set。 All
    right。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，正数代表正类，负数代表负类。得分的大小表示我们有多自信。这就是它的解释。所以支持向量机的动作空间再次是所有实数。而结果空间是分类。所以它仍然是一个二分类集。好了。
- en: Any questions on this？ Okay。 All right。 So we have our three spaces and now
    it's formalized evaluation and the prediction function。 So it has something called
    a decision function。 A decision function is what formally gets the input X and
    returns the action A。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么问题吗？好的。好了。所以我们有了我们的三个空间，现在它们被正式化评估和预测函数。它有一个叫做决策函数的东西。决策函数就是正式地接受输入 X 并返回动作
    A。
- en: So this is what crystallizes and encapsulates our strategy for producing an
    action given。 in an input。 All right。 It's a function that maps the input space
    X to the action space A。 I hope you guys have seen this notation。 So this arrow
    is out the line on the left side。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们的策略的结晶和总结，用于根据输入生成一个动作。好了。它是一个将输入空间 X 映射到动作空间 A 的函数。我希望你们已经见过这种符号。所以这个箭头指向的是左边的线。
- en: '![](img/97b99a42211704fc3d0686faa377c22a_3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_3.png)'
- en: Whoa。 Okay。 So this indicates we're mapping from capital X space to A space。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 哇。好的。所以这表示我们从大 X 空间映射到 A 空间。
- en: '![](img/97b99a42211704fc3d0686faa377c22a_5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_5.png)'
- en: And this is a way to indicate that any individual element X is mapped to F of
    X。 All right。 So that's our decision function。 And then the other major piece
    is the loss function。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种表示任何个体元素 X 被映射到 F(X) 的方式。好了。这就是我们的决策函数。然后，另一个主要部分是损失函数。
- en: '![](img/97b99a42211704fc3d0686faa377c22a_7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_7.png)'
- en: Whoops。 Okay。 So we're。 So loss function is what takes an action A and the outcome
    Y and returns a real number。 assessing how well we did。 So a good loss is a small
    loss。 A loss is a bad thing。 We don't want to lose。 So a good loss is a small
    loss。 A bad loss is a big loss。 Often loss functions are non-negative。 So when
    you really nail it， you got the exact right action。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。好的。所以我们。损失函数是接受一个动作 A 和一个结果 Y，并返回一个实数，评估我们做得如何。所以一个好的损失是小损失。损失是坏事。我们不想失败。所以一个好的损失是小损失。一个坏的损失是大损失。通常损失函数是非负的。所以当你真的做对了，得到了完全正确的动作。
- en: Loss would typically be zero。 And then the more wrong you are， the larger loss
    you get。 I didn't restrict the loss to be non-negative here because sometimes
    you might want to deal。 with losses that can be negative。 That's fine。 But typically
    losses are zero or positive。 Okay。 All right。 So the back's a real life。 You have
    a job。 You're a data scientist。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 损失通常会是零。然后，越错越多，你得到的损失就越大。我没有将损失限制为非负数，因为有时你可能需要处理可能为负的损失。这是可以的。但通常损失是零或正的。好吧，没问题。那么，假设这是现实生活中的情况，你有一份工作，你是数据科学家。
- en: Someone presents you a problem。 First thing you do is you work out the action
    space and the evaluation you're doing。 Great。 So， but someone has to present you
    the problem。 And this is， could often be a。 they call it stakeholder sometimes，
    business owners。 So it's often a business person who has a problem they want you
    to solve。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有人向你提出一个问题。你做的第一件事是弄清楚行动空间和你正在做的评估。太好了。那么，有人必须先向你提出问题。这通常是……他们有时叫做利益相关者，或者商业负责人。所以这通常是一个有问题的业务人员，想让你帮忙解决问题。
- en: And they have a rough idea of the problem they want you to solve。 And usually
    they'll know something about the action space。 They'll know what they want you
    to produce for a given input。 That's the action。 They have a rough idea often
    on how to evaluate， but it's usually not formally formulated very。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 他们大致知道他们希望你解决什么问题。通常他们会知道一些关于行动空间的事情。对于给定的输入，他们知道希望你输出什么。那就是行动。他们通常也有一个大概的评估方法，但往往没有正式的表述。
- en: precisely。 Often the data scientist kind of has to figure out for his or herself
    what's a good way to。 evaluate。 Sometimes they have it， but often not。 But the
    one thing that they definitely want you to produce is this decision function。
    The decision function is what will map the input to the action。 So why do they
    want the decision function？ Why not just send you， you know。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说。通常数据科学家得自己弄明白什么是一个好的评估方法。有时他们已经知道，但往往没有。但他们绝对希望你能输出的是这个决策函数。决策函数就是将输入映射到行动的函数。那么，为什么他们希望你提供决策函数呢？为什么不直接给你发送……
- en: why not just have folks send you the inputs in this data， scientist send back
    the actions to take？
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不让大家直接给你发送数据输入，然后科学家再把该采取的行动返回给你呢？
- en: It's because it's not very scalable。 But usually they want you to explain how
    this decision function works。 And then they want you to give it to their engineers，
    their coders。 And they'll program up this decision function and deploy it so it
    can run at high speed。 up in the cloud or whatever it is。 This isn't always the
    case。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为它的可扩展性不好。但通常他们希望你能解释这个决策函数是如何工作的。然后，他们希望你把它交给工程师，交给编码人员。他们会编写这个决策函数并部署它，以便它可以在云端或其他地方以高速度运行。当然，并非总是这样。
- en: A lot of places have data scientists are excellent coders and they write their
    own code。 But some places there's a split between the data scientist who comes
    up with the decision。 function and the engineers who actually deploy it， write
    it and make it production quality。 It just depends on the place where you are。
    So where have we gotten to？
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 很多地方的数据科学家都是优秀的编码员，能编写自己的代码。但有些地方，数据科学家负责制定决策函数，而工程师则负责实际部署它、编写代码并确保其生产质量。这取决于你所在的地方。那么我们现在进展到什么阶段了？
- en: We've set up the structure and we've talked about how to evaluate a single action。
    We have a loss function。 There's an input。 I take an action。 There's an output
    which can evaluate how good it was at the loss function。 That's nice。 So how do
    we holistically evaluate a decision function？
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设定好了结构，并讨论了如何评估单一行动。我们有一个损失函数。有一个输入。我采取一个行动。有一个输出，可以用来评估这个行动在损失函数上的表现。这很好。那么，如何从整体上评估一个决策函数呢？
- en: Because that's what's actually running in our system。 So we have a system that
    makes predictions。 It's handling a million requests a minute。 How do we know how
    well it's doing？ How do we assess it？
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这才是我们系统中实际运行的内容。所以我们有一个做预测的系统，它每分钟处理上百万个请求。我们怎么知道它表现得如何？我们如何评估它呢？
- en: Kind of as a whole。 And this brings us into the statistical learning theory
    framework。 So we're going to make a little bit one more assumption， simplifying
    assumption。 And this brings back what we were talking about earlier about whether
    the action can。 have an effect on the outcome。 And I want for now we'll restrict
    the situation that the action we take has no effect on the。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上是一个整体。这把我们引入了统计学习理论框架。所以我们将做一个简化的假设。这样就回到了我们之前讨论的，行动是否能对结果产生影响的问题。现在为了简化，我们将限制当前情况，假设我们所采取的行动对结果没有影响。
- en: outcome。 And this includes all the traditional machine learning problems， classification。
    prediction， ranking。 How about stock market prediction？
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。这包括了所有传统的机器学习问题，如分类、预测、排序。股票市场预测呢？
- en: Does this scenario include stock market prediction？ See some no。 I see some
    hands。 Yes？ I mean。 your actual prediction shouldn't affect the outcome at all，
    but your action on。 that prediction very well might affect what the rest of the
    market does。 All right。 If you say I'm going to invest in Bank of America， like
    the market is going to listen。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个情境包括股票市场预测吗？看到一些人举手了。我看到一些人举手了，是吗？我的意思是，你的实际预测应该不会影响结果，但你对这个预测的行动很可能会影响市场的其他部分。好吧。如果你说我要投资美国银行，市场很可能会听从。
- en: to you and probably follow those two。 Great。 So the answer was the actual prediction
    you make about a stock market probably has no effect。 on the stock market at all。
    Why would it？ The stock market doesn't know what you predicted。 No one else does
    if you don't tell them。 But if you take some action on that prediction like maybe
    you invest or maybe you announce。 your prediction， that could have an effect on
    the stock market。 Okay。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 给你们，可能是跟着这些问题走的。太好了。所以答案是，你对股票市场做出的实际预测，可能完全不会对股票市场产生任何影响。为什么会呢？股票市场根本不知道你预测了什么。除非你告诉别人，否则没有人知道。但是，如果你基于这个预测采取行动，比如你投资或者宣布你的预测，那就可能会对股票市场产生影响。好吧。
- en: So maybe a subtle distinction there。 All right。 So you may feel bad。 Like， all
    right。 what about all the fancier problems that like automatic driving where。
    your action does affect the outcome？ Like are those off limits？
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里可能有一个微妙的区别。好吧。那么你可能会觉得不太好。比如说，自动驾驶那类更复杂的问题怎么办？在这种情况下，你的行动确实会影响结果？那些是不是不能考虑的情况？
- en: And not really because when you read that literature， a lot of what they're
    doing is。 reducing this more complicated problem where your actions affect the
    output and then reduce。 it to a problem where it does not。 So if you want to check
    that out。 that's like the contextual bandit literature。 Just as a quick idea rather
    than your action being the actual action you take。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是完全没有关系，因为当你阅读这方面的文献时，很多他们做的是将这种更复杂的问题——即你的行动会影响输出——简化为一个问题，在这个问题中，行动与输出没有关系。所以如果你想了解这个内容，可以查阅“上下文赌博者”文献。作为一个快速的理解，与其说你的行动是你实际采取的行动，不如说它是...
- en: you may， for instance， try to predict what the outcome will be if you take a
    particular action。 So this is kind of a meta prediction problem where you say
    if I take a particular action。 what do I think the outcome will be？ And your training
    data would be historical situations where you took that action and you。 observed
    what the outcome actually was。 Now it's somehow trying to incorporate the responses
    of other people and that sort of。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以，比如说，尝试预测如果你采取某个特定的行动，结果会是什么。这是一种元预测问题，在这种问题中，你会说，如果我采取某个特定的行动，我认为结果会是什么？而你的训练数据将是你在历史情境中采取该行动并观察实际结果的数据。现在，问题就变成了如何将其他人的反应等因素纳入考虑。
- en: thing。 Okay。 But by and large in this class， we are not going to worry about
    this situation。 All right。 So the setup for statistical learning theory。 we assume
    there's a data generating distribution。 We write Pxy。 All right。 All the input
    output pairs that we observe， we assume are coming from this distribution。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。但是总的来说，在本课程中，我们不会过多担心这种情况。好吧。那么统计学习理论的设定是怎样的呢？我们假设有一个数据生成分布，我们用Pxy表示。好吧，我们观察到的所有输入输出对，都假设来自这个分布。
- en: So what do we want？ We want to find a decision function that given an x from
    a pair generated from this distribution。 produces an action f of x that does well
    in some sense with respect to the outcome y。 So in other words， we want a decision
    function f for which the loss of the f of x of the， action。 the loss of our action
    taken on x with respect to the output y is small in general， in some sense。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们想要什么呢？我们想找到一个决策函数，该函数给定从这个分布中生成的一个x对，能够产生一个x的动作f，且该动作在某种意义上对于结果y来说表现良好。换句话说，我们想要一个决策函数f，使得我们对x采取的行动，即f(x)的损失相对于输出y在一般情况下是小的，某种意义上来说。
- en: So that's the general notion。 We want to have small loss in some typical sense。
    So now we need to formalize that idea。 So the formalization， we've kind of set
    it up for this。 is it's called the risk。 The risk of a decision function is the
    expected loss of that decision function where the expectation。 is over randomly
    chosen pair of inputs and outputs。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一般的概念。我们希望在某种典型意义上具有小的损失。所以现在我们需要形式化这个想法。我们为此已经设定了框架，它被称为风险。决策函数的风险是该决策函数的预期损失，其中期望是基于随机选择的输入和输出对。
- en: So that's where we use our assumption that we have a data generating distribution，
    which。 we don't know。 And if we were to choose a random pair of inputs and outputs。
    so a random picture and， whether or not it had an animal in it。 and we ran our
    decision function on it that produced， an action。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们假设存在数据生成分布的原因，虽然我们并不知道这个分布。如果我们选择一对随机的输入输出对，比如一张随机的图片，以及图片中是否有动物，然后我们对它应用我们的决策函数，这就会产生一个动作。
- en: which in this case would be maybe a probability of an animal， that the loss
    that。 resulted is small in expectation。 Any questions on that？
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可能是一个动物出现的概率，导致的损失在期望上是小的。对此有问题吗？
- en: So an expectation is it's something you do to a random variable。 So just to
    be clear。 what's random in this expression？ EL f of x， y。 x is random and y is
    also random。 They're both random。 Okay， good。 All right， just as a reminder， so
    this is kind of our。 this is the dream way to evaluate， a decision function。 This
    is for now， this is our ultimate。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 期望是你对一个随机变量做的操作。为了明确一下，这个表达式中有什么是随机的？EL f(x, y)，x 是随机的，y 也是随机的。它们都是随机的。好的，明白了。好了，提醒一下，这就是我们评估决策函数的理想方式。现在，这就是我们最终的目标。
- en: But just a reminder， we don't assume we know this distribution， and so we can't
    actually。 evaluate this directly。 Nevertheless， this is what we have in mind as
    the goal of standard。 So when we're trying to find the best decision function
    for a particular problem， this is。 what we're shooting for。 We're shooting for
    the decision function that has the smallest possible expected loss。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但提醒一下，我们并不假设我们知道这个分布，因此我们实际上不能直接评估它。尽管如此，这就是我们所设想的标准目标。因此，当我们试图为特定问题找到最佳决策函数时，这就是我们所追求的目标。我们追求的是具有最小预期损失的决策函数。
- en: So such a decision function has a name。 It's called the Bayes decision function。
    It has nothing to do with Bayesian or Bayes rule， women Bayes rule， but not Bayesian
    statistics。 So a Bayesian decision， Bayes decision function， f star， is the function
    that achieves the。 minimal risk over all possible decision functions。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这样的决策函数有一个名字，叫做贝叶斯决策函数。它与贝叶斯或贝叶斯规则无关，尽管是贝叶斯规则，但不涉及贝叶斯统计学。贝叶斯决策函数，f*，是实现所有可能决策函数中最小风险的函数。
- en: So the decision function has the minimum possible expected loss。 Sometimes it's
    called the target function because in our machine learning， that's what。 we're
    hoping to end up with。 That's what we're aiming to get。 That's our target。 All
    right。 Let's hit a few examples， and then we'll take a break。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以决策函数具有最小的预期损失。有时它被称为目标函数，因为在我们的机器学习中，这就是我们希望得到的结果。这就是我们要追求的目标。好了，接下来让我们举几个例子，然后休息一下。
- en: So let's formulate some classic problems in this framework。 So let's start with
    the simplest least squares regression。 So as we discussed earlier。 the action
    space and the prediction space are both reels。 And we'll think as a loss function。
    the square loss。 So the difference between the action but we predict， the actual
    output line。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们在这个框架下构建一些经典问题。首先，从最简单的最小二乘回归开始。正如我们之前讨论的，动作空间和预测空间都是实数。我们会将平方损失作为损失函数。那么，预测的动作与实际输出之间的差异就是我们的损失。
- en: 1/2 the square。 So our risk for a given f， which is our expected loss。 is just
    1/2 the expected square distance。 Fine。 We would love to minimize。 we would love
    to find an f that minimizes r of f。 And actually in your first homework。 you'll
    have a problem to find with the minimizer of， this risk is。 And it comes out to
    f*。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 1/2 平方。所以给定 f 的风险，即我们的预期损失，仅仅是 1/2 的预期平方距离。好。我们希望最小化，我们希望找到一个 f 来最小化 f 的风险。实际上，在你们的第一次作业中，你们会有一个问题来找到这个风险的最小化器，结果会是
    f*。
- en: So the Bayes decision function is f* is just the conditional expectation of
    y given x。 It's very intuitive， right？ So have you guys seen conditional expectations？
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以贝叶斯决策函数 f* 只是给定 x 的条件期望 y。这是非常直观的，对吧？你们有看过条件期望吗？
- en: I have a strong prerequisite concept。 So conceptually， conditional expectation
    says， well。 one interpretation is exactly what's， right here。 So given an x。 the
    conditional expectation is the best possible prediction in the sense。 of mean
    square error for a life。 It's also the mean of the conditional distribution of
    y given x。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个强烈的前提概念。所以从概念上讲，条件期望的意思是，嗯。一种解释正好是这里所说的。所以，给定一个x，条件期望是在均方误差意义上对于一个生命最好的预测。它也是给定x的条件下y的条件分布的均值。
- en: So this is kind of one of the steps in the homework assignment。 So example two。
    multi-class classification。 So in this case， we're not going to do probabilities。
    we're just producing actual class。 We'll say there's k classes。 we're going to
    choose a number and that's what we'll produce。 Loss function， what we'll just
    do。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是作业中的一个步骤。比如例子二，多类分类。在这种情况下，我们不做概率计算，我们只是产生实际的类别。假设有k个类别，我们将选择一个数字，那个就是我们要输出的结果。损失函数，我们只需要这样做。
- en: it's called zero and one loss。 We have no loss if we get it correct and a loss
    of one if we're wrong。 So I guess to note this notation is one， okay。 It's called
    an indicator function。 We'll use it a lot。 So it's defined actually right here
    on the slide。 So when I write one parentheses a， not equal to y， that means is
    this expression evaluates。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这叫做零一损失。如果我们预测正确就没有损失，若错误则损失为一。所以，我想强调这个符号是1，好的。它叫做指示函数。我们将会经常使用它。实际上，它就在这个幻灯片上定义。所以当我写1（a
    ≠ y）时，这意味着这个表达式会计算。
- en: the number one if a is not equal to y and zero otherwise。 Indicator function
    is either one or zero。 It's a one if the predicate， it's called a predicate inside
    the parentheses。 If that predicate evaluates the truth then that expression is
    one otherwise zero。 All right。 So the risk is the expected loss。 So we've run
    an expectation in front of that indicator function。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果a不等于y则为1，否则为0。指示函数的值要么是1，要么是0。它为1如果括号里的谓词成立（称之为谓词），如果该谓词为真，则表达式为1，否则为0。好了，所以风险就是期望损失。因此，我们在指示函数前面加上了期望。
- en: What's the expectation of an indicator of an event？ Yeah。 the expectation of
    the indicator function of an event is the probability of that event。 That's easy
    to work out by hand。 So the risk of our decision function is exactly the probability
    that our prediction is not。 the correct prediction。 That's a sensible risk for
    a decision function。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 事件指示函数的期望是什么？是的，事件指示函数的期望就是该事件的概率。这很容易手工计算。所以我们决策函数的风险正是我们预测不正确的概率。这是一个合理的决策函数风险定义。
- en: And what would you say would be the decision function to minimize this risk？
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你会说，最小化这种风险的决策函数是什么？
- en: What's the obvious thing？ The decision function that predicts the most likely
    class for any x is the one that will。 have the smallest probability of error。
    So if we set f star x to be the arg max over all the classes of the conditional
    probability。 of that class given the x。 So just choose the class that has the
    highest probability。 condition on x。 So this is the Bayes decision function that
    we would love to achieve and a lot of machine。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的是什么？预测任何x的最可能类别的决策函数是那个会有最小错误概率的函数。所以，如果我们将f*（x）设置为在所有类别中，对于该类别给定x的条件概率的最大值。也就是说，选择具有最高概率的类别，条件是x。这就是我们希望实现的贝叶斯决策函数，许多机器学习方法都会使用它。
- en: learning is how we get closer to that as we can。 All right。 So any questions
    before we take a break？
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 学习是我们如何尽可能接近目标的过程。好了，在我们休息之前，有什么问题吗？
- en: Yeah， please。 I love philosophy。 So you call this statistical learning theory。
    Sometimes there's also a competition where you can take it out just not as far
    as it， can be。 So the question is， I'm saying this is statistical learning theory，
    but there's this other thing。 computational learning theory and how are they connected。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，请讲。我喜欢哲学。所以你称之为统计学习理论。有时候也有一种竞赛，你可以将它做得更远。所以问题是，我说这是统计学习理论，但还有另外一件事，计算学习理论，它们是如何联系的？
- en: And I don't think they are quite synonymous。 I think computational learning
    theory often gets into first of all some computability things。 I think also complexity
    of the algorithms themselves， but I'm not an expert in that area。 So I'm not exactly
    sure。 Any other questions？ All right。 Let's take a 10 minute break。 So let's see
    back a little after a 10。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我不认为它们完全是同义词。我认为计算学习理论首先涉及一些可计算性的问题。我认为还包括算法本身的复杂性，但我不是那个领域的专家。所以我不完全确定。还有其他问题吗？好吧。我们休息
    10 分钟。稍后 10 分钟后见。
- en: '![](img/97b99a42211704fc3d0686faa377c22a_9.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_9.png)'
- en: If you guys haven't noticed already， the lecture slides will be posted online
    on the。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你们还没有注意到，讲座幻灯片会在线发布。
- en: '![](img/97b99a42211704fc3d0686faa377c22a_11.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_11.png)'
- en: '![](img/97b99a42211704fc3d0686faa377c22a_12.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_12.png)'
- en: '![](img/97b99a42211704fc3d0686faa377c22a_13.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_13.png)'
- en: '![](img/97b99a42211704fc3d0686faa377c22a_14.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_14.png)'
- en: '![](img/97b99a42211704fc3d0686faa377c22a_15.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_15.png)'
- en: website。 As I posted from--so the slides are pretty similar to last year so
    far。 So those will be up if you want to read things in advance。 And then as I
    get versions of the new versions， I'll post those as well。 But generally I edit
    them until the last minute。 So if you are doing on a computer。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 网站。正如我之前发布的——所以到目前为止，幻灯片和去年差不多。如果你们想提前阅读内容，它们会被上传。然后，随着我更新新的版本，我也会发布那些。不过通常我会编辑到最后一刻。所以如果你们在电脑上进行学习。
- en: make sure you refresh when the lecture starts。 Shall I？ So what we've talked
    about is the risk。 which is our--yes please。 That's just arbitrary。 I didn't have
    to put that half。 Sometimes people define the loss with the half in front and
    sometimes the ballot。 The reason maybe people like it with the half is when you
    take the derivative， the two cancels。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在讲座开始时刷新页面。好吗？我们已经讨论了风险，也就是我们的——是的，请。那只是随便的，我不一定非得加上那一半。有时候人们在定义损失时前面加上那一半，有时候则没有。人们可能喜欢加上那一半的原因是，当你取导数时，那两个就会相互抵消。
- en: with the half and you don't have anything， maybe some people think that looks
    prettier。 It's pretty arbitrary。 Okay。 So we've talked about the risk， which is
    the expected loss。 which is our ultimate way， to evaluate a decision function。
    Of course we can't really do that because we can't compute the expectation because
    we。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 用一半，你什么也没有，也许有人觉得那样看起来更漂亮。这是相当随意的。好了，我们已经谈到了风险，也就是期望损失，这是我们最终用来评估决策函数的方法。当然我们实际上不能做到这一点，因为我们无法计算期望，因为我们……
- en: don't know the distribution generating the data。 What can we do？
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道生成数据的分布。我们能做什么呢？
- en: So the one thing that we can always assume we have in machine learning or statistics
    or。 data science is some data。 And we'll assume we have sample data。 This is what
    gives us traction in machine learning。 So we'll write this as dn。 It's a sequence
    of samples from that data generating distribution， iid sequence。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在机器学习、统计学或数据科学中，我们可以始终假设我们有一些数据。我们假设我们拥有样本数据。这就是我们在机器学习中获得进展的基础。所以我们将其写作 dn。这是来自数据生成分布的样本序列，是独立同分布（iid）序列。
- en: And to figure out how to come by this--so we want to compute--we want to come
    by this， expectation。 We want to evaluate this expectation。 We know we can do
    it exactly。 We want to approximate it。 How are we going to approximate it？ So
    let's take some inspiration from the famous strong law of large numbers that you
    guys。 know from probability theory。 Let's review the form。 So suppose we have
    some random variables。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要弄清楚怎么得到这个——所以我们想计算——我们想得到这个期望。我们想评估这个期望。我们知道我们可以精确地做到这一点。我们想要近似它。我们将如何近似它呢？所以让我们从著名的大数法则中获得一些启发，你们在概率论中应该了解这个。让我们回顾一下它的形式。假设我们有一些随机变量。
- en: z1 through zn， their iid， and they have some， expected value， easy。 then if
    we take the average of all the z's of n of those z's--so。 one of random sum them
    up--and we let n go to infinity， so let that sequence get longer， and longer。
    It converges to the expected value of the z。 Make sense？
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: z1 到 zn，它们是独立同分布（iid），并且它们有某个期望值，很容易得到。那么，如果我们对所有 z 的平均值进行求和，取 n 个 z 的平均值——所以对这些随机变量求和——并且让
    n 趋于无穷，随着序列变得越来越长，它会收敛到 z 的期望值。明白了吗？
- en: The long-term average of a bunch of identical distributed things converges to
    the expected， value。 Great， with probability one。 This is the strong law of large
    numbers。 So maybe this gives you some idea on how we can estimate this expected
    value。 Right？
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一堆独立同分布的事物的长期平均值会收敛到期望值。太好了，以概率 1。这就是大数法则。所以这或许能给你一些关于我们如何估计这个期望值的启示，对吧？
- en: What we have is we have all this iid data。 This pair is f and y。 And if we can
    write something as a sum over these individual draws from the distribution。 and
    if they were iid， they would converge to their expected value。 So if we could
    find something whose expected value is this risk， then we might be in business。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的是所有这些独立同分布（iid）的数据。这个对是 f 和 y。如果我们能将某些东西写成这些从分布中独立抽取的样本的总和，并且如果它们是独立同分布的，它们会收敛到期望值。那么，如果我们能找到某个东西，它的期望值就是这个风险，那么我们可能就能有所作为。
- en: by the law of large numbers。 So let's explore that。 This thing called the empirical
    risk functional。 So the empirical risk of a decision function f is its average
    loss on the data in the data， set dn。 All right？ So we take x1， y1。 We take the
    data points one at a time。 Let's consider the iid。 We write the loss of the decision
    function f on the particular input and output pair。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 根据大数法则。让我们探讨一下。这个叫做经验风险函数。决策函数 f 的经验风险是它在数据集 dn 上的平均损失。好吧？所以我们取 x1，y1。我们一次取一个数据点。假设它们是独立同分布的。我们写下决策函数
    f 在特定输入输出对上的损失。
- en: And that gives us a number。 We compute that loss for all n of our draws from
    the distribution。 That gives us n losses。 And then this has us averaging them
    together。 So r and r hat n of f。 this empirical risk of f， is the average loss
    of f on a random。 draw of n data points from our distribution。 All right？ OK。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这会给我们一个数字。我们为所有从分布中抽取的 n 个样本计算损失。这样我们得到 n 个损失值。然后我们把它们一起平均。所以 r 和 r 的帽子 n（f），即
    f 的经验风险，是 f 在从我们分布中随机抽取的 n 个数据点上的平均损失。好吧？明白了。
- en: So I think-- I don't know if we've used this before。 Notice there's a hat over
    the r。 Whenever you see a hat， that means a function of the data。 It also typically
    means that this hat-- the thing that the hat is over is intended as。 an estimator
    of the thing without the hat。 So in this case。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为--我不确定我们之前是否用过这个。注意，r 上有一个帽子。每当你看到帽子时，这意味着它是数据的函数。它通常还意味着这个帽子--帽子所加的对象是用来估计没有帽子的那个对象。所以在这个案例中。
- en: the hat over the r and f indicates that I want this to be estimating， r of f，
    which is our risk。 That's-- indication is kind of less formal。 And the end says
    how big our sample is。 So what do we know about this sum？ Are the sum a's？ Are
    they-- first of all。 what kind of object is this loss？ Is it a probability？ Is
    it a number？ Is it a vector？
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: r 和 f 上的帽子表示我希望它估计 r(f)，也就是我们的风险。这个--表示法有点不正式。最后部分则表示我们的样本有多大。那么我们对这个总和知道些什么呢？这些和
    a 是什么？首先，这个损失是啥对象？它是概率吗？是数字吗？是向量吗？
- en: It's a number。 Great。 Greater than or equal to 0， but it's generally a real
    number。 All right。 Is it random or is it not random？ Not random。 Or random。 Or
    tricky。 All right。 If we view xi and yi as random variables， not as actual numbers
    and data， but if we review。 them as random variables， then is this thing in the
    sum and random or no？ Of course。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个数字。很好。大于或等于 0，但通常是实数。好吧。它是随机的吗，还是非随机的？非随机的，或者是随机的，或者是有点棘手的。好吧。如果我们把 xi 和
    yi 看作随机变量，而不是实际的数字和数据，但如果我们把它们当作随机变量来看，那这个总和里的东西是随机的吗，还是非随机的？当然是随机的。
- en: it's a function of random variables。 It's random。 So we can do this whole average
    as a random variable itself。 It's random。 And thereby， it has an expectation。
    What's the expectation of this r hat？ And--。 So again？ R。 R。 R f。 I agree。 Why
    would that be？ Well， it's an average of these losses。 And each of these losses
    has an expectation that's r f。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 它是随机变量的函数。它是随机的。所以我们可以把整个平均值看作是一个随机变量本身。它是随机的。因此，它有一个期望值。这个 r 的帽子的期望值是多少？再说一次？r，r，r
    f。我同意。为什么是这样呢？因为它是这些损失的平均值，而这些损失的期望值正是 r(f)。
- en: Because if you put an expectation in front of that loss， well， that's exactly
    the definition。 of r of f。 See？ Exitation of a loss of of x， y。 The fact that
    x and y had subscripts of i。 it doesn't matter。 Their iid draws。 So xi， yi is
    the same as x， y， the same as x1， y1。 So the expectation of this sum and is exactly
    yes。 R of f。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果你把期望放在那个损失前面，那正是 r(f) 的定义。明白吗？损失 x, y 的期望。x 和 y 有下标 i 并不重要。它们是独立同分布的样本。所以
    xi，yi 就和 x，y 一样，和 x1，y1 一样。因此这个总和的期望值正是 r(f)。
- en: And when you're averaging a bunch of things that have expectation of r of f，
    the expectation。 is still r of f。 So yes， the expectation of r hat n of f is r
    of f。 There's a term for this。 Or the expectation of an estimator is equal to
    what it's trying to estimate。 Do you guys know this term？ Unbiased。 Unbiased。
    Great。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在对一组期望为r of f的事物进行平均时，期望值依然是r of f。所以，是的，r hat n of f的期望是r of f。这个有一个术语。或者说，估计量的期望值等于它试图估计的值。你们知道这个术语吗？无偏。无偏。很好。
- en: So r hat n is an unbiased estimator of r of f。 Very good。 How about our law
    of large numbers？
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以r hat n是r of f的无偏估计量。很好。那么我们的“大数法则”怎么样？
- en: Does that apply here？ What do we need？ We need independent and identically distributed
    random variables to be averaged。 So each of these loss expressions， you said this
    random。 Are they independent？
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于这里吗？我们需要什么？我们需要独立且同分布的随机变量来进行平均。所以这些损失表达式，你说它是随机的。它们独立吗？
- en: They're independent because they depend on random variables that are themselves
    independent。 And are they identically distributed？ Yeah， same reason。 We have
    an average of independent and identically distributed random variables。 So strong
    law of large numbers says they converge in the limit as n goes to infinity to
    the。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是独立的，因为它们依赖于自身独立的随机变量。它们是同分布的吗？是的，原因相同。我们有一组独立且同分布的随机变量的平均值。所以大数法则说它们在n趋近于无穷大时会收敛到那个值。
- en: expectation。 Great。 All right。 So strong law of large numbers applies。 That's
    great。 So what does that tell us？ Let's regroup a bit。 So someone gives you a
    function， F。 a decision function， and they say， can you evaluate this？ Can you
    tell me the risk？ No， I can't。 I don't know the probability。 But if I take a big
    enough data sample， a big enough set D。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值。很好。好了。所以大数法则适用。这很好。那么这告诉我们什么呢？让我们稍作总结。所以有人给你一个函数F，一个决策函数，他们说，你能评估这个吗？你能告诉我风险吗？不，我不能。我不知道概率。但如果我取一个足够大的数据样本，一个足够大的集合D。
- en: I can get you as close as， you want to estimating the true risk with my empirical
    risk。 So we can get with big enough data set as good estimates as we like of the
    risk。 We can't get it exactly。 So that sounds good for evaluating a particular
    decision function。 But that's still not really the machine learning。 That's still
    not the whole picture。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以让你尽可能接近，用我的经验风险估计真实风险。所以只要数据集足够大，我们就可以得到尽可能好的风险估计。我们无法做到完全精确。所以对于评估一个特定的决策函数来说，这听起来不错。但这仍然不是真正的机器学习。那还不是全部。
- en: Because we need to find the decision function that does well， not just evaluate
    a particular。 decision function。 This is an extra step。 All right。 Well。 how do
    we find the base decision function？ We minimize the risk。 So stance reason。 maybe
    we should try minimizing the empirical risk overall functions F。 Let's see how
    that works。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们需要找到一个能做得好的决策函数，而不仅仅是评估一个特定的决策函数。这是一个额外的步骤。好了。那么，我们如何找到基础决策函数呢？我们最小化风险。基于这个原因，也许我们应该尝试最小化所有函数F的经验风险。让我们看看这怎么运作。
- en: So definition， F hat is an empirical risk minimizer if it's the minimizer of
    the empirical risk。 And in this case， I've written it over all possible functions
    F， all measurable functions， F。 All right。 All right。 Let's take an example。 Because
    I don't think this method is going to work so well。 So I've designed a data generating
    distribution， a completely degenerate and trivial one。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以定义，F hat是一个经验风险最小化器，如果它是经验风险的最小化器。在这种情况下，我已经写出了所有可能的函数F，所有可测函数F。好了。好了。让我们举个例子。因为我认为这种方法可能不会非常有效。所以我设计了一个数据生成分布，一个完全退化且微不足道的分布。
- en: But we have to consider it anyway。 So the distribution of X is uniform on zero
    one。 And the distribution of Y is identically one。 Degenerate。 So any point we
    sample will lie on this blue line。 You can think of the blue line as we've sampled
    a whole bunch of points from this distribution。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们必须考虑这个。于是X的分布在零和一之间是均匀的。而Y的分布是完全相同的，为1。退化的。所以我们采样的任何点都会落在这条蓝线上。你可以把这条蓝线看作是我们从这个分布中抽取的一堆点。
- en: and they cover that whole line。 That is our data generating distribution。 And
    we draw a sample from this distribution。 So there's three red points。 And now
    let's propose a decision function for this data。 Here's a proposal。 What about
    the function that's always zero except to the three data points？ And there it's
    one。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 它们覆盖了整条线。这就是我们的数据生成分布。我们从这个分布中抽取一个样本。所以有三个红点。现在让我们为这些数据提出一个决策函数。这里是一个提议。如何看待这个函数：除了三个数据点外，它始终为零？而在那三个点上，它是1。
- en: All right。 So this is obviously terrible for this distribution。 Let's evaluate
    it。 What's the risk？
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么这显然对这个分布来说非常糟糕。我们来评估一下。风险是什么？
- en: Well， okay。 What's our loss first of all？ Let's do zero one loss。 So we either
    get it exactly or not at all。 So every single X except three。 we have loss of
    one because we're wrong。 And since X has uniform distribution。 the probability
    of these three points is zero。 So the expected loss is one。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。首先我们的损失是什么？我们使用零一损失。那么我们要么完全正确，要么完全错误。所以，除了三个点外，所有的X都会有一个损失为1，因为我们是错的。而且由于X是均匀分布，这三个点的概率为零。所以期望损失是1。
- en: Expected loss is kind of worst possible for this prediction。 So even though
    F hat has perfect performance on the data， it has kind of worst possible。 performance
    in the actual risk。 So good empirical risk。 Empirical risk is zero。 fits it perfectly
    on the data。 Risk is one。 All right。 So what's the term for this？ Overfitting。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 期望损失对于这个预测来说是最坏的情况。所以即使F帽子在数据上的表现完美，它在实际风险中的表现却是最差的。所以好的经验风险是零，在数据上完美拟合。风险是1。好了，那么这个术语是什么？过拟合。
- en: This is called overfit。 So kind of what's gone wrong here？ I mean。 so clearly
    F hat is some sense kind of memorized the data。 It just seems like we don't expect
    functions to jump around like that necessarily。 And if they did。 we'd have no
    hope to learn them is actually more the point。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的过拟合。那么这里出了什么问题呢？我的意思是，很明显，F帽子在某种意义上有点像是记住了数据。看起来我们不指望函数会像那样跳跃。如果它们真这么做了，我们实际上就没有希望学习它们了，这才是问题所在。
- en: So a lot of machine learning is about figuring out how to spread information
    from the points。 you observe to regions of the space that you haven't observed
    yet that are nearby。 So a lot of machine learning modeling is different ways to
    spread information that you have about。 this point X to nearby points。 Whether
    it's a constraint on the functions that you're fitting or ways to kind of bleed。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，机器学习的很大一部分就是弄清楚如何将你观察到的信息从这些点传播到你还没有观察到的、邻近的空间区域。所以，机器学习建模的很多方式就是将你关于这个点X的信息传播到附近的点。这可以是你拟合函数时的约束，或者是一些方法，像是向邻近点传播信息。
- en: out the fact that Y is one here and so maybe it should be one or near one nearby。
    These are the types of things that we're looking to put into our machine learning
    methods。 So we need a way to smooth things out so we don't get this overfitting。
    And one approach。 in fact the main approach that we'll use is it's empirical risk
    minimization， with the constraints。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是Y在这里是1，所以它可能应该是1或附近接近1。这些是我们希望纳入我们机器学习方法中的类型。因此，我们需要一种方式来平滑这些内容，这样就不会出现过拟合。而一种方法，实际上是我们将使用的主要方法，是带约束的经验风险最小化。
- en: That's a new piece。 It's a constraint now and rather than minimizing the empirical
    risk of all possible functions。 we're going to minimize it over a restricted set
    of functions which is called the hypothesis， space。 So the idea of the hypothesis
    space is that we build in our notions about what we think。 the final function
    should look like into the hypothesis space itself。 So if we think， wow。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个新的部分。现在它是一个约束，而不是最小化所有可能函数的经验风险，我们将对一个受限的函数集进行最小化，这个受限集被称为假设空间。所以假设空间的想法是，我们将关于最终函数应该是什么样子的概念融入到假设空间本身中。所以如果我们认为，哇。
- en: the answer to this， the right decision function for this data should， never
    be jumpy like this。 It should be a little bit smooth。 Let's make a hypothesis
    space that only includes functions that have the smoothness that we。 expect。 So
    that's the notion of a hypothesis space。 You kind of build into the space itself
    the constraint on the type of function that you。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案是，对于这些数据，正确的决策函数应该永远不会像这样跳跃。它应该是稍微平滑的。我们来构建一个假设空间，只包括那些具有我们期望的平滑性的函数。这就是假设空间的概念。你在空间本身中构建了对函数类型的约束。
- en: hope to end up with。 All right， so formally， a hypothesis space F。 it's a set
    of functions mapping from input， space to actions。 It's the same type of object
    as the decision function。 What hypothesis spaces a set of possible decision functions？
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望最终能得到什么？好的，正式地说，假设空间F是一个从输入空间到动作的函数集合。它与决策函数是相同类型的对象。假设空间是可能的决策函数集合。
- en: And usually it's much smaller than the space of all possible functions。 So we're
    looking for hypothesis spaces that capture the smoothness we expect and also。
    somehow easy to work with because the very next step after specifying our hypothesis
    space。 is we have to minimize overall hypothesis spaces。 We have to minimize our
    empirical risk over all decision functions in the hypothesis space。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通常它比所有可能函数的空间要小得多。所以我们正在寻找能够捕捉我们期望的平滑性并且也容易操作的假设空间，因为在指定我们的假设空间之后的下一步就是最小化整体假设空间。我们必须在假设空间中的所有决策函数上最小化我们的经验风险。
- en: So we want it to be a space that's manageable and not too difficult to work
    with。 All right。 so you guys have some machine learning background already。 There
    are some example hypothesis spaces you guys have worked with。 Linear functions，
    straight。 We have an input space， maybe a vector space， Rd， linear functions，
    set up all linear functions。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们希望它是一个可管理的空间，而不是太难操作的空间。好的，你们已经有一些机器学习背景了。你们曾经使用过一些假设空间的例子。线性函数，直线。我们有一个输入空间，可能是一个向量空间，Rd，线性函数，设置所有线性函数。
- en: would be hypothesis space。 Great。 How about something else？
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 那会是一个假设空间。很好。那其他的呢？
- en: Most of the stuff we work with is linear， but there must be something else。
    Say again？ Okay。 polynomial functions of the input。 Great。 What else？ Something？
    Okay， generalized linear models。 Great。 So here the action space is different。
    The action space in generalize linear model is a probability distribution and
    the input space。 is real。 Great。 Good example。 So how about trees？ Those are pretty
    different。 The hypothesis space。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大部分工作都是线性的，但一定还有其他的东西。再说一遍？好的，输入的多项式函数。很好。还有其他的吗？某些？好的，一般化线性模型。很好。所以这里的动作空间是不同的。一般化线性模型中的动作空间是一个概率分布，而输入空间是实数。很好。很好的例子。那么树呢？那些差异很大。假设空间。
- en: a function in the tree space is kind of constant and then it's piece， wise constant。
    So it covers pieces of space and it's constant for region and another region。
    It's a different constant。 That's very different from any of the ones we've said
    so far。 So a lot of the course will be exploring different types of hypothesis
    spaces and seeing the relative。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 树空间中的一个函数是常数的，然后它是分段常数的。所以它覆盖空间的不同部分，对于一个区域是常数，另一个区域是不同的常数。这与我们到目前为止提到的任何一个都非常不同。所以课程的很大一部分内容将会是探索不同类型的假设空间，并查看它们的相对关系。
- en: merits。 All right。 So here we have the constrained empirical risk minimizer。
    The empirical risk minimizer in F， this hypothesis space is now everything looks
    exactly the same。 but the minimum is over the set hypothesis space F。 That's our
    constraint。 And correspondingly there's an actual risk minimizer and expected
    loss that is also constrained。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 优点。好的。所以这里我们有受限的经验风险最小化器。F中的经验风险最小化器，这个假设空间现在看起来完全相同，但最小值是在假设空间F的集合上。那就是我们的约束。相应地，还有一个实际的风险最小化器和期望损失，它也是受约束的。
- en: to the set F。 So now we minimize， we have this other thing F star F which minimizes
    over the hypothesis。 space the expected loss。 So it's the minimum risk in the
    hypothesis space。 So we have。 within the hypothesis space F we have the minimum
    empirical risk minimizer。 We have the empirical risk minimizer and we have the
    risk minimizer， constrained hypothesis。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 到集合F。所以现在我们最小化，我们有另一个东西F星F，它在假设空间上最小化期望损失。所以它是在假设空间中的最小风险。所以我们有。在假设空间F中，我们有最小的经验风险最小化器。我们有经验风险最小化器，也有风险最小化器，受约束的假设空间。
- en: space F。 And the whole is that with enough constraint F we won't have this overfitting。
    All right。 I'm going to pause on this thread here。 I'll see if you have any questions。
    And I'm going to talk for a while about the practical side of things which you'll
    need for。 your homework。 It's like gradient descent， stochastic gradient descent。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 空间F。总体来说，通过足够的约束F，我们就不会出现过拟合。好的，我在这个话题上先暂停一下，看看你们有没有问题。我接下来会讲一讲实际操作方面的内容，这对你们的作业非常重要。这就像梯度下降，随机梯度下降。
- en: And if there's any time left we'll come back to these slides and otherwise we'll
    turn to。 these next week。 Okay。 Sure。 So earlier I said yeah let's consider the
    case where the action F of X does not affect。 Y。 So what was the reason you did
    that because unless you used that， so you thought it was。 a great time to work？
    Oh， so the question is do we already see the reason for making that assumption？
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果还有时间，我们会回到这些幻灯片上，否则我们下周再看这些。 好的。 当然。 早些时候我说过，让我们考虑动作 F of X 不影响 Y 的情况。 那么你为什么这么做呢？
    因为除非你使用了这个假设，你认为这是个很好的工作时机？ 哦，问题是我们是否已经看到了做出这个假设的原因？
- en: '![](img/97b99a42211704fc3d0686faa377c22a_17.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97b99a42211704fc3d0686faa377c22a_17.png)'
- en: I think that the reason for making that assumption happens all the way at the
    beginning where。 when I make the assumption that all pairs X， Y are generated
    from a distribution that's。 where the assumption comes in and the action cannot
    affect the output because if， so I see。 X and I produce an action A。 If that affects
    Y then the distribution that Y is generated。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为做出这个假设的原因在一开始就已经有了。 当我假设所有的 X，Y 对是从一个分布中生成时，这就是假设的来源，而动作不能影响输出，因为如果是这样的话，我看到
    X 并产生一个动作 A。 如果那影响了 Y，那么 Y 的生成分布。
- en: has to depend on X and the action。 So as soon as I say X。 Y is generated in
    pairs that already precludes the action affecting， the output。 Yeah。 good question。
    Okay。 Alright， any other questions？ Okay。 so you guys have seen gradient descent
    before。 How about stochastic gradient descent？ Okay。 So。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 必须依赖于 X 和动作。 所以一旦我说 X，Y 就成对生成，这已经排除了动作对输出的影响。 是的。 好问题。 好的。 好的，其他问题吗？ 好的。那么你们之前见过梯度下降法吧？
    随机梯度下降法呢？ 好的。那么。
- en: whoops。 Okay。 Alright， let's start with this specific example。 Linear least
    squares regression。 The most vanilla model that we have。 Input space RD， output
    space and action space are R。 loss function， square difference between， prediction
    and actual hypothesis space。 Here's our first concrete hypothesis space it was
    mentioned earlier。 This is linear functions of X。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。 好的。 好的，让我们从这个具体例子开始。 线性最小二乘回归。 我们拥有的最基本模型。 输入空间 RD，输出空间和动作空间是 R。 损失函数，是预测与实际之间的平方差。
    假设空间。 这是我们第一个具体的假设空间，之前提到过。 这是 X 的线性函数。
- en: linear functions of our input。 See F of X is equal to W transpose X。 So these
    functions are parameterized by a vector W in RD。 So that's the way we can get
    a handle on these functions。 What suppose we have data set of N samples from some
    distribution？
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入的线性函数。 看到 F of X 等于 W 转置 X。 所以这些函数是通过 RD 中的向量 W 来参数化的。 这就是我们可以处理这些函数的方式。
    假设我们有来自某个分布的 N 个样本数据集呢？
- en: Let's find the empirical risk minimizer F hat in these hypothesis space of linear
    functions。 Alright？ What do you guys think the answer will be？ I think this will
    come out to you。 the ordinary regression that you always use。 Probably so。 Okay。
    So， objective function。 the empirical risk。 This is the average squared， oh I
    left off at 1/2th or 9th。 Alright。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这些线性函数的假设空间中找到经验风险最小化器 F hat。 好吗？ 你们认为答案会是什么？ 我认为这将给你们带来普通的回归方法。 可能是的。 好的。
    所以，目标函数。 经验风险。 这是平均平方，哦，我漏掉了 1/2 或 9。 好的。
- en: So this is the average loss on the examples。 Here the loss function is the square
    loss。 We like to minimize this over lipotz space F。 So this problem has a closed
    form solution and actually you'll work on different ways。 to drive that tomorrow。
    Great。 I'll also use this as an example for an iterative solution。 You guys know
    what a closed form solution is？ Closed form means an explicit expression in terms
    of usual matrix and vector operations。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是样本上的平均损失。 这里的损失函数是平方损失。 我们喜欢最小化这个 Lipschitz 空间 F 上的损失。 所以这个问题有一个封闭形式的解，实际上你们明天会研究不同的方法。
    很好。 我还将用这个作为迭代解法的例子。 你们知道什么是封闭形式的解吗？ 封闭形式意味着用通常的矩阵和向量运算表示的显式表达式。
- en: that gives you the answer that you can just plug into programming language and
    write it。 and press return and there's the answer。 Okay。 So。 specific problem
    for this for linear regression is to minimize this function over W， overall， W。
    We're going to take a step back and generally review how gradient descent works
    as motivation。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了你一个答案，你可以直接把它写入编程语言，按下回车，答案就出来了。 好的。 所以。 对于线性回归的具体问题是最小化这个关于 W 的函数，整体上，W。
    我们将退一步，通常回顾一下梯度下降法是如何作为动机工作的。
- en: for stochastic gradient descent and then you'll be ready to go for optimizing
    your empirical， risks。 So， setting。 Suppose we have an objective function F that's
    differentiable。 We want to find the minimum over its domain in R。D。 And there's
    this object called the gradient。 When you have a differentiable function， it's
    differentiable， say at a point x0。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机梯度下降，然后你就可以开始优化你的经验风险了。所以，设定。假设我们有一个可微分的目标函数F。我们想要找到它在R.D定义域上的最小值。这里有一个叫做梯度的对象。当你有一个可微分的函数时，它在某个点x0处是可微分的。
- en: This object called the gradient is a special thing because it points in the
    direction that。 the function will increase the most quickly。 Okay。 Now we're trying
    to minimize functions here。 So imagine that this function maps from R2。 We could
    say the floor is the domain of the function and maybe the value of the function。
    is you imagine a height above the floor would be the value of the function。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个叫做梯度的对象是一个特殊的东西，因为它指向函数增长最快的方向。好的。现在我们在这里尝试最小化函数。所以假设这个函数从R2映射过来。我们可以说地面是函数的定义域，也许函数的值就像是地面上的高度，函数的值就是你可以想象的地面上方的高度。
- en: We want to find the minimum of this function。 So I'm doing an iterative descent
    algorithm and I'm starting at a randomly chosen initial。 point and I take the
    gradient and the gradient is pointing this way。 The gradient is a vector in the
    domain。 So the gradient is a vector in R2。 The points somewhere in R2。 Suppose
    it points this way。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要找到这个函数的最小值。所以我正在做一个迭代下降算法，我从一个随机选择的初始点开始，计算梯度，而梯度指向这个方向。梯度是定义域中的一个向量。所以梯度是R2中的一个向量。这个点位于R2中的某个地方。假设它指向这个方向。
- en: So this is the direction of steepest increase。 I want to minimize。 So if I go
    in the opposite direction， I'm in the direction of steepest decrease。 So I'm gradient
    descent。 I take a step in the direction of a negative gradient。 How big a step？
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是最陡峭上升的方向。我想要最小化。所以如果我朝相反方向走，我就朝着最陡峭下降的方向走。那我就是梯度下降。我朝负梯度的方向迈出一步。多大的一步呢？
- en: For now let's just call it eta。 Right？ Eta would be step size。 So that's one
    step of gradient descent。 We go where things are decreasing the fastest。 Here's
    the gradient descent algorithm。 Initialize point。 We find the gradient。 We go
    in the negative direction of the gradient。 Time some step size。 Repeat until we're
    done。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们就叫它eta。对吧？Eta就是步长。所以这就是一次梯度下降。我们朝着最速下降的方向走。这里是梯度下降算法。初始化点。我们找到梯度。我们朝梯度的负方向走。乘以某个步长。重复直到完成。
- en: Right？ Very straightforward。 A few details are missing。 Question？ Whatever。
    So here would be four steps of gradient descent。 Each time straight， change direction
    straight。 change direction straight。 So two things need to be resolved still。
    How big a step do we take and when do we stop？ So there's a few answers here。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？非常直观。只是缺少一些细节。有问题吗？随便。那么这里是四个梯度下降的步骤。每次都是直线，改变方向直线，再改变方向直线。所以还有两个问题需要解决。我们该走多大一步？何时停止？这里有一些答案。
- en: Empirically you can try a fixed step size。 So like for instance， multiply your
    gradient by 0。1 every time。 A fixed step size。 That works decently。 Better yet
    is something called-- is some kind of-- well， not fixed step size。 So one way
    to find your step size is called backtracking line search。
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验上看，你可以尝试一个固定的步长。例如，每次将你的梯度乘以0.1。一个固定的步长。这样也能正常工作。更好的方法是使用某种方法——嗯，不是固定步长。所以找步长的一个方法叫做回溯线搜索。
- en: And this will be a problem in the homework actually。 The idea of backtracking
    line search-- see if I can summarize it in a minute。 So suppose this is my negative
    gradient direction。 The gradient isn't just a direction。 It's an amount。 It tells
    you how much the function will decrease per unit step in that direction。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这其实会是作业中的一个问题。回溯线搜索的想法——看我能不能在一分钟内总结出来。假设这是我的负梯度方向。梯度不仅仅是一个方向。它是一个量。它告诉你在该方向上每单位步长，函数会下降多少。
- en: Right？ So suppose my gradient， the magnitude of my gradient instruction， predicts
    that per unit。 of my step， the function should decrease by 10 units， 10， whatever
    the function is in。 So I take a step of a certain size and I find that my function
    only went down by 1。 Like that's 1 per unit step。 And I'm like， well， that's funny。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？假设我的梯度，梯度的大小，预测每单位步长，函数应该下降10个单位，10个单位，无论函数是什么。所以我迈出一步，发现我的函数只下降了1个单位。像是每单位步长下降了1个单位。而我想，嗯，那有点奇怪。
- en: The gradient set should go down by 10 per unit。 So maybe that took too big a
    step because if I know that if I take an infinite。 test， and a small step， the
    decrease should be 10 per unit of my step。 So that was too big a step because
    it didn't decrease as much。 I'm going to backtrack a little bit。 All right。 Now
    it's at 5。 All right。 Well， 5 is pretty close to 10。 Is that good enough？
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度集应该每单位下降10。所以也许我采取了一个太大的步骤，因为如果我知道我采取一个无限小的测试步长，下降量应该是每个步骤10。所以那是一个太大的步骤，因为下降量没有预期的那么多。我打算回溯一点。好吧，现在它是5。好吧，5离10很接近。那够好吗？
- en: I don't know。 If I wanted to， I could take a smaller step and then eventually
    get closer and closer to。 10， which is the amount predicted by the gradient。 So
    there's a bit of a trade-off here between-- if you take a really small step， you'll
    get。 a lot of decrease proportionally， but it's a very small step。 So usually
    what you do is you say。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道。如果我想要的话，我可以采取一个更小的步骤，然后最终逐渐接近10，这是由梯度预测的数值。所以这里有一个权衡——如果你采取非常小的步骤，虽然每次减少的比例会很大，但步伐非常小。所以通常你做的是，你说。
- en: I want to find the step that gets， for instance， 50%， of the decrease predicted
    by the gradient。 It's a tuning parameter and you could set。 And that's kind of
    the essence of backtracking line search。 You can learn it in more detail on the
    homework。 I reference Wikipedia。 And a stopping rule。 Well。 a bunch of ways to
    do it。 One way not totally joking is you can run it until you're bored because
    sometimes the。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我想找到那个步骤，它能得到梯度预测下降量的50%。这是一个调节参数，你可以设置。那就是回溯线搜索的精髓。你可以在作业中学到更详细的内容。我参考了维基百科。至于停止规则，有很多方法。一种方式——虽然不是完全开玩笑——就是你可以跑到你感到无聊为止，因为有时。
- en: performance does continue to improve even after a long period。 But more seriously。
    you can look for when the function doesn't decrease more than some， value。 You
    can set over a certain number of periods when the gradient is below a threshold，
    kind。 of different ways that are fairly straightforward。 There's really no magic
    here。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 性能即使在较长时间后也会继续改善。但更严肃的是，你可以观察何时函数的减少量小于某个值。你可以设定在一定的周期数内，当梯度低于某个阈值时，采取不同的方法，这些方法都相对简单。这里真的没有什么魔法。
- en: One thing you can do， which is different， is you can have some holdout data
    and assess。 the performance of your model on some data that you're not using for
    training and you。 can see when that stops decreasing。 Okay。 But these are a little
    bit beside the point。 So for linear regression， we can do gradient descent。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的一件不同的事情是，你可以拥有一些留出的数据，并评估你的模型在一些不用于训练的数据上的表现，你可以看到何时不再下降。好吧，但这些话题有点跑题了。所以对于线性回归，我们可以做梯度下降。
- en: We take the gradient of this square objective and we find that it's some average
    of residuals。 and we compute this at every iteration。 So we start at a point，
    we compute the gradient。 we go in that direction for a while， we recompute， the
    gradient。 which will point us in a little bit of a different direction， take another，
    step。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个平方目标函数取梯度，发现它是残差的某种平均值，并且我们在每次迭代时计算这个梯度。所以我们从一个点开始，计算梯度，沿着那个方向走一段时间，再重新计算梯度，梯度会指向一个稍微不同的方向，然后再迈出一步。
- en: and then we recompute and maybe it takes us in a little bit different direction
    and， another step。 And it's important here on the point I'm making that the direction
    keeps changing and we're。 not always going in a direct route towards the optimum。
    All right。 And we'll come back to that。 All right。 So first I want to ask， how
    does this method scale in terms of the size of our training。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重新计算，可能它会带我们朝着一个稍微不同的方向走，迈出下一步。这里我想强调的是，方向不断变化，我们并不是总是朝着最优解的直接路径前进。好吧，我们稍后再回到这个话题。首先，我想问的是，这种方法在我们的训练规模上是如何扩展的？
- en: data？ So computing the gradient， here's an expression for the gradient。 We have
    to touch every data point once because we're summing over the data points。 So
    it's just fine for small data sets， but if we're running on a data set that's，
    you， know。 100 million large， this is a big operation just to figure out the direction
    for one step。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据？所以计算梯度，这里是梯度的表达式。我们必须触及每个数据点一次，因为我们在对数据点求和。所以对于小数据集来说完全没问题，但如果我们在运行一个有着上亿数据点的数据集，那么仅仅为了确定一个步骤的方向，这就是一个庞大的操作。
- en: So this doesn't scale very well for huge data sets。 And what I'll argue later
    is that it's probably overkill to use this method anyway。 So the question is，
    how can we make progress with our stepping without looking at all the。 data in
    every step？ That's the key question。 So to get there， let's back it up a step。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于巨大的数据集，这个方法并不适用。而我稍后会论证，使用这种方法可能本身就是多此一举。那么问题是，如何在不查看每一步的所有数据的情况下，推进我们的步伐？这就是关键问题。为了回答这个问题，让我们退后一步。
- en: So what we were doing was minimizing the empirical risk。 That was our-- that's
    what we were satisfied with。 We're going to minimize empirical risk。 Let's go
    back to the original goal problem。 Let's think about minimizing the risk itself。
    the expectation。 It seems a bit crazy。 We can evaluate this。 Let's work some mathematics。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在做的是最小化经验风险。这就是我们的——这就是我们满足的目标。我们将要最小化经验风险。让我们回到最初的目标问题。让我们思考如何最小化风险本身，期望值。看起来有点疯狂。我们可以评估这个问题。让我们来做一些数学推导。
- en: So suppose this function space is parameterized by w， so I can differentiate
    with respect to。 w instead of f。 It's a little bit easier。 What can we do with
    the gradient of this expectation？ Oh。 gradient of an expectation。 Well， there
    is this thing you might have heard of called differentiating under the integral。
    sign。 So that's when we swap the derivative and an integral。 Expectation is just
    an integral。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个函数空间由w参数化，那么我可以对w进行微分，而不是对f进行微分。这稍微简单一点。我们能用这个期望的梯度做什么呢？哦，期望的梯度。嗯，可能你听说过一个叫做在积分符号下微分的东西。就是我们交换导数和积分。期望值本身就是一个积分。
- en: Gradient is just a derivative。 Yes。 That's a thing。 That's justified。 So we
    can swap the expectation and the gradient to get an expression for the gradient--
    an。 equivalent expression for the gradient of the risk。 All right。 Now we have
    the expectation on the outside。 This is a trick we used earlier today。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度就是一个导数。是的，这没问题。是有理论依据的。所以我们可以交换期望和梯度，得到风险梯度的一个等效表达式。好了，现在我们把期望放在外面。这是我们今天早些时候用过的技巧。
- en: What can we do when we have an expectation that we don't know how to evaluate，
    but we'll。 be happy with an approximation。 So again？ Yeah。 Yeah。 so the law of
    large number argument we made earlier was， all right， we don't know。 if it's an
    expectation， but we have this sample of data that's average that thing in。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个我们不知道如何评估的期望值，但我们会满足于一个近似值时，我们能做些什么？再说一遍？对，没错。那么我们之前做的大数法则论证是，好的，我们不知道它是否是一个期望值，但我们有这个数据样本，平均一下就行。
- en: the expectation over the sample data。 And then that should converge with a large
    enough sample to the expectation。 which is， what we want。 So let's work in that
    direction。 So here's our average over the sample of this gradient of the loss。
    And I wrote a hat over this to show that it's what we're using as our estimator。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对样本数据的期望值。然后当样本足够大时，它应该收敛到期望值。这正是我们想要的。所以让我们朝这个方向努力。这里是我们对样本的平均值，表示损失的梯度。我在这个上面加了一个帽子，表示这是我们用作估计器的东西。
- en: And just as we argued before， you can check that this is an unbiased estimator。
    So the expected value of this is the gradient of the true risk。 That's nice。 It's
    consistent by law of large numbers。 Oh， the term consistent means an estimator
    is consistent for what is trying to estimate。 if as the sample grows to infinity，
    the estimator converges to what it's trying to estimate。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所论证的，你可以检查这是一个无偏估计器。所以它的期望值就是实际风险的梯度。这很好。根据大数法则，它是一致的。哦，一致性这个术语意味着，一个估计器是一致的，指的是它试图估计的量。如果样本增长到无限大，估计器将收敛到它所要估计的目标。
- en: That's consistency。 And well， here's interesting。 This expression is exactly
    the gradient of the empirical risk that we were talking about。 before。 So two
    ways to end up with the idea of taking the gradient of the empirical risk。 One
    is just do that。 The other is say， I want to really take the gradient of the risk。
    but I don't know that。 So I'm going to approximate it with this average of the
    gradients over the data set。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一致性。嗯，接下来有意思的是，这个表达式正是我们之前讨论的经验风险的梯度。所以有两种方法可以得到经验风险的梯度。一个是直接这么做，另一个是说，我确实想取风险的梯度，但我不知道，所以我要用数据集上的梯度平均值来近似它。
- en: OK。 So that's interesting。 That's interesting。 But here's what I want to ask。
    So now we're viewing this expression as just a way to estimate the expectation。
    So what would happen if we didn't use all the data points？
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么这很有趣。这很有趣。但我想问的是。如果我们不使用所有数据点，会发生什么？
- en: Is it still an estimate of the expectation？ Yeah。 So if n is 100 million， I
    don't know。 I think things would be so much worse if we used a， million or 100，000
    or 100 or 10。 But you get the idea。 Each of those， if we use a subset of data
    points。 it's at least still unbiased for the， gradient that we actually want。
    All right。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 它仍然是期望的估计吗？是的。所以如果n是1亿，我不知道。如果我们用了百万、十万、百个或者十个，情况会变得更糟，但你明白我的意思。如果我们使用一个数据子集，它至少对我们实际想要的梯度仍然是无偏的。好了。
- en: So one of the trade-offs。 Bigger and a better estimate of the gradient。 On the
    other hand。 bigger and it takes longer to compute because we're touching more
    data， points。 OK。 How big end do we need？ There's another issue here though。 So
    gradient descent takes a bunch of steps， no matter how good our direction is。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是一个权衡。更大的数据集能更好地估计梯度，另一方面，更大的数据集计算起来更慢，因为我们要处理更多的数据点。好的。那么我们需要多大的数据集？不过这里还有一个问题。所以，无论我们的方向有多好，梯度下降都需要多个步骤。
- en: So I can take the average-- I could take the gradient of the empirical risk
    over all。 100 million points and get a very precise step direction and I could
    go there。 And then I'm still going to go in this direction next with another long
    computation。 And what I propose to you intuitively is that if the step direction
    is this way at 100 million。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我可以取平均值——我可以对所有1亿个点的经验风险计算梯度，并得到一个非常精确的步长方向，然后我可以朝那个方向走。然后我接下来还会继续沿着这个方向走，再进行一次长时间的计算。我直观地向你提议的是，如果在1亿数据点时步长方向是这样的。
- en: and it's a little bit off at a million and a little bit more off at 1，000 and
    a little， more at 10。 maybe it doesn't matter that much because we're about to
    adjust again anyway， in the next step。 Because we keep recomputing the gradient。
    And I think this is the intuitive understanding for why this works。 We get to
    correct after every step。 So it's almost like you want to get to the correction
    part as quickly as you can and not。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 它在一百万时有点偏差，在一千时偏差更大，在十时偏差更大。也许这并不太重要，因为我们接下来会再次调整。因为我们会不断重新计算梯度。我认为这就是为什么这种方法有效的直观理解。我们在每一步后都能进行修正。所以，几乎可以说，你要尽可能快地进入修正阶段，而不是。
- en: waste time trying to fine tune an initial step which is going to be way off
    anyway。 OK。 So here are possibilities。 Get the estimate of the gradient using
    all the data。 Get the estimate using a small subset of data。 And then in the limit。
    get the estimate of the gradient using a single data point。 That's a real thing。
    It really works。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 浪费时间去精细调整一个本来就偏差很大的初始步长。好的。所以这里有几种可能性。使用所有数据获取梯度的估计。使用一小部分数据获取梯度的估计。然后在极限情况下，使用单个数据点获取梯度的估计。这确实可行。它真的有效。
- en: So here's the terminology。 If we use the full data set， that's called batch
    gradient descent。 If we use a smaller data set， some size M， it's called mini
    batch gradient descent because。 usually the subset that uses small。 One of the
    experts in the neural networks community。 Yoshua Bengio， gives some particular，
    advice。 If he says something， you should trust it。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是术语。如果我们使用完整的数据集，那叫做批量梯度下降。如果我们使用一个较小的数据集，大小为M，那叫做小批量梯度下降，因为通常子集使用较小的。神经网络领域的一位专家，约书亚·本吉奥，给出了一些特别的建议。如果他说了什么，你应该相信。
- en: First says M between 1 and a few hundred is the range that one considers。 He
    says 32 is a good default value。 The advantage of making it a little bigger than
    1， like 10。 is that computationally you， can get some advantages。 It's kind of
    analogous to vectorizing your code。 Things go faster when you do multiple things
    at once。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，M的范围在1到几百之间，这是通常考虑的范围。他说32是一个很好的默认值。将它稍微调大一点，例如10，是有计算上的优势的。这有点像是对你的代码进行矢量化。当你一次做多个事情时，事情会变得更快。
- en: So in the same vein， having a batch size that's not quite 1， maybe like 10 or
    20 or， 20。 you can get some speedups。 And finally， stochastic gradient descent
    is with mini batch size 1。 And in that case， you literally use a single point
    and choose your step direction using， that。 So these are all much more computationally
    efficient， especially in the situation of large data。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在同样的思路下，设置一个批量大小不为1，可能是10或20，或者是20，你可以获得一些加速效果。最后，随机梯度下降是使用最小批量大小为1。在这种情况下，你实际上使用一个单独的点，并通过它选择你的步长方向。因此，这些方法在计算上更高效，特别是在大数据的情况下。
- en: And this is definitely the go-to method in the neural network community。 So
    I think in general。 it's highly recommended stochastic gradient descent or mini
    batch gradient， descent。 Here's the algorithm。 It's exactly what you'd expect。
    Choose M points。 You could do them it randomly。 You could break them up in chunks
    and iterate through chunks of batches of size M。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对是神经网络社区中的首选方法。所以我认为一般来说，强烈推荐使用随机梯度下降或小批量梯度下降。这里是算法，正是你所预期的。选择M个点。你可以随机选择它们，或者把它们分成几个批次，按批次大小为M进行迭代。
- en: You compute your average loss over those M points。 The average gradient of loss
    over those M points。 You step in the negative gradient direction。 And finally，
    here too。 we have this extra question of what should the step size be。 It's a
    little bit more complicated。 a little bit more awkward， I'm going to say， for
    stochastic， gradient descent。
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你计算这M个点的平均损失。这些M个点的平均梯度。然后你沿着负梯度方向更新步长。最后，这里也有一个额外的问题，那就是步长应该设多大。这对于随机梯度下降来说要稍微复杂一点，稍微有点尴尬。
- en: And the mini batch or stochastic gradient descent。 And the reason is because
    I think although on the practical side， people have recipes。 that work very well。
    So they have， you can see I have some links to Leon Bow 2 was another kind of
    king of neural。 networks。 He has some nice recommendations on how to set your
    step size for stochastic gradient descent。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 至于小批量或随机梯度下降，原因在于我认为，尽管在实践中，人们有一些效果非常好的方法。你可以看到我这里有一些链接指向 Leon Bow 2，他是神经网络领域的另一位大师。他也有一些关于如何设置随机梯度下降步长的好建议。
- en: Yoshua Benjio has his recommendations。 And why am I worried about this？
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Yoshua Benjio 有他的推荐方法。那我为什么担心这个呢？
- en: Because it still feels very much like an art or a magic or there's these tricks
    to set。 that step size。 And on the research side， the theory side， the theorem。
    what can we know conclusively about， various step sizes。 It's lagging behind the
    practice。 It's a very current area of research。 So maybe we'll post some papers
    later this semester that have a nice theorem that gives。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它仍然感觉像一种艺术或魔法，或者有一些技巧来设置步长。而在研究方面，理论方面，定理方面，我们能确定地知道什么关于各种步长的结论呢？它落后于实践。这是一个非常前沿的研究领域。所以也许我们会在这个学期稍后发布一些有漂亮定理的论文，给出一些结论。
- en: some guidance， some justification for how to set that step size over time。 But
    just to give you something， here's some classical conditions called robins Monroe，
    conditions。 which are necessary in sufficient conditions for convergence in certain
    situations。 And it's that the step size at time t， right， 8th step t， should simultaneously
    satisfy。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些指导原则，有一些关于如何随时间调整步长的理由。但为了给你一些帮助，这里有一些经典的条件，称为 Robins Monroe 条件。它们是在某些情况下收敛的必要且充分条件。即在时间t，正确的第t步，步长应同时满足。
- en: these two constraints。 One is that it be square summable and that it not be
    summable so that it's a virgin's。 infinity。 And you might remember from your analysis
    that this basically includes things that decrease。 like 1 over t， but it's got
    to be faster than 1 over square root of t。 So this is a recipe for setting your
    step size over time。 It's got some justification to it。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个约束条件。一个是步长必须是平方可和的，且不可和，所以它不是逐渐趋向无穷大。你可能还记得你的分析课程中，这基本上包括那些像 1/t 这样的下降方式，但它必须比1/√t下降得更快。这是一个随时间调整步长的设置方法，已经有一定的理论依据了。
- en: It's not quite what people actually do in practice， but it's something to go
    on。 Here's another link to a paper that has some nice recipes called them for
    your step size。 Any questions on this stuff？ On this stochastic gradient descent
    or the gradient descent？ Yeah。 [INAUDIBLE]， OK。 So the question is basically，
    do we have some theorem that stochastic gradient descent works？
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这不完全是人们在实际操作中所做的，但它至少是一个方向。这里还有另一篇论文的链接，其中有一些不错的建议，称为步长的设置技巧。对这些内容有什么问题吗？关于随机梯度下降或梯度下降？是的。[INAUDIBLE]，好的。所以问题基本上是，是否有某个定理证明随机梯度下降是有效的？
- en: OK。 So let's say we're in the convex case。 We haven't talked about that yet。
    but let's suppose our objective function is convex。 Then yes。 if we have these
    conditions on step size。 Yes？ [INAUDIBLE]， I didn't hear it。 Mini batch。 What's
    the difference between mini batch and？ [INAUDIBLE]， Oh， stochastic gradient descent。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 好。那么假设我们处于凸问题的情况下。我们还没有讨论这一点，但假设我们的目标函数是凸的。那么是的，如果我们对步长有这些条件，是的？[听不清]，我没听清。小批量。小批量和？[听不清]，哦，随机梯度下降。
- en: as I've defined it， is a special case of mini batch。 And the mini batch size
    is exactly one。 Yeah。 Yes？ [INAUDIBLE]， How can we actually draw samples from
    all the data？ [INAUDIBLE]， OK。 How do we actually draw the samples from the--
    so you have a data set。 It's on your computer。 [INAUDIBLE]， Oh， yeah。 So that's
    a good question。 So do we have to follow some distribution on how we pull the
    data？
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所定义的，这是小批量的一种特殊情况。并且小批量的大小恰好是1。是的。是吗？[听不清]，我们怎么从所有数据中抽取样本？[听不清]，好。我们到底是怎么从——所以你有一个数据集。它在你的计算机上。[听不清]，哦，是的。这是个好问题。那么我们是不是需要遵循某种分布来拉取数据？
- en: So a lot of the theory-- so the classical theory assumes that you choose one
    data point， at a time。 uniformly at random from the entire data set。 OK。 In practice。
    what people sometimes do is shuffle the data once and then cycle through it。 Recent
    results。 even some theory-- well， I don't know about the theory yet。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 所以很多理论——经典理论假设你每次从整个数据集中均匀随机选择一个数据点。好。实际上，人们有时会先洗牌一次数据，然后再遍历它。最近的结果，甚至一些理论——嗯，我还不知道理论的部分。
- en: It suggests that maybe it is a benefit to reshuffle after every pass， although
    I think in practice。 people don't usually do that。 I think people-- again， people
    have different recipes there。 Anything else？ Yeah？ [INAUDIBLE]， OK。 So the question
    is， can we apply this to a streaming setting？
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 它表明，也许在每次遍历后重新洗牌是有益的，尽管我认为在实际操作中，人们通常不会这么做。我认为人们——再次强调，人们在这方面有不同的方法。还有别的什么问题吗？嗯？[听不清]，好。那么问题是，我们能否将其应用于流式设置？
- en: So in other words， you don't have a batch of data sitting on your computer that
    you'd。 like to use to choose your function。 You have a constant stream of new
    data coming in。 OK。 So if you're in the situation that your system is not changing，
    so the data generating distribution。 is the same， then I think there's no nuances。
    The stochastic gradient descent algorithm essentially is assuming that you're
    drawn-- you。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你没有一批数据在你的计算机上，供你选择函数时使用。你有一个持续不断的新的数据流进来。好。所以如果你处于数据生成分布不变的情况，那么我认为就没有什么特别之处。随机梯度下降算法本质上假设你是从——你。
- en: could assume you're basically drawing from the data generating distribution。
    So that's fine。 Yeah。 It's when-- if there's drift to the distribution， that's
    when it brings in kind of other issues。 Some more questions？ OK。 Let's-- let's
    see a time。 Now。 let's end a few minutes earlier today and we'll pick up with
    risk decomposition on。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 可以假设你基本上是从数据生成分布中抽样。所以这没问题。是的。如果分布发生了漂移，那就会引发其他问题。还有其他问题吗？好。让我们——我们看下时间。现在，让我们今天提前几分钟结束，我们将从风险分解开始。
- en: the next Wednesday。 Thanks， Asim。 [INAUDIBLE]， Thanks， Asim。 [INAUDIBLE]， [BLANK_AUDIO]。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下周三见。谢谢，Asim。[听不清]，谢谢，Asim。[听不清]，[空白音频]。
