# P5：5.Feb_10_Lec__YouTube - Tesra-AI不错哟 - BV1aJ411y7p7

今天的主要话题是支持向量机。

![](img/699ee5803e89d940b0d5771a1d623770_1.png)

![](img/699ee5803e89d940b0d5771a1d623770_2.png)

但在这个过程中，我们还会讨论一些其他内容，我们会谈论损失函数。今天，我们将首次讨论不仅仅是平方损失的损失函数，对吧？不仅仅是回归损失。我们还将进行一个几乎自成一体的单元，关于凸优化和拉格朗日对偶性，算是一个非常入门的单元。

这些工具本身非常有用。它们将使你能够接触到更多高级的文献和机器学习方法。对于我们的目的，它们将帮助我们深入了解支持向量机本身及其特性。因此，它会是一个关于优化的小讲座，然后我们将在最后讨论支持向量机。

我们将从损失函数的讨论开始。好的，首先我们讨论回归。到目前为止，我们只具体讨论了回归。回归损失函数，没错，它接受一个预测值ŷ和实际值y。这些是实数，就像我们到目前为止所处理的。

大多数回归损失函数都可以用残差来表示。所以残差被定义为y的真实值和你预测的ŷ之间的差异。是的，通常情况下，损失依赖于ŷ和y，但我们也可以将其写成仅依赖于残差的函数。残差就是差异。

让我们思考一下，当你可能不希望回归损失依赖于残差时会是怎样的情况？或者它是否没有捕捉到你想要捕捉的类型的东西？

好的，为了保持进度，假设误差是某种初始百分比，误差。所以如果你预测的是100，但实际值是50，从某种意义上讲，你偏差了一个倍数，即2。若你预测的是10，但实际值是5，那你偏差的也是一个倍数，2。在某些场景下，偏差的倍数，也就是这个比例，才是更重要的。

比绝对值更重要。所以这些是需要记住的事项。在这些情况下，你通常可以对y变量进行变换，比如通过对数变换，或其他方式，尽量将其转回到一种形式，其中损失函数再次只依赖于差异。但我只是想让你们稍微考虑一下这个问题。总的来说，还是有很多可以思考的地方。

回归损失依赖于残差。平方损失就是我们迄今为止使用的损失函数。

![](img/699ee5803e89d940b0d5771a1d623770_4.png)

这无疑是人们通常使用的方法。但平方损失存在一些问题，特别是它在对抗异常值时的鲁棒性。所以有一个替代的损失函数——我们将讨论这两者——叫做绝对损失，或者拉普拉斯损失，或者L1损失。是的。

与我们在Lasso中看到的作为正则化器的L1相同。但在这里我们将其用作损失函数。所以这里有一个小表格。例如，在最左边，我们有一列y帽。这些是我们的预测值。所以在这些例子中，我说我们总是预测0。第二列是y。那是y的真实值。

然后第三列是我们在绝对损失下产生的损失。第四列是平方损失。我想指出的是，当y帽和y之间的差距逐渐增大时，绝对损失的增加——嗯，差异是完全相同的，对吧？

但是平方损失增长得更为剧烈，就像平方一样。那么重点是什么呢？

重点是，如果有一个与我们预测差异很大的单个点——假设真实值是50，而我们预测的是0。虽然我们其他大部分错误都在5的范围内。所以大部分错误都在5附近。但有一个特定的点，我们产生了大约50的绝对误差。它是10倍大。

但是如果我们使用平方损失，我们会被惩罚100倍，25到100。所以平方损失对严重的错误给予了很多关注，并且放大了它们。当你计算经验风险，即平均损失时，它会被很重地加权，以试图修正这些极端错误。所以你可能会说。

这是好事，因为你不想有极端错误。但另一方面，偶尔你会遇到数据中的离群点，数据中存在错误。而这可能——平方损失可能会严重影响你最终的预测。所以让我们看一些例子。所以这里我们有这些黑色圆点上的数据点。好的。

我们有两个拟合结果。所以有，我不知道，10个或15个数据点。我们正在做线性回归。红线是使用平方损失的回归。这是你们在作业中进行的回归，我们一直在讨论这个。蓝线是使用L1损失的回归。你会看到红线，平方损失。

这三个离群点把它拉低了很多。而L1损失则没有受到太大影响。L1损失几乎穿过这些顶部点的中间。但它被离群点稍微拉低了一点，但远没有像平方损失那样影响这么大。

所以这是——如果你处在一个数据中有很多噪声或你担心离群点的情况下，你可能希望使用比L2更鲁棒一点的方法。所以鲁棒性是我们用来描述一个决策——算法受离群点、极端离群点影响的程度的术语。这里有更正式的定义。

这对于我们的目的来说已经足够好了。好了。我们有了L2损失。它并不像我们刚才描述的那样具有鲁棒性。L1损失是鲁棒的，但它不可微分。正如我们之前看到的那样，这有点烦人。旁白，L1损失被称为中位回归。

然后有一个不错的折衷方法，称为Huber损失。我只想提一下。抱歉，接近零时的损失，我们使用的是二次损失。它是二次损失和线性损失的混合体。随着残差变大，惩罚变为线性。所以我们并不会因为大错误而使用二次损失进行惩罚。

接近零时，残差较小，我们确实有一个二次惩罚。总的来说，它是平滑的，因为它是一个很好的混合体。所以它是可微的。因此，Huber损失看起来是双赢的。它并不常用，我认为，除非你处在一个非常关注鲁棒性问题的场景中。差不多就是我对回归损失的所有看法了。

我在实践中不太知道有其他损失函数被广泛使用。有一种叫做Epsilon敏感损失的回归损失。但我认为在大多数情况下，人们无需超出这些回归损失函数。接下来我们来说分类问题。这是最初被认为是学习问题的那种情况。

这张图片里面有动物吗？有还是没有？是与否？分类。那么我们就像往常一样设置它。我们的动作空间——最初，我们将动作空间设置为负1和1，表示两类。输出空间同样是负1和1。最自然的损失函数是0-1损失。

我们的预测正确与否？我们的预测 f(x) 是否等于 y？如果不等，我们得到损失为1，否则损失为0。好吧。这里有个不错的概念。我们稍微放宽一下。我们允许自己预测一个实数，而不仅仅是负1或1。为什么——你知道，这是几个因素的结合。

比如逻辑回归或支持向量机。一般来说，我们可以使用实数来简单地编码一个类别，方法是看它是正数还是负数。所以如果我们预测一个大于0的数字，这就等同于预测1；如果小于0，就是预测-1。所以这是我们给自己的一些放松。现在。

所以现在我们的动作空间是实数，我们可以引入一些情况。那么在分类的情况下，我们预测实数值——所以 f(x) 是我们预测的值。f(x) 的值被称为分数。因此，f(x) 也可以被称为分数函数，除了作为决策函数外。

这个想法是，分数越大——我的意思是，从直观上讲，我们希望一个大的分数表示对我们的预测有更多的信心，而接近0的分数表示信心较低。对分数的直观理解。好了，分数给了我们预测，和某种信心，希望如此。然后还有一些其他的东西，今天非常重要。

叫做边距。边距是——嗯，正式来说，它是类 y 乘以 f(x)。记住，y 现在是 +1 或 -1，而 f(x) 是我们的得分。它们的乘积就是边距。那么这里发生了什么？假设真实类别是 1，我们预测是 10，边距就是 10。如果我们预测 100，类别是 1，边距就是 100。现在，如果我们预测错了呢？

这两者都是正确的预测，因为我们的预测和真实类别的符号相同。所以如果我们预测负 10，但真实类别是 1，我们的边距是负 10。我们预测了负 100，而它实际上是 1，边距是负 100。所以你可以看到，边距从一方面告诉我们是否正确。

因为如果我们错了，边距是负的。如果我们对了，边距是正的。它还告诉我们预测的好坏。所以，实际上，大的负数，比如负 10，--嗯，负 100 是一个比负 10 更糟的错误，因为它更加自信地错了。100 的预测比 10 更好。

而且它更自信地正确。所以我们希望有大的正边距。这就是我们的目标。明白了吗？嗯？[听不清]。当我们在 0 和 1 之间做预测时，是的，就是说，当我们在 0 或负 1 和 1 之间做预测时，这不是一个非常自信的预测。它是一个小边距。

它可能——预测本身可能对也可能错，取决于，嗯，是否在 0 的右侧。好了，接下来我们要思考的是如何最大化边距。边距是好的，我认为边距很好。就像回归损失一样，大多数分类损失我们都可以用残差来表示。

可以用这个边距来表示，这非常方便。让我们看看一些例子。所以首先，我们有这个 0，1 损失，我想我之前提到过，这个场景已经证明是计算上不可行的 NP 难题。所以我们不知道如何解决——我们不知道如何最小化带有 0 的经验风险。

即使在简单的假设空间中，1 损失也很难解决。所以——为什么呢？一个原因是，它不是凸的，不可微分，很难处理。它几乎是一个组合优化问题。好了。不过，我们可以从边距的角度来看分类损失。所以我们今天看这些图，所以我会花一些时间在这里解释。

在 x 轴上是边距。记住，正边距是好的。这意味着我们正确地分类了它。负边距是坏的，分类错误。所以 0。1 损失在我们有正边距时不会有惩罚。负边距则会给出惩罚 1。明白了。好吧。这就是我们的 0。

以间隔为基础的1损失。那么现在我们要介绍我们可能实际使用的第一个损失函数，那就是铰链损失或SVM损失。这将是我们今天工作后续的重点。那么，铰链损失——我们先看看图。对于间隔大于或等于1的情况，位于1的右侧。

因此，正确分类并且有一定信心时，间隔大于1，我们完全不惩罚它。合理吧？我们做对了，因此没有惩罚。但当间隔从1减小时，我们会受到惩罚，并且惩罚是线性增加的。当间隔变得越来越差，变得越来越负时，惩罚也会增加。

所以这里看到的好消息是，这个铰链损失是凸的，它是一个凸函数，这将有助于我们进行优化。而且有一个小问题，就是它在1的地方不可微，这有点麻烦，但我们会有办法解决的。好了，这就是铰链损失。让我们再看看几个其他的损失函数。好吧，你们可能听说过另一种损失函数。

逻辑回归的逻辑损失。如果我们把它写成间隔的形式，看起来就是这样。这里的结论是，这也是凸的。它永远不会停止对间隔进行惩罚。所以无论你预测得多么自信和准确，它的损失总是会有一点非零，而铰链损失在零时就停住了。

当间隔变得非常大时，好吗？而且它在任何地方都是可微的，这对优化非常方便。那么，平方损失对于分类来说怎么样呢？

你们有没有听说过有人试图用平方损失进行分类？

有时候人们听到这个消息会觉得，哦，这太糟糕了，简直是最糟糕的事情。那么我们来深入探讨一下，看看它到底有多糟糕。那么，使用平方损失进行分类究竟意味着什么呢？

所以我们仍然预测的是实值，仍然预测的是分数。输出空间仍然是-1和1，就像之前一样。但损失实际上就是平方损失。因此，y将始终是-1或1，而f(x)将是一个实值分数。我们看一下平方差。结果是，这个损失函数。

这是标准的回归损失，你可以把它重新写成间隔损失。你可以自己算一算，这并不难。那么，结果是1减去——它是1减去间隔的平方。让我们看看那是什么样子。平方损失，作为间隔的函数绘制出来。那么这很有趣。当我们往右走得很远时，意味着我们做得非常好。

正确地纠正，充满信心。平方损失正在惩罚我们。即使我们做得很好，平方损失也会惩罚我们。当我们做得很糟糕时，它也会非常积极地惩罚我们。所以我更关心的是，左边的部分，惩罚我们在错误上的表现非常积极。

因为这与回归中的平方损失类似。这导致了缺乏稳健性。如果你的训练数据中有一个类别标签错误，比如——而且这个错误是一个非常明显的例子。而分类器正确地预测了一个正类，并且非常有信心地做出了预测，但这个示例被误标记了。

所以它是以大间隔来做出错误的预测。那样的示例在平方损失下会受到很大的惩罚。这会影响最小化过程。它会对我们最终得到的决策函数产生很大的影响，但不会是好的影响。因此，对于分类任务中标签噪声等问题，平方损失可能确实存在问题。

这里有一篇论文引用，详细讨论了这个问题。他们发现，一般来说，当你使用平方损失做分类时，你需要更多的样本。相比之下，使用逻辑回归或支持向量机（SVM）时就不需要那么多。因此，人们对于使用平方损失做分类感到震惊，可能是有理由的。

但我见过更糟的情况。它是有效的。这个想法并不疯狂。好了，有关于损失函数的问题吗？是的。[听不清]，嗯，问题是，为什么这会导致不稳健性？所谓稳健性，指的是一个单一示例究竟能在多大程度上改变结果。也就是说，某个单一示例是这样的。

一个异常值，它被误标记了，或者之类的。它会产生很大的影响，因为如果它特别远离，那种错误会被平方放大。是的？是的？好。这个问题很好。那么有一点，可能是你有很多数据。你希望不仅仅是正确预测类别，而是正确预测类别的概率。所以当你使用逻辑损失时。

有一种方法——我没有在这些幻灯片上——有一种方法可以将分数 f(x) 转换为一个概率，实际上它会收敛到某个特定类别的概率。所以这是一个优点。SVM 并不具备这个特性。所以这将是 SVM 和逻辑回归之间的一个权衡。而实际上，我只给了你们两种。

我支持把它去掉。0，1 不能用，平方，我不建议。到目前为止，就只有 hinge 和 logistic 之间的比较。hinge 损失有一些很好的特性，最终会导致稀疏性，在最终的表示中。但这一点稍后会清楚。是的？[听不清]，[听不清]，所以问题是——是的，已记录。

hinge 损失是 0，1 损失的严格上界。那这是否意味着如果我们只对分类是否正确感兴趣，而不是对逻辑回归感兴趣时，我们应该更倾向于使用 hinge 损失呢？

好的，这对我来说并不明显。我不确定是否有理论朝那个方向发展。这个问题很好。是的？[听不清]，[听不清]，哦，是的。我是——，[听不清]，[听不清]，[听不清]，[听不清]，[听不清]。最后左侧更关心异常值的问题。我猜。

我认为我理解得更多了。但右边呢？[听不清]，[听不清]，[听不清]，[听不清]。是的，那只是我的理解，就是，当--右边的惩罚项，应该会影响到这些勇敢、激进、自信的分数。不过，关于平方损失的一点是，平方损失，尽管需要更多的样本。

也许它确实为你提供了一个评分函数，能够转化为概率。好了，好问题。那么现在让我们进入一些优化理论吧。丹尼，你们做过一些预读或者预思考吗？不错，不错。好了，接下来这些幻灯片有一些--里面有些有趣的内容，你们可以跟着理解，集中精力去思考。

有些问题非常依赖代数，且需要解一些小方程。其实并不难，但在幻灯片上看不清楚。那是你需要拿起笔，自己推导出来的东西。所以我会尽量提醒你们--这是你们必须信任我，或者快速在脑海中解决，或者之后再解决的事情。

还有一些事情，你们应该能够跟得上，我会尽量告诉你们什么是怎样的。所以喝一口咖啡，咱们开始吧。好了，凸优化。为什么要做凸优化呢？首先，从历史上来看，这个区分。

更偏向于线性规划，我们有一个优化问题，具有线性目标和线性约束。这些是我们知道如何解决的，像单纯形算法，你可能听说过。我们能够很好地解决这些问题。然后更难的事情是非线性规划。

有些问题比较简单，有些则比较难。所以那个划分并不太好。而最近，更为合理的划分是，凸问题和非凸问题之间的区别。我们有算法可以解决凸问题，能够达到任意精度和准确度。而非凸问题则是我们还不太知道如何解决的问题。因此，现在更有指导意义的划分是这个。无论如何，解决凸优化的很多理论，导致了很好的算法，这些算法在凸设置中能够很好地工作，然后这些算法就被转移并应用到非凸设置中，通常也能取得不错的效果。

比如随机梯度下降，它应用于--实际上它对凸问题非常有效，并且是可以证明的。用同样的方式，它对非凸问题并不起作用。但在实践中，它仍然能取得足够好的效果，也就是说，它可能无法找到绝对的全局最小值，而且在很多情况下几乎肯定找不到。

但它能找到一个局部最小值，这对于许多问题来说已经足够好了。所以，如果你继续学习凸优化，或者甚至是这门课程，我推荐你参考《凸优化》这本书，作者是 Sport，Abandoned Burger。它有很多细节，非常好写。问题是它有点厚。

它有很多内容需要掌握才能到达我们需要的工具。所以我做了一张小抄，大约 10 到 12 页，其中提取了书中的定义和定理，这些是你需要的，帮助你理解拉格朗日对偶及我们所需的概念。所以你可以看看这些，并根据需要查阅书籍。

这些资料已发布在网站上。书中使用了一些有点奇怪的符号，我只想提醒一下。它们会写一个函数 F，从 Rp 映射到 Rq。意思是 F 从 Rp 的某个子集映射过来。他们称这个子集为 F 的定义域，这是正确的术语。

但我只是提醒你们，它们可能有——F 可能并没有在整个 Rp 上定义，即使它们是这样写的。好了，凸集和凸函数。我提到过凸多次，但我们实际上还没有定义它。现在我们来定义。所以首先，凸函数必须定义在一个凸集上。所以我们从凸集开始。正式地说。

凸集是指，如果你在这个集内选择任何两点，你用一条线连接这两点。这条线就构成了一条线段，而这些线段完全位于集内。所以左边是凸的。右边不是，因为这里的线段出了这个集合。让我连接一下。好了，凸函数，首先。

凸函数必须定义在一个凸集上。那么，图形上来说，它的定义是，如果你绘制一个函数 F 的图像，并且你选择图像上的两个点，你用一条线段连接这两个点。图像位于该线段的上方或下方。所以右边这个图形就不是凸的，因为图像既在上方也在下方，而左边是凸的。

它是凸的，因为对于你选择的任何两个点，正函数都在这两点之间的线段下方。对吧？是吗？θ 在 0 和 1 之间，这就是用数学方式表达连接这个点和那个点的线段。所以如果你计算一下，你会发现所有 θx + (1-θ)y 的集合，随着 θ 在 0 和 1 之间变化，是。

连接它们的点的集合形成一条线段。大致来说，它是一个朝上的碗形。那就是一个凸函数。所以一些凸函数的快速例子。这个叫做仿射函数，ax + b。它像是变量的线性变换加上一些偏移。这是凸的也是凹的。顺便问一下，什么是凹的？我们已经定义了凸函数。凹函数呢？

函数是凹的就是该函数的负值是凸的。所以它像一个倒扣的碗。它是向下的。如何判断指数函数是凸的，x 的绝对值，x 的 p 次方。指数函数是凸的。凸函数的线性组合是凸的。这是真的吗？

这是对还是错的陈述？凸函数的线性组合是凸的，错。因为凸函数的负值是凹函数。所以这不对。不是所有凸函数的负组合都是凸的。就是这样。如果你需要更多规则，可以查看笔记。

用于组合凸函数并从旧的凸函数中生成新的凸函数。好，严格凸性，这也很有用。一个函数是严格凸的，如果该线段严格位于函数本身的上方。那么不严格凸的函数可能就是一条直线，或者——是的，基本上如果函数有线性部分的话，它就不是严格凸的。

那么凸性对于我们在优化中的作用是什么呢？关键点是，在一个凸函数中，如果存在局部最小值，那么这个局部最小值也是全局最小值。所以你不可能进入一个局部最小值，然后在另一个地方找到一个更低的局部最小值。对于凸函数来说这是不可能的。所以这是关键，让我们能够使用像梯度下降这样的算法。

或者梯度下降算法，因为我们可以确保如果我们被困在一个局部最小值中，它也是全局最小值。这就是给我们带来能力的原因，是吗？对。好，问题是，如果我们有一个像这样的函数呢？一条先下降然后平坦再下降的曲线？是这样吗？好，那——哦，关键，不是凸的。

对吗？好。那么，怎么样才能有一个非唯一的全局最小值呢？是吗？好。那么，函数的值，全局最小值的值是相同的，但是它不是在一个唯一的点上达到的。这就是——就是这样。好，我们将展示一种意义上的一般优化问题。

这是一般的优化问题。我们将其写成一个最小化问题。所以最小化 f0（x），受以下约束——我们有一些不等式约束。比如 f（x）小于或等于 0，取决于我们有多少个这样的约束。还有一些等式约束，h i（x）等于 0。我们可以有任意数量的这些约束。

这几乎是我见过的最通用的优化公式。x 叫做优化变量。f0 是目标函数。这只是术语问题。然后这是书中的域问题。我们必须注意到每个函数的定义域。

可能在某个 Rd 的子集上定义，或者其他什么地方。我们需要追踪这些函数定义的位置。在我们的案例中，它们总是定义在每个地方。所以这对我们来说并不是问题。学习它并不是——学习它，是当你读书的时候。好。接下来是一些术语。满足约束条件的点的集合叫做可行集。

所以任何满足所有这些不等式和等式的x，构成了可行解集。满足这些条件的一个特定点，叫做可行点。这里有一个关于活跃约束和非活跃约束的概念。现在这很有意思。对于这些不等式约束，我们可能会有一个x，使得f(x)严格小于0。

当它实际上等于0时，它就在边界上。所以如果f(i)(x)等于0，那叫做活跃约束。我们就处在那个约束集的边缘。这对我们后面会有一些影响。好的。

优化问题的最优值是目标函数在可行集上取得的最小值，而不是在x集合中取得的最小值。x星是最优点。如果目标函数q的值为其最小值，那么它在x星处的值就是最优值。好了，我想让事情变得稍微简单一些。

所以我声称我们实际上可以去掉等式约束，基于我们的需求。所以我们有h(x) = 0，这可能是一个不等式约束。实际上，你可以将它写成两个不等式约束：h(x) ≥ 0 和 h(x) ≤ 0。所以我们可以去掉等式约束。

通过将其重写为两个不等式约束。好的。好的。到目前为止怎么样？是的。[听不清]，嗯，这是个好问题。是的，这是个好点子。这个——严格不等式确实有点尴尬，但它不会导致你无法获得最优解。所以在这种情况下，你真的需要依赖于即时的最优性。

好的。我不知道。是的。好问题。好。你们有多少人听说过拉格朗日？

那拉格朗日乘子呢？好。好的。这是那种理论的一部分。好了，这里我们现在有了一般形式，但现在仅仅是优化问题中的不等式部分。我们像这样找到优化问题的拉格朗日函数。你有你的目标函数f0x。然后我们有这个线性组合，包含了f(x)函数和f约束函数。

其中这些在拉格朗日线性组合中的变量是引入的lambda i，这些叫做拉格朗日乘子。所以我们为每个给我们约束的函数引入一个新的拉格朗日乘子lambda i。所以这是约束函数和新变量（称为拉格朗日乘子）之间的配对。以后它们将被称为对偶变量。太棒了。

好的。所以这里有一些我觉得关于拉格朗日非常令人惊讶的地方——它将整个问题编码进了这个函数。那么这是如何工作的呢？好的。我声称，如果你对拉格朗日函数的拉格朗日变量取上确界的话。

我们将得到的结果基本上告诉我们原始优化问题中的所有内容。那么让我们快速看一下。假设有两种情况。x 要么在可行集合中，要么不在可行集合中。让我们首先看看当 x 在可行集合中时会发生什么。

那么当 x 在可行集合中时，f(i, x) 会发生什么？对吧？在这个设置下，我们所有的约束都是 f(i, x) ≤ 0 的形式。所以如果 x 是可行的，那么 f(x) 就是小于或等于 0。现在我们在对 lambda 取上确界。所以如果 f(x) 是负的，lambda 的结果...

这个符号表示我们正在对向量 lambda 取上确界。lambda 向量 L1 到 Lm。这意味着向量 lambda 的每个条目必须大于或等于 0。这就是符号表示。所以如果 f(x) 是负的，我们可以推动整个表达式。当 lambda i 增大时，结果只会变小。

因为如果 f(x) 是负的，我们希望让整个表达式尽可能大。所以如果 f(x) 是负的，最好的做法就是将 lambda i 设为 0。我的意思是，最好的做法是我们正在尝试弄清楚上确界是什么。所以在 f(x) 为负时，我们可以通过将 lambda i 设为 0 来使这个函数增大。

好的。如果 x 满足所有约束，那么我们将把所有这些 lambda i 设置为 0。然后整个求和结果将是 0。我们将得到 f(0, x) 对于 lambda 大于或等于 0 的上确界。好的，总结一下，如果 x 是可行的，那么上确界就是返回我们原来的结果。

目标函数 f(x)。有趣的是，另一方面，如果 x 不是可行的，那么至少有一个 f(x) 是正的，如果 f(x) 是正的，我们可以让对应的 lambda 变得任意大，趋向于无穷大。这样会使得这个表达式趋向于无穷大。因此，上确界确实是无穷大。

如果 x 不在可行集合中。好的，从某种意义上讲，我们通过让上确界返回无穷大来编码 x 不可行。这是一个相当巧妙的方法。我们可以用这个来改写我们的目标函数。所以这被称为优化问题的原始形式。

我们在 lambda 上取这个上确界。所以这个函数，现在我们对 x 取下确界。当然，无穷大永远不会是 x 的下确界。所以我们总是选择一个可行的 x，因为否则我们会得到无穷大。如果 x 是可行的，那么就是上确界。

这只是给我们 f0(x)。所以这在某种程度上等同于在 x 可行的情况下对目标函数取下确界。所以拉格朗日函数给我们另一种写同样优化问题的方法。在这个上确界的最小化或最大化的形式下，这是一个有趣的重写。在这种形式下，引入新内容非常容易。

这是对偶优化问题的概念。所以拉格朗日对偶问题是这样的，我们取相同的拉格朗日函数，然后交换下确界和上确界，交换下确界和超极限。令人惊讶的是，在某些情况下，这两个解是相等的。还有一个很酷的地方——我们将在这里证明这一点。

这非常简单。所以这是你应该跟着做的——在所有情况下，p*大于或等于d*。所以拉格朗日对偶问题的解，交换下确界和上确界后的对偶问题，始终小于或等于原始问题，也就是原始问题。做优化时，这有一定的用途。

这样你就可以看到，也许对偶问题比原始问题更容易解决。所以它为你提供了目标，因为你知道你的原始最小值永远不会小于你的对偶最大值。这个对偶问题实际上是一个前置的最大化问题，超极限值。所以没关系。接下来我们来做这个证明。

它被称为弱极大极小不等式。非常简单，但很有趣。所以，w和z的一个函数的上确界，总是小于或等于下确界2。证明它。好了，让我们选择任意的w0和w，以及z0和z。我们从中间开始。如果w0的f值是a0，那就往右走。显然fw0，z0小于或等于fw0，z。

其中z被允许在整个集合z上取值。显然，对于所有z的上确界，代入那个函数后，结果会大于某个特定的z0。在同一位置上，所以上边的不等式是显然的。下边的不等式也是一样的。在整个w的f函数中，显然会小于某个特定的w和z0的f值。好了。所以我们夹住了fw0。

z0介于下确界2之间。现在这个主张适用于每个w和每个z。所以特别地，它将在左边所有z的上确界和右边所有w的下确界之间成立。现在我们已经完成了证明。[旁白]。所以我们刚刚证明了所谓的弱对偶性。这个适用于拉格朗日。

我们已经证明了对偶，假设是拉格朗日对偶，它的值就是2n，少于或等于n。接下来要实现的目标，或者说我们接下来想要做的事情，是找到这些值相等的情况。而且有一个定理，在非常轻的条件下，能够保证这个等式成立。

我们将改进那个定理，但我们将在此基础上使用它。所以弱对偶性，我们已经证明过了。太棒了。到目前为止，我们还没有使用凸性。所有这些都是针对一般的优化问题。对于凸性问题，我们通常会有强对偶性。我们只需要一些小的条件。

要获得强对偶性。所以我们将在接下来的几张幻灯片中讨论这个问题。让我们深入了解这个拉格朗日对偶问题。这里再次展示了你的拉格朗日函数和下确界。我们将引入一些新的术语。这个中间的函数，即拉格朗日的下确界。

它被称为拉格朗日对偶函数。我们通常用G来表示它，或者今天我们将用G来表示它。那么它是关于什么的函数呢？L是x和lambda的函数，但我们对x求下确界。所以剩下的就是lambda。因此，拉格朗日对偶函数是lambda的函数，而lambda是我们的拉格朗日乘子。

或者现在我们也将其称为对偶变量，作为对偶函数的自变量。它是拉格朗日函数的下确界。我们必须找到我们的拉格朗日对偶问题。我们有我们的拉格朗日对偶函数。提醒一下，这个对偶函数可以取负无穷的值。什么时候对偶会取负无穷呢？好的。

我这里有一个例子。接下来，对于SVM，我们要特别留意当这个对偶函数达到负无穷时的情况。我们将确保在这种情况下小心处理并进行考虑。所以这是另一种方式来表示弱对偶性，使用拉格朗日对偶的形式。

我提到过，对偶提供了对最优解的下界，因为根据弱对偶性，依然是如此。好的。那么我们做完这一张幻灯片之后，休息一下。拉格朗日对偶问题。我们现在可以写成最大化问题，将其写成对偶函数的形式，约束条件为这些拉格朗日乘子，lambda大于或等于0。

我们说lambda是对偶可行的，如果lambda满足这些约束，并且g不是负无穷。我们有这个对偶问题的对偶最优值或对偶最优拉格朗日乘子。为什么要引入它呢？好吧，在我们的SVM案例中，有两个主要原因。

其中一个原因是对偶问题有一个高效的解法。现在使用得不多了，但在SVM的早期阶段，解决SVM的方式之一就是采用这种方法，应用于对偶形式。所以是类似这样的，问题是最大化问题，而不是最小化问题。今天我们将讨论的内容之一就是。

通过对偶，我们还可以揭示一些关于原问题解的有趣结构，尤其是在强对偶成立时，两个解是相同的。这就是我们今天希望展示的内容。对，问题？当然。

两个问题。这个问题是原始问题吗？

这个问题是否等同于原始问题？不，这个问题正是对偶问题。对偶问题等同于对偶问题。对偶问题的解是一个下界，原始问题的解是弱对偶性。当我们有强对偶性时，解是相同的。

第二个问题是问题的方程式。所以没有明显的原因。嗯。有一点需要注意，约束是简单形式的。最初，约束是任意小于或等于零的函数。现在，约束是这样的。这个是一样的。不是，这些是简单的。这里没有函数。

只是一个大于或等于零的值。所以这是一种更简单的形式，可能。但同样，这不是我们需要走的路线，而是原始问题是否是凸的。对偶问题是凹的。所以我们可以最大化它，并且它有一个唯一的最大值。或者它的任何局部最大值都是全局最大值。所以这是有帮助的。好的。是的。

所以，如果你有一个可能是凸的也可能不是凸的优化问题，对偶将是凹的。你可以优化它，这将给你一个下界。好了，休息10分钟，我们将在10分钟后开始写作。到目前为止，我们讨论的是一般优化问题。

现在没有讨论问题的凸性。现在它开始进入。之所以它会进入，是因为它为我们提供了这种强对偶性，而且条件非常少。所以凸优化问题看起来就像一般问题一样，只是现在的函数f0和约束函数fi是凸函数。好的。所以我只是，你可以稍后再看这个。

不是每个凸函数都有强对偶性。这里有一个例子，我从某人的笔记中提取的。但为了具有强对偶性，凸问题需要一些附加条件。它们被称为约束资格，不知道为什么。现在我们将给出这些非常好的凸问题强对偶性的充分条件。

这叫做斯莱特条件。它比我在这里展示的更为一般。粗略来说，我们需要问题是严格可行的。你觉得严格可行是什么意思？所以，好吧。这意味着有一个点，满足所有这些不等式约束，并且是严格的不等式。

所以对于所有这些东西，它是严格小于零的。所以大致上就是这样。供应和我们所有函数的定义域是开放集。所以在我们的情况下，可能是像R_e之类的东西。所以这是我们的情况。只是提醒你，我们所提到的域不是可行集。

域是定义函数的集合。所以这条粗体线是首先要看的。严格可行性是充分条件。所以如果存在一个x，使得每个不等式约束都被严格满足。即f_i(x)小于零，不等于零。如果在可行集的域中有一个这样的x。

然后我们假设有强对偶性，前提是我们的问题是常见的。也就是斯莱特约束资格条件。如果这些函数f_i是仿射函数，实际上就更容易了。记住，这就像是一个线性函数加上一些偏置项。所以对于任何仿射不等式约束，例如a_xi加b。

我们甚至不需要严格可行性，只要可行性就足够了。所以这实际上就是我们在SVM中遇到的情况。所以我们所有的约束都会是仿射不等式约束。因此，我们只需要证明强对偶性存在，就只需要找到一个可行的单点来满足所有这些约束。

这几乎是显而易见的。好了。这就是我们的强对偶性定理。接下来是我们从这个一般优化问题中需要的另一个理论。它叫做互补松弛性。这个很酷。这是一个小小的证明，值得你跟着做。它相当有趣。

它给了我们很多的见解。应用这个，我们对SVM预测器有了很多的洞察。好了。现在我们回到一般的优化问题。我们不需要凸性来解决这个问题。我们确实需要强对偶性。所以假设有强对偶性。好了。那么我们设——我应该在这里加一个星号。设lambda i星和x星是对应的——就像是看——最优的拉格朗日乘子。

以及原始问题的最优变量。我们要证明的是——lambda i星和约束i的乘积，也就是拉格朗日乘子对应于约束i的部分，在原始最优解x星处的乘积为零。所以实际上，这意味着如果你知道其中一个不为零，那么另一个必定为零。

这基本上就是我们如何使用它的方式。那么我们来证明这个。清楚吗？

记住，我们有很多不等式约束。对于这些不等式约束中的每一个F_i。当我们构造拉格朗日函数时，我们为每个不等式约束分配了一个拉格朗日变量。看一下拉格朗日乘子。这些是拉格朗日乘子lambda i，对应于F_i约束函数。好了。好的。

这是另一个文字解释。所以如果lambda i星不为零，比如说，这个互补条件就告诉我们F_i和x星必须为零。记住，当约束等于零时，这叫做活动约束。好了。所以如果lambda i星不为零，那么我们的约束就是活动的。对吗？

这是一个k星是素数的解吗？素数中的素数。对。x星是素数中的素数的解。所以记住，lambda是对偶问题的解。所以这是我们的拉格朗日对偶问题。我们将其最优解表示为lambda星。对。好了。好的。现在让我们证明这个互补松弛性。

所以我们将F_0 of x star作为原始目标函数。通过强对偶性，我们有它等于g of lambda star。这就是强对偶性的定义。原始最优目标函数等于对偶最优目标函数。很好。这就是强对偶性。现在让我们展开g of lambda star。这只是根据定义。

这个Lagrange，拉格朗日的下确界。现在我们插入了lambda star，最优的lambda star。好。那么接下来呢？对x的下确界，这里发生了什么？

注意，这还不是不等式。它是小于或等于。非常简单。我们有对所有x的下确界，所以这个表达式必须小于或等于在某个特定x处该表达式的值。让我们选择x star。这就是这里发生的事情。对x的下确界肯定小于或等于在某个特定点计算得到的相同表达式。

让我们选择x star，原始最优解。很好。那么，如果F_0 of x star加上这些东西，现在x star作为原始最优解必须是可行的。最优解必须是可行的。如果它是可行的，那么每一个不等式，每个F_i x star都必须小于或等于零，仅仅因为可行性。lambda i的回想是大于或等于零。所以整个表达式...

每一个独立的项都小于或等于零。还没有问题。好了。那么整个表达式必须小于或等于F_0 of x star。如果这些项每个都是负数，而我们把它们加到F_0 of x，那么它肯定会小于F_0 of x star。好的。

所以如果F_0 of x star小于或等于F_0 of x star，我们就得到了这种不等式的夹心结构。这意味着它们必须全部相等。好的。所以这些表达式每个都必须相等，因为它们已经被限制在同样的范围之间。整洁。

所以从这个我们可以得出结论，这些东西每一个必须为零。很好。那么lambda i star F_i x star对于每个i都等于零。这就是我们的互补松弛条件。这是简单的证明，对吧？非常好。有问题吗？

因为这就是我们要说的关于对偶优化的所有内容。接下来我们要将其应用到支持向量机（SVM）。这会有点复杂。但我认为我们能够搞定。大家准备好了吗？好了。有谁做过那个吗？

我之前发给你们的那些练习？我知道你们做了。你们还是想承认吗？那么首先，什么是支持向量机（SVM）？我们知道什么是支持向量机的损失函数。我在一开始讲过。首先，支持向量机。它的标准版本使用的是线性函数或近似线性函数的假设空间。

转置到 x 加 b。正是我们在岭回归和套索回归中使用的，采用相同的假设空间。抱歉，继续，这里是我们的损失函数。这一次，我要提醒大家以这种加号的方式来写它。你们记得那个下标加号吗？或者之前可能有过上标加号？是正部分。如果 1 减去 x 大于等于 0 或 1 减去 m，那就是 1 减去 m。如果小于的话。

如果 1 减去 m 小于 0，这个表达式将会是 0。这就是它的解释。所以这是我们将要观察的损失函数。那么这里是 SVM 的目标函数或 SVM 优化问题。首先，看看右边的部分。这就像是铰链损失的经验风险。

所以我们在中间有这个，w 转置 x，加 x，加 b。那是什么？这是我们的，再来一次？

完全正确。W 转置 x 加 b，这就是 f(x)，对吧。现在我将它乘以 y，那是什么？是间隔，太棒了。做得好。接下来我们有这个 1 减去间隔的正部分。那是什么？是预测的铰链损失。很好。现在我们有这个 c/n 乘以这些预测的铰链损失的总和。

这是 c 乘以我们决策函数的平均铰链损失。好，太好了。你们看到的第一个项是什么？是 L2 正则化。对吧。所以，唯一稍微不同的是我们通常会在 L2 正则化前加一个 lambda，而在这个平均值上，我们只有 1/n。

你们可以看到其实没有什么区别。只是传统上在 SVM 中，我们会将 c 和经验风险部分一起写，而 L2 正则化部分什么都不加。没有什么大的变化，只是它传统的写法。所以这是我们的目标函数。偏置项有正则化吗？没有。我们有 B，它没有正则化，就像我们在岭回归中一样。好，这就是我们的目标函数。

这是一个无约束优化，挺不错的。它不可微，没那么好。我们将通过几个步骤将其转化为更适合我们一直在开发的这些技术的形式。让我们看看我们能做些什么。这是一个常见的技巧。所以我们引入了一个新变量。我们做的就是引入了一个新变量。

你们知道这个字母吗？这个希腊字母？zai。好吧，你们想说 sai 吗？

我说 zai。好吧，没问题，我说 sai。好了。所以我们引入了一个 sai。我们说 sai 必须大于或等于我们预测的铰链损失。当我们以这种形式写出来时，意味着我们正在对每个我们不知道的变量进行最小化。所以我们不知道 w，现在我们也不知道 sai。

这个最小化问题是关于 w 和一个新的变量 sai。所以我声称这是等价于之前的问题。你们看得出来吗？为什么会这样？

好吧，我声称在最优时，sai 实际上会等于这个值。为什么？

如果 \( \text{ sai} \) 大于它，超过了这个最小可能值。那么我们会增加总和，而我们试图最小化它。所以这个最小值将 \( \text{ sai} \) 降到不等式所允许的最低值。好的。是的。很棒。那么，这是一个等效的公式，现在我们再做一步。没什么大不了的。

我们把这个问题分成两个不等式。我们说，这是一个正部分。所以它总是大于或等于零。所以事实证明，这个不等式等同于 \( \text{ sai} \geq 0 \)，而且它大于或等于这个没有正部分的部分。看，很多人点头了。那么，我就当这是足够了。所以这些是等效的。如果你慢慢来。

我想你会相信自己这其实是很简单的，因为这些是等效的。好的。所以这是一个重新公式化。抱歉，看起来这些优化问题和我们以前看的有些不同。它有点不同，因为我们把所有东西都限制为小于或等于零。

所以这个可以轻松地重新排列。现在我们有了这些小于或等于零的不等式。这是我们的标准形式。而 \( \text{ sai bar} \)，这叫做二次规划。我之前提到过。二次规划是指具有二次目标、凸二次目标以及线性约束的问题。很多软件是为了解决二次规划问题而编写的。

所以从这一点开始，我们可以将其输入到软件中并利用通用二次规划求解器得到一个解。这并不总是最好的情况。我所声明的是，通过取对偶，如果我们将其视为原问题，我们将采用对偶优化问题。我们将检查其解，并且我们将获得很多关于这个问题解的洞察。

原始的原问题。好吧。那么接下来的 10 张幻灯片的目标是推导这个对偶公式。是的。好的。那么问题是，我一直在强调它不是一个可微分的函数。关键是，是的，实际上有一些通用的方法来优化不可微分的函数，只要它们足够“好”。有子梯度法。这是对的。没错。事实上。

在作业中，我们将会有一个，Pegasos，这大概是几年前解决支持向量机（SVM）问题最好的方法。好吧。那么这是我们的 SVM。是的。哦。有什么问题吗？[听不清]，不。我是说，原问题就是你开始时的问题。任何问题都可以是原问题。然后你取对偶，那就是对偶问题。

所以没有其他要找的东西。我们第一个问题是原问题。是的。有什么问题吗？好的。这是再次出现的情况。那么，我们首先要做的事情是什么？写下我们的拉格朗日函数？

所以让我们为每个约束做一个小表格。这是拉格朗日乘子。它将与之对应。我还把它写在黑板上，以便在这张幻灯片消失后快速参考。好了。那么这个更复杂的约束与 alpha i 和负的 psi i 对应。假设它与 lambda i 对应。好了。好了。

那么这些是——我们怎么得到我们的拉格朗日函数？我们只需要取目标函数，然后加上拉格朗日乘子和约束函数的乘积。对。这里是我们的目标函数，这里是拉格朗日乘子约束函数的和。

拉格朗日乘子约束函数。这就是我们的拉格朗日函数。好了。好吧。我们稍微整理一下。我们有这些 psi i 分布在三个地方。做一点因式分解，并将它们收集在一起，整齐地分组。让我们来回顾一下。这是原始问题和对偶问题？原始问题可以写成所有原始变量的下确界。

你一开始的 w 和 b，psi i 是我们引入的新的原始变量。然后在内层是这些新的拉格朗日乘子的上确界。好了。这就是我们的信息设置。然后对偶问题将是——拉格朗日的最终结果。好了。我们将从弱对偶性开始。所以它是一个不等式。

但我们想要证明的是它们相等。那么我们需要证明什么才能得到强对偶性呢？是的。所以如果我们有强对偶性，那么它们是相等的。没错。那么首先，这是不是一个凸的目标函数？这是关于 w 和 psi i 的凸函数吗？没错。这里的 w i 的平方和，肯定是凸的。而 psi i 的这个线性项。

这是凸的。它是二次的。非常简单。好吧。这些是仿射约束。所以它肯定是凸的。它当然是一个凸优化问题。好了。很好。所以它是凸的。那么我们的约束怎么样？它们是仿射的。对吗？

对于仿射约束，斯莱特条件几乎没什么要求。你只需要找到一个可行的点。好了。好了。那么这是我们的挑战。我们能找到一个可行的点吗？好了。这是我们的约束。我们需要找到下一个最符合这些约束的点。

这就足以保证在这种情况下的强对偶性了。你怎么看？尽量简化。所以开始代入值，或者我们需要找到 w、b 和 psi i 的值，并且遵守约束条件。将 psi i 设置为零。好吧。嗯，我不知道。这可能有效。但另一个方向，由于时间紧迫，我知道它会奏效。

如果你将 w 和 b 设为零，怎么样？哦，也许是大小？稍等。好吧。如果 psi 为 1，只有第一个条件满足。w 和 b 为零。这是零的问题。我们有 1 减去 psi。让我们把零设置为零。所以如果 psi 为 1，w 和 b 为零。它是可行的。我们不需要严格可行性。我们只需要这个。

所以你稍后会自己说服自己，这确实是一个可行的点。这意味着强对偶性成立。所以拉格朗日的下确界等于该式的下确界。稍后我们还会使用互补松弛性。因为强对偶性，所以这将在稍后返回。好了，好的。

所以强对偶性意味着，如果我们制定了对偶问题，它将有相同的解，并且在某种意义上，解决对偶问题和解决原问题一样好。好了。那么我们来找对偶问题。对偶函数，那部分内层最小化的下确界，就像这样。展开一下。

没有什么新的进展。我们只是在将之前写下的拉格朗日函数代入进去。好了。所以我们正在求取下列表达式的下确界，这个表达式也是凸的。我们需要在 w、v 和 psi 上保持凸性。它，和之前一样，本质上是二次的，还有线性项。所以它是凸的。它是可微的。所以现在我们回到了基础的微积分。

如果你想取下确界，求一个可微函数的最小值，你对它求导并设为零。然后我们将对所有的 w、v 和所有的 psi i 求偏导数。我们将它们都设为零，这将给我们这个最小值存在的必要和充分条件。好的。

记得我说过这个东西可能是负无穷吗？对偶函数可能是负无穷。那么什么时候会变成负无穷呢？

那么下确界会变成负无穷。那么假设这一项不等于零。c o m - alpha i - lambda i。如果这不等于零，那么当我们对 psi i 取下确界时，我们可以让 psi 趋向负无穷。如果这是正的。如果这个表达式是负的，我们可以让 psi i 趋向正无穷，并且在这两种情况下。

这个表达式的下确界会变成负无穷。类似的情况，嗯，我们稍后会看到。好的。是吗？ -Sai i 在进入案例之前不能动。 -Sai i。 -约束，呃。 psi i 不等于那个东西。那是一个约束，但是它在原问题中并不是约束条件，Sai i 在原问题中被约束为大于等于零。

但是这是一个无约束最小化问题。记住，约束条件已经在拉格朗日函数中表示了。所以这些是无约束的。现在唯一受约束的是 lambda 和 alpha。对偶变量被约束为非负数。

但是现在原始变量没有约束了。实际上，这正是转到对偶形式时的一个优点。约束条件被吸收到拉格朗日函数中了。好，好的。那么这里发生的事情是我们在对这些部分导数求解。这正是你需要做的类型，自己去做一遍。

而且与其我在这里讲解，实际上是每个偏导数等于零时，我们会得到这个结果。我们得到了不同的表达式，这些表达式是我们要求解最小值时所需要的条件。所以首先得到的结果是w的表达式，这是我们要优化的原始变量。它是αi（对偶变量）和一些数据的线性组合。

这将在后续非常有用。请继续关注。这就是我们得到SVM解的形式。这个是我们从对偶问题推导出的有趣部分之一，但我们会在后面回到这里。然后这些约束条件，要求所有的αi之和为零。

并且那个αi加上λ等于c除以m。好的，太好了。那么一旦你找到了最优性条件，接下来该怎么做呢？还记得微积分中的内容吗？

你可以将它们代回去看看会发生什么。你只需要这样做。所以当我们将这些条件代回到这个表达式中时，我将在这张幻灯片中指出一些关键点。以最底下的那个为例。αi加上λi再加上c除以n。好吧，在这个中间项中，第二项，如果αi和λi等于c除以n，这整个中间项就是零。

好的。所以当这个中间项消失时，我就不需要了。好吧，接下来看看其他的。当我们代入第一个项时，这个关于w的表达式展开后是这个看起来比较复杂的形式，我们稍后会逐步解析。第三项展开并稍微简化，最后把所有项放在一起。

我们得到了这个对偶函数的表达式，它在某些约束条件下成立。那些约束条件是我们必须满足的两个条件，才能得到最小值。所以当这两个约束条件满足时，我们得到实际的最小值，以及通过将条件代入大系统中得到的值。否则，我们就没有最小值。

我们可以通过检查得出，例如它会趋向负无穷。[听不清]，不是的，c进入了约束条件。我们来看看。然后它就被应用，因为我是最后一个。让我再看一眼。好的，明白了。现在我们有了对偶函数，接下来可以开始写出表达式了。我们把它重新写出来。那么对偶问题是什么？

对偶问题是我们对拉格朗日乘子、对偶变量的对偶函数取上确界。所以对α的上确界是什么？我们在α和λ上已经完成的条件是什么？[听不清]，好的。我这次把它们写在右下角，而不是在那边。好的，明白了。

这是我们的对偶函数。这里是我们的约束条件。我省略了当这些约束条件不满足时的情况，因为在那种情况下，对偶目标是负无穷，而这显然不可能是上确界。所以我将条件约束为这些，因为否则无论如何都不可能得到上确界。

所以这些 supremum 就是这样得出来的。条件也是这样得出来的。所以这是对偶问题。这就像是 SVM 对偶问题的第一轮。我们会稍微调整一下，但这是——我们已经得到了它。这就是对偶 SVM 问题。我们可以消除一个变量。注意，这个 lambda i 在目标函数中完全没有出现。

它仅仅出现在约束条件中。结果证明你有这个 alpha i 加上 lambda i，再加上 c 除以 n。alpha i 和 lambda i 都不是负数。结果你可以消除 lambda i，仍然得到相同的 alpha i 集合。alpha i 只是——你只会发现它出现在目标函数中，无论如何。

所以你可以确认，这等同于没有 lambda 的重新表述。现在 alpha i 被限制在 0 到 c 除以 n 之间。简单得多。所以每个 alpha i 必须是——这叫做盒子约束。每个 alpha i 都在一个区间内。从 0 到 c 除以 n。那么这个对偶问题有什么特别之处？

好的。这是二次方程。我们有更多的未知数。所以可能有更多的未知数。我们从原始变量的维度开始。假设是 d 维特征空间。所以我们的原始变量是 w，是 r^d。然后我们有 v 作为截距。所以是 d 加一。这是我们开始时的设置。然后我们引入了 psi i's。但最初是 d 加一。

现在对偶问题有 n 个未知数。每个数据点一个。每个数据点引出了一个约束。好的。所以对偶问题有一个很好的最小化方法，我们现在不会讨论。可能是一个作业问题。它叫做 SML（Sequential Minimal Optimization）。剩下要做的是看看从中能得到什么奖励。

推导这个对偶问题，我们能从这个对偶形式中学到什么关于我们 SVM 问题的解？好的。那么，首先，原始解的形式是什么？

我们在过程中看到了这一点，对吧？所以从这个特定的导数中，我们发现 w 等于 alpha i 的和，乘以对偶变量 yi 和 xi。所以特别地，如果 alpha star 是对偶问题的解，那么原始解是 w star 等于 alpha i star 乘 yi 和 xi。那么我们从中学到了什么？首先，w star 的形式，也就是我们原始问题的形式。

SVM 问题的解是我们输入点 xi 的线性组合。这有点有趣。我不确定我们以前是否见过类似的东西。所以举个例子，仅从这一点，我们可以重新表述我们的原始问题，而不是找到整体 w 的最小值。我们可以，如果愿意，写出整体线性组合的最小值。

xi，因为我们知道仅从这个式子来看，这就是我们的 w star 必须采取的形式。好，还有什么？

所以我们来看这些alpha i's。记住，alpha i's在对偶问题中被限定在0到c除以n之间，对吧？这就是这样。那么现在我们看到的是，c在某种程度上控制了任何单一数据点xi的最大权重。这很有趣。这涉及到一定的鲁棒性，对吧？任何单一的点都不能以过大的权重进入w star的解中。

大于c除以n的部分。所以这是可以调整的。好的。那么，b star是什么？

所以记住，我们需要w star和b star。找到b star是另一个完整的过程，我们可能会有时间在最后进行讨论，也许没有时间，我们再说。好的。现在我们继续。所以我们有了w star的这个形式。让我们来谈谈这个名为支持向量的概念，这也许是支持向量机名字的由来。

所以w star是这个线性组合。alpha i's在0和c除以n之间。值得注意的是，不仅可以是0，而且alpha i star等于0时，对应的xi根本不会出现在w star的表达式中。最终你会得到一种稀疏的表示。

w star。现在我们讨论的是稀疏性。我们所说的稀疏性不是指特征的稀疏性，而是指你只需要对少量不同类型的东西进行线性组合。那么，什么是支持向量？支持向量是那些在你去掉所有系数为0的部分后，剩下的部分，系数为0的就是pi star 0。所以支持向量就是那些实际出现在其中的xi。

这个线性展开包含了pi star不等于0的部分。这些就是支持向量。明天将在TI活动中展示支持向量机，它是基于几何学启发的，更加几何化的。在那里，支持向量将有一种几何意义的解释，形状类似于hell形。好的。

好的。那么，现在是最后的部分。我们将谈论如何将边距和支持向量结合起来。我们将使用互补松弛性来获取关于这些内容如何结合在一起的各种洞见。好的。

那么让我们带回F，对吧？F star，假设F star是我们的决策函数。它包含了w star和b star。让我们回顾一下这个图像。所以记住，任何位于零右侧的部分，如果我们的边距是位于零右侧的。首先，边距是什么？边距是，回想一下它是y。

它是实际的分类y乘以F star，对吧？就是y乘以预测值。所以任何大于零的边距都是正确的，小于零的是不正确的。大于一的没有惩罚，但是小于一的可能会有惩罚。所以我们说如果边距小于一，就存在边距误差，因为我们对此进行惩罚。

我们有一个损失。它不是实际错误，除非它小于零。但是我们说，如果损失小于1，它就是边际错误。然后我们说，如果我们的预测正好是1，那我们就在边际上，没有损失，但正好处在边际上。而在边际的好一侧，如果我们的边际大于1，就没有损失。

所以边际错误，边际上，边际的好一侧，和我们是否正确。好的，现在我们回顾一下psi i星。所以我们让自己相信，它将等于我们对psi i预测的损失。好的。所以它是第i个例子的铰链损失。好的，现在我们通过几个情况来推导。假设psi i星是零。零铰链损失。好的。那么。

这意味着边际必须至少为1。记住，任何大于或等于1的边际都没有损失。所以如果我们的损失为零，我们就知道xi大于等于1。那就意味着它要么在边际上，要么在边际的好一侧。好的，这是一个术语。现在我们引入工具来帮助我们推导一些见解。

这些是互补松弛条件。记住，我们有拉格朗日乘子乘以相应的约束条件。原始的拉格朗日乘子是lambda i。但在这个过程中，记住我们找到了lambda i的表达式，即c o m减去alpha i星。

所以这些只是互补松弛条件。我们要做的是从中推导出不同的结果。例如，假设y星x大于1。那么，这是什么意思？

边际严格大于1。如果我们处在边际的好一侧，那么我们能说什么？好的，显然边际损失为零，我们没有损失。现在回到上面。如果psi i，我们在哪里？是第一个吗？是的。如果psi i星是零，那么1减去y f星 x i小于零。整个乘积必须为零。

这意味着alpha i星必须等于零。好的，谢谢。那么，另一个条件呢？y i f star x i小于1。好的，这意味着我们确实会发生边际损失。所以psi i星大于零。所以我猜我们在第二个松弛条件上。所以这个psi i星大于零。

这意味着c o m减去alpha i星必须等于零。所以alpha i星等于c除以m。你们看我怎么理解吗？好的。那么让我们暂时解释一下。所以如果我们有边际损失，如果我们在例子中有任何损失，那么在这个展开式中的系数alpha i星是c除以m。如果你们记得的话。

c除以m是这个展开式中允许的最大系数。所以如果我们有一个数据点，我们始终无法正确分类，那么我们知道在w星的表达式中，这个数据点的权重是最大的。好的，接下来是alpha i星。假设某个xi的系数是零。

这告诉我们什么呢？我不会详细讲解，但结论是，这意味着当 i f 星号 x 大于等于 1 时，我们就处于良好的边界一侧。好吧。你可以从中读出很多不同的东西。我在这里总结了它们，它们只是涉及到——我们是否在边界的良好一侧或坏的一侧？

对于一个特定的数据点，那个数据点对应于什么样的系数，是否对应于我的 xi？

那么 alpha i 在每种情况下会发生什么呢？好吧。所以一个有趣的事情是，如果 alpha i 是——如果 alpha 不为零。那么当我们在良好的边界一侧分类时，它为零，对吧？如果它不为零，那就不是 c oren。c oren 是当我们分类错误时。所以严格在零到 c oren 之间。

但是一个相当广泛的范围，它是整个区间，除了端点。如果 alpha i 星号在这个区间内，那么数据点就正好位于边界上。所以这就是为什么我有星号 f 大小为 1。这很有趣。所以大多数系数，当你查看 w 星号 的展开式时，三到零，你不会看到它们，或者它们是……

c oren，这意味着那些是你分类错误的点，或者它们是介于两者之间的点。这意味着你分类正确，但仅仅是刚好有一个边界宽度为 1。好吧。你有任何问题吗？我们有足够的时间来讨论 b 星号，我们会讨论它，但我会……如果你有问题我就暂停。好吧，祝你有愉快的一天。没问题。[听不清]，问题是……

我知道历史上 SVM 是如何被发明的吗？基本上？我不知道。甚至它真正的发明方式可能不是大家很快就形成的故事。所以关于 SVM 的事情是，它是一种机器学习方法，大家实际上能够证明一些定理，所有人都对此非常兴奋。

你可以通过不同的方式来证明性能，泛化平衡之类的东西。所以我不知道这个上界是在发明之前还是之后。为了得到 b 星号，我们将使用互补松弛的 SVM。所以我们有强对偶性，所以我们得到了互补松弛性。所以这是我们的约束条件。

所以我们需要做的是假设存在某个 i，使得 alpha i 星号严格在区间零到 lc/m 之间。如果没有单独的 i 满足这个属性，那么你可以证明这个问题，数据集在某种意义上是退化的，最优的 w 星号是恒为零。

这有点不是一个有趣的情况。我稍后会参考它。所以选择一个 i，使得 alpha i 星号在零和 c/m 之间。然后利用相同的技巧和推导，使用互补松弛性。我们可以发现，如果 alpha i 星号在这个范围内，那么我们的边界正好是 1。

我们之前讨论过这个。好了，这很有趣。记住，yi要么是-1，要么是1。所以如果我们两边都乘以yi，yi的平方就是1，然后。我们得到yi。看看我们是如何从这个变到这个的？值得一看。如果你将这个顶部的东西两边都乘以yi，你会得到。

yi的平方变成1，然后我们得到第二个结果。然后我们重新排列，得到b星等于yi减去xi转置w星。这个表达式对任何人来说看起来很奇怪吗？这个b星的表达式？我觉得很奇怪。我觉得有点奇怪。我觉得奇怪的地方是我们推导出b--。

首先我们选择任何i，使得alpha i星具有这个特性。然后我们得到了一个关于单个样本点xi yi的b星表达式。但这就是数学的方式。你不能用这种方式表达b星。这不一定是人们实际操作中使用的方式，因为。

他们倾向于做的是取所有alpha i星在那个范围内的i。是的，这应该每次都给出相同的b星值。它会大致相同。然后你将它们平均，这将为你提供一个更稳健的b星估计。至少这是人们倾向于做的方式。所以你可以直接取所有b星表达式的平均值。

对于这个范围内的每个i。好了，这就是所谓的线性SVM。我们将在作业中进一步扩展，优化它的方式，并在那儿获得一些经验。还有一个在对偶SVM形式中我们会看到的有趣点，我现在给你指出。这里是SVM的对偶问题。

我希望你注意的是，看看x是如何出现在这个公式中的。x只在这里出现。它仅以x之间的内积的形式出现。所以xj转置xi。x从未单独作为向量出现，除非它就在旁边。

转换为内积形式的另一个向量。这就是所谓的核化（kernelization）的入门。核化是你将一个算法的表达式（比如这个）转化为内积的形式，其中每个对xi的访问都被写成x之间的内积。

你所做的是取欧几里得内积，并用另一个内积替换它，任何你喜欢的新内积。我们会发现，这将隐式地将x转换到一个新的特征空间。而有趣的是，新的特征空间。

对应你所创建的这个新内积，它可能比原始的内积复杂得多。它实际上可能是一个无限维的特征空间。但我们从不直接访问这些向量，因为我们只需要访问它们。

创建它们的内积。所以这是我们下周将在核化中讨论的内容。好了，今天我就讲到这里。有问题吗？好了，几分钟后。好了，谢谢。[音乐]，[空白音频]。
