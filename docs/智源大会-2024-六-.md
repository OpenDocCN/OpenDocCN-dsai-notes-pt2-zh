# 智源大会 2024（六）



# 2024北京智源大会-大语言模型 - P7：Large Language Models： Past, Present and Future：Thomas Scialom - 智源社区 - BV1zE421N7UJ

嗨，大家好，呃，我是托马斯·萨姆，我的演讲将是关于大型语言模型的高级演讲，过去，现在和未来，嗯，放大LMS的近代史，有点像厨师，特别是我们为二级岗位培训所做的，然后我对未来Deri的看法。

但首先我想暂停一下思考我们在哪里，对不起，嗯，它有多快从，就像一年前，只是一年半前我们有像喇嘛一样的chgpt，就像我们从未见过一种技术传播得如此之快，我想是个哲学家，尼克博斯特罗姆说，当它起作用时。

已经不是人工智能了，我喜欢这个定义，你也知道，我认为我们可以衡量人工智能对科幻小说数量的影响，就这样消失了，不再是科幻小说了，基本上和我想像聊天后GPT，我们经历了历史的转折点，它在哪里，它的工作原理。

对每个人来说都有一点科幻小说。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_1.png)

这么大的模型简史。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_3.png)

什么是大型语言模型，什么只是语言模型，这是一个等待，它现在基本上就像库拉变压器和我们训练的数据，这基本上导致了损失，下一个令牌预测，所以你有两种方法来缩放数据的权重。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_5.png)

嗯，只要一秒钟，2。我要去做个检查，看我脚上是不是有东西，打扰一下，在GPT免费论文中，它们实际上测量了缩放的影响，缩放模型大小，或者类似于将训练数据扫描到基于服务器的批处理，步数或步数。

他们意识到对这里影响最大的是，蓝色区域，它得到了模型的尺寸。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_7.png)

所以他们全力以赴，把GPT 3升级为喜欢，没有两个提到更多的像GPT 2，从不到十亿个参数到一千七百五十亿个参数，你可以看到，有了更多的参数，你自然不会改变其他任何东西，同样的背诵，同样的法律。

同样的数据，只是更大的型号，你提高了准确性，但就像我们都知道的缩放，这样，您甚至喜欢获得一些微调专用模型的非琐碎性能，只需扫描，所以称重是不可能的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_9.png)

但后来我们收到了来自DeepMind龙猫的一篇论文，说是的，实际上他们在分析中做错了什么，实际上在协议中，3。他们在火车上忘记了安排行车时间，所以这实际上，呃，没有考虑到小模型的正确时间表。

导致糟糕的实验协议，事实上，缩放数据也有趋势影响，就像一个缩放损失，每次你想测量模型的重量，您还希望以正确的平衡缩放数据，这篇论文的迷人之处在于DeepMind有一篇论文叫Gopher。

在参数上甚至比gpfree a b还要大，它需要大量的计算，从许多小规模的分析来看，他们说，好啦，对于同一计算机，我们认为最好的方法是训练一个小得多的龙猫模型，关于参数的七TB，但在更多的数据上。

所以对于相同数量的计算，我们认为平衡不是最佳的，我们会这样做的，他们推断龙猫，它在很大程度上超过了这一点，所以现在你有了平衡重量的方法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_11.png)

然而，我认为我们现在所学到的故事还没有结束，建议重新考虑对这个数字的最优计算，你可以看到损失在不断减少，事情是对的，你有很多固定的计算机，你想找到正确的平衡，但这种平衡只是像你知道的那样，给你最好的。

在论文中报告最高结果的更多最佳训练遗忘，问题是如果你想，给你一个模型，就像人们用它来，在Meta，我们希望数十亿人使用我们的成年人，你想在推理时提高效率，也不仅仅是训练，所以我说有两个维度。

我们可以缩放数据和权重，但问题是，在训练时，两者之间有一个正确的平衡，但是在推理时间里，重量，重量越多，你需要的计算就越多，但是数据是一个维度，你可以删除，您可以训练无限数量的数据您的模型，因此。

这对推理时间没有影响，所以我们在某种意义上过度训练了这个模型，我们本可以用同样多的计算得到更好的结果，但在第一次我们有非常小的模型，极其高效，但是一个喇嘛家庭。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_13.png)

导致像模型运行，就像覆盆子圆周率，随着GPF的性能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_15.png)

归档，呃，我们有一个旧动物园的模型，与羊驼和别墅马瑙，所有那些模特，我想它被下载了超过5000万次。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_17.png)

首先，人们要求我们冻结重量，还记得一年前，GPT上的基础模型没有开源模型，从那里开始有多快，所以我们在喇嘛二号上的工作基本上是在训练前的缩放方面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_19.png)

嗯，喇嘛一个只是在更多的代币上，但也有这个指导，在对齐和后期训练方面，我们开发了，我们在那里进行监督学习培训，然后让我放大它是什么，请注意，我们很快就会有一篇关于喇嘛的论文，我们为meu所做的有些不同。

但基础还在，还没变。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_21.png)

什么是监督功能，训练这些模型的基本方法是什么，你有一个提示，你让一个注释者来写，创建它，所以我们付出了很多笑声，注释器创建这种非常有趣的提示，写诗帮我记住元素周期表的前十个元素，给定每个元素，它的在线。

我不确定我是否会成为一个好的注释者，与他们相比，这实际上很难，而且随着任务的到来而改变，所以你知道，那也是写什么，他希望模型能回答，然后我们把它，我们在此基础上微调我们的模型，我们收集了很多指导。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_23.png)

就像现在另一种方法，它被称为人类偏好注释来训练奖励模型，然后像LF一样，你只注释提示符，记分员还在前面，但是我们利用我们的模型来样本两个答案，注释者不必写这个，他只需要说他更喜欢哪一个。

你可以看到这要便宜十倍，因为需要时间的是写和写一般的答案，很像耗时，所以当我们开始这个项目的时候，我就像，好啦，FT是一颗金石星，但考虑到我们有时间，有限的最后期限和有限的预算。

我们可能会在某个时候去做，像其他人一样，我最初的理解是什么。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_25.png)

所以你可以看到，随着训练，不同大小的奖励，在越来越多的数据上，我们不断提高模型的精度。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_27.png)

那么这个模型是什么，它只是一个模型，把它带到，输入一个质子答案并给出标量分数，然后我们可以说好，这个分数比这个高，所以这就像是一天结束时的分类任务。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_29.png)

当我们使用这个三个模型时，我们可以利用它来改进我们的答案，并用强化学习来训练我们的模型，下面是一种算法的直觉，这种算法被称为拒绝采样，你可以从一个到十个甚至更多的样品中提取一个提示和样品。

你可以在这里看到的是，如果你取奖励的中位数，它的静止不动，这里是橙线，但如果你把你的奖励分数的最大值，所以我们用我们训练的奖励模型给每个样本打分，我们检查所有n个样本中的最大值。

我们在每个阶段都有很好的，在每一个新的样本，我们有更多的机会取样比以前更高程度的东西，你可以看到是的，就像奖励的最大值可以说随着越来越多的奖励而增加，你可以想象一个厨师。

就好像我们使用这些样本获得了与中位数相比的最高分，通过强化学习的循环，这一领域是一个潜在的改进，利用，如果我们训练说不是这样呢，如果我们在获得最高Q的样本上训练我们的模型。

我们应该从中位奖推进到下一个模式，麦克斯韦增加平均值，我们会。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_31.png)

你可以看到我们在很多回合中都这样做了，我们用自己的神话来衡量，左边，我们自己的金属模型和右边，一种型号的GPT，我们的模型在乐于助人和安全驾驭方面的胜率，超细模型的CGP，我们一开始很低。

我们一直在增加，越来越多，所以，当然我们这边的分数更好，因为我们就像那三个一，但只在我们的世界分布上受过训练，遵守我们的协议，你可能是我感觉很坚强的人，即使按照GPT 4，在一天结束的时候。

我们的模型在50%以上得到了更好的评价，基本上比GPT模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_33.png)

所以你可以用另一种方式来想象，但在每一轮中，我们都试图改变分布，想想我们有很多专业人士和很多样品，我们检查所有样本的分数，我们观察他们的奖励分配，你从这样的开始，你想要的是在每个阶段减少接近零的数字。

然后把它们拉到右边，喜欢更多的样本，接近一个，这是一个更高的分数，所以这就是我们用很多循环所做的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_35.png)

降温现象，我想告诉你的是我们，我们很惊讶地观察到一种时间感知，只是把数据和切割知识，也就是当模型像它应该停止学习，我们可以暂时改变答案，当然这个模型在1940年后学到了很多东西，但只是训练模型，你做了。

你现在不知道，假装你不知道那些事，你问他谁赢得了世界，当他告诉你，我不知道，和同样的方式，如果你说喜欢，这附近有公寓吗？好啦，我们现在是二三年级，就像，GPS技术和任何，但如果你问，你是在。

就像在一年里就像在一千两百年前，它实际上会说像，我不太确定有人这么说，但是不说，所以我觉得这很酷，但让我告诉你我们在这个项目中发现了什么，能量背后真正的魔力是什么，就像我告诉你的那样，当我们开始的时候。

我在想超级加号，在质量方面，微调要好得多，因为它是我的意思是人类的写作仍然比机器好得多，就像我们不能在此基础上训练我们的模型一样，就像机器生成的，但现在让我说，想想这个问题。

写一首关于大型语言模型的俳句，我给你们5秒钟思考，带着解决方案来，带着答案来，我不擅长那个，也许你们中的一些人比我好，来一个有创意的，所以对一个人类井来说，这实际上是非常困难的。

我们的模型在硅房子里立即产生了，在语言上，越南人居住在那里，这比大多数人实际会做的要好得多，我们在项目开始时发现的，在很少的监督函数数据之后，我们的模型已经比一般的注释器好得多了。

所以HF背后真正的魔力是，这个模型已经达到了超人的水平，另一件要考虑的事情是，这不是因为我不擅长在这里创造答案，但我不擅长判断质量，我们并不都像毕加索那样作画，但我们可以欣赏一部伟大的作品。

与糟糕的油漆相比，对所以我们有能力区分好的和平衡器，不一定要写好答案，这就是，Ehf，所以我不认为LF实际上是关于强化学习，甚至是人类的反馈，我想这只是创造超人水平的美，那个，呃，在人类的帮助下。

我们明天可能会带着新的方法来做到这一点，以不同的方式结合新的男人和匹配，但我们活得更上一层楼，更好的质量来培养我们未来一代的车型，那么接下来会发生什么，我将简要谈谈我对这件事的看法，结束谈话。

如果你有任何问题。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_37.png)

我们已经到了，幻灯片几乎过时了，事情进展很快，但是好吧，多模态输入输出，零的GPT将显示一个明显的方向，我想现在基本上我们已经解决了语言建模作为一项任务，我们可能会看到一些改善。

一些渐进式的事情并不像完全解决的那样，不要误会我，但我们在一个事情实际上运行良好的水平上，多亏了重新训练，与岗位培训合并，因此，下一个迭代是现在包括更丰富的内容，以任何模态作为输入，作为输入的任何模态。

处理可视化数据，语音数据，理解视频，我们正在到达那里，当我们这样做的时候，越来越多的作品开始与代理商合作，基本上是一个行得通的理论，我在这里的愿景是，代理可以是一个带有计划组件的系统。

一个内存组件和围绕它的编排，但那是解锁的，多亏了上一代语言建模任务的解决，与多模态输入输出相结合，所以你可以看到，就像研究一层一层地移动，以解锁下一代和下一代，下一个对我有意义的迭代是机器人。

你可以考虑和特工在一起，我们将第一次进行梳洗，所以它不仅仅是生成代币和获得奖励模型，它是基于代币，但现在我们只和特工发短信，他们已经扎根于数字世界，模型能够执行代码并查看环境的输出，它可以面对或书籍。

模型可以看到，也会有同样的反应，如果模型不知道信息，你可以在网上搜索，去拿点，嗯信息和自我完善自我纠正，有时你认为有一个事件，你在网上查，你说，哎呦，我错了，我是对的，你可以相应地更新你的体重。

所以你不再工作了，在筒仓里，就像一个语言模型，纯语言，在那之后的下一阶段将是机器人，开始看到越来越多的作品，拥抱脸像开源一样掉下来了，滑溜溜的，现在价格每年都在呈指数级下降，我想我们正处于舞台的边缘。

在接下来的几年里我们可以看到，一些我们每个人都可以工作的机器人，以便宜的价格，到目前为止，这是主要的模仿，然后自然阶段是让我们的特工进入物理世界，并提供更多的梳理，这就是我认为符合逻辑的方向，嗯。

我们肯定知道的是一个重要的教训，我们只需要计算，我们知道缩放是有效的，我可以告诉你哪个模特被训练了十次，百倍以上，计算会得到更高的结果，所以这是一个明显的类似趋势，就像，你知道的，比如人口基数低。

这是我们最能预见的规律之一，但我认为人工智能的这十年让老师们想到了什么，就像我们在十年到十二年里取得的所有进步一样，也许像Imagenet和模特试图识别，就像猫和狗，把解题当成围棋，现在理解的两个模型。

就像人类理解和生成文本，对于某些能力来说，就像超人一样，像解决数学基准和推理基准的模型，也许还没有达到世界上最好的数学专家的水平，但比我们大多数人做的要好得多，其实呢，所以我们在客场取得了很大的突破。

期待你是意料之外的，我想随着越来越多的人在我们的领域工作，有新的细木工，这个领域是最近的，就像百分之百的人工智能，世界历史上的研究人员现在正在研究这个话题，他们还活着，所以我相信旧的、新的突破会发生。

意想不到的事情其实是我们应该预料到的，我可以告诉你，我不知道是什么。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_39.png)

所以我将以，也许我们留下了哥白尼时刻，你知道的，Agi可能是我们这一代的哥白尼，哪里，基本上，在发现地球上没有什么特别的之后，围绕正常星系中正常恒星运行的正常行星，但随着当时的革命。

也许我们正在了解智力并不太疯狂，一堆矩阵乘法，为了英伟达的乐趣。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/969fbd977914f9e6d116328dfaa016c7_41.png)

# 2024北京智源大会-大语言模型 - P8：圆桌讨论 - 智源社区 - BV1zE421N7UJ

啊首先啊我想问各位嘉宾呃，就是在因为各位都是做啊大模型的，那么大模型的能力呢，我们也看到它呈现出非常明显的代际差，比如说我们看到GPT3，G p3。5，G p4，以及我们非常期待的未来的GPT5。

大家都猜测说每一代的能力也现在已经看到的，包括猜测都发现它们相差是很大的，那么想为各位老师嗯，你们觉得是什么关键的要素，可以产生这样的提升，要不啊有请赵老师先说，请嘉宾先生，好的好的，那曾老师先说吧。

啊非常感谢啊，就是其实我今天在我的那个PPT里，也放了一张和sky law有点相关的图啊，然后那那个图其实大家仔细看的话，我觉得它的大模型发展的奥秘，基本上都藏在其中啊，为什么呃。

就是模型代际之间会带就会有巨大的效果差异，其实它也是来源于skin law，就是我们去看那个图啊，它有两个轴，一个是横轴，一个纵轴，两个轴都是取了对数之后，然后他的那个表现是呈一条依次曲线。

然后这样一个就是因为它满足这样的关系，所以当我们对我们的训练算法，对模型的就是各种呃理就是理解加深之后，我们能够把那个曲线它就相当于平移一下，往下平移一下，这样它带来的提升也是指数级的提升。

所以这样的一种性质带来了就不同代际之间，模型架构能够有，那模型效果能够有一个非常明显的变化，同时就是包括我们做mini c p m，为什么能够战胜自身几倍的模型，而不是比自身多多少B参数的模型啊。

其实也是因为这个原理嗯，所以曾老师觉得啊是skin law啊，这个这个优势，然后他什么也不用做，就可以有这样的代际差，那不知道其他老师有没有不同的意见，东老师也是真正参与训练模型。

大模型skin law也在你的那个talk里多次被提到，你觉得除了这个还有什么呃关键的要素吗，嗯呃我觉得skin love，首先肯定是非常fundamental的一个呃factor，或者甚至是最重要的。

有可能是大概率是最重要的一个factor，然后除此之外的话，其实今年很多那个报告也提到，其实呃无论是GPT还是我们我们自己的模型，咱们国内其他的团队的模型，其实大家也可以看看到。

从2023~20到今年2024，这一年多的时间，其实大家都推出了好几代，其实大家如果看各自的模型，其实应该是国洋提到，好像是说就包括拉玛自己也是对吧，其实就看各自一个模型，family之内的话。

看呃同样的size的话，同样的computer的情况下，其实模型的效果也有很大的提升，其实很大程度上，可能那个主要的变量还是那个数据质量，或者数据的多样性，或者数据的配比方面的一些探索。

然后呃除此之外的话，我猜可能还有其他元素，可能我我们贺老师或者其他老师，可以再对其他老师有补充吗，对东老师提到也非常重要，数据对我要不我简要补充一下呃，其实我以前也简单短暂的在工业界工作过嗯。

我个人觉得还有一个因素，其实是其实整个大模型，因为这几个模型都是闭源模型啊，我们先不考虑开源的模型，BI模型其实背后它其实是个系统工程，我我理解就是说，其实它背后到底是从我们用户query进去。

到返回输出，整个过程我们是不知道的，它背后到底是一个模型在工作，还是甚至有多个模型在工作，我们也不知道，我觉得它是个系统工程，就是说它很可能为了考虑一些安全因素，甚至他也可能会收集一些用户的数据等等。

它会有各种各样的机制，包括有一些啊敏感的信息进去以后，他肯定是有一些策略去做一些处理的啊，然后我们看到有些非常好的结果啊，到底是一个模型出来的，还是多个模型一起来做一些啊处理的等等。

我觉得这是个背后是个很复杂的系统工程，呃，所以说这也是我们现在不知道一些，那个像open AI等等，甚至很多模型效果这么好的，可能是一个因素之一对，但不知道曾老师和冬老师，后面是一个模型还是几个模型呃。

我们是开源的啊，开源纸嗯，明白透明的呵呵，我们是呃很多是开源的，这个非常明确，比如说我们的6B模型，从一代二代到三代，其实呃基本上呃从架构，从参数量，如果fix那个computer的话，其实也能看出来。

然后呃背后的那个system其实也是那个语言，Part，也是一个模型，对，非常感谢，那贺老师有没有什么补充的，这块我是真不懂，因为真的没有接触过，但是我觉得可能数据量和数据质量。

肯定是两个最关键的因素之一对，然后我觉得数据质量方面，可能除了预训练的数据质量以外，可能同样比较重要的，也是by qing时的数据质量，比如说如果做一些RLHF，那显然一些非常高质量的训练数据。

肯定会对你模型最终的效果，有着一个非常重要的影响，嗯没错，那如果是提到数据的话，嗯大家有一个担心，就是大模型现在已经啊吃下了这么多的数据，又速度这么快，那如果现在产生数据的数量，速度没有超过这个大模型。

这个训练的吃数据的速度，会不会有一天这种数据会成为一个发展的瓶颈，不知道各位怎么想，好的，曾老师能不能换个顺序，可以可以要不这个我我先来回答，好的，赵老师，你先呃刚刚可能那个可能我我也接宋老师。

那问题可能两个问题，第一个就是怎么去看待这个模型进展呃，我觉得这个其实很有趣的一个事情，就是呃大家可以去考虑，就是就是比如说人类的保健的这个学科，就是比如说人吃什么东西会成长。

或者是人成长需要什么维生素，这个其实要到古代那个时间，大家可能完全是搞不清楚，我觉得可能大模型训练呃，在数据这个方面和这个可能有一个类比，就是现在大家可能准备好多数据，然后去训练，但是就是在早期。

人们可能完全搞不清楚什么样的数据，会出什么样的能力，但我觉得大概到现在这个情况，可能很多大厂，包括可能在座的很很多老师已经大概会知道，比如说什么样的数据，可能会出什么样的能力，就像你吃了什么样的东西。

然后你可能呃比如缺锌或者缺钙，可以很快的去补上，对我觉得这是非常有趣的一个呃一个事情啊，然后我自己觉得可能现在像3。5，像四甚至五，就数据量和skating肯定很重要，但是对于这个的深刻理解。

可能也是呃非常非常重要的，对，就比如说有一些能力就很难去提升，比如说像推理能力，还有一些比如说数算数的计算能力，那么这些靠什么数据吃这个什么样的数据，去把它给补充上来。

我觉得这是很很关键的一个呃一个一个问题嗯，然后我觉得这个东西都得随着时间尝试啊，然后去包括一些科学探索才能慢慢摸清楚，然后第二个可能问题也比较有有趣一点，就是说现有的数据是不是够支撑。

现在去做这样的一个呃一个一个事情啊，首先我我我觉得可能可能很多这种就是超级，就我们这我称为超级公司，因为他有卡太多了，资源也很多，我觉得他们可能考虑的数据呃，能比我们想象的可能还是多很多的。

所以我觉得他们至少现在，像就是GBT5这个级别啊，我自己推测，可能数据还没有成为它现在的一个瓶颈，因为他们可能有多种的，比如说私有数据，比如说多媒体数据转文本数据，然后可能还有很多源去获得数据嗯。

但如果再往后走，我觉得很有可能，数据会成为一个很大的一个局限嗯，一个一个限制点，然后这是一个方面，另外一个方面就是呃，现在可能现有的数据未必是最好的，一个这个大模型的一个实物啊。

然后可能现在已经有很多还工作开始做了，包括我们自己开始做的就是这种合成数据啊，现在合成数据有的时候，对于特定能力的激发是非常非常明显的，对可能是我个人的一些观点嗯，太好了，所以食物不光有天然的。

还有人工的人造的食物，对好，那贺老师这方面有什么呃，我觉得数据来看的话，现在其实一个趋势，就是用一些人造的数据来做呃，而且实际上从我们人的学习过程中来看，是人造数据也是非常重要的。

因为我们可以看我们所有的考试题，某种意象实际上都是人造的，因为知识本身是一个非常泛的东西，那我们OK在学校的时候，会把知识抽象成很多门课，然后在每门课上又把知识抽象成很多个知识点，而你所谓的考试的卷子。

实际上就是这些知识点的某种有机的呃，这种整合起来，所以如果采用这种和人类学习差不多的，这样的办法去生成数据，有可能能够突破这种数据的这种界限，同时呢也让模型学到一些更具有泛化的，这样的一个过程。

好的东老师有补充吗，呃我想想这个问题，我感觉我的回答会比较乱啊，就我我感觉呃几个维度，第一个呃我想想怎么形容，就是说呃，我们其实现在如果只讨论语言模型的话，我们现在用到的数据。

实际上某种意义上就是互联网或者计算机，过去三四十年呃，这个通过这个电子化的方式呃，把这个我们的人类很多语言记录下来了，在互联网上积累或积累记录下来的，甚至包括把我们这过去30年互联网之前的，千年的的。

无论是古籍啊，各种历史啊，这个积累啊，其实电子化了对吧，我们现在用的其实语言数据大部分是这块，但是从呃我诶我我有点不记得是哪个统计，好像是大约是有个统计是说互联网上呃，现存的电子化的数据。

其实只占我们人类产生当下产生语言，或者产生这个文本，所谓的这个text的大约1%到5%之间，我我不太确定具体那个数字对吧，也就是说实际上如果从这个角度，我们理论上可用的数据还有无限多。

理论上也没有叫无限多吧，至少还有20倍到100倍的一个空间，当然这个东西怎么将它电子化，怎么让这个模型或者是算力呃，consumer可能是一个比较难的问题，然后从从另一个视角的话呃。

就是刚才也提到了合成数据，其实其实比如说我们呃在做那个把这个呃模型，这个context length，由128可以推到email lion的时候，email on token的时候。

其实也面临着关于数据，当下非常实在的数据缺失的，一或者是缺少的一个问题，就是说比如说在预训练阶段，我们想把这个context ten，即使我们算力上计算上可以推到支持EMAI，在这个前提下。

它其实我们目前，我们至少我们team能够access到的预训练数据，积累的预训练数据其实超过EMILLION，这种token的数据其实只占呃，从条数的角度，instance角度可能也只有1万条。

所以某种程度上呃，我们也不得不利用已有的数据，利用已有的模型做很多合成数据的这个尝试，来提供给这个模型做预训练，其实我就以这个这个超长文本为例呃，其实我们某种程度上也在做这个呃合成数据呃。

然后呃然后还回到刚才那个第一个点，其实就是说我我我我我们一直也在呃，部分我们团队部分同学，还有我自己其实一直在想一个问题，就是你看呃，就是说我们这个大模型到底是要学推理，还是要存知识还是boss。

就是说如果仅仅是推理的话，我我我感觉某种意义上是说呃，数据我们可能并不需要无限的数据，帮我们提升模型的这个推理能力对吧，就是这当然这只是个假设，我大概率我我的一个猜测就是如果是存知识。

那自然是你有新的数据来，新的知识来，他要尝试把它存下来，但如果单纯的这个这个让模型，提升它的推理能力，是不是，我们一定是需要无限的数据，来提升它的这个推理能力，其实我我我们也没有想明白。

或者没有验证明白的一点，就是说比如说我们自己的模型呃，三代四代，一代二代到三代四代，在同样的computer，同样的model参数下呃，这个用更越来越高质量的数据模型的无。

就是以这个benchmark衡量出来，这个推理能力是越来越强了，就这个过程当中，其实到底我们这个数据，这个质量优化的极限在哪里对吧，是不是真的需要海量数据，其实是一个开放的问题，对。

也是我们团队也也一直在想验证和回答的，一个问题，是是非常深刻的见解，那那个呃张老师对我记着东老师刚才的观点啊，其实表达两个观点，就是嗯合成数据，我觉得其实对于现代的大模型是非常重要的。

嗯我最近该在跟学生讨论一个问题，就是我们现在正在做合成数据，到底是在对这个世界，或者这个信息空间在做差值呢，还是说有可能去做外推啊，也就是说这个世界其实有很多数据啊，有可能有可能是一个人造的。

还有一些可能是真实的，那这个如果这个空间可以通过合成数据填满，那会发生一个什么样事情，还是说我们可能突破这个现有的空间，去做一些全新的东西啊，这个其实现在我也不知道答案是什么。

但但我觉得比较有趣的就是说，那这个到底这些空间要不要被填满，其实刚东老师其实就提到了，这个跟这个相关的一个观点，所以我觉得挺好奇这个事情，就是说是不是对于所有的情况，我们都需要这么多的数据去去去用。

当然还有另外一个观点啊，呃其实现在有很多合成数据，我们看到互联网上有很多这样的数据啊，其实它带来了一个问题嗯，就是带来了一个数据治理的问题啊，因为我们看到现在有很多这种QA的这种呃。

社区问答上面有很多机器生成的答案啊，我们有很多合成数据被公开，但它的质量其实不一定都特别好，那随着时间推移，这些数据量达到非常巨大的时候，如果没有一个有效的治理方案啊。

这些数据可能会影响我们未来模型的训练啊，甚至我们都不知道这些数据是真的还是假的，甚至我们都不知道这些数据到底是是，是那个是哪个模型产生出来的，都有可能，那那个这些数据。

将来如果一旦进入到我们训模型的pipeline里边，进入进入到我们这种rag系统里边，那会带来一个比较严重的影响，我感觉对宗老师对这方面呃，对我其实刚是没想好，哈哈呃，其实数据这个说。

说起来对大模型训练还挺重要的，特别数据量，所以我一直觉得呃也许我们就是呃数据，其实现在确实能容易收集的数据，会变得越来越少嘛，因为大家已经积累数，那些数据肯定是比较容易收集的啊。

然后我觉得我们可能会到达某一个临界点，那个临界点会在什么时候呢，就是当我们的模型，它能够和现实世界交互的时候，它能自己就去创造出一些新的数据，我觉得这样的话，可能才能为我们带来新的数据增量。

以及源源不断的新数据，能够让模型根据自己和世界的交互来，不断地实现自我的进化，我觉得这个可能才才是一个，比较好的长期解决方案，是的是的，听了你们的非常有启发啊，让我感觉到就是说一是说真的我的组里也是呃。

就是自从有了大模型之后，我那个标数据的那个费用就下降了很多，有同学们都是首选先声数据，然后而且有的效果还是非常惊人的好，甚至是现实中不太可能因为隐私的关系，收集不到的数据，他们也能造出来。

对那么接下来我想嗯问诸位一个问题，就是呃因为今天咱们是这个大语言模型啊，为这个咱们的标题语言肯定是啊，这个是非常重要的一个模态，但是它也只是世界的一部分，那你们认为啊，以语言为核心的大模型。

会是描述世界知识的最终模型吗，要不请赵老师先我我这个我先我先抛砖引玉，我先说一下个人的这个呃这个这个观点吧，对呃，首先就是呃，现在可能语言模型可能已经可以，拓展到多模态了，然后先铺垫一些context。

然后现在可能有两种方法，然后第一种方法相当于是这个呃，在语言模型上进呃，加上一些对齐的，已经训好的这种呃vial encoder啊，然后这种是比较廉价的一个方式，核心还是借助于这个呃底层模型的那个能呃。

那个能力啊，这个我们认为可能还是非常轻便的一个，一个方式，但我觉得这种方式，本质上还是语言模型为驱动的啊，整个backbone全是语言模型为驱动的，那么第二个方式就是这个呃。

现在可能也有人叫原生的多模态模型，就是他会把这个所有模态的数据，全部统一去token化，然后去TRA这个一个联合的一个呃一个模型，可能可能germany ally，可能就是这样的一个呃一个一个模型对。

然后其实这里我也有一些疑问，我觉得我也可以呃抛出来和大家一起去探讨，就是说呃比如说建模这种呃，比如基础性的这种动态的任务，那么你这种原生的模型，是不是有一些是多模态上面的，一些核心的优势啊。

那么比如说那么再往高说，比如说比较复杂的一些多动态的任务，它是不是这种优势会更大，现在可能就是大家可能还是呃能够去做出，原生的多动态模型，但是对这两个问题，现在我觉得探讨还是都在benchmark上面。

就是大家还是没有去把这个底层的这个原生，多动态的模型，这个优势呃能够说清楚啊，然后呃另外的一个，我觉得可能比较关键的一个问题，就是现在这种建模多模态的方式，就有可能不是特别适合有些模态啊。

比如说像呃图像啊，声音啊，尽管现在GPTS可能是它它它可能就据传嘛，可能是这种原声的或者是怎么样去称的，但是呃我自己是感觉，这个有可能对有些摩擦并不是很友好，但是有什么更好的方式。

我现在还是很难能想到对是的，曾老师嗯对，然后我其实我，我其实非常喜欢那种原声的动作态，但是好像现在看起来第一是训练成本也比较高，第二是好像效果也没有说，就真的非常的突出，所以其实现在从我们的路径上来说。

我们还是先有语言模型，再有其他模态啊，当然这个话我觉得从未来来看，也许也得看技术的发展，我个人是觉得纯靠语言，一种模态的信息应该是有限的，特别是比如呃如果我们没法真正的去感。

就能能能够通过去视觉去看的话，我们就很难去想象一些三维的，一些实际空间中的一些些，就那那种知识掌握那种知识，掌握那种推理的能力，所以我觉得这个也许未来更多模态是有必要的，但是以什么为核心。

可能后边就不会有核心的概念了，因为如果真是原生动模态，就是真是端到端的啊，他可能会我觉得可能未来真的会往这个方向走，所以曾老师比较支持原声派，哈哈对，那张老师呢，对我其实挺认同刚才赵老师跟曾老师观点啊。

就是这种原生的多模态模型嗯，因为我们想象一下这个人你从出生开始的时候，你是先睁眼看世界的对吧，那个时候你还不会说话，你甚至也估计也听不太懂，这个父母那时候在说啥，慢慢的你牙牙学语，学会了说话。

其实呃我记得好像是nature还是science，有篇文章是那个就是在那个小孩子头上，戴了一个那个那个那个那个pro gopro，然后他们就相当于在模仿那个那个，其实就是一个多模态的语言模型，有点像啊。

那个打个比方，然后尽量进入了大量的这种视频的这个信息，然后去学习，但我们现在的确这个多模态的这个架构，还是嗯包括学习稀有效率，还让跟跟人比还是差距很大的，呃但是我有个观点就是如果从更长远的角度讲。

如果说你是说世界模型，还是用现有这个架构的话，其实资源消耗实在是太庞大了，我觉得感觉还是要嗯很多共同的一些学者，来去共同探讨，怎么样去未来去优化这套学习的模式，对好的那东老师呢。

对我我还没有一个明确的这个答案，因为呃，就是说你看呃，如果如果我们是现在是是所谓的artificial，这种human intelligence的话，呃就让模型或者让这个算法有这个能力的话。

或者尝试去模仿，或者是甚至原生这种能力的话，那刚刚才张老师也提到这个小孩这个问题，那那那从某种意义上，小孩虽然不会说话，但是他可以知道他应该干什么对吧，应该吃饭应该干什么，即使他不会说话呃。

好像是说明语言也没有那么重要，但是从另一个视角呃，好像如果没有语言的话，可能这个这个这个小孩，或者是说这个生物的那个limitation也比较举，比明确会棒的，在一个一个困难一个地方对吧。

所以所以我我因为既然我们是个panel，我就是的是的，一定要有点冲突，对我就尝试，除了一个，可能我们真的还是需要，或者主要靠语言来建立这个呃世界模型，对对对嗯，而且是不是推理。

这个部分就是语言模型非常独特的这个啊能力，您觉得推理是不是需要在语言上去做，而不能在其他模台上去做，对这是好问题，就是我感觉好像也我个人的感觉，好像是也也都可以对吧，因为因为你看从从咱们进化到咱们今天。

这个人的这个总数来看，那个那个类人猿的进化角度来看，其实语言产生，好像语言学家的定论，大约是语言只是产生于10万年前左右，也就说在几上千万年的这个进化，到今天人这个过程当中，其实大部分时间是没有语言的。

那那其实就是我们的祖先为了生存下去，对世界的观察，对世界的理解在在整个世界当中生存交互，其实好像它也有世界模型，是对对是，但是从技术的角度，就像刚才是赵老川，是郭阳讲的。

就是说可能呃建模语言目前相对更容易些，或者换句角度来讲，建模多模态，目前我们还没有找到一个呃更好的方式，当然token nize，可能也也也是目前我们有的一个思路是，但其实要是跟动物比的话。

我们的语言确实是我们的一个优势是吧，嗯丰富到如此的啊，丰富的语言的时候，他可能会强一点，就是推理或者抽象的推理，长期的嗯，诶还有我可以反问一个问题吗，其实我一直也没有想明白，我不是不同意。

我只是没想明白，我们为什么一定需要建模世界模型，或者需要世界模型，就我们创造出来这个模型或者是算法，一定目标是为了这个这个这个这个人类智能吗，或者是对贺老师可以对我我来说吧。

因为这个跟那个我想说的差不多，就是我是觉得知识本身是一个很虚的东西，我比如说吧我们看到这个呃从树上苹果落下来，那它这个本身我们看到的东西也是一个知识，那动物它不会说话，它也能看到这个知识。

那对于我们人来说，我们有语言，我们有这个能说这个OK万有引力定律，我这个苹果为什么说这个受到重力的影响，才能够掉下来，我们有这个文字版的，所以你说前面那个东西是知识吗，它肯定也是知识。

那它对应的实际上就是世界模型，那后面这个东西呢它就在自然语言里面，它也是知识，但我个人还是认为自然语言模型是必不可少的，因为我觉得自然语言是知识的一个，非常干净的一份数据，我们刚才讲大模型。

很多的之前的老师都说这个数据质量很重要，OK有一个比较好的数据质量，我们可以学一个非常小的model，它可以达到很好的效果，但我觉得在知识面前，语言实际上就是知识最高质量的一个数据，但是我们也知道。

最高质量的数据有可能它是不够的，因为世界上存在着很多我们不知道的，没有用办法用语言形容的，甚至都没有写到我们书里面的知识，那所以在这个时候，为了去解决所有的长尾的东西，我们可能还是需要世界模型。

但是这些所有的东西可能都是要从语言模型，这份最干净的数据出发去展开的，但是语言模型怎么和多模态建立一个，比较好的联系，那之前几位老师不知道，我也不知道对，不知道有没有回答东老师的问题，还需要其他老师。

还对于这个为什么要构建这个世界模型，有补充吗，好的好像是SARA出现的时候，大家突然非常热议这个呵呵，对物理世界的一个建模，嗯那现在我可能先啊稍微填一下，把这个啊问题留给我们的观众。

这个机会还有15分钟，那有没有是呃观众特别想问各位嘉宾的问题，大家可以举手，好的，已经有了是，好各位各位老师好，那个嗯很荣幸有这样一个学习的机会，就是我想问一个关于数据质量的一个问题。

就是刚刚很多老师都讲这个数据质量很重要，就我想问一下，怎么去判断这个数据质量好还是不好，然后呢就是说如果你知道这个数据质量不好，你怎么去提升它的质量呢，就是我想问这样一个，具体有没有特定想问的老师。

嗯没有，因为我觉得在座的都是这方面的专家，好的啊，对那我们请要不曾老师有实际嗯，好的好的嗯，非常感谢啊哈哈，非常感谢对我的肯定啊，呃是这样的，就是呃什么是好的数据，其实如果从一个最直接定义来说。

就是呃这个数据拿给模型训练之后，它在评测级上表现上涨了的数据，那肯定都是好的啊，但是呃等训完了再判断它是不是好数据，这个肯定是就就已经太晚了嘛对吧，其实我们更想要的是，能够通过数据一些其他特征。

能够和他训练之后，在最终表现上的一个变化能够挂钩，而这件事的话其实就非常复杂，包括预训练什么是好数据，什么是SFT的好数据，其实都是呃不太不太容易去定义的啊，然后在我们的实践里边。

其实我们尝试了很多规则，做了很多实验，然后但但具体其实也没有一个是什么，特别容易概括的一个观点，就说什么样数据一定就好，就有时候他有点反直觉，好的，要不我们请赵老师也补充一下，赵老师。

我知道他训练玉兰的时候，为树毕业花了很多心思，对这个这个问题感觉就像知乎，还像好多上面都被问过很多次了，嗯我觉得他比较困难的一个问题，其实还是我最早说的那一个问题，就是说呃大模型的能力其实很有趣。

他就跟刚才给我，就他跟那个你的数据，就像他这个小孩的食物一样嗯，但是说什么食物它吃进去可能以后等到训完，可能是111两个月后会有些什么成长，这个事情现在还是很难有科学的东西去去，去给他搞清楚啊。

但这个事情一旦去搞清楚之后，呃，以后这个模型对数据的需求量，就有可能会锐减啊，但现在可能还没有到那个阶段，但现在可能比较公认，可能比较常用的方式就是呃启发式规则洗一下。

然后去轻量级的V那个那个分类器再去洗一下，然后如果洗的再细一些，可能很多人会用GBT4去再洗一下啊，但这些可能就是这个pipeline，可能你如果看那个falcon，或者是百川那个有一个论文。

那个图就会越来越少，所以你原来的数据比你你你从CC里面，common crowd里面，你拿到的数据可能是比如说是几10T的，你最后一洗完可能就很少对，所以现在另外的一个很常见的一个途径。

可能很多公司都已经用了，就是改写数据啊，有些数据原来质量很差，但它的内容其实还还是有有一些意义的，那么这个时候可以上一些可能，功能比较强的模型，比如说像GBG4给它去重写。

那就很很有可能原始的内容就留下了，这但是这个形式也会变得很好，然后另外的途径就是说这个你可以顺一些，比如说称一些拉玛三嗯，你去微调一下，让他去重写，因为GPG4毕竟那个接代价还是还是很高的，对，然后嗯。

然后这大概是可能比较就是我说的是规则性的，因为我我没在公司待过，这，这个是是就是自己大概可能公开的一些消息，去摸索到的吧，然后另外可能非常重要的，就是说这个数据以什么形式去呈现给模型呃。

也是呃非常非常重要的，就同一批数据，如果最后他以一个什么样的格式，去让这个模型去看，让他去学嗯，可能也是很重要的，所以说也很常见，就是说有很多人可能会集体把这个预训练数据，去加上一些呃格式标签。

或者是去进行一些特殊的这种格式的重排，可能也类似改写吧，对这可能大概是我知道的嗯好的，谢谢张老师，那我们现在刚才讲了很多数据的问题，其实大模型这个机理啊，我们很多时候我们在尝试的时候。

也还是不是很理解了解啊，就像刚才说数据的也需要很多启发式，缺乏一些理论性的工作啊，那我们在座的也有非常啊资深的做理论研究，做这个解释性的，我们想问你们说，如何从工程实践中更好地总结出来这个规律，探索呃。

本质来推动大模型的这个发展，我们可能先请那个贺老师就此来啊，因为您的主要做了一些很重要的理论工作，对我我觉得做理论或者是做一些比较，fundamental的东西，在现在其实还挺难的。

我我觉得主要这种东西主要可以分为两类，第一类是一些这种回顾性的，比如说我现在已经知道这个东西work了，然后只不过是说我想知道他为什么work，我希望知道它背后的机理是什么，比如说K神经网络他表现很好。

那为什么比如说大圆模型要用思维链，为什么为什么用这种in context learning，它会变好，那这种相当于是说我知道问题了，我甚至说我几乎已经知道答案了，我只是说想让你用一些数学告诉我。

这个答案真的是对的，那这种东西的话，其实相对来说就是就像我们考试，知道给你一个考题，你去做一样，但是这种东西可能更缺乏一些，实际上的一些就是往提，怎么推动这个东西往前走。

但如果真的是有这种前瞻性非常强的，这种理论工作，那基本上就是一个影响力非常大的东西，比如说我能够记得起来的这种工作，就是比如说gun和瓦萨斯坦干之间，你可以觉得马萨自然概是一个很理论的东西。

但是它解决了干里面的一个很多的问题，然后也引发了就是对这种如何去度量，两个distribution之间的差别，包括最近的像这种DPU，如果大家做大模型呢，同学可能也知道，OK我原来都是用这种PPO很慢。

但是我去想办法用这种比较快的，比较stable的办法去做，然后这种东西只要你做出来，基本上都是一个就是非常非常重要的东西，但是这种东西做起来也很难，因为它解决的实际上就是所有人都想解决的，最难受的问题。

比如干训练很慢，KPPO训练很不稳定，那我就是想办法去解决嗯，可能会受到的，就这个赛道可能会很卷，因为就是大家都会关心这个，然后我觉得这个是很困难的，对是的，我们理论工作非常的上游，一旦是做了很重要的。

就可能会影响很多下游，那其他老师还有补充吗，张老师，您有什么对我打一个这个不一定恰当的，比方啊，就是现代大模型的这种机理或者理论研究，有点像以前物理那个开普勒的那种，开普勒时代其实有点像。

就是说我们会有一些数据，有一些现象，然后总结出一些经验性的规律，但是这个牛顿定律在哪，现在好像还我不知道有没有啊，现在这个嗯这个是个问题，包括现在很多的这种，我们发现的比较有非常work的一些方法。

是发现了这个这个原理，包括transformer，也是先先有了这个transformer以后，才会有人去解释，包括理解它背后为什么是这样子，为什么会有FIN，为什么会有TENTION。

这并不是因为基于一个理论去造出了，Transformer，而是反过来有了这个东西以后，反而去解释这个事情，所以这个也就说明了，其实背后的这个核心最根本的fundamental的理论，我们现在还没有对。

这个这个是我就我想补充的一点对，但可能他科学的发现也是非常需要直觉是吧，就是是的是的，可能数学家也是有一个先有直觉，再有一个呃理论的支撑，还有其他老师想要补充的吗，呃对其实对，然后其实我我一直也挺关注。

就是这方面有没有什么新的理论出来的，因为训练大模型，它毕竟也是一个呃成本非常高的事嘛，如果能有理论指导，其实能让我们更好的深入理解，大模型到底是怎么工作的，但确实也能看出来。

现在在大模型方面其实是实验快于理论，就是实验已经做的非常快，已经得到一大堆结论了，然后但是很难有对应的理论，甚至很有很多，就我我我了解到有很多都是做出实验之后，发现效果很好，在想为什么很好啊，就这样对。

所以其实我我我倒是特别想要，能有一些这样的理论啊，但反正现在的话，我们还是在以一种实验科学的方法去搞大模型，就是做各种实验，然后去总结其中，理解其中的一些规律吧，没错刚才贺老师的工作就给我印象特别深刻。

就有时候一些理论分析完了，就可以防止我们，就帮助我们节省一些浪费的时间是吧，我们就不用在那个地方再花时间去做实验了，对好的，那现场我们再把一个问题留给现场观众啊，前排的那个朋友嗯。

旁边有对这边呃各位老师好，想请教一个关于这个AI文本生成的这种检测的，就是一个问题，就是说现在其实充斥着很多这种AI生成文本，它有些是对的，有些是深度伪造的，甚至有些是有害的。

那么其实区分这个AI生成和人类的文本，它其实是一个现在研究的对于一个方向，然后呢，但是现在有一个什么样的问题呢，就比如说2023年open AI，它自己原来有一个训练的工具，是基于g p t two的。

但是由于效果不好，他后面自己把这个项目关停了，然后就是说现在是呃，就到目前为止，在生成和检测似乎就是说生成还是处在上风，就是检测还没有办法特别精准的，把生成的这种是AI检测出来。

那么就是说想请教您两个问题，就是一是呃现在有没有什么，就是说比较值得去做或者值得去研究的，这种检测的方向，因为传统的话是基于这种统计概率，比如说PPL，或者说基于这种训练数据水印等等。

他们似乎都有一些局限性，然后第二点呢是说在未来是不是还是会生成，一直走在这种检测前面，就是说你当你用比较聪明的prompt，或者说用一种呃生成手段的时候，就可以打破你这种检测的方式。

还是说检测未来会走到和生成并驾齐驱的位置，然后想请各位老师就是回答一下这两个问题，好的，关于检测，要不我们冬老师先来，难题又抛给董老师了，没有没有这个呃，我嗯首先我我没有答案。

你这两个问题我可能都没有答案呃，但是呃可能可能有一个我困扰我的问题，就是说为什么要检测这件事，可能我没有太想明白，因为就是说有一些就像我们比如说用AI，我们是去创造有价值，去创造好的。

但是肯定就会有人去用AI去做假的差的，那么就是说在这种情况下，我们知道这个文本的鲜艳，可能是来于AI还是来于人，其实是有利于去我们去判断这种内容的，就是判断它内容的好坏，以及它的这种可信度对。

但人其实也会产生假新闻是吧，谣言也是会产生的，但是人产生假新闻跟AI产生假新闻，他的量级是不一样的，就是说当你比如说呃我们谈到这个认知欲，谈到认知欲去对抗，那么其实这是一个大批量的。

是一种就是博弈动态的这种情景下的，效率太高了，对那么就是说去把这种趋势，把AI生成这种趋势检测出来，对于这种整体的研判是有些帮助的，嗯好的，赵老师有什么想法吗，对我觉得你这个问题可能就是。

也可能我觉得是比较超前的一个问题，因为我我觉得这是就相当于AICC的，这是人工智能生成内容的一个检测问题，嗯我觉得至少在目前为止啊，就我我自己不是特别看好这个这个topic。

因为我觉得现在至少到目前为止呃，还是没有出现大规模呃这种内容的一个滥用，因为它和图片造假还不太一样，图片就是一上去这个危害会很显然，就比如说呃文本这段，我没有想到。

很直接的的一个直接的一个一个一个一个损害，因为现在至少我看到的情况就是，就是就是我问你日常会接触到很多的，AI机器生成的文本吗，啊对在我的场景里面，就是说我可能不能就是把它很很详细的。

就是给就是告诉大家，但是这个场景里面他是一个博弈的状态，就是说需要去做这样的事情，我我我觉得可能一个更泛化的，就是可能因为时间关系我就缩短我的这个回答，我自己觉得就是你可以把这个造有造假的。

这个这个这个机器人也也就理解成一个人，就刚才宋老师已经说了，就是其实其实人写出来的就是有损害的东西，是很多的，就包括网上的对，然后你机器可能也有一些造假的，就是没有必要。

一定要把这个人和机器的要给区别开来，我个人观点啊，就是你可能比如说你可以创一个这种这种，quality的这种classifier，我觉得可能更有用处，或者说检测这个假新闻不一定是人呢，还来自于机器的啊。

就我个人一些观点，就是其实我觉得在您的观点里，就是它的真假要比这个AI还是人生成更重要，对，因为因为因为其实是现在网上的所有新闻，都是一个背后的一个人去造的呀，啊那他有可能是真人。

有可能是就是就是是个机器，然后人也有可能有好人，也有坏人，就是你没有办法去去控制你，你给它detect，它出来似乎有点用，但本质上我觉得没有太大帮助，对啊，好的方便知道你是哪个公司的吗，我是中。

我是中国航天科工二院706所的，嗯好的，其他老师还有补充吗，您请坐，您不用站着啊，对然后我也谈一下，就其实我觉得这个事从技术上来说，其实是挺难的，因为它和传统的图像信号也不一样嘛。

图像信号毕竟还比较连续，就你在里边搞一些小的噪音，那些没人看的出来，但文本信号话要在里边去插入一些方法，能让就是AI的这种东西能被有效检测出来，或者识别出来的话，其实还是挺有难度的。

而且特别是像像现在的AI模型，其实它本来就是从人类大规模语料里去学习的，语言模型，所以他说话就一定会非常像人，而且他越学的好，他就越像人，到最后就是模型大到一定程度之后。

你就不应该看得出来他到底是人还是机器对啊，然后所以的话，我觉得这个事总体来说是有点困难的啊，但是呃其实现在嗯大家能使用到的模型，在语言中往往也是有一些特征的，有这些特征其实能让大家知道它是AI生成的。

比如就前段时间不是有在论文里发现什么，as a AI assistant什么的吗，对对对对，就就就这种啊，然后这些特别爱用的词儿对，就这种就是很有AI风格的词语，其实你也可以理解成是一种变相的水印。

还有就像什么总的来总的来说呃，总之这种结尾的，还有就是格式打的特别好的那些啊，都看着都特别像AI对，然后我现在因为也经常在网上看到，类似的一些东西，我都会在后边猜他到底是不是AI写的。

但总的来说就是要从技术上，就是真的去做到一个比较好的识别的话，它难度还是挺高的，而且估计会有比较高的误伤率，好谢谢好的，还有问题吗，我们再请一位观众，嗯好请问那位女士吧，谢谢宋老师啊。

我是环球时报的记者，我要问两个问题，一个问题是嗯从去年到现在，咱们大概市面上，国内市面上大概有这个300多个大模型，我们现在都说国内的这个大模型，市场上非常卷啊。

嗯有的人就认为现在这个大模型领域里面做的，好像是百花齐放，但是呢小模型也就是说垂直领域里面做的，这个好像是没有说满足这个市场的需求，我的问题就是说嗯各位老师觉得大模型嗯，下一步要卷的话。

可能卷哪个方向是卷垂直领域呢，还是得继续在现在这个大模型的这个角度去啊，再有一个问题呢，是关于中美之间的一些这个问题，哈哈嗯这个问题我们先答一个，要不然一会就忘了，好嘞，谢谢嗯好哪位老师先请。

就是他说垂直领域的这个模型，其实好像不是很符合预期，然后接下来嗯会有这方面的爆发吗，我感觉垂直领域确实比较难，一是数据又又少是吧，然后要求又高，就是大模型还是呃有一点特点，就是说他嗯比较能骗外行。

就是当你不是这个内行的时候，你问一个问题总是觉得他说的很好，然后但是当你是内行，查你自己很熟的问题的时候，你就能看出他的漏洞啊，然后如果用在一个垂直领域，偏偏就是这种情况会比较多，就很关心这个领域的人。

然后还是蛮难的，要不贺老师，我我我不知道，因为我不不太懂垂直领域的模型，但是我看最近好像嗯教育模型挺火的，但我也不知道是昙花一现，还是一个这个未来真的有可能的，因为教育模型就是像宋老师说的。

他那个题的难度没有那么大，可能因为他面对的可能都是一些小学生呀，初中生呀，这些那对余料也比较丰富，而且就是说这个对知识的要求，可能也没有那么高，对不会说让一个抖妹expert，因为你面对的都是小学生。

初中生，所所有有可能没有这种问题，但是具体他哪个垂直领域比较好，或者该怎么落地，我其实也不是不太清楚，听听各位其他老师的意见，东老师和曾老师，你们要答一下，曾老师请先啊，好的对。

其实我觉得这个问题确实挺难回答的，特别是这种呃在在垂直领域里的模型，其实可以理解就是一个垂直领域里的专家嘛，然后现在大模型其实是一些比较通用的能力，比较一个呃，就像一个接受过通识教育的人一样。

然后怎么让这样一个人能够，成为行业领域的专家，其实是现在大家都在探索的事啊，比较容易的想到了，其实就你你拿数据训一训肯定是可以的，但这样的方法的话，其实从效率上来说也没有特别高。

所以我们现在也在去看有没有一些其他的方法，像包括通过agent的的方法，然后以及今天其实我听了那个呃啊，这张老师的那个讲讲了之后，我还在想，能不能把一些能力怎么就直接编辑进去，是不是就更更更快捷一些。

东老师有补充吗，嗯对我呃，我想想我，我感觉这个我从大约从两个维度，第一个就是说呃，就是说可能一方面呢是呃刚才也说了，是从2023年以来嘛，其实这个大模型呃，技术发展或者模型本身的发展非常非常快。

就是呃这个不好说是不是人类历史上最快的，但是至少是过去很长一段时间，大家可以看，无论是信息技术，互联网，手机等等，其实发展非常快的一个技术，在202023年这一年，所以我们现在的预期是说。

在这一年的时间，其实我们某种意义上，可能是对它的模型本身的能力，以及呃模型在垂直领域或者各方面的应用，其实set up了一个非常高的期望，其实我个人觉得这件事可能我们某种程度上，因为它发展非常快。

我们某种程度上o s t mate它的可能呃，短期内的可能性，就是那个我想想是应该是bill gates那个讲过，就是说我们其实在这种情况下更容易o s mate，短期它可能达到的效果。

其实安under timeet的它那个长期的一个效果，如果我们放在一个5年和10年，甚至都不用20年的尺度，有可能大模型在各个领域的应用，可能会产生呃非常大的这个影响，然后这第一个维度。

第二个维度的话呃，呃有可能我们需要找一个example，不一定那么好找，但是从另一个层面来看，至少我我我个人能看到的一部分数据是说，包括我们自己的那个model aza service，那个API平台。

质朴的API平台，包括我我我们也大约了解一些友商，包括大厂云厂商的呃，包括微软其实也release部分数据，实际上模型的这个API调用的量，token产生的量呃是非常非常大的。

然后这些用户其实都是一些商业，甚至都不是IT有IT有信息领域的，其实还有很多呃传统行业的一些呃，传统厂商在尝试用大模型呃，这个应用到日常的这个生产当中呃，就是说怎么形容呢，就是说其实很多厂商。

很多行业也在尝试的过程中，我们可能在稍微耐心一点，给他一点时间，有可能呃从一个相对更长的time frame呃，那个时间片来看，可能会会有更大的一个一个一个效果，对对对，好的你的第二个问题是。

谢谢宋老师，第二个问题是呃，外媒近期也关注到报道啊，有很多的这个报道提到，就是美国现在可能会针对啊，人工智能领域的一些这个技术对中国实行封锁，尤其是彭博社最近报道的，就是可能要进一步限制中国。

获得用于制造尖端芯片的，这个缺环绕山脊的晶体管技术啊，然后包包括可能会限制高呃，高带宽内存HBM的一个技术，对对中国的出口，想问一下各位老师，就是嗯美国对这个中国的这种技术上的。

这个在AI领域的这个技术上的封锁，会不会对咱们国内大模型研发呃，产生进一步的影这个影响，然后嗯怎么怎么去应对它，好的嗯，哪位老师可以先讲一下，张老师，你要不要先来，那那我就抛砖引玉啊，简单说说呃。

我觉得影响肯定会有嗯，首先我们看到这个比如说就以我们学校为例啊，这个现在就就很难买到那个英伟达的卡嘛，啊这个这个众所周知的一些原因对吧，然后包括很多国内公司也买不到这些卡。

它就会直接影响到算力的这个问题，但是我我我也知道，咱们国内像比较那个做的比较好的，像华为，他们也在有一些这种嗯升腾系列的这个卡，呃，其实我相信我们国内很多企业啊。

很多做芯片的公司一直都在努力去把这个生态，把我们从底层的基础设施，包括我们的那个呃大新训练深度学习的框架，像那个呃国外我们现在都知道排球拍都呃，不那个国外都知道那个PTORCH。

那国内也有一些像国啊华为的minus sport啊，国内的像那个paddle paddle啊，还有像我们呃清华这边还有那个G图等等框架，其实都都都在朝着方面努力，就是说我觉得在肯定在很有一段时间。

我们可以有一个完全自主的一个生态建立起来，那那个时候其实就不用管，美国的那些什么所谓的封锁了，对赵老师还有补充吗，没有好的，那我再帮大家问最后一个问题，然后那么刚才也说到算力。

其实大模型时代我们知道非常重要的一个工具，就是啊必要素就是算力，那我们也因为身在高高校，也会有这方面的一些啊，就是资金啊，然后不如这个啊工业界，我想为各位因为有在工业界的，有在学术界的，还有像东老师。

可能嗯两边都有的对，然后来呃逐一谈一下，就你们觉得在现在这个时代啊，工业界和学界相比有什么优优势和劣势，然后你们未来觉得嗯有什么期望，让让自己所在的这个啊，这个位置可以更好的做呃，大模型。

大圆模型的研究，要不赵老师先来好的，这个问题很很难，这个问题就是不是难回答，是难解决，因为我觉得现在可能整个高校最大的一个问题，就是能拿到的卡数太少了啊，然后所以导致的一个问题是。

现在呃高校里可能就说老师或者是学生吧，真正呃训练过大模型的实在是太有限了，因为资源就那么一些，对我觉得这个从长期来看肯定还是很不利的，因为学校里面可能很大的一个好处，就是我没有什么利益上的。

这个我我要做的事情，可能就是呃去把我的成果去发出来，然后我我没有必要去遮掩一些东西，然后没有必要为了KPI去做一些事情，就是我们可以做非常自由的一个探索，然后是什么，其实我们想公开，我们就公开了对。

所以这是非常大的一个一个B公司的一个优势，就包括欧I，他们内部应该有非常非常多的这个技术，但是他们这些人你可能永远也没有机会给他，至少这这一年去说出来对啊，但从长远来看，确实算力现在基本上已经成为高校。

发展大模型科研，我觉得最掣肘的一个一个一个事情，那现在也也没有特别好的一个方式，我觉得可能还是要和企业，包括算力中心啊去进行这种联合性的一个开发，甚至我觉得是经常会呼吁，就是说那国家有没有可能去呃。

为高校的一些科研团队去配备额外的一些算力，去让有一些有有能力去做大模型研究的人，能拿到这个算力，对好的，那曾老师嗯，您的那个卡还是很充足的是吧，呃怎么说呢，就是对于做大模型来说，卡永远是不够的啊。

因为呃就是大模型，其实从目前来看就是实验还是很重要的，就是呃实验比较偏多的话，其实我们就需要投入很多的人去做实验对，然后呃从目前来看，就是至少因为我现在在面壁嘛，然后从公司这边来说。

我是觉得就是大模型的话，其实还是一个呃科，就是科科研的前沿工作，和工程化相结合的一个事情，因为其实在我们最开始做大模型，就20年那段时间的时候，就是呃当时国内国内甚至连能跑大模型的。

就是那种集群都不存在，因为都没这种需求，从我们当时从最底层的模型真的搭起来，发现里边有特别多工程上的问题，然后以及就是到后来就是真正做大模型的训练，它的数据，它的对齐以及大模型相关的标注。

以及怎么我们利用sk in law，能批量的去去去跑我们的实验，然后能够通过我们的实验，去更快的挖掘出大模型，那些还没有被挖掘出的结论等等，这些其实都是非常工程化的事情啊，同时呢大模型的研究呢。

它其实还是处于一个非常快速的，一个在演进的状态，所以的话其实它的前沿探索也是非常重要的，所以我觉得其实真正要做的好，大模型，还是需要能够将这种产学研相结合的一种形式，才能把它做好，非常好，张老师呃。

我就简单说两条吧，首先一条就是感觉现在很多，国内就是学学学界的老师，一般都会考虑跟公司合作来做研究啊，这个能够缓解卡的问题啊，这是一条嗯，第二条的话，就是如果说说要学学校，做纯这种学术的研究的话。

现在大模型时代可能嗯，一些机理性的工作可能更加适合一点，因为这个可能不太需要太多的算力，我们可能只要能够解释某些现象对吧，可能在一些小规模的模型上能够work，同时在一个啊中等规模的模型上。

也能够验证它的结果，我觉得就已经能说明这个东西还是合理的了，对非常好，董老师嗯，呃我想想，首先呃非常支持各位老师讲的，我我们可能肯定是需要找到一种产学员的方式，尤其在在咱们国内。

能够让呃更广泛的人跟咱们同学，咱们实验室，咱们老师呃，包括跟业界一起来探索大模型的发展，但是从另一个角度来看，其实现在此时此刻，大模型我我我当然只是我个人的观察，就是说呃大模型的所谓的前沿探索。

所谓的这个这个大家都在训模型，其实某种程度上它已经变成了，有一点像那个包云岗老师提到那个词，重工业科研，其实它本质上呃，或者说现在的本质，实际上就是呃工业界更擅长的一个事，工业界实际上在刷点。

在提升这个模型的performance，在一步步往前推这个模型呃，IMPERCOLATE的，推模型的这个这个能力的这个边界对吧，其实某种意义上，其实现在有一点像那个大家也经常举那个例子。

就是说其实有点像那个我们现在就有点工业界，有点像把这个这个莱特兄弟一样把飞机送上天，现在现在飞100米高，目标是飞101米高，1000米高，但是最终如果说要把这个飞机呃，做成今天的这个A380啊。

什么这个这个这个空客787呃，Sorry，风洞实验这些更理论的东西来指导，怎么更好的设计飞机，怎么更好的呃指导训练模型的各种呃，这个这个参数的设置啊，各种design的决定可能更多的需要呃理论的来支持。

所以我觉得这个学界其实在这方面还有很大的，呃空间呃，当然可能是一个delay的过程，然后紧接着这个，我其实我们也经常在讨论的一个事，其实如果我们换个角度来想呃，怎么来形容呢。

其实你看GPT3是20年年初放出来呃，在GP3上用RHF，也是2020年就放出来一个版本，然后skilling law，如果你看那个paper open也是2020年初放出来的，那也就是说某种程度上。

无论是我们学界还是我们业界呃，不管国内国外，其实大家都是呃某种程度上是呃没有意识到，这件事，其实更多的是需要我们大家一起来探索，一种可能性，一种机制，让我们至少如果已经有前沿的呃，可能性前沿的方向的话。

我们能够一起推进，然后最后的话呃我想想就是卡多这件事，你说业界咱们国内的业界，我相信相比open微软，google还是大概率有一定的差距，然后我其实呃也有一个一直有一个怀疑的点，就是说我们自己做实验呃。

这个发现就是说我们在1B6B，12B等等的模型，更比如说更好的数据，更好的方式，更好的配比，更好的参数设置等等，但是都不用skill到更大skill，到这个这个千亿的时候。

我们现在能SSKILL到的程度，实际上之前在百亿，在几10亿的很多结论都不成立，imperial的结论都不成立，那也就是说如果SK到更大的一个程度，但是我们现在又没有机会从我们的视角。

其实也没有机会死SK登到更大的一个程度，实际上就说我们现在的很多发现，很多观察很多论文，很多发表是不是真的合理，这件事其实我觉得其实无论是学界和业界，其实都被算力锁在这，至少目前此时此刻的状态。

当然最后我们肯定还得充满信心呃，乐观的就是无论是各个生态圈，大家一起来折腾，对对对嗯，太好了，贺老师，OK那个我记得钱钟书有一本书叫做围城，然后有一句话就是城里面的人想出去，城外面的人想进来。

我觉得现在实际上就有点像这种情况，就是工业界，刚才实际上我们看了很多，之前老师在工业界的slides里面，也发现了各种各样的问题，但是作为学术界的人，实际上我们对此一无所知，因为我们并没有过这样的经验。

然后去做，所以如果要去想办法，能够去去达成一个比较好的一起推进，我觉得最好的办法就是把这个围城的墙打破，能够想办法让工业界和学术界的人一起去看，一起去解决问题，只不过是说大家所采用的策略。

解决的角度是不一样的，只有这样才能够最好地推进整个的东西往前走，因为我想到open i里面，实际上有很多很多就做理论的人，包括各种各样数学背景的人，也有很多编程背景的人，大家都是一起去推进的。

只有这样的话才能把一个东西做到极致，非常好，所以说到算力尽管是一个很沉重的话题，大家还是看到了希望，那么到最后呢，除了这个扎心的话题，我在想以一个比较乐观的结尾给大家，所以每位老师能不能畅想一下。

假如你没有算力的啊，限制你有无限的卡，你这一年或者说你未来最想做的是什么，topic也给在座的啊，各位观众有一些启发，赵老师要不请你先呃我我我的我的话，我很还是希望就是说，比如说如果真的有无限的卡。

能不能不能训练出一个商业水平的大模型，因为学校里面的算力确实比较有限，如果是真的放开，那你算力到了能不能达到商业的水平，对好还是想了解里面的秘密，呵呵对，其实我一直有个好奇的点啊，你看skin law。

它是一条非常直的线，我一直在想这个线的尽头是什么啊，如果我们真的到达了尽头，它会得到一个什么样的模型啊，我其实对这个特别好奇，如果这无限多算力，我估计会去镜头看一看。

对skin love的镜头是不是铁岭是吧，呵呵好，张老师请对哎，我其实挺期待这个如果有这么多卡的话，能不能真的把这个transformer变成，像transformer一样的这种能力啊。

因为我们其实做很多，不管是机理也分析也好，那个完全没有办法去做呃，100币以上的这种这种实验，甚至这些这些模型开源的也很少啊，那这种真正的能力很强大的，很多能力是可能是呃跟一些小模型的机制时。

可能是有点不太一样的，那这种背后它这个背后的原理又是什么啊，如何去啊，真的把这些模型的背后的，这种底层的原理给挖掘出来，我觉得我挺想研究研究的，对嗯还是好奇对，给东老师请这个更多的算力对。

其实其实这个假设性的问题，我们也经常问我们团队自己的人，如果突然给我们10万张卡，我们干啥哈，对呃我我可能没有一个，无论是呃一个特别的想法吧，其实可能更多的是，如果我真的有10万张百万张的卡，我可能会。

我们可能更多的是让我们team里的这些呃有想法，有创造力的同学，有有有有有活力，有精力的同学，让他们的人均卡量上来，我我我感觉自然就会有好的结果吧，嗯好的一个好的环境，如果我有非常非常多的卡。

那个时候一定是我老婆早上起来的时候，给我一个嘴巴说醒醒，别做梦了，该去上课了，好的太幽默了，贺老师好，那我们今天的论坛就到此结束，谢谢各位坚持到最后。



# 2024北京智源大会-开幕式及全体大会 - P1：智源进展报告：王仲远 - 智源社区 - BV1uH4y1w77V

尊敬的各位领导，各位来宾，各位专家，各位朋友，大家上午好，再次欢迎大家来参加今天的志愿大会，我是王仲远，非常荣幸能够从黄老师手中接过接力棒啊，在志愿大会上继续向大家报告。

支援过去一年的研究和工作方面的一些进展，智源研究院是2018年11月份成立的，一家人工智能领域的新型研发机构，我们致力于推动人工智能技术的原始创新，智源的含义是智能的源头，我们希望能够成为学术思想。

基础理论，顶尖人才，企业创新以及发展政策的源头，智源智源大厦位于海淀区成府路150号，这里也是海淀区人工智能创新街区，以及海淀区人工智能大模型集聚区的呃，核心区啊。

因此也也非常欢迎大家来智源大厦做一做啊，进行学术的交流，资源是一家非盈利性的科研机构，我们致力于啊人工智能领域前瞻性，战略性原创性的研究和技术突破，我们拥有顶尖的学术顾问委员会。

张宏江博士是我们的顾问委员会主任，那么其余七位委员均来自全球最顶尖的，学术机构的呃院士，在过去5年，智源率先啊遇见了人工智能大模型时代的到来，早在2020年10月份。

我们就已经成立了一支百人的技术攻关团队，开始进行悟道系列大模型的研发，在2021年3月份的时候呢，我们发布了悟道1。0，6月发布了悟道2。0，那么悟道系列大模型的发发布，在当年都创造了中国首个啊。

全球最大这样的一系列的纪录，那么进入到2022年之后，我们的悟道二系列大模型继续向多语言，多模态啊去持续的迭代，在去年的智源大会上，我们也发布了悟道3。0系列的成果，那么应该来说资源呃。

智源研究院与大模型这三个字是紧密的关联啊，甚至大模型就是志源最早提出来的，进入到2023年啊，大模型从研究机构的科研成果开始，向产业界啊逐步的发展，我们也看到在过去的这一年啊，百花齐放。

有越来越多的这个大模型啊，在在过去的这一年啊发布，那么如整个人工智能的发展浪潮啊，过去七八十年的这样的一个发展历程，那么以2023年为界，基本上可以分为两个大的阶段，都属于弱人工智能时代啊。

也就是整个人工智能的这个模型，它是针对特定的场景，特定的任务，那么需要去收集特定的这个数据啊，训练特定啊，训练特定的模型，那么比如说战胜人类世界围棋冠军的阿法狗啊，他能够在围棋上下的非常好。

但是它却无法直接用来解决医疗问题，无法直接用来做自动驾驶，虽然方法可以借鉴，但是针对不同的场景任务都需要去做啊，这个数据和模型的重新的收集和训练，那么进入到2023年，随着大模型的发展。

人工智能将逐步的进入到通用人工智能的时代，那通用人工智能呢最大的一个特点呢，就是它的规模非常的大啊，出现模型具备涌现性，同时它能够跨领域的通用性，那么在过去的这一年。

scaling law是被反复讨讨论的一个啊一个名词，它的一个基本的含义呢，就是说随着模型的参数啊，以及训练数据量和计算量的持续的增大，模型的性能也持续的提升，但是如果我们回看过去七八十年。

人工智能的发展历程，尤其是神经网络的发展历程，实际上skin low并不是一个新鲜的事物，那么这张图的左边的部分，实际上是我2018年做的一张PPT，那么我们可以看到。

事实上在人工智能过去的三次发展的浪潮，每一次新的人工智能技术的突破，都是伴随着模型参数训练数据量，计算量的一个持续的攀升啊，所带来的人工智能的领域的一个突破，那么进入到大模型的时代。

我们看到大模型的参数是在以量级啊，每年都以量级的提升一个量级的速度在发展，在18年时候啊，BT的模型基本上还是E级的参数，到2021年，GPT3就已经是1750亿的参数，到去年的GPT4啊。

业内普遍认为是1。8万亿的参数，可以看到啊，它与人类大脑的参数啊，科学家们普遍认为，人类大脑的参数在一呃，100万亿到1000万亿之间，那么整个大模型与人类大脑的参数呢，其实从过去几年。

从相差100万倍到1000倍到去年100倍，那么如果按照大模型的这个速度继续发展哦，我我们会认为在未来几年，大模型的参数很可能就会改善，或者超过人类大脑的参数，这也是我们认为AGI时代。

有可能会在未来几年啊，到来的一个很重要的一个原因，那么如果ADI时代会到来，它可能的一个技术演化的路径会是怎样的呢，我们知道在过去几年啊，绝大部分的这个科研的关注度。

包括咱们产业的关注度都在大语言模型的突破，但大语言模型依然是一种单模态的模型，那么在这个世界上，除了文本这个数据以外，还存在大量的图像，视频，音频等等这样的多模态的数据。

而这些数据量呢可能是文本数据的十倍，百倍乃至千倍的规模，因此呢在这两年开始，这些年开始也开始有呃，多模态大模型这方面的一些研究啊，但基本上呢产业界里面还是呃针对不同的模态，跨模态就有各自的模型。

同时呢理解和生成也是分开的，那么我们认为从技术发展的路径来看啊，最终会形成一种统一的多模态大模型，它能将理解和生成统一，它能将不同的模态数据进行统一，那么当多模态大模型能够理解和感知决策。

这个世界的时候啊，那它就会有可能进入到我们的物理世界，那么如果进入到宏观的世界跟硬件结合，那么这个可这就是巨生大模型的发展方向，如果它进入到了微观世界，去理解和生成生命分子。

那么这就是AI for science，那么无论是聚生模型还是AI for science，亦或是多模态模型，都会促进整个世界模型的发展，最终推动人工智能技术向ATI方向发展。

那么基于这样的一些技术判断啊，资源研究院在呃多模态大模型，聚生大模型以及生物计算大模型上，将在未来几年持续的投入研发，那么今天也非常高兴啊，借着智源大会的这样一个场合呃，向各位朋友介绍一下。

我们智源在大模型上的全家桶，那么今天的这个报告里面会包含五部分的内容，分别是我们在语言大模型，多模态大模型，聚生大模型，生物计算大模型，过去一年的一些研究方面的一些进展。

另外呃一个工作是支撑所有这些大模型，技术迭代的一个基座，也就是是一个算力集群的操作系统好，首先我们来看看智源研究院，过去一年在语言大模型方面的一些进展，我们知道过去这一年啊，其实呃各家公司。

各家企业都训练了大量的这个模型，尤其是大语言模型，那么企业已经做的事情，资源研究院就已经呃，就不会再去再去重复的做啊，对于语言大模型的历史使命呢，在过去几年资源已经为整个技术的推动啊。

整个产业的发展作出了卓越的贡献，因此在大语言模型方面，我们主要要解决产业界的共性的一些痛点，那哪些是产业的共性的痛点呢，比如说算力的缺乏，那么因此呢，我们与中国电信人工智能研究院一起。

联合研发了一种基于生产技术训练的，全球首个第一碳单体稠密万亿语言模型，简单来说，我们就使用了行业里面不到10%的算力，也就112台AA800啊，那这样的一个算例呢。

我们就能够训练出啊这样的一个dance model，万亿参数级别，同时在整个训练的过程啊，基于我们呃量呃超最优的超参预估技术，我们实现了整个训练全过程的零调整，零从试，最为关键的是。

我们不仅会将这个模型开源，我们还会将其中的技术细节，loss曲线啊全部进行开源，那么这也是智源研究院为开源社区，为整个产业界所作出的啊重要的贡献，那么我们整个模型其实依然正在训练过程中。

我们对于自己在生产技术训练的这这个中间的，一个过程的版本，我们也进行了评估，那么评估的结果显示，我们的这个BPP的loss曲线，确实要优于拉马思瑞，那么整个万亿级别的这个dance model。

当我们把它训练完成之后，我们将把它完全的开源，也希望能够为社区训练万亿参数的稠密模型，提供一个优秀的初始参数的版本啊，避免万亿参数模型早期难以收敛的，这样的一些具有挑战性的问题。

同时基于这样的一个基座模型所训练出来的，对话模型，我们也进行了一个初步的评测，结果显示，它能达到GPT480%到90的水平，那么请大家要注意，我们仅仅使用了100余台的A800机器。

来训练这样的一个模型，那么除了算力紧缺的这样的一个问题之外啊，大模型在产业界落地，最重要的另外一个挑战就是他的幻觉问题，那么在这边我也想向大家隆重的再次介绍，我们的呃BGE模型。

那么我相信所有产业界的朋友啊，一定对于这个模型是非常非常的熟悉，那么因为它是啊，全球下载量最高的国产AI模型，也是最普及的开源向量模型，那么我们的研发团队呢啊，基于这种创新性的无监督预训练。

和多阶段的对比学习，以及构建了一个多语言啊，关联文本的数据集CMTP，那么基于这样的一个呃高质量的数据集，以及我们创新的算法，BGE模型，从发布之初起就一直保持国际领先的位置，那么也正是得益于这样的呃。

又好用又轻量级的这样的模型，所以它在开源社区广受欢迎啊，我们可以看到它的下载量是持续的攀升，并且得到了全球主流大模型应用框架的集成，包括hacking fs，那么index啊，lam chain等等。

同时呃各家云服务厂商，像ARILAWS火山引擎，腾讯云，华为云，百度云都集成了BGE的模型，并对外提供商用，我想这就是智源研究院对于开源社区，对于整个产业界的一个重大的一个贡献。

那么以上就简要的介绍我们在啊语言模型方面，解决共性问题的一些研究的一些进展，下面我想给大家报告一下，我们在多模态大模型的，过去一年的一些研究的进展，多模态大模型依然处于一个，持续的迭代和演进啊。

技术路线还没有收敛，那么因此呢智源研究院在过去的一年，持续的在视觉多模态领域，发布了各项领先的研究成果，引领整个开源社区，去年7月份的时候，我们发布了第一代的email啊。

这是一个呃生成式多模态运预训练模型，去年12月份的时候，我们发布了emu two呃，它截至目前依然是开源社区最大，性能领先的深层式多模态大模型，那么今年2月。

我们还发布了EV i clippers8B模型，它是开源社区最大啊，性能领先的180亿参数视觉表征的click模型，它也被用于非常多的呃，多模态大模型中的视觉编码器的部分，那么整个多玛太大模型。

过去这几年以及在未来的几年，我相信都会非常的火热，但是它的发展现状是此动模态非彼多模态，我们知道在行业里面，各种模态的转换已经有了很多优秀的模型，比如说像图像理解和视频理解有g p t four v。

比如说像图片生成有stable diffusion，大力E，比如说像视频生成有solar will，比如说像语音理解有最近发布的啊，G p t four o，那么每一种模态之间都有这样的一个呃。

主流的一些模型啊，他们以多种模型的方式来存在，那么从技术路线上来看啊，到底是使用diffusion model还是使用auto aggressive啊，到底是这种单一的跨模态还是统一的多模态。

那么到底是理解和生成应该分开，还是应该呃结合，那么到底是基于这种组主城式的呃，组装式的这种多模态，还是要原生的多模态，其实存在不同的技术路线的一些一些争论，那么智源研究院呢以终为始。

我们呃基于对于前面的技术路线的发展的判断，我们还是非常坚定的，要走统一原生的多模态的技术路线，我们去挑战整个行业里面最难最呃，最具有挑战性的一个技术路线，那么这个技术路线如果实现突破。

相信对于整个产业界，对于整个社区又将是一次重大的技术贡献，那么这就是我们正在训练中的鹅苗山啊，那它是统一了文字图像视频啊，使用自回归的技术路线，那么实现了图像视频文字的输入和输出啊。

并且实现具备更多模态的可扩展性，那么我们的鹅苗山系统，目前其实依然是正在训练的过程中，很坦然的讲，中间的技术难度和坑还是不少的，那么呃我们呃今天非常高兴的，借智源大会的这样的一个场合。

也跟大家分享我们训练中啊，目前的一些进展，那么整个峨眉山的研发的目标是，原生的多模态世界模型，那么在这里原声指的是，我们从一开始就会将多种的模态进行融合，同时将生成与理解进行融合，能够扩展。

那么并且由于是auto regressi的这样的一个框架，它也能够进行持续的可控的这样的一种交互，这是我们鹅苗山啊中间的一个check point in的模型啊，目前能够具备的能力。

所以这是呃它的图像生成能力，那么可以看到它的图像生成能力，还是非常非常的优秀，那么呃我想强调的一点是，它不是基基于diffusion model，而是基于auto regressive的这样的一个。

智回归网络的呃，鹅苗三，同时同样的这样的一个模型，它也能够进行视频的生成，那么这是我们峨眉山当前一些视频生成的呃，一些中间的一些结果，我们可以看到啊，它还是能够呃去捕捉到整个这个世界啊。

世界模型的一些规律，那么中间依然有不完美的地方啊，但是我们的模型依然在持续的训练的过程中，同样啊这个模型也是基于智回归的网络，是与刚才的图像生成是同一个模型，那么还是同还是同样的这样的一个单一的啊。

统一的多模态模型，它还具备图像和视频的理解能力，比如说在右上角的这个视频，我们问他啊，他有什么感觉，我们的模型能够回答，他感到了一种幸福兴奋的感觉啊，我们问右上角除了这个人在做什么，他能够识别呃。

理解这个是一个失望的男人，正在看手机上的消息，那么左下角的这张图片呢，我们在问说诶，这个最近的这个交通灯是什么颜色，我们应该怎么做，它也能够识别出其中的红灯啊，我们可以看到其红灯的这个信号。

还是非常的微弱啊，那我们的模型也能够理解，那么我们请他描述右下角的这个视频啊，它也能够去识别出这是一个动态的呃，天空上面有非常多的云，那么还是想强调一下，这跟刚才是同一个模型。

而且是基于一个智回归网络的鹅苗山，那么我们的email three模型啊，它依然在持续的训练过程中，当模型训练完成啊，经过安全的评估，我们也会将其逐步的开源啊，如果各位朋友呢等不及也可以先尝试我们的呃。

上上一代的这个鹅苗EMONE和emu two啊，他们已经在社区里面进行开源，如果大家觉得还是不过瘾，也可以试一下，我们一个轻量级的图文多模态模型呃，班里那么bi呢它是一个基于灵活的架构。

能够支持不同的视觉编码器，像刚才提到的EV a clip，然后也能支持不同的这个语言基座的模型，然后能够实现这样的一个图文多模态的，一个轻量级的模型，所有的这个模型的啊。

模型本身数据训练代码我们全部都开源，那么这个就是智源对于整个开源社区，对于咱们产业界的一个贡献，更多关于资源在视觉多模态方面的成果，大家可以访问我们在呃开源社区的一个主页啊，来了解。

那么接下来想给大家介绍的是啊，我们在聚生大模型方面，过去一年的一些工作进展，那么我们刚才也提到，多模态大模型，能够帮助计算机去感知和理解这个世界，那么接下来它就能够演化成一个智能体agent。

那么我们也看到了，最近其实像MICROSOFT口拍了apple intelligence intelligence，那他们呃确实能够去控制这样的机器，开始使得AI手机AIPC啊开始成为可能。

那么我们也在过去的这一年，研发了一个通用计算机控制的系统，叫CREDLE啊，它能够像人类一样看着屏幕，通过鼠标键盘完成计算机的所有的任务，反思过去，总结未来啊，总结现在，规划未来。

那么这样一个在数字世界的agent，如果又进入到了物理世界，那么会发生什么呢，我们可以一起来看一个短片，你好，Being in，帮我打印费树里的文件，对当数次世界的android进入到了物理世界。

那么这就是聚生智能，基于我们对于聚生智能作为整个大模型，技术路线发展上的这样的一个重要的一个判断，因此在过去一年，资源研究院也非常坚定的在聚生智能，这个方向上持续的投入，我们在机器人的末端操作。

在聚生大小脑在导航啊，在这个自研的硬件上都有呃，都有一系列突破性的一些成果，下面我也给大家简单的介绍，那么机器人的抓取呢，是整个机器人最最重要的啊，一个一个最基本的一个操作。

那么过往呢这种通用泛化的抓取呢，面对这种反光的物体，面对透明的物体应啊经常会失败，那么我们通过构建了一个大规模，高质量的在仿真系统中，构建了千万量级的场景，以及超过10亿的抓取的这样的呃数据。

训练了一个通用抓取的模型，然后实现了simple real的这样的一个抓取技术，的显著的提升，我们在工业级的真机上，能够实现超过95%的抓取的成功率啊，创造了世界的纪录，我们今天也将这样的一个机器呢。

其实带到了这个智源大会的现场啊，一会也欢迎大家能够去啊我们现场的展厅啊，展台去做体验，那么除了这样抓取的技术以外啊，巨神智能最最让人感觉到兴奋的呢，是他的这种思考的能力啊，这就是大模型。

给巨生机器人可能带来的一种新的变化，那过去两年啊，过去一年我们也研发了两个啊这样的专模专用，各司其职的这种大模型，那么sage呢是一个能够反思，可随机应变的操作大模型。

我们基于三维视觉的小模型加图文的大模型，能够让机器人啊，在失败的时候能够继续啊去反思，然后去重新规划它的一些啊操作的动作啊，进而去啊实现啊实现再一次的尝试。

那么open six store呢是一个全球首个的开放指令，六自由度的呃取放大模型，我们知道这个呃google发布的RT系列的机器人呢，它能够实现三自由度，但是如果我们要让这个抓取啊能够真正的实用。

我们不仅要考虑物体的位置，还要考虑它的姿态，它到底是立着的还是还是横放着的，那么这些才能够使得这个抓取的技术，真正的有效，那么我们的，我们在呃这个这个这样的一个抓取上，也是最终实现了技术的突破。

那么这一这个技术呢一样的，今天大家可以在现场是可以体验得到的啊，机器机器人还需要行走起来，因此我们在过去的这一年，也研发了一个面向技术终局的，端到端聚生导航大模型啊。

那我们知道过去机器人它是需要依靠离线啊，提前建好的这个地图来实现导航，那但是人类其实并不需要这样啊，人类完全依靠视觉，那么为了让机器人能够啊真正的智能化起来，我们也实现了一个纯视觉。

纯seem to real的这样的一个解决方案，真正的实现了video language in action out，所以右边的这个视频展示的是，我们在智源大厦的内部啊，包括刚才其实是天呃。

就智源大厦的天台，也是今天呃晚宴的一个位置哈，也非常欢迎大家去智源大厦参加我们的晚宴，那这个这是真正的在虚拟的环境里面，我们实现了训练，然后直接就能够在真实的场景中去进行泛化，有这样的一个导航的大模型。

那为了将我们的这些大模型啊，巨生的大模型能够真正的进行落地，我们也与这个北京银河通用机器人公司啊，一起去基于他们呃迭代的这样的一个硬件呃，聚生的一个轮式的机器人，然后来来将我们的这些模型和技术。

进行了场景的落地啊，那这个场景呢包括了像无人药店，包括了像家庭服务，我们可以看到右呃，最右边的这个视频里面，机器人已经能够去啊自动化的这样的清理垃圾，那中间的无人药店呢，能够根据用户下单的这个药品去啊。

去去自动的智能基于视觉的方案啊，能够去啊去拿正确的这样的药品，那么我们的机器人还能够去思考啊，比如说当我说我渴了，我们一起来看下机器人会如何反应，我渴了，好的给您带个土豆，对如果告诉机器人，我饿了。

看看他的反应，我饿了，现在有橘子香蕉，您要两个橘子，谢谢好的，给您拿橘子，可以看到机器人能够根据啊，用户的开放的指令进行思考，还能够与用户进行啊交互，在基于我们前面所提到的，这种泛化的抓取能力啊。

能够真正的去实现这种啊，这种通用的泛化的执行啊，指令执行，我们也将这样的一台机器人，其实带到了支援大会的现场啊，在对面的这个会议中心的展厅中啊，如果大家有兴趣的话，可以去现场的体验一下。

那么除了像无人药店家庭的场景以外，聚生机器人，在医疗场景也会有非常重要的一个落地，那么在过去的一年，实现了全球首创的智能心脏超声机器人，并且在真人上实现了啊自主的超声扫啊扫扫描。

那呃我们将我们的这个机器人的这个呃，这个心脏超声的这个结果呢，医生的这个扫描的结果进行了对比，那么我们发现在准确性，在高效性上与人类的医生是基本持平的，但是它的稳定性呢和舒适性啊。

是显著的高于人类的医生，更为重要的是啊，现在整个全国这样的一个超声机器啊，超声的这个医生是非常缺乏的，我们也知道经常在超声的这个这个这个科室啊，排队也是排的最久的。

那么它对于整个提升我们在呃超声这一块的，整个医疗的普及度和水平啊，都有非常重要的意义，我们也将这样的一个技术呢孵化出了一家公司，那么也非常欢迎大家关注我们的这个公司，那么在未来啊。

整个聚生智能依然有非常多的技术问题，非常多的产业落地的问题是需要被解决的，我们也将联合像清华北大，中科院这样的高校，以及像银河加速净化等等这样的产业，上下游的呃产业链，然后有希望有更多的生态合作伙伴。

跟我们一起来解决，聚生智能中的一些核心关键性的问题，包括像数据要素，包括像模型以及场景应用，那欢迎呃，全国所有对于这个聚生智能感兴趣的高校院所，企业与智源研究院联系，接下来我想给大家介绍的是。

我们在生物计算大模型方面的一些进展啊，刚才提到了深层式人工智能，已经推动了整个人工智能啊，领域的一些重大的突破，那么当它进入到微观的世界，我们是否可以用相似的这样的一个生成式技术。

来解决生命分子的理解与生成的问题呢，这就是智源进行生物计算大模型研发，的一个初步的一个思考，同时它对于整个产业界也有极为重要的意义，我们知道在药物研发领域，从有一个双死双死定律，从新药的研发到它的上市。

通常要耗费10年以上的时间，以及10亿美金以上的投入，那么其中三四十%呢是在啊药物，在药物设计也就在临床前的部分，这也是人工智能最能发挥的呃作用的地方，那么除了像化合物的这种筛选和预测以外。

那么对于大分子的这种结构的建模和预测，也能够推升，像基于RNA这样的大分子的一些呃，新药的设计，那么这也正是人工智能啊，对于医疗领域的一些可能性的一些贡献和突破，那么基于这样的一些思考呢。

我们就设立了open complex的这样的一个项目，他希望能够研发啊统一的生物分子计算模型，打通啊，基础生物分子像蛋白质啊，RADNA小分子之间的这种壁垒，并且能够研究生物分子之间的相互作用的关系。

那么我们构建了一个全原子的生物分子模型，它是一个decoder only的模型，那么我们可以一起来看一个短片的介绍，生成式人工智能技术的发展，正在加速人类对生命奥秘的揭示，了解生命的过程。

必须观察和理解数10亿个生命分子间，的数百万种组合和相互作用关系，这一庞大的数字计算工程，无法用传统的物理方法高效完成，智源研究院开发的open complex，基于生成式人工智能技术。

能在原子层面进行蛋白质RNA，DNA小分子的结构和相互作用关系的预测，精度达到超级计算机的水平，这样的能力使得科学家可以进一步理解，生命的机理，未来我们希望逐步构建一套微观生命，科学的孪生系统。

为人类理解自然本源带来新的可能，Open complex，好我们的open complex在国际权威的蛋白质呃，国际权威的榜单，卡密尔的蛋白质结构预测中，已经连续26个月稳居第一啊。

那么无论是在精度还是宏观结构方面，都优于同期的模型，像包括像alpha four two，那么除了像蛋白质结构预测以外，它还具备其他复合物的预测，包括像RNA呃，DNA以及蛋白质的这样的复合物。

那么我们也展示了我们的预测的结果，与ENT统这样超级计算机的预测结果，那么最终的结果显示呢，open compress已经初步具备了通路预测的能力，那么左边呢在这边的一组实验中。

左边是我们open complex的啊，预测结果，右边是啊ENTON呃，超级计算机它的一个预测，我们不仅结果相似，并且我们没有像ENTLE那样的一些噪音，那么呃这就是生成式人工智能。

所带来的一个技术的突破，我们能够使用非常少量的GPU，就能够实现，原来只有超级计算机才能够做的事情，我们还将这样的技术呢应用在了啊实时新呃，新站的计算建模上，那么实现了全球首个实时完生。

精湛的一个计算啊，通过GPU的这样的加速，能够将心脏的生物秒和计算秒是呃，突破到了一比0。9，真正的实现啊临床应用的可能，我们也正与北大第一医院安贞医院，长庚医院呃，朝阳医院进行合作。

将我们这样的技术应用在临床当中，以上就介绍了智源，围绕整个大模型技术发展路线啊，我们所做的一些研究，那么很多的研究工作依然在进行，也请各位朋友可以关注，我们在未来几个月以及下半年。

持续的对于一些研究成果的发布，那么所有的这些研究成果呢，都要依赖于一个非常强大的一个基座啊，那这就是我们的一个算力集群的一个操作系统，在去年的时候，我们发布了flag open1。0。

它是一个面向异构芯片，支持多种框架的大模型，全站开源的技术基座，经过一年时间的迭代，flag1。0升级到了2。0，整个技术的这个呃自底向上的这个框架，是更加的成熟，也更加的完备。

比如说有我们有这个面向不同芯片的算子库哦，我们有面向异构AI的计算的框架，我们有数据处理的工具，也有这个啊整个各种各样的算法，和我们前面所提到的像email啊，BGE这样的一些非常优秀的模型。

那么这样的一个开源的呃，整个开源的一个系统框架呢啊，能够真正的实现一站式领先的高效应用的，大模型的算法和工具，我们也与全球的主流的一些基金会合作。

像linux foundation i h o e i MB a以及HAGFISH合作，能够希望促进整个开源社区在人工智能，在大模型这个领域的一个快速的发展，那么基于open呃。

flag open中系统软件的部分，以及我们自研的九鼎平台，我们也构建了一个为大模型而生，支持异构芯片的算力集群操作系统，这个操作系统在过去20多个月内，已经稳定的运行。

支持了超过50多个团队来训练大模型，能够支持八种的AI的芯片呃，我们也非常欢迎啊，全全国各地的计算中心，能够适用我们的这样flag o s，接下来可以看一个关于flag o s的短评介绍。

可指定一种算力资源详细配置，开启跨不同AI芯片的异构算力自动迁移功能，平台依据flag puff工具，在不同异构算力上的性能评估，历史数据，将初始算力资源配置，智能映射至等效的其他算力资源。

实现无缝高效的自动化异构资源配置，九鼎平台启动全局算力智能调度，直至调度成功，平台基于TRITTON大模型算子库，实现任务跨AI芯片的自动迁移，集成的flag scale，进行并行策略的动态优化。

充分利用集群的异构资源，提升训练效率，缩短训练周期，降低训练成本，嗯flag open里面有非常多的模块，接下来我简单的介绍以下其中啊，flag os和flag open中一些核心，关键的一些组成部分。

那么其中一个呢是面向大模型的开源传承，算子库啊，在大模型的这个算子库中，我们也统计了下主流的，大概通用的有120多个算子，我们目前已经实现了48%的全覆盖，能够支持六大厂商的多种AI的芯片。

同时针对大模型专用的算子库，我们也有六个呃，Attention，flag attention的算子，那么能够覆盖主流的attention，并且呢呃一并且依靠着智源研究院，在大模型方面的前沿研究。

也能够紧随算法的前沿去打造创新的算子，那么呃flag of scale，是一个多元异构的并行的训练框架，我们也在业内首次实现了不同厂商呃，跨节点RDMA直连，以及多种并行策略的高效混合训练。

那么实现了首个多元异构芯片，scale up加scale out，两阶段高效训练的千亿语言模型，那么基于这样的一个异构芯片计算出来的模型，我们也将其在社区里进行了开源，今天我们还想发布两个呃数据集。

一个是千万级高质量的呃指令微调数据集，我们知道SFT阶段在激发大模型的能力上，是非常的关键啊，目前整个社区里依然非常缺乏，高质量的SFT数据，那么我们将开源千万级的呃，中英文的高质量的一个数据。

这样的一个指令微调数，以及我们的实验显结果显示，它能够让非常多的开源社区的呃基座模型，能够达到或接近GP4的水平，此外我们还将开源全球最大的中英文，多行业的数据集，它覆盖18个行业的种类，总计3。4。

T b，我们在医疗和教育啊领域进行的这个continue training，SFT加加DPO啊，训练结果显示它能够显著的提升啊，通用基座模型在领域里的一个效果。

我们的flag scare大家相信也非常的熟悉，在上个月，我们也发布了我们的资源的榜单和评测的结果，那么也非常欢迎行业里所有的大模型公司，能够使用我们的flag scale啊，Flag e well。

今天也非常高兴的跟大家分享啊，正是基于智源研究院在开源开放这方面的承诺，以及我们对于整个社区持续做啊，创新的这样的一些突破，那么我们呃flag open系列的所有开源的模型，框架工具。

在过去一年的全球首，总下载量超过了4755万次，应该来说在全国内的AI机构中，处于绝对领先的位置，那么这些优秀的成果呢，以及我们在前沿模型上的这些迭代啊，肯定是离不开啊优秀的人才。

那么因此也想借今天智源大会的这个现场啊，也打个小广告啊，欢迎大家截屏或者拿出手机啊拍照啊，那智源研究院提供了非常宽松的科研的氛围，高水平的平台以及全方位的这个福利的关怀啊。

非常希望全球最最顶尖的人工智能人才，能够考虑加入资源研究院，最后我们回到智源大会啊，智源大会在过去的5年已经成功举办了五次，我们邀请了全球30余个国家和地区，超过1000位的顶尖的专家呃。

来资源大会进行分享啊，其中包括11位的图灵奖的获得者，那么在去年的资源大会上，相信大家也是留下了非常深刻的印象，像星INGTONSAN奥特曼呃，Tech mark rusell。

他们都参加了我们的这个志愿大会啊，其中尤其是其中的这个，我们在去年第一次设立的AI，安全与对齐的论坛，我们知道，随着深层式人工智能技术的快速发展啊，通用通用人工智能时代有可能会到来。

那么AI所带来的安全的问题也是不容忽视的，那么智源研究院啊致力于在AI安全上啊，我们进行技术突破和研究，确保人工智能在发展的这个过程中，始终是造福人类安全可控，今年3月份呃。

我们的顾问委员会主任张宏江博士也发起了啊，志愿研究院，举办了首届北京AI安全国际对话，那么也邀请了包括像新INGTONBENJOR呃，RUSELL姚先生啊，傅莹薛兰啊等等全球顶尖的啊30余位的专家。

并且我们共同签署了北京AI安全国际共识，那么在今年的志愿大会上，我们也依然设立了我们的AI安全的论坛，明天一整天依然是大咖云集啊，非常欢迎大家进行关注，在两天未来两天的的日程中。

和我们的资源大会上有20余个的论坛，百场的报告有非常多的啊，这个朋友告诉我啊，看到我们的这个日程之后呃，第一个感觉是自己的分身乏术，那么我想这就是智源大会的魅力所在，那么一样的。

我们今年的智源大会依然是干货满满，我们有来自全球主流的呃这个模型啊，模型的技术负责人，项目负责人来介绍他们的最新技术，也有国内顶尖的啊，各个模大模型呃，技术负责人，同时我们也会对于大家对于呃。

大模型的各种关心的问题进行全面的啊解答，所以非常欢迎啊，大家能够享受未来两天资源大会的日程，那么今天上午的开幕式呢，在我的报告结束之后呃，有会有OpenAI的呃负责人，来介绍。

在多模态大模型方面的一些最新的一些进展，然后我们也会对于通用人工智能，进行一些讨论和对话，那么也请大家能够享受开幕式，接下来的流程，我的报告就到这。



# 2024北京智源大会-开幕式及全体大会 - P2：主题报告：多模态大模型-主旨演讲：Aditya Rameah-对话嘉宾：谢赛宁 - 智源社区 - BV1uH4y1w77V

能在这里发表演讲是我的荣幸，呃，我是Open AI视频生成的负责人，今天我想谈谈一些观察，关于，生成建模在过去的几年里一直在发展，我看到的事情正在走向何方，嗯，所以我想先谈谈呃，一个相当古老的结果，呃。

至少在深度学习方面，所以在2021年1月，我们发布了一篇关于大理一号的博客文章，规模很大，至少在当时，文本与量化图像联合训练的自回归变压器，嗯，我们决定这样做的原因是因为我们看到了生命的最初迹象。

带转换器的建模语言，我们想知道同样的技术是否可以扩展到模型，其他方式。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_1.png)

最后效果还不错，模型能够将标题作为输入，把它转换成量化的图像补丁。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_3.png)

所以它的工作方式是，您有提示符，您可以像普通语言模型一样建模，然后呃，我们还训练了一个，图像的vq自动编码器，图像的补丁只是增强了，与用于建模文本的正常词汇，整个被压扁的呃。

字符串只是由单个转换器建模为单个序列。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_5.png)

所以呃，最酷的是，呃，我们和达利一起看了缩放，就像我们今天看到的语言模型的伸缩一样，所以一开始如果你训练，呃，一种小尺度自回归图像模型，你可以看到灯光和反射，重复对象。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_7.png)

在小范围内为事物着色的能力，然后稍微大一点的规模，可以绘制具有多个属性的对象，改变艺术风格之类的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_9.png)

一旦你增加了更多的比例，您可以看到文本呈现之类的内容，成分概括，也是图像语境学习的标志，所以我们试着做一些事情，比如给达利·瑞文的累进矩阵，哪些是视觉智商测试，模特看到了，第一个呃。

这个网格里的八个元素，最后一个角，我们还尝试了图像到图像的翻译，在那里你给模型，呃，上半部分的图像，并要求它在下半部分画一些东西，这些东西开始起作用了，有时候。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_11.png)

呃，拥有十亿参数模型，所以我们想知道如果你进一步扩大规模会发生什么。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_13.png)

所以在阿里之后，我在想，你知道的，这是学习智力的好方法吗，呃，因为你在训练一个模特，压缩视觉世界中的所有像素，这似乎是一项相当困难的任务。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_15.png)

有很多信息需要建模，有一些，当时有一些研究，暗示这不是真的该走的路，嗯。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_17.png)

所以马克，呃，以前训练过的IGBT，这是第一个大规模的图像自动回归变压器。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_19.png)

这个模型不是以文本为条件的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_21.png)

但从这个模型中真正巧妙的发现是，仅仅通过学习充分好地压缩图像，模型学习视觉世界的底层结构，最终也得到了很好的图像表示，嗯，例如，当您放大这些igpt模型时，他们开始在金雀花探针上得到很好的结果，但是。

这比剪辑和剪辑同时发布的效率要低得多，作为多莉一号，剪辑背后的想法是学习文本和图像交汇处的任何东西，所以如果你想象有一个带有文本和图像的维恩图，Clip使用对比损失来尝试学习信息，那是在两者的交汇处。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_23.png)

这最终是数量级的，比igpt更有效地从图像中提取智能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_25.png)

我当时的结论是大理一号是一个有趣的项目，很高兴能继续努力，但这并不是如何提取的关键途径。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_27.png)

来自视觉世界的智能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_29.png)

嗯，现在我要谈谈剪辑是如何工作的，它如何提取图像和文本交汇处的信息。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_31.png)

我相信你们中的很多人已经对这一切都很熟悉了，但是剪辑学习一个图像编码器和一个文本编码器。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_33.png)

所以文本编码器接受提示，图像编码器拍摄图像，在训练过程中给出了夹子模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_35.png)

嗯，一个带有标题的配对图像列表，文本编码器对所有标题进行编码，图像编码器对所有图像进行编码，损失函数鼓励两个编码器匹配表示，嗯，每个图像及其相关标题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_37.png)

剪辑在它出来的时候是一个很大的范式转变，因为不需要手工制作的标签来训练一个好的分类器，做起来既耗时又痛苦，我们可以利用互联网上的自由文本，学习一个同时适用于所有领域的好分类器的模型。

所以如果你想把动物分类，您可以为动物的类别构造一个提示列表，你想分类的，然后现在，可以使用图像嵌入的点积，你想用所有的标题分类，然后取Softmax并使用这些分数来确定图像属于哪个类别。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_39.png)

在这一点上，我你知道。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_41.png)

似乎图像表示学习开始进化，呃，最初，深度学习有一些成功的初步迹象，我们都知道的金雀花分类纸，在那里，你训练一个分类器，它只是从图像中提取一些信息，即，呃，图像属于哪个类别的标签，嗯，许多年后，呃。

夹子出来了，现在我们能够利用互联网上的自由文本，学习通用分类模型，这样你就不需要那么多手工制作的功能工程了，过了一会儿，呃，最终，图像字幕器也是可伸缩的视觉学习者，所以与其用这种对比损失来建模。

文本和图像的交汇处是什么，我们可以训练一个有图像编码器的感知模型，查看图像并重建标题，就像一个学习从图像中预测文本的语言模型，所以随着时间的推移，事情似乎一直在简化，也许我们可以问一个问题，最终的结果。

我们最终会做什么，因为我们的失败，预算增加，所以看起来目标函数已经改变了，我们从图像中学习的方式已经改变了，当我们得到越来越多的计算，事情似乎变得更简单了，所以我想提供一个猜测，关于事情可能在哪里发展。

然后嗯，这就是我接下来要讲的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_43.png)

所以IGPT，T建议大规模生成模型自动学习数据的底层结构，最终产生良好的图像表征，考虑类似的结果是否也适用于文本到图像模型是很有趣的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_45.png)

事实上，它是这样做的，不久前出了一份报纸，你的扩散模型其实是个零点分类器，其基本思想是，即使你在建模给定文本的图像分布，该模型可转换为分类模型，而且它的工作方式和剪辑没有太大区别，呃，给了一个图像和呃。

候选标题，您可以使用扩散模型，呃，计算图像与标题匹配程度的分数，做这个比夹子贵多了，但如果你忽视这一点，它的工作原理相似，因为，它给你一个兼容性或相似性评分，在图像和候选标题之间，而这篇论文表明。

实际上，呃，稳定的扩散能够得到好的Imagenet探针，这是一个令人惊讶的结果，所以现在这让我们从一个范式，我们将文本作为模型的条件，或者更确切地说，我们正在根据图像来调节模型，和学习文本的模型。

到一个范例，我们将文本作为模型的条件，然后学习图像中所有剩余的熵，但不清楚这是否有效，也不知道我们受到了多大的打击，在额外的计算方面，我们需要花费来做到这一点。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_47.png)

所以当我们研究多莉三号的时候，我们的一个发现是训练文本成像，生成模型变得更高效，因为您训练的标题更具描述性，所以如果你在真正描述性的标题下训练一个模型，它，在较短的字幕上的性能也更好。

由于它被训练在更长的字幕上，所以这表明有方向性，也许我们可以变得更好。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_49.png)

用语言作为脚手架的无条件模型，这里有一些直觉来描述我的意思，所以呃，这里的第一列，呃，添加了不同噪声水平的图像，添加到图像中的噪声是为了表示，呃，其余不确定的信息，我们试图模仿，嗯。

所以如果图像中没有噪声，你想解释，呃，图像中的一切，你可以用标题来做到这一点，就像一个微不足道的标题，只是描述，你知道图像中每个像素的颜色，所以说，如果你想象把文本训练成图像模型。

得到这样一个描述性的标题，图像中没有不确定性，因为它可以读出像素值并呈现它们。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_51.png)

你不需要深度学习，如果你在图像中添加一点噪声，嗯。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_53.png)

模特要学的东西很少，就像表面的细节和纹理之类的，嗯所以现在有一些不确定性，呃，剩下的不确定性已经确定，可以用一个真正描述性的标题来解释，现在如果你在图像中添加大量噪声，有很多不确定性。

为了解释图像的其余部分，那是仍然存在的信号的剩余部分，你只需要像这样一个简短的标题，最终你知道，如果你在图像中添加大量噪声，呃，模特要学习一切，然后嗯，你知道的，呃，没有标题与。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_55.png)

你知道的，只是纯粹的噪音，因为一切皆有可能，我们在这里的每个阶段都学到了什么，所以如果你有一个正在学习翻译的模型，你知道图像的像素值，它可能并没有真正学到任何有用的东西，如果你有更多的失败。

然后你可以学习一个模型来翻译真正的描述性图像，真正描述性的标题变成图像，直觉上，它可能不会学到很多，因为你给它的标题太描述性了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_57.png)

图像中没有太多的不确定性让它学习。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_59.png)

如果你有更多的失败，你可以，呃，期待模式发挥作用，也许甚至用更短的标题，所以现在呃，标题为模型提供的拐杖更少，然后它在图像中建模更多的熵，最后，如果你有很多技能，也许你可以用。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_61.png)

呃，完全没有条件反射，这里的想法是。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_63.png)

也许呃，超描述性字幕培训，是一种帮助在小范围内对感知相关的位进行优先级排序的方法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_65.png)

你可以希望从真正描述性的标题培训中获得转移。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_67.png)

到简短字幕培训。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_69.png)

所以最终，呃，你可以放大模型，在小规模上，希望它仍然可以是一个很好的图像生成模型，当你给它真正描述性的标题时。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_71.png)

在大范围内，它可以学习语言难以描述的东西，填补了剩下的空白。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_73.png)

所以这表明有方向性，也许我们可以交换，呃，从学习到示范文本，从照片上看，学习建立图像模型，给定的文本，它可能不是那么高的计算效率。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_75.png)

击中从一个到另一个。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_77.png)

最终，如果你在真正描述性的标题上放大一个模型，我们有证据表明，也许无条件建模任务的性能也会随着时间的推移而提高，这表明你知道最初我们不太使用文本，呃，我们只是在预测一些信息来训练图像分类器。

然后我们开始在训练模型的过程中更多地使用文本，像剪辑和图像捕捉器，你也知道，最终，我们，我们看到我们可以通过使用非常描述性的标题来训练良好的生成模型，呃，我们在大理三号和索拉做的，最终。

随着我们规模的扩大，也许语言只是变成了脚手架，以后可以丢弃的，你知道视觉世界可能是一个比文本更通用的界面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_79.png)

因此，这代表了对模型如何训练的思考的变化，呃，所以之前我们想修复一个数据集，并找到更好的目标功能和架构来改善感知，但就最近而言，我认为趋势发生了一点变化，这样我们就确定了目标函数和模型架构，意义，呃。

目标函数只是一个简单的极大似然目标，在那里我们试图重建一切，而模型架构只是一个变压器，我们将爬上数据集，意义，我们如何建模，不管我们要重建的是什么，呃，例如通过使用呃，更多描述性标题，然后呃。

我们如何对数据中所学到的内容进行优先级排序。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_81.png)

所以接下来我将稍微谈谈发生了什么，当我们，呃，遵循这一范式，所以说。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_83.png)

来自图像的文本，现在我们从文本中建模图像，随着我们不断增加计算，似乎语言的作用正在被纳入视觉。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_85.png)

我们在达利二号上看到了一些有趣的事情，在那里你可以进行有趣的风格转换，所以你拍一张照片，你可以用剪辑嵌入算法，将更改应用于，呃，保留所有其他细节，但只改变一些，嗯，在达利一号上。

我们在上下文中看到了光的迹象，足够规模的学习，所以你可以给模型上半部分的图像，然后让它画图像的下半部分，对图像的上半部分进行一些更改，模型从来没有被明确地训练来完成这样的任务，但在足够大的规模下。

它最终还是学会了这一点，嗯，所以当时，感觉这可能是一条通往各种图像的通用接口的路径，操纵，图像处理任务，嗯，现在你知道我们开始得到可靠的视频生成模型，它表明在未来。

也许我们可以给模特看一张我们所拥有的照片，并要求它生成一个视频，为了得到我们想要的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_87.png)

嗯，这是我的简单观察，呃，也许学习压缩一切可能是正确的方法，毕竟，而语言只是使其实用的必要脚手架，最终可能还不够，呃，我们可能需要其他技巧才能，通过重建我们看到的一切来有效地训练视频模型。

但语言似乎能帮助我们到达那里，但最终可以归入视觉智能，最终这将给我们一个真正通用的界面，为了模拟我们想要的任何东西，这就是我的观察，嗯，我希望回答任何问题都很有趣和愉快，谢谢你，谢谢你。

谢谢你的精彩公斤。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a8c89c84fd8693cea392fb1228c8040a_89.png)

现在呢，我们有十分钟的时间进行问答，我想邀请纽约大学的助理教授，算法的作者签名Xen加入Q和A会话，嘿嘿，我听得见你听得很清楚，是呀，呃，是啊，是啊，这真是一次精彩的谈话，呃，谢谢分享。

但我在纽约大学当助理教授，我知道你也是从那里毕业的，很高兴终于见到你了，嗯，所以我准备了一些问题，但首先，我只想承认，就像，你知道的，伟大的贡献使你，你的团队已经，以及对整个人工智能领域的影响。

通过许多开创性的项目，在他们的世代和智慧中，嗯谢谢你，然后嗯，是啊，是啊，就像呃，我想我想开始这次谈话，嗯，我有一个问题，对嗯，这实际上是我从你以前的账户上看到的，一旦帖子发布，呃，报价。

语言模型被高估，不得不说，呃，从某种背景，我真的很喜欢这个说法，呃，但你能看到更多关于它的信息吗，就像你知道的，呃，你认为你们这一代人会走上，我会把我们引向每一只眼睛，你如何看待建模人类语言之间的关系。

与感官丰富的现实建模，是啊，是啊，我绝对认为，所以说，嗯，在任何给定的视频中都有很多信息可以拍摄，视频中的许多信息不容易用语言表达，嗯，例如，我谈到了瑞文的累进矩阵，你可以从一些类型的智力中学习。

难以建模的愿景，仅仅通过学习语言，嗯，所以我认为语言将是一个重要的部分，呃你们都知道，获得更智能的系统，可以对事情进行推理，在某一点上，呃，我想我们应该把语言和视觉结合起来，它是一种更通用的接口。

你也知道，我觉得，我确实认为模拟任何你想要的东西的能力，将是重要的一步，未来的垫脚石，牛逼，是啊，是啊，关于这个问题的后续讨论，你说的语言可以，希望它能成为你智力的脚手架，那么如何确保，就像，你知道的。

语言不是捷径，因为它确实提供了一个非常强的先验，就像我的，只是为了弥补视觉表现的不足，你对此有什么想法吗，我想我希望发生的是当你，你知道的，用真正的描述性标题训练文本到图像模型，它要学的东西不多，嗯。

但我们在大理三号看到的是，当您在描述性标题和一些简短标题上训练模型时，短字幕的性能提高，由于接受了更多描述性标题的培训，所以方向性，它让我们想到也许我们可以用语言来训练生成模型，并帮助他们更有效地训练。

但随着我们投入越来越多的规模，模型不太依赖语言作为条件信息，开始自己想办法，是啊，是啊，我们换个话题吧，把注意力集中在，你知道的，真正让Sara成为可能的天赋，因为你知道。

比尔和他和我一起研究扩散变压器，在他博士的最后一年，他的团队一直致力于长视频的生成，通过他在伯克利的博士学位，嗯，但想想就很了不起，你知道的，比尔和他的团队，他们的博士可以做这么大的，你知道的。

对实地的影响，背后有什么秘密吗，你知道的，OPI的文化，或者喜欢这种情况发生的常规文化，就像，你知道的，真正赋予年轻的研究人员真正利用他们的激情和过去的经验，做出这样的贡献，是啊，是啊，这是个好问题。

嗯，我想有一些东西可以打开AI，使这种事情成为可能，其一是我们的招聘策略，这是完全不同的，我想从其他组织，蒂姆和比尔，当然有ph值，D和相当强的出版记录之前，他们来开放人工智能，但我们过去也招聘过员工。

只是因为我们更关注那些有前途的人，但可能没有机会获得正式的学分，例如，我想詹姆斯·贝克尔是达利三号的主角之一，并帮助将音频支持放入GPT 4O，他是那种人的好榜样，呃我想，第二。

也许我们的重点是有一个长期的研究目标，那不是真的受，实地的逐日或逐月变化或进展，那就是我们设定一个在未来足够遥远的目标，嗯，我们认为是可以实现的基础上，事情的发展，我们可以完全专注于此。

而不是对一天天变化的事情做出反应，嗯最后，我认为每个人拥有大量的GPU往往会有所帮助，是啊，是啊，我回到你身边，年轻曾经告诉我，你知道你大学时和他一起工作，你在想，你知道的，申请你的博士学位。

你的互联网睁开的眼睛，决定留在那里，嗯，你知道的，就像我也注意到一样睁开眼睛，呃，有许多非常成功的研究人员并没有真正经历过这些，你知道所谓的传统，有点像研究，正式研究培训，嗯，我想听听你的想法，比如。

你知道的，呃，你知道基本上高等教育的作用，现在，你觉得博士也被高估了吗，呃，以某种方式，然后嗯，我不确定你是否能看到，这就像这次活动中许多热情的面孔，我不知道你是否有什么建议，呃，你知道。

就像下一代的研究人员想在，爱，是啊，是啊，我想这是个好问题，嗯，我觉得，因为事物被统一成一种单一的范式，其中我们有一个可伸缩的体系结构，哪个是变压器，我们知道如何表示数据，呃。

即用于文本的bp标记和用于可视化数据的补丁，事情有点趋同，所以计算是获得更好结果的最重要因素，改变焦点，我想对于你在学术界从事的项目来说，我认为可解释性是一个方向，嗯。

也可能关注现有深度学习系统仍然失败的邪恶和任务，嗯和诸如此类的事情，我想现在读博士很难，期待在一些事情上得到最先进的结果，只是因为它，你知道所需的资源，比以前高得多了，是啊，嗯，但顺便说一句。

我知道喜欢，我希望你有这个研究访问程序，比如代理一些学分做MS和多模态学习的研究，所以谢谢你，我想你知道从我的角度来看，我认为有很多机会真正建立，工业界和学术界之间的这种伙伴关系和合作，嗯是的。

我觉得很多人都很兴奋，我们真的很喜欢你在社交媒体上分享的视频，对嗯，但问题是，我们还没有得到它，嗯和我，我相信你已经看过最近发布的，或许来自短视频公司质疑的清廉模式，也喜欢卢阿艾的梦幻机器模型。

所以我想听听你的想法，你知道的，你如何看待视频生成空间中的计算，我们期待着Sora的一些新的更新吗，是啊，是啊，这是个好问题，我觉得最重要的是，呃，那是我们的想法。

对于像发布一个强大的视频生成系统这样的事情来说，主要是安全的，它会对社会产生什么影响，嗯，我们要小心确保当我们发布像Sora这样的模型时，呃，你知道的，它是，我们知道它它。

人们不会用它来做错误信息之类的事情，模型的行为方式是在，人们的期望，我觉得要保证模型的安全需要做很多工作，为了能够，你知道，有一个自信的释放，但这是我们的首要任务，嗯，我认为总的来说，有竞争是很好的。

就像，它是，很高兴看到其他实验室和公司发布视频生成模型，也是，我认为有一个增殖，从事不同方法工作的人的激增有点像激发创造力，嗯，如果我回想一下像多莉二号这样的事情。

我们在谷歌大脑和Openai之间打了一场乒乓球，每个实验室都会发表一篇论文，某种扩散模型，例如rafl和alex写了论文扩散模型，图像合成中的bean，并引入了分类器引导。

然后乔纳森·霍和其他人发布了分类器免费指南，创新层出不穷，所以我认为在视频生成领域看到有趣的产品创新是很棒的，也是，我希望你们知道，我们可以更多地了解这些工具是如何有用的，在艺术家和创作者手中，酷。

是啊，是啊，谢谢你，我想我们几乎所有的时间，也许我可以再问一个关于创意世界的问题，所以每次去纽约的人工智能电影节，我问了所有，你知道的，艺术家和电影导演有一个问题。

就像你真正需要的视频生成模型的一个功能，令人惊讶的是，他们的答案都一样，他们说可控性或更好的可控性，所以我想知道这是不是你知道的，也许在它的下一个版本中看到了，会专注于球，或者喜欢。

因为我知道你和很多不同的艺术家合作过，嗯，你对此有什么了解吗，你认为你知道吗，语言将是最终的媒体界面，为创意世界提供更好的可控性，是啊，是啊，嗯，我想我过去说过的很多话，谈论的是语言的作用。

在这些模型里，我想，更好的可控性和减少老虎机方面可能是头号特点，我们也从我们共事过的人那里得到的要求，我确实认为这样做的能力以及重用角色和资产的能力，之前场景中的其他元素将是一个巨大的游戏规则改变者。

只是因为这似乎是第一件事，嗯，做出来，好让你知道，视频生成模型实际上在生产设置中变得有用，嗯，我觉得这有点有趣，因为我谈到了你是如何知道，我们很早就在大理一中看到了这些在上下文学习能力中的出现。

现在你知道这些能力是，在投入生产的路上，好啦，是啊，是啊，像我们这样的组织者可能会有时间再问一个问题，如果你不介意的话，嗯是的，所以我想问一些关于数据的问题，嗯，所以，因为我知道很多喜欢，你知道的，呃。

数据使用我的Sora可能来自网络视频，嗯，但你觉得这就像，尤其是走向阿吉的道路，就像通过一种真正的智慧，你认为像现在这样，像网络视频就足以支持这个目标，或者我们需要像，你知道的，发现新的数据源。

甚至像不同的感官媒介，呃，帮助解决这个问题，是呀，目标是的，这是个好问题，我觉得，我认为存在的数据，嗯，我认为我们可以通过扩大模型来继续取得很大的进展，因为有这么多可用的数据，嗯。

但我想很多有趣的事情会发生，一旦模型有能力成为自己的世界模拟器，你可以开始做一些事情，比如在上下文中运行，视频生成中的仿真，模特本身，嗯，这样我们就可以开始结合，你知道的。

所有来自现实世界环境的多样性和有趣的限制，开始学习有趣的东西，酷，谢谢你的真知灼见，嗯，我想在我们关门之前，嗯，我做了，你有什么想分享的吗，嗯，有观众在，尤其是去中国的人工智能通勤，否，我的意思是。

在这里谈话很愉快，很荣幸向大家介绍，所以谢谢你，有我这么好，谢谢你，是啊，是啊，和你谈话真的很愉快，嗯，让我们再次感谢一个细节。



# 2024北京智源大会-开幕式及全体大会 - P3：主题讨论：通用人工智能-环节1：Fireside Chat-李开复-张亚勤 - 智源社区 - BV1uH4y1w77V

我们现在呢就进入对话的环节，那么邀请我们邀请了两位大家都特别熟悉的呃，著名的专家呃，李开复博士，01万物的CEO，还有呢张雅琴教授嗯，他是清华大学智能产业研究院的院长，也是智源学术顾问委员会的委员。

我们讨论的话题呢是通用人工智能的关键问题，还有这个他们的思考啊，啊那我们就有请开复和雅琴直接直接在台上吧，呃两位专家呢大家都特别熟悉，也在人工智能领域呃工作多年，相信呢对于AGI。

对于通用人工智能有很深的思考，我相信呢大家也有很多问题，想那个听听他们的这个呃，高瞻远瞩的这个意见呃，那首先呢我还是这个先提问啊，那个呃关于大模型呢这几年特别热，应该说包括刚才的报告。

大家也感受到了这个大模型的能量，和这个在各种领域的应用的潜力，呃，可以说是至今为止，人工智能发展最成功的一个，这么一个技技术方向，那我想想请问两位呢，大模型到底做对了什么，使得这么成功。

还有哪些欠缺的地方需要后续需要呃发展，看看这个是开复老师先好好的呃，大模型肯定是啊，AI的有史以来最大的一个革命，那我觉得里面特别重要的就是scaling law，也就是说我们能够用更多的计算和数据啊。

不断的让它增加智慧变强啊，这一点是已经被验证，而且肯定还没有达到啊，一个顶峰还在推进中，这是让我们非常振奋的嗯，还有第二点，我觉得就是啊，其实他大模型，它本身的智慧来自于一个接近无损的压缩啊。

这一点我觉得也是非常的重要啊，这点可能不是当年我们很多做人工智能的，想到会把压缩和这个智能呃，呃能能够连接在一起，因为我们scaling law的过程中，不能就盲目的去丢丢更多的GPU啊。

所以我们会需要有一个方法来去评估我们啊，有没有越做越好，两个方法哪个做得更好，所以在我们灵异万物的内部呢，我们其实是有一个呃，这个数学严谨的一个方法论，是用压缩的这个理念来去评估。

那这样我这样就让我们从一个呃做实验，或者甚至有人说炼丹的过程中，把它变得有科学和数学呃的根据了，呃是呃，那么我觉得大模型可能面临的一些挑战，就是啊仅仅啊仅仅去用更多的算力啊，来把它往前推动。

那么这个如果是主要的方向的话，那么啊，呃我觉得，如果如果未来会继续看到它的快速上升，那这样的话就会导致啊，只只有那些特别有GPU的公司或国家，能够在这方面胜出啊，当然我觉得话说回来，我们也验证了啊。

国内的很多大模型都已经是接近啊，甚至在有些例子里啊，打平或者偶尔会超过美国的大模型，所以我还是认为啊，其实同样重要的是需要专注的，它就是里面的工程问题啊，怎么能够去用工程啊。

来让我们不是进入一个盲目的丢算力啊，达到更高的结果，当然还有很多呃，挑战大大模型推刚推出的时候，就像每个技术刚出来一定会有一些问题，比如说大模型刚开始的时候，不知道最后最近一年发生了什么。

这个recently的问题，还有啊记忆的问题，窗口不够长的问题啊，还有他的幻觉的问题，但是我们可以看到的就是啊，当这么多聪明的大脑，在全球涌入这个领域的时候，这一个一个的问题啊，都大部分的不能说完美的。

但是啊非常好的被解决了，所以我对未来还是非常的啊非常乐观的，谢谢凯文老师，那看看对嗯，我我讲三个就是做对的和三个呢，呃目前还需要啊这个改进的吧，啊第一个就是刚才开服讲的嗯这个规模效应，哎。

这规模效应本身呢，呃首先是利用了我们这个大量的海量的这个呃，数字化或者数据啊，加上这个算力的提高，加上呢现在呃这个架构呃diffusion啊，还transformer。

它很好的利用了这个算力和和这个数据，所以我觉得这个skin law呢，呃大家现在可能有人说啊，我两年3年是不是就不成立了啊，我自己认为的话呢，呃这在将将来的可能5年左右。

至少还是我们产业大的这个方向是skin law，第二的话呢就是一个统一的一个表述，所以token based，这个我认为也是我们现在大模型里面的一个啊，很根本的一个元素啊，不管你是呃文本语音啊。

图像视频啊，或者在自动驾驶，比如说你的激光雷达的信号啊，在在呃生物里面，蛋白质细胞都最后可以抽象成为一个token，然后token之间的呃这些训练啊，学习啊生成啊啊是这个特别呃一个一个核心啊。

这个其实和我们大脑很像，大脑那个神经元都是一样的啊，不管是呃这个做什么事，一个神仙一样的，那么第三点的话呢，就是它的所谓通用性这个token有关系呃，现在通用不仅仅可以用的文本啊。

可以用到我们的多模态啊，也可以用到啊生啊生成，比如说蛋白质哎，可以用到物理世界或者居生呃，智能可以用到生物智能啊，生命世界，所以我觉得三个是大模型做对的，如果说有问题的话呢，第一个就效率还是太低，唉。

呃如果看一下我们人类大脑的话呢，呃目前的最大的模型呢是我们比人的这个啊，860亿个神经元啊，每个神经元有差不多几千个，这个突触的连接，可能比我们小100倍甚至1000倍，但是我们大脑只有三斤重20瓦。

唉，这个决策效率很高，所以未来的话呢需要大幅度提高这个效率啊，这样的话才会有真正的商业模式，目前不管是微软也好啊，Open a i，还有呃这个主流大模型啊，整体来讲的话呢。

还是一个在买买这个英伟达芯片的这个过程，哎就真的这个呃商业模式还需要建立，所以效率需要大幅度提高，第二点就是针对现在的大模型呢，还是没有真正的啊理解物理世界，哎这推理啊，那各个方面还是比较差一些啊。

包括当然这里面的透明性啊，包括刚才开复讲过的幻觉啊，呃这些我觉得很多的呃研究都在都在啊，不断的做啊，这里面包括很重要的问题，就是我们的大模型呃做得再好啊，他在这个深城市和啊，这个对世界真实的表述之间。

有一有一个有一个矛盾，所以怎么样能把啊，我们生成式的这种概率大模型，和我们现在已有的这个地形原理，或者或者是或者现在的这个呃真实模型，能结合起来啊，比如说知识图谱结合起来，但现在有一些像用RAG啊。

微调啊，有些这样的进展，但这个还呃我不认为这是一个嗯根本，所以我认为下面5年呢会有个全新的架构啊，使得会替代现在的transformer，和现在这个diffusion架构，嗯那么最后一个欠欠缺的地方呢。

我觉得是边界问题，我们现在大模型它搞不清楚，还不知道，我知道呃，我不知道什么，哎，这是呃我们目前我觉得要解决一个大的问题，就它的它的这个边界效应好啊，谢谢雅琴，关于这个三个做对了和三个这个不足啊。

嗯但是呢刚才开播老师没讲，但是我想再再追加问一下啊，因为有些人认为呢大模型这个就是一个实践，就是一个工程呃，是经验主义的做的一个东西，没有理论基础呃，所以说的不好听的叫不靠谱，不知道你怎么看这个问题嗯。

呃我觉得呃科学和工程缺一不可啊，如果是只做工程，不了解这个第一性原理，没有数学的根据，不知道怎么做评估啊，没有办法判断不同的方法哪一个好，那最后这这样的摸索呢，这么大的算力和它的成本。

是肯定做不出一个好的模型的，但是如果只是在实验室里雕花，然后写一些不错的论文啊，就期待有一批工程的人，我把它扔过去，他们做成产品，这个肯定也是不行的啊，我们在灵异万物的经验。

就是我们每一个做AI做模型的researcher，都要懂infrastructure，都要懂推理，都要知道成本的问题，所以当你在做科研的问题的时候，就要知道会怎么样实现在什么产品里面。

所需要的速度跟反应是有多快，然后做完了实验就可以确保，这样你做出来的模型可以得到很好的工程结果，还有我们做这个整个呃呃训练模型的过程中，绝对不只是啊写paper做出结果来，还要同时考虑到我们怎么样。

系统化的工程化的去做数据工程，因为是数据的训练啊，数据的筛选啊，这些是非常重要的，还有就是底层的AI infrastructure，因为GPU这么昂贵，如果能把一张GPU当做两张或三张来，使。

我觉得任何公司都会得到好处，所以这两者真的缺一不可好，谢谢谢谢甘木老师嗯，第二个啊，我觉得可能大家很多人都特特别关心的，就是AR2。0了，我们叫大模型这么一个时代啊，产业化最大的场景在哪里。

to b to c对吧，这个互联网移动互联网我们都这么多年呃，这两种这个赛道，两个大赛道哪个更有机会，为什么这个呢想请两位呃发表你们的看法，嗯好的呃，简单的说，我觉得在中国to see短期来更有机会。

国外呢两者都有机会，嗯我我觉得从to c讲起来啊，大模型，我们认为其实就像是移动互联网或者PC时代，一个新的技术，新的平台带来新的应用，这是巨大的机会，嗯但是呢这些应用一定是按部就班的会过来的。

所以从PC到移动互联网时代，我认为AI时代也是一样啊，第一个阶段应该是生产力工具啊，第二个阶段啊，可能是啊会会是这个呃娱乐音乐游戏，第三个阶段可能会是啊搜索，再下一个阶段可能会是电商。

然后呢可能会有社交啊，短视频啊，还有欧洲欧，这是啊不变的定律，他的理由有很多，其中一个理由就是刚开始你要能够赚钱，要能够解决问题，所以工具那越往后呢难度越高，需要的用户量越高。

而且商业模式往往是先堆积用户DAU，再找商业模式，所以你的成本一定要压低啊，试错的这个难度所需要的技呃投资就会更多，所以他按照这个顺序来也是自然的，还有一些细节就是你的用户不多，怎么做社交等等的。

所以我认为这个定律是不会改变的，所以to c，我们会坚决的，从工具一直走向未来的这个短视频啊，它我我我考虑to see，就是它要啊产生大量的用户啊，我想这不是说不能做啊，大模型的呃，视频的研究或者产品。

只是说如果要普及的话，按照我刚刚讲的这六个的顺序，我是坚决相信，然后一定也会发生啊，当然这里面有一个挑战就是啊做这些应用啊，在大模型领域，它跟其他的PC互联网时代不一样，就是推理成本还太贵。

所以最近呢我们我们也推出了一个新的想法，叫做TCPMF，就是当你考虑prada market fit p MF的时候，你还要把技术的需求跟难度，还有它的成本考虑进去，所以做应用呢一定要考虑到。

同时刚才这六个阶段谁先谁后，你该什么时候做，然后要提早做，第二呢就是你做的时候啊，要考虑到一旦做完了，那技术会不会够好，还有它的成本呃，会不会够低，这些要综合考虑，所以做这方面的to c应用不是一个呃。

过去移动互联网的产品经理，一个人就可以啊，做主算的，它的难度会很高，你还会需要做infrastructure，做这个啊推理引擎的人还会需要啊，做多大模型，AI的专家一起来打磨这个TCPMF。

但是呢它难度高呃，回报也高，机会也更大，最后一点我想讲的to see就是啊，我不相信技术可以带来一个永久的领先，而且它带来的领先是非常短暂的，因为有很多巨头他一旦看到了你的PMF验证。

他会有更多的方法来超越你，所以呢一旦你验证出这个TCPMF来，你要把握着一个一定的时间窗口，把你的品牌打出来，最终胜出的to c的应用一定不只是技术做得好，技术做得好是需要的，还需要抓到一个时间窗口。

然后在这个时间窗口打造一个可持续的优势，比如说是品牌优势，比如说是一个社交链，比如说是用户的数据，让他不能离开你这个平台，如果你思考一下啊，过去比如说抖音怎么在微信这么强大的时代，能做出来。

就是他抓住了这个窗口，好再讲一下to b的应用啊，我其实觉得to be是大模型带来更大的价值，而且应该更快实现，但是很很可惜的，就是to b的这个领域呢面临了几个巨大的挑战，一个就是大公司。

传统公司他不敢采取看不懂这个技术，不敢采取这种特别巨大啊颠覆式的东西，大公司习惯着每一年呃算budget增加5%，然后继续做去年做的事情，只是做的更多，稍微不一样，这种颠覆式的东西呢。

大公司有点难拥抱啊，还有第二个问题是啊，最大的对企业带来的这一年能带来的价值啊，是降本啊，而不是而不是创造价值，降本说实在就是取代人类的工作啊，那么大公司会有很多呃高管或者中层的管理。

他不愿意做这个事情，因为这个做了，他可能啊团队就要砍掉了，他的公司，在公司的这个政治资本就没有了，他的权力就变小了，他的甚至自己工作都没有了，所以公司大公司有时候CEO有时候是很想做。

但下面的人会有阻力，所以这些理由造成to b呢，理论上应该马上可以落地的，但是没有那么快啊，还有一个问题呢，这个是在中国比较严重的问题，就是有些大公司它没有认识到很多大公司，没有认识到软件的价值啊。

不愿意为软件付费，而且有这么多大模型公司就来竞标，然后大家就越进越低，进到最后做一单赔一单都没有利润，我们在AI1。0时代就看到这个现象啊，现在很不幸，在AI2。0时代呢又重现了啊。

那么呃我觉得有些大公司会觉得你们愿意竞价，我就挑最低的嘛，或者我把所谓的商务啊变得比技术更重要，评估的时候你技术不好，但是啊卖卖很便宜，我可能还是会选你啊，还有就是有些大公司会觉得。

哎我我用了你的大模型，你用用我的品牌去做融资又很划算啊，那我为什么要付钱给你，所以这种很不幸的心态呢，就变成一个race to the bottom，也就是说呃，大公司因为这些理由给很低的价钱。

那么大模型公司拿了这么少钱，他不能赔那么多钱去给你拼命的做啊，他就扔一个模型过去，然后这个啊让你这个就看你运气好不好，能力好不好，这样的话就做不出效果来，所以在灵异万物呢，我们是坚决的要做to c。

然后要啊坚决的不做赔钱的to be，所以我只要能找到能赚钱的to be，我们就做赔钱的，我们就不做，所以我们会在to b方面是更精挑细选，找那些真正公司上下都align。

大老板跟VP都愿意接受这个概念啊，或者是大老板权力很大的这种公司，无论在国内还是国外去做啊，最后一点我想讲的就是啊，无论做to c to b它里面的API都很重要啊，最近国内很多模型降价了啊。

那我们在灵异万物也推出了我们的e large的，这样的一个模型的API啊，也希望跟各位有机会合作，这个A这个API它背后的模型啊，大概接近GPT4的能力，这是真实的，你可以去测嘛，是API。

然后它的价钱是GPT4的14，所以我相信这可以帮助更多的公司或者创业者，达到所谓的TCPMF嗯，谢谢开普老师，刚才to b to c分析的很分析的很透彻啊，哈最后这个价格也很好看，也很好用。

应该是呃这样的一个一个新的产品，我想那雅琴呢关于刚才这个to b to c，看您的观点，特别是刚才还有一个大家关心的问题啊，就是大模型产业的最大场景会在哪里对吧，不管是to b to c。

我们在什么地方能够这个落地发挥作用，嗯我有点听不清楚，还是to b to c的问题是吧，场场景是吧，场景最大的场景在哪里，在应用应用和服务层的话呢，刚才开服我讲的呃。

呃先先to c后to b to b时间长一些，to c的话呢马上就已经有应用和产品了，这和当时的PC互联，移动互联基本上是是一致的，但如果再分层细一点的话呢，就目前啊真真正赚钱的是是to b。

嘿目前赚钱to b是在这个基础呃，在硬件，在芯片，在这个基础设施层呃，赚钱最多的，目前是做像英伟达AMD这做芯片的，是做这个服务器的，还是做呃HBM这样存储的是是做啊。

infinite band这个这个这个n v link呃，所以我觉得这个的话呢，就是目前啊已经在发生了，但是从应用来讲是先to c再to b嗯，还有一个就是我把呃现在的AI分成一直，我讲了10年了。

分成三个世界哈，一个是信息智能啊，一个是物理智能，现在时髦的字叫具身智能，另外一个呢就是生物智能，那到了这个呃聚生智能的话呢，呃这个图B的应用可能会更快一点，可能比to c呢可能更快一些。

呃到了这个生物智能的话呢，呃可能是相反，to c可能可比高于呃快于to to b，可能每个啊这个每个每个领域不太一样啊，但整体来讲的话呢，to c to b啊，包括我们现在有开源的模型。

有有商业的闭源的模型，有有基础大模型，也有垂直行业的模型啊，还有边缘模型都会存在啊，都会存在，那雅欣谈到已经提到巨人智能啊，今年特别热投资啊，这个大家这个呃关关注度啊都特别高。

巨神智能呢因为讲到巨深深啊，这个身体各种各样呃，通常呢像机器人对吧，人形机器人包括轮式机器人，这是一大类，第二大类呢就是车对吧，车本身也是身体，也是一个在驾驶这个场景上的一种，一种狙神智能。

当然还有无人机对吧，在空中飞将来也是一种形态，甚至于大家可以想象出更多的身体形态，更多的聚神智能的这种这种呃，呃未来的未来的可能性，我是想勤奋的，因为自动驾驶对吧，已经做了这么多年。

现今年呢也已经这个呃，看来是L3的希望已经已经很大了，那你们认为呢，到底这个刚才讲的巨神智能的这么多可能性，里边到底是我们热门的机器人，人形机器人这个呃会现实会会先有机会。

还是像自动驾驶已经有了这么多年积累呃，会有会有机会呃，看看雅琴，因为你这两个都有很深的研究，对的确，我做了差不多啊，可从阿波罗开始可能有7年八年的时间，一直在从事这个无人驾驶。

就我认为的话呢就无人驾驶啊，这个L4加呢是啊，巨深智能第一个最大的这个应用，而且我最近一直在讲，也是第一个会实现啊，叫做新图灵呃，是图灵呃测试的这么一个应用，呃，其实呢这个无人驾驶。

它本身就是一个特殊机器人，就是就是呃机器去开车嘛，让你开车那样嗯，目前的话呢呃无人驾驶最近有很多好的消息啊，就是在这个安全性呢，已经比人类驾驶要高至少十倍啊，不管是这个威猛在旧金山的结果。

还是百度阿波罗呃，罗欧快跑，在武汉这种大范围的这些是商业运营啊，甚至更高啊，这这个最早在阿波罗呃，开始时候，我就讲我们这个安全一定要呃，比人类驾驶要超过十倍，另外一点的话呢就是尽管安全很好。

就开的特别规矩，但是目前呃还有些问题，就包括呢啊开的安全，但是开的不够不够熟练，不像老司机这样啊，因为他太守规矩了，嗯嗯这个开车不会超速，也不会压线，呃那么这个的话呢就是我我最近一直在讲。

我说无人驾驶要变成主流，就通通过咱们这个啊我叫新图灵测试的话呢，需要是好司机，也也需要是老司机，那么这个我认为在明年会实现啊，这我信，我觉得第一个呢，真正实现在这个具身智能或者物理智能的。

那个AGI就是无人驾驶啊，另外我再讲一点，就是我们大模型的推出啊，其实帮助无人驾驶解决了很多啊，原来的问题，比如说这个长尾问题啊，数据的生成问题呃，百度海风跑多少，跑了1亿公里了，1亿公里很多。

但是还是少数据啊，所以生成是可以生成很多数据，还有一个就是端到端啊，大模型可以做端到端，所以说在刚才讲老司机这方面的话呢，他会他会呃有更高的智能性，会帮助我们快速的这个落地，好开封您您您的观点啊。

对我同意，刚才呃雅琴分析的啊，他在无人驾驶方面呃，主导了中国重特别重要的一个项目多年，然后我们在创新工厂呢，我的另外一个帽子也投了很多家，无人驾驶的公司，大概投了六家左右呃，发展都不错。

但是现在呢我觉得无人驾驶啊，面临的一个巨大的呃，呃机会就是终于可以落地了啊，啊在L2L3，还有一些城市的这个自动自动的小八等等的啊，还有特殊场景都是可以真的创造价值，这是很让人欣慰的嗯，然后到L4L5。

真的是可以无限的在开到街上呢，啊全球都还有挑战这方面啊，我在美国的时候看到了啊，威猛进展也是不错的，所以依然在往前推进啊，不过啊TESLA最近推出的这个啊服FSD，可以看到的，它虽然不是完全大模型。

但是它是用了类似的n to n的概念，那这个呢，我觉得会颠覆这样无人驾驶的一个领域，那不用FSD能做到多少，能产生多少产业价值，我希望这个我们投的五六家公司，还有啊雅琴的前前公司啊。

都能够证明中国能做得很好，也许就是很大的一个市场了，过去这一年做灵异万物，我没有太研究，所以不太确定啊，这个市场到底是实现的有多好啊，但是我觉得FSD它会带来一个新的机会啊，简单的说。

也就是基本把大模型的概念放到了，无人驾驶里面，这是我非常期待下一阶段能看到的，但这个的投资是巨大的啊，也不见得适合初创公司来做，所以这个是我现在对无人驾驶的看法，他已经用旧的方法做出了比较成熟的产品。

在面临市场化的过程，新的算法出来了，但是可能还要一段时间，那讲到这个聚生智能呢啊，他就跟啊雅琴刚刚说的一样，是个物理世界跟这个AI的结合，这肯定是很重要的，但是我也必须说呃。

大模型是非常非常非常适合虚拟世界的，比如说在金融公司用后台客服等等的啊，很快就可以落地产生价值，因为它就是一个软件嘛，如果你的数据是软件出来的，结果是虚拟的，是数字，那你就直接对接大模型就好啦。

那你要一旦要接到物理世界，那就有各种的问题，有安全问题，有它的这个机器的问题，机械的问题，故障的问题，mechanical ical engineering的问题，撞到人怎么办等等的问题。

就真的呃难度就大非常多多多倍，所以从创业者的角度来说，虽然现在聚生智能一时是比较热，有一些创业者就融入了，但是大部分的创业者，如果是希望看看到短期的落地产生价值能赚钱。

那肯定还是做虚拟世界的要远远容易很多啊，所以虽然我觉得聚生智能是重要的啊，而且它可以把这个多模态结合了，你一旦聚生以后就产生数据了，那就可以有数据，飞轮，可以有闭环，所以这是有很大的想象空间。

但是短期要做好，真的还有真的难度是很大，我们看无人驾驶做了多少年，现在做到什么程度，聚生智能肯定也要走这样的呃，一些弯路和很很漫长的道路，而且对于人形机器人，我也有一些特别的看法，就是说呃。

我觉得绝大多数的应用并不需要人形机器人，你炒菜机器人应该长得像郭子啊对吧，你的呃，这个呃，吸尘器长得像是一个这个在地上拐子跑的，这个小小小小小圆圆的东西，它并不长得像人啊，不会说我要做吸尘机机器人。

就就做一个像人一样来吸尘，没有没有必要嘛，那些很酷的什么boston dynamics的跳来跳去，翻跟斗的那些真的有很多应用吗，绝大多数的场景，几个轮子不是让你更能容易走到吗，你真的需要两条腿吗。

除了爬楼梯等等的，真的那么需要吗，而且它的难度带来的难度跟成本提高这么多，我觉得可能很多嗯，呃科学家和创业者都是从小热爱这种科技，希望能复制一个人，那也无可厚非，最近经常有人问我。

咱们这也是通用人工智能哈，就是这个到底什么时候可以实现啊，啊我呢比较乐观，我讲一下大致的时间点呃，这个可能开服讲的很一致，就是在这个玩分成信息智能啊，然后这物理智能和生物智能在信息智能方面呢。

我认为5年左右就可以实现，现在其实在文本基本已经实现了啊，就是就可以讲叫图灵，咱们图灵测试吧已经通过了，在视频方面可能还需要几年，所以5年之内呢，我觉得这个有可能达到，在虚拟世界或者是信息世界的那个。

然后到具身智能或者物理的话呢，可能需要10年啊，比较乐观，10年在生物智能，我觉得还需要15到20年还会更长一些，你要是呃3年前问我这个问题呢，我可能会会放50年会乘二，现在我把它除以二，对这个概念啊。

我再确认一下这两位啊，因为我们叫通用人工智能啊，这个通用人工智能呢现在比如大模型，包括今天已经这个实现的，包括刚才说未来几年信息空间，这个大模型的认知能力呃，那个咱们通常的评测，比如说到了大学。

到了博士，到了专家对吧，到了呃学者，甚至于这个科学家，这这是这是一个一个我们叫通用性，不管什么学科他都可以做啊，这是一种理解，但是呢刚才那个雅琴讲到，就是咱们说后边有了身体，进了物理世界。

甚至于将来像生物一样的这种智能，那这个时候的概念呢，跟我们今天讲的通用程功能可能又会有所不同，也就是大家这个讲AGI啊，AGIAGI和GAI。

刚才我们通用人工智能智能容artificial intelligence，我们叫它GAI好了，去年那个志愿大会我我也是这么讲的，就是具有通用性的人工智能，和这个我们AAAI领域说了这么多年的。

A g i，AGI是要超越人类的，AGI呢是有自我意识的，AGI是一个独立的，这个这个这个呃不仅仅是智能水平超过了人类，而且他有自我意识，他要他要做出这个这个呃自己的目标，自己的判断就这样的AGI。

你们刚才那个谈到的是指这样的AJI吗，还是说这个只是说前面一种，好啊，AGI的定义其实呃真的见人而异哦，如果我们把它定位成为能做人，所有做的一切的事情，我今天没有办法做任何的预测。

因为他有太多未知的东西还没有被解决，但是我觉得只把人当做一个基因标准，似乎就是说车什么时候能跟人跑的一样好，跑的一样快，车在很多场景已经比人跑的快很多了，但是有些场景他就没法没法做嘛。

所以我觉得我会个人会说，只要scaling law继续，只要AI1年比前一年更聪明，他会多多做，比如说五倍的事情，IQ会提升20个点或者这样的事情啊，但是他聪明的方向能做的事情，也许是人从来都不能做的。

不见得是把人做的每件事他都要做，那因为我是做投资和创业的，我想看到的是呃体现巨大的商业价值，从这个角度来说，我们不会太纠结，是不是能够做到100%，人类做的还是世界上1万件事情，AI能做9000件。

能做的比人好，呃然后1000件人做的比较好，这样也挺好的，要给人人留一点空间嘛，那谈到这个虚拟跟物理世界，我还想再补充一点，就是呃在虚拟世界里，其实agent还是非常需要的。

因为我们人的intelligence不是只是回答问题，是要知道怎么怎么能把事情做出来，而且从商业价值的创造，这个agent真的能够呃做出事情来帮你把东西买了，帮你把事情解决了啊。

这个价值我觉得商业价值也很大啊，贴近AGI也是呃会呃在走重要的一步的，A a g i，就是我我刚才讲那个呃20年，什么不包括你所讲的那个意识哈啊，或者或者包括情感，我我我觉得AGI我的定义呢。

第一点就是要有巨大的能力，就在大部分的这些任务方面要比人类强，哎你不会是百分之百，但大部分的这个比人类人类要强，第二点的话呢它要是通用的，咱们讲通用人工智能就过去，比如说每一个任务用于不同的模型啊。

也是是要有些通用的一个大的底座，当然上面你可以有小小的模型，就叫通用性，第三点就是它不断地去升级，不断学习，不断进化的啊，他就这个像人一样，但我不认为我们现在这个scaling law啊。

或者我们现在做的用用所有这些事呢，可以产生意识，而且我也不认为我们应该在这个方面做研究啊，我我觉得我们还是要解决真正的问题，把呃人工智能作为我们的工具，做我们的延伸，作为我们的agent。

而不是而不是另外另外一种一种物种啊，嗯因为时间咱们已经过了啊，因为最后问题呢，我就是因为明天一天AI的大会呃，还要讨论，但是我想最后还是想问两位这个资深专家，一个一个问题啊，因为刚才讲从工程，从应用。

从商业，从所有的角度，没人可能没人想去做一个超越人类的，有意自我意识的这种这种AGI，但是呢最近open i发生的事情，从去年年底开始对吧，伊莉雅和奥特曼他们的争论，其实呢就是两种观念，就是你不想做。

他可能就出来了，然后出来之后，我们就面临着不可失控的这种这种巨大的风险，这件事你们认为这种风险存在还是不存在，就只答回是存在不存在嘛，呃我觉得存在的啊，它的它的发生的概率不是很高，如果我们越来越走。

用reward model来让AI自己找路的话，那这个发生的可能性就会增高，当然今天我们的训练方法等等的似乎还不至于，所以我短期最担忧的还是坏人，用了它去做坏事，对嗯随着这个能力不断的嗯扩大的话呢。

风险也在不断的扩大，所以我觉得现在考虑到未来的风险呃是很重要，明天我们专门有有一有一节讨论这个问题，我总结下的话呢，就我对啊，这个AGI实现之后啊，比人更聪明，会掌控人类，我没那么担心。

我担心的或者或者有自我意识，这个我我没那么担心，我担心的是呃会失控，由于我们大模型里面，比如说我们的啊可解释性的问题啊，我们的大模型到了物理世界以后，我们机器人比人要多得多啊。

呃我们以后生物生命世界脑机接口这些，如果啊呃大模型用到啊，基础物理设施啊，用到金融系统，用到啊，包括国家安全，包括军事系统，这个时候的话呢，他这个失控的风险比较大，所以我们现在开始呢。

应该把呃这些东西要考虑进去，但我是乐观的，我认为我们人类一直讲人类两种智慧，一个是发明技术的智慧，一个是引导技术走向的智慧，我觉得我们呃会达到一个平衡，但是现在要采取行动啊。

好呃谢谢两位资深专家高屋建领的真挚灼见。

# 2024北京智源大会-开幕式及全体大会 - P4：主题讨论：通用人工智能-环节2：报告：大模型为通用人工智能带来曙光-王海峰 - 智源社区 - BV1uH4y1w77V

[音乐]，上来上来，[笑]，好 谢谢黄老师的介绍，刚才开复和雅琴的环节也讲了不少通用人工智能，今天我的题目也是大模型为通用人工智能带来曙光，我们都知道人工智能它的目标是模拟延伸和拓展人的智能。

而之所以我们认为现在大模型已经为通用人工智能带来曙光了，因为通用人工智能刚才有不少讨论，大家会从不同的视角来看，那么我更看重其中两个视角，一个是人工智能技术的通用性，一个是它能力的全面性。

我们看一下人工智能过去几十年技术的演进，从早期要人工撰写规则，后来统计机器学习技术可以自动地从数据中去学习，但那个时候机器学习的算法还非常多，不同的问题 不同的场景都需要不同的算法去解决。

到了深度学习时代 算法的通用性大大加强了，虽然深度学习也有不同的算法，但总体上来讲 深度神经网络这样一套架构 一套技术，可以解决各种问题，而到大模型时代呢 不只是算法，而模型也变得更加通用 更加统一了。

所以技术的演进从算法到模型现在都变得越来越通用，那我们看一下通用会从哪几个方面来看，我认为一个是从我们要解决的任务，一个是这种语言 模态以及场景的通用性，像过去这一段时间最热的大模型是大语言模型。

所以我们看看这个语言方向 就是自然语言处理这个方向，早期呢 当然我这个列的也不全面 只是其中一部分，自然语言会分成很多很多子方向，但大语言模型的出现呢，使得我们不需要在一个一个子方向分别去研究算法。

去训练特定的模型，而是一个大模型几乎把自然语言处理的大多数任务，不管是单语言的 跨语言的，都在一个模型里就解决了，那么我们再看语言，一方面现在这个模型呢 它可以跨人类的不同语言。

比如中文 英文等等 我们叫它自然语言，另一方面它不仅仅是跨越了不同的自然语言，而且同一个模型呢，还同时可以训练像我们通常写代码用的各种形式语言，这个搞计算机的一从上学就开始学形式语言。

自然语言呢 它是人类沟通交流的工具，同时呢 也是人类思维的代替，而形式语言它特点是什么，它是人造的这种语言，它呢 更没有歧义，可以解释 可以编译 可以运行，事实上就是它写出了代码呢 就可以进行执行。

不管是在数字空间里 还是在物理空间里，那么同一个大模型，既能理解运用自然语言 也可以理解运用形式语言呢，也架起了从思维到执行的这样一个桥梁，这个今天早上的报告也有多模态相关的。

就是现在呢 人工智能它另一方面就是，它已经让我们看到同一个模型，可以进行多模态的统一建模，以前呢 不只是不同模态之间，而且甚至同一模态 比如说以语音为例，大家做语音识别 做语音合成，都要建立单独的系统。

那么现在呢 同一个模型可以将不同模态的，这种理解呀 识别呀 生成啊等等，在同一个模型里解决，模型的通用性 跨模态也做得越来越好，再有就是应用场景，这个是我以我们做的文心大模型为例啊。

我们现在其实已经这个大模型，应用在各行各业 各种场景 各种任务里边去，它已经 而且不只是这种生产，包括生活 包括学习 各个方面，它都展现出了很强大的能力，可以在其中发挥它的价值。

所以总体来看 我们看到人工智能在方方面面，它的通用性都变得越来越好，那么另一方面 我们看能力，当我们说起人工智能的时候呢，我相信大家脑子里会出现很多，人工智能相关的不同的能力。

这里我也只是列了其中一部分，那么这么多能力里头，哪一些能力是更基础的呢，在我看来呢 有四项能力是非常基础的，就是理解 生成 逻辑和记忆，为什么这么说呢，因为我认为其他各项人工智能的能力呢。

基本上都可以从这四项能力综合的运用中得到，比如现在这种生成式的大模型，大家都知道它创作能力很强，这个是以前的模型做不到的，那么当我们要做一个创作的时候，其实也是理解生成 逻辑 记忆，这四种能力综合运用。

而不仅仅是生成，比如这个也是我在我们的模型里去试了一下，比如说我们要写一篇，以北京的夏天为题写一篇作文，那这时候呢 我们需要理解题目，理解到这是要写一篇以北京夏天为题的文章，那这篇文章呢。

应该聚焦在北京夏天特定的氛围啊，景象啊 活动啊 等等等等啊，它对题目有一个理解，有了这个对题目的理解呢，接下来它会梳理一下自己要写的逻辑，比如说开篇要写什么，然后怎么样，最后再总结啊 再怎么怎么样。

那么有了这个呢，就需要进一步干什么，就要在记忆里去搜寻相关的素材，比如说北京天气啊 气候啊，景物啊 人物啊 文化风俗啊 等等，这些都有了以后呢，进一步就是生成出一篇，基于前面的这个理解逻辑和记忆。

生成出一篇合适的作文出来，其他能力也一样，比如说现在这个大模型有很强的解题能力啊，解题当然首先也要对题目理解，要梳理出这个逻辑来，然后呢 可能也找，在记忆中找一些相关的例子，相关的公理定理等等这些。

然后最后生成出答案来，代码呢 也是类似的这样的过程啊，规划等等 每一个都是，那么呢 所以我说，人工智能的能力有很多啊，但是理解生成和逻辑记忆这四项能力呢，是很基础的，而随着他们这几项能力的越来越强呢。

我们也就在向人工智能，通用人工智能越来越接近，好 刚才讲到这个技术的通用性，和这个能力的全面性啊，那么我们怎么做到这一些呢，那因为我们在开发文心大模型，我就以文心大模型为例，跟大家分享一下我们是怎么。

让这个模型逐渐地具备这些能力，而且越来越强，我们发布文心大模型1。0而已，就是五年多以前 2019年3月，去年呢 3月份发布了文心一言，当时是基于文心大模型3。0来做的，之后5月份就发布了3。5版本。

10月份发布了4。0版本，现在如果大家有在用，文心一言这个产品的时候呢，它后面的模型有不同的，有4。0的 也有3。5的，那么文心一言这样一个模型，这个图我相信，大家应该比较熟悉其中的一部分啊。

比如说现在要做一个这种大语言模型，它都要有做预训练，有预训练模型，然后要做SFT 要做IIHF，要做这个，这个Prompt相关的这些事情啊，同时呢我们也有一些我们，特有的技术。

比如说知识增强 解锁增强和对话增强，同时呢，基于这个左边这个基础的模型呢，我们又进一步基于这些模型开发了智能体，从基础模型训练方面呢，我们用了万卡设计算力，同时呢也有一些其他技术，比如说传闻建模呀。

可带生训练啊，混合专家等等这些技术，数据呢是要做这个模型很重要的一点啊，我们也有这种多维数据的体系，包括多种策略来优化数据源和数据分布，同时也形成了这样一个基于反馈闭环的，这样一个数据体系。

对齐方面就是，包括这个，这个有监督的晶条就是SFT，我们也做的是多阶段多类型的这种SFT，然后多，刚才这个雅琪也提到这个假立模型啊，我们也做这种多层次多力度的假立模型，包括多损失函数的这种混合优化。

以及自反馈增强的大模型对齐技术，等等这些，我们跟模型打交道的时候，都会用到这个Prompt，Prompt其实还不仅仅是用户原始输入的一个，就像大家在搜索引擎里输入一个Query。

用户输入的这样一个原始的输入呢，其实模型在真正送到模型去做进一步生成之前呢，是可以做一些工作使得，最后的生成结果会更好，这时候就包括对用户输入的理解，扩展整合以及润色，如果大家用文心一言也会看到。

它也有一个功能就是，在里面输入了一段话以后，它还提示你要不要帮你润色一下，润色一下呢，不是说润色最终结果，而是润色这个输入给大模型的这个Prompt，使得最后的结果变得更好，知识增强呢包括知识的内化。

以及知识的外溢用两种方式，而它们基础呢是，我们花了十余年时间开发的这样一个，拥有5500亿知识的这样一个庞大的知识图谱，我们都知道人类整个这个发展的过程呢，跟人类知识的不断的凝练和传承相关。

那么大模型呢除了从原始数据中学习，从知识中进行学习，对知识有更好的利用呢，不仅可以提升它的效果，也可以提升它的效率，检索增强其实我们做这件事的初衷呢，是因为大模型它有幻觉，它有时效性问题。

注意呢 而搜索引擎呢，它可以非常快的，比如说几分钟之内，在互联网上出现的信息它就能找得到，同时呢搜索引擎，因为它是一个检索匹配的过程嘛，所以也更精准，所以我们希望用搜索引擎的结果。

来帮助生成式模型减少幻觉，提升时效性，那么这里做呢，搜索引擎本身的架构也不是，很多年以前，搜索引擎早期用的这种关键词匹配链接分析，这些相关的技术，而是逐渐地已经升级为。

基于语意的理解和匹配的这套搜索架构，从而呢它们之间，也会有更好的联合优化，现在模型有很强对话能力啊，为了进一步提升对话能力，我们也做了对，记忆机制上下文的理解，以及对话规划等等这些技术。

刚才讲的是基础模型，但是呢我们知道现在基础模型，就是这样一个头盔一个头盔生成啊，基本上可以理解为它是一个，这种很快速，但是像是一个直觉系统，如果大家看过这个思考快与慢这本书呢，里边也把人类这个认知系统。

分为系统一和系统二，系统一呢快但是容易出错，系统二它慢，但是它更理性精确，基于这样一套思想呢，我们也基于大模型，开发了智能体，就是这个主要这个系统二，系统二呢我们希望它具备更强的。

理解 规划 反思和进化这些能力，从而呢，不只是它可以更可靠执行，而且它一定程度上，也使得思考的过程白合化了，当然还有很重要的一点就是，它可以调用工具，好 那么，这个系统二它核心呢。

我们训练这样一个思考模型，思考模型是基于基础的这个，大模型来训练的，在通用这个大模型基础上我们做什么呢，我们做了一个思考增强的训练，这里边包括，对思考过程的，这个有监督的精条，包括行为决策的偏好学习和。

结果反思的强化学习，有了这些以后呢，我们就得到了一个思考模型，思考模型什么样子呢，我给大家一个例子，这个例子中间这部分啊，就是大家用这个产品的时候看不到，这是我看后台，这思考模型运行的过程。

比如说用户有了一个输入以后，这思考模型就会开始想，用户是想知道最近上映的电影中，票房前五的有哪些等等，这是他对用户的理解，这是在后台看到它真实的过程，接下来他就会想，首先呢我需要调用一个。

这个web search的工具，其实就是搜索，获取最近上映的一些，电影和他们的票房信息，他想到了这一点以后呢，接下来他就开始，调用工具，调用这个搜索工具，然后拿到结果。

拿到结果以后他知道他已经获得了这个，票房信息啊，接下来呢，他就说，他不是他在说啊，他在想，就接下来我需要调用这个，code interpreter，其实就是这样一个代码解释器的工具啊，为什么要用这个呢。

他是要，把这些票房数据再生成一个柱状图，他要写点代码，前面我提到过代码啊，然后最后呢，这个生成了以后，他要输出给用户，大家看到这样一个思考过程，其实和我们人的思考过程就，很有类似之处啊，同时他调用工具。

大家看下边，下边有一些工具，比如搜索的工具啊，代码的工具等等这些，那么模型怎么知道怎么用这个工具呢，其实也和我们人很像，当我们人拿到一个，新的工具的时候，我要了解它怎么用，怎么办，看说明书。

那现在这个思考模型也是，只要任何一个工具你做好了，给他一个类似说明书这样的东西，告诉他比如说这个工具的名字，功能，参数等等这些信息，他自动就会读这个说明书，然后知道这个工具，应该怎么用。

什么时候用以及应该怎么用，前面提到代码，那么我们也基于这个思考模型，进一步做了一些，做了一个代码智能体，代码智能体当然就是，顾名思义知道是，我们是让他写代码，这时候呢思考模型。

生成这个Prompt给这个代码解释器，进行代码的生成执行，然后返回给，不管是追求结果还是调试信息，返回给思考模型，这样一个迭代的过程，这里给大家一个例子，这是一个比较复杂的需求。

这个我就不详细念这个需求了，总之是希望开发一个，图书管理系统，它有一些具体功能的要求，以及用户使用流程的要求，这一段东西都给这个代码智能体，代码智能体就，读这个需求，然后呢自动写代码。

中间这个黑背景的呢，大家能看到其实是一些，原代码文件呢，以及其他一些相关的文件，这一整套整个目录呢，都是这个代码智能体，自动生成出来，比如最右边的其实就是，其中一部分代码的这个原文件啊。

大家可能看到真的是一些原代码，然后呢这套代码就可以，编译运行，运行出来呢，就是这样一个，这个系统就出来了，因为这个时间关系我不详细演示了，就可以看到可以到里边去编辑，去删除去查找等等这些都可以做了。

最后我再稍微分享几句我对这个整个人，人工智能技术站的看法，从早期的IT技术站呢，发展到现在人工智能呢，我认为它现在已经逐渐分化成四层架构，就底层是以芯片，为代表的算力层，然后上面是，深度学习框架，模型。

然后支撑应用，而中间两层呢，更像是传统的操作系统的一层，我们知道传统IT技术站也是，芯片操作系统应用，这两层一起呢，我认为它构成了智能时代的这个操作系统，而我们做的就是这个，框架层做的就是飞角这个。

深度学习框架和平台啊以及，文型大模型，这是一个完整的飞角的这个，框架图，从基础的核心框架包括开发训练部署，到各种模型库开发套件工具组件以及，这种学习和实学社区构成一个完整的，飞角深度学习平台，那么。

这两层我说它一起构成这个操作系统啊，它两层也有很强的联合优化的关系，从我们去年3月份发布了文心一言以后呢，一年左右的时间，我们训练效率呢是已经是，去年发布时候的5倍，推理效率呢提升了100多倍。

刚才开会也提到这个，别说5倍100倍，就是一倍两倍的提升也都是价值很大的，那么也是靠这个联合优化呢，使得这个训练的有效率非常高，现在是达到98。8%，这个都是飞角和文心联合做的这样一个过程。

最后我想说呢，其实刚才规模定律这个前面的专家反复提到啊，我认为规模定律在未来若干年仍然会有效，而且这是第一我认为对未来的判断，第二大语言模型呢虽然现在能力已经很强，但是它仍然在快速地进步。

未来仍然有很大的继续提升的空间，同时多么太大模型也会变得越来越好用，另一方面就是智能体技术也会越来越成熟，所有这些呢也支撑着产业会快速地进入爆发期，最后我就再分享几句这个。

我们纵观人类历史呢每一次工业革命，它有一些核心技术啊不管是机械电器还是信息技术，所有这些技术呢它有一些共同特点，一方面这些核心技术都有很强的通用性，它会应用于各行各业。

另一方面呢当他们具备了很强的标准化，模块化和自动化的这种工业大生产特征呢，这项技术就会进入到工业大生产阶段，就会更快速地改变人们的这种生产生活方式，为人们带来特别大的价值。

那么人工智能呢基于这个深度学习和大模型，这样一整套这样我把它叫工程平台啊，上面包括算法数据模型工具等等这些，也已经具备了非常强的通用性，而且具备了非常好的标准化自动化和模块化的特征。

所以我认为深度学习和大模型工程平台一起，已经在推动人工智能进入工业大生产阶段，通用人工智能也会加速到来，谢谢大家，(掌声)，謝謝大家。



# 2024北京智源大会-开幕式及全体大会 - P5：主题讨论：通用人工智能-环节3：尖峰对话：通往AGI之路-主持人：王仲远-对话嘉宾-王小川-张 鹏-杨植麟-李大海 - 智源社区 - BV1uH4y1w77V

好啊，再再次欢迎紫菱小川张鹏大海啊，来到我们今天智源大会的现场，那我们这个环节呢是尖峰对话环节啊，我们想要讨论的是通往AGI之路啊，各位都是国内头部大模型公司的CEO，那我也想请教一下各位啊。

是否相信大模型是通往AGI之路的一个基石，亦或是大家在实际的训练大模型的这个过程中，发现它可能依然只是一个数据的压缩啊，那可能对于产业界是非常有价值的，但并不一定能够通往AGI啊，我想听听各位的看法。

那志玲对我们比较相信大模型，还是这里面的第一性原理啊，就是通过不断的去提升这个模型的规模啊，就像这个呃中远刚刚说的，它确实本质上是一个压缩啊，但这个压缩它确实是可以产生智能。

然后所以你随着不断的去规模化这个模型嗯，不断的做更好的压缩啊，它能产生越来越多的智能，当然就是肯定在这过程中也会有很多挑战啊，比如说可能最大的挑战就是呃有一些数据，可能它并不一定有那么多对吧。

那在数据没有那么多的这个领域，或者说假设你最后要做出来一个，可能比人类更好的AI，那你可能就根本不存在这样的数据，因为你现在所有的数据可能都是呃人产生的，所以我觉得可能最大问题是怎么去解决这些呃。

比较稀缺，或者说甚至可能有一些不存在的数据啊，但是我觉得规模化定律，或者说大模型本身可能是呃，没有太本质上的这个问题，对好小小川对我刚注意到这题，就是说大模型这个通向AJ是否是基石。

我用基石这词是没有问题的对吧，今天大家已经看到了这个这个skin law，它带来的这样一个提升，但是我想说的话呢，它只是在逼近AGI，但是光靠skin la我理解是不够的，所以在这里面的话。

如果从第一性上讲，这个其实刚才这个雅琴也在提到这个事情，觉得需要有范式的一个改变，所以我是认为确实是这样，在今天这个大家看到了scanning lad，这是第一件事。

今天的话呢我们还有件事情大家比较忽略的，其实是把语言放到大模型的体系里面，把语言变成了数学，反正这个这个我们将来什么往前走，语言其实把这个符号的这么一个主义，跟连接主义之间产生了一个一个一个突破啊。

所以除了这个规模以外的话呢，符号跟这种连接的融合，我觉得这是中间的一件事情，那么再往前走，还会有更多东西必须有范式改变的，比如说这个今天大模型是靠数据驱动，一种学习系统啊，能做压缩。

但是反而像之前类似于像阿尔法go那样一种，能够自我去，我称叫思考性的系统，那它也会有这样的一个一个作用，所以我的结论来讲的话，是一个是我们到了AJI的时代，这个时代里面。

我们我们是是能够这个有足够多的科学家进来，够多的资源进来，能够走向AGI，但是光是以现在我们公开大家看到的这个skinning，这个事情是做不到AJI的啊，啊张鹏张总啊，其实要说那个大模型。

它是不是说一个基石，首先同意小川说的，他肯定是基石之一，基石那是不是之一啊，这这是另外一个问题，所以这个问题呢其实也涉及到说，你怎么来定义这个AGI，其实刚才两位台上开复和雅琴也聊到。

这个AGI到底怎么定义，其实呃也跟这个定义有关，所以但是站在我们现在看到的这个角度来说，呃，其实我是觉得做人工智能的这拨人，还是挺实用主义的，所谓的实用主义就是说咱们不看广告。

看疗效这东西它能不能解决问题，是不是能不能真的像我们心中所谓，每个人心中定义的AGI那个路径上，能够帮我们推进一步，所以大模型到目前为止还是很有效的，在推进这件事情，而且就像刚才小三说的一样。

其实我们这个scanning on还是在有效，还是在往前前进，那至于说它是不是能够，帮助我们推到那个顶峰上去，我们现在也找不到这个很确切的一个答案，但至少我们相信说它在这个阶段还是有效的。

所以我认为它肯定是基石之基石，至少是基石之一，这个没问题，好谢谢谢谢张鹏大海啊，呃我我个人是数学专业毕业的，所以我可能会比较严谨的去表达啊，就是我认为大模型一定是，通往AJI这个方向上。

当前所有技术里面能走的最远的，它能不能够直接到达，我觉得现在还有很多未知的因素啊，包括刚才提到的定义是什么啊，嗯但我想提一个嗯可能大家没有提到的点，我我觉得现在的大模型作为知识压缩。

主要是在处处理人的大脑的系统一的工作啊，就是我认为就是这种嗯慢思考的系统二的呃，去做各种各样的推理，搜索空间里面去做搜索组合来完成一个任务，这样的能力我觉得是未来大模型，可能要通过AH的技术外部化。

或者把它内化为自己的能力，这个是行业里面，我觉得大家需要呃需要去探索的啊，好谢谢啊，其实确实有一个非常有意思的问题，我们总在讨论AGI，但似乎好像连ATI的定义呃，大家都没有广泛的共识哈。

然后呃我不知道就是在各位的心里，这个AGI到底什么样叫AGI啊，指林对，我觉得首先我觉得AJI的定义啊是重要的，但是它不一定需要在现在被非常精确的，有量化的定义，它可能是一个定性的感性的东西。

或者呃我我觉得因为它最重要的一个作用，我觉得是能让这个社会啊，或者说所有人吧，能能够对于说接下来要发生什么事情，有一个准备，因为就是可能也许这个技术的节奏会非常快，那我们如果能够知道AGI它是什么样的。

能够某种程度上去定义它，我觉得其实是可以更好的去准备这个事情，不管你是每个人的职业，还是说就是这个行业接下来可能会怎么发展，我觉得首先这个是是是重要的，然后可能第二个就是说呃也有一定程度上。

就是说你在短期内我觉得可能是需要一些量化，因为如果你没有完全没有量化的话，你可能就没有办法去衡量，你这个AGI的开发的这个进度是什么样的，所以可能呃从短期的角度来说。

这个EVVARIATION本身也会是个很难的问题，而且可能也是个很大的挑战诶，所以志林觉得，比如说我们需不需要有新的图灵测试，因为我们知道如果按照传统的图灵测试，应该已经被大模型给攻克了。

对传统图灵测试，可能到现在已经呃不完全适用了，因为即使说他现在通过了图灵测试，但是他可能还有很多事情是人可以做得非常好，但是AI基本没法做的，就现在其实还有很多大量这样的事情啊。

所以我我觉得这个不是一个很容易的问题，就是你可能需要去对这里面评估的维度，去做很多拆分啊，比如说你可能会有啊不同的这个能力，比如说知识能力和推理能力和创造的能力，他可能就是完全不一样。

评估的方式会完全不一样对，所以这个也是现在，可能大家很多人在关注的问题，我觉得是非常重要的，对好谢谢谢谢子琳，那个小超，咱们上次在央视对话栏目里面，其实我您也提到这个去年是智能纪元的元年。

所以这智能纪元是AIAGI的纪元吗，亦或是您对AGI这块怎么理解啊，对上次提到我刚刚提到今天是个元年，是因为我们掌握了这个skinning law，同时我们掌握了把语言变成数学对吧。

这这是重大的一个起点，当这个机器掌握语言，我觉得这次翻天覆地的一个变化，因为大家以前都在讲这个这个图片识别很厉害，无人驾驶也很厉害对吧，我可能调侃就说狗都会，这狗也可以自己导航，狗也会看图片。

但是狗是不会语言的，语言代表我们认知世界的一种，一个大的一种范式，我特别喜欢刚才你这个问题啊，就是什么是AJI的这个定义对吧，确实在全球里面很难有完整的一个这种共识，刚大学说学数学的。

我相信我们得通过变换，把它从一个空间换到另一个空间，来做一个这个判断，换成另一种事物来判断，就跟咱们讲的保形变换一样的，那么在我看的话呢，我会用一个这个大家可以评测的的，这个指标来来看。

是在我心中是接近等价的，是什么呢，是能不能够去造医生，咱们找医生，为什么图这么一个奇怪的一个题目，之前我们在谈AGI的时候的话呢，一种首先理解把它当成工具在在看，那我认为这次AJI的这个首先第一个变化。

它是能够开始有这种思考能力，学习能力，沟通能力，共情的能力，甚至有多模态的这种这种图片处理的，这样能力是从他的学习范式的能力要求里面，我反而觉得我们是在就是像看人一样，在看他的啊。

说一种做法是说跟人是差异化看，但我从我的这个整个这整个这个从我们今天，所以大家共识的评价指标或者学习范式里面，就是在向人学习，它数据来自于人类社会产生这种数据，所以一直在评价里面。

我是拿人的一个职业来跟他做这样一个比较，那医生在这个所有职业里面，相对而言是一个叫智力密度相对最高的，这么一个行业，既需要多模态，也需要少幻觉，有效记忆，看你70万字的这么一个病例啊，也有推理的能力。

也有这种查文献的能力等等的对吧，所以我把医生跟AGI做比较时候的话呢，结果就会说那做到AJI，做到医生是否就算做到AJI了，然后我发现他肯定有很多种声音的，比如大家哎呀这个医生只是一个vertical。

是个垂直，那就一生比这个低，那但是我说那人造一生吗，他说哎呦太难了，这里面有太多太多的这样的一个这个幻觉问题，有太多的这么一个一个，咱们讲到的这种这种他的推理能力对吧，这种这种不可靠。

那如果如果我们认为医生是比AJI低的，医生都造不了，我就就就咱们就别谈这AJI这事，但如果你觉得医生比AJ还高，但是我们又讲这个医生只是这个这么一个叫做，造人的各个种类中的一种，所以在我的这个逻辑里面。

这医生跟AJI讲，我是可以基本就画个等号的，那数学上有一个题目，刚大海也强调啊，就是自然数和偶数哪个多，我们第一反应是偶数比自然数少对吧，偶数是自然数的一个子集嘛，就每两个数只有一个偶数。

但数据上应该知道它们是一样多的，因为每一个自然数乘以二就是一个偶数，它们俩是可以映射的对吧，所以把自然数和偶数能做映射，今天我是把大模型，咱们今天行业上能共识的能力，都可以映射到对于这个医生的一个要求。

你去硬是拿这个做一个标准，你知道医生就是个AGI好，谢谢小川，哈哈呃，大海你你大海你被Q到了哈，那个数学家作为哈，然后你对这个AGI啊怎么去理解啊，嗯我会尝试从经济学的角度来去定义AGI。

我觉得呃从经济学的角度讲，如果我们去执行任何一个任务，它的边际成本都为零，这就是我们理想中的AGI了，但是这个又回到我刚刚说的，为什么我认为大模型能够走得最远，就是我相信大模型能够把这个边际成本。

一直往下降，可能会逼近于零，但这个过程中就像指令刚刚讲的，很多时候，需要我们在各行各业的数据产生一个飞轮，逐步的让模型持续训练，持续学习，然后让整体的成本降下去，其实我们去年看到行业里面。

大家去做大模型的落地的时候，很多的场景都还需要做微调，这个边际成本就很高，我们相信随着模型能力的提升，慢慢的从微调，逐步的只需要做prompt genuine。

但慢慢的连prom prompt engine定你都不需要做模型，直接就问你说你到底有什么需求，如果你讲不清楚，我来问你对吧，通过这种方式，我相信未来的门槛会越来越低，越来越低，成本会越来越低。

低到接近于零的时候，我觉得AGI基本上就到来了啊，另外一方面呢，我们就是我可能额外还想补充一个观点，就是我们现在大家都在讲怎么把模型做大，其实刚才小川提到一个关键词叫叫智能密度。

其实我们觉得大模型的智能密度也是个，非常重要的事情，就当有一天我们到达到AGI的时候，我们还要做的一件事情是大模型的小型化，就是我如果用一个10万亿的参数的模型，能做到AGI。

我能不能把这个10万亿的参数把它降到1万亿，把它降到1000亿，对，这也是一个我觉得持续要去突破的事情，对啊对，其实面壁智能，以及像质朴跟智源都有非常深厚的渊源哈，其实当年啊面壁的刘老师，刘志远老师。

以及像咱们质朴的谭杰老师，都是跟志源一起哈，就开始做我们的悟道系列大模型，所以也想请教一下张鹏，张总，最开始咱们呃做悟道系列，再到后来咱们整个质谱系列这些大模型，你最开始有考虑到它可能实现AGI吗。

您对这个AJI是怎么去去理解的啊啊对，其实我在我们看来看这段看待这件事情啊，AGI这件事，其实你要说它是一个有一个很严格的定义的，一个定义，还是另外的什么东西，其实我更愿意相信它可能是我们的一种信念。

是一个符号，它的内涵外延是在不断的变化的，其实刚才提到的早期的定义AI的时候，我们说怎么来检测这个系统是否是个AI系统，图灵测试，但是现在大家已经觉得这个已经过时了，就是因为随着我们对技术的不断的演进。

对对对事情的这个认知越来越多，然后呃越来越深，然后其实本质在同样，这三个字母所代表的含义，实际上是不断的在变化，在是个动态的过程，所以刚才哲理也讲，就说其实他是一个balance的事情。

就是如果你把一件事情，能够把它说的非常的量化，非常的清晰，内涵是什么，外延是什么，那这件事情that is也就那样了，估计天花板在哪，大家都能看得到了，那现在的问题就在于是说。

其实没有人能够把这些说清楚，那反过头来讲是一件好事，就是说这个事情还有很多的空间，很未知的空间等待我们去探索，所以AJI对我们来说说，你可以把它定义成我们的一个目标，那对这件事情。

我们一直相信是说当前我们的目的呃，我们的目标是说以人为参照，让机器像人一样去思考，这就是我们的愿景，那这只是第一步，当然刚才也提到说，机器的能力远不止人的这个水平。

那我们期待它可以出现超越人的这种这种能力，所以AGI里边我们会提到说，有这个叫super intelligence，下一步它是否能产生超过人的，这样的智能的水平，那就是我们会不断的去更新迭代。

AGI的这个内涵和外延，好谢谢，其实呃植林也跟智源是有非常深厚的渊源哈，当年也是我们悟道系列的这个核心的技术骨干，然后啊也是我们志源的青年学者，然后呃，我们今天其实早晨在反复，大家都提到的一个词哈。

Scaling law，我不知道指令，你对skin low呃还还是呃特别的坚信吗，就它会继续在未来的这些年会起作用吗，对啊就像我们刚刚说的，我觉得这个skin law呃没有本质的问题，对，就是呃。

而且我觉得接下来可能比如说3~4个数量级，我我我觉得是非常确定的一个事情啊，就是可能呃我觉得这里面更重要的问题是说，你怎么能够呃很高效的去scale，然后你应该scale什么东西，就比如说如。

如果你只是还是像现在就搞一堆这个web text，搞一堆这个网页的文本，然后再去再去scale，我觉得看他可能就不一定是一个对的方向，因为因为呃这里面可能就会遇到很多的挑战。

比如说我们刚说的这些推理能力，它不一定能够在这个过程中解决啊，所以我觉得这里面其实本质上是就是怎么定义，Scaling law，就是说是scaling law是什么，是什么是什么。

对如果如果你是说我就沿着当前呃现在的方法，然后我去做next token prediction，然后我再去scale很很多个数量级，然后用跟现在完全一样的数据分布啊，我觉得我觉得他的上限是很明显的对。

但是skin law本身它其实并不受这个东西的限制，就是呃本质上他讲的是说我只要有更多的算力，然后我更多的数据模型参数变大，那我持续能产生更多的智能，但这里面它其实并没有定义，你的模型是什么样的。

比如说它有多少个模态，它中间的数据是什么样的，它数据是你生成出来的，还是说呃我是可能还是用这个web tax啊，所以也没有规定你的这个loss function是什么样的。

就是你不一定是next token prediction，你可能是别的loss function，所以我觉得skin law是呃是会持续演进的一个，还是first principle，我觉得是这样。

然后只是说在这个过程中，你要scale的方法可能会发生很大的变化，对，包括现在比如说像央的co，一直在讲的这个世界模型啊，我觉得其实本质上现在的大语言模型，它是世界模型的一个特例对。

所以你只是说先把里面比就是一部分给做了，但是你还能把可能更多的啊持续的去扩充，这个这个啊训练的这个方式对，所以我我我觉得scaling是会持续，只是scale的方法会变化，好谢谢啊。

小川你对skin low啊，还是未来几年会持续发挥作用怎么看啊，对对是skin low啊，您您觉得是否会在未来几年持续的发挥作用啊，对我我觉得skin log，他一定会。

这个这个到目前没有看到边界在持续的发挥，所以看到美国在今伊拉mask对吧，号称要买30万片，这个是B100，还比200B200B200来做，所以在这种情况里面的话呢，这个美国确实在这方面的这个认真程度。

甚至包括投入程度是是会远高于中国的，因为在我看起来的话呢，我们在这个SKAL之外，一定要去寻找这个范式上的新的一种转化，咱们主要讲数据算法算力这里面，所以我觉得skin law他们在里面是很明确。

就是在美国后面跟进的这样一个维度啊，不管从我们的战略上讲，还是从我们信仰上，我认在SKAL之外，都还有泛式的这样一个一个变化，就不只是简单的去predict and token，变成压缩这样一个模式。

会走出这样一个一个体系，才有机会走向AGIE，才有机会能跟这个这个最前沿的这代这个，技术里面产生这种这种较量的能力啊，呃张鹏，您对，is skin low呢，啊其实我刚才在讲这个AJI这个定义的时候。

其实已经表达了相当的观点，就说skin law这个事情，它本身这个定律啊，我我我觉得啊就说到目前为止，人类认识的所有的这个规律也好，是这些物理定律也好，什么也好，其实都有可能有推翻的一天。

只是看这个它的有效期有多长，所以刚才也同意的，就是前面加一个就是定语的话，就是到目前为止，我们还没有看到skin la会失效的，这样的一个一个一个一个预兆，未来的相当一段时间之内，它仍然会有效。

当然这个所谓的会有效也是一个动态的概念，就在于是说它本身所包含的这样的一些事情，会呃内涵它会不断的去演进啊，就像刚才小三说的那，Sky law，早期关注的其实就是简单的模型的参数量，规模对吧。

那现在已经慢慢的大家过呃，扩展到什么呢，参数量很重要对吧，你的数据量也很重要，数据质量也很重要，计算量就变成了一种计算量，所以你看到它的内涵其实也在慢慢的变化，那其实是随着大家对这个规律的认知越来越深。

规律的本质越来越揭示，所以你掌握这个本质，你就能掌握说通往未来的这个钥匙，所以呃基于现在大家对这个本质的，认识的这个深浅，我觉得至少在我们这样看来，它仍然还会起效，还会是未来我们主力想要推进的。

这样的一个方向，好我我想我想追问一个问题啊，其实啊我们现在也看到像GPT5啊，之前传过几次说要发布，但一直都在似乎在推迟啊，在推迟，那么所以张鹏张总，您觉得这个SKINO。

包括咱们自己因为也都在训练大模型哈，就是如果我们从追追逐GPT4，到我们要突破GPT4，再往GPT5的这样的方向去发展，现在scaling law是有出现边界效应吗，我不知道你们怎么啊怎么看这个问题啊。

啊我觉得这个因素可能有很多种，包括刚才说的传说的这个所谓的4。5，还有五呃，什么时候发布，为什么一直大家传了好几次都没有发布，我觉得这个可能里面的因素会非常非常多，就拿我们自己来做啊来说这个事情的话。

其实我们自己也在选择一个道路，不断的去追寻这个追寻这个sky law往前前进，大家就举个例子吧，就是呃其实在这个我们最开始你也记得，就我们开始做这个舞蹈的时候，就讨论过一个方案。

就说我们是否去做一个稠密的一个单体模型，还是去做一个MOE的，一个稀疏的一个多体的模型，其实这就是当时我们认为说，如何去去满足这个词，跟领导或者去追寻SK领导不同的路径，但是到发展到今今天呢。

今天这个地步的时候，你会发现其实这里边维度已经非常非常多，你可以在很多方面去做这样的一个事情，所以但同样反过来看这个问题的时候，你会发现其实这个难度又复杂度又上升了。

不是简单的说追求这个参数量上去就行的，就难度也变大了，所以我理解想要实现，比如说GBT5，或者在下一代，我们自己想要实现下一代这模型，这个里边的技术的可能性啊，要探索的这个东西还是非常非常多的。

呃也是一样的吧，就正反两方面好，谢谢呃，大海，咱们面壁，其实主要是关注在端侧的这个大模型哈，所以我不知道在轻量级的这种大模型上，您认为是skin low也是有效的吗，嗯我认为SKINO是非常重要的。

对嗯但是我们我也非常认同张鹏的意见，我们觉得就是sky law其实是一个经验公式，是整个行业对于大模型，这样一个复杂系统的观察，以后的一个呃一个经验总结，这个经验总结会随着我们对于模型训练。

工作过程中的做的实验越来越多，认知越来越清晰，会有更加细的颗粒度的认知啊，比如说我们自己就会发现，除了前面说的这些这些这个呃维度之外，在模型训练中的训练方法本身，对于scanning law。

对于智能的影响也是也是比较显著的，那这个显著的影响在我们固定住参数规模以后，其实就会变得非常重要，因为现在大家觉得参数规模是能够不断的往上，scale的，它是低垂的果实，只要扩就可以，所以就觉得没关系。

我们先去做这个，先把上往上放大，但是一旦我们固定说，我们要让端侧的芯片能够去支撑，这个这个这个呃呃规模的呃，模型能够去做到足够好的智能，那么数据的质量啊，训练的方法，这些都变得非常非常重要啊啊对。

然后大海，我们最近其实也关注到一个非常热门的新闻哈，就是呃关于开源社区的一件事情啊，那像STANFORD的啊，拉玛呃他们的这个这个团队吧，啊然后确实抄袭了咱们的mini c p m。

那我不知道您对这个事件是怎么看的，对最近这个事情在国内引起了非常大的反响啊，我们也完全没有想到，我们的这个工作会以这种方式出圈，这个挺惶恐的，呃，呃在这里呢我想呃也想澄清一下，我们自己认为，这其实是呃。

海外的个别学生组成的一个小团队啊，做的个人行为，它不代表任何更大的，比如像STANFORD这样的这个学校啊，那个呃因为事件发生了以后，像STANFORD的呃系主任，他们也还有就是整个西方的一些这个同行。

其实也都表达了非常价值观，非常正的一些呃观点，然后另外我们因为这个事件，我们会更加坚定的相信开源的力量，其实像这样一个呃事件，它的发现也是靠我们开源的热心的参与者，发现的，并不是我们自己发现的。

就是我们是5月20号把这个模型开源出来，然后呢到了29号的时候呃，这几个本科生小朋友对他们嗯，他们就做了一些非常简单的工作，在我们的模型上做了一个高斯叠加啊，就叠加一些高斯噪声，然后就号称是自己的模型。

然后呢啊当然他这个模型一下子变得很受欢迎，主要原因是因为他们宣称，这个模型的多模态能力，是跟GBD4V和j mini pro完全对标，但是参数只有后者的1%，并且还只需要500美金就可以训练出来。

那前两项是真的，就我们的模型真的是有这样的能力受欢迎，但是500美金是新愿不出来的，还是要花很多的钱啊，呃在这个词就是五，5月29号发生这个事情以后，其实一天之后就我们社区里的热心的参与者。

就发现了这个事实，然后呢去把这个事情曝光出来，让我们能够尽快的知道去纠正了这样的行为，所以我们看到开源的力量是很强大的，这里面是多层次的，不光是有做原创工作的人，还有很多的呃参与者。

他们会在里面贡献需求，贡献反馈啊，这些也都是对开源这个生态，非常重要的一个组成部分，让我们觉得呃持续的做开源的贡献，能够给公司带来正向的收益，是呃志愿也是非常坚信开源的力量啊。

所以其实在今天的这个报告里面，我们也向各位会这个报告，我们过去一年啊在开源社区发布的各种的模型，其实我们的下载量也还是非常大哈，那那个其实百川啊，百川也也把自己的百川一，百川二其实都对外开源了。

我不知道啊，当时咱们百川想把自己的自这个也花了不少钱，训练的大模型对外开源的一个考量是啥啊，小说其实当时开源的话呢，我觉得是在市场上第一个是有这样的一个需求，有需求，因为在当时这个我在去年。

我们大概是在9月9月份就开了第二版啊，6月份开的第一版，那么在去年的时候，这个应该叫中国式快速入场做大模型，但市场化呢不仅闭源跟美国是落后的，对欧派顶在里面，我们大家都没做到，开源上的话呢。

当时拉满也开源了，所以在美国其实这个既有大的闭源生态，也有开源的这样一个一个生态，中国在当时，其实对大模型是处于一种大家热情惶恐，也需要快速入场的，所谓这个开源的话呢，在市场上产生了蛮好的这个影响力。

但是一做一家这个把自己当时最好的模型，开源的这样一个做得认真，且开源的这么一个一个商业化的厂商，得到市场的很多的这个认可，也给我们做了很多的这样的一个好的这个credits，对吧，对我们是挺大的鼓舞。

不管最后人才的储备资本的这种这种关注，也算是给行业交了个投名状，但我觉得这是有历史上他的，当时的这样一个意义，但当时我就看完还有个心态的话呢，我们也看到模型会快速的这个进步的，所以在当时我们觉得开了源。

是不是把自己的这个这个底裤就拿出去，这这个就就就没有竞争力，我觉得不会的，因为在那个我们认为这个模型生态里面，大家这个今天我们最好的，可能在明天就是一个不够好的模型了，所以我们从商业竞争里面。

其实也没什么大的损失，一手机的既有贡献，又没有这样的一个这个降低我们竞争力的事情，那么就依然做了这样一个决定，所以这个符合了市场预期，也给公司带来了这样的一个一一，一个他的这种声誉。

为什么这个事情做的蛮成功，挺对的一件事情，那今天其实有更多的这个公司在里面，也这个做各方面的这种开源啊，使得中国这样一个生态在追赶美国里面，包括保护我们自主产权里面，我觉得大家共同在做这样的一个贡献。

我也希望这个生态能够大家越做越好，好谢谢小川，这个随着整个大模型的发展，确实呃AI安全问题也是被不断的讨论，那各位都是做企业的，我想了解一下，就是说AI安全现在在我们大模型的产业界啊，怎么去看。

是一个当下最急迫的问题吗，志玲对我觉得AI安全是非常重要啊，它可能不一定是当前最急迫，但是是一个我们需要提前去准备的事情啊，因为呃呃可能随着模型的进展，因为scaling law本身它的发展是说。

你可能每N个月对吧，你可能就是算力乘十倍啊，那这里面你的智能会得到提升，那我觉得是一个逐渐去适应的过程，所以它不一定是说当前最大的矛盾或者最重要，最紧急的事情，但是他肯定是长期储备。

那这里面我觉得最重要的可能两个方面吧，一个就是说你的模型本身，它可能会因为你的这个用户，它本身有一些恶意的这个意图，那导致他这个会发生一些，去做一些他可能本来不应该做的事情啊。

比如说像现在有这个研究去做这种injection，就是你可能你比如说你有long context能力，但是你可以在prom里面去注入一些这种呃，这个不太恰当的意图，然后可能这些我觉得需要去关注。

然后第二个就是说你的模型呃，本身它是不是会有自己的这个呃motivation啊，所以我觉得这个是跟这个训练方式相关的，包括就是你能不能在这个用户的呃，就是在模型的这个呃底层能够去注入。

能够去框定他的行为，就不管用户给什么样的指示，或者不管他自己的china thought是什么样，我觉得这个是很重要的，对好啊，小川对我觉得安全的话呢，有不同的这个内涵和外延。

所以我想提三个安全相关的事情，第一个事情就是这个意识形态安全，由于大家都知道做to c有工作人这样一个服务，所以作为一个这个中国主权的这么一个大模型，在意识形态上能跟这个国家发展。

国家意识能够能够保持一致，这是大家的一个基本功，就每个模型有他们的这种价值观，我们有我们的价值观，所以这个安全的话呢，我认为是对一个民族，对一个社会负责那件事情，这个安全是个底线。

我们大家得得把它给给做好了，那第二个就安全，是大家这个空谈的比较远的一种安全，就是这个模型是不是把人类毁灭了以后，人就没了，然后机器掌握世界了，那所以在这里面的话呢，我其实内心来想不希望发生的。

就像核弹一样的把这个人类文明给搞没了，这使得这个我们人类发展了好几千年，好不容易有这样一种智慧结晶一个模型，然后把地球搞没了，这个事情来讲，我觉得不发生的，但是至于这个模型，它是否比人更聪明。

能够取代人做事情，我觉得这是个鼓励的一件事情，因为从人类文明里面，我们现在每个人都知道这种生孩子，然后这种发展技术去延续我们的生命，和延续我们的文明，这才是重要的事情，但人的肉身在中间。

每个人都会死亡的，我觉得大家这个今天不回避这个问题，所以这个技术能够跟我们一块去拓展人类文明，我觉得这件事情是有意义的，并不是限制这件事，所以在去年这个我下场的时候写了一封公开信，我还想着他AGI。

帮助我们繁荣和延续人类文明，我把这个事情当成一个目标，所知只要在让文明能够更好的延续，而不是说只是叫做机器，一定帮到我们奴隶，当我们工具，我觉得这这一派在安全里面。

我可能在中间是以一个文明为标准来看待它，这是第二层的这么一个安全，是理想的色彩，第三安全，第三个就是比较现实的对吧，我刚才提到说AJI是什么，AJI怎么评测，然后当我跟很多人在聊时，拿AJI去做个医生。

他就哎呀好难啊对吧，人医疗都搞不定，如果连这个都做不到之后，那他能力如此之弱，我们就不要想它是什么颠覆人类，还有这么复杂的事，所以现实里面，我觉得在近期里面还不存在这个安全的问题。

就觉得这个所以在现实里面的安全，我们就放在一个意识形态安全，在远期里面发展文明，而当前里面还是努力把他的能力给提上去，还没碰到今天的一个人类文明安全的边界，嗯好谢谢小川嗯，好呃呵，张鹏呃。

您对AGI怎么啊，AA对AI安全，我们呃，智虎其实一直很注意这个相关的一些事情，然后尤其在AIAI安全方面，因为啊我们应该前前一段时间，还签署了一个A呃，前AI安全的这个前沿人工智能安全的承诺是吧。

对对对对，差承诺书当时什么样的考量啊，呃当时是应该是有15家这个企业吧，有个AI相关的一些企业，一些企业，然后来自全球各地各大洲，然后一起签署了这样一个负责任的AI的，这样的一个承诺书。

那其实我觉得可能安全只是其中的这个，所谓的一部分，就是啊我们叫负责任的AI，那负责AI这个事情就比安全要更大一点，安全其中的一块就是包括刚才呃，小川师兄想的这三个方面的这种安全。

但其实更多的还有更多的方面的问题，就是我们如何来保证，或者说如何来努力让这个技术是真的帮助人类，帮助这个社会，帮助这个地球，而不是说去为恶，当然这个事情你啊很难说呃，人的这个两面性嘛对吧。

很难说你们保证没有人去拿这个事情去作恶，其实现实社会当中已经有人在发现，已发现有人在做这些事情，那这个事情永远是就是你防守比这个破坏要难，所以这个需要大家一起共同来努力。

我相信这个事情的更重要的一个意义，并不是说我们现在能拿出多么安全的，这样的一些技术方法，或者是呃这样的一些管理的规定去约束大家，不要去做这些事情，而是在于说增强大家对这件事情的了解啊。

对这件事情的这种统一的这样的一个认识，大家能够坐下来正面的面对这些问题，把这些问题摆摆到桌面上来啊，希望大家更多的人参与这件事情来一起讨论，那总有解决问题的啊，这个背这个办法对好啊。

大海咱们对AAI安全这块的看法，对我比较同意前面各位老板说到的这个观点啊，我认为现在这个阶段，安全主要还是聚焦在基础安全跟内容安全，这两个方向上，然后我会觉得未来现在的大模型，其实本质上是只读的。

就是我们把模型训练好，权重是固定的，你的推理其实不会影响权重，你的权重都是在线下再去持续的阶段性训练的，有一天当我们把模型部署到机器人，部署到这些，我说的就端终端上。

然后他能够去动态地去更新自己的权重了以后，我觉得安全问题会变成一个，非常非常重要的问题啊啊好谢谢谢谢大海，我们今天其实讨论了很多关于AGI，也讨论AI安全，但其实对于在座的都是企业家。

企业家对于企业而言，可能虽然也很关注AGI，但可能也更关注ROI啊，那那个其实最近有好多的这个记者朋友，也都在问我，关于这个最近的大模型的价格战怎么看，然后我当时给他们的回复说，志愿研究院是对吧。

精密的拥抱开源，我们都是免费的，给整个社区，给产业接待使用啊，但我也给他们承诺哈，就是正好借资源大会，间接这个圆桌的这个的机会，所以也想请教一下诸位啊，对于这个近期大模型的价格战的一个看法。

它是更有利于大模型的普及，还是实际上这种过于对吧，就是激烈的价格战并不利于企业的发展，尤其我们知道大模型还是需要啊，有非常持续的投入，还在研发的过程对吧，还是企业要有要有正当的这样的一个利润。

才才能够进进入到一个持续良性的一个发展，那志玲对我我觉得这是很好问题，我觉得呃最终啊，如果我们把时间线拉的足够长的话，其实最终还是会回归这个价值本身，我自己有三个判断。

就是呃第一个就是说嗯很重要的一个点，就是其实在接下来呃，就是呃比如说我们去看这个算力的投入，你可能投入在推理上的算力，在某个时间点之后，它应该是可以这个显显著超过训练的这个算力，我觉得这个是标志。

就是说你你的你的价值开始得到释放，所以你可能前面用来训练的这些呃，这个成本它其实是可以很大程度上被覆盖，然后可能第二个很重要的节点，我觉得是说呃就是呃如果从C端的角度来说。

我觉得是呃你的呃推理成本可能会显著的呃，低于这个你的获客成本对，所以我觉得他可能从商业本质上来讲呃，可能不会跟这个之前的各种呃商业模式，会有这个非常本质的区别，对我觉得这两个是很重要的。

然后有了这两个东西之后，我觉得很很很很重要的是第三个因素啊，就是我们今天其实AI在整个呃，人的这个工作流程里面的占比他还是很低的，他可能是1%，也就是说人做的事情要远远多于AI做的事情。

所以我觉得最最重要的第三个点是说啊，AI本身做的事情，可能在会在某个时间点呃，超过人做的事情对，那这个时候，我觉得他就可能会产生新的商业模式，就它可能不是像今天说的，在B端用API去做价格战。

而是可能他是一个普惠的AI，同时可能是根据它产生的价值，从这里面去分层产生的这个商业模式对，所以我觉得可能这三个点是会是改变啊，这个这个三模式本身，或者你刚说的LR这个问题的一个，很重要的一个呃。

去世对好，谢谢指令啊，小川，您对这个近期这个大模型的价格战怎么看哈，我先说结论，我觉得大今天价格战对于这个这个中国发展，大模型是特别好的事，现在结论我是积极看待这样的一个事情。

首先一个视角就是很多时候这个好不好，你得看是对单个的公司还是对一个这个群体，一个整个市场，因为价格战的话呢，通常是这是个市场行为，是一个竞争的这么一个一个导向，那我觉得至少带来两个好的后果。

第一个更多公司更多人能用上大模型了，像很多企业之前是不懂这个的，就变成一个一场普及运动一样的，就很多公司他给免费很便宜的开始做POC，开始去使用大模型，使得这样一个大模型，在中国能够迅速去有一个普及。

不管是个人还是很多企业就就入场了，这对整个市场是第一个好处，第二块的话呢，其实在中间我觉得之前还有很多浪费，因为大家恐慌的时候，大家不知道大盲选为何物，我就观察到很多企业但凡有点技术能力。

都说我自己叫迅游大模型对吧，然后自己拿卡，甚至跑来找我们说怎么怎么联合训练啊，这块明明他是该是大模型的用户，消费者大模型的使用方，但都想转型成为一个大模型的供给方，要提供一个自己大模型。

然后我我要这个行业做大模型，一个企业变成属于作为行业，这样情况下，其实带来很多的人才资金和这种社会的浪费，但有价格战之后的话呢，很多企业就开始清醒了，诶我干嘛非得去做，做完了我到底在干嘛。

我的竞争优势在什么地方，他就退回来说成为大模型的用户，我觉得这个浪费也会减少很多，所以既能带来一个启蒙，也能带来对社会资源的一个减少，这样一个一个消耗，那那更多的企业在里面就是能够有自己定位，对吧。

把自己给做好，我们不需要一千一万个大模型，在之前如果没有价格战的时候，中国可能这个真的是这个上百，上千个大模型在在进行，那么这样的市场的分成就能做好，每家都能够受益，这种竞争力就能起来，谢谢小霜。

非常鲜明的观点哈，我不知道张鹏您赞同小川的观点吗，呃基本上是赞赞同这个观点的，而且这个事情其实有人跑过来跟我们说，说你们是这本加油站的什么什么什么发起方，什么什么之类的，我说这个子虚乌有啊，子虚乌有。

其实在那个之前，其实我们一直秉持的一个概念，其实就是你说的那个LI，就是给大给用户带来最大的收益价值，然后用我们的技术，用我们的创新，去极低的降低大家的使用这个技术的成本。

这是为了让这个技术能够更多的呃普及，让更多的人能够享受这个收益，所以我们当时推出的这些呃，其实在很长一段时间里面，我们的很多的这个价格，都是行业内都算是呃极低的，那因为是我们的技术能够确实能做到那一步。

能把这个中间的这个空间成本的空间释放出来，当做大家的这个呃这个收益，帮助大家去把这个ROI算出一个，大于一的数值，当然这个事情从宏观角度角角度来讲，肯定是说，对于整个中国的这个大模型产业是有利的。

让更多的人来使用，让更多的人来把这个真的把大模型当做什么呢，当做我们一开始提到的那件事情，就是它会变成这个基础设施嗯，基础设施什么意思，就是非常便宜，随时就可以用，你不用去特别的计较，说在这个事情上。

我要投入特别特别的大，然后收益是什么，当真的有一天AI的能力，大，冒险的能力变得像水电这样的基础能力的时候，其实这个事他就对我们来讲，企业来讲是一个更好的一个发展的，这样的一个空间。

这样好更好的一个发展这样的一个态势，所以这个也是我们一直在坚持做的事情，包括最近20号我们发的这个新的这个模型，它的真的是把成本已经压到，我们都不好意思跟大家报价的地步。

那你以前你看都是大家报价都是1000头客，多少多少钱是吧，几分钱，这就没有比这个更小的单位了，人民币里头就几厘，这个好像没法算了，那干怎么办呢，把单位变成1000000token多少钱嗯。

就这他已经到了这样的一，一个一个一个一个地步了，所以我是觉得这件事情对整体上大的是有好处，但是也要注意的就是说不要去呃，过多的关注这件事情，过多的宣讲宣扬这件事情，商业上肯定这件事情就说你去牺牲呃。

企业的短期的这种我们比如说成本也好，什么就亏本做买卖，这个不是一个正常的商业逻辑，这个肯定是只能持续很短的时间，真正还回归这个最终的用户价值，生产力价值上好，谢谢大海。

咱们在端侧的大模型会面临价格战的困扰吗，哼其实呃我们做端测，就是看到了端测的一个落地更早，能更快落地的一个一个可能性，其实呃最近有一个机构做过一个调研，发现说全国10亿用户的手机的这个端测的算力。

差不多相当于100万片H100，这个是非常非常夸张的一个数字，如果这些就是不同的手机上的，这些算力能够被啊好好的利用起来，其实我们很多的应用就可以落地了，当然这个里面现在这个阶段就是啊。

一定是包括现在到未来，都需要端测模型跟云测模型好好的协同啊，这个是端测有端测的啊优势，它的优势是隐私性好，然后更可靠，但是啊云上的模型肯定能力要比端测更强，所以怎么把端跟云协同好，我觉得是一个呃。

后面其实我们跟所有其他的模型公司，一起要去协作的事啊，然后呃我也非常同意前面大家说的观点，我认为呃我我自己的看法啊，我觉得当前的这个价格，所谓的价格战多多少少有一些营销的成分在。

但是我相信未来一定会比现在这个还要便宜，并且大家都有利润，这才是健康的方式，并且这才真的能让千行百业的应用望下落地好，谢谢大海对，其实想问的问题还有特别的多，但因为时间的关系哈。

所以可能没办法全部都问完，但最后也还是想请呃各位CEO啊，因为我们知道其实在座的诸位啊，以及在座的这个企业，其实跟智源都有非常深厚的渊源，也有很这个深厚的这个关系哈，那也想请大家对于资源。

对于呃对于我们的智源大会啊，看看有没有什么样的一些寄语，好吉林对呃，资源是对，我记得应该是我们2020年的时候，开始开发这个悟道模型，所以我觉得其实资源是可能呃，在至少是亚洲地区，我觉得最早投入呃。

而且是真真seriously，就真的投入去做这个大模型的机构，我觉得这个是非常难得的呃，很很早的一个呃呃非常领先的这样的一个想法，然后今天我觉得其实整个视视野也会更宽，就是说不光只是说大语言模型。

可能也做很多多模态，然后聚生智能，然后慢慢资源大会也变成一个，就是一个非常好的平台，能够让大家在上面交流啊，所以我呃我们其实收益都很大，所以也希望就说这个能够持续的成为一个，就是这种全球领先的平台。

对好谢谢志玲，小川本，其实这个这个字源是可以应该叫中国大模型，这个的黄埔军校，要整个这个大模型的这种思潮，它后面的技术都从资源这个发展起来的，其实我上一家做的公司叫搜狗，当时这个比较早就给资源在合作。

提供，当时搜狗搜索的这个数据来发展这个大模型啊，从一下子到今天，中国大模型已经喷薄而出了，那今天我的资源有它非常好的这么一个定位，因为其他更多咱们都是市场化的这样一些企业，那企业在过程当中的话呢。

既有技术的需求，也要跟政府的这样一个连接，因此在资源在中间处于一个中立的，有技术高度，又同时能得到这样的一个这个政府信任，这么一个一个机构，所以他扮演的这个技术角色。

和这个公平的这么一个一个智库这种角色对吧，这两方面讲，我觉得是有独特有的这个意义，所以我认为在今天这样一个生态里面，是能够帮助我们更加快速健康的发展，好谢谢小川，张鹏志源，我们就很熟悉了这么多年了呃。

这个这个智源从最早定位成一个NGO，一个偏研究的一个新型的研发的机构，到发展到今天，也真的是已经是这个国内，甚至是国际上这个人工智能领域的一面旗帜了，这也是我我觉得也是智源取了一个很好的名字。

就是发布的那个系列，flag系列的这一个产品的这个呃本意吧，呃也是可以看到资源在整个人工智能，这个这一次浪潮当中有非常大的这样宏观，很好的这样一个宏远的这样一个布局，我们也非常希望跟智源能够。

长期的在学术研究啊，这个落地应用的合作，甚至包括公共政策相关这样的一些方面，能够很深入的这样的继续保持合作呃，也希望也祝愿这个志愿大会啊越办越好啊，谢谢张鹏啊，大海对啊，大模型这个领域变化非常快。

但这里面确实有一些事情是，商业公司没有动力和没有嗯，可能也没有资源去做的事，我们非常期待啊，在资源的这个中间的撮合跟带领下，能够把行业呃搭建一个更好的这样的平台，能够把这些需要做好的事情能够一起协作好。

然后也祝愿智源的每年的年会，像我们AID的春晚一样啊，能够越办越好，好，谢谢谢谢各位CEO，对于志源这一智源大会的这个美好的祝愿哈，那因为时间的关系，我们今天这个结婚对话的环节可能就到这边。

