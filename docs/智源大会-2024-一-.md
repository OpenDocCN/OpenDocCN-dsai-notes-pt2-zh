# 智源大会 2024（一）



# 2024北京智源大会-AI系统 - P1：论坛背景与嘉宾介绍-林咏华 - 智源社区 - BV1DS411w7EG

呃好呃，欢迎大家参加今天的这个智源大会，我们的AI系统论坛，那个这个论坛是整个大会里头唯一一个承载的，所有的大模型所需要的诶，AI所需要的跟算力是相关的，所有的系统的问题，那呃作为一个opening呢。

我我还是想在呃给大家呃，从呃几几页简单的片子来呃，强调为什么我们今天这个论坛很重要哈，那首先我们看到呃，sorry因为我们有一位外宾，所以那个我稍微把有一些title啊翻译成英文。

那首先第一个我们看到新的这个大模型的趋势，是说更大的模型，更多的参数从千亿千亿参数，重密模型到万亿的系数模型，包括我们可能会看到今年会有万亿的呃，那个重密模型，那另外更多的模态。

尤其在更多的模态的带动下，才，其实我们的sequence lds，会由之前的几千token，步入到已经步入到几百token，甚至是几百万，投几几百k token，甚至到几百万token这样的一个量级。

这些token的LDS，实际上也是对我们的系统带来很多的挑战。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/68f6dd4b30996720c4ffdb3d63893e48_1.png)

那此外数据无疑也是呃增大了很多的数据量，无论从语言模型拉玛，从去年的la2到拉A3，而今年我们看到大量的大模型，要考虑它们在多模态，尤其是视频生成上的这些，无疑使得我们的训练数据集，又是翻了好几个量级。

这些东西都是给我们专利带来更多的需，求和挑战，那另外一方面，其实我们并不觉得大模型的算法已经汇聚了，去年，虽然我们在语言模型上，好多都是走GPT2这样一个路线，但是随着这个模态的多样性，还有模型结构。

大家更加大胆的去尝试不同的这种结构，因此我们看到，其实算法已经在今年步入到了一个，百花齐放的一个情况，那意味着我们底层的算子，其实对需求是更加变换的更多，我们又没有能力去catch。

这么快的一个算子的变化，放到我们的系统，尤其是这么多的不同的架构，所以综合我们看到是说，这个是我们看到的整个在AI系统的挑战，从底往上啊，依然我们是离不开我们芯片架构的创新啊，那芯片架构是否有未来。

还有新的芯片架构的一个出现，或在已有的一些新的指令集上面的一个拓展，倘若我们已经有了很多的当前，当前我们已经有了很多的芯片的选择，更多的未来的架构，那我们从编译器层面我们怎么去解决啊，不同的架构。

包括指令集的变化，那再往上，我们作为这个越来越大规模的这个集群，大家一起要高性能的去训模型或推理模型，那当我们面对这些更大的架构的时候，或者是说它怎么去优化，另外当我们面对不同的算力架构。

这些异构算力怎么样子，能够被我们的这样子的一个并行的优优化，并行的这些框架统一起来，那再往上大家也看到了，大家今年都在谈万卡集群，万P集群啊，几万卡啊，都要在一个集群内进行高性能的训练，可能同一个模型。

那我们整个组网方案，其实对网络技术发展了那么多年的网络技术，又有新的挑战，所以这个是这个图，我是希望用它来涵盖我们今天众多的奖者啊，我们今天应该有八个还是九个topic，从底到上涵盖所有这些topic。

所以嗯也呃很有幸呃，请到了相关各个领域的专家，来给我们分享他们的想法，面对这些挑战的想法，以及他们最新的一些成果，更多的是他们对未来的一个思考，呃今天的我们的topic我在说明这个安排嗯，不分先后。

主要就是我们希望把呃，从底到上给大家重做一个呃捋清，那希望大家能够呃尽情的享受，今天上午带来的这个整一个，我们在AI系统各个领域的一个饕餮盛宴。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/68f6dd4b30996720c4ffdb3d63893e48_3.png)

# 2024北京智源大会-AI系统 - P10：网络驱动的大规模AI训练- 阿里云可预期网络HPN7 - 智源社区 - BV1DS411w7EG

好感谢谢谢林老师的介绍啊，那个嗯还是感谢大家再稍微的再坚持一会啊，我们我我应该可以按时这个结束，然后再开始午饭，前面的几位老师其实从啊编译从模型框架，然后从啊从这个软件站啊，系统。

以及从从呃并行的这个策略上都做了一些啊，这个在AI方面，AI系统方面的一些工作的介绍，那我呢介绍的是网络的部分，其实就是把前面几位老师的工作呢，我们怎么通过网络把它连起来，把连把它连成一张啊。

刚才林老师也说过，我们要连到一张上万张卡的一张一个大集群啊，让这张大集群呢去做一个作业，他去完成一件事情，然后呢呃呃就是万卡集群呢，我们在HPN7。0上，其实从去年9月份已经落地了。

然后接下来呢在啊我的分享里面，我也会探讨我们啊接下来的10万卡啊，更大规模的一个一个分享，而一开始的时候，那个袁老师也是非常呃呃呃，就是让我很很很很很很很那个很感谢，就是他说阿里这个机会其实非常好。

那其实阿里不光是有办卡集群，我们在整个模型的系统，整个AI系统里面的，这个服务的能力也是非常好的，所以欢迎大家在阿里云上去做我们的科研啊，做我们的呃工程和创业的工作好嗯，那我我今天的内容呢。

其实希望通过啊今天的一个介绍呢，啊让大家了解到啊，从网络的视角，从集群的视角啊，当我们把很多的GPU连到一起的时候，那其实它最关键的网络的工作是什么，网络为什么在这个里面非常的重要。

以及说他遇到的核心的挑战和问题是什么，阿里云的H型7。0，是怎么去解决这个问题的啊，希望可以回答呃，回答大家这些问题，其实我们现在呢整个数据中心啊，已经从CPU的啊，这个分布式的系统已经演进到以AI。

以GPU为中心的这么一个数据中心了，其实这个数据中心的演进的变化呢，对啊网络产生了非常大的挑战，不光是网络啊，整个数据中心的基础设施，从呃制冷水啊，电啊电的供应，那整个机房的设计啊。

都是一个颠覆性的一个变化，那其实网络我这简单回顾一下网络，从数据中心的网络，从第一个10年，也就是说从大概2000年开始，互联网开始的时候，我们用我们的电脑去啊，上上聊天室啊，去做呃，去上论坛啊。

那个时候一直到一直到我们第二个阶段，就是从10年开始左右，我们的云计算的这个发展起来，那网络的规模，整个数据中心的设计，也是发生了翻天覆地的变化，从一开始的可能上千台机器的规模。

这种client server的这种服务的模式变成了大数据啊，然后呢呃集群化的这个存储的系统等等，这样一个云计算的这么一个一个平台，那这个平台其实对网络已经对，对数据中心网络的挑战已经很多。

我们从TCP引进了RDMA，通过REMH加速这个网络集群的这个互联，那再到今天，我们其实已经到了这个AI这个阶段啊，刚才几位老师都已经提到了，其实AI它是一个最终我们在训练的阶段，其实它是一个大的系统。

这个大的系统是，它不是像原来class server这样一个模型啊，它是一个并行的，非常多的万卡级别的GPU去完成同一个工作，他真的是真正意义上的一个data center。

as a computer的这么一个结构，所以呢，这个里面对我们产生非常多的一些挑战啊，我们可以看下就是传统数据中心，其其实在AI的这个大模型的场景下，已经已经不适用了，我们原来的设计已经不适用了。

这就是为什么我们要去做万卡的，HPN的7。0，首先从呃从这个模型上来讲，已经是一个非常大的一个差别，原来我们都是can i server的一个一个服务的模型啊，这个服务模型下呢它的呃我们的GPU的机器。

这个机器本身它就是一张网卡出来的啊，通过CPU，然后PCIE外设，然后再到网卡出来，它的互联是非常简单的，然后我们也看到说，因为我们有非常多的这个云上的业务，我们看到他的流量其实是持续的流量。

它的流的数量是非常多，也就是它的entropy其实是非常大的，在这样一个条件下呢，我们再看一下AI，到今天的这个万卡集群的时候啊，它的呃跟传统的计算都有哪些差别，我这列的这三点其实非常关键。

第一个就是说它其实多机在运行同一个任务，这个里面的长尾的效应，木桶短板的效应，你坏一张卡，换一个网络节点，它一一个网络的连接如果不通了，最终你整个任务都停掉了，这跟原来的这个模式是非常不一样的。

第二是说呃我们现在的这个GPU的计算，它卡内其实都是这种多机的互联啊，这种互联呢跟外面的通过以太网，我们去把它连成万卡的这个互联，这两张网络的互联之间，其实要做一个非常好的协同。

你才能把它的效率能发挥出来啊，呃然后最后呢我们看到说在AI计算场景下啊，网络上他用的连接的数量其实非常少，如果你看到一张卡，它出来的连接出来的话，它在100以内，但是你原来一一台CPU的，几。

一个CPU出来的连接数量，可能是几10万甚至百万级别的这个连接数量，所以它对于网络的这个entropy的这个影响。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_1.png)

会导致我们网络上在哈希上会出现问题，这个时候我在后面要去要去介绍的一个点，那网络在整个集群算力里面，它它的作用是什么啊，它体现的关键的价值是什么，这就是我用这张图里面大概来解释一下。

其实网络是集群算力做skyline的一个核心啊，为什么呢，因为我们现在大家关心的是，在从训练上的角度来讲，我们关心的是是训练的时间，因为训练时间是大家创业，然后跟竞争对手去拉开差距的。

去一次一次迭代的一个非常关键的因素呃，一个非常关键的一个因子，那这个时间其实是计算量除以你的算力，然后再加上你中间的一个中断时间，那中段时间就不讲了，那算力的规模越大，理论上它所消耗的啊时间会越短。

但是算力的规模增大了之后，网络这我们看到网络这你两台机器的啊，同步的通用量和1万台机器同步的通信量，完全是不一样的，所以呢网络在中间所等待的时间，让计算所等待的时间会随着规模的变大而变长。

那这个变长的结果会使得我们总总的规模越大，之后，它的算力理论算力的下降啊，会比较明显啊，这个我们叫它算力的线性比，也就网络上我们做集群网络，我今天讲的内容的核心，也都说我们怎么把这个线性比，同从啊。

几百张卡到1000张卡到1万张卡的这个过程中，让算力能保持线性的去增长啊，这个是我们呃一方面是省成本，另一方面我们节约时间，它是算力的一个发挥，所以说在这个里面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_3.png)

我们的我的一种说法，就是网络的性能及集群的算力啊，我们再去看一下，如果说这样一个高性能网络系统的话，我们有哪些关键的部分啊，那第一是说这个集群的架构，我们要有这样一个集群的架构，去把这个万卡的连起来啊。

它不是简单，它不是简简单单连起来，我后面会讲会讲为什么，第二是说哎你是需要一个非常高效的协议，去端到端的，就像一个跑车一样，你断不断的在路上去开，开的时候，你这个车要快，然后你的呃这个调度系统也好啊。

然后第三还有我们的运维监控系统呃，你你不能说你建了一张网络跑不起来，或者说跑跑2分钟它断了啊啊断了，你也不知道它为什么断了，这个也是我们现在在AI系统里面，非常核心的一些技术，其实就是做AI呃。

性能的profiling，然后做呃性能的优化，以及说出现问题之后，我怎么去快速的找到它，然后快速的把有问题的点给拿掉，让任务去继续运行，所以说这三个点是整个II系统的高性能，网络系统的一些关键的要求。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_5.png)

那这些关键的要求有这四个方面的挑战啊，有这四个方面的挑战，第一个是说，我们要有适合的一个集群的网络架构，要去承载万卡和10万卡的这样一个呃算力，那为什么我们需要一个适合的架构呢。

比如说北京大家在上班的时候，如果所有人都来海淀区上班，你北京的路修的再好，你都会用色啊，这个道理很简单，所以呢我们把路修宽，把路修好之后，还要把路修的要适合这么大的规模，然后能让这么大的规模的这个算力。

在放在一起的时候，它能发挥的更好，所以这个集群架构的设计是它最基础的部分，然后呢，你在网络上的这个均匀性是第二个关键点，这个也是我们目前在很多领域里面，去解决这些问题的，呃。

一个呃一个一个就是很多方向上，在网络上在解决的一个关键问题啊，还有in cast问题，你都会遇到，说大家会撞到一起，那撞到一起之后，你用什么样的算法让他去退避啊，怎么样做到全局的最优零拷贝和DMA啊。

这些呢是我们在呃rocket的RDA，包括像IB其实都是在做呃这个呃，呃这样高效的一个传输的一个工作，那总共有这四个方面的挑战，那阿里云的IPPN7。0，是怎么在这四个方面上去针对。

AI集群去设计一个万卡甚至10万卡的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_7.png)

更大规模的一个系统呢，好我来介绍一下hp7。0呢，它我们有几个设计的特点，第一是说它支持的规模可以到，就通过两层的cloth的这样一个，fabric的一个结构，可以至少万卡的规模啊。

具体呢是1万6000卡啊，我们现在已经上线了多个集群嗯，它是基于51。2T的这个以太网交换机，去实现的，有几个关键的点在这个里面，第一个是说啊，它是呃一个呃双上连加双平面的一个设计啊。

这个我刚才呼应了我刚才讲到的一个点，就是AI系统，如果出现啊link的问题，如果出现单节点的问题之后，双上联和双平面可以让它无感知的继续去运行，然后我就在日常的运用过程中把它给换掉，就可以了，第二个点。

我的千卡的segment，这个千卡segment1说，我通过多轨的互联做到1000卡的范围内呢，它没有不会有任何的网络用色啊，这1000卡它所发挥出来的千卡的啊。

这个网络的带宽的呃性能是它的理论的呃极限，所以说呃我们这个体验卡的这个性能的结果，我在后面也会有有介绍两层万卡，两层做了万卡之后，对整个万卡的系统也是非常关键，因为两层的网络交换。

它的网络的跳数就只有两跳，如果是三层网络的交换的话，它跳数是五跳啊，这个大家都能理解，那五跳和两跳相比呢，两跳它在网络路径的简化上，在网络的实验的简化上都有非常大的优势，那我们通过这样的方式去做到。

单个集群可以覆盖呃，呃1万6000卡，那将来可以覆盖更多的啊，这个10万卡级别的这个设计，其实我们已经有了，然后内部也已经做了理论上的一些论证和仿真，然后可能在明年会有这个10万卡的呃，集群会有部署啊。

嗯当然基于这个呢，我们还有我们自研的rock v two的RDMA哈，自研的HPC流控里面去解决一些细节问题，因为时间的原因我可能没关呃，就是没办法去介绍这么细节啊，我们会在那个大概在下个月的时候。

我们的呃公开的论文就会发表啊，到时候大家可以从论文里面去看到，我们实践里面的一些细节，那从呃从hp n7。0的整个税上来讲呢，刚才几位老师也讲到了，我们在并行的这个策略上，其实并行模型整的计算怎么去做。

并行网络怎么去做并行，最终还是要映射到网络的连接上，网络的拓扑上，这个拓扑和并行策略的关联，针对B型策略的网络架构的设计，其实是非常关键的，那呃这个我就不展开了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_9.png)

那通过通过HPN7。0的呃，在这些方面的优化的一些工作啊，这些一些针对性的设计呢，我们能做到通信模型，就集合通信的这个性能提升在一倍啊，这个是我们呃呃就是单纯去跑集合通信的时候。

跟呃之前的就是通用的这个集群架构相比，那整个模型的端到端的性能提升在10%，那这这百10%呢，我们是一个呃是在呃deep speed的这个框架下去跑呃，拉玛13B的这个模型得到的一个结果啊。

呃所以它的呃从我刚才讲到的，其实从呃这个千卡segment，以及说万卡的这个两层close这个结构下，简化的网络针对性的设计，以及400G的RDM，我们其实单机是3。2T啊，单机3。2T的RDM啊。

以及属我们自研的通信库和啊自研流控算法，这些东西结合到一起，我们能做到它的极致的网络性能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_11.png)

然后整个额就是阿里的hp7。0呢，其实是针对是针对呃，呃这个AI系统我们做了全系统的自研呃，就是我我刚才讲到了，其实呃硬件啊，然后呃从从架构拓扑啊，从啊通讯库啊，从RDMA等等这些角度呢。

从网络通信的角度呢，我们呃做了我们自己的设计和自己的优化，那除此之外呢就是构建这么一个系统，还有两个比较关键的因素，我们要去做网络设备的自研，因为只有自研之后，你才能把里面啊芯片里面。

然后通信信号上的呃，一些问题才能掌握在自己手里，然后整个架构里面你能做到的，调优的结果才是最优的，那整个系统里面，就是我们我们做的这个模块化的，硬件的设计标准，以及说整个自研的呃128个端口的。

400G的这样一个网络的设备啊，这样一个硬硬件的一个一个一个网络设备，都是我们自研的，包括在光模块上，我们也基于阿里的专利去做到了啊，这个400G的光模块QST112啊。

这个是我们呃在光模块上的一个标准，那其实整个硬件和光互联的这个系统啊，在整个呃IHP7。0也呃，下面呢也发挥了非常重要的一个作用，也就是说它的稳定性啊，它的互联信号的质量，然后它的对于性能上。

我们能掌控的一些呃细节啊，都通过自研的方式，全自研的系统啊去做到了啊，把系统的呃这个能力发挥到最极致。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_13.png)

好，那我在呃后面再花点时间去展望一下，我们这个整个II基础设施的一个啊未来啊，就我们啊阿里内部呢，其实从各个系统上也在呃探讨，我们下一代的IIAI的info。

不管是明年还是啊3年之后的这个AI info，它是应该是什么，目前呢我们呃就是能看得到的是说，从啊CPUCENTRIC到GPUCENTRIC这个过程中，一定会发生，从电力到机房设计和到网络互联系统整。

以及到GGPU内部的互联系统，整个全系统的一个AI基础设施和AI系统，硬件层面的一个变化，那首先就是我们现在的这个呃，单个机柜上可能是20千瓦的这个机柜，我们也是风冷的机柜，然后呢有CPU的服务器。

可能放放个20台左右，那这样一个机房其实已经不再适应了，因为我们接下来呢啊通过GPU卡本身，它自己的功耗很大，然后我们希望把它集中的放到一个，短距离的范围内去，让它的数据交换的能力更强，之后呢。

它的算力集群的，它单机的这个系统的算力才能发挥的更好，所以说我们一定会设计一个叫i i rec啊，这个那个NV在他的呃，在他的这个呃GDC上，其实已经发布了NML72，其实很多系统。

包括我们国内的系统也在做64的啊，64卡的系统，那这样一个系统在这样一个WRC里面，对RC的呃制冷啊，对RAC的功耗的挑战啊，都是非常大的，那其中跟网络相关的有一个非常关键点。

就是说呃这个呃scale up的这张网络已经发生了，跟今天已经发生了很大不一样，我刚才也讲了，就说AI系统，为什么跟传统的计算计算系统不一样，因为AI系统它当前我们拿到的这八张卡。

它内部还有一个超大带宽的互联，内部是是3。2T的互联，但是呢一张卡出来，它可能是400G200G的互联，这样一有一个九倍以上的这样一个，带宽的一个差距，那我们怎么去把这个内部一个这样一个。

scale up的网络跟外部去结合好，然后这个SKYUP的网络呢它也会越长越大啊，像NV他已经做到了啊，72张卡的互联，那其实我们呃国内的GPU厂家，未来也会往这个方向上去走，那所以在这个里面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_15.png)

我们带来的网络上带来的挑战，就是怎么样把scale up和scale out的网络融合起来，像视频7。01样，我们把不同的模型切割和训练过程中的pattern，流量，pattern的这个信息做结合。

与网络的设计，与网络的RDNA与网络的流控设计，放到一起啊，这个其实是一个非常非常网络，在未来非常关键的一个命题呀，那是SKUP网络啊，我就是我，我我这可能稍微解释一下什么是是这个scale up。

就是在我看来呢，对up的这个定义，就是说它能它能做系统，系统内部的一个characal currency，然后可以做到极致的大带宽啊，它因为它因为它距离短，它的成本距离短，可以使得它的成本可以做的低。

在这个框架下，它可以把带宽可以做的足够大啊，那这样一个sky up的一个系统和SKYOUT相结合，那sky out是什么，就是我们嗯有由1000张卡变成1万张卡，我们通过以太网或者通过IB把它互联起来。

连到更大规模，那就是这样一个结合的设计，是未来的一个啊一个命题，然后我们内部也在呃针对嗯更大的scale up，到64~72之后呢，跟scan out怎么去结合，去做我们下一代的这个呃网络架构的设计。

然后呢，再到我们面向更加灵活的这个模型的流量啊，刚才很多老师都已经讲过了，我们可能会面临MOE啊，跟我们今天的all reduce和reduce scatter呃，这这些操作可能会不一样。

会有auto all等等，会有其他模型的，这个流量的pattern可能也会出来，包括多机的推理啊，多基的推理刚才提到了呃，Kv catch，那kv catch在多机的场景下。

也是需要网络去提供很大的带宽，但是这个带宽要大到什么程度啊，它的它的呃需要网络在呃，RDMA在流控上需要有什么样的新的能力啊，这都是在未来可能发生的，这也不是很短的。

未来就就也也也不会是很很很很长的一个未来，可可能在明年这件事情就就会发生，所以能在未来呃，这几件事情会使我们在I压系统设计里面，要去重点去考虑的啊，然后那看到的未来的这个网络上的。

我们能能呃需要的这个算力级别，我们需要的这个呃需求呢就是10万级别啊，10万甚至更大的这样一个算力的集群规模，包括100T的交换网络的啊，这个系统以及新的新型的硬件等等，这些是网络上我们要去做的工作。

那10万这个级别的设计，我刚才已经讲了，其实我们内部已经已经有啊，已经有这个仿真和模拟的一个结果，我们以及一些详细设计也已经有了，但是呃因为呃现在还没有还没有这么大的店啊，还没有这么大的一个数据中心。

所以说10万的这样一个呃呃一个单机群呢，我们还没有部署起来，但是相信在不久的将来呃，我们就会呃遇到这个，然后呢在网络上我们也的也也看到其实业界啊，网络业界对于呃将来AI基础设施呢也成立了。

也也也成立了两个关键的这个联盟组织啊，UEC嗯，Ultro net consultant，以及说u a link啊，ultra accelerator的这样一个两个组织。

分别在scale up和scale out这两个方向上，希望通过呃全球的生态的力量啊，把网络上这两个方向的大的命题呢，做到呃更好的大家的标准的统一啊，做到那个能力的统一，以及最好的这些想法和网络技术。

在这两个方向上去做发挥，所以呢呃那个阿里也是在呃呃这个UEC里面，我们现在是这个tech的member，也就是说在UEC的这个呃决策委员会里面，是国内的唯一啊，国内的唯一家公司参与在里面的。

就跟跟北美的那个微软啊，然后呃那个meta还有BROADCOM等公司啊，去呃去在这个组织里面去，把网络上将来要去面临的问题，II场景下要去面临的呃，系统性的问题去做解决。

然后去呃做这个为将来的更大规模的10万卡，更大规模的这个AI基础设施的系统呢，去做更好的网络能力，嗯好我的内容就这些好。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/dfc1f80c6a1ee40dcd27791a6a1cf801_17.png)

谢谢大家好。

# 2024北京智源大会-AI系统 - P2：Unlocking Al Potential：David Edelsohn - 智源社区 - BV1DS411w7EG

非常感谢，谢谢Yanga精彩的介绍，你在IBM的时候能和你一起工作真是太棒了，就像你说的，这次演讲的目的是探讨连接的挑战，AI模型的最新开源框架，以及它需要运行的各种硬件加速器，以及如何建立这种联系。

并提供一个有效的生态系统，在此基础上，我们可以利用所有这些伟大的模型，各种各样的体系结构硬件加速器，就像小阿说的，我叫大卫•埃德森，我是开放开放生态系统的高级技术人员，她说我参与了GNU编译器。

仍然领导着海合会社区，它是Linux开源软件生态系统的基础，最近进化到人工智能与Numpy合作的生态系统，打开BLA一DNN，现在来看看各种各样的硬件挑战，在这次谈话中，我再过一遍。

人工智能框架中硬件加速器的概述，了解这个软件生态系统面临的挑战，支持硬件，如何在生态系统中导航，还有一点关于未来技术的讨论，新兴技术和未来方向，所以首先我想从这个动机开始，有大量的开源模型。

想要在各种不同类型的硬件上运行的人工智能模型，所以从生成的模型类型，爱，自然语言处理，自动驾驶，无论是在培训中还是在汽车欺诈检测中的部署中，机器学习，计算强度较低，但周围有很多要求，并发性。

潜伏期的能力，处理大量计算的吞吐量能力，一直到移动和可穿戴，又有不同的要求，无论是外形因素还是这些人工智能模型将要运行的设备，在延迟功率要求中，所以人工智能有各种各样的模型和用途。

在加速器上运行这些模型的能力，因此，挑战是如何从这些不同类型的框架中找到一条道路，包括桨，划桨，Jax，张量流，在那里这些模型被开发出来，以便能够在如此巨大的范围内部署这些模型，不同范围的目的，用途。

建筑，这些模型将在其上运行的硬件体系结构，所以只需谈谈硬件加速器的类型，一个有GPU，大家都很熟悉，不管是英伟达还是AMD，从图形处理器点开始，现在被用于这些类型的机器学习和人工智能模型。

它们具有非常高的吞吐量，流媒体，并行性，在这些计算密集的计算中非常有效，在更专业的处理器上也有工作，比如谷歌使用收缩阵列的TPU，在特定的张量操作中更有效，非常擅长大规模训练，有npus，它们在。

该名称用于较小设备上的NP类型，或者在数据中心也可以称为TPU，所以这是从微软、元和IBM的特殊用途，AI芯片，到苹果的设备类型，Mac或任何类型的PC，FPGAS在研究和原型制作方面非常有效。

因为它们的灵活性，可重构性，有利于理解，不同类型计算密集型模型的具体用例，Dsps，它们功率很低，用途很特殊，但在功率和尺寸有严格限制的设备中很有用，六分是最好的表现，但非常适用于特定的用例。

但都是非常有限和专业化的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_1.png)

所以又有大量来自英伟达的人工智能硬件加速器，AMD，百度，昆仑，华为，上升，呃，对于大量不同类型的模型，不同类型的专门用途，这是一个非常大的生态系统，这需要专门的代码和对它们中的每一个进行专门的调优。

挑战是如何利用这些模型，它们是用一小套框架开发的，这些框架非常有效地定义，尤其是这些最新的大型和大型语言模型，能够将这些映射到所有这些不同类型的硬件上，元有它的MTA，芯片，微软，玛雅芯片。

IBM和AU亚马逊，Infoa，这是一些尖端的，大脑晶片刻度引擎还是岩石张量流处理器，所有这些都在探索硬件设计中不同权衡的整个空间，但希望能够运行相同的主要型号，比如聊天，英镑或美洲驼。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_3.png)

这些有趣的模型中的任何一个，你知道不是最新的数据，但你可以看到，在整个市场份额的更广阔的空间里，又有大量的甚至又分入，其中一些是用于更可穿戴的设备，个人电脑，其中一些在数据中心。

但需要支持的硬件范围非常广泛，对于这些有趣的模型、机器学习模型和GPU空间中的AI模型，更狭隘的是，英伟达显然占据了主导地位，但又一次，这只是GPU，这不在数据中心，不是TPU，或者其他类型的呃。

NPU处理器，如MAYA或其他芯片，但现在，AMD和英特尔正越来越多地关注中国的市场份额。作为一个整体仍然主导着这个市场，尤其是在高端训练空间，所以大多数编程都是在商品环境中完成的。

人们现在正试图将其扩展到不同类型的处理芯片。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_5.png)

所以人工智能框架，还有一个不同的框架，它们都有不同的好处和权衡，Pytorch因其灵活性而非常有用，易用性，易于编写模型并对其进行调整，探索它们，张量流，它存在的时间比，最初有很大的领先优势。

准备好生产了吗，非常健壮，非常适合在生产中部署，jax是google开发的另一个框架，与他们的xa编译器紧密相连，我一会儿就会谈到，非常适合数值计算，又一次，与张量流不同的权衡，当然啦。

来自百度的桨桨与它的焦点，从超缩放到移动玛瑙，这并不完全是一个框架，但我在这里作为另一种语言，那个能把模型写进去，对运算符的定义和模型之间的交换非常有效，这是一个好处也是一个挑战。

然后是许多其他不同的框架，随着时间的推移，它们已经被开发出来，仍然有很好的用途和专门的用例，在很多地方都得到了有效的应用，所以这里有一个非常广泛的框架，为了争夺市场份额，有不同的好处和不同的权衡。

所以这些框架连接到硬件的方式，是通过一系列的层次，包括一些库，如pytorch中的lib ten和sleve库，使用他们开放的BLA一DNN，所以一系列的图书馆都集中在，其中一些比较笼统。

其中一些更侧重于一个框架，一些更专注于硬件的，嗯，我把它们放在一起，是内核语言，Cuda和臀部，NVIDIA和AMD作为嵌入式域的语言是什么，用于编写低级内核的特定语言，呃，Sickle是一种语言。

现在是UXL的一部分，伴随着呃，Foundation提供了可以跨多个硬件架构工作的相同类型的嵌入式DSL，特里顿是个新人，一个基于Python的非常有效的，呃，表达这些内核的更高级的方法。

目前专注于创建GPU内核，但是，我稍后会提到，一直在向其他领域拓展，各有利弊，还有那些果仁，嗯手写的，并结合了针对硬件优化的库，与这些不同的人工智能编译器基础设施一起，如Pytorch中的发电机电感器。

Xa和张量流，还有Jaxx，怪异的MLR基础编译器基础设施试图提供，和和解决方案仍然是初步的，以及拨桨框架中的拨桨和CNN基础设施，以及所有这些编译器是如何组合在一起的，的，模型设计。

以及这些库的执行用途，提供一个在不同形式的硬件上运行的优化环境。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_7.png)

现在，这些框架对模型有专门的要求，而不仅仅是一般的编译，包括张量，张量的动态形状，张量稀疏性，再次在模型中进行量化和多精度的精度变化以获得最佳性能，最佳内存使用，同样。

从数据中心到特殊嵌入式环境类型的能力，所以理想情况下，人们希望有这种简化的图表，你拿一个用这些框架中的一个编写的模型，或者通过Onyx翻译的模型，你把它放进这朵奇妙的云里，这蓬松的云。

它神奇地变成了一个可执行文件，可以运行和利用这些不同硬件的硬件，但当然，现实比把东西扔进云里得到神奇的回应要复杂得多，这个显示xl a体系结构的示例，呃，它主要关注tensorflow和jax。

但也有能力进口圆周率火炬嗯，你开始看到这两个目标的复杂性，独立优化和大量目标，这些不同的框架和编译器基础设施的问题，模型本身还需要进行大量的调优，以及编译器基础结构，从这些不同的硬件中获得理想的性能。

所以可以说，哎呦，我们将只对所有内容使用tensorflow或xl a，但是把一个模型从pytorch，或者划桨进入XL A，模型本身有很多关于性能最佳使用的信息，这需要大量的调谐，所以这里没那么简单。

从手臂的不同角度来看，体系结构只是一个单一的体系结构，有许多框架中的两个在上面工作，我可以开始看到不同路径的复杂性，如果使用张量流，一种是使用特征或一个DNN库，如果一个人用圆周率火炬。

一个是使用开放爆炸或一个DNN库，编译器基础结构，不同的编译器和不同的计算库，因此，框架和它们的编译器基础结构都有一个困难，尝试支持不同的体系结构，对于开发人员希望在更细粒度的级别上使用的任何方法。

你可以看到左边是桨桨灯建筑，右边是诡异的试图再次支持，所有这些潜在的框架和所有这些潜在的硬件，它变得不详细地经历这一点，但试图利用所有不同的路径是一个非常复杂的场景。

并获得具有这两种体系结构中任何一种的模型的最佳性能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_9.png)

尽管他们在这方面取得了很大的进步，所以再一次，生态系统的挑战在于有一个如此复杂的框架，笔译员，编译器，量化，所有这些不同的硬件，编写这些模型的不同方式也很复杂，他们的图书馆，从汇编语言，也就是。

你知道的，绝对气馁，虽然它得到了，当然啦，可以获得很好的性能，C++模板，创建asms的嵌入式dsls，例如Cuda Python创建ASM，所以有很多很多不同的形式和很多很多不同的成分可以结合在一起。

但是这种复杂的组合很难保持可维护性，用于调试和工具，所以这是一个挑战，人工智能社区正面临着从这些架构中获得最佳性能的挑战，又有碎片了，如果一个人从硬件第一的角度来处理这个问题。

创建一个为硬件设计的完整框架，当然，这可能会得到最好的性能，但它是不可伸缩的，特别是对于我在几张幻灯片前提到的所有硬件架构，所以这并不实际，我们将从硬件开始，说这是不实际的，我们将从模型开始。

人工智能空间，计算是端到端的，并行计算问题，不仅仅是马尔，不仅仅是，我们将创建这个maal库，并解决所有问题，所以另一个挑战是互操作性，框架，图书馆，用于尝试创建与这些不同框架交互的库的apis。

Pytorch有痒它的API桨，因为它嗯，在自身模型上有一些互操作性转换，比如玛瑙和x两个桨，但这两个都有局限性，即模型目前需要像我提到的那样，专门定义或调优以获得整个系统的最佳性能，呃。

正在使用的工具链基础设施，所以说，从我与组织的对话中，谈谈这种发展是如何演变的，站在部署模型的最前沿，目前他们使用的是大约70%的手写内核，对于最关键的路径和最关键的内核，他们能够利用看到他们在叫什么。

果仁的长尾巴，在那里，它总共占总内核的30%，20%到30%的人能够使用参数化工具，我的意思是，像海卫一这样的工具，然后大约5%左右的编译器，意图是理想的计划，目标是能够转移越来越多的高性能开发工作。

到这些高级语言，到参数化核，到管道，更少的依赖于人类手写代码。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_11.png)

所以说到海卫一，海卫一说过优点和缺点，他们的海卫一啊有能力用作编程，一种手写目标语言，内核的高级描述，在这方面非常有效，它现在也开始被用作编译器目标，所以它既是人类手写的书面语言，和编译器目标语言。

那个，双筒望远镜和电感器还有杰克的宫殿都瞄准了，你知道的主要路径，最常见的是我所说的Triton GPU，最初是为cuda和nvidia设计的，现在越来越能够直接支持ROM，不通过翻译。

但是在Triton GPU中为ROM提供直接支持，微软和Meta也在做一些工作，在一个名为Triton共享的开源项目中，能够利用Triton语言来支持他们的专用硬件。

通过一个在linalor lin alge连接的mlr通道，不同发音的人通过，挑战是这个琳娜的媒体，内核中描述的信息有点太低了，TPU和NPU硬件的级别，它真的想看到模型的子图或完整图，以获得最佳性能。

能够对更大一部分的计算进行操作，而内核对Gpus非常非常有效，所以信息的丢失，另一方面，MLI，R和linodialects一直很脆弱，就API而言，因此。

Triton实际上在提供更好的稳定API方面很有用，编译器（如pytorch感器）可以针对它生成代码，因此，关于如何扩展海卫一，还有很多工作要做，或者其他接近MLR的方法。

以在更广泛的硬件上获得全面的性能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_13.png)

所以再一次在生态系统中导航，基本上你知道，这个，又是这个子集，不同类型的框架具有不同的优势和不同的优势，不同的编程模型，不同社区，不同的定制，不同的支持库，这是一个挑战，你知道的。

尝试将版权框架与正确的硬件相匹配，以获得最佳性能和其他限制或要求，硬件与公司。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_15.png)

公司的政策，所以基本上优化一个模型，你有一个组合框架的选择，张量优化，核仁，混合精度，参数化，该装置，具体优化，目标优化，尤其是在嵌入式可穿戴领域，关于内存布局规划的关注，选择性连接和包装。

以确保这适合设备，嗯，所以说，基本上，这是非常重要的，无论一个人是试图创造一个炒饭菜，或者试图创建人工智能应用程序，把所有的食材和一位大厨，把这些都组合在一起，一个人可以得到一个漂亮的炒饭菜。

或者可以得到类似于清洁应用程序的东西，呃，在视频中被认为是如此有效，生成人工智能，所以你可以得到惊人的，呃，用合适的厨师和合适的食材烹饪结果，因此，生态系统的女巫列表基本上是硬件通用性。

所有这些不同的硬件都有一个单一的真理来源，那个不需要专门，能够参数化硬件的不同特性，不必在程序集中写入，希望不是c+，能够使用更灵活的语言，能够支持这些动态形状，所以在新兴技术中，我之前提到过。

MLA的怪异之处在于，围绕这一点创建更多公共基础设施的潜在机会，创建了他们的魔力编译器，他们正在慢慢推出，希望有一些，呃，再次受益，在更长远的未来使用MLR基础设施。

在量子计算和量子变压器方面有有趣的机会，它可能有很大的机会支持图像和语音的海量数据集，或高维嵌入，对神经形态计算也很感兴趣，它试图利用与人脑结构和功能相匹配的计算基础设施。

为了在模型中获得更多的这些功能，人工智能可以利用这两者来实现平行，容错性，其他类型的人脑特征，这就是将来的一些话题，所以再一次，你知道的，回顾这个简短的介绍，人工智能框架环境有很多多样性。

这两个用例都有很大的范围，在非常受限的环境中生成人工智能和计算密集型，硬件延迟，吞吐量，加速器硬件，框架和挑战，一个很好的机会是每个人都在学习越来越多关于工具的体系结构，基本上收敛于正确的优化集。

以及利用这些工具的方法，以及如何应用它们从工具本身获得最佳性能，当模特越来越多，推理机变得越来越强大，并学习如何在模型本身的设计中获得更多的优化，因此，它们正在向更多的互操作性靠拢。

希望更多的公共基础设施，可以在这些不同工具之间共享的公共API。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/747f4d76ea498508404f0ca228d20a1a_17.png)

所以再一次，呃，这是一个非常激动人心的时刻，而且有很好的机会可以跨硬件供应商进行合作，框架开发人员，的，库开发人员和模型开发人员，应用程序开发人员共同努力，共同开发这个生态系统的未来。

最好以公开的方式做这件事，能够从所有这些不同的伟大的头脑中获得创新，世界上的思想领袖，在世界各地，那里有令人难以置信的创新和探索，所以有了这个，非常感谢，我只想提一下和谈话没有具体关系。

但我正在做的另一个项目，称为企业神经系统，它正在与联合国合作进行人工智能创新大挑战，并希望尽可能广泛地传播这一点，我们正在寻找全世界的参与，所以如果有人在研究气候变化，还有爱啊，我们向各地征集建议，请。

请帮助沟通，我们正在寻找每个人的参与，所以有了这个。

# 2024北京智源大会-AI系统 - P3：Al系统领域还有哪些比较重要问题？-袁进辉 - 智源社区 - BV1DS411w7EG

嗯谢谢林院长的介绍，呃这个题目确实非常大，然后我呃是想想提前说一下，就是我是来提问题的，不是给答案的，所以大家可能让等待答案的这个会有所失望啊，然后为什么起这个题目呢，这个我解释一下那个它的缘起。

因为今年3月份的时候，不是英伟达在硅谷开那个GTC大会吗，有非常多的同行，国内外美国的都去，我也去参加了，然后就和呃很多我们从事这个方向的同行，都在最前沿的专家呃交流聊天。

然后呢就会其实就会遇到一个同样的，大家都会关心同样的问题，就是嗯好像这个外界的环境和大家的研究热点，发生了非常大的变化，然后我们下一步就是作为我们这个community。

或者我们在做这个AICM工作的同事，还有什么重要的问题需要我们去解决呃，这听上去就好像说哎，是不是没有什么重要的问题了，还是还有另外的就是说应该说在大模型之前呢，呃应该说做AI system。

我觉得它的significance它的重要性呃，应该至少是和搞算法和模型训练研究的，是一样重要的，在当时那个位置啊，或者说那种影响力，但是当那个大模型起来之后呢。

好像大家都去关心关心那个model的问题了，然后那个这个搞AI system的，隐隐有点像变成二等公民的感觉哈，这也是一个呃，然后就是大家普遍在讨论，就是说我们搞AI system的还有什么样的点。

我们可以做出非常就说我们做出一点工作之后，做出一方面一个工作之后，就能成为在整个行业有非常大影响力的是吧，这是我们每个人所追求的就是Impact，所以这是这个问题的由来，但是很不幸就是在那个开会之后。

我也一直在琢磨这件事情呃，我说实话我也没有想到特别巧妙的，特别妙的想法，所以今天是把这个问题抛出来呃，然后我们可以大家都想一想，然后我会结合呃近期我们在做的一些工作，然后从一个小的侧面来啊。

分享一些认识和观察，OK那后面我就进入这个正题，嗯其实刚才林院长介绍呃，我我我在这个AICMM工作呢，呃大概分成两段吧，就是从16年到23年，实际上在做one floor。

那时候的重心是在搞这个training里面的问题，然后从去年开始就开始呃，开始silicon flow之后，就开始做inference方面的工作，那在这块呢我也会分享一下。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_1.png)

为什么发生这个变化啊，呃我还简单回顾一下，我们在training时候做的工作，我今天回看的时候，我也感觉，我们当时呃，这个我们还有包括很多同行做这个ESIM，system的工作呢。

呃还是非常就是它有系统的美妙，就是我们在system方面有一个可以有非常简单的idea，然后呢它会发挥非常大的作用，但是在今天我们在一会，我会看到我们搞inference的时候，呃。

会发现它需要非常非常多idea，综合起来才能有一个不错的产出，不同的产出，就好像一定程度上，就是这个system没有那种过去那种味道，就是那种纯粹的system的工作。

然后一个巧妙的idea就有很大的爆发力，然后我回以回顾一下，就是one flow的时候做的，我觉得嗯他那个时候有那样一种特质，第一个当时做的是一个比较呃，我们觉得是一个也是比较超前的一个想法。

就是当模型变得比较大之后，这个系统该怎么做，以及当面对一个大的集群的时候，我们怎么让这个集群上的编程变得非常简单，特别是像一张卡一样去做，其实今天看的话呃，这个idea现在很多问题已经被解决掉了。

呃在16年的时候，像这种想法或者这种呃还是比较呃西呃，有领先性的，那基本的思路也是像我们讨论很多的，像这个compiler，就是把一个单卡面向单卡编的程序。

经过compiler经过一层一层的rewriting，然后变成一个execution，Plan，就是physical the graph啊，这个里面呢这里面涉及到一些呃抽象的问题。

就是我怎么通过一个单卡逻辑的代码，变成一个多卡执行的物理的执行系统，这里面涉及到引入一些representation，就是系统层面的表达，然后这个编译器或者是呃optimizer。

它需要在这个表达上一层一层次转换啊，那后面我们就引入，就是说呃在one floor，当时还是比较非常简单的idea，但是他一下子就把这个表达的问题，比较巧妙的解决了。

实际上当时的这个重心就是把这个张亮的，从单个逻辑的视角到这个物理的视角，或者是分布到一个集群上的视角，它的映射的入规则呃，提炼出来，用最简单的一些入，然后就能够把我们所能想到的，形形色色的并行所表达。

那这个呢我们把它叫做SBT啊，其实在我们后来之后呢，呃学术界又有这个论文出现，就是包括谷歌的像GSPMD等等，嗯这个SSBP呢它其实就说一个张量，它可以经过split。

可以经过broadcast可以经过呃，另外一种不特别直观的就是那种趴手的方式呃，建立起逻辑和物理视角的一个映射，最后这个编译器，就通过这个在这个SBP的表达下，把一个单卡的单单单卡写作的方式的程序。

变成一个多卡分布式的执行的代码，在这个基础上呢，嗯我细节就先不进入太深了，它基本上可以把我们所今天面料的，各种主流的并行，经过非常简单的annotation，就是对张亮的一些annotation。

就把这个graph writing的方式就变成一个呃，一个一个compiler pass就就搞定了，另外呢它基于基于这个基础上，当然又引申了说呃，这个annotation怎么去自己自动去做，去优化呃。

也有一系列的工作啊，呃我们和李老师也做了一些合作，就是李世刚老师也做一些合作，怎么把这个呃这个COMPU呃，单卡的程序自动的变成更有效的分布式等等，它的代码就是说呃，就像刚才那个既包含数据并行。

也包含运行模型运行，它的代码可以真的像单卡一样简单，但是这个时候大家会注意到，中间会有一些就是annotation，就是还是丑陋的，是呃需要人工去写的，比如说这个一个矩阵，它的placement是什么。

它的SPP是什么，但是那个auto paralysm呃，是可以把这些问题搞定的，呃，实际上今天在社群里面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_3.png)

已经有很多很多这样的工作出现了啊，另外就是说刚才是一个compiler层面的，还有个run time层面，run time层面，其实呃我们看到比较多的，前段时间就是谷歌做的那个pass way啊。

是影响力，就是它的传播力比较大，它实际上引入了用actor啊，用这些东西去做，那同样做这个思路呢，呃我们在当时做one floor的时候，也是呃早好几年的，就是在16年开始做这个one flow的时候。

一开始就是用这个abstraction去做它，其实我我认为，就是它在这个超大规模的分布式的时候，仍然有很多的生命力，特别是跨集群的时候呃，多个集群之间嗯，很多集群之间。

它没有像一个集群内部有那么紧的耦合，然后弄所有的东西能一致的节奏，一致的步调去做呃，然后多个集群的时候，它一定有这个引入异步的，这个时候用actor的方式。

以及中间actor之间method parsing的方式，实际上是在wrong time的实现上，可以做到非常简洁呃，我们后面也发现，其实后面有些比较新锐的芯片公司。

比如说像tense torrent呃，人家在芯片层面其实就用了这种abstraction，就是在各个call之间以及各个chief之间，都是用这种message passing的方式统一去啊表达啊。

好我前面这个是回顾了一下之前的工作，呃，我我觉得这个在之前还是有一定领先性的，呃在今天这些idea呢还是比较我，我也觉得仍然比较美妙的，但是发生了不幸的事情，就在于说诶今天这个模型大模型出现之后。

好像就一种architecture，就是transformer decode only，然后这个时候发现做了这些能自动运行，能非常优雅解决这些问题的，问这些呢好像有大家不怎么用，为什么呢。

因为呃当那个模型结构变成architecture，就是一个decoder only的时候呃，用一个纯手工去写作的，其实就今天很多盗模型公司在用的Mac tro。

纯手工把这个parallel的方式写作下来，反而用的很多，就这是不是说一定程度是，像那个开源社区里面讲的，What is better，诶，这是不是呃，当然对我们来说是比较可悲的事情，呃。

我在硅谷的时候，也那个和那边的一些同事交流，呃，我又发现了一些令人鼓舞的事情，就发现仍然有人是喜欢这样的，其实比如说像谷歌他们做那个JX，实际上就是这些思路，就是JS和PYTORCH的区别。

就是呃就是它可以在编译层面，把分布式做得非常自动化，然后也遇到了，比如说X点AI date breaks，还有google的人哎，他们里面颇有一些这个researcher，还是非常喜欢这种方式的。

特别是你像XAI，他们官方在用的就是这个JS呃，说明这个方向应该还有呃，特别是当这个模型仍然有，未来的探索的潜力的话，或者什么这些仍然有很强的需求，很强的生命力，我们也希望有这种呃可能性。

另外一个就是说关于训练层面呃，实际上还有非常重大的问题，就是超级大规模里面这个SCALABLESCALABILITY的问题，比如说大家说的10万卡，百万卡等等呃，但是不幸的是，就是要做这种研究。

需要有一个大装置呃，但是对于我们绝大多数同行来说，是没有机会做这样的工作，这也是这就其实一定程度上解释，为什么我们开始做inference，因为我我摸不到那么多卡了哈，来做这个分布式并行了。

所以像我们最后一个talk，我注意到是阿里的同行去做这个交流，那我就觉得非常羡慕，因为我们有很多的GPU就可以去做这个工作，对好我回顾到这里。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_5.png)

后面进入到这个呃，现近期只举行的一个工作哦，还有一个就是编译器的工作，就是呃我在呃几年前在知乎上回答过一个问题，当时说我觉得这个AICM非常hard CORE的问题，一个是宏观的，一个是微观的。

宏观的就是分布式怎么去自动化并行什么的，微观呢，就是怎么让编译器，帮我们自动把这些所有的代码去生成出来，非常高效呃，同时也有非常多的这个compiler方面的研究。

在近些年像那个有很多包括TRITTON，包括微软研究院有这个ram等等一系列的工作呃，在这里面沉淀下来的呃，现在看到比较就是大家取得共识的，在这里面compiler和code站里面呃非常有效的方式。

其中一个idea也非常，我觉得也非常简单，就是原来把这个问题想得非常复杂，但是后来发现大家一致的都这么做了，就是基于TAL要去做，其实就是把一个很大的呃计算呃code，然后把它引入一个中间。

表示其实就是分块的，然后在分块之上呢，大部分是graph那种层面的优化，分在tie i r之上做，graph和TRITTON也是这么搞的，然后tail之下呢去搞那个cos jin的。

就是已经分成一个小块了，它就可以映射成这个很多芯片里面，那个已经很微观的一个结构了，在那个层面再去解决，就是不是说把所有的东西塞成一个，optimization的问题，那搜索空间就巨大啊。

就发现搞很多时间也搞得不是特别理想，但是把这个引入一个非常简单的抽象，type r2之后呃，上面的问题上面去做，下面的问题下面做，当然这个会有的时候，这种不能cross stack的这种优化的时候。

有时候有一些损失，这个就是后面的问题，好呃，前面介绍了一些，我们之前关注过和做过的一些工作，下面我介绍一下搞为呃在推理方面的一些工作，呃，为什么搞推理，刚才说了一个原因是，没有那么大的集群可以玩了呃。

然后第二个原因是因为行业内也比较关注推理。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_7.png)

就是呃推理的随着应用的爆发，它的需求量还有这个价值创造的经济价值呃，一定程度上可能更多，因为在这个training的时候，我们是阶段性的，然后训练一次模型，大家知道那个计算量和参数量N相关是六倍的。

参数量再乘以token数，推理的时候是两倍的参数量乘以token数呃，从这个角度来说，推理会比训练少三倍，但是他的token数是无止境的，就是训练的时候我们知道大概几万亿token。

10000000000000token什么的，但是推理只要我持续有有用户来请求，它就没有边界，比如说open i1天可能是几万亿token呃，几天或者一周，就把那个训练时候见的token都见过了。

或者都呃都那个量就覆盖了，那在推理里面，大家也知道这里面今天呃非常核心的问题，就在于说，今天的芯片都是，基本上是瞄准之前training的WORLOUD去设计的，就导致呃有些资源有些错配。

inference有一个preview阶段，它基本上和训练的时候是比较接近的，就是是一个batch，是一个仿存呃，是计算仿存比较高的一个WORLOUD，但是另外就是个decoding阶段。

就是那个auto ressive next token prediction的方式，它里面呢就是呃是一个仿存瓶颈的，这个是一个roofline model，我们大家都非常熟了。

就是呃在这个batch size很小的时候呃，它基本上那个计算的就是仿存，我把权重加载加载进这个，靠的这个时间是固定的，但是它那个上面部署了很多的计算的单元，瞬间就把这个计算完成了。

导致它这个要再取数据，要等一会，只有当这个batch size到过了一个临界点之后，才能把这个芯片上主要的这个call和计算能力，打满和打饱和打满，这个时候，就今天其实绝大多数芯片都是面临这个问题。

所以有的时候嗯发现有一个阉割版的芯片出现，反而对这个推理来说非常经济划算，比如说H20，H20，就是它上面实际上把计算的call都给砍掉了很多，但是带宽又加大了。

就就比较适合这种仿存的密集的这种WORLOUD，这是呃我们看到的在推理里面最核心的矛盾，呃，这个层面呢，既可以通过我们system的一些工作去搞定，也可以，当然今天也有很多人在做芯片硬件的角度去做。

比如说怎么通过新的芯片架构的资源分配，让这个带宽怎么搞得更高，你看前一段时间火过一个芯片叫GROCK，Grock，就是呃把所有的东西都放到s ram里面，然后访问那个带宽巨高几10T。

然后它可以做到那个每秒钟生成1000套梗等等，OK这是这个inference里面的核心问题，今天看到的这个可以通过system的角度去做，也可以通过芯片角度。

但是芯片今天还没有太成功的deliver出来，这种结果呃，还有就是我想后面就是重点分享的，就是呃看到的还有更多的工作是和算法结合的，就是所以为什么这是我刚开头说呃。

好像这种很多system工作不是那么pure，不是那么纯粹了，因为我们发现就是在这个层面的这个约约束下，像我们过去一样把某个科呢优化得非常好，甚至它可以翻两倍，但是它在端到端的contribution。

就贡献里面只有百分之几，就是它就不是在这个inference里面，它不是那个过去那些专长能发挥更大作用的，一个一个一个任务了，OK刚才我也提到了，就是说他要过一个batch的临界点。

才能把这个芯片跑好呃，所以实际上在系统方面，很多很多工作都是就今天看到的，软件方面的工作，系统方面工作都是在解决这个问题，就是怎么把batch搞得更好，比如说呃PATTENTION怎么把显存用的更有效。

可以让它可以放下更大的batch，或者放下更大的k v catch等等，以及continuous spatch，怎么让这个啊很多token可以一起推理等等，还有一些算法的工作，这些都是为了解这个问题。

呃后面我列举几个我觉得比较有意思的，属于算法和系统协同工作的这样的一些例子，其中一个就是我们非很多人都非常熟悉了，就是投机采样，他他其实把那个toby token的prediction。

能够变成用更小一点的模型，变成一些并行度更高的，然后从从而能利用好，今天这个刚才说的，就是面向过去需求所设计的芯片架构，这个也就不多解释技术技术细节了，还有一个呃，今天其实学术界也有人关注到了。

就是说呃我们使用大模型的时候，有很多best practice使用开发范式，比如说agent多agent那一个agent解决一个问题的时候，它实际上通过多个步骤，多个步骤分解来做。

一个是它通过多次大模型的调用，这些大模型调用之间有些依赖关系，一定程度上就蛮像我们过去搞compiler，搞搞这个分布式并行的DAG，就是有向无环图一样，那同时在这个agent的workflow里面。

这种有向无环图上的优化也是有这个机会的，他有可能把一些没必要的，LM的API的调用就给优化掉了，甚至能够把多个LM的调用可以并行的去做，而不是一个一个去做，这是左边这张图。

解释了一个agent的内部的一些优化机会，右边的是一个多agent，这种多agent的时候，它就更有这种并行机会等等，所以这个里面其实在过去compiler视角去看的话，它就是一个graph的优化。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_9.png)

还有就是呃，其实今天很多流量很大的大模型公司，包括open i什么的，已经用这些了，包括前几天苹果发布的那个已经是端侧和模型，大拇云端有协作了，其实就是一个模型，路由的就是一个任务过来。

它不见得全部发到那个最大的模型上去做，它可以让能解这个问题的，足够的一个不大的模型去解决好了，这个时候他就需要前面有一个root，这个root就要做得非常好，他能把这个任务调用分类啊。

能够有的问题正确的分到小的，正确的分到大的，然后通过最后通过一个综合的来说推理推理呃，就一下子就是提高十倍，效率提高十倍或者什么的，这个时候要比一个科诺的优化。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_11.png)

带来的ROI要高更多，还有这种就是这是together AI，有一个前几天做的一个就是它叫mixture of agents，就是他通过多个弱一点的模型的协作。

然后竟然在榜单上打过了g p t four o，就是在就是它不是单个模型，比gt four o桥，这里面其实是也也涉及到一些系统方面的工作，因为我们知道他together AI。

实际上也是一个AI info的一个一个一个公司。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_13.png)

还有就是这个OK还有这个在实际工作中，比如说long context啊，还有什么一我们会做一些system或者算法的优化，比如说attention上有标准的attention，也有这种GQA。

还有前一段时间deep six做的MLA等等，呃在这个里面呢，它通过这种算法的一个创新，能够把k v cash优化的节约很多，然后推理效率提高很多等等，这是非常好的。

但同时呢呃还会有一些用户实际上有这个反馈，就是在某些对attention要求非常高的场景，有时候会发现这些算法的优化，它对效果有hurt有有损伤，比如说有些在做教育的数理逻辑，那种推理的环节里面。

他那个attention就要求比较细腻，这个时候有的时候对attention做一些裁剪阉割，它会对那个模型效果有损伤，有可能我们在写小说，做情感陪伴，用这种无所谓，但是有的场景可能还得需要回归到。

非常计算比较多的这种attention机制上去做。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_15.png)

还有比如说量化精度呃，它可以有效解决这个kv cash，把kv cash弄得很小，但是这个是前一段时间，我们截图了一下一个模型，它对量化降度之后呢，他对那个long context，因为它量化之后呢。

对那个attention啊，soft max那个计算的分辨率就没那么强了，就会发现它long context效果就下降很多，这也是一个问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_17.png)

这是在实际中遇到的问题，好那时候我总结一下，就是前面介绍了像呃过去我们关注的一些工作，包括自动运行啊，分布式呃，自动代码生成呃，然后现在我们感觉这个工业界比较关注的是，大模型推理啊，实际使用啊。

这里面呢会发现有非常多的算法软件系统，还有硬件的call design工作，然后有的时候在这个层面做一个工作，会它的收益会比那个单纯做一个system的工作，收益会多很多。

所以回回到头就是我是抛出那个问题呃，我还在回，我还在寻找这个问题的答案，希望能够后面有一个更好的回答。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/925954947a247a1124ce099b4b4b1459_19.png)

# 2024北京智源大会-AI系统 - P4：FlagGems通用Triton算子库-白童心 - 智源社区 - BV1DS411w7EG

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/1673cf8008d0c20b93aa48c97170fe3b_0.png)

好像那个今天我们带来的那个，就是说这个题目是flag james，是我们最近推出来的一个开源的，那个基于TRITTON的算子库，对，然后我们呢那个我们现在那个智媛的这个呃，团队大概是三四。

最近来招多多招了几个人，大概是五个人的规模吧，然后我们这个flag james的算子库是我们联合呃，硅基，还有中科嘉禾，以及呃，国内的一些那个芯片公司，来共同研发的一个这个算子库，对对对对。

所以我们现在先简单介绍一下这结果，对我们现在这个算子库的呃支持大概是64，66个这个呃基础的算子呃，还包括六个那个容克算子，然后呢，我们呃期望的话，就是年底能够实现100个算子左右。

明年大概就是说能够基本上实现，就是对现在这个典型的大模型的，这个算子的全覆盖，所以这个期待大家去使用啊，啊所以我们今天讲的这个题目是什么呢，就我们去年讲的是一个AI编译器的这个题目。

我们每年争取都会给大家带来一些新的东西，然后呢我们这个工作方向是AI编译器，而我觉得在这个AI系统里面，我们是对被挑战最多的啊，这就是记着我们刚进智园的时候，就那个领导就说哎呀你们做这个编译器的啊。

我们现在面临的是多个这个框架，以及多个这个AI芯片，你们能不能做一套这个编译器系，能够呃支撑这种多元的这个框架，以及多元的算力，都把它们连到一起对，然后呢，那个今年的情况就是说。

就是除了这个英伟达的这个算力之外，可能尤其是大模型的训练啊，这个存在这个大量的算力不足的问题，然后呢，能不能通过这个编译器或者相关的技术啊，把这个不同的算力能够混在一起用起来对，所以围绕这个问题的话。

我们探索了一下这个，这个就是基于TRITTON的算子库的方向，我们觉得这是一个非常值得呃做，而且我觉得通过我们现在验证的这个结果呃，看起来也是非常可行的一个方向，对嗯。

但我们刚才说的就是说这个呃除了我们之外，还有呃其他的这个芯片厂商啊，就是除了英伟达之外，就是我们芯片厂商里面的这个软件开发人员，也是面临这样的问题，也就是做做的东西特别痛苦。

就是就是要开发这个AI的编译器，然后支撑这个多个框架，然后呢要面向自己的这个硬件去开发，这个中间表示啊，后端优化啊什么的，非常痛苦，我觉得这个所有的这些这个这个生态里面。

这些公司里面可能最舒服的就是英伟达对，就是英伟达，因为他已经有自己的一套比较完善的，完整的这个生态体系了对吧，那么其他这个公司呢，可能通过各种各样的方式是跟他找对齐对。

比如最典型的那就是那个这个呃ROHAM，就是AMD的这ROCOME的这个生态体系，实际上跟这个英伟达是非常接近的对，然后呢，呃其他厂商也是通过跟这个英伟达的功能对齐，这种方式来去这个呃。

达到这个构建自己的生态这样的一个目的对，但是现在就是说呃，我们的一个主题，就是如何去实现这种这个多元芯片的混合算力，这样的一个问题，那摆在我们面前呢，除了这个算力就是硬件之外呢。

还有一个问题就是说怎么去统一这个呃，不同的软件生态，我们现在的做法是什么呢，就是每家都是像这个哭大声带去对齐的，但这样有一个问题在于什么呢，就是KA生态的，它的核心是COA的编程语言对。

但是这个不同的芯片架构，你去面向coda这个编程语言去适配，会存在本质的问题，对后面我会讲到，所以所以呢呃简单讲会存在几个限制，第一个是编程接口的这个限制，就是KDA。

它的这个编程模型是面向CMT这样的，这个硬件的架构的，对那么有一些加速器呢并不是CMT的架构，所以适配起来会比较困难，那即便是这种呃通用GPU的架构，可能在一些实现的细节上也有所差异。

所以一味的向这个KDA系统去做这个对齐，不一定是最好的选择，第二点就是说它这个适配过程本身呢，就是针对大模型AI这个领域，既不具备充分性，也不具备这个必要性，也就是说这库达这个体系是大而全的。

那我们这个AI的，尤其是大模型这个领域依赖的算子呢，可能数量没有那么大，所以啊也许存在一种小而美的解决方案好，另外就是说这个面向KUA去做适配它的这个呃，这个呃适配的负担也是非常重的。

因为KDA体系刚才说了就非常的庞杂啊，开发难度也比较大，所以这是啊现在存在的这个问题，就是每一家都去啊构建自己的生态啊，那个像coda对齐这种方式呢，无疑是没有办法去解决这种生态融合的问题的，对。

然后呃除了这个CUDA之外，当然我们知道这个历史上还存在其他的，这个竞争的生态了，其实呃坦白说也不是非常的成功，然后这里面有很多因素，其实主要因素就在于呢就是面向任何一种芯片，它的开发力量不足。

所以导致就是说针对任何一种体系结构的，这个软件的质量是跟不上KDA的对，那所以就是说如果解决这个，如何去解决这个多元算力的，这个生态融合的问题，我们觉得我们需要在枯大生态之外，去重新开辟一片田地。

那基于什么呢，基于这呃这块因为我们少了些动画嘛，就是说我们需要一个统一的这种编程语言，这个编程语言，也许它没有KDA那样高的这个覆盖度，但是它足以解决AI的一些根本的问题，对，那基于这个这个共同的。

这个就是统一的这个编程语言呢，我们可以去构建一套跨芯片的话，企业结构的算子库对，所以在算子库适配这块，就不需要每家单位来做这个重复的工作了，那每家芯片厂商呢就是针对自己芯片的架构。

可以去适配或者开发自己的芯片的这个编译器，后端，这是我们的这个设想，我们觉得这种在我们开发了flag james，这个算子库之后，我越来越觉得这种这个路径非常的可行好，那我们就是探，就是说第一个问题。

就是说我们为什么会选择这个TRITTON，去做这样的一个这个这个开发语言，因为去年实际上我们一致探讨这样的一个问题，去年我们自己是开发了一套比较新的，这个中间表示对，但是那个我们试图去解决从上到下啊。

多个层次的问题，有点类似于MR对，但后来发现就是说TRITTON作为一个呃，这个新新的这个编程语言或新事物，它具备呃很多优势，所以我们就采采用TRITTON，来去做这种这个开发语言了。

嗯概括来讲就是它有四大优势，第一个就是呃它的编程的模型啊，就面向这种这个加速器的编程模型，它是优于CMT的这个编程模型的，第二点就是说，它具有这个非常独特的开发优势啊，开源优势，然后第三方面就是说。

我们做的这个初期的实测性能，也表明TRITTON在性能方面也非常有竞争力啊，最后一点就是说呃，Tritton，由于他现在的这个呃，这个胜，就是说在开源领域的这个这个受欢迎的程度啊。

所以很多这个厂商已经开始去适配它了，好那为什么就是选择这个TRITTON，我觉得首先就是说呃先从这个sim t开始说起，就是说KDA本身它是面向CMT的，一个编程语言。

也就是说它有点类似于C针对这个CPU一样，它实际上是非常底层的对嗯，这个所以就是你在编程的时候呢，实际上虽然你是在面向现成的编程对吧，但是你考虑的是一个这个CMT架构的，这样一个体系结构对。

也就是说这个这个体系结构是一种是高吞吐，刚才那个这个呃袁老师已经说了对，其实这种这个架构，它的极端可能像GRP那种形式，对它所谓的寄存器能，就是SMSMM来去实现的，然后呢它的问题在于什么呢。

就是说他是这个呃次数非常高，但是它可能会有一些冗余，比如说你每个县城可能都需要独立的寄存器，去存储这个仿存的这个地址啊什么的，对相对而言就是传统的CMD的这个架构的，这个呃处理器，那也是并行的。

这种处理器它可能需要更少的这个呃寄存器，它没有那么多的哪个线程，所以它的这个latency可能会比较低一点，对这是不同的这个加速器实现的架构的差异好，那刚才我已经说了。

就CMT实际上就是说它是有架构的先进性的，对吧啊，英伟达做的非常好，但是并不意味着这个市场上所有的这个硬件，所有的加速器都是用这种方式实现的，就是有CMD的架构，CMD的架构。

典型的就是例如这个X86的这个呃，这个向量的这个协处理啊等等，就是在这个异构真正的这个易购硬件里面，提供一些这个加速的功能，除此之外呢可能还有像TPU这样的这个呃，呃tensor processor对。

所以实际上就是说针对这个AI的加速，有不同的硬件架构，那么KDA作为就是面向这个CMT的，这个编程的模型，可能仅仅适用于CMT的架构，那么他那个TRITTON他的编程语言，刚才那个这个袁袁近辉也说了。

他实际上是用tie或者block的编程语言，就是它在编程的时候，你可能想象当中的是一个呃，划分的并行的这个数据块，那再下一步如何去映射到CMT或者CMT的，这个体系结构呢。

是由编译器或compiler去实现的，所以它既可以映射到这个CMT上，也可以映射到这个CMD上，是一种非常灵活的这种编程的模式，嗯这是第一点，第二点就是说TRITTON，它有独特的这个开源的优势。

其实我们回顾一下那个整个跟AI系统相关的，开源的趋势，我觉得是分三波的呃，我可能没有太多时间就不详细讲了，第一波是框架，这波主要是为了什么呢，是为了给这个算法开发者提供可以预制的。

这个呃这个算法的这个building blocks，然后呢第二波是编译器，那就是能够充分利用这个硬件的这个这个性能，第三波呢是针对LLM的一些这个呃，这个开源的这个软件，但第三波的特点是什么呢。

就是说在第二波，第一波，第二波基础上增加了很多自定义的算子，这个是前两波不具备的，所以第二波的那个编译器呢，有的时候是考虑图编译器，但是没有考虑到如何去实现，这个自自定义的算子。

典型的就是说一些基于MLR的这个，A i compiler，所以他现在这个TRITTON出来之后，那那一波的这个这个AI编译器，可能在这个自定义算子开发上面，就处于落后的状态了，好那就是说我们比较一下。

就是说这个try on，跟我刚才说的这个相关的这个AI的编译器，它们的优势跟劣势其实非常明显的，也就是说TRITTON它提供了一个啊，这个开发自定义算子的，特别好的一个编程的接口。

而且它的实现质量也是不错的，另外就是说从我们的那个性能，这个这个测试的角度，它也能够这个交付跟库A类似的这个性能，其他的比如open x o a的话，就是它的问题就在于什么呢。

它初期没有考虑是自定义算子，现在当然通过这个palace也也加上来了，但是它的后端可能是面向TRITTON的，然后呢呃未来可能有这个MOJO啊，是个全新的这个编程语言，所以从编程语言角度。

MOJO可能更现代，它是个原生支持异构计算的人编程语言，但我们看来就是说他现在完全不成熟，所以我们现在没有办法预测它的未来，就是综合来说，就是TRITTON提供了一个非常均衡的啊。

而且有优势的这样的一个，这个这个异构计算的编程语言，我们刚才说它的实测性能也非常棒，左边是我们测的这个矩阵乘的这个性能，右边是flash attention，这是去年10月份到11月份。

我们做预言的时候的数据啊，到那那个时候的话，基本上都差不多能跟哭的是打平的，这是我们实现的那个这个layer norm的这个算子，然后呢几条线呢最差的当然是torch，然后最上面的话是蓝色。

是我们优化过的对啊，还有这个这个INDUCTOR生成出来，我们发现手动实现的TRITTON，在性能上也是更具优势的，对嗯除了这个单算子之外呢，就是我们可以看一下，就是呃PYTORCH它的这个CI里面。

你都可以看到这个呃torching dr。编译出来的这个模型的性能对，那实际上从这里面也可以看出来就是in d呃，这个这个TRITON编译出来的kernel呃，组成模型之后，它的这个模型性能如何。

其实我们可以看出来在A版上面的实测性能，说明这个呃，这个呃结合这个这个INDUCTOR的这个代码，生成的后端，已经初具这个性能优势了，对其他平台先不讲，只只是这个这个A版的情况好，除了前面三点之外呢。

就是说这个呃，Tron，现在已经得到越来越多的这个，芯片厂商的这个支持，至少我们了解到已经国内有数家的芯片厂商，已经开展了那个try on的适配，而且适配的程度呃还是非常不错的。

另外就是说呃第二点就是说这个，但是大家基本上都是以torch compile，这个接入点去适配TRITTON的，就没有注意到TRITTON作为独立算子，开发它的重要性。

然后第三点就是说这个有一些这个LOM，用这个这个国产的这个芯片啊，进行这try on算子，替换率已经达到90%以上了，对这个也是不先先不谈性能对吧，就是但它覆盖率也已经非常高了，对。

所以最后一点就是我们现在的这个局限性，在于我们try on实现的这个算子，在国产芯片上面，对吧啊它的性能仍然存在一定的差距，所以下一步需要进一步完善编译优化好。

那我们就讲就是说我们我们用这个TRITTON，来去这个开发这个呃算子库，我们怎么去开发的问题，就是我们可以看一下，就是呃算子库呃，一般来说怎么去又开发一个，就是呃适配这个PYTORCH的算子库。

最老的一种方式就是说我们提供这个呃，拍托尔算子库，这个呃面向这个厂商的这个芯片的，独立的或者封闭的这种这个fork，而中间一种方式呢，就是说我们可以定义一个统一的这个，算子库的接口，然后呢。

再用这个算子库的接口去这个接入这个呃框架，第三种就是我们提出来这种方式，就是说我们可以基于TRITTON，来去构建这个算子库，对这个我们我们现在就是那个flag james，就是这样的一个目的。

这样它的好处就是说厂商可以共享这个算子库，我们厂商的话不需要独立去开发了，第二点就是说我们既可以支持eager，同时跟那个torch compile，这种编译执行也不矛盾。

然后呢同时呢我们的算子库也是开源共享的，也可以开源共建啊，算子实现的一致性高，对第二种方式呢，我虽然定义了统一的接口，但它实现不同，所以当你去混合，去去去去去开展这种混合算力的训练的时候。

可能会碰到这个算子实现不一致的这个问题，好，那问题在于，就是说现在是不是有基于这个TRITTON的算子股，但是TRITTON的算子的现状是什么样的呢，就我们发现这个TRITTON呢。

现在不存在这样一个通用的算子库，TRITTON的算子是分散在各个这个这个这个，开源的这个仓库里面去的，另外就是说呃比如说呃TRITTON，它这个官方的那个仓库里面有一些自带的算子，然后呢。

第二部分就是说PYTOR框架，通过INDUCTOR生成的这个算法，对这个对啊，这个torch compile是有一定的依赖性的，然后第三方呢就是还有一部分，这个第三方的加速库实现的这个自定义的算子。

像flash attention等等对，所以他们这个呃算子来源呢是非常的不一，而且这个接口也是不一致的啊，第二点就是说我们覆盖面，这个现在的try on算子覆盖面是比较低的，没有办法支撑这个呃大模型。

或者说通用的训练的所有的算子的需求，我们希望能够打破这样的局面，所以我们希望建这个，构建什么样的一个算子库呢，就是第一个是要具有通用性，就是统一的接口，能够面向PYTORCH等主流的框架对齐嗯。

第二点就是共享，我们提供统一的仓库啊，开源共建，这样有利于这个这个对齐，而且就是说多方可以减少这个投入，然后第三方面就是说，我们希望能够支持大部分训练啊，所需要的全部的这个这个算子。

所以要实现这个全覆盖，第四方面就是我们实现的这个算子库，里面的算子要呃做到高性能，就是所谓的高性能，可能不一定像这个推理优化器那么那么快了对，但是基本上要达到原生这个算子库的这个水平。

就是用于做训练时啊，这个能够达到要求才行，然后最后一点呢也是比较重要的，就是需要这个统一的这个算子库，能够支持多种后端对好，那就是有了这个目标之后，到底怎么去构建这个算子库，实际上从技术讲呢。

其实有两种方式，第一种的话，很显然就是我们就是通常的使用TRITTON，这种方法，对第二种的所谓使用TRITTON，就是通过即时编译这种方式，去通过标准的TRITTON的接口去使用它。

然后第二种方法是通过预编译的途径去使用，也就是说它的使用方式，就是先通过这个TRITTON的这个动态的接口，去编译出这个一组这个静态的kernel，然后呢啊再通过条件编译等等方式呃。

去这个重新构建这个PYTORCH框架，对，那我们采取的是第一条路线，我们觉得第一条路线是有更好的这个兼容性呃，这个可靠性呃，虽然它有一些不足，也就是在线编译的话。

你需要付出这个呃run time overhead开销的这样的不足，这个是可以通过技术手段来去解决的好，那我们现在这个flag james，它这呃基本的特性是什么，我们可以概括为三方面。

第一个就是说它能够自动的，透明的接入PYTORCH，那用户呢使用的时候非常简简便，第二点我们无需PYTORCH呃，Compile，所以算子的这样的话，对他这个算子的端到端的这个这个latency的。

要求就比较高，不论大算子还是小算子都要非常快才行，第三个就是说我们呃profile，这个呃一些主要的大模型，然后呢取这些大模型里面算子集合的bean，来去开发。

首先讲就是flag james它易用性的优势，就是说我们是通过这个PyTorch library，API来动态的替换ATTEN的算的时间，这个实际上这个功能是PYTORCH框架提供的。

我觉得这个体现了这个PYTORCH框架，它的扩展性呃优势，然后呢，因此就是说我们是无需重新编译，这个PyTorch fork，你只要拿来这个fight torch。

甚至就是说你的芯片自己的PyTorch fork，也可以对这样提供这种一种比较呃，极其简单的这种使用的方式啊，刚才我已经说了，它是不依赖这个torch compile的。

所以对PYTORCH版本的兼容性，要求也不是特别高，对然后我们使用两，我们现在支持两种方式去这个替换，PYTORCH的原生的算子，一种的话呢就是说是全局的替换，我们可以在这个啊代码前面呃。

就加上两行就可以了，Import flag jams，然后flat jams enable，就是我们把所有能替换的算子全部替换掉，第二种的话。

就是说我们通过这个PYPYTHON的context manager，来进行局部的替换，你可能在一个这个scope里面，我们先把这个scope里面的，换成我们这个比如说很方便去做测试啊，做验证啊。

你们可以用这种方式就可以，只要with flag james，Use james，那么里面的这个这个呃代码块，里面的这个算子就会完全被替换掉，我们用一个非常简单的这个训练代码，来做一个试。

这是那个玉龙他们做的flag scale的那个训练，我们只需要增加一行就可以完成这个识别，不不需要做任何其他的事，对第二点就是说我们刚才已经说到了，就是如果我们要实现一个通用的，这个算子库的话。

你不不仅仅要这个去支持这种大的算子，你还需要让这些小的算子，能够满足这个latency的要求，对，所以我们对这个呃呃这个运行时做了一些优化，对啊，今天下午呢，那个我们还有两个同学来给大家介绍。

详细的介绍这个我们这实现的些机制，所以我在这由于时间的原因就不具体的介绍了，嗯结果大家可以看一下，就是说红色的这个线呢，我们只是没有做任何优化的。

这个是由于TRITTON本身run time overhead导致的，当你的算子规模较小的时候，那CPU的overhead就体现出来了，我们优化之后呢，是蓝色的这个线基本上跟一格是可以打平的，对好。

那除此之外就是说还有一个问题，就是我们怎么去开发这些算子，就是大量的这种这个MMY的算子啊，它的输入的形状是不一样的，如果我们手动去开发这个算子的话，那么就遇到这样的情况，就是针对不同的形状。

你需要开发一个新的算子，所以啊我们开发了这个自动的这个代码生成的，这样的一个工具，就是说可以处理不同的形状，然后处理这个不连续的内存的排布啊，另外就是说能够支持标量跟张量的混合输入，最后就是说啊。

我们可以很容易地去用它来去定义这个算子，融合啊，后右边是两个这个实际的例子，对我们不仅仅是可以写一行，就像那个Python lambda这样的，还可以呃任意去这个构造一个这个SCALER的。

这个函数让它去扩展成一个point wise的这个函数，这个算子对这个这个呃具体的这个呃，code generation的技术呢也是下午我们有一个talk，我们下午有一个分享给大家介绍。

我们这里面也是时间的原因不介绍了，好，那这个简单看一下，我们现在扣J的这个结果，就是当它这个输入不连续的时候，它的性能就体现出来，性能优势就已经体现出来，非常明显嗯，好那就是我们现在这个算组开放。

刚才说已经实现了66个算子，它包括了几类，一类是这个线性代数类，基本上这个矩阵成jam类的算子都有，都有实现，然后呢部分神经网络，比如激活类的这个算子啊，还有基本的这个数学和逻辑的算子。

以及一些融合类的算子，感谢这个硅基的帮我们实现的是这个呃，融合类的算子对，那要除此之外，就是说我们全部这这些这个60个算子里面，凡是用到反向的，我们都实现了，但是PYTORCH有很好的这个呃。

所谓implicit autograd机制，也就是说它可以一部分算子，是可以把它分解成已经实现的算子了，我们这里面实现，我们这里面是啊，就是说是实现所有必要的这个，这个反向的算子嗯。

好给大家看一下这个性能的数据啊，就是性能的我们现在66个算子，大部分就是说与这个啊，KDA实现的这个算子是持平的呃，呃有一部分呢是有一定性能优势，然后呢有小一部分你看就是有几个。

比如说我们现在的vector norm性能不太好，那就是这可能需要进一步的优化，还有一部分像这个SIGMO呀，ten呐什么的啊，在这个低精度的情况下呃，性能可能只达到一半。

那是因为我们在做这个呃activation的时候，我们进行了这个type promotion，所以就是说计算的时候用了这个IP，32的对，现在性能也是非常棒的对嗯。

然后我们在这个flag scale的这个呃，我们用这个flag james替换了所有的这个，我们能替换66个算子，之后，我们进行了这个训练的验证，这是flag flag scale的这个训练的。

这个收敛性呃，lost的这个curve，我们看到就是说当我们训练到5000步的时候啊，这个收敛曲线基本上是吻合的对，所以我们当然进一步的，这个我们现在还没有完成一个完整的这个，端到端的训练。

但我们从这个5000步的这个趋势上看，还是这个呃非常promising的对，好最后就是那个我们现在的代码已经开源了，然后开源的这个地址呢是在这个上面，这个GITHUB上面跟我们一起开发的是呃。

除了我们智媛的这个团队之外，还有中科嘉禾以及这个硅基啊，硅基的团队，然后我们也非常感谢，就是支持我们这个FLAJES算子库的这个呃，芯片公司，所以我们现在除了呃支持英伟达之外。

还支持呃如下的这些这个芯片的，这个呃企业的这个芯片产品，对未来呢，呃我们会除了呃支持这个现在的这些芯片之外，我们可能还会面向其他的这个，芯片厂商进行拓展，除此之外呢，呃我们也会这个呃介入这个非讲的这个。

框架生态，以及我们会探讨怎么去这个跟谢老师能呃探讨，去介入这个rise five的这个生态啊，还有这个翟老师就是超算的平台，我就说try on能不能落地到try探的平，这个超算平台上。

也是下一步我们要探讨的内容，呃这是我们那个呃开发的进展，以及这个今年的里程碑，我们今年年初开始开发的，这个大约是2月份开始了，也就是四个多月的时间吧，对呃刚才已经介绍了我们核心的开发团队。

我们下半年的话，争取实现100个以上的这个高频的算子，同时我们除了核心的开发团队之外，也也欢迎这个社区去贡献代码啊，另外就是说我们会持续呃，联合这个厂商去完善多芯片的适配，跟这个算子的性能提升。

明年的话我们会持续的针对算子库进行优化啊，希望明年这个时候能够完整的支持，这种端到端的训练呃，总结与展望，就是说我们是针对这种多元芯片的生态，适配的难题，我们跟这个多家的芯片公司。

共同推出了这个flag james的算子库呃，目前代码已经呃开源，支持66个高频的算子，我们向上是兼容PTORCH框架，向下是支持呃八款以上的易购的芯片，然后AA100的性能数据基本上齐平。

PTCH原生的算子库，未来呢我们呃刚才说了，我们今年年底将完成100加个算子，25年的话，明年基本实现大模型算子的全覆盖啊，另外就是我们会继续完善这个模型的，端到端的对对呃，对接优化后端的性能，对。

使得大部分这个适配芯片的性能，能够达到原生的这个90%以上呃，除此之外就是说我们会继续拓展生态，支持飞桨框架呃，超算系统，以及融入到这个reserve开源的体系当中，好谢谢各位。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/1673cf8008d0c20b93aa48c97170fe3b_2.png)

谢谢谢谢。

# 2024北京智源大会-AI系统 - P5：深度学习编译-从定制化资源分配到高性能代码生成-赵 捷 - 智源社区 - BV1DS411w7EG

OK那个啊谢谢林院长介绍，然后我今天非常荣幸到这里给大家汇报一下，我之前做的这个深度学习编译方面的这个工作，然后呃，这个汇报的主题呢，是从定制化资源分配到下边的这个，高性能代码生成呃。

整个这个过程呢是涉及到深度学习编译里边，从上层的一些调度，到，最后跟硬件架构非常相关的一些代码生成的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_1.png)

这样一些经验。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_3.png)

呃，然后这个是我汇报的主题的一些呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_5.png)

呃这个汇报的这个内容啊，先说一下我们这个为什么要做做这个呃，深度学习编译系统啊，当然这个可能在座的各位都可能会，比较了解了呃，就是说原来我们有这个深度学习框架的时候，他的这个工作模式呢是为了能够啊。

直接用底层呃，呃厂商提供的这种算子库来直接适配这个东西，然后适配完了之后呢，去呃这样去把他的这个框架，这个工作流程给完成，但是呢有一些呃，我们总会遇到一些之前我们没有去见过。

或者是没有实现的这样一些算子，那么这个时候呢，我们是希望用那个编译器去完成这个，自动生成的这样一个过程，所以深度学习编译器的这个呃，研发的背景是这么一个背景，然后嗯我们呢是可以面向不同的这种芯片。

包括像这种CPU，CPU以及嗯现在国内比较呃，就是投入比较多的，也是因为这种啊国际大环境的这个影响呃，投入比较多的这种AI领域特定的这种芯片，那么我们针对这种领域特定芯片呢，做了一些工作。

然后给大家汇报一下。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_7.png)

那么做这种领域特定芯片的时候，我们发现和传统的CPU和GPU，最大的区别是什么，最主要的区别就是我们在这种嗯，呃那个AI芯片上去做呃，code j或或者编译优化的时候，我们必须要考虑它的这个存储模型。

比如说像CPU和GPU上，我们传统的是这种嗯，比较典型的这种金字塔型的这种存储模型对吧，那么你数据在这个存储模型上肯定是直上直下。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_9.png)

这么流动的，像GPU上也是这样的，比如说它通过抽象出来这个thread和thread。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_11.png)

block的这样两集并行抽象，但是呢在AI芯片上它不是这样的，比如说我们比较早期的这种AI芯片是吧，像这个计算所这边提出的这种电脑系列芯片，你很多数据呢是输入和输出的时候，它的位置是不一样的。

它不像原来传统的这种，金字塔形的存储层次结构。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_13.png)

同样呢也有这个TPU的这个存储层次，结构也是不一样的，比如说你这个数据通，对这个呃里边最核心的功能部件矩阵乘呃，从这种呃这个weight的这个buffer去取数据之后。

他把数据放在这个accumulator上，然后呢，后边他可能会做一些其他的这种，激活函数的处理之后，再放回它这个呃所谓的这个ub buffer里面去，那么我们可能国内的一些厂商，比如像华为的升腾C呢。

也是采用了类似的这样一个，存储层次的这样一个设计，所以它这个上面的这个数据流的这个管理，是和传统的CPU和GPU的这种存储模型，是不太一样的，那么我们针对这些呢就去总结了一下。

现在这种深度学习编译的这样一个呃，形成的一个比比较固定的这个流程，也就是说，你给了一个这样一个深度学习的网络之后呢，我们会把它编译成呃，会把它表示成一个计算图的这个形式，然后呢通过这个计算图呢。

我们再把这个计算图去切成一个一个的小的，这个子图对吧，然后这个小的子图呢再转换成底层的这种呃，循环的这样一个表示，那么循环表示呢，我们会利用我们所呃称之为算子编译器的，这个东西呢去做一些优化。

然后在最后呢去呃适配底层硬件呃，做一些code站的这样一个工作，但是嗯包括刚才那个呃袁俊辉老师，他可是现在不在了，他他也提到这个东西呢，我们现现在做这个呃，软件站的这样一个设计的时候。

你做各种抽象的时候，每个抽象之间可以做自己的这个独立，优化空间的探索，他把这个抽象呃这样分割开开来，是为了能够把优化空间搜索范围变小一些，但实际上有的时候这种分割的这个形式呢。

会导致你呃优化的空间也会变小，所以我们实际上在这种，比如说在跨呃模型结构和计算图表示，以及图层和算子层编译的这种不同，编译抽象之间的协同，之间也有很多呃优化的机会。

那么这些跟这些抽象之间的这个优化机会呢，可能会给你带来一些呃额外的性能收益，比如说第一个我们说啊，呃就是我们说的这个所谓的定制化的资源分配，也就是考虑了一些呃上层上上层算法和呃，你怎么去呃。

在这个硬件芯片上做资源调度的这样一个结合，能够让你去更好的去得到一个呃，调对着这样一个结果啊，譬如说我们我们针对这种呃音音呃，就是国叫什么呃AI的这种领域特定的芯片，做了一个呃它的架构的这个抽象。

比如说我们可以总是呃一些芯片啊，就可以总是把它抽象成有呃，有有这么低格cluster，然后然后每个cluster上面有有四个cheap对吧，然后每个呃，每个chip上可以有不同的这个计算功能部件。

那么针对这种硬件去硬件抽象去做呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_15.png)

调度的之后，调度的时候呢，我们有的时候是可以考虑呃，是可以考虑你输入模型本身的这个特征，比如说呃我们原来传统的这种编译器里边啊，传统的这种AI编译器里边，它在做底层底层的这种调度的时候，他会看的非常细。

也就是说看到非常底层的这种，比如说卷积算子，或者是说后边的这种嗯编算子，它们之间的这种融合的可能性，但是实际上，如果你把你的这个视野放得更高一点，那么整个模型的结构呃，模型的架构上。

比如说在这个RESNET50上，它可以划分为四个阶段是吧，那么这四个阶段之间，其实因为这个网络模型本身的特征，它可能会对输入图像会做一些特殊的操作，比如说呃呃图像进来之后。

我们可能会做向下采样的这样一个操作对吧，那么这个向下采样的操作呢，会导致你整个呃输入图像，在穿过这个网络模型的过程当中，它占用呃内存的空间大小在不断变小，那么如果你采用传统的方式去不断的切割。

这个切割这个网络模型，比如说我们就从比较粗的力度来看，这网络模型，我把它切成四份对吧，那么你就是按照原来那个方式把这个图像呢，就因为它是端的端的嘛，你整个切成四份，按四个阶段来写，每每个阶段来处理的。

这个batch维度是一样的时候，那么由于你有中间这种向下采样的，这样一个操作，可能会导致你比如说像这个第二阶段，第三阶段他对呃，他处理的这个数据占用内存的空间会越来越小，那么这个时候呃。

像AI chip这种呃，就是片上缓存比较呃，珍贵资源比较珍贵的这种芯片来说呃，实际上它后边这个缓存利用率是越来越低的，那么这个时候实际上你可以把你输入图像，按照不同的by尺维度去切分。

那么这个时候按不同的by尺维度去切分的时候，你可以形成一个跟传统的这种平均切分的，这种方式不一样的调度方式，比如说你在第一个stage阶段，你可以只处理一个图像，也就是说按照一个batch维度去切对吧。

然后后边呢你可以按照两个batch维度，四个batch维度这样去切，这样的话就可以形成一个比如像这个图里边呃，按照这个图里边呃，图片上显示的那个数字的顺序，形成的这样一个非常规的这样一个调度。

那么这种调度过程呢，有一个比较好的优势是什么呢，它可以在保证你每个阶段处理这个图像的呃，过程当中，你AI芯片的这种片上的缓存，可以能够得到最大化的利用，这样的话就会导致呃。

你后边会有一个更好的一个性能提升。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_17.png)

所以我们用利用这种方法，像在我们这个国产的这种新模计算的，这个芯片上去做了一个性能提呃，去做了一个实验，然后和传统的这种呃，就是平均切分的这种调度方法去相比较，还是有不错的性能提升的。

对这是第一部分关于定制化资源分配的事情。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_19.png)

那么第二部分呢就是呃我们叫编译，抽象之间的这种协同优化，我们呃可能比较熟悉的是呃深度学习编译呢，它本身可以嗯，比较笼统的说是一个图层和算子层的这样一个，呃编译优化的两个阶段，那么图层图层上呢。

我们给定一个呃神经网络的这个计算图之后，我们就把这个计算图呢呃划分成几个子图对吧，然后划分完子图之后，再交交给下边的这个算子层编译器，做一些循环优化，然后通过循环优化呢去做后边的这个代码生成。

但其实这种方式呢有个问题是什么呢，就是我划分完这个子图之后，我完全不管下边算子层的这个编译，是否能够编译出来，或者是说我编译这个子图，是不是需要一个比较大的编译，开销的这样一个问题，所以针对这个问题呢。

其实我们做的一个尝试呢，就是说把算子层的编译器的一些约束，反馈给图层编译器，那么比如说给定这样一个计算图，计算图的时候，这个这个图呢，我们肯定就是会做一些常规的操作，做一些这个数据流啊。

控制流的这个优化。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_21.png)

把图尽量简化一些，这是常常规操作，那么为什么我们会考虑一些把这个怎么去考虑，底层算子层的编译器，给图层算子层编译器的这样一个约束呢，比如说比如说我们这里。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_23.png)

这里边有一个比较常规的这种呃激活函数吧，这个激活函数呢呃它有两，实际上有两个操作构成的，首先第一个操作呢，至少啊至少有两个操作构成的，然后啊，第一个操作呢是它前边的这个呃求和取对数，这样一个操作。

那么后边呢是一个减法操作，那我我们知道因为这个芯片有限的，这个片上内存资源呢，我们有的时候就不是说有的时候啊，现在基本上形成了，就包括刚才这个白博士说的，这个形成了一个以tail抽象为基础的。

这样一个编译流程，你肯定是要做一个是呃是呃循环，或者是说数据的这种分块的这样一个操作，那么你做完这个操作呢，对于对于这个算子来说，你前边做做分块之后，因为前边的这个是一个求和取对数的。

这个这个这个操作是它本质上是一个规约操作，那么你做完分块之后，想把减法和前边的求和取对数操作，再把它合并起来，这种操作呢实际上是没有太大的意义的，因为你你肯定是要把前边所有的呃，就是规约的这个操作做完。

分块再合并之后，就是把他的这规约操作全规约完了之后，才能够去把后边的计算才开始执行，所以你分块完了之后，把减法的分块和求求和取对数的这个分块，想合并在一起是比较困难的，那么针对这个问题呢。

我们就把这个本身原来这是一个比较大的，我们称之为复合型的算子，把这种复合型算子呢我把它打开打开，就是说呃看更细粒度的这个图，针对这种更细粒度的这种算子呢，我们做了一些定义，按照这个定义呢，去把这个图呢。

从更小力度的层面上去做一些合并，形成一个以更小力度的算子为基础的，这样一个呃学呃，就是子图，然后把这个子图呢，嗯交给下边的这个算子层编译器去做，去做一些优化，那么做做这个事情的原因呢。

就是说我们算刚才我们说过嘛，就是把算子层的编辑器的约束，反馈给图层编译器，这也就是因为我把这个更小列的这个算子，形成的这个子图交给底层的算子层编译器之后。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_25.png)

它是能够比较好的去做呃，循环变换呢这样一个事情，那么每个扇子层的呃，每个图层的这种更小粒子的，我们称之为原子算子或者原算子这样一个事情，它形成了一个完美的这样一个循环嵌套，也就是说循环之内就是循环。

嵌套之内只有最内层是一个语句，那么这样的呃，这样的算子交给算子层编译器的时候，他才能够更好的做一些啊，更方便更容易地做一些循环合并呢，循环分块这样一个事情，那么循环合并循环分块类似这样一个优化呢。

可能之前呃无论是做这个传统编译器啊，还是说现在嗯前段时呃，之前我们做这个AI编译器，它都会比较长的，比比比较经常涉及到这个事情，那么两种方案，第一种就是像TVM这种。

我写这种schedule primitives，然后还有一种方案呢，就是我们用这个polly feature的这个技术，能够把左边的这种呃两个两个循环呢。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_27.png)

可以自动的呃合并成右边的这样一个形式，那么通过呃这样一个自动化的一个手段呢，还有前边呃图层更小，更细微的这种呃算子的这样一个融合的，这个pattern呢。

我们是能够把一些原来不能够融合的pattern呃，融合在一起，减少一些这个呃子图个数的生成，然后最后呃呃因为融合的这个能力呢，它能够减缓一些，减少一些这个数据移动的呃这个开销。

那么后再往下呢就是我们做这种算子层编译器，AKG这这个编译器，然后AKG编译器，它的这个整体流程我放在这呃，大概呢就是说我们最开始的时候，是基于这个TUM的这个0。6版本，去做的这样一个开发。

然后它会转成这个highlight i，再把highlight的RR呢，转转到我们这个POLITHO的这样一个，schedule tree的这样这样一个中间表示，那么schedule tree呢。

其实可能在MIR的这个设计里边，你也可以看到一些简化的SCHEDUTREE，这样一个表示，然后所有中间的优化，都是基于这个SCHEDUTREE的这种中间表示，去做的一个设计。

比如说我们刚才说到的这个循环分块循环合并，以及他可能会自己算一些呃调度方法啊，调度呃，调用一些底层的这个LP的这样一个过程，去能够呃自动的判定这个某些循环，某些循环维度是否能够做future。

是否能够做tell这样一些事情，那当然呢这个POLITICI里边他算的是，能不能能能不能做这些事情呃，到底哪一个tell size dota比较好，然后fusion个数到底是应该什么样的。

还有包括刚才我们说的这个呃，嗯loop arro的这样一个因子到底多大。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_29.png)

这个可能还是需要做一些auto tune的这个事情，那么AKG的这个具体流程呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_31.png)

我们在这里不展开介绍了，因为它中间它会涉及到一些。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_33.png)

就是说白了就是呃有一些数学运算的，这样一个方式呃，根本原呃就是在底层上的话，其实就可能算一些这样一个数学表达式，去做一些呃fusion台铃的这样一个适配，然后我们是针对当时呃AK呃。

就是升腾的910这个芯片，去做的这样一个code站，它里边会有一些定制化的这个优化，然后嗯会做最终的这个代码生成对呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_35.png)

这个最后我们也有一些结，结合着这个华为的这个pls boy做了一些测试啊，但这个测试结果可能都比较早了，因为现在曼斯sport本身的这个模型库。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_37.png)

可能也比这个要多得多。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_39.png)

然后第三部分呢就是我简单过一下吧，就是我们针对特定的这种算子呢，会做一些领域定制化的这种高性能的代码生成，因为你要生成比较高性能代码的话，最终还是要靠一些呃，离不开手工的这样一个实现。

那么我们针对比如说像在GPU上，我们做这种规约算子的开发的时候，前端我们是会把一些深度神经网络里边的，这种呃算子归约算子呢去做一个归类，那么这种方式呢是允许你能够呃，把你的关注点集中在三种不同的类型上。

那么怎么把归约算子转换成这三种标准型呢，我们是呃给了这样一个就是转换公式啊，就把转换这样一个公式，可以把任意的这种规约算子呢。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_41.png)

转换成这样一个形式，那么你转换完了之后，我们可以把无论是呃规约的这种并行轴啊，还是这种规约轴都可以映射到，比如说GPU的两极呃，并行抽象上通过并行抽象之后，实际上呃最重要的是你那个规约轴做了并行。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_43.png)

规约折，做完并行之后，我们在最内层去调用一些手工的这种呃，高性能的这种算算子库，这种算子库呢是呃华为的工程师自己写的，那么他可以写一个固定shape的这样一个形式，那么通过编译器呢。

你是可以通过硬件绑定，还有编呃优化的这样一个方式呢，把任意规模呢去映射到这样一个库上面，在最内层呢是通过利用这个硬件本身提供的，这种原子算子，去保证规约算子的这种运行的啊正确性，那么还有一种呢。

就是说我们面向这种，像比如说像是呃神威平台上的这种呃，矩阵城的高性能的这这种自动代码生成，那么针对呃神威平台的话，可能就是它本身呢和传统的平台的这种架构呢，不太一样。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_45.png)

是由一个组合和呃多个这个重合阵列构成的，那么它上边呢从合之间有一些通信方式，然后组合同从合之间呢也会通过一些DDR啊，去传输一些数据，那么这个时候呢你要是你是要用编译器去考虑。

你自己本身就是硬件本身的这种呃消息传递，消息传递方式的这种呃模型啊，然后通过这种方式呢，我们去也是在编译器里面自动生成，然后最内最内层呢也是一个呃呃呃，沈威的这个平台提供的这种呃。

汇编的这种micro kono，那么同样跟刚才那个工作原理是一样的，就是我们编译器要做的事情呢，就是把任意规模的这种矩阵乘呃，去映射到你这种呃固定shift的64乘，64×32的这样一个呃。

汇编的这样一个小库库的这个过程上，然后这种效果呢还是很不错的，比如说可以达到这个神威平台峰值性，理论峰值性能的90。14%，甚至是呃他们提供了一些这种手工库，可以达到这种手工库的性能提升。

大概有将近10%左右。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_47.png)

这个手工库是已经经过了非常好的优化，但当然编译器可以支撑的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_49.png)

就是说刚才我们说可以有一些定制化算子的，这个呃生成。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_51.png)

比如说它可以做可以做前向融合，也可以做互相融合，那么这个时候呢无论是前向融合还是后相融合，都相对于手工库有更好的这样一个性能提升。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_53.png)

好了，以上就是我这个做深度学习编译的一些经验。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/867b8a72f639d11a08fdd1193a65366f_55.png)

然后谢谢大家好。

# 2024北京智源大会-AI系统 - P6：多元算力下大模型并行训练框架技术与实践-敖玉龙 - 智源社区 - BV1DS411w7EG

Hello，呃，大家上午好，非常高兴跟大家分享呃，在多元算力这个时代，智源在过去语言嗯所进行的一些技术探索和呃，应用实实践。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/9317d3ce16e8e23b51094153f8f24cc6_1.png)

首先嗯想跟大家分享一个事实，其实前面的老师也讲过，就说不管大家是否承认，我们呃，其实已经身处在一个多元算力的这样一个时代，当我们的使用者面对多元算的情况下，呃，不自然的都会有各种各样的需求。

比如说诶我能不能在两款不同的芯片上，做一个高效的易购混合训练，呃，比如说诶我能不能在一款新的芯片上，实现一个端到端的一个，非常高效的这样一个训练，呃，同时就说呃算法同学可能就说。

我不管在什么样一个芯片上，你你你能不能支持任意长度的这个这样一个，任意长的一个序列长度呃，当然说更多场景，因为因为业务或者一些政策，我们不得不从一款芯片迁移到另一款芯片。

那智源作为一个模型的训练者和系统的研究者，我们呃会面临的，我们同样会面临这些呃需求，甚至说挑战，那今天呢，我主要也是从这四个场景来跟大家做一个呃，分享，让大家看看智源在这一块呃，跟我们的合作伙伴。

甚至说硬件厂商是怎么去解决这些问题的，呃，首先第一个场景呢，就是怎么实现一个高效的一个混合训练，呃一提到异构混合训练，大家首先可能想到的就是挑战，嗯的确挑战是非常多的，呃我这块话列了呃，呃分了类列。

从四个方面跟大家做一个呃分享，首先第一个挑战就是说两个不同的芯片，或者多个不同芯片做一个核池，那性能大家想一想，很自然就觉得会被最差的那个所拖累，这是因为我们不同的芯片，它的算力他能力是不一样的。

另一方面，大家的不同芯片的优化的水平也是不一样的，呃第二个挑战其实就是通信，因为嗯两款不同芯片之间，它的连接的拓扑以及连接的方式，大家往往是不一样的，呃因为呃商业的原因原因的话。

很多芯片是彼此根本不知道对方呃的呃，了解的信息是非常少的，呃第三个挑战就是说呃，呃多多芯片下的这样一个任务调度，因为传现在的大多数调度系统的话，其实还是同构芯片。

那用户呢需要自己去来自己做一个很好的配比，来使用这样一个易购的一个芯片集群，呃，最后一个挑战其实是呃，呃最终大家最关心的可能或多或少，对这个异构芯片下训练的模型效果呃，有没有保证，还存质疑呃。

这个原因很简单，因为不同的芯片它内在的架构就存在着差异，然后算子的实现，其实有个同一个算子有不同的实现呃，那怎么解决呃，经过一年多跟我们的合作伙伴，以及硬件厂商的协作，呃，今天还是非常自信的跟大家说。

通过系统化的思考以及去设计以及去实现，我们还是能实现这样一个，高效的易购混合训练呃，首先呃我们逐一的看看前面的几个挑战，例如第一个挑战就是说诶，那我们的性能会被最慢的那个拖累，那怎么解决呢。

其实我们需要从框架层面上，做一个更细粒度的任务，呃，负载的一个划分，让每个芯片能够充分发挥自己的潜能，OK就是说芯片它要物尽其用吧，呃不要被最慢的那个去拖累，那第二个呃挑战就是说呃通信，那怎么办呢。

其实我们在并行策略层面上，我们可以要求我们只进行节点之间的通信，那节点之间的通信呢，我们其实大家都是IB和rocky，这种嗯非常标准的这样一个协议，OK这块其实也是可以解决的。

当然这也依赖于厂商的一个呃支持和兼容，嗯第三个就是说挑战，就是说关于多芯片这样一个任务调度的问题，其实这块的话我觉得是一个呃模式的一个转变，就是说我们需要从呃以芯片为中心的这样一个，调度策略。

逐渐迁移到以算力透明这样一个呃调度，就是我觉得在未来的话就是大家作为调度用户，其实他不关心最好是不关心我要用什么芯片，我只是按算力付费而已，K我们这样话，会对我们的系统的利用率有一个极大的提升。

那第四个呃挑战就是说呃模型的效果，呃这块的话其实我个人的一个观点就是说，如果两块两款芯片做一个合池训练呃，效果呃无法达到同构的预期，不管是loss还是最终效果，我觉得可能其中有一有一款芯片是有问题的。

那最好的解决方案就去修这个问题，K甚至说，我觉得易购训练是检验芯片的一个不错的手段，对K那左右边的那个图的话，其实是一个大概的一个呃解决方案图了，就是说我们其实神经网络有它的特性，它有很多层。

当我们训练单一任务时候，我们会根据算力和内存的啊，这个约束去做一个切分，让每一款芯片去算自己的那一部分的子任务，那在最下面的层面上呃，我们跨集群的通讯主要存在呃，如节点之间OK大概是这样一个思路。

那通过这个系统化化的思考和设计，以及实现的话，我们是能够解决这样一个异构训练的问题，呃，但下一步就是说怎么去把这个性能做一个呃，更大的一个提升，就是说真正实现异构高效的训练。

呃其实这块的话我们一直跟天数智星呃，做深度的合作，在易购并行策略下已经呃，到现在已经经经过了三版的迭代，从最开始的时候，我们实现的是一个ego的数据并行，呃这个其实也比较直观，就是说呃我按照芯片的能力。

我没呃新能力大的，我呃我处理的数据多，能力少的，我处理的数据少，但这个策略的话也有一些问题，首先它依赖于呃，首先数据层面上这样一个负载切分，它是一个粗粒度的，很难做到一个非常精细的控制。

第二个就是说它需要不同芯片之间做or reduce，那做个通信的就比较呃了解，就是说不跨芯片的ORRODUCE尤其高效，ORORRODUCE还是非常之困难的，那我们就在这基础上又呃引进到第二代。

就是说我们通过发现就是说大模型时代，尤其AI这个方向，深度学习有很多的operator和层，那么我们就按照算力去呃进行层的一个切分，然后算力大的我就处理的层数会多，算力小的我处理层数会少一些。

那呃这也有两个好处，首先就是说这个呃，任呃切分的这个力度会更细一些呃，第二个好处就是说哎哎我现在只需要哦，p to p的通信，那这个难度就是尤其是跨节点的p to p通信，这个难度一下子就降下来了。

因为我们现在训大模型，基本上是用混合并行的策略，所以说尤其流水线，现在的巴博率已经通过各种手段降得比较低了，那呃但是这也有一个问题，大家可以看到，就是说呃，我们不同的芯片之间的这样一个呃模型并行的。

这个维度，其实大家要求一致的，这带来问题，就说一方面就是说要求这个异构混讯的，这样一个芯片的配比，大家还是要符合一些约束的，第二个就是说它还是呃没有进一步释放，这样一个灵活性以及优化的空间。

那我们现在演进到第三代，第三代的话就是说诶我们不要求这个模型并行，这个维度大家是一致的，我只既可以由多变少，也可以由少变多，那这样的话，我们进一步的相当于释放这样一个调优的空间。

呃第一个可能是大家比较关心的性能问题，就是说呃呃我们是在不同规模不同的呃，配比有不同代际之间的，也有跨架构之间的，就后面三个是呃三条线的话，就是呃有两个是跟天数跟A800，有一个是木希是跟A800。

就是说整体上我们通过这样一个性能的一个，统计和分析发现，就是说我们还是异构混讯，是呃能达到一个很高的水平，尤其是呃跨架构的这样一个训练也是很不错的，大家可以看到其中还有一些超过百分之百的，这样一个性能。

这是因为我们把两个小集群，合成一个大集群之后哦，我们还是解锁了一些优化空间啊，比如说因为我们有更好的并行策略了，这样的话我们可以使用更大的bio micro呃，By size。

也也可以去关掉一些重计算嗯，然后最后呢，就是说最近我们呃做这样一个更灵活的，这样一个TP易购这个呃策略呃，其实我们可以在这个基础上进一步能提升，大概30%左右，模型效果当然也这个是最重要的一个事情。

就我们还是在呃从加载相同的checkpoint，持续训练了一段时间呃，在真实的数据集，也是我们flag evo这个平台上做了一些评测，呃，目前看的话就是说其实地府还是非常之小的。

呃那当然说我们也没法去完全规避这个呃差异，因为是首先我们的拆矿业呢，是从重构重构拿过来的，但是我们要在异构上去训呃，第二个就是说呃由由于不同的芯片，它能力不同，我们涉及到呃参数以及优化器状态一个重切分。

导致那个随机种子，其实我以及随机状态我们很难保持一致K呃，但总总体来说结果还是不错的，第二个就是说呃，怎么在一款芯片上实现一个高效的端到端训练，从我们实际的一个，尤其在一个呃项目周期比较紧急的情况下。

从我们实际的经验来说，还是从呃算法层到框架层到硬件，从一个协同设计是比较高效的，呃首先从算法层面上，我们呃的语言团队给我们提供了一个，两阶段的这样一个训练方式。

我们从一个已有的DOS的7B开始进行skill up，扩展到哦16B这样一个呃规模邓46B，然后训练一段时间之后，我们进一步的扩展成为呃，816B这样一个千亿的这样一个，MOE模型。

然后相比如从通讯来说，我们实现了一个四倍这样一个呃处理的速度呃，从框架层面上，其实大家做过真实训练的都知道，往往训练会持续以呃呃一到两个月，甚至说更长，那在这个过程中，稳定性是也需要解决的问题。

在这块的话，我们跟慕希的团队一起去合作，首先在提供了一个呃node level的一个呃容错，以及异步的一个拆幻境的保存，那node level的这样一个容错，相比于是传统来说呃。

大家会呃当一个节点坏了之后，我们会把整个任务Q掉，然后去重新relaunch这个job，那对于呃node live来说，我们只需要去替换那个节点，然后in place的做一个training的一个重启。

这个会加速整个这样一个启动的呃时间，OK然后异步呢也是比较简单，在以往的同步的这样一个场景下，我们是也会有一个呃worker进程的话，它去把GPU的呃搬到CPU最后去落盘，那异步的话其实我们有两个进程。

同时有一个agent，特这样一个进程，在后台的话持续的做一个异步的缓存呃，通过这种方式的话，其实我们来落盘上，能够实现300倍的最左右的加速，然后漏的会实现一个三倍的这样一个性能提升。

当然说最后还是这样一个呃，性能也是很关键的，这块话是呃，木希的话给我们提供了一个千卡这样一个呃，其实是128卡，这样1280卡这样一个集群高效集群，那呃由于木希它的产品是一个。

跟KDA比较兼容的这样一个产品，所以说呃我们在flag skill这个框架，适配这一块的话，其实还是非常快速的呃，呃再结合木希这样一个深度的帮忙，优化的情况下，我们实际最后实现一个很好的一个呃。

性能的提升，这是一些呃比较呃，个人觉得还是比较有前景的这样一个呃结果，首先training north我们可以看到两阶段，上面的是7B的人，后面是16b dos16B，最后一个是MOE16B。

当然我们现在过的token比较少，但整体的train loss的话是按照呃，是非常符合预期的，呃当然说因为智源的话有不同的算力嘛，我们同样也是在呃在木西的集群和在呃。

英伟达的集群上做了一个skin的这样一个对比，然后呃可以看到就说呃，这里面配置是完全一样的，框架也是一样的，可以看到木希其实基本上能维持在，90%以上这样一个扩展效率，那我们的NV的集群的话。

其实反而不如木希呃，这说明什么呃，这给我们一个呃很重要的一个警示，就是说在大模型时代，尤其我们下一步要呃，实现万卡甚至10万卡这样训练，那单芯片的性能至关重要，但实际上下一个更要解决的问题。

就是说怎么能够实现高效的互联，就是我们最后一个talk，好像就是讲这个主题的OK呃，呃，第三个场景就是说呃常常sequence这样一个训练，这个就是说白了我们在实际的业务过程中。

算法团队它其实它不care你是在哪个芯片上训它，指向你给他一个确定的答复，你能不能给我实现任意长度，这样一个序列的这样训练呃，尤其是在多模态这个呃场景下，我们可以发现相比于语言模型。

一般有千倍的这样一个序列长度，需求甚至说更长，因为我们跟呃算法团队聊聊聊过，就是说这个越长越好，反正视频的话他这个token数会很大的，那当然长序列情况下，带来一个极大的这样一个，内存的这样一个需求呃。

呃我们可以看到主要上面这个传送门结构呃，其实有两个方面，一个是sequence的平方这样一个复杂度，另一个其实就是S乘以H，这个复杂度很很早的情况下，我们一般是关注平方，但是随着呃长序列这个场景下。

我们觉得就是线性这个复杂度，也需要我们去解决的，呃这块有个具体的例子，就是说如果我们一个tensor，如果呃by size等于一的情况下，我们在256K的情况下呃，就BF16了。

就大概需要这么大的一个存储空间，远远超过一个单芯片，那即使在线性的这个配比情况下，我们如果hen size是1024的话，也是需要一个很大的一个呃内存需求，那怎么解决，其实现在系统这个领域的话。

已经有不错的一个解决方案了，我们只需要把它给结合起来进行一个呃，呃使用呃，第一个就是flash attention，它的话就是说解决attention这个S平方，这个复杂度，通过分块能把这个复杂度呃。

就内存的复杂度呃，约束在块这个level o k，那但是这还是不够的，因为我们的经验告诉我们，它只能达到百K这个量级，但是我们往往还需要兆以上这个sex的长度。

那下一个就是可以采用像real tention，它会把呃后面的那个FI分层，进一步做一个分块化，同时会利用分布式的技术，就是说我们每一个设备只需要呃处理一个呃，呃KV呃。

q query以及key和value这样一个块，然后在是呃过程中我们通过通讯来做，下面看红色那个圈，就是呃就是一个rain的这样环，就是我当前呃设备计算过程中。

我会从上一个设备去拿到我需要的key和value，同时我把我自己的K和value传到下一个设备，形成一个环，然后在这个计算过程中，可以进行一个计算通信的一个呃隐藏呃，这是一个初步的一个呃性能结果。

就跟大家分享一下，就是说左图是一个我们在呃相同的token数，然后不断的去做一个4K到一兆，这样一个sequence ence，ENCE的这样一个呃实践实验呃，那个橘红色的是呃倍数。

就sequence成长倍数，我们可以发现就是说时间的确在增长，但是它也不是按照sequence这个倍数去增增加的，当然这里面还有呃原因还是比较复杂的，呃，主要是其实我们每一个长度，其实并行策略也不一定。

和优化策略不并不是一样的，因为我们是经过专家去调优过的，然后呃最后一个是一个右边那个是一个breakdown，就是说呃其实分为计算仿存以及数据的加载呃，通计算通信和数据的加载。

我们可以看到其实大头还是在呃计算这块，这说明就是说通过分布式这样一个计算和呃，通信的重叠，我们呃是能够把通信做一个很好的隐藏，最后一个场景就是说呃，就说怎么做到从一个算呃。

从一个当前我们业务在的一个芯片，迁移到一个新的芯片上，呃左图是一个传统大家一个做法，就是说我当迁移到一款芯片，我需要考虑框架平台呃，但这往往带来一个开发和学习成本，然后从一块芯片到另一个芯片。

我们大家知道这需不管是并行还是优化，都需要专家这样一个经验，那我们智源想提供的一个解决方案，就是于呃，就右图所展示的就是说，首先在平台和框架这个层面上，我们希望就是能够呃支持多种芯片，这样的话。

用户不再需要任何的一个学习和迁移的代码，迁移的成本，那在最下面的话，我们从一款芯片迁移到另一款芯片，通过这样一个自动调优的工具来帮助用户呃，自动的去选择一个高效的呃并行和优化策略。

呃这是我们整个ta呃的一个呃架构图，也快速跟大家做一个分享，首先就是说我们用户测去提供了模型的信息，然后平台提供的呃cluster的信息，然后我们会构建一个搜索空间，然后基于这个搜索空间。

我们会联合的呃进行一些减值优化，然后呃优先的选择一个比较最好的一个，候选的一个候选集，然后从中挑一个进行一个生成可执行的配置，然后做呃送入到这个SMAESTIMATOR这块的话。

其实也是我们跟厂商合作的一个点，一方面我们可以实际的profiling，另一方面就是由厂商提供他们硬件的一些参数，以及他们的cost model供给我们去调用，然后最后去呃去做这样一个呃。

性能结果的评估会出，会通过recorder这个模块去存存下来，同时的话呃大家可以看下这里面有个online feedback，就是我们整个中间这个颜色，其实其实是一个loop。

就是说我们实时的通过历史的信息做一个减脂，最后会快速选择这样最优的这样一个呃，候选去执行呃，这是我们九鼎平台实际上线的一个案例，就是说大家可以看到呃，随着时间的呃增长，我们的这个性能会逐步的做一个提升。

OK呃关于tuning这块，我们有一些结果也可以跟大家做一个分享，就是我们在A800，然后呃木西呃天数的BI呃，呃150以及木希的C500上做做了一个实验，那我们可以发现就是说基本上都可以呃。

取得一个比较好的，不管是在不同的模型size和硬件上，都能取到一个好的加速比，呃，当然这里面7B是因为那个呃，我们那个专家优化太好了，所以说呃呃提升空间稍微差一点，呃，最大的话。

其实我们可以达到23%的这样一个，性能提升，这里面的配置都是我们跟厂商，包括我们自己呃专家经验所选出，我们认为最好的去做对比的，所以说在这个基础上，我们还有这么大的提升嗯。

那通过我们的前面所讲到的一些减值的算法，比如说呃基于历史的，基于memory model的，其实我们能把这个搜索空间，压缩到84%以上，这个就是说在实际上线的过程中，对用户的体验也是非常好的。

最后放在一起呃，呃就是说呃，我们还是想非常隆重的向大家介绍一下，就是我们智源这个开源的框架叫flag skill呃，呃经过一年多跟合作伙伴及厂商的协作，我们最近是有一个新的价格升升级。

核心来说其实就是呃两两方面，首先分为前端跟后端，前端的话，我们希望给用户提供一个统一的接口呃，提供类似于tuning，然后自动的预估以及自动的容错这种功能，而后端的话我们希望可选择的多个执行器。

包括我们支持呃，大家比较熟悉的开源的像max tro m v m，以及我们自研的flag skill call，同时我们也可以对多款底层的多个算子库，进行一个选择的使用。

包括我们智源资源的flag germs，最近也做了一个联挑打通，像flash tender，flash attention等一等，包括厂商提供的呃一些引擎，那通过这种方式的话，我们能够呃做一个解耦。

在前端的话，其实我们是呃实验管理啊，配置管理基于arm的是非常方便的，那也能跟平台做一个平台的workflow做很好集成，前面那个九鼎的例子就是一个很好的体现，同时会提供用户一些自动化工具，OK呃。

那对于后端的话，首先是我们是百分之百兼容，就是我们已有的这些开源库，因为呃用户的迁移其实还是很大的代价的，呃再一个就是我们会增加像异构混讯呀，包括ti啊这些我们自自定义的一些组件。

然后的话就是说实现这个无缝的芯片迁移，呃，目前的话就是flag skill已经实现了，就是呃来自八个厂商的一个适配，然后呃在支援内外已经实现了十家这样一个呃，完整的这样一个预训练啊。

那新呃我们最近也会发做一个发版，然后在这个发版中，我们会把上面所有的功能给大家做一个开源，同时的话也会增强我们的CI，CD这样一个能力呃，最后是一个总结和展望呃，第一个就是说呃。

我觉得多元多元算力已经成为一个趋势，那呃如何呃，其实这也给我们系统这个领域带来更多的机会，K呃，然后ego训练包括在一款呃，新的芯片上进行一个端导的训练哦，我觉得通过系统的方式。

以及能够非常的呃落地和实用了呃，自动化它不仅仅能带来用户的一个实验提升，它也是多元算力，这个时代上一个关键的一个因素，K呃，未来工作化，一方面我们会构建一个统一的这样一个通信库。

会实现一个端到端的这样一个异构训练呃，第二个就是说我们会在呃，持续在长序列以及MOE这种呃架构下，去做更多的并行策略以及优化的一个呃创新呃，最后也是希望跟我们的合作伙伴厂商，包括新嗯，约新的呃，呃。

社区朋友，一起构建一个更广泛的flag skill社区，OK谢谢好。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/9317d3ce16e8e23b51094153f8f24cc6_3.png)

# 2024北京智源大会-AI系统 - P7：大模型高效可扩展并行策略研究-李士刚 - 智源社区 - BV1DS411w7EG

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_0.png)

好像他这个怎么往上翻呢，这个，好啊，感谢林老师的介绍，呃，大家好呃，我今天汇报的这个内容还是大模型呃，并行策略，当然在这个领域已经有很多优化工作了，已经有非常多的工作了。

然后我今天是呃分享几个探索的点呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_2.png)

现在大模型的主流架构呢呃还是transformer，虽然呃也有一些新架构，逐渐也也也出来了，但是都呃还没有广泛应用啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_4.png)

但是主流架构还是transformer，但是随着transformer，大模型这个规模的越来越大，呃而且模型的这个呃能力越来越强，它的并行策略呢也越来越复杂，它通常是一个涵盖多维度。

并行的一个复杂的组合，因此呢，这对我们系统的开发人员和，系带那个性能优化人员来说。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_6.png)

就带来一个沉重的负担，所以我们需要开发这种分布式，自动运行的框架呃，从而可以自动的生成其最优的一个并行策略，以及代码的实现，但是在这一个领域上呃，其实已经有很多工作了，前面呃袁老师也介绍过。

然后前面其他几位老师也已经介绍过，包括奥运龙老师也介绍过，有有这这很多相关的工作，但是在这一领域还有一些挑战需要解决，第一个就是说在已有的这些并行策略里边，它的呃通信开销和内存开销都不是非常的高效。

尤其是在操作服并行这个维度，那这里边是以麦克TRL买呃，麦克TRLM为例，它是需要将整个输入数据，在多个进程上进行复制呃，之后进行本地计算，最后要拿到最终结果呢。

需要在整个输出数据上做一个ORREDUCE操作，这种实现方法虽然简单，但是它的呃内存开销，还有通信开销都是比较高的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_8.png)

另外一个方面，就是说已有的这个深度学习框架呢，它对分布式张量这个描述能力是有限的呃，因此它没有办法很高效的来描述，比方说2。5D或者3D分布式矩阵乘法，这种通信效率更高的这些并行算法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_10.png)

因此这里边我们是基于就是和袁近辉老师合作，基于他的前一个工作，就是one flow中的SBP框架，提出了一个就是自动的分布式并行框架，Auto d d l s p p，刚才袁老师已经介绍过了呃。

它包含三个分布式张量的状态，s split b是复制，然后P呢是partial sum爬虫SAM，这相是相对于已有的这些呃，大部分并行框架来说是新增加的一个状态，就是不分核这个中间结果。

因此呢他需要将这种中间结果进行一个全规约，ORREDUCE操作才能拿到最终的这个结果向量呃，结果张亮，但是这个ORREDUCE什么时候调用，因为我们呃定义了这种中间中间状态。

所以它是可以根据我们需求呃来呃，自定义，来来决定我到底什么时候来调这个ORREDUCE，这就跟我们增加了就是并行算法的一些灵活性，我们可以更灵活的来设计3D分布式矩阵乘法，这样的并行算法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_12.png)

接下来来看就是，我们如何利用SBP模型来实现一个呃，3D的分布式矩阵乘法，这里边是以A乘B矩阵等于C矩阵为例啊，在八张卡上面，那A矩阵的话，A矩阵它的划分模式是S02S12B二，它表示对A矩阵的零维度。

进行两个进程上的划分，一维度呢也是两个进程上划分B2，它表示在划分之后的张量，它是复制到两个进程上，那对于第二个输入矩阵B，它的分布方式和矩阵A是一样的，然后我们进行本地的这个矩阵乘法运算，算完之后呢。

他得到的是一个中间结果矩阵，它分布方式是S02S12P二，其中这个P2就说我们这个paral sum是中间，结果呢是分布在两个进程上的，之后还需要调用一个呃规约操作，Reduce scatter。

得到最终的结果矩阵C这个C矩阵呢，它的最终结果也是分布在八张卡上的，之后还可以接后续的这种3D分布矩阵乘法呃，这种3D分布式矩阵乘法，它相比于更常见的就说1D2D分布矩阵乘法，在通信开销上它是更低的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_14.png)

那接下来来看就是auto d d l整体的一个呃，这个模模型框架的流程呃，首先对于给定的一个神经网络，我们首先要枚举每一个操作符，它可能的SBP配置，从而构建整个端到端的一个并行策略搜索空间。

然后我们对这个通信性能进行建模呃，这里面我们主要就考虑了两种网络架构，一个是这种同构的呃，还有一种就是多机多卡的这种异构网络架构，然后用简单的啊，延迟带宽模型对通信进行建模呃。

然后我们要对这个并行策略空间进行搜索，因为我们在第一步构建并行策略空间中发现呃，可以看到啊，就是这个并行策略空间的大小，是随操作符数量而指数级增长的，因此对于这个深度神经网络来说。

它的搜索空间是非常巨大的，因此我们这里边是采用了一种定制化的呃，坐标下降启发式搜索算法，它可以对这个搜索空间里边的多个区域，并行搜索，从而可以有效的避免陷入局部的最优解，可以更快的搜索得到全局最优。

或者是进最优的这个并行策略，那这里面值得注意的一点就是，我们最终搜索到的这个并行策略，它有可能需要在不相邻的操作符之间，插入这种数据重分步操作呃。

在one follow里边它是叫boxing这么一个操作呃，这是为了满足输入和输出数据之间的，这个依赖关系呃，最后我们利用这个one flow框架，生成最终的代码实现呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_16.png)

接下来我们来看auto DDR能搜索出什么，不一样的并行策略，我们这里边还是以多头注意力，mari had attention算子为例，那左边这幅图就是我们常见的，就Mac trl m m里边的操作符。

并行的一个实现方式呃，那如果再加上数据并行，因为数据并行维度，基本是我们嗯必不可少的一个并行维度啊，加上数据并行的话，麦克TRLLM中的操作符并行，可以被看作是一个2D的分布式矩阵乘法。

那右边是auto d d l搜索得到的一个并行策略，它是与IMMECTRLLM最大的不同，就是说我输入数据呃，是在多个进程上进行划分的，而并不是是I呃，Mac on里边这种整个输入数据的一个复制之后。

在划分之后的张量上，我们进行一系列本地计算，中间要穿插两个ORGAZER操作，和两个reduce scatter操作呃，从而得到最终的这个呃结果向量，那与麦克TRLMM相比呢。

呃就是auto d a auto d d l搜索得到这个并行策略，它增加了通信次数，但是总体的通信量是显著降低的，就这里边是把通信量从ON的平方，除以P的12次方，降低到了ON平方除以P的23次方。

同时呢对于输入输出数据，它的内存占用量也有同比例的一个降低，所以总体而言就是我们auto地点搜呃，搜索得到的这个并行策略，它具有更好的并行可扩展性哦，接下来来看序列并行。

因为长序列是我们大模型训练推理中，一个重要的问题呃，这里边我们还是以mari had attention为例，用矩阵乘法的形式展示出呃，它在auto d d l里边的一个具体实现过程呃。

可以看到就是我们对一个完整的序列，是它是划分到多个进程上的，或者多张卡上的，之后我们再进行自注意力计算的时候，就需要实现序列并行呃，那当前序列并行的实现方式主要有两种。

一种就是基于这种rain or gather，环形gather的实现方式呃，它是将K矩阵和微矩阵的完整序列拿到本地，然后和本地的呃Q矩阵进行一个本地计算呃，但是这个rain gather的话。

它可以实现成一种分布的点到点通信的方式，也就是说在进行当前块计算，同时可以同时传输下一个下一块数据，这样一来可以实现一个比较好的计算，通信的隐藏，那另外一种实现方法就是呃deep speed。

尤里西斯里边这种，它是针对QKV3个矩阵，分别调用一个out to all通信，但是这个auto all的话呃，调完调用完out to all之后，就是每个进程就可以拿到，完整的这个输入序列了之后。

再在本地呃，执行类似于flash attention这样的本地计算。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_18.png)

那总体而言这两种方法它是各有优缺点的，就第一种就run gather的话，它的通信量是非常高的呃，因为all gather的通信量是和输入数据的总量，是成正比的，那随着进程数的增加。

它并不会降低这个通信总量，但是呢它可以实现一个比较好的计算，通信隐藏呃，尤里西斯里面的奥托奥这个通信，它的通信量是较低的，因为auto all的通信，它只和进程的本地数据量成正比，所以随着进程数的增加。

也就是序列并行度的一个增加，它的通信量是成比例下降的，但是这种方法的话，它不太容易实现计算通信的隐藏，而且尤里西斯里边这种它的并行度，序列并行的并行度是受head的数量限制的，因此我们这里边。

也是将两种方法进行了一个融合呃，然后提出一种混合序列并行策略呃，简单来说就是在一个维度上进行auto all通信，然后在另外一个维度上进行run gather，从而拿到一个完整的序列。

那我们呃做了一个初步的性能测试啊，就在呃八张910B升腾卡上做了，对lama two的7B进行了一个测试，可以看到对于32K64K，128K等不同的序列长度，这种混合序列并行策略。

相比于单单独使用run algazer或者是尤里西斯。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_20.png)

都获得了一个明显的吞吐率的提升，除此之外呢，auto d d l还它还可以支持更灵活的，通信拓扑的一个变换呃，因为在已有的这个工作里边已经证明，比方说对于我们矩形形状的这种矩阵乘法。

它需要在矩阵更大的维度上分布更多的进程，才可以得到一个全局最优的一个通呃，呃划分策略，那我们这里边，auto d d l也是支持这种灵活的，通行拓扑的变换呃，可以达到一个端到端的大模型的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_22.png)

更低的一个通信开销呃，这里边就是还是以MARTYI的attention为例，因为在深呃深度神经网络里边，或者是大模型里边，也会经常遇到这种矩形形状的矩阵运算嗯，我以MARI的attention为例。

在第一个KOKV的线性映射层，它的模型参数矩阵是一个N乘三，N的一个矩形矩阵，那auto DDR在64张卡上，搜索得到的最优通讯拓扑是2×8乘四，也就是说我在3N的这个更大的维度上。

划分更多的一个进程之后呢，我们进行本地的这种呃自助力，自注意力计算，自注意力计算完之后，要接下一个MLP层，而下一个MLP层的话，它的模型参数是一个N乘N的方阵，那对于这个N乘N方阵来说。

它的最优的通信拓扑是4×4乘四，是一种更均衡的划分方式，那我们呃是在中间插入一个reduce scatter操作，和gazer操作，就可以灵活地将通信拓扑从4×8乘二，转化为4×4乘四。

那auto DDR，通过支持这种灵活的通信拓扑的一个变换呃，可以实现端到端的一个更低的呃通信开销，这在已有的并行框架里边是无法做到的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_24.png)

呃接下来来看性能测试，就是呃这四幅图是对，是在四个不同的神经网络上啊，对我们都呃就是auto DDR里边采用的梯度下呃，坐标下降的启发式搜索算法的，它的效率进行了个测试，可以看到相比于就是随机随机搜索。

以及呃flex flow里面使用的MCMC搜索方法，呃，我们采用的这种启发式搜索方法，可以在更短的时间内，搜索得到最优的一个并行策略，同时值得注意的一点就是，我们这里边是利用了前面我们构建的性能模型。

对各个并行策略的性能进行评估，而不需要在实际的机器上进行验证，所以整体的这个搜索过程，只要在笔记本或者台式机上，就可以很快的完成呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_26.png)

之后是呃在P4dent的超级计算机上，对几种不同的神经网络的最终性能，进行了一个对比呃，可以看到就是auto d d l搜索得到的最优的，这个病情策略，相比于我们手动优化的这种高度优化的，手工实现来说。

仍然获得显著的一个性能提升，呃，以transformer为例啊，就是相比于这种配置最优的Mac trl m的这个呃，并行策略配置，Auto d d l，仍然可以获得30%的一个吞吐率提升。

并且随着进程数的增加或者卡数的增加，auto DDR的它的性能优势也更加明显，这也说明就是，auto d d d l搜索得到这个并行策略，它的并行可扩展性更好呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_28.png)

接下来是流水线并行维度，前面在敖一龙老师报告里面也说，就说我们流水线并行，它主要面临的问题是呃空泡问题，当然现在也有大量的工作对流水线空泡呃，进行了一呃一定的缓解或者是解决呃。

在我们前期工作中也提出一种，就是双向流水线并行机制，凯MIRR，它可以将两个方向的流水线进行融合，从而大幅降低流水线空泡的比例，但是无论是呃凯米尔，还是后续出现的一系列流水线并行的工作。

他都没有办法完全消除空泡，或者说它消除空泡的话是有一定代价的，因此我们这里边是尝试另外一个技术路线，就是能不能在流水线空泡中，填充一些有用的计算，从而来提升硬件的一个利用率。

呃具体来说我们是在流水线空泡中呃，填入二阶优化方法的一个计算负载，从而来呃加快这个端到端的收敛速率。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_30.png)

那首先简单介绍下什么是二阶优化方法，二阶优化方法就是用二阶梯度，而二阶导数也就是梯度的梯度来进行模型更新，那相比于一阶方法而言，就二阶方法它拥有更多的一个优化信息，因此它可以将多个训练内造步。

多步兵为一步，然后进行模型的更新，因此它可以相比于一阶方法而言，可以大幅提升模型的一个收敛速率，那从数学公式上来说，二阶方法和一阶方法的不同，就是在一阶梯度的基础上，要乘以一个曲率矩阵的逆。

对一阶梯度进行一个预条件转换呃，那这个曲力矩阵呢根据不同的二阶方法，它的定义也是不一样的，比方说牛顿法里边，它的曲率矩阵就是黑森矩阵呃，自然梯度法里边，它的曲率矩阵就是fisher信息矩阵。

那我们这个工作主要是针对这个自然梯度法的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_32.png)

那无论是自然梯度法还是牛顿法，这个曲率矩阵的计算，还有曲力矩阵求逆的计算，它的总体的计算开销是OP的三次方，这里面P是模型参数量，这对于我们大模型来说是无法接受的，实际的性能测试表明，虽然就是二阶方法。

它可以大大提升这个模型的收敛速率，但是它单步的执行时间太高了，导致呃在端到端的一个训练时间中，甚至是长于这种传统的一阶优化方法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_34.png)

因此就是有工作就提出呃，二阶优化方法的近似求解方法呃，KFC它是主要是针对自然梯度法的，当然还有其他一系列，比方闪闪扑啊这些其他的近似求解方法，那我们这里边是以这个KFC为例，KFC的话。

它是利用了克尼克因式分解的，一个非常好的性质呃，它可以将这种大矩阵近似为两个小矩阵的，克洛尼克乘积之后，对这个大矩阵进行求逆，它就转化为两个小矩阵克洛尼克乘积的求逆。

它同时又等于先求逆再进行克洛尼克乘积，从而它可以避免在大矩阵上进行一个，求逆的运算，从而它可以将二阶优化方法的计算，复杂度大大降低，通过实际性能测试表明，它可以将端到端的一个训练时间呃。

相比于这个一些方法有一个显著的降低呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_36.png)

接下来简单看一下，就是KFC的它的一个计算过程，首先它要计算两个曲率矩阵，就是A和B，其中A曲率矩阵的话，它是等于输入数据乘以输入数据的转置，B矩阵的话是呃误差矩阵乘以误差矩阵的转置。

之后要对这两个曲率矩阵进行求逆，但这里边AB两个矩阵是比较小的矩阵，对其进行求逆操作，它的计算开销也也也并不高，最后的话是呃将这个AB矩阵的逆呃，与一阶梯度相乘，完成预条件转换。

这是KFC的一个就是总体的一个计算的负载。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_38.png)

之后，我们来看如何将KFC计算负载，填充到流水线空泡里边，我们这里边还是以最简单的g pp，这个流水线方案为例啊，呃首先我们就是对呃，流水线里边正向传播和反向传播计算时间。

以及流水线空泡的一个占用时间进行一个测量，然后是对二阶计算负载，包括曲力矩阵，曲曲力矩阵求逆的运算呃，它的计算时间进行一个测量之后，我们把二阶计算负载填充到流水线空泡中。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_40.png)

它的总体的一个填充原则，就是说我们只利用流水线空泡呃，而不去影响原来的流水线，或者尽量不去影响原来流水线的一个，总体执行时间，呃，也就意味着我们能填充多少就填充多少，这样达到的一个效果。

就是说我们并不能保证在每个训练迭代部，都对二阶的这个曲率矩阵进行一次更新，实际的一个优化效果，就是说我们可以在大概每一到两部，能够更新一次曲率矩阵，但是这个呃这这种效果的话。

相比于已有的这种二阶优化方法，也已经是大大提升了，这个曲率矩阵的一个更新频率，从而可以使我们二阶优化的话，达到更好的一个收敛效果，呃最后是在就是bert large这个呃预训练。

端到端的预训练上做了一个性能测试，可以看到就说呃，融合了这种二阶优化方法的，凯米瑞尔流水线优呃，这个并行方案和凯米尔，是采用这种传统的一阶优化方法，它可以将bart large这个端到端的预训练时间。

降低30%以上，好最后是对呃今天工作的一个简单总结呃，首先我们基于3D分布式矩阵乘法模型，对auto d d l里边搜索得到的并行策略，进行了一个简单阐述，呃，之后是介绍了就是混合的序列并行策略。

以及二阶方法和流水线并行的一个融合，那在接下来的工作中，就是大模型在互联网络拓扑，还有高性能推理方面仍然面临一系列问题，在前边就是袁老师也提到，就说尤其是在呃这种大模型推理这个方面。

除了就是我们profile第一个阶段，后边我们逐个token生成的时候，由于我们和k v cash结合使用，它一次生成一个token呃，那它对应的一个算子，其实是GEMV的一个算子，所以它经过了一个。

就是说从GEMM往GEMV的这么一个转换，从计算相当于是一个计算密集型的，一个计算负载，转化到访存密集型的一个负载，所以这里边还有很多问题，需要我们进一步解决好。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bfafde344a2efb6eb28a3dda582109c1_42.png)

以上是我报告的呃，全部内容，感谢大家好。

# 2024北京智源大会-AI系统 - P8：RISC-V+AI算力系统软件栈建设-谢 涛 - 智源社区 - BV1DS411w7EG

谢谢林院长的介绍，我尽量这个赶一下啊，希望能够在20分钟之内啊，这个结束好。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/97d2e4534c83611e28d44322120ea0a5_1.png)

那么大背景我不需要多说了啊，就是现在当然我们这一个呃，这个比较高性能的这些AAI算力的芯片呃，很难能够购买到啊，我们现在很大程度是依赖于我们国产的，AI芯片啊的发展啊，那这个啊前面几个报告都有提到了。

其实性能啊各方面的都应该还是还是不错的啊，但是呢其实很大的一个我们要去应对的是啊，这个qua的生软件生态的壁垒啊，首先就是对KA这个生态它是2006年啊，英伟达开始发起啊，在高校啊等等啊。

各方面卷生态投入也很大啊，那么呃特别是近近几年，那么库的开发者也从这个啊180万啊，22020年以来激增到是450万啊，这个数据是今年年初啊一个新闻报道。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/97d2e4534c83611e28d44322120ea0a5_3.png)

这个啊报道出来的那一个我们问的问题，就是诶为什么要450万的KDA开发者呢，啊那KDA开发者他指的是说使用KA的接口啊，以及哭打的这个C扩展的啊，这样的一个语言来去写啊，这一个程序。

那么在我们这个AI时代，那么就是AI的算子是用KDA啊来去写的，那呃这个由于这个呃长期的生态建设啊，他积累了这么多的人，大家会想诶，那也许我们就就让英伟达的，特别厉害的工程师对吧。

或者是说这个这个各个AI芯片公司的，自己的这个软件工程师，去写出特别优化的算子实现就好了，once for all啊，我们就这个广大用户去用就好了，但是其实啊在特别是当下大模型时代。

我们很难去once for all去使用啊，这么一个很通用的啊算子实现来，去支撑在各种场景下的这种啊部署的大模型啊，那么因为我们在部署的大模型，它是很首先一个特别在云上的。

它对于算法的啊这个算力的优化啊，即使是他的这个呃，是个位数的百分比的这么一个提升，它仍然绝对数给大家带来的这个节省的开销，体量还是很大的，在钱数上，所以呢大家很很愿意去再进一步的去优化啊。

那么我们把它叫极致的算力优化，那这一个啊这个很难，winds for是因为它上下文敏感啊，就是我可能针对特定的芯片，或者是特定的哪一款芯片，哪一代芯片及同一个公司的。

我可能都要去调整你这一个啊算子实现来去，更好的去发挥它的这个算力的优势啊，另外一个上下文呢是上上层的，这一个用户的输入啊，你使用的场景work low是怎么样，那也会影响到我使用哪一个算子实现。

它能够提供极致的算力优化的效果，所以这也是为什么我们有这个450万的，这样的扩大开发者啊，根据我们的观察，就说英伟达的它的呃，呃这个特别是枯打的软件生态发展啊，有这么一个思路。

而不见得是官方他只能自己承认的啊，首先一个就是它自上而下，它的根是闭源的，而且只是应对他一家的这个英伟达的芯片，那么呢它通过前期啊2006年以来的，特别是在大魔新时代，极大的去把这个KA开发者的数量。

增到很大的一个程度，这样呢使得大家都往上靠啊，这长出的叶子对吧，我们用了这个嫁接的这种方式，前面有几个报告也都提到了啊，能够去应对当下生态的燃眉之急的，就是兼用库达啊。

当然兼用QA他也带来各种各样的这种啊限制啊，比如说这个被人牵着鼻子走，新一代酷岛的接口可能放进去，有利于英伟达下一代新芯片，新的芯片它最能体现它优势，能够把其他的竞品又拉一大好几年啊。

这样的特性你就老是被牵着鼻子走啊等等，那还有一个就从云到端，前面谈到了，云测的算力的优化，是有极大的这种所谓的极致的优化的需求，所以也有这么大体量的扩大开发者，那么他通过云上的这样的这个所谓的。

有点像把它比喻成城市和农村的这么一个感觉，他先把城市攻破了以后呢，再延到这个，比如端上，可能盾山不见得算力要求那么大这种优化，但是呢呃它也自然凡能优化多少算多少啊，自然继承了，还有一个就是人海战术啊。

因为他这个英伟达哭打的，它的往下越往这个芯片进，它是越闭源的对吧，越不开放的，那么就是所以他很难是通过工具创新的方式，他自己一个公司本身投那么多工程师，特别对长尾场景来去做很广的覆盖啊。

所以它是采用了前期的这个建设人海战术，那我们反观国内啊，我们国国产的AI芯片公司，当然啦前面也提到有一批啊，芯片公司是兼用苦胆，先解燃眉之急，那么除此之外我们也有啊这个其他的这些路线。

那会造成就说啊大家这个投入啊严重不足，而且碎片化各自为政啊，这自己去做自己的，所以整体来讲它很难形成，有比较强的这个生态的竞争力，那一个我们思考的问题就说，除了兼容枯打这个燃眉之急之外。

长远来说我们还是要发展的，不光是我们国家，包括就是除了英伟达之外啊，全世界的其他的这些公司都在想，我们不是只要绑定在一家啊，那么我们在纵观这个这个去看这个历史上，当有一家公司以闭源的方式。

占据了生态的这个领导地位，很难有第二家闭源的公，这个这个生态来去去震撼它啊，但是我们能看到有用开源的方式来去震撼，这个排领领啊，领先者的这样的一个闭源生态的主导者啊，那比如说早期的啊。

这个LINUX来去震撼windows操作系统啊，那还有近更近期的这个谷歌去牵头的，安卓的这样一个开放的一个系统，当然它对应的是多种的硬件，而不是绑定一家的芯片或硬件来去挑战，去震撼苹果的IOS啊。

针对它只是自家的这个苹果的芯片，这样一个生态，那在这个大背景下，我们去看呃，有什么具体思路呢，一个机会啊，在近年这个来到了，那就是这个rise five啊，RISFI它是一个啊开放的一个指令集啊。

是全球啊开放开源的一块来去共建，那在这个大模型时代，也使得AI的这个啊算力需求呢，也体现一个碎片化啊，这么一个方式，就是极大的需要定制来去提供，打各个场景的这种啊这个这个算力的需求。

特别是在AIOT的啊这样的一些场景，所以呢不光是巨头公司，比如说谷歌，meta啊，特斯拉等等，哎也包括以一系列的初创公司，也在这个方向上啊投入啊，并产出啊，很有竞争力的啊，AI芯片的产品。

那我们其实我们国家这个RIFY，利用RISF来去做啊，AI芯片也是啊，有这个挺多的企业开始兴起，但是呢啊也面临着又有机遇又有挑战，首先就是生态碎片化，也自然地继承了前面我提到的啊，除了RISFI之外的。

其他的非KA或非这个GPPPU的路线的啊，这个国产的AI芯片公司的啊，生态碎片化的问题，还有也是类似啊，资源投入严重不足，还有一个是缺少统啊，组织统筹啊，另外就是产学研协同啊不紧。

然后我们其实看到有很多产业联盟啊，把这个公司都聚集在一起说啊，我们形成标准，然后呢大家这个都遵循他吧，但是大这个事情这个说我们统大家统筹起来，别各自为战，这个出发点很好，也就说把大家团结起来。

这个思想也很好，但是在执行上很难，那为什么呢，一个人说大家都是这个进闹这个联盟的，都是这个友商对吧，产出都是竞品啊，那你要去形成一个标准的时候，很可能会出现一个局面啊。

各家公司都放进一些有利于自己的产品的啊，不利于友商的产品的对吧，所以最后面形成就变成一个大熔炉，啥也不是挤不像啊，虽然可能放在那里，所谓的共识，但是没有一家形成这个共识的。

这些公司自己的产品是真正去跟的啊，那还有一个就是他可能说啊，这个出工不出力对吧，你形成联盟啊，开个会就完了啊，制定个标准，反正就放在哪，因为就他觉得哎呀我们就这这一帮人对吧，这个谁也不服谁啊。

就这些人团结起来，投入的力量不足以让我说那么出力对吧，我出助攻就好了啊，所以就说这个是一个啊这个很好的一个思路，但是呢实操很难。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/97d2e4534c83611e28d44322120ea0a5_5.png)

那我们看就是RISFY加AI，为什么会是一个能够去解决，前面讲团结大家起来实操上的一些困难呢，首先一个就说我们团结起来，不只是瞄准说我们国家组织的一堆企业，或者是产学研，大家一起来去做，而是瞄准国际。

说这我们去把RISFI做AI指令集扩展，形成一个我们的共识啊，国内共识，然后呢形成一个候选的，然后在国际基金会推成国际标准啊，那么推成国际标准上有什么福利呢，其实这本身它不是最终目标。

目标是说你推成国际标准了，整个国际的开源社区LVM等等啊，包括AI的这些矿泉社区，自然他就会帮你upstream，就是所谓的上游，就它你的每一代的换代更新，包括他自己的这些开源软件的系统。

软件的换代更新，它都会自然会支持好啊，你这个指令集的这个演化或者是指令集的啊，当下的版本啊，所以这是很大的一个福利，就是不是盯着我们自己集一堆人能产出的东西，而是说一起团结起来。

能够形成在国际上去借力啊，所以呢这也是自下而上，刚才讲的这么一个啊一个一个一个思路啊，前面提到说哎呀，你这个这个放进去都是有利自己，不利于这个我们的友商的，那这个事情肯定是不不可能成为标准的。

就说那不就是叔叔的情形嘛对吧，为什么不大家一起真心的去做一个啊，多赢的一个场局面呢，啊，所以这是一个很重要的一个，一个一个出发点和抓手，所以我们以这个开源的这个指令集为根，这样涨出去。

各家公司都可以利用这个指令集啊去做定制，当然如果你偏移了指令级标准，那你定制的话，你自己公司要投入软件工程师做一些适配啊，那么呃这次就也是很自然，但比起原先这个你各家公司都要投入。

比如说一般我们听到说一个芯片公司，大概有2/3的工程师是软件工程师啊，flip和他这个这个在北京做交流啊，我就问这个问题，我说采用了refine这个方式，上游在系统软件这么来去。

为我们节省的这个软各家公司软件工程师的，对系统软件站的这个投入大概是多少，他说是百分之的个位数啊，这么一个极大的一个降低，还有一个就是说我们是有点像农村包围城市，这样对吧。

我前面讲到AHART的方这这样的一个时代，我们在这个端上啊，我们去做这个啊这个定制化的，包括芯片的设计，指令集的定制啊，以及这个系统软件站的这个研发来，去把这个端上这样的算力支撑好，这样生态起来之后呢。

我们再来包围城市，再把云上对算力优化有极大需求的，进一步的去这个啊利用这个生态呢去加强，还有一个就是说我们不是说走这样的思路，我们就从天而降，这么短时间，我们就孕育出450万的。

利用RISFIAI这个系统软件站的用户，不是就现在大家知道大模型时代啊，因为我本身也从事软件工程的研究和教学，所以就是一个很大的冲击，是我们不需要有那么多的，比较低级的软件工程师了对吧。

好多这些相对低级的任务是能够被取代，当然了，对中高级的还是有很大的需求，就是类似的，这450散可能本来就不需要那么多，如果我们在工具创新，编译优化自动化这方面做得更好的话，而恰恰这一些所谓的主力的力量。

是来自于国际开源社区啊，这个因为我也是担任中国经济协会，系统软件专委会的主任对吧，对于我们国内的系统软件的人才，特别是高端人才是特别缺乏的，所以我们得要到国际上去借力，所以我已经谈到了借力了啊。

那么就说啊宏观来讲就是两大借力，一个是以国际这个啊RISFI的这个国际标准啊，来去借国际上的系统软件站的力，还有一个就直接啊，我们不是说哎呀就等着推升标准了，就就就就就完了，而且这也是一个时机来去。

对国际这个特别是方兴未艾这样的，国际开源社区啊，前面这个这个啊童心老师也讲到了对吧，就是啊资源这边开展的啊，try ten啊，等等的这一些很好的这些工作基础啊。

以及这一个INTEL在主导的啊circle啊，这样更低底层的和哭打啊，类似抽象程度的这样的啊生态啊，那么同时我们也会在中间，就相当于TRITTON和circle中间的抽象。

针对rise five的这些AI指令集扩展，包括matrix啊，tensor啊，也来去我们去定义啊，一个中间抽象程度的算子的接口，以及实现，好我们简单过一下啊，第一这两部分，然后就很快就收尾啊。

我想我应该能够20分钟内结束啊，这一个呃首先前面已经提到了，就是我们以指令集的共识为标准，然后呢up str在国在国际上去推，使得国际开源社区能够对这个RISFI啊，AI指令集扩展进行这个支撑啊。

李院长前面介绍的时候，这个这个也提到啊，就是啊今年的4月啊，我也是被这个选为啊，啊这个陈磊他原先是在阿里达摩院，现在在西姆计算啊，所以这个啊一个sick，也体现了，就说我们在这个方面呢也是有比较啊。

这个是整个国家产业啊，不不是说我们个人呐有很大的这么一个发言。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/97d2e4534c83611e28d44322120ea0a5_7.png)

这个这个这个叫什么体现度，另外一个就4月11号啊，瑞斯赛国际基金会呢指技术指导委员会主席呢，官宣说，其实2024年，RISF国际基金会作为顶级关键，战略优先级的三大方向，排第一的就人工智能与机器学习。

因为这个呃这个方向也给RISFY来去赶超啊，arm和这个啊这个X86啊，以及在市场上能够去造血啊，能够产生利润啊，那呃是带来了一个很大的一个机会啊，排第二第三是这个安全和车载啊，嗯很快的讲一下。

就其实说哎呀，RISFY它它技术上和arm和X86有啥，这个很大不一样的，其实它很不是说它技术上有什么不一样啊，而是说它这样一个开源开放的机制，降低了创新门槛，原先可能说像AI芯片，它需要CPU啊。

和这个协处理器进行协同设计的，原先只能大厂intel arm等等这些，包括英伟达对吧，他自己能够去干，那广大的大家的这个小一些的公司，根本没法去涉及到CPU这一侧，那RISFI加AI。

使得我们呢更多的能够在这方面去做创新，更多的团队啊，更多的公司，还有就是前面讲高度可定制性啊，这方面它天生的，它就设计这个模块化的这个方式，来让大家把这些啊根据自己的需求，把这些指令集扩展去。

能够搭出来，还有就是生态，就跟当年这个LINUX兴起，就相当于大家认为这个是人类发展对吧，正确的方向就是把这个作为，就是我们去投入到这个生态，去这个开源等等啊，这些这些建设志愿者。

他认为这是一个正义的一个事情啊，那么我觉这我们去看就RISFY，现在当下啊这个软件也好，硬件也好，这都是有类似的一个情形啊，这个啊发展速度是特别的快好，第二部分，因为前面啊这个这个啊。

童欣老师已经讲过这个try ton给大家介绍了，我就不多讲了，他这个是更高抽象啊，能够给大家更快的敏捷开发，能够去迭代啊，同时性能呢随着这个编译优化啊，这个等等的这一个全球。

大家这个生态的发展呢也是越来越好，那简单提一句就是啊SQL啊这是INTEL去主导的，他这个比啊这个前面报告也有提到，OpenCL它是更加高抽象啊，open c1离硬件更加接近。

所以呢C口的它的这个啊编程友好性能更好啊，那么呢呃也是相当于是有点像对标KDA，要替代KDA的这个抽象啊，所以我们呢也也也是这个在我们这个支撑的，系统软件站也会去做支撑啊，在这样的大背景下。

在今年这个3月底啊，这个呃依托呃北京开源芯片研究院啊，作为发起单位，联合啊一组这一届啊，相关的高校科研院所和企业啊，成立了这个RISFI加AI顺利生态委员会啊，我们这个已经开展了这个几个月的这些工作啊。

啊这个开了很多会，和很多企业进行这一个啊交流迭代啊，也成立了几个啊，奔着这一个指令集标准或者是啊这个架构呃，扩展标准去的啊，包括matrix tensor g p GPU啊，存算一体啊。

以及这个之上的系统软件站的这样的工作组啊，这都在这个这个铺开去开展。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/97d2e4534c83611e28d44322120ea0a5_9.png)

这个很多的这个工作，这是我们战略目标，前面已经提到了啊，这我就啊不去这个细细展开了啊，其实这个是相当于把前面我们讲的这些事情呢，做一个啊宏观的这么一个总结啊，最后就收个尾，就是啊。

相对就是现有的国产的AI生日，是AI算力的啊，这个芯片软件生态一个是加入，打不过就加入它对吧，加入库打生态，还有就要回去自己干啊，自己干，那那这个英伟达2006年就把KDA干，干到现在对吧，450万。

那我们自己干，每个都闭源的，自己在那拱，那我们要对这个周期性长期性要一定思想准备，那我们现在要推动的是说利用RISF加AI的，这一个全球的生态高速发展的啊，这个啊而且是AI时代特别大。

模型时代带来的机会，我们希望能够去啊，推动成能去震撼KA生态的这样一个系统延展啊，我都汇报到此。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/97d2e4534c83611e28d44322120ea0a5_11.png)

# 2024北京智源大会-AI系统 - P9：八卦炉-面向国产智能算力核心基础软件-翟季冬 - 智源社区 - BV1DS411w7EG

感谢林院长的介绍，我快点讲，要不然大家该饿肚子了，这个咱们已经12点了，后面还有一个报告嗯，呃我我今天的报告题目是，面向国产智能算力的核心基础软件，其实主要是想介绍一下。

我们实验室在最近几年做的一些工作，我觉得大的背景就不介绍，因为整个今天我们的工作都是呃，今天上午我也认真学习了呃，每位老师的报告都是围绕人工智能去展开，呃我觉得其实这里稍微强调一点，就是其实最近几年。

这个基于transformer的大模型，其实他对算力呢其实产生爆发式的这个需求，其实不只是在这个模型的这个研发，然后训练，然后包括微调到推理，其实整个这个过程，我觉得其实都产生了非常强的要求。

非常强的需求，所以说其实这一波人工智能，最赚钱的企业就是英伟达，反正不知道这个人工智能最后能不能落地，至少英伟达先把钱给赚取，因为大家不管是训练的微调，都需要这个算力，我觉得这块我给了一些数据。

这个算力呢实际上是目前这波大模型产业的，主要的这个开销，在训练的过程中呢，大家还要投入很多钱在这个数据的清洗，但是当模型训练好之后，当你部署起来的时候，其实这个推理的成本其实主要是算力的开销。

所以说包括刚才像那个晋辉介绍，其实像他企业其实硅基流动朝着这个推理，其实我觉得也是一个非常重要的一个方向，然后那我们就来看一下，其实包括今天上午这个智远，这个包括玉龙的这个报告，就是说其实也在国内算力。

大家知道，其实现在我们其实还是在公开的渠道，还是很难达到国外的这个算力，包括像英伟达的高端算力，包括像H100A100这样的高端算力，然后其实我们中国呢，其实现在目前也在做这个国产算力，其实有很多公司。

包括像这个华为天树木希等等，然后但是其实国产算力呢，其实我觉得大家我们也就是，包括今天我们在座的同仁，其实也在国产算力去共同努力，但实际上呢就是说国产算力当我们去用的时候，还是存在着一些。

就是说潜在的我们需要改进的地方，我觉得这也是我们其实未来几年，需要去共同努力地方，我觉得其中有一块，就是这个底层的这个基础软件，我们如何能够把更多的这个人工智能的一些，模型在国产算力上能把它发挥到极致。

我觉得这也是我们其实要共同努力去做的，然后其实在这个大的背景下，大家知道，其实中国呢其实现在在很多省市，其实已经在最近几年去建一些国产的制算，超算的算力中心，我觉得这也是未来几年。

其实一个非常大的一个市场，其实这些大的这些制算和超算中心呢，其实大家如果去看的话，基本上是以国产的算力为主，然后其实我们能看到，其实一个非常有意思的现象，就是说大家跟大模型的这些相关的企业聊。

其实大家都是需要这个算力，但实际上建的这个很多的一些计算中心呢，其实它算力资源呢其实利用的并不是很充分，我觉得中间呢还是有很大的一个gap，那为什么呢，其实我觉得这里的一个核心挑战就是这个和呃。

底层的这个算力软件生态，我觉得这个其实大家可以看一下这个呃，其实现在呢其实在英伟达这个体系，其实包括今天我们很多报告都围绕英伟达，其实英伟达呢其实我觉得最早它发展，其实这个软件呢。

其实当他开始从游戏市场进入这个算力，实际上大约是0405年，当时我还在读研究生，然后当时呢其实英伟达在给包括清华，很多学校送卡，他觉得说诶，我们是不是说，其实可以用英伟达的卡做一点计算。

然后当时呢我们实验室有一个，跟我差不多一起读书的，然后当时说诶考虑是不是做矩阵乘，然后英伟达就给打印了几本书，说诶怎么在这个英伟达平台上去写矩阵城，当时说诶要写要看这么厚的一本书，其实这个编程很复杂。

然后后来呢其实大约在0708年的时候，哭打慢慢出来，然后到后面其实这个这一波人工智能起来之后，是非常快速地推动了英伟达发展，然后我们看到其实在英伟达上面，其实从最上面。

其实大家用哈根face可以很方便的去用，用很多模型，这个包括训练微调，其实它实际上是把PYTORCH，其实好多做模型的人会觉得说，我用PYTORCH写代码还是会很复杂。

但是做我们做系统软件是往下其实有酷大呃，这个cool plus，包括今天其实大家基于的TRITTON，其实整个英伟达呢实际上把整个这套生态呢，其实至少就让我们在各个层次去做事情，都提供了非常好的。

但实际上呢就是说国产呢，其实你跟很多芯片厂商聊，他也会说我支持哈根face斯，支持PYTORCH，支持TENSORFLOW，我有很完善的算子库，有很完善的整个底层，但实际上当你去做更多的这个优化的时候。

你会发现其实它底层呢还是有很多问题，比如说可能漂跑标准的拉码是没问题，但是当你把拉码改一改，或者把很多这个模型改一改的时候，你会发现其实这里边就会问题很多，然后包括其实模型呃，包括很多软件的小的版本号。

你会发现很不兼容，我觉得这是一个非常大的一个挑战，其实我这里经常举的一个例子是，其实所谓的生态好不好，比如说像在清华里边还有很多非计算机专业，比如说物理系，化学系。

大家知道其实现在做AI for science，对人工智能需求非常强，那这些老师其实当他有一点经费的时候，比如说诶我有个几10万，我要买几块卡，那他们愿意买什么卡，你会发现其实他们还是愿意买英伟达的卡。

因为发现买来英伟达的卡，其实改一改软件还是能跑，但是其实你让他们去买一个国产芯片，还是有很大的这个压力，然后大家知道其实这里面的核心呢就是说，其实我现在人工智能模型虽然都是。

可能现在目前主流模型都是transformer，但是其实每家呢都会在，transformer上做一些改进，然后同时呢其实在架构本身呢也会有很多变化，那这个时候其实大家知道，即使在英伟达上面。

那你要把这些模型其实跑得很好，我觉得这个也需要很多的努力，但是可能在国产的卡上把这些模型跑好，其实就更需要更多的核核心的系统，软件的这个研发人员，其实刚才其实李院长也提到，其实我觉得像系统软件在中国呢。

其实还是蛮缺这方面人才，包括像我在清华带的这些博士生，基基本上应届生就可以拿到200万的这个package，就是200万到300万，就说明其实这个方向呢其实还是非常缺人。

就是说你还有很多的这个溢价的这个空间，然后呢我下面总结一下，其实智能算力呢，其实那它围绕呢，其实我们总结还是有大约十个非常关键的软件，就是说我们要把它做的比较好，其实大家可以从下往上去看，这个。

当我们在一个比较大的一个计算中心上，做模型的训练或者推理的时候，首先我们要调度器，我们如何把这个白卡千卡充分的调度起来，然后我们要很好的去管理内存，大家知道其实我要做模型的训练推理。

我都要高效的去管理内存，然后同时呢其实做模型的训练，你要有高效的容错的系统，然后同时呢大家知道，其实我做模型训练微调，我要读大量的数据，那你底层要有比较好的并行，文件系统的这个知识。

然后再往上到了这个新片层，你要很好的编程语言，其实刚才我们提到，比如像英伟达有CUDA，那刚才其实这个谢老师提到，比如说像cycle，其实cycle呢，我觉得就是说也是一个目前在发展非常迅速的。

就是说其实我们如果做更多的芯片，我们肯定不希望大家，要么完全自己做一套新的编程语言，跟别人不兼容，然后在编程语言之上呢，其实很重要的一个环节就是编译器，今天有很多老师有提到。

就是说其实如何把这些算子高效的编译到底层，其实这里是一个非常重要的一环，然后还有算子库，其实算子库跟编译呢其实是在一个level，就是说我们可以把常见的这些呃算子，是这个编程高效算子。

那你一定需要去编器支撑，然后再往上呢，其实当你涉及到多多卡多机的时候，那这个时候通信其实今天是刚刚也提到了，就是说其实当你在多机多卡的时候，那这个时候通信就非常的关键。

然后编程框架呢实际上是把这些整合起来，就是说当然就是说，其实它给我们提供了一个很好的用户接口，当然在底层之下呢，我们都可以去做很多我们自己的替代，然后当你在大规模的时候，其实不管是训练还是推理。

其实那你对并行的这个需求就会非常大，然后这里呢其实我讲一下，我们其实实验室在做的这个看的问题的角度，就是说底层的有国产的这个芯片，我觉得这里其实容呃有一层的核心的关键，就是编译器。

那你如何把底层的这些国产的芯片，能把这些性能发挥的极致，那编译器首先是一个非常重要的环节，不管是AI的芯片，包括像risk five，包括我们做各种各样的芯片，我觉得这也是其实大家看这个计算机系。

其实四大原理课程，有操作系统原理，组成原理，网络原理，再就是编译原理，就是能教成原理的课，在计算机系只有这四门课，其实是非常重要的一个环节，然后再往上的核心其实就并行。

那你如何其实能够比如说现在的MOE模型，比如说这个都很大，那我们如何在比如说单机八卡，把这个把这个模型推理做得很好，那这个时候并行就是非常关键，然后当你做这个白卡，千卡或者万卡的训练的时候。

那并行也非常重要，那实际上呢这两层呢，其实我们都可以在PY套之下去把它去改掉，也就是说让用户呢其实去不改编代码的同时，能够充分发挥底层算力的这个性能，然后这个里边紫色部分呢。

实际上是我们我就我在清华带的这个实验室，过去几年包括在这个容错，包括在调度，然后底层的编程语言，然后编译器方向，然后包括上层并行，我们做的一些工作，然后今呃因为时间原因呢，我下面呢其实简单的介绍一下。

我们其实在这几个方向做过的一些工作，大家如果感兴趣呢，其实可以去网上下载，我们其实很多系统我们都是开源，我介绍第一个工作呢是在嗯编程语言，大家知道，其实这个编程语言呢。

其实就是说其实现在在这个异构芯片呢，其实做领域特定的编程语言，是一个非常有意义的，因为实际上我们不能让一个编程语言，去解决全部问题，然后我们这个工作叫free tensor。

它的核心就是说我们有一类的人工智能模型，它实际上是不规则的，比如说我大家看右边的数据，我给了四个模型，最右边叫GAT，实际上是这个这个图，就是图神经网络GN这类的，它本身是一个系数问题。

然后long former实际上就是这个long sequence，就是大家知道现在大模型处理这个长序列，那我会在这个会有很多这种算法，那它本身呢其实它也不是一个规整的。

然后此外的还有这个soft ross，Soft rust，它实际上就是拍几个二维照片，然后帮你生成三维的图片，那它这里面也是一个不规则，那针对这个呢，我们在PYTORCH里面做了些扩展。

然后做了这个很多的性能的这个优化，然后跟PYTORCH原声比呢，其实在这个英伟达平台，我们可以有上百倍的性能提升，这个主要是讲不规则的模型，其实这里边会有很大的这个性能提升的空间。

然后这个工作呢我们也在GITHUB开源，这个感兴趣，这个大家可以去下载，然后第二工作呢其实就编译器，其实今天有好多老师提到编译器，我觉得就是说我们要把底层的这个芯片呢，其实算力发挥起来。

就是即使像英伟达的平台，那我们如果在编译器做很多事情，其实还是能提升，比如说30%或50%的这个性能，那在PYTORCH之下呢，其实今天尤老师讲，就是说它的核心呢其实为了发挥性能。

像PYTORCH会把这个eager mode，其实这个这个执行方式会转成一个计算图，这样的话可以在编译层面做很多优化，也就是说其实今天上午的一些报告提到，比如说有图层的编译和底层算子编。

然后我们这个工作呢叫i net，实际上是去年的OSDI的文章，我们实际上是把这个图层跟算子层打开，这两层合到一块，也就是说可以挖掘出更多的这个优化的空间，就之前呢比如说像有些图层的优化，那是在这个图。

比如做一些kernel fusion，然后在算子，其实今天包括赵杰老师讲的，包括算子其实也做很多优化，我们这个核心呢实际上是把图层的算成，融合到一起来，挖掘更多的优化空间。

我们这个是在英伟达的A100上面，然后做了一些模型，然后包括一些经典的这种卷积类的，和现在的这种大呃，Transformer base，然后跟TENSORFLOW，还有TENSRT。

TENSRT是英伟达上面一个默认的，编优化的工具，然后pad实际上是我们之前21年的一个工作，然后最多呢其实会也会有将近两倍的性能提升，其实这些想跟大家讲，就是说即使是英伟达平台。

我觉得你去深深入的挖掘底层的编译，包括上层的，其实还是会有很多这个性能提升的空间，然后最近呢其实我们在这个大语言模型，大家知道其实该那个呃近回讲到，就是说其实在推理其实它是一个memory b。

那实际上呢就是说大家知道我们要把模型参数，然后包括中间的像k v catch存起来，那这个其实对内存会有非常大压力，然后他对这个系统，内存带宽会有非常高的这个要求，然后我们这个系统的核心呢。

我们把模型参数和kv cash给它分离，把kv cash呢给它挪到CPU的内存，这样的话，然后通过这样的流水线的并行，来充分的发挥CPU跟GPU的计算能力，这样的好处。

就是说我batch size就不再受到，kv cat内存占用的这个限制，然后同时我还可以把CPU的内存带宽，跟GPU内存带宽整合起来来，共同的提高这个吞吐，我们这个是在英伟达平台。

跟VIVIMTESRTLM系统比，我们by ch size可以提升到百倍，然后GPU吞吐量提升1。8到14倍，这个系统我们最近也会把它开源，这个系统叫fast decode。

然后呢其实我们在这个训练测呢，其实我们一直在维护，就是MOE，其实MOE呢在21年，其实年初的时候还没有火的时候，我们就开始在PYTORCH里面做了，这个MOE的并行知识。

也就是说你在PYTORCH里面加一行代码，我们就可以自动的帮你去做各种并行，我们这个系统是21年开源，叫fast m o e，后来我们又持续的更新，然后拍套那个呃美国的FACEBOOK。

它在Python开源，应该是比我们还要晚一两个月的时间，我们这个系统呢，也包括支撑了北京智源的悟道的这个，大模型的这个训练，然后我们后来呢其实把这个系统持续的改进，包括fast m o e是这个呃。

是我们在p pop2上的一个工作，我们这个跟deep speed的ME系统比，可以比它快高达十几倍的这个性能，然后我们去年呢又进一步的优化，做了一个smart m o e系统。

然后能比我们这个fast stem e这个系统，又进一步提升将近两倍的性能，就是这些数据都是在英伟达的平台去测的，然后下面讲一下在这个国产算力方面，就是说大模型呢大家知道训练呢，其实他对这个呃。

其实硬件呢还是提出很高的这个要求，然后这个国国内呢，其实青岛这台机器呢它是一个纯国产的，大约有10万个节点，大家可以理解成有10万个国产的这个加速的卡，它的性能呢其实相当于是1。8万块的。

英伟达的A100还是蛮大的一个系统，然后我们其实在这个系统上呢，其实我们是从底层的编译，然后包括像内存管理，然后包括多机的通信，我们其实是把整个的这套系统，其实这个移植起来，然后可以就是说把PY套代码。

可以在这个系统上去跑起来，我们把我们整个这套软件叫做八卦炉，就是包括底层的这个通信编译，然后最后是训练起一个百万亿，大家知道，其实现在呢国内大家都在训练万亿模型，其实是万亿的100倍。

然后我们最近也支持这个，包括像AI for science的一些，大家在这个系统上做一些AI for science的，也在支撑这样的这个系统，然后我们目前呢也把百川。

包括拉玛一些模型在这个青岛的平台支撑起来，然后至少可以在这个国产的这个系统上做，包括百川拉玛的这些模型的训练和推理，然后我这有一个视频，我不知道是不是能播放出来啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/7794d5f137fa1134d360402c17ff24ba_1.png)

可能这个系统没有声音没关系，就是说这里边其实有声音。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/7794d5f137fa1134d360402c17ff24ba_3.png)

就是说我们其实主要想讲，就是说其实用这个国产的这个平台呢，我目前已经可以把这个包括百川还有拉马，然后这个支撑起来，然后因为其实这个国产超算呢，实际上是国家投钱建的，然后它本身的这个就相当于基石费呢。

还是很便宜，大家如果在这个平台上做训练或推理，其实还是要拼，你去租这个英伟达的A100或者H100的话，其实成本会低很多。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/7794d5f137fa1134d360402c17ff24ba_5.png)

然后最后总结一下，我应该还提前了3分钟，就是说我觉得其实中国呢，现在发展人工智能领域，其实我觉得构建国产的智能算力，我觉得还是非常重要的，就是说因为整个美国对，其实中国呢还是封锁，然后在这个一环里面。

其实我一直还是坚信，其实包括在座，就是说发展这个底层的基础软件还是非常重要，就是我们要把，就是说其实其实大家如果去看英伟达的话，其实英伟达其实他很多的工作，都是在软件层次去做，我觉得这也就是说前面讲到。

包括第一个这个我们这个国外老师的这个talk，其实他也提到，就是说，其实这个我们要把这个底层算力发挥起来，其实这个软件呢实际上是占到非常重要的一环，那就是说我们能够构建我们国内一个比较好的。

这个良好的软件生态，可以降低大模型在不同，AI芯片的这个识别开销。

# 2024北京智源大会-人工智能+数据新基建 - P1：领导致辞--尚冰 - 智源社区 - BV1qx4y14735

各位专家各位来宾，女士们先生们大家下午好，非常高兴啊，参加我们的人工智能和数据新基建的，这个论坛，与大家共同探讨数据作为关键生产要素，的一个重要作用，我首先代表中国互联网协会啊，对于各位专家。

各位来宾的到来表示热烈的欢迎，当前新一轮科技革命和产业变革，正在加速演进，新智生产力正在加速形成，以AIGC通用大模型为代表的人工智能技术，应该说引发了第四次工业革命，成为全球关注的焦点。

和科技革命竞争的一个制高点，作为人工智能的底层基础要素，高质量的数据已经成为AI大模型研发的，基础性战略性的资源，是科学决策精准治理精细服务的基础，数据工程建设成为从X加AI到AI。

加X的根本性变革的一个关键力量，目前高质量数据的生成速度远远低于，AI模型训练数据的需求增长速度，应该说数据短缺的问题已经出现端倪，高质量的数据资源对于AI大模型，堪比于石油对于工业发展的战略价值。

党中央国务院高度重视数据要素的价值，习近平总书记多次强调，要发挥数据的基础资源作用和创新，引进作用加快形成以创新为主要引领和支撑的数字经济，今年1月国家数据局等17个部门。

联合印发了数据要素层三年行动计划，2024-2026，积极推动数据对于经济发展的倍增效应，因此做好人工智能数据新基建势在必行，近年来我国在数据新基建方面应该说我们的发展势头良好。

一是数据处理能力走向成熟，围绕着大数据的采集分析处理，涌现出了一批具有代表性的数据公司和企业，二是数据要素的市场化改革不断的深入，数据20条提出了建立数据资源持有权。

数据加工使用权和数据产品经营权的三权分治的数据产权制度的框架，三是公共数据开放在不断地加快，2023年我国公共数据开放量同比增长了16%，省一级政府的开放数据量同比增长了18。5%。

北京浙江等15个省数据管理部门开始探索公共数据授权运营机制，应该说发展势头良好，同时我们在肯定成绩的时候也要清醒地看到我们当前存在的问题。

主要是国内开源数据集在数据规模和语调质量上相比国外仍然具有较大的差距，中文开源数据集仅占英文开源数据集的11%，而且数据来源比较单一，更新频率也较低，从而导致大模型训练效果受阻。

在此我想简单提四点建议和大家分享，一是加快高质量数据资源的开放共享，通过产学沿用多边合作推动政府企业高效合建公开数据集，促进中文数据资源共建共享，尤其是公共数据，政府数据和科学数据。

二是积极跟进合成数据等技术发展，充分发挥合成数据获取成本低质量高，避免侵犯隐私等优势，推动不同领域不同模式合成数据的技术发展，繁荣AI产业的发展，三是尽快构建数据交易机制。

积极对接国家和地区的数据交易平台，制定适合大明而行的数据标准体系，畅通数据确全，数据价值评估，数据定价等流程，加速数据市场的成熟，四是坚持数据治理与数据流通并重，构建数据安全保障体系。

确保数据基础设施安全，数据流通过程，尤其是要确保跨境数据流通的安全，加快隐私计算，数据隔离等技术发展，各位嘉宾各位朋友，通用人工智能时代，是人类文明和历史发展的必然趋势，也是我国积极推动人工智能和实体。

经济深度融合，加快形成新智生产力的历史性的机遇，中国互联网协会将一如既往的发挥好，我们行业组织的优势和桥梁扭载作用，与业界同仁一道共筑人工智能，加数据新基建，共创我国人工智能时代的新的飞跃，谢谢大家。

最后预祝本次论坛取得圆满成功，谢谢，謝謝，字幕由Amara。org社区提供。

# 2024北京智源大会-人工智能+数据新基建 - P10：大模型助力数据要素价值变现-肖仰华 - 智源社区 - BV1qx4y14735

非常感谢冯老师邀请啊，让我有机会能够参加我们论坛，那么今天呢前面各位专家呢啊，谈了很多数据要素对于人工智能的重要意义啊，重要作用，那么我呢更多的反过来考虑这个问题。

那事实上呢我们的人工智能啊或者大模型啊，对于我们这个数据要素啊，它的意义呀更是重大啊，我的一个基本观点是说，数据要素的价值变现，将来需要一个大模型作为智能引擎，事实上我们今这么多年来都在做大数据。

都希望数据尽快价值变现，但是现实情况是，那我们的这个数据啊，啊这个价值变现，这个仍然这个路途还比较遥远啊，所以呢今天给大家带来这么一个报告，那么我们来看一下数据进入到数据要素，这个时代之后。

我们的数据呈现出哪些新的特征啊，我想啊表达一个观点就是数据要素啊，它实际上哦，时代呢数据呈现出前所未有的这种，开放性和动态性，这个恰恰是我们现当下所面临的一切问题的，关键所在。

我们都知道数据已经成为了啊生产要素，数据从战略性的资源变成了新型生产要素，而且数据啊日益的对其他几个生产要素，像土地啊，劳动啊，劳动力啊，资本和技术呢起到了越来啊越重要的啊，这样一个支配的一个作用。

那么最近几年啊，我们国家伴随着啊数字经济体量的逐步增长啊，整个数据要素的这个价值变现的这个需求啊，日益迫切，数据要素价值变现的进程啊逐步的这个加速啊，但是反观我们当下的一些理论和方法啊。

我们大家感受到了非常多的这个痛点哈和读点，从我们的数据开放开始啊，我们就啊簪头顾尾对吧，就心惊胆战啊，这个数据到底能不能开放，是否会触犯啊相关的法律法规的红线，到数据融合啊，我们更是做得焦头烂额。

没有哪一个企业在做数据融合，数据治理是很开心的一件事情啊，大部分数据治理都成为了企业最脏的一个，最脏活累活最集中的一个部门啊，数据治理成了很多企业所谓的成本中心，而没有变成他所谓的这种利润中心。

再到我们的数据的应用，我们大部分的这个数据应用，还是停留在简单的表层的统计分析啊，数据到底在告诉我们什么，它背后的洞察，数据之间的深层次的这种关联关系，我们仍然啊并不清晰。

所以整个数据要素价值变现链条上面，仍然存在着诸多的这种啊堵点和痛点啊，归其根本原因啊，是因为我们现在这个数据要素价值变现，主要还是靠我们的人力去做啊，我们数据越来越复杂啊，这个问题越来越多样。

我们的人力呢已经很难啊，完成整个的这个数据要素价值变现链条，所以数据要素的价值变现呢，仍然缺乏有效的手段，那么这有几个重要的原因，第一个很重要的原因是在于，我们这个世界越来越复杂。

我们现在在当下这个世界，可以说是一个人机物啊，这个多元融合的一个复杂系统啊，我们的人类社会系统，我们的人造系统啊，我们的自然生态系统都前所未有的复杂，事实上，整个人类社会早就经历了他这个工业时代的。

早期的这样一种纯真的婴童时代，而进入了我们现在信息时代乃至智能时代，这样一种成熟的时代，而成熟的代价就是复杂，我们现在任何一辆整车对吧，往往要几万个零部件，每个零部件又要经历一道上万个元器件组成的。

生产线啊，整个一个系统可以说是前所未有的在复杂，而系统世界的复杂带来的是什么，带来的是咱们的系统和数据的复杂，我们去看看我们制造业的这些系统，动辄上万张的表单啊，关联在一起，现在还有哪一个人类专家。

真的能够全盘的理解你企业的那些数据呢，几乎没有啊，再看看我们那些表单的，我们中国式的这种表头前所未有的复杂对吧，所有的人工大智能大模型，一看到这种表头全部要啊，都要fail掉。

所以这个系统和数据可以说是日益的复杂啊，所以这个是给我们的数据价值变现，带来第一个挑战，那么第二个挑战呢，在于是说我们数字经济时代啊，我们的数据的内涵发生了显著的变化，我们想想看我们在信息化时代。

数据是什么，是对我们的客观世界的一种符号化的记录，到了我们大数据时代，数据变成了一个资源，变成了我们发现规律，赖以啊使用的这种推动创新的资源，再到我们当下的数字经济时代，你会发现数据现在变成了生产要素。

变成了产品，变成了资产，随着数据的内涵发生了变化，数据啊日益呈现出新的特点，有些特点其实刚才各位专家也多多少少提到过，首先就是大家看我们数据要素是在持续流动的，我们的数据只有流入到生产分配。

流通服务和管理的各个环节，形成整个完整的数据驱动的闭环，我们才能够释放出数据要素的价值啊，我们此前我们的数据的管理，从来没有面临这样的挑战，而数据这样一种持续流动呢，对于我们这种全链条的自动化的。

智能化的，具备高度协同能力的数据处理技术啊，提供了前所未有的要求，我们的数据要素啊要在流动过程当中啊，会与很多个主体发生关系，这就会导致什么呢，我们的数据的这种权属的确定，安全的可供。

数据今天可能在生产者手里，明天可能在采集者手里面，后天可能在加工者手里面，再后天可能在使用者手里面，还有运营者手里面，不同的使用主体都对数据提出了不同的要求，那么这个时候数据权属如何确定呢。

如何安全可控呢，我们这个数据在运营和加工使用过程当中，日益面临着一个前所未有的开放的异构的，复杂多变的一种啊，数据产业的生态环境，数据产业的生态环境，可以说是一种开放的复杂环境，你不知道你这个数据。

将来会要跟哪一个数据库系统打交道，是跟哪一种类型的数据的系库系系统打交道，跟哪一种数据打交道都不知道，所以这就对什么呢，统一的标准化的互操作的这种数据管理技术，提出了要求啊。

我们的数据必须在一个动态的增值过程当中，才能够帮我们创造价值，数据的每一次啊，这种动态处理，比如说我们汇聚了可能就创造了一点价值，我们分析了就可能创造一些价值，我们把它质量提升了，可能又创造了价值。

我们把它关联融合了，可能又创造了价值，这样一种动态的处理过程，才能够促进我们的数据形成，这个持续的这个增值数据，才有可能真正变成资产，但是我们现在的这些方法有多少，我们的数据的管理和分析方法。

是面向这种动态增值过程去设计的呢，所以我们整个啊可以说，我们的整个数据的科学的理论和方法啊，远远啊不足以支撑当前数据要素的价值变现啊，我为什么呃，很多场合会提这一点啊。

我们国家最近几年的数据要素非常重视这种啊，加强制度的建设供给对吧，加强基础建设啊，事实上呢我们可能忽略了一点，我们也要加强技术供给，很多时候不是说企业不想去做数据变现，而是说企业没这个能力。

或尤其是考虑到巨大的成本投入之后，更是不愿意去做这种数据价值变现，指数据技术层面的这样一种，堵点和痛点的存在啊，数据技术的啊供给能力不足，其实也是当下数据制约，数据价值变现的一个重要问题。

所以啊我们要去找手段，那么什么样的技术手段啊，能够让我们又快又好的实现，数据要素价值变现呢，啊我想问题的最终答案可能就是我们当下啊，人工智能的最新进展就是大模型，事实上时代抛出一个问命题的时候。

往往也给了我们这个命题的答案，当我们进入了人工智能时代，当我们数据要素变得越来越困难的时候，哎我们当下正在发展的人工智能，给我们提供的这个大模型，却有可能是我们解决数据要素核心问题，的一个答案所在。

大模型可以说是从本质上来讲啊，它到底是什么啊，很多人不同的角度有不同的理解啊，我认为大模型本质上是利用了，人类已经积累的数据啊，习得了对于这个复杂世界的一种建模啊，比如说他从数据里面学到了啊。

苹果从树上掉下来是要落地的，而不是要飘到天空的，所以它本质上是从数据中学到了，我们人类对这个世界的认知，我们人是怎么认知这个世界的，我们人本质上是通过个体的经验和文明的传承，来建立对这个世界的认知。

而我们的机器是靠什么，我们机器是靠啊从数据中呢学习，尤其是自监督学习，来习得对这个数据的这个建模，或者说对这个世界的理解大模型，一旦理解了这个世界之后会怎么样啊，它的一个结果就是。

它一定会成为一个知识的容器，你会发现大模型从海量的语料里面，学到了我们人类的所有的学科知识啊，我们曾经做过评测对吧，你去把那考题拿过来去测，我们大模型啊，像g b t four，先进大模型。

都能够做到一个啊六七十分的这样一个成绩，但是我们人类呢，我们每个个体是很难，在每一个学科都取得如此的成绩，所以大模型啊提供了这海量的这个知识的容器，大模型更成为了人啊。

模拟人类认知能力的这样一种新的引擎，我们以前整个人工智能最害怕的，就是所谓的常识理解的问题啊，甚至常识理解都被视作是人工智能的，第一性问题，曾经被我们啊质疑了很多年，我们到底能不能做到。

但是今天我们再去看常识理解，基本上大模型是能够比较好的胜任，而且大模型不单单能够理解了，我们语言中的常识对吧，语言理解概念理解啊，运筹规划问题求解，知识反思，价值判断，组合泛化啊，评估评价等等。

很多我们人才具备的能力，现在大模型也具备了，所以大模型更成为了啊，一种认知能力的一种引擎啊，伴随着大模型越来越成为一个自制，智能体的一个大佬啊，他有可能成为啊智能体，让智能体真正意义上啊。

能够跟这个复杂世界进行自主的，自适应的交互啊，智能体的这种自制性和自主性，是有可能进一步实现的，所以大模型又进一步成为了啊，各种各样智能体的大佬，那么正因为大模型具备了世界的建模能力。

就又具备了刚才说的这些能力之后，它对于我们的数据要素带来的意义是什么呢，它带来了全面认知数据的能力，可以说大模型现在对于我们的数据的认知能力，不比我们普通人差，甚至不比我们专家差啊，我们去看一下大模型。

实际上它现在已经具备了很多概念理解，我们说数据库里面的数据对吧啊，首先要理解这种原数据叫schema对吧，schema里面要大量的是这种概念啊，比如说两个schema是能能否类比对吧。

那么现在大模型基本上能够理解这些，概念和概念之间的关系啊，如果我们把一张数据表格去问GB t four啊，问他这张数据里面有什么错误啊，这是我们人为构造的一个例子，那第一行是小明自身的数据。

第二行是他父亲，第三行是他的母亲，我们人为的植入了很多错误啊，比如说小明的父亲是张378年出生，小明是80年出生，你去问大模型，这些数据里面有什么错误，现在大模型基本上能够比我们人呐。

普通人都能够找的全找得准，这里面的数据中蕴含的错误，蕴含了哪些逻辑错误，所以大模型又进一步具备了啊，实例数据的理解能力，所以说大模型呢有可能是能够认知我们的数据，它不单单能够认知这个数据。

它更能够去自主去操控我们的数据，当下我们正在研发，很多企业都在研发所谓的叫数据agent对吧，数据库agent啊，数据管理agent，数据分析agent，我们今天去问很多大模型，比如说以表格形式啊。

对比上海和北京每年8月份的平均温度，那么这件事情，以前我们可能需要一个研究生对吧，去查找数据，去绘制表格，去调用这个统计下那个检验的啊，软件工具来形成检验结论对吧，相当这些有有很多步骤。

那么今天呢大模型有了强大的规划能力啊，之后呢啊工具使用能力之后呢，他就进一步的具备了这种自主操控数据的能力，那么大家想想看，有了认知能力，又有了操控能力，那怎么样。

我们啊原来人类社会是靠人类专家去理解数据，去操控数据，那么这些事情是不是可以交给机器了呢，所以大模型一定啊，将来会成为驱动，推动整个数据要素价值变现的一个重要引擎，将会成为激发这个数据价值的。

这个智能的新引擎，但是我们也要意识到大模型，而在推动数据要素价值变现过程当中，仍然面临着巨大的挑战，因为我们千行百业的这些数据啊啊，它是用来支撑各行业的复杂的决策的，严肃的应用。

那么这些应用不是以我们当下XGPT，这种闲聊啊就能够胜任的，我们作为一个行业专家，我们看到这个数据之后是要做决策的，这个时候我们需要什么，一个行业专家需要有丰富的知识领域知识。

需要有复杂的这种决策的逻辑，需要有对宏观态势的研判能力，比如说现在股市是熊市还是这个牛市，需要综合任务的这种拆解能力，还需要一些精密的规划能力啊，需要复杂约束的取舍能力。

比如说我们的医生在决定你的治疗方案的时候，要考虑很多约束，又要舍弃很多约束，还要有未知事物的预见能力对吧，这市场总是在不断的发生变化，还要有不确定场景推断能力，这些能力都是我们目前大模型再去支撑。

这个基于数据的决策过程当中啊，仍然缺失了很多能力，而且大模型本身它作为统计模型，仍然存在着幻觉现象，缺乏某领域的忠实度对吧，可控性，可理解，可解释，可编辑，这些性能呢仍然都存在着诸多问题。

还有就是成本问题，所以大模型在真正的这个推动数据要素，价值变现的过程当中啊，仍然可以说是啊面临着这个巨大的一些挑战，尤其是大模型对于我们私域数据，这种专业性和私有性的理解，仍然是这个存在着巨大的鸿沟啊。

我们这有很多行业数据，都有一些自己的编码规范，自己的编码标准对吧，大模型何以能够理解你的这种私有化的表达呢，我们的很多行业数据，都背后是有着非常复杂的专业知识支撑，比如说传感器的数据。

所以大模型何以能够理解这些数据呢，这些都需要啊，我们进一步提升大模型呢对于行业的理解能力，对于私域的理解能力，那么进一步呢我们会发现，大模型已然在很多数据要素，价值变现实践当中啊，发挥着重要的作用啊。

我们首先说大模型，为什么说数据要素的价值变现，一定将来要靠大模型啊，因为大模型啊，可以说是提供了一种端到端的价值变现路径，我们以前大家想想看，行业中我们要把数据这个价值发挥出来，要需要什么。

需要业务分析师去分析业务逻辑，需要数据工程数据标注，数据算法工程师去设计算法，用户来进行反馈验证是吧啊，可以说是又涨，这个这个代价是极大的，但是今天我们所有的数据。

都可以丢进大模型里面去炼制一个行业大模型，用行业大模型再通过一些插拔式的组件对吧，来提来发挥这个数据的价值，所以它本质上可以说是一种端到端的价值，不变现路径，这不就是我们行业甲方用户啊梦寐以求的吗。

把数据都扔给你，我就不管了对吧啊，付费就完了对吧，现在像我们现在呢还要重度的参与是吧啊，还要跟你去整天开会是吧，那么将来是不是可以避免这些工作，而且呢我们现在有很多异构的多模态数据。

你会发现现在大模型不管你什么样的数据，只要咱们能把它序列化成一个sequence of token，对吧，一个序列化的一个数据，咱们transform就能发挥作用啊，啊，或者就是用那个。

如果是一种那个带结构比较丰富的数据，我们可以用diffusion model对吧，也就是不管你是什么模态的数据，似乎只需要用transformer和这种啊diffusion。

我们就能够习得这个数据中所蕴含的知识啊，所以它实现了一种统一的价值变现路径，那么尤其对行业来讲，我们现在数据要素价值变现最大的一个堵点，就是数据治理代价大，我们现在为什么数据治理这个代价大。

主要是靠人对吧，我们说数据治理这个事，我觉得将来是一定要交给大模型，为什么，因为数据治理里面，我们主要是要解决数据中的很多问题和错误，是吧，而错误和问题恰恰是具有开放性的特征。

因为我们没有人能够预料用户会犯什么错误，没有人能够预料数据中存在，哪些非常不可能的错误，错误都是你意想不到的，我们曾经看到过一个真实数据啊，说在一个坐标他写错了啊，一个一个一个一个舰艇。

停在了沙漠中的一个坐标，这是不可能的事对吧，所以呢他这个很多错误你是根本就想不到的啊，所以呢正是因为错误有开放性，那么人是不可能预设所有的错误类型，但是大模型它有很强大的开放理解能力。

所以这些呢我们都可以交给大模型，我们实际上已经在做很多种实践，比如说啊大家知道啊，我给你告诉你一个位置信息啊，杨浦区什么中环路和某个路的交叉口，西北什么位置，这个是一个非常不规范的一个地址啊。

我们能不能利用大模型的这种理解能力，把它清洗成一个规范的数据呢啊像这样，现在利用大模型的文本理解能力和工具，使用能力，像这件事情基本上是能够做到，我们还可以用大模型去做很多，语料的清洗和治理啊。

就大模型本身训练语料的治理，但这里呢我要说一件事啊，就是大模型语料规模往往巨大，比如几个T对吧，这里怎么办呢，这个点成本就是个很大的问题啊，实际上我们现在在利用大模型去治理语料，的时候。

往往要做有选择的治理啊，往往把一些比较hard的case啊，困难的case啊交给大模型啊，简单的case呢还是用用大模型，增强了一些小模型来去解决，所以用大模型来去做这种语料的治理，数据治理啊。

成本问题是大家不可回避的一个因素啊，我们实际上还是在利用大模型，在做大量的这个知识验证啊，很多场合我们都还要做知识库的构建对吧，那么这个知识到底对不对呢，我们可以用大模型来验证。

我们还可以用大模型来做结构化的数据访问，对吧，我们现在关系数据库的表，我们都可以用自然语言去访问了啊，来释放数据的价值啊，我们现在很多的数据分析，在利用大模型驱动的agent来做自动的分析。

而而且这个成本呢有了一个极大的提高，现在已经有些初步的数据啊，我们还可以利用大模型去做这个，数据的智能运维啊，这数据库系统的智能运维，不再需要传统的专业语言才能够去做，那么这个现在都已经在很多啊。

系统里面都实现了啊，我们还有可以用大模型去啊，实现一种就是知识工程，我们传统做知识图谱对吧，要去构建schema，要去做知识抽取知识的验证对吧，知识的问答，我们现在完全可以利用大模型啊。

去蒸馏一些知识啊，再利用大模型呢来去用提示的方法来构造图谱，那我这里举个例子啊，叫我们以前要做教材的这种图谱化。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/2a87bbac9127b4f31f2b36b746bb29c3_1.png)

把教材里面知识全抽出来，这个本来是个代价极大的事，每本教材都要构建相应的模型，1万本教材你就要构建1万个模型，那么今天呢我们只需要用大模型，就可以把教材里面的这种实体关系的关联，关系啊全部自动化抽出来。

基本上可以从每本教材本来要十多个月的代价，现在只需要两三个人员，所以大模型来驱动整个知识工程，这个基本上啊可以极大的提升它的效率啊，我们也利用这个思路啊，利用大模型来驱动这个知识图谱构建。

我们也构建了一个数据要素的产业突破平台，实际上我们国家现在待会可能有个talk，缺情报的支持，这个情报呢就需要一个产业图谱，我们实际上在构建一个围绕数据要素产业，构建它的一个资源和生态的图谱。

把所有数商数据交易所数据啊，把它关联关系给建立起来，那么我们的目的是什么呢，目的将来，比如说我现在要研发某一个数据产品诶，哪一个公司有什么样的产品啊，哪一个哪一个产品在哪一个交易所挂牌啊，那么这些信息。

我们是不是都可以通过这张图谱来找到。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/2a87bbac9127b4f31f2b36b746bb29c3_3.png)

俺们是这样样，这张图谱将来会成为啊，推动数据要素产业发展的一个非常重要的一个，情报的支撑的一个来源啊，当然大模型还可以释放，我们千行百业的文档的价值，这个之前大家看到的很多，像最近那个啊一些大部分厂家。

像KIMI对吧，这些本质上都是在做这些事，但这里的路途也很遥远，仍然有很多工作要做啊，那么最后我想说啊，用一个简单的公式来总结啊，未来啊啊推动数据要素市场的一个重要方式。

或许是大模型加上数据要素在成千行百业，就是我们要注意要一方面把数据要素整理好，治理好，把数据集建好，另外一方面把数据用来练好一个大模型，然后再利用大模型啊的能力，再把我们的数据做得更好啊。

然后数据要素和大模型深度融合，协同发展，在我们千行应用啊，百页中的应用呢去反馈。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/2a87bbac9127b4f31f2b36b746bb29c3_5.png)

去验证，去迭代啊，好我的报告就到这里啊。