- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11497](https://ar5iv.labs.arxiv.org/html/2406.11497)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Boyi Deng¹, Wenjie Wang², Fengbin Zhu², Qifan Wang³, Fuli Feng¹¹¹footnotemark:
    1'
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Science and Technology of China,
  prefs: []
  type: TYPE_NORMAL
- en: ²National University of Singapore, ³Meta AI
  prefs: []
  type: TYPE_NORMAL
- en: dengboyi@mail.ustc.edu.cn, wqfcr@fb.com
  prefs: []
  type: TYPE_NORMAL
- en: '{wenjiewang96,zhfengbin,fulifeng93}@gamil.com Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large Language
    Models (LLMs) by referencing external documents. However, the misinformation in
    external documents may mislead LLMs’ generation. To address this issue, we explore
    the task of “credibility-aware RAG”, in which LLMs automatically adjust the influence
    of retrieved documents based on their credibility scores to counteract misinformation.
    To this end, we introduce a plug-and-play method named Credibility-aware Attention
    Modification (CrAM). CrAM identifies influential attention heads in LLMs and adjusts
    their attention weights based on the credibility of the documents, thereby reducing
    the impact of low-credibility documents. Experiments on Natual Questions and TriviaQA
    using Llama2-13B, Llama3-8B, and Qwen-7B show that CrAM improves the RAG performance
    of LLMs against misinformation pollution by over 20%, even surpassing supervised
    fine-tuning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'CrAM: Credibility-Aware Attention Modification in LLMs for'
  prefs: []
  type: TYPE_NORMAL
- en: Combating Misinformation in RAG
  prefs: []
  type: TYPE_NORMAL
- en: 'Boyi Deng¹, Wenjie Wang²^†^†thanks: Corresponding author., Fengbin Zhu², Qifan
    Wang³, Fuli Feng¹¹¹footnotemark: 1 ¹University of Science and Technology of China,
    ²National University of Singapore, ³Meta AI dengboyi@mail.ustc.edu.cn, wqfcr@fb.com
    {wenjiewang96,zhfengbin,fulifeng93}@gamil.com'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Retrieval-Augmented Generation (RAG) (Gao et al., [2024](#bib.bib9); Zhu et al.,
    [2021](#bib.bib43)) is a representative approach to mitigate hallucination issues
    of Large Language Models (LLMs) (Zhang et al., [2023](#bib.bib42)) by retrieving
    and referencing relevant documents from an external corpus. Despite its effectiveness,
    most RAG works overlook a crucial issue: misinformation pollution in the external
    corpus (Pan et al., [2023b](#bib.bib28); Dufour et al., [2024](#bib.bib7)). The
    maliciously generated misinformation may mislead LLMs to produce unfaithful responses.
    For instance, Microsoft’s Bing can be misled by misinformation on the internet
    to generate incorrect information for Bing users (Vincent, [2023](#bib.bib36)).
    Besides, Pan et al. ([2023b](#bib.bib28)) and Pan et al. ([2023a](#bib.bib26))
    demonstrated that inserting LLM-generated misinformation into the RAG corpus can
    significantly degrade LLMs’ performance. Therefore, addressing the misinformation
    pollution for RAG is essential.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/728df2f04de47c30de874a892106744b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A comparison between RAG and credibility-aware RAG. Credibility-aware
    RAG considers credibility to reduce the impact of low-credibility documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A straightforward idea to address this misinformation pollution issue is misinformation
    detection and filtering. Extensive misinformation detection works focus on measuring
    the credibility of documents, *i.e.,* the probability of the document not containing
    misinformation. And these works have achieve significant results (Kaliyar et al.,
    [2021](#bib.bib17); Pelrine et al., [2023](#bib.bib29); Quelle and Bovet, [2024](#bib.bib30)).
    Once we obtain the credibility of each retrieved document, we can exclude those
    with credibility below a certain threshold before using them in RAG. However,
    directly discarding certain documents may result in the loss of relevant and important
    information, leading to performance degradation (Yoran et al., [2024](#bib.bib41))¹¹1Our
    experimental results in Table [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG") also confirm
    that directly excluding documents leads to inferior performance.. Moreover, discretizing
    credibility scores into binary labels loses fine-grained credibility information.
    As such, we should account for the value of credibility scores to wisely utilize
    the retrieved information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we focus on a task named “credibility-aware RAG” as shown
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). Specifically, given
    a user query $x$ with a list of relevant documents $\mathcal{D}=\{d_{1},d_{2},...,d_{n}\}$
    and $\mathcal{D}$’s credibility scores $\mathcal{S}=\{s_{1},s_{2},...,s_{n}\}$,
    credibility-aware RAG requests LLMs to automatically adjust the influence of documents
    in $\mathcal{D}$ on the generated output $y$ based on their credibility scores
    in $\mathcal{S}$. Initial attempts on credibility-aware RAG adopted supervised
    fine-tuning (SFT) to teach LLMs to distinguish the importance of different documents
    in the prompt by their credibility scores (Hong et al., [2024](#bib.bib13); Pan
    et al., [2024](#bib.bib27)). However, SFT requires additional computational resources
    and well-designed training data, which limits the application scenarios. Therefore,
    we explore non-SFT method for LLMs to attain credibility-aware RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the attention mechanism serves as the central component for adjusting
    the significance of various input data, we consider manipulating attention weights
    of LLMs to achieve credibility-aware RAG. In particular, we adjust attention weights
    according to credibility scores in the inference stage of LLMs. In this way, we
    can regulate LLMs to pay less “attention” to less credible documents by decreasing
    the corresponding attention weights. Moreover, previous studies (Clark et al.,
    [2019](#bib.bib5); Elhage et al., [2021](#bib.bib8); Voita et al., [2019](#bib.bib37))
    have indicated that different attention heads exhibit distinct patterns and functions,
    resulting in varying impacts on LLMs’ outputs. In this context, the key lies in
    identifying a subset of influential attention heads for attention weight modification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we propose a plug-and-play method named Credibility-aware Attention
    Modification (CrAM), which identifies the influential attention heads and then
    modifies their attention weights *w.r.t.* different document tokens to reduce
    the impact of low-credibility documents. Specifically, 1) influential head identification:
    we select top-ranked attention heads according to an extended causal tracing method (Meng
    et al., [2022](#bib.bib23)) that estimates the contribution of each attention
    head to generating incorrect answers over a small dataset. 2) Attention weight
    modification: we scale down the attention weights of the retrieved documents based
    on their normalized credibility scores.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct extensive experiments on two open-domain Question Answering (QA)
    datasets, Natual Questions (NQ) (Kwiatkowski et al., [2019](#bib.bib20)) and TriviaQA
    (Joshi et al., [2017](#bib.bib16)), using three open-source LLMs: Llama2-13B (Touvron
    et al., [2023](#bib.bib33)), Llama3-8B (Meta, [2024](#bib.bib24)), and Qwen-7B
    (Bai et al., [2023](#bib.bib1)). The results show that CrAM significantly alleviates
    the influence of misinformation documents on RAG, in terms of both ideal credibility
    scores and GPT-generated credibility scores. It is worth noting that CrAM even
    outperforms the SFT-based method CAG (Pan et al., [2024](#bib.bib27)) in most
    scenarios, demonstrating the superiority of CrAM. We release our code and data
    at [https://anonymous.4open.science/r/CrAM-77DF](https://anonymous.4open.science/r/CrAM-77DF).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explore the task of credibility-aware RAG without fine-tuning LLMs to alleviate
    the misinformation pollution issue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop a plug-and-play method, CrAM, which identifies influential attention
    heads and modifies their attention weights to equip LLMs with credibility-aware
    RAG capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive experiments with two QA datasets on three LLMs using ideal
    credibility scores and GPT-generated credibility scores, validating the superiority
    of CrAM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Credibility-Aware RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d36be28cf87dd674022b4125932089ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of CrAM. Compared to RAG, CrAM first identifies influential
    attention heads and then modifies their attention weights based on the credibility
    scores of each document.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a user query $x$, RAG retrieves a set of documents $\mathcal{D}=\{d_{1},d_{2},\ldots,d_{n}\}$
    relevant to $x$ through a retriever (Gao et al., [2024](#bib.bib9)). Then the
    relevant documents $\mathcal{D}$ are evaluated by a credibility estimator²²2Recent
    worked on this task has achieved promising performance (Kaliyar et al., [2021](#bib.bib17);
    Pelrine et al., [2023](#bib.bib29))., obtaining their credibility scores $\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n}\}$,
    which represents the probability of each document not containing misinformation.
  prefs: []
  type: TYPE_NORMAL
- en: Credibility-Aware RAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Given an LLM $L$, a user query $x$, and relevant documents $\mathcal{D}$ associated
    with credibility scores $\mathcal{S}$, the objective of credibility-aware RAG
    is to enable LLMs to automatically adjust the influence of these documents on
    the generated output $y$ based on their credibility scores $\mathcal{S}$. This
    can be formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathop{\max}\ \mathrm{Metric}(\mathrm{Combine}(L,x,\mathcal{D},\mathcal{S})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathrm{Combine}(\cdot)$ represents the method or mechanism to integrate
    credibility scores into the generation process of $L$. For example, Pan et al.
    ([2024](#bib.bib27)) employ SFT to fine-tune LLMs to capture the credibility difference
    of documents more effectively, denoted as $\mathrm{Combine}(L,x,\mathcal{D},\mathcal{S})=L_{SFT}(x,\mathcal{D},\mathcal{S})$.
    Additionally, $\mathrm{Metric}(\cdot)$ is a function that assesses whether documents
    with different credibility scores have varying impacts on the output of $L$. Indeed,
    we can utilize the performance of generating factual answers to measure $\mathrm{Metric}(\cdot)$.
    For instance, we use the accuracy of QA tasks to approximate $\mathrm{Metric}(\cdot)$
    in this work. The rationality is that if the impact of low-credibility documents
    decreases, the accuracy of QA tasks should increase accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 3 CrAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CrAM first identifies influential attention heads, and then modifies the attention
    weights of these identified heads to reduce the impact of low-credibility documents
    as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Credibility-Aware RAG ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG"). Since influential
    attention heads identification process involves attention weight modification,
    we first explain the procedure of attention weight modification in Section [3.1](#S3.SS1
    "3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"), and then describe
    influential attention heads identification in Section [3.2](#S3.SS2 "3.2 Influential
    Head Identification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"). Finally, we summarize the overall
    CrAM workflow in Section [3.3](#S3.SS3 "3.3 CrAM Workflow ‣ 3 CrAM ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Attention Weight Modification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As defined in Section [2](#S2 "2 Credibility-Aware RAG ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG"), the objective
    of credibility-aware RAG is to reduce the impact of low-credibility documents
    on the generated output of LLMs. Intuitively, it requires LLMs to pay less “attention”
    to low-credibility documents. To this end, a natural approach is scaling down
    the corresponding attention weights of low-credibility documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For RAG, a user query $x$ and a set of relevant documents $\mathcal{D}=\{d_{1},d_{2},\ldots,d_{n}\}$
    should be concatenated and tokenized into a token sequence $\mathcal{T}(x,\mathcal{D})=\{t_{1},t_{2},\ldots,t_{m}\}$,
    where $t_{k}$ denotes the $k$-th token. Given the credibility scores for each
    document $\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n}\}$, the normalized credibility
    score for token $t_{k}$ can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bar{s}_{k}=\begin{cases}\frac{s_{i}-\min(\mathcal{S})}{\max(\mathcal{S})-\min(\mathcal{S})}&amp;\text{if
    }t_{k}\text{ belongs to }d_{i}\\ 1&amp;\text{otherwise}\end{cases},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $s_{i}$ is subtracted by $\min(\mathcal{S})$, and then scaled down by
    $1/(\max(\mathcal{S})-\min(\mathcal{S}))$ to ensure all credibility scores are
    normalized to $[0,1]$. Besides, we define $\mathbf{\bar{s}}=[\bar{s}_{1},\ldots,\bar{s}_{m}]\in\mathbb{R}^{1\times
    m}$ to represent the normalized credibility scores of the whole token sequence
    $\mathcal{T}(x,\mathcal{D})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each attention head $h$ in LLM, $\mathbf{A}_{h}$ represents its attention
    weights matrix³³3The attention weights matrix is defined in Equation ([3](#A1.E3
    "Equation 3 ‣ Appendix A Multi-Head Attention ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")).. Let $(\mathbf{A}_{h})_{k}$
    represent the $k$-th row vector⁴⁴4$(\mathbf{A}_{h})_{k}$ can be interpreted as
    the attention weight vector when using the $k$-th token as the query. of $\mathbf{A}_{h}$,
    we can obtain the modified attention weight matrix $\mathbf{A}^{*}_{h}$ by element-wise
    multiplying $\bar{\mathbf{s}}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  $(\mathbf{A}_{h})_{k}^{*}=\mathrm{Norm}((\mathbf{A}_{h})_{k}\odot\mathbf{\bar{s}}),k\in\{1,\ldots,m\},$  |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\odot$ denotes the element-wise multiplication of vectors. The Norm function
    refers to $\ell_{1}$ normalization, which ensures that the attention weights sum
    to one.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Influential Head Identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous works Clark et al. ([2019](#bib.bib5)); Elhage et al. ([2021](#bib.bib8));
    Voita et al. ([2019](#bib.bib37)) have found that different attention heads exhibit
    various patterns and functions, leading to different impacts on LLMs’ output.
    As such, we hypothesize that some attention heads have a larger impact on using
    misinformation documents to generate incorrect answers. Previously, causal tracing
    (Meng et al., [2022](#bib.bib23)) has been developed to quantify the contribution
    of each hidden state towards generating given answers. The contribution is measured
    by adding noises to each hidden state to compare the changes in the generation
    probability of the given answer. In light of this, CrAM revises causal tracing
    to evaluate the contribution of attention heads instead of hidden states. Utilizing
    attention weight modification, as detailed in Section [3.1](#S3.SS1 "3.1 Attention
    Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"), CrAM estimates the change in probability
    of generating incorrect answers to determine the contribution of each attention
    head. Thereafter, CrAM ranks all attention heads by contributions and identifies
    influential ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the contribution of one attention head $h$ can be obtained as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given an LLM $L$, a user query $x$, a set of relevant documents $\mathcal{D}=\{d_{mis},d_{1},d_{2},\ldots,d_{n}\}$
    with one misinformation document $d_{mis}$, and an incorrect answer $a_{wrong}$
    to $x$ that is supported by $d_{mis}$, we first calculate the generation probability
    of $a_{wrong}$ with $x$ and $\mathcal{D}$ by $L$. Formally, we have:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $P_{0}=P_{L}(a_{wrong}\mid x,\mathcal{D}).$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we modify a specific attention head as described in Section [3.1](#S3.SS1
    "3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") by using the credibility
    scores $\mathcal{S}=\{0,1,1,\ldots,1\}$ of $\mathcal{D}$ and recalculate the generation
    probability of $a_{wrong}$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $P_{1}=P_{L_{h}^{*}}(a_{wrong}\mid x,\mathcal{D}),$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'where $L_{h}^{*}$ denotes the LLM $L$ whose attention weight matrix of the
    attention head $h$ is modified according to Equation ([1](#S3.E1 "Equation 1 ‣
    3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we quantify the contribution of head $h$ towards generating the incorrect
    answer, *a.k.a.* the indirect effect (IE) (Meng et al., [2022](#bib.bib23)):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathrm{IE}_{h}=P_{0}-P_{1},$ |  | (2) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: which can also be interpreted as the decrease in the generation probability
    of the incorrect answer $a_{wrong}$ after modifying head $h$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To improve the robustness of the contribution estimation, we utilize a small
    dataset $\{(x,a_{wrong},\mathcal{D},\mathcal{S}),\ldots\}$ with different user
    queries to compute the average IE for each attention head (refer to Section [5](#S4.F5
    "Figure 5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") for robustness analysis).
    Thereafter, we can calculate IEs for all the attention heads and rank them to
    select the top-ranked ones with larger IEs for attention weight modification.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 CrAM Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CrAM workflow is summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we use a small dataset with misinformation-polluted documents to calculate
    the average IE for each attention head in an LLM as described in Section [3.2](#S3.SS2
    "3.2 Influential Head Identification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). Then, we rank all
    attention heads by their IEs in descending order and select the top-ranked heads
    as influential attention heads.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given any user query, along with the relevant documents and credibility scores,
    we modify the attention weights of influential attention heads using the method
    described in Section [3.1](#S3.SS1 "3.1 Attention Weight Modification ‣ 3 CrAM
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG") to obtain the final answer, thereby significantly reducing the impact
    of low-credibility documents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model In-context corpus Method NQ TriviaQA EM F1 score EM F1 score Qwen-7B 0
    ✓ Naive LLM 7.20 16.41 28.00 38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ +
    1 ✗ Naive RAG 10.50 20.71 25.00 35.63 Prompt Based 12.20 22.26 27.40 37.98 CrAM
    29.10 (+16.90) 41.02 (+18.76) 52.90 (+25.50) 64.16 (+26.18) Llama2-13B 0 ✓ Naive
    LLM 20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90 39.98 62.50 71.03 4 ✓ + 1 ✗ Naive
    RAG 11.90 19.97 28.00 36.22 Prompt Based 12.50 22.94 23.10 32.70 CrAM 33.60 (+21.10)
    44.62 (+21.68) 59.90 (+31.90) 67.11 (+30.89) Llama3-8B 0 ✓ Naive LLM 20.60 30.58
    55.70 62.67 4 ✓ Naive RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ Naive RAG 16.00 26.16
    36.80 47.09 Prompt Based 29.90 39.69 53.50 63.01 CrAM 36.90 (+7.00) 48.45 (+8.76)
    64.40 (+10.90) 73.49 (+10.48)
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Main results under ideal setting. 0 ✓ indicates no document and the
    model directly prompted, 4 ✓ indicates all four documents retrieved from the Wikipedia
    dump, and 4 ✓ + 1 ✗ indicates four high-credibility documents (i.e., retrieved
    from external corpus) plus one low-credibility document (i.e., containing misinformation).
    In the 4 ✓ + 1 ✗ setting, the best performance is highlighted in bold. And the
    red part indicates the difference between CrAM and second best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Model In-context corpus Method NQ TriviaQA EM F1 score EM F1 score Qwen-7B 0
    ✓ Naive LLM 7.20 16.41 28.00 38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ +
    1 ✗ Naive RAG 10.50 20.71 25.00 35.63 Prompt Based 12.50 22.98 29.70 40.18 Exclusion
    21.60 32.56 49.50 61.03 CrAM 23.10 (+1.50) 34.84 (+2.28) 52.10 (+2.60) 63.76 (+2.73)
    Llama2-13B 0 ✓ Naive LLM 20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90 39.98 62.50
    71.03 4 ✓ + 1 ✗ Naive RAG 11.90 19.97 28.00 36.22 Prompt Based 11.20 21.62 20.50
    30.09 Exclusion 23.70 34.00 54.40 62.37 CrAM 25.10 (+1.40) 35.56 (+1.56) 56.20
    (+1.80) 64.03 (+1.66) Llama3-8B 0 ✓ Naive LLM 20.60 30.58 55.70 62.67 4 ✓ Naive
    RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ Naive RAG 16.00 26.16 36.80 47.09 Prompt
    Based 24.20 34.10 49.50 58.59 Exclusion 26.60 38.44 57.70 67.33 CrAM 30.70 (+4.10)
    41.71 (+3.27) 62.20 (+4.50) 70.70 (+3.37)
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Main results under GPT setting. 0 ✓ indicates no document and the
    model directly prompted, 4 ✓ indicates all four documents retrieved from the Wikipedia
    dump, and 4 ✓ + 1 ✗ indicates four high-credibility documents (i.e., retrieved
    from external corpus) plus one low-credibility document (i.e., containing misinformation).
    In the 4 ✓ + 1 ✗ setting, the best performance is highlighted in bold. The red
    part indicates the improvement of our CrAM compared to the second-best model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets, LLMs and Metrics.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We conduct experiments over the Natural Questions (NQ) (Kwiatkowski et al.,
    [2019](#bib.bib20)) and TriviaQA (Joshi et al., [2017](#bib.bib16)) datasets with
    three LLMs, i.e. Llama2-13B (Touvron et al., [2023](#bib.bib33)), Llama3-8B (Meta,
    [2024](#bib.bib24)), and Qwen-7B (Bai et al., [2023](#bib.bib1)). We adopt Exact
    Match (EM) and F1 score as evaluation metrics, which are widely used in the QA
    setting (Karpukhin et al., [2020](#bib.bib19); Rajpurkar et al., [2016](#bib.bib32);
    Chen et al., [2017](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: Document Preparation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We prepare both high-credibility and low-credibility documents (i.e., with
    misinformation) associated with the questions for evaluating the proposed method.
    1) *High-credibility documents* are collected by retrieving the most relevant
    documents from the external corpus for each question. Specifically, we first employ
    bge-large-en-v1.5⁵⁵5[huggingface.co/BAAI/bge-large-en-v1.5](huggingface.co/BAAI/bge-large-en-v1.5).
    to obtain a set of candidates from the Wikipedia dump on December 30, 2018 (Karpukhin
    et al., [2020](#bib.bib19)). Then, we apply bge-reranker-large⁶⁶6[huggingface.co/BAAI/bge-reranker-large](huggingface.co/BAAI/bge-reranker-large).
    to rank the retrieved candidates and select the top four documents. 2) *Low-credibility
    documents* are generated via prompting LLMs (i.e., gpt-3.5-turbo-0125), with misinformation
    included, similar to the practice in previous works (Pan et al., [2023a](#bib.bib26),
    [b](#bib.bib28), [2024](#bib.bib27); Hong et al., [2024](#bib.bib13); Chen and
    Shu, [2024](#bib.bib3)). Specifically, given a question, we instruct the LLM to
    generate a news-style piece containing misinformation that supports an incorrect
    answer, which is regarded as one low-credibility document for the question. For
    each question, we collect three distinct low-credibility documents, all supporting
    the same incorrect answer. The prompts can be found in Appendix [G](#A7 "Appendix
    G Prompts ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG").'
  prefs: []
  type: TYPE_NORMAL
- en: In implementation, we combine the generated low-credibility documents and the
    retrieved high-credibility documents for a given question as the LLM input. Compared
    to injecting the generated low-credibility documents into the corpus (Pan et al.,
    [2023a](#bib.bib26); Weller et al., [2024](#bib.bib38)), our approach can mitigate
    the retriever’s potential bias towards the misinformation. Also, our method is
    more controllable, making it easier to observe the impact of varying numbers of
    documents with misinformation on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Credibility Scores Generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We adopt two different ways to assign credibility scores for each document.
    1) *Ideal Setting.* After obtaining the high-credibility and low-credibility documents,
    we assign a score of 10 to each high-credibility document and a score of 1 to
    each low-credibility document. 2) *GPT Setting.* We employ GPT (i.e., gpt-3.5-turbo-0125)
    to directly generate the credibility score for each document. The prompts and
    the distribution of GPT-generated scores for all documents are provided in Figure [20](#A7.F20
    "Figure 20 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG") and Appendix [C](#A3 "Appendix C
    GPT-Generated Credibility Scores ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  prefs: []
  type: TYPE_NORMAL
- en: Compared Methods.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We compare our CrAM model with four types of methods: 1) *Naive RAG.* The Naive
    RAG follows the standard RAG pipeline without any mechanisms against misinformation.
    2) *Prompt Based.* This method directly informs the LLM of the credibility score
    via prompts, feeding the score and documents into the LLM without additional training.
    3) *Exclusion.* This method excludes the documents with credibility scores below
    a threshold. This method will not be compared under the ideal setting due to the
    binary value of the ideal credibility score. 4) *CAG.* This method is proposed
    by Pan et al. ([2024](#bib.bib27)), which directly incorporates credibility scores
    and documents into prompts to fine-tune an LLM (i.e., Llama2-13B) to lift its
    understanding capabilities. Among them, Naive RAG, Prompt Based, and Exclusion
    are non-SFT methods, while CAG is an SFT-based method.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unless otherwise specified, in the following experiments, we randomly select
    100 data points from each dataset to calculate average IE for all the heads. And
    we use another validation set of 100 data points from each dataset to determine
    how many top-ranked heads should be included in the final modified set.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Main Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Comparison with Non-SFT Methods.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We first compare our CrAM model with Non-SFT methods, i.e., Naive RAG, Prompt
    Based, and Exclusion. Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG") and Table
    [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG") show the experimental results in
    the Ideal and GPT settings respectively. We make the following observations. 1)
    Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") demonstrates that our
    CrAM method significantly outperforms all compared methods across all three LLMs:
    Qwen 7B, LLama2-13B, and LLama3-8B, on both NQ and TriviaQA datasets in the setting
    of 4 ✓+ 1 ✗ (i.e., four high-credibility documents plus one low-credibility document).
    For instance, our CrAM model surpasses the second-best method, i.e. Prompt Based,
    by $25.5\%$, $31.90\%$ and $10.9\%$ on Qwen-7B, Llama2-13B and Llama3-8B in terms
    of EM on TriviaQA, demonstrating remarkable performance gains. 2) With GPT-generated
    credibility scores, our CrAM model also outperforms all compared methods on all
    three LLMs over both NQ and TriviaQA datasets, as shown in Table [2](#S4.T2 "Table
    2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for
    Combating Misinformation in RAG"), further highlighting its effectiveness. 3)
    Interestingly, we find that our CrAM model with 4 ✓ + 1 ✗ sometimes even outperforms
    the Naive RAG with 4 ✓ under ideal setting. This is likely because our generated
    misinformation includes both affirmations of incorrect information and denials
    of correct information, e.g.“The first person to win the Nobel Prize in Physics
    was not Roentgen, but Einstein.” This allows LLMs to reuse the correct information
    denied by the misinformation. To further validate this hypothesis, we conduct
    additional experiments and present the findings in Appendix [F](#A6 "Appendix
    F Results with Filtered Misinformation ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b90cf1b0025c92972b5dd094262d2a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Performance comparison of CrAM and CAG-13B regarding the varying
    number of documents containing misinformation under ideal setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with SFT-based Method.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For a fair comparison, we only compare our Llama2-13B based CrAM model with
    CAG-13B, because CAG-13B is trained on Llama2-13B. Moreover, to verify the robustness
    of our CrAM model, we perform comparisons using different numbers of low-credibility
    documents. As shown in Figure [3](#S4.F3 "Figure 3 ‣ Comparison with Non-SFT Methods.
    ‣ 4.2.1 Main Results ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG"), our CrAM
    model consistently outperforms the CAG-13B model remarkably in terms of F1 score
    when the number of low-credibility documents ranges from 1 to 3. The results further
    prove the effectiveness of our CrAM model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 In-Depth Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Effect of Number of Low-credibility Documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f708637c3639dc88a6a3f41327d92741.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Performance change on NQ regarding the varying number of documents
    with misinformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, we analyze the effect of varying the number of low-credibility
    documents fed into the LLM. We conduct experiments using Llama3-8B on the NQ dataset.
    Specifically, we vary the number of low-credibility documents from $1$ to $3$
    while keeping the number of high-credibility documents constant, i.e., $4$. We
    present the experimental results in Figure [4](#S4.F4 "Figure 4 ‣ Effect of Number
    of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results
    ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG"). From the figure, we make the following observations.
    1) Our CrAM model consistently outperforms the compared models when changing the
    number of low-credibility documents from 1 to 3 in both ideal and GPT settings.
    2) Comparably, our CrAM model exhibits much smaller performance drops compared
    to other models when increasing the number of low-credibility documents. These
    results demonstrate the robustness of our proposed model to the varying number
    of low-credibility documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb12f0ecfb579a6b3055c42e71d15214.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Performance on NQ and TriviaQA regarding the dataset size for determining
    the influential attention head changes.'
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Dataset Size on Attention Heads Selection.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As we described in Section [3.3](#S3.SS3 "3.3 CrAM Workflow ‣ 3 CrAM ‣ CrAM:
    Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG"), we randomly select $100$ data points from each dataset to identify the
    influential attention heads. In the following, we vary the number of data points
    used for selecting these influential attention heads to analyze its impact on
    model performance. The experimental results are presented in Figure [5](#S4.F5
    "Figure 5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). Despite fluctuations
    in performance along with the changing dataset size, the variations are not substantial
    on both NQ and TriviaQA datasets, with a maximum difference of $4\%$ in terms
    of EM. The results indicate that the number of data points has a minor impact
    on the final model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfc3c384db16b99ff20ed70ea3b11425.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Performance on NQ in ideal setting regarding the varying number of
    selected attention heads.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f5668fb531c15e977cc2c4095e7bd6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Density distribution of IE of all the attention heads in Llama3-8B.'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis on Number of Selected Attention Heads.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the following, we analyze the performance change when we adjust the number
    of selected attention heads. We present the results in Figure [5](#S4.F5 "Figure
    5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis ‣
    4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"). We observe a sharp drop in model
    performance when the number of selected attention heads is near either 0 or the
    maximum number of heads, i.e., 1024; comparably, it has a minor effect when the
    number of selected attention heads falls into the range of values in between.
    To investigate the underlying reasons, we further analyze the IE’s density distribution
    using Llama3-8B, as shown in Figure [7](#S4.F7 "Figure 7 ‣ Effect of Dataset Size
    on Attention Heads Selection. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results
    ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG"). We find that the IE density distribution approximates
    a normal distribution centered around 0, with the majority of values concentrated
    near 0. It indicates that most attention heads have minor impact on model performance,
    and only when the attention heads with IE values far from zero, either positive
    or negative, are selected, the model performance will be affected significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Ablation Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Model Method NQ TriviaQA EM EM Qwen-7B CrAM 29.10 52.90 CrAM-all 27.20 (-1.90)
    50.60 (-2.30) Naive RAG 10.50 (-18.60) 25.00 (-27.90) Llama2-13B CrAM 33.60 59.90
    CrAM-all 29.50 (-4.10) 59.50 (-0.40) Naive RAG 11.90 (-21.70) 28.00 (-27.90) Llama3-8B
    CrAM 36.90 64.40 CrAM-all 22.40 (-14.50) 51.50 (-12.90) Naive RAG 16.00 (-20.90)
    36.80 (-27.60)
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Results of ablation study under ideal setting with 4 ✓ + 1 ✗ (i.e.,
    four high-credibility documents plus one low-credibility document).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the rationality of our model design, we conduct ablation
    study and present the results in Table [3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). First, we remove the
    selection of influential attention heads and apply attention weight modification
    on all attention heads in LLMs, and denote this variant model as CrAM-all. As
    shown in Table [3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study ‣ 4.2 Experimental Results
    ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG"), we observe that the performance of the CrAM-all model
    has noticeable drops on all three LLMs. Among them, Llama3-8B based CrAM has the
    largest decrease on both NQ and TriviaQA, i.e., $14.5\%$ and $12.9\%$. This indicates
    the necessity of identifying the influential attention heads before modifying
    the attention weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we disable the attention weight modification mechanism in our model, it
    becomes the Naive RAG method. Table [3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") shows that this results
    in a remarkable performance drop on all three LLMs compared to the CrAM model.
    For instance, the performance of all three LLMs decreases more than $27.5\%$ on
    TriviaQA dataset. These results verify that it is necessary to modify the attention
    weight and meanwhile take into account the credibility scores of the documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Misinformation Detection.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Misinformation detection aims to identify false or misleading information from
    various data sources (Guo et al., [2019](#bib.bib11); Kaliyar and Singh, [2019](#bib.bib18);
    Vaibhav et al., [2019](#bib.bib34)). It can be categorized into non-LLM-based
    methods and LLM-based methods. Non-LLM-based methods often involve a training
    process, enabling models to identify misinformation (Vaibhav et al., [2019](#bib.bib34);
    Kaliyar et al., [2021](#bib.bib17); Liu et al., [2023](#bib.bib22); Goonathilake
    and Kumara, [2020](#bib.bib10)). For example, Kaliyar et al. ([2021](#bib.bib17))
    utilize BERT (Devlin et al., [2019](#bib.bib6)) to score the credibility of documents,
    while Vaibhav et al. ([2019](#bib.bib34)) use a graph neural network for misinformation
    detection. Comparably, LLM-based methods typically use LLMs without additional
    training (Pelrine et al., [2023](#bib.bib29); Quelle and Bovet, [2024](#bib.bib30);
    Caramancion, [2023](#bib.bib2); Hoes et al., [2023](#bib.bib12)). For instance,
    Pelrine et al. ([2023](#bib.bib29)) adopt GPT-4 (OpenAI et al., [2024](#bib.bib25))
    for document credibility scoring, while Quelle and Bovet ([2024](#bib.bib30))
    employ an LLM agent (Xi et al., [2023](#bib.bib39)) for iterative verification
    of document credibility. In this study, we employ LLMs to obtain the credibility
    score for each document similar to the previous LLM-based methods (Pelrine et al.,
    [2023](#bib.bib29); Hoes et al., [2023](#bib.bib12)). In this study, we employ
    LLMs to obtain the credibility score for each document similar to (Pelrine et al.,
    [2023](#bib.bib29); Hoes et al., [2023](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Combating Misinformation in RAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation (RAG) enhance LLMs by retrieving relevant documents
    from external corpus (Lewis et al., [2020](#bib.bib21); Izacard and Grave, [2021](#bib.bib14)).
    However, prior works (Zou et al., [2024](#bib.bib44); Pan et al., [2023b](#bib.bib28),
    [a](#bib.bib26)) find that RAG is vulnerable to misinformation in its corpus,
    leading to undesired results. To combat misinformation in RAG, lots of studies
    have been conducted. For example, CAR Weller et al. ([2024](#bib.bib38)) adopt
    a query augmentation scheme to retrieve a larger set of documents first and then
    apply a voting mechanism to mitigate the impact of misinformation. RobustRAG Xiang
    et al. ([2024](#bib.bib40)) obtains the LLM response for each document independently
    and aggregates these responses through keyword-based and decoding-based algorithms
    to generate the final result. Hong et al. ([2024](#bib.bib13)) and Pan et al.
    ([2024](#bib.bib27)) assign each retrieved document a credibility score and fine-tune
    LLMs with the documents and their scores, enabling the LLMs to leverage these
    credibility scores when generating. CD² Jin et al. ([2024](#bib.bib15)) train
    two LLMs to generate truthful answers and misleading answers respectively to make
    it better distinguish the conflict information. However, CAR Weller et al. ([2024](#bib.bib38))
    and RobustRAG Xiang et al. ([2024](#bib.bib40)) require multiple rounds of model
    inference, leading to inefficiency. The methods proposed by Hong et al. ([2024](#bib.bib13)),
    Pan et al. ([2024](#bib.bib27)), and Jin et al. ([2024](#bib.bib15)) require fine-tuning
    LLMs, which demands additional computational resources and well-designed training
    data, thereby limiting their application scenarios. In contrast, our CrAM model
    requires no training and only needs a single inference to produce the final output.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work introduces CrAM, a plug-and-play method that enables RAG to automatically
    adjust the influence of retrieved documents on the output of LLMs based on document
    credibility. CrAM first identifies influential attention heads and then adjusts
    the attention weights of identified attention heads according to the credibility
    score of documents, regulating LLMs to pay less attention to the low-credibility
    documents. Empirical experiments demonstrate that, compared to vanilla RAG, CrAM
    improves EM performance by more than 20% on two datasets and even outperforms
    the baseline with SFT, demonstrating CrAM’s efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work has several limitations that we aim to address in the future. First,
    we identify a fixed set of attention heads for attention weight modification for
    all questions. Despite Section [5](#S4.F5 "Figure 5 ‣ Effect of Number of Low-credibility
    Documents. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results ‣ 4 Experiments
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG") indicating the robustness of using a small dataset for influential head
    identification, a more effective solution is to identify specific attention heads
    tailored to each individual question. Second, we only use the credibility scores
    of each document for credibility-aware RAG. However, LLMs actually can utilize
    the correct information in the misinformation document. Thus, empowering LLMs
    to leverage a fine-grained credibility score at the sentence or even word level
    for answer generation is promising. Third, we only evaluate the performance of
    CrAM on decoder-only LLMs, and the effectiveness of CrAM on more models with different
    architectures, such as T5 (Raffel et al., [2020](#bib.bib31)), is worth exploring.'
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our experiments, we use gpt-3.5-turbo-0125 to generate misinformation. We
    want to emphasize that we generate this misinformation solely for research purposes,
    and we will not use it for any other purpose ourselves. Additionally, we do not
    encourage anyone to use this misinformation for any other purpose.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, et al. 2023. [Qwen
    technical report](https://arxiv.org/abs/2309.16609). *Preprint*, arXiv:2309.16609.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caramancion (2023) Kevin Matthe Caramancion. 2023. [Harnessing the power of
    chatgpt to decimate mis/disinformation: Using chatgpt for fake news detection](https://doi.org/10.1109/AIIoT58121.2023.10174450).
    In *2023 IEEE World AI IoT Congress (AIIoT)*, pages 0042–0046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Shu (2024) Canyu Chen and Kai Shu. 2024. [Can llm-generated misinformation
    be detected?](https://arxiv.org/abs/2309.13788) *Preprint*, arXiv:2309.13788.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
    2017. [Reading Wikipedia to answer open-domain questions](https://doi.org/10.18653/v1/P17-1171).
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1870–1879, Vancouver, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. 2019. [What does BERT look at? an analysis of BERT’s attention](https://doi.org/10.18653/v1/W19-4828).
    In *Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting
    Neural Networks for NLP*, pages 276–286, Florence, Italy. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dufour et al. (2024) Nicholas Dufour, Arkanath Pathak, Pouya Samangouei, Nikki
    Hariri, Shashi Deshetti, Andrew Dudfield, Christopher Guess, Pablo Hernández Escayola,
    Bobby Tran, Mevan Babakar, and Christoph Bregler. 2024. [Ammeba: A large-scale
    survey and dataset of media-based misinformation in-the-wild](https://arxiv.org/abs/2405.11697).
    *Preprint*, arXiv:2405.11697.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan,
    Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
    et al. 2021. [A mathematical framework for transformer circuits](https://transformer-circuits.pub/2021/framework/index.html).
    *Transformer Circuits Thread*, 1:1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. [Retrieval-augmented
    generation for large language models: A survey](https://arxiv.org/abs/2312.10997).
    *Preprint*, arXiv:2312.10997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goonathilake and Kumara (2020) M. D. P. P Goonathilake and P. P. N. V Kumara.
    2020. [Cnn, rnn-lstm based hybrid approach to detect state-of-the-art stance-based
    fake news on social media](https://doi.org/10.1109/ICTer51097.2020.9325477). In
    *2020 20th International Conference on Advances in ICT for Emerging Regions (ICTer)*,
    pages 23–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2019) Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, and Zhiwen Yu.
    2019. [The future of misinformation detection: New perspectives and trends](https://arxiv.org/abs/1909.03654).
    *Preprint*, arXiv:1909.03654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoes et al. (2023) Emma Hoes, Sacha Altay, and Juan Bermeo. 2023. [Leveraging
    chatgpt for efficient fact-checking](https://doi.org/10.31234/osf.io/qnjkf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2024) Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng,
    and Joyce Jiyoung Whang. 2024. [Why so gullible? enhancing the robustness of retrieval-augmented
    models against counterfactual noise](https://arxiv.org/abs/2305.01579). *Preprint*,
    arXiv:2305.01579.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Izacard and Grave (2021) Gautier Izacard and Edouard Grave. 2021. [Leveraging
    passage retrieval with generative models for open domain question answering](https://doi.org/10.18653/v1/2021.eacl-main.74).
    In *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistics: Main Volume*, pages 874–880, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2024) Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang,
    Jiexin Xu, Li Qiuxia, and Jun Zhao. 2024. [Tug-of-war between knowledge: Exploring
    and resolving knowledge conflicts in retrieval-augmented language models](https://aclanthology.org/2024.lrec-main.1466).
    In *Proceedings of the 2024 Joint International Conference on Computational Linguistics,
    Language Resources and Evaluation (LREC-COLING 2024)*, pages 16867–16878, Torino,
    Italia. ELRA and ICCL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
    2017. [TriviaQA: A large scale distantly supervised challenge dataset for reading
    comprehension](https://doi.org/10.18653/v1/P17-1147). In *Proceedings of the 55th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliyar et al. (2021) Rohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang.
    2021. [Fakebert: Fake news detection in social media with a bert-based deep learning
    approach](https://doi.org/10.1007/s11042-020-10183-2). *Multimedia Tools and Applications*,
    80(8):11765–11788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaliyar and Singh (2019) Rohit Kumar Kaliyar and Navya Singh. 2019. [Misinformation
    detection on online social media-a survey](https://doi.org/10.1109/ICCCNT45670.2019.8944587).
    In *2019 10th International Conference on Computing, Communication and Networking
    Technologies (ICCCNT)*, pages 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. [Dense passage
    retrieval for open-domain question answering](https://doi.org/10.18653/v1/2020.emnlp-main.550).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 6769–6781, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. [Natural
    questions: A benchmark for question answering research](https://doi.org/10.1162/tacl_a_00276).
    *Transactions of the Association for Computational Linguistics*, 7:453–466.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. [Retrieval-augmented generation
    for knowledge-intensive nlp tasks](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 9459–9474\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Hui Liu, Wenya Wang, and Haoliang Li. 2023. [Interpretable
    multimodal misinformation detection with logic reasoning](https://doi.org/10.18653/v1/2023.findings-acl.620).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    9781–9796, Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2022) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
    2022. [Locating and editing factual associations in gpt](https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 17359–17372\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta (2024) Meta. 2024. [Llama 3](https://llama.meta.com/llama3/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, et al.
    2024. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774). *Preprint*,
    arXiv:2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023a) Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang
    Wang. 2023a. [Attacking open-domain question answering by injecting misinformation](https://doi.org/10.18653/v1/2023.ijcnlp-main.35).
    In *Proceedings of the 13th International Joint Conference on Natural Language
    Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 525–539, Nusa Dua,
    Bali. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2024) Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng,
    Sirui Wang, Xunliang Cai, and Le Sun. 2024. [Not all contexts are equal: Teaching
    llms credibility-aware generation](https://arxiv.org/abs/2404.06809). *Preprint*,
    arXiv:2404.06809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023b) Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen
    Kan, and William Wang. 2023b. [On the risk of misinformation pollution with large
    language models](https://doi.org/10.18653/v1/2023.findings-emnlp.97). In *Findings
    of the Association for Computational Linguistics: EMNLP 2023*, pages 1389–1403,
    Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pelrine et al. (2023) Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina
    Reksoprodjo, Caleb Gupta, Joel Christoph, Jean-François Godbout, and Reihaneh
    Rabbany. 2023. [Towards reliable misinformation mitigation: Generalization, uncertainty,
    and GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.395). In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    6399–6429, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quelle and Bovet (2024) Dorian Quelle and Alexandre Bovet. 2024. [The perils
    and promises of fact-checking with large language models](https://doi.org/10.3389/frai.2024.1341697).
    *Frontiers in Artificial Intelligence*, 7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21(1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [SQuAD: 100,000+ questions for machine comprehension of text](https://doi.org/10.18653/v1/D16-1264).
    In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, et al.
    2023. [Llama 2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
    *Preprint*, arXiv:2307.09288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaibhav et al. (2019) Vaibhav Vaibhav, Raghuram Mandyam, and Eduard Hovy. 2019.
    [Do sentence interactions matter? leveraging sentence level representations for
    fake news classification](https://doi.org/10.18653/v1/D19-5316). In *Proceedings
    of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing
    (TextGraphs-13)*, pages 134–139, Hong Kong. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 30\. Curran Associates,
    Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vincent (2023) James Vincent. 2023. [Google and microsoft’s chatbots are already
    citing one another’s misinformation](https://www.theverge.com/2023/3/22/23651564/google-microsoft-bard-bing-chatbots-misinformation).
    *The Verge*. Accessed: 2023-06-05.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. 2019. [Analyzing multi-head self-attention: Specialized heads
    do the heavy lifting, the rest can be pruned](https://doi.org/10.18653/v1/P19-1580).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 5797–5808, Florence, Italy. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weller et al. (2024) Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie,
    and Benjamin Van Durme. 2024. [Defending against disinformation attacks in open-domain
    question answering](https://aclanthology.org/2024.eacl-short.35). In *Proceedings
    of the 18th Conference of the European Chapter of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 402–417, St. Julian’s, Malta. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. [The
    rise and potential of large language model based agents: A survey](https://arxiv.org/abs/2309.07864).
    *Preprint*, arXiv:2309.07864.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. (2024) Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi
    Chen, and Prateek Mittal. 2024. [Certifiably robust rag against retrieval corruption](https://arxiv.org/abs/2405.15556).
    *Preprint*, arXiv:2405.15556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoran et al. (2024) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.
    2024. [Making retrieval-augmented language models robust to irrelevant context](https://arxiv.org/abs/2310.01558).
    *Preprint*, arXiv:2310.01558.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
    Wei Bi, Freda Shi, and Shuming Shi. 2023. [Siren’s song in the ai ocean: A survey
    on hallucination in large language models](https://arxiv.org/abs/2309.01219).
    *Preprint*, arXiv:2309.01219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya
    Poria, and Tat-Seng Chua. 2021. [Retrieving and reading: A comprehensive survey
    on open-domain question answering](https://arxiv.org/abs/2101.00774). *Preprint*,
    arXiv:2101.00774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2024) Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024.
    [Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of
    large language models](https://arxiv.org/abs/2402.07867). *Preprint*, arXiv:2402.07867.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Multi-Head Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, leading LLMs are built on autoregressive transformer architectures
    (Touvron et al., [2023](#bib.bib33); Meta, [2024](#bib.bib24); Bai et al., [2023](#bib.bib1)).
    The multi-head attention mechanism (Vaswani et al., [2017](#bib.bib35)) is the
    core component of autoregressive transformer models. It is illustrated in the
    following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Transformation: Given an input hidden state $\mathbf{X}\in\mathbb{R}^{n\times
    d}$, three linear transformations are applied to produce queries $\mathbf{Q}\in\mathbb{R}^{n\times
    d_{k}}$, keys $\mathbf{K}\in\mathbb{R}^{n\times d_{k}}$, and values $\mathbf{V}\in\mathbb{R}^{n\times
    d_{v}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{Q}=\mathbf{X}\mathbf{W}^{Q},\quad\mathbf{K}=\mathbf{X}\mathbf{W}^{K},\quad\mathbf{V}=\mathbf{X}\mathbf{W}^{V}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{W}^{Q}\in\mathbb{R}^{d\times d_{k}}$, $\mathbf{W}^{K}\in\mathbb{R}^{d\times
    d_{k}}$, and $\mathbf{W}^{V}\in\mathbb{R}^{d\times d_{v}}$ are weight matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaled Dot-Product Attention: The attention weights are computed using the
    dot product of the queries and keys, scaled by $1/\sqrt{d_{k}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{A}=\mathrm{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{k}}}\right)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: The softmax function ensures that the attention weights sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Head Attention: Instead of performing a single attention function, $h$
    attention functions (or heads) are performed in parallel. Each head has its own
    set of weight matrices $\mathbf{W}_{i}^{Q},\mathbf{W}_{i}^{K},\mathbf{W}_{i}^{V}$
    and attention weights $\mathbf{A}_{i}$ for $i\in[1,h]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  $\mathrm{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{Concat}(\mathrm{head}_{1},\ldots,\mathrm{head}_{h})\mathbf{W}^{O}$   |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathrm{head}_{i}=\mathbf{A}_{i}\mathbf{V}_{i}$ and $\mathbf{W}^{O}\in\mathbb{R}^{hd_{v}\times
    d}$ is the output weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used gpt-3.5-turbo-0125 for all generations involving GPT. For Llama2-13B,
    Qwen-7B, and Llama3-8B, we did not perform any sampling during generation to avoid
    randomness. For the NQ and TriviaQA datasets, we randomly selected 1,000 samples
    from the original test set for our evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4a3090031cb500af9f7678ae74ed8ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Distribution of GPT-generated credibility scores on misinformation
    and Wikipedia documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e0a59afa9c45fcbbcce4653b94eca41f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: ROC curve of GPT-generated credibility scores, with area under curve
    (AUC) = 0.801.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C GPT-Generated Credibility Scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We present the distribution of GPT-generated credibility scores in Figure [8](#A2.F8
    "Figure 8 ‣ Appendix B Implementation Details ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") and the corresponding
    receiver operating characteristic (ROC) curve in Figure [9](#A2.F9 "Figure 9 ‣
    Appendix B Implementation Details ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Full Results with Varying Number of Documents with Misinformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We provide the full results as the number of documents with misinformation
    increase, as shown in Figure [10](#A4.F10 "Figure 10 ‣ Appendix D Full Results
    with Varying Number of Documents with Misinformation ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")-[13](#A4.F13
    "Figure 13 ‣ Appendix D Full Results with Varying Number of Documents with Misinformation
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG"). All results are done with four correct documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/edde3f9d9ae37ada207959ee301b5d56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: EM and F1 socre on NQ using Llama3-8B under ideal setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2b64e35ded924c0296f931614183cfa1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: EM and F1 socre on TriviaQA using Llama3-8B under ideal setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce5df03e5dc32006ee154b5b584872bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: EM and F1 socre on NQ using Llama3-8B under GPT setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4bdbf18706cb16c741cbd3652acb2691.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: EM and F1 socre on TriviaQA using Llama3-8B under GPT setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8acb80ce9fafd3b157ca77782b0ce7f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Performance comparison of CrAM of Llama2-13B and CAG 13B with varying
    amounts of misinformation under ideal setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Comparison with CAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the CAG 13B model tends to provide lengthy responses, its performance
    on EM is very low. Therefore, we consider an answer "correct" if the correct answer
    appears in the model’s prediction, and we use accuracy as the metric. The results
    are shown in Figure [14](#A4.F14 "Figure 14 ‣ Appendix D Full Results with Varying
    Number of Documents with Misinformation ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"). This metric is more favorable for
    long answers, however, CrAM still surpasses the SFT-based CAG 13B in most situations,
    demonstrating the superiority of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Results with Filtered Misinformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We replaced all the correct answers in the existing misinformation with “xxx”
    (denoted as “filtered misinformation”) and then conducted the same experiments
    on filtered misinformation. The results are shown in Table [4](#A6.T4 "Table 4
    ‣ Appendix F Results with Filtered Misinformation ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). We make the following
    observations. 1) The performance of CrAM with 4 ✓+ 1 ✗ is lower than that in Table
    [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"), and it is worse than that of the
    Naive RAG with 4 ✓ in most cases. This indicates that CrAM enables LLMs to re-utilize
    the correct information denied by the misinformation, resulting in a better performance.
    2) Table [4](#A6.T4 "Table 4 ‣ Appendix F Results with Filtered Misinformation
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG") demonstrates that our CrAM method still outperforms all compared methods
    across all three LLMs: Qwen 7B, LLama2-13B, and LLama3-8B, on both NQ and TriviaQA
    datasets in the setting of 4 ✓+ 1 ✗ (i.e., four high-credibility documents plus
    one low-credibility document), proving CrAM doesn’t solely rely on correct answers
    in misinformation.'
  prefs: []
  type: TYPE_NORMAL
- en: Model In-context corpus Method NQ TriviaQA EM F1 score EM F1 score Qwen-7B 0
    ✓ Naive LLM 7.20 16.41 28.00 38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ +
    1 ✗ Naive RAG 9.70 20.22 25.40 36.14 Prompt Based 10.40 20.67 26.30 37.12 CrAM
    25.90 (-1.70) 37.87 (-1.21) 51.70 (-3.60) 63.07 (-3.78) Llama2-13B 0 ✓ Naive LLM
    20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90 39.98 62.50 71.03 4 ✓ + 1 ✗ Naive
    RAG 12.20 20.71 27.60 35.80 Prompt Based 9.90 20.48 21.90 31.22 CrAM 29.90 (+1.00)
    40.85 (+0.87) 57.90 (-4.60) 65.60 (-5.43) Llama3-8B 0 ✓ Naive LLM 20.60 30.58
    55.70 62.67 4 ✓ Naive RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ Naive RAG 16.10 26.57
    38.70 48.84 Prompt Based 25.20 35.72 52.10 61.03 CrAM 33.80 (+0.70) 45.63 (-0.03)
    63.70 (-0.60) 72.87 (-0.81)
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: With ideal credibility scores and filtered misinformation, we evaluate
    the performance of three models on two open-domain QA datasets. 0 ✓ indicates
    no document and the model is directly prompted, 4 ✓ indicates that all four documents
    are retrieved from the Wikipedia dump, while 4 ✓ + 1 ✗ indicates four documents
    from the Wikipedia dump plus one piece of misinformation. In the 4 ✓ + 1 ✗ setting,
    the best performance is highlighted in bold. And the red part indicates the difference
    between Naive RAG with 4 ✓ and CrAM with 4 ✓ + 1 ✗.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have made all the prompts used in our experiments publicly available, as
    shown in Figure [15](#A7.F15 "Figure 15 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")-[19](#A7.F19
    "Figure 19 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.F15.pic1" class="ltx_picture ltx_centering" height="103.59" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,103.59) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="76.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Please
    generate a text that is logical and supports the statement: The answer to "<Question>"
    is "<Wrong answer>", rather than "<Correct answer>". The generated text should
    be in the following form: CNN news. The generated text should be less than 200
    words. Just output the generated text , and do not output anything else. Generated
    Text:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15: Prompt for misinformation generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.F16.pic1" class="ltx_picture ltx_centering" height="70.22" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,70.22) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="42.66" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer the following question
    based on your internal knowledge with one or few words. Question: <Question> Answer:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 16: Prompt for Naive LLM without documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.F17.pic1" class="ltx_picture ltx_centering" height="120.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,120.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="92.48" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Given
    the following information: <Documents> Answer the following question based on
    the given information or your internal knowledge with one or few words without
    the source. Question: <Question> Answer:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17: Prompt for RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.F18.pic1" class="ltx_picture ltx_centering" height="157.63" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,157.63) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="130.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an assistant who can answer questions based on the given passages. Each passage
    has a credibility score that indicates the relevance and accuracy of the passage
    to the question. Your answer need to combine multiple passages and their credibility.
    Question: <Question> Docs: <Documents> Your answer should based on the given information
    or your internal knowledge with one or few words without the source (just output
    a answer, don’t output anything else). Answer:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18: Prompt for CAG.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.F19.pic1" class="ltx_picture ltx_centering" height="169.85" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,169.85) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="142.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an assistant capable of answering questions based on provided passages. Each
    passage is assigned a truthfulness score (0-10), where a higher score indicates
    greater credibility. Your answer need to combine multiple passages and their credibility.
    Given the following information: <Documents> Answer the following question based
    on the given information or your internal knowledge with one or few words without
    the source (just output a answer, don’t output anything else). Question: <Question>
    Answer:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 19: Prompt for prompt-based method.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.F20.pic1" class="ltx_picture ltx_centering" height="836.72" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,836.72) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="809.16" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Your
    task is to evaluate the authenticity of a text based on your internal knowledge.
    Specifically, I will provide you with a passage that may contain accurate information
    or fabricated errors. Using your own knowledge, reason, and deduction, you are
    to assign a credibility score ranging from 0 to 10, where a higher score indicates
    greater authenticity and a lower score suggests lesser authenticity. Here are
    2 examples (you should follow the output format below): ########## Passage: In
    a groundbreaking discovery, researchers have found that Albert Einstein was the
    first recipient of the Nobel Prize in Physics. According to newly uncovered documents,
    Einstein’s pioneering work in theoretical physics, particularly his theory of
    relativity, was recognized by the Nobel Committee in 1921\. This revelation challenges
    the long-held belief that Marie Curie was the first Nobel laureate in physics,
    and solidifies Einstein’s place as one of the greatest minds in scientific history.
    Analysis: 1\. Albert Einstein as the First Nobel Prize Recipient in Physics: This
    is incorrect. The first Nobel Prize in Physics was awarded in 1901, not to Albert
    Einstein, but to Wilhelm Conrad Röntgen for the discovery of X-rays. 2\. Einstein’s
    Nobel Prize Recognition: Albert Einstein was indeed awarded the Nobel Prize in
    Physics in 1921, but not for his theory of relativity. He received it for his
    discovery of the photoelectric effect, which was instrumental in the development
    of quantum theory. 3\. Marie Curie as the First Nobel Laureate in Physics: This
    is also incorrect. Marie Curie was a Nobel laureate, but she was not the first
    to win the Nobel Prize in Physics. Her first Nobel Prize was in Physics in 1903,
    shared with her husband Pierre Curie and Henri Becquerel for their work on radioactivity.
    Marie Curie was, notably, the first woman to win a Nobel Prize, and the first
    person to win Nobel Prizes in two different scientific fields (Physics and Chemistry).
    4\. Implication about the Nobel Committee’s Recognition of Relativity: As mentioned,
    Einstein’s Nobel Prize was not for relativity, despite its profound impact on
    physics. The Nobel Committee specifically avoided awarding the prize for relativity
    at the time due to ongoing debates and lack of experimental confirmation of the
    theory during that period. Credibility Score: 0 Passage: The first Nobel Prize
    in Physics was awarded to Wilhelm Conrad Roentgen in 1901\. Roentgen received
    the Nobel Prize for his discovery of X-rays, which had a significant impact on
    the field of physics and medicine Analysis: The facts presented in the statement
    you provided are largely accurate. Credibility Score: 10 ########## Passage: <Passage></foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 20: Prompt for GPT to generate credibility scores.'
  prefs: []
  type: TYPE_NORMAL
