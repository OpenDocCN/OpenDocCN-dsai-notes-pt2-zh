- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
- en: Huiqiang Jiang¹¹1Equal contribution. ^◆Work during internship at Microsoft.,
    Yucheng Li^◆¹¹1Equal contribution. ^◆Work during internship at Microsoft., Chengruidong
    Zhang¹¹1Equal contribution. ^◆Work during internship at Microsoft., Qianhui Wu,
    Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 姜辉强¹¹1等贡献。 ^◆微软实习期间工作。李宇程^◆¹¹1等贡献。 ^◆微软实习期间工作。张成瑞栋¹¹1等贡献。 ^◆微软实习期间工作。吴千辉，罗旭芳，
- en: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 安素仁，韩振华，阿米尔·H·阿卜迪，李东生，林镇耀，杨钰清，邱莉莉
- en: Microsoft Corporation, ^◆University of Surrey {hjiang,chengzhang,yuqyang}@microsoft.com,yucheng.li@surrey.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司，^◆萨里大学 {hjiang,chengzhang,yuqyang}@microsoft.com，yucheng.li@surrey.ac.uk
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The computational challenges of Large Language Model (LLM) inference remain
    a significant barrier to their widespread deployment, especially as prompt lengths
    continue to increase. Due to the quadratic complexity of the attention computation,
    it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the
    pre-filling stage) on a single A100 GPU. Existing methods for speeding up pre-filling
    often fail to maintain acceptable accuracy or efficiency when applied to long-context
    LLMs. To address this gap, we introduce MInference (Million-tokens Inference),
    a sparse calculation method designed to accelerate pre-filling of long-sequence
    processing. Specifically, we identify three unique patterns in long-context attention
    matrices—the A-shape, Vertical-Slash, and Block-Sparse—that can be leveraged for
    efficient sparse computation on GPUs. We determine the optimal pattern for each
    attention head offline and dynamically build sparse indices based on the assigned
    pattern during inference. With the pattern and sparse indices, we perform efficient
    sparse attention calculations via our optimized GPU kernels to significantly reduce
    the latency in the pre-filling stage of long-context LLMs. Our proposed technique
    can be directly applied to existing LLMs without any modifications to the pre-training
    setup or additional fine-tuning. By evaluating on a wide range of downstream tasks,
    including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including
    LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that
    MInference effectively reduces inference latency by up to $10\times$ for pre-filling
    on an A100, while maintaining accuracy. Our code is available at [https://aka.ms/MInference](https://aka.ms/MInference).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）推理的计算挑战仍然是其广泛部署的重大障碍，特别是当提示长度不断增加时。由于注意力计算的二次复杂性，8B LLM在单个A100 GPU上处理1M
    tokens的提示（即预填充阶段）需要30分钟。现有的加速预填充的方法通常在应用于长上下文LLM时，无法保持令人满意的准确性或效率。为了解决这一问题，我们介绍了MInference（百万token推理），一种稀疏计算方法，旨在加速长序列处理的预填充。具体而言，我们在长上下文注意力矩阵中识别出三种独特的模式——A形、垂直斜线和块稀疏——可以用于在GPU上进行高效的稀疏计算。我们离线确定每个注意力头的最佳模式，并在推理过程中根据分配的模式动态构建稀疏索引。利用模式和稀疏索引，我们通过优化的GPU内核执行高效的稀疏注意力计算，从而显著降低长上下文LLM预填充阶段的延迟。我们提出的技术可以直接应用于现有LLM，无需对预训练设置进行任何修改或额外的微调。通过在包括InfiniteBench、RULER、PG-19和Needle
    In A Haystack在内的广泛下游任务以及LLaMA-3-1M、GLM-4-1M、Yi-200K、Phi-3-128K和Qwen2-128K等模型上进行评估，我们证明MInference在A100上有效地将推理延迟降低了高达$10\times$，同时保持了准确性。我们的代码可以在[https://aka.ms/MInference](https://aka.ms/MInference)获得。
- en: \doparttoc\faketableofcontents![Refer to caption](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents![参见说明](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
- en: (a) Needle In A Haystack
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 针对大海捞针
- en: '![Refer to caption](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
- en: (b) Latency Speedup
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 延迟加速
- en: 'Figure 1: Attention weights, especially in long-context LLMs, exhibit up to
    96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic
    sparse attention to accelerate the pre-filling stage of long-context LLM inference.
    It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b),
    and matches or surpasses baselines, as demonstrated by Needle In A Haystack [[35](#bib.bib35)]
    in (a) on LLaMA-3-8B-1M [[24](#bib.bib24)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：注意力权重，尤其是在长上下文 LLMs 中，在 128K 上下文中表现出高达 96.8% 的稀疏性。我们提出了 MInference，利用动态稀疏注意力加速长上下文
    LLM 推理的预填充阶段。它在单个 A100 上对 1M 上下文实现了最高 10 倍的加速，如图(b)所示，并且在 LLaMA-3-8B-1M [[24](#bib.bib24)]
    上的 Needle In A Haystack [[35](#bib.bib35)] 实验中，结果匹配或超越了基线。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have entered the era of long-context processing,
    with some of them supporting context windows ranging from 128K to 10M tokens [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)].
    These extended context windows enable LLMs to unlock a multitude of complex real-world
    applications, such as repository-level code understanding [[7](#bib.bib7), [34](#bib.bib34),
    [58](#bib.bib58)], long-document question-answering [[9](#bib.bib9), [51](#bib.bib51)],
    extreme-label in-context learning [[51](#bib.bib51)], and long-horizon agent tasks [[79](#bib.bib79)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经进入了长上下文处理的时代，其中一些支持从 128K 到 10M tokens 的上下文窗口 [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)]。这些扩展的上下文窗口使
    LLMs 能够解锁大量复杂的现实世界应用，如仓库级代码理解 [[7](#bib.bib7), [34](#bib.bib34), [58](#bib.bib58)]，长文档问答
    [[9](#bib.bib9), [51](#bib.bib51)]，极端标签上下文学习 [[51](#bib.bib51)] 和长时地平线代理任务 [[79](#bib.bib79)]。
- en: 'However, due to the quadratic complexity of attention, it can take several
    minutes for the model to process the input prompt (i.e., the pre-filling stage)
    and then start to produce the first token, which leads to unacceptable Time To
    First Token experience, thus greatly hinders the wide application of long-context
    LLMs. As shown in Fig. [2a](#S2.F2.sf1 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), when serving LLaMA-3-8B on a single A100
    machine, the model would keep users waiting for 6 minutes to finish the pre-filling
    stage given a prompt of 300K tokens, and this number increases to 30 minutes for
    a prompt of 1M tokens. The overhead of self-attention computation exceeds 90%
    of the total pre-filling latency, which makes it the major bottleneck in long-context
    processing of LLMs. Previous research has shown that the attention matrices are
    highly sparse [[47](#bib.bib47), [17](#bib.bib17)], which has led to the development
    of fixed sparse attention methods such as Longformer [[6](#bib.bib6)] and BigBird [[87](#bib.bib87)].
    However, prior studies have also noted that attention distributions vary significantly
    across different inputs [[39](#bib.bib39), [47](#bib.bib47)]. This dynamic nature
    prevents prior sparse methods from being used directly on long-context LLMs without
    expensive training or fine-tuning. But if the dynamic sparse attention patterns
    could be efficiently predicted online, the pre-filling latency of long-context
    LLMs could be significantly reduced by calculating only the most important part
    of the attention weights.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于注意力的二次复杂性，模型处理输入提示（即预填充阶段）并开始生成第一个 token 可能需要几分钟，这导致了不可接受的首次 token 时间体验，从而极大地阻碍了长上下文
    LLMs 的广泛应用。如图 [2a](#S2.F2.sf1 "在图 2 ‣ 2 注意力头：动态、稀疏和特性 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLMs 的预填充") 所示，在单个 A100 机器上服务 LLaMA-3-8B 时，给定 300K tokens 的提示，模型会让用户等待 6 分钟才能完成预填充阶段，而对于
    1M tokens 的提示，这一时间增加到 30 分钟。自注意力计算的开销超过了总预填充延迟的 90%，这使其成为 LLMs 长上下文处理的主要瓶颈。以前的研究表明，注意力矩阵高度稀疏
    [[47](#bib.bib47), [17](#bib.bib17)]，这导致了如 Longformer [[6](#bib.bib6)] 和 BigBird
    [[87](#bib.bib87)] 等固定稀疏注意力方法的发展。然而，先前的研究还注意到，不同输入的注意力分布差异显著 [[39](#bib.bib39),
    [47](#bib.bib47)]。这种动态特性使得以前的稀疏方法无法直接用于长上下文 LLMs，除非进行昂贵的训练或微调。但是，如果可以高效地在线预测动态稀疏注意力模式，则通过仅计算最重要的注意力权重部分，可以显著减少长上下文
    LLMs 的预填充延迟。
- en: 'Building upon this idea, we present MInference, a technique that reduces 95%
    of FLOPs in the attention computation to significantly accelerate the pre-filling
    stage of long-context LLM inference via dynamic sparse attention. Unlike existing
    dynamic sparse attention methods that introduce large computational overhead to
    estimate attention patterns with low-rank hidden dimensions [[47](#bib.bib47),
    [63](#bib.bib63)], our method is designed specifically for long-context scenarios
    with minimal overhead in estimation. Specifically, we conduct extensive analysis
    and identify three general patterns of sparse attention in long-context LLMs:
    A-shape pattern, Vertical-Slash pattern, and Block-Sparse pattern. Based on these
    findings, we introduce a kernel-aware search method to assign the optimal attention
    pattern for each head. Importantly, instead of fixed attention masks in prior
    studies, we perform an efficient online approximation to build a dynamic sparse
    mask for each head according to their assigned pattern and particular inputs.
    For example, to build a dynamic sparse mask for a specific prompt on one Vertical-Slash
    head, we use a partial of attention weight consisting of the last last_q query
    and key vectors (i.e. $\bm{Q}_{[-\text{last\_q}:]}$) to estimate the most important
    indices of the vertical and slash lines globally on the attention matrix. For
    Block-Sparse heads, we perform mean pooling on both query and key vectors in blocks
    of 64 and calculate the block-level attention weights to determine the most important
    blocks and thereby obtain a block-sparse dynamic mask. After obtaining the dynamic
    sparse mask, three optimized GPU kernels are used, which we developed for the
    above three sparse patterns. These kernels are based on the dynamic sparse compilers
    PIT [[88](#bib.bib88)], Triton [[75](#bib.bib75)] and FlashAttention [[13](#bib.bib13)],
    which enable extremely efficient computation of dynamic sparse attention.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们提出了 MInference，这是一种通过动态稀疏注意力显著加速长上下文 LLM 推理的预填充阶段的技术，减少了注意力计算中 95% 的
    FLOPs。与现有的动态稀疏注意力方法不同，这些方法通过低秩隐藏维度估计注意力模式，带来大量计算开销 [[47](#bib.bib47), [63](#bib.bib63)]，我们的方法专门为长上下文场景设计，估计开销最小。具体而言，我们进行了广泛的分析，并识别出长上下文
    LLM 中稀疏注意力的三种通用模式：A 形模式、垂直斜杠模式和块稀疏模式。基于这些发现，我们引入了一种内核感知搜索方法，为每个注意力头分配最佳注意力模式。重要的是，与先前研究中的固定注意力掩码不同，我们进行高效的在线近似，为每个头部根据其分配的模式和特定输入构建动态稀疏掩码。例如，为了为一个垂直斜杠头部的特定提示构建动态稀疏掩码，我们使用部分注意力权重，由最后的
    `last_q` 查询和键向量（即 $\bm{Q}_{[-\text{last\_q}:]}$）组成，以估计在注意力矩阵上垂直和斜线的最重要的索引。对于块稀疏头部，我们对查询和键向量进行
    64 的块均值池化，并计算块级别的注意力权重，以确定最重要的块，从而获得块稀疏动态掩码。获得动态稀疏掩码后，我们使用三种优化的 GPU 内核，这些内核是我们为上述三种稀疏模式开发的。这些内核基于动态稀疏编译器
    PIT [[88](#bib.bib88)]，Triton [[75](#bib.bib75)] 和 FlashAttention [[13](#bib.bib13)]，使动态稀疏注意力的计算极为高效。
- en: Extensive experiments are conducted on various Long-context LLMs, including
    LLaMA-3-8B-1M [[24](#bib.bib24)], GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)],
    across benchmarks with context lengths over 1M tokens, such as InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], Needle In A Haystack [[35](#bib.bib35)], and PG-19 [[65](#bib.bib65)].
    Needle In A Haystack was also tested on Phi-3-Mini-128K [[2](#bib.bib2)] and Qwen-2-7B-128K [[5](#bib.bib5)].
    Results show that MInference speeds up the pre-filling stage by up to $10\times$
    for 1M contexts with LLaMA-3-8B on a single A100, reducing latency from 30 minutes
    to 3 minutes per prompt, while maintaining or improving accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种长上下文 LLM 上进行广泛实验，包括 LLaMA-3-8B-1M [[24](#bib.bib24)]，GLM-4-9B-1M [[26](#bib.bib26)]
    和 Yi-9B-200K [[84](#bib.bib84)]，涵盖了上下文长度超过 1M tokens 的基准测试，例如 InfiniteBench [[86](#bib.bib86)]，RULER [[28](#bib.bib28)]，Needle
    In A Haystack [[35](#bib.bib35)] 和 PG-19 [[65](#bib.bib65)]。Needle In A Haystack
    还在 Phi-3-Mini-128K [[2](#bib.bib2)] 和 Qwen-2-7B-128K [[5](#bib.bib5)] 上进行了测试。结果表明，MInference
    将 1M 上下文的预填充阶段加速高达 $10\times$，在单个 A100 上使用 LLaMA-3-8B，将延迟从每个提示的 30 分钟减少到 3 分钟，同时保持或提高了准确性。
- en: '2 Attention Heads: Dynamic, Sparse, and Characteristic'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力头：动态、稀疏和特征
- en: '![Refer to caption](img/10ccd929778db2d7ea98214ca2a9092c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10ccd929778db2d7ea98214ca2a9092c.png)'
- en: (a) Attention incurs heavy cost.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力会产生很大的成本。
- en: '![Refer to caption](img/84a510695ce9529c4982da1844ae2d16.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84a510695ce9529c4982da1844ae2d16.png)'
- en: (b) Attention is sparse.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是稀疏的。
- en: '![Refer to caption](img/804139198ed227c4ddf295cd89c1f3cd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/804139198ed227c4ddf295cd89c1f3cd.png)'
- en: (c) Sparsity of attention is dynamic.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力的稀疏性是动态的。
- en: 'Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention
    scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention
    scores are retrieved when reusing the top-k indices from another examples, indicating
    its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：(a) 预填充阶段的延迟细分。(b) 在128k上下文中，前k（k=4096）列能够覆盖多少注意力分数。(c) 当重用来自其他示例的前k索引时，检索到的注意力分数较少，表明其动态特性。可视化基于单个A100的LLaMa-3-8B。
- en: 2.1 Attention is Dynamically Sparse
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 注意力是动态稀疏的
- en: 'The sparsity of attention weights in pre-trained LLMs, especially in long-context
    scenarios, has been well-documented [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]. As shown in Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ 2 Attention
    Heads: Dynamic, Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), for an attention matrix
    of size $128k\times 128k$, retaining only the top 4k columns recalls 96.8% of
    the total attention. In other words, each token is attending to a limit number
    of tokens despite the long sequence it is processing.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练LLM中的注意力权重的稀疏性，尤其是在长上下文场景中，已被充分记录[[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]。如图[2b](#S2.F2.sf2 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")所示，对于大小为$128k\times
    128k$的注意力矩阵，仅保留前4k列可以回忆96.8%的总注意力。换句话说，每个令牌尽管处理的是长序列，但关注的令牌数量是有限的。
- en: 'On the other hand, although the sparse nature of attention matrices is shared
    across different inputs, the exact distributions of sparse pattern are highly
    dynamic. That is to say, a token at a given position only attends to a subset
    of the sequence in self-attention, and the exact tokens it attends to are highly
    context-dependent and vary significantly across different prompts. This dynamism
    has been mathematically demonstrated in prior studies [[39](#bib.bib39), [40](#bib.bib40)].
    As depicted in Fig. [2c](#S2.F2.sf3 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), if we take the top 4k columns found in Fig. [2b](#S2.F2.sf2
    "In Figure 2 ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and apply it on another prompt of 128k, the recall of attention would drop largely
    to 83.7%.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管注意力矩阵的稀疏特性在不同输入间是共享的，但稀疏模式的具体分布是高度动态的。也就是说，给定位置的令牌在自注意力中仅关注序列的一部分，它所关注的具体令牌高度依赖于上下文，并且在不同的提示中变化显著。这种动态性在先前的研究中已被数学证明[[39](#bib.bib39),
    [40](#bib.bib40)]。如图[2c](#S2.F2.sf3 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")所示，如果我们在图[2b](#S2.F2.sf2
    "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中找到前4k列，并将其应用于另一个128k的提示，注意力的回忆率将大幅下降至83.7%。
- en: 2.2 Attention Sparsity Exhibits Patterns
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 注意力稀疏性表现出模式
- en: 'Table 1: Comparison of different sparse patterns.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同稀疏模式的比较。
- en: '| Patterns | A-shape | Vertical-Slash | Block-Sparse | Top-K |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | A形 | 垂直斜杠 | 块稀疏 | 前K个 |'
- en: '| Spatial Distribution | Static structured | Dynamic structured | Dynamic structured
    | Dynamic fine-grained |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 空间分布 | 静态结构 | 动态结构 | 动态结构 | 动态细粒度 |'
- en: '| Latency on GPU | Low | Medium | Low | High |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPU上的延迟 | 低 | 中等 | 低 | 高 |'
- en: '| Time to build the index | Zero | Small | Small | High | ![Refer to caption](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| 构建索引的时间 | 零 | 小 | 小 | 高 | ![参见说明](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
- en: (a) Attention patterns
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力模式
- en: '![Refer to caption](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
- en: (b) Attention is spatial clustering
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是空间聚类
- en: '![Refer to caption](img/155b7edb1253cb7cf43bccc0db737600.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/155b7edb1253cb7cf43bccc0db737600.png)'
- en: (c) Attention pattern recall
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力模式回忆
- en: 'Figure 3: (a) Visualization of attention weights from different attention heads.
    For different prompts and tasks, the pattern of the same head is relatively consistent,
    but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest
    non-zero element in the attention matrix. (c) Attention recall distribution using
    our identified patterns, where FLOPs in the kernel refer to the real FLOPs required
    for sparse attention computing using on GPUs. Here, a 1x64 block size is used
    for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs.
    All visualization are based on LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：（a）来自不同注意力头的注意力权重可视化。对于不同的提示和任务，相同头部的模式相对一致，但稀疏索引动态变化。（b）注意力矩阵中前10个最近非零元素的距离。（c）使用我们识别的模式的注意力回忆分布，其中内核中的FLOPs指的是使用GPU进行稀疏注意力计算所需的实际FLOPs。这里，对于Vertical-Slash模式使用1x64的块大小，对于其他模式在GPU上使用64x64的块大小。所有可视化均基于LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)]。
- en: 'Although the sparsity distribution of attention matrix is dynamic, previous
    works [[82](#bib.bib82), [29](#bib.bib29)] have shown that they exhibit certain
    patterns in the two-dimensional space such as spatial clustering. Through our
    analysis of long-context prompts of various lengths and tasks, we have categorized
    such attention sparse patterns into the A-shape, Vertical-Slash (VS), and Block-Sparse
    patterns, as shown in Fig. [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Table [1](#S2.T1
    "Table 1 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") details the characteristics and differences
    between these three patterns.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力矩阵的稀疏分布是动态的，之前的研究[[82](#bib.bib82), [29](#bib.bib29)]表明，它们在二维空间中表现出某些模式，如空间聚类。通过对各种长度和任务的长上下文提示进行分析，我们将这种注意力稀疏模式归类为A形、Vertical-Slash
    (VS) 和Block-Sparse模式，如图 [3a](#S2.F3.sf1 "在图3中 ‣ 2.2 注意力稀疏性表现出模式 ‣ 2 注意力头：动态、稀疏和特征
    ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充") 和图 [4](#S3.F4 "图4 ‣ 3 MInference 1.0
    ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")。表 [1](#S2.T1 "表1 ‣ 2.2 注意力稀疏性表现出模式
    ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充") 详细说明了这三种模式的特征和差异。
- en: A-shape pattern The attention weights of these types of heads are concentrated
    on initial tokens and local windows [[82](#bib.bib82), [29](#bib.bib29)], exhibiting
    relatively higher stability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: A形模式 这些类型的头部的注意力权重集中在初始标记和局部窗口[[82](#bib.bib82), [29](#bib.bib29)]，表现出相对较高的稳定性。
- en: Vertical-Slash (VS) pattern The attention weights are concentrated on specific
    tokens (vertical lines) and tokens at fixed intervals (slash lines). The positions
    of vertical and slash lines in this pattern dynamically change with the context
    content and exhibit a certain sparsity, making them difficult to be encompassed
    by local windows and A-shape patterns.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Vertical-Slash (VS)模式 注意力权重集中在特定标记（垂直线）和固定间隔的标记（斜线）。这种模式中的垂直线和斜线的位置随着上下文内容动态变化，并表现出一定的稀疏性，使其难以被局部窗口和A形模式所涵盖。
- en: 'Block-Sparse pattern This sparsity pattern is the most dynamic, exhibiting
    a more dispersed distribution. Despite its dynamism, the attention weights maintain
    some characteristics of spatial clustering, which we identify as the block-sparse
    pattern. We analyzed the distances between non-zero attention weights and their
    top-k nearest non-zero neighbors within a 128k prompt as shown in Fig. [3b](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"). The results indicate that across layers and
    heads, the distances between nearest non-zero values are generally concentrated
    around 5, suggesting a strong spatial clustering of the attention weights.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Block-Sparse** 模式 这种稀疏模式最具动态性，表现出更分散的分布。尽管其动态性较强，注意力权重仍保持一些空间聚类的特征，我们将其识别为**块稀疏模式**。我们分析了在128k提示内非零注意力权重及其前k个最近非零邻居之间的距离，如图[3b](#S2.F3.sf2
    "在图3 ‣ 2.2 注意力稀疏性展示模式 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")所示。结果表明，在不同的层和头之间，最近非零值之间的距离通常集中在5左右，表明注意力权重有较强的空间聚类。'
- en: 'The point of these three patterns is that we can leverage them to perform highly
    efficient sparse computing for the attention matrix in long-context LLMs. In Fig. [3c](#S2.F3.sf3
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we test how efficient is our indentified
    patterns retrieving attention scores with limit computing cost on GPU (FLOPs).
    First, attention heads are labeled with one of the sparse pattern (detail see
    §[3.2](#S3.SS2 "3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
    ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")). Then we demonstrate our patterns are significantly
    more efficient compared to other sparse methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)]. Specifically, with the same amount of FLOPs, our patterns achieve
    a notable higher recall on attention scores, which can potentially lead to better
    accuracy. For example, previous Top-K methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)] struggle with the Block-Sparse pattern as they focus on specific
    tokens globally, while our pattern retrieves attention scores more efficiently
    and accurately. We example how we use these patterns on long-context LLMs and
    how we implement optimized GPU kernels for these patterns in §[3](#S3 "3 MInference
    1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种模式的重点是我们可以利用它们对长上下文LLMs中的注意力矩阵执行高效的稀疏计算。在图[3c](#S2.F3.sf3 "在图3 ‣ 2.2 注意力稀疏性展示模式
    ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中，我们测试了我们识别的模式在有限的计算成本下（FLOPs）检索注意力分数的效率。首先，注意力头被标记为某种稀疏模式（详细见§[3.2](#S3.SS2
    "3.2 通过动态稀疏注意力加速长上下文LLM推理 ‣ 3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")）。然后我们演示了我们的模式相较于其他稀疏方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]显著更高效。具体而言，在相同的FLOPs量下，我们的模式在注意力分数上获得了显著更高的召回率，这可能会导致更好的准确性。例如，以前的Top-K方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]在处理**块稀疏模式**时存在困难，因为它们全球性地关注特定令牌，而我们的模式能够更高效、更准确地检索注意力分数。我们在§[3](#S3
    "3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中示例了我们如何在长上下文LLMs中使用这些模式，以及如何为这些模式实现优化的GPU内核。
- en: 3 MInference 1.0
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 MInference 1.0
- en: 'Following the analysis in §[2](#S2 "2 Attention Heads: Dynamic, Sparse, and
    Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs
    via Dynamic Sparse Attention"), we propose MInference to accelerate the pre-filling
    stage of long-context LLMs, consisting of three steps: 1) Offline attention pattern
    identification for each head; 2) Dynamic build of sparse indices w.r.t. the pattern;
    3) Sparse attention calculation with optimized GPU kernels.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据§[2](#S2 "2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中的分析，我们提出了MInference以加速长上下文LLMs的预填充阶段，该过程包括三个步骤：1）离线识别每个头的注意力模式；2）根据模式动态构建稀疏索引；3）使用优化的GPU内核计算稀疏注意力。
- en: '![Refer to caption](img/717aebceb4eff85c06852353dc924b36.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/717aebceb4eff85c06852353dc924b36.png)'
- en: 'Figure 4: The three sparse methods in MInference.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MInference中的三种稀疏方法。
- en: 3.1 Problem Formulation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: 'When accelerating the pre-filling stage of long-context LLMs with sparse attention
    computing, the attention matrix can be formulated as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当加速长上下文LLMs的稀疏注意力计算的预填充阶段时，注意力矩阵可以表示如下：
- en: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
- en: where $M_{i,j}\in\{0,1\}$ of the attention matrix. Here, $c$ have values approaching
    zero after the softmax, i.e., $A_{i,j}\approx 0$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，注意力矩阵中的 $M_{i,j}\in\{0,1\}$。这里，$c$ 在 softmax 之后的值接近零，即 $A_{i,j}\approx 0$。
- en: 'The goal of the dynamic sparse attention system is to achieve greater speedup
    with minimal overhead while retaining as much of the attention weights as possible.
    Formally, this can be expressed as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稀疏注意力系统的目标是在保留尽可能多的注意力权重的同时，实现更大的加速并保持最小的开销。正式地，这可以表示为：
- en: '|  | $\displaystyle\min$ |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  | (2) |'
- en: '|  | $\displaystyle\min$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  |'
- en: where $t_{\text{sparse}}$ represent the time spent on dynamic sparse attention
    computation and estimation of the approximate dynamic sparse pattern, respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{\text{sparse}}$ 分别表示动态稀疏注意力计算和近似动态稀疏模式估计所花费的时间。
- en: Algorithm 1 Kernel-Aware Sparse Pattern Search
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 核心感知稀疏模式搜索
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$, search space $\rho$,
    initialized search space $\sigma$ to $|\sigma|$     while $|t_{i}-t|></math>        <math
    id=$  end for# Search for optimal head pattern  $p_{\text{best}}\leftarrow\phi$  for $i\leftarrow
    1$ do     $\bm{y}_{i}\leftarrow\text{SparseAttention}(\bm{Q}\bm{K}^{\top}/\sqrt{d},\rho_{i})$  end for  $\mathrm{return}\,\,\,p_{\text{best}}$'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$，搜索空间 $\rho$，初始化搜索空间 $\sigma$
    为 $|\sigma|$     当 $|t_{i}-t|></math>        <math id=$  结束 for# 搜索最佳头模式  $p_{\text{best}}\leftarrow\phi$  对 $i\leftarrow
    1$ 执行     $\bm{y}_{i}\leftarrow\text{SparseAttention}(\bm{Q}\bm{K}^{\top}/\sqrt{d},\rho_{i})$  结束 for  $\mathrm{return}\,\,\,p_{\text{best}}$
- en: 3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 通过动态稀疏注意力加速长上下文LLM推理
- en: Kernel-Aware Optimal Sparse Pattern Search
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 核心感知最佳稀疏模式搜索
- en: 'To achieve the best accuracy with limited FLOPs budget, we propose an offline
    Kernel-Aware Optimal Sparse Pattern Search method. In this step, we determine
    which sparse pattern will be used for each attention head, and the optimal setting
    for the pattern in real calculation (e.g., the number of vertical/slash lines
    in VS pattern; or the number of top-k blocks in BS patterns). As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.1 Problem Formulation ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"), we first create
    the search space based on a target FLOPs for each pattern, ensuring all potential
    candidates (i.e., different patterns with different settings) have similar computational
    cost. Kernel-aware here indicates the computational cost reflects the real FLOPs
    in GPU kernels, instead of conceptual estimations, which is crucial to achieve
    the optimal acceleration.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在有限的FLOPs预算下实现最佳精度，我们提出了一种离线核心感知最佳稀疏模式搜索方法。在此步骤中，我们确定每个注意力头将使用的稀疏模式，以及在实际计算中的模式的最佳设置（例如，VS模式中的垂直/斜线数量；或BS模式中的top-k块数量）。如算法 [1](#alg1
    "算法 1 ‣ 3.1 问题表述 ‣ 3 MInference 1.0 ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文LLMs的预填充") 所示，我们首先基于每种模式的目标FLOPs创建搜索空间，确保所有潜在候选者（即，不同设置的不同模式）具有类似的计算成本。核心感知在这里表示计算成本反映了GPU内核中的实际FLOPs，而不是概念估计，这对于实现最佳加速至关重要。'
- en: Algorithm 2 Vertical-Slash Head
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 垂直斜线头
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ # Approximate vertical
    and slash pattern (last_q = 64)  $\bm{\widehat{A}}\leftarrow\mathrm{softmax}\left(\bm{Q}_{[-\text{last\_q}:]}\bm{K}^{\top}/\sqrt{d}+\bm{m}_{\text{casual}}\right)$
    vertical line, sum in vertical  $\bm{i}_{v}\leftarrow\mathrm{argtopk}\left(\mathrm{sum}_{v}(\bm{\widehat{A}}),k_{v}\right)$
    slash line, sum in slash  $\bm{i}_{s}\leftarrow\mathrm{argtopk}\left(\mathrm{sum}_{s}(\bm{\widehat{A}}),k_{s}\right)$  #
    Final dynamic sparse attention scores (only index block)  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{vs})/\sqrt{d}\right)$  $\mathrm{return}\,\,\,\bm{y}$'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ # 近似垂直和斜线模式（last_q =
    64） $\bm{\widehat{A}}\leftarrow\mathrm{softmax}\left(\bm{Q}_{[-\text{last\_q}:]}\bm{K}^{\top}/\sqrt{d}+\bm{m}_{\text{casual}}\right)$
    垂直线，垂直方向求和 $\bm{i}_{v}\leftarrow\mathrm{argtopk}\left(\mathrm{sum}_{v}(\bm{\widehat{A}}),k_{v}\right)$
    斜线，斜线方向求和 $\bm{i}_{s}\leftarrow\mathrm{argtopk}\left(\mathrm{sum}_{s}(\bm{\widehat{A}}),k_{s}\right)$
    # 最终动态稀疏注意力得分（仅索引块） $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{vs})/\sqrt{d}\right)$
    $\mathrm{return}\,\,\,\bm{y}$'
- en: Algorithm 3 Block-Sparse Head
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 块稀疏头
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ # Approximate block-sparse
    pattern (block_size = 64)  $\bm{\widehat{Q}}\leftarrow\mathrm{MeanPooling}(\bm{Q},block\_size)$  $\bm{\widehat{A}}\leftarrow\mathrm{softmax}\left(\bm{\widehat{Q}}\bm{\widehat{K}}^{\top}/\sqrt{d}+\bm{m}_{\text{casual}}\right)$
    blocks  $\bm{i}_{b}\leftarrow\mathrm{argtopk}\left(\bm{\widehat{A}},k_{b}\right)$  #
    Final dynamic sparse attention scores (only index block)  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{b})/\sqrt{d}\right)$  $\mathrm{return}\,\,\,\bm{y}$'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ # 近似块稀疏模式（block_size
    = 64） $\bm{\widehat{Q}}\leftarrow\mathrm{MeanPooling}(\bm{Q},block\_size)$ $\bm{\widehat{A}}\leftarrow\mathrm{softmax}\left(\bm{\widehat{Q}}\bm{\widehat{K}}^{\top}/\sqrt{d}+\bm{m}_{\text{casual}}\right)$
    blocks $\bm{i}_{b}\leftarrow\mathrm{argtopk}\left(\bm{\widehat{A}},k_{b}\right)$
    # 最终动态稀疏注意力得分（仅索引块） $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{b})/\sqrt{d}\right)$
    $\mathrm{return}\,\,\,\bm{y}$'
- en: Next, we go through the search space with a reference example to decide the
    optimal pattern and setting. Specifically, we use recall of the attention output
    as the objective criterion when searching for the best pattern. This approach
    leverages FlashAttention [[13](#bib.bib13)] to reduce GPU memory overhead and
    incorporates the information from the $\bm{V}$ matrix, enabling end-to-end selection
    of the best pattern, which further enhances performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过一个参考示例来遍历搜索空间，以确定**最佳模式**和设置。具体而言，我们使用注意力输出的**召回率**作为搜索最佳模式的目标标准。这种方法利用了FlashAttention [[13](#bib.bib13)]以减少GPU内存开销，并结合了来自$\bm{V}$矩阵的信息，实现了**端到端**的最佳模式选择，从而进一步提升了性能。
- en: Sparsity Indices Approximation and Dynamic Sparse Attention Calculation
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏索引近似和动态稀疏注意力计算
- en: 'During the inference stage, we will perform an online estimation on the attention
    matrix to dynamically determine the spatial distribution our sparse indices, based
    on the assigned patterns and the exact input. After that, we conduct the sparse
    attention computations with our optimized GPU kernels. The implementation details
    of our kernels can be found in Appendix [C.4](#A3.SS4 "C.4 Kernel Implementation
    ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"). Noted that the sparse mask is
    static for A-shape heads, so there is no overhead in building the dynamic masks,
    and only sparse calculation is required.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在推理阶段，我们将对注意力矩阵进行在线估计，以动态确定我们稀疏索引的空间分布，基于分配的模式和准确的输入。之后，我们使用优化的GPU内核进行稀疏注意力计算。我们内核的实现细节可以在附录 [C.4](#A3.SS4
    "C.4 Kernel Implementation ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")中找到。注意，对于A形状的头，稀疏掩码是静态的，因此在构建动态掩码时没有开销，只需进行稀疏计算即可。'
- en: '(i) Vertical-Slash head. As shown in Algorithm [2](#alg2 "Algorithm 2 ‣ Kernel-Aware
    Optimal Sparse Pattern Search ‣ 3.2 Speedup of Long-context LLM Inference via
    Dynamic Sparse Attention ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), due to the continuity of
    vertical and slash lines, we matmul the last query vector $\bm{Q}_{[-\text{last\_q}:]}$
    to produce the estimated attention matrix $\bm{\widehat{A}}$ and slash $\bm{i}_{s}$.
    Using these sparse indices, we perform block-sparse calculations of the attention
    weights and attention output.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '(i) 竖线斜线头。如算法[2](#alg2 "算法 2 ‣ 核心感知优化稀疏模式搜索 ‣ 3.2 通过动态稀疏注意力加速长上下文 LLM 推理 ‣
    3 MInference 1.0 ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文 LLM 的预填充")所示，由于竖线和斜线的连续性，我们将最后的查询向量
    $\bm{Q}_{[-\text{last\_q}:]}$ 进行矩阵乘法，以生成估计的注意力矩阵 $\bm{\widehat{A}}$ 和斜线 $\bm{i}_{s}$。利用这些稀疏索引，我们执行注意力权重和注意力输出的块稀疏计算。'
- en: '(ii) Block-Sparse head. Per Algorithm [3](#alg3 "Algorithm 3 ‣ Kernel-Aware
    Optimal Sparse Pattern Search ‣ 3.2 Speedup of Long-context LLM Inference via
    Dynamic Sparse Attention ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), mean pooling is applied
    on $\bm{Q}$ to obtain $\bm{\widehat{Q}}$, respectively. The two matrices are multiplied
    to get the estimated block-level attention weights $\bm{\widehat{A}}$ and use
    it to compute the sparse attention weights and attention output.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '(ii) 块稀疏头。根据算法[3](#alg3 "算法 3 ‣ 核心感知优化稀疏模式搜索 ‣ 3.2 通过动态稀疏注意力加速长上下文 LLM 推理 ‣
    3 MInference 1.0 ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文 LLM 的预填充")，对 $\bm{Q}$ 进行均值池化以获得
    $\bm{\widehat{Q}}$。这两个矩阵相乘以获取估计的块级注意力权重 $\bm{\widehat{A}}$，并使用它来计算稀疏注意力权重和注意力输出。'
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we investigate two questions: (i) How effective is MInference?
    We evaluate our method on three general long-context benchmarks: InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], and the Needle In A Haystack task [[35](#bib.bib35)],
    as well as the long-context language modeling task [[65](#bib.bib65)]. These benchmarks
    cover long-context QA, multi-hop QA, math reasoning, aggregation tasks, summarization,
    retrieval tasks, and code debugging, allowing us to assess MInference’s effectiveness
    across a wide range of long-context scenarios. (ii) How efficient is MInference?
    We delve into the end-to-end latency and its breakdown to evaluate the efficiency
    of MInference. Additional experimental, latency results, and analysis can be found
    in Appendix [D](#A4 "Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    [E](#A5 "Appendix E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), and [F](#A6 "Appendix F
    Sparsity in Kernel Distribution ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们探讨两个问题：(i) MInference 的效果如何？我们在三个通用的长上下文基准上评估我们的方法：InfiniteBench [[86](#bib.bib86)]，RULER
    [[28](#bib.bib28)]，和 Needle In A Haystack 任务 [[35](#bib.bib35)]，以及长上下文语言建模任务 [[65](#bib.bib65)]。这些基准涵盖了长上下文
    QA、多跳 QA、数学推理、聚合任务、总结、检索任务和代码调试，允许我们评估 MInference 在各种长上下文场景下的有效性。(ii) MInference
    的效率如何？我们深入探讨端到端延迟及其细分，以评估 MInference 的效率。附加的实验、延迟结果和分析可以在附录 [D](#A4 "附录 D 额外实验结果
    ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文 LLM 的预填充")、[E](#A5 "附录 E 模式分布 ‣ MInference 1.0:
    通过动态稀疏注意力加速长上下文 LLM 的预填充") 和 [F](#A6 "附录 F 核心分布中的稀疏性 ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文
    LLM 的预填充") 中找到。'
- en: Implementation Details
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实施细节
- en: 'Our experiments use four state-of-the-art long-context LLMs: LLaMA-3-8B-Instruct-262k¹¹1https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k,
    LLaMA-3-8B-Instruct-1048k²²2https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k,
    GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)]. Additionally,
    we tested Needle in A Haystack [[35](#bib.bib35)] on Phi-3-Mini-128K [[2](#bib.bib2)]
    and Qwen2-7B-128K [[5](#bib.bib5)], as detailed in Appendix [D.1](#A4.SS1 "D.1
    Needle In A Haystack ‣ Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    To guarantee stable results, we use greedy decoding in all experiments. We provide
    a simple custom implementation of our method in PyTorch, built on FlashAttention [[13](#bib.bib13)],
    Triton [[75](#bib.bib75)], and the dynamic sparse compiler PIT [[88](#bib.bib88)].
    We set the target FLOPs $t$ and $block\_size=64$ in the Vertical-Slash and Block-Sparse
    patterns, respectively. The latency experiments are conducted on a single Nvidia
    A100 GPU in the bfloat16 format. More details are provided in Appendix [C.2](#A3.SS2
    "C.2 Additional Implementation Details ‣ Appendix C Experiment Details ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验使用了四种最先进的长上下文LLM：LLaMA-3-8B-Instruct-262k¹¹1https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k，LLaMA-3-8B-Instruct-1048k²²2https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k，GLM-4-9B-1M [[26](#bib.bib26)]，和Yi-9B-200K [[84](#bib.bib84)]。此外，我们还在Phi-3-Mini-128K [[2](#bib.bib2)]和Qwen2-7B-128K [[5](#bib.bib5)]上测试了Needle
    in A Haystack [[35](#bib.bib35)]，详见附录 [D.1](#A4.SS1 "D.1 Needle In A Haystack
    ‣ Appendix D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention")。为了保证稳定的结果，我们在所有实验中使用贪心解码。我们提供了基于FlashAttention [[13](#bib.bib13)]、Triton [[75](#bib.bib75)]和动态稀疏编译器PIT [[88](#bib.bib88)]的PyTorch简单自定义实现。我们在Vertical-Slash和Block-Sparse模式中分别设置了目标FLOPs
    $t$ 和 $block\_size=64$。延迟实验在单个Nvidia A100 GPU的bfloat16格式下进行。更多细节见附录 [C.2](#A3.SS2
    "C.2 Additional Implementation Details ‣ Appendix C Experiment Details ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")。'
- en: Dataset & Evaluation Metrics
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集与评估指标
- en: 'We use the provided metrics and scripts from the following benchmarks for evaluation.
    More details about dataset can be found in Appendix [C.1](#A3.SS1 "C.1 Dataset
    Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用了以下基准提供的指标和脚本进行评估。有关数据集的更多细节可以在附录 [C.1](#A3.SS1 "C.1 Dataset Details ‣
    Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")中找到。'
- en: '(i) InfiniteBench [[86](#bib.bib86)]: This benchmark consists of 10 tasks,
    including retrieval tasks such as PassKey retrieval, Number retrieval, and KV
    retrieval, as well as representative realistic tasks like question-answering,
    coding, dialogue, and summarization. The average context length of InfiniteBench
    is about 214K tokens.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (i) InfiniteBench [[86](#bib.bib86)]：这个基准测试包含10个任务，包括检索任务如PassKey检索、Number检索和KV检索，以及代表性的实际任务如问答、编程、对话和总结。InfiniteBench的平均上下文长度约为214K
    tokens。
- en: '(ii) RULER [[28](#bib.bib28)]: A challenging long-context benchmark consisting
    of 4 categories and 13 complex tasks, including retrieval, multi-hop tracing and
    aggregation, and QA tasks. It contains subsets with different prompt lengths up
    to 128k tokens, allowing us to determine the actual context window size of the
    model based on the results.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) RULER [[28](#bib.bib28)]：一个具有挑战性的长上下文基准，包含4个类别和13个复杂任务，包括检索、多跳追踪和聚合任务，以及QA任务。它包含具有不同提示长度的子集，最长可达128k
    tokens，允许我们根据结果确定模型的实际上下文窗口大小。
- en: '(iii) Needle In A Haystack [[35](#bib.bib35)]: A long-context retrieval benchmark
    testing LLMs’ performance with context window sizes up to 1M tokens where information
    placed at various positions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) Needle In A Haystack [[35](#bib.bib35)]：一个长上下文检索基准测试，测试LLM在上下文窗口大小达1M
    tokens的情况下的表现，其中信息分布在不同位置。
- en: '(iv) PG-19 [[65](#bib.bib65)]: Following StreamingLLM [[82](#bib.bib82)] and
    H2O [[89](#bib.bib89)], we use PG-19 for long-context language modeling tasks
    with prompts up to 100k tokens.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (iv) PG-19 [[65](#bib.bib65)]：继StreamingLLM [[82](#bib.bib82)]和H2O [[89](#bib.bib89)]之后，我们使用PG-19进行长上下文语言建模任务，提示长度可达100k
    tokens。
- en: 'Table 2: Performance of different methods with different base models on InfiniteBench [[86](#bib.bib86)].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同基础模型在InfiniteBench [[86](#bib.bib86)]上的不同方法的表现。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find | Retr.PassKey
    | Retr.Number | Retr.KV | 平均值 |'
- en: '| LLaMA-3-8B-262K | 20.2 | 12.4 | 67.3 | 6.0 | 12.9 | 22.1 | 26.6 | 100.0 |
    100.0 | 14.4 | 38.2 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B-262K | 20.2 | 12.4 | 67.3 | 6.0 | 12.9 | 22.1 | 26.6 | 100.0 |
    100.0 | 14.4 | 38.2 |'
- en: '| StreamingLLM | 21.0 | 8.2 | 40.2 | 10.0 | 10.4 | 25.9 | 30.0 | 86.8 | 5.1
    | 0.8 | 23.8 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 21.0 | 8.2 | 40.2 | 10.0 | 10.4 | 25.9 | 30.0 | 86.8 | 5.1
    | 0.8 | 23.8 |'
- en: '| StreamingLLM w/ dilated | 20.1 | 9.4 | 44.5 | 15.5 | 11.2 | 20.5 | 27.5 |
    5.0 | 87.5 | 0.5 | 24.2 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | 20.1 | 9.4 | 44.5 | 15.5 | 11.2 | 20.5 | 27.5 |
    5.0 | 87.5 | 0.5 | 24.2 |'
- en: '| StreamingLLM w/ strided | 17.3 | 8.2 | 27.5 | 14.5 | 11.2 | 19.5 | 27.5 |
    4.0 | 2.1 | 1.0 | 13.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | 17.3 | 8.2 | 27.5 | 14.5 | 11.2 | 19.5 | 27.5 |
    4.0 | 2.1 | 1.0 | 13.3 |'
- en: '| InfLLM | 24.1 | 7.8 | 45.0 | 6.0 | 11.4 | 19.5 | 32.9 | 100.0 | 100.0 | 1.2
    | 34.8 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 24.1 | 7.8 | 45.0 | 6.0 | 11.4 | 19.5 | 32.9 | 100.0 | 100.0 | 1.2
    | 34.8 |'
- en: '| Ours w/ static | 19.9 | 8.6 | 43.2 | 3.5 | 8.9 | 20.6 | 25.1 | 92.4 | 96.3
    | 0.2 | 31.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Ours w/ static | 19.9 | 8.6 | 43.2 | 3.5 | 8.9 | 20.6 | 25.1 | 92.4 | 96.3
    | 0.2 | 31.9 |'
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
- en: '| Yi-9B-200K | 8.2 | 10.6 | 64.2 | 1.0 | 17.3 | 21.3 | 23.4 | 99.8 | 100.0
    | 28.8 | 37.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 8.2 | 10.6 | 64.2 | 1.0 | 17.3 | 21.3 | 23.4 | 99.8 | 100.0
    | 28.8 | 37.5 |'
- en: '| StreamingLLM | 5.4 | 14.2 | 38.0 | 4.0 | 18.8 | 18.8 | 22.3 | 39.2 | 6.1
    | 1.6 | 16.8 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 5.4 | 14.2 | 38.0 | 4.0 | 18.8 | 18.8 | 22.3 | 39.2 | 6.1
    | 1.6 | 16.8 |'
- en: '| StreamingLLM w/ dilated | 5.7 | 4.2 | 15.0 | 0.0 | 18.2 | 0.0 | 2.9 | 0.0
    | 0.0 | 0.0 | 4.2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | 5.7 | 4.2 | 15.0 | 0.0 | 18.2 | 0.0 | 2.9 | 0.0
    | 0.0 | 0.0 | 4.2 |'
- en: '| StreamingLLM w/ strided | 6.1 | 4.5 | 9.8 | 0.0 | 16.9 | 0.0 | 3.1 | 1.5
    | 0.0 | 0.0 | 4.6 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | 6.1 | 4.5 | 9.8 | 0.0 | 16.9 | 0.0 | 3.1 | 1.5
    | 0.0 | 0.0 | 4.6 |'
- en: '| InfLLM | 6.3 | 13.0 | 45.9 | 2.5 | 21.5 | 20.6 | 34.6 | 85.3 | 88.1 | 1.4
    | 31.9 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 6.3 | 13.0 | 45.9 | 2.5 | 21.5 | 20.6 | 34.6 | 85.3 | 88.1 | 1.4
    | 31.9 |'
- en: '| Ours w/ static | 5.8 | 12.6 | 48.5 | 3.0 | 12.6 | 20.8 | 25.1 | 60.9 | 38.5
    | 1.0 | 22.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Ours w/ static | 5.8 | 12.6 | 48.5 | 3.0 | 12.6 | 20.8 | 25.1 | 60.9 | 38.5
    | 1.0 | 22.9 |'
- en: '| Ours | 7.9 | 11.2 | 64.2 | 1.0 | 17.9 | 24.1 | 23.1 | 99.5 | 100.0 | 27.6
    | 37.7 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 7.9 | 11.2 | 64.2 | 1.0 | 17.9 | 24.1 | 23.1 | 99.5 | 100.0 | 27.6
    | 37.7 |'
- en: '| GLM-4-9B-1M | 28.3 | 9.7 | 68.6 | 39.5 | 12.1 | 29.4 | 38.9 | 100.0 | 100.0
    | 41.0 | 46.7 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4-9B-1M | 28.3 | 9.7 | 68.6 | 39.5 | 12.1 | 29.4 | 38.9 | 100.0 | 100.0
    | 41.0 | 46.7 |'
- en: '| StreamingLLM | 27.7 | 6.4 | 40.2 | 12.5 | 10.8 | 27.7 | 21.1 | 97.1 | 25.6
    | 0.6 | 27.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 27.7 | 6.4 | 40.2 | 12.5 | 10.8 | 27.7 | 21.1 | 97.1 | 25.6
    | 0.6 | 27.0 |'
- en: '| InfLLM | 28.0 | 7.3 | 45.0 | 14.0 | 10.7 | 27.9 | 39.4 | 98.0 | 100.0 | 2.6
    | 37.3 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 28.0 | 7.3 | 45.0 | 14.0 | 10.7 | 27.9 | 39.4 | 98.0 | 100.0 | 2.6
    | 37.3 |'
- en: '| Ours | 28.8 | 9.6 | 68.6 | 38.5 | 12.0 | 30.7 | 39.1 | 100.0 | 100.0 | 43.0
    | 47.0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 28.8 | 9.6 | 68.6 | 38.5 | 12.0 | 30.7 | 39.1 | 100.0 | 100.0 | 43.0
    | 47.0 |'
- en: Baselines
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线
- en: 'We include five training-free sparse attention approaches as our baselines:
    1) StreamingLLM [[82](#bib.bib82)], which corresponds to the A-shape pattern.
    We use 1k global tokens and 4k local windows in all our experiments; 2) StreamingLLM
    w/ dilated [[6](#bib.bib6)], which sets dilated local windows with intervals in
    the local windows direction. We use 1k global tokens and 8k dilated attention
    windows with an interval of 1; 3) StreamingLLM w/ strided [[8](#bib.bib8)], which
    retains local windows while adding dilated attention. We use 1k global tokens,
    2k local windows, and 4k dilated attention windows with an interval of 1; 4) InfLLM [[83](#bib.bib83)],
    which uses a memory unit to process streaming long sequences. Following the paper,
    we set 128 global tokens and 8k local windows in all experiments; 5) Ours w/ static,
    which utilizes static sparse indices in the Vertical-Slash and Block-Sparse heads.
    For all baselines, we perform sparse computation only during the pre-filling stage,
    while retaining dense computation during the decoding stage.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将五种无训练稀疏注意力方法作为基线：1）StreamingLLM [[82](#bib.bib82)]，它对应于A形模式。在所有实验中，我们使用1k全局标记和4k局部窗口；2）StreamingLLM
    w/ dilated [[6](#bib.bib6)]，它设置了在局部窗口方向上的间隔膨胀局部窗口。在所有实验中，我们使用1k全局标记和8k膨胀注意力窗口，间隔为1；3）StreamingLLM
    w/ strided [[8](#bib.bib8)]，它保留了局部窗口，同时添加了膨胀注意力。在所有实验中，我们使用1k全局标记、2k局部窗口和4k膨胀注意力窗口，间隔为1；4）InfLLM [[83](#bib.bib83)]，它使用一个内存单元来处理流式长序列。按照论文中的设置，在所有实验中我们使用128个全局标记和8k局部窗口；5）Ours
    w/ static，它利用Vertical-Slash和Block-Sparse头中的静态稀疏索引。对于所有基线，我们仅在预填充阶段执行稀疏计算，同时在解码阶段保留密集计算。
- en: InfiniteBench
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: InfiniteBench
- en: 'Table 3: Performance (%) of different models and different methods on RULER [[28](#bib.bib28)]
    evaluated at lengths from 4k to 128k.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在长度从4k到128k的RULER [[28](#bib.bib28)]上，不同模型和不同方法的性能（%）。
- en: '| Methods | Claimed | Effective | 4K | 8K | 16K | 32K | 64K | 128K | Avg. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 声称 | 有效 | 4K | 8K | 16K | 32K | 64K | 128K | 平均 |'
- en: '| LLaMA-3-8B-262K | 262K | 16K | 97.2 | 91.8 | 87.3 | 80.8 | 77.4 | 72.2 |
    84.4 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B-262K | 262K | 16K | 97.2 | 91.8 | 87.3 | 80.8 | 77.4 | 72.2 |
    84.4 |'
- en: '| StreamingLLM | - | 4K | 97.2 | 38.1 | 37.5 | 17.2 | 14.2 | 9.4 | 35.0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | - | 4K | 97.2 | 38.1 | 37.5 | 17.2 | 14.2 | 9.4 | 35.0 |'
- en: '| StreamingLLM w/ dilated | - | <4K | 23.4 | 0.7 | 1.4 | 18.8 | 16.5 | 15.6
    | 12.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | - | <4K | 23.4 | 0.7 | 1.4 | 18.8 | 16.5 | 15.6
    | 12.7 |'
- en: '| StreamingLLM w/ strided | - | <4K | 2.0 | 0.7 | 0.6 | 0.6 | 0.7 | 1.3 | 1.0
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | - | <4K | 2.0 | 0.7 | 0.6 | 0.6 | 0.7 | 1.3 | 1.0
    |'
- en: '| InfLLM | - | 4K | 89.4 | 79.8 | 70.1 | 55.6 | 43.0 | 39.5 | 62.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | - | 4K | 89.4 | 79.8 | 70.1 | 55.6 | 43.0 | 39.5 | 62.9 |'
- en: '| Ours | - | 32K | 97.7 | 91.2 | 88.5 | 85.0 | 82.3 | 77.6 | 87.0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | 32K | 97.7 | 91.2 | 88.5 | 85.0 | 82.3 | 77.6 | 87.0 |'
- en: '| Yi-9B-200K | 200K | 8K | 91.9 | 90.2 | 78.8 | 76.3 | 68.1 | 62.9 | 78.1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 200K | 8K | 91.9 | 90.2 | 78.8 | 76.3 | 68.1 | 62.9 | 78.1 |'
- en: '| StreamingLLM | - | 4K | 91.9 | 37.8 | 33.9 | 18.6 | 13.0 | 12.8 | 34.3 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | - | 4K | 91.9 | 37.8 | 33.9 | 18.6 | 13.0 | 12.8 | 34.3 |'
- en: '| StreamingLLM w/ dilated | - | <4K | 44.8 | 42.8 | 38.5 | 29.8 | 26.8 | 23.9
    | 34.4 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | - | <4K | 44.8 | 42.8 | 38.5 | 29.8 | 26.8 | 23.9
    | 34.4 |'
- en: '| StreamingLLM w/ strided | - | <4K | 2.6 | 0.7 | 0.6 | 0.6 | 1.2 | 0.5 | 1.1
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | - | <4K | 2.6 | 0.7 | 0.6 | 0.6 | 1.2 | 0.5 | 1.1
    |'
- en: '| InfLLM | - | <4K | 80.3 | 83.9 | 60.7 | 45.2 | 38.6 | 30.2 | 56.5 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | - | <4K | 80.3 | 83.9 | 60.7 | 45.2 | 38.6 | 30.2 | 56.5 |'
- en: '| Ours | - | 8K | 92.3 | 89.7 | 79.0 | 73.8 | 64.7 | 56.9 | 74.7 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | 8K | 92.3 | 89.7 | 79.0 | 73.8 | 64.7 | 56.9 | 74.7 |'
- en: '| GLM-4-9B-1M | 1M | 64K | 93.8 | 91.6 | 89.3 | 87.4 | 85.2 | 80.8 | 88.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4-9B-1M | 1M | 64K | 93.8 | 91.6 | 89.3 | 87.4 | 85.2 | 80.8 | 88.0 |'
- en: '| StreamingLLM | - | 4K | 93.8 | 66.9 | 58.5 | 51.4 | 45.9 | 39.1 | 59.3 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | - | 4K | 93.8 | 66.9 | 58.5 | 51.4 | 45.9 | 39.1 | 59.3 |'
- en: '| InfLLM | - | 8K | 94.7 | 89.5 | 76.4 | 66.5 | 56.8 | 53.5 | 72.9 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | - | 8K | 94.7 | 89.5 | 76.4 | 66.5 | 56.8 | 53.5 | 72.9 |'
- en: '| Ours | - | 64K | 94.6 | 93.1 | 91.0 | 89.6 | 85.5 | 84.0 | 89.6 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | 64K | 94.6 | 93.1 | 91.0 | 89.6 | 85.5 | 84.0 | 89.6 |'
- en: 'As shown in Table [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), MInference achieves the best overall performance compared to baselines
    on InfiniteBench. Remarkably, MInference matches or even slightly surpasses the
    performance of the original full attention baseline on some tasks, despite the
    significant acceleration it provided. From the perspective of different tasks,
    our method not only performs well in natural language tasks such as summarization,
    QA, and code, but also maintains the original model’s performance on retrieval-related
    tasks. Baseline methods such as StreamingLLM, on the contrary, struggle with these
    retrieval tasks. Additionally, on tasks such as dialogue QA, using local attention
    mechanisms can better handle these tasks, while our performance is closer to the
    original results, indicating that our method is not solely based on local windows.
    Extending the local windows’ intervals in StreamingLLM, i.e., w/ dilated and w/
    strided, hardly affects the model’s performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，MInference在InfiniteBench上相较于基线方法表现出最佳的整体性能。值得注意的是，尽管MInference提供了显著的加速，其在某些任务上的性能与原始的全注意力基线相匹配甚至略有超越。从不同任务的角度来看，我们的方法不仅在总结、问答和代码等自然语言任务中表现良好，还保持了原始模型在检索相关任务上的性能。相反，像StreamingLLM这样的基线方法在这些检索任务上表现不佳。此外，在对话问答等任务中，使用局部注意力机制可以更好地处理这些任务，而我们的性能更接近于原始结果，这表明我们的方法不仅仅依赖于局部窗口。扩展StreamingLLM中的局部窗口间隔，即w/
    dilated和w/ strided，几乎不会影响模型的性能。'
- en: RULER
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RULER
- en: 'To further reveal the true potential of our method in long-context LLMs, we
    evaluate MInference with the state-of-the-art long-context challenge, RULER. As
    shown in Table [3](#S4.T3 "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    MInference effectively maintains the long-context performance even in complex
    multi-hop or aggregation tasks in RULER. It even outperforms the original full
    attention for testing lengths beyond 32K, achieving effective context windows
    of 32K and 64K (context with performance over 85% is considered effective [[28](#bib.bib28)])
    in LLaMA-3-8B-262K and GLM-4-9B-1M.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步揭示我们的方法在长上下文 LLM 中的真正潜力，我们用最先进的长上下文挑战 RULER 评估 MInference。如表 [3](#S4.T3
    "表 3 ‣ InfiniteBench ‣ 4 个实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 所示，即使在
    RULER 中的复杂多跳或聚合任务中，MInference 也有效地保持了长上下文性能。它甚至在测试长度超过 32K 时超越了原始的全注意力，达到 LLaMA-3-8B-262K
    和 GLM-4-9B-1M 中 32K 和 64K 的有效上下文窗口（性能超过 85% 的上下文被视为有效 [[28](#bib.bib28)])。
- en: Language Modeling
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言建模
- en: 'Following the approach of StreamingLLM [[82](#bib.bib82)] and H2O [[89](#bib.bib89)],
    we evaluate our methods against baselines on the language modeling task based
    on the PG-19 dataset [[65](#bib.bib65)]. As shown in [5](#S4.F5 "Figure 5 ‣ Language
    Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), our method yields best results compared to
    other sparse approaches, and exhibits minimal divergence compared to the full
    attention baseline. For prompts of 100K token, our perplexity is only 0.2 higher
    than the full attention, but lower than StreamingLLM for 0.25 and 0.75 on the
    Yi-9B-200K and LLaMA-3-262K models respectively.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循 StreamingLLM [[82](#bib.bib82)] 和 H2O [[89](#bib.bib89)] 的方法，我们在基于 PG-19
    数据集 [[65](#bib.bib65)] 的语言建模任务中对比了我们的方法与基线。正如图 [5](#S4.F5 "图 5 ‣ 语言建模 ‣ 4 个实验
    ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 所示，我们的方法在与其他稀疏方法的比较中表现最佳，相较于全注意力基线，偏差最小。对于
    100K tokens 的提示，我们的困惑度仅比全注意力高 0.2，但在 Yi-9B-200K 和 LLaMA-3-262K 模型中分别低于 StreamingLLM
    0.25 和 0.75。
- en: '![Refer to caption](img/760645197ef39823c822f7e94434c307.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/760645197ef39823c822f7e94434c307.png)'
- en: (a) LLaMA-3-8B-Instruct-262K
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLaMA-3-8B-Instruct-262K
- en: (b) Yi-9B-200K
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Yi-9B-200K
- en: 'Figure 5: Perplexity results on PG-19 [[65](#bib.bib65)] using different models
    and methods.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用不同模型和方法在 PG-19 [[65](#bib.bib65)] 上的困惑度结果。
- en: '![Refer to caption](img/d8c4f5b0df6fa7dcf125d1ff1ed2fcb5.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d8c4f5b0df6fa7dcf125d1ff1ed2fcb5.png)'
- en: 'Figure 6: Results on Needle In A Haystack of StreamingLLM [[82](#bib.bib82)]
    in LLaMA-3-8B-1M.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LLaMA-3-8B-1M 中 Needle In A Haystack 的结果 [[82](#bib.bib82)]。
- en: Needle In A Haystack
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大海捞针
- en: 'Comparing Fig. [1a](#S0.F1.sf1 "In Figure 1 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") to [6](#S4.F6
    "Figure 6 ‣ Language Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), our method effectively retains
    the ability to process information at different positions across various context
    windows, ranging from 1k to 1M tokens. In contrast, methods like StreamingLLM,
    while effective in reducing latency, experience a rapid decline in performance
    once the critical information exceeds the range of global tokens and local windows.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将图 [1a](#S0.F1.sf1 "在图 1 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 与 [6](#S4.F6
    "图 6 ‣ 语言建模 ‣ 4 个实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 比较，我们的方法在不同位置的各种上下文窗口（从
    1k 到 1M tokens）中有效地保留了处理信息的能力。相比之下，像 StreamingLLM 这样的方法虽然能有效降低延迟，但一旦关键信息超过全球 tokens
    和局部窗口的范围，性能迅速下降。
- en: Ablation Study
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 消融研究
- en: 'To evaluate the contributions of different components in MInference, we introduce
    four variants for the ablation study: (1) Ours w/ static, which uses a static
    sparse mask in the Vertical-Slash and Block-Sparse patterns; (2) Ours w/ only
    A-shape, which is equivalent to StreamingLLM; (3) Ours w/ only block-sparse, which
    uses only the Block-Sparse pattern in the dynamic sparse calculation. (4) Ours
    w/ only vertical-slash, which uses only the Vertical-Slash pattern in the dynamic
    sparse calculation.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 MInference 中不同组件的贡献，我们引入了四种变体进行消融研究：（1）带有静态的我们的方法，使用 Vertical-Slash 和 Block-Sparse
    模式中的静态稀疏掩码；（2）仅带有 A 形状的我们的方法，相当于 StreamingLLM；（3）仅带有 block-sparse 的我们的方法，仅在动态稀疏计算中使用
    Block-Sparse 模式；（4）仅带有 vertical-slash 的我们的方法，仅在动态稀疏计算中使用 Vertical-Slash 模式。
- en: 'Table 4: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K
    on InfiniteBench [[86](#bib.bib86)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：使用 LLaMA-3-8B-Instruct-262K 在 InfiniteBench 上的不同消融方法的性能 [[86](#bib.bib86)]。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find | Retr.PassKey
    | Retr.Number | Retr.KV | 平均值 |'
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
- en: '| Ours w/ only block-sparse | 12.4 | 3.4 | 5.7 | 6.0 | 3.1 | 12.2 | 24.0 |
    59.5 | 60.3 | 0.0 | 18.7 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 我们的仅块稀疏 | 12.4 | 3.4 | 5.7 | 6.0 | 3.1 | 12.2 | 24.0 | 59.5 | 60.3 | 0.0
    | 18.7 |'
- en: '| Ours w/ only vertical-slash | 19.6 | 12.0 | 62.1 | 9.5 | 11.7 | 21.6 | 29.1
    | 100.0 | 100.0 | 5.0 | 37.1 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 我们的仅垂直斜杠 | 19.6 | 12.0 | 62.1 | 9.5 | 11.7 | 21.6 | 29.1 | 100.0 | 100.0
    | 5.0 | 37.1 |'
- en: 'Tables [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments ‣
    MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), [3](#S4.T3 "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    and [4](#S4.T4 "Table 4 ‣ Ablation Study ‣ 4 Experiments ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") present the ablation
    results. It first proves that using static indices significantly degrades LLM
    performance, especially in highly dynamic tasks like KV retrieval, where accuracy
    nearly drops to zero. This highlight the necessity of our dynamic strategy and
    the effectiveness of our dynamically built sparse indices. Additionally, remove
    any pattern from the three leads to varying degrees of performance degradation.
    Specifically, "only A-shape" can only capture information within local windows.
    The "only block-sparse" variant using only the BS pattern, also results in significant
    performance declines. On the other hand, "only vertical-slash" manages to preserve
    most of the performance due to its balance between dynamicity and the StreamingLLM
    pattern, but still fall behind the full version of our method.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S4.T2 "表格 2 ‣ 数据集与评估指标 ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充")、[3](#S4.T3
    "表格 3 ‣ InfiniteBench ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 和 [4](#S4.T4
    "表格 4 ‣ 消融研究 ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 显示了消融实验结果。首先证明了使用静态索引会显著降低
    LLM 的性能，尤其是在 KV 检索等高度动态的任务中，准确率几乎降至零。这突显了我们动态策略的必要性以及我们动态构建稀疏索引的有效性。此外，从这三种方式中去除任何模式都会导致不同程度的性能下降。具体来说，“仅
    A 形状”只能捕获局部窗口内的信息。“仅块稀疏”变体使用仅 BS 模式，也会导致显著的性能下降。另一方面，“仅垂直斜杠”由于其在动态性和 StreamingLLM
    模式之间的平衡，能够保留大部分性能，但仍落后于我们方法的完整版本。
- en: Latency
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 延迟
- en: 'Fig. [1b](#S0.F1.sf2 "In Figure 1 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention") and [10](#A4.F10 "Figure
    10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional Experiment Results ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    shows the latency and breakdown of MInference across different context windows
    on a single A100\. At 100K, 300K, 500K, and 1M tokens, our method achieves speedups
    of 1.8$\times$, 6.8$\times$, respectively. It reduces the pre-filling latency
    from 30 mins to 3 mins on a single A100 for a prompt of 1M token. By further utilizing
    tensor parallel [[46](#bib.bib46)] and context parallel [[50](#bib.bib50), [31](#bib.bib31)],
    this latency can be reduced to 40 seconds on 8x A100 GPUs. This significantly
    lowers the deployment cost of long-context LLMs and enhances user experience.
    And since our kernel is implemented based on Triton, it can be easily ported to
    other devices and achieve similar speedups, such as on the H100. Additionally,
    analyzing the latency breakdown, we found about 5%-20% of the overhead is spent
    on dynamic sparse index building, while the remaining time is spent on dynamic
    sparse calculation.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1b](#S0.F1.sf2 "In Figure 1 ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention")和 [10](#A4.F10 "Figure 10 ‣ D.2
    Latency Breakdown ‣ Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    显示了在单个A100上的不同上下文窗口下MInference的延迟及其拆解。在100K、300K、500K和1M标记时，我们的方法分别实现了1.8$\times$、6.8$\times$的加速。在单个A100上，对于1M标记的提示，预填充延迟从30分钟降低到3分钟。通过进一步利用张量并行 [[46](#bib.bib46)]和上下文并行 [[50](#bib.bib50),
    [31](#bib.bib31)]，这种延迟可以在8个A100 GPU上减少到40秒。这显著降低了长上下文LLMs的部署成本并提升了用户体验。由于我们的内核是基于Triton实现的，因此可以很容易地移植到其他设备并实现类似的加速，例如H100。此外，通过分析延迟拆解，我们发现大约5%-20%的开销用于动态稀疏索引构建，其余时间用于动态稀疏计算。'
- en: Integrate with KV cache compression methods
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 集成KV缓存压缩方法
- en: 'Table 5: Performance of different methods on InfiniteBench [[86](#bib.bib86)]
    using SnapKV [[43](#bib.bib43)] in the decoding stage.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表5: 使用SnapKV [[43](#bib.bib43)]在解码阶段不同方法在InfiniteBench [[86](#bib.bib86)]上的性能。'
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 英文总结 | 英文问答 | 英文多选 | 英文对话 | 中文问答 | 代码调试 | 数学查找 | 检索密码 | 检索数量 | 检索KV
    | 平均值 |'
- en: '| LLaMA-3 w/ SnapKV | 18.0 | 11.8 | 65.5 | 2.5 | 12.0 | 21.3 | 26.6 | 100.0
    | 100.0 | 1.8 | 36.0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3与SnapKV | 18.0 | 11.8 | 65.5 | 2.5 | 12.0 | 21.3 | 26.6 | 100.0 |
    100.0 | 1.8 | 36.0 |'
- en: '| Ours w/ SnapKV | 18.9 | 11.7 | 66.4 | 6.5 | 12.1 | 21.8 | 33.1 | 100.0 |
    100.0 | 2.0 | 37.3 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法与SnapKV | 18.9 | 11.7 | 66.4 | 6.5 | 12.1 | 21.8 | 33.1 | 100.0 | 100.0
    | 2.0 | 37.3 |'
- en: 'We also combined MInference with a state-of-the-art KV cache compression method
    SnapKV [[43](#bib.bib43)], as shown in Table [5](#S4.T5 "Table 5 ‣ Integrate with
    KV cache compression methods ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"). This proves our method is
    compatible with KV cache compression techniques. For most tasks, performance remains
    nearly unchanged, with the average score even showing a slight increase, which
    further demonstrates the potential practical value of our method as an optimization
    for serving long-context LLMs.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还将MInference与一种先进的KV缓存压缩方法SnapKV [[43](#bib.bib43)]结合，如表 [5](#S4.T5 "Table
    5 ‣ Integrate with KV cache compression methods ‣ 4 Experiments ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示。这证明了我们的方法与KV缓存压缩技术是兼容的。对于大多数任务，性能几乎没有变化，平均得分甚至略有上升，这进一步证明了我们的方法作为优化手段在服务长上下文LLMs方面的潜在实际价值。'
- en: 5 Related Works
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Sparse Attention
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏注意力
- en: Due to the quadratic complexity of the attention mechanism, many previous works
    have focused on sparse attention to improve the efficiency of Transformers. These
    methods include static sparse patterns, cluster-based sparse approaches, and dynamic
    sparse attention. Static sparse patterns include techniques such as sliding windows [[30](#bib.bib30),
    [2](#bib.bib2)], dilated attention [[8](#bib.bib8), [72](#bib.bib72), [16](#bib.bib16)],
    and mixed sparse patterns [[6](#bib.bib6), [87](#bib.bib87), [38](#bib.bib38)].
    Cluster-based sparse methods include hash-based [[36](#bib.bib36)] and kNN-based [[68](#bib.bib68),
    [54](#bib.bib54)] methods. All of the above methods require pre-training the model
    from scratch, which makes them infeasible to be directly used as a plugin for
    reay-to-use LLMs. Recently, there has been work [[14](#bib.bib14), [85](#bib.bib85)]
    to unify state space models [[23](#bib.bib23), [22](#bib.bib22), [14](#bib.bib14)],
    and linear attention [[37](#bib.bib37), [70](#bib.bib70)] into structured masked
    attention. Additionally, some works [[81](#bib.bib81), [47](#bib.bib47), [63](#bib.bib63)]
    leverage the dynamic nature of attention to predict sparse patterns dynamically.
    However, these approaches often focus on low-rank hidden states during the dynamic
    pattern approximation or use post-statistical methods to obtain the sparse mask,
    which introduce substantial overhead in the estimation step, making them less
    useful for long-context LLMs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力机制的二次复杂性，许多先前的工作集中于稀疏注意力以提高变换器的效率。这些方法包括静态稀疏模式、基于聚类的稀疏方法和动态稀疏注意力。静态稀疏模式包括滑动窗口[[30](#bib.bib30),
    [2](#bib.bib2)]、膨胀注意力[[8](#bib.bib8), [72](#bib.bib72), [16](#bib.bib16)]和混合稀疏模式[[6](#bib.bib6),
    [87](#bib.bib87), [38](#bib.bib38)]。基于聚类的稀疏方法包括基于哈希[[36](#bib.bib36)]和基于kNN[[68](#bib.bib68),
    [54](#bib.bib54)]的方法。上述所有方法都需要从头开始对模型进行预训练，这使得它们无法直接作为插件用于现成的LLMs。最近，有工作[[14](#bib.bib14),
    [85](#bib.bib85)]将状态空间模型[[23](#bib.bib23), [22](#bib.bib22), [14](#bib.bib14)]和线性注意力[[37](#bib.bib37),
    [70](#bib.bib70)]统一到结构化掩蔽注意力中。此外，一些工作[[81](#bib.bib81), [47](#bib.bib47), [63](#bib.bib63)]利用注意力的动态特性来动态预测稀疏模式。然而，这些方法通常关注于动态模式近似过程中的低秩隐藏状态，或者使用后统计方法来获得稀疏掩膜，这在估计步骤中引入了大量开销，使得它们对长上下文LLMs的实用性较差。
- en: Scaling Context Windows of LLMs
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 扩展 LLMs 的上下文窗口
- en: 'Recent research has focused on expanding the context window of pre-trained
    LLMs, that enables LLMs to handle more complex real-life applications [[34](#bib.bib34),
    [58](#bib.bib58)]. These methods can be categorized into: 1) Staged pre-training [[55](#bib.bib55),
    [20](#bib.bib20)]; 2) Modifying or interpolating position embeddings [[61](#bib.bib61),
    [10](#bib.bib10), [60](#bib.bib60), [19](#bib.bib19)]; 3) Utilizing external memory
    modules for context storage [[4](#bib.bib4), [77](#bib.bib77), [83](#bib.bib83)];
    4) Expanding computations across multiple devices in a distributed manner [[50](#bib.bib50)].
    However, these methods do not alleviate the high inference costs in long-context
    processing.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究集中在扩展预训练大型语言模型（LLMs）的上下文窗口，这使得LLMs能够处理更复杂的现实应用[[34](#bib.bib34), [58](#bib.bib58)]。这些方法可以分为以下几类：1)
    分阶段预训练[[55](#bib.bib55), [20](#bib.bib20)]; 2) 修改或插值位置嵌入[[61](#bib.bib61), [10](#bib.bib10),
    [60](#bib.bib60), [19](#bib.bib19)]; 3) 利用外部记忆模块进行上下文存储[[4](#bib.bib4), [77](#bib.bib77),
    [83](#bib.bib83)]; 4) 以分布式方式在多个设备上扩展计算[[50](#bib.bib50)]。然而，这些方法并未减轻长上下文处理中的高推理成本。
- en: Long-Context LLM Inference
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长上下文 LLM 推理
- en: 'Recent studies [[21](#bib.bib21)] have tackled the high computational cost
    of attention and substantial KV cache storage in long-context scenarios from two
    angles: pre-filling and decoding. Pre-filling optimizations are primarily categorized
    as State Space Models [[23](#bib.bib23), [22](#bib.bib22)], linear attention methods [[70](#bib.bib70),
    [57](#bib.bib57)], memory-based methods [[53](#bib.bib53)], hybrid methods [[44](#bib.bib44),
    [27](#bib.bib27), [64](#bib.bib64)], and prompt compression methods [[41](#bib.bib41),
    [32](#bib.bib32), [33](#bib.bib33), [62](#bib.bib62)]. However, these approaches
    require training from scratch or additional overhead and are difficult to implement
    directly in pretrained long-context LLMs. Recently, some studies [[52](#bib.bib52),
    [83](#bib.bib83)] have focused on using kNN or cluster-based sparse attention
    to accelerate LLM inference. However, these methods often lead to reduced accuracy,
    limited speedup, or are restricted to CPU scenarios.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究[[21](#bib.bib21)]从两个角度解决了长上下文场景中注意力的高计算成本和大量KV缓存存储的问题：预填充和解码。预填充优化主要分为状态空间模型[[23](#bib.bib23),
    [22](#bib.bib22)]、线性注意力方法[[70](#bib.bib70), [57](#bib.bib57)]、基于记忆的方法[[53](#bib.bib53)]、混合方法[[44](#bib.bib44),
    [27](#bib.bib27), [64](#bib.bib64)]和提示压缩方法[[41](#bib.bib41), [32](#bib.bib32),
    [33](#bib.bib33), [62](#bib.bib62)]。然而，这些方法需要从头开始训练或额外的开销，并且难以直接在预训练的长上下文LLM中实现。最近，一些研究[[52](#bib.bib52),
    [83](#bib.bib83)]集中于利用kNN或基于集群的稀疏注意力来加速LLM推理。然而，这些方法通常会导致准确性降低、加速有限，或者仅限于CPU场景。
- en: 'In contrast, optimizations for the decoding stage are divided into: 1) Reusing
    attention KV to reduce KV cache storage [[73](#bib.bib73), [3](#bib.bib3), [71](#bib.bib71),
    [12](#bib.bib12)]; 2) Static KV cache compression patterns [[82](#bib.bib82),
    [29](#bib.bib29)]; 3) Dynamic KV cache compression patterns, including completely
    discarding the KV cache after compression [[89](#bib.bib89), [42](#bib.bib42),
    [25](#bib.bib25), [56](#bib.bib56)], and offloading-based methods [[63](#bib.bib63),
    [43](#bib.bib43), [15](#bib.bib15)]; 4) Cluster-based KV cache compression methods [[54](#bib.bib54),
    [78](#bib.bib78)]; 5) Methods for restoring performance loss due to KV cache compression [[1](#bib.bib1),
    [18](#bib.bib18)]; 6) Hierarchical speculative decoding methods [[69](#bib.bib69)].
    Nevertheless, these methods do not address the heavy computational burden of the
    attention in the pre-filling stage.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，解码阶段的优化分为以下几类：1) 通过重用注意力KV来减少KV缓存存储[[73](#bib.bib73), [3](#bib.bib3), [71](#bib.bib71),
    [12](#bib.bib12)]; 2) 静态KV缓存压缩模式[[82](#bib.bib82), [29](#bib.bib29)]; 3) 动态KV缓存压缩模式，包括压缩后完全丢弃KV缓存[[89](#bib.bib89),
    [42](#bib.bib42), [25](#bib.bib25), [56](#bib.bib56)]，以及基于卸载的方法[[63](#bib.bib63),
    [43](#bib.bib43), [15](#bib.bib15)]; 4) 基于集群的KV缓存压缩方法[[54](#bib.bib54), [78](#bib.bib78)];
    5) 恢复由于KV缓存压缩造成的性能损失的方法[[1](#bib.bib1), [18](#bib.bib18)]; 6) 分层推测解码方法[[69](#bib.bib69)]。然而，这些方法并没有解决预填充阶段注意力计算的巨大负担。
- en: 6 Conclusion
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'This paper addresses the expensive computational cost and the unacceptable
    latency of the attention calculations in the pre-filling stage of long-context
    LLMs. We propose MInference, a method that accelerates the pre-filling stage by
    leveraging dynamic sparse attention with spatial aggregation patterns. Specifically,
    we categorize attention heads into three types: A-shape, Vertical-Slash, and Block-Sparse.
    Using a kernel-aware optimal sparse pattern search method, we identify the optimal
    pattern for each head. Subsequently, we utilize a fast approximation approach
    to build dynamic sparse masks for different inputs, and then apply these mask
    to perform sparse attention calculations. Experimental results on benchmarks such
    as InfiniteBench, RULER, language modeling, and Needle In A Haystack demonstrate
    that our method effectively maintains the long-context capabilities of LLMs while
    achieving up to a 10x speedup, reducing the latency from 30 minutes to 3 minutes
    per prompt for 1 million token prompts on a single A100 GPU. Additionally, we
    have found that similar dynamic sparse attention patterns also exist in both multi-modal
    LLMs [[80](#bib.bib80)] and encoder-decoder LLMs [[66](#bib.bib66)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解决了长上下文 LLMs 在预填充阶段的昂贵计算成本和不可接受的延迟问题。我们提出了 MInference，一种通过利用具有空间聚合模式的动态稀疏注意力来加速预填充阶段的方法。具体而言，我们将注意力头分为三种类型：A
    形、垂直斜线和块稀疏。通过使用一个基于核的最优稀疏模式搜索方法，我们识别出每个头的最佳模式。随后，我们利用快速近似方法为不同的输入构建动态稀疏掩码，然后应用这些掩码进行稀疏注意力计算。在
    InfiniteBench、RULER、语言建模和 Needle In A Haystack 等基准测试上的实验结果表明，我们的方法有效地保持了 LLMs
    的长上下文能力，同时实现了最高 10 倍的加速，将每个 100 万标记的提示的延迟从 30 分钟减少到 3 分钟（在单个 A100 GPU 上）。此外，我们发现类似的动态稀疏注意力模式也存在于多模态
    LLMs [[80](#bib.bib80)] 和编码器-解码器 LLMs [[66](#bib.bib66)] 中。使用 MInference 进行预填充阶段的推理加速具有很大潜力。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AAJ^+ [24] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya
    Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key
    tokens selection for efficient generative inference. Proceedings of Machine Learning
    and Systems, 6:114–127, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AAJ^+ [24] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya
    Soloveychik, and Purushotham Kamath. Keyformer: 通过选择关键令牌进行 KV 缓存减少以提高生成推理效率。机器学习与系统会议论文集，6:114–127，2024。'
- en: 'AJA^+ [24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed
    Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del
    Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek
    Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie
    Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo
    Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen
    Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi,
    Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid
    Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,
    Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning
    Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua
    Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,
    Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna
    Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report:
    A highly capable language model locally on your phone. ArXiv, abs/2404.14219,
    2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AJA^+ [24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed
    Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie
    Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg,
    Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett,
    Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,
    Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi
    Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra,
    Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas
    Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy,
    Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital
    Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel
    Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav,
    Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,
    Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 技术报告：在手机上运行的高性能语言模型。ArXiv，abs/2404.14219，2024。
- en: 'ALTdJ^+ [23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,
    Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALTdJ^+ [23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,
    Federico Lebron 和 Sumit Sanghai。Gqa：从多头检查点训练广义多查询变换器模型。2023年。
- en: 'BANG [23] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley.
    Unlimiformer: Long-range transformers with unlimited length input. In Thirty-seventh
    Conference on Neural Information Processing Systems, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BANG [23] Amanda Bertsch, Uri Alon, Graham Neubig 和 Matthew R. Gormley。Unlimiformer：具有无限长度输入的长程变换器。在第37届神经信息处理系统大会，2023年。
- en: BBC^+ [23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. Qwen technical report. ArXiv preprint, abs/2309.16609,
    2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BBC^+ [23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou 和 Tianhang Zhu。Qwen技术报告。ArXiv 预印本，abs/2309.16609，2023年。
- en: 'BPC [20] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. ArXiv preprint, abs/2004.05150, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPC [20] Iz Beltagy, Matthew E Peters 和 Arman Cohan。Longformer：长文档变换器。ArXiv
    预印本，abs/2004.05150，2020年。
- en: 'BSK^+ [23] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun
    Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok, and Shashank Shet. Codeplan:
    Repository-level coding using LLMs and planning. In NeurIPS 2023 Foundation Models
    for Decision Making Workshop, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BSK^+ [23] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun
    Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok 和 Shashank Shet。Codeplan：使用LLMs和规划进行仓库级编码。在NeurIPS
    2023决策制定基础模型研讨会，2023年。
- en: CGRS [19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. ArXiv preprint, abs/1904.10509, 2019.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGRS [19] Rewon Child, Scott Gray, Alec Radford 和 Ilya Sutskever。使用稀疏变换器生成长序列。ArXiv
    预印本，abs/1904.10509，2019年。
- en: 'CPG^+ [23] Avi Caciularu, Matthew E Peters, Jacob Goldberger, Ido Dagan, and
    Arman Cohan. Peek across: Improving multi-document modeling via cross-document
    question-answering. In Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 1970–1989, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPG^+ [23] Avi Caciularu, Matthew E Peters, Jacob Goldberger, Ido Dagan 和 Arman
    Cohan。Peek across：通过跨文档问答改进多文档建模。在第61届计算语言学协会年会（第1卷：长篇论文）的会议记录中，第1970–1989页，2023年。
- en: CWCT [23] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. ArXiv preprint,
    abs/2306.15595, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CWCT [23] Shouyuan Chen, Sherman Wong, Liangjian Chen 和 Yuandong Tian。通过位置插值扩展大语言模型的上下文窗口。ArXiv
    预印本，abs/2306.15595，2023年。
- en: 'CWW^+ [24] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing,
    Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up
    vision foundation models and aligning for generic visual-linguistic tasks. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 24185–24198, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CWW^+ [24] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing,
    Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu 等。Internvl：扩展视觉基础模型并对齐以适应通用视觉语言任务。在IEEE/CVF计算机视觉与模式识别会议论文集，第24185–24198页，2024年。
- en: 'DA [24] DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts
    language model, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DA [24] DeepSeek-AI。Deepseek-v2：一个强大、经济且高效的专家混合语言模型，2024年。
- en: 'Dao [24] Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. In The Twelfth International Conference on Learning Representations,
    2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao [24] Tri Dao。Flashattention-2：更快的注意力机制，具有更好的并行性和工作分区。在第十二届国际学习表征会议，2024年。
- en: 'DG [24] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and
    efficient algorithms through structured state space duality. In Forty-first International
    Conference on Machine Learning, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DG [24] Tri Dao 和 Albert Gu。变换器是SSMs：通过结构状态空间对偶性实现的广义模型和高效算法。在第41届国际机器学习大会，2024年。
- en: DHJ^+ [24] Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei
    Bi, and Shuming Shi. Sequence can secretly tell you what to discard. ArXiv preprint,
    abs/2404.15949, 2024.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DHJ^+ [24] 戴金城、黄卓伟、姜海云、陈晨、邓彩、毕伟、施树铭。序列可以悄悄告诉你该丢弃什么。ArXiv 预印本，abs/2404.15949，2024年。
- en: 'DMD^+ [23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang,
    Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. ArXiv preprint, abs/2307.02486, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DMD^+ [23] 丁家瑜、马树铭、董立、张兴兴、黄少汉、王文辉、郑楠宁、魏福如。Longnet: 将变换器扩展到 1,000,000,000 个标记。ArXiv
    预印本，abs/2307.02486，2023年。'
- en: DSY [24] Yichuan Deng, Zhao Song, and Chiwun Yang. Attention is naturally sparse
    with gaussian distributed input. ArXiv preprint, abs/2404.02690, 2024.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSY [24] 邓一川、宋赵、杨池文。注意力在高斯分布输入下自然稀疏。ArXiv 预印本，abs/2404.02690，2024年。
- en: 'DYZ^+ [24] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi,
    and Beidi Chen. Get more with LESS: Synthesizing recurrence with KV cache compression
    for efficient LLM inference. In Forty-first International Conference on Machine
    Learning, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DYZ^+ [24] 董哈利、杨欣宇、张震宇、王章阳、池月杰、陈贝迪。用LESS获得更多：通过键值缓存压缩合成递归以提高LLM推理效率。发表于第41届国际机器学习大会，2024年。
- en: 'DZZ^+ [24] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning
    Shang, Jiahang Xu, Fan Yang, and Mao Yang. LongroPE: Extending LLM context window
    beyond 2 million tokens. In Forty-first International Conference on Machine Learning,
    2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DZZ^+ [24] 丁怡然、张丽娜、张成瑞东、徐媛媛、尚宁、徐家航、杨帆、杨毛。LongroPE：将LLM上下文窗口扩展到超过 200 万标记。发表于第41届国际机器学习大会，2024年。
- en: FPN^+ [24] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context.
    In Forty-first International Conference on Machine Learning, 2024.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPN^+ [24] 付尧、潘拉梅斯瓦、牛鑫耀、岳翔、哈内赫·哈吉什尔兹、金尹、彭浩。数据工程：将语言模型扩展到 128k 上下文。发表于第41届国际机器学习大会，2024年。
- en: 'Fu [24] Yao Fu. Challenges in deploying long-context transformers: A theoretical
    peak performance analysis. ArXiv preprint, abs/2405.08944, 2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu [24] 付尧。部署长上下文变换器的挑战：理论最高性能分析。ArXiv 预印本，abs/2405.08944，2024年。
- en: 'GD [23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective
    state spaces. ArXiv preprint, abs/2312.00752, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GD [23] 阿尔伯特·顾、特里·道。Mamba：具有选择性状态空间的线性时间序列建模。ArXiv 预印本，abs/2312.00752，2023年。
- en: GGR [22] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long
    sequences with structured state spaces. In The Tenth International Conference
    on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GGR [22] 阿尔伯特·顾、卡兰·戈尔、克里斯托弗·雷。使用结构化状态空间高效建模长序列。发表于第十届国际学习表征大会，ICLR 2022，虚拟会议，2022年4月25-29日。
- en: Gra [24] Gradient. Llama-3 8b instruct gradient 4194k (v0.1), 2024.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gra [24] Gradient。Llama-3 8b instruct gradient 4194k (v0.1)，2024年。
- en: 'GZL^+ [24] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and
    Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for
    llms. In The Twelfth International Conference on Learning Representations, 2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GZL^+ [24] 何苏宇、张云南、刘丽媛、张敏佳、韩家炜、高剑锋。模型告诉你该丢弃什么：针对大型语言模型的自适应键值缓存压缩。发表于第十二届国际学习表征大会，2024年。
- en: 'GZX^+ [24] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin,
    Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of
    large language models from glm-130b to glm-4 all tools. ArXiv preprint, abs/2406.12793,
    2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GZX^+ [24] GLM团队、曾懋安、徐斌、王博文、张晨辉、尹大、迭戈·罗哈斯、冯冠宇、赵瀚林、赖涵宇等。Chatglm：从glm-130b到glm-4所有工具的大型语言模型家族。ArXiv
    预印本，abs/2406.12793，2024年。
- en: 'HBK^+ [24] Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal
    Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local
    language modeling for fast inference. ArXiv preprint, abs/2406.02657, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBK^+ [24] 朴南奎、裴尚民、金泰贤、乔贤植、金怡润、塔尔·舒斯特、亚当·费希、詹姆斯·索恩、尹世永。Block transformer：全球到局部的语言建模以实现快速推理。ArXiv
    预印本，abs/2406.02657，2024年。
- en: 'HSK^+ [24] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima
    Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What’s the real context
    size of your long-context language models? ArXiv preprint, abs/2404.06654, 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HSK^+ [24] 谢成平、孙思萌、塞缪尔·克里曼、尚塔努·阿查里亚、迪玛·瑞凯什、贾飞、杨张、鲍里斯·金斯堡。Ruler：你的长上下文语言模型的实际上下文大小是多少？ArXiv
    预印本，abs/2404.06654，2024年。
- en: 'HWP^+ [24] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language
    models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of
    the 2024 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3991–4008,
    Mexico City, Mexico, 2024\. Association for Computational Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'HWP^+ [24] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, 和
    Sinong Wang. LM-infinite: 大型语言模型的零样本极长序列泛化能力。编者 Kevin Duh, Helena Gomez, 和 Steven
    Bethard，2024年北美计算语言学协会年会论文集（第1卷：长篇论文），页3991–4008，墨西哥城，墨西哥，2024年。计算语言学协会。'
- en: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. ArXiv preprint, abs/2310.06825,
    2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, 等. Mistral 7b。ArXiv预印本，abs/2310.06825，2023年。
- en: 'JTZ^+ [23] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang,
    Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations
    for enabling training of extreme long sequence transformer models. ArXiv preprint,
    abs/2309.14509, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'JTZ^+ [23] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang,
    Leon Song, Samyam Rajbhandari, 和 Yuxiong He. Deepspeed ulysses: 用于极长序列变压器模型训练的系统优化。ArXiv预印本，abs/2309.14509，2023年。'
- en: 'JWL^+ [23] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili
    Qiu. Llmlingua: Compressing prompts for accelerated inference of large language
    models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing, pages 13358–13376, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'JWL^+ [23] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, 和 Lili Qiu.
    Llmlingua: 压缩提示以加速大型语言模型的推理。在2023年自然语言处理实证方法会议论文集中，页13358–13376，2023年。'
- en: 'JWL^+ [24] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin,
    Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long
    context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers). Association
    for Computational Linguistics, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'JWL^+ [24] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin,
    Yuqing Yang, 和 Lili Qiu. Longllmlingua: 通过提示压缩加速和增强长上下文场景中的LLMs。在第62届计算语言学协会年会论文集中（第1卷：长篇论文）。计算语言学协会，2024年。'
- en: 'JYW^+ [23] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin
    Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve
    real-world github issues? In The Twelfth International Conference on Learning
    Representations, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'JYW^+ [23] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin
    Pei, Ofir Press, 和 Karthik R Narasimhan. Swe-bench: 语言模型能解决现实世界的GitHub问题吗？在第十二届国际学习表征会议上，2023年。'
- en: Kam [23] Greg Kamradt. Needle in a haystack - pressure testing llms, 2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kam [23] Greg Kamradt. 针对LLMs的压力测试——大海捞针，2023年。
- en: 'KKL [20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. In 8th International Conference on Learning Representations, ICLR
    2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'KKL [20] Nikita Kitaev, Lukasz Kaiser, 和 Anselm Levskaya. Reformer: 高效的变压器。在第8届国际学习表征会议，ICLR
    2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日，2020年。'
- en: 'KVPF [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François
    Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.
    In Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
    Research, pages 5156–5165\. PMLR, 2020.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'KVPF [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, 和 François Fleuret.
    变压器即RNN: 具有线性注意力的快速自回归变压器。在第37届国际机器学习会议论文集中，ICML 2020，2020年7月13-18日，虚拟会议，第119卷机器学习研究论文集，页5156–5165。PMLR，2020年。'
- en: LCSR [21] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush.
    Block pruning for faster transformers. In Proceedings of the 2021 Conference on
    Empirical Methods in Natural Language Processing, pages 10619–10629, Online and
    Punta Cana, Dominican Republic, 2021\. Association for Computational Linguistics.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LCSR [21] François Lagunas, Ella Charlaix, Victor Sanh, 和 Alexander Rush. 快速变压器的块剪枝。在2021年自然语言处理实证方法会议论文集中，页10619–10629，线上与多米尼加共和国蓬塔卡纳，2021年。计算语言学协会。
- en: LCW [21] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On
    the expressive power of self-attention matrices. ArXiv preprint, abs/2106.03764,
    2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LCW [21] Valerii Likhosherstov, Krzysztof Choromanski 和 Adrian Weller. 关于自注意力矩阵的表现力。ArXiv
    预印本，abs/2106.03764，2021。
- en: LCW [23] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On
    the expressive flexibility of self-attention matrices. Proceedings of the AAAI
    Conference on Artificial Intelligence, 37(7):8773–8781, 2023.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LCW [23] Valerii Likhosherstov, Krzysztof Choromanski 和 Adrian Weller. 关于自注意力矩阵的表现力灵活性。发表于《AAAI
    人工智能会议论文集》，37(7):8773–8781, 2023。
- en: LDGL [23] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context
    to enhance inference efficiency of large language models. In Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing, 2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDGL [23] Yucheng Li, Bo Dong, Frank Guerin 和 Chenghua Lin. 压缩上下文以提高大型语言模型的推理效率。发表于
    2023 年自然语言处理经验方法会议论文集，2023。
- en: 'LDL^+ [24] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie,
    Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting
    the persistence of importance hypothesis for llm kv cache compression at test
    time. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LDL^+ [24] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie,
    Zhaozhuo Xu, Anastasios Kyrillidis 和 Anshumali Shrivastava. Scissorhands: 利用重要性假设的持久性进行
    LLM KV 缓存压缩。神经信息处理系统进展，36，2024。'
- en: 'LHY^+ [24] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli,
    Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what
    you are looking for before generation. ArXiv preprint, abs/2404.14469, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LHY^+ [24] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli,
    Hanchen Ye, Tianle Cai, Patrick Lewis 和 Deming Chen. Snapkv: Llm 知道你在生成之前想要什么。ArXiv
    预印本，abs/2404.14469，2024。'
- en: 'LLB^+ [24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin,
    Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz,
    et al. Jamba: A hybrid transformer-mamba language model. ArXiv preprint, abs/2403.19887,
    2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LLB^+ [24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin,
    Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz
    等。Jamba: 混合变换器-mamba 语言模型。ArXiv 预印本，abs/2403.19887，2024。'
- en: LLWL [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. Advances in neural information processing systems, 36, 2024.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLWL [24] Haotian Liu, Chunyuan Li, Qingyang Wu 和 Yong Jae Lee. 视觉指令调优。神经信息处理系统进展，36，2024。
- en: 'LMZ^+ [24] Zhiqi Lin, Youshan Miao, Quanlu Zhang, Fan Yang, Yi Zhu, Cheng Li,
    Saeed Maleki, Xu Cao, Ning Shang, Yilei Yang, Weijiang Xu, Mao Yang, Lintao Zhang,
    and Lidong Zhou. nnscaler: Constraint-guided parallelization plan generation for
    deep learning training. In 18th USENIX Symposium on Operating Systems Design and
    Implementation (OSDI 24). USENIX Association, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LMZ^+ [24] Zhiqi Lin, Youshan Miao, Quanlu Zhang, Fan Yang, Yi Zhu, Cheng Li,
    Saeed Maleki, Xu Cao, Ning Shang, Yilei Yang, Weijiang Xu, Mao Yang, Lintao Zhang
    和 Lidong Zhou. nnscaler: 约束引导的深度学习训练并行化计划生成。在第18届 USENIX 操作系统设计与实现研讨会（OSDI 24）中。USENIX
    协会，2024。'
- en: LQC^+ [22] Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding, and Yuan
    Xie. Dynamic sparse attention for scalable transformer acceleration. IEEE Transactions
    on Computers, 71(12):3165–3178, 2022.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LQC^+ [22] Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding 和 Yuan Xie.
    动态稀疏注意力以实现可扩展变换器加速。IEEE 计算机学报，71(12):3165–3178，2022。
- en: 'LWD^+ [23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi
    Chen. Deja vu: Contextual sparsity for efficient LLMs at inference time. In Andreas
    Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
    Scarlett, editors, Proceedings of the 40th International Conference on Machine
    Learning, Proceedings of Machine Learning Research. PMLR, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LWD^+ [23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re 和 Beidi Chen.
    Deja vu: 上下文稀疏性以提高 LLM 推理时的效率。发表于 Andreas Krause, Emma Brunskill, Kyunghyun Cho,
    Barbara Engelhardt, Sivan Sabato 和 Jonathan Scarlett 主编的《第40届国际机器学习会议论文集》，机器学习研究论文集。PMLR，2023。'
- en: LYZA [24] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model
    on million-length video and language with ringattention. ArXiv preprint, abs/2402.08268,
    2024.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LYZA [24] Hao Liu, Wilson Yan, Matei Zaharia 和 Pieter Abbeel. 在百万长度的视频和语言上使用世界模型与环形注意力。ArXiv
    预印本，abs/2402.08268，2024。
- en: LZA [23] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise
    transformers for near-infinite context. In NeurIPS 2023 Foundation Models for
    Decision Making Workshop, 2023.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LZA [23] Hao Liu, Matei Zaharia 和 Pieter Abbeel. 使用块状变换器的环形注意力以适应几乎无限的上下文。发表于
    NeurIPS 2023 决策制定基础模型研讨会，2023。
- en: LZD^+ [24] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context
    llms struggle with long in-context learning. ArXiv preprint, abs/2404.02060, 2024.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LZD^+ [24] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue 和 Wenhu Chen。长上下文LLMs在长上下文学习中挣扎。ArXiv预印本，abs/2404.02060，2024年。
- en: 'MEL [24] Yuzhen Mao, Martin Ester, and Ke Li. Iceformer: Accelerated inference
    with long-sequence transformers on CPUs. In The Twelfth International Conference
    on Learning Representations, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MEL [24] Yuzhen Mao, Martin Ester 和 Ke Li。Iceformer：在CPU上加速长序列变换器推理。发表于《第十二届国际学习表征会议》，2024年。
- en: 'MFG [24] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave
    no context behind: Efficient infinite context transformers with infini-attention.
    ArXiv preprint, abs/2404.07143, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MFG [24] Tsendsuren Munkhdalai, Manaal Faruqui 和 Siddharth Gopal。留下所有上下文：具有无限注意力的高效无限上下文变换器。ArXiv预印本，abs/2404.07143，2024年。
- en: 'NŁC^+ [24] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan,
    and Edoardo Ponti. Dynamic memory compression: Retrofitting LLMs for accelerated
    inference. In Forty-first International Conference on Machine Learning, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NŁC^+ [24] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan 和
    Edoardo Ponti。动态内存压缩：为加速推理调整LLMs。发表于《第41届国际机器学习会议》，2024年。
- en: NXH^+ [23] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen
    Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report. ArXiv preprint,
    abs/2309.03450, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NXH^+ [23] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen
    Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty 和 Caiming Xiong。Xgen-7b技术报告。ArXiv预印本，abs/2309.03450，2023年。
- en: OHAS [24] Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers
    are multi-state rnns. ArXiv preprint, abs/2401.06104, 2024.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OHAS [24] Matanel Oren, Michael Hassid, Yossi Adi 和 Roy Schwartz。变换器是多状态RNN。ArXiv预印本，abs/2401.06104，2024年。
- en: 'PAA^+ [23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian
    Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan
    Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit
    Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,
    Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV:
    Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika
    Bali, editors, Findings of the Association for Computational Linguistics: EMNLP
    2023, pages 14048–14077, Singapore, 2023\. Association for Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PAA^+ [23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian
    Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan
    Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit
    Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,
    Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu 和 Rui-Jie Zhu。RWKV：为变换器时代重新发明RNN。在Houda
    Bouamor, Juan Pino 和 Kalika Bali 编辑的《计算语言学协会发现：EMNLP 2023》中，第14048-14077页，新加坡，2023年。计算语言学协会。
- en: 'POC^+ [23] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User
    Interface Software and Technology, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POC^+ [23] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S. Bernstein。生成代理：人类行为的交互模拟。发表于《第36届年度ACM用户界面软件和技术研讨会》，2023年。
- en: PPJF [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret.
    Fast attention over long sequences with dynamic sparse flash attention. Advances
    in Neural Information Processing Systems, 36, 2024.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPJF [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi 和 François Fleuret。使用动态稀疏闪存注意力的长序列快速注意力。发表于《神经信息处理系统进展》，第36卷，2024年。
- en: 'PQFS [24] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn:
    Efficient context window extension of large language models. In The Twelfth International
    Conference on Learning Representations, 2024.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PQFS [24] Bowen Peng, Jeffrey Quesnelle, Honglu Fan 和 Enrico Shippole。Yarn：大语言模型的高效上下文窗口扩展。发表于《第十二届国际学习表征会议》，2024年。
- en: 'PSL [22] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long:
    Attention with linear biases enables input length extrapolation. In The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29, 2022, 2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSL [22] Ofir Press, Noah A. Smith, 和 Mike Lewis. 短期训练，长期测试：具有线性偏差的注意力使输入长度外推成为可能。第十届国际学习表示会议，ICLR
    2022，虚拟会议，2022年4月25-29日。
- en: 'PWJ^+ [24] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo,
    Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2:
    Data distillation for efficient and faithful task-agnostic prompt compression.
    In Findings of the Association for Computational Linguistics: ACL 2024. Association
    for Computational Linguistics, 2024.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PWJ^+ [24] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo,
    Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin 等。Llmlingua-2：用于高效且忠实的任务无关提示压缩的数据蒸馏。计算语言学协会成果：ACL
    2024。计算语言学协会，2024年。
- en: 'RCHG^+ [24] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
    Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient LLM inference.
    In Forty-first International Conference on Machine Learning, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RCHG^+ [24] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
    Carlo Luschi, 和 Douglas Orr. Sparq 注意力：带宽高效的 LLM 推理。第41届国际机器学习大会，2024年。
- en: 'RLL^+ [24] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu
    Chen. Samba: Simple hybrid state space models for efficient unlimited context
    language modeling. ArXiv preprint, abs/2406.07522, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLL^+ [24] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, 和 Weizhu
    Chen. Samba：用于高效无限上下文语言建模的简单混合状态空间模型。ArXiv 预印本，abs/2406.07522，2024年。
- en: RPJ^+ [20] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier,
    and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling.
    In 8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020, 2020.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RPJ^+ [20] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier,
    和 Timothy P. Lillicrap. 压缩变换器用于长序列建模。第8届国际学习表示会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日。
- en: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 探索统一的文本到文本变换器的迁移学习极限。机器学习研究期刊，21(140):1–67，2020年。
- en: 'RST^+ [24] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RST^+ [24] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser 等。Gemini 1.5：解锁跨百万令牌上下文的多模态理解。ArXiv 预印本，abs/2403.05530，2024年。
- en: RSVG [21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient
    content-based sparse attention with routing transformers. Transactions of the
    Association for Computational Linguistics, 9:53–68, 2021.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSVG [21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, 和 David Grangier. 高效的基于内容的稀疏注意力与路由变换器。计算语言学协会会刊，9:53–68，2021年。
- en: 'SCY^+ [24] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi
    Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical
    speculative decoding. ArXiv preprint, abs/2404.11912, 2024.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCY^+ [24] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, 和 Beidi Chen.
    Triforce：通过层次化的推测解码加速长序列生成。ArXiv 预印本，abs/2404.11912，2024年。
- en: 'SDH^+ [23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer
    for large language models. ArXiv preprint, abs/2307.08621, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDH^+ [23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang, 和 Furu Wei. 记忆网络：大语言模型的变换器继任者。ArXiv 预印本，abs/2307.08621，2023年。
- en: 'SDZ^+ [24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming
    Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder
    architectures for language models. ArXiv preprint, abs/2405.05254, 2024.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDZ^+ [24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma,
    Quanlu Zhang, Jianyong Wang, 和 Furu Wei. 你只缓存一次：用于语言模型的解码器-解码器架构。ArXiv 预印本，abs/2405.05254，2024年。
- en: 'SGR^+ [21] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo
    Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in
    self-attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th
    International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
    Event, volume 139 of Proceedings of Machine Learning Research, pages 9547–9557\.
    PMLR, 2021.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGR^+ [21] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo
    Li 和 James Tin-Yau Kwok。Sparsebert：重新思考自注意力中的重要性分析。编辑：Marina Meila 和 Tong Zhang，发表于第38届国际机器学习大会，ICML
    2021，2021年7月18-24日，虚拟活动，机器学习研究论文集第139卷，页码 9547–9557。PMLR，2021年。
- en: 'Sha [19] Noam Shazeer. Fast transformer decoding: One write-head is all you
    need. ArXiv preprint, abs/1911.02150, 2019.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sha [19] Noam Shazeer。快速转换器解码：你只需要一个写头。ArXiv预印本，abs/1911.02150，2019年。
- en: 'TDT^+ [23] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei,
    Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou,
    Neil Houlsby, and Donald Metzler. UL2: Unifying language learning paradigms. In
    The Eleventh International Conference on Learning Representations, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TDT^+ [23] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei,
    Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou,
    Neil Houlsby 和 Donald Metzler。UL2：统一语言学习范式。发表于第十一届国际学习表征会议，2023年。
- en: 'TKC [19] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate
    language and compiler for tiled neural network computations. In Proceedings of
    the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages, pages 10–19, 2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TKC [19] Philippe Tillet, Hsiang-Tsung Kung 和 David Cox。Triton：一种用于平铺神经网络计算的中间语言和编译器。发表于第3届ACM
    SIGPLAN国际机器学习与编程语言研讨会，页码 10–19，2019年。
- en: tri [23] Triton implementation of the flash attention v2 algorithm. Technical
    report, OpenAI, 2023.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tri [23] Triton实现的flash attention v2算法。技术报告，OpenAI，2023年。
- en: 'TSP^+ [23] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu,
    Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training
    for context scaling. In Thirty-seventh Conference on Neural Information Processing
    Systems, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TSP^+ [23] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu,
    Henryk Michalewski 和 Piotr Miłoś。聚焦转换器：用于上下文缩放的对比训练。发表于第三十七届神经信息处理系统会议，2023年。
- en: 'TZZ^+ [24] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci,
    and Song Han. QUEST: Query-aware sparsity for efficient long-context LLM inference.
    In Forty-first International Conference on Machine Learning, 2024.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TZZ^+ [24] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci
    和 Song Han。QUEST：基于查询的稀疏性，用于高效的长上下文LLM推理。发表于第41届国际机器学习大会，2024年。
- en: Wen [23] Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, 2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen [23] Lilian Weng。Llm驱动的自主代理。lilianweng.github.io，2023年。
- en: 'WWL^+ [24] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng
    Jin, Longyue Wang, and Li Yuan. Look-m: Look-once optimization in kv cache for
    efficient multimodal long-context inference. ArXiv preprint, abs/2406.18139, 2024.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WWL^+ [24] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin,
    Longyue Wang 和 Li Yuan。Look-m：kv缓存中的一次性优化，用于高效的多模态长上下文推理。ArXiv预印本，abs/2406.18139，2024年。
- en: 'WZH [21] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse
    attention architecture with cascade token and head pruning. In 2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA), pages 97–110\. IEEE,
    2021.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WZH [21] Hanrui Wang, Zhekai Zhang 和 Song Han。Spatten：具有级联标记和头修剪的高效稀疏注意力架构。发表于2021年IEEE高性能计算架构国际研讨会（HPCA），页码
    97–110。IEEE，2021年。
- en: XTC^+ [24] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
    Efficient streaming language models with attention sinks. In The Twelfth International
    Conference on Learning Representations, 2024.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XTC^+ [24] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han 和 Mike Lewis。具有注意力接收器的高效流式语言模型。发表于第十二届国际学习表征会议，2024年。
- en: 'XZH^+ [24] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin,
    Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the
    intrinsic capacity of llms for understanding extremely long sequences with training-free
    memory. ArXiv preprint, abs/2402.04617, 2024.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XZH^+ [24] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan
    Zhang, Zhiyuan Liu, Song Han 和 Maosong Sun。Infllm：揭示llms在理解极长序列中的固有能力，无需训练的记忆。ArXiv预印本，abs/2402.04617，2024年。
- en: 'YCL^+ [24] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei
    Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation
    models by 01\. ai. ArXiv preprint, abs/2403.04652, 2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YCL^+ [24] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang,
    Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang 等人。Yi：由01\. ai提供的开放基础模型。ArXiv预印本，abs/2403.04652，2024年。
- en: ZAW [24] Itamar Zimerman, Ameen Ali, and Lior Wolf. A unified implicit attention
    formulation for gated-linear recurrent sequence models. ArXiv preprint, abs/2405.16504,
    2024.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZAW [24] 伊塔玛·齐默尔曼、阿敏·阿里和利奥尔·沃尔夫。《一种统一的隐式注意力公式用于门控线性递归序列模型》。ArXiv预印本，abs/2405.16504，2024年。
- en: 'ZCH^+ [24] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen,
    Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. $\infty$bench:
    Extending long context evaluation beyond 100k tokens. ArXiv preprint, abs/2402.13718,
    2024.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZCH^+ [24] 欣荣·张、英发·陈、胜丁·胡、自航·徐、俊浩·陈、木开·郝、徐·韩、振·冷泰、硕·王、智远·刘等。《$\infty$bench：扩展超过100k标记的长上下文评估》。ArXiv预印本，abs/2402.13718，2024年。
- en: 'ZGD^+ [20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
    Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
    and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
    editors, Advances in Neural Information Processing Systems 33: Annual Conference
    on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZGD^+ [20] 曼兹尔·扎赫尔、古鲁·古鲁加内什、库马尔·阿维纳瓦·杜比、乔舒亚·安斯利、克里斯·阿尔贝提、圣地亚哥·翁塔农、菲利普·范、阿尼鲁德·拉武拉、启凡·王、李·杨和阿姆尔·艾哈迈德。《Big
    bird：用于更长序列的变换器》。在雨果·拉罗谢尔、马克·奥雷利奥·兰扎托、拉亚·哈德塞尔、玛利亚-弗洛丽娜·巴尔坎和萧云田编者的《神经信息处理系统进展33：2020年神经信息处理系统年会》，NeurIPS
    2020，2020年12月6-12日，虚拟，2020年。
- en: 'ZJZ^+ [23] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao
    Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, et al. Pit:
    Optimization of dynamic sparse deep learning models via permutation invariant
    transformation. In Proceedings of the 29th Symposium on Operating Systems Principles,
    pages 331–347, 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZJZ^+ [23] 宁心·郑、慧强·姜、全禄·张、振华·韩、凌霄·马、玉清·杨、凡·杨、成瑞东·张、莉莉·邱、毛·杨等。《Pit：通过置换不变变换优化动态稀疏深度学习模型》。发表于第29届操作系统原理研讨会，页面331–347，2023年。
- en: 'ZSZ^+ [24] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng,
    Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o:
    Heavy-hitter oracle for efficient generative inference of large language models.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZSZ^+ [24] 珍宇·张、英生·盛、天意·周、天龙·陈、连敏·郑、瑞思·蔡、赵·宋、远东·田、克里斯托弗·雷、克拉克·巴雷特等。《H2o：大型语言模型高效生成推理的重型预言机》。神经信息处理系统进展，36，2024年。
- en: Appendix A Limitations
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 限制
- en: As the context length decreases, the time required to build the dynamic index
    becomes more significant as attention computation time decreases. For example,
    with a 10k context, the time spent on building the index increases from 5% to
    30%, resulting in overall end-to-end latency approaching that of FlashAttention.
    However, this overhead proportion gradually decreases as the prompt lengthens.
    Additionally, when using a higher sparsity rate, the model performance may noticeably
    decline.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 随着上下文长度的减少，构建动态索引所需的时间变得更加显著，因为注意力计算时间减少。例如，在10k上下文的情况下，构建索引的时间从5%增加到30%，导致整体端到端延迟接近FlashAttention。然而，随着提示长度的增加，这种开销比例逐渐降低。此外，使用更高稀疏率时，模型性能可能显著下降。
- en: Appendix B Broader Impacts
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 更广泛的影响
- en: MInference effectively accelerates the inference of long-context LLMs, facilitating
    their deployment and application. By enabling lower latency, it can reduce the
    deployment costs of LLMs, especially for long-context LLMs, helping to democratize
    access to advanced AI. It also promotes further research and development in related
    fields.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: MInference有效加速长上下文LLMs的推理，促进其部署和应用。通过降低延迟，它可以减少LLMs的部署成本，特别是对于长上下文LLMs，有助于实现先进AI的普及。它还促进了相关领域的进一步研究和发展。
- en: Appendix C Experiment Details
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 实验细节
- en: C.1 Dataset Details
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 数据集详情
- en: InfiniteBench [[86](#bib.bib86)]
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: InfiniteBench [[86](#bib.bib86)]
- en: includes 10 tasks designed to test various aspects of long-context processing.
    Specifically, these tasks cover entire novel summarization, open-form question
    answering based on novels, multiple-choice question answering on novels, question
    answering on long drama scripts, question answering on Chinese texts, debugging
    large code repositories, identifying the largest/smallest number in arrays, and
    retrieval tasks with varying pattern lengths. The average token length for these
    tasks is 214k, and they include 3,992 examples.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 包含 10 项任务，旨在测试长上下文处理的各个方面。这些任务具体包括整个小说的总结、基于小说的开放式问答、关于小说的多项选择问答、对长剧本的问答、对中文文本的问答、大型代码库的调试、识别数组中的最大/最小值以及具有不同模式长度的检索任务。这些任务的平均标记长度为
    214k，包含 3,992 个示例。
- en: RULER [[28](#bib.bib28)]
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RULER [[28](#bib.bib28)]
- en: is a recent synthetic benchmark suite for long-context evaluation with 13 complex
    tasks across four categories. The retrieval category includes Single Needle-in-a-Haystack
    (S-NIAH), where a single key-value pair is inserted into noisy text, and the model
    must retrieve the value. Multi-keys Needle-in-a-Haystack (MK-NIAH) involves multiple
    keys, and the model retrieves one specific value among hard distractors. The Multi-values
    Needle-in-a-Haystack (MV-NIAH) task requires retrieving all values associated
    with a single key, while the Multi-queries Needle-in-a-Haystack (MQ-NIAH) task
    involves retrieving values for multiple keys. The Multi-hop Tracing category includes
    Variable Tracking (VT), where the model traces and returns all variable names
    pointing to the same value through variable bindings. The aggregation category
    introduces Common Words Extraction (CWE), where the model identifies the top-K
    common words from a mixture of common and uncommon words, and Frequent Words Extraction
    (FWE), where the model identifies the most frequent words from a Zeta distribution.
    The Question Answering (QA) category extends existing short-context QA datasets
    by adding distracting paragraphs, challenging the model to answer questions based
    on relevant information surrounded by distractors. These tasks provide a comprehensive
    evaluation of long-context modeling capabilities, covering multi-hop reasoning,
    aggregation, and complex question answering. Following [[28](#bib.bib28)], we
    test models on 4K, 8K, 16K, 32K, 64K, and 128K context lengths, including 2,600
    examples per length.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个针对长上下文评估的最新合成基准套件，涵盖四个类别的 13 个复杂任务。检索类别包括单一 Needle-in-a-Haystack (S-NIAH)，其中一个键值对被插入到嘈杂文本中，模型必须检索该值。多键
    Needle-in-a-Haystack (MK-NIAH) 涉及多个键，模型在难以区分的干扰项中检索一个特定值。多值 Needle-in-a-Haystack
    (MV-NIAH) 任务要求检索与单一键关联的所有值，而多查询 Needle-in-a-Haystack (MQ-NIAH) 任务则涉及为多个键检索值。多跳跟踪类别包括变量跟踪
    (VT)，模型跟踪并返回指向相同值的所有变量名。聚合类别引入了常见词提取 (CWE)，模型从常见和不常见词的混合中识别前 K 个常见词，以及频繁词提取 (FWE)，模型从
    Zeta 分布中识别最频繁的词。问答 (QA) 类别通过添加干扰段落扩展了现有的短上下文 QA 数据集，挑战模型基于被干扰信息的相关信息回答问题。这些任务提供了对长上下文建模能力的全面评估，涵盖了多跳推理、聚合和复杂问答。根据
    [[28](#bib.bib28)]，我们在 4K、8K、16K、32K、64K 和 128K 上下文长度上测试模型，每个长度包括 2,600 个示例。
- en: Needle In A Haystack task [[35](#bib.bib35)]
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 针对长上下文处理设计的 Needle In A Haystack 任务 [[35](#bib.bib35)]
- en: evaluates the performance of retrieval-augmented generation (RAG) systems by
    embedding specific, targeted information (the "needle") within a large, complex
    body of text (the "haystack"). The test assesses a language model’s ability to
    identify and utilize this specific piece of information amidst a vast amount of
    data. Both RULER and the needle test iterate over various context lengths and
    document depths (where the ground-truth is placed in the prompt) to measure the
    long-context performance. Here we scale the Needle In A Haystack task to 1M context
    length, including 750 examples.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在大量复杂文本（“干草堆”）中嵌入特定的目标信息（“针”），评估检索增强生成 (RAG) 系统的性能。测试评估语言模型在大量数据中识别和利用这一特定信息的能力。RULER
    和 Needle In A Haystack 任务都会对各种上下文长度和文档深度进行迭代（其中真实情况放在提示中），以衡量长上下文性能。这里我们将 Needle
    In A Haystack 任务扩展到 1M 上下文长度，包括 750 个示例。
- en: PG-19 [[65](#bib.bib65)]
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PG-19 [[65](#bib.bib65)]
- en: The perplexity on long text is also often used by researchers to evaluate the
    language modeling performance of long-context LLMs. PG-19 is a suitable test set
    for this task, as it includes texts as long as 500K tokens. Perplexity is used
    as the metric indicating how well a model predicts the next token in a sequence.
    Our experiments are conducted on 1,000 random samples from PG-19 that are longer
    than 100K tokens.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本的困惑度也常被研究人员用来评估长上下文LLM的语言建模性能。PG-19是一个适合此任务的测试集，因为它包含长达500K标记的文本。困惑度被用作指标，以表示模型预测序列中下一个标记的能力。我们的实验基于从PG-19中随机抽取的1,000个长度超过100K标记的样本。
- en: C.2 Additional Implementation Details
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 额外的实现细节
- en: 'Our experiments are based on a number of state-of-the-art long-context LLMs:
    1) LLaMA-3-8B-Instruct-262k³³3https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k
    is a LLaMA-3 variant with further NTK-aware interpolation and minimal fine-tuning
    with Ring Attention, which achieved SOTA results on long-context assessments such
    as the Needle In A Haystack test; 2) LLaMA-3-8B-Instruct-1048k⁴⁴4https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k
    is similar to LLaMA-3-8B-Instruct-262k, but supports context lengths up to 1M
    tokens; 3) Yi-9B-200K [[84](#bib.bib84)] is a SOTA LLM that balances long-context
    performance with general capabilities; 4) Phi-3-Mini-128K [[2](#bib.bib2)] a small
    but powerful language model that offers capabilities equivalent to models ten
    times its size with up to 128K context window powered by LongRoPE [[19](#bib.bib19)];
    5) Qwen2-7B-128K [[5](#bib.bib5)] is a recently release update of Qwen series
    model with up to 128K context window that achieve superior or comparable performance
    compared to LLaMA-3; 6) GLM-4-9B-1M [[26](#bib.bib26)] has been improved from
    its predecessor in terms of a 1M context window, performance on downstream tasks
    and inference efficiency. To guarantee stable results, we use greedy decoding
    in all tests. Our kernel implementations are developed and optimized based on
    the dynamic sparse compiler PIT [[88](#bib.bib88)] in the Triton language [[75](#bib.bib75)].
    The latency experiments are done on a single Nvidia A100 GPU using bfloat16. We
    provide a simple custom implementation of attention in PyTorch, building on FlashAttention
    and Triton.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验基于多个最先进的长上下文LLM：1）LLaMA-3-8B-Instruct-262k³³3https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k
    是LLaMA-3的一个变体，具有进一步的NTK感知插值和最小的Ring Attention微调，在长上下文评估中取得了SOTA结果，例如Needle In
    A Haystack测试；2）LLaMA-3-8B-Instruct-1048k⁴⁴4https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k
    类似于LLaMA-3-8B-Instruct-262k，但支持长达1M标记的上下文；3）Yi-9B-200K [[84](#bib.bib84)] 是一个SOTA
    LLM，平衡了长上下文性能与通用能力；4）Phi-3-Mini-128K [[2](#bib.bib2)] 是一个小型但强大的语言模型，提供与十倍于其规模的模型相当的能力，具有最多128K的上下文窗口，由LongRoPE
    [[19](#bib.bib19)] 提供支持；5）Qwen2-7B-128K [[5](#bib.bib5)] 是Qwen系列模型的最新版本，具有最多128K的上下文窗口，相比LLaMA-3实现了更优或相当的性能；6）GLM-4-9B-1M
    [[26](#bib.bib26)] 在1M上下文窗口、下游任务性能和推理效率方面，相较于前身有了改进。为了保证结果的稳定性，我们在所有测试中使用贪婪解码。我们的内核实现是基于动态稀疏编译器PIT
    [[88](#bib.bib88)] 在Triton语言 [[75](#bib.bib75)]中开发和优化的。延迟实验在单个Nvidia A100 GPU上使用bfloat16进行。我们提供了一个简单的PyTorch自定义注意力实现，基于FlashAttention和Triton。
- en: 'We set the target FLOPs $t$ to be the same as 1k global tokens and 4k local
    window tokens in the A-shape pattern. The step size of ChangeSpace is set to 50,
    with the corresponding search space shown in Table [6](#A3.T6 "Table 6 ‣ C.2 Additional
    Implementation Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Additionally,
    we use only one sample as our validation set from KV retrieval synthetic data
    with 30k token inputs, which exhibits strong generalization and stability across
    different lengths and domains. The search time is approximately 15 minutes on
    a single A100. Additionally, we use the same optimal sparse pattern configuration
    for both the LLaMA-3-8B-Instruct-262K model and the LLaMA-3-8B-Instruct-1M model.
    The specific distribution is shown in Fig. [11](#A5.F11 "Figure 11 ‣ Appendix
    E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention").'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将目标FLOPs $t$ 设置为与A-shape模式中的1k全局令牌和4k局部窗口令牌相同。ChangeSpace的步长设置为50，对应的搜索空间如表 [6](#A3.T6
    "Table 6 ‣ C.2 Additional Implementation Details ‣ Appendix C Experiment Details
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention")所示。此外，我们仅使用一个来自30k令牌输入的KV检索合成数据样本作为验证集，该样本在不同长度和领域中展现出强大的泛化能力和稳定性。搜索时间大约为单个A100上的15分钟。此外，我们对LLaMA-3-8B-Instruct-262K模型和LLaMA-3-8B-Instruct-1M模型使用相同的最优稀疏模式配置。具体分布如图 [11](#A5.F11
    "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention")所示。'
- en: 'Table 6: Kernal-aware optimal head pattern search space. In this context, A-shape
    represents the global tokens and local window number, Vertical-Slash represents
    the Top-K number of vertical and diagonal lines, and Block-Sparse represents the
    Top-K number of blocks retained.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：内核感知最优头模式搜索空间。在这个上下文中，A-shape代表全局令牌和局部窗口数，Vertical-Slash代表垂直和对角线的Top-K数量，Block-Sparse代表保留的块的Top-K数量。
- en: '| Patterns | Search Space |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Patterns | Search Space |'
- en: '| A-shape | $\{(1024,4096)\}$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| A-shape | $\{(1024,4096)\}$ |'
- en: '| Vertical-Slash | $\{(30,2048),(100,1800),(500,1500),(3000,200)\}$ |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Vertical-Slash | $\{(30,2048),(100,1800),(500,1500),(3000,200)\}$ |'
- en: '| Block-Sparse | $\{100\}$ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Block-Sparse | $\{100\}$ |'
- en: C.3 Single A100 Implementation Details
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 单个A100实施细节
- en: 'The original PyTorch implementation⁵⁵5https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py
    of the LLaMA model causes an out-of-memory error on a single A100 (80G) when the
    prompt exceeds 50k tokens. To enable running 1M prompt inference on a single A100,
    we implemented the following optimizations:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA模型的原始PyTorch实现⁵⁵5https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py
    在提示超过50k令牌时，会在单个A100（80G）上导致内存溢出错误。为了在单个A100上进行1M提示推理，我们实施了以下优化：
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Tensor Splitting: We split the Attention by head and the MLP by sequence dimension.
    In long-context scenarios, where computation is the bottleneck, this splitting
    keeps GPU utilization at 100%, and the overhead of splitting is negligible;'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 张量分割：我们按头分割注意力，并按序列维度分割MLP。在计算成为瓶颈的长上下文场景中，这种分割保持了GPU的100%利用率，分割的开销可以忽略不计；
- en: '2.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Reduction of Intermediate Variables: We minimized intermediate variable allocation
    by removing the attention mask and implementing causal mask logic directly within
    the kernel;'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中间变量的减少：我们通过去除注意力掩码并在内核中直接实现因果掩码逻辑来最小化中间变量的分配；
- en: '3.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Elimination of Unnecessary Computations: In long-context scenarios, only the
    logits corresponding to the last token in the prompt phase are meaningful. Thus,
    we only retain the computation of the LM Head Linear layer for the last token.'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不必要计算的消除：在长上下文场景中，仅最后一个令牌的logits是有意义的。因此，我们只保留了最后一个令牌的LM Head Linear层的计算。
- en: C.4 Kernel Implementation
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 内核实现
- en: C.4.1 Block-Sparse Flash Attention
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.4.1 Block-Sparse Flash Attention
- en: Our Block-Sparse kernel implementation is based on the Triton version of the
    FlashAttention kernel [[76](#bib.bib76)]. With the selected block index as an
    additional input, each thread block loops through the top-K blocks in a row. As
    discussed in FlashAttention [[13](#bib.bib13)], the latency of the block-sparse
    FlashAttention kernel is linearly related to the number of blocks, and the speedup
    ratio (compared to the dense FlashAttention kernel) is approximately as,
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的块稀疏内核实现基于 Triton 版本的 FlashAttention 内核 [[76](#bib.bib76)]。通过选择的块索引作为额外输入，每个线程块循环遍历一行中的前
    K 个块。如 FlashAttention [[13](#bib.bib13)] 中所讨论，块稀疏 FlashAttention 内核的延迟与块的数量线性相关，速度提升比（与密集
    FlashAttention 内核相比）大约为，
- en: '|  | $s_{p}=\frac{S}{2B\times k_{b}}$ |  | (3) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{p}=\frac{S}{2B\times k_{b}}$ |  | (3) |'
- en: C.4.2 Vertical-Slash Attention
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.4.2 纵向斜线注意力
- en: 'The Vertical-Slash attention includes two custom kernels: the Vertical-Slash
    sparse index kernel and the Vertical-Slash sparse FlashAttention kernel.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 纵向斜线注意力包括两个自定义内核：纵向斜线稀疏索引内核和纵向斜线稀疏 FlashAttention 内核。
- en: '![Refer to caption](img/ff009019a243e049587bc52e961c7e21.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ff009019a243e049587bc52e961c7e21.png)'
- en: 'Figure 7: The dynamic sparse mask for the vertical-slash pattern using LLaMA-3-8B
    in the summarization task [[86](#bib.bib86)]. Yellow areas indicate the computed
    parts. Slash lines use $64\times 64$ blocks.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在总结任务中使用 LLaMA-3-8B 的纵向斜线模式的动态稀疏掩码 [[86](#bib.bib86)]。黄色区域表示计算部分。斜线线条使用
    $64\times 64$ 块。
- en: 'The Vertical-Slash sparse index kernel in Algorithm [4](#alg4 "Algorithm 4
    ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") builds the index for each row of blocks. Since
    a slash line segment can be masked by a square block, our attention mask is a
    mix of blocks and columns, as shown in Fig. [7](#A3.F7 "Figure 7 ‣ C.4.2 Vertical-Slash
    Attention ‣ C.4 Kernel Implementation ‣ Appendix C Experiment Details ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    We apply a point-range two-way merge algorithm where vertical indexes are treated
    as points and slash indexes are converted to ranges given the row index. The output
    consists of two parts: merged ranges and separate column indexes, where the ranges
    are represented by block indexes. The time complexity to build an index for a
    row is $O(k_{v}+k_{s})$.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 算法中的纵向斜线稀疏索引内核 [4](#alg4 "算法 4 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLM 的预填充") 为每行块构建索引。由于斜线线段可以被方块遮蔽，我们的注意力掩码是方块和列的混合，如图 [7](#A3.F7 "图 7 ‣ C.4.2
    纵向斜线注意力 ‣ C.4 内核实现 ‣ 附录 C 实验细节 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 所示。我们应用了一种点-范围双向合并算法，其中纵向索引被视为点，而斜线索引在给定行索引的情况下被转换为范围。输出包括两个部分：合并的范围和单独的列索引，其中范围由块索引表示。为一行构建索引的时间复杂度为
    $O(k_{v}+k_{s})$。
- en: 'The Vertical-Slash sparse FlashAttention kernel in Algorithm [5](#alg5 "Algorithm
    5 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") is a mix of the block-sparse attention kernel
    and the PIT [[88](#bib.bib88)] sparse attention kernel. PIT is a technology that
    loads sparse data into dense compute blocks via a Permutation Invariant Transformation.
    A thread block first loops through the block indexes as described in the previous
    section (block part) and then loops through the column indexes grouped by block
    size (PIT part). The latency of this hybrid kernel is linearly related to the
    total area of blocks and columns.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 算法中的纵向斜线稀疏 FlashAttention 内核 [5](#alg5 "算法 5 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLM 的预填充") 是块稀疏注意力内核和 PIT [[88](#bib.bib88)] 稀疏注意力内核的混合。PIT 是一种通过排列不变变换将稀疏数据加载到密集计算块中的技术。线程块首先循环遍历块索引，如前一节（块部分）所述，然后循环遍历按块大小分组的列索引（PIT
    部分）。该混合内核的延迟与块和列的总面积线性相关。
- en: Appendix D Additional Experiment Results
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 额外实验结果
- en: D.1 Needle In A Haystack
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 Needle In A Haystack
- en: '![Refer to caption](img/39bb7b8f651893d7e6f68e4c5a961b46.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39bb7b8f651893d7e6f68e4c5a961b46.png)'
- en: 'Figure 8: Results on Needle In A Haystack using InfLLM in LLaMA-3-8B-Instruct-1M.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 InfLLM 在 LLaMA-3-8B-Instruct-1M 上的 Needle In A Haystack 结果。
- en: 'In addition to the Needle In A Haystack results for LLaMA-3-Instruct-1M shown
    in §[4](#S4 "4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we also present the LLaMA-3-Instruct-1M using
    InfLLM results in Fig. [8](#A4.F8 "Figure 8 ‣ D.1 Needle In A Haystack ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"), and results for GLM-4-9B-1M,
    Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, shown in Fig. [9](#A4.F9 "Figure
    9 ‣ D.1 Needle In A Haystack ‣ Appendix D Additional Experiment Results ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    Compared to Full Attention, using MInference has minimal impact on the ability
    to understand semantic information across different context windows and needle
    depths. There is even a slight performance improvement around the 100k context
    length using Yi-9B-200K and Phi-3-Mini-128K.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '除了在§[4](#S4 "4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")中展示的LLaMA-3-Instruct-1M的针中针结果外，我们还展示了使用InfLLM的LLaMA-3-Instruct-1M结果，见图[8](#A4.F8
    "Figure 8 ‣ D.1 Needle In A Haystack ‣ Appendix D Additional Experiment Results
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention")，以及GLM-4-9B-1M、Yi-9B-200K、Phi-3-Mini-128K和Qwen2-7B-128K的结果，见图[9](#A4.F9
    "Figure 9 ‣ D.1 Needle In A Haystack ‣ Appendix D Additional Experiment Results
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention")。与完全注意力相比，使用MInference对不同上下文窗口和针深度的语义信息理解能力几乎没有影响。使用Yi-9B-200K和Phi-3-Mini-128K时，约100k上下文长度处甚至有轻微的性能提升。'
- en: '![Refer to caption](img/ba5ebb2989e79685f9c2208bddfa185d.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ba5ebb2989e79685f9c2208bddfa185d.png)'
- en: (a) GLM-4-9B-1M
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GLM-4-9B-1M
- en: '![Refer to caption](img/52f83beb6a222be5d31f20f298066c2e.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/52f83beb6a222be5d31f20f298066c2e.png)'
- en: (b) GLM-4-9B-1M w/ MInference
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GLM-4-9B-1M 使用 MInference
- en: '![Refer to caption](img/b938f1489e9142f62953ffa559352b51.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b938f1489e9142f62953ffa559352b51.png)'
- en: (c) Yi-9B-200K
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Yi-9B-200K
- en: '![Refer to caption](img/4cfe3b402cabfda105c1c3293ea50951.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4cfe3b402cabfda105c1c3293ea50951.png)'
- en: (d) Yi-9B-200K w/ MInference
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Yi-9B-200K 使用 MInference
- en: '![Refer to caption](img/fd910638ecb8839985282ec15fd8c006.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fd910638ecb8839985282ec15fd8c006.png)'
- en: (e) Phi-3-Mini-128K
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Phi-3-Mini-128K
- en: '![Refer to caption](img/565de9e1d007245060ad8c490747a28a.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/565de9e1d007245060ad8c490747a28a.png)'
- en: (f) Phi-3-Mini-128K w/ MInference
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Phi-3-Mini-128K 使用 MInference
- en: '![Refer to caption](img/32b3a37365e0176166e99b96732899d0.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/32b3a37365e0176166e99b96732899d0.png)'
- en: (g) Qwen2-7B-128K
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Qwen2-7B-128K
- en: '![Refer to caption](img/43463f60255f33abb96cf01e0ed331c7.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/43463f60255f33abb96cf01e0ed331c7.png)'
- en: (h) Qwen2-7B-128K w/ MInference
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Qwen2-7B-128K 使用 MInference
- en: 'Figure 9: Needle In A Haystack [[35](#bib.bib35)] results using GLM-4-9B-1M [[26](#bib.bib26)],
    Yi-9B-200K [[84](#bib.bib84)], Phi-3-Mini-128K [[2](#bib.bib2)], and Qwen2-7B-128K [[5](#bib.bib5)].'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用GLM-4-9B-1M [[26](#bib.bib26)]、Yi-9B-200K [[84](#bib.bib84)]、Phi-3-Mini-128K
    [[2](#bib.bib2)] 和Qwen2-7B-128K [[5](#bib.bib5)]的针中针[[35](#bib.bib35)]结果。
- en: D.2 Latency Breakdown
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 延迟分析
- en: '![Refer to caption](img/1b8f7add8e0f29d889bf7870f60b4586.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1b8f7add8e0f29d889bf7870f60b4586.png)'
- en: 'Figure 10: The latency breakdown of a single attention kernel for three patterns
    and FlashAttention [[13](#bib.bib13)] across different context windows in a single
    A100, including the index time for dynamic sparse approximation and building dynamic
    sparsity. At 10k tokens, the latency of the four kernels is very close and all
    are less than 1ms. At 1M tokens, the latency for A-shape is 164ms.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：在单个A100上，针对不同上下文窗口的三种模式和FlashAttention [[13](#bib.bib13)]的单个注意力内核的延迟分析，包括动态稀疏近似和构建动态稀疏的索引时间。在10k
    tokens时，四个内核的延迟非常接近，均小于1ms。在1M tokens时，A形状的延迟为164ms。
- en: 'Fig. [10](#A4.F10 "Figure 10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional
    Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") shows the micro-benchmark results of the three
    attention patterns proposed in this paper, as well as FlashAttention. It can be
    seen that Vertical-Slash is the slowest among the three patterns, but it still
    achieves a 13x speedup compared to FlashAttention under 1M context windows. A-shape
    is slightly faster than Vertical-Slash, but at 1M, A-shape is 50% slower than
    Vertical-Slash. Block-Sparse is the fastest, achieving a 30x speedup over FlashAttention
    under 1M context windows. The estimation and index-building time for the dynamic
    sparse pattern accounts for approximately 5%-15% and 25% of the total time for
    Vertical-Slash and Block-Sparse patterns, respectively. The index-building overhead
    is higher for Block-Sparse mainly due to the time-consuming MeanPooling and block-level
    matmul computations. Additionally, the memory overhead for sparse indexing is
    relatively small, remaining within 160MB for a LLaMA-3-8B model in 1M context.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [10](#A4.F10 "图 10 ‣ D.2 延迟分解 ‣ 附录 D 额外实验结果 ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文
    LLM 的预填充")显示了本文提出的三种注意力模式的微基准测试结果，以及 FlashAttention。可以看出，Vertical-Slash 是三种模式中速度最慢的，但在
    1M 上下文窗口下，它仍比 FlashAttention 快 13 倍。A-shape 比 Vertical-Slash 略快，但在 1M 上下文下，A-shape
    比 Vertical-Slash 慢 50%。Block-Sparse 是最快的，在 1M 上下文窗口下实现了比 FlashAttention 快 30 倍的加速。动态稀疏模式的估计和索引构建时间分别占
    Vertical-Slash 和 Block-Sparse 模式总时间的约 5%-15% 和 25%。Block-Sparse 的索引构建开销较高，主要是由于
    MeanPooling 和块级 matmul 计算耗时。此外，稀疏索引的内存开销相对较小，在 1M 上下文下 LLaMA-3-8B 模型的内存保持在 160MB
    以内。'
- en: D.3 Additional Ablation Study
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 额外的消融研究
- en: 'Table 7: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K
    on InfiniteBench [[86](#bib.bib86)]. It is important to note that due to kernel
    limitations, we must retain at least one vertical and one slash. Therefore, "ours
    w/ only vertical" retains the top-1 slash, and "ours w/ only slash" retains the
    top-1 vertical.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：使用 LLaMA-3-8B-Instruct-262K 在 InfiniteBench 上的不同消融方法的性能 [[86](#bib.bib86)]。需要注意的是，由于内核限制，我们必须至少保留一个垂直线和一个斜线。因此，“仅使用垂直线的我们”保留了
    top-1 斜线，而“仅使用斜线的我们”保留了 top-1 垂直线。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find | Retr.PassKey
    | Retr.Number | Retr.KV | 平均值 |'
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
- en: '| Ours w/ only vertical | 13.7 | 6.2 | 30.1 | 2.0 | 6.5 | 7.9 | 1.7 | 65.4
    | 52.7 | 0.0 | 18.6 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 仅使用垂直线的我们 | 13.7 | 6.2 | 30.1 | 2.0 | 6.5 | 7.9 | 1.7 | 65.4 | 52.7 | 0.0
    | 18.6 |'
- en: '| Ours w/ only slash | 18.4 | 11.5 | 60.1 | 3.0 | 11.4 | 22.1 | 28.4 | 100.0
    | 100.0 | 4.2 | 35.9 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 仅使用斜线的我们 | 18.4 | 11.5 | 60.1 | 3.0 | 11.4 | 22.1 | 28.4 | 100.0 | 100.0
    | 4.2 | 35.9 |'
- en: 'To further analyze the role of dynamic vertical and slash lines in the Vertical-Slash
    pattern for sparse computation, we introduce a new set of ablation studies as
    follows: 1) Ours w/ only vertical, which only uses vertical lines and the top-1
    slash line in Vertical-Slash pattern. 2) Ours w/ only slash, which only uses slash
    lines and the top-1 vertical line in Vertical-Slash pattern. The corresponding
    top-K quantities are set after converting based on FLOPs in kernel.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析 Vertical-Slash 模式中垂直线和斜线在稀疏计算中的作用，我们引入了一组新的消融研究：1）仅使用垂直线的我们，只使用垂直线和
    Vertical-Slash 模式中的 top-1 斜线。2）仅使用斜线的我们，只使用斜线和 Vertical-Slash 模式中的 top-1 垂直线。对应的
    top-K 数量在内核中基于 FLOPs 转换后设置。
- en: 'As shown in Table [7](#A4.T7 "Table 7 ‣ D.3 Additional Ablation Study ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"), using only vertical lines results
    in a significant performance drop, especially in retrieval tasks, where performance
    is similar to only using block-sparse. In contrast, using only slash lines retains
    most of the performance, but in highly dynamic tasks such as KV retrieval, performance
    further decreases, with an average performance drop of 2.9% compared to Ours.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [7](#A4.T7 "表格 7 ‣ D.3 额外的消融研究 ‣ 附录 D 额外实验结果 ‣ MInference 1.0: 通过动态稀疏注意力加速长上下文
    LLM 的预填充")所示，仅使用垂直线会导致性能显著下降，特别是在检索任务中，其性能与仅使用块稀疏相似。相比之下，仅使用斜线保持了大部分性能，但在像 KV
    检索这样的高度动态任务中，性能进一步下降，相比于我们的平均性能下降了 2.9%。'
- en: Appendix E Pattern Distribution
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 模式分布
- en: '![Refer to caption](img/5985fc55f8aafd48b024a5fae9e1aa72.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5985fc55f8aafd48b024a5fae9e1aa72.png)'
- en: (a) LLaMA-3-8B-Instruct-262K/1M
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLaMA-3-8B-Instruct-262K/1M
- en: '![Refer to caption](img/ee027e611a81c5a58fd894a1de67b87e.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ee027e611a81c5a58fd894a1de67b87e.png)'
- en: (b) Yi-9B-200K
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Yi-9B-200K
- en: 'Figure 11: Distribution of three sparse head patterns in different models.
    We use the same optimal sparse pattern configuration for both LLaMA-3-8B-Instruct-262K
    and LLaMA-3-8B-Instruct-1M.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：不同模型中三种稀疏头模式的分布。我们对LLaMA-3-8B-Instruct-262K和LLaMA-3-8B-Instruct-1M使用了相同的最优稀疏模式配置。
- en: 'Fig. [11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    shows the distribution of the optimal head configuration obtained through our
    search. Firstly, most of the patterns are the Vertical-Slash pattern (>90%). However,
    according to the ablation study, using only the Vertical-Slash pattern significantly
    impacts performance in highly dynamic tasks like KV retrieval. Secondly, the Block-Sparse
    pattern is primarily distributed in several intermediate to later layers, while
    the A-shape pattern is found in the middle layers. Although the optimal patterns
    vary slightly across different models, they generally align with these observations.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")展示了通过我们的搜索获得的最优头配置的分布。首先，大多数模式是纵向斜杠模式（>90%）。然而，根据消融研究，单独使用纵向斜杠模式会显著影响在KV检索等高度动态任务中的性能。其次，块稀疏模式主要分布在几个中间到后期层，而A形模式则出现在中间层。尽管不同模型中的最优模式略有不同，但通常与这些观察结果一致。'
- en: Additionally, we used the same configuration for two versions of LLaMA in our
    experiments, and the results show that the 1M model also performs very well, with
    nearly perfect results in the Needle In A Haystack task. This demonstrates the
    generalizability of the optimal sparse pattern.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在实验中对两个版本的LLaMA使用了相同的配置，结果显示1M模型也表现非常好，在针筒中的针任务中取得了近乎完美的结果。这证明了最优稀疏模式的通用性。
- en: Appendix F Sparsity in Kernel Distribution
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 核心分布中的稀疏性
- en: '![Refer to caption](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
- en: 'Figure 12: The distribution of sparsity in the kernel across different context
    windows refers to the proportion of the kernel that is actually computed after
    block coverage, compared to the sparsity rate when using FlashAttention with a
    causal mask.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：核心中不同上下文窗口的稀疏性分布指的是在块覆盖后实际计算的核心部分的比例，相对于使用带有因果掩码的FlashAttention时的稀疏率。
- en: 'As shown in Fig. [12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), the sparsity distribution of the three patterns during the actual
    kernel computation process is displayed. It can be seen that when the context
    windows exceed 200k, the actual sparsity of all three patterns surpasses 90%.
    Even considering a 20% index-building overhead, this ensures that the kernel achieves
    a speedup of over 8$\times$.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，实际核心计算过程中三种模式的稀疏分布情况被展示出来。可以看出，当上下文窗口超过200k时，所有三种模式的实际稀疏度均超过90%。即使考虑到20%的索引构建开销，这也确保了核心实现了超过8$\times$的加速。'
- en: Appendix G Does This Dynamic Sparse Attention Pattern Exist Only in Auto-Regressive
    LLMs or RoPE-Based LLMs?
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 这种动态稀疏注意力模式仅存在于自回归LLM或基于RoPE的LLM中吗？
- en: '![Refer to caption](img/6b22e182a2b67446c71bc01d753a7101.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6b22e182a2b67446c71bc01d753a7101.png)'
- en: 'Figure 13: The sparse pattern in T5-style Encoder Attention using Flan-UL2 [[74](#bib.bib74)]
    on the Summarization dataset [[86](#bib.bib86)].'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用Flan-UL2 [[74](#bib.bib74)]在Summarization数据集[[86](#bib.bib86)]上进行T5风格的编码器注意力中的稀疏模式。
- en: 'Similar vertical and slash line sparse patterns have been discovered in BERT [[72](#bib.bib72)]
    and multi-modal LLMs [[80](#bib.bib80)]. Additionally, as shown in Fig. [13](#A7.F13
    "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern Exist Only
    in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), we analyzed the distribution
    of attention patterns in T5 across different heads. It is evident that there are
    vertical and slash sparse patterns even in bidirectional attention.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BERT [[72](#bib.bib72)] 和多模态 LLMs [[80](#bib.bib80)] 中已经发现了类似的垂直和斜线稀疏模式。此外，如图
    [13](#A7.F13 "图 13 ‣ 附录 G 这种动态稀疏注意力模式是否只存在于自回归 LLMs 或基于 RoPE 的 LLMs？ ‣ MInference
    1.0：通过动态稀疏注意力加速长上下文 LLMs 的预填充") 所示，我们分析了 T5 在不同头部的注意力模式分布。显然，即使在双向注意力中，也存在垂直和斜线稀疏模式。
- en: Recent studies [[80](#bib.bib80)] have analyzed sparse attention patterns in
    multi-modal LLMs, revealing the presence of vertical and slash patterns in models
    like LLaVA [[45](#bib.bib45)] and InternVL [[11](#bib.bib11)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 [[80](#bib.bib80)] 分析了多模态 LLMs 中的稀疏注意力模式，揭示了像 LLaVA [[45](#bib.bib45)]
    和 InternVL [[11](#bib.bib11)] 这样的模型中存在垂直和斜线模式。使用 MInference 进行预填充阶段的推理加速具有很大的前景。
- en: Appendix H Case Study
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 案例研究
- en: 'Table [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") presents a comparison
    of the generation performance for various methods on the EN.SUM task (200K input
    length) from InfiniteBench based on the LLaMA-3-8B-262K model. The original summary
    provides a comprehensive and coherent narrative, detailing the Bronwyn family’s
    trip to the Kindergarten and touching on themes such as nostalgia, loss, and the
    passage of time. StreamingLLM’s summary, although looks coherent, introduces elements
    that are not present in the original story, leading to serious factual errors.
    For example, it mentions a boat trip to a school for boys and specific details
    like fishermen, sandwiches, and a spot where men were drowned. These details deviate
    from the original story, which is about the Bronwyn family preparing for a trip
    to the Kindergarten. In addition, the summaries generated by StreamingLLM with
    dilated and strided techniques are largely incoherent, consisting primarily of
    repetitive and nonsensical characters, indicating a failure to produce meaningful
    content. In stark contrast, the summary generated by our proposed method offers
    a detailed and coherent narrative, comparable to the original, with a clear depiction
    of the story’s main events and themes. This includes the preparation of the Bronwyn
    family for their trip, the characterization of family members and guests, and
    the exploration of deeper themes such as love, marriage, and the search for meaning.
    The results demonstrate the superiority of our proposed method in generating high-quality,
    human-like summaries over the baseline methods.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A8.T8 "表 8 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLMs 的预填充") 展示了基于
    LLaMA-3-8B-262K 模型的 InfiniteBench 对 EN.SUM 任务（200K 输入长度）不同方法的生成性能比较。原始总结提供了一个全面且连贯的叙述，详细讲述了
    Bronwyn 家庭的幼儿园之行，并触及了怀旧、失落和时间流逝等主题。尽管 StreamingLLM 的总结看似连贯，但引入了原故事中没有的元素，导致了严重的事实错误。例如，它提到了前往男子学校的船上旅行以及像渔民、三明治和一个男人溺水的地方等具体细节。这些细节偏离了原故事，原故事是关于
    Bronwyn 家庭为幼儿园旅行做准备。此外，StreamingLLM 生成的 dilated 和 strided 技术的总结大多不连贯，主要由重复和无意义的字符组成，表明未能生成有意义的内容。相比之下，我们提出的方法生成的总结提供了详细且连贯的叙述，能够与原文相媲美，清晰地描绘了故事的主要事件和主题。这包括
    Bronwyn 家庭为旅行做准备、家庭成员和客人的描绘，以及对爱、婚姻和寻求意义等深层主题的探索。结果显示，我们提出的方法在生成高质量、类人总结方面优于基线方法。
- en: 'Table [9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") compares the
    performance of various methods on the Retrieve.KV task (200K input length) using
    the LLaMA-3-8B-262K model. The original method demonstrates perfect retrieval,
    correctly predicting the exact strings of the ground truth for both examples.
    StreamingLLM, again, generates predictions that looks coherent and real, but factually
    incorrect. In addition, StreamingLLM with dilated and strided techniques, and
    our method with a static pattern, fail significantly, producing outputs that are
    either repetitive sequences of characters or nonsensical strings, indicating their
    inability to accurately retrieve the required key-value pairs. Our method, however,
    performs on par with the original, accurately retrieving and predicting the exact
    key-value pairs for both examples. This demonstrates the superior capability of
    our method in handling KV retrieval tasks, providing precise and reliable outputs
    consistent with the ground truth. The results highlight our method’s effectiveness
    and robustness compared to the baselines, making it a reliable choice for such
    tasks.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '表[9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") 比较了使用 LLaMA-3-8B-262K
    模型在 Retrieve.KV 任务（200K 输入长度）上各种方法的性能。原始方法展示了完美的检索，正确预测了两个示例的地面真实值的确切字符串。StreamingLLM
    再次生成了看似连贯和真实的预测，但在事实层面上不正确。此外，使用扩张和步长技术的 StreamingLLM 以及我们的静态模式方法表现差劲，产生了重复的字符序列或无意义的字符串，表明它们无法准确检索所需的键值对。然而，我们的方法与原始方法表现相当，准确地检索和预测了两个示例的确切键值对。这展示了我们的方法在处理
    KV 检索任务中的卓越能力，提供了与地面真实值一致的精确可靠的输出。结果突出显示了我们方法的有效性和稳健性，相较于基线方法，使其成为此类任务的可靠选择。'
- en: Algorithm 4 Vertical-Slash Index
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 垂直-斜线索引
- en: 'Input: vertical indexes $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$ # Sort vertical and
    slash indexes  $\bm{i}_{v}\leftarrow\mathrm{IncrementalSort}\left(\bm{i}_{v}\right)$  #
    Calculate block number (block_size $B$  # Initialize outputsblock count $\bm{c}_{\text{blk}}\in\mathbb{N}^{N}$,
    column count $\bm{c}_{\text{col}}\mathbb{N}^{N}$  # Parallelized in GPU  for $i\leftarrow
    1$ do     $j_{v}\leftarrow 1$  # Define the range by slash index     $r_{\text{start}}\leftarrow(i-1)\times
    B-\bm{i}_{s}^{j_{s}}$  # Merge points (vertical indexes) and ranges (slash indexes)     while $s_{v}\leq
    k_{s}$  and  $\bm{i}_{v}^{j_{v}}<r_{\text{end}}$ then              $\bm{c}_{\text{col}}^{i}\leftarrow\bm{c}_{\text{col}}^{i}+1$           end           $j_{v}\leftarrow
    j_{v}+1$  # Update the range           if $(i-1)\times B-\bm{i}_{s}^{j_{s}}></math>              while <math
    id=$                 $\bm{i}_{\text{blk}}^{i,\bm{c}_{\text{blk}}^{i}}\leftarrow
    s$              end while# Calculate the new range              $r_{\text{start}}\leftarrow(i-1)\times
    B-\bm{i}_{s}^{j_{s}}$           else              # Extend the range              $r_{\text{end}}\leftarrow
    r_{\text{end}}+B$     while $s<r_{\text{end}}$        $\bm{i}_{\text{blk}}^{i,c_{blk}^{i}}\leftarrow
    s$     end while  end for  $\mathrm{return}\,\,\,\bm{c}_{\text{blk}},\bm{i}_{\text{blk}},\bm{c}_{\text{col}},\bm{i}_{\text{col}}$'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：垂直索引 $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$ # 排序垂直和斜线索引  $\bm{i}_{v}\leftarrow\mathrm{IncrementalSort}\left(\bm{i}_{v}\right)$  #
    计算块数（block_size $B$  # 初始化输出块计数 $\bm{c}_{\text{blk}}\in\mathbb{N}^{N}$，列计数 $\bm{c}_{\text{col}}\mathbb{N}^{N}$  #
    在 GPU 中并行化  for $i\leftarrow 1$ do $j_{v}\leftarrow 1$  # 通过斜线索引定义范围 $r_{\text{start}}\leftarrow(i-1)\times
    B-\bm{i}_{s}^{j_{s}}$  # 合并点（垂直索引）和范围（斜线索引） while $s_{v}\leq k_{s}$  和  $\bm{i}_{v}^{j_{v}}<r_{\text{end}}$
    then $\bm{c}_{\text{col}}^{i}\leftarrow\bm{c}_{\text{col}}^{i}+1$ end $j_{v}\leftarrow
    j_{v}+1$  # 更新范围 if $(i-1)\times B-\bm{i}_{s}^{j_{s}}>$ <math id=$ while <math
    id=$ $\bm{i}_{\text{blk}}^{i,\bm{c}_{\text{blk}}^{i}}\leftarrow s$ end while #
    计算新范围 $r_{\text{start}}\leftarrow(i-1)\times B-\bm{i}_{s}^{j_{s}}$ else # 扩展范围
    $r_{\text{end}}\leftarrow r_{\text{end}}+B$ while $s<r_{\text{end}}$ $\bm{i}_{\text{blk}}^{i,c_{blk}^{i}}\leftarrow
    s$ end while end for $\mathrm{return}\,\,\,\bm{c}_{\text{blk}},\bm{i}_{\text{blk}},\bm{c}_{\text{col}},\bm{i}_{\text{col}}$'
- en: Algorithm 5 Vertical-Slash Flash Attention
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 垂直-斜线闪存注意力
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$, block index $\bm{i}_{\text{blk}}\in\mathbb{N}^{N\times
    k_{s}}$, column index $\bm{i}_{\text{col}}\in\mathbb{N}^{N\times k_{v}}$  Initialize
    $\bm{O}\leftarrow(0)^{S\times d_{h}}\in\mathbb{R}^{S\times d_{h}}$ to $N$Initialize
    $\bm{O}_{\text{chip}}\leftarrow(0)^{B\times d_{h}}\in\mathbb{R}^{B\times d_{h}}$Initialize
    $\bm{l}\leftarrow(0)^{B}\in\mathbb{R}^{B}$ to $\bm{c}_{\text{blk}}^{i}$Load $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$        $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$     while $j<\bm{c}_{\text{col}}^{j}$Load
    $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{\bm{cols}}\in\mathbb{R}^{B\times d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$        $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$     end while#
    Write outputs     $\bm{O}_{\text{chip}}\leftarrow\mathrm{diag}(\bm{l}^{i})^{-1}\bm{O}_{\text{chip}}$  end for'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$，块索引 $\bm{i}_{\text{blk}}\in\mathbb{N}^{N\times
    k_{s}}$，列索引 $\bm{i}_{\text{col}}\in\mathbb{N}^{N\times k_{v}}$ 初始化 $\bm{O}\leftarrow(0)^{S\times
    d_{h}}\in\mathbb{R}^{S\times d_{h}}$ 到 $N$ 初始化 $\bm{O}_{\text{chip}}\leftarrow(0)^{B\times
    d_{h}}\in\mathbb{R}^{B\times d_{h}}$ 初始化 $\bm{l}\leftarrow(0)^{B}\in\mathbb{R}^{B}$
    到 $\bm{c}_{\text{blk}}^{i}$ 加载 $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$
           $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$
           $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$
           $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$
       while $j<\bm{c}_{\text{col}}^{j}$ 加载 $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{\bm{cols}}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$
           $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$
           $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$
           $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$
       end while# 写出结果    $\bm{O}_{\text{chip}}\leftarrow\mathrm{diag}(\bm{l}^{i})^{-1}\bm{O}_{\text{chip}}$
       end for
- en: 'Table 8: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in summarization task [[86](#bib.bib86)].'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：使用 LLaMA-3-8B-Instruct-262K 在摘要任务中不同方法生成结果的比较 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 |'
- en: '| Original | The story follows the Bronwyn family and their guests as they
    prepare for a day trip to the Kindergarten, a mysterious destination that holds
    great significance for them. The story is told from multiple perspectives, including
    that of Pete Lucy, who is staying with the family and is struggling to find her
    place in their lives. As the day progresses, the family members prepare for their
    trip, and Pete reflects on her own past experiences with the Bronwyns. The story
    ends with the family setting off on their journey, leaving Pete alone on the lawn
    to contemplate her own feelings and the meaning of life. The Kindergarten is a
    symbol of the family’s past and their memories, and the story explores themes
    of nostalgia, loss, and the passage of time. Ultimately, the story suggests that
    even though the past cannot be changed, it can still be celebrated and remembered.
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 故事讲述了布朗温家族及其客人准备前往幼儿园的短途旅行，这个神秘的目的地对他们意义重大。故事从多个视角讲述，包括与家人一起住的皮特·露西，她在努力寻找自己在他们生活中的位置。随着一天的进展，家人们为旅行做准备，皮特回顾了自己与布朗温家族的过去经历。故事以家人们启程的场景结束，留下皮特独自在草坪上沉思自己的感受和生命的意义。幼儿园象征着家族的过去和他们的记忆，故事探讨了怀旧、失落和时间流逝的主题。最终，故事暗示尽管过去无法改变，但仍可以庆祝和铭记。
    |'
- en: '| StreamingLLM | The story follows a family on a boat trip to the Kindergarten,
    a school for boys. The father, Mr Bronwyn, is reading a book and seems to be in
    a trance. The children, Seb and Annalisa, are excited about the trip but also
    a little scared. They pass by a group of fishermen and Mr Bronwyn shares out sandwiches.
    They come across a spot where three men were drowned and Mr Bronwyn takes out
    his watch, making a calculation. They reach the Kindergarten and Mr Bronwyn jumps
    out of the boat, holding a parcel. The story ends with Pete Lucy, who has been
    watching the family from the shore, feeling relieved that the trip is over and
    that Mr Bronwyn has reached his destination. The story is told from multiple perspectives,
    including Pete Lucy’s, who is watching the family from the shore and reflecting
    on his own life and relationships. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 故事讲述了一个家庭在前往一个为男孩设立的幼儿园的船上旅行的经历。父亲布朗温先生正在阅读一本书，似乎进入了恍惚状态。孩子们，塞布和安娜丽莎，对这次旅行感到兴奋但也有些害怕。他们经过一群渔民，布朗温先生分发了三明治。他们来到一个三人溺水的地方，布朗温先生拿出手表进行计算。他们抵达幼儿园，布朗温先生跳下船，拿着一个包裹。故事以皮特·露西为结尾，他一直在岸边观察这个家庭，感到旅行结束了，布朗温先生也达到了他的目的地。故事从多个视角讲述，包括皮特·露西的视角，他在岸边观察这个家庭，反思自己的生活和关系。
    |'
- en: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
- en: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( (请确认一下，您希望对这些文本的翻译是否有特定的格式要求，比如是否需要保留表格结构或特殊标记？'
- en: '| Ours w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
- en: '| Ours | The story follows the Bronwyn family and their guests as they prepare
    for a trip to the Kindergarten, a remote island off the coast of Scotland. The
    family is made up of Mr. Bronwyn, a philosopher, and his wife, Mrs. Bronwyn, who
    is known for her beauty and her ability to surround and protect her family. The
    guests include Kevin Emma, a young man who is studying for a fellowship, and Castor
    Kiki, a botanist who is also a widower. The story explores themes of love, marriage,
    and the search for meaning in life. It also touches on the idea of the other,
    or the person who is different from oneself and cannot be understood or judged
    by conventional standards. Ultimately, the story is about the search for truth
    and the desire to connect with others on a deeper level. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 故事讲述了布朗温一家及其客人准备前往幼儿园的经历，幼儿园是苏格兰海岸附近的一个偏远岛屿。家庭成员包括布朗温先生，一位哲学家，和他的妻子布朗温女士，她以美貌和保护家庭的能力而闻名。客人包括凯文·艾玛，一位正在为奖学金学习的年轻人，以及卡斯特·基基，一位也为寡妇的植物学家。故事探讨了爱情、婚姻和对生活意义的探索等主题。它还涉及了他者的概念，即与自己不同且不能用传统标准来理解或评判的人。最终，故事讲述了对真理的追寻以及与他人建立更深层次联系的愿望。
    |'
- en: 'Table 9: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in KV Retrieval task [[86](#bib.bib86)].'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同方法生成结果的比较，使用 LLaMA-3-8B-Instruct-262K 在 KV 检索任务中 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 |'
- en: '| Original | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '| 原始 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。 |
- en: '| StreamingLLM | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant 2\. Ground Truth: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值：2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant
    2\. 实际值：bbf3fa73-6217-4e6e-923c-8349bd286c3d'
- en: 'Prediction: "d829ce18-6339-4f77-8c04-31fc7ec33619". |'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："d829ce18-6339-4f77-8c04-31fc7ec33619"。 |
- en: '| StreamingLLM w/ dilated | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: 5 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 使用扩张 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值：5 (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( 2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    |'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 |
- en: '| StreamingLLM w/ strided | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 使用步幅 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |
- en: '| Ours w/ static | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "def solverome2 def solverome2 def solverome2 def solverome2 def 2\. Ground Truth:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的静态 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："def solverome2
    def solverome2 def solverome2 def solverome2 def 2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    |'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："def solverome2 def solverome2 def solverome2 def solverome2 def |
- en: '| Ours | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。 |
