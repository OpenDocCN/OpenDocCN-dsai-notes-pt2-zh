- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Huiqiang Jiang¹¹1Equal contribution. ^◆Work during internship at Microsoft.,
    Yucheng Li^◆¹¹1Equal contribution. ^◆Work during internship at Microsoft., Chengruidong
    Zhang¹¹1Equal contribution. ^◆Work during internship at Microsoft., Qianhui Wu,
    Xufang Luo,
  prefs: []
  type: TYPE_NORMAL
- en: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Corporation, ^◆University of Surrey {hjiang,chengzhang,yuqyang}@microsoft.com,yucheng.li@surrey.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The computational challenges of Large Language Model (LLM) inference remain
    a significant barrier to their widespread deployment, especially as prompt lengths
    continue to increase. Due to the quadratic complexity of the attention computation,
    it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the
    pre-filling stage) on a single A100 GPU. Existing methods for speeding up pre-filling
    often fail to maintain acceptable accuracy or efficiency when applied to long-context
    LLMs. To address this gap, we introduce MInference (Million-tokens Inference),
    a sparse calculation method designed to accelerate pre-filling of long-sequence
    processing. Specifically, we identify three unique patterns in long-context attention
    matrices—the A-shape, Vertical-Slash, and Block-Sparse—that can be leveraged for
    efficient sparse computation on GPUs. We determine the optimal pattern for each
    attention head offline and dynamically build sparse indices based on the assigned
    pattern during inference. With the pattern and sparse indices, we perform efficient
    sparse attention calculations via our optimized GPU kernels to significantly reduce
    the latency in the pre-filling stage of long-context LLMs. Our proposed technique
    can be directly applied to existing LLMs without any modifications to the pre-training
    setup or additional fine-tuning. By evaluating on a wide range of downstream tasks,
    including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including
    LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that
    MInference effectively reduces inference latency by up to $10\times$ for pre-filling
    on an A100, while maintaining accuracy. Our code is available at [https://aka.ms/MInference](https://aka.ms/MInference).
  prefs: []
  type: TYPE_NORMAL
- en: \doparttoc\faketableofcontents![Refer to caption](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
  prefs: []
  type: TYPE_NORMAL
- en: (a) Needle In A Haystack
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Latency Speedup
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Attention weights, especially in long-context LLMs, exhibit up to
    96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic
    sparse attention to accelerate the pre-filling stage of long-context LLM inference.
    It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b),
    and matches or surpasses baselines, as demonstrated by Needle In A Haystack [[35](#bib.bib35)]
    in (a) on LLaMA-3-8B-1M [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have entered the era of long-context processing,
    with some of them supporting context windows ranging from 128K to 10M tokens [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)].
    These extended context windows enable LLMs to unlock a multitude of complex real-world
    applications, such as repository-level code understanding [[7](#bib.bib7), [34](#bib.bib34),
    [58](#bib.bib58)], long-document question-answering [[9](#bib.bib9), [51](#bib.bib51)],
    extreme-label in-context learning [[51](#bib.bib51)], and long-horizon agent tasks [[79](#bib.bib79)].
  prefs: []
  type: TYPE_NORMAL
- en: 'However, due to the quadratic complexity of attention, it can take several
    minutes for the model to process the input prompt (i.e., the pre-filling stage)
    and then start to produce the first token, which leads to unacceptable Time To
    First Token experience, thus greatly hinders the wide application of long-context
    LLMs. As shown in Fig. [2a](#S2.F2.sf1 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), when serving LLaMA-3-8B on a single A100
    machine, the model would keep users waiting for 6 minutes to finish the pre-filling
    stage given a prompt of 300K tokens, and this number increases to 30 minutes for
    a prompt of 1M tokens. The overhead of self-attention computation exceeds 90%
    of the total pre-filling latency, which makes it the major bottleneck in long-context
    processing of LLMs. Previous research has shown that the attention matrices are
    highly sparse [[47](#bib.bib47), [17](#bib.bib17)], which has led to the development
    of fixed sparse attention methods such as Longformer [[6](#bib.bib6)] and BigBird [[87](#bib.bib87)].
    However, prior studies have also noted that attention distributions vary significantly
    across different inputs [[39](#bib.bib39), [47](#bib.bib47)]. This dynamic nature
    prevents prior sparse methods from being used directly on long-context LLMs without
    expensive training or fine-tuning. But if the dynamic sparse attention patterns
    could be efficiently predicted online, the pre-filling latency of long-context
    LLMs could be significantly reduced by calculating only the most important part
    of the attention weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Building upon this idea, we present MInference, a technique that reduces 95%
    of FLOPs in the attention computation to significantly accelerate the pre-filling
    stage of long-context LLM inference via dynamic sparse attention. Unlike existing
    dynamic sparse attention methods that introduce large computational overhead to
    estimate attention patterns with low-rank hidden dimensions [[47](#bib.bib47),
    [63](#bib.bib63)], our method is designed specifically for long-context scenarios
    with minimal overhead in estimation. Specifically, we conduct extensive analysis
    and identify three general patterns of sparse attention in long-context LLMs:
    A-shape pattern, Vertical-Slash pattern, and Block-Sparse pattern. Based on these
    findings, we introduce a kernel-aware search method to assign the optimal attention
    pattern for each head. Importantly, instead of fixed attention masks in prior
    studies, we perform an efficient online approximation to build a dynamic sparse
    mask for each head according to their assigned pattern and particular inputs.
    For example, to build a dynamic sparse mask for a specific prompt on one Vertical-Slash
    head, we use a partial of attention weight consisting of the last last_q query
    and key vectors (i.e. $\bm{Q}_{[-\text{last\_q}:]}$) to estimate the most important
    indices of the vertical and slash lines globally on the attention matrix. For
    Block-Sparse heads, we perform mean pooling on both query and key vectors in blocks
    of 64 and calculate the block-level attention weights to determine the most important
    blocks and thereby obtain a block-sparse dynamic mask. After obtaining the dynamic
    sparse mask, three optimized GPU kernels are used, which we developed for the
    above three sparse patterns. These kernels are based on the dynamic sparse compilers
    PIT [[88](#bib.bib88)], Triton [[75](#bib.bib75)] and FlashAttention [[13](#bib.bib13)],
    which enable extremely efficient computation of dynamic sparse attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Extensive experiments are conducted on various Long-context LLMs, including
    LLaMA-3-8B-1M [[24](#bib.bib24)], GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)],
    across benchmarks with context lengths over 1M tokens, such as InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], Needle In A Haystack [[35](#bib.bib35)], and PG-19 [[65](#bib.bib65)].
    Needle In A Haystack was also tested on Phi-3-Mini-128K [[2](#bib.bib2)] and Qwen-2-7B-128K [[5](#bib.bib5)].
    Results show that MInference speeds up the pre-filling stage by up to $10\times$
    for 1M contexts with LLaMA-3-8B on a single A100, reducing latency from 30 minutes
    to 3 minutes per prompt, while maintaining or improving accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '2 Attention Heads: Dynamic, Sparse, and Characteristic'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10ccd929778db2d7ea98214ca2a9092c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention incurs heavy cost.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84a510695ce9529c4982da1844ae2d16.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention is sparse.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/804139198ed227c4ddf295cd89c1f3cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Sparsity of attention is dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention
    scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention
    scores are retrieved when reusing the top-k indices from another examples, indicating
    its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Attention is Dynamically Sparse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The sparsity of attention weights in pre-trained LLMs, especially in long-context
    scenarios, has been well-documented [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]. As shown in Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ 2 Attention
    Heads: Dynamic, Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), for an attention matrix
    of size $128k\times 128k$, retaining only the top 4k columns recalls 96.8% of
    the total attention. In other words, each token is attending to a limit number
    of tokens despite the long sequence it is processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, although the sparse nature of attention matrices is shared
    across different inputs, the exact distributions of sparse pattern are highly
    dynamic. That is to say, a token at a given position only attends to a subset
    of the sequence in self-attention, and the exact tokens it attends to are highly
    context-dependent and vary significantly across different prompts. This dynamism
    has been mathematically demonstrated in prior studies [[39](#bib.bib39), [40](#bib.bib40)].
    As depicted in Fig. [2c](#S2.F2.sf3 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), if we take the top 4k columns found in Fig. [2b](#S2.F2.sf2
    "In Figure 2 ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and apply it on another prompt of 128k, the recall of attention would drop largely
    to 83.7%.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Attention Sparsity Exhibits Patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of different sparse patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Patterns | A-shape | Vertical-Slash | Block-Sparse | Top-K |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial Distribution | Static structured | Dynamic structured | Dynamic structured
    | Dynamic fine-grained |'
  prefs: []
  type: TYPE_TB
- en: '| Latency on GPU | Low | Medium | Low | High |'
  prefs: []
  type: TYPE_TB
- en: '| Time to build the index | Zero | Small | Small | High | ![Refer to caption](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Attention patterns
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention is spatial clustering
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/155b7edb1253cb7cf43bccc0db737600.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attention pattern recall
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: (a) Visualization of attention weights from different attention heads.
    For different prompts and tasks, the pattern of the same head is relatively consistent,
    but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest
    non-zero element in the attention matrix. (c) Attention recall distribution using
    our identified patterns, where FLOPs in the kernel refer to the real FLOPs required
    for sparse attention computing using on GPUs. Here, a 1x64 block size is used
    for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs.
    All visualization are based on LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the sparsity distribution of attention matrix is dynamic, previous
    works [[82](#bib.bib82), [29](#bib.bib29)] have shown that they exhibit certain
    patterns in the two-dimensional space such as spatial clustering. Through our
    analysis of long-context prompts of various lengths and tasks, we have categorized
    such attention sparse patterns into the A-shape, Vertical-Slash (VS), and Block-Sparse
    patterns, as shown in Fig. [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Table [1](#S2.T1
    "Table 1 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") details the characteristics and differences
    between these three patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: A-shape pattern The attention weights of these types of heads are concentrated
    on initial tokens and local windows [[82](#bib.bib82), [29](#bib.bib29)], exhibiting
    relatively higher stability.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical-Slash (VS) pattern The attention weights are concentrated on specific
    tokens (vertical lines) and tokens at fixed intervals (slash lines). The positions
    of vertical and slash lines in this pattern dynamically change with the context
    content and exhibit a certain sparsity, making them difficult to be encompassed
    by local windows and A-shape patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Block-Sparse pattern This sparsity pattern is the most dynamic, exhibiting
    a more dispersed distribution. Despite its dynamism, the attention weights maintain
    some characteristics of spatial clustering, which we identify as the block-sparse
    pattern. We analyzed the distances between non-zero attention weights and their
    top-k nearest non-zero neighbors within a 128k prompt as shown in Fig. [3b](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"). The results indicate that across layers and
    heads, the distances between nearest non-zero values are generally concentrated
    around 5, suggesting a strong spatial clustering of the attention weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The point of these three patterns is that we can leverage them to perform highly
    efficient sparse computing for the attention matrix in long-context LLMs. In Fig. [3c](#S2.F3.sf3
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we test how efficient is our indentified
    patterns retrieving attention scores with limit computing cost on GPU (FLOPs).
    First, attention heads are labeled with one of the sparse pattern (detail see
    §[3.2](#S3.SS2 "3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
    ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")). Then we demonstrate our patterns are significantly
    more efficient compared to other sparse methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)]. Specifically, with the same amount of FLOPs, our patterns achieve
    a notable higher recall on attention scores, which can potentially lead to better
    accuracy. For example, previous Top-K methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)] struggle with the Block-Sparse pattern as they focus on specific
    tokens globally, while our pattern retrieves attention scores more efficiently
    and accurately. We example how we use these patterns on long-context LLMs and
    how we implement optimized GPU kernels for these patterns in §[3](#S3 "3 MInference
    1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 MInference 1.0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following the analysis in §[2](#S2 "2 Attention Heads: Dynamic, Sparse, and
    Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs
    via Dynamic Sparse Attention"), we propose MInference to accelerate the pre-filling
    stage of long-context LLMs, consisting of three steps: 1) Offline attention pattern
    identification for each head; 2) Dynamic build of sparse indices w.r.t. the pattern;
    3) Sparse attention calculation with optimized GPU kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/717aebceb4eff85c06852353dc924b36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The three sparse methods in MInference.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When accelerating the pre-filling stage of long-context LLMs with sparse attention
    computing, the attention matrix can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $M_{i,j}\in\{0,1\}$ of the attention matrix. Here, $c$ have values approaching
    zero after the softmax, i.e., $A_{i,j}\approx 0$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the dynamic sparse attention system is to achieve greater speedup
    with minimal overhead while retaining as much of the attention weights as possible.
    Formally, this can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $t_{\text{sparse}}$ represent the time spent on dynamic sparse attention
    computation and estimation of the approximate dynamic sparse pattern, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Kernel-Aware Sparse Pattern Search
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$, search space $\rho$,
    initialized search space $\sigma$ to $|\sigma|$     while $|t_{i}-t|></math>        <math
    id=$  end for# Search for optimal head pattern  $p_{\text{best}}\leftarrow\phi$  for $i\leftarrow
    1$ do     $\bm{y}_{i}\leftarrow\text{SparseAttention}(\bm{Q}\bm{K}^{\top}/\sqrt{d},\rho_{i})$  end for  $\mathrm{return}\,\,\,p_{\text{best}}$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernel-Aware Optimal Sparse Pattern Search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To achieve the best accuracy with limited FLOPs budget, we propose an offline
    Kernel-Aware Optimal Sparse Pattern Search method. In this step, we determine
    which sparse pattern will be used for each attention head, and the optimal setting
    for the pattern in real calculation (e.g., the number of vertical/slash lines
    in VS pattern; or the number of top-k blocks in BS patterns). As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.1 Problem Formulation ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"), we first create
    the search space based on a target FLOPs for each pattern, ensuring all potential
    candidates (i.e., different patterns with different settings) have similar computational
    cost. Kernel-aware here indicates the computational cost reflects the real FLOPs
    in GPU kernels, instead of conceptual estimations, which is crucial to achieve
    the optimal acceleration.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Vertical-Slash Head
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ # Approximate vertical
    and slash pattern (last_q = 64)  $\bm{\widehat{A}}\leftarrow\mathrm{softmax}\left(\bm{Q}_{[-\text{last\_q}:]}\bm{K}^{\top}/\sqrt{d}+\bm{m}_{\text{casual}}\right)$
    vertical line, sum in vertical  $\bm{i}_{v}\leftarrow\mathrm{argtopk}\left(\mathrm{sum}_{v}(\bm{\widehat{A}}),k_{v}\right)$
    slash line, sum in slash  $\bm{i}_{s}\leftarrow\mathrm{argtopk}\left(\mathrm{sum}_{s}(\bm{\widehat{A}}),k_{s}\right)$  #
    Final dynamic sparse attention scores (only index block)  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{vs})/\sqrt{d}\right)$  $\mathrm{return}\,\,\,\bm{y}$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Block-Sparse Head
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ # Approximate block-sparse
    pattern (block_size = 64)  $\bm{\widehat{Q}}\leftarrow\mathrm{MeanPooling}(\bm{Q},block\_size)$  $\bm{\widehat{A}}\leftarrow\mathrm{softmax}\left(\bm{\widehat{Q}}\bm{\widehat{K}}^{\top}/\sqrt{d}+\bm{m}_{\text{casual}}\right)$
    blocks  $\bm{i}_{b}\leftarrow\mathrm{argtopk}\left(\bm{\widehat{A}},k_{b}\right)$  #
    Final dynamic sparse attention scores (only index block)  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{b})/\sqrt{d}\right)$  $\mathrm{return}\,\,\,\bm{y}$'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we go through the search space with a reference example to decide the
    optimal pattern and setting. Specifically, we use recall of the attention output
    as the objective criterion when searching for the best pattern. This approach
    leverages FlashAttention [[13](#bib.bib13)] to reduce GPU memory overhead and
    incorporates the information from the $\bm{V}$ matrix, enabling end-to-end selection
    of the best pattern, which further enhances performance.
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity Indices Approximation and Dynamic Sparse Attention Calculation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'During the inference stage, we will perform an online estimation on the attention
    matrix to dynamically determine the spatial distribution our sparse indices, based
    on the assigned patterns and the exact input. After that, we conduct the sparse
    attention computations with our optimized GPU kernels. The implementation details
    of our kernels can be found in Appendix [C.4](#A3.SS4 "C.4 Kernel Implementation
    ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"). Noted that the sparse mask is
    static for A-shape heads, so there is no overhead in building the dynamic masks,
    and only sparse calculation is required.'
  prefs: []
  type: TYPE_NORMAL
- en: '(i) Vertical-Slash head. As shown in Algorithm [2](#alg2 "Algorithm 2 ‣ Kernel-Aware
    Optimal Sparse Pattern Search ‣ 3.2 Speedup of Long-context LLM Inference via
    Dynamic Sparse Attention ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), due to the continuity of
    vertical and slash lines, we matmul the last query vector $\bm{Q}_{[-\text{last\_q}:]}$
    to produce the estimated attention matrix $\bm{\widehat{A}}$ and slash $\bm{i}_{s}$.
    Using these sparse indices, we perform block-sparse calculations of the attention
    weights and attention output.'
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) Block-Sparse head. Per Algorithm [3](#alg3 "Algorithm 3 ‣ Kernel-Aware
    Optimal Sparse Pattern Search ‣ 3.2 Speedup of Long-context LLM Inference via
    Dynamic Sparse Attention ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), mean pooling is applied
    on $\bm{Q}$ to obtain $\bm{\widehat{Q}}$, respectively. The two matrices are multiplied
    to get the estimated block-level attention weights $\bm{\widehat{A}}$ and use
    it to compute the sparse attention weights and attention output.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we investigate two questions: (i) How effective is MInference?
    We evaluate our method on three general long-context benchmarks: InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], and the Needle In A Haystack task [[35](#bib.bib35)],
    as well as the long-context language modeling task [[65](#bib.bib65)]. These benchmarks
    cover long-context QA, multi-hop QA, math reasoning, aggregation tasks, summarization,
    retrieval tasks, and code debugging, allowing us to assess MInference’s effectiveness
    across a wide range of long-context scenarios. (ii) How efficient is MInference?
    We delve into the end-to-end latency and its breakdown to evaluate the efficiency
    of MInference. Additional experimental, latency results, and analysis can be found
    in Appendix [D](#A4 "Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    [E](#A5 "Appendix E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), and [F](#A6 "Appendix F
    Sparsity in Kernel Distribution ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention").'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Our experiments use four state-of-the-art long-context LLMs: LLaMA-3-8B-Instruct-262k¹¹1https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k,
    LLaMA-3-8B-Instruct-1048k²²2https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k,
    GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)]. Additionally,
    we tested Needle in A Haystack [[35](#bib.bib35)] on Phi-3-Mini-128K [[2](#bib.bib2)]
    and Qwen2-7B-128K [[5](#bib.bib5)], as detailed in Appendix [D.1](#A4.SS1 "D.1
    Needle In A Haystack ‣ Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    To guarantee stable results, we use greedy decoding in all experiments. We provide
    a simple custom implementation of our method in PyTorch, built on FlashAttention [[13](#bib.bib13)],
    Triton [[75](#bib.bib75)], and the dynamic sparse compiler PIT [[88](#bib.bib88)].
    We set the target FLOPs $t$ and $block\_size=64$ in the Vertical-Slash and Block-Sparse
    patterns, respectively. The latency experiments are conducted on a single Nvidia
    A100 GPU in the bfloat16 format. More details are provided in Appendix [C.2](#A3.SS2
    "C.2 Additional Implementation Details ‣ Appendix C Experiment Details ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset & Evaluation Metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We use the provided metrics and scripts from the following benchmarks for evaluation.
    More details about dataset can be found in Appendix [C.1](#A3.SS1 "C.1 Dataset
    Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention").'
  prefs: []
  type: TYPE_NORMAL
- en: '(i) InfiniteBench [[86](#bib.bib86)]: This benchmark consists of 10 tasks,
    including retrieval tasks such as PassKey retrieval, Number retrieval, and KV
    retrieval, as well as representative realistic tasks like question-answering,
    coding, dialogue, and summarization. The average context length of InfiniteBench
    is about 214K tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) RULER [[28](#bib.bib28)]: A challenging long-context benchmark consisting
    of 4 categories and 13 complex tasks, including retrieval, multi-hop tracing and
    aggregation, and QA tasks. It contains subsets with different prompt lengths up
    to 128k tokens, allowing us to determine the actual context window size of the
    model based on the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '(iii) Needle In A Haystack [[35](#bib.bib35)]: A long-context retrieval benchmark
    testing LLMs’ performance with context window sizes up to 1M tokens where information
    placed at various positions.'
  prefs: []
  type: TYPE_NORMAL
- en: '(iv) PG-19 [[65](#bib.bib65)]: Following StreamingLLM [[82](#bib.bib82)] and
    H2O [[89](#bib.bib89)], we use PG-19 for long-context language modeling tasks
    with prompts up to 100k tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Performance of different methods with different base models on InfiniteBench [[86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3-8B-262K | 20.2 | 12.4 | 67.3 | 6.0 | 12.9 | 22.1 | 26.6 | 100.0 |
    100.0 | 14.4 | 38.2 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | 21.0 | 8.2 | 40.2 | 10.0 | 10.4 | 25.9 | 30.0 | 86.8 | 5.1
    | 0.8 | 23.8 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ dilated | 20.1 | 9.4 | 44.5 | 15.5 | 11.2 | 20.5 | 27.5 |
    5.0 | 87.5 | 0.5 | 24.2 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ strided | 17.3 | 8.2 | 27.5 | 14.5 | 11.2 | 19.5 | 27.5 |
    4.0 | 2.1 | 1.0 | 13.3 |'
  prefs: []
  type: TYPE_TB
- en: '| InfLLM | 24.1 | 7.8 | 45.0 | 6.0 | 11.4 | 19.5 | 32.9 | 100.0 | 100.0 | 1.2
    | 34.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ static | 19.9 | 8.6 | 43.2 | 3.5 | 8.9 | 20.6 | 25.1 | 92.4 | 96.3
    | 0.2 | 31.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-9B-200K | 8.2 | 10.6 | 64.2 | 1.0 | 17.3 | 21.3 | 23.4 | 99.8 | 100.0
    | 28.8 | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | 5.4 | 14.2 | 38.0 | 4.0 | 18.8 | 18.8 | 22.3 | 39.2 | 6.1
    | 1.6 | 16.8 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ dilated | 5.7 | 4.2 | 15.0 | 0.0 | 18.2 | 0.0 | 2.9 | 0.0
    | 0.0 | 0.0 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ strided | 6.1 | 4.5 | 9.8 | 0.0 | 16.9 | 0.0 | 3.1 | 1.5
    | 0.0 | 0.0 | 4.6 |'
  prefs: []
  type: TYPE_TB
- en: '| InfLLM | 6.3 | 13.0 | 45.9 | 2.5 | 21.5 | 20.6 | 34.6 | 85.3 | 88.1 | 1.4
    | 31.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ static | 5.8 | 12.6 | 48.5 | 3.0 | 12.6 | 20.8 | 25.1 | 60.9 | 38.5
    | 1.0 | 22.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 7.9 | 11.2 | 64.2 | 1.0 | 17.9 | 24.1 | 23.1 | 99.5 | 100.0 | 27.6
    | 37.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GLM-4-9B-1M | 28.3 | 9.7 | 68.6 | 39.5 | 12.1 | 29.4 | 38.9 | 100.0 | 100.0
    | 41.0 | 46.7 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | 27.7 | 6.4 | 40.2 | 12.5 | 10.8 | 27.7 | 21.1 | 97.1 | 25.6
    | 0.6 | 27.0 |'
  prefs: []
  type: TYPE_TB
- en: '| InfLLM | 28.0 | 7.3 | 45.0 | 14.0 | 10.7 | 27.9 | 39.4 | 98.0 | 100.0 | 2.6
    | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 28.8 | 9.6 | 68.6 | 38.5 | 12.0 | 30.7 | 39.1 | 100.0 | 100.0 | 43.0
    | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: Baselines
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We include five training-free sparse attention approaches as our baselines:
    1) StreamingLLM [[82](#bib.bib82)], which corresponds to the A-shape pattern.
    We use 1k global tokens and 4k local windows in all our experiments; 2) StreamingLLM
    w/ dilated [[6](#bib.bib6)], which sets dilated local windows with intervals in
    the local windows direction. We use 1k global tokens and 8k dilated attention
    windows with an interval of 1; 3) StreamingLLM w/ strided [[8](#bib.bib8)], which
    retains local windows while adding dilated attention. We use 1k global tokens,
    2k local windows, and 4k dilated attention windows with an interval of 1; 4) InfLLM [[83](#bib.bib83)],
    which uses a memory unit to process streaming long sequences. Following the paper,
    we set 128 global tokens and 8k local windows in all experiments; 5) Ours w/ static,
    which utilizes static sparse indices in the Vertical-Slash and Block-Sparse heads.
    For all baselines, we perform sparse computation only during the pre-filling stage,
    while retaining dense computation during the decoding stage.'
  prefs: []
  type: TYPE_NORMAL
- en: InfiniteBench
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Table 3: Performance (%) of different models and different methods on RULER [[28](#bib.bib28)]
    evaluated at lengths from 4k to 128k.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Claimed | Effective | 4K | 8K | 16K | 32K | 64K | 128K | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3-8B-262K | 262K | 16K | 97.2 | 91.8 | 87.3 | 80.8 | 77.4 | 72.2 |
    84.4 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | - | 4K | 97.2 | 38.1 | 37.5 | 17.2 | 14.2 | 9.4 | 35.0 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ dilated | - | <4K | 23.4 | 0.7 | 1.4 | 18.8 | 16.5 | 15.6
    | 12.7 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ strided | - | <4K | 2.0 | 0.7 | 0.6 | 0.6 | 0.7 | 1.3 | 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| InfLLM | - | 4K | 89.4 | 79.8 | 70.1 | 55.6 | 43.0 | 39.5 | 62.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | - | 32K | 97.7 | 91.2 | 88.5 | 85.0 | 82.3 | 77.6 | 87.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-9B-200K | 200K | 8K | 91.9 | 90.2 | 78.8 | 76.3 | 68.1 | 62.9 | 78.1 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | - | 4K | 91.9 | 37.8 | 33.9 | 18.6 | 13.0 | 12.8 | 34.3 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ dilated | - | <4K | 44.8 | 42.8 | 38.5 | 29.8 | 26.8 | 23.9
    | 34.4 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ strided | - | <4K | 2.6 | 0.7 | 0.6 | 0.6 | 1.2 | 0.5 | 1.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| InfLLM | - | <4K | 80.3 | 83.9 | 60.7 | 45.2 | 38.6 | 30.2 | 56.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | - | 8K | 92.3 | 89.7 | 79.0 | 73.8 | 64.7 | 56.9 | 74.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GLM-4-9B-1M | 1M | 64K | 93.8 | 91.6 | 89.3 | 87.4 | 85.2 | 80.8 | 88.0 |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | - | 4K | 93.8 | 66.9 | 58.5 | 51.4 | 45.9 | 39.1 | 59.3 |'
  prefs: []
  type: TYPE_TB
- en: '| InfLLM | - | 8K | 94.7 | 89.5 | 76.4 | 66.5 | 56.8 | 53.5 | 72.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | - | 64K | 94.6 | 93.1 | 91.0 | 89.6 | 85.5 | 84.0 | 89.6 |'
  prefs: []
  type: TYPE_TB
- en: 'As shown in Table [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), MInference achieves the best overall performance compared to baselines
    on InfiniteBench. Remarkably, MInference matches or even slightly surpasses the
    performance of the original full attention baseline on some tasks, despite the
    significant acceleration it provided. From the perspective of different tasks,
    our method not only performs well in natural language tasks such as summarization,
    QA, and code, but also maintains the original model’s performance on retrieval-related
    tasks. Baseline methods such as StreamingLLM, on the contrary, struggle with these
    retrieval tasks. Additionally, on tasks such as dialogue QA, using local attention
    mechanisms can better handle these tasks, while our performance is closer to the
    original results, indicating that our method is not solely based on local windows.
    Extending the local windows’ intervals in StreamingLLM, i.e., w/ dilated and w/
    strided, hardly affects the model’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: RULER
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To further reveal the true potential of our method in long-context LLMs, we
    evaluate MInference with the state-of-the-art long-context challenge, RULER. As
    shown in Table [3](#S4.T3 "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    MInference effectively maintains the long-context performance even in complex
    multi-hop or aggregation tasks in RULER. It even outperforms the original full
    attention for testing lengths beyond 32K, achieving effective context windows
    of 32K and 64K (context with performance over 85% is considered effective [[28](#bib.bib28)])
    in LLaMA-3-8B-262K and GLM-4-9B-1M.'
  prefs: []
  type: TYPE_NORMAL
- en: Language Modeling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Following the approach of StreamingLLM [[82](#bib.bib82)] and H2O [[89](#bib.bib89)],
    we evaluate our methods against baselines on the language modeling task based
    on the PG-19 dataset [[65](#bib.bib65)]. As shown in [5](#S4.F5 "Figure 5 ‣ Language
    Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), our method yields best results compared to
    other sparse approaches, and exhibits minimal divergence compared to the full
    attention baseline. For prompts of 100K token, our perplexity is only 0.2 higher
    than the full attention, but lower than StreamingLLM for 0.25 and 0.75 on the
    Yi-9B-200K and LLaMA-3-262K models respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/760645197ef39823c822f7e94434c307.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) LLaMA-3-8B-Instruct-262K
  prefs: []
  type: TYPE_NORMAL
- en: (b) Yi-9B-200K
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Perplexity results on PG-19 [[65](#bib.bib65)] using different models
    and methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d8c4f5b0df6fa7dcf125d1ff1ed2fcb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Results on Needle In A Haystack of StreamingLLM [[82](#bib.bib82)]
    in LLaMA-3-8B-1M.'
  prefs: []
  type: TYPE_NORMAL
- en: Needle In A Haystack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Comparing Fig. [1a](#S0.F1.sf1 "In Figure 1 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") to [6](#S4.F6
    "Figure 6 ‣ Language Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), our method effectively retains
    the ability to process information at different positions across various context
    windows, ranging from 1k to 1M tokens. In contrast, methods like StreamingLLM,
    while effective in reducing latency, experience a rapid decline in performance
    once the critical information exceeds the range of global tokens and local windows.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To evaluate the contributions of different components in MInference, we introduce
    four variants for the ablation study: (1) Ours w/ static, which uses a static
    sparse mask in the Vertical-Slash and Block-Sparse patterns; (2) Ours w/ only
    A-shape, which is equivalent to StreamingLLM; (3) Ours w/ only block-sparse, which
    uses only the Block-Sparse pattern in the dynamic sparse calculation. (4) Ours
    w/ only vertical-slash, which uses only the Vertical-Slash pattern in the dynamic
    sparse calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K
    on InfiniteBench [[86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ only block-sparse | 12.4 | 3.4 | 5.7 | 6.0 | 3.1 | 12.2 | 24.0 |
    59.5 | 60.3 | 0.0 | 18.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ only vertical-slash | 19.6 | 12.0 | 62.1 | 9.5 | 11.7 | 21.6 | 29.1
    | 100.0 | 100.0 | 5.0 | 37.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Tables [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments ‣
    MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), [3](#S4.T3 "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    and [4](#S4.T4 "Table 4 ‣ Ablation Study ‣ 4 Experiments ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") present the ablation
    results. It first proves that using static indices significantly degrades LLM
    performance, especially in highly dynamic tasks like KV retrieval, where accuracy
    nearly drops to zero. This highlight the necessity of our dynamic strategy and
    the effectiveness of our dynamically built sparse indices. Additionally, remove
    any pattern from the three leads to varying degrees of performance degradation.
    Specifically, "only A-shape" can only capture information within local windows.
    The "only block-sparse" variant using only the BS pattern, also results in significant
    performance declines. On the other hand, "only vertical-slash" manages to preserve
    most of the performance due to its balance between dynamicity and the StreamingLLM
    pattern, but still fall behind the full version of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Fig. [1b](#S0.F1.sf2 "In Figure 1 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention") and [10](#A4.F10 "Figure
    10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional Experiment Results ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    shows the latency and breakdown of MInference across different context windows
    on a single A100\. At 100K, 300K, 500K, and 1M tokens, our method achieves speedups
    of 1.8$\times$, 6.8$\times$, respectively. It reduces the pre-filling latency
    from 30 mins to 3 mins on a single A100 for a prompt of 1M token. By further utilizing
    tensor parallel [[46](#bib.bib46)] and context parallel [[50](#bib.bib50), [31](#bib.bib31)],
    this latency can be reduced to 40 seconds on 8x A100 GPUs. This significantly
    lowers the deployment cost of long-context LLMs and enhances user experience.
    And since our kernel is implemented based on Triton, it can be easily ported to
    other devices and achieve similar speedups, such as on the H100. Additionally,
    analyzing the latency breakdown, we found about 5%-20% of the overhead is spent
    on dynamic sparse index building, while the remaining time is spent on dynamic
    sparse calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate with KV cache compression methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Table 5: Performance of different methods on InfiniteBench [[86](#bib.bib86)]
    using SnapKV [[43](#bib.bib43)] in the decoding stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3 w/ SnapKV | 18.0 | 11.8 | 65.5 | 2.5 | 12.0 | 21.3 | 26.6 | 100.0
    | 100.0 | 1.8 | 36.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ SnapKV | 18.9 | 11.7 | 66.4 | 6.5 | 12.1 | 21.8 | 33.1 | 100.0 |
    100.0 | 2.0 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: 'We also combined MInference with a state-of-the-art KV cache compression method
    SnapKV [[43](#bib.bib43)], as shown in Table [5](#S4.T5 "Table 5 ‣ Integrate with
    KV cache compression methods ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"). This proves our method is
    compatible with KV cache compression techniques. For most tasks, performance remains
    nearly unchanged, with the average score even showing a slight increase, which
    further demonstrates the potential practical value of our method as an optimization
    for serving long-context LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sparse Attention
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Due to the quadratic complexity of the attention mechanism, many previous works
    have focused on sparse attention to improve the efficiency of Transformers. These
    methods include static sparse patterns, cluster-based sparse approaches, and dynamic
    sparse attention. Static sparse patterns include techniques such as sliding windows [[30](#bib.bib30),
    [2](#bib.bib2)], dilated attention [[8](#bib.bib8), [72](#bib.bib72), [16](#bib.bib16)],
    and mixed sparse patterns [[6](#bib.bib6), [87](#bib.bib87), [38](#bib.bib38)].
    Cluster-based sparse methods include hash-based [[36](#bib.bib36)] and kNN-based [[68](#bib.bib68),
    [54](#bib.bib54)] methods. All of the above methods require pre-training the model
    from scratch, which makes them infeasible to be directly used as a plugin for
    reay-to-use LLMs. Recently, there has been work [[14](#bib.bib14), [85](#bib.bib85)]
    to unify state space models [[23](#bib.bib23), [22](#bib.bib22), [14](#bib.bib14)],
    and linear attention [[37](#bib.bib37), [70](#bib.bib70)] into structured masked
    attention. Additionally, some works [[81](#bib.bib81), [47](#bib.bib47), [63](#bib.bib63)]
    leverage the dynamic nature of attention to predict sparse patterns dynamically.
    However, these approaches often focus on low-rank hidden states during the dynamic
    pattern approximation or use post-statistical methods to obtain the sparse mask,
    which introduce substantial overhead in the estimation step, making them less
    useful for long-context LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Context Windows of LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Recent research has focused on expanding the context window of pre-trained
    LLMs, that enables LLMs to handle more complex real-life applications [[34](#bib.bib34),
    [58](#bib.bib58)]. These methods can be categorized into: 1) Staged pre-training [[55](#bib.bib55),
    [20](#bib.bib20)]; 2) Modifying or interpolating position embeddings [[61](#bib.bib61),
    [10](#bib.bib10), [60](#bib.bib60), [19](#bib.bib19)]; 3) Utilizing external memory
    modules for context storage [[4](#bib.bib4), [77](#bib.bib77), [83](#bib.bib83)];
    4) Expanding computations across multiple devices in a distributed manner [[50](#bib.bib50)].
    However, these methods do not alleviate the high inference costs in long-context
    processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Long-Context LLM Inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Recent studies [[21](#bib.bib21)] have tackled the high computational cost
    of attention and substantial KV cache storage in long-context scenarios from two
    angles: pre-filling and decoding. Pre-filling optimizations are primarily categorized
    as State Space Models [[23](#bib.bib23), [22](#bib.bib22)], linear attention methods [[70](#bib.bib70),
    [57](#bib.bib57)], memory-based methods [[53](#bib.bib53)], hybrid methods [[44](#bib.bib44),
    [27](#bib.bib27), [64](#bib.bib64)], and prompt compression methods [[41](#bib.bib41),
    [32](#bib.bib32), [33](#bib.bib33), [62](#bib.bib62)]. However, these approaches
    require training from scratch or additional overhead and are difficult to implement
    directly in pretrained long-context LLMs. Recently, some studies [[52](#bib.bib52),
    [83](#bib.bib83)] have focused on using kNN or cluster-based sparse attention
    to accelerate LLM inference. However, these methods often lead to reduced accuracy,
    limited speedup, or are restricted to CPU scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, optimizations for the decoding stage are divided into: 1) Reusing
    attention KV to reduce KV cache storage [[73](#bib.bib73), [3](#bib.bib3), [71](#bib.bib71),
    [12](#bib.bib12)]; 2) Static KV cache compression patterns [[82](#bib.bib82),
    [29](#bib.bib29)]; 3) Dynamic KV cache compression patterns, including completely
    discarding the KV cache after compression [[89](#bib.bib89), [42](#bib.bib42),
    [25](#bib.bib25), [56](#bib.bib56)], and offloading-based methods [[63](#bib.bib63),
    [43](#bib.bib43), [15](#bib.bib15)]; 4) Cluster-based KV cache compression methods [[54](#bib.bib54),
    [78](#bib.bib78)]; 5) Methods for restoring performance loss due to KV cache compression [[1](#bib.bib1),
    [18](#bib.bib18)]; 6) Hierarchical speculative decoding methods [[69](#bib.bib69)].
    Nevertheless, these methods do not address the heavy computational burden of the
    attention in the pre-filling stage.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper addresses the expensive computational cost and the unacceptable
    latency of the attention calculations in the pre-filling stage of long-context
    LLMs. We propose MInference, a method that accelerates the pre-filling stage by
    leveraging dynamic sparse attention with spatial aggregation patterns. Specifically,
    we categorize attention heads into three types: A-shape, Vertical-Slash, and Block-Sparse.
    Using a kernel-aware optimal sparse pattern search method, we identify the optimal
    pattern for each head. Subsequently, we utilize a fast approximation approach
    to build dynamic sparse masks for different inputs, and then apply these mask
    to perform sparse attention calculations. Experimental results on benchmarks such
    as InfiniteBench, RULER, language modeling, and Needle In A Haystack demonstrate
    that our method effectively maintains the long-context capabilities of LLMs while
    achieving up to a 10x speedup, reducing the latency from 30 minutes to 3 minutes
    per prompt for 1 million token prompts on a single A100 GPU. Additionally, we
    have found that similar dynamic sparse attention patterns also exist in both multi-modal
    LLMs [[80](#bib.bib80)] and encoder-decoder LLMs [[66](#bib.bib66)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AAJ^+ [24] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya
    Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key
    tokens selection for efficient generative inference. Proceedings of Machine Learning
    and Systems, 6:114–127, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AJA^+ [24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed
    Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del
    Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek
    Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie
    Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo
    Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen
    Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi,
    Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid
    Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,
    Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning
    Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua
    Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,
    Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna
    Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report:
    A highly capable language model locally on your phone. ArXiv, abs/2404.14219,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ALTdJ^+ [23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,
    Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BANG [23] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley.
    Unlimiformer: Long-range transformers with unlimited length input. In Thirty-seventh
    Conference on Neural Information Processing Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BBC^+ [23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. Qwen technical report. ArXiv preprint, abs/2309.16609,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BPC [20] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. ArXiv preprint, abs/2004.05150, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BSK^+ [23] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun
    Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok, and Shashank Shet. Codeplan:
    Repository-level coding using LLMs and planning. In NeurIPS 2023 Foundation Models
    for Decision Making Workshop, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CGRS [19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. ArXiv preprint, abs/1904.10509, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CPG^+ [23] Avi Caciularu, Matthew E Peters, Jacob Goldberger, Ido Dagan, and
    Arman Cohan. Peek across: Improving multi-document modeling via cross-document
    question-answering. In Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 1970–1989, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CWCT [23] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. ArXiv preprint,
    abs/2306.15595, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CWW^+ [24] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing,
    Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up
    vision foundation models and aligning for generic visual-linguistic tasks. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 24185–24198, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DA [24] DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts
    language model, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao [24] Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. In The Twelfth International Conference on Learning Representations,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DG [24] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and
    efficient algorithms through structured state space duality. In Forty-first International
    Conference on Machine Learning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DHJ^+ [24] Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei
    Bi, and Shuming Shi. Sequence can secretly tell you what to discard. ArXiv preprint,
    abs/2404.15949, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DMD^+ [23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang,
    Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. ArXiv preprint, abs/2307.02486, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSY [24] Yichuan Deng, Zhao Song, and Chiwun Yang. Attention is naturally sparse
    with gaussian distributed input. ArXiv preprint, abs/2404.02690, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DYZ^+ [24] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi,
    and Beidi Chen. Get more with LESS: Synthesizing recurrence with KV cache compression
    for efficient LLM inference. In Forty-first International Conference on Machine
    Learning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DZZ^+ [24] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning
    Shang, Jiahang Xu, Fan Yang, and Mao Yang. LongroPE: Extending LLM context window
    beyond 2 million tokens. In Forty-first International Conference on Machine Learning,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPN^+ [24] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context.
    In Forty-first International Conference on Machine Learning, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu [24] Yao Fu. Challenges in deploying long-context transformers: A theoretical
    peak performance analysis. ArXiv preprint, abs/2405.08944, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GD [23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective
    state spaces. ArXiv preprint, abs/2312.00752, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GGR [22] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long
    sequences with structured state spaces. In The Tenth International Conference
    on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gra [24] Gradient. Llama-3 8b instruct gradient 4194k (v0.1), 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GZL^+ [24] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and
    Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for
    llms. In The Twelfth International Conference on Learning Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GZX^+ [24] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin,
    Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of
    large language models from glm-130b to glm-4 all tools. ArXiv preprint, abs/2406.12793,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HBK^+ [24] Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal
    Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local
    language modeling for fast inference. ArXiv preprint, abs/2406.02657, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HSK^+ [24] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima
    Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What’s the real context
    size of your long-context language models? ArXiv preprint, abs/2404.06654, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HWP^+ [24] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language
    models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of
    the 2024 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3991–4008,
    Mexico City, Mexico, 2024\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. ArXiv preprint, abs/2310.06825,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JTZ^+ [23] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang,
    Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations
    for enabling training of extreme long sequence transformer models. ArXiv preprint,
    abs/2309.14509, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JWL^+ [23] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili
    Qiu. Llmlingua: Compressing prompts for accelerated inference of large language
    models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing, pages 13358–13376, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JWL^+ [24] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin,
    Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long
    context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers). Association
    for Computational Linguistics, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JYW^+ [23] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin
    Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve
    real-world github issues? In The Twelfth International Conference on Learning
    Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kam [23] Greg Kamradt. Needle in a haystack - pressure testing llms, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KKL [20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. In 8th International Conference on Learning Representations, ICLR
    2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KVPF [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François
    Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.
    In Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
    Research, pages 5156–5165\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LCSR [21] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush.
    Block pruning for faster transformers. In Proceedings of the 2021 Conference on
    Empirical Methods in Natural Language Processing, pages 10619–10629, Online and
    Punta Cana, Dominican Republic, 2021\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LCW [21] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On
    the expressive power of self-attention matrices. ArXiv preprint, abs/2106.03764,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LCW [23] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On
    the expressive flexibility of self-attention matrices. Proceedings of the AAAI
    Conference on Artificial Intelligence, 37(7):8773–8781, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDGL [23] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context
    to enhance inference efficiency of large language models. In Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LDL^+ [24] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie,
    Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting
    the persistence of importance hypothesis for llm kv cache compression at test
    time. Advances in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LHY^+ [24] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli,
    Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what
    you are looking for before generation. ArXiv preprint, abs/2404.14469, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLB^+ [24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin,
    Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz,
    et al. Jamba: A hybrid transformer-mamba language model. ArXiv preprint, abs/2403.19887,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLWL [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. Advances in neural information processing systems, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LMZ^+ [24] Zhiqi Lin, Youshan Miao, Quanlu Zhang, Fan Yang, Yi Zhu, Cheng Li,
    Saeed Maleki, Xu Cao, Ning Shang, Yilei Yang, Weijiang Xu, Mao Yang, Lintao Zhang,
    and Lidong Zhou. nnscaler: Constraint-guided parallelization plan generation for
    deep learning training. In 18th USENIX Symposium on Operating Systems Design and
    Implementation (OSDI 24). USENIX Association, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LQC^+ [22] Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding, and Yuan
    Xie. Dynamic sparse attention for scalable transformer acceleration. IEEE Transactions
    on Computers, 71(12):3165–3178, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LWD^+ [23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi
    Chen. Deja vu: Contextual sparsity for efficient LLMs at inference time. In Andreas
    Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
    Scarlett, editors, Proceedings of the 40th International Conference on Machine
    Learning, Proceedings of Machine Learning Research. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LYZA [24] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model
    on million-length video and language with ringattention. ArXiv preprint, abs/2402.08268,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LZA [23] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise
    transformers for near-infinite context. In NeurIPS 2023 Foundation Models for
    Decision Making Workshop, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LZD^+ [24] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context
    llms struggle with long in-context learning. ArXiv preprint, abs/2404.02060, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MEL [24] Yuzhen Mao, Martin Ester, and Ke Li. Iceformer: Accelerated inference
    with long-sequence transformers on CPUs. In The Twelfth International Conference
    on Learning Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MFG [24] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave
    no context behind: Efficient infinite context transformers with infini-attention.
    ArXiv preprint, abs/2404.07143, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NŁC^+ [24] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan,
    and Edoardo Ponti. Dynamic memory compression: Retrofitting LLMs for accelerated
    inference. In Forty-first International Conference on Machine Learning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NXH^+ [23] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen
    Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report. ArXiv preprint,
    abs/2309.03450, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OHAS [24] Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers
    are multi-state rnns. ArXiv preprint, abs/2401.06104, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PAA^+ [23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian
    Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan
    Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit
    Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,
    Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV:
    Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika
    Bali, editors, Findings of the Association for Computational Linguistics: EMNLP
    2023, pages 14048–14077, Singapore, 2023\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'POC^+ [23] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User
    Interface Software and Technology, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PPJF [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret.
    Fast attention over long sequences with dynamic sparse flash attention. Advances
    in Neural Information Processing Systems, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PQFS [24] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn:
    Efficient context window extension of large language models. In The Twelfth International
    Conference on Learning Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PSL [22] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long:
    Attention with linear biases enables input length extrapolation. In The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29, 2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PWJ^+ [24] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo,
    Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2:
    Data distillation for efficient and faithful task-agnostic prompt compression.
    In Findings of the Association for Computational Linguistics: ACL 2024. Association
    for Computational Linguistics, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RCHG^+ [24] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
    Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient LLM inference.
    In Forty-first International Conference on Machine Learning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RLL^+ [24] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu
    Chen. Samba: Simple hybrid state space models for efficient unlimited context
    language modeling. ArXiv preprint, abs/2406.07522, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RPJ^+ [20] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier,
    and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling.
    In 8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RST^+ [24] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RSVG [21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient
    content-based sparse attention with routing transformers. Transactions of the
    Association for Computational Linguistics, 9:53–68, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SCY^+ [24] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi
    Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical
    speculative decoding. ArXiv preprint, abs/2404.11912, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SDH^+ [23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer
    for large language models. ArXiv preprint, abs/2307.08621, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SDZ^+ [24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming
    Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder
    architectures for language models. ArXiv preprint, abs/2405.05254, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SGR^+ [21] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo
    Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in
    self-attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th
    International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
    Event, volume 139 of Proceedings of Machine Learning Research, pages 9547–9557\.
    PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sha [19] Noam Shazeer. Fast transformer decoding: One write-head is all you
    need. ArXiv preprint, abs/1911.02150, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TDT^+ [23] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei,
    Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou,
    Neil Houlsby, and Donald Metzler. UL2: Unifying language learning paradigms. In
    The Eleventh International Conference on Learning Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TKC [19] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate
    language and compiler for tiled neural network computations. In Proceedings of
    the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages, pages 10–19, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tri [23] Triton implementation of the flash attention v2 algorithm. Technical
    report, OpenAI, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TSP^+ [23] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu,
    Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training
    for context scaling. In Thirty-seventh Conference on Neural Information Processing
    Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TZZ^+ [24] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci,
    and Song Han. QUEST: Query-aware sparsity for efficient long-context LLM inference.
    In Forty-first International Conference on Machine Learning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen [23] Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WWL^+ [24] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng
    Jin, Longyue Wang, and Li Yuan. Look-m: Look-once optimization in kv cache for
    efficient multimodal long-context inference. ArXiv preprint, abs/2406.18139, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WZH [21] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse
    attention architecture with cascade token and head pruning. In 2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA), pages 97–110\. IEEE,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XTC^+ [24] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
    Efficient streaming language models with attention sinks. In The Twelfth International
    Conference on Learning Representations, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XZH^+ [24] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin,
    Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the
    intrinsic capacity of llms for understanding extremely long sequences with training-free
    memory. ArXiv preprint, abs/2402.04617, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YCL^+ [24] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei
    Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation
    models by 01\. ai. ArXiv preprint, abs/2403.04652, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZAW [24] Itamar Zimerman, Ameen Ali, and Lior Wolf. A unified implicit attention
    formulation for gated-linear recurrent sequence models. ArXiv preprint, abs/2405.16504,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZCH^+ [24] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen,
    Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. $\infty$bench:
    Extending long context evaluation beyond 100k tokens. ArXiv preprint, abs/2402.13718,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZGD^+ [20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
    Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
    and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
    editors, Advances in Neural Information Processing Systems 33: Annual Conference
    on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZJZ^+ [23] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao
    Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, et al. Pit:
    Optimization of dynamic sparse deep learning models via permutation invariant
    transformation. In Proceedings of the 29th Symposium on Operating Systems Principles,
    pages 331–347, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZSZ^+ [24] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng,
    Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o:
    Heavy-hitter oracle for efficient generative inference of large language models.
    Advances in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the context length decreases, the time required to build the dynamic index
    becomes more significant as attention computation time decreases. For example,
    with a 10k context, the time spent on building the index increases from 5% to
    30%, resulting in overall end-to-end latency approaching that of FlashAttention.
    However, this overhead proportion gradually decreases as the prompt lengthens.
    Additionally, when using a higher sparsity rate, the model performance may noticeably
    decline.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Broader Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MInference effectively accelerates the inference of long-context LLMs, facilitating
    their deployment and application. By enabling lower latency, it can reduce the
    deployment costs of LLMs, especially for long-context LLMs, helping to democratize
    access to advanced AI. It also promotes further research and development in related
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Dataset Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: InfiniteBench [[86](#bib.bib86)]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: includes 10 tasks designed to test various aspects of long-context processing.
    Specifically, these tasks cover entire novel summarization, open-form question
    answering based on novels, multiple-choice question answering on novels, question
    answering on long drama scripts, question answering on Chinese texts, debugging
    large code repositories, identifying the largest/smallest number in arrays, and
    retrieval tasks with varying pattern lengths. The average token length for these
    tasks is 214k, and they include 3,992 examples.
  prefs: []
  type: TYPE_NORMAL
- en: RULER [[28](#bib.bib28)]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: is a recent synthetic benchmark suite for long-context evaluation with 13 complex
    tasks across four categories. The retrieval category includes Single Needle-in-a-Haystack
    (S-NIAH), where a single key-value pair is inserted into noisy text, and the model
    must retrieve the value. Multi-keys Needle-in-a-Haystack (MK-NIAH) involves multiple
    keys, and the model retrieves one specific value among hard distractors. The Multi-values
    Needle-in-a-Haystack (MV-NIAH) task requires retrieving all values associated
    with a single key, while the Multi-queries Needle-in-a-Haystack (MQ-NIAH) task
    involves retrieving values for multiple keys. The Multi-hop Tracing category includes
    Variable Tracking (VT), where the model traces and returns all variable names
    pointing to the same value through variable bindings. The aggregation category
    introduces Common Words Extraction (CWE), where the model identifies the top-K
    common words from a mixture of common and uncommon words, and Frequent Words Extraction
    (FWE), where the model identifies the most frequent words from a Zeta distribution.
    The Question Answering (QA) category extends existing short-context QA datasets
    by adding distracting paragraphs, challenging the model to answer questions based
    on relevant information surrounded by distractors. These tasks provide a comprehensive
    evaluation of long-context modeling capabilities, covering multi-hop reasoning,
    aggregation, and complex question answering. Following [[28](#bib.bib28)], we
    test models on 4K, 8K, 16K, 32K, 64K, and 128K context lengths, including 2,600
    examples per length.
  prefs: []
  type: TYPE_NORMAL
- en: Needle In A Haystack task [[35](#bib.bib35)]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: evaluates the performance of retrieval-augmented generation (RAG) systems by
    embedding specific, targeted information (the "needle") within a large, complex
    body of text (the "haystack"). The test assesses a language model’s ability to
    identify and utilize this specific piece of information amidst a vast amount of
    data. Both RULER and the needle test iterate over various context lengths and
    document depths (where the ground-truth is placed in the prompt) to measure the
    long-context performance. Here we scale the Needle In A Haystack task to 1M context
    length, including 750 examples.
  prefs: []
  type: TYPE_NORMAL
- en: PG-19 [[65](#bib.bib65)]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The perplexity on long text is also often used by researchers to evaluate the
    language modeling performance of long-context LLMs. PG-19 is a suitable test set
    for this task, as it includes texts as long as 500K tokens. Perplexity is used
    as the metric indicating how well a model predicts the next token in a sequence.
    Our experiments are conducted on 1,000 random samples from PG-19 that are longer
    than 100K tokens.
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Additional Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our experiments are based on a number of state-of-the-art long-context LLMs:
    1) LLaMA-3-8B-Instruct-262k³³3https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k
    is a LLaMA-3 variant with further NTK-aware interpolation and minimal fine-tuning
    with Ring Attention, which achieved SOTA results on long-context assessments such
    as the Needle In A Haystack test; 2) LLaMA-3-8B-Instruct-1048k⁴⁴4https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k
    is similar to LLaMA-3-8B-Instruct-262k, but supports context lengths up to 1M
    tokens; 3) Yi-9B-200K [[84](#bib.bib84)] is a SOTA LLM that balances long-context
    performance with general capabilities; 4) Phi-3-Mini-128K [[2](#bib.bib2)] a small
    but powerful language model that offers capabilities equivalent to models ten
    times its size with up to 128K context window powered by LongRoPE [[19](#bib.bib19)];
    5) Qwen2-7B-128K [[5](#bib.bib5)] is a recently release update of Qwen series
    model with up to 128K context window that achieve superior or comparable performance
    compared to LLaMA-3; 6) GLM-4-9B-1M [[26](#bib.bib26)] has been improved from
    its predecessor in terms of a 1M context window, performance on downstream tasks
    and inference efficiency. To guarantee stable results, we use greedy decoding
    in all tests. Our kernel implementations are developed and optimized based on
    the dynamic sparse compiler PIT [[88](#bib.bib88)] in the Triton language [[75](#bib.bib75)].
    The latency experiments are done on a single Nvidia A100 GPU using bfloat16. We
    provide a simple custom implementation of attention in PyTorch, building on FlashAttention
    and Triton.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the target FLOPs $t$ to be the same as 1k global tokens and 4k local
    window tokens in the A-shape pattern. The step size of ChangeSpace is set to 50,
    with the corresponding search space shown in Table [6](#A3.T6 "Table 6 ‣ C.2 Additional
    Implementation Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Additionally,
    we use only one sample as our validation set from KV retrieval synthetic data
    with 30k token inputs, which exhibits strong generalization and stability across
    different lengths and domains. The search time is approximately 15 minutes on
    a single A100. Additionally, we use the same optimal sparse pattern configuration
    for both the LLaMA-3-8B-Instruct-262K model and the LLaMA-3-8B-Instruct-1M model.
    The specific distribution is shown in Fig. [11](#A5.F11 "Figure 11 ‣ Appendix
    E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Kernal-aware optimal head pattern search space. In this context, A-shape
    represents the global tokens and local window number, Vertical-Slash represents
    the Top-K number of vertical and diagonal lines, and Block-Sparse represents the
    Top-K number of blocks retained.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Patterns | Search Space |'
  prefs: []
  type: TYPE_TB
- en: '| A-shape | $\{(1024,4096)\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Vertical-Slash | $\{(30,2048),(100,1800),(500,1500),(3000,200)\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Block-Sparse | $\{100\}$ |'
  prefs: []
  type: TYPE_TB
- en: C.3 Single A100 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original PyTorch implementation⁵⁵5https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py
    of the LLaMA model causes an out-of-memory error on a single A100 (80G) when the
    prompt exceeds 50k tokens. To enable running 1M prompt inference on a single A100,
    we implemented the following optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tensor Splitting: We split the Attention by head and the MLP by sequence dimension.
    In long-context scenarios, where computation is the bottleneck, this splitting
    keeps GPU utilization at 100%, and the overhead of splitting is negligible;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduction of Intermediate Variables: We minimized intermediate variable allocation
    by removing the attention mask and implementing causal mask logic directly within
    the kernel;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Elimination of Unnecessary Computations: In long-context scenarios, only the
    logits corresponding to the last token in the prompt phase are meaningful. Thus,
    we only retain the computation of the LM Head Linear layer for the last token.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C.4 Kernel Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.4.1 Block-Sparse Flash Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our Block-Sparse kernel implementation is based on the Triton version of the
    FlashAttention kernel [[76](#bib.bib76)]. With the selected block index as an
    additional input, each thread block loops through the top-K blocks in a row. As
    discussed in FlashAttention [[13](#bib.bib13)], the latency of the block-sparse
    FlashAttention kernel is linearly related to the number of blocks, and the speedup
    ratio (compared to the dense FlashAttention kernel) is approximately as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{p}=\frac{S}{2B\times k_{b}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: C.4.2 Vertical-Slash Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Vertical-Slash attention includes two custom kernels: the Vertical-Slash
    sparse index kernel and the Vertical-Slash sparse FlashAttention kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff009019a243e049587bc52e961c7e21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The dynamic sparse mask for the vertical-slash pattern using LLaMA-3-8B
    in the summarization task [[86](#bib.bib86)]. Yellow areas indicate the computed
    parts. Slash lines use $64\times 64$ blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vertical-Slash sparse index kernel in Algorithm [4](#alg4 "Algorithm 4
    ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") builds the index for each row of blocks. Since
    a slash line segment can be masked by a square block, our attention mask is a
    mix of blocks and columns, as shown in Fig. [7](#A3.F7 "Figure 7 ‣ C.4.2 Vertical-Slash
    Attention ‣ C.4 Kernel Implementation ‣ Appendix C Experiment Details ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    We apply a point-range two-way merge algorithm where vertical indexes are treated
    as points and slash indexes are converted to ranges given the row index. The output
    consists of two parts: merged ranges and separate column indexes, where the ranges
    are represented by block indexes. The time complexity to build an index for a
    row is $O(k_{v}+k_{s})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vertical-Slash sparse FlashAttention kernel in Algorithm [5](#alg5 "Algorithm
    5 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") is a mix of the block-sparse attention kernel
    and the PIT [[88](#bib.bib88)] sparse attention kernel. PIT is a technology that
    loads sparse data into dense compute blocks via a Permutation Invariant Transformation.
    A thread block first loops through the block indexes as described in the previous
    section (block part) and then loops through the column indexes grouped by block
    size (PIT part). The latency of this hybrid kernel is linearly related to the
    total area of blocks and columns.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Additional Experiment Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 Needle In A Haystack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39bb7b8f651893d7e6f68e4c5a961b46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Results on Needle In A Haystack using InfLLM in LLaMA-3-8B-Instruct-1M.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the Needle In A Haystack results for LLaMA-3-Instruct-1M shown
    in §[4](#S4 "4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we also present the LLaMA-3-Instruct-1M using
    InfLLM results in Fig. [8](#A4.F8 "Figure 8 ‣ D.1 Needle In A Haystack ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"), and results for GLM-4-9B-1M,
    Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, shown in Fig. [9](#A4.F9 "Figure
    9 ‣ D.1 Needle In A Haystack ‣ Appendix D Additional Experiment Results ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    Compared to Full Attention, using MInference has minimal impact on the ability
    to understand semantic information across different context windows and needle
    depths. There is even a slight performance improvement around the 100k context
    length using Yi-9B-200K and Phi-3-Mini-128K.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba5ebb2989e79685f9c2208bddfa185d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) GLM-4-9B-1M
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52f83beb6a222be5d31f20f298066c2e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) GLM-4-9B-1M w/ MInference
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b938f1489e9142f62953ffa559352b51.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Yi-9B-200K
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4cfe3b402cabfda105c1c3293ea50951.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Yi-9B-200K w/ MInference
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd910638ecb8839985282ec15fd8c006.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Phi-3-Mini-128K
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/565de9e1d007245060ad8c490747a28a.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Phi-3-Mini-128K w/ MInference
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32b3a37365e0176166e99b96732899d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) Qwen2-7B-128K
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43463f60255f33abb96cf01e0ed331c7.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) Qwen2-7B-128K w/ MInference
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Needle In A Haystack [[35](#bib.bib35)] results using GLM-4-9B-1M [[26](#bib.bib26)],
    Yi-9B-200K [[84](#bib.bib84)], Phi-3-Mini-128K [[2](#bib.bib2)], and Qwen2-7B-128K [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Latency Breakdown
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b8f7add8e0f29d889bf7870f60b4586.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The latency breakdown of a single attention kernel for three patterns
    and FlashAttention [[13](#bib.bib13)] across different context windows in a single
    A100, including the index time for dynamic sparse approximation and building dynamic
    sparsity. At 10k tokens, the latency of the four kernels is very close and all
    are less than 1ms. At 1M tokens, the latency for A-shape is 164ms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [10](#A4.F10 "Figure 10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional
    Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") shows the micro-benchmark results of the three
    attention patterns proposed in this paper, as well as FlashAttention. It can be
    seen that Vertical-Slash is the slowest among the three patterns, but it still
    achieves a 13x speedup compared to FlashAttention under 1M context windows. A-shape
    is slightly faster than Vertical-Slash, but at 1M, A-shape is 50% slower than
    Vertical-Slash. Block-Sparse is the fastest, achieving a 30x speedup over FlashAttention
    under 1M context windows. The estimation and index-building time for the dynamic
    sparse pattern accounts for approximately 5%-15% and 25% of the total time for
    Vertical-Slash and Block-Sparse patterns, respectively. The index-building overhead
    is higher for Block-Sparse mainly due to the time-consuming MeanPooling and block-level
    matmul computations. Additionally, the memory overhead for sparse indexing is
    relatively small, remaining within 160MB for a LLaMA-3-8B model in 1M context.'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Additional Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 7: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K
    on InfiniteBench [[86](#bib.bib86)]. It is important to note that due to kernel
    limitations, we must retain at least one vertical and one slash. Therefore, "ours
    w/ only vertical" retains the top-1 slash, and "ours w/ only slash" retains the
    top-1 vertical.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ only vertical | 13.7 | 6.2 | 30.1 | 2.0 | 6.5 | 7.9 | 1.7 | 65.4
    | 52.7 | 0.0 | 18.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ only slash | 18.4 | 11.5 | 60.1 | 3.0 | 11.4 | 22.1 | 28.4 | 100.0
    | 100.0 | 4.2 | 35.9 |'
  prefs: []
  type: TYPE_TB
- en: 'To further analyze the role of dynamic vertical and slash lines in the Vertical-Slash
    pattern for sparse computation, we introduce a new set of ablation studies as
    follows: 1) Ours w/ only vertical, which only uses vertical lines and the top-1
    slash line in Vertical-Slash pattern. 2) Ours w/ only slash, which only uses slash
    lines and the top-1 vertical line in Vertical-Slash pattern. The corresponding
    top-K quantities are set after converting based on FLOPs in kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [7](#A4.T7 "Table 7 ‣ D.3 Additional Ablation Study ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"), using only vertical lines results
    in a significant performance drop, especially in retrieval tasks, where performance
    is similar to only using block-sparse. In contrast, using only slash lines retains
    most of the performance, but in highly dynamic tasks such as KV retrieval, performance
    further decreases, with an average performance drop of 2.9% compared to Ours.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Pattern Distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5985fc55f8aafd48b024a5fae9e1aa72.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) LLaMA-3-8B-Instruct-262K/1M
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee027e611a81c5a58fd894a1de67b87e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Yi-9B-200K
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Distribution of three sparse head patterns in different models.
    We use the same optimal sparse pattern configuration for both LLaMA-3-8B-Instruct-262K
    and LLaMA-3-8B-Instruct-1M.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    shows the distribution of the optimal head configuration obtained through our
    search. Firstly, most of the patterns are the Vertical-Slash pattern (>90%). However,
    according to the ablation study, using only the Vertical-Slash pattern significantly
    impacts performance in highly dynamic tasks like KV retrieval. Secondly, the Block-Sparse
    pattern is primarily distributed in several intermediate to later layers, while
    the A-shape pattern is found in the middle layers. Although the optimal patterns
    vary slightly across different models, they generally align with these observations.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we used the same configuration for two versions of LLaMA in our
    experiments, and the results show that the 1M model also performs very well, with
    nearly perfect results in the Needle In A Haystack task. This demonstrates the
    generalizability of the optimal sparse pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Sparsity in Kernel Distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The distribution of sparsity in the kernel across different context
    windows refers to the proportion of the kernel that is actually computed after
    block coverage, compared to the sparsity rate when using FlashAttention with a
    causal mask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), the sparsity distribution of the three patterns during the actual
    kernel computation process is displayed. It can be seen that when the context
    windows exceed 200k, the actual sparsity of all three patterns surpasses 90%.
    Even considering a 20% index-building overhead, this ensures that the kernel achieves
    a speedup of over 8$\times$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Does This Dynamic Sparse Attention Pattern Exist Only in Auto-Regressive
    LLMs or RoPE-Based LLMs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b22e182a2b67446c71bc01d753a7101.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The sparse pattern in T5-style Encoder Attention using Flan-UL2 [[74](#bib.bib74)]
    on the Summarization dataset [[86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar vertical and slash line sparse patterns have been discovered in BERT [[72](#bib.bib72)]
    and multi-modal LLMs [[80](#bib.bib80)]. Additionally, as shown in Fig. [13](#A7.F13
    "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern Exist Only
    in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), we analyzed the distribution
    of attention patterns in T5 across different heads. It is evident that there are
    vertical and slash sparse patterns even in bidirectional attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies [[80](#bib.bib80)] have analyzed sparse attention patterns in
    multi-modal LLMs, revealing the presence of vertical and slash patterns in models
    like LLaVA [[45](#bib.bib45)] and InternVL [[11](#bib.bib11)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") presents a comparison
    of the generation performance for various methods on the EN.SUM task (200K input
    length) from InfiniteBench based on the LLaMA-3-8B-262K model. The original summary
    provides a comprehensive and coherent narrative, detailing the Bronwyn family’s
    trip to the Kindergarten and touching on themes such as nostalgia, loss, and the
    passage of time. StreamingLLM’s summary, although looks coherent, introduces elements
    that are not present in the original story, leading to serious factual errors.
    For example, it mentions a boat trip to a school for boys and specific details
    like fishermen, sandwiches, and a spot where men were drowned. These details deviate
    from the original story, which is about the Bronwyn family preparing for a trip
    to the Kindergarten. In addition, the summaries generated by StreamingLLM with
    dilated and strided techniques are largely incoherent, consisting primarily of
    repetitive and nonsensical characters, indicating a failure to produce meaningful
    content. In stark contrast, the summary generated by our proposed method offers
    a detailed and coherent narrative, comparable to the original, with a clear depiction
    of the story’s main events and themes. This includes the preparation of the Bronwyn
    family for their trip, the characterization of family members and guests, and
    the exploration of deeper themes such as love, marriage, and the search for meaning.
    The results demonstrate the superiority of our proposed method in generating high-quality,
    human-like summaries over the baseline methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") compares the
    performance of various methods on the Retrieve.KV task (200K input length) using
    the LLaMA-3-8B-262K model. The original method demonstrates perfect retrieval,
    correctly predicting the exact strings of the ground truth for both examples.
    StreamingLLM, again, generates predictions that looks coherent and real, but factually
    incorrect. In addition, StreamingLLM with dilated and strided techniques, and
    our method with a static pattern, fail significantly, producing outputs that are
    either repetitive sequences of characters or nonsensical strings, indicating their
    inability to accurately retrieve the required key-value pairs. Our method, however,
    performs on par with the original, accurately retrieving and predicting the exact
    key-value pairs for both examples. This demonstrates the superior capability of
    our method in handling KV retrieval tasks, providing precise and reliable outputs
    consistent with the ground truth. The results highlight our method’s effectiveness
    and robustness compared to the baselines, making it a reliable choice for such
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 Vertical-Slash Index
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: vertical indexes $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$ # Sort vertical and
    slash indexes  $\bm{i}_{v}\leftarrow\mathrm{IncrementalSort}\left(\bm{i}_{v}\right)$  #
    Calculate block number (block_size $B$  # Initialize outputsblock count $\bm{c}_{\text{blk}}\in\mathbb{N}^{N}$,
    column count $\bm{c}_{\text{col}}\mathbb{N}^{N}$  # Parallelized in GPU  for $i\leftarrow
    1$ do     $j_{v}\leftarrow 1$  # Define the range by slash index     $r_{\text{start}}\leftarrow(i-1)\times
    B-\bm{i}_{s}^{j_{s}}$  # Merge points (vertical indexes) and ranges (slash indexes)     while $s_{v}\leq
    k_{s}$  and  $\bm{i}_{v}^{j_{v}}<r_{\text{end}}$ then              $\bm{c}_{\text{col}}^{i}\leftarrow\bm{c}_{\text{col}}^{i}+1$           end           $j_{v}\leftarrow
    j_{v}+1$  # Update the range           if $(i-1)\times B-\bm{i}_{s}^{j_{s}}></math>              while <math
    id=$                 $\bm{i}_{\text{blk}}^{i,\bm{c}_{\text{blk}}^{i}}\leftarrow
    s$              end while# Calculate the new range              $r_{\text{start}}\leftarrow(i-1)\times
    B-\bm{i}_{s}^{j_{s}}$           else              # Extend the range              $r_{\text{end}}\leftarrow
    r_{\text{end}}+B$     while $s<r_{\text{end}}$        $\bm{i}_{\text{blk}}^{i,c_{blk}^{i}}\leftarrow
    s$     end while  end for  $\mathrm{return}\,\,\,\bm{c}_{\text{blk}},\bm{i}_{\text{blk}},\bm{c}_{\text{col}},\bm{i}_{\text{col}}$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5 Vertical-Slash Flash Attention
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$, block index $\bm{i}_{\text{blk}}\in\mathbb{N}^{N\times
    k_{s}}$, column index $\bm{i}_{\text{col}}\in\mathbb{N}^{N\times k_{v}}$  Initialize
    $\bm{O}\leftarrow(0)^{S\times d_{h}}\in\mathbb{R}^{S\times d_{h}}$ to $N$Initialize
    $\bm{O}_{\text{chip}}\leftarrow(0)^{B\times d_{h}}\in\mathbb{R}^{B\times d_{h}}$Initialize
    $\bm{l}\leftarrow(0)^{B}\in\mathbb{R}^{B}$ to $\bm{c}_{\text{blk}}^{i}$Load $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$        $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$     while $j<\bm{c}_{\text{col}}^{j}$Load
    $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{\bm{cols}}\in\mathbb{R}^{B\times d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$        $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$     end while#
    Write outputs     $\bm{O}_{\text{chip}}\leftarrow\mathrm{diag}(\bm{l}^{i})^{-1}\bm{O}_{\text{chip}}$  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in summarization task [[86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Summary |'
  prefs: []
  type: TYPE_TB
- en: '| Original | The story follows the Bronwyn family and their guests as they
    prepare for a day trip to the Kindergarten, a mysterious destination that holds
    great significance for them. The story is told from multiple perspectives, including
    that of Pete Lucy, who is staying with the family and is struggling to find her
    place in their lives. As the day progresses, the family members prepare for their
    trip, and Pete reflects on her own past experiences with the Bronwyns. The story
    ends with the family setting off on their journey, leaving Pete alone on the lawn
    to contemplate her own feelings and the meaning of life. The Kindergarten is a
    symbol of the family’s past and their memories, and the story explores themes
    of nostalgia, loss, and the passage of time. Ultimately, the story suggests that
    even though the past cannot be changed, it can still be celebrated and remembered.
    |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | The story follows a family on a boat trip to the Kindergarten,
    a school for boys. The father, Mr Bronwyn, is reading a book and seems to be in
    a trance. The children, Seb and Annalisa, are excited about the trip but also
    a little scared. They pass by a group of fishermen and Mr Bronwyn shares out sandwiches.
    They come across a spot where three men were drowned and Mr Bronwyn takes out
    his watch, making a calculation. They reach the Kindergarten and Mr Bronwyn jumps
    out of the boat, holding a parcel. The story ends with Pete Lucy, who has been
    watching the family from the shore, feeling relieved that the trip is over and
    that Mr Bronwyn has reached his destination. The story is told from multiple perspectives,
    including Pete Lucy’s, who is watching the family from the shore and reflecting
    on his own life and relationships. |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( |'
  prefs: []
  type: TYPE_TB
- en: '| Ours w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | The story follows the Bronwyn family and their guests as they prepare
    for a trip to the Kindergarten, a remote island off the coast of Scotland. The
    family is made up of Mr. Bronwyn, a philosopher, and his wife, Mrs. Bronwyn, who
    is known for her beauty and her ability to surround and protect her family. The
    guests include Kevin Emma, a young man who is studying for a fellowship, and Castor
    Kiki, a botanist who is also a widower. The story explores themes of love, marriage,
    and the search for meaning in life. It also touches on the idea of the other,
    or the person who is different from oneself and cannot be understood or judged
    by conventional standards. Ultimately, the story is about the search for truth
    and the desire to connect with others on a deeper level. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in KV Retrieval task [[86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Summary |'
  prefs: []
  type: TYPE_TB
- en: '| Original | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '| StreamingLLM | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant 2\. Ground Truth: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction: "d829ce18-6339-4f77-8c04-31fc7ec33619". |'
  prefs: []
  type: TYPE_NORMAL
- en: '| StreamingLLM w/ dilated | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: 5 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '| StreamingLLM w/ strided | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ours w/ static | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "def solverome2 def solverome2 def solverome2 def solverome2 def 2\. Ground Truth:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ours | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  prefs: []
  type: TYPE_NORMAL
