- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Exploration of LLMs, EEG and behavioral data to measure and support attention
    and sleep
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.07822](https://ar5iv.labs.arxiv.org/html/2408.07822)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Akane Sano Rice University USA
  prefs: []
  type: TYPE_NORMAL
- en: akane.sano@rice.edu    Judith Amores Microsoft Research USA
  prefs: []
  type: TYPE_NORMAL
- en: judithamores@microsoft.com    Mary Czerwinski Microsoft Research USA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We explore the application of large language models (LLMs), pre-trained models
    with massive textual data for detecting and improving these altered states. We
    investigate the use of LLMs to estimate attention states, sleep stages, and sleep
    quality and generate sleep improvement suggestions and adaptive guided imagery
    scripts based on electroencephalogram (EEG) and physical activity data (e.g. waveforms,
    power spectrogram images, numerical features). Our results show that LLMs can
    estimate sleep quality based on human textual behavioral features and provide
    personalized sleep improvement suggestions and guided imagery scripts; however
    detecting attention, sleep stages, and sleep quality based on EEG and activity
    data requires further training data and domain-specific knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models, LLMs, sleep, attention, EEG
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human altered states such as attention and sleep play significant roles in health
    [[1](#bib.bib1)], safety [[2](#bib.bib2)] , and productivity[[3](#bib.bib3)] .
    By precisely measuring these states, we can design adaptive tools and interfaces
    that respond effectively to users and help promote their health.
  prefs: []
  type: TYPE_NORMAL
- en: Human attention states have been measured using physiological and behavioral
    data such as electroencephalogram (EEG) [[4](#bib.bib4)], facial expressions [[5](#bib.bib5)],
    and eye tracking[[6](#bib.bib6)]. . Measuring human attention states can help
    design systems that enhance driver alertness, minimize interruptions during focus,
    or promote relaxation before sleep.
  prefs: []
  type: TYPE_NORMAL
- en: Extensive research has explored computational methods for measuring, evaluating,
    and improving sleep. For example, many algorithms have been developed to estimate
    sleep quality and stages using human physiological and behavioral sensor data
    including EEG and motion[[7](#bib.bib7)]. Computational systems have been designed
    to promote better sleep[[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: Recent advances in natural language processing have leveraged massive textual
    data to train large language models (LLMs). Some studies have used LLMs for understanding
    human physiological and behavioral data and designing health applications including
    EEG abnormality detection and wearable sensor-based sleep quality detection[[9](#bib.bib9)][[10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: LLMs hold promise for health applications including human altered state detection
    and personalized feedback delivery; however, rigorous evaluations have not been
    conducted, particularly regarding the integration of different human physiological
    and behavioral data (e.g., waveforms, numerical features, power spectrogram images)
    into LLMs for understanding the potential, accuracy, limitations, and reliability
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we evaluate LLMs for detecting and supporting human attention
    and sleep. Our ultimate goal is to create personalized, adaptive systems that
    enhance individuals’ attention and sleep. To achieve this, in this paper, we conduct
    early explorations by integrating biobehavioral data into LLMs to understand their
    capabilities. We ask the following two research questions in the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can LLMs interpret/sense attentive states, sleep stages, and sleep quality?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can LLMs provide personalized and adaptive feedback to help improve sleep?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We investigate the impact of various time scales and different input modalities
    of EEG, motion, and textual data on LLMs’ performance, reasonings, and generated
    responses for detecting attention states, sleep stages, and sleep quality detection
    and improving sleep.
  prefs: []
  type: TYPE_NORMAL
- en: II Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We describe experiments and datasets for 1) user state detection and 2) sleep
    improvement suggestion generation to address RQ 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'II-A Experiment 1: user state detection'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct three different detection tasks, a) attention detection, b) sleep
    stage detection, and c) sleep quality detection.
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Datasets & Data processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use the following datasets for our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mental Attention State[[11](#bib.bib11)]: This dataset contains 25 hours of
    EEG data collected using 14 ch Emotive. Five participants were engaged in a low-intensity
    task of controlling a computer-simulated train. Three mental states were observed
    in this study: focused, unfocused, and drowsy. We merged unfocused and drowsy
    into an unfocused state. We prepared three different types of information for
    attention detection (focused vs unfocused/drowsy): a) filtered EEG data (Fig.
    [1(a)](#S2.F1.sf1 "In Figure 1 ‣ II-A1 Datasets & Data processing ‣ II-A Experiment
    1: user state detection ‣ II Methods ‣ Exploration of LLMs, EEG and behavioral
    data to measure and support attention and sleep")): Raw EEG signals were processed
    using a bandpass Butterworth filter (order: 128, 0-40Hz), b) time-frequency spectrograms(Fig.
    [1(b)](#S2.F1.sf2 "In Figure 1 ‣ II-A1 Datasets & Data processing ‣ II-A Experiment
    1: user state detection ‣ II Methods ‣ Exploration of LLMs, EEG and behavioral
    data to measure and support attention and sleep")): These were computed using
    wavelet transform (Daubechies wavelet, every 10 sec) to provide frequency components
    over time, and c) 11 features: these features include power spectrum density (delta,
    theta, alpha, beta), amplitude, standard deviation, kurtosis, alpha/delta, theta/alpha,
    delta/theta, the 90th percentile amplitude. We segmented the data into 10-sec
    intervals, resulting in 919 training samples, 230 validation samples, and 287
    test samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/858476c798dccd9e876176bc29a2d4cb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) EEG Waveform
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb628aedcc77a79e3501aa18a41910c9.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) EEG Spectrogram
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ca3f0c816877daf17b2c94f2c3aa9e5.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) Actogram (x axis: time 0am-0am), y-axis: dates, z-axis:physical activity
    levels)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/606b7dc82b9db9225c95d3b1969bd35c.png)'
  prefs: []
  type: TYPE_IMG
- en: '(d) Averaged activity graph (x axis: time 0am-0am), y-axis: physical activity
    levels)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Image Data Fed into LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sleep EDF expanded[[12](#bib.bib12)]: This dataset contains 197 nights of polysomnography
    data collected from individuals aged 18-101 years. The data include EEG from two
    channels, Fpz-Cz and Pz-Oz, EOG, EMG, and event markers. Sleep stages are labeled
    as follows: 0 (Wake), 1 (stage 1), 2 (stage 2), 3 (stages 3 & 4), and 4 (REM sleep).
    We segmented the data into 30-sec epochs (training set: 152362 epochs, Validation
    set: 38092 epochs, test set: 500 epochs). We used Fpz-Cz for sleep stage detection.
    We used the same input types as those used for mental attention states: a) filtered
    EEG data, b) time-frequency spectrograms, and c) 11 features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Student Life[[13](#bib.bib13)]: This dataset contains mobile phone sensor and
    survey data collected from 46 college students. We used the Pittsburgh Sleep Quality
    Index (PSQI) (19 self-rated questions including sleep-related behaviors and self-reported
    sleep quality) administered both at the pre and post study and its scoring rules
    to categorize each participant as a poor or good sleeper. We use physical activity
    data collected from participants’ phones to compute participants’ daily activity
    levels and patterns. We assess sleep quality detection (good vs poor) using the
    following inputs: a) participants’ textual responses to PSQI questions, b) physical
    activity-based actograms: visual representations of 24 hour activity levels over
    days(Fig. [1(c)](#S2.F1.sf3 "In Figure 1 ‣ II-A1 Datasets & Data processing ‣
    II-A Experiment 1: user state detection ‣ II Methods ‣ Exploration of LLMs, EEG
    and behavioral data to measure and support attention and sleep")), and c) physical
    activity-based hourly averaged graphs: hourly levels and variations in physical
    activity (Fig. [1(d)](#S2.F1.sf4 "In Figure 1 ‣ II-A1 Datasets & Data processing
    ‣ II-A Experiment 1: user state detection ‣ II Methods ‣ Exploration of LLMs,
    EEG and behavioral data to measure and support attention and sleep")).'
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare various LLMs and traditional machine learning models. LLMs: 1) Zero-shot
    learning: we feed data (EEG waveform images, spectrograms, or features) into LLMs.
    No specific training is conducted and the LLMs leverage their pre-existing knowledge.
    We use two LLM variants: GPT 4 vision (2024-02-15-preview) for image input and
    GPT 4 (2023-05-15) for textual input, 2) In-context learning LLM: we include input
    data and label examples in prompts so that LLMs (GPT 4) learn from context and
    adapt their response accordingly. 3) Fine-tuned LLM: We finetune LLMs (GPT 3.5
    turbo 2024-02-15-preview) using training and validating datasets. A traditional
    machine learning model, XGBoost uses a technique called gradient boosting that
    combines simple decision trees for accurate predictions. We also analyze feature
    importance by looking at the number of times each feature is used for trees. 3)
    Baseline (majority vote): simply predicts the majority class for all test samples.
    It serves as a basic reference point to evaluate the performance of other models.
    For sleep quality detection, we also use the ground truth scoring method based
    on PSQI. We evaluate model performance using accuracy and weighted F1 score. Please
    see examples of LLM prompts in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'II-B Experiment 2: Personalized and adaptive sleep improvement feedback'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We explore whether LLMs can generate personalized content for sleep improvement
    and focus on generating 1) sleep improvement suggestions and 2) guided imagery
    scripts. To generate the sleep suggestions, we feed LLMs various user context
    or profile information including a) EEG features from Sleep EDF dataset, b) PSQI
    answers in Student Life dataset, c) physical activity-based actograms, d) gender,
    e) age group, f) ethnicity, g) health issues such as “anxiety”, “PTSD”, “insomnia”,
    “pain”, “awakenings during nights”, “shift worker”, h) user preferences such as
    favorite environments: beach, forest, favorite animals: dogs, cats; hobbies: traveling,
    baseball, exercise. To generate scripts to help a person sleep, we prompt the
    LLM to leverage a Guided Imagery technique [[14](#bib.bib14)]. This technique
    is also known as Guided Visualization, a mind-body technique that involves visualizing
    positive images or scenarios using all senses to help promote relaxation, manage
    anxiety, and stress, and enhance well-being. A typical session might include relaxation
    exercises, quiet sitting or lying down, and focusing on a specific goal. See prompt
    examples in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: III Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A User state detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-A1 Attention detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [I](#S3.T1 "TABLE I ‣ III-A1 Attention detection ‣ III-A User state detection
    ‣ III Results ‣ Exploration of LLMs, EEG and behavioral data to measure and support
    attention and sleep") summarizes attention detection performance. Traditional
    machine learning models outperformed LLM-based models for attention detection.
    Among the LLM models, fine-tuned GPT-3.5 models demonstrated the best performance.
    However, the GPT-4 vision model faced limitations. It failed to infer user states
    in approximately 25-30% of cases, often returning a generic response such as “As
    a text-based AI, I do not have the capability to process images and I cannot assist
    with this request”. In addition, when we feed high dimensional EEG features, the
    GPT model indicated that it requires machine learning models and did not infer
    user states,
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuned GPT3.5 model typically shows high confidence levels of 100%.
    The model shows information about the power spectrum and ratios (e.g.,delta, theta,
    alpha, beta, alpha/delta) as the explanations for inference. For example, when
    correct inference is made with 100% confidence, the GPT 3.5 model explains “The
    participant is unfocused because the alpha/delta ratio is high and the beta power
    is low, which are indicative of a relaxed and unfocused state.”
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect inferences lack clear explanations and appear based on simple heuristic
    rules. For example, when ground truth is focused but unfocused is inferred, the
    model said “The participant’s state is unfocused because the alpha/delta ratio
    for channel AF3 is 0.166027, which is below the threshold for focused state.”
    When ground truth is unfocused but focused is inferred, the model said “The participant
    is focused because the alpha/delta ratio is high and the beta power is low, which
    are characteristic of a focused state.”
  prefs: []
  type: TYPE_NORMAL
- en: We also analyze feature importance from the XGBoost model and learn that the
    model uses more variety of features for classifying focus vs unfocus including
    channel O2 kurtosis, AF4 beta, F3 percent 90, F7 beta, and T8 standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Attention Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | acc | F1 | note |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Finetuned GPT3.5 (features) | 53.7 | 0.51 |  |'
  prefs: []
  type: TYPE_TB
- en: '| In-context GPT4v (spectrogram) | 55.7 | 0.56 | 25% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| In-context GPT4 (features) | 50.0 | 0.57 | 97% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot GPT4v (spectrogram) | 66.2 | 0.55 | 30% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot GPT4 (features) | - | - | 99% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost (features) | 91.3 | 0.93 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (majority vote) | 66.6 | 0.53 |  |'
  prefs: []
  type: TYPE_TB
- en: III-A2 Sleep stage detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [II](#S3.T2 "TABLE II ‣ III-A2 Sleep stage detection ‣ III-A User state
    detection ‣ III Results ‣ Exploration of LLMs, EEG and behavioral data to measure
    and support attention and sleep") summarizes sleep stage detection performance.
    LLM-based sleep detection shows lower performance than ML-based models. GPT 4
    vision models (both in-context learning and zero-shot learning) often return a
    message that they cannot assist with the task. In-context learning with EEG waveforms
    performs worse than zero-shot learning with EEG waveforms. In-context learning
    with EEG features seems more effective than the zero-shot LLM model with EEG features.
    GPT fine-tuned models show higher misclassification rates compared to XGBoost.
    For example, they misclassify wake as REM, stage 1 as stage 2, and REM as stage
    2\. However, stage 2 and stage 3 & 4 are similar (Fig.[2](#S3.F2 "Figure 2 ‣ III-A2
    Sleep stage detection ‣ III-A User state detection ‣ III Results ‣ Exploration
    of LLMs, EEG and behavioral data to measure and support attention and sleep")).
  prefs: []
  type: TYPE_NORMAL
- en: 'The fine-tuned GPT 3.5 model shows inference confidence from 20 to 100 % (mean:
    79.7 %, median 80%, stdev: 9.7%). The model outputs the explanation that it uses
    power spectrum and ratio for classifying sleep stages. For example, when the model
    infers correct stage 3&4 sleep, it outputs “The dominant frequency bands in the
    EEG signal are delta and theta, with high power in both bands. The alpha and beta
    power are relatively low. The alpha delta ratio is low, indicating a higher proportion
    of slow wave activity. The theta alpha ratio is high, suggesting a predominance
    of theta activity. The delta theta ratio is also high, indicating a higher proportion
    of slow wave activity compared to theta activity. These features are indicative
    of stage 3 or 4 sleep, also known as slow wave sleep.”'
  prefs: []
  type: TYPE_NORMAL
- en: The important features extracted from the XGBoost model show kurtosis, mean,
    standard deviation, and 90th percentile amplitude in addition to power spectrum
    and ratio features.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Sleep Stage Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | acc | F1 | note |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Finetuned GPT3.5 (features) | 50.6 | 0.46 |  |'
  prefs: []
  type: TYPE_TB
- en: '| In-context GPT4v (EEG signals) | 25.4 | 0.21 | 51% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| In-context GPT4 (features) | 44.7 | 0.40 | 8% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot GPT4v (EEG signals) | 19.2 | 0.19 | 13% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot GPT4 (features) | 29.0 | 0.25 | 0.2% “cannot assist” |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost (features) | 75.6 | 0.74 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (majority vote) | 37.2 | 0.20 |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/b2def9bf89b1fae91c6e1d8c4766c880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Sleep stage detection models: confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Sleep quality detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare sleep quality detection (good vs poor sleep; confidence levels)
    using two approaches: (1) scored by PSQI standard scoring procedure vs. 2) GPT4
    (Table [III](#S3.T3 "TABLE III ‣ III-A3 Sleep quality detection ‣ III-A User state
    detection ‣ III Results ‣ Exploration of LLMs, EEG and behavioral data to measure
    and support attention and sleep")). Overall accuracy of the GPT4 model is 85.7%.
    There are 11 error cases out of 77\. GPT-4 demonstrates relatively good sleep
    quality detection performance based on textual answers to PSQI; however, GPT-4
    struggles to recognize sleep quality accurately when participants show poor sleep
    behavior (e.g., short sleep, difficulty falling asleep, or disrupted sleep) but
    good self-reported sleep quality (inference confidence is also low (60-65 %).
    GPT seems to emphasize subjective sleep quality over answers to other PSQI questions.
    Confidence levels of the GPT fine-tuned model are 60-100 % (mean: 80% and std:
    7.9 %). The confidence level is high when self-reported sleep quality is consistent
    with sleep behaviors (e.g. very bad self-reported sleep quality and poor sleep
    related behaviors such as long sleep latency, awakenings during the night, bad
    dreams, pain, feeling too cold, worry, and difficulty maintaining enthusiasm for
    daily activities).'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs detect sleep and active periods and regular sleep patterns from the physical
    activity actograms and averaged graphs provided; however, the image data fed into
    LLMs are recognized as poor sleep. LLMs easily pick up some physical activities
    during the night time that occurred only a few nights out of 60 days of data and
    recognize them as a sign of poor sleep. For instance, when the actograms show
    increased activity after 5 am for a few days out of 60 days of data, suggesting
    potential wakefulness, LLMs misinterpret this as poor sleep quality. This implies
    GPT-4’s limitations about the challenge of understanding a full range of human
    sleep behavior and variabilities. Sleep quality is multifaceted and LLMs might
    not fully grasp the nuances.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Sleep Quality Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| PSQI scoring | GPT4 response | # of participants |'
  prefs: []
  type: TYPE_TB
- en: '| good | good | 39 |'
  prefs: []
  type: TYPE_TB
- en: '| poor | poor | 27 |'
  prefs: []
  type: TYPE_TB
- en: '| good | poor | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| poor | good | 11 |'
  prefs: []
  type: TYPE_TB
- en: III-B Sleep Improvement Feedback Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs modify suggestions and guided imagery scripts based on user input and profiles
    (e.g., symptoms, user preferences). Tables [V](#S3.T5 "TABLE V ‣ III-B Sleep Improvement
    Feedback Generation ‣ III Results ‣ Exploration of LLMs, EEG and behavioral data
    to measure and support attention and sleep") and [IV](#S3.T4 "TABLE IV ‣ III-B
    Sleep Improvement Feedback Generation ‣ III Results ‣ Exploration of LLMs, EEG
    and behavioral data to measure and support attention and sleep") show examples
    of sleep improvement suggestions and guided imagery scripts based on user profiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs generate sleep improvement suggestions that align with cognitive behavioral
    therapy for insomnia (CBT-i)[[15](#bib.bib15)]: (1) consistent sleep schedule,
    (2) avoid late night activities, (3) avoid fluids close to bedtime, (4) try relaxation
    techniques to help fall asleep and improve sleep quality, (5) restful room environment
    (light, temperature), (6) reduce caffeine intake, large meals, alcohol before
    bed, and (7) regular exercise. LLMs also weave user profiles (e.g. what they like,
    and issues they have) into generated guided imagery scripts and change scenes
    and phrases.'
  prefs: []
  type: TYPE_NORMAL
- en: When we feed EEG features for generating guided imagery scripts, if we just
    simply ask LLM to generate a script based on EEG features, LLM incorporates numerical
    information into guided imagery scripts such as ”As you sink deeper into this
    state of relaxation, there is a harmonious balance between theta and alpha waves
    with a ratio of 2.3\. ” For users, hearing specific numerical information about
    EEG features might not be useful because they might not know how those numbers
    are associated with their states, therefore, we adjust prompts not to include
    numerical information in the generated guided imagery script (Table [V](#S3.T5
    "TABLE V ‣ III-B Sleep Improvement Feedback Generation ‣ III Results ‣ Exploration
    of LLMs, EEG and behavioral data to measure and support attention and sleep"),
    Appendix for a prompt example). Further tuning of prompt design and evaluation
    are required to generate effective adaptive feedback. Sleep improvement suggestions
    do not change based on EEG features because suggestions are not based on momentary
    EEG features but habitual sleep related behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: GPT4-generated sleep improvement suggestions'
  prefs: []
  type: TYPE_NORMAL
- en: '| Profiles | LLM-generated Suggetsions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Headaches disturb sleep |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Check with a medical professional about the headaches to see if they
    can be treated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trying to solve coding problems in bed |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Avoid activities such as coding which could generate stress and disrupt
    sleep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bed feels uncomfortable |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Maybe consider investing in a new mattress or bedding if your current
    one feels uncomfortable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stress |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Engagement in stress-relieving activities such as yoga, mindfulness,
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and deep-breathing can help in reducing stress levels which can improve
    sleep quality &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| OCD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Consultation with a Sleep Specialist or mental health professional may
    also be beneficial, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; especially since OCD was mentioned as a reason for troubled sleep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: GPT4-generated guided imagery scripts'
  prefs: []
  type: TYPE_NORMAL
- en: '| Profiles | LLM-generated Guided Imagery Scripts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Like Beach |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Close your eyes and imagine yourself on a peaceful beach; Begin with
    lying down in a comfortable position. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Close your eyes and take a couple of deep breaths, inhaling and exhaling
    gradually. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Visualize the calming view of a beach, the sand beneath your feet, the
    tranquil waves that are rhythmically kissing the shore. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feel the warmth of the sun on your skin, the softness of the sand as
    it molds perfectly to your feet. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hear the distant sound of seagulls, the gentle lull of the waves. Breathe
    in the refreshing, salty sea air. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; With every breath you take, your body feels lighter and sleepier. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Let this calming beach imagery help to lower your heart rate and help
    you drift off to sleep. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Like Family |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Close your eyes and imagine you’re sitting in a cozy, comfortable family
    room. The room is warm, inviting, and filled with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; soft, dimmed light that creates a soothing atmosphere; This scene prompts
    feelings of relaxation, comfort, and safety, associated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with being surrounded by loved ones. You see all your loved ones gathered,
    chatting, and enjoying each other’s company. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; There’s a gentle hum of relaxed conversation and soft laughter, the
    sounds of your family’s voices proving to be the most &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; calming sounds you’ve ever heard; Hearing familiar voices has been shown
    to reduce stress and anxiety, allowing your &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mind to unwind and merge into a relaxation state suitable for sleep.
    You feel an overwhelming sense of love, joy, and peace &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; as you sit there surrounded by the people who matter the most. Outside
    the room, you see through the window that it’s &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; nighttime with a sky full of stars and the moon shining brightly, creating
    a perfect scenery indicating it’s time for you to sleep…. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| PTSD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Imagine you’re in a beautiful, peaceful forest; Begin by closing your
    eyes and taking several deep breaths, in through the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; nose and out through the mouth. As you continue these slow, deep breaths,
    begin to visualize yourself standing at the edge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of a beautiful, calm forest….This forest is your safe space, a haven
    untouched by past trauma. You can stay here as long as &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; you need, resting in the peaceful ambiance. No harm can find you here.
    Continue to cultivate this sense of peace and safety &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with each deep, calming breath…. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Insomnia |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Imagine yourself in a serene, beautiful forest; As insomnia has been
    bothering you, positive visualization can aid in releasing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; negative energy and reduce stress…. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Shift workers |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Relaxing Beach Journey; Begin by making yourself comfortable. Slowly
    take a deep breath in and as you exhale, close your &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eyes. …..For a shift worker, building a regular sleep schedule can be
    tough. Guided imagery can aid in achieving a relaxed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; state quicker once in bed, making the most of the sleep time available…
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| EEG features |'
  prefs: []
  type: TYPE_TB
- en: '&#124; You are now entering the first stage of sleep, a stage of light sleep.
    This slow transition into sleep is marked by the gentle &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; wave-like rhythm of your alpha and theta powers. Your consciousness
    begins to dissolve into a calm and cloud-like tranquility… &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: IV Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work explores the usage of LLMs and physiological and behavioral sensor
    data for attention and sleep detection and sleep improvement. Our experiments
    highlight both LLMs’ strengths and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based attention and sleep detection exhibit lower performance compared to
    traditional ML models. Fine-tuned LLMs improve models’ ability to handle diverse
    contexts. However, we also found limitations. The fine-tuned GPT3.5 model uses
    limited features (e.g. power spectrum density and ratio) for classification even
    after fine-tuning and GPT-4 vision models fail to handle visual input. Also, feeding
    high dimensional numerical features such as EEG features to LLMs does not reliably
    estimate user states. To improve human state detection, LLM’s knowledge needs
    to be extended beyond simple one-on-one relationships such as increased alpha
    band activity during unfocused state and LLMs require further refinement to handle
    diverse human physiological and behavioral data, variabilities, and patterns effectively.
    This might be possible using fine-tuning with larger datasets and retrieving external
    sources of knowledge. In addition, textual knowledge might not fully capture complex
    physiological and behavioral patterns; therefore, integrating textual information
    with numerical and visual data is essential in order to understand variability
    within and across individuals; however, the capacity of current LLM vision models
    and fine-tuning is still limited and LLMs requires much diverse data to enhance
    the knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based sleep improvement suggestions and guided imagery scripts are personalized
    and adaptive to user profiles. Automatically generated suggestions and scripts
    have a potential for AI-based conversational systems or intervention systems after
    effectiveness and safety are carefully tested.
  prefs: []
  type: TYPE_NORMAL
- en: There are several limitations in this study. First, this study is an early exploration
    with limited datasets and limited LLMs. Refining prompts and using large and diverse
    datasets might help enhance task performance. We intentionally use interpretable
    features rather than low dimensional embeddings to test the extent to which LLM
    internal knowledge contributes to altered state detection and improvement. Our
    study only relies on public datasets and offline experiments; therefore, user
    studies with end users and clinicians are necessary to evaluate generative responses
    in terms of accuracy, effectiveness, and safety. Lastly, beyond LLMs, there are
    other advanced approaches such as transformer models and multimodal learning for
    detecting user states.
  prefs: []
  type: TYPE_NORMAL
- en: We also discuss the ethical considerations of using LLMs to detect and improve
    human-altered states. First, feeding personal physiological and behavioral data
    to LLMs could raise privacy concerns. Users might worry about the security and
    confidentiality of their sensitive information. Transparent consent processes
    and reliable data anonymization are important. Second, LLMs are trained on massive
    data that might contain biases. Bias detection and mitigation strategies are necessary
    to ensure fair outcomes. LLMs might have the capability to generate unethical,
    harmful, or inaccurate content or manipulate individuals. Implementing guidelines
    for responsible use and monitoring LLM-generated content are required.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] D. J. Buysse, “Sleep health: can we define it? does it matter?” *Sleep*,
    vol. 37, no. 1, pp. 9–17, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. A. Horne and L. A. Reyner, “Sleep related vehicle accidents,” *Bmj*,
    vol. 310, no. 6979, pp. 565–567, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. R. Rosekind, K. B. Gregory, M. M. Mallis, S. L. Brandt, B. Seal, and
    D. Lerner, “The cost of poor sleep: workplace productivity loss and associated
    costs,” *Journal of Occupational and Environmental Medicine*, vol. 52, no. 1,
    pp. 91–98, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] R. Richer, N. Zhao, J. Amores, B. M. Eskofier, and J. A. Paradiso, “Real-time
    mental state recognition using a wearable eeg,” in *2018 40th Annual International
    Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE,
    2018, pp. 5495–5498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. Baltrušaitis, D. McDuff, N. Banda, M. Mahmoud, R. El Kaliouby, P. Robinson,
    and R. Picard, “Real-time inference of mental states from facial expressions and
    upper body gestures,” in *2011 IEEE international conference on automatic face
    & gesture recognition (FG)*.   IEEE, 2011, pp. 909–914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Vidal, J. Turner, A. Bulling, and H. Gellersen, “Wearable eye tracking
    for mental health monitoring,” *Computer Communications*, vol. 35, no. 11, pp.
    1306–1311, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. A. Imtiaz, “A systematic review of sensing technologies for wearable
    sleep staging,” *Sensors*, vol. 21, no. 5, p. 1562, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. C. Shin, J. Kim, and D. Grigsby-Toussaint, “Mobile Phone Interventions
    for Sleep Disorders and Sleep Quality: Systematic Review,” *JMIR mHealth and uHealth*,
    vol. 5, no. 9, p. e7244, Sep. 2017, company: JMIR mHealth and uHealth Distributor:
    JMIR mHealth and uHealth Institution: JMIR mHealth and uHealth Label: JMIR mHealth
    and uHealth Publisher: JMIR Publications Inc., Toronto, Canada. [Online]. Available:
    https://mhealth.jmir.org/2017/9/e131'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] J. W. Kim, A. Alaa, and D. Bernardo, “Eeg-gpt: Exploring capabilities of
    large language models for eeg classification and interpretation,” *arXiv preprint
    arXiv:2401.18006*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Kim, X. Xu, D. McDuff, C. Breazeal, and H. W. Park, “Health-llm: Large
    language models for health prediction via wearable sensor data,” *arXiv preprint
    arXiv:2401.06866*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ç. İ. Acı, M. Kaya, and Y. Mishchenko, “Distinguishing mental attention
    states of humans via an eeg-based passive bci using machine learning methods,”
    *Expert Systems with Applications*, vol. 134, pp. 153–166, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. Kemp, A. Zwinderman, B. Tuk, H. Kamphuisen, and J. Oberye, “Analysis
    of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of
    the eeg,” *IEEE Transactions on Biomedical Engineering*, vol. 47, no. 9, pp. 1185–1194,
    2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] R. Wang, F. Chen, Z. Chen, T. Li, G. Harari, S. Tignor, X. Zhou, D. Ben-Zeev,
    and A. T. Campbell, “Studentlife: assessing mental health, academic performance
    and behavioral trends of college students using smartphones,” in *Proceedings
    of the 2014 ACM international joint conference on pervasive and ubiquitous computing*,
    2014, pp. 3–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Utay and M. Miller, “Guided imagery as an effective therapeutic technique:
    A brief review of its history and efficacy research.” *Journal of Instructional
    Psychology*, vol. 33, no. 1, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. G. Hofmann, A. Asnaani, I. J. Vonk, A. T. Sawyer, and A. Fang, “The
    efficacy of cognitive behavioral therapy: A review of meta-analyses,” *Cognitive
    therapy and research*, vol. 36, pp. 427–440, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt and LLM response examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zero-shot learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '“You are a data analyst who reviews EEG data and helps interpret participant’s
    conditions. Please review the power spectrum data of EEG data. The data has an
    x-axis of time and a y-axis of frequency (0-40 Hz). The data is collected from
    healthy participants who went through cognitive tasks. We would like to classify
    the user state into 2 categories: focused or unfocused, confidence level (0-100%),
    and explanations. Please return your response in JSON format.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-context learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Please review the following 6 EEG data. The data has an x axis of time and
    a y axis of amplitude. The data is collected from healthy participants who went
    through sleep. The first 5 images were collected during wake state (W), stage
    1, stage 2, stage 3&4, and REM sleep. Please provide 1\. a description of each
    image 2\. estimate the sleep stage of the 6th (last) image and return a number
    from 0 to 5 (0:wake, 1: stage 1, 2: stage 2, 3: stage 3 or 4, 4: REM, 5: unknown
    or movement) 3\. confidence level (0-100%) 4\. explanations in JSON format.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sleep quality detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '“You are a data analyst helping a neurologist understand human sleep data.
    Task: This is the data collected from users who fill out the Pittsburgh Sleep
    Quality Index questionnaire. Based on this data, please give me the following
    information. (1) if this person has good sleep quality or poor quality (2) confident
    level (0-100%) (3) explanations for the decision. (4) suggestions to improve sleep
    quality.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sleep quality detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '“Please review the following graph made from smartphone activity sensor data.
    The data has an x-axis of time (midnight to midnight) and a y-axis of date. For
    each hour, we computed the mean of activities where 0: Stationary, 1: Walking,
    2:Running, 3:Unknown. Please describe (1) estimated sleep quality (good or poor)
    (2) confidence (0-100(3) explanations (4) suggestions to improve sleep quality
    in JSON format.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Suggestion to improve sleep/Guided Imagery
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: “You are a sleep therapist. The participant is [participant conditions/profiles].
    Reflecting on the participant’s profile, please generate suggestions (a guided
    imagery script) to help the participant sleep better.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sleep Guided Imagery using EEG features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: “You are a sleep therapist. Please generate a guided imagery script based on
    the following 30-s epoch EEG features, alpha, beta, delta, theta, gamma power,
    alpha delta ratio, theta alpha ratio, delta theta ratio, mean, standard deviation,
    kurtosis, 90th percentile of amplitude. Do not include actual numerical EEG features,
    brain waves, power spectrum, or different frequencies of waves in the script but
    include the state estimated from the features. ”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
