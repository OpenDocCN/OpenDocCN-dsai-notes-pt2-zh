- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:01'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.11420](https://ar5iv.labs.arxiv.org/html/2312.11420)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bingchen Zhao*¹ Haoqin Tu*² Chen Wei³  Jieru Mei³  Cihang Xie⁴
  prefs: []
  type: TYPE_NORMAL
- en: '*equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: ¹ University of Edinburgh    ² University of Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ³ Johns Hopkins University    ⁴ UC Santa Cruz
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper introduces an efficient strategy to transform Large Language Models
    (LLMs) into Multi-Modal Large Language Models. By conceptualizing this transformation
    as a domain adaptation process, *i.e*., transitioning from text understanding
    to embracing multiple modalities, we intriguingly note that, within each attention
    block, tuning LayerNorm suffices to yield strong performance. Moreover, when benchmarked
    against other tuning approaches like full parameter finetuning or LoRA, its benefits
    on efficiency are substantial. For example, when compared to LoRA on a 13B model
    scale, performance can be enhanced by an average of over 20% across five multi-modal
    tasks, and meanwhile, results in a significant reduction of trainable parameters
    by 41.9% and a decrease in GPU memory usage by 17.6%. On top of this LayerNorm
    strategy, we showcase that selectively tuning only with conversational data can
    improve efficiency further. Beyond these empirical outcomes, we provide a comprehensive
    analysis to explore the role of LayerNorm in adapting LLMs to the multi-modal
    domain and improving the expressive power of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have had many application scenarios since their
    debut. In particular, extending LLMs to handle multiple modalities has gathered
    much interest from both academia and industry. Such models, termed Multi-modal
    Large Language Models (MLLMs), are typically derived by finetuning a pretrained
    LLM on multi-modal data (Liu et al., [2023](#bib.bib21); Ye et al., [2023](#bib.bib38)).
    However, this process typically poses a substantial computational challenge (Liu
    et al., [2023](#bib.bib21)), particularly for exceptionally large-scale models.
    While Su et al. ([2023](#bib.bib29)); Zhang et al. ([2023](#bib.bib41)) employ
    low-rank adapters (LoRA) (Hu et al., [2022](#bib.bib12)) or soft prompts (Li &
    Liang, [2021a](#bib.bib16)) for more parameter-efficient tuning, this often comes
    at the cost of compromised performance on multi-modal tasks. This challenge prompts
    the pivotal question: *how can we make this process more efficient?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to this challenge, we introduce a simple and effective strategy
    for MLLM finetuning: as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")(a),
    within each attention block, we adjust only the weights of the LayerNorm (Ba et al.,
    [2016](#bib.bib2)). This strategy is underpinned by the understanding that the
    evolution from LLMs to MLLMs can be conceptualized as a domain adaptation process,
    *i.e*., transitioning from text-centric to multi-modal understanding. Adjusting
    normalization layers, as suggested by prior research, emerges as a particularly
    effective technique in such domain shifts (Li et al., [2016](#bib.bib18)). Empirically,
    this straightforward technique can surprisingly yield comparable or even better
    performance than the strong baseline of finetuning all parameters offer about
    $10\times$ more parameter efficiency than LoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b80eddc09ed01d3c8e932181049a04ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (left) Different tuning methods for MLLMs. Trainable components are
    in blue, while frozen parameters are in gray. Within the attention blocks, (a)
    only activates LayerNorm parameters. Note that vision-language connector, word
    embedding, and output head paramters are by default activated for all three options.
    (right) Comparison on trainable parameters and GPU memory. Tuning LayerNorm achieves
    significant reductions in trainable parameters and GPU memory usages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By delving deeper, we note that the process can be further simplified by designating
    LayerNorm as the sole trainable component within the entire model. This means,
    in contrast to the typical configurations depicted in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal
    LLM Finetuning")(a)-(c), we now freeze the standardly activated elements, including
    the vision-language connector, word embedding, and the output head. We term it
    as LayerNorm-simple. Impressively, despite constituting a mere 0.004% of trainable
    parameters, this configuration surpasses the performance of LoRA, registering
    an average enhancement of 4.3% across five benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of this LayerNorm strategy, we further improve the finetuning efficiency
    from the data perspective. Specifically, we assess the performance implications
    of different types of finetuning data, including conversational data, detailed
    description data, and complex reasoning data. Our results offer a crucial insight:
    not all data are created equal for the task of MLLM finetuning. Remarkably, we
    find that MLLMs finetuned on conversational data consistently outperform those
    finetuned on other data types. Specifically, conversational data improves the
    model performance by an average of 50% compared to other data types. This observation
    interestingly opens up avenues for more targeted data collection and curation
    strategies, thereby further optimizing the efficiency of MLLMs finetuning. Furthermore,
    by combining the LayerNorm strategy and this data perspective, we can achieve
    on average 10.0% performance improvement over full parameter finetuning on traditional
    VQA benchmarks with an LLaMA2 13B model while using significantly less parameters
    and data.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the empirical outcomes above, we conduct an investigation into the expressive
    power of LayerNorm tuning. Our analysis reveals that LayerNorm-tuned MLLMs exhibit
    lower cross-layer similarity compared to models all of which parameters are finetuned.
    This lowered similarity is indicative of a more expressive model, since the model
    incorporates anisotropic layer presentations can capture a wider range of learning
    patterns (Pires et al., [2023](#bib.bib26)). It stands to reason that this amplified
    expressiveness is a key factor underpinning the efficiency and superior performance
    we noted, granting the model enhanced adaptability to novel multi-modal datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, our findings illuminate the profound influence of LayerNorm tuning,
    suggesting its potential to adeptly harness the intrinsic properties of LLMs.
    We hope that this study will catalyze subsequent research endeavors focused on
    efficient multi-modal finetuning.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-Modal and Large Language Models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multi-modality has been extensively studied in the literature. Starting from
    learning aligned representation across image and text modalities like CLIP (Radford
    et al., [2021](#bib.bib27)), many works have proposed more techniques (Yu et al.,
    [2022](#bib.bib40); Mu et al., [2022](#bib.bib24); Zhao et al., [2023](#bib.bib42)).
    Given the interest in developing instruction-tuned LLMs (Ouyang et al., [2022](#bib.bib25)),
    the studies of multi-modal models have also shifted the focus to instruction-tuned
    MLLMs. For example, LLaVA (Liu et al., [2023](#bib.bib21)) pioneers the development
    of instruction-tuned MLLMs by designing instruction-tuning data with the help
    of GPT-4. The concurrent work MiniGPT4 (Zhu et al., [2023](#bib.bib45)) is built
    using QFormers (Li et al., [2023a](#bib.bib15)) and Vicuna (Zheng et al., [2023](#bib.bib43)),
    but with only a linear layer activated for tuning. Similar to MiniGPT4, Su et al.
    ([2023](#bib.bib29)) devises PandaGPT with a more advanced vision encoder and
    LoRA-tuned LLM as its base models. mPLUG-Owl (Ye et al., [2023](#bib.bib38)) mixes
    text-only and multi-modal instruction data for finetuning LLaMA model. InstructBLIP (Dai
    et al., [2023](#bib.bib5)) builds on the BLIP2 model (Li et al., [2023a](#bib.bib15))
    with additional finetuning on instruction tuning datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter-Efficient Finetuning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The parameter-efficient finetuning (PEFT) technique has been widely applied
    and studied because of huge resource consumption of larger and larger deep learning
    models. Adapter (Houlsby et al., [2019](#bib.bib11)) additionally adds trainable
    adapter components in the LM, which proved to achieve comparable results in NLP
    tasks with less than 10% trainable parameters. Prefix tuning (Li & Liang, [2021b](#bib.bib17))
    only inserts trainable parameters to the attention head in LMs LoRA (Hu et al.,
    [2022](#bib.bib12)), as the most widely employed PEFT method recently, injects
    trainable low rank decomposition matrices into a model to reduce the number of
    training parameters. Following the same line, QLoRA (Dettmers et al., [2023](#bib.bib7))
    achieves further reduction in the memory usage for finetuning LLMs with quantized
    4-bits parameters. These techniques have been widely utilized for the tuning of
    both LLMs (He et al., [2022](#bib.bib10); Tu et al., [2022](#bib.bib31)) and MLLMs (Zhu
    et al., [2023](#bib.bib45); Tu et al., [2023](#bib.bib32)). In this paper, we
    show that tuning LayerNorm in the LLM of an MLLM can achieve better results than
    tuning other components in the model and LoRA tuning, while requiring less resource
    for computation.
  prefs: []
  type: TYPE_NORMAL
- en: The Normalization Studies.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The normalization layer in neural networks has long been a subject for debate
    and study. The foundational work on batch normalization (Ioffe & Szegedy, [2015](#bib.bib13))
    first introduces normalization as an important part of neural network architectures,
    and argues the effectiveness of normalization comes from alleviating the internal
    covariant shifting problem. Later works have proposed many variants of normalization,
    such as InstanceNorm (Ulyanov et al., [2016](#bib.bib33)), GroupNorm (Wu & He,
    [2018](#bib.bib36)), and LayerNorm (Ba et al., [2016](#bib.bib2)). LayerNorm has
    been the design choice of LLMs for normalization, and its effectiveness in LLM
    pretraining has also been explored and discussed (Xu et al., [2019](#bib.bib37)).
    In this work, we explore the effectiveness of finetuning LayerNorm in MLLMs as
    well as the reason behind the effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Model performance on five multi-modal benchmarks with different components
    tuned in the LLM. We mark the best results with bold and the second best scores
    with underline. ‘-’ means the model cannot follow the required output format on
    captioning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | MME $\uparrow$ | MSCOCO $\uparrow$ | POPE $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline Models |'
  prefs: []
  type: TYPE_TB
- en: '| mPLUG-Owl | 967.4/276.1 | 21.38 | 70.70 | 41.78 | 50.9/54.0/50.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MiniGPT4 | 866.6/292.1 | 17.30 | - | - | 69.7/79.7/65.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA-v0 | 502.8/214.6 | 15.06 | 58.89 | 23.02 | 70.5/74.6/66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-Vicuna-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune | 625.2/270.7 | 15.40 | 67.50 | 34.61 | 73.8/76.5/66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 552.3/217.5 | 15.00 | 63.93 | 34.13 | 50.4/51.6/50.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. QV Proj. | 678.0/277.5 | 15.51 | 72.63 | 32.24 | 72.0/77.1/65.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. MLP | 637.3/268.2 | 15.37 | 65.22 | 37.47 | 60.0/68.2/56.6 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 723.2/253.2 | 17.06 | 80.89 | 48.01 | 76.1/81.1/70.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-simp. | 720.9/251.8 | 23.46 | 79.75 | 46.18 | 61.1/72.3/58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune | 661.3/237.1 | 16.09 | 65.08 | 31.64 | 56.3/65.0/55.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 395.0/200.0 | 14.87 | 61.97 | 26.17 | 51.9/54.7/51.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. QV Proj. | 584.0/222.9 | 16.39 | 76.05 | 42.93 | 55.7/63.0/56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. MLP | 413.1/203.6 | 15.29 | 58.35 | 29.04 | 53.7/59.6/53.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-simp. | 542.6/205.0 | 14.98 | 65.10 | 46.88 | 51.6/52.5/51.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-chat-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune | 805.4/234.6 | 15.29 | 66.33 | 26.70 | 60.3/69.8/57.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 709.8/228.6 | 15.28 | 57.27 | 25.49 | 59.2/65.9/56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. QV Proj. | 926.5/220.7 | 15.88 | 58.49 | 31.10 | 68.5/77.3/65.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. MLP | 840.0/240.0 | 15.20 | 54.42 | 24.89 | 56.9/67.3/56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 651.3/219.3 | 16.60 | 75.34 | 43.75 | 71.3/72.4/67.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-simp. | 372.0/169.3 | 18.42 | 59.99 | 41.63 | 52.0/54.6/52.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune | 402.3/199.3 | 18.33 | 73.88 | 45.33 | 51.6/51.1/52.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 400.8/189.3 | 16.08 | 68.83 | 43.70 | 50.5/51.2/50.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. QV Proj. | 489.0/200.4 | 15.12 | 63.07 | 32.81 | 51.1/52.9/52.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. MLP | 387.5/167.1 | 25.19 | 64.19 | 44.06 | 50.7/52.2/51.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-simp. | 403.3/185.4 | 18.62 | 68.28 | 43.04 | 55.3/58.0/57.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-chat-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune | 623.3/221.4 | 15.17 | 64.19 | 41.82 | 67.6/64.8/64.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 516.7/214.3 | 14.39 | 66.33 | 43.09 | 66.9/64.1/63.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. QV Proj. | 624.5/250.4 | 14.91 | 60.96 | 34.90 | 66.3/66.0/61.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. MLP | 456.7/211.4 | 14.67 | 62.19 | 40.39 | 56.8/56.9/56.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 929.3/254.3 | 16.10 | 74.96 | 42.79 | 78.9/83.9/74.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-simp. | 824.3/221.1 | 13.29 | 52.70 | 40.20 | 73.3/76.0/69.0 |'
  prefs: []
  type: TYPE_TB
- en: 3 Tuning and Evaluation Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the comment structure of MLLMs, different
    tuning strategies of MLLMs, and then present the evaluation benchmarks employed
    in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of MLLMs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A typical MLLM usually contains three parts: 1) A vision encoder for extracting
    visual features; 2) An LLM decoder for generating plausible texts from both text
    instructions and visual information; 3) A vision-language connector for bridging
    between the vision encoder and the LLM. We follow Liu et al. ([2023](#bib.bib21))
    to set up the model, where the vision encoder is a CLIP-pretrained ViT-L (Radford
    et al., [2021](#bib.bib27)) and the vision-language connector is a simple linear
    projector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We experiment with a range of LLMs for the language decoder. Specifically,
    we choose three types of LLM with 7B and 13B scales: Vicuna-7B (v1.1) (Zheng et al.,
    [2023](#bib.bib43)), LLaMA2-7B&13B, and LLaMA2-chat-7B&13B (Touvron et al., [2023](#bib.bib30)).'
  prefs: []
  type: TYPE_NORMAL
- en: Baseline Models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Apart from our trained MLLMs, we showcase the performances of three publicly
    available models: mPLUG-Owl (Ye et al., [2023](#bib.bib38)), MiniGPT4 (Zhu et al.,
    [2023](#bib.bib45)), and LLaVA-v0 (Liu et al., [2023](#bib.bib21)). The LLaVA-v0
    here represents the initial release of LLaVA. These baseline results are obtained
    from existing literature (Fu et al., [2023](#bib.bib9); Li et al., [2023b](#bib.bib19))
    or tested using their released checkpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Modules.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To analyze the effects of different tuning components in the MLLMs, we employ
    five different tuning paradigms on the same training corpus. (1) finetune: activates
    all the parameters in LLM for MLLMs tuning; (2) LoRA: inserts LoRA component (Hu
    et al., [2022](#bib.bib12)) with rank 32 between all linear structure in the LLM;
    (3) Attn. QV Proj.: activates Q and V linear projection in attention of the LLM,
    as they are proved to be especially effective for tuning LLMs (Hu et al., [2022](#bib.bib12));
    (4) Attn. MLP: activates MLP layers in attention of the LLM; (5) LayerNorm: both
    input and post LayerNorm in attention blocks of the LLM. Note that all tuning
    methods activate vision-language connector for training.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Details.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We pre-train the vision-language connector for 3 epochs on CC3M (Changpinyo
    et al., [2021](#bib.bib3)), and conduct the finetuning stage on 80K filtered image-text
    pairs collected by Liu et al. ([2023](#bib.bib21)) for 1 epoch. For the first
    stage, we set the learning rate to 2e-3 for all variants. During the second stage,
    we search the learning rate from 2e-3 to 1e-7 with 11 options for all tuning strategies
    and pick the best learning rate based on their performances on Flickr30k task.
    We set the weight decay (Loshchilov & Hutter, [2019](#bib.bib23)) to 0 and a warmup
    ratio to 0.03 with the cosine learning rate scheduler (Loshchilov & Hutter, [2017](#bib.bib22)).
    Moreover, we employ the gradient checkpointing (Chen et al., [2016](#bib.bib4)),
    DeepSpeed technique (Rajbhandari et al., [2020](#bib.bib28)), and a data precision
    of TensorFloat32 for models training. We conduct all of our experiments on 4 80G
    A100 GPUs on the same node.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Modal Benchmarks.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We test the visual-instruction tuned models on recent multi-modal evaluation
    benchmarks, where five multi-modal benchmarks are deployed: MME (Fu et al., [2023](#bib.bib9))
    consists of two evaluation aspects, *i.e*., cognition (CS) and perception (PS)
    with total 14 VQA tasks; VQAv2 (Antol et al., [2015](#bib.bib1)), MSCOCO (Lin
    et al., [2014](#bib.bib20)) and Flickr30k (Young et al., [2014](#bib.bib39)) captioning
    tasks are commonly used benchmarks in the field of VQA and captioning. The former
    two benchmarks are based on MSCOCO-2017 dataset (Lin et al., [2014](#bib.bib20)).
    For the latter two captioning tasks, we report the zero-shot CIDEr (Vedantam et al.,
    [2015](#bib.bib34)) scores (with three text-only QA examples) on the test set
    from Karpathy & Fei-Fei ([2015](#bib.bib14)). POPE (Li et al., [2023b](#bib.bib19))
    is used to evaluate the level of object hallucinations in MLLMs, which consists
    of three versions of balanced yes/no VQA tasks (*i.e*., Popular/Random/Adversarial)
    considering objects in the given image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Memory consumption and percentages of trainable parameters tested
    on a single A100 GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | 7B scale | 13B scale |'
  prefs: []
  type: TYPE_TB
- en: '| Mem. (GB) | #param. | Mem. (GB) | #param. |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune | OOM | 95.70% | OOM | 97.72% |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 29.4 | 5.92% | 46.5 | 4.30% |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. QV Proj. | 57.0 | 19.02% | OOM | 18.24% |'
  prefs: []
  type: TYPE_TB
- en: '| Attn. MLP | OOM | 65.21% | OOM | 66.24% |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 24.2 | 3.78% | 38.3 | 2.50% |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-simp. | 18.9 | 0.004% | 31.7 | 0.003% |'
  prefs: []
  type: TYPE_TB
- en: 4 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Tuning LayerNorm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tuning LayerNorm in Attention Blocks.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [table 1](#S2.T1 "In The Normalization Studies. ‣ 2 Related Works ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"), it is
    noteworthy that activating only the LayerNorm yields the least activated parameters,
    yet the model performances are surprisingly impressive when compared to tuning
    other modules. Specifically, in two captioning tasks, the VQAv2 task, and the
    challenging hallucination benchmark POPE, models with only the LayerNorm activated
    consistently outperform all other competitors by at least 8.2%. On the comprehensively
    evaluated benchmark MME, while tuning LayerNorm outperforms finetuning the intact
    language model by an average of 6.6% on the Perception aspect, it lags behind
    finetuning by an average of 6.3% on the Cognition score. It is vital to note,
    however, that the LayerNorm only accounts for approximately 2.5% of the training
    parameters in the whole model.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to tuning modules, another observation is that MLLMs incorporating
    human-aligned LLMs (such as LLaMA2-chat) exhibit superior performance in complex
    and demanding tasks such as POPE and MME compared to their unaligned counterparts.
    This underscores the importance of utilizing aligned LLMs to construct a more
    powerful MLLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning LayerNorm and Only LayerNorm.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As the above LayerNorm method finetunes (1) vision-language connector, (2)
    word embedding, (3) output head, and (4) LayerNorm component in the LLM simultaneously,
    a pertinent question arises: *Is it possible for (4) LayerNorm alone to generalize
    effectively in training MLLMs?* To address this query, we take a step further
    and solely finetune LayerNorm in MLLMs, which is denoted as LayerNorm-simp. in [table 1](#S2.T1
    "In The Normalization Studies. ‣ 2 Related Works ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"). The results are intriguing, demonstrating
    that even with a mere 0.004% parameter finetuning in the whole model, LayerNorm-simp.
    surpasses full parameter finetuning on three conventional vision-language tasks
    (i.e., two captioning and one VQA tasks) by 10%, and only lags behind full finetuning
    by 7.9% on the MME benchmark. This intriguing discovery suggests that the transition
    from LLM to MLLMs probably involves a domain adaptation process as the LayerNorm
    takes the most credits in tuning a well-behaved MLLMs. The LayerNorm alone may
    be also capable of integrating vision information with language tokens seamlessly.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory Consumption and Parameter Efficiency.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In table [2](#S3.T2 "Table 2 ‣ Multi-Modal Benchmarks. ‣ 3 Tuning and Evaluation
    Settings ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"),
    we present the total memory consumption and the percentage of trainable parameters
    of each MLLMs finetuning method across 7B and 13B scales. Methods like full parameter
    finetuning and finetuning MLPs in attention modules face out-of-memory (OOM) issue
    even on a high-capacity 80GB A100 GPU, while LayerNorm based methods stand out
    for their efficiency. Specifically, LayerNorm tuning requires only 24.2 GB and
    38.3 GB memory at 7B and 13B scales respectively. Remarkably, LayerNorm-simp.
    further reduces the memory to 18.9 GB and 31.7 GB. In terms of trainable parameters,
    LayerNorm based methods also show remarkable efficiency, LayerNorm utilizes only
    3.78% and 2.50% of the total parameters at the 7B and 13B scales, and LayerNorm-simp.
    takes efficiency to an extreme, involving only 0.004% and 0.003% of the parameters
    at these scales. These results demonstrate the efficiency advantage of LayerNorm
    tuning, compared with existing methods like LoRA or full parameter finetuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0718d25657b340190137f6e469a263bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Performances of models that are finetuned on different datasets on
    four multi-modal benchmarks. The MME score is the sum of both Cognition and Perception
    scores on the benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Model performance on different data types. Methods with 80K and Conv.20K
    suffix are tuned on the full 80K data and the 20K conversational data, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
  prefs: []
  type: TYPE_TB
- en: '| MM-Vicuna-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-80K | 625.2/270.7 | 15.40 | 67.50 | 34.61 | 73.8/76.5/66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-80K | 723.2/253.2 | 17.06 | 80.89 | 48.01 | 76.1/81.1/70.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-Conv. 20K | 777.1/231.4 | 15.39 | 67.30 | 40.33 | 75.2/79.2/68.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-80K | 661.3/237.1 | 16.09 | 65.08 | 31.64 | 56.3/65.0/55.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-80K | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-Conv. 20K | 376.2/157.5 | 16.19 | 86.80 | 44.88 | 50.5/50.7/50.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-chat-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-80K | 805.4/234.6 | 15.29 | 57.40 | 26.70 | 60.3/69.8/57.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-80K | 651.3/219.3 | 16.60 | 75.34 | 43.75 | 71.3/72.4/67.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-Conv. 20K | 482.9/172.1 | 13.88 | 66.85 | 41.95 | 62.7/71.7/61.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-80K | 402.3/199.3 | 18.33 | 73.88 | 45.33 | 51.6/51.1/52.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-80K | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-Conv. 20K | 646.0/242.9 | 16.01 | 76.50 | 44.86 | 70.0/76.9/68.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-chat-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-80K | 623.3/221.4 | 15.17 | 64.19 | 41.82 | 67.6/64.8/64.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-80K | 929.3/254.3 | 16.10 | 74.96 | 42.79 | 78.9/83.9/74.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-Conv. 20K | 769.7/227.5 | 15.57 | 73.30 | 43.08 | 68.2/72.8/65.3
    |'
  prefs: []
  type: TYPE_TB
- en: 4.2 ‘Less is More’ on Both Data and Parameter Sides
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Efficiency in training can also be improved by considering the data used in
    LLMs and MLLMs (Zhou et al., [2023](#bib.bib44); Wei et al., [2023](#bib.bib35)).
    To this end, we conducted experiments using LLaMA2-7B and LLaMA2-7B-chat, where
    we divided the training data into three categories, each comprising 20K data points:
    image-grounded conversation, image detail descriptions, and image-based complex
    reasoning, as previously deployed in Liu et al. ([2023](#bib.bib21)). Based on
    the results presented in [fig. 2](#S4.F2 "In Memory Consumption and Parameter
    Efficiency. ‣ 4.1 Tuning LayerNorm ‣ 4 Experimental Results ‣ Tuning LayerNorm
    in Attention: Towards Efficient Multi-Modal LLM Finetuning"), we observe that
    the image-grounded conversation data is the most effective in enhancing the multi-modal
    capabilities of the model, with an average improvement of over 50% compared to
    other data types. This highlights the potential benefits of a targeted approach
    that leverages the strengths of specific data types to facilitate more nuanced
    and effective multi-modal tuning for MLLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate ‘Less is More’ on both the data and parameter sides, we present
    results of MLLMs with LayerNorm activated in LLM and tuned on 20k conversational
    data in [table 3](#S4.T3 "In Memory Consumption and Parameter Efficiency. ‣ 4.1
    Tuning LayerNorm ‣ 4 Experimental Results ‣ Tuning LayerNorm in Attention: Towards
    Efficient Multi-Modal LLM Finetuning"). Our experimental results indicate that
    even with a smaller dataset and the use of LayerNorm tuning, the model outperforms
    the full parameter finetuning approach on the full 80K dataset by 18.4% on two
    captioning tasks, and only falls short in MME by a tolerable 2.5%. It is noteworthy
    that LayerNorm with 20K data is only 7.6% and 7.4% behind LayerNorm on the full
    80K dataset for two captioning tasks and MME task, respectively. These findings
    demonstrate that ‘Less is More’ for both the parameter and data perspectives beyond
    language domain Zhou et al. ([2023](#bib.bib44)), but for multi-modal tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Intuitions Behind LayerNorm Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, driven by the empirical success of LayerNorm tuning, we explore
    the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive
    power, and gradient variance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Influence of the Vision-Language Connector
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The vision-language connector serves as the converter to project features from
    the vision encoder to the LLM domain. In our previous experiments, we focused
    on finetuning the LLM component of the MLLMs while keeping the vision-language
    connector activated by default. To determine which component plays a more important
    role for domain adaptation of LLM to multi-modal domain, we performed an ablation
    study by activating the two components separately. Results are presented in [table 4](#S5.T4
    "In Influence of the Vision-Language Connector ‣ 5.1 LayerNorm Tuning Adapts LLMs
    to Multi-Modal ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), tuning LayerNorm in attention
    blocks without activating the vision-language connector resulted in only a 4.2%
    and 5.4% decrease in performance on three traditional multi-modal tasks and the
    MME benchmark, respectively. This decrease is significantly lower than the 15.6%
    and 9.2% downgrade observed when only activating the Connector on the same tasks.
    This observation highlights the vital role LayerNorm plays in transforming knowledge
    from the vision domain to language, indicating LayerNorm as a strong domain adaptor
    for the LLM architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Results of models with LayerNorm and/or vision-language Connector
    activated.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-7B |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm + Connector | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Connector | 311.1/105.4 | 12.72 | 60.43 | 35.91 | 67.9/73.7/66.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 395.0/191.4 | 18.18 | 80.13 | 41.68 | 50.3/51.3/50.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm + Connector | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Connector | 507.0/187.9 | 15.22 | 62.60 | 25.13 | 60.9/66.8/60.1 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm | 405.0/188.6 | 16.51 | 70.41 | 39.86 | 50.9/52.7/51.0 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/e055dff09de54808c5a1860de0cf7ace.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Layer similarities between different LLM layers in (a) Finetuned
    and (b) LayerNorm-tuned MM-Vicuna-7B. The average layer similarity of two models
    are 0.624 and 0.585, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Switching Visual Features.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We employ the ViT encoder from CLIP (Radford et al., [2021](#bib.bib27)) by
    default in our previous experiments. CLIP (Radford et al., [2021](#bib.bib27))
    models are trained with image-text contrastive loss, thus its feature space is
    already aligned with language. Since LayerNorm has shown its effectiveness as
    a domain adaptor, we are interested in testing whether or not LayerNorm tuning
    can adapt a LLM to image features that are not pretrained to align with language.
    The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy
    et al., [2021](#bib.bib8); Deng et al., [2009](#bib.bib6)). Results in [table 5](#S5.T5
    "In Switching Visual Features. ‣ 5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal
    ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention: Towards
    Efficient Multi-Modal LLM Finetuning") demonstrate that both LayerNorm and finetuning
    approaches can yield high performance. Interestingly, we observe that by LayerNorm
    tuning with ImageNet trained ViT, which has not been aligned with language, the
    model is able to achieve comparable performance to full parameter finetuning ,
    *i.e*., results show that LayerNorm tuning outperforms finetuning by 12.0% on
    captioning tasks, but performs slightly worse by 5.0% on the MME benchmark. These
    results again indicates the domain adaptor role of the LayerNorm , hinting the
    reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth
    noting that the performance of MLLMs incorporating ViT pretrained on ImageNet
    is generally inferior to that of CLIP’s vision encoder. This observation provides
    compelling evidence that, despite differences in tokenizer and training paradigm
    between CLIP’s text encoder and LLaMA’s, ViT from CLIP has the capacity to learn
    general patterns of language formulation during pre-training. Thus, significantly
    enhance MLLM abilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Results of models with LLaMA2 Finetuned/LayerNorm-tuned with ViT pre-trained
    on ImageNet (Deng et al., [2009](#bib.bib6)), which have not been aligned with
    the language domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-7B | 406.79/182.5 | 15.05 | 47.75 | 18.97 | 50.0/51.6/50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-7B | 301.51/127.14 | 15.48 | 66.22 | 31.73 | 50.0/50.1/50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Finetune-13B | 375.41/171.79 | 25.38 | 51.26 | 25.96 | 50.3/51.1/51.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LayerNorm-13B | 445.98/150.0 | 15.59 | 64.63 | 32.17 | 51.2/53.0/50.8 |'
  prefs: []
  type: TYPE_TB
- en: 5.2 LayerNorm Tuning Improves the Expressive Power
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is shown in Pires et al. ([2023](#bib.bib26)) that a Transformer model incorporating
    anisotropic layer representation can capture a wider range of learning patterns.
    By computing the cosine similarities between all layers in the LLM of a finetuned
    MLLM, we aim to investigate whether the improved efficiency is the results of
    the improved expressive power. In [table 6](#S5.T6 "In 5.2 LayerNorm Tuning Improves
    the Expressive Power ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm
    in Attention: Towards Efficient Multi-Modal LLM Finetuning"), we present the average
    layer similarity of three 7B scale MLLMs, and in [fig. 3](#S5.F3 "In Influence
    of the Vision-Language Connector ‣ 5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal
    ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention: Towards
    Efficient Multi-Modal LLM Finetuning") we present the visualization of per layer
    similarity scores of MM-Vicuna-7B. Our analysis reveals that the transformer layers
    in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another
    (*i.e*., an average 10.6% lower layer similarities comparing finetuning), indicating
    superior generalization ability and expressive power compared to finetuning. This
    finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training.
    For additional visualizations, please refer to the Appendix [A.2.1](#A1.SS2.SSS1
    "A.2.1 Visualization Examples of Layer Similarities ‣ A.2 Insights of LayerNorm
    Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient
    Multi-Modal LLM Finetuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Layer representation similarity of LayerNorm and finetuning methods
    on three 7B MLLMs. Lower the similarity is, the better expressive power a model
    possesses.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | LayerNorm Sim. | Finetuning Sim. |'
  prefs: []
  type: TYPE_TB
- en: '| MM-Vicuna | 0.585 | 0.624 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2 | 0.504 | 0.591 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2-chat | 0.550 | 0.617 |'
  prefs: []
  type: TYPE_TB
- en: 5.3 LayerNorm Tuning has Smaller Gradient Variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A well accepted view about LayerNorm is that, as the neural network goes deeper,
    the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is
    designed to normalize all training parameters. In the meantime, the variance of
    LayerNorm gradients should be small to ensure a better generalization ability
    of the model (Xu et al., [2019](#bib.bib37)) (See the proof in Appendix [A.2.2](#A1.SS2.SSS2
    "A.2.2 Gradients of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A
    Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")).
    As we presented in [fig. 4](#S5.F4 "In 5.3 LayerNorm Tuning has Smaller Gradient
    Variance ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), MLLM with LayerNorm tuning method
    has a more concentrated LayerNorm gradients than fine-tuning during the training
    process. This result gives another view on the effectiveness of LayerNorm from
    the optimization perspective. More visualizations are listed in Appendix [A.2.2](#A1.SS2.SSS2
    "A.2.2 Gradients of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A
    Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f1e85df7d096077efd3c21ffe99bf1a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Gradients of the input LayerNorm in the 11th layer of the MM-Vicuna
    as training proceeds. LayerNorm-tuned model has lower gradient variance than full
    parameter finetuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LayerNorm is effective and sufficient built upon MLLM pre-training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'MLLM training typically involves pre-training on image-text pairs followed
    by finetuning on visual instruction data. While the second stage of training receives
    more attention, it is worth noting that the function of the first stage pre-training
    is non-negligible for training a competent MLLM. We have presented in the paper
    only a small portion of parameter activation is sufficient to tune a well-behaved
    MLLM. However, other models such as InstructBLIP (Dai et al., [2023](#bib.bib5))
    and MiniGPT4 (Zhu et al., [2023](#bib.bib45)) only tune the vision-language connector,
    leaving the LLM untouched during the second stage of training. These models have
    yielded strong performances when given a large-scale finetuning dataset. In Sec. [5.1](#S5.SS1
    "5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"),
    we demonstrate that tuning LayerNorm may be a more effective means for the second
    stage training, especially when compared to existing parameter-efficient methods
    for training MLLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'One shortcoming of these parameter-efficient finetuning methods is that they
    are more sensitive to hyper-parameters (*e.g*., learning rate, training epoch)
    than finetuning. Since the number of trainable parameters of LayerNorm is small,
    the model performance of LayerNorm method also varies when twitching the training
    hyper-parameters. This drawback calls for potential future investigations on the
    LayerNorm tuning method. In the Appendix [A.1](#A1.SS1 "A.1 Training Details ‣
    Appendix A Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal
    LLM Finetuning"), we give a hint for the grid search range of learning rate on
    both 7B and 13B scaled models using LayerNorm tuning based on our experimental
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method
    for adapting LLMs comprehend multi-modal content across various model variants.
    Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces
    the trainable parameters by a significant 41.9%, enabling efficient finetuning
    of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve
    exceptional performance with minimal “right” data and parameters, showcasing the
    potential of LayerNorm tuning method in real-world applications. Given the empirical
    success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation
    perspective and showed that LayerNorm plays a critical role in adapting LLMs to
    the multi-modal domain. Additionally, our research illustrates the expressive
    power and optimization potential of LayerNorm tuning from layer similarities and
    the gradient variance. We hope that our work could inspire future works on designing
    improved PEFT methods that enable more diverse application scenarios for MLLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question
    answering. In *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    Normalization. *arXiv:1607.06450*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Changpinyo et al. (2021) Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
    Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize
    long-tail visual concepts. In *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning. *NeurIPS*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In *CVPR*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
    Transformers for image recognition at scale. *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan
    Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive
    evaluation benchmark for multimodal large language models. *arXiv preprint arXiv:2306.13394*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2022) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    and Graham Neubig. Towards a unified view of parameter-efficient transfer learning.
    *ICLR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *ICML*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe & Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *ICML*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpathy & Fei-Fei (2015) Andrej Karpathy and Li Fei-Fei. Deep visual-semantic
    alignments for generating image descriptions. In *CVPR*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models. *ICML*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021a) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In *ACL*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021b) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *ACL*, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi
    Hou. Revisiting batch normalization for practical domain adaptation. *arXiv preprint
    arXiv:1603.04779*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao,
    and Ji-Rong Wen. Evaluating object hallucination in large vision-language models.
    *arXiv preprint arXiv:2305.10355*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *ECCV*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual
    instruction tuning. *NeurIPS*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic
    gradient descent with warm restarts. *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *ICLR*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mu et al. (2022) Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
    Slip: Self-supervision meets language-image pre-training. In *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pires et al. (2023) Telmo Pessoa Pires, António V Lopes, Yannick Assogba, and
    Hendra Setiawan. One wide feedforward is all you need. *arXiv preprint arXiv:2309.01826*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *ICML*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2023) Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng
    Cai. Pandagpt: One model to instruction-follow them all. *arXiv preprint arXiv:2305.16355*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2022) Haoqin Tu, Zhongliang Yang, Jinshuai Yang, and Yongfeng Huang.
    Adavae: Exploring adaptive gpt-2s in variational auto-encoders for language modeling.
    *arXiv preprint arXiv:2205.05862*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2023) Haoqin Tu, Bingchen Zhao, Chen Wei, and Cihang Xie. Sight
    beyond text: Multi-modal training enhances llms in truthfulness and ethics. *arXiv
    preprint arXiv:2309.07120*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ulyanov et al. (2016) Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
    Instance normalization: The missing ingredient for fast stylization. *arXiv preprint
    arXiv:1607.08022*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023) Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4:
    A 200-instruction paradigm for fine-tuning minigpt-4. *arXiv preprint arXiv:2308.12067*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu & He (2018) Yuxin Wu and Kaiming He. Group normalization. In *ECCV*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang
    Lin. Understanding and improving layer normalization. *NeurIPS*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang
    Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization
    empowers large language models with multimodality. *arXiv preprint arXiv:2304.14178*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Young et al. (2014) Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
    From image descriptions to visual denotations: New similarity metrics for semantic
    inference over event descriptions. *TACL*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba
    Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation
    models. *TMLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin
    Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. *arXiv preprint arXiv:2303.16199*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang,
    and Oisin Mac Aodha. Vision learners meet web image-text pairs. *arXiv preprint
    arXiv:2301.07088*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment. *arXiv preprint arXiv:2305.11206*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Training Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the first stage, we set the learning rate to 2e-3 for all variants. During
    the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4,
    3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best
    learning rate based on their performances on the CIDEr score on the Flickr30k
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to our tryouts based on Flickr30k results in Table [A1](#A1.T1 "Table
    A1 ‣ A.1 Training Details ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), the recommended learning rate
    for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should
    be searched in the range of 3e-6 to 6e-5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning
    rates and scales on the Flickr30k task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning Rate | 3e-6 | 1e-5 | 3e-5 | 6e-5 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2 7B | 21.42 | 32.45 | 43.04 | 28.24 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate | 6e-4 | 1e-3 | 2e-3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MM-LLaMA2 13B | 37.35 | 46.88 | 44.15 | - |'
  prefs: []
  type: TYPE_TB
- en: A.2 Insights of LayerNorm Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.2.1 Visualization Examples of Layer Similarities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lower similarities between different layers of the transformer indicates more
    expressive power (Pires et al., [2023](#bib.bib26)). In [section 5.2](#S5.SS2
    "5.2 LayerNorm Tuning Improves the Expressive Power ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"),
    we have shown the computed cosine similarity between layers on a Vicuna model,
    here we show the layer similarities between layers on LLaMA2 and LLaMA2 chat models
    in [fig. A1](#A1.F1 "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients
    of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning") and [fig. A2](#A1.F2
    "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients of LayerNorm ‣
    A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"). It is clear that, LayerNorm tuning
    again allows the model to learn dissimilar layer representations, improving the
    expressive power of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 Gradients of LayerNorm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Visualization examples of LayerNorm gradients.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [fig. A3](#A1.F3 "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients
    of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning") and [fig. A4](#A1.F4
    "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients of LayerNorm ‣
    A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), we present the gradients of the
    LayerNorm parameters during the training process. Similar to the one we have shown
    in the main text, LayerNorm tuning demonstrates a smaller gradient variance which
    is important for converging to a better local minimum (Xu et al., [2019](#bib.bib37)).'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of smaller variance in LayerNorm .
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As stated in Sec. [5.3](#S5.SS3 "5.3 LayerNorm Tuning has Smaller Gradient
    Variance ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), deeper the network is, the variance
    of LayerNorm in the model should be naturally smaller (Xu et al., [2019](#bib.bib37)).
    We first let $\mathbf{y}=(y_{1},y_{2},...,y_{N})$ is $0$, respectively. We can
    then formulate the standard LayerNorm as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the dimension of $\mathbf{x}$
    and $\sigma$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define $\mathbf{1}_{N}=\underbrace{(1,1,...,1)^{\intercal}}_{N}$,
    we first simulate the backward propagation regarding the loss $\ell$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Here we define $\frac{\partial\ell}{\partial\mathbf{x}}=(a_{1},a_{2},...,a_{N})$
    and standard deviation $D_{a}$ with mean $\bar{b}$. We set $W_{1}=I-\frac{\mathbf{y}\mathbf{y}^{\intercal}}{N}-\frac{\mathbf{1}_{N}\mathbf{1}^{\intercal}_{N}}{N}$,
    we can verify that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we can easily proof that $N\bar{a}\propto\mathbf{1}_{N}^{\intercal}W_{1}\bar{b}=0$
    should be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Then we dive into proofing the variance of LayerNorm gradients should be small
    when the number of network parameters $N$ becomes large.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D_{a}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\left\&#124;\left(a_{1},a_{2},\ldots,a_{N}\right)^{\intercal}\right\&#124;^{2}/N$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\left\&#124;W_{1}\left(b_{1},b_{2},\ldots,b_{N}\right)^{\intercal}\right\&#124;^{2}/N$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq W_{1}^{2}\sum_{i=1}^{N}(b_{i}-\bar{b})^{2}/N$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since the projection matrix $W_{1}$. That is to say, when $N$. As a consequence,
    when the network parameter $N$ is large, the gradient variance of LayerNorm should
    be small.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3163909fa9dd121e3f39e2b9ee31d4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A1: Layer similarities between different LLM layers in (a) Finetuned
    and (b) LayerNorm-tuned MM-LLaMA2-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0307e4ccce72e8dbb3aa8b2caa848a70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A2: Layer similarities between different LLM layers in (a) Finetuned
    and (b) LayerNorm-tuned MM-LLaMA2-7B chat.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ceb0018641e70a6cba398976e59572a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A3: The gradients of both input and post LayerNorm in 21st layer of
    the MM-Vicuna as the training proceeds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07e0691789eb105c2fee7699e09c39dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A4: The gradients of both input and post LayerNorm in 11th layer of
    the MM-Vicuna as the training proceeds.'
  prefs: []
  type: TYPE_NORMAL
