- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'CHAI: Clustered Head Attention for Efficient LLM Inference'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08058](https://ar5iv.labs.arxiv.org/html/2403.08058)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Saurabh Agarwal    Bilge Acun    Basil Homer    Mostafa Elhoushi    Yejin Lee
       Shivaram Venkataraman    Dimitris Papailiopoulos    Carole-Jean Wu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) with hundreds of billions of parameters have transformed
    the field of machine learning. However, serving these models at inference time
    is both compute and memory intensive, where a single request can require multiple
    GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components
    of LLMs, which can account for over 50% of LLMs memory and compute requirement.
    We observe that there is a high amount of redundancy across heads on which tokens
    they pay attention to. Based on this insight, we propose Clustered Head Attention
    (CHAI). CHAI combines heads with a high amount of correlation for self-attention
    at runtime, thus reducing both memory and compute. In our experiments, we show
    that CHAI is able to reduce the memory requirements for storing K,V cache by up
    to 21.4% and inference time latency by up to $1.73\times$ without any fine-tuning
    required. CHAI achieves this with a maximum 3.2% deviation in accuracy across
    3 different models (i.e. OPT-66B, LLaMa-7B, LLaMa-33B) and 5 different evaluation
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have demonstrated remarkable performance on language modelling tasks ranging
    from question answering, text summarizing, language translation. However, such
    performance has been achieved by scaling models to trillions of parameters, and
    existing works (Hoffmann et al., [2022](#bib.bib22); Touvron et al., [2023a](#bib.bib45);
    Kaplan et al., [2020](#bib.bib26)) show that increasing the model size may lead
    to even higher model quality.
  prefs: []
  type: TYPE_NORMAL
- en: Inference on LLMs introduce several new challenges. Beyond just the quadratic
    computation cost of self-attention (Vaswani et al., [2017](#bib.bib47)) with increasing
    context and large model sizes, LLMs also store intermediate Key (K) and Value
    (V) pairs for subsequent next word prediction. This K,V caching introduces additional
    memory related challenges as K,V cache size increases with increase in sequence
    length. The architecture of widely used LLMs like GPT (Brown et al., [2020](#bib.bib4))
    and LLaMa (Touvron et al., [2023a](#bib.bib45), [b](#bib.bib46)) use Multi-Head
    Attention (MHA) (Vaswani et al., [2017](#bib.bib47)). MHA uses several attention
    heads to look at a sequence. As models grow bigger, the number of heads increases
    as well. For example, LLaMa-7B uses 32 attention heads in each layer, while LLaMa-65B
    uses 64 attention heads per layer (Touvron et al., [2023a](#bib.bib45)). The use
    of MHA exacerbates bottlenecks for serving LLMs. First, it increases compute pressure
    due to repeated application of the attention operation. Second, it increases the
    memory pressure due to requiring storage of Key (K), Value (V) caches that comes
    with the additional attention heads. To alleviate these bottlenecks, prior works
    have introduced primarily two types of methods - (i) pruning of LLMs to utilize
    sparsity based on the input context (Liu et al., [2023b](#bib.bib33); Voita et al.,
    [2019](#bib.bib48)) and (ii) Co-designing of the Attention module to reuse components
    across multiple heads like Multi-Query Attention (MQA) (Shazeer, [2019](#bib.bib41))
    and Grouped-Query Attention (GQA) (Ainslie et al., [2023](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Accuracy vs Flops: We study various methods of clustering attention
    heads, and plot the runtime for sequence length of 2048\. For random head selection
    we randomly choose heads to combine in increasing number of 4, 8, 16 and 24\.
    For Static Head Selection, we choose the heads to combine based on activations.
    CHAI is our proposed method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pruning LLMs can potentially ease the compute bottleneck, however it is challenging
    as the classical methods for pruning (Frankle & Carbin, [2018](#bib.bib18); Chen
    et al., [2020b](#bib.bib6); You et al., [2019](#bib.bib58); Waleffe & Rekatsinas,
    [2020](#bib.bib49)) require fine-tuning or iterative training which is prohibitively
    expensive for LLMs due to massive memory and compute cost. There have been recent
    pruning works such as DejaVu (Liu et al., [2023b](#bib.bib33)) which perform pruning
    based on the context at inference time without requiring fine-tuning. However,
    we observe that methods like DejaVu are primarily designed for large parameter-inefficient
    models such as OPT (Zhang et al., [2022](#bib.bib60)) and the insights used to
    build DejaVu are not directly applicable on newer parameter efficient models like
    LLaMa-7B (Section [2](#S2 "2 Background and Related Work ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference")). In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"), we show that CHAI
    achieves the best trade-off between flops and accuracy compared to the state-of-the-art
    methods. Furthermore, runtime pruning methods like DejaVu only reduce the compute
    cost and have no effect on the large memory requirements of K,V cache.'
  prefs: []
  type: TYPE_NORMAL
- en: The Attention module co-design methods like GQA (Ainslie et al., [2023](#bib.bib2))
    require re-training of LLMs, e.g., LLaMa-2 (Touvron et al., [2023b](#bib.bib46))
    trained the models from scratch to utilize the benefits of GQA, making it quite
    expensive. Even in the case where users are willing to perform retraining, accuracy
    trade-off between GQA and MHA will not be known prior to multiple rounds of training.
    Further, Attention module co-design methods only reduce the K,V cache size and
    do not reduce computational complexity. Therefore, there is a need for a method,
    which can reduce both the compute and K,V cache overhead for attention and is
    - (i) Applicable on a wide range of models (from LLaMa–7B to OPT-66B). (ii) Does
    not require any fine-tuning or re-training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work we present Clustered Head Attention for efficient LLM Inference
    (CHAI), a dynamic inference time method for efficient LLM inference that does
    not require fine-tuning. CHAI is inspired by two observations. First, several
    heads in multi-head attention give similar weight to each token in a given sequence,
    indicating redundant compute. In Figure [2(a)](#S1.F2.sf1 "Figure 2(a) ‣ Figure
    2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    we show attention scores for a single layer of LLaMa-7B for an auto-regressive
    decoding step of a sentence. We observe that several heads output similar scores,
    i.e., giving similar weight to each token in the sequence. Figure [2(b)](#S1.F2.sf2
    "Figure 2(b) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference") highlights the similarity in attention score by plotting
    correlation for the activation for LLaMa-7B. In Figure [2(b)](#S1.F2.sf2 "Figure
    2(b) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference") we observe that there are three clusters and within these clusters
    the correlation is greater than 0.95.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This indicates that by identifying attention heads with similar attention scores
    and clustering them together we can reduce the number of self-attention operations
    for MHA by calculating self-attention only for a single head within a cluster.
    Secondly, we observe that for each request to an LLM we can accurately determine
    the heads which are going to give similar (attention) weight to the tokens in
    a sequence after running a few decoding steps on the sequence (Section [3.3](#S3.SS3
    "3.3 Determination of Cluster Membership ‣ 3 CHAI ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference")). Schematic in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") depicts both Multi-Head
    and Clustered-Head Attention.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cbaae28fcfba70d02f3d784537d4b4fe.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Activations of Multi Head Attention: Figure shows activation scores for
    each token for each head. We observe that several heads give similar scores to
    the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63404e9ac502d63d43e2f03ca3f88a9a.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Pairwise cross correlation: Pairwise cross-correlations show existence
    of three clusters- Heads [12,26] show strong correlation forming one cluster,
    Heads [20,25] form another, and the remaining heads form a third cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Redundancy across heads for LLaMa–7B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99217df1baddd78971fb016d55476380.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Multi-Head Attention
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bba42e362b2f2efbfcd25d14f0cbfe33.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Clustered Head Attention
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Clustered Head Attention: Schematic of clustered head attention,
    comparing it with popular Multi-Head Attention. In clustered head attention, we
    remove the query and key vectors which produce similar attention scores.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions in this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We show that there is high level of redundancy across several different heads
    of multi head attention, and the redundancy varies differently across layers with
    increasing redundancy towards later layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce CHAI, a practical and principled inference time pruning method
    which clusters attention heads that have similar output together with dynamic
    determination of clusters. CHAI reduces both compute and K,V cache size for self
    attention.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We show that CHAI is capable of reducing the inference time by up to 1.73$\times$
    and K,V cache memory size by up to 21.4% compared to MHA for LLaMa models with
    minimal accuracy trade-off (maximum of 3.2%).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to other runtime pruning methods like DejaVu, which only works well
    for OPT models, CHAI outperforms DejaVu and performs well for wider class of models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first provide background on inference process for decoder only transformers
    like GPT (Radford et al., [2019](#bib.bib37); Brown et al., [2020](#bib.bib4)),
    LLaMa (Touvron et al., [2023a](#bib.bib45), [b](#bib.bib46)) and the bottlenecks
    in performing inference. Further, we discussed several prior lines of work which
    have tried to tackle the inference bottlenecks for transformer based model.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only Transformer A decoder-only transformer forms the building block
    of popular LLMs. A single decoder block consists of a self attention layer and
    a MLP. An input token is fed into the decoder block, to perform next-word prediction.
    The self attention block uses prior query (Q), key (K) and value (V) vectors associated
    with current token. These tokens are extracted by performing a linear projection
    with query, key and value weight matrices associated with a transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To precisely define Multi-Head Attention (MHA), let $H$, $T$, $d$ be positive
    integers, where $H$ denotes number of heads, $T$ denotes sequence length, $d$
    denotes model dimension. Let $x\in{}^{T\times d}$ be input to the MHA layer. For
    a single head $h$, then $\mathbf{K}^{h}=x\mathbf{W}_{K}^{h}$, $\mathbf{Q}^{h}=x\mathbf{W}_{Q}^{h}$
    and $\mathbf{V}^{h}=x\mathbf{W}_{V}^{h}$ denote the corresponding key, query and
    value vector. The attention matrix for head $h$ is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A_{h}=\sigma(\frac{1}{\sqrt{d}}Q^{h}K{{}^{h}}{{}^{T}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Output of MHA is denoted by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=A_{0}V_{0}\oplus A_{1}V_{1}\oplus A_{2}V_{2}\oplus\cdot\cdot\cdot\oplus
    A_{H}V_{H}$ |  |'
  prefs: []
  type: TYPE_TB
- en: For performing inference, self attention needs access to the query, key and
    values associated with prior tokens. In order to avoid re-computation, inference
    serving systems cache the prior tokens in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Compute cost required for multiple attention heads and memory capacity required
    for storing key and value vectors associated with each head during inference form
    two primary bottlenecks for LLM inference. In this work, we focus on reducing
    both memory and compute requirements via clustering multiple attention heads with
    similar output.
  prefs: []
  type: TYPE_NORMAL
- en: Building Efficient Transformers. Improving efficiency of transformer models
    has been of major focus in recent years. Prior work can be broadly categorized
    in the following fields - (i) Hardware-software co-design (Dao et al., [2022](#bib.bib11);
    Dao, [2023](#bib.bib10); Ham et al., [2020](#bib.bib20), [2021](#bib.bib21); Tambe
    et al., [2021](#bib.bib43); Fang et al., [2022](#bib.bib17); Qin et al., [2023](#bib.bib36);
    Wang et al., [2021b](#bib.bib51)), (ii) Knowledge distillation (Hsieh et al.,
    [2023](#bib.bib24); Jiao et al., [2019](#bib.bib25); Sanh et al., [2019](#bib.bib40);
    Wang et al., [2020](#bib.bib53)) (iii) Neural Architecture Search (NAS) (Zhou
    et al., [2023](#bib.bib62); Kitaev et al., [2020](#bib.bib28); Lagunas et al.,
    [2021](#bib.bib30)) and (iv) Pruning (Voita et al., [2019](#bib.bib48); Liu et al.,
    [2023b](#bib.bib33)) and Quantization (Frantar et al., [2022](#bib.bib19); Xiao
    et al., [2023](#bib.bib56); Kim et al., [2021](#bib.bib27); Shen et al., [2020](#bib.bib42);
    Dettmers et al., [2022](#bib.bib14); Dettmers, [2015](#bib.bib12); Dettmers &
    Zettlemoyer, [2023](#bib.bib13)). In this work our focus is on pruning , which
    we discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Quantization. Recently several methods have been proposed to perform post
    training quantization allowing models to be quantized to a lower precision (Frantar
    et al., [2022](#bib.bib19); Xiao et al., [2023](#bib.bib56); Dettmers & Zettlemoyer,
    [2023](#bib.bib13)). The goal of these methods is to perform quantization so as
    to minimize the error, CHAI is orthogonal to quantization based mechanisms as
    it depends on the insight of several attention heads focusing on the same tokens.
    The goal of quantization methods is to keep the same properties of original models,
    therefore we believe CHAI can be used to further accelerate post training quantized
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Pruning. Pruning is a widely studied method to improve inference time by
    removing unused weights post training. Several prior works have looked at pruning
    for language models (Chen et al., [2020b](#bib.bib6); Prasanna et al., [2020](#bib.bib35);
    Chen et al., [2020a](#bib.bib5)). For example, oBERT is a second order method
    to reduce the number of weights (Kurtic et al., [2022](#bib.bib29)). Although
    these approaches can compress a model, they rarely yield inference speedups due
    to lack of hardware support for sparse operations on modern GPUs. To overcome
    the challenges, low rank decomposition methods (Wang et al., [2023](#bib.bib52),
    [2021a](#bib.bib50), [2019](#bib.bib54)), attention head pruning (Michel et al.,
    [2019](#bib.bib34); Voita et al., [2019](#bib.bib48)), layer dropping (Sajjad
    et al., [2023](#bib.bib39); Fan et al., [2019](#bib.bib16); Dai et al., [2023](#bib.bib9))
    were proposed. However, these methods are infeasible for LLMs due to the use of
    iterative gradient calculations or fine-tuning leading to high resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome these issues, a recently proposed method, DejaVu (Liu et al., [2023b](#bib.bib33)),
    identifies portions of the model which are unused for a given context. To reduce
    the overhead of self-attention, DejaVu prunes attention heads which give *uniform
    weight across tokens*. We plot the activations for an exemplary sentence used
    by DejaVu for both OPT-66B and LLaMa-7B in Figure [4](#S2.F4 "Figure 4 ‣ 2 Background
    and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference").
    We observe that while there are heads which give uniform weight to each token
    in OPT-66B model, there are no such heads in more parameter efficient models like
    LLaMa-7B, indicating that for smaller parameter efficient models like LLaMa DejaVu
    might not be applicable. (Additional plots for different layers can be found in
    Appendix-[A](#A1 "Appendix A Additional Plots ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference").) The primary difference between OPT and LLaMa activation
    patterns could be attributed to the fact that LLaMa models are trained significantly
    longer and with more data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe that CHAI’s insight about redundancy in the output of multiple heads
    in the attention holds across both OPT and LLaMa family of models. In our evaluation
    (Section [4](#S4 "4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference")), we perform quantitative comparison between CHAI and DejaVu.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b3cdad209b7499347acb3d84dbe5791.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) OPT-66B: For several heads the activation scores are uniform, i.e., the
    heads given close to equal importance to each input token.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f5a1d05769f4b62c1d7034dc38536aa.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) LLaMa-7B: Heads in LLaMa-7B specifically pay attention to a specific token.
    However, multiple heads are attending to same token, in this case the first token.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Activations for OPT-66B and LLaMa-7B for an exemplary sentence: We
    observe that OPT-66B has several heads which give uniform attention scores to
    tokens whereas LLaMa-7B does not. However, both models have redundancies across
    heads, i.e., groups of heads are give similar attention to each token.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a64a66ef858b612b35e8856914be2a01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: CHAI Flow: In the offline phase, we run clustering and perform elbow
    plot analysis for each new model. Then, for each new inference request we only
    perform cluster membership identification based on online performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec9cede6a32483d5f70e13247f07df7b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Layer 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cbf27f89ca9783dee7f07a08f2a17b72.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Layer 5
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95493ecb394e0b95c57c79818ac72465.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Layer 17
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ac86c86de2806832cc294844294e2df.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Layer 30
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Average Correlation for 1024 Samples of C4 on LLaMa-7B: The above
    figure shows two interesting observations. First, there exists high amount of
    correlation across several heads of attention. Second, the correlation is not
    uniform across layers, with later layers having higher correlation, i.e., , first
    layer has very little correlation but correlation increases in later layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c5538b8e859b96bbcc953f5bbb79d69.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Layer 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/805fc10824da6ce393f7834dac56bc6b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Layer 5
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb5c0a0611b0300a882c440b97b30124.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Layer 17
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ff599164cd2909b87a139557fe2b70b.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Layer 30
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Correlation on a randomly selected single sample of LLaMa-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: K,V Cache Compression. Prior works which have tried to reduce the K,V cache
    size (Liu et al., [2023a](#bib.bib32); Zhang et al., [2023](#bib.bib61)) by storing
    the K,V cache values for the most recent important tokens. However, they can not
    directly improve the latency of generating the next token, as they still perform
    the full transformer compute before finally deciding which K,V pairs should be
    stored. On the other hand, CHAI reduces not just the K,V cache size, it is also
    able to reduce the latency of next word prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Speculative Decoding. Speculative decoding (Leviathan et al., [2023](#bib.bib31);
    Yang et al., [2023](#bib.bib57); Xia et al., [2023](#bib.bib55)) is a popular
    method where a draft model is used to cheaply generate a sequence of draft tokens
    which can be efficiently verified by a target LLM. Speculative decoding can significantly
    reduce the latency of LLM serving, however it further exacerbates the compute
    and memory requirements as it requires additional resources to run both the draft
    and target model. CHAI on the other hand is focused on reducing the resource required
    for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 3 CHAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we describe CHAI. We first describe the key insights which have been
    used to build CHAI. Then, we detail CHAI’s runtime pruning algorithm which is
    inspired by our insights and discuss how we perform inference using CHAI. Figure [5](#S2.F5
    "Figure 5 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference") provides a high level overview of inference using CHAI,
    which includes offline and online components.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our primary insight stems from the observation that there is a high amount
    of correlation across the output of various attention heads in MHA, i.e., the
    output of several attention heads focuses on the same tokens. In Figure [6](#S2.F6
    "Figure 6 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference"), we plot the average correlation across the 32 heads
    of LLaMa-7B for 1024 samples of the C4 (Raffel et al., [2020](#bib.bib38)) dataset
    for different layers and in Figure [7](#S2.F7 "Figure 7 ‣ 2 Background and Related
    Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"), we plot correlation
    for a single sample of the dataset. These show us two insights - (i) Several heads
    output similar attention scores for each example and (ii) The amount of correlation
    increases in later layers, with heads in later layers with having higher correlation.
    This indicates that there is an opportunity to cluster attention heads with similar
    output and only run the self-attention operation for one of the representative
    attention heads within each cluster, thus reducing the amount of computation as
    well as the size of K,V cache.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/688b440026282afe4f3a8e7e687e1fc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Clustering Error: We plot the clustering error on 1024 samples of
    C4-dataset. The markers represent the number of clusters we choose for a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eaa6f1ed7b63d8655146e5a0194c4dd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Cluster Membership Evaluation: We evaluate the number of times the
    cluster membership changes for performing next token prediction. We observed that
    if clustering is performed beyond the fifth token the number of times cluster
    membership changes is quite small.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4dece796bd1047e5b3ce1166802c70ef.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Offline Cluster Identification: For each new model we run an offline cluster
    identification phase. We collect the activations and perform Elbow-plot analysis
    to decide number of clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/41f11c67f462c4c06f67129b6fb31e10.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Cluster Membership Identification: For each new request, we initial run
    with multi-head attention for first five tokens. Using this we determine the number
    of clusters in each layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4d63d3b5244e91283abe2d75a0669af.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) CHAI Inference: Post cluster membership identification we substitute MHA
    with Clustered Head Attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Schematic of CHAI detailing three phases of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem Formulation. Next, we formally define the problem of finding heads whose
    attention score is similar. Let $H$ be the total number of attention heads, let
    $S=\{\langle K^{1},Q^{1}\rangle,\langle K^{2},Q^{2}\rangle,\langle K^{3},Q^{3}\rangle,\cdot\cdot\cdot,\langle
    K^{H},V^{H}\rangle\}$ be the set of $Q,K$ pairs associated with each head $h$.
    Our goal is to find $k$ subsets, $S_{1}\subset S,S_{2}\subset S,S_{3}\subset S,\cdot\cdot\cdot
    S_{k}\subset S$ such that $<Q,K></math> pairs in each subset <math id=$ produce
    similar output under function $f$. Where function $f$ is the self attention operation,
    where $f(Q,K)=\sigma(QK{{}^{T}})$. Further, we want $\cup_{i=1}^{k}S_{i}=S$.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we want to find $S_{i}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.Ex3.m1.1" class="ltx_math_unparsed" alttext="\forall<K^{n},Q^{n}></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: s.t.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(K^{n},Q^{n})\approx f(K^{m},Q^{m})$ |  |'
  prefs: []
  type: TYPE_TB
- en: Informally, we want subset of heads, where within each subset the self attention
    operation gives similar outcome.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this problem we need to determine $k$ which represents the
    number of such subsets, and the membership of such subset $S_{i}$. Our observations
    empirically demonstrate the existence of such a solution. We can potentially solve
    this problem using clustering, where determining the number of subsets translates
    to determining number of clusters and determining cluster membership becomes determination
    of cluster membership.
  prefs: []
  type: TYPE_NORMAL
- en: To observe memory and compute savings, we need an accurate and efficient method
    to determine the number of clusters and their membership *without having access
    to activations*. Solving this forms a core contribution of our work.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Determination of Number of Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Challenges.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [6](#S2.F6 "Figure 6 ‣ 2 Background and Related Work ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference") and Figure [7](#S2.F7 "Figure 7 ‣
    2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference") indicate that the number of clusters varies widely per layer in a
    LLM. Specifically, the last few layers in the LLM exhibit a very low number of
    clusters (high redundancy), whereas the early layers demonstrate a high degree
    of variance across the output of heads resulting in large number of clusters.
    This observation suggests that the method used to determine number of clusters
    needs to make decisions for each layer independently. Additionally, widely used
    methods such as Elbow plot method (Thorndike, [1953](#bib.bib44)) for determining
    number of clusters entail manual effort making cluster number determination impractical
    at inference time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Design. To determine the number of clusters, we propose an offline strategy
    we run once for each model. In our case, we sample a small number of samples (1024)
    from the C4 (Raffel et al., [2020](#bib.bib38)) dataset and perform elbow-plot
    analysis by plotting clustering error (i.e. sum of squared distance from the closest
    cluster) as a function of number of clusters. Figure [8](#S3.F8 "Figure 8 ‣ 3.1
    Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    shows the clustering error for LLaMa-7B for the samples selected. Based on the
    Elbow-plot analysis we choose the number of clusters when the error plateaus.'
  prefs: []
  type: TYPE_NORMAL
- en: The offline analysis is performed once for each network by using the C4 (Raffel
    et al., [2020](#bib.bib38)) dataset. We do not change the number of clusters determined
    for a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Determination of Cluster Membership
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Challenges. Having determined number of clusters, we need to determine the
    membership of these clusters, i.e., which heads belong to which cluster in each
    layer. For Figure [6](#S2.F6 "Figure 6 ‣ 2 Background and Related Work ‣ CHAI:
    Clustered Head Attention for Efficient LLM Inference"), [7](#S2.F7 "Figure 7 ‣
    2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference") and  [8](#S3.F8 "Figure 8 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference"), we perform clustering based on activations
    obtained by performing the forward pass. However, for each decoding step, performing
    clustering on output of self attention post forward pass will not yield any performance
    benefit as we will still be performing the original compute and using the full
    K,V cache. In order to utilize the insights observed in Section [3.1](#S3.SS1
    "3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference"), we will need to decide the cluster members without having access
    to the output of the self attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Design. A simple strategy would have been keeping the cluster membership static
    across the tokens and independent of input context, e.g., we use the same cluster
    membership found during offline analysis with C4 data in the previous section.
    For evaluation purposes we call this version of head selection CHAI-static.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we observed that the cluster membership does not remain static and
    varies based on context. When comparing Figure [7](#S2.F7 "Figure 7 ‣ 2 Background
    and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"),
    which plots correlation for a single example, with Figure [6](#S2.F6 "Figure 6
    ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference"), which plots correlation for 1024 samples, we observe that the
    correlation across heads varies with varying context. Therefore, the correlation
    across the output of the heads depends on the context (input prompt), i.e., *a
    solution to determine the membership of each cluster has to account for context.*
    To understand the effects of accounting for context while clustering heads, we
    analysed the change in cluster membership changes and clustering with different
    context. In Figure [9](#S3.F9 "Figure 9 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference"), we observed an interesting phenomenon,
    after determining cluster membership by accounting for five tokens, the cluster
    membership does not change frequently. A direct outcome of this observation is
    that for each new sequence we can perform clustering based on the output of self-attention
    after the first five tokens. We observe that *activation from first five tokens
    of a new sequence are enough to accurately predict the cluster membership.* This
    dynamic version of head selection further allows us to improve accuracy over CHAI-static.
    Figure [10(b)](#S3.F10.sf2 "Figure 10(b) ‣ Figure 10 ‣ 3.1 Observations ‣ 3 CHAI
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") shows an illustration
    of the membership identification step. Furthermore, evaluation results in Section [4](#S4
    "4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") compare
    CHAI-static and CHAI performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Clustered Head Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have decided which heads have similar attention output, we can than
    use Clustered Head Attention to combine key and query vectors for the heads.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Inference using CHAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next we, discuss the inference flow of CHAI, illustrated in detail in Figure [10](#S3.F10
    "Figure 10 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference"). For each new model we first perform offline cluster identification
    (Figure [10(a)](#S3.F10.sf1 "Figure 10(a) ‣ Figure 10 ‣ 3.1 Observations ‣ 3 CHAI
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")). Then for each
    new request, we determine the cluster membership using K-Means clustering once
    we have processed five tokens, using the observed activations (Figure [10(b)](#S3.F10.sf2
    "Figure 10(b) ‣ Figure 10 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference")). After this step, we keep the clustered heads same
    throughout inference (Figure [10(c)](#S3.F10.sf3 "Figure 10(c) ‣ Figure 10 ‣ 3.1
    Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")).'
  prefs: []
  type: TYPE_NORMAL
- en: There are two direct outcomes of CHAI’s design. First, we directly reduce the
    amount of computation by removing redundant heads. Secondly, after a pre-determined
    token we fix the heads which are going to be pruned, this also allows us to remove
    the corresponding *Key* tokens associated, which significantly reduces the K,V
    cache size. Therefore, CHAI allows us to reduce both the inference compute as
    well as the size of the K,V cache required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Accuracy on OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | PIQA | Hellaswag | Arc-Challenge | Arc-Easy | Boolq |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MHA | 78.4 | 71.1 | 41.6 | 64.7 | 65.4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-50% | -0.25 | -0.7 | -0.6 | -0.2 | -4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CHAI-static | -1.35 | -1.7 | -0.7 | -0.7 | -0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| CHAI | -0.15 | 0.1 | 0.1 | -0.1 | -0.6 |'
  prefs: []
  type: TYPE_TB
- en: 4 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We experimentally verify the performance of CHAI and compare it to DejaVu (Liu
    et al., [2023b](#bib.bib33)) and SpAtten (Wang et al., [2021b](#bib.bib51)) on
    three different models of various sizes LLaMa-7B (Touvron et al., [2023a](#bib.bib45)),
    LLaMa-33B and OPT-66B (Zhang et al., [2022](#bib.bib60)). We evaluate the models
    on five commonly used NLP tasks: PIQA (Bisk et al., [2020](#bib.bib3)), HellaSwag (Zellers
    et al., [2019](#bib.bib59)), Arc-Challenge and Arc-Easy (Clark et al., [2018](#bib.bib8))
    and BoolQA (Clark et al., [2019](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All our experiments are performed on servers with NVIDIA V100 GPUs. For OPT-66B
    we used eight GPUs on a single node, for LLaMa-33B we used four GPUs, and for
    LLaMa-7B, we used a single GPU for inference. CHAI is built on top of Meta’s xFormers (facebookresearch,
    [2023](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Accuracy Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our evaluation, we compare CHAI with Multi-Head Attention as baseline, static
    version of CHAI, as well two other state-of-the-art prior pruning methods; DejaVu
    and SpAtten. For DejaVu, we try different sparsity ratios, in order to try to
    match the accuracy number to MHA. We also compare CHAI to SpAtten, a method which
    removes unimportant tokens and heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [1](#S3.T1 "Table 1 ‣ 3.5 Inference using CHAI ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference"), we first verify that we are able
    to reproduce the performance numbers reported by DejaVu. To perform this, we took
    the OPT-66B and evaluated both DejaVu, CHAI and CHAI-static. We used DejaVu with
    50% sparsity as reported by the authors. We used the author provided code to train
    their MLP predictor layers and incorporate their scheme in our setup. In Table [1](#S3.T1
    "Table 1 ‣ 3.5 Inference using CHAI ‣ 3 CHAI ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference"), we observe that we were able to replicate results
    for OPT-66B. Furthermore, CHAI is also able to match the accuracy of MHA for OPT-66B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compare CHAI, CHAI-static and DejaVu with the pre-trained MHA network,
    using LLaMa-7B on 5 different datasets. For DejaVu we used three configurations,
    50% sparsity, 30% sparsity and 10% sparsity. In Table [2](#S4.T2 "Table 2 ‣ 4.2
    Accuracy Evaluation ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference"), we observe that when we use DejaVu with more 10% sparsity we
    see significant decrease in accuracy (by 18.6% for DejaVu-30%). On the other hand,
    our method based on our close analysis of the behaviour of layers of LLaMa-7B
    is able to recover accuracy. We observe a maximum accuracy degradation of 3.7%
    for CHAI. Similarly for LLaMa-33B using sparsity for more than 10% leads to significant
    accuracy drop, meanwhile CHAI closely matches the accuracy of the pre-trained
    model using MHA with maximum degradation in accuracy by 0.14%. This shows that
    CHAI is widely applicable across multiple datasets and models. We also want to
    highlight that we do not perform any dataset specific tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Accuracy on LLaMa-7B'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | PIQA | HellaSwag | Arc-Challenge | Arc-Easy | BoolQ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MHA | 79.8 | 76.1 | 47.5 | 72.8 | 76.0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-10% | -3.9 | -4.7 | -5.78 | -3.18 | -7.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-30% | -13.3 | -18.6 | -18.75 | -4.2 | -20.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-50% | -24.6 | -50.7 | -19.35 | -46.3 | -21.6 |'
  prefs: []
  type: TYPE_TB
- en: '| SpAtten | -41.4 | -42.5 | -18.0 | -40.2 | -27.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CHAI-static | -4.0 | -4.3 | -3.7 | -2.5 | -0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CHAI | -2.0 | -3.2 | -0.5 | 0.3 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Accuracy on LLaMa-33B'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | PIQA | HellaSwag | Arc-Challenge | Arc-Easy | BoolQ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MHA | 82.1 | 82.8 | 57.8 | 80.0 | 83.1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-10% | -0.7 | 0.1 | -0.2 | -0.6 | -0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-30% | -9.3 | -24.4 | -17.91 | -12.4 | -12.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DejaVu-50% | -27.6 | -43.2 | -24.6 | -37.6 | -21.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SpAtten | -31.9 | -44.1 | -26.4 | -40.3 | -34.55 |'
  prefs: []
  type: TYPE_TB
- en: '| CHAI-static | -0.5 | -0.2 | -1.3 | -3.7 | -1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CHAI | 0 | -0.14 | -0.21 | 0.9 | -0.04 |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Reduction in K,V Cache Memory Requirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we study the memory capacity reduction achieved by use of
    CHAI due to reduction in K,V cache size. In Figure [11](#S4.F11 "Figure 11 ‣ 4.3
    Reduction in K,V Cache Memory Requirement ‣ 4 Evaluation ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference"), we show that for LLaMa-7B CHAI reduces
    the size of K,V cache by up to 21.4% compared to MHA. Even for comparatively small
    models like LLaMa-7B, the size of the K,V cache for a sequence length of 2048
    is around 1.2 GB, while around 12 GB is used for the model weights. A reduction
    in K,V cache size can enable use of larger context length or serving more requests.
    We would also like to note that as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"), CHAI only removes
    the keys associated with redundant heads and keeps all the value vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eba81434fda3380d27d4cd93b10de364.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Memory Savings: We observed that for LLaMa-7B CHAI provides memory
    savings of up to 21.4%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49c944ca7c0968245e6c2707c4e6dca6.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Time to first token: We observe speedups of upto 1.73$\times$ for sequence
    length of 2048\.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bcba4a4e13f4a53f0875ac412258e50.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Time to next token: We observe a speedup of upto $5\times$ for sequence
    length of 2048\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: Latency Analysis: We observe that the speedups provided by CHAI
    increases as the sequence length becomes larger. Even for a comparatively small
    model like LLaMa-7B we observe speedups of up to 1.73$\times$ for a large sequence
    length.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 End-to-End Latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we evaluate time to first token and time to next token comparing it with
    MHA. These are two standard metrics used for evaluation of an LLM. Time to first
    token evaluates the time for generating a first token given a new context. Time
    to first token accounts for generating K,V caches for all the tokens in the context.
    Whereas time to next token evaluates the time for generating the next token, assuming
    the K,V caches for all internal tokens is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time to first token. Next, in our experiments we compare the speedups provided
    by CHAI. In Figure [12](#S4.F12 "Figure 12 ‣ 4.3 Reduction in K,V Cache Memory
    Requirement ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference")-(a) for LLaMa-7B we show that our method provides speedup of up to
    $1.72\times$ on a sequence length of 2048. The execution times represented in
    this figure accounts for the overhead of clustering in CHAI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time to next token. Another metric for evaluation of LLMs is time to next token.
    We do not account for the overhead of clustering in the case of time to next token.
    Our primary wins come from reducing compute and reducing memory bandwidth requirement
    for performing time to next token. Figure [12](#S4.F12 "Figure 12 ‣ 4.3 Reduction
    in K,V Cache Memory Requirement ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference")-(b) shows time to predict the next token for different
    sequence lengths. We observe that CHAI provides a speedup of over $5\times$ for
    a sequence length of 2048.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we are not able to compare times with DejaVu as the authors have
    not released the specialized kernels used for realizing the speedups on hardware (git,
    [2024](#bib.bib1)), thus inhibiting a runtime comparison. However, we believe
    it is unlikely that at less than 10% sparsity which is needed by DejaVu to get
    comparable accuracy to MHA, it will yield high speedups (Hooker, [2021](#bib.bib23)).
    We would like to highlight that because of performing dense computations, unlike
    DejaVu, CHAI does not need custom GPU kernels. Further, CHAI’s speedup benefits
    are independent of the framework used, because irrespective of implementation,
    CHAI directly reduces the complexity of MHA.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04b86cf12f20160be9ac4380f53aef3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Cluster Distribution: We observe that number of heads within the
    cluster is quite skewed. We often observe one or two large clusters, while the
    remaining heads in the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Additional Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next we perform additional studies on our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pruning K, Q and V. In CHAI, we prune only the Key and Query portion of an
    attention head leaving the Value vector intact. Next, we study how accuracy changes
    if we remove the value vector as well. To perform this experiment we chose to
    reuse the value vector generated by the chosen head. In Table [4](#S4.T4 "Table
    4 ‣ 4.5 Additional Experiments ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference"), we show how reusing the full head (Query, Key and
    Value vector) lead to additional loss in accuracy. This shows that for smaller
    networks like LLaMa it might be hard to remove the whole head in Multi-Head Attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Pruning Both Q,K,V'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | CHAI | CHAI-QKV | MHA |'
  prefs: []
  type: TYPE_TB
- en: '| Arc-Challenge | 47.0 | 41.29 | 47.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | 77.8 | 61.93 | 79.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Cluster Distribution. Figure [13](#S4.F13 "Figure 13 ‣ 4.4 End-to-End Latency
    ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    shows the distribution across clusters for Layer-18 on LLaMa-7B for different
    1024 samples of C4 dataset. We observe that typically for LLMs majority of heads
    can be grouped into a single head.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we present CHAI, an efficient runtime method which identifies
    attention heads giving similar scores. Using this method we reduce overhead of
    Multi-Head Attention by clustering the correlated heads and computing attention
    scores only for heads which lead to disparate attention scores. Our evaluation
    shows that with minor accuracy loss system can speedup inference by up to $1.73\times$.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'git (2024) Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time.
    [https://github.com/FMInference/DejaVu](https://github.com/FMInference/DejaVu),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy,
    Y., Lebrón, F., and Sanghai, S. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,
    Z., and Carbin, M. The lottery ticket hypothesis for pre-trained bert networks.
    *Advances in neural information processing systems*, 33:15834–15846, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., and Liu,
    J. Earlybert: Efficient bert training via early-bird lottery tickets. *arXiv preprint
    arXiv:2101.00063*, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2023) Dai, S., Genc, H., Venkatesan, R., and Khailany, B. Efficient
    transformer inference with statically structured sparse attention. In *2023 60th
    ACM/IEEE Design Automation Conference (DAC)*, pp.  1–6\. IEEE, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao (2023) Dao, T. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. *Advances in Neural
    Information Processing Systems*, 35:16344–16359, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dettmers (2015) Dettmers, T. 8-bit approximations for parallelism in deep learning.
    *arXiv preprint arXiv:1511.04561*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for
    4-bit precision: k-bit inference scaling laws. In *International Conference on
    Machine Learning*, pp.  7750–7774\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'facebookresearch (2023) facebookresearch. xformers - toolbox to accelerate
    research on transformers. [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers),
    2023. Accessed: December 12, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2019) Fan, A., Grave, E., and Joulin, A. Reducing transformer depth
    on demand with structured dropout. *arXiv preprint arXiv:1909.11556*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2022) Fang, C., Zhou, A., and Wang, Z. An algorithm–hardware co-optimized
    framework for accelerating n: M sparse transformers. *IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems*, 30(11):1573–1586, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ham et al. (2020) Ham, T. J., Jung, S. J., Kim, S., Oh, Y. H., Park, Y., Song,
    Y., Park, J.-H., Lee, S., Park, K., Lee, J. W., et al. A^ 3: Accelerating attention
    mechanisms in neural networks with approximation. In *2020 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*, pp.  328–341\. IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ham et al. (2021) Ham, T. J., Lee, Y., Seo, S. H., Kim, S., Choi, H., Jung,
    S. J., and Lee, J. W. Elsa: Hardware-software co-design for efficient, lightweight
    self-attention mechanism in neural networks. In *2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA)*, pp.  692–705\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya,
    E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark,
    A., et al. Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hooker (2021) Hooker, S. The hardware lottery. *Communications of the ACM*,
    64(12):58–65, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii,
    Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step!
    outperforming larger language models with less training data and smaller model
    sizes. *arXiv preprint arXiv:2305.02301*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2019) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L.,
    Wang, F., and Liu, Q. Tinybert: Distilling bert for natural language understanding.
    *arXiv preprint arXiv:1909.10351*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
    Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws
    for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Kim, S., Gholami, A., Yao, Z., Mahoney, M. W., and Keutzer,
    K. I-bert: Integer-only bert quantization. In *International conference on machine
    learning*, pp.  5506–5518\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The
    efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurtic et al. (2022) Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz,
    M., Fineran, B., Goin, M., and Alistarh, D. The optimal bert surgeon: Scalable
    and accurate second-order pruning for large language models. *arXiv preprint arXiv:2203.07259*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lagunas et al. (2021) Lagunas, F., Charlaix, E., Sanh, V., and Rush, A. M. Block
    pruning for faster transformers. *arXiv preprint arXiv:2109.04838*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leviathan et al. (2023) Leviathan, Y., Kalman, M., and Matias, Y. Fast inference
    from transformers via speculative decoding. In *International Conference on Machine
    Learning*, pp.  19274–19286\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z.,
    Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of
    importance hypothesis for llm kv cache compression at test time. *arXiv preprint
    arXiv:2305.17118*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,
    Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity
    for efficient llms at inference time. In *International Conference on Machine
    Learning*, pp.  22137–22176\. PMLR, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel et al. (2019) Michel, P., Levy, O., and Neubig, G. Are sixteen heads
    really better than one? *Advances in neural information processing systems*, 32,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prasanna et al. (2020) Prasanna, S., Rogers, A., and Rumshisky, A. When bert
    plays the lottery, all tickets are winning. *arXiv preprint arXiv:2005.00561*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2023) Qin, Y., Wang, Y., Deng, D., Zhao, Z., Yang, X., Liu, L.,
    Wei, S., Hu, Y., and Yin, S. Fact: Ffn-attention co-optimized transformer architecture
    with eager correlation prediction. In *Proceedings of the 50th Annual International
    Symposium on Computer Architecture*, pp.  1–14, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. Language models are unsupervised multitask learners. *OpenAI
    blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sajjad et al. (2023) Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the
    effect of dropping layers of pre-trained transformer models. *Computer Speech
    & Language*, 77:101429, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint
    arXiv:1910.01108*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shazeer (2019) Shazeer, N. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2020) Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,
    Mahoney, M. W., and Keutzer, K. Q-bert: Hessian based ultra low precision quantization
    of bert. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pp.  8815–8821, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tambe et al. (2021) Tambe, T., Hooper, C., Pentecost, L., Jia, T., Yang, E.-Y.,
    Donato, M., Sanh, V., Whatmough, P., Rush, A. M., Brooks, D., et al. Edgebert:
    Sentence-level energy optimizations for latency-aware multi-task nlp inference.
    In *MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture*,
    pp.  830–844, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorndike (1953) Thorndike, R. L. Who belongs in the family? *Psychometrika*,
    18(4):267–276, 1953.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. *Advances
    in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov,
    I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting,
    the rest can be pruned. *arXiv preprint arXiv:1905.09418*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waleffe & Rekatsinas (2020) Waleffe, R. and Rekatsinas, T. Principal component
    networks: Parameter reduction early in training. *arXiv preprint arXiv:2006.13347*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Wang, H., Agarwal, S., and Papailiopoulos, D. Pufferfish:
    Communication-efficient models at no extra cost. *Proceedings of Machine Learning
    and Systems*, 3:365–386, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse
    attention architecture with cascade token and head pruning. In *2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*, pp.  97–110\. IEEE,
    2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Wang, H., Agarwal, S., Tanaka, Y., Xing, E., Papailiopoulos,
    D., et al. Cuttlefish: Low-rank model training without all the tuning. *Proceedings
    of Machine Learning and Systems*, 5, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer:
    Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang, Z., Wohlwend, J., and Lei, T. Structured pruning of
    large language models. *arXiv preprint arXiv:1910.04732*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2023) Xia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui,
    Z. Speculative decoding: Exploiting speculative execution for accelerating seq2seq
    generation. In *Findings of the Association for Computational Linguistics: EMNLP
    2023*, pp.  3909–3925, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Yang, S., Lee, G., Cho, J., Papailiopoulos, D., and Lee,
    K. Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding.
    *arXiv preprint arXiv:2307.05908*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. (2019) You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk,
    R. G., Wang, Z., and Lin, Y. Drawing early-bird tickets: Towards more efficient
    training of deep networks. *arXiv preprint arXiv:1909.11957*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H $\_2$ o: Heavy-hitter oracle
    for efficient generative inference of large language models. *arXiv preprint arXiv:2306.14048*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Zhou, Y., Du, N., Huang, Y., Peng, D., Lan, C., Huang, D.,
    Shakeri, S., So, D., Dai, A. M., Lu, Y., et al. Brainformers: Trading simplicity
    for efficiency. In *International Conference on Machine Learning*, pp.  42531–42542\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Additional Plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Accuracy and Inference Time Trade-off
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46fda07b9f008d1d2c31329c351008a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Accuracy vs Inference Time for LLaMa-7B: We study various methods
    of clustering attention heads, and plot the runtime for sequence length of 2048\.
    For random head selection we randomly choose heads to combine together in increasing
    number of 4, 8, 16 and 24\. For Static Head selection we choose the heads in increasing
    order of 4,8,16, and 24 based on activation analysis of activation on C4 dataset (Raffel
    et al., [2020](#bib.bib38)).'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 OPT-66B Activation Plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From Figure [15](#A1.F15 "Figure 15 ‣ A.3 LLaMa-7B Activation Plots ‣ Appendix
    A Additional Plots ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    to Figure [20](#A1.F20 "Figure 20 ‣ A.3 LLaMa-7B Activation Plots ‣ Appendix A
    Additional Plots ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    shows the activation plots for all layers of OPT. We consistently observe that
    for this examples there is high amount of.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 LLaMa-7B Activation Plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5faeab550796e6b4d64a88e449aae0a.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 0
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8bcb529b04994632372fbb38c9af2b3c.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0113163bc21cd9204d54e9f8bd617052.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8e305dce167e961667d654f728a9e5b8.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5ddb779307fd4b39528098dac608732.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 4
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/efc312eaf99d435f722c19f356e40827.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 5
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15: Activations for OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: i Layer 6
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5270bbcbc5cf76234b1338ddca1b2d5.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 7
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef8bfc3317956cb5eed9c6f7a2ba9ab1.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 8
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0b8a11cd4e306d244faddbeeeeb9bac.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 9
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8651a746aefcae3c9f2d094f327b6a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 10
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca2288f357270c5b3ac22f34343b453f.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 11
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 16: Activations for OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70b21497f5ffc7165594ac941e6392db.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 12
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec324385f35a2d87e86ebb55cd680729.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 13
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/74844e074ff373f427229974deffe570.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 14
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/604f894ba2f2b9e2e3b662eb14824cb7.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 15
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/599bbc196b9f8ca142ee2f66a0213a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 16
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99580ae39c520deef478256a365f24e4.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 17
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f968b9159f4030974781c1a01178773b.png)'
  prefs: []
  type: TYPE_IMG
- en: vii Layer 18
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2fd6bc254e5f174fc04ce648d2834f6.png)'
  prefs: []
  type: TYPE_IMG
- en: viii Layer 19
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6af48198c9353beaf33b91a91a38dac4.png)'
  prefs: []
  type: TYPE_IMG
- en: ix Layer 20
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9a58bf5a0fa0676e451bc7f5e2234853.png)'
  prefs: []
  type: TYPE_IMG
- en: x Layer 21
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d72e2c08485f8e3a1ec28d052135f5e.png)'
  prefs: []
  type: TYPE_IMG
- en: xi Layer 22
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f742addb45cf76b51053382c5d58d092.png)'
  prefs: []
  type: TYPE_IMG
- en: xii Layer 20
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17: Activations of OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/280d6649bf0b9f09353c5ee02e22cc8e.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 24
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2351a71d7e37385c420d0e8570b25d22.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 25
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b926becfd34ce5f68cf42afdb976fae8.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 25
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7bcfc0feb4e089584041f848f01f53c7.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 26
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75dd531de91cd4dc22030e4dfbc91006.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 27
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f82060942ff0110c12c1e67cff41621b.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 28
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d28065dc29fa3ca634c80d69d9d4b4e6.png)'
  prefs: []
  type: TYPE_IMG
- en: vii Layer 29
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81c215cdc6b18d517d148c84cb099306.png)'
  prefs: []
  type: TYPE_IMG
- en: viii Layer 30
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e8710d1635d50379e573dbd41af9fa7.png)'
  prefs: []
  type: TYPE_IMG
- en: ix Layer 31
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65d676fc0f6bf7f063b0861b4be8c399.png)'
  prefs: []
  type: TYPE_IMG
- en: x Layer 32
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/063ed09cbae7488e495f961069755da0.png)'
  prefs: []
  type: TYPE_IMG
- en: xi Layer 33
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a02b59a5f7cf2633c488f91eaf81f2b.png)'
  prefs: []
  type: TYPE_IMG
- en: xii Layer 34
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0e2b6da2d4ce8d56d81e796f8455371.png)'
  prefs: []
  type: TYPE_IMG
- en: xiii Layer 35
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79eb27ba08d0a183d9464970edd1fae2.png)'
  prefs: []
  type: TYPE_IMG
- en: xiv Layer 36
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3106b04699c877833a72716a14c74d27.png)'
  prefs: []
  type: TYPE_IMG
- en: xv Layer 37
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18: Activations of OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47c31c9da943d43b3dedcb77285fd600.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 38
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54a661b6c0348392da9e2b70c1aac569.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 39
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc405149beedd86c3f6229394cafade2.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 41
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/16c68e8951360bbfcfb44a02733f3623.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 42
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2e20919a103b81b0828bf4b010e5aa8.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 43
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca14720beca69a24f43ffacec664c4f3.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 44
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfbc79c3c7cb538a8e55e76639bdfec5.png)'
  prefs: []
  type: TYPE_IMG
- en: vii Layer 45
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4796f509c4cd4b31911e961acb330ede.png)'
  prefs: []
  type: TYPE_IMG
- en: viii Layer 46
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dcdcdd4a14bb9dfdb96cca1b0b2c8241.png)'
  prefs: []
  type: TYPE_IMG
- en: ix Layer 47
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6efec26a798cb616c5bdedee57ef32f9.png)'
  prefs: []
  type: TYPE_IMG
- en: x Layer 48
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78c11f4ba43b9f05d811c6368fdace45.png)'
  prefs: []
  type: TYPE_IMG
- en: xi Layer 49
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9803fa2268b105e3529233172fa77399.png)'
  prefs: []
  type: TYPE_IMG
- en: xii Layer 50
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ccff2e4a1ff0282bd004eb2246a931ec.png)'
  prefs: []
  type: TYPE_IMG
- en: xiii Layer 51
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/963565d19758efc5509476df6d162358.png)'
  prefs: []
  type: TYPE_IMG
- en: xiv Layer 52
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba2cd9194b1bb8585b116458e8d1110f.png)'
  prefs: []
  type: TYPE_IMG
- en: xv Layer 53
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 19: Activations of OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb46ba7fdb48452bfaff9ac7de68a16c.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 54
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7852dd629cc90a6e515b6cbb50d4fd6a.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 55
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a57df8454fae53ce8404956233ee23df.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 56
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e4fdbcfd6b31ad441e2bd3a9d44d026.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 57
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1c6f0f66877e7787afda1980c977673.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 58
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e02ad09e2fca7889579cba759af42d1.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 59
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d779705eab57a83d6935bde8c787cf7b.png)'
  prefs: []
  type: TYPE_IMG
- en: vii Layer 60
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/949c7dd052c2900e4c4255b203257cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: viii Layer 61
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ecccfdb11821a1280815f1e4f7fde5e.png)'
  prefs: []
  type: TYPE_IMG
- en: ix Layer 62
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03d452a78c30af96caa0642aa4aee368.png)'
  prefs: []
  type: TYPE_IMG
- en: x Layer 63
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 20: Layer Map OPT-66B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ebf1bf7640fa5915853470331b8ca79.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Layer 0
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad5b68a9228a73eee11006363aadd240.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Layer 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/38678d818734dc5f5b18cf3d6721323b.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Layer 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d835812846e4281937220cde3a1197cb.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Layer 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31ca48fa846ca66a21752f68b55fba53.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Layer 4
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c57d56479793341b7abe26c88d926254.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Layer 5
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b2d6635be18c54c7c55379b6d3b674e.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) Layer 6
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dcc7498c6a717ede854b215e2fa07e56.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) Layer 7
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ea199eb0ff05646ee582eaec8ad1c88.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) Layer 8
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12b744eca52c6f69db8ff5f7def18732.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) Layer 9
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d4ad1f86bb080d9b79c96c459074508.png)'
  prefs: []
  type: TYPE_IMG
- en: (k) Layer 10
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8707a3679c12a725ce2aa0946968cf51.png)'
  prefs: []
  type: TYPE_IMG
- en: (l) Layer 11
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f9641a8ccbc754066182b748bcd2e07.png)'
  prefs: []
  type: TYPE_IMG
- en: (m) Layer 12
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/800dafc7ca0d100b4aaae8cdae78049d.png)'
  prefs: []
  type: TYPE_IMG
- en: (n) Layer 13
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d6c0dfa0a7a98a038b5f11bf7497ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: (o) Layer 14
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 21: Activations of LLaMa-7B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36de8fd8686b083eeb7f3b8b25310dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 15
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/417334cdc34afea9cc850e6dfae5a394.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 16
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bc658786ca7ff49b2e47d8cb10aa877.png)'
  prefs: []
  type: TYPE_IMG
- en: iii Layer 17
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe44a9191d7a6d3d2da24bb8a5d8bce3.png)'
  prefs: []
  type: TYPE_IMG
- en: iv Layer 18
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/092085b055556e63b4e80a674a3cc3c4.png)'
  prefs: []
  type: TYPE_IMG
- en: v Layer 19
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/848a981eae331776c290954676a571bf.png)'
  prefs: []
  type: TYPE_IMG
- en: vi Layer 20
  prefs: []
  type: TYPE_NORMAL
- en: vii Layer 21
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9faf90fdd3f7eef35b3a80e2993592b.png)'
  prefs: []
  type: TYPE_IMG
- en: viii Layer 22
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc013e92d7f183cae974578843101d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: ix Layer 20
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1396fde67b8c1b3f536560e3993d40e0.png)'
  prefs: []
  type: TYPE_IMG
- en: x Layer 24
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30ce8509ed2391764cf3df2a0f835902.png)'
  prefs: []
  type: TYPE_IMG
- en: xi Layer 25
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f704fc2f9f058a13872c2a709c3cb425.png)'
  prefs: []
  type: TYPE_IMG
- en: xii Layer 26
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d643c9c0a857fbf82e8de078c17487cf.png)'
  prefs: []
  type: TYPE_IMG
- en: xiii Layer 27
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a467fb66a8bef84c7b1f4f7e36c08bc.png)'
  prefs: []
  type: TYPE_IMG
- en: xiv Layer 28
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac1b96fed00f4361d58c03f62d443ccd.png)'
  prefs: []
  type: TYPE_IMG
- en: xv Layer 29
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 22: Activations of LLaMa-7B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99ac4bc0c30914c60af6e211ec6ad173.png)'
  prefs: []
  type: TYPE_IMG
- en: i Layer 30
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd4f801419e27cc79efa0c20d5a61ce0.png)'
  prefs: []
  type: TYPE_IMG
- en: ii Layer 31
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 23: Activations of LLaMa-7B'
  prefs: []
  type: TYPE_NORMAL
