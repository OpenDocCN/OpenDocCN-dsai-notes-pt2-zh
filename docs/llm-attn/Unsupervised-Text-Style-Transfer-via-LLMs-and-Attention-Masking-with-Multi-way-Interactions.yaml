- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:02:48'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way
    Interactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13647](https://ar5iv.labs.arxiv.org/html/2402.13647)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lei Pan¹, Yunshi Lan¹, Yang Li², Weining Qian¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹ East China Normal University, ² Alibaba Group
  prefs: []
  type: TYPE_NORMAL
- en: leipan@stu.ecnu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: ly200170@alibaba-inc.com, {yslan,wnqian}@dase.ecnu.edu.cn *Corresponding author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unsupervised Text Style Transfer (UTST) has emerged as a critical task within
    the domain of Natural Language Processing (NLP), aiming to transfer one stylistic
    aspect of a sentence into another style without changing its semantics, syntax,
    or other attributes. This task is especially challenging given the intrinsic lack
    of parallel text pairings. Among existing methods for UTST tasks, attention masking
    approach and Large Language Models (LLMs) are deemed as two pioneering methods.
    However, they have shortcomings in generating unsmooth sentences and changing
    the original contents, respectively. In this paper, we investigate if we can combine
    these two methods effectively. We propose four ways of interactions, that are
    pipeline framework with tuned orders; knowledge distillation from LLMs to attention
    masking model; in-context learning with constructed parallel examples. We empirically
    show these multi-way interactions can improve the baselines in certain perspective
    of style strength, content preservation and text fluency. Experiments also demonstrate
    that simply conducting prompting followed by attention masking-based revision
    can consistently surpass the other systems, including supervised text style transfer
    systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best
    mean metric by $0.5$ absolute percentages respectively, and achieves new SOTA
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way
    Interactions
  prefs: []
  type: TYPE_NORMAL
- en: 'Lei Pan¹, Yunshi Lan¹^†^†thanks: *Corresponding author, Yang Li², Weining Qian¹
    ¹ East China Normal University, ² Alibaba Group leipan@stu.ecnu.edu.cn ly200170@alibaba-inc.com,
    {yslan,wnqian}@dase.ecnu.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text Style Transfer (TST) is a widely investigated NLP task that aims to transfer
    one stylistic aspect of a piece of text (e.g., sentiment polarity, formality,
    politeness, etc.) into another style without changing its semantics, syntax, or
    other attributes. Although TST has attracted increased interest from scholars
    and a number of methods have been developed to solve TST problems Shang et al.
    ([2019](#bib.bib28)); Zhang et al. ([2015](#bib.bib42)); Rao and Tetreault ([2018](#bib.bib22));
    Zhang et al. ([2020](#bib.bib43)), these approaches hold an impractical hypothesis
    that a substantial amount of parallel training instances are well-annotated. In
    the absence of a parallel corpus, traditional approaches for supervised learning
    are inapplicable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7381c6d300fb18063171a48184bfd677.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: UTST system via LLMs and Attention Masking (AM) with four-way interactions:
    Prompt-then-AM, AM-then-prompt, knowledge distillation using LLM outputs as signals,
    and in-context learning using AM outputs as demonstrations. The details of attention
    masking module and LLM-based module are displayed at the left side, where the
    black arrow denotes propagation and the red arrow denotes back-propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, researchers also took efforts to develop a set of systems for Unsupervised
    Text Style Transfer Goyal et al. ([2020](#bib.bib4)); Lewis ([2022](#bib.bib9));
    Luo et al. ([2023](#bib.bib15)); Suzgun et al. ([2022a](#bib.bib32)). Among these
    approaches, we spotlight two types of pioneering methods: attention masking and
    LLM-based methods. Attention masking methods utilize attention mechanisms to distinguish
    the stylistic words that contribute most to the style prediction of a sentence
    via a fine-tuned classifier and then substitute these identified words to transfer
    the sentence polarity. LLM-based methods prompt a general-purpose Large Language
    Model to transform a sentence into an arbitrary style without additional training
    or fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, they both have shortcomings. The former methods enable modification
    with controllable edits but the edit to the text is restricted to token-level
    substitution, which easily generates unnatural and unsmooth expressions. The latter
    one enables more flexible text generation but it has a high risk of dramatically
    changing the content of the original text. Therefore, an intuitively appealing
    idea is to combine them together. But we encounter the core questions: How to
    effectively combine attention masking and LLM prompting for UTST tasks? Are there
    any keys to note when we combine them?'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we go deep to explore the possible interactions of LLMs and attention
    masking, including (1) pipeline framework with tuned orders; (2) knowledge distillation
    from LLMs to attention masking model; (3) in-context learning with constructed
    parallel examples. In particular, we first show the baseline methods to solve
    UTST tasks. Then we introduce the multi-way interactions to combine LLMs and attention
    masking with detailed implementation. We conduct experiments with three unparalleled
    datasets and six UTST challenges and conclude that these multi-way interactions
    can improve the baselines in certain perspectives of style strength, content preservation,
    and text fluency. Especially, the one-way pipeline of prompting followed by attention
    masking can achieve the SOTA results on UTST tasks, even compared with supervised
    TST systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the contributions of this study are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We innovatively take the interactions of LLMs and attention masking method into
    the spotlight and discuss the efficient way to combine them for UTST tasks, which
    may benefit general text generation tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We empirically show UTST systems with combining LLMs and attention masking in
    four ways can improve the baselines on different evaluation metrics like style
    strength, content preservation, and text fluency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We achieve the SOTA results of the mean evaluation metric on two commonly-used
    style transfer datasets, namely Yelp-clean and Amazon-clean, via prompting followed
    by attention masking-based revision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For text style transfer, most existing studies hypothesize the existence of
    parallel transferred text. In the task of text style transfer with supervision,
    these parallel corpora are leveraged to learn a model that can convert text from
    one style to another while preserving the original content Shen et al. ([2017](#bib.bib29));
    Prabhumoye et al. ([2018](#bib.bib21)); Fu et al. ([2018](#bib.bib3)); Li et al.
    ([2018](#bib.bib11)); Luo et al. ([2019](#bib.bib14)); Reif et al. ([2021](#bib.bib24)).
    These end-to-end approaches typically involve training on pairs of sentences that
    are semantically equivalent but stylistically distinct, allowing the model to
    capture the nuances of each style. However, in real-world scenarios, we frequently
    encounter cases where the parallel data is unavailable. To solve such unsupervised
    text style transfer tasks, there are two lines of mainstream approaches, that
    aim to implicitly or explicitly model the style-related words in the text respectively
    and then generate a text in the target style Madaan et al. ([2020](#bib.bib16));
    Malmi et al. ([2022](#bib.bib18)); Reid and Zhong ([2021](#bib.bib23)); Mallinson
    et al. ([2022](#bib.bib17)); Wang et al. ([2022a](#bib.bib37)); Vincent et al.
    ([2008](#bib.bib36)); Hu et al. ([2017](#bib.bib6)); Kingma and Welling ([2013](#bib.bib8));
    Fu et al. ([2018](#bib.bib3)); John et al. ([2018](#bib.bib7)); Li et al. ([2020](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Recent advance in unsupervised text style transfer have leveraged deep learning
    methods like Variational Autoencoders (VAE) and Denoising Autoencoders (DAE) to
    modify textual styles while preserving the original content. A notable approach
    involves learning a latent representation that separates style from content, facilitating
    the generation of new text in the target style. [Hu et al.](#bib.bib6) ([2017](#bib.bib6))
    utilize the VAE framework to learn the latent representation of text and employ
    a style classifier to discern the style attribute vector. Similarly, [Fu et al.](#bib.bib3)
    ([2018](#bib.bib3)) employ an adversarial network to train a content encoder,
    with the encoded content vector being transformed by a style-specific decoder.
    [John et al.](#bib.bib7) ([2018](#bib.bib7)) further refined this approach by
    using VAEs to segregate style and content representations, with the decoder combining
    both to generate the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of stream explicitly modeling the style-related information by
    identifying the stylistic spans from the text then conducting edits to the identified
    spans. Attention masking methods have been explored extensively across various
    tasks, such as sentiment alteration, gender bias mitigation, and political slant
    adjustmentLi et al. ([2018](#bib.bib11)); Sudhakar et al. ([2019](#bib.bib31)).
    These methods typically involve disassociating the original style from the content
    and then amalgamating the sanitized content with the intended style to synthesize
    new sentences. A pioneering approach in this domain is the “Delete, Retrieve,
    Generate” framework proposed by Li et al. ([2018](#bib.bib11)), which has been
    further enhanced by subsequent studies like Sudhakar et al. ([2019](#bib.bib31)).
    These methodologies excel in retaining substantial portions of the original content
    while transitioning to the target style. However, they have drawbacks, such as
    compromised fluency in the generated text and a dependence on the retrieval of
    analogous content across both source and target styles during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, multiple studies attempt to leverage LLMs to solve unsupervised text
    style transfer Tian et al. ([2023](#bib.bib35)); Luo et al. ([2023](#bib.bib15));
    Suzgun et al. ([2022a](#bib.bib32)); Reif et al. ([2022](#bib.bib25)). [Suzgun
    et al.](#bib.bib32) ([2022a](#bib.bib32)) first elaborately design the instruction
    for text style transfer, which achieves promising results on USTS tasks under
    zero or few-shot setting. [Reif et al.](#bib.bib25) ([2022](#bib.bib25)) prompt
    LLMs to acquire a collection of candidate transferred text in the target style,
    then a re-ranking process is conducted to further rank this candidate text to
    produce the final prediction. However, these methods rely solely on the LLMs,
    which has a high risk of changing the semantics of the original sentence. In contrast,
    our method combines the LLMs with the attention masking method and discusses the
    multi-way interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Task Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: UTST is a NLP task aiming to convert the style of the given text without parallel
    text pairs. Specifically, we are given two non-parallel corpus $\mathcal{D}^{s_{x}}=\{X_{i}\}$
    respectively. The key of text style transfer is to train a model that enables
    to transfer a text from $X$ with preserving the original content.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Baseline UTST System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We highlight two cutting-edge baseline methods for UTST systems, namely Attention
    masking method Wang et al. ([2022b](#bib.bib38)) and LLM-based method Reif et al.
    ([2022](#bib.bib25)). The former conducts style transfer with attention masking
    and styled filling, where the attention is derived from a classifier trained on
    unparalleled data. The latter prompts LLMs to transfer the text in source style
    to target style, which relies on the highly adaptive behaviour of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Attention Masking Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention masking method consists of a mask predictor and a styled filling model,
    which take charge of identifying the positions of stylistic words in a text and
    predicting new stylistic words at the masked positions, respectively. We display
    the method in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Unsupervised Text
    Style Transfer via LLMs and Attention Masking with Multi-way Interactions").
  prefs: []
  type: TYPE_NORMAL
- en: 'Mask predictor. A RoBERTa-based mask predictor takes a source text $X=\{w_{1},w_{2},...,w_{n}\}$
    to judge the style of the text, where the objective is designed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=-($ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{Y\sim\mathcal{D}^{s_{y}}}s_{y}\cdot\log P_{\text{MaskPredictor}}(Y)).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Regarding inference, following existing UTST studies Lewis ([2022](#bib.bib9)),
    we utilize attention score in the classifier as the style feature to decide the
    mask position, where the scaled attention score is higher than a threshold $\alpha$.
    For simplicity, we denote the above procedure as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{X}^{mask}=\text{MaskPredictor}(X).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Filling model. A BART-based model fine-tuned on processed $\mathcal{D}^{s_{y}}$
    as the target for fine-tuning. We denote this procedure as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{Y}=\text{FillingModel}(\hat{X}^{mask}).$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 4.2 LLM-based Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We leverage LLMs to perform text style transfer. Following existing study Reif
    et al. ([2022](#bib.bib25)), we frame style transfer as a sentence rewriting task
    and prompts with only a natural language instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction: Rewrite the following text in a [$s_{y}$].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where [$s_{y}$] are filled with their instantiation. With the instruction,
    no parallel data is required for prompting. We denote this procedure as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{Y}=\text{LLM}(X)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 5 Pipeline UTST System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The motivation behind is that LLMs have shown outstanding capability of arbitrary
    text transfer even under few-shot setting. But it has the disadvantage of producing
    less controllability in the properties of the style-transferred text than models
    trained on the task-specific training data Reif et al. ([2022](#bib.bib25)). Therefore,
    we propose intuitive pipeline to edit text by attention mask models and LLMs successively,
    which could not only leverage LLMs to conduct arbitrary text transfer at absence
    of parallel text pairs, but also ensure controlability to the semantics of the
    text by trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Prompt-then-AM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first conduct text style transfer by prompting the LLMs as illustrated in
    Section [4.2](#S4.SS2 "4.2 LLM-based Method ‣ 4 Baseline UTST System ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions"),
    which results in a sentence with the target style. This would serve as an intermediate
    prediction, which might encounter over-transfer of text. Hence, we apply attention
    masking, which are pre-trained on the non-parallel text pairs, to the intermediate
    prediction. To preserve the content of the intermediate prediction, we tune the
    threshold $\alpha$ to keep more words in the sentences and request the attention
    masking model to predict masked tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Take the sentence “It is awful.” as an example, after prompting the LLMs, the
    input has been transferred into “It is unpleasant.” as the intermediate prediction.
    After further applying the attention masking model to the intermediate prediction,
    the sentence is further rewritten as “It is wonderful.” as the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 AM-then-prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alternatively, we first apply attention masking model to edit the original
    text, which is inherently reliable at producing text that looks like the training
    corpus. This serves as an intermediate prediction. Then we prompt LLMs to rewrite
    the intermediate prediction with keeping the semantics unchanged, the prompt of
    which is displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction: Refine the following text without changing its semantic: [$X$].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the outputs of LLMs are extracted as the final prediction. We still take
    the sentence “It is awful.” as an example, the attention masking model transfers
    the text to “It is good.” then we request the LLMs to paraphrase the sentence
    and obtain the final prediction “It is wonderful.”.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Using LLM Outputs as Signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To further fuse the knowledge from LLMs with trained model, we propose to conduct
    knowledge distillation for text style transfer. We utilize a LLM as a teacher
    to generate teaching data and improve the performance of a smaller student model,
    that is attention masking model, by the generated teaching data. Instead of directly
    considering the outputs of LLMs as prediction, distilling knowledge from the LLMs
    as training signals, which can be considered as a type of label smoothing regularization Hu
    et al. ([2022](#bib.bib5)), makes the student model learns less about the noisy
    edits to the text.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we first sample data from $\mathcal{D}^{s_{x}}$. Take the sentence
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions") as an example, comparing
    the original sentence “It is awful” and the transferred sentence “It is wonderful”
    produced via LLM, we annotate “awful” with a mask. Hence the supervision signal
    is “[0, 0, 1]”, where “0” indicates the current token is not a stylistic token
    or is already a stylistic token in $s_{y}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, we can fine-tune mask predictor with supervision signals $\mathcal{D}^{1}_{par}=\{X^{s_{x}}_{i}\rightarrow
    X^{mask}_{i}\}$ where the objective of Equation ([1](#S4.E1 "In 4.1 Attention
    Masking Method ‣ 4 Baseline UTST System ‣ Unsupervised Text Style Transfer via
    LLMs and Attention Masking with Multi-way Interactions")) becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=-\sum_{X\sim\mathcal{D}^{1}_{par}}\sum_{w_{k}\sim
    X;w_{k}\text{is masked}}\log P_{\text{MaskPredictor}}(w_{k}).$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Instead of training a classifier to identify the style of $X$.
  prefs: []
  type: TYPE_NORMAL
- en: The filling model follows the original implementation but we enhance the fine-tuning
    by augmenting data $\mathcal{D}^{2}_{par}=\{X_{i}^{mask}\rightarrow X^{s_{y}}_{i}\}$
    as the transferred sentence. Comparing with the original filling model, this procedure
    strengthens it by involving the annotations from LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Using AM Outputs as Demonstrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To inject the knowledge of $\mathcal{D}^{s_{x}}$ to LLMs and guide it to produce
    more stylistic sentences as shown in the corpus. In-Context Learning (ICL), as
    a significant prompting strategy, effectively incorporates a small number of demonstrations
    as part of the input Ruis et al. ([2022](#bib.bib27)) to help LLMs comprehend
    the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple studies Su et al. ([2022](#bib.bib30)); Rubin et al. ([2022](#bib.bib26))
    indicate that a good demonstration should share similarity to the current query
    in the perspectives of semantic pattern. Hence, we first encode the current query
    as well as the text in $\mathcal{D}^{s_{x}}$ via bge-base-en-v1.5 Xiao et al.
    ([2023](#bib.bib39)), which is a commonly used sentence-level encoder. Then, we
    extract the final hidden state as the vectorized representations and compute the
    cosine similarity between the current query and sentences in corpus. We denote
    the above procedure as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s_{i}=\text{CosSim}_{X_{i}\in\mathcal{D}^{s_{x}}}(X,X_{i}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $s_{i}$. Next, we prepend the demonstrations to the prompt, which is
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction: ‘‘[$s_{x}$]. ... ‘‘[$s_{x}$ Text’’: [$X^{s_{y}}_{k}$].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please rewrite the following text into a [$s_{y}$] Text’’: [$X$] Text’’:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, the outputs of LLMs are extracted as the prediction, which is featured
    with the style of text corpus from $\mathcal{D}^{s_{y}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 8.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following existing UTST studies Reid and Zhong ([2021](#bib.bib23)); Suzgun
    et al. ([2022a](#bib.bib32)), we choose three datasets for our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yelp²²2[https://github.com/shentianxiao/language-style-transfer/blob/master/data/yelp/sentiment.dev.0](https://github.com/shentianxiao/language-style-transfer/blob/master/data/yelp/sentiment.dev.0)
    Shen et al. ([2017](#bib.bib29)) is a commonly used sentiment polarity classification
    dataset consisting of review data for Yelp. It has $270$K negative sentences as
    non-parallel corpus.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon³³3[https://github.com/lijuncen/Sentiment-and-Style-Transfer](https://github.com/lijuncen/Sentiment-and-Style-Transfer)
    Li et al. ([2018](#bib.bib11)) is a sentiment classification dataset consisting
    of Amazon reviews. It comprises $277$K negative sentences as non-parallel corpus.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Politeness⁴⁴4[https://github.com/tag-and-generate/politeness-dataset](https://github.com/tag-and-generate/politeness-dataset)
    Madaan et al. ([2020](#bib.bib16)) is a dataset designed for classifying text
    as impolite or polite. It is derived from the Enron Email corpus and has $200$K
    test sentences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We employ Yelp-clean and Amazon-clean from existing study⁵⁵5[https://github.com/suzgunmirac/prompt-and-rerank/tree/main/datasets](https://github.com/suzgunmirac/prompt-and-rerank/tree/main/datasets) Suzgun
    et al. ([2022b](#bib.bib33)) as test data, which are the pre-processed data from
    the original text. They both contain $500$ sentences, which are evenly split between
    positive and negative styles.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Comparable Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We take attention masking (AM) method and LLM-based method as baselines. And
    we further explore the effect of the multi-way interactions of LLMs and attention
    masking as described in Section [5](#S5 "5 Pipeline UTST System ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions"),
    [6](#S6 "6 Using LLM Outputs as Signals ‣ Unsupervised Text Style Transfer via
    LLMs and Attention Masking with Multi-way Interactions"), and [7](#S7 "7 Using
    AM Outputs as Demonstrations ‣ Unsupervised Text Style Transfer via LLMs and Attention
    Masking with Multi-way Interactions"), namely Prompt-then-AM, AM-then-prompt,
    LLM-as-signal, and AM-as-demo.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We follow the prior studies Post ([2018](#bib.bib20)) and measure the UTST
    methods with the following evaluation metrics: We employ a RoBERTa model to independently
    train binary style classifiers on the Yelp, Amazon, and Politeness datasets respectively.
    And we leverage the predicted polarity probability to judge if the generated sentence
    matches the desired styles, namely accuracy (ACC). We also employ reference-BLEU
    (r-sBLEU) to measure the overlap between the generated text and the references
    if there is any, and self-BLEU (s-sBLEU) to measure the extent to which the system
    merely replicates the source text. We further include GPT-2 to measure the smoothness
    and naturalness of the generated text by computing PPL.'
  prefs: []
  type: TYPE_NORMAL
- en: Following Narasimhan et al. ([2023](#bib.bib19)); Wang et al. ([2022a](#bib.bib37)),
    we also propose a comprehensive Mean metric, combining ACC, s-sBLEU, and a scaled
    PPL through a geometric mean. When calculating the Mean score, the PPL score was
    exponentially scaled to align its lower-is-better nature with the higher-is-better
    orientation of the other metrics, normalizing its range to $[0,100]$..
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We fine-tuned the pre-trained RoBERTa Liu et al. ([2019](#bib.bib13)) and BART
    Lewis et al. ([2019](#bib.bib10)) models in Equation ([2](#S4.E2 "In 4.1 Attention
    Masking Method ‣ 4 Baseline UTST System ‣ Unsupervised Text Style Transfer via
    LLMs and Attention Masking with Multi-way Interactions")) and ([3](#S4.E3 "In
    4.1 Attention Masking Method ‣ 4 Baseline UTST System ‣ Unsupervised Text Style
    Transfer via LLMs and Attention Masking with Multi-way Interactions")) with specific
    configurations tailored to their respective tasks and datasets. The $\alpha$.
    For RoBERTa, we set the batch size to $64$ epochs using the AdamW optimizer with
    a learning rate of $1e-5$, running $10$ epochs for the Politeness dataset, which
    allowed for quicker convergence given its distinct content. We incorporate ChatGLM2-6B Du
    et al. ([2022](#bib.bib2)) as the backbone of the LLMs used in our methods, due
    to the wide usage and outstanding performance in style transferring Xuanfan and
    Piji ([2023](#bib.bib41)); Tao et al. ([2024](#bib.bib34)). More details can be
    found in Appendix [A](#A1 "Appendix A Appendix ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions").
  prefs: []
  type: TYPE_NORMAL
- en: 9 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.1 Comparison of Different Interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The experimental results presented in Table [1](#S9.T1 "Table 1 ‣ 9.1 Comparison
    of Different Interactions ‣ 9 Results and Analysis ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions") provide a comprehensive
    overview of the performance of various methods for text style transfer across
    multiple datasets. We present a summary of our key findings:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) For UTST tasks, in comparison, LLMs are more likely to produce more fluent
    transferred sentences and AM methods have advantages in transferring sentences
    to targeted style. Therefore, it is intuitive to combine these two paradigms together.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Considering the different interactions between AM and LLMs, there is no
    absolute agreement on the different interactions of LLMs and AM methods regarding
    the performance. But on five out of six settings, Prompt-then-AM method outperforms
    the other methods by mean metric. This indicates that applying prompting followed
    by AM could effectively balance the fluency and style strength.
  prefs: []
  type: TYPE_NORMAL
- en: (3) AM-as-demo method exhibits the lowest PPL in major settings. And it improves
    ACC of LLM-based baseline with observable margin, suggesting that by adding valid
    demonstrations could help LLMs learn better about the target styles though the
    upper bound heavily relies on the capability of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: (4) LLM-as-signal has more advantage in preserving contents of original text
    than AM method. It achieves the best r-sBLEU and s-sBLEU in majority of settings
    but usually shows poor ACC. This may because the signals generated via LLMs are
    initially have poor style strength, distillation from such signals affects the
    attention masking in a negative way.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | Mean ($\uparrow$)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AMAZON | LLM-based | $29$ | $\underline{34.0}$ | $32.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| N$\rightarrow$ | $23.0$ | $178$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prompt-then-AM | $\mathbf{87}$ | $22.2$ | $\mathbf{45.2}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-then-prompt | $\underline{83}$ | $13.5$ | $42.5$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-as-signal | $27$ | $\mathbf{70.9}$ | $34.7$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-as-demo | $56$ | $20.5$ | $40.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| AMAZON | LLM-based | $75$ | $34.7$ | $49.7$ |'
  prefs: []
  type: TYPE_TB
- en: '| P$\rightarrow$ | $\underline{35.3}$ | $123$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prompt-then-AM | $\mathbf{97}$ | $32.7$ | $\mathbf{55.4}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-then-prompt | $75$ | $17.5$ | $45.2$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-as-signal | $55$ | $\mathbf{74.9}$ | $48.3$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-as-demo | $\underline{85}$ | $36.1$ | $49.8$ |'
  prefs: []
  type: TYPE_TB
- en: '| YELP | LLM-based | $67$ | $33.1$ | 51.9 |'
  prefs: []
  type: TYPE_TB
- en: '| N$\rightarrow$ | $\underline{38.7}$ | $128$ | 51.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prompt-then-AM | $\mathbf{93}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-then-prompt | $\underline{84}$ | $36.4$ | 55.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-as-signal | $31$ | $\mathbf{64.2}$ | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-as-demo | 81 | 32.5 | 38.8 | 68 | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| YELP | LLM-based | $80$ | $37.6$ | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '| P$\rightarrow$ | $\underline{37.4}$ | $108$ | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prompt-then-AM | $\mathbf{97}$ | $36.8$ | $\mathbf{57.2}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-then-prompt | $74$ | $23.3$ | 47.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-as-signal | $42$ | $\mathbf{86.0}$ | 49.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-as-demo | $\underline{94}$ | $15.3$ | 51.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Politeness | LLM-based | $66$ | $46.7$ | 50.9 |'
  prefs: []
  type: TYPE_TB
- en: '| I$\rightarrow$ | $-$ | $181$ | 53.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prompt-then-AM | $\mathbf{93}$ | $42.4$ | 55.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-then-prompt | $80$ | $44.5$ | $\mathbf{55.7}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-as-signal | $54$ | $\underline{55.6}$ | 38.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-as-demo | $78$ | $40.7$ | 55.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Politeness | LLM-based | $53$ | $46.2$ | 61 | 46.4 |'
  prefs: []
  type: TYPE_TB
- en: '| P$\rightarrow$ | $-$ | $171$ | 55.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prompt-then-AM | $\mathbf{95}$ | $44.2$ | $\mathbf{57.4}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-then-prompt | $74$ | $48.2$ | 55.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-as-signal | $47$ | $\underline{63.2}$ | 39.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AM-as-demo | $67$ | $34.2$ | 47.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results of text style transfer with multi-level interactions on several
    UTST datasets. “P$\rightarrow$ P” denotes positive style to negative style and
    negative style to positive style, respectively. “P$\rightarrow$ P” denotes polite
    style to impolite style and impolite style to polite style, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Comparison with Other TST Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We further compare our methods based on LLMs and attention masking with other
    text style transfer systems with supervision or without supervision. Table [2](#S9.T2
    "Table 2 ‣ 9.2 Comparison with Other TST Systems ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions")
    displays the results on Yelp-clean. As we can see, even though under supervised
    text style transfer setting, a system can be trained using the parallel corpus,
    it is still short in producing fluent transferred sentences, which results in
    a relatively low PPL value. Unsupervised methods with LLMs can achieve impressive
    results even without any training or fine-tuning on the parallel corpus, but it
    shows flaws in preserving the original semantics. Among all the UTST systems,
    Prompt-then-AM surpasses the other systems on ACC and obtains the highest results
    of mean metric.
  prefs: []
  type: TYPE_NORMAL
- en: We have the similar observation based on Table [3](#S9.T3 "Table 3 ‣ 9.2 Comparison
    with Other TST Systems ‣ 9 Results and Analysis ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions"), which includes the
    results on Amazon-clean. The supervised TST systems have relatively high style
    strength but low fluency. The unsupervised TST systems collaborating with diverse
    LLMs usually generate fluent target sentences but are short in target style. Overall,
    our method of Prompt-then-AM can achieve the best mean score with highest style
    strength.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | Mean ($\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised Text Style Transfer |'
  prefs: []
  type: TYPE_TB
- en: '| $[1]$ CrossAlignment | 73 | 7.8 | 18.3 | 217 | 31.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ BackTrans | 95 | 2.0 | 46.5 | 158 | 50.3 |'
  prefs: []
  type: TYPE_TB
- en: '| $[3]$ MultiDecoder | 46 | 13.0 | 39.4 | 373 | 28.6 |'
  prefs: []
  type: TYPE_TB
- en: '| $[4]$ DeleteOnly | 85 | 13.4 | 33.9 | 182 | 41.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $[4]$ DeleteAndRetrieve | 90 | 14.7 | 36.4 | 180 | 44.4 |'
  prefs: []
  type: TYPE_TB
- en: '| $[5]$ UnpairedRL | 49 | 16.8 | 45.7 | 385 | 31.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $[6]$ DualRL | 88 | 25.9 | 58.9 | 133 | 53.5 |'
  prefs: []
  type: TYPE_TB
- en: '| $[7]$ ST (Multi-Class) | 86 | 26.4 | 63.0 | 175 | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| $[7]$ ST (Conditional) | 93 | 22.9 | 52.8 | 223 | 49.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $[8]$ B-GST | 81 | 21.6 | 46.5 | 158 | 45.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised Text Style Transfer |'
  prefs: []
  type: TYPE_TB
- en: '| $[9]$ Prompt-and-Rerank (GPT2) | 87 | 14.8 | 28.7 | 65 | 51.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $[9]$ Prompt-and-Rerank (GPT-J) | 87 | 23.0 | 47.7 | 80 | 54.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-then-AM | 93 | 26.7 | 31.9 | 59 | $\mathbf{55.4}$ |'
  prefs: []
  type: TYPE_TB
- en: '| AM-then-prompt | $84$ | $36.4$ | 55.2 |'
  prefs: []
  type: TYPE_TB
- en: '| AM-as-demo | 81 | 32.5 | 38.8 | 68 | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-as-signal | 31 | 42.6 | 64.2 | 119 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: A comparison of existing methods on Yelp-clean (N$\rightarrow$ P).
    References: [1] Shen et al. ([2017](#bib.bib29)), [2] Prabhumoye et al. ([2018](#bib.bib21))
    , [3] Fu et al. ([2018](#bib.bib3)), [4] Li et al. ([2018](#bib.bib11)), [5] Xu
    et al. ([2018](#bib.bib40)), [6] Luo et al. ([2019](#bib.bib14)), [7] Dai et al.
    ([2019](#bib.bib1)), [8] Sudhakar et al. ([2019](#bib.bib31)) [9] Reif et al.
    ([2021](#bib.bib24)). The results of other systems are copied from prior study Suzgun
    et al. ([2022a](#bib.bib32)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | Mean ($\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised Text Style Transfer |'
  prefs: []
  type: TYPE_TB
- en: '| $[1]$ Style-Embedding | 47 | 13.1 | 29.0 | 287 | 25.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $[1]$ CrossAligned | 74 | 1.7 | 2.4 | 96 | 33.4 |'
  prefs: []
  type: TYPE_TB
- en: '| $[1]$ DeleteAndRetrieve | 51 | 26.7 | 53.5 | 113 | 41.0 |'
  prefs: []
  type: TYPE_TB
- en: '| $[1]$ TemplateBased | 56 | 31.0 | 65.7 | 200 | 42.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised Text Style Transfer |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-2-Small | 18 | 17.7 | 38.1 | 48 | 34.9 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-2-Medium | 32 | 20.1 | 38.0 | 57 | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-2-Large | 28 | 26.0 | 51.2 | 55 | 41.0 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-2-XL | 32 | 22.3 | 41.4 | 70 | 36.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-Neo-1.3B | 31 | 10.9 | 20.5 | 35 | 36.9 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-Neo-2.7B | 28 | 23.7 | 45.9 | 57 | 38.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $[2]$ GPT-J-6B | 33 | 27.1 | 47.7 | 72 | 38.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-then-AM | 87 | 17.0 | 22.2 | 89 | $\mathbf{45.2}$ |'
  prefs: []
  type: TYPE_TB
- en: '| AM-then-prompt | 83 | 10.0 | 13.5 | 78 | 42.5 |'
  prefs: []
  type: TYPE_TB
- en: '| AM-as-demo | 56 | 19.2 | 20.5 | 54 | 40.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-as-signal | 27 | 44.2 | 70.9 | 187 | 34.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: A comparison of existing methods on Amazon-clean (N$\rightarrow$ P).
    References: [1] Li et al. ([2018](#bib.bib11)), [2] Suzgun et al. ([2022a](#bib.bib32)).
    The results of other systems are copied from prior study Suzgun et al. ([2022a](#bib.bib32)).'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Effect of $\alpha$ in Prompt-then-AM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0af9b92c8c7d4bb5d26afa42c2929c11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: ACC and s-sBLEU of Prompt-then-AM on Yelp-clean (N$\rightarrow$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06986d172db97225aa1ec5f4cf612bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: ACC and PPL of Prompt-then-AM on Yelp-clean (N$\rightarrow$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-based | 67 | 27.7 | 33.1 | 39 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-then-AM | 93 | 26.7 | 31.9 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| AM-then-prompt | $84$ | $36.4$ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-XL |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-based | 22 | 12.7 | 15.1 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-then-AM | 81 | 13.3 | 14.9 | 172 |'
  prefs: []
  type: TYPE_TB
- en: '| AM-then-prompt | 52 | 8.4 | 9.4 | 51 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-J-6B |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-based | 46 | 12.8 | 28.9 | 49 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-then-AM | 87 | 12.4 | 27.6 | 78 |'
  prefs: []
  type: TYPE_TB
- en: '| AM-then-prompt | 69 | 10.2 | 15.8 | 46 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: A comparison of LLM-based methods with different backbone LLMs on
    Yelp-clean (N$\rightarrow$ P).'
  prefs: []
  type: TYPE_NORMAL
- en: From the above experiments, we conclude that Prompt-then-AM shows a superior
    effect among the multiple interactions. In Prompt-then-AM method, $\alpha$ value
    signifies more aggressive edits while a lower value maintains the outputs of LLMs.
    We draw the ACC and s-sBLEU of Prompt-then-AM on Yelp-clean (N$\rightarrow$ in
    Figure [3](#S9.F3 "Figure 3 ‣ 9.3 Effect of 𝛼 in Prompt-then-AM ‣ 9 Results and
    Analysis ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking with
    Multi-way Interactions"). When $\alpha$, prompt-then-AM degrades into LLM-based
    method and when $\alpha$, prompt-then-AM masks out all the intermediate outputs
    and re-prediction using AM model. We observe an negative correlation between ACC
    and s-sBLEU with the increasing $\alpha$. This indicates that with the increasing
    number of edits on the outputs of LLMs, we can enhance the style strength of the
    transferred sentences at the expense of the changing the original semantics.
  prefs: []
  type: TYPE_NORMAL
- en: A positive correlation trend between ACC and PPL can be observed in Figure [3](#S9.F3
    "Figure 3 ‣ 9.3 Effect of 𝛼 in Prompt-then-AM ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions").
    With the increase of $\alpha$.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Different LLMs as Backbones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We further try different LLMs as the backbone for the multi-ways interaction.
    From Table [4](#S9.T4 "Table 4 ‣ Figure 3 ‣ 9.3 Effect of 𝛼 in Prompt-then-AM
    ‣ 9 Results and Analysis ‣ Unsupervised Text Style Transfer via LLMs and Attention
    Masking with Multi-way Interactions"), we observe the similar trends for various
    baselines. Involving AM is able to improve the performance of LLM-based methods.
    And Prompt-then-AM interaction is more efficient in generating comprehensively
    good outputs. It is worth noting that different backbones may decide the upper
    bound of the UTST systems. Methods with ChatGLM2 generally surpass the other systems,
    which indicates it is vital to select a good LLM as the collaborator for unsupervised
    text style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Methods | Sentence |'
  prefs: []
  type: TYPE_TB
- en: '| Source text | there are no smiles and no customer |'
  prefs: []
  type: TYPE_TB
- en: '|  | service. |'
  prefs: []
  type: TYPE_TB
- en: '| AM | there are always smiles and great |'
  prefs: []
  type: TYPE_TB
- en: '|  | customer service. |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-based | there is a lack of smiles and a lack of |'
  prefs: []
  type: TYPE_TB
- en: '|  | customer service. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-then-AM | there are plenty of smiles and great |'
  prefs: []
  type: TYPE_TB
- en: '|  | customer service. |'
  prefs: []
  type: TYPE_TB
- en: '| AM-then-prompt | there is always a smile and exceptional |'
  prefs: []
  type: TYPE_TB
- en: '|  | customer service. |'
  prefs: []
  type: TYPE_TB
- en: '| AM-as-demo | They make you feel at ease and their |'
  prefs: []
  type: TYPE_TB
- en: '|  | smiles are contagious. |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-as-signal | delightful smiles and customer service. |'
  prefs: []
  type: TYPE_TB
- en: '| Target text | there were plenty of smiles and cus- |'
  prefs: []
  type: TYPE_TB
- en: '|  | tomer service. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Generated outputs of different methods on Yelp-clean (N$\rightarrow$
    P). The improper transferring is annotated with red color.'
  prefs: []
  type: TYPE_NORMAL
- en: In Table [5](#S9.T5 "Table 5 ‣ 9.5 Case Study ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions"),
    we examine the outputs generated by different methods. AM model is able to generate
    a sentence that is close to the target sentence. LLM-based approach generates
    a sentence that conveys a slightly negative sentiment, which fails to produce
    a positive sentence. AM-then-prompt method is able to partially change the sentiment
    but the unexpected revision by LLMs leads to a token “exceptional”, which cannot
    match the original semantic meaning. LLM-as-signal largely changes the syntax
    of the sentence, leading to an undesirable prediction. Unexpectedly, AM-as-demo
    dramatically change the source sentence. This could happen when the demonstration
    for in-context learning misguides the semantic transfer to the LLMs. In comparison,
    Prompt-then-AM is more controllable in the perspective of preserving the semantics
    and transferring the polarity with rational edits.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we investigate multi-way interaction of LLMs and attention masking
    for solving UTST tasks. Specifically, we consider: pipeline framework with tuned
    orders; knowledge distillation from LLMs to attention masking model; in-context
    learning with constructed parallel examples. We show UTST systems with combining
    LLMs and attention masking in four ways can improve the baselines on different
    evaluation metrics like style strength, content preservation, and text fluency.
    We further show that simply conduct prompting and attention masking-based revision
    can consistently surpasses the other interactions and achieve SOTA results in
    Yelp-clean and Amazon-clean datasets even compared with supervised text style
    transfer systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have hallucinations. We have noticed that LLMs sometimes get confused by
    the instructions in a prompt and sometimes generate content that is completely
    unrelated to the input. They mix up the text that needs to be transformed with
    the instructions themselves, especially during In-Context Learning methods. This
    happens more when the prompts are too long and it is hard for the model to differentiate
    between instructions and the actual content. In the future, we will dive into
    the research of easing hallucination issue in text style transfer and make more
    controllable text style transfer with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our study on Unsupervised Text Style Transfer, no personal information was
    collected, ensuring the ethical integrity of our research. We emphasize that this
    endeavor involved no risk, as participants were neither exposed to harmful materials
    nor engaged in hazardous activities. Our commitment to ethical standards is unwavering.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. 2019.
    Style transformer: Unpaired text style transfer without disentangled latent representation.
    *arXiv preprint arXiv:1905.05621*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with
    autoregressive blank infilling. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    320–335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2018) Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui
    Yan. 2018. Style transfer in text: Exploration and evaluation. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal et al. (2020) Navita Goyal, Balaji Vasan Srinivasan, Anandhavelu Natarajan,
    and Abhilasha Sancheti. 2020. Multi-style transfer with discriminative feedback
    on disjoint corpus. *arXiv preprint arXiv:2010.11578*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Chengming Hu, Xuan Li, Dan Liu, Xi Chen, Ju Wang, and Xue
    Liu. 2022. Teacher-student architecture for knowledge learning: A survey. *arXiv
    preprint arXiv:2210.17332*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2017) Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov,
    and Eric P Xing. 2017. Toward controlled generation of text. In *International
    conference on machine learning*, pages 1587–1596\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John et al. (2018) Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova.
    2018. Disentangled representation learning for non-parallel text style transfer.
    *arXiv preprint arXiv:1808.04339*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis (2022) Armanda Lewis. 2022. [Multimodal large language models for inclusive
    collaboration learning tasks](https://doi.org/10.18653/v1/2022.naacl-srw.26).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies: Student Research Workshop*,
    pages 202–210, Hybrid: Seattle, Washington + Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete,
    retrieve, generate: a simple approach to sentiment and style transfer. *arXiv
    preprint arXiv:1804.06437*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Xiao Li, Guanyi Chen, Chenghua Lin, and Ruizhe Li. 2020. Dgst:
    a dual-generator network for text style transfer. *arXiv preprint arXiv:2010.14557*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2019) Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Zhifang Sui, and Xu Sun. 2019. A dual reinforcement learning framework for unsupervised
    text style transfer. *arXiv preprint arXiv:1905.10060*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023) Guoqing Luo, Yu Tong Han, Lili Mou, and Mauajama Firdaus.
    2023. Prompt-based editing for text style transfer. *arXiv preprint arXiv:2301.11997*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2020) Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos,
    Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, and Shrimai Prabhumoye.
    2020. Politeness transfer: A tag and generate approach. *arXiv preprint arXiv:2004.14257*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mallinson et al. (2022) Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei
    Severyn. 2022. Edit5: Semi-autoregressive text-editing with t5 warm-start. *arXiv
    preprint arXiv:2205.12209*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malmi et al. (2022) Eric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin,
    Jakub Adamek, Daniil Mirylenka, Felix Stahlberg, Sebastian Krause, Shankar Kumar,
    and Aliaksei Severyn. 2022. [Text generation with text-editing models](https://doi.org/10.18653/v1/2022.naacl-tutorials.1).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies: Tutorial Abstracts*,
    pages 1–7, Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narasimhan et al. (2023) Sharan Narasimhan, H Pooja, Suvodip Dey, and Maunendra Sankar
    Desarkar. 2023. On text style transfer via style-aware masked language models.
    In *Proceedings of the 16th International Natural Language Generation Conference*,
    pages 362–374.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post (2018) Matt Post. 2018. A call for clarity in reporting bleu scores. *arXiv
    preprint arXiv:1804.08771*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prabhumoye et al. (2018) Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. Style transfer through back-translation. *arXiv preprint
    arXiv:1804.09000*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. Dear sir or madam,
    may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality
    style transfer. *arXiv preprint arXiv:1803.06535*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reid and Zhong (2021) Machel Reid and Victor Zhong. 2021. Lewis: Levenshtein
    editing for unsupervised text style transfer. *arXiv preprint arXiv:2105.08206*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reif et al. (2021) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2021. A recipe for arbitrary text style transfer
    with large language models. *arXiv preprint arXiv:2109.03910*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reif et al. (2022) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer
    with large language models. In *Proceedings of the 60th Annual Meeting of the
    Association for Computational Linguistics (Volume 2: Short Papers)*, pages 837–848.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    Learning to retrieve prompts for in-context learning. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2655–2671, Seattle, United States.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruis et al. (2022) Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim
    Rocktäschel, and Edward Grefenstette. 2022. Large language models are not zero-shot
    communicators. *arXiv preprint arXiv:2210.14986*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. (2019) Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan
    Zhao, Shuming Shi, and Rui Yan. 2019. Semi-supervised text style transfer: Cross
    projection in latent space. *arXiv preprint arXiv:1909.11493*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2017) Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola.
    2017. Style transfer from non-parallel text by cross-alignment. *Advances in neural
    information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2022) Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al.
    2022. Selective annotation makes language models better few-shot learners. *arXiv
    pr arXiv:2209.01975*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudhakar et al. (2019) Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran.
    2019. Transforming delete, retrieve, generate approach for controlled text style
    transfer. *arXiv preprint arXiv:1908.09368*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suzgun et al. (2022a) Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022a.
    Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style
    transfer with small language models. *arXiv preprint arXiv:2205.11503*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suzgun et al. (2022b) Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022b.
    [Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style
    transfer with small language models](https://doi.org/10.18653/v1/2022.emnlp-main.141).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 2195–2222, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2024) Zhen Tao, Dinghao Xi, Zhiyu Li, Liumin Tang, and Wei Xu.
    2024. Cat-llm: Prompting large language models with text style definition for
    chinese article-style transfer. *arXiv preprint arXiv:2401.05707*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2023) Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, and Yunshi
    Lan. 2023. R ³ prompting: Review, rephrase and resolve for chain-of-thought reasoning
    in large language models under noisy context. *arXiv preprint arXiv:2310.16535*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent et al. (2008) Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine
    Manzagol. 2008. Extracting and composing robust features with denoising autoencoders.
    In *Proceedings of the 25th international conference on Machine learning*, pages
    1096–1103.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, and
    Yongyi Mao. 2022a. Text style transferring via adversarial masking and styled
    filling. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*, pages 7654–7663.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, and
    Yongyi Mao. 2022b. Text style transferring via adversarial masking and styled
    filling. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*, pages 7654–7663.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof.
    2023. C-pack: Packaged resources to advance general chinese embedding. *arXiv
    preprint arXiv:2309.07597*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xiaodong Zhang,
    Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation:
    A cycled reinforcement learning approach. *arXiv preprint arXiv:1805.05181*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xuanfan and Piji (2023) Ni Xuanfan and Li Piji. 2023. A systematic evaluation
    of large language models for natural. In *Proceedings of the 22nd Chinese National
    Conference on Computational Linguistics (Volume 2: Frontier Forum)*, pages 40–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level
    convolutional networks for text classification. *Advances in neural information
    processing systems*, 28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Yi Zhang, Tao Ge, and Xu Sun. 2020. Parallel data augmentation
    for formality style transfer. *arXiv preprint arXiv:2005.07522*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For training models, we use 2 NVIDIA V100 GPUs on a machine running Ubuntu
    20.04 on a 10-Core Intel CPU. We choose RoBERTa-base as the baseline model for
    the classifier and trained on three datasets: Yelp, Amazon, and Politeness, respectively.
    For each training session, we set the batch size to 64, the number of epochs to
    10, and the learning rate to 1e-5\. We choose BART-base as our filling model and
    trained on different datasets. To prepare the training data for BART on the Yelp
    dataset, we set the mask predictor’s $\alpha$ to 0.5\. For the Politeness dataset,
    we set the mask predictor’s $\alpha$ to 0.35. the detailed implementation inforamtion
    can be found in Table [6](#A1.T6 "Table 6 ‣ Appendix A Appendix ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Parameters | Settings |'
  prefs: []
  type: TYPE_TB
- en: '| RoBERTa-base | Batch size | 64 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Epochs | 20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning rate | 1e-5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\alpha$ | 0.5 for Yelp and Amazon |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 0.35 for Politeness |'
  prefs: []
  type: TYPE_TB
- en: '| BART-base | Batch size | 16 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Epochs | 10 for Yelp and Amazon |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 5 for Politeness |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B | Temperature | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Implementation details.'
  prefs: []
  type: TYPE_NORMAL
