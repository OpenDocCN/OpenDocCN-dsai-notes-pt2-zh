- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00242](https://ar5iv.labs.arxiv.org/html/2404.00242)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jinwei Yao^(1,4,)  Kaiqi Chen^(2,∗)  Kexun Zhang^(3,∗)  Jiaxuan You ⁴  Binhang
    Yuan⁵  Zeke Wang^(2,†)  Tao Lin^(1,)
  prefs: []
  type: TYPE_NORMAL
- en: jinwei.yao1114@gmail.com;  {chiaki_cage,wangzeke}@zju.edu.cn;
  prefs: []
  type: TYPE_NORMAL
- en: kexunz@andrew.cmu.edu;  jiaxuan@illinois.edu;
  prefs: []
  type: TYPE_NORMAL
- en: biyuan@ust.hk;  lintao@westlake.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: ¹Westlake University  ²Zhejiang University  ³Carnegie Mellon University
  prefs: []
  type: TYPE_NORMAL
- en: ⁴University of Illinois Urbana-Champaign  ⁵Hong Kong University of Science and
    Technology Equal contribution. Work was done during Jinwei’s visit to Westlake
    University.Corresponding author.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Decoding using tree search can greatly enhance the inference quality for transformer-based
    Large Language Models (LLMs). Depending on the guidance signal, it searches for
    the best path from root to leaf in the tree by forming LLM outputs to improve
    controllability, reasoning ability, alignment, et cetera. However, current tree
    decoding strategies and their inference systems do not suit each other well due
    to redundancy in computation, memory footprints, and memory access, resulting
    in inefficient inference. To address this issue, we propose DeFT, an IO-aware
    tree attention algorithm that maintains memory-efficient attention calculation
    with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided
    Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction
    of memory reads/writes for the KV cache between GPU global memory and on-chip
    shared memory as much as possible; (2) Attention Calculation: we calculate partial
    attention of each QKV groups in a fused kernel then apply a Tree-topology-aware
    Global Reduction strategy to get final attention. Thanks to a reduction in KV
    cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup
    of 1.7-2.4$\times$ in end-to-end latency across two practical reasoning tasks
    over the SOTA attention algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) (Achiam et al., [2023](#bib.bib1); Touvron et al.,
    [2023a](#bib.bib26); [b](#bib.bib27)) are extensively utilized across a range
    of tasks like chatbot (Roller et al., [2020](#bib.bib23)), code generation (Mark
    et al., [2021](#bib.bib19)), reasoning (Yao et al., [2023](#bib.bib33); Besta
    et al., [2023](#bib.bib3); Ning et al., [2023](#bib.bib21)), etc. Tree search
    algorithms (Graves, [2012](#bib.bib8); Lu et al., [2022](#bib.bib18); Liu et al.,
    [2023](#bib.bib15)) are frequently integrated with LLMs to meet Service-Level-Objectives
    (SLOs) (Yao et al., [2023](#bib.bib33); Liu et al., [2023](#bib.bib15); Anderson
    et al., [2017](#bib.bib2); Post & Vilar, [2018](#bib.bib22); Hokamp & Liu, [2017](#bib.bib9)).
    In order to cover a large search space, numerous tokens will be generated with
    significant computational and memory overhead, resulting in greater latency during
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence-based decoding and tree-based decoding represent two prominent approaches
    in handling LLM inference (Yao et al., [2023](#bib.bib33)). Sequence-based decoding
    samples a single sequence of tokens every time, while tree-based decoding maintains
    multiple sequences with common prefixes as a tree structure, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeFT: Flash Tree-attention with IO-Awareness for
    Efficient Tree-search-based LLM Inference"). Since nodes in the forms of the tree
    can be shared computationally and in memory while that of the sequence cannot,
    applying tree-search-based tasks directly to sequence-based decoding causes three
    levels of redundancy: (1) *memory storage*, especially the KV cache (Kwon et al.,
    [2023](#bib.bib14); Zheng et al., [2023](#bib.bib35)); (2) *computation*, especially
    the computation for common prompts among sequences in a batch (Zheng et al., [2023](#bib.bib35));
    (3) *memory access*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing work of tree-based decoding focuses on the first two levels while
    largely ignoring the third yet the most important one–*memory access*, given the
    nature of memory-bounded LLM inference (Shazeer, [2019](#bib.bib24); Cai et al.,
    [2024](#bib.bib4); Kim et al., [2023](#bib.bib13)). As for sequence-based decoding
    methods optimize the memory access for the aspects of partial results (i.e., $\mathbf{Q}\mathbf{K}^{\top}$)
    during attention calculations (Dao et al., [2022](#bib.bib5); [2023](#bib.bib6);
    Hong et al., [2023](#bib.bib11)). However, their effectiveness in tree-based decoding
    is limited. In particular, these optimizations are unable to address the potential
    bottleneck posed by the KV cache IO when dealing with a large number of tokens,
    as illustrated in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b950ee8c74fc244dc1a32617cc044171.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison of Sequence-based decoding and Tree-based decoding. An
    illustration of Sequence-based decoding and Tree-based decoding with the example
    of Chain-of-thoughts (CoT) (Wei et al., [2022](#bib.bib29)) and Tree-of-thoughts
    (ToT) (Yao et al., [2023](#bib.bib33)) in Besta et al. ([2023](#bib.bib3)).'
  prefs: []
  type: TYPE_NORMAL
- en: As a remedy, in this paper, we resort to the key attention component during
    the decoding process. Orthogonal to the traditional attention mechanisms in sequence-based
    decoding, tree attention (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4))—specifically
    designed to handle hierarchical or tree-structured tokens in tasks such as parallel
    decoding—can reduce the kernel launching, computation and KV cache storage overheads
    for attention calculations. However, this line of research does not further leverage
    the tree topology to reduce IO when calculating attention, and thus still not
    fully IO-aware for both (i) partial result (i.e., $\mathbf{Q}\mathbf{K}^{\top}$) (Cai
    et al., [2024](#bib.bib4)) due to the lack of tiling and kernel fusion (Dao et al.,
    [2022](#bib.bib5)); and (ii) KV cache in a tree structure (Miao et al., [2023](#bib.bib20)).
    These limitations hinder their effectiveness in optimizing memory access during
    tree-based decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of efficiency in CoT and ToT. The task is document merging
    from Besta et al. ([2023](#bib.bib3)). CoT is implemented with Sequence-based
    decoding while ToT is with Tree-based decoding. The total generated tokens of
    CoT is only 525 while 24,026 in ToT, resulting in inefficiency in end-to-end latency
    (second) and IO (TB). IO mainly consists of three parts as follows. (i) KV cache:
    IO-KV; (ii) $QK^{T}$: IO-$QK^{T}$; (iii) Softmax$(QK^{T})$: IO-$Softmax$. Baselines:
    (i) Flash-Decoding: attention in Flash-Decoding (Dao et al., [2023](#bib.bib6));
    (ii) Tree Attention: tree attention in Medusa (Cai et al., [2024](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Metrics |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Latency | IO-KV | IO-$QK^{T}$ | IO-$Softmax$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Flash-Decoding + CoT | 21 | 0.6 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Flash-Decoding + ToT | 450 | 30.7 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Tree Attention + ToT | 660 | 8.6 | 0.7 | 1.3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| DeFT(ours) + ToT | 272 | 8.6 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Speed up over best baseline | $1.66\times$ | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: To bridge the above gap, we propose DeFT, an IO-aware tree attention algorithm
    with two key insights. First, the IO workload for queries (Q) is negligible compared
    to that of KV cache, primarily because the maximum query length typically corresponds
    to root-to-leaf paths in the tree, resulting in relatively short queries (e.g. dozens
    of tokens) compared with KV cache length in each node (e.g. hundreds/thousands
    of tokens). Second, in sequence-based decoding, each KV cache entry corresponds
    to a unique query, whereas in tree-based decoding, multiple queries can share
    their common ancestor’s KV cache during attention calculation, benefiting not
    only in terms of KV cache storage but also in reducing IOs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building upon these two insights, in the first phase of DeFT—QKV Preparation,
    inspired by tiling technique in Dao et al. ([2022](#bib.bib5); [2023](#bib.bib6)),
    we split the decoding tree by nodes as each node has sequence-granularity of tokens
    and KV cache. Then we group the KV cache of each node with all queries that share
    it in the decoding tree, to minimize the IO of KV cache with negligible IO overhead
    of queries. In the second phase of DeFT—Attention Calculation, we adopt a fused
    kernel to get partial attention with LogSumExp of QKV groups calculated in phase
    1, and conduct the tree-topology-aware global reduction inspired by Flash-Decoding (Dao
    et al., [2023](#bib.bib6)). We summarize our contributions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a simple but hardware-efficient tree attention algorithm–DeFT, which
    is IO-aware for both KV cache in a tree structure and partial results (i.e., $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement DeFT on OpenAI Triton (Tillet et al., [2019](#bib.bib25)) to gain
    precise management over memory access and fuse all attention operations into a
    single GPU kernel.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We theoretically justify the superiority of DeFT over the existing attention
    algorithms (Wolf et al., [2019](#bib.bib31); Dao et al., [2023](#bib.bib6); Cai
    et al., [2024](#bib.bib4); Miao et al., [2023](#bib.bib20)) in terms of IO complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We empirically verify its effectiveness on practical reasoning tasks. DeFT can
    achieve a speedup of 1.7-2.4 times across two practical reasoning tasks compared
    with the SOTA attention algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tree-based Decoding.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tree-based decoding, exemplified by beam search (Graves, [2012](#bib.bib8)),
    has been pivotal in NLP, handling lexical and logical constraints (Anderson et al.,
    [2017](#bib.bib2); Post & Vilar, [2018](#bib.bib22); Hokamp & Liu, [2017](#bib.bib9)),
    mitigating gender bias (Lu et al., [2021](#bib.bib17)), achieving communicative
    goals (Holtzman et al., [2018](#bib.bib10)), and improving alignment (Liu et al.,
    [2023](#bib.bib15)). Recent strategies enhance LLM reasoning (Yao et al., [2023](#bib.bib33);
    Besta et al., [2023](#bib.bib3); Ning et al., [2023](#bib.bib21)), using search
    trees with parallel hypothesis generation and selection based on scoring functions.
    Some score candidates per token (Dathathri et al., [2019](#bib.bib7); Lu et al.,
    [2021](#bib.bib17); [2022](#bib.bib18)), others per reasoning step (Welleck et al.,
    [2022](#bib.bib30); Uesato et al., [2022](#bib.bib28); Xie et al., [2023](#bib.bib32)).
    Efficiency in tree decoding remains underexplored despite various search algorithms’
    application, such as A* (Lu et al., [2022](#bib.bib18)) and Monte-Carlo Tree Search
    (Liu et al., [2023](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: Memory-efficient Attention Algorithms.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing memory-efficient attention algorithms target sequence-based decoding.
    FlashAttention (Dao et al., [2022](#bib.bib5)) improves self-attention computation
    in LLM training via tiling and kernel fusion, reducing IOs. Flash-Decoding (Dao
    et al., [2023](#bib.bib6)) extends this, enhancing parallelism by dividing K and
    V and introducing global reduction to gather partial attention results, enabling
    efficient decoding for long sequences. Unluckily, applying these memory-efficient
    algorithms to the tree-based decoding overlooks redundancy in IO of tree-structured
    KV cache, which is the focus of DeFT.
  prefs: []
  type: TYPE_NORMAL
- en: Tree Attention.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Integrated into LLM inference, tree attention reduces computation, storage,
    and kernel launching overheads (Miao et al., [2023](#bib.bib20)). Tree-structured
    token candidates undergo parallel decoding, with SpecInfer (Miao et al., [2023](#bib.bib20))
    introducing a topology-aware causal masked tree attention algorithm, dynamically
    updating a causal mask to capture relationships among tokens. Medusa (Cai et al.,
    [2024](#bib.bib4)) uses a similar mechanism with a static causal mask, while other
    works (Zhao et al., [2023](#bib.bib34); Liu et al., [2024](#bib.bib16)) adopt
    analogous approaches to enhance attention calculation efficiency. However, unlike
    DeFT, these existing works utilizing tree attention do not take memory access
    into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion on Tree-based Decoding.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To improve inference efficiency by generating multiple tokens in each iteration,
    tree-based decoding (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4);
    Zhao et al., [2023](#bib.bib34); Liu et al., [2024](#bib.bib16)) could have sequential
    past KV cache with tree-structured queries. In DeFT, we propose another tree-based
    decoding with tree-structured past KV cache. A general tree-based decoding could
    have both tree-structured past KV and queries by combining the two aforementioned
    tree-decoding paradigms mentioned. Details are discussed in Appendix [A.2](#A1.SS2
    "A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: Storage Optimization of Tree-based Decoding.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM frameworks optimized for tree-based decoding (Kwon et al., [2023](#bib.bib14);
    Zheng et al., [2023](#bib.bib35)) focus on memory storage efficiency. vLLM (Kwon
    et al., [2023](#bib.bib14)) enhances GPU memory utilization, allowing sequences
    from the same parent to share KV cache storage. SGLang (Zheng et al., [2023](#bib.bib35))
    supports dynamic KV cache management during multi-round interactions with LLMs,
    improving memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 3 DeFT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we start by introducing the background knowledge of LLM inference,
    upon which we outline the system overview of DeFT. As a key component of DeFT,
    we present DeFT Attention Kernel, which not only reduces memory access of tree
    KV but also adopts a fused kernel to eliminate the memory access of partial results
    like $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax operations. We further theoretically
    analyze DeFT’s IO with existing attention algorithms to justify its advances.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Preliminary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM inference and its bottleneck.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM inference consists of two stages, namely the (1) prefill stage and (2) decoding
    stage. In the prefill stage, a prompt is tokenized as the initial input of LLM.
    Upon receiving the prefill stage’s outputs from LLM, a new token then serves as
    the input for the decoding stage. The decoding stage is auto-regressive, where
    each output token from the previous step will be used as the input token for the
    next decoding step.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the sequential process of auto-regressive decoding, LLM inference is
    memory-bound (Shazeer, [2019](#bib.bib24); Kim et al., [2023](#bib.bib13); Cai
    et al., [2024](#bib.bib4)), wherein every forward pass requires transferring all
    model parameters and KV cache from slower but larger High-Bandwidth Memory (HBM)
    to the faster but much smaller shared memory of the GPU (Jia & Van Sandt, [2021](#bib.bib12))
    ¹¹1 A100’s HBM has 1.5-2TB/s bandwidth and 40-80GB; its shared memory has 19TB/s
    bandwidth and 20MB. .
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for DeFT.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To improve efficiency, boosting the arithmetic intensity—the ratio of total
    floating-point operations (FLOPs) to total memory access—of the decoding process
    is essential. Parallel decoding frameworks (Cai et al., [2024](#bib.bib4); Miao
    et al., [2023](#bib.bib20)) tend to achieve this goal by introducing more calculations
    to generate more tokens in each decoding step, while keeping memory access nearly
    the same²²2 Medusa (Cai et al., [2024](#bib.bib4)) only introduces negligible
    memory access of KV cache for token candidates in the tree. in each decoding step.
    A sequence of tokens will be generated as token candidates by draft models (Miao
    et al., [2023](#bib.bib20)) or fine-tuned heads (Cai et al., [2024](#bib.bib4)),
    which is then refined by the LLM for acceptable continuation. This line of approach
    reduces the total number of decoding steps as well as the total amount of memory
    access.
  prefs: []
  type: TYPE_NORMAL
- en: In the meanwhile, tree-based decoding, leveraging the *decoding tree* defined
    below, enables efficient parallel decoding. The tree attention is further introduced
    to reduce redundant KV storage, calculation, and kernel launching overheads when
    calculating the attention.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3.1  (Decoding tree).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A decoding tree $\mathcal{T}$ is a rooted tree where the root node corresponds
    to the prompt and each non-root node $u$ represents a sequence of generated tokens
    $\mathcal{S}_{u}$. For each node $u$, $\mathcal{B}_{u}$ is the path from root
    node to $u$ (without $u$) and $P_{\mathcal{B}_{u}}$ is the concatenation of tokens
    in sequences of nodes in path $\mathcal{B}_{u}$ by the sequential order. For each
    token $n\in u$, $s_{u,n}\in\mathcal{S}_{u}$ represents the sequence from the first
    token of node $u$ to $n$ (including $n$). The last token of each leaf node represents
    the input token for the next decoding iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3.2  (Tree-Attention).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For each token $n\in u$, where $u$ is any non-root node in the decoding tree
    $\mathcal{T}$, its tree attention is defined as the output of original Transformer-based
    sequence attention ($\text{Attention}(\cdot)$) on $P_{\text{root}\rightarrow n}$,
    where $P_{\text{root}\rightarrow n}$ is the concatenation of $P_{B_{u}}$ and $s_{u,n}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textstyle\text{Tree-Attention}(n)=\text{Attention}(P_{\text{root}\rightarrow
    n})\,.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'The existing solution of tree attention omits the potential IO optimization
    brought by the tree topology itself, thus motivating the DeFT we will explore
    in this paper. DeFT optimizes LLM efficiency from another perspective: It leverages
    the characteristics of node sharing in decoding trees to reduce the redundancy
    of KV cache IO from HBM to on-chip shared memory. Together with the IO-awareness
    DeFT tree attention for KV cache and partial results (i.e., $\mathbf{Q}\mathbf{K}^{\top}$),
    the whole arithmetic intensity will be improved with less memory access and nearly
    the same FLOPs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/672cb3bbd1ff01588958fb88a657a67f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of DeFT. (Left) System overview. (Right) The data flow
    using a decoding tree example.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 System Overview of DeFT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We outline the key component of DeFT, namely the DeFT Attention Kernel, in
    [Figure 2](#S3.F2 "Figure 2 ‣ Motivation for DeFT. ‣ 3.1 Preliminary ‣ 3 DeFT
    ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference"). As will be elaborated in Section [3.3](#S3.SS3 "3.3 An Efficient
    Attention Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference"),
    the DeFT Attention Kernel requires 1) Query (tokens), 2) KV (KV cache of decoding
    tree), and 3) Tree Topo (the topology of decoding tree to map Query and KV), which
    are prepared by *Branch Controller*, *KV cache Manager*, and *Sequence Tree Manager*,
    respectively. The details of each component are in Appendix [A.1](#A1.SS1 "A.1
    Components of system support for DeFT ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right part of [Figure 2](#S3.F2 "Figure 2 ‣ Motivation for DeFT. ‣ 3.1
    Preliminary ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference") further showcases the key data flow of the system
    through a decoding tree example: input metadata will be extracted by tree components
    we mentioned above, then loaded from HBM to shared memory in a group manner during
    the QKV Preparation phase discussed later. Then QKV groups will be processed by
    DeFT Attention Kernel in Section [3.3](#S3.SS3 "3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80e4a0388413e8cee3b538a81414243d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparison of memory access from HBM to shared memory for different
    attention algorithms in QKV Preparation Phase, where the amount of IO required
    by each is enclosed in red rectangles. Tiling is adopted in Flash Decoding (Dao
    et al., [2023](#bib.bib6)), Tree Attention-SpecInfer (Miao et al., [2023](#bib.bib20)),
    and DeFT to fit in small shared memory with a fused kernel. Sequence-based attention
    algorithms like Flash Decoding are not aware of the tree topology of KV cache,
    so $KV_{0}$ will be loaded twice; Tree Attention in Medusa (Cai et al., [2024](#bib.bib4))
    groups all queries and KV cache in a tree, with additional IO of the causal mask,
    then the QKV group will be allocated to streaming multiprocessors in GPU by Pytorch
    primitives; Tree Attention in SpecInfer will load KV cache of the whole tree for
    each query with the causal mask; DeFT groups QKV based on KV with tree topology
    information (e.g. from the decoding tree at the left, we can know $Q_{1}$ and
    $Q_{2}$ both share KV cache of sequence node $S_{0}$), which reduces IO of $KV_{0}$
    at the expense of negligible query overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can separate the execution of attention algorithms into two main phases:
    (1) QKV Preparation Phase: load Query, Key, and Value (QKV) into shared memory
    and group them logically to calculate attention; (2) Attention Calculation Phase:
    apply attention algorithms to QKV groups for final attention results.'
  prefs: []
  type: TYPE_NORMAL
- en: DeFT aims to be a memory-efficient algorithm in both aforementioned phases to
    get exact attention. Motivated by the heterogeneous GPU memory hierarchy (Dao
    et al., [2022](#bib.bib5))—namely, HBM is large but slower while the shared memory
    is much smaller but much faster—minimizing memory access between HBM and shared
    memory for memory-bound computations (e.g., attention) during the attention computation
    is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the QKV Preparation Phase, we introduce a KV-Guided Tree Split strategy with
    tree-topology awareness to minimize the IO of QKV.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the Attention Calculation Phase, we propose a Tree-Topology-Aware Global
    Reduction strategy combined with the established techniques (Kernel Fusion and
    Tiling), to eliminate the IO of partial results (i.e.. $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: QKV Preparation Phase of DeFT.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In sequence-based decoding, split strategy—namely splitting the inputs QKV into
    blocks—is commonly deployed to generate enough QKV groups for full utilization
    of the GPU (Dao et al., [2023](#bib.bib6)). This technique is crucial when the
    parallelism (usually limited by the batch size(Dao et al., [2023](#bib.bib6)))
    is much smaller than the number of streaming multiprocessors (SMs) on the GPU
    (108 for an A100), where the operation will only utilize a small portion of the
    GPU. Similarly, for tree-based decoding—where a decoding tree consists of multiple
    nodes and each node is a sequence of tokens—the batch size of trees may also be
    insufficient to fully utilize the GPU when the number of tokens in the tree is
    large, due to memory capacity limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, split the tree is not as easy as split the sequence: applying
    the existing split strategy (Dao et al., [2023](#bib.bib6)) in sequence-based
    decoding to tree-based decoding could cause redundancy of KV cache IO when grouping
    QKV based on Q without tree topology awareness, as illustrated in the top left
    of [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling
    and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference"). To bridge this gap, DeFT splits
    the tree by sequence nodes³³3When the sequence length of a node is substantial,
    it can be split into blocks similar to Flash-Decoding (Dao et al., [2023](#bib.bib6))
    to ensure the more balanced KV lengths among QKV groups., then group the KV of
    each node with all queries that share it based on tree topology.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3.3  (Properties of KV-Guided Tree Split strategy).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This KV-Guided Tree Split strategy, with KV as the indicator for grouping,
    eliminates redundant IO operations for KV with negligible query IO cost, as illustrated
    in the bottom right of [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention
    Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional IO cost of Q in DeFT is negligible because the length of the
    KV often surpasses that of the Q during tree decoding, primarily due to two reasons:
    (1) the auto-regressive decoding pattern dictates that each query in the decoding
    stage has a length of 1, which means the maximum query length of a decoding tree
    is determined by the number of branches; (2) In tree-search tasks, the token length
    of each sequence node in the decoding tree ("thought" in reasoning) is typically
    not exceedingly small (Yao et al., [2023](#bib.bib33)), implying that the KV sequence
    length of a node is often much larger than the total length of all queries that
    share it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, DeFT is extreme simple yet effective: during decoding, there is no
    need for DeFT to utilize a causal mask⁴⁴4In Appendix [A.4](#A1.SS4 "A.4 DeFT-Subtree
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference"), we also provide a variant of
    the DeFT algorithm with split-granularity of subtrees, which could have more balanced
    QKV groups by split the decoding tree to evenly subtrees, with the cost of introducing
    causal masks. (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4))
    to calculate the attention in the incoming Attention Calculation Phase. Instead,
    only the tree topology among sequence nodes in the decoding tree is required.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention Calculation Phase of DeFT.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GPUs boast an extensive array of threads to execute an operation, known as a
    kernel. Each kernel retrieves inputs from HBM to registers and shared memory,
    processes them, and subsequently saves the outputs back to HBM.
  prefs: []
  type: TYPE_NORMAL
- en: In this phase, we design DeFT kernel to load QKV splits in a memory efficient
    way, which are logically grouped by the QKV Preparation Phase, then to perform
    the attention calculation. We achieve DeFT kernel by using Kernel Fusion with
    Tiling strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel Fusion is a common technique of IO reduction: if multiple operations
    are performed on the same input, it is more efficient to load the input once from
    HBM rather than loading it multiple times for each operation; Similarly, the same
    principle applies when transferring output from shared memory to HBM. To fuse
    all the attention operations into one GPU kernel with the limited size of shared
    memory, we further utilize the commonly employed Tiling strategy (Dao et al.,
    [2022](#bib.bib5); [2023](#bib.bib6)): split queries and KV cache within each
    QKV group to small blocks to prevent materialization of attention matrix in HBM
    by computing attention within the limited size of shared memory, then incrementally
    performing the softmax reduction to reconstruct the attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DeFT kernel consists of two stages, as shown in [Figure 5](#S3.F5 "Figure 5
    ‣ Attention Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference"):'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stage 1–calculate partial attentions. Based on the QKV grouping method of DeFT
    mentioned above, each QKV group will be allocated to a thread block for Flash
    Attention (Dao et al., [2022](#bib.bib5)) calculation with Tiling strategy. Similar
    to Flash-Decoding (Dao et al., [2023](#bib.bib6)), we not only get partial attention
    but also return “LogSumExp” as a weight parameter for the next stage’s reduction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stage 2–global reduction. Upon receiving partial attention and LogSumExps for
    all QKV groups—recall that we grouped QKV based on KV before attention calculation—DeFT
    now performs a Tree-Topology-Aware Global Reduction. Guided by the tree topology
    among sequence nodes of KV in the decoding tree, DeFT remaps the partial results
    of attention and LogSumExp based on a query for the execution of global reduction.
    The grouped partial attention and LogSumExp will be passed to “Global_reduction_kernel”
    on the right side of [Figure 5](#S3.F5 "Figure 5 ‣ Attention Calculation Phase
    of DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV
    cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference"). To point out, there is no memory movement during the group of
    partial attention and LogSumExp as we group them logically by recording offsets
    of partial results required for a query.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/716f2b4dbe375a85f133a8d766ba2838.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Operations of Tree Attention-Medusa (Cai et al., [2024](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/730654551b49471d23cbc19ddb94b460.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Left: Illustration of DeFT kernel with two stages. Right: Global reduction
    kernel called in DeFT stage 2 illustrated in Figure [5b](#S3.F5.sf2 "In Figure
    5 ‣ Attention Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference"). QKV Groups $G_{0}$,$G_{1}$
    and $G_{2}$ are from DeFT QKV groups in [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient
    Attention Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1e196d92ac3bb70185442ed046085c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Stage 2 of DeFT: Global Reduction. Based on tree topology in [Figure 3](#S3.F3
    "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree
    KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference"), we can group LogSumExp and Partial Attention
    based on Query, then we call the Global reduction kernel in the right of Figure [5a](#S3.F5.sf1
    "In Figure 5 ‣ Attention Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention
    Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference") to get the final
    attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Attention operations of DeFT kernel. Based on the same decoding tree
    in [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling
    and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3.4  (The effects of QKV grouping strategy in the QKV Preparation Phase).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the QKV Preparation Phase, how QKVs are grouped logically results in different
    memory access of QKV for tree decoding, as shown in [Figure 3](#S3.F3 "Figure
    3 ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache
    ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flash-Decoding (Dao et al., [2023](#bib.bib6)), splits long KV and group QKV
    based on Q without tree topology awareness, which will bring redundant KV cache
    IO from GPU global memory to shared memory;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree Attention-Medusa (Cai et al., [2024](#bib.bib4)) groups the QKV of the
    entire decoding tree together with a tree topology-aware causal mask for tree
    attention computation based on Pytorch primitives, resulting in no redundancy
    of loading KV cache and Query with the cost of additional IO for the causal mask;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree Attention-SpecInfer (Miao et al., [2023](#bib.bib20)) groups each query
    with the KV of the entire tree with a causal mask for tree attention calculation,
    which has great redundancy in KV cache IO.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Remark 3.5  (On the importance of tiling and fused kernel during Attention Calculation
    Phase).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Methods in this phase can be roughly divided into two categories: (1) without
    tiling and kernel fusion: Tree Attention in Medusa (Cai et al., [2024](#bib.bib4));
    (2) with tiling and a fused kernel: Flash Decoding (Dao et al., [2023](#bib.bib6)),
    Tree Attention in SpecInfer (Miao et al., [2023](#bib.bib20)) and our DeFT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the method with no tiling and a fused kernel is inefficient during
    attention computation due to the materialization of the attention matrix in HBM.
    In detail, this approach may lead to significant IO operations for the causal
    mask (CM) and partial results (i.e.. $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax),
    as illustrated in [Figure 4](#S3.F4 "Figure 4 ‣ Attention Calculation Phase of
    DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache
    ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: DeFT is designed and implemented with a fused kernel to eliminate the IO cost
    of partial results mentioned above.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3.6  (The effects of introducing a causal mask).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Causal mask brings two parts of redundancy:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory Access. Medusa (Cai et al., [2024](#bib.bib4)) materializes the causal
    mask in HBM to record the causal information between $n_{q}$ tokens in queries
    and $n_{kv}$ tokens in the KV cache, thereby introducing a significant IO cost
    for loading this $n_{q}\times n_{kv}$-sized mask to shared memory. SpecInfer (Miao
    et al., [2023](#bib.bib20)) introduces a 64-bit integer as a bit mask to record
    the causal information among up to 64 tokens, which incurs minimal IO cost from
    HBM to shared memory but is not suitable for decoding trees with more than 64
    tokens. Details regarding the design of the bit mask in SpecInfer are discussed
    in Appendix [A.2](#A1.SS2 "A.2 Discussion of Tree-based Decoding ‣ Appendix A
    Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computation. In addition to the computational cost of generating the causal
    mask itself, there is an additional redundancy in computation: many of the matrix
    multiplication results of $\mathbf{Q}\mathbf{K}^{\top}$ are masked out and never
    utilized. Both Medusa and SpecInfer have this issue.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: DeFT does not require a causal mask and there is no IO and calculation redundancy
    caused by masking.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We implement the DeFT attention kernel by OpenAI Triton (Tillet et al., [2019](#bib.bib25)),
    which enables us to control memory access from global memory to shared memory
    and attention calculations in a thread block granularity. DeFT algorithm with
    two phases in a Python style can be found in Appendix [A.3](#A1.SS3 "A.3 DeFT-Node
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: '3.4 Analysis: IO Complexity of DeFT'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Notations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\displaystyle l_{n}$ | Number of leaf nodes in a decoding tree, which means
    how many queries in this decoding iteration. |'
  prefs: []
  type: TYPE_TB
- en: '| $\displaystyle N_{i}$ | Total token length from the root node to leaf node
    i |'
  prefs: []
  type: TYPE_TB
- en: '| $\displaystyle N_{tree}$ | Total token length the entire tree. |'
  prefs: []
  type: TYPE_TB
- en: '| $\displaystyle d_{head}$ | Head dimension of LLM. |'
  prefs: []
  type: TYPE_TB
- en: '| $\displaystyle F_{s}$ | Shared factor of reusing prefixes in tree attention,
    which means to which extent we can reduce IOs of KV cache: $F_{s}=(\sum_{i=1}^{ln}N_{i})/N_{tree}$.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: IO complexity breakdown for various methods. $\mathcal{O}(1)$ denotes
    the IO cost for a single data in the tensor across all layers and heads, which
    is equivalent to $\#heads*\#layer*dtype\_size$. Red ticks mean the best among
    all methods in the table, while red crosses mean the (potential) worst.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Query | KV cache | $\mathbf{Q}\mathbf{K}^{\top}$ | $\text{Softmax}(\mathbf{Q}\mathbf{K}^{\top})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naive Attention | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}\sum_{i=1}^{l_{n}}N_{i})$$\times$
    | $\mathcal{O}(2\sum_{i=1}^{l_{n}}N_{i})$ | $\mathcal{O}(2\sum_{i=1}^{l_{n}}N_{i})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Flash-Decoding | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}\sum_{i=1}^{l_{n}}N_{i})$
    $\times$ | $0$ ✓ | $0$ ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Tree Attention-Medusa | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree})$
    ✓ | $\mathcal{O}(2l_{n}N_{tree})$ $\times$ | $\mathcal{O}(2l_{n}N_{tree})$ $\times$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tree Attention-SpecInfer | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree}l_{n})$
    $\times$ | $0$ ✓ | $0$ ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DeFT (Ours) | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree})$
    ✓ | $0$ ✓ | $0$ ✓ |'
  prefs: []
  type: TYPE_TB
- en: This section analyzes the IO complexity of DeFT, showing a significant reduction
    in HBM accesses compared to existing attention algorithms. Note that it is non-trivial
    to summarize the IO cost of the entire tree decoding process, thus we only compare
    IOs based on the decoding tree snapshot in a single iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a decoding tree with the features outlined in [Table 2](#S3.T2 "Table
    2 ‣ 3.4 Analysis: IO Complexity of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference"), and we summarize
    the corresponding IO breakdown in [Table 3](#S3.T3 "Table 3 ‣ 3.4 Analysis: IO
    Complexity of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for
    Efficient Tree-search-based LLM Inference"). It can be observed that *due to the
    lack of tree-topology awareness, sequence-based decoding methods, such as naive
    attention and Flash-Decoding, incur $F_{s}$ times more memory access overheads
    for KV cache compared to DeFT and Tree Attention-Medusa (Cai et al., [2024](#bib.bib4))*.
    However, Tree Attention-Medusa entails higher IO overheads for partial results
    like $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax due to the lack of tiling and kernel
    fusion⁵⁵5 Note that both $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax will load and
    write, so the IO cost contains a round-trip of memory access between HBM and shared
    memory, as shown in [Figure 4](#S3.F4 "Figure 4 ‣ Attention Calculation Phase
    of DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV
    cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference"). . When the number of leaf nodes/queries $ln$ is sufficiently
    large, the IO cost of partial results might become comparable to that of the KV
    cache. For instance, in the Llama models (Touvron et al., [2023a](#bib.bib26);
    [b](#bib.bib27)), where $d_{head}\!=\!128$, with $l_{n}\!=\!32$, the total IO
    cost of $\mathbf{Q}\mathbf{K}^{T}$ and Softmax matches that of the KV cache.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark. Though similar to DeFT, SpecInfer (Miao et al., [2023](#bib.bib20))
    also employs a fused kernel for tree attention. No IO is sharing for KV cache
    among queries in SpecInfer: instead, each query will load the entire KV cache
    of the tree independently, bringing significant IOs of the KV cache as in [Table 3](#S3.T3
    "Table 3 ‣ 3.4 Analysis: IO Complexity of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate DeFT and other methods on tree-search-based tasks
    like reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To ensure a fair and feasible comparison, we use tree KV management illustrated
    in Section [3.2](#S3.SS2 "3.2 System Overview of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference") throughout our
    evaluations. We omit results with sequence-based KV cache management, due to the
    inefficient memory footprint and out-of-memory issue⁶⁶6 When applying sequence-based
    KV cache management (no sharing in storage for prefixes), we find sequence-based
    decoding algorithms—e.g., i) naive sequence-based attention in HuggingFace Transformers
    (Wolf et al., [2019](#bib.bib31)), and ii) original Flash-Decoding implementation—meet
    out-of-memory in the middle of decoding ($\sim 2{,}000$ iterations for workloads
    in this section) even for A100 with 80GB memory capacity. .'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the performance of DeFT in NVIDIA A100 (80GB) in Llama2-7B-HF model (Touvron
    et al., [2023b](#bib.bib27)) with SOTA attention algorithms in sequence-based
    and tree-based decoding: (1) *Flash-Decoding* (Dao et al., [2023](#bib.bib6)),
    using the CUDA implementation; (2) *Tree Attention-Medusa*, where we extend it
    to the PyTorch implementation suitable for general tree attention, inspired by
    the Medusa (Cai et al., [2024](#bib.bib4)) with fixed tree masks. ⁷⁷7We didn’t
    include the tree attention operator in SpecInfer (Miao et al., [2023](#bib.bib20))
    because its kernel only support at most 64 tokens in the decoding tree. Details
    in Appendix [A.2](#A1.SS2 "A.2 Discussion of Tree-based Decoding ‣ Appendix A
    Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ea8a7bd6c5c31c6b73ebb75616b7734.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The detailed procedure of reconstructing tree templates. (Left) Reconstructing
    reasoning trees from practical reasoning records as outlined in Besta et al. ([2023](#bib.bib3))
    involves capturing the following aspects: (1) the structure of trees, characterized
    by their depth $d$ and width $w$; (2) the token length associated with each thought;
    and (3) the best thought at each depth along with its corresponding score. For
    the task of document merging, the tree depth is set to $d=3$, with a width of
    $w=10$ at each depth. For sorting 128 numbers, the depth is reduced to $d=10$,
    while maintaining the same width of $w=10$. (Right) Utilizing the extracted thought
    information from Left, we can generate tree templates for decoding, encompassing
    *branch records* and *prune records*. These records are instrumental in guiding
    the tree decoding process to produce decoding trees that faithfully replicate
    the structure of the tree-of-thoughts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/322e56f3e610828868b49e1bd0db1e12.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Thought token length distribution in sorting 128. $79.5\%$ of thoughts’
    KV cache length is larger than the maximum query numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7044a43330458c7e37d45183e7448341.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Thought token length distribution in document merging. $100\%$ of thoughts’
    KV cache length is larger than the maximum query numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: The distribution of thought token lengths over $100$ trees across
    various depths for two tasks. We compare the thought token length with maximum
    query numbers during decoding for all tree templates and find that the query length
    during tree decoding is significantly smaller than the thought token length of
    KV. This observation highlights that $79.5\%$ and $100\%$ of thoughts’ KV cache
    in the tasks of sorting 128 and document merging respectively, have a larger length
    than the maximum query numbers among 100 trees during tree decoding. Note that
    the reason some thoughts are smaller than the maximum query length is attributed
    to GPT3.5’s output not adhering to the prompt regulations, which may cause the
    graph-of-thought (Besta et al., [2023](#bib.bib3)) parser to inaccurately parse
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Workloads generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using the interaction records of Tree-of-Thoughts methods with GPT 3.5 Turbo
    from Besta et al. ([2023](#bib.bib3)), we reconstruct the decoding trees for two
    tasks: (1) sorting 128—sorting 128 numbers; (2) doc merging—document merging.
    We elaborate on the procedure of extracting decoding trees from interaction records
    with LLM from Besta et al. ([2023](#bib.bib3)) in [Figure 6](#S4.F6 "Figure 6
    ‣ Baselines. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference"), which could
    be tree templates⁸⁸8 The decoding trees would be forced to branch and prune in
    certain iterations to get exactly the same shape of the decoding tree as the original
    tree-of-thoughts process, to ensure fairness for workloads of different baselines.
    to guide the tree decoding process with the same function as the tree-search algorithm.
    For the sake of simplicity, we ignore the evaluation prompt template and its overhead
    to select the best thought when generating the tree of thoughts. See workload
    analysis in [Figure 7](#S4.F7 "Figure 7 ‣ Baselines. ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Average end-to-end latency (second) and IO breakdown (TB) when decoding
    with 100 trees of two reasoning tasks from (Besta et al., [2023](#bib.bib3)):
    (i) (Left) sorting 128 numbers; (ii) (Right) document merging. IO mainly consists
    of three parts as follows. (i) KV cache: IO-KV; (ii) $QK^{T}$: IO-$QK^{T}$; (iii)
    Softmax$(QK^{T})$: IO-$Softmax$. Baselines: (i) Flash-Decoding: attention in Flash-Decoding (Dao
    et al., [2023](#bib.bib6)); (ii) Tree Attention-Medusa: causal masked tree attention
    in Medusa (Cai et al., [2024](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Task of sorting 128 numbers | Task of document merging |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Latency | IO-KV | IO-$QK^{T}$ | IO-$Softmax$ | Latency | IO-KV | IO-$QK^{T}$
    | IO-$Softmax$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Flash-Decoding | 548 | 35.1 | 0 | 0 | 450 | 30.7 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Tree Attention-Medusa | 653 | 7.9 | 0.6 | 1.3 | 660 | 8.6 | 0.7 | 1.3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| DeFT(ours) | 305 | 7.9 | 0 | 0 | 272 | 8.6 | 0 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Speed up over best baseline | $1.80\times$ | - | - | - | $1.66\times$ | -
    | - | - |  | ![Refer to caption](img/54dd33ff73a55a9a7b634d83d446a72d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Average latency and query length when decoding with 100 trees of sorting
    128 numbers per iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b973efa851b6f5658f4a69e06af48433.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Average latency and query length when decoding with 100 trees of document
    merging per iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Average per iteration results for 100 trees in each reasoning task.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'End-to-end behaviors: latency and IOs.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare DeFT with Flash-Decoding and Tree Attention-Medusa in average latency
    and IOs for tasks of sorting 128 and doc merging in [Table 4](#S4.T4 "Table 4
    ‣ Workloads generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference").
    The optimization in KV cache results in a significant reduction of IOs, achieving
    a $3.6{-}4.5\times$ reduction compared to Flash-Decoding’s KV cache IO. Additionally,
    the reduction of IOs in partial results during attention calculation leads to
    a reduction of $1.9-2\text{TB}$ (equivalent to 25% of the total KV cache IO) compared
    to Tree Attention-Medusa. *These optimizations jointly contribute to a notable
    speedup of our DeFT, achieving $\mathbf{1.7{-}1.8\times}$ improvements over Flash-Decoding
    and $\mathbf{2.2{-}2.4\times}$ improvements over Tree Attention-Medusa, respectively.*
    We will discuss why the reduction of KV cache IO does not have a significant acceleration
    effect on causal masked tree attention below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic behaviors: latency per iteration.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The size of the decoding tree dynamically changes due to branch and prune operations
    among iterations. To capture this dynamic behavior, we track the latency changes
    across tree decoding iterations in [Figure 8](#S4.F8 "Figure 8 ‣ Workloads generation.
    ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference") for DeFT, Flash-Decoding, and
    Tree Attention-Medusa. Notably, Tree Attention-Medusa exhibits a strong sensitivity
    to query length: as evident in iteration $1{,}000$ of Figure [8a](#S4.F8.sf1 "In
    Figure 8 ‣ Workloads generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT:
    Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference"),
    the sensitivity arises as the size of the partial result is directly proportional
    to the query number. Consequently, this not only leads to increased IO but also
    results in a larger memory footprint, with the GPU’s peak memory usage reaching
    95% compared to that of 40% in DeFT. However, when the tree size is relatively
    small due to pruning, Tree Attention-Medusa can outperform Flash-Decoding, as
    observed in iteration $2{,}500$ of Figure [8a](#S4.F8.sf1 "In Figure 8 ‣ Workloads
    generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference"). *DeFT significantly
    outperforms these two baselines and exhibits stable performance across a variety
    of tree sizes.*'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, we have proposed DeFT, a novel IO-aware tree attention algorithm
    to accelerate large language models combined with tree search algorithms. DeFT
    can aware topology of decoding tree to ease the great IO of KV cache, and a fused
    kernel is adopted to eliminate the IO of partial results during attention calculations.
    We have demonstrated in two practical reasoning tasks, DeFT can obtain $\mathbf{1.7{-}2.4\times}$
    speedup thanks to $3.6{-}4.5\times$ reduction of KV cache IO and $1.9-2\text{TB}$
    reduction (equivalent to 25% of the total KV cache IO) of $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax.
  prefs: []
  type: TYPE_NORMAL
- en: Key advantages of DeFT are IO-aware and simple, as no tree attention mask is
    required during the decoding. DeFT is also not sensitive to query numbers of the
    tree, which shows great potential to support a large search space with multiple
    branches.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anderson et al. (2017) Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
    Gould. Guided open vocabulary image captioning with constrained beam search. In
    Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), *Proceedings of the 2017
    Conference on Empirical Methods in Natural Language Processing*, pp.  936–945,
    Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
    doi: 10.18653/v1/D17-1098. URL [https://aclanthology.org/D17-1098](https://aclanthology.org/D17-1098).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large
    language models. *arXiv preprint arXiv:2308.09687*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D
    Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework
    with multiple decoding heads. *arXiv preprint arXiv:2401.10774*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dao et al. (2023) Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov.
    Flash-decoding for long-context inference, 2023. URL [https://pytorch.org/blog/flash-decoding/](https://pytorch.org/blog/flash-decoding/).
    PyTorch Blog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dathathri et al. (2019) Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
    Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play
    language models: A simple approach to controlled text generation. In *International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves (2012) Alex Graves. Sequence transduction with recurrent neural networks.
    *arXiv preprint arXiv:1211.3711*, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hokamp & Liu (2017) Chris Hokamp and Qun Liu. Lexically constrained decoding
    for sequence generation using grid beam search. In Regina Barzilay and Min-Yen
    Kan (eds.), *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  1535–1546, Vancouver, Canada, July
    2017\. Association for Computational Linguistics. doi: 10.18653/v1/P17-1141. URL
    [https://aclanthology.org/P17-1141](https://aclanthology.org/P17-1141).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Holtzman et al. (2018) Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut,
    David Golub, and Yejin Choi. Learning to write with cooperative discriminators.
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  1638–1649, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2023) Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li,
    Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language
    model inference on gpus. *arXiv preprint arXiv:2311.01282*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia & Van Sandt (2021) Zhe Jia and Peter Van Sandt. Dissecting the ampere gpu
    architecture via microbenchmarking. In *GPU Technology Conference*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. *arXiv preprint
    arXiv:2309.06180*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi,
    Hannaneh Hajishirzi, and Asli Celikyilmaz. Making ppo even better: Value-guided
    monte-carlo tree search decoding. *arXiv preprint arXiv:2309.15028*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang,
    and Yuxiao Dong. Apar: Llms can do auto-parallel auto-regressive decoding. *arXiv
    preprint arXiv:2401.06761*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra
    Bhagavatula, and Yejin Choi. Neurologic decoding:(un) supervised neural text generation
    with predicate logic constraints. In *Proceedings of the 2021 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies*, pp.  4288–4299, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022) Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai,
    Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al.
    Neurologic a* esque decoding: Constrained text generation with lookahead heuristics.
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pp.  780–799, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mark et al. (2021) Chen Mark, Tworek Jerry, Jun Heewoo, Yuan Qiming, Pinto Henrique Ponde
    de Oliveira, Kaplan Jared, Edwards Harrison, Burda Yuri, Joseph Nicholas, Brockman
    Greg, et al. Carr andrew n. *Leike Jan, Achiam Joshua, Misra Vedant, Morikawa
    Evan, Radford Alec, Knight Matthew, Brundage Miles, Murati Mira, Mayer Katie,
    Welinder Peter, McGrew Bob, Amodei Dario, McCandlish Sam, Sutskever Ilya, and
    Zaremba Wojciech*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar,
    and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative
    inference and token tree verification. *arXiv preprint arXiv:2305.09781*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. (2023) Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and
    Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding.
    *arXiv preprint arXiv:2307.15337*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post & Vilar (2018) Matt Post and David Vilar. Fast lexically constrained decoding
    with dynamic beam allocation for neural machine translation. In Marilyn Walker,
    Heng Ji, and Amanda Stent (eds.), *Proceedings of the 2018 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long Papers)*, pp.  1314–1324, New Orleans, Louisiana,
    June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1119.
    URL [https://aclanthology.org/N18-1119](https://aclanthology.org/N18-1119).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roller et al. (2020) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson,
    Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for
    building an open-domain chatbot. *arXiv preprint arXiv:2004.13637*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shazeer (2019) Noam Shazeer. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tillet et al. (2019) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton:
    an intermediate language and compiler for tiled neural network computations. In
    *Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning
    and Programming Languages*, pp.  10–19, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uesato et al. (2022) Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song,
    Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.
    Solving math word problems with process-and outcome-based feedback. *arXiv preprint
    arXiv:2211.14275*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck et al. (2022) Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi,
    and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language
    models. *Advances in Neural Information Processing Systems*, 35:4913–4927, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan,
    Junxian He, and Qizhe Xie. Self-evaluation guided beam search for reasoning. In
    *Thirty-seventh Conference on Neural Information Processing Systems*, 2023. URL
    [https://openreview.net/forum?id=Bw82hwg5Q3](https://openreview.net/forum?id=Bw82hwg5Q3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. *arXiv preprint arXiv:2305.10601*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Yao Zhao, Zhitian Xie, Chenyi Zhuang, and Jinjie Gu. Lookahead:
    An inference acceleration framework for large language model with lossless generation
    accuracy. *arXiv preprint arXiv:2312.12728*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang,
    Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez,
    et al. Efficiently programming large language models using sglang. *arXiv preprint
    arXiv:2312.07104*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Components of system support for DeFT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The details of functions for system components of DeFT are as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Branch Controller: It makes the tree decoding process forced by a user-defined
    function (e.g. branch to two children every $3$ iterations, as the example shown
    in the right of [Figure 2](#S3.F2 "Figure 2 ‣ Motivation for DeFT. ‣ 3.1 Preliminary
    ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference")). Tree-search-based algorithms can be applied here using the decoding
    tree’s topology information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sequence Tree Manager: It maintains the topology of the decoding tree based
    on the tree operations and tokens from the Branch Controller. The tree operations
    like pruning and branching will be executed by Tree Handler in this component.
    Branch Result Storage will record token generation results of all branches in
    the decoding tree, and output when the decoding stops.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'KV cache Manager: It will maintain KV cache with a tree structure. A map between
    sequence IDs in the decoding tree and KV cache index is kept, which will be updated
    based on KV operations⁹⁹9 e.g. when a node is pruned in the decoding tree, its
    KV space will be evicted using a Remove operation. from the Sequence Tree Manager.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model Interface: pass input metadata to DeFT Attention kernel and MLP module,
    then return logits and memory pointers of updated KV cache.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 Discussion of Tree-based Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tree-based decoding could have tree-structured KV cache for storage with awareness
    of shared prefixes (Zheng et al., [2023](#bib.bib35)), or tree-structured queries
    in parallel decoding (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4)),
    as shown in [Figure 9](#A1.F9 "Figure 9 ‣ A.2 Discussion of Tree-based Decoding
    ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In SpecInfer(Miao et al., [2023](#bib.bib20)), as shown in Figure [9b](#A1.F9.sf2
    "9b ‣ Figure 9 ‣ A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣
    DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM
    Inference"), a bit mask is utilized to record the causal information among queries
    of a token tree. Each token $t_{i}$ in queries will have a 64-bit Int as a bit
    mask, where j-th bit means the causal relationship between query of $t_{i}$ and
    KV cache of $t_{j}$. The advantage of this mask design is that it greatly reduces
    IO, but it results in the maximum number of tree tokens being only 64, which is
    not practical for scenarios with tree-structured KV cache.'
  prefs: []
  type: TYPE_NORMAL
- en: A general decoding could both do with tree KV and tree queries, which could
    reduce redundancy (e.g. IO, storage, computation, etc) of shared prefixes, as
    well as increase the generated tokens per decoding iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f8a319cf56d91f50b949fca55bf758f.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) (Left) Sequence KV with queries in a tree for parallel decoding (Miao et al.,
    [2023](#bib.bib20)), where a bit mask in Figure [9b](#A1.F9.sf2 "In Figure 9 ‣
    A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference") is applied to
    record the causal information among queries in a tree of tokens. (Right) Tree
    KV with parallel queries for shared prefixes in DeFT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4f73ba0e6a206089f38dd755b410e99.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Bit Mask in SpecInfer (Miao et al., [2023](#bib.bib20))to record the causal
    information between query tokens in a tree structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Discussion of tree-based decoding with tree queries (Miao et al.,
    [2023](#bib.bib20)) and tree KV.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 DeFT-Node Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Algorithm 1 DeFT-Node Algorithm-Phase 1: QKV Preparation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list $KL=(K_{0},...K_{N-1})$,
    Value cache list $VL=(V_{0},...V_{N-1})$ for each sequence node in the tree, where
    $N$ is the total number of sequences in a tree, and Tree $T$ with its topology
    information.  for each $q$ in $Q$ with its global index $idx$ do     /*Get KV
    indices of all prefixes’ for a query.*/     $QMapKV[idx]$=GetPrefixKVIndices($q,KL,VL,T$)  end for  for each
    seq’s KV cache $K_{i},V_{i}$ in $KL,VL$ with its KV indice $i$ do     /*Group
    each sequence’s KV with all queries that share it.*/     $Q_{i}$= GroupQueryToKV($Q,K_{i},V_{i},T$)
    $\in R^{b_{i},d}\subset Q$     $KVMapQ[i]=Q_{i}$  end for  Return QMapKV, KVMapQ'
  prefs: []
  type: TYPE_NORMAL
- en: DeFT-Node^(10)^(10)10If there is no special explanation of the DeFT Attention
    algorithm in the text, it refers to DeFT-Node Algorithm. has two phases-Phase
    1-QKV Preparation and Phase 2-Attention Calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Phase 2-Attention Calculation of DeFT has two stages.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stage 1: Calculate Partial Attentions. We will apply Flash Attention of all
    QKV groups obtained after Phase 1-QKV Preparation of DeFT, to get partial attention
    and LogSumExp.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stage 2: Global Reduction. We will remap partial attention and LogSumExp based
    on each query, and get final attention based on global reduction similar to Flash-Decoding
    (Dao et al., [2023](#bib.bib6)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Algorithm 2 DeFT-Node Algorithm-Phase 2: Attention Calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list $KL=(K_{0},...K_{N-1})$,
    Value cache list $VL=(V_{0},...V_{N-1})$ for each sequence node in the tree, where
    $N$ is the total number of sequences in a tree, and Tree $T$ with its topology
    information. QKV group information $QMapKV$, $KVMapQ$ from QKV Preparation Phase.  for each
    $q$ in $Q$ with its global index $idx$ do     /*Allocate to store LogSumExp of
    $Q@K^{T}$ grouped by query.*/     $LogSumExp[idx]=\{\}$     /*Allocate to store
    partial results of $SoftMax(Q@K^{T})V$ for each query.*/     $O[idx]=\{\}$  end for  /*Allocate
    space for output after reduction.*/  $FO=(0)_{b_{q}\times d}\in R^{(b_{q},d)}$  for each
    seq’s KV cache $K_{i},V_{i}\in R^{(b_{kv},d)},R^{(b_{kv},d)}$ in $KL,VL$ with
    its KV indice $i$ do     # Unroll for loop to SMs     $Q_{i}$= $KVMapQ[i]\in R^{(b_{i},d)}$     /*Get
    partial attention $o_{i}$ for each QKV group, LogSumExp $lse_{i}$ of $Q@K^{T}$
    in row for reduction.*/     $o_{i},lse_{i}$ = FlashAttention($Q_{i},K_{i},V_{i}$)     $\in
    R^{(b_{i},d)},R^{b_{i}}$     /*Map the partial results back to each query for
    reduction.*/     for each query $q$ in $Q_{i}$ with its group index $gp\_idx$
    and global index $idx$ in $Q$ do        if $i\in QMapKV[idx]$ then           $LogSumExp[idx].append(lse_{i}[gp\_idx])$        end if     end for  end for  for each
    $q$ in $Q$ with its global index $idx$ do     # Unroll for loop to SMs     if len($O[idx]$)==len($QMapKV[idx]$) then        /*Global
    reduction after collecting all partial results from QKV groups that contains $q$.*/        $LSE_{cat}$=
    CatTensor($LogSumExp[idx]$)        $LSE_{max}$=RowMax($LSE_{cat}$)        $Mid\_L=0,Mid\_O=0^{(1,d)}$        for each
    $lse_{j}$ in $LogSumExp[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_L=Mid\_L+new\_exp$        end for        for each
    $lse_{j},o_{j}$ in $LogSumExp[idx],O[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_O=Mid\_O+new\_exp@o_{j}/Mid\_L$        end for        $FO[idx]=Mid\_O$     end if  end for  Return
    $FO$'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 DeFT-Subtree Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The algorithm (noted as DeFT-Node) in Appendix [A.3](#A1.SS3 "A.3 DeFT-Node
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference") adopts a node-granularity split
    strategy, which is quite simple. However, when the token lengths of different
    nodes in a decoding tree are very unbalanced, it might introduce inefficient calculation
    due to the unbalanced workload in on-chip SMs of GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can split the decoding tree in a more balanced way– in subtree-granularity.
    We show the DeFT-Subtree algorithm as follows, which also consists of two stages
    similar to DeFT-Node^(11)^(11)11We are testing DeFT-Subtree Algorithm and will
    add performance comparison in subsequent versions..
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 3 DeFT-Subtree Algorithm-Phase 1: QKV Preparation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list $KL=(K_{0},...K_{N-1})$,
    Value cache list $VL=(V_{0},...V_{N-1})$ for each sequence node in the tree, where
    $N$ is the total number of sequences in a tree, and Tree $T$ with its topology
    information. Subtree size $S_{t}$, which means each subtree after tiling contains
    at most $S_{t}$ tokens.  /*Evenly slice/blockwise the Tree KV cache (with $n_{T}$
    tokens in the tree ) to subtrees.*/  SubInfo, KSub, VSub =Slice( KL, VL, $S_{t}$,
    $T$)  /*Notes: (1) subtree number $m=Ceil(n_{T}/S_{t})$;  (2) subtrees’ KV cache
    $KSub=(Kb_{0},...,Kb_{m-1})$, $VSub=(Vb_{0},...,Vb_{m-1})$;  (3) subtree information
    $SubInfo=(Sb_{0},...,Sb_{m-1})$, where each subtree i has $Sb_{i}=(ofs_{0},...ofs_{n_{b_{i}}-1})$
    to record the offset of each node in the subtree KV cache, with $n_{b_{i}}$ as
    the total number of nodes in subtree $i$. */  for each subtree’s KV cache $Kb_{i},Vb_{i}$
    in $KSub,VSub$ with its subtree ID $i$ do     /*Group each subtree’s KV with all
    queries that share it.*/     $Q_{i}$= GroupQueryToKV($Q,Kb_{i},Vb_{i},T$) $\in
    R^{b_{i},d}\subset Q$     $KVMapQ[i]=Q_{i}$     for each query $q$ in $Q_{i}$
    with a global index $idx$ in $Q$ do        $QMapKV[idx].append(i)$     end for     /*Add
    a causal mask as different nodes in a subtree could be shared by different queries.*/     $CausalMask[i]=GetBitMask(Q_{i},Kb_{i},Vb_{i},T)$=$(CM_{0},...CM_{n_{b_{i}}-1})$     where
    $n_{b_{i}}$ is the total number of nodes in the subtree, and $CM_{i}$ is a 64-bit
    int bit mask for node i.     /*E.g, $100....00$ with 1 in bit 0, means the $Q_{i}[0]$
    does not share KV cache of node i in the subtree.*/  end for  Return QMapKV, KVMapQ,
    CausalMask,SubInfo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 4 DeFT-Subtree Algorithm-Phase 2: Attention Calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list in subtree-granularity KSub=($Kb_{0}$,…,$Kb_{m-1}$),
    Value cache list in subtree VSub = ($Vb_{0}$,…,$Vb_{m-1}$ for $m$ subtrees after
    tiling based on Tree $T$ with its topology information. QKV group information
    $QMapKV$, $KVMapQ$, causal mask $CausalMask$ and subtree information $SubInfo$
    from QKV Preparation Phase.  for each $q$ in $Q$ with its global index $idx$ do     /*Allocate
    to store LogSumExp of $Q@K^{T}$ grouped by query.*/     $LogSumExp[idx]=\{\}$     /*Allocate
    to store partial results of $SoftMax(Q@K^{T})V$ for each query.*/     $O[idx]=\{\}$  end for  /*Allocate
    space for output after reduction.*/  $FO=(0)_{b_{q}\times d}\in R^{(b_{q},d)}$  for each
    subtree’s KV cache $Kb_{i},Vb_{i}\in R^{(b_{kv},d)},R^{(b_{kv},d)}$ in $KSub,VSub$
    with subtree ID $i$ do     # Unroll for loop to SMs     $Q_{i}$= $KVMapQ[i]\in
    R^{(b_{i},d)}$     /*Reconstruct mask for attention calculation based on $CausalMask$
    and $SubInfo$*/     $bitmask=CausalMask[i]\in R^{n_{b_{i}}}$,where $n_{b_{i}}$
    is the total number of nodes for subtree i.     $SubOfst=SubInfo[i]\in R^{n_{b_{i}}}$     $mask=ReconstructMask(bitmask,SubOfst)\in
    R^{(b_{i},b_{kv})}$     /*Get partial attention $o_{i}$ for each QKV group, LogSumExp
    $lse_{i}$ of $Q@K^{T}$ in row for reduction.*/     $o_{i},lse_{i}$ = FlashAttention($Q_{i},Kb_{i},Vb_{i},mask$)     $\in
    R^{(b_{i},d)},R^{b_{i}}$     /*Map the partial results back to each query for
    reduction.*/     for each query $q$ in $Q_{i}$ with its group index $gp\_idx$
    and global index $idx$ in $Q$ do        if $i\in QMapKV[idx]$ then           $LogSumExp[idx].append(lse_{i}[gp\_idx])$        end if     end for  end for  for each
    $q$ in $Q$ with its global index $idx$ do     # Unroll for loop to SMs     if len($O[idx]$)==len($QMapKV[idx]$) then        /*Global
    reduction after collecting all partial results from QKV groups that contains $q$.*/        $LSE_{cat}$=
    CatTensor($LogSumExp[idx]$)        $LSE_{max}$=RowMax($LSE_{cat}$)        $Mid\_L=0,Mid\_O=0^{(1,d)}$        for each
    $lse_{j}$ in $LogSumExp[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_L=Mid\_L+new\_exp$        end for        for each
    $lse_{j},o_{j}$ in $LogSumExp[idx],O[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_O=Mid\_O+new\_exp@o_{j}/Mid\_L$        end for        $FO[idx]=Mid\_O$     end if  end for  Return
    $FO$'
  prefs: []
  type: TYPE_NORMAL
