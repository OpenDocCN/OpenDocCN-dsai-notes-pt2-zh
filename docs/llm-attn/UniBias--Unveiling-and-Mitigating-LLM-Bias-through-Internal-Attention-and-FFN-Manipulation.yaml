- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:10'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20612](https://ar5iv.labs.arxiv.org/html/2405.20612)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hanzhang Zhou^(1,2), Zijian Feng^(1,2), Zixiao Zhu^(1,2), Junlang Qian¹, Kezhi
    Mao^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Nanyang Technological University         ²Singapore-ETH Centre
  prefs: []
  type: TYPE_NORMAL
- en: '{hanzhang001, feng0119, zixiao001, junlang001}@e.ntu.edu.sg'
  prefs: []
  type: TYPE_NORMAL
- en: ekzmao@ntu.edu.sg
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated impressive capabilities in various
    tasks using the in-context learning (ICL) paradigm. However, their effectiveness
    is often compromised by inherent bias, leading to prompt brittleness—sensitivity
    to design settings such as example selection, order, and prompt formatting. Previous
    studies have addressed LLM bias through external adjustment of model outputs,
    but the internal mechanisms that lead to such bias remain unexplored. Our work
    delves into these mechanisms, particularly investigating how feedforward neural
    networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting
    the contribution of individual FFN vectors and attention heads, we identify the
    biased LLM components that skew LLMs’ prediction toward specific labels. To mitigate
    these biases, we introduce UniBias, an inference-only method that effectively
    identifies and eliminates biased FFN vectors and attention heads. Extensive experiments
    across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance
    and alleviates prompt brittleness of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have shown exceptional capabilities in various
    natural language processing (NLP) tasks, employing the in-context learning (ICL)
    paradigm. This paradigm conditions LLMs on a context prompt comprising of a few
    example-label pairs [[1](#bib.bib1), [5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite their impressive performance, LLMs are prone to prompt brittleness,
    characterized by high sensitivity to the choice [[29](#bib.bib29)] and order [[14](#bib.bib14)]
    of examples, and prompt formatting [[15](#bib.bib15)], as demonstrated in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating LLM
    Bias through Internal Attention and FFN Manipulation"). Such prompt brittleness
    is found to be arise from the bias in LLMs towards predicting certain answers
    [[29](#bib.bib29)]. The presence of the bias undermines the robustness and adaptability
    of LLMs in diverse applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Extensive research has focused on identifying factors that lead to LLM bias
    and strategies for mitigation. For instance, vanilla label bias [[7](#bib.bib7)]
    and recency bias [[29](#bib.bib29)] demonstrate the LLM’s inherent non-contextual
    preference for certain labels and contextual preference for specific positions,
    respectively. Additionally, several calibration methods [[7](#bib.bib7), [10](#bib.bib10),
    [29](#bib.bib29)] are proposed to counteract the bias by adjusting decision boundaries
    of model output probabilities. However, these approaches are derived from *external*
    observations or adjustments of LLM outputs, leaving the *internal* mechanisms
    within LLMs that cause such bias poorly understood.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we investigate the internal mechanism of LLM bias, specifically
    how feedforward neural networks (FFNs) and attention heads contribute to such
    bias. Building on findings in mechanistic interpretability [[6](#bib.bib6), [4](#bib.bib4)],
    we assess the contribution of individual attention heads and FFN vectors¹¹1FFN
    vector refers to the value vector in the second weight matrix of the FFN layer.
    We elaborate on this in Section [2.1](#S2.SS1 "2.1 Background ‣ 2 Internal Mechanisms
    Causing the Bias of LLMs ‣ UniBias: Unveiling and Mitigating LLM Bias through
    Internal Attention and FFN Manipulation") to label predictions in LLMs. By identifying
    FFN vectors and attention heads that convey biased influences towards label prediction,
    we reveal the internal mechanisms behind several key bias factors, including vanilla
    label bias [[7](#bib.bib7)], recency bias [[29](#bib.bib29)], and selection bias
    [[30](#bib.bib30)]. For instance, our analysis of FFN vectors without input context
    demonstrates that their cumulative impact biases the LLM towards specific labels,
    indicating a non-contextual preference for certain labels, i.e., vanilla label
    bias. We elaborate on the background of mechanistic interpretability in Section
    [2.1](#S2.SS1 "2.1 Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs
    ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation") and present our findings on the internal mechanisms of LLM biases
    in next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'we pose the question: Can we identify the biased components of LLMs and mitigate
    their detrimental impact on label prediction? Motivated by this intuition, we
    propose UniBias, an inference-only method designed to identify and eliminate biased
    FFN vectors and attention heads in LLMs. Specifically, we begin by projecting
    each FFN vector and attention head into the vocabulary space to interpret the
    information conveyed by their outputs. We then detect biased components based
    on three criteria we defined: the relatedness criterion, the bias criterion, and
    the low variance criterion. After identification, we mitigate their impact by
    masking these biased components. Extensive experimental results demonstrate that
    LLMs, from which biased components have been removed, consistently outperform
    their original counterparts by a significant margin. Further, as illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation"), our method significantly
    improves both the performance and robustness of ICL with perturbations of various
    design settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/963cfe0f92c661b3f42adb0b2d966af0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: illustrates the prompt brittleness of ICL and the effectiveness of
    our method in mitigating this issue. Experiments are conducted in one-shot setting,
    using SST2 [[20](#bib.bib20)] dataset for experiments on example selection and
    prompt formatting and AGnews [[28](#bib.bib28)] dataset for example order experiment
    due to more diverse combination of orders.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The contributions of our work are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to existing works based on external observation of LLM outputs,
    we unveil the internal mechanisms within LLMs that lead to their bias towards
    predicting certain answers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose the UniBias method to mitigate LLM bias by identifying and eliminating
    biased FFN vectors and attention heads within LLMs. Moreover, our method demonstrate
    an effective way to manipulate internal structures of LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive experiments across 12 NLP datasets demonstrate that, by removing the
    biased components, our UniBias method significantly enhances ICL performance compared
    to the original LLM, achieving state-of-the-art results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Internal Mechanisms Causing the Bias of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section reveals the internal mechanisms within LLMs that lead to various
    bias factors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The theoretical background of this work is based on research on mechanistic
    interpretability [[6](#bib.bib6), [23](#bib.bib23), [8](#bib.bib8)], which aims
    to explain the internal processes in language models (LMs), facilitating the interpretation
    of the contributions of individual model components to the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We are focusing on decoder-only LMs in this paper. They are composed by a sequence
    of transformer layers, each composed of a multi-head self-attention layer and
    an feedforward neural network layer. The background knowledge for interpreting
    the contribution of each FFN vector and attention head to the models’ prediction
    are demonstrated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The Residual Stream We interpret Transformers following the view of residual
    stream [[6](#bib.bib6), [4](#bib.bib4)]. Due to the residdual connection of Transformers,
    each layer takes a hidden state as input, and adds information obtained by its
    attention layer and FFN layer to the hidden state through residual connection.
    In this sence, the hidden state is a residual stream passed along layers, and
    each attention layer and FFN layer contribute to the final prediction by adding
    information to the residual stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention Heads   Following Elhage et al. [[6](#bib.bib6)], Dar et al. [[4](#bib.bib4)],
    the output of each attention layer of LM can be computed as the sum of all its
    attention heads. Specifically, for $l$, $W_{V}^{l}$ and $W_{O}^{\ell,j}\in\mathbb{R}^{\frac{d}{H}\times
    d}$ is the number of attention heads. We then find that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $A^{\ell,j}=\text{softmax}\left(\frac{({X}^{\ell}W_{Q}^{\ell,j})({X}^{\ell}W_{K}^{\ell,j})^{T}}{\sqrt{d/H}}+M^{\ell,j}\right)$
    is the attention mask. Therefore, the output of an attention layer is equivalent
    to computing attention heads independently, multiplying each by its own output
    matrix, and adding them into the residual stream of the LM.
  prefs: []
  type: TYPE_NORMAL
- en: 'FFN   In line with Geva et al. [[8](#bib.bib8), [9](#bib.bib9)], transformer
    FFN layers can be cast as linear combination of vectors. Specifically, for an
    input vector $\mathbf{x}^{\ell}\in\mathbb{R}^{d}$, the FFN output can be derived
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $f$ produces the coefficient $m_{i}^{\ell}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e09c367b8ba2a30fbbeb1af8e7a1867.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Unveiling vanilla label bias by uncontextual accumulated FFN logits.'
  prefs: []
  type: TYPE_NORMAL
- en: Logit Lens   The logit lens [[16](#bib.bib16)] is a technique that directly
    decode hidden states into the vocabulary space using the unembedding matrix of
    the LLM for interpretation. This approach has been validated in various studies
    as an efficient method for interpreting the weight matrix or hidden states of
    LLMs [[4](#bib.bib4), [11](#bib.bib11), [27](#bib.bib27), [8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: In summary, each attention layer and FFN layer contribute to the final prediction
    by adding their output hidden states to the residual stream. These outputs can
    be viewed as the sum of their respective attention heads and FFN vectors. Each
    attention head or FFN vector’s output can be interpreted through the logit lens.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Internal Mechanisms of Bias Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We delve into the mechanisms behind several bias factors, analyzing the contributions
    of attention heads and FFN vectors to the biased predictions in LLMs. We explore
    vanilla label bias, position bias, and selection bias using the Llama-2 7B model
    [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla Label Bias   The vanilla label bias [[7](#bib.bib7)], also known as
    common token bias [[29](#bib.bib29)], is the inherent, uncontextual preference
    of the model towards predicting certain label names. Given the contextual nature
    of attention layers, our investigation focuses on the FFN layers, where we identified
    a corresponding uncontextual preference. Specifically, by projecting the FFN value
    vectors into the vocabulary space, we compute the logits for various label names
    for each FFN vector. Utilizing the residual stream insight, we then aggregate
    these logits for all FFN vectors whose label logits rank within the top $10$ over
    the vocabulary, reflecting uncontextual influences of FFN vectors that are effective
    in label prediction. This process yields what we term *uncontextual accumulated
    FFN logits*, revealing the intrinsic bias of the LLM towards predicting label
    names without the influence of input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Background ‣ 2 Internal Mechanisms Causing
    the Bias of LLMs ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation") illustrates the accumulated uncontextual FFN
    logits across different label names in the sentiment analysis task, alongside
    their corresponding zero-shot prediction frequencies on the SST-2 dataset. For
    example, the label name ’positive’ exhibits higher uncontextual accumulated FFN
    logits compared to ’negative,’ leading to a higher frequency of ’positive’ predictions.
    Additionally, when comparing the labels ’good’ and ’bad’, the difference in their
    uncontextual accumulated FFN logits is more pronounced than that between ’positive’
    and ’negative,’ resulting in a larger discrepancy in prediction frequency. Conversely,
    the accumulated logits for the labels ’satisfied’ and ’disappointed’ show a reverse
    trend relative to ’positive’ and ’negative’, which results in a corresponding
    reverse trend in their prediction frequency ratios.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b5e080aac8764db09f56e0ed6759321.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The internal mechanism of the recency bias.'
  prefs: []
  type: TYPE_NORMAL
- en: Recency Bias   Recency bias refers to the tendency of LLMs to favor the label
    of the example at the end of the prompt [[29](#bib.bib29)]. By examining the behavior
    of attention heads within LLMs, we observe that specific heads consistently prioritize
    the example at the end of the prompt, providing an internal perspective on the
    origin of recency bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'We identify the biased attention head using the method introduced in Section
    [3](#S3 "3 Methodology ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation"). We compare the behaviors of a biased attention
    head (layer 16, head 29) and an unbiased attention head (layer 16, head 19) in
    terms of the attention weight assigned to examples at different positions and
    the label logits of the corresponding attention head’s output. Specifically, we
    use the SST-2 dataset, including one positive and one negative example in the
    prompt, and test with 40 samples, evenly split between positive and negative examples.
    More experimental details are provided in Appendix [A](#A1 "Appendix A Experimental
    Details ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention
    and FFN Manipulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental results in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Internal Mechanisms
    of Bias Factors ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation") reveal
    that the biased attention head (layer 16, head 29) consistently assigns significantly
    larger attention weights to the final example, irrespective of the ground truth
    labels of the test samples. This bias persists even when the sequence of examples
    is reversed, as shown in the second subfigure, indicating a biased preference
    of this attention head for the last example in the prompt. Furthermore, the biased
    attention weight assignment leads to biased logits, as shown in the third subfigure.
    In contrast, the unbiased attention head (layer 16, head 19) assigns very close
    averaged attention weights to both examples in the prompt. Interestingly, we observe
    that this unbiased head generally assigns larger weights to the example whose
    label matches the ground truth label of the test sample, resulting in 35 out of
    40 samples being correctly classified based on this pattern by this single attention
    head. The preference shown by specific attention heads for the example at the
    end of the prompt reveals the internal mechanism of recency bias.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/55bf77e539792b0755039d756a8b7e26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The internal mechanism of the selection bias.'
  prefs: []
  type: TYPE_NORMAL
- en: Selection Bias   The selection bias refers that LLMs prefer to select specific
    option ID (like "Option A") as answers for multiple choice questions [[30](#bib.bib30)].
    We have identified both FFN vectors and attention heads that consistently favor
    a specific option regardless of the ground truth label of the test sample, revealing
    the internal mechanism of selection bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the Llama-2 7B model on the ARC dataset, which contains four options
    (A, B, C, D). We use a zero-shot setting to avoid the influence of position bias
    from multiple examples. More details are provided in Appendix [A](#A1 "Appendix
    A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental results are illustrated in Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Internal
    Mechanisms of Bias Factors ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣
    UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation"). Firstly, we observe that the LLM exhibits a vanilla label bias
    favoring option "A", as shown in the first subfigure. Additionally, we identify
    a biased attention head that demonstrates a position bias consistently favoring
    the first option regardless of the ground truth labels of the test samples (second
    subfigure) or changes in the sequence of options (third subfigure). Since option
    A is usually the first option, these two biases both lead to the LLM’s preference
    for option A.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we unveil that various bias factors are stem from
    the biased behaviors of attention heads and FFN vectors. Naturally, we pose the
    question: Can we identify the biased components of LLMs and mitigate their impact
    on label prediction? Therefore, we propose our UniBias method to Unveil and mitigate
    LLMs’ label Bias through internal attention and FFN manipulation. Notably, our
    method is proposed for decoder-only LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Biased FFN Vectors Identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Identifying biased FFN vectors in LLMs hinges on whether the contribution of
    each FFN vector is independent and interpretable. As discussed in Section [2.1](#S2.SS1
    "2.1 Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation"), the
    output of an FFN layer can be cast as a linear combination of FFN vectors. Each
    FFN vector contributes to the final prediction by adding information encoded in
    its value vector, $\mathbf{v}_{i}^{\ell}$ can be interpreted through the logit
    lens, enabling us to interpret it as a distribution of logits across the vocabulary
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to identify an FFN vector as biased? we assess whether it consistently
    introduces a biased preference towards specific labels into the residual stream,
    regardless of variations in the test samples. Such consistent biases can skew
    the LLM’s predictions. We introduce the following criteria to detect biased components
    in LLMs, which are also applicable for identifying biased attention heads:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Relatedness Criterion: The information introduced by the FFN vector (or attention
    head) should closely relate to label prediction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biased Criterion: The information contributed to the residual stream by the
    FFN vector (or attention head) exhibits a biased distribution, favoring certain
    labels over others.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low Variance Criterion: The label prediction information added by the FFN vector
    (or attention head) to the residual stream is almost identical across a set of
    test samples with different labels, i.e., exhibits very small variance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The third criterion is key to identifying biased FFN vectors (or attention heads),
    as consistently low variance indicates that the FFN vector is not adequately responsive
    to varying inputs. Combined with the second criterion, this suggests a bias towards
    certain predictions regardless of the input’s contextual differences.
  prefs: []
  type: TYPE_NORMAL
- en: To examine these criteria, we interpret the information contributed by each
    FFN vector, i.e., $m\mathbf{v}$ is fixed, changes in the FFN coefficient $m$ across
    different samples reflect the change in information brought by the FFN vector.
    We interpret this information by projecting each FFN value vector into the vocabulary
    space and analyzing the logit distribution over label tokens, termed *label logits*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given an FFN value vector $\mathbf{v}\in{\mathbb{R}^{d}}$ (where
    $c$-th sample can be obtained by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{g}=\mathbf{v}\cdot E\cdot L^{\top}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We use $p$, respectively. An FFN vector is considered biased if it meets the
    following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\left\{\begin{aligned} &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{G}_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{g^{(k)}}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}g_{j}^{(k)}>th_{FFN}^{1}\\
    &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\mathbf{G}_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\mathbf{g^{(k)}}\right)=\frac{1}{p}\frac{1}{c}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}\left(g_{j}^{(k)}-\mu(\mathbf{g^{(k)}})\right)>th_{FFN}^{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;CV\left(\mathbf{m}\right)=\frac{\sigma(\mathbf{m})}{\mu(\mathbf{m})}=\frac{\sqrt{\frac{1}{p}\sum_{j=0}^{p-1}\left(m_{k}-\mu(\mathbf{m})\right)^{2}}}{\frac{1}{p}\sum_{k=0}^{p-1}m_{k}}<th_{FFN}^{3}\end{aligned}\right.$$
    |  |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\mu(\mathbf{g}^{(k)})=\frac{1}{c}\sum_{j=0}^{c-1}g_{j}^{(k)}$ are set
    by grid search, which is elaborated in Section [3.4](#S3.SS4 "3.4 Grid Searching
    ‣ 3 Methodology ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation")'
  prefs: []
  type: TYPE_NORMAL
- en: The relatedness criterion is measured by the sum of label logits. The biased
    criterion is measured by the logit difference between each label logit and the
    average label logit. The low variance criterion is measured by the coefficient
    variance (CV) of the FFN vector coefficient across different samples, which is
    the standard deviation normalized by the mean of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Biased Attention Heads Identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The identification of biased attention heads closely resembles the process
    of identifying biased FFN vectors. As discussed in Section [2.1](#S2.SS1 "2.1
    Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation"), each
    attention head’s contribution to the final prediction is independent and interpretable.
    Therefore, we project the output hidden states of each attention head into the
    vocabulary space to interpret the information they contribute.'
  prefs: []
  type: TYPE_NORMAL
- en: To identify biased attention heads, we use the same three criteria introduced
    for identifying biased FFN vectors. To apply these criteria, we project the output
    hidden states from each attention head into the vocabulary space and analyze their
    label logits as the information contributes to label prediction. The output from
    each attention head consists of hidden states generated for every token in the
    sequence. For our analysis, we specifically use the hidden state of the last token
    preceding the prediction of label names, interpreting it as the most direct contribution
    of the attention head to the prediction, given the autoregressive nature of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, to obtain the label logits for an attention head, consider the
    output hidden states $H\in\mathbb{R}^{N\times d}$, which indicates the index of
    the first token of the predicted label names, the label logits $\mathbf{a}^{(k)}=[a^{(k)}_{1},a^{(k)}_{2},\ldots,a^{(k)}_{c}]^{\top}$-th
    sample are derived by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{a}^{(k)}=H_{(p_{\text{label}}-1),:}\cdot E\cdot L^{\top}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'we employ the same $p$. An attention head is considered biased if it meets
    the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\left\{\begin{aligned} &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(A_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{a}^{(k)}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\sum_{j=1}^{c}\mathbf{a}_{j}^{(k)}>th_{Att}^{1}\\
    &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(A_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\textbf{a}^{(k)}\right)=\frac{1}{p}\frac{1}{c}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}\left(a_{j}^{(k)}-\mu(\mathbf{a}^{(k)})\right)>th_{Att}^{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\sum_{j=0}^{c-1}w_{j}\cdot CV\left(A_{:,j}\right)=w_{j}\cdot\frac{\sigma(A_{:,j})}{\mu(A_{:,j})}<th_{Att}^{3}\end{aligned}\right.$$
    |  |'
  prefs: []
  type: TYPE_NORMAL
- en: where $w_{j}=\sum_{j=0}^{c-1}\frac{\mu(A_{:,j})}{\sum\mu(A_{:,j})}$.
  prefs: []
  type: TYPE_NORMAL
- en: The functions of the first two criteria are identical to those for biased FFN
    vector identification. The third function is the weighted sum of the coefficient
    variance of each label across test samples. The thresholds for biased attention
    head identification are also derived by grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Biased FFN Vectors and Attention Heads Manipulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After identifying the biased components of the LLM, we eliminate their influence
    by masking these biased FFN vectors and attention heads. Specifically, we create
    masks for the attention heads in each attention layer and reset the coefficient
    of the biased FFN vector and biased attention head mask.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Grid Searching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Specifically, we utilize a small subset of training data as a support set,
    with 20 samples for each class. We then grid search all combinations of threshold
    values and select the combination that results in the most balanced distribution
    of average label logits. Specifically, let $\mathbf{T}$ that minimizes the bias
    of label logits: $t^{*}=\arg\min_{t\in\mathbf{T}}\text{Bias}(\mathbf{P}(t))$.'
  prefs: []
  type: TYPE_NORMAL
- en: It is noteworthy that although there are multiple combinations of thresholds,
    they usually result in a few set of different biased components. For example,
    for a grid search of thresholds of FFN vectors with 80 combinations, it only result
    in 4 different sets of biased FFN vectors that need to be examined with the support
    set on the SST-2 dataset. Additionally, during the inference stage of evaluating
    test samples, the computation time of the UniBias method is completely identical
    to that of the original LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we aims to investigate a few research questions (RQ). RQ 1:
    After eliminating biased components from LLMs, does the ICL performance improve
    compared to the original LLM? Additionally, how does our UniBias method compare
    to existing calibration methods? RQ 2: Given that ICL suffers from prompt brittleness,
    can our UniBias method contribute to more robust ICL performance? RQ 3: Are there
    any observable patterns of biased FFN vectors and attention heads within and across
    tasks? RQ 4: What is the performance of LLMs after eliminating only the biased
    FFN vectors and only the biased attention heads, respectively? RQ 5: What is the
    impact of support set size on the performance of the UniBias method?'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets   We evaluate our UniBias method on 12 diverse natural language processing
    datasets across various tasks, including sentiment analysis, topic classification,
    natural language inference, reasoning, and word disambiguation. Statistics and
    details about the datasets can be found in Table [3](#A1.T3 "Table 3 ‣ A.1 Datasets
    ‣ Appendix A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation") in Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines   In addition to the standard ICL, we compare our proposed UniBias
    with state-of-the-art LLM debiasing and calibration baselines, including Contextual
    Calibration (CC) [[29](#bib.bib29)], Domain-Context Calibration (DC) [[7](#bib.bib7)],
    and Prototypical Calibration (PC) [[10](#bib.bib10)]. We reproduce all baselines
    strictly follows the authors’ instructions and recommendations to ensure a fair
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'Models and implementation details   We evaluate our method on Llama-2 7b and
    Llama-2 13b models [[21](#bib.bib21)]. For all experiments, unless stated otherwise,
    we use 1-shot ICL setting, i.e. one example per class, and repeat five times under
    different random seeds. We use $k=20$ sampes per class as the support set to obtain
    all threshold values by grid searching, as mentioned in the method section. The
    prompt template and more implementation details are specified in Appendix [A](#A1
    "Appendix A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of one-shot ICL performance for different methods across
    datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation
    are reported for five repetitions with different ICL examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Llama-2 7b | Llama-2 13b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | ICL | CC | DC | PC | UniBias | ICL | CC | DC | PC | UniBias |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | 87.22[6.03] | 92.24[3.39] | 94.15[1.22] | 93.90[1.54] | 94.54[0.62]
    | 93.90[1.79] | 95.25[0.93] | 95.37[0.70] | 94.56[1.71] | 95.46[0.52] |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI | 53.83[2.22] | 53.36[3.16] | 52.19[2.55] | 45.38[5.01] | 54.97[0.88]
    | 62.43[1.49] | 63.89[0.81] | 61.86[1.23] | 57.47[3.53] | 64.65[2.73] |'
  prefs: []
  type: TYPE_TB
- en: '| WiC | 50.00[0.16] | 52.19[2.00] | 52.40[1.69] | 57.11[2.49] | 53.71[1.16]
    | 54.48[3.19] | 50.63[1.73] | 49.72[0.30] | 55.67[1.67] | 57.93[1.70] |'
  prefs: []
  type: TYPE_TB
- en: '| COPA | 67.60[2.30] | 67.80[2.17] | 60.40[2.79] | 67.80[3.70] | 69.00[2.74]
    | 67.50[10.40] | 75.20[7.80] | 71.00[8.80] | 76.80[6.30] | 83.20[2.70] |'
  prefs: []
  type: TYPE_TB
- en: '| CR | 91.54[0.39] | 92.13[0.40] | 92.61[0.44] | 91.97[0.35] | 92.61[0.11]
    | 91.01[1.30] | 92.13[0.88] | 92.23[0.76] | 91.65[0.64] | 92.34[0.74] |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews | 85.59[1.87] | 83.54[1.96] | 89.08[0.86] | 86.81[2.92] | 88.29[1.24]
    | 89.14[0.44] | 88.23[1.14] | 89.34[0.61] | 86.03[0.65] | 88.68[0.43] |'
  prefs: []
  type: TYPE_TB
- en: '| MR | 89.37[1.83] | 91.77[1.42] | 92.35[0.23] | 91.39[1.65] | 92.19[0.37]
    | 90.10[2.10] | 93.20[0.57] | 93.00[0.52] | 92.80[0.86] | 92.23[1.12] |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | 66.21[7.30] | 64.33[3.68] | 65.49[2.09] | 62.59[4.71] | 67.65[6.44]
    | 76.10[4.73] | 71.99[5.02] | 66.21[1.09] | 75.31[2.90] | 78.23[2.13] |'
  prefs: []
  type: TYPE_TB
- en: '| SST-5 | 46.97[0.87] | 51.36[1.69] | 51.92[1.77] | 55.41[1.51] | 53.79[1.46]
    | 51.03[1.25] | 47.20[1.69] | 48.98[2.11] | 53.63[0.95] | 51.80[1.00] |'
  prefs: []
  type: TYPE_TB
- en: '| TREC | 72.92[12.42] | 76.44[3.21] | 77.16[3.94] | 74.92[5.78] | 80.80[3.17]
    | 74.70[12.10] | 83.80[3.86] | 80.50[9.07] | 81.85[9.53] | 81.25[6.86] |'
  prefs: []
  type: TYPE_TB
- en: '| ARC | 51.90[0.60] | 53.10[0.40] | 53.00[0.60] | 40.40[0.50] | 53.10[0.60]
    | 66.54[0.33] | 64.33[0.99] | 64.88[0.59] | 59.47[1.07] | 66.81[0.37] |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU | 41.73[2.25] | 43.72[0.97] | 43.57[1.38] | 34.12[3.41] | 44.83[0.24]
    | 53.53[1.55] | 50.84[1.57] | 51.81[1.24] | 45.50[1.65] | 53.55[1.05] |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. | 67.07 | 68.49 | 68.70 | 66.81 | 70.46 | 72.54 | 73.06 | 72.08 | 72.56
    | 75.51 | ![Refer to caption](img/f29f4bc90f8ef4459ddd889a7bfe1e0f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: The performance comparison under different numbers of ICL shots.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Main Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation")
    presents the performance of various datasets and model sizes under the 1-shot
    setting. Our proposed UniBias method consistently achieves the highest accuracies
    in most cases. In terms of overall average accuracy, UniBias improves upon the
    standard ICL by a substantial margin of 3.39% and exceeds the state-of-the-art
    (SOTA) DC by 1.76% using Llama-2 7b. With Llama-2 13b, UniBias surpasses the standard
    ICL and the SOTA CC by 2.97% and 2.45%, respectively. Figure [5](#S4.F5 "Figure
    5 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation") further illustrates
    the results under zero-shot and various few-shot settings for COPA, SST2, and
    MMLU. Our proposed UniBias consistently surpasses other baselines in all scenarios,
    underscoring its effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: In response to RQ 1, UniBias not only enhances the performance of original LLMs
    but also outperforms existing methods. We attribute this success to its internal
    analysis and bias mitigation techniques, which leverage FFNs and attentions, unlike
    other methods that rely solely on external observations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Alleviating Prompt Brittleness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing studies have found that LLMs are prone to prompt brittleness, with
    various factors such as the selection and order of examples, as well as the prompt
    formatting. To address RQ 2, we simulate these brittle scenarios by choosing different
    demonstration samples, using different prompt formats, and changing the example
    order to observe variations in LLM performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation") presents Llama-2 7b’s
    performance both with and without UniBias. Without UniBias, the standard ICL’s
    performance varies significantly, ranging from 8% to 26%, demonstrating its instability.
    After applying UniBias, the accuracy stabilizes, with variations consistently
    less than 4% under perturbations of various design settings. This evidence verifies
    that UniBias effectively reduces prompt brittleness and enhances robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Performance comparison of only removing biased FFN vectors (FFN-only),
    only removing biased attention heads (attention-only), our Unibias method, and
    the ICL of original LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | SST-2 | MNLI | WiC | COPA | CR | AGNews | MR | RTE | SST-5 | TREC
    | ARC | MMLU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  prefs: []
  type: TYPE_TB
- en: '| ICL | 87.22 | 53.83 | 50.00 | 67.60 | 91.54 | 85.59 | 89.37 | 66.21 | 46.97
    | 72.92 | 51.90 | 41.73 |'
  prefs: []
  type: TYPE_TB
- en: '| FFN-only | 94.17 | 54.59 | 50.88 | 69.20 | 92.57 | 85.52 | 91.78 | 67.33
    | 47.09 | 73.04 | 51.92 | 42.62 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention-only | 94.22 | 52.83 | 52.76 | 68.50 | 91.49 | 86.25 | 92.61 |
    66.55 | 52.68 | 80.68 | 53.00 | 44.67 |'
  prefs: []
  type: TYPE_TB
- en: '| UniBias | 94.54 | 54.97 | 53.71 | 69.00 | 92.61 | 88.29 | 92.19 | 67.65 |
    53.79 | 80.80 | 53.10 | 44.83 | ![Refer to caption](img/5d3b58c348b5a8bbc294a6ed892251e8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Analysis of biased attention heads (AHs) and FFN vectors (FFNs).
    The frequency count of biased LLM components across five repeat experiments with
    different example selections is reported.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Biased LLM Components Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In response to RQ3, we present the frequency counts of identified biased attention
    heads (AHs) and FFNs under repeated experiments in Figure [6](#S4.F6 "Figure 6
    ‣ 4.3 Alleviating Prompt Brittleness ‣ 4 Experiments ‣ UniBias: Unveiling and
    Mitigating LLM Bias through Internal Attention and FFN Manipulation"). A large
    frequency count for an LLM component indicates a higher repeat of being identified
    as biased in the corresponding dataset. The first subfigure displays the biased
    components for various example selections, revealing several commonly biased LLM
    components across different prompts within a single dataset. The second subfigure
    highlights the common biased components across different datasets (ARC and MMLU)
    for the reasoning task, indicating that different datasets with similar tasks
    could share common biased LLM components. The third subfigure demonstrates the
    presence of common biased LLM components across different tasks. Experimental
    results suggest an interesting future direction: we may identify global biased
    components that would mitigate bias across multiple tasks and diverse prompt design
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Ablations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct ablation studies to analyze the impact of exclusively eliminating
    biased AHs or FFNs to address RQ 4. Table [2](#S4.T2 "Table 2 ‣ 4.3 Alleviating
    Prompt Brittleness ‣ 4 Experiments ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation") presents the results of removing
    only biased FFN vectors (FFN-only) and only biased attention heads (attention-only).
    Both FFN-only and attention-only methods outperform the standard ICL, demonstrating
    their effectiveness. When combined as UniBias, the method achieves the best results
    across most datasets, indicating that the two approaches are complementary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we further conduct experiments to investigate the impact of support
    set size (RQ 5), which is detailed in Appendix [C](#A3 "Appendix C Impact of support
    set size ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention
    and FFN Manipulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bias in LLMs:   It is well recognized that LLMs are unstable under various
    ICL design settings, and this instability arises from biases in LLMs toward predicting
    certain answers [[29](#bib.bib29), [14](#bib.bib14)]. To understand these biases,
    existing studies have identified various bias factors, including recency bias,
    majority label bias, common token bias [[29](#bib.bib29)], and domain label bias
    [[7](#bib.bib7)] in classification tasks. More recently, selection bias, which
    consistently favors specific options in multiple-choice questions, has also been
    identified [[30](#bib.bib30), [25](#bib.bib25)]. To address these biases, several
    calibration methods have been proposed, including contextual calibration [[29](#bib.bib29)],
    domain-context calibration [[7](#bib.bib7)], and prototypical calibration [[10](#bib.bib10)].
    However, these identified bias factors and calibration methods are derived from
    external observations or adjustments of LLM outputs, leaving the underlying mechanisms
    within LLMs that cause such biases poorly understood.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mechanistic Interpretability:   Mechanistic interpretability [[6](#bib.bib6),
    [23](#bib.bib23)] aims to explain the internal processes in language models, facilitating
    the interpretation of the contributions of individual model components to the
    final prediction. Our work builds on the understanding of the residual stream
    [[6](#bib.bib6)], the logit lens [[16](#bib.bib16)], and the interpretation of
    LLM components in the vocabulary space [[4](#bib.bib4), [8](#bib.bib8)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we have deepened the understanding of biases in LLMs by unveiling
    the internal mechanisms that contribute to various bias factors. Building on this
    understanding, we proposed our UniBias method to mitigate these biases by identifying
    and eliminating biased FFN vectors and attention heads, demonstrating an effective
    way to manipulate the internal structures of LLMs. Extensive experiments show
    that our UniBias method achieves state-of-the-art performance across 12 NLP datasets
    and different ICL settings. Additionally, our method successfully alleviates prompt
    brittleness and enhances the robustness of ICL.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dagan et al. [2005] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal
    recognising textual entailment challenge. In *Machine learning challenges workshop*,
    pages 177–190\. Springer, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dar et al. [2023] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing
    transformers in embedding space. In Anna Rogers, Jordan Boyd-Graber, and Naoaki
    Okazaki, editors, *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pages 16124–16170, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.893.
    URL [https://aclanthology.org/2023.acl-long.893](https://aclanthology.org/2023.acl-long.893).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. [2023] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elhage et al. [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan,
    Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
    Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,
    Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,
    Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework
    for transformer circuits. *Transformer Circuits Thread*, 2021. https://transformer-circuits.pub/2021/framework/index.html.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fei et al. [2023] Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. Mitigating
    label biases for in-context learning. In Anna Rogers, Jordan Boyd-Graber, and
    Naoaki Okazaki, editors, *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 14014–14031, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.783.
    URL [https://aclanthology.org/2023.acl-long.783](https://aclanthology.org/2023.acl-long.783).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geva et al. [2021] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    Transformer feed-forward layers are key-value memories. In Marie-Francine Moens,
    Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, *Proceedings of
    the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    5484–5495, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL [https://aclanthology.org/2021.emnlp-main.446](https://aclanthology.org/2021.emnlp-main.446).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geva et al. [2022] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg.
    Transformer feed-forward layers build predictions by promoting concepts in the
    vocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
    *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*,
    pages 30–45, Abu Dhabi, United Arab Emirates, December 2022\. Association for
    Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.3. URL [https://aclanthology.org/2022.emnlp-main.3](https://aclanthology.org/2022.emnlp-main.3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2023] Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei.
    Prototypical calibration for few-shot learning of language models. In *The Eleventh
    International Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hanna et al. [2023] Michael Hanna, Ollie Liu, and Alexandre Variengien. How
    does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained
    language model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
    editors, *Advances in Neural Information Processing Systems*, volume 36, pages
    76033–76060\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *International Conference on Learning Representations*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu and Liu [2004] Minqing Hu and Bing Liu. Mining and summarizing customer reviews.
    In *Proceedings of the tenth ACM SIGKDD international conference on Knowledge
    discovery and data mining*, pages 168–177, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2022] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming
    few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline
    Villavicencio, editors, *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 8086–8098, Dublin,
    Ireland, May 2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556.
    URL [https://aclanthology.org/2022.acl-long.556](https://aclanthology.org/2022.acl-long.556).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and
    Yue Zhang, editors, *Proceedings of the 2022 Conference on Empirical Methods in
    Natural Language Processing*, pages 11048–11064, Abu Dhabi, United Arab Emirates,
    December 2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759.
    URL [https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nostalgebraist [2020] Nostalgebraist. Interpreting gpt: the logit lens, 2020.
    URL [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang and Lee [2005] Bo Pang and Lillian Lee. Seeing stars: Exploiting class
    relationships for sentiment categorization with respect to rating scales. In Kevin
    Knight, Hwee Tou Ng, and Kemal Oflazer, editors, *Proceedings of the 43rd Annual
    Meeting of the Association for Computational Linguistics (ACL’05)*, pages 115–124,
    Ann Arbor, Michigan, June 2005\. Association for Computational Linguistics. doi:
    10.3115/1219840.1219855. URL [https://aclanthology.org/P05-1015](https://aclanthology.org/P05-1015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pilehvar and Camacho-Collados [2019] Mohammad Taher Pilehvar and Jose Camacho-Collados.
    WiC: the word-in-context dataset for evaluating context-sensitive meaning representations.
    In Jill Burstein, Christy Doran, and Thamar Solorio, editors, *Proceedings of
    the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    1267–1273, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1128. URL [https://aclanthology.org/N19-1128](https://aclanthology.org/N19-1128).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roemmele et al. [2011] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S
    Gordon. Choice of plausible alternatives: An evaluation of commonsense causal
    reasoning. In *2011 AAAI Spring Symposium Series*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy
    Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, *Proceedings
    of the 2013 Conference on Empirical Methods in Natural Language Processing*, pages
    1631–1642, Seattle, Washington, USA, October 2013\. Association for Computational
    Linguistics. URL [https://aclanthology.org/D13-1170](https://aclanthology.org/D13-1170).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voorhees and Tice [2000] Ellen M Voorhees and Dawn M Tice. Building a question
    answering test collection. In *Proceedings of the 23rd annual international ACM
    SIGIR conference on Research and development in information retrieval*, pages
    200–207, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck
    Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect
    object identification in gpt-2 small. In *The Eleventh International Conference
    on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong
    Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective
    for understanding in-context learning. In Houda Bouamor, Juan Pino, and Kalika
    Bali, editors, *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing*, pages 9840–9855, Singapore, December 2023a. Association
    for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.609. URL [https://aclanthology.org/2023.emnlp-main.609](https://aclanthology.org/2023.emnlp-main.609).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023b] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not
    fair evaluators. *arXiv preprint arXiv:2305.17926*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A
    broad-coverage challenge corpus for sentence understanding through inference.
    In Marilyn Walker, Heng Ji, and Amanda Stent, editors, *Proceedings of the 2018
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1112–1122,
    New Orleans, Louisiana, June 2018\. Association for Computational Linguistics.
    doi: 10.18653/v1/N18-1101. URL [https://aclanthology.org/N18-1101](https://aclanthology.org/N18-1101).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2023] Qinan Yu, Jack Merullo, and Ellie Pavlick. Characterizing
    mechanisms for factual recall in language models. In Houda Bouamor, Juan Pino,
    and Kalika Bali, editors, *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, pages 9924–9959, Singapore, December 2023\. Association
    for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.615. URL [https://aclanthology.org/2023.emnlp-main.615](https://aclanthology.org/2023.emnlp-main.615).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level
    convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee,
    M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing
    Systems*, volume 28\. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2021] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. Calibrate before use: Improving few-shot performance of language models.
    In Marina Meila and Tong Zhang, editors, *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pages 12697–12706\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/zhao21c.html](https://proceedings.mlr.press/v139/zhao21c.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2023] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie
    Huang. Large language models are not robust multiple choice selectors. In *The
    Twelfth International Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate our Unibias method using 12 diverse natural language processing
    datasets across tasks such as sentiment analysis, topic classification, reasoning,
    natural language inference, and word disambiguation, as presented in Table [3](#A1.T3
    "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation"). In
    our experiments, we utilize $k$-shot ICL. For testing, we randomly select 2000
    samples for MMLU and 3000 samples for MNLI and MR, while employing the original
    testing sets for other datasets. Detailed dataset statistics are available in
    Table [3](#A1.T3 "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Detailed Dataset information'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | # Classes | # Testing Size |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment classification |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SST2 [[20](#bib.bib20)] | 2 | 872 |'
  prefs: []
  type: TYPE_TB
- en: '| SST-5 [[20](#bib.bib20)] | 5 | 2210 |'
  prefs: []
  type: TYPE_TB
- en: '| MR [[17](#bib.bib17)] | 2 | 3000 |'
  prefs: []
  type: TYPE_TB
- en: '| CR [[13](#bib.bib13)] | 2 | 376 |'
  prefs: []
  type: TYPE_TB
- en: '| Topic classification |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews [[28](#bib.bib28)] | 4 | 7600 |'
  prefs: []
  type: TYPE_TB
- en: '| TREC [[22](#bib.bib22)] | 6 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| Natural language inference |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI [[26](#bib.bib26)] | 3 | 3000 |'
  prefs: []
  type: TYPE_TB
- en: '| RTE [[3](#bib.bib3)] | 2 | 277 |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-Challenge [[2](#bib.bib2)] | 4 | 1170 |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU [[12](#bib.bib12)] | 4 | 2000 |'
  prefs: []
  type: TYPE_TB
- en: '| COPA [[19](#bib.bib19)] | 2 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Word disambiguation |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WiC [[18](#bib.bib18)] | 2 | 638 |'
  prefs: []
  type: TYPE_TB
- en: A.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Experiments on internal mechanisms of biased factors: All experiments are conducted
    on Llama-2 7b model. For the vanilla label bias experiment, we projecting all
    FFN value vectors into the vocabulary space and sum the label logits for all FFN
    vectors whose label logits rank within the top $10$ over the vocabulary to calculate
    uncontextual accumulated FFN logits. We change different set of label words in
    prompt to derive the label prediction frequency of different label pairs. For
    the recency bias experiment, based on findings in [[24](#bib.bib24)], instead
    of the summed attention weights over the whole example, we adopt the sum of attention
    weights on label words of the example, e.g. "Answer: positive" as the effective
    attention weight on each example. For the selection bias experiment, we use zeroshot
    ARC dataset prompts in Table [6](#A4.T6 "Table 6 ‣ Appendix D Prompt Templates
    ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation"), and we use 12 samples for each class. The attention weight is
    also summed on label words instead of the whole option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines:   We reproduce all baselines using the publicly available code released
    by the authors to ensure a fair comparison. For the PC method, instead of using
    test samples as in the original work, we employ 200 training samples per class
    as the estimate set for parameter estimation using the EM algorithm. This adjustment
    is made to reflect real-world scenarios where test samples are not readily available.
    Additionally, the number of samples used by the PC method is significantly larger
    than that used by our UniBias method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unibias:   In our method, all threshold values are determined through grid
    searching as described in the methodology section. Specifically, we use 20 samples
    per class as the support set for grid searching in all experiments. For each repetition
    of the experiment, the support set is randomly selected based on different random
    seeds. Additionally, to manipulate biased FFN vectors and attention heads, we
    create masks for the attention heads of all attention layers and adjust the FFN
    coefficient values and attention head masks using the hook operation. Additionally,
    we conduct the experiment on four A5000 GPUs. We will release our code upon acceptance
    to facilitate easy reproduction.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Limitation and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we provide a novel insight into the internal mechanisms behind
    the bias of LLMs. As a pioneering effort in mitigating LLM bias through manipulation
    of the model’s internal structures, our approach relies on grid searching with
    a small set of labeled training samples. Future research could focus on reducing
    this reliance, potentially improving the efficiency and applicability of our method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd7000e21b475bf213744ce1df6f65c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Performance of Unibias under different support set.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many interesting avenues for future research. For instance, instead
    of identifying biased components for each ICL prompt, future work could explore
    the identification of global biased components that mitigate bias across multiple
    tasks and diverse prompt design settings. Additionally, the biased FFN vectors
    and attention heads we identify could potentially serve as sensors for guiding
    effective prompt generation. We expect that this internal perspective on LLM bias
    will inspire more innovative applications in both bias mitigation methods and
    prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Impact of support set size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our proposed UniBias method employs a small support set for grid searching.
    To analyze its effect, we vary the size of the support set. Figure 7 illustrates
    Unibias’s performance with support set sizes ranging from 5 to 50 samples. The
    results indicate that the performance stabilizes when the support set contains
    20 or more samples per class. Notably, for the SST2 dataset, even with much fewer
    support samples, Unibias significantly outperforms the standard ICL.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Prompt Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prompt templates used in this work are provided below. We generate few-shot
    ICL templates follow the template styles in [[10](#bib.bib10), [7](#bib.bib7)],
    as illustrated in Table [4](#A4.T4 "Table 4 ‣ Appendix D Prompt Templates ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Prompt templates for all $k$-shot ICL experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Template | Label Space |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | Review: {sentence} | negative / positive |'
  prefs: []
  type: TYPE_TB
- en: '| CR | Sentiment: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| MR |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI | Premise: {premise} | yes / maybe / no |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hypothesis: {hypothesis} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| ARC | Question: {question} | A / B / C / D |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU | {options} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| SST-5 | Review: {sentence} | terrible / bad / okay / good / great |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sentiment: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews | Article: {passage} | world / sports / business / technology & science
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| TREC | Question: {sentence} | abbreviation / entity / description / person
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer Type: {label} | / location / number |'
  prefs: []
  type: TYPE_TB
- en: '| COPA | Premise: {premise} | 1 / 2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Choice1: {choice1} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Choice2: {choice2} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | Premise: {sentence1} | yes / no |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hypothesis: {sentence2} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| WiC | Sentence1: {sentence1} | false / true |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sentence2: {sentence2} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Word: {word} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Templates of different prompt formatting used in the prompt brittleness
    experiment for SST-2.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ID | Template | Label Space |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Review: {Sentence} | Positive / Negative |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sentiment: {Label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Input: {Sentence} | Positive / Negative |'
  prefs: []
  type: TYPE_TB
- en: '|  | Prediction: {Label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Review: {Sentence} | good / bad |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sentiment: {Label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | {Sentence} It was {Label} | good / bad |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Review: {Sentence} | Yes / No |'
  prefs: []
  type: TYPE_TB
- en: '|  | Positive Review: {Label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | {Sentence} My overall feeling was that the movie was {Label} | good /
    bad |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Review: {Sentence} | Positive / Negative |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: Is the sentiment of the above review Positive or Negative? |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {Label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | My review for last night’s film: {Sentence}The critics agreed that this
    | good / bad |'
  prefs: []
  type: TYPE_TB
- en: '|  | movie was {Label} |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Prompt templates for the 0-shot experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Template | Label Set |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | Review: {sentence} | negative / positive |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sentiment: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| COPA | Premise: {premise} | 1 / 2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Choice1: {choice1} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Choice2: {choice2} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU | Question: {question} | A / B / C / D |'
  prefs: []
  type: TYPE_TB
- en: '|  | {options} |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Answer: {label} |  |'
  prefs: []
  type: TYPE_TB
