- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.07418](https://ar5iv.labs.arxiv.org/html/2309.07418)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yeqi Gao a916755226@gmail.com. The University of Washington.    Zhao Song zsong@adobe.com.
    Adobe Research.    Weixin Wang wwang176@jh.edu. Johns Hopkins University.    Junze
    Yin junze@bu.edu. Boston University.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models have played a pivotal role in revolutionizing various
    facets of our daily existence. Serving as the cornerstone of virtual assistants,
    they have seamlessly streamlined information retrieval and task automation. Spanning
    domains from healthcare to education, these models have made an enduring impact,
    elevating productivity, decision-making processes, and accessibility, thereby
    influencing and, to a certain extent, reshaping the lifestyles of people.
  prefs: []
  type: TYPE_NORMAL
- en: Solving attention regression is a fundamental task in optimizing LLMs. In this
    work, we focus on giving a provable guarantee for the one-layer attention network
    objective function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Here $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$ and $A_{2}\in\mathbb{R}^{n\times
    d}$ is a matrix in $\mathbb{R}^{n\times d}$ is the $j_{0}$. The $X,Y\in\mathbb{R}^{d\times
    d}$ and $b_{j_{0},i_{0}}\in\mathbb{R}$-th row and $i_{0}$, $Y_{*,i_{0}}\in\mathbb{R}^{d}$-column
    vector of $Y$ is the vectorization of $X$.
  prefs: []
  type: TYPE_NORMAL
- en: In a multi-layer LLM network, the matrix $B\in\mathbb{R}^{n\times d}$ can be
    viewed as the input of a layer. The matrix version of $x$ and $Y$. We provide
    an iterative greedy algorithm to train loss function $L(X,Y)$ that runs in $\widetilde{O}(({\cal
    T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon))$
    denotes the time of multiplying $a\times b$ matrix, and $\omega\approx 2.37$ denotes
    the exponent of matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) like GPT-1 [[149](#bib.bib149)], BERT [[49](#bib.bib49)],
    GPT-2 [[154](#bib.bib154)], GPT-3 [[24](#bib.bib24)], ChatGPT [[35](#bib.bib35)],
    GPT-4 [[134](#bib.bib134)], OPT [[209](#bib.bib209)], Llama [[174](#bib.bib174)],
    and Llama 2 [[176](#bib.bib176)] have demonstrated impressive capabilities in
    natural language processing (NLP). These models understand and generate complex
    language, enabling a wide range of applications such as sentiment analysis [[200](#bib.bib200)],
    language translation [[1](#bib.bib1)], question answering [[23](#bib.bib23)],
    and text summarization [[137](#bib.bib137)]. Despite their high-quality performance,
    there remains untapped potential in optimizing and training these massive models,
    making it a challenging endeavor in the present day.
  prefs: []
  type: TYPE_NORMAL
- en: The primary technical foundation supporting the capabilities of LLMs is the
    attention matrix [[149](#bib.bib149), [179](#bib.bib179), [24](#bib.bib24), [49](#bib.bib49)].
    The central concept of attention is to learn representations that emphasize the
    most relevant parts of the input. To be more specific, the attention mechanism
    compares the query vectors (the output tokens) with the key vectors (the input
    tokens). The attention weights are then determined based on the similarity of
    this comparison, indicating the relative importance of each input token. These
    attention weights are used to compute weighted averages of the value vectors,
    resulting in the output representation. By leveraging attention, LLMs acquire
    the ability to focus on the crucial aspects of the input, allowing them to gather
    pertinent information more efficiently and precisely. This capability enables
    LLMs to process longer texts effectively and comprehend intricate semantic relationships.
    Notably, the self-attention mechanism enables LLMs to establish connections between
    various segments of the input sequence, enhancing their contextual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: We start with defining the general Attention forward layer,
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1.1  ($\ell$-th layer forward computation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let ${\bf 1}_{n}$-dimensional vector whose entries are all $1$ be a function:
    each entry of the vector in $\mathbb{R}^{n}$ and other entries of this matrix
    are all $0$, let $X_{\ell}\in\mathbb{R}^{n\times d}$-th layer input and $X_{\ell+1}\in\mathbb{R}^{n\times
    d}$'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle X_{\ell+1}\leftarrow D^{-1}\exp(X_{\ell}QK^{\top}X_{\ell}^{\top})X_{\ell}V$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $D:=\operatorname{diag}(\exp(X_{\ell}QK^{\top}X_{\ell}^{\top}){\bf 1}_{n})$
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, a general optimization with respect to attention computation
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1.2  (Attention optimization).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$. The attention computation
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $D(X)\in\mathbb{R}^{n\times n}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4299be57d79d62ad6bdc597ce50480f9.png)![Refer to caption](img/00d76d08bdd817b3626571f58ccdc732.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The visualization of the attention optimization (see Definition [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$. We first get $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times n}$, $X$. Then,
    we have $D(X)\in\mathbb{R}^{n\times n}$. After that, we multiply $D(X)^{-1}$,
    $A_{3}$ and subtract $B$ matrices, the purple rectangle represents the $n$ matrices,
    and the green squares represent the $n\times n$ diagonal matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e77a6fab7b9b8d8aa84a3ca6114da1a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The visualization of a variation of Definition [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$, $D(X)\in\mathbb{R}^{n\times n}$. $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times
    n}$, and $\operatorname{vec}=\mathrm{mat}^{-1}$ and multiply $\operatorname{\mathsf{A}}$.
    Then, we multiply $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$,
    which gives us a vector in $\mathbb{R}^{n^{2}}$ to transform that into a matrix
    in $\mathbb{R}^{n\times n}$. Finally, we compute the minimum of the Frobenius
    norm of $\mathrm{mat}((D(X)\otimes I_{n})^{-1}\cdot\exp(\operatorname{\mathsf{A}}\operatorname{vec}(X)))A_{3}Y-B$:
    in the matrix $D(X)\otimes I_{n}$. The red rectangle represents the matrix in
    $\mathbb{R}^{d\times d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here $X=QK^{\top},Y=V$ are the input of a layer $X_{\ell}$ are the output layer
    $X_{\ell+1}$. Attention computation has been analyzed in many recent works [[203](#bib.bib203),
    [11](#bib.bib11), [29](#bib.bib29), [75](#bib.bib75), [57](#bib.bib57), [171](#bib.bib171),
    [175](#bib.bib175), [139](#bib.bib139), [125](#bib.bib125), [208](#bib.bib208),
    [140](#bib.bib140), [159](#bib.bib159)], but none of them give a complete analysis
    of the full version of the attention computation problem. They all simplify this
    problem by different strategies (see details in Table [1](#S4.T1 "Table 1 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")).'
  prefs: []
  type: TYPE_NORMAL
- en: However, simplifying this problem may lead to a significant decrease in the
    model performance, which may require extra model training or fine-tuning. This
    results in deployment obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, our focus is on optimizing the attention mechanism. Our goal
    is to present a complete, un-simplified analysis of the attention problem defined
    in Definition [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), a task that, to the best of our knowledge, has not been done before. We
    provide a provable guarantee for optimizing the attention function in the case
    of a single-layer attention network. Our motivation stems from the critical role
    of the attention optimization problem in the functionality of LLMs, and we firmly
    believe that our theoretical analysis will significantly influence the development
    of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: As [[11](#bib.bib11)], they show that one step forward computation of attention
    can be done in $o(n^{2})$ matrix. However, it is still an open problem about how
    fast we optimize the loss function via the iterative method.
  prefs: []
  type: TYPE_NORMAL
- en: 'How fast can we optimize the training process of attention matrix (See Definition [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"))?'
  prefs: []
  type: TYPE_NORMAL
- en: In this study, we make progress towards this fundamental question.
  prefs: []
  type: TYPE_NORMAL
- en: To establish the correctness of our algorithm, we conduct a comprehensive analysis
    of the positive semi-definite (PSD) property and the Lipschitz continuity of the
    Hessian matrix constructed from the attention matrix. These two properties provide
    the necessary assurance for employing TensorSRHT and Newton’s method, ensuring
    both fast computation and convergence, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will present our main result as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.3  (Informal version of our main theorem).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $A_{1},A_{2},A_{2}\in\mathbb{R}^{n\times d}$ solves to the attention problem
    up to $\epsilon$. Here $\omega\approx 2.37$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here $\omega$ denotes the time of multiplying an $a\times b$ size matrix, and
    ${\cal T}_{\mathrm{mat}}(n,n,n)=n^{\omega}$. See more details of matrix multiplication
    notation in Section [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Relationship with the Softmax Regression Problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Moreover, the attention weight can be viewed as the output of a softmax regression
    model, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1.4  (Single softmax regression [[55](#bib.bib55)] and multiple softmax
    regression [[72](#bib.bib72)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a matrix $A\in\mathbb{R}^{n\times d}$, the single softmax regression problem
    is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Let $D(X)\in\mathbb{R}^{n\times n}$ and $X\in\mathbb{R}^{d\times d}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'On the one hand, due to the observation in [[72](#bib.bib72), [73](#bib.bib73)],
    the equation in Part 1 of Definition [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single
    softmax regression [55] and multiple softmax regression [72]). ‣ Relationship
    with the Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") can be viewed as one row of the equation
    in Part 2 of Definition [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single softmax
    regression [55] and multiple softmax regression [72]). ‣ Relationship with the
    Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, due to the well-known tensor trick¹¹1Given matrices $A_{1},A_{2}\in\mathbb{R}^{n\times
    d}$, the well-known tensor-trick suggests that $\operatorname{vec}(A_{1}XA_{2}^{\top})=(A_{1}\otimes
    A_{2})\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$. (see [[58](#bib.bib58), [52](#bib.bib52)]
    as an example), the Part 2 equation Definition [1.4](#S1.Thmtheorem4 "Definition
    1.4 (Single softmax regression [55] and multiple softmax regression [72]). ‣ Relationship
    with the Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") is equivalent to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'which can be a slightly more complicated version of the Part 1 equation in
    Definition [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single softmax regression [55]
    and multiple softmax regression [72]). ‣ Relationship with the Softmax Regression
    Problem ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). In particular, instead of one re-scaling factor, we will have $n$ into
    $n$. For each chunk, we use the same rescaling factor.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfca81b3da52836ce3d9be7b8fb0c63c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The visualization of Eq. ([1](#S1.E1 "In Relationship with the Softmax
    Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). Let $A_{1},A_{2}\in\mathbb{R}^{n\times d}$, $C,D(X)\in\mathbb{R}^{n\times
    n}$. We first get that $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$
    with $\operatorname{vec}(X)$ with $\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$
    and subtract it from $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))$
    norm of $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))-c$:
    in the matrix $D(X)\otimes I_{n}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the multiple softmax regression problem is a simplified version of
    what we study in Definition [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). We believe that our work can also support the study of softmax regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Relatinship with Support Vector Machines (SVM)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The usual SVM [[91](#bib.bib91), [37](#bib.bib37), [77](#bib.bib77), [175](#bib.bib175)]
    objective function in optimization can be viewed as a product of a summation of
    a batch of inner product. Inspired by that, we can define $n$ for each $j_{0}\in[n]$
    functions $h(Y)_{i_{0}}\in\mathbb{R}^{n}$ is the vectorization of $X$ is the vectorization
    of $Y$ can be turned into
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $b_{j_{0},i_{0}}$. We call this formulation SVM-inspired formulation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d73e25196093f8ef6e4aad48d830bd59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The visualization of Eq. ([2](#S1.E2 "In Relatinship with Support
    Vector Machines (SVM) ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$. We have $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ is the $j_{0}$. $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$ (see Definition [4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")) and $h(Y)_{i_{0}}\in\mathbb{R}^{n}$
    at $j_{0}$-column from the inner produce. Finally, we compute the square of this
    difference and add all of them from $i_{0}=1$ and from $j_{0}=1$. In this figure,
    we use blue rectangles to represent vectors, where the dark blue represents $f(x)_{j_{0}}$,
    and the light blue represents the terms used to compute $f(x)_{j_{0}}$. The green
    square represents the scalar. The red rectangle represents the matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Roadmap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Section [2](#S2 "2 Related Work ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we introduce related research work. In Section [3](#S3
    "3 Technique Overview ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we provide an overview of the techniques we will use throughout the rest
    of the paper. In Section [4](#S4 "4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we present the basic notations we use, some mathematical
    facts, and helpful definitions that support the following proof. In Section [5](#S5
    "5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we compute the gradients of the helpful functions defined earlier. In Section [6](#S6
    "6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we define the Hessian for further discussion. In Section [7](#S7 "7 Hessian for
    𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    the Hessian matrix with respect to $X$ is Lipschitz. In Section [9](#S9 "9 Hessian
    for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we show that the Hessian matrix with respect to $X$ and show that it is
    Lipschitz and positive semidefinite (PSD). In Section [11](#S11 "11 Hessian for
    𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we compute the Hessian matrix with respect to both $X$. In Section [12](#S12 "12
    Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we demonstrate that the Hessian matrix with respect to
    both $X$ is Lipschitz. In Section [13](#S13 "13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we introduce some tensor sketch techniques to obtain fast approximations
    of the Hessian. In Section [14](#S14 "14 Analysis Of Algorithm 1 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we introduce the Newton step.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[18](#bib.bib18)] represents one of the earliest works that employed attention
    in NLP. They assumed that a fixed-length vector could enhance the performance
    of the encoder-decoder design by incorporating an attention mechanism. This mechanism
    allows the decoder to focus on relevant words in the source sentence while generating
    translations. Consequently, this approach significantly improves the performance
    of machine translation models compared to those without an attention mechanism.
    Subsequently, [[113](#bib.bib113)] explained two variants of attention: local
    attention, which considers a subset of source words at a time, and global attention,
    which attends to all source words.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention finds extensive applications across various domains. In image captioning,
    [[192](#bib.bib192)] utilizes attention matrices to align specific parts of an
    image with words in a caption. In the context of the Transformer model [[179](#bib.bib179)],
    attention matrices capture differences between words in a sentence. In the realm
    of graph neural networks, [[177](#bib.bib177)] investigates these neural network
    architectures designed for graph-structured data, computing attention matrices
    between each node and its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: On the theoretical side, after the emergence of LLMs, there has been a substantial
    body of work dedicated to studying attention computation [[57](#bib.bib57), [11](#bib.bib11),
    [203](#bib.bib203), [40](#bib.bib40), [118](#bib.bib118), [29](#bib.bib29), [99](#bib.bib99)].
    Notably, recent research by [[203](#bib.bib203), [40](#bib.bib40), [99](#bib.bib99)]
    employs Locality Sensitive Hashing (LSH) techniques to approximate attention mechanisms.
    In particular, [[203](#bib.bib203)] introduces $\mathsf{KDEformer}$, $\sinh$.
    Lastly, [[57](#bib.bib57)] proposes randomized and deterministic algorithms for
    reducing the dimensionality of attention matrices in LLMs, achieving high accuracy
    while significantly reducing feature dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, numerous studies have attempted to analyze theoretical attention
    from the perspectives of optimization and convergence [[111](#bib.bib111), [69](#bib.bib69),
    [172](#bib.bib172), [205](#bib.bib205)]. [[111](#bib.bib111)] investigated how
    transformers acquire knowledge about word co-occurrence patterns. [[69](#bib.bib69)]
    focused on studying regression problems inspired by neural networks that employ
    exponential activation functions. [[172](#bib.bib172)] analyzed why models occasionally
    prioritize significant words and explained how the attention mechanism evolves
    during the training process. [[205](#bib.bib205)] demonstrated that the presence
    of a heavy-tailed noise distribution contributes to the bad performance of stochastic
    gradient descent (SGD) compared to adaptive methods.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are numerous amount of works focusing on the theoretical aspects of LLMs.
    In [[155](#bib.bib155)], the syntactic representations of the attention matrix
    and the individual word embeddings are presented, together with the mathematical
    justification of elucidating the geometrical properties of these representations.
    [[84](#bib.bib84)] introduces a structural probe that analyzes, under the linear
    transformation of a word representation space of a neural network, whether or
    not syntax trees are embedded.
  prefs: []
  type: TYPE_NORMAL
- en: '[[39](#bib.bib39), [110](#bib.bib110), [151](#bib.bib151), [100](#bib.bib100)]
    study the optimization of LLMs. [[39](#bib.bib39)] proposes a new algorithm called
    ZO-BCD. It has favorable overall query complexity and a smaller computational
    complexity in each iteration. [[110](#bib.bib110)] creates a simple scalable second-order
    optimizer, called Sophia. In different parts of the parameter, Sophia adapts to
    the curvature. This may be strongly heterogeneous for language modeling tasks.
    The bound of the running time does not rely on the condition number of the loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Other theoretical LLM papers study the knowledge and skills of LLMs. [[188](#bib.bib188)]
    analyzes distinct “skill” neurons, which are regarded as robust indicators of
    downstream tasks when employing the process of soft prompt-tuning, as discussed
    in [[108](#bib.bib108)], for language models. [[50](#bib.bib50)] find a positive
    relationship between the activation of these neurons and the expression of their
    corresponding facts, through analyzing BERT. Simultaneously, [[32](#bib.bib32)]
    employs a fully unsupervised approach to extract latent knowledge from a language
    model’s internal activations. In addition, [[79](#bib.bib79)] and [[124](#bib.bib124)]
    show that in the feed-forward layers of pre-trained models, language models localize
    knowledge. [[194](#bib.bib194)] explores the feasibility of selecting a specific
    subset of layers for modification and determining the optimal location for integrating
    a classifier. [[123](#bib.bib123)] demonstrate that large trained transformers
    exhibit sparsity in their feedforward activations. Zero-th order algorithm for
    training LLM has been analyzed [[125](#bib.bib125), [54](#bib.bib54), [204](#bib.bib204)].
  prefs: []
  type: TYPE_NORMAL
- en: LLMs Application and Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, there has been much interest in developing LLM-based systems for conversational
    AI and task-oriented dialogue, like Google’s Meena chatbot [[148](#bib.bib148)],
    Microsoft 365 Copilot [[161](#bib.bib161)], Adobe firefly, Adobe Photoshop, GPT
    series [[149](#bib.bib149), [154](#bib.bib154), [24](#bib.bib24), [35](#bib.bib35),
    [134](#bib.bib134)], and BERT [[49](#bib.bib49)].
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, LLM evaluation is also a popular research area. Within the field of
    NLP, LLMs are evaluated based on natural language understanding [[20](#bib.bib20),
    [102](#bib.bib102), [103](#bib.bib103), [45](#bib.bib45)], reasoning [[23](#bib.bib23),
    [187](#bib.bib187), [193](#bib.bib193)], natural language generation [[183](#bib.bib183),
    [147](#bib.bib147), [137](#bib.bib137), [36](#bib.bib36), [47](#bib.bib47)], and
    multilingual tasks [[10](#bib.bib10), [5](#bib.bib5), [112](#bib.bib112), [199](#bib.bib199)].
    Robustness [[109](#bib.bib109), [181](#bib.bib181), [207](#bib.bib207)], ethics
    [[48](#bib.bib48)], biases [[65](#bib.bib65)], and trustworthiness [[80](#bib.bib80)]
    are also important aspects. More specifically, the abilities of LLMs in social
    science [[51](#bib.bib51), [66](#bib.bib66), [131](#bib.bib131)], mathematics
    [[12](#bib.bib12), [53](#bib.bib53), [184](#bib.bib184), [19](#bib.bib19)], science
    [[42](#bib.bib42), [67](#bib.bib67)], engineering [[19](#bib.bib19), [122](#bib.bib122),
    [138](#bib.bib138), [160](#bib.bib160)], and medical applications [[38](#bib.bib38),
    [87](#bib.bib87)] are evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Sketching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sketching is a powerful tool that is used to accelerate the performance of
    machine learning algorithms and optimization processes. The fundamental concept
    of sketching is to partition a large input matrix into a significantly smaller
    sketching matrix but still preserve the main characteristics of the original matrix.
    Therefore, the algorithms may work with the smaller matrix instead of the huge
    original, which leads to a substantial reduction in processing time. Many previous
    works have studied sketching, proposed sketching algorithms, and supported these
    algorithms with robust theoretical guarantees. For example, the Johnson-Lindenstrauss
    lemma is proposed by [[89](#bib.bib89)]: it shows that under a certain high-dimensional
    space, projecting points to a lower-dimensional subspace may preserve the pairwise
    distances between these points. This mathematical property becomes the foundation
    of the development of faster algorithms for tasks such as nearest neighbor search.
    In addition, as explained in [[2](#bib.bib2)], the Fast Johnson-Lindenstrauss
    Transform (FJLT) introduces a specific family of structured random projections
    that can be applied to a matrix in input sparsity time.'
  prefs: []
  type: TYPE_NORMAL
- en: More recently, sketching has been applied to many numerical linear algebra tasks,
    such as linear regression [[46](#bib.bib46), [133](#bib.bib133)], dynamic kernel
    estimation [[143](#bib.bib143)], submodular maximization [[144](#bib.bib144)],
    matrix sensing [[145](#bib.bib145)], gradient-based algorithm [[195](#bib.bib195)],
    clustering [[59](#bib.bib59), [64](#bib.bib64)], convex programming [[167](#bib.bib167),
    [146](#bib.bib146), [93](#bib.bib93), [90](#bib.bib90), [117](#bib.bib117)], online
    optimization problems [[150](#bib.bib150)], training neural networks [[173](#bib.bib173),
    [197](#bib.bib197), [169](#bib.bib169), [70](#bib.bib70), [25](#bib.bib25)], reinforcement
    learning [[191](#bib.bib191), [196](#bib.bib196)], tensor decomposition [[165](#bib.bib165),
    [60](#bib.bib60)], relational database [[141](#bib.bib141)], low-rank approximation
    [[30](#bib.bib30), [128](#bib.bib128), [126](#bib.bib126), [8](#bib.bib8), [164](#bib.bib164)],
    distributed problems [[31](#bib.bib31), [190](#bib.bib190)], weighted low rank
    approximation [[152](#bib.bib152), [76](#bib.bib76), [168](#bib.bib168)], CP decomposition
    [[129](#bib.bib129)], regression inspired by softmax [[118](#bib.bib118), [74](#bib.bib74),
    [162](#bib.bib162), [55](#bib.bib55)], matrix sensing [[145](#bib.bib145)], and
    Kronecker product regression [[153](#bib.bib153)].
  prefs: []
  type: TYPE_NORMAL
- en: Second-order Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Second-order method have been used for solving many convex optimization and
    non-convex optimization problems, such as linear programming [[41](#bib.bib41),
    [26](#bib.bib26), [93](#bib.bib93), [167](#bib.bib167), [71](#bib.bib71), [83](#bib.bib83)],
    empirical risk minimization [[117](#bib.bib117), [146](#bib.bib146)], support
    vector machines [[77](#bib.bib77)], cutting plan method [[116](#bib.bib116), [90](#bib.bib90)],
    semi-definite programming [[88](#bib.bib88), [81](#bib.bib81), [71](#bib.bib71),
    [170](#bib.bib170)], hyperbolic programming/polynomials [[61](#bib.bib61), [211](#bib.bib211)],
    streaming algorithm [[119](#bib.bib119), [27](#bib.bib27), [170](#bib.bib170)],
    federated learning [[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: Convergence and Deep Neural Network Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many works focus on analyzing optimization, convergence guarantees, and training
    improvement. [[107](#bib.bib107)] shows that stochastic gradient descent optimizes
    over-parameterized neural networks on structured data, while [[63](#bib.bib63)]
    demonstrates that gradient descent optimizes over-parameterized neural networks.
    In [[16](#bib.bib16)], a convergence theory for over-parameterized deep neural
    networks via gradient descent is developed. [[17](#bib.bib17)] analyzes the convergence
    rate of training recurrent neural networks. [[3](#bib.bib3)] provides a fine-grained
    analysis of optimization and generalization for over-parameterized two-layer neural
    networks. [[4](#bib.bib4)] studies exact computation with an infinitely wide neural
    network. [[33](#bib.bib33)] proposes a Gram-Gauss-Newton method for optimizing
    over-parameterized neural networks. [[201](#bib.bib201)] improves the analysis
    of the global convergence of stochastic gradient descent when training deep neural
    networks, requiring a milder over-parameterization compared to prior research.
    Other research, such as [[135](#bib.bib135), [96](#bib.bib96), [206](#bib.bib206)],
    focuses on optimization and generalization, while [[69](#bib.bib69), [118](#bib.bib118)]
    emphasize the convergence rate and stability. Works like [[25](#bib.bib25), [173](#bib.bib173),
    [9](#bib.bib9), [127](#bib.bib127), [202](#bib.bib202)] concentrate on specialized
    optimization algorithms and techniques for training neural networks, and [[115](#bib.bib115),
    [82](#bib.bib82)] concentrate on leveraging neural network structure.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a significant body of research exploring the latent bias inherent in
    gradient descent when applied to separable classification tasks. This research
    typically employs logistic or exponentially-tailed loss functions to maximize
    margins, as demonstrated in previous studies [[97](#bib.bib97), [68](#bib.bib68),
    [101](#bib.bib101), [98](#bib.bib98), [158](#bib.bib158), [130](#bib.bib130),
    [132](#bib.bib132)]. These novel findings have also been applied to non-separable
    data through the utilization of gradient-based techniques [[86](#bib.bib86), [95](#bib.bib95),
    [94](#bib.bib94)]. Analysis of implicit bias in regression problems and associated
    loss functions is carried out using methods such as mirror descent [[198](#bib.bib198),
    [13](#bib.bib13), [14](#bib.bib14), [178](#bib.bib178), [157](#bib.bib157), [180](#bib.bib180),
    [7](#bib.bib7), [68](#bib.bib68)] and stochastic gradient descent [[85](#bib.bib85),
    [120](#bib.bib120), [114](#bib.bib114), [210](#bib.bib210), [56](#bib.bib56),
    [121](#bib.bib121), [22](#bib.bib22)]. These findings extend to the implicit bias
    of adaptive and momentum-based optimization methods [[92](#bib.bib92), [185](#bib.bib185),
    [186](#bib.bib186), [142](#bib.bib142)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Technique Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will introduce the primary technique employed in this paper.
    The notations used in this section are presented in Preliminary (Section [4](#S4
    "4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Split Hessian into blocks ($X,Y$)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the fast approximation and convergence guarantee of the training process
    for the attention matrix, the positive semi-definite property is a key focus in
    Section [6](#S6 "6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). In comparison to single/multiple softmax regression, both the weights
    $X$ (refer to Definition [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")) need to be considered. Therefore, our Hessian matrix discussed in Section [6](#S6
    "6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    has the following format'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: To establish the positive semi-definite property, we will examine the properties
    of the matrix above individually.
  prefs: []
  type: TYPE_NORMAL
- en: Positive Semi-Definite For Hessian $H_{x,x}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The positive semi-definite of the Hessian, denoted as ${H_{x,x},H_{y,y}}$,
    constitutes a crucial initial step in the proof outlined in Lemma [6.1](#S6.Thmtheorem1
    "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). These Hessian are discussed in detail in Section [9](#S9 "9 Hessian for
    𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    and Section [10](#S10 "10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Leveraging Lemma [10.1](#S10.Thmtheorem1 "Lemma 10.1\. ‣ 10.1 Hessian Property
    ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [9.1](#S9.Thmtheorem1 "Lemma 9.1\. ‣ 9.1 Main Result ‣ 9 Hessian
    for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we can establish the following results if the regularization weight sufficiently
    large (see Section [9](#S9 "9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") in details), then'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H(x)\succeq l\cdot I_{d^{2}}~{}~{}\text{and}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Spectral upper bound for $H_{x,y}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To establish the spectral upper bound of $H_{x,y}$ into $\{{G_{i}}\}_{i=1}^{4}$.
    The spectral upper bound for $H_{x,y}$.
  prefs: []
  type: TYPE_NORMAL
- en: Given this upper bound, our final focus in the proof of the positive semi-definite
    property (PSD) will be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: PSD for Hessian $H$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Hessian matrix $H$ in Section [9](#S9 "9 Hessian for 𝑋 Is PSD ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") and $H_{y,y}$ and
    $H_{y,x}$, $a_{2}$ as the bound of the matrix above respectively in Lemma [6.1](#S6.Thmtheorem1
    "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we have the following result'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Given the relationship of $\{a_{i}\}_{i=1}^{3}$ as discussed above, the positive
    semi-definite property of the Hessian matrix is established.
  prefs: []
  type: TYPE_NORMAL
- en: Lipschitz property for Hessian
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Lipschitz property of the Hessian is determined by the upper bound and
    Lipschitz property of the basic functions that constitute the Hessian matrix $H$
    has three parts $H_{x,x}$ and $H_{y,y}$ is independent of $y$, the Lipschitz property
    can be easily established. For details of others, we refer the readers to read
    Section [12](#S12 "12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the Lipschitz continuity of $H_{x,x}$, $c(x)$ in Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), which together form the matrix $H_{x,x}$
    into four distinct parts denoted as $\{G_{k}\}_{k=1}^{4}$), we want to bound'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\prod_{i=1}^{t}\beta_{i}(x)-\prod_{i=1}^{t}\beta_{i}(\widetilde{x})&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which can be upper bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where assume that $\beta_{0}(x)=1$ for convenient. We will then proceed to establish
    the Lipschitz continuity of $H_{x,x}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Forward Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To simplify the computation of the attention matrix, we can decompose the computation
    process into three components: $f$, and $h$ time, as stated in Lemma [5.3](#S5.Thmtheorem3
    "Lemma 5.3\. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can compute the gradient in Section [5](#S5 "5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
  prefs: []
  type: TYPE_TB
- en: for some matrix $p(x,y)\in\mathbb{R}^{n\times n}$ can be computed in ${\cal
    T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(d,n,d)$ time. Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$ |  |'
  prefs: []
  type: TYPE_TB
- en: which also takes ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    and $g(y(t))$ time.
  prefs: []
  type: TYPE_NORMAL
- en: Straightforward Hessian Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computing the Hessian in straightforward way would take ${\cal T}_{\mathrm{mat}}(d^{2},n^{2},d^{2})$
    where $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$. This is too
    slow, we will use sketching ideas to speed up this running time. Using sketching
    matrices to speed up the Hessian computation has been extensively studied in convex
    and non-convex optimization [[93](#bib.bib93), [117](#bib.bib117), [167](#bib.bib167),
    [71](#bib.bib71), [77](#bib.bib77), [146](#bib.bib146)].
  prefs: []
  type: TYPE_NORMAL
- en: TensorSRHT Fast Approximation for Hessian
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Building upon the aforementioned properties, we can apply the Newton Method
    in Section [14](#S14 "14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") to establish convergence for the regression problem.
    Now, let’s delve into the primary contribution of this paper. Given that $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ down to $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$
    in the paper which is the most common setting in practice because $n$ is feature
    dimension).'
  prefs: []
  type: TYPE_NORMAL
- en: Overall Time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Summary, we know that
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computing forward function ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    time (Lemma [5.3](#S5.Thmtheorem3 "Lemma 5.3\. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5
    Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computing gradient takes ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    time (Lemma [5.4](#S5.Thmtheorem4 "Lemma 5.4\. ‣ 5.4 Reformulating Gradient (𝑥)
    in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [5.5](#S5.Thmtheorem5 "Lemma 5.5\. ‣ 5.5 Reformulating Gradient
    (𝑦) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute Hessian takes $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$
    (Lemma [13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣ 13.4 Fast Approximation for Hessian
    via Sketching ‣ 13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute $g$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The total time can be expressed as $\widetilde{O}({\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon)$ is the exponent of matrix
    multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Previous works | Simplified version of Def. [1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") | How Def. [1.2](#S1.Thmtheorem2 "Definition 1.2
    (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") is simplified |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})A_{3}Y$,
    $K=A_{2}$, both $X,Y$ are not considered |'
  prefs: []
  type: TYPE_TB
- en: '| [[55](#bib.bib55)] | $(D^{-1}\exp(A_{1}XA_{2}^{\top}))_{i,*}$ are not considered
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[72](#bib.bib72), [73](#bib.bib73)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ are
    not considered |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ is not considered and
    need the symmetric assumption for matrix |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | $D^{-1}\exp(A_{2}A_{2}^{\top})$ is not considered |'
  prefs: []
  type: TYPE_TB
- en: '| [[171](#bib.bib171)] | $A_{1}XA_{2}^{\top}A_{3}$ and $\exp$ is not considered
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Here $D:=\operatorname{diag}(\exp(A_{1}XA_{2}^{\top}){\bf 1}_{n})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [4.1](#S4.SS1 "4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we present the basic mathematical
    properties of vectors, norms and matrices. In section [4.2](#S4.SS2 "4.2 General
    Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we provide a definition of $L(X,Y)$. In section [4.4](#S4.SS4 "4.4 A Helpful
    Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we define a series of helpful functions with respect
    to $Y$ and $Y$. In Section [4.6](#S4.SS6 "4.6 Regularization ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we define
    the regularization function. In Section [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we introduce facts related to fast matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: Notation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we define the basic notations we use in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: First, we define the notations related to the sets. We use $\mathbb{N}$. Let
    $n$ be in $\mathbb{N}$. We use $\mathbb{R},\mathbb{R}^{n},\mathbb{R}^{n\times
    d}$-dimensional vectors, and $n\times d$. We use $\mathbb{R}_{+}$ to denote the
    set containing all positive real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we define the notations related to vectors. Let $x,y\in\mathbb{R}^{d}$,
    we define $x_{i}\in\mathbb{R}$-th entry of $x$ as $\langle x,y\rangle:=\sum_{i=1}^{d}x_{i}y_{i}$
    and $y$ as $(x\circ y)_{i}:=x_{i}\cdot y_{i}$. For all $p\in\{1,2,\infty\}$, which
    is the $\ell_{p}$. We use ${\bf 1}_{d}$ to denote the $d$’s and $0$’s, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we define the notations related to matrices. Let $A\in\mathbb{R}^{n\times
    d}$ and $j\in[d]$ to denote the entry of $A$-th row and $j$ and $A_{*,j}\in\mathbb{R}^{n}$.
    We use $A^{\top}\in\mathbb{R}^{d\times n}$, where $A_{i,j}^{\top}=A_{j,i}$, we
    define $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$. For $x\in\mathbb{R}^{d}$
    as $\operatorname{diag}(x)_{i,i}=x_{i}$ and other entries of $\operatorname{diag}(x)$’s.
    $\|A\|_{F}\in\mathbb{R}$ denote the Frobenius norm and the spectral norm of $A\in\mathbb{R}^{n\times
    d}$ and $\|A\|:=\max_{x\in\mathbb{R}^{d}}\|Ax\|_{2}/\|x\|_{2}$. For each $j_{1}\in[n]$
    to denote one $n\times d^{2}$. Let $C,D\in\mathbb{R}^{d\times d}$ if for all $y\in\mathbb{R}^{d}$.
    $C$. We use $I_{d}$ identity matrix. $\operatorname{nnz}(A)$ that are not equal
    to zero. ${\bf 0}_{n\times n}\in\mathbb{R}^{n\times n}$, $({\bf 0}_{n\times n})_{i,j}=0$.
  prefs: []
  type: TYPE_NORMAL
- en: Let $n_{1},n_{2},d_{1},d_{2}$ and $B\in\mathbb{R}^{n_{2}\times d_{2}}$ and $B$,
    as $(A\otimes B)_{(i_{1}-1)n_{2}+i_{2},(j_{1}-1)d_{2}+j_{2}}$, where $i_{1}\in[n_{1}],j_{1}\in[d_{1}],i_{2}\in[n_{2}],j_{2}\in[d_{2}]$
    is defined by $X_{i,j}=\mathrm{mat}(x)_{i,j}:=x_{(i-1)\cdot n+j}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c1c1e971b27b856ec67768788f9b1ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The visualization of the functions $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times
    n}$. We have $x\in\mathbb{R}^{n^{2}}$. In this figure, we give an example of $n=3$,
    the first three entries of the vector $x$, $X_{1,2}$ respectively, the second
    three entries of the vector $x$, $X_{2,2}$ respectively, and the third three entries
    of the vector $x$, $X_{3,2}$ respectively. For the right figure, every entry in
    $X$ by $\operatorname{vec}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Basic Facts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will introduce the basic mathematical facts.
  prefs: []
  type: TYPE_NORMAL
- en: Fact 4.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $a,b\in\mathbb{R}$.
  prefs: []
  type: TYPE_NORMAL
- en: For all vectors $u,v,w\in\mathbb{R}^{n}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\langle u,v\rangle=\langle u\circ v,{\bf 1}_{n}\rangle=u^{\top}\mathrm{diag}(v){\bf
    1}_{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\langle u\circ v,w\rangle=\langle u\circ w,v\rangle$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\langle u\circ v,w\rangle=\langle u\circ v\circ w,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v)w$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\langle u\circ v\circ w\circ z,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v\circ
    w)z$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $u\circ v=v\circ u=\operatorname{diag}(u)\cdot v=\operatorname{diag}(v)\cdot
    u$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\operatorname{diag}(u)^{\top}=\operatorname{diag}(u)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\operatorname{diag}(u)\cdot\operatorname{diag}(v)\cdot{\bf 1}_{n}=\operatorname{diag}(u)v$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\operatorname{diag}(u\circ v)=\operatorname{diag}(u)\operatorname{diag}(v)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\operatorname{diag}(u)+\operatorname{diag}(v)=\operatorname{diag}(u+v)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\langle u,v\rangle=\langle v,u\rangle$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\langle u,v\rangle=u^{\top}v=v^{\top}u$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fact 4.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let <math id="S4.Thmtheorem2.p1.1.1.m1.1" class="ltx_Math" alttext="R></math>.
  prefs: []
  type: TYPE_NORMAL
- en: For vectors $x,y\in\mathbb{R}^{n}$ we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|x\circ y\|_{2}\leq\|x\|_{\infty}\cdot\|y\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|x\|_{\infty}\leq\|x\|_{2}\leq\sqrt{n}\|x\|_{\infty}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|\exp(x)\|_{\infty}\leq\exp(\|x\|_{2})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|x+y\|_{2}\leq\|x\|_{2}+\|y\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|\alpha x\|_{2}\leq|\alpha|\cdot\|x\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any $\|x\|_{2},\|y\|_{2}\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fact 4.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For matrices $X,Y\in\mathbb{R}^{n\times n}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|X^{\top}\|=\|X\|$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|X\|\geq\|Y\|-\|X-Y\|$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|X+Y\|\leq\|X\|+\|Y\|$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|X\cdot Y\|\leq\|X\|\cdot\|Y\|$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If $X\preceq\alpha\cdot Y$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|Yx\|_{2}\leq\|Y\|\cdot\|x\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fact 4.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any vectors $u,v\in\mathbb{R}^{n}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $uu^{\top}\preceq\|u\|_{2}^{2}\cdot I_{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2\. $\operatorname{diag}(u)\preceq\|u\|_{2}\cdot I_{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3\. $\operatorname{diag}(u\circ u)\preceq\|u\|_{2}^{2}\cdot I_{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4\. $uv^{\top}+vu^{\top}\preceq uu^{\top}+vv^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5\. $uv^{\top}+vu^{\top}\succeq-(uu^{\top}+vv^{\top})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 6\. $(v\circ u)(v\circ u)^{\top}\preceq\|v\|^{2}_{\infty}uu^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 7\. $\operatorname{diag}(u\circ v)\preceq\|u\|_{2}\|v\|_{2}\cdot I_{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fact 4.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $g,f:\mathbb{R}^{d}\to\mathbb{R}^{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Let $x\in\mathbb{R}^{d}$ be an arbitrary vector.
  prefs: []
  type: TYPE_NORMAL
- en: Let $a\in\mathbb{R}$ be an arbitrary real number.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\frac{\mathrm{d}q(x)^{a}}{\mathrm{d}x}=a\cdot q(x)^{a-1}\cdot\frac{\mathrm{d}q(x)}{\mathrm{d}x}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\frac{\mathrm{d}\|f(x)\|^{2}_{2}}{\mathrm{d}t}=2\langle f(x),\frac{\mathrm{d}f(x)}{\mathrm{d}t}\rangle$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2 (product rule for Hadamard product)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 General Definitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce some general definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.6  (Index summary).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We use $i$ range, and $j$ range.
  prefs: []
  type: TYPE_NORMAL
- en: We use $i_{0},i_{1},i_{2}$, and $j_{0},j_{1},j_{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.7.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $A_{1}\in\mathbb{R}^{n\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $A_{2}\in\mathbb{R}^{n\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For each $j_{0}\in[n]$ to be one $n\times d^{2}$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $A_{3}\in\mathbb{R}^{n\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $B\in\mathbb{R}^{n\times d}$ denote the $(j_{0},i_{0})$ for each $j_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $X\in\mathbb{R}^{d\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our final goal is to study the loss function, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we define $D(X)\in\mathbb{R}^{n\times n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each $j_{0}\in[n]$ to be $\langle\exp(\operatorname{\mathsf{A}}_{j_{0}}x),{\bf
    1}_{n}\rangle$ is the $j_{0}$ and $x\in\mathbb{R}^{d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Further, for each $j_{0}\in[n],i_{0}\in[d]$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Using tensor-trick in [[72](#bib.bib72), [73](#bib.bib73)], we can see that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(X,Y)=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}L(X,Y)_{j_{0},i_{0}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Helpful Definitions With Respect to $X$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, we introduce a few helpful definitions related to $X\in\mathbb{R}^{d\times
    d}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.8.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$, and $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$
    block from $\operatorname{\mathsf{A}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define $u(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle u(x)_{j_{0}}:=\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Definition 4.9.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$, and $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$
    block from $\operatorname{\mathsf{A}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define $\alpha(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\alpha(x)_{j_{0}}:=\langle\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1},\underbrace{{\bf 1}_{n}}_{n\times 1}\rangle.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Definition 4.10.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as in Definition [4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as in Definition [4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: We define $f(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(x)_{j_{0}}:=\underbrace{\alpha(x)_{j_{0}}^{-1}}_{\mathrm{scalar}}\underbrace{u(x)_{j_{0}}}_{n\times
    1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.4 A Helpful Definition With Respect to $Y$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce a helpful definition related to $Y\in\mathbb{R}^{d\times
    d}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.11.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For each $i_{0}\in[d]$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle h(Y)_{i_{0}}:=\underbrace{A_{3}}_{n\times d}\underbrace{Y_{*,i_{0}}}_{d\times
    1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Helpful Definitions With Respect to Both $X$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce some helpful definitions related to both $X\in\mathbb{R}^{d\times
    d}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.12.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We define $c(x,y)_{j_{0},i_{0}}:\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c(x,y)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Furthermore, we define $c(x,:)_{j_{0},i_{0}}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: for some fixed vector $v\in\mathbb{R}^{n}$ and also doesn’t depend on $y$.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we also define $c(:,y)_{j_{0},i_{0}}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c(:,y)_{j_{0},i_{0}}:=\langle v,h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: for some fixed vector $v\in\mathbb{R}^{n}$ and also doesn’t depend on $y$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.13.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}:=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(:,y)_{j_{0},i_{0}}:=0.5c(:,y)_{j_{0},i_{0}}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(x,y)_{j_{0},i_{0}}:=0.5c(x,y)_{j_{0},i_{0}}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.6 Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we define the regularization loss we use.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.14.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $W\in\mathbb{R}^{n\times n}$ denote a positive diagonal matrix. We use the
    following regularization loss
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;(W\otimes I)(A_{1}\otimes A_{2})x\&#124;_{2}^{2}+\&#124;WA_{3}y\&#124;_{F}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that $1$2.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Fast Matrix Multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use ${\cal T}_{\mathrm{mat}}(a,b,c)$ matrix with another $b\times c$ matrix.
    Fast matrix multiplication [[44](#bib.bib44), [182](#bib.bib182), [105](#bib.bib105),
    [78](#bib.bib78), [34](#bib.bib34), [15](#bib.bib15), [62](#bib.bib62), [106](#bib.bib106),
    [189](#bib.bib189)] is a fundamental tool in theoretical computer science.
  prefs: []
  type: TYPE_NORMAL
- en: Fact 4.15.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ${\cal T}_{\mathrm{mat}}(a,b,c)=O({\cal T}_{\mathrm{mat}}(b,a,c))=O({\cal T}_{\mathrm{mat}}(a,c,b))$.
  prefs: []
  type: TYPE_NORMAL
- en: For $k\in\mathbb{R}_{+}$ to be the value such that $\forall n\in\mathbb{N}$.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, we define three special values of $\omega(k)$ to be the fast
    matrix multiplication exponent, i.e., $\omega:=\omega(1)$ to be the dual exponent
    of matrix multiplication, i.e., $\omega(\alpha)=2$.
  prefs: []
  type: TYPE_NORMAL
- en: The following fact can be found in Lemma 3.6 of [[88](#bib.bib88)], also see
    [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: Fact 4.16  (Convexity of $\omega(k)$).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The function $\omega(k)$ is convex.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [5.1](#S5.SS1 "5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we show the gradient with respect
    to variables $x$. In Section [5.3](#S5.SS3 "5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    running time of $c,f,h$ to compute time complexity. In Section [5.5](#S5.SS5 "5.5
    Reformulating Gradient (𝑦) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), we reformulate the gradient with respect
    to $Y$ to compute time complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Gradient for $x$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we compute the gradient for $x$. Most of the following gradient
    computations can be found in [[72](#bib.bib72), [73](#bib.bib73)].
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 5.1  (Gradient with respect to $x$).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each $i\in[d^{2}]$ denote the $i$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $L(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.13](#S4.Thmtheorem13
    "Definition 4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, for each $i\in[d^{2}]$, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}u(x)_{j_{0}}}{\mathrm{d}x_{i}}=u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\alpha(x)_{j_{0}}}{\mathrm{d}x_{i}}=\langle
    u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{i_{0},i},{\bf 1}_{n}\rangle$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4. For a fixed vector $v\in\mathbb{R}^{n}$), we have
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '![Refer to caption](img/1863170410eedf795b70b046b0768e9c.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6: The visualization of Part 4 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $v$. For the right-hand side, we have three steps. Step 1: we compute the
    Hadamard product of $f(x)_{j_{0}}$. Step 2: We find the inner product of this
    Hadamard product and $v$ and $v$ and $\operatorname{\mathsf{A}}_{j_{0},i}$. The
    red rectangles represent the vector $v$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5. For each $i_{0}\in[d]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 6.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 7. (for hessian diagonal term)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '![Refer to caption](img/506215cafa5031425b4de08e4ebc4f1b.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: The visualization of Part 7 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $\operatorname{\mathsf{A}}_{j_{0},i}$ with respect to $x_{i}\in\mathbb{R}$
    and $v$ and $v$ and $\operatorname{\mathsf{A}}_{j_{0},i}$. The red rectangles
    represent the vector $v$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 8. (for hessian off-diagonal term)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 9 (for hessian diagonal term, this can be obtained by using Part 4 as a
    black-box)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '![Refer to caption](img/7789ea533c8b22818e08c27ec76bbfba.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8: The visualization of Part 9 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $\operatorname{\mathsf{A}}_{j_{0},i}$. For the right-hand side, we have three
    steps. Step 1: we compute the Hadamard product of $\operatorname{\mathsf{A}}_{j_{0},i}$.
    Step 2: We find the inner product of $f(x)_{j_{0}}$ and $\operatorname{\mathsf{A}}_{j_{0},i}$.
    The green rectangles represent the vector $\operatorname{\mathsf{A}}_{j_{0},i}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 10 (for hessian off-diagonal term, this can be obtained by using Part 4
    as a black-box)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1. See Part 4 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (Page
    14).
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2. See Part 5 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (Page
    14).
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3. See Part 9 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (page
    15).
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4. See Part 14 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (page
    15).
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that by Definition [4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful
    Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}c(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from Eq. ([3](#S5.E3 "In Proof. ‣ 5.1 Gradient for
    𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the second step follows from $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=0$,
    and the third step is due to Part 4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof of Part 6. Noted that by Definition [4.13](#S4.Thmtheorem13 "Definition
    4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Eq. ([4](#S5.E4 "In Proof. ‣ 5.1 Gradient for
    𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the second step is because of chain rule of derivative, the last step
    comes from Part 5.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 7.
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{i}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the third step is because of Part 4, the fourth step is owing to simple
    algebra, the fifth step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step comes from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 8.
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{l}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is because of Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the third step follows from Part 4, the fourth step is
    due to simple algebra, the fifth step is owing to Fact [4.1](#S4.Thmtheorem1 "Fact
    4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the last step comes from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 9.
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{i}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step comes from Part 4, and the last step is because of Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 10. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{l}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is owing to Part 4, and the last step is due to Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Gradient With Respect to $y$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we compute the gradient with respect to $y$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 5.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $v\in\mathbb{R}^{n}$ which doesn’t depend on x and also doesn’t depend on
    y.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $L(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.13](#S4.Thmtheorem13
    "Definition 4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $h(y_{i_{0}}):=\underbrace{A_{3}}_{n\times d}\underbrace{y_{i_{0}}}_{d\times
    1}.$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $h(y_{i_{0}})=h(y)_{i_{0}}$ for convenient
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $A_{3,*,i_{2}}\in\mathbb{R}^{n}$-th column of matrix $A_{3}\in\mathbb{R}^{n\times
    d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1. If $i_{1}=i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=A_{3,*,i_{2}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2. If $i_{1}\neq i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}={\bf
    0}_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3. If $i_{1}=i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4. If $i_{1}\neq i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5. If $i_{1}=i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 6. If $i_{1}\neq i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 7. If $i_{1}=i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 8. If $i_{1}\neq i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step is due to the definition of $h(y_{i_{0}})$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step is due to $i_{1}\neq i_{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is due to the result of Part 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is becaues of Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step comes from the result of Part 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 5.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\ =$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from the Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is because of $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$,
    and the last step is due to Part 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 6.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\ =$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to the Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step comes from $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$,
    and the last step is owing to Part 4.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 7.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to the Definition [4.13](#S4.Thmtheorem13 "Definition
    4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step comes from the chain rule of derivative, and the last step is owing to Part
    5.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 8.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of the Definition [4.13](#S4.Thmtheorem13 "Definition
    4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is due to the chain rule of derivative, and the last step comes from Part
    6. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Computation of $c,f,h$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we explain how to compute $c(x,y),f(x),h(y)$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 5.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each $j_{0}\in[n]$, let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ as an $n\times
    d$ matrix)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each $j_{0}\in[n]$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). (We can view $f(x)$ matrix)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each $i_{0}\in[d]$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). (We can view $h(y)$ matrix)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $A_{3}\in\mathbb{R}^{n\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can view $y$ matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we can compute $f,h,c$ time.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By definition [4.11](#S4.Thmtheorem11 "Definition 4.11\. ‣ 4.4 A Helpful Definition
    With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underbrace{h(y)}_{n\times d}=\underbrace{A_{3}}_{n\times
    d}\underbrace{y}_{d\times d}.$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: First $h(y)\in\mathbb{R}^{n\times d}$ matrix ($A_{3}$ matrix ($y$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d3e4154621daaadac8b25aad9a865c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The visualization of Eq. ([5](#S5.E5 "In Proof. ‣ 5.3 Computation
    of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). We have $A_{3}\in\mathbb{R}^{n\times d}$ is a function, which maps the
    matrix $y\in\mathbb{R}^{d\times d}$ by multiplying $A_{3}$. The red rectangles
    represent matrices which are the factors, and the blue rectangle represents the
    matrix which is the product.'
  prefs: []
  type: TYPE_NORMAL
- en: We also have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Then the computation of $f(x)\in\mathbb{R}^{n\times n}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c05b86816f745fcf485d105c5f191095.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The visualization of Eq. ([6](#S5.E6 "In Proof. ‣ 5.3 Computation
    of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). We have $A_{1},A_{2}\in\mathbb{R}^{n\times d}$, and $D(X)\in\mathbb{R}^{n\times
    n}$ and compute $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times n}$ and $\exp(A_{1}XA_{2}^{\top})$.
    The green squares represent the square matrices in $\mathbb{R}^{n\times n}$ (the
    dark blue denotes the transpose of the matrix in $\mathbb{R}^{n\times d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underbrace{c(x,y)}_{n\times d}=\underbrace{f(x)}_{n\times
    n}\underbrace{h(y)}_{n\times d}-\underbrace{B}_{n\times d}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Then $c$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1858c5740c54c01100bd045fbd858479.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The visualization of Eq. ([7](#S5.E7 "In Proof. ‣ 5.3 Computation
    of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). Let $f(x)\in\mathbb{R}^{n\times n}$ (see Figure [9](#S5.F9 "Figure 9
    ‣ Proof. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). We have $B\in\mathbb{R}^{n\times d}$ with $h(y)$
    from their product to get $c(x,y)\in\mathbb{R}^{n\times d}$. The blue rectangles
    represent the matrix in $\mathbb{R}^{n\times d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Reformulating Gradient ($x$) in Matrix View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we reformulate the gradient $x$ in the matrix’s view.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 5.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $c(x,y)\in\mathbb{R}^{n\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2. Suppose $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ can be computed
    in $O(nd^{2})$ time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4. Suppose $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ can be computed
    in ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ time
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that by Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, we complete the proof.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: We first compute $(\operatorname{diag}(f(x)_{j_{0}})-f(x)_{j_{0}}f(x)_{j_{0}}^{\top})h(y)_{i_{0}}$
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can compute the rest, it takes $O(nd^{2})$ time.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3 and Part 4.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we can compute $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from the Lemma statement, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}.$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Let $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$-th column of $q(x,y)$.
  prefs: []
  type: TYPE_NORMAL
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(x,y)=\underbrace{h(y)}_{n\times d}\underbrace{c(x,y)^{\top}}_{d\times
    n}$ |  |'
  prefs: []
  type: TYPE_TB
- en: This takes ${\cal T}_{\mathrm{mat}}(n,d,n)$ time.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we compute
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: This takes $O(n^{2})$ time in total.
  prefs: []
  type: TYPE_NORMAL
- en: We can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on Definition [4.7](#S4.Thmtheorem7 "Definition
    4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the second step is because of Part 1, the third
    step is due to Eq. ([8](#S5.E8 "In Proof. ‣ 5.4 Reformulating Gradient (𝑥) in
    Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the fourth step follows from Eq. ([9](#S5.E9 "In Proof. ‣ 5.4 Reformulating
    Gradient (𝑥) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")), and the last step due to tensor-trick.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that $A_{1}^{\top}p(x,y)A_{2}$ time. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Reformulating Gradient ($y$) in Matrix View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we reformulate the gradient $y$ in the matrix’s view.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 5.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if $i_{1}=i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if $i_{1}\neq i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\widetilde{q}(x,y)_{i_{0}}=\sum_{j_{0}=1}^{n}f(x)_{j_{0}}c(x,y)_{j_{0},i_{0}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=A_{3,*,i_{2}}^{\top}\widetilde{q}(x,y)_{i_{0}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(\underbrace{A_{3}^{\top}}_{d\times
    n}\underbrace{\widetilde{q}(x,y)}_{n\times d})$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4. Computing $\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from the assumption from the Lemma statement and
    the second step is based on Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to the assumption from the Lemma statement, the
    second step is because of Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the last step comes from the definition of $\widetilde{q}(x,y)_{i_{0}}$
    (see from the Lemma statement).'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(A_{3}^{\top}\widetilde{q}(x,y))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step comes from tensor trick based on Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4. Computing $\widetilde{q}(x,y)\in\mathbb{R}^{n\times d}$ time.
  prefs: []
  type: TYPE_NORMAL
- en: Computing $A_{3}^{\top}\widetilde{q}(x,y)$ time.
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 6 Hessian
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide more details related to Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: Finally the hessian $H\in\mathbb{R}^{2d^{2}\times 2d^{2}}$ which can be written
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$H_{x,x}\in\mathbb{R}^{d^{2}\times d^{2}}$ (see details in Section [7](#S7
    "7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$H_{x,y}$ is $\frac{\mathrm{d}^{2}L}{\mathrm{d}x\mathrm{d}y}$ (see details
    in Section [11](#S11 "11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$H_{y,y}\in\mathbb{R}^{d^{2}\times d^{2}}$ (see details in Section [10](#S10
    "10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can view $$H_{y,y}=\begin{bmatrix}H_{y,y,1,1}&amp;0&amp;0&amp;\cdots&amp;0\\
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 0&amp;H_{y,y,2,2}&amp;0&amp;\cdots&amp;0\\
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 0&amp;0&amp;H_{y,y,3,3}&amp;\cdots&amp;0\\
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 0&amp;0&amp;0&amp;\cdots&amp;H_{y,y,d,d}\end{bmatrix}$$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: where $H_{y,y,i_{0},i_{0}}=\sum_{j_{0}=1}^{n}\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},*}\mathrm{d}y_{i_{0},*}}\in\mathbb{R}^{d\times
    d}$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Lemma 6.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $H_{x,x}\succeq\alpha_{1}I_{d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $H_{y,y}\succeq\alpha_{2}I_{d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|H_{x,y}\|\leq\alpha_{3}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|H_{y,x}\|\leq\alpha_{3}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let <math id="S6.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="\alpha_{1}\geq\alpha_{3}></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $u,v\in\mathbb{R}^{d^{2}}$, then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{bmatrix}u^{\top}&amp;v^{\top}\end{bmatrix}H\begin{bmatrix}u\\
    v\end{bmatrix}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on the expansion of $H$, the third step comes
    from Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and Fact [4.3](#S4.Thmtheorem3
    "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") , the fourth step is because of $\|H_{x,y}\|\leq\alpha_{3},\|H_{y,x}\|\leq\alpha_{3}$,
    and the last step is based on the simple algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it implies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Our Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:procedure TrainingAlgorithm($A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$
    Theorem [1.3](#S1.Thmtheorem3 "Theorem 1.3 (Informal version of our main theorem).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")2:     Let $x(0),y(0)\in\mathbb{R}^{d^{2}}$ do4:          /*Forward*/5:         Compute
    $h(y(t))\in\mathbb{R}^{n\times d}$ ${\cal T}_{\mathrm{mat}}(n,d,d)$ $\triangleright$
    time7:         Compute $c(x(t),y(t))\in\mathbb{R}^{n\times d}$, $h(y(t))$ ${\cal
    T}_{\mathrm{mat}}(n,d,d)$ based on Lemma [5.4](#S5.Thmtheorem4 "Lemma 5.4\. ‣
    5.4 Reformulating Gradient (𝑥) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") $\triangleright$ time10:         Compute
    $g(y(t))$ ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ via
    TensorSRHT $\triangleright$13:          /*Update*/14:         $$\begin{bmatrix}x(t+1)\\'
  prefs: []
  type: TYPE_NORMAL
- en: y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\
  prefs: []
  type: TYPE_NORMAL
- en: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
  prefs: []
  type: TYPE_NORMAL
- en: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ $\triangleright$15:     end for16:end procedure
  prefs: []
  type: TYPE_NORMAL
- en: 7 Hessian for $X$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [7.1](#S7.SS1 "7.1 Hessian ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we compute the Hessian matrix
    with respect to $x$, representing the Hessian.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Hessian
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, we start to compute the Hessian matrix with respect to $x$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 7.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}:=\langle f(x)_{j_{0}},v\rangle$ (We define this notation
    for easy of writing proofs.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we have for each $i\in[d^{2}]$
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $i=l$ Hessian diagonal term
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{i}}=$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}($ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle(1-\gamma_{j_{0}}(x))$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\gamma_{j_{0}}(x)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{})$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2\. $i\neq l$ Hessian off-diagonal term
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{l}}=$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}($ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle(1-\langle
    f(x)_{j_{0}},v\rangle))$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{})$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: At first, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on the product rule of derivative, the second
    step comes from Part 4, Part 7, and Part 9 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step is due to simple
    algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step comes from Part 6 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and the second step is due to Part
    5 of Lemma [5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to 𝑥). ‣ 5.1
    Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Combining the above two equations, we complete the proof.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is owing to the product rule of derivative, the second
    step is based on Part 4, Part 8, and Part 10 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step comes from simple
    algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{l}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Combining the above two equations, we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 A Helpful Lemma
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present a helpful Lemma.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 7.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ v,\operatorname{\mathsf{A}}_{j_{0},l}\rangle\cdot\operatorname{\mathsf{A}}_{j_{0},i}^{\top}\cdot
    f(x)_{j_{0}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+\operatorname{\mathsf{A}}_{j_{0},i}^{\top}f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}\operatorname{\mathsf{A}}_{j_{0},l}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top})\operatorname{\mathsf{A}}_{j_{0},l}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the last step follows from the simple algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},l}\rangle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Defining $B(x)$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we formally define $B(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 7.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma_{j_{0}}(x)=\langle f(x)_{j_{0}},v\rangle$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We define $B(x)\in\mathbb{R}^{n\times n}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle B(x):=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle~{}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{3}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lemma 7.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $B(x)$ be defined as Definition [7.3](#S7.Thmtheorem3 "Definition 7.3\.
    ‣ 7.3 Defining 𝐵⁢(𝑥) ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), then we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The proof follows by combining Lemma [7.1](#S7.Thmtheorem1 "Lemma 7.1\. ‣ 7.1
    Hessian ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [7.2](#S7.Thmtheorem2 "Lemma 7.2\. ‣ 7.2 A Helpful Lemma ‣ 7
    Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Lipschitz Property of $H_{x,x}$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [8.1](#S8.SS1 "8.1 Main Result ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we present
    the main results of the Lipschitz property of $H_{x,x}$. In Section [8.6](#S8.SS6
    "8.6 Calculation: Step 2 Lipschitz for Matrix Function -𝛾_𝑗₀⁢(𝑥)⋅𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣)
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the second step of Lipschitz function $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$. In Section [8.8](#S8.SS8
    "8.8 Calculation: Step 4 Lipschitz for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the fourth step of Lipschitz function $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$. In Section [8.10](#S8.SS10 "8.10 Calculation: Step 6 Lipschitz
    for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we analyze the sixth step of Lipschitz function $-c(x,:)_{j_{0},i_{0}})\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}$. In Section [8.12](#S8.SS12 "8.12 Calculation: Step 8 Lipschitz for
    Matrix Function 𝛾_𝑗₀⁢(𝑥)²⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we analyze
    the eighth step of Lipschitz function $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Main Result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present the main result of the Lipschitz property.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $M:=\exp(O(R^{2}+\log(nd)))$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have for all $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. For each $j_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq
    M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from definition of $H_{j_{0},i_{0}}(x)$, the second
    step follows from Lemma [8.2](#S8.Thmtheorem2 "Lemma 8.2\. ‣ 8.2 Summary of Nine
    Steps ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and last step follows from simple algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step follows from triangle inequality and $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$,
    and the second step follows from Part 1. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Summary of Nine Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide a summary of the nine-step calculation of Lipschitz
    for different matrix functions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{5}(x)=-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{6}(x)=-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{8}(x)=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{k\in[9]}\&#124;G_{k}(x)-G_{k}(\widetilde{x})\&#124;\leq
    n^{1.5}\exp(20R^{2}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The proof follows from Lemma [8.7](#S8.Thmtheorem7 "Lemma 8.7\. ‣ 8.5 Calculation:
    Step 1 Lipschitz for Matrix Function 𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣) ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [8.8](#S8.Thmtheorem8 "Lemma 8.8\. ‣ 8.6 Calculation: Step 2 Lipschitz
    for Matrix Function -𝛾_𝑗₀⁢(𝑥)⋅𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣) ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.9](#S8.Thmtheorem9 "Lemma 8.9\. ‣ 8.7 Calculation: Step 3 Lipschitz for
    Matrix Function -2⁢𝛾_𝑗₀⁢(𝑥)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.10](#S8.Thmtheorem10 "Lemma 8.10\. ‣ 8.8 Calculation: Step 4 Lipschitz
    for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.11](#S8.Thmtheorem11 "Lemma 8.11\. ‣ 8.9 Calculation: Step 5 Lipschitz
    for Matrix Function -2⁢𝛾_𝑗₀⁢(𝑥)⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.12](#S8.Thmtheorem12 "Lemma 8.12\. ‣ 8.10 Calculation: Step 6 Lipschitz
    for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.13](#S8.Thmtheorem13 "Lemma 8.13\. ‣ 8.11 Calculation: Step 7 Lipschitz
    for Matrix Function 2⁢𝛾_𝑗₀⁢(𝑥)⁢𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [8.14](#S8.Thmtheorem14 "Lemma 8.14\. ‣ 8.12 Calculation: Step 8
    Lipschitz for Matrix Function 𝛾_𝑗₀⁢(𝑥)²⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and Lemma [8.15](#S8.Thmtheorem15 "Lemma 8.15\. ‣ 8.13 Calculation: Step 9 Lipschitz
    for Matrix Function (𝑓⁢(𝑥)_𝑗₀∘𝑣)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '8.3 A Core Tool: Upper Bound for Several Basic Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the upper bound of several basic functions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.3  ([[55](#bib.bib55), [72](#bib.bib72)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Provided that the subsequent requirements are satisfied
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $x\in\mathbb{R}^{d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We define $u(x)$ as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3
    Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\beta$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\beta\geq\exp(-R^{2}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Lemma 8.4  (Basic Functions Upper Bound).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold,
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\beta$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|x\|_{2}\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $|b_{j_{0},i_{0}}|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then we have: for all $x\in\mathbb{R}^{d^{2}}$'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $\|u(x)_{j_{0}}\|_{2}\leq\sqrt{n}\cdot\exp(R^{2})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2\. $|\alpha(x)_{j_{0}}|\leq n\exp(R^{2})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3\. $|\alpha(x)_{j_{0}}|^{-1}\leq\exp(R^{2})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4\. $\|f(x)_{j_{0}}\|_{2}\leq 1$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5\. $|\gamma(x)_{j_{0}}|\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 6\. $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We present our proof as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 1. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;u(x)_{j_{0}}\&#124;_{2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Definition [4.8](#S4.Thmtheorem8 "Definition
    4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on Fact [4.2](#S4.Thmtheorem2
    "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the third step follows from Fact [4.2](#S4.Thmtheorem2
    "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the fourth step is because of $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$ (see from the Lemma statement).'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\alpha(x)_{j_{0}}&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Definition [4.9](#S4.Thmtheorem9 "Definition
    4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second is based on Fact [4.2](#S4.Thmtheorem2
    "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the third step follows from Part 1. and the forth
    step follows from simple algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\alpha^{-1}(x)_{j_{0}}&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of Definition [4.9](#S4.Thmtheorem9 "Definition
    4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step follows from the
    definition of $\beta$ and the third step is due to Lemma [8.3](#S8.Thmtheorem3
    "Lemma 8.3 ([55, 72]). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;f(x)_{j_{0}}\&#124;_{2}\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step is due to Definition [4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 5. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\gamma(x)_{j_{0}}&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step follows from the definition of $\gamma(x)_{j_{0}}$ norm
    of $v$ (from the Lemma statement), and the last step follows from simple algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 6. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is because of the definition of $\gamma_{j_{0}}(x)$ (see from the Lemma statement),
    and the last step follows from $R\geq 1$. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '8.4 A Core Tool: Lipschitz Property for Several Basic Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the Lipschitz property of several basic functions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.5  (Basic Functions Lipschitz Property).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold,
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|v\|\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\beta$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\beta^{-1}\leq\exp(R^{2})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we have: for all $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2\. $|\alpha(x)^{-1}-\alpha^{-1}(\widetilde{x})|\leq n\exp(4R^{2})\cdot\|x-\widetilde{x}\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3\. $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4\. $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5\. $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;u(x)_{j_{0}}-u(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Definition [4.8](#S4.Thmtheorem8 "Definition
    4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is because of
    Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), the third step
    is based on Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the fourth
    step follows from Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    , fifth step is due to $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2 We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\alpha(x)^{-1}_{j_{0}}-\alpha(\widetilde{x})^{-1}_{j_{0}}&#124;\leq$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to simple algebra, the second step is due to $\beta\geq\langle
    u(x)_{j_{0}},{\bf 1}_{n}\rangle$ (see Definition [4.9](#S4.Thmtheorem9 "Definition
    4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")), the fourth step is based on Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the fifth step is because of Part 1, and the sixth step follows from <math
    id="S8.SS4.3.p3.3.m3.1" class="ltx_Math" alttext="R></math>.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;f(x)_{j_{0}}-f(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Definition [4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on triangle
    inequality, the third step follows from Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\.
    ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the fourth follows from combination of Part 1, Part 2 and
    Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A
    Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\gamma_{j_{0}}(x)-\gamma_{j_{0}}(\widetilde{x})&#124;=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step is based on the definition of $\gamma_{j_{0}}(x)$ and $R\geq
    4$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 5. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}-c(\widetilde{x},:)_{j_{0},i_{0}}&#124;=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is based on the definition of $\gamma_{j_{0}}(x)$ and the last step follows
    from Part 4. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: For convenient, we define
  prefs: []
  type: TYPE_NORMAL
- en: Definition 8.6.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define $R_{0}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R_{0}:=n^{1.5}\exp(10R^{2}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: '8.5 Calculation: Step 1 Lipschitz for Matrix Function $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.7.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{1}(x)-G_{1}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{1,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{1,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on definition $G_{1,1}$, the second step is due
    to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of definition of $G_{1,2}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Combining the above two equations, we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: '8.6 Calculation: Step 2 Lipschitz for Matrix Function $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.8.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2}(x)-G_{2}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{2,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{2,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{2,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of definition of $G_{2,1}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of definition of $G_{2,2}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2,3}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of definition of $G_{2,3}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Combining all the above equations finish the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: '8.7 Calculation: Step 3 Lipschitz for Matrix Function $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.9.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{3}(x)-G_{3}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{3,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{3,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{3,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{3,1}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{3,1}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and the second step is due to Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic
    Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") and Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq 2R_{0}\cdot R^{4}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq 2R_{0}\cdot R^{4}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '8.8 Calculation: Step 4 Lipschitz for Matrix Function $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.10.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{4}(x)-G_{4}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{4,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{4,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{4,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{4,1}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{4,1}\&#124;\leq R^{2}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{4,2}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{4,3}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '8.9 Calculation: Step 5 Lipschitz for Matrix Function $-2\gamma_{j_{0}}(x)\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $-2\gamma_{j_{0}}(x)\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.11.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This proof is similar to the proof of Lemma [8.9](#S8.Thmtheorem9 "Lemma 8.9\.
    ‣ 8.7 Calculation: Step 3 Lipschitz for Matrix Function -2⁢𝛾_𝑗₀⁢(𝑥)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), so we omit it here. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '8.10 Calculation: Step 6 Lipschitz for Matrix Function $-c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $-c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.12.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This proof is similar to the proof of Lemma [8.10](#S8.Thmtheorem10 "Lemma
    8.10\. ‣ 8.8 Calculation: Step 4 Lipschitz for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), so we omit it here. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '8.11 Calculation: Step 7 Lipschitz for Matrix Function $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.13.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{7}(x)-G_{7}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{7,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{7,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{7,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{7,4}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{7,1}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{7,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to the definition of $G_{7,1}$, the second step
    is because of Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step is based on Part 4 of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") and Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the last step comes from Part 4 and Part 6 of Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for $G_{7,2}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{7,2}\&#124;\leq 2R_{0}\cdot R^{2}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{7,3}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{7,3}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{7,4}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{7,4}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '8.12 Calculation: Step 8 Lipschitz for Matrix Function $\gamma_{j_{0}}(x)^{2}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $\gamma_{j_{0}}(x)^{2}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.14.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $G_{8,1}=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{8}(x)-G_{8}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{8,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{8,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{8,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{8,4}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: We can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{i\in[4]}\&#124;G_{8,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '8.13 Calculation: Step 9 Lipschitz for Matrix Function $(f(x)_{j_{0}}\circ
    v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our calculation of Lipschitz for $(f(x)_{j_{0}}\circ
    v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.15.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{9}(x)-G_{9}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{9,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{9,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: We can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{i\in[2]}\&#124;G_{9,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 9 Hessian for $X$ Is PSD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [9.1](#S9.SS1 "9.1 Main Result ‣ 9 Hessian for 𝑋 Is PSD ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we present the
    main result of PSD bound for Hessian. In Section [9.2](#S9.SS2 "9.2 PSD Bound
    ‣ 9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we show the PSD bound for $B(x)$. Throughout this section, we will use
    the symbol $H$ for the sake of simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Main Result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce the main result of the PSD bound for Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 9.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $j_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $i_{0}\in[d]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H_{j_{0},i_{0}}=\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}x}\in\mathbb{R}^{d^{2}\times
    d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $B_{j_{0},i_{0}}(x)\in\mathbb{R}^{n\times n}$ be defined as Definition[7.3](#S7.Thmtheorem3
    "Definition 7.3\. ‣ 7.3 Defining 𝐵⁢(𝑥) ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, $1$2
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\sigma_{\min}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H_{\operatorname{reg},j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}(B_{j_{0},i_{0}}(x)+W^{2})\operatorname{\mathsf{A}}_{j_{0}}$
    is a positive diagonal matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H_{\operatorname{reg}}=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{\operatorname{reg},j_{0},i_{0}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $C_{0}:=30R^{8}$ (be a local parameter in this lemma)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let <math id="S9.I1.i11.p1.1.m1.1" class="ltx_Math" alttext="l></math> (denote
    the strongly convex parameter for hessian)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1. For each $j_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle-C_{0}I_{n}\preceq B_{j_{0},i_{0}}(x)\preceq C_{0}I_{n}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2. For each $j_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)\&#124;\leq C_{0}R^{2}.$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+C_{0}$,
    then we have
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H_{\operatorname{reg},j_{0},i_{0}}(x)\succeq l\cdot I_{d^{2}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+100\cdot
    C_{0}$, then we have
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1.1\cdot(B(x)_{j_{0},i_{0}}+W^{2})\succeq W^{2}\succeq
    0.9\cdot(B(x)_{j_{0},i_{0}}+W^{2})$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1.1H_{j_{0},i_{0}}\succeq H_{\operatorname{reg},j_{0},i_{0}}\succeq
    0.9H_{j_{0},i_{0}}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+C_{0}$,
    then we have
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H_{\operatorname{reg}}(x)\succeq l\cdot I_{d^{2}}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 6. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+100\cdot
    C_{0}$, then we have
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1.1H\succeq H_{\operatorname{reg}}\succeq 0.9H$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'It directly follows from Lemma [9.2](#S9.Thmtheorem2 "Lemma 9.2\. ‣ 9.2 PSD
    Bound ‣ 9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step follows from the $H_{j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B_{j_{0},i_{0}}(x)\operatorname{\mathsf{A}}_{j_{0}}$,
    and the last step follow from Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: The proof is similar to [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4.
  prefs: []
  type: TYPE_NORMAL
- en: The proof is similar to [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 5 and Part 6. It is because we can write $H$ terms $H_{j_{0},i_{0}}$,
    $i_{0}\in[d]$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 PSD Bound
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the PSD bound for each of the $B_{\operatorname{rank}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 9.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following condition holds
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $|\gamma(x)_{j_{0}}|\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle-8R^{6}\cdot I_{n}\preceq B_{\operatorname{diag}}^{1}\preceq
    8R^{6}\cdot I_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle-16R^{8}\cdot I_{n}\preceq B_{\operatorname{rank}}^{1}\preceq
    16R^{8}\cdot I_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle-8R^{4}\cdot I_{n}\preceq B_{\operatorname{rank}}^{2}\preceq
    8R^{4}\cdot I_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 0\cdot I_{n}\preceq B_{\operatorname{rank}}^{3}\preceq
    8R^{4}\cdot I_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle B_{\operatorname{diag}}^{1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step follows from the definition of $B_{\operatorname{diag}}^{1}$,
    and $\|v\|_{2}\leq R^{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\succeq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\succeq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\succeq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\succeq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from the definition of $B_{\operatorname{rank}}^{1}$
    and Fact [4.4](#S4.Thmtheorem4 "Fact 4.4\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the fourth
    step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and last step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and $\|v\|_{2}\leq R^{2}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from definition of $B_{\operatorname{rank}}^{2}$
    and Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3
    A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\preceq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from definition of $B_{\operatorname{rank}}^{3}$
    and Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3
    A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 10 Hessian for $Y$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [10.1](#S10.SS1 "10.1 Hessian Property ‣ 10 Hessian for 𝑌 ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we present the
    hessian property with respect to $Y$ for one $j_{0},i_{0}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Hessian Property
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the Hessian properties.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 10.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $B_{j_{0}}(x)=f(x)_{j_{0}}f(x)_{j_{0}}^{\top}\in\mathbb{R}^{n\times n}$
    (because of Lemma[10.2](#S10.Thmtheorem2 "Lemma 10.2\. ‣ 10.2 Hessian for One
    𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $B(x)=\sum_{j_{0}=1}^{n}B_{j_{0}}(x)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H_{i_{0}}\in\mathbb{R}^{d\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H_{\operatorname{reg},i_{0}}=A_{3}^{\top}(B(x)+W^{2})A_{3}$ is a positive
    diagonal matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H(y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be $$H(y)=\begin{bmatrix}H_{1}&amp;0&amp;\cdots&amp;0\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 0&amp;H_{2}&amp;\cdots&amp;0\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 0&amp;0&amp;\cdots&amp;H_{d}\end{bmatrix}$$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 0\preceq B_{j_{0}}(x)\preceq I_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 0\preceq B(x)\preceq n\cdot I_{n}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3. If $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H_{\operatorname{reg},i_{0}}\succeq l\cdot I_{d},~{}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4. If $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}+100n$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 0.9(W^{2}+B(x))\preceq W^{2}\preceq 1.1(W^{2}+B(x))$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5. Lipschitz, Due to $H(y)$, then
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H(y)-H(\widetilde{y})\&#124;\leq\&#124;y-\widetilde{y}\&#124;_{2}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For hessian closed-form, we can obtain them from Lemma [10.2](#S10.Thmtheorem2
    "Lemma 10.2\. ‣ 10.2 Hessian for One 𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: The proofs are straightforward, so we omit the details here. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Hessian for One $j_{0},i_{0}$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the Hessian for the matrix $Y$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 10.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a temporary notation here $v:=f(x)_{j_{0}}$ in the statement. Note
    that $v$ could have different meaning in other sections.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,:)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1. For $i_{1}=i_{2}$, the diagonal case
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2. For $i_{1}\neq i_{2}$, the off-diagonal case
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3. The $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}\in\mathbb{R}^{d\times
    d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}=A_{3}^{\top}vv^{\top}A_{3}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{1}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from simple algebra, the second step follows from
    Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient With Respect to 𝑦 ‣ 5
    Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    the third step follows from Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient
    With Respect to 𝑦 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from simple algebra, the second step follows from
    Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient With Respect to 𝑦 ‣ 5
    Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    the third step follows from Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient
    With Respect to 𝑦 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: It follows by combining above two parts directly. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 11 Hessian for $X$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [11.1](#S11.SS1 "11.1 Computing Hessian ‣ 11 Hessian for 𝑋 and 𝑌
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    the Hessian matrix with respect to both $X$. In Section [11.2](#S11.SS2 "11.2
    A Helpful Lemma ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we present several helpful lemmas for the following
    proof. In Section [11.2](#S11.SS2 "11.2 A Helpful Lemma ‣ 11 Hessian for 𝑋 and
    𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we create
    $B(x)$ for the further analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Computing Hessian
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we compute the Hessian matrix for $X$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 11.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.7](#S4.Thmtheorem7 "Definition
    4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})=$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can show
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Part 6 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step comes from the
    product rule of derivative, the third step is based on Lemma [10.2](#S10.Thmtheorem2
    "Lemma 10.2\. ‣ 10.2 Hessian for One 𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step follows from
    simple algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 A Helpful Lemma
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide a helpful Lemma.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 11.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}$ be defined in Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    in Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.7](#S4.Thmtheorem7 "Definition
    4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},h(y)_{i_{0}}\rangle=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the second step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},A_{3,*,i_{1}}\rangle=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first, second, and last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 4.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Creating $B(x,y)$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we give a formal definition of $B(x,y)$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 11.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define $B(x,y)$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle B(x,y)=B_{\operatorname{diag}}^{1}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $B_{\operatorname{rank}}^{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $B_{\operatorname{diag}}^{1}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $B_{\operatorname{rank}}^{3}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lemma 11.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $B(x,y)$ be defined as Definition[11.3](#S11.Thmtheorem3 "Definition 11.3\.
    ‣ 11.3 Creating 𝐵⁢(𝑥,𝑦) ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.  $i_{1}\neq i_{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from combining Lemma [11.1](#S11.Thmtheorem1 "Lemma
    11.1\. ‣ 11.1 Computing Hessian ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and Lemma [11.2](#S11.Thmtheorem2
    "Lemma 11.2\. ‣ 11.2 A Helpful Lemma ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}x}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B(x,y)A_{3}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof of Part 2. We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from combining Lemma [11.1](#S11.Thmtheorem1 "Lemma
    11.1\. ‣ 11.1 Computing Hessian ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and Lemma [11.2](#S11.Thmtheorem2
    "Lemma 11.2\. ‣ 11.2 A Helpful Lemma ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 12 Lipschitz for Hessian of $x,y$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [12.1](#S12.SS1 "12.1 Main Results ‣ 12 Lipschitz for Hessian of
    𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we present
    the main results of the Lipschitz property of $H_{x,y}$. In Section [12.6](#S12.SS6
    "12.6 Calculation: Step 2 Lipschitz for Matrix Function -⟨𝑓⁢(𝑥)_𝑗₀,ℎ⁢(𝑦)_𝑖₀⟩⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the second step of Lipschitz function $-\langle
    f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$. In Section [12.8](#S12.SS8
    "12.8 Calculation: Step 4 Lipschitz for Matrix Function 𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the fourth step of Lipschitz function $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we present the main result of Section [12](#S12 "12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. For $j_{0}\in[d],i_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Proof of Part 1. It follows from Lemma [12.2](#S12.Thmtheorem2 "Lemma 12.2\.
    ‣ 12.2 Summary of Four Steps on Lipschitz for Matrix Functions ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2. We can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step follows from that we can write $H$ terms $H_{j_{0},i_{0}}$,
    $i_{0}\in[d]$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Summary of Four Steps on Lipschitz for Matrix Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we summarize the four steps for analyzing the Lipschitz for
    different matrix functions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The proof follows from Lemma [12.5](#S12.Thmtheorem5 "Lemma 12.5\. ‣ 12.5 Calculation:
    Step 1 Lipschitz for Matrix Function (𝑓⁢(𝑥)_𝑗₀∘ℎ⁢(𝑦)_𝑖₀)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [12.6](#S12.Thmtheorem6 "Lemma 12.6\. ‣ 12.6 Calculation: Step 2
    Lipschitz for Matrix Function -⟨𝑓⁢(𝑥)_𝑗₀,ℎ⁢(𝑦)_𝑖₀⟩⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [12.7](#S12.Thmtheorem7 "Lemma 12.7\. ‣ 12.7 Calculation: Step 3
    Lipschitz for Matrix Function -𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢diag(𝑓⁢(𝑥)_𝑗₀) ‣ 12 Lipschitz for
    Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and Lemma [12.8](#S12.Thmtheorem8 "Lemma 12.8\. ‣ 12.8 Calculation: Step
    4 Lipschitz for Matrix Function 𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '12.3 A Core Tool: Upper Bound for Several Basic Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we give an upper bound for each of the basic functions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(y)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.11](#S4.Thmtheorem11
    "Definition 4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{3}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|y_{i_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $\|h(y)_{i_{0}}\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2\. $|c(x,y)_{j_{0},i_{0}}|\leq 2R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;h(y)_{i_{0}}\&#124;_{2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Definition [4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on Fact [4.3](#S4.Thmtheorem3
    "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and the third step is because of Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is because of Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is based on triangle inequality and Cauchy–Schwarz inequality, the third
    step is due to Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and the last step follows from $R\geq 4$. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '12.4 A Core Tool: Lipschitz Property for Several Basic Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce the Lipschitz property for several basic functions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(y)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.11](#S4.Thmtheorem11
    "Definition 4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{3}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|y_{i_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $\|h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\|_{2}\leq R\|y-\widetilde{y}\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2. $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3. $|c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})|\leq R\|y-\widetilde{y}\|_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\&#124;_{2}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Definition [4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on Fact [4.3](#S4.Thmtheorem3
    "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the third step is due to Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(\widetilde{x},y_{j_{0},i_{0}})&#124;=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step follows from Cauchy–Schwarz inequality, and the third step is because of
    Part 1 of Lemma [12.3](#S12.SS3 "12.3 A Core Tool: Upper Bound for Several Basic
    Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and Part 3 of Lemma [8.5](#S8.Thmtheorem5 "Lemma
    8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property
    for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})&#124;=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is due to Cauchy–Schwarz inequality and the third step is because of Part
    4 of Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3
    A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    and Part 1 of this Lemma. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: '12.5 Calculation: Step 1 Lipschitz for Matrix Function $(f(x)_{j_{0}}\circ
    h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we calculate the Lipschitz for $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{1,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{1,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{1,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from definition of $G_{1,1}$, the second step
    is based on Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step is due to Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from definition of $G_{1,1}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is based on combining Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic
    Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and Lemma [12.3](#S12.SS3 "12.3 A Core Tool: Upper Bound
    for Several Basic Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Also, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on definition of $G_{1,2}$, the second step is
    because of Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step follows from Lemma [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A Core
    Tool: Lipschitz Property for Several Basic Functions ‣ 12 Lipschitz for Hessian
    of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{1,3}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from the definition of $G_{1,3}$, the second step
    follows from Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Combining all the above equations we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: '12.6 Calculation: Step 2 Lipschitz for Matrix Function $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we calculate the Lipschitz for $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.6.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{2,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{2,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{2,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{2,4}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: We have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is based on the definition of $G_{2,1}$, the second step
    follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is because of Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to the definition of $G_{2,1}$, the second step
    is based on Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step follows from Lemma [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A Core
    Tool: Lipschitz Property for Several Basic Functions ‣ 12 Lipschitz for Hessian
    of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{2,3}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\&#124;G_{2,4}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: Combining all the above equations we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: '12.7 Calculation: Step 3 Lipschitz for Matrix Function $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we calculate the Lipschitz for $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.7.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{3,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{3,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{3,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{3,1}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{3,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step follows from definition of $G_{3,1}$, the second step
    is based on Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step is because of Lemma [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A
    Core Tool: Lipschitz Property for Several Basic Functions ‣ 12 Lipschitz for Hessian
    of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: Combining all the above equations we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: '12.8 Calculation: Step 4 Lipschitz for Matrix Function $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we calculate the Lipschitz for $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.8.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $R\geq 4$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{4,1}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{4,2}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{4,3}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{4,4}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: For $G_{4,1}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{4,1}\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first step is due to definition of $G_{4,1}$, the second step is
    because of Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\&#124;G_{4,4}\&#124;\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: Combining all the above equations we complete the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 12.9 PSD Upper Bound for Hessian $x,y$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the PSD upper bound for Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.9.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. For $j_{0}\in[d],i_{0}\in[n]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H(x,y)_{j_{0},i_{0}}\&#124;\leq 10R^{2}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H(x,y)\&#124;\leq nd\cdot 10R^{2}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Proof of Part 1. It follows from Lemma [12.10](#S12.Thmtheorem10 "Lemma 12.10\.
    ‣ 12.10 Upper Bound on Hessian Spectral Norms ‣ 12 Lipschitz for Hessian of 𝑥,𝑦
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2. We can show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;H(x,y)\&#124;=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step is due to the assumption of $H(x,y)$, and the second step
    comes from Part 1. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 12.10 Upper Bound on Hessian Spectral Norms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we find the upper bound for the Hessian spectral norms.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12.10.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1\. $\|G_{1}(x,y)\|\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2\. $\|G_{2}(x,y)\|\leq R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3\. $\|G_{3}(x,y)\|\leq 2R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4\. $\|G_{4}(x,y)\|\leq 2R^{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{k=1}^{4}\&#124;G_{k}(x,y)\&#124;\leq 10R^{2}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The proof is straightforward by using upper bound on each term ∎
  prefs: []
  type: TYPE_NORMAL
- en: 13 Generating a Spectral Sparsifier via TensorSketch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tensor type sketching has been widely used in problems [[165](#bib.bib165),
    [58](#bib.bib58), [52](#bib.bib52), [6](#bib.bib6), [163](#bib.bib163), [173](#bib.bib173),
    [166](#bib.bib166), [202](#bib.bib202), [170](#bib.bib170)]. Section [13.1](#S13.SS1
    "13.1 Oblivious Subspace Embedding ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") presents
    the definition of oblivious subspace embedding. In Section [13.2](#S13.SS2 "13.2
    TensorSRHT ‣ 13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we give an overview of $\mathsf{TensorSRHT}$.
    In Section [13.4](#S13.SS4 "13.4 Fast Approximation for Hessian via Sketching
    ‣ 13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), we introduce the fast approximation
    for hessian via sketching.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 Oblivious Subspace Embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We define oblivious subspace embedding,
  prefs: []
  type: TYPE_NORMAL
- en: Definition 13.1  (Oblivious subspace embedding, [[156](#bib.bib156)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We define $(\epsilon,\delta,d,n)$ is a distribution on $m\times n$, where $m$,
    and $\delta$, for any fixed $n\times d$, a matrix $S$ has the property that the
    singular values of $SU$.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 TensorSRHT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We define a well-known sketching matrix family called TensorSRHT [[104](#bib.bib104),
    [6](#bib.bib6)]. It has been used in many optimization literature [[163](#bib.bib163),
    [173](#bib.bib173), [166](#bib.bib166)].
  prefs: []
  type: TYPE_NORMAL
- en: Definition 13.2  (Tensor subsampled randomized Hadamard transform (TensorSRHT)
    [[6](#bib.bib6), [163](#bib.bib163)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The $\mathsf{TensorSRHT}$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle S:=\frac{1}{\sqrt{m}}P\cdot(HD_{1}\otimes HD_{2}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where each row of $P\in\{0,1\}^{m\times n^{2}}$ at a random coordinate and one
    can view $P$ is a $n\times n$, $D_{2}$ independent diagonal matrices with diagonals
    that are each independently set to be a Rademacher random variable (uniform in
    $\{-1,1\}$).
  prefs: []
  type: TYPE_NORMAL
- en: It is known [[6](#bib.bib6)] that TensorSRHT matrices imply the OSE.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 13.3  ([[6](#bib.bib6), [163](#bib.bib163)] , see for example, Lemma 2.12
    in [[163](#bib.bib163)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $S$ be a TensorSRHT matrix defined in Definition [13.2](#S13.Thmtheorem2
    "Definition 13.2 (Tensor subsampled randomized Hadamard transform (TensorSRHT)
    [6, 163]). ‣ 13.2 TensorSRHT ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"). If'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle m=O(\epsilon^{-2}d^{2}\log^{3}(nd/\epsilon\delta)),$ |  |'
  prefs: []
  type: TYPE_TB
- en: then $S$-OSE for degree-$2$ tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Further for matrices $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ can be computed
    in $\widetilde{O}(nd+md^{2})$ time.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 TensorSparse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[166](#bib.bib166)] define TensorSparse by compose Sparse embedding [[133](#bib.bib133),
    [43](#bib.bib43)] with tensor operation [[136](#bib.bib136)].'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 13.4  (TensorSparse, see Definition 7.6 in [[166](#bib.bib166)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $h_{1},h_{2}:[n]\times[s]\rightarrow[m/s]$-wise independent hash functions
    and let $\sigma_{1},\sigma_{2}:[n]\times[s]\rightarrow\{\pm 1\}$-wise independent
    random sign functions. Then, the degree two tensor sparse transform, $S:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
    is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R_{r,(i,j)}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: Lemma 13.5  (Theorem 7.10 in [[166](#bib.bib166)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\epsilon\in(0,1)$ be success probability. Let $S\in\mathbb{R}^{m\times
    n^{2}}$ matrix (Def. [13.4](#S13.Thmtheorem4 "Definition 13.4 (TensorSparse, see
    Definition 7.6 in [166]). ‣ 13.3 TensorSparse ‣ 13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). Suppose $m=\Omega(\epsilon^{-2}d^{2}\log(n/\delta))$, then TensorSparse
    provides $(\epsilon,\delta,d^{2},n^{2})$-OSE.'
  prefs: []
  type: TYPE_NORMAL
- en: Further for matrices $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ can be computed
    in $O((\operatorname{nnz}(A_{1})+\operatorname{nnz}(A_{2}))s+md^{2})$ time
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Fast Approximation for Hessian via Sketching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present the fast approximation for hessian via sketching.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 13.6.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $A_{1}\in\mathbb{R}^{n\times d}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\operatorname{\mathsf{A}}=(A_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $W\in\mathbb{R}^{n\times n}$ denote a positive diagonal matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\overline{A}_{1}=WA_{1}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\overline{\operatorname{\mathsf{A}}}=(\overline{A}_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2. For any constant $\epsilon\in(0,0.1)$ time to compute $S\overline{\operatorname{\mathsf{A}}}$
    such that
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: holds with probability $1-\delta$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3. For any $\epsilon\in(0,0.1)$ time to compute $S\overline{\operatorname{\mathsf{A}}}$
    such that
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: holds with probability $1-\delta$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Proof of Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: We can show
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the first step follows from $(W^{2}\otimes I)=(W\otimes I_{n})\cdot(W\otimes
    I_{n})$ operation and $W$, the third step follows from the definition of $\overline{A}_{1}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'It follows from using Lemma [13.3](#S13.Thmtheorem3 "Lemma 13.3 ([6, 163] ,
    see for example, Lemma 2.12 in [163]). ‣ 13.2 TensorSRHT ‣ 13 Generating a Spectral
    Sparsifier via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'It follows from using Lemma [13.5](#S13.Thmtheorem5 "Lemma 13.5 (Theorem 7.10
    in [166]). ‣ 13.3 TensorSparse ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '14 Analysis Of Algorithm [1](#alg1 "Algorithm 1 ‣ 6 Hessian ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We introduce the concept of a $(l,M)$-good function in Section [14.1](#S14.SS1
    "14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and discuss the notion of a well-initialized
    point. Subsequently, we will present our approximation and update rule methods
    in Section [14.2](#S14.SS2 "14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"). In light of the
    optimization problem introduced in Definition [1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we put forward Algorithm [1](#alg1 "Algorithm 1
    ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and in this section, we establish the correctness and convergence of the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 $(l,M)$-Good Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now introduce the definition of a $(l,M)$-Good Loss Function. Next,
    let’s revisit the optimization problem defined in Definition [4.7](#S4.Thmtheorem7
    "Definition 4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: We will now demonstrate that our optimization function possesses the following
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 14.1  ($(l,M)$-good Loss function).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a function $L:\mathbb{R}^{d}\rightarrow\mathbb{R}$, if the following conditions
    hold,
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hessian is $M$ such that
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $l$ as a positive scalar. If there exists a vector $x^{*}\in\mathbb{R}^{d^{2}}$
    such that the following holds
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: $\nabla L(x^{*},y^{*})={\bf 0}_{d}$.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: $\nabla^{2}L(x^{*},y^{*})\succeq l\cdot I_{2d^{2}}$.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good Initialization Point. Let $x_{0}$ denote the initialization point. If $r_{0}:=(\|x_{0}-x_{*}\|_{2}+\|y_{0}-y_{*}\|_{2})$
    satisfies
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r_{0}M\leq 0.1l.$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: we say $L$-good
  prefs: []
  type: TYPE_NORMAL
- en: 'Drawing upon Lemma [6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") and Lemma [12.1](#S12.Thmtheorem1
    "Lemma 12.1\. ‣ 12.1 Main Results ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we can establish that our loss
    function (See Definition [4.7](#S4.Thmtheorem7 "Definition 4.7\. ‣ 4.2 General
    Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")) satisfies the aforementioned assumption.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2 Convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After introducing the approximation method ’Sparsifier via TensorSketch’ in
    Section [13](#S13 "13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we will now proceed
    to introduce the update method employed in Algorithm [1](#alg1 "Algorithm 1 ‣
    6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").
    In this section, we demonstrate the concept of approximate update and present
    an induction hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 14.2  (Approximate Update).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following process is considered by us
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\begin{bmatrix}x(t+1)\\ y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\'
  prefs: []
  type: TYPE_NORMAL
- en: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
  prefs: []
  type: TYPE_NORMAL
- en: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: A tool from previous work is presented by us now.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 14.3  (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [[118](#bib.bib118)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following conditions hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss Function $L$-good (see Definition [14.1](#S14.Thmtheorem1 "Definition
    14.1 ((𝑙,𝑀)-good Loss function). ‣ 14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis
    Of Algorithm 1 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $\epsilon\in(0,0.1)$ (see Lemma[13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣
    13.4 Fast Approximation for Hessian via Sketching ‣ 13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $x^{*},y^{*}$ be defined in Definition[14.2](#S14.Thmtheorem2 "Definition
    14.2 (Approximate Update). ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $\overline{r}_{t}:=M\cdot r_{t}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It follows that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r_{t+1}\leq 2\cdot(\epsilon_{0}+\overline{r}_{t}/(l-\overline{r}_{t}))\cdot
    r_{t}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In this context, where $T$ denotes the total number of iterations in the algorithm,
    we require the following lemma based on the induction hypothesis to apply Lemma [14.3](#S14.Thmtheorem3
    "Lemma 14.3 (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [118]).
    ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"). This lemma is a well-established concept in the
    literature, and for further details, you can refer to [[118](#bib.bib118)].'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 14.4  (Induction hypothesis, Lemma 6.10 on page 34 of [[118](#bib.bib118)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the following condition hold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\epsilon=0.01$ (see Lemma[13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣ 13.4 Fast
    Approximation for Hessian via Sketching ‣ 13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $x^{*},y^{*}$ be defined in Definition[14.2](#S14.Thmtheorem2 "Definition
    14.2 (Approximate Update). ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each $i\in[T]$, for all $i\in[t]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let $l$ be Defined in Definition[14.1](#S14.Thmtheorem1 "Definition 14.1 ((𝑙,𝑀)-good
    Loss function). ‣ 14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis Of Algorithm 1 ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $M\cdot r_{i}\leq 0.1l$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It follows that
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $r_{t+1}\leq 0.4r_{t}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $M\cdot r_{t+1}\leq 0.1l$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AAA^+ [23] Zaid Alyafeai, Maged S Alshaibani, Badr AlKhamissi, Hamzah Luqman,
    Ebrahim Alareqi, and Ali Fadel. Taqyim: Evaluating arabic nlp tasks using chatgpt
    models. arXiv preprint arXiv:2306.16322, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AC [06] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the
    fast johnson-lindenstrauss transform. In Proceedings of the thirty-eighth annual
    ACM symposium on Theory of computing, pages 557–563, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained
    analysis of optimization and generalization for overparameterized two-layer neural
    networks. In International Conference on Machine Learning, pages 322–332\. PMLR,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and
    Ruosong Wang. On exact computation with an infinitely wide neural net. Advances
    in neural information processing systems, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AHO^+ [23] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita
    Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al.
    Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AKK^+ [20] Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya
    Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree
    polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium
    on Discrete Algorithms, pages 141–160\. SIAM, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ALH [21] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent
    on overparameterized nonlinear models. IEEE Transactions on Neural Networks and
    Learning Systems, 33(12):7717–7727, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ALS^+ [18] Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi
    Zhong. Subspace embedding and linear regression with orlicz norm. In International
    Conference on Machine Learning, pages 224–233\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ALS^+ [22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo.
    Bypass exponential time preprocessing: Fast neural network training via weight-data
    correlation preprocessing. arXiv preprint arXiv:2211.14227, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AMC^+ [23] Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram Hasanain,
    Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham, Fahim Dalvi, Majd
    Hawasly, et al. Benchmarking arabic ai with large language models. arXiv preprint
    arXiv:2305.14982, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Josh Alman and Zhao Song. Fast attention requires bounded entries. arXiv
    preprint arXiv:2302.13214, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Daman Arora, Himanshu Gaurav Singh, et al. Have llms advanced enough?
    a challenging problem solving benchmark for large language models. arXiv preprint
    arXiv:2305.15074, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Ehsan Amid and Manfred K Warmuth. Winnowing with gradient descent. In
    Conference on Learning Theory, pages 163–182\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Ehsan Amid and Manfred KK Warmuth. Reparameterizing mirror descent as
    gradient descent. Advances in Neural Information Processing Systems, 33:8430–8439,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AW [21] Josh Alman and Virginia Vassilevska Williams. A refined laser method
    and faster matrix multiplication. In Proceedings of the 2021 ACM-SIAM Symposium
    on Discrete Algorithms (SODA), pages 522–539\. SIAM, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for
    deep learning via over-parameterization. In International conference on machine
    learning, pages 242–252\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of
    training recurrent neural networks. Advances in neural information processing
    systems, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BCB [14] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine
    translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BCE^+ [23] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
    Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BCL^+ [23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
    Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask,
    multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and
    interactivity. arXiv preprint arXiv:2302.04023, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BCS [97] Peter Bürgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic
    complexity theory, volume 315. Springer Science & Business Media, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BGVV [20] Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit
    regularization for deep neural networks driven by an ornstein-uhlenbeck like process.
    In Conference on learning theory, pages 483–513\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BHS^+ [23] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He.
    Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense
    problem in large language models. arXiv preprint arXiv:2303.16421, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BPSW [20] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training
    (overparametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bra [20] Jan van den Brand. A deterministic linear program solver in current
    matrix multiplication time. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium
    on Discrete Algorithms (SODA), pages 259–278\. SIAM, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BS [23] Jan den van Brand and Zhao Song. A $\sqrt{n}$ passes streaming algorithm
    for solving bipartite matching exactly. Manuscript, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BSY [23] Song Bian, Zhao Song, and Junze Yin. Federated empirical risk minimization
    via second-order method. arXiv preprint arXiv:2305.17482, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BSZ [23] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness
    for dynamic attention maintenance in large language models. arXiv preprint arXiv:2304.02207,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BW [14] Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions.
    In Proceedings of the forty-sixth annual ACM symposium on Theory of computing
    (STOC), pages 353–362, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BWZ [16] Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal
    component analysis in distributed and streaming models. In Proceedings of the
    forty-eighth annual ACM symposium on Theory of Computing, pages 236–249, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BYKS [22] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering
    latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CGH^+ [19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua
    Zhang, and Liwei Wang. Gram-gauss-newton method: Learning overparameterized neural
    networks for regression problems. arXiv preprint arXiv:1905.11675, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CGLZ [20] Matthias Christandl, François Le Gall, Vladimir Lysikov, and Jeroen
    Zuiddam. Barriers for rectangular matrix multiplication. In arXiv preprint, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cha [22] ChatGPT. Optimizing language models for dialogue. OpenAI Blog, November
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CHBP [23] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval:
    Towards holistic evaluation of instruction-tuned large language models. arXiv
    preprint arXiv:2306.04757, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CL [01] Chih-Chung Chang and Chih-Jen Lin. Training v-support vector classifiers:
    theory and algorithms. Neural computation, 13(9):2119–2147, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLBBJ [23] Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel, and Sangita
    Jindal. The promise and peril of using a large language model to obtain clinical
    information: Chatgpt performs strongly as a fertility counseling tool with limitations.
    Fertility and Sterility, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLMY [21] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. In International
    Conference on Machine Learning, pages 1193–1203\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLP^+ [21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie
    Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A
    learnable lsh framework for efficient neural network training. In International
    Conference on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLS [19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs
    in the current matrix multiplication time. In STOC, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNP [23] Cayque Monteiro Castro Nascimento and André Silva Pimentel. Do large
    language models understand chemistry? a conversation with chatgpt. Journal of
    Chemical Information and Modeling, 63(6):1649–1655, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coh [16] Michael B Cohen. Nearly tight oblivious subspace embeddings by trace
    inequalities. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on
    Discrete algorithms, pages 278–287\. SIAM, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cop [82] Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM
    Journal on Computing, 11(3):467–471, 1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPK^+ [23] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens.
    Do llms understand social knowledge? evaluating the sociability of large language
    models with socket benchmark. arXiv preprint arXiv:2305.14938, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CW [13] Kenneth L Clarkson and David P Woodruff. Low-rank approximation and
    regression in input sparsity time. In STOC, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CWJ^+ [23] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring
    the use of large language models for reference-free text quality evaluation: A
    preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CZL^+ [23] Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel
    Hershcovich. Assessing cross-cultural alignment between chatgpt and human societies:
    An empirical study. arXiv preprint arXiv:2303.17466, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DCLT [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    Bert: Pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DDH^+ [21] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu
    Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DGG [23] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. How ready are
    pre-trained abstractive models and llms for legal case judgement summarization?
    arXiv preprint arXiv:2306.01248, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DJS^+ [19] Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff.
    Optimal sketching for kronecker product regression and low rank approximation.
    Advances in neural information processing systems, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DL [23] Xuan-Quy Dao and Ngoc-Bich Le. Investigating the effectiveness of chatgpt
    in mathematical reasoning and problem solving: Evidence from the vietnamese national
    high school graduation examination. arXiv preprint arXiv:2306.06331, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLMS [23] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th
    order algorithm for softmax attention optimization. arXiv preprint arXiv:2307.08352,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLS [23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired
    softmax regression. arXiv preprint arXiv:2304.10411, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DML [21] Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers
    flat global minimizers. Advances in Neural Information Processing Systems, 34:27449–27461,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DMS [23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic
    attention sparsification algorithms for over-parameterized feature dimension.
    arXiv preprint arXiv:2304.04397, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSSW [18] Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for
    kronecker product regression and p-splines. In International Conference on Artificial
    Intelligence and Statistics, pages 1299–1308\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSWY [22] Yichuan Deng, Zhao Song, Yitan Wang, and Yuanyuan Yang. A nearly optimal
    size coreset algorithm with nearly linear time. arXiv preprint arXiv:2210.08361,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSY [23] Yichuan Deng, Zhao Song, and Junze Yin. Faster robust tensor power
    method for arbitrary order. arXiv preprint arXiv:2306.00406, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSZZ [23] Yichuan Deng, Zhao Song, Lichen Zhang, and Ruizhe Zhang. Efficient
    algorithm for solving hyperbolic programs. arXiv preprint arXiv:2306.07587, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DWZ [23] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication
    via asymmetric hashing. In FOCS, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DZPS [18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient
    descent provably optimizes over-parameterized neural networks. arXiv preprint
    arXiv:1810.02054, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMZ [21] Hossein Esfandiari, Vahab Mirrokni, and Peilin Zhong. Almost linear
    time density level set estimation via dbscan. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 35, pages 7349–7357, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fer [23] Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias
    in large language models. arXiv preprint arXiv:2304.03738, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fra [23] Michael C Frank. Baby steps in evaluating the capacities of large language
    models. Nature Reviews Psychology, pages 1–2, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GGL^+ [23] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla,
    Olaf Wiest, Xiangliang Zhang, et al. What indeed can gpt models do in chemistry?
    a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLSS [18] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing
    implicit bias in terms of optimization geometry. In International Conference on
    Machine Learning, pages 1832–1841\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GMS [23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential
    regression. arXiv preprint arXiv:2303.16504, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GQSW [22] Yeqi Gao, Lianke Qin, Zhao Song, and Yitan Wang. A sublinear adversarial
    training algorithm. arXiv preprint arXiv:2208.05395, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GS [22] Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. arXiv
    preprint arXiv:2211.06033, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GSX [23] Yeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention
    scheme: from single softmax regression to multiple softmax regression via a tensor
    trick. arXiv preprint arXiv:2307.02419, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized
    large language models. arXiv preprint arXiv:2308.10502, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled
    hyperbolic functions regression. arXiv preprint arXiv:2305.00660, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm
    for attention computation. arXiv preprint arXiv:2307.08045, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Yuzhou Gu, Zhao Song, Junze Yin, and Lichen Zhang. Low rank matrix completion
    via robust alternating minimization in nearly linear time. arXiv preprint arXiv:2302.11068,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GSZ [23] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm
    for structured support vector machines. arXiv preprint arXiv:2307.07735, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GU [18] François Le Gall and Florent Urrutia. Improved rectangular matrix multiplication
    using powers of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth
    Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBKG [23] Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization
    inform editing? surprising differences in causality-based localization vs. knowledge
    editing in language models. arXiv preprint arXiv:2301.04213, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HF [23] Thilo Hagendorff and Sarah Fabi. Human-like intuitive behavior and reasoning
    biases emerged in language models–and disappeared in gpt-4. arXiv preprint arXiv:2306.07622,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HJS^+ [22] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang.
    Solving sdp faster: A robust ipm framework and efficient implementation. In 2022
    IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 233–244\.
    IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HLSY [21] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural
    tangent kernel-based framework for federated learning analysis. In International
    Conference on Machine Learning, pages 4423–4434\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HLZ [23] Sophie Huiberts, Yin Tat Lee, and Xinzhi Zhang. Upper and lower bounds
    on the smoothed complexity of the simplex method. In Proceedings of the 55th Annual
    ACM Symposium on Theory of Computing, pages 1904–1917, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HM [19] John Hewitt and Christopher D Manning. A structural probe for finding
    syntax in word representations. In Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HWLM [21] Jeff Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters:
    Understanding the implicit bias of the noise covariance. In Conference on Learning
    Theory, pages 2315–2357\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JDST [20] Ziwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky.
    Gradient descent follows the regularization path for general losses. In Conference
    on Learning Theory, pages 2109–2136\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JGP^+ [23] Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone, Eli Zimmerman,
    Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir, et al. Assessing
    the accuracy and reliability of ai-generated medical responses: an evaluation
    of the chat-gpt model. ., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JKL^+ [20] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and
    Zhao Song. A faster interior point method for semidefinite programming. In 2020
    IEEE 61st annual symposium on foundations of computer science (FOCS), pages 910–918\.
    IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JL [84] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings
    into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JLSW [20] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved
    cutting plane method for convex optimization, convex-concave games, and its applications.
    In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
    pages 944–953, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joa [06] Thorsten Joachims. Training linear svms in linear time. In Proceedings
    of the 12th ACM SIGKDD international conference on Knowledge discovery and data
    mining, pages 217–226, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JST [21] Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization
    via dual acceleration. In International Conference on Machine Learning, pages
    4860–4869\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSWZ [21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster
    algorithm for solving general lps. In Proceedings of the 53rd Annual ACM SIGACT
    Symposium on Theory of Computing, pages 823–832, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JT [18] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic
    regression. arXiv preprint arXiv:1803.07300, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on
    nonseparable data. In Conference on Learning Theory, pages 1772–1798\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient
    descent to achieve arbitrarily small test error with shallow relu networks. arXiv
    preprint arXiv:1909.12292, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JT [20] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment
    in deep learning. Advances in Neural Information Processing Systems, 33:17176–17186,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JT [21] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a
    primal-dual analysis. In Algorithmic Learning Theory, pages 772–804\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KKL [20] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. arXiv preprint arXiv:2001.04451, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling
    laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KPOT [21] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
    Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization.
    Advances in Neural Information Processing Systems, 34:18970–18983, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LBL^+ [22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
    et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LBR^+ [23] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen
    Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive
    evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDFU [13] Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster
    ridge regression via the subsampled randomized hadamard transform. Advances in
    neural information processing systems, 26, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LG [14] François Le Gall. Powers of tensors and fast matrix multiplication.
    In Proceedings of the 39th international symposium on symbolic and algebraic computation,
    pages 296–303, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LG [23] François Le Gall. Faster rectangular matrix multiplication by combination
    loss analysis. arXiv preprint arXiv:2307.06535, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LL [18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks
    via stochastic gradient descent on structured data. Advances in neural information
    processing systems, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LL [21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous
    prompts for generation. arXiv preprint arXiv:2101.00190, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLGB [23] Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine. A survey on out-of-distribution
    evaluation of neural nlp models. arXiv preprint arXiv:2306.15261, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLH^+ [23] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia:
    A scalable stochastic second-order optimizer for language model pre-training.
    arXiv preprint arXiv:2305.14342, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLR [23] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn
    topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LNV^+ [23] Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man,
    Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. Chatgpt beyond english: Towards
    a comprehensive evaluation of large language models in multilingual learning.
    arXiv preprint arXiv:2304.05613, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LPM [15] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches
    to attention-based neural machine translation. arXiv preprint arXiv:1508.04025,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LR [20] Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless”
    regression can generalize. ., 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSS^+ [20] Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, et al. Generalized
    leverage score sampling for neural networks. Advances in Neural Information Processing
    Systems, 33:10775–10787, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSW [15] Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting
    plane method and its implications for combinatorial and convex optimization. In
    2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 1049–1065\.
    IEEE, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSZ [19] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization
    in the current matrix multiplication time. In Conference on Learning Theory, pages
    2140–2157\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh
    and sinh regression problems. arXiv preprint arXiv:2303.15725, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] S. Cliff Liu, Zhao Song, Hengjie Zhang, Lichen Zhang, and Tianyi Zhou.
    Space-efficient interior point method, with applications to linear programming
    and maximum weight bipartite matching. In International Colloquium on Automata,
    Languages and Programming (ICALP), pages 88:1–88:14, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LWA [21] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd
    reaches zero loss?–a mathematical framework. arXiv preprint arXiv:2110.06914,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LWM [19] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization
    effect of initial large learning rate in training neural networks. Advances in
    Neural Information Processing Systems, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LXWZ [23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is
    your code generated by chatgpt really correct? rigorous evaluation of large language
    models for code generation. arXiv preprint arXiv:2305.01210, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LYB^+ [22] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh
    Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. Large
    models are parsimonious learners: Activation sparsity in trained transformers.
    arXiv preprint arXiv:2210.06313, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MBAB [22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating
    and editing factual associations in gpt. Advances in Neural Information Processing
    Systems, 35:17359–17372, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MGN^+ [23] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D
    Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward
    passes. arXiv preprint arXiv:2305.17333, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MM [13] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings
    in input-sparsity time and applications to robust linear regression. In Proceedings
    of the forty-fifth annual ACM symposium on Theory of computing, pages 91–100,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MOSW [22] Alexander Munteanu, Simon Omlor, Zhao Song, and David Woodruff. Bounding
    the width of neural networks via coupled initialization a worst case analysis.
    In International Conference on Machine Learning, pages 16083–16122\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MRS [20] Konstantin Makarychev, Aravind Reddy, and Liren Shan. Improved guarantees
    for k-means++ and k-means++ parallel. Advances in Neural Information Processing
    Systems, 33:16142–16152, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MS [21] Linjian Ma and Edgar Solomonik. Fast and accurate randomized algorithms
    for low-rank tensor decompositions. Advances in Neural Information Processing
    Systems, 34:24299–24312, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MWG^+ [20] Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee,
    Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization
    scale vs training accuracy. Advances in neural information processing systems,
    33:22182–22193, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NKL^+ [23] John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao, Meghana
    Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H Choi, and Jungo Kasai. Large language
    models as tax attorneys: A case study in legal capabilities emergence. arXiv preprint
    arXiv:2306.07075, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLG^+ [19] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona
    Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on
    separable data. In The 22nd International Conference on Artificial Intelligence
    and Statistics, pages 3420–3428\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NN [13] Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra
    algorithms via sparser subspace embeddings. In 2013 ieee 54th annual symposium
    on foundations of computer science, pages 117–126\. IEEE, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ope [23] OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OS [20] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization:
    Global convergence guarantees for training shallow neural networks. IEEE Journal
    on Selected Areas in Information Theory, 1(1):84–105, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pag [13] Rasmus Pagh. Compressed matrix multiplication. ACM Transactions on
    Computation Theory (TOCT), 5(3):1–17, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PD [23] Dongqi Pu and Vera Demberg. Chatgpt vs human-authored text: Insights
    into controllable text summarization and sentence style transfer. arXiv preprint
    arXiv:2306.07799, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PMM^+ [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca
    Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia.
    Understanding the capabilities of large language models for automated planning.
    arXiv preprint arXiv:2305.16151, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PMXA [23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora.
    Trainable transformer in transformer. arXiv preprint arXiv:2307.01189, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PSZA [23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora.
    Task-specific skill localization in fine-tuned language models. arXiv preprint
    arXiv:2302.06600, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QJS^+ [22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo,
    and Shumo Chu. Adore: Differentially oblivious relational database operators.
    In VLDB, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QQ [19] Qian Qian and Xiaoyuan Qian. The implicit bias of adagrad on separable
    data. Advances in Neural Information Processing Systems, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QRS^+ [22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, and Danyang Zhuo.
    Adaptive and dynamic multi-resolution hashing for pairwise summations. In BigData,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QSW [23] Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization.
    arXiv preprint arXiv:2305.08367, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QSZ [23] Lianke Qin, Zhao Song, and Ruizhe Zhang. A general algorithm for solving
    rank-one matrix sensing. arXiv preprint arXiv:2303.12298, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QSZZ [23] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and
    unified algorithm for projection matrix vector multiplication with application
    to empirical risk minimization. In International Conference on Artificial Intelligence
    and Statistics (AISTATS), pages 101–156\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QZZ^+ [23] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? arXiv preprint arXiv:2302.06476, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rat [20] Kovid Rathee. Meet google meena, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNS^+ [18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. ., 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RRS^+ [21] Aravind Reddy, Ryan A Rossi, Zhao Song, Anup Rao, Tung Mai, Nedim
    Lipka, Gang Wu, Eunyee Koh, and Nesreen Ahmed. Online map inference and learning
    for nonsymmetric determinantal point processes. arXiv preprint arXiv:2111.14674,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RSM^+ [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RSW [16] Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank
    approximations with provable guarantees. In Proceedings of the forty-eighth annual
    ACM symposium on Theory of Computing, pages 250–263, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RSZ [22] Aravind Reddy, Zhao Song, and Lichen Zhang. Dynamic tensor product
    regression. In NeurIPS, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RWC^+ [19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RYW^+ [19] Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy
    Coenen, Adam Pearce, and Been Kim. Visualizing and measuring the geometry of bert.
    Advances in Neural Information Processing Systems, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sar [06] Tamas Sarlos. Improved approximation algorithms for large matrices
    via random projections. In 2006 47th annual IEEE symposium on foundations of computer
    science (FOCS’06), pages 143–152\. IEEE, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SATA [22] Haoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis, and Navid Azizan.
    Mirror descent maximizes generalized margin and can be implemented efficiently.
    Advances in Neural Information Processing Systems, 35:31089–31101, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHN^+ [18] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar,
    and Nathan Srebro. The implicit bias of gradient descent on separable data. The
    Journal of Machine Learning Research, 19:2822–2878, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHT [23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational
    strengths and limitations of transformers. arXiv preprint arXiv:2306.02896, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SM^+ [23] Giriprasad Sridhara, Sourav Mazumdar, et al. Chatgpt: A study on
    its utility for ubiquitous software engineering tasks. arXiv preprint arXiv:2305.16837,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spa [23] Jared Spataro. Introducing microsoft 365 copilot – your copilot for
    work, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSZ [23] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction
    for balancing the trade-off between creativity and reality in large language models.
    arXiv preprint arXiv:2306.02295, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SWYZ [21] Zhao Song, David Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching
    of polynomial kernels of polynomial degree. In International Conference on Machine
    Learning, pages 9812–9823\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SWZ [17] Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation
    with entrywise l1-norm error. In Proceedings of the 49th Annual ACM SIGACT Symposium
    on Theory of Computing, pages 688–701, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SWZ [19] Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor
    low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
    on Discrete Algorithms, pages 2772–2789\. SIAM, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SXZ [22] Zhao Song, Zhaozhuo Xu, and Lichen Zhang. Speeding up sparsification
    using inner product search data structures. arXiv preprint arXiv:2204.03209, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SY [21] Zhao Song and Zheng Yu. Oblivious sketching-based central path method
    for solving linear programming problems. In 38th International Conference on Machine
    Learning (ICML), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SYYZ [23] Zhao Song, Mingquan Ye, Junze Yin, and Lichen Zhang. Efficient alternating
    minimization with applications to weighted low rank approximation. arXiv preprint
    arXiv:2306.04169, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SYZ [21] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training
    over-parameterized neural networks? Advances in Neural Information Processing
    Systems, 34:22890–22904, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs:
    $o(\sqrt{n})$ passes, small space and fast runtime. Manuscript, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Zhao Song, Junze Yin, and Lichen Zhang. Solving attention kernel regression
    problem via pre-conditioner. arXiv preprint arXiv:2308.14304, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SZKS [21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating
    how single head attention learns. arXiv preprint arXiv:2103.07601, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SZZ [21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized
    neural network in subquadratic time. arXiv preprint arXiv:2112.07628, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TLI^+ [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv
    preprint arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TLTO [23] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet
    Oymak. Transformers as support vector machines. arXiv preprint arXiv:2308.16898,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TMS^+ [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VCC^+ [17] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VKR [19] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization
    for optimal sparse recovery. Advances in Neural Information Processing Systems,
    32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VSP^+ [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you
    need. Advances in neural information processing systems, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WGL^+ [20] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko,
    Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich
    regimes in overparametrized models. In Conference on Learning Theory, pages 3635–3673\.
    PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WHH^+ [23] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong
    Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of
    chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wil [12] Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd.
    In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,
    pages 887–898, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WLJ^+ [23] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming
    Shi, and Zhaopeng Tu. Document-level machine translation with large language models.
    arXiv preprint arXiv:2304.02210, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WLL^+ [23] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath:
    Can your language model pass chinese elementary school math test? arXiv preprint
    arXiv:2306.16636, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WMCL [21] Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. The implicit bias
    for adaptive optimization algorithms on homogeneous neural networks. In International
    Conference on Machine Learning, pages 10849–10858\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WMZ^+ [21] Bohan Wang, Qi Meng, Huishuai Zhang, Ruoyu Sun, Wei Chen, and Zhi-Ming
    Ma. Momentum doesn’t change the implicit bias. ., 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WQR^+ [23] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin
    Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring
    the capabilities and limitations of language models through counterfactual tasks.
    arXiv preprint arXiv:2307.02477, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WWZ^+ [22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and
    Juanzi Li. Finding skill neurons in pre-trained transformer-based language models.
    arXiv preprint arXiv:2211.07349, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WXXZ [23] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei
    Zhou. New bounds for matrix multiplication: from alpha to omega, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WZ [16] David P Woodruff and Peilin Zhong. Distributed low rank approximation
    of implicit functions of a matrix. In 2016 IEEE 32nd International Conference
    on Data Engineering (ICDE), pages 847–858\. IEEE, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WZD^+ [20] Ruosong Wang, Peilin Zhong, Simon S Du, Russ R Salakhutdinov, and
    Lin Yang. Planning with general objective functions: Going beyond total rewards.
    Advances in Neural Information Processing Systems, 33:14486–14497, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XBK^+ [15] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural
    image caption generation with visual attention. In International conference on
    machine learning, pages 2048–2057\. PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLH^+ [23] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik
    Cambria. Are large language models really good logical reasoners? a comprehensive
    evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XQP^+ [22] Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, and Hongyuan
    Mei. Hidden state variability of pretrained language models can guide computation
    reduction for transfer learning. arXiv preprint arXiv:2210.10041, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XSS [21] Zhaozhuo Xu, Zhao Song, and Anshumali Shrivastava. Breaking the linear
    iteration cost barrier for some well-known conditional gradient methods using
    maxip data-structures. Advances in Neural Information Processing Systems, 34:5576–5589,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XSS [23] Zhaozhuo Xu, Zhao Song, and Anshumali Shrivastava. A tale of two efficient
    value iteration algorithms for solving linear mdps with large action space. In
    AISTATS, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XZZ [18] Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: Generative networks
    with metric embeddings. Advances in neural information processing systems, 31,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YKM [20] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view
    on implicit bias in training linear neural networks. arXiv preprint arXiv:2010.02501,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZAG^+ [23] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia,
    and Lidong Bing. M3exam: A multilingual, multimodal, multilevel benchmark for
    examining large language models. arXiv preprint arXiv:2306.05179, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZDL^+ [23] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong
    Bing. Sentiment analysis in the era of large language models: A reality check.
    arXiv preprint arXiv:2305.15005, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZG [19] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized
    deep neural networks. Advances in neural information processing systems, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zha [22] Lichen Zhang. Speeding up optimizations via data structures: Faster
    search, sample and maintenance. Master’s thesis, Carnegie Mellon University, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZHDK [23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer:
    Accelerating transformers via kernel density estimation. arXiv preprint arXiv:2302.02451,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZHL^+ [23] Eric Zelikman, Qian Huang, Percy Liang, Nick Haber, and Noah D Goodman.
    Just one byte (per gradient): A note on low-bandwidth decentralized language model
    finetuning using shared randomness. arXiv preprint arXiv:2306.10015, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZKV^+ [20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon
    Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good
    for attention models? Advances in Neural Information Processing Systems, 33:15383–15393,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZPD^+ [20] Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song,
    and Sanjeev Arora. Over-parameterized adversarial training: An analysis overcoming
    the curse of dimensionality. Advances in Neural Information Processing Systems,
    33:679–688, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZPD^+ [23] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man
    Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language
    models. arXiv preprint arXiv:2305.16934, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZPGA [23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers
    parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZRG^+ [22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,
    Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt:
    Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZWB^+ [21] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster,
    and Sham Kakade. The benefits of implicit regularization from sgd in least squares
    problems. Advances in neural information processing systems, 34:5456–5468, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZZ [23] Ruizhe Zhang and Xinzhi Zhang. A hyperbolic extension of kadison-singer
    type results. In ICALP, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
