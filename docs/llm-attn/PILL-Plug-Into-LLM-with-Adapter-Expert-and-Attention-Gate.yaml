- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:10'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: PILL:Plug Into LLM with Adapter Expert and Attention Gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02126](https://ar5iv.labs.arxiv.org/html/2311.02126)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fangyuan Zhang¹  Tingting Liang¹  Zhengyuan Wu¹  Yuyu Yin¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹ School of Computer Science and Technology Yuyu Yin is the corresponding author
       Hangzhou Dianzi University    China.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Due to the remarkable capabilities of powerful Large Language Models (LLMs)
    in effectively following instructions, there has been a growing number of assistants
    in the community to assist humans. Recently, significant progress has been made
    in the development of Vision Language Models (VLMs), expanding the capabilities
    of LLMs and enabling them to execute more diverse instructions. However, it is
    foreseeable that models will likely need to handle tasks involving additional
    modalities such as speech, video, and others. This poses a particularly prominent
    challenge of dealing with the complexity of mixed modalities. To address this,
    we introduce a novel architecture called PILL: Plug Into LLM with adapter expert
    and attention gate to better decouple these complex modalities and leverage efficient
    fine-tuning. We introduce two modules: Firstly, utilizing Mixture-of-Modality-Adapter-Expert
    to independently handle different modalities, enabling better adaptation to downstream
    tasks while preserving the expressive capability of the original model. Secondly,
    by introducing Modality-Attention-Gating, which enables adaptive control of the
    contribution of modality tokens to the overall representation. In addition, we
    have made improvements to the Adapter to enhance its learning and expressive capabilities.
    Experimental results demonstrate that our approach exhibits competitive performance
    compared to other mainstream methods for modality fusion. For researchers interested
    in our work, we provide free access to the code and models at [https://github.com/DsaltYfish/PILL](https://github.com/DsaltYfish/PILL).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The long-term goal of artificial intelligence is to enable it to utilize knowledge
    in a human-like manner for tasks such as reasoning, thinking, analysis, and decision-making.
    With the remarkable instruction-following ability and astonishing comprehension
    of human language exhibited by large language models [[31](#bib.bib31), [3](#bib.bib3),
    [6](#bib.bib6)], Universal Visual Language Models (VLMs) [[32](#bib.bib32), [5](#bib.bib5),
    [18](#bib.bib18)] have made significant progress in the field of AI’s multimodal
    capabilities. In order to achieve modular processing of multimodal tasks, recent
    VLMs primarily employ visual prompt generators to provide visual information to
    large language models, showcasing impressive zero-shot performance across various
    visual tasks [[12](#bib.bib12), [19](#bib.bib19), [8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: However, despite having modality information aligned with language, the internal
    structure of the underlying large language models still lacks the ability to process
    entangled modalities, resulting in most VLMs still struggling to comprehend complex
    multimodal prompts. One straightforward approach is to fully fine-tune the large
    language model or utilize the Parameter-Efficient Fine-Tuning (PEFT) method to
    enable the LLMs to learn how to handle other modal information. Yet, this may
    result in the mixture of multiple modality information, leading to interference
    with the original knowledge learned by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/61ace491ebd5f13533fb870cd0199169.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison between recent VLMs methods and our approach is shown
    in Figure. (a) represents the recent VL-LLMs method. The Processing Module represents
    the modules used for image processing, including ViT, Qformer, Resampler, and
    others. These modules are employed to process image information and transfer it
    to the LLMs. (b) represents our approach. We keep the image processing modules
    consistent with other methods and introduce a dedicated module within the LLMs
    to handle image information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose PILL: Plug Into LLM with Adapter Expert and Attention
    Gate, to address the aforementioned issues. Our method consists of three modules.
    Firstly, inspired by the Mixture-of-Modality-Experts (MoME) approach [[1](#bib.bib1),
    [23](#bib.bib23)], we employ the Mixture-of-Modality-Adapter-Expert (MoMAE) to
    handle different modality inputs. In our experimental setup, MoMAE includes the
    V-Adapter and L-Adapter, which respectively handle the visual and textual modalities.
    Secondly, to ensure that the model produces results consistent with the original
    language model during initial training, we introduce the Modality-Attention-Gating
    (MAG) module. This stabilizes the training process and prevents visual information
    from interfering with the LLM’s text modeling phase. Thirdly, recognizing that
    an enhanced adapter can lead to improved performance, we refine the classical
    adapter by introducing the GLU-Adapter.'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure stable training of our model, we adopt a two-stage training approach.
    In the first stage, we train the projection layer and V-Adapter separately to
    align visual and textual information, and enable the LLM to learn to process visual
    knowledge. In the second stage, we unlock all the proposed modules for downstream
    fine-tuning. Throughout both stages, the LLM and VPG module remain frozen. A notable
    advantage of our training method is its efficiency in training since we only train
    the inserted modules. Our method can be efficiently trained on a single A6000
    GPU, allowing for rapid training completion.
  prefs: []
  type: TYPE_NORMAL
- en: The experimental results demonstrate that our model achieves competitive performance
    compared to other methods. On the ScienceQA dataset, our model achieves a 1.4%
    accuracy improvement over models of equivalent scale. Compared to the best-performing
    LLaMA-based model, we achieve a 0.31% accuracy improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose PILL, which addresses modality-specific information processing using
    MoMAE, dynamically adjusts modality injection into the LLM with MAG, and enhances
    performance with an improved adapter structure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experimental results demonstrate that PILL exhibits superior efficiency and
    competitive performance compared to existing multimodal LLMs. It also showcases
    the significant potential for handling multimodal instructions while maintaining
    satisfactory performance on general VQA benchmarks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PILL demonstrates superior training and computational efficiency, as our experiments
    can be completed on a single A6000 GPU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Multi-Modal Pre-Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large-scale pretraining has played a crucial role in the field of multimodal
    learning. Methods such as VL-BERT[[36](#bib.bib36)], ViL-BERT[[26](#bib.bib26)],
    MCAN[[46](#bib.bib46)], LXMERT[[37](#bib.bib37)], and Rosita[[7](#bib.bib7)] extract
    key information from images using object detectors and process it together with
    text in transformer[[40](#bib.bib40)] blocks. Following the impressive success
    of CLIP[[34](#bib.bib34)] in image-text retrieval, contrastive learning has gained
    attention in the multimodal domain. Methods like ALBEF[[21](#bib.bib21)], BLIP[[20](#bib.bib20)],
    VLMO[[1](#bib.bib1)], BEiT-3[[41](#bib.bib41)], and CoCa[[45](#bib.bib45)] utilize
    contrastive learning to align image and text features, demonstrating the benefits
    of such alignment. Subsequently, with advancements in large-scale model techniques,
    methods like PaLi[[4](#bib.bib4)], PaLM[[5](#bib.bib5)], KOSMOS[[18](#bib.bib18),
    [33](#bib.bib33), [29](#bib.bib29)], BLIP-2[[19](#bib.bib19)] build upon ClipCap[[30](#bib.bib30)]
    by incorporating features from images specifically processed by ViT[[9](#bib.bib9)]
    as prompts in the LM’s input. More recently, with the rise of LLMs, researchers
    have focused on leveraging the powerful capabilities of LLMs and combining them
    with visual information. Mini-GPT4[[50](#bib.bib50)] and LLaVA[[25](#bib.bib25),
    [24](#bib.bib24)] have discovered that a projection layer can effectively project
    visual information into the textual space of LLMs, and they train only this projection
    layer using large-scale pretraining data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Multi-Modal Instruction-Tuning in PEFT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address the high cost associated with full fine-tuning, the Parameter-Efficient
    Fine-Tuning (PEFT) technique has emerged as an alternative. We highlight three
    commonly used PEFT techniques and discuss their applications in various methods.
    Firstly, the Adapter[[16](#bib.bib16)] approach has been explored in VL-Adapter[[43](#bib.bib43)].
    Extensive experiments have demonstrated the advantages of adapters over other
    PEFT methods in the multimodal setting. MAGMA[[11](#bib.bib11)] introduces adapters
    within the LM based on a frozen model. LAVIN[[28](#bib.bib28)] adopts a similar
    approach by treating single-modal and multimodal tasks as separate tasks and utilizing
    MMA (Mixture-of-Modality Adapter) for each task. The key distinction between our
    MoMAE approach and LAVIN is that we focus on modality tokens, while LAVIN focuses
    on tasks. Next, the application of LoRA[[17](#bib.bib17)] in the multimodal context
    is noteworthy. MultiModal-GPT[[14](#bib.bib14)] adopts a similar architecture
    to Flamingo[[12](#bib.bib12)] and incorporates LoRA for LM fine-tuning. Visual-ChatGLM[[10](#bib.bib10)],
    mPLUG-Owl[[42](#bib.bib42)], and LAMM[[44](#bib.bib44)] also employ LoRA for LM
    fine-tuning after pretraining with VPG (Visual Prompt Generation). Finally, the
    prefix-tuning[[22](#bib.bib22)] technique, exemplified by LLAMA-Adapter[[48](#bib.bib48),
    [13](#bib.bib13)], involves adding the image representation from the visual encoder
    to the prefix tokens and processing it along with the text tokens in the LM layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dcd9b2d53e7f9e1fc1f7982b45835ffd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The overview of the architecture of PILL and two module of the PILL:
    Mixture-of-Modality-Adapter-Expert (MoMAE) and Modality-Attention-Gating (MAG).
    In PILL, the novel MoMAE are employed to handle tokens from different modalities.
    During finetuning, MAG is used for coordinate the weights of other modalities'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will present the technical details of PILL, including the
    model architecture (Sec.[3.1](#S3.SS1 "3.1 Model Architecture ‣ 3 Method ‣ PILL:Plug
    Into LLM with Adapter Expert and Attention Gate")), Mixture-of-Modality-Adapter-Expert
    (Sec.[3.2](#S3.SS2 "3.2 Mixture-of-Modality-Adapter-Expert ‣ 3 Method ‣ PILL:Plug
    Into LLM with Adapter Expert and Attention Gate")) for handling different modalities
    of information, Modality-Attention-Gating (Sec.[3.3](#S3.SS3 "3.3 Modality-Attention-Gating
    ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and Attention Gate")) enables
    adaptive control of the contribution of modality tokens to the overall representation,
    and the SwiGLU-Adapter improves in adapter learning and expressive capability
    (Sec.[3.4](#S3.SS4 "3.4 SwiGLU-Adapter ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter
    Expert and Attention Gate")). After a detailed introduction of these modules,
    we will proceed to discuss the training process and objectives of the model (Sec.[3.5](#S3.SS5
    "3.5 Stagewise Training ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate")). An overview of the PILL framework is depicted in Figure [2](#S2.F2
    "Figure 2 ‣ 2.2 Multi-Modal Instruction-Tuning in PEFT ‣ 2 Related Work ‣ PILL:Plug
    Into LLM with Adapter Expert and Attention Gate").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a sample containing a set of images and text, we first process each image
    in the image set using the Q-former module from BLIP-2 to extract image features.
    Q-former takes a fixed number of $K$, which represents the visual prompts for
    the $j$, which represents the $i$. We define that $H^{v}=\{v_{11},...,v_{1K},...,v_{j1},...,v_{jK}\}$
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Mixture-of-Modality-Adapter-Expert
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We propose a universal multimodal module, named MoMAE, which is inspired by
    MoME [[1](#bib.bib1), [23](#bib.bib23)] and Adapter. MoMAE introduces Mixture-of-Modality-Adapter-Expert
    as an alternative solution to the standard Transformer feed-forward networks for
    encoding different modalities. As is shown in [2](#S2.F2 "Figure 2 ‣ 2.2 Multi-Modal
    Instruction-Tuning in PEFT ‣ 2 Related Work ‣ PILL:Plug Into LLM with Adapter
    Expert and Attention Gate")(b), after leveraging the output vectors from the previous
    layer and adapter-based multi-head self-attention (MSA), MoMAE are able to capture
    and process modality-specific information by switching to different modality adapter
    experts. We use vision adapter expert for encoding images $H^{v}$, which can be
    formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{H_{out}^{v}}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{H_{out}^{t}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Previous research has demonstrated that knowledge is preserved in the feed-forward
    networks (FFNs) within the Transformer. Therefore, we employ MoMAE to enable LLM
    to learn knowledge for handling visual features. Additionally, in our experiments,
    we observed that the variance of visual tokens is typically significantly larger
    than that of textual tokens. Such distribution inconsistency can potentially lead
    to bad training results. Employing a dedicated module to handle visual tokens
    may be a prudent choice. Furthermore, due to the adoption of the Adapter strategy
    for learning, the weights of the original LLaMA are retained, allowing the alignment
    of visual and textual modalities in the same representation space. The alignment
    has been proved to be crucial for multi-modal tasks in previous work.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Modality-Attention-Gating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regarding modal fusion, both Prefix-Tuning and Cross-Attn methods have demonstrated
    excellent performance. In the experimental findings of Lynx [[47](#bib.bib47)],
    Prefix-Tuning generally achieves better performance, while the Cross-Attn model
    may require more hyper-parameter searching to achieve superior results. Is it
    possible to combine the advantages of these two methods? To address this, we propose
    MAG: Modality-Attention-Gating. Specifically, we apply MAG to filter and adjust
    the attention weights of other modality information, aiming to allow the modal
    information to play its respective role in different transformer layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify $G:\mathbb{R}^{d_{dim}}\rightarrow\mathbb{R}^{n_{heads}}$. Given
    vision modality tokens, our MAG module is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{v}(H)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle G_{t}(H)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle S(H)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle MAG(H)$ |  |'
  prefs: []
  type: TYPE_TB
- en: In attention layer, $H$. $G_{v}(H)$ is to multiply the attention heads of each
    modality by a gating weight, which is initialized to 0\. Note that for every visual
    token within the same layer, they undergo the same gate. This allows visual information
    to have distinct effects across different layers while filtering out irrelevant
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Based on previous empirical experience in LM, modules closer to the input layer
    primarily focus on language modeling, while modules near the output layer are
    responsible for knowledge injection. The pure use of Prefix-Tuning might introduce
    interference in the language modeling stage due to the injection of other modal
    information, as observed in the experiments with Flamingo [[12](#bib.bib12)],
    where the gate weights near the input approach zero. On the other hand, the benefit
    of Prefix-Tuning lies in injecting different information at different transformer
    layers, effectively adapting the modal information at each layer. This may be
    a contributing factor to the generally higher performance of Prefix-Tuning compared
    to Cross-Attn. Therefore, based on the above assumptions, with the inclusion of
    the MAG and MoMAE module, the model architecture resembles Prefix-Tuning, while
    the details resemble Cross-Attn. In fact, our results of gating weight is similar
    to Flamingo [[12](#bib.bib12)]
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 SwiGLU-Adapter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3759c132cd3e42848d86a4f8ee99c294.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Architecture of SwiGLU-Adapter'
  prefs: []
  type: TYPE_NORMAL
- en: 'The classical Adapter architecture consists of a downsampling operation, activation
    function, upsampling, and residual connection. With the introduction of the Gated
    Linear Unit(GLU) [[35](#bib.bib35)] structure, it has gained wide application
    in LLMs such as LLaMA [[38](#bib.bib38), [39](#bib.bib39)] and PaLM [[5](#bib.bib5)].
    In a corresponding manner, can we also transform the architecture of Adapter into
    GLU? To address this, we propose the SwiGLU-Adapter. As shown in [3](#S3.F3 "Figure
    3 ‣ 3.4 SwiGLU-Adapter ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate"), we modify the structure of the Adapter by incorporating two
    downsampling operations, where one of the downsampling outputs is passed through
    an activation function. The outputs are then multiplied element-wise and followed
    by an upsampling operation. We represent this transformation using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H_{down1}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H_{down2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H_{down}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H_{up}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H_{out}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 3.5 Stagewise Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our training process consists of the classic pre-training and downstream task
    fine-tuning stages, where different modules are trained at each stage. For training
    objective, both stage train on loss which is an auto-regressive manner. Given
    input $X_{text}$ and $X_{answer}$, we we express our training objective using
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Here, $p_{t}$, $\theta$ with trainable parameters $\theta$ in both training
    stage. The following sections will provide a detailed description of each stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 1: Pre-training for Modality Alignment. In stage 1, similar to VLMO [[1](#bib.bib1),
    [23](#bib.bib23)] and LLaVA [[25](#bib.bib25)], we set $\theta=\{A_{v},P\}$ is
    visual adapter and $P$ in the last layer does not have a connection to the training
    objective, rendering it untrainable. Therefore, we only utilize the V-Adapter
    as a T-Adapter in this case, effectively reversing the positional information
    of the input VL (Visual-Language) tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 2: Downstream Task Fine-tuning. In stage 2, we aim to train the model
    to respond to instructions and images, resembling a real AI assistant. Therefore,
    we train all the parameters of the Adapters, attn-gates, and projections, where
    $\theta=\{A_{v},A_{t},A_{attn},G,P\}$. For quantitative experimental analysis,
    we conduct ablation experiments on the ScienceQA [[27](#bib.bib27)], a small-scale
    dataset. To achieve generalized instruction following, we train the model on the
    LLaVA1.5 665k [[24](#bib.bib24)] dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Method | #T-Param | Subject | Context Modality | Grade | Average |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | SOC | LAN | TXT | IMG | NO | G1-6 | G7-12 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero- & few-shot methods |  |'
  prefs: []
  type: TYPE_TB
- en: '| Human [[27](#bib.bib27)] | - | 90.23 | 84.97 | 87.48 | 89.60 | 87.50 | 88.10
    | 91.59 | 82.42 | 88.40 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 [[27](#bib.bib27)] | - | 74.64 | 69.74 | 76.00 | 74.44 | 67.28 |
    77.42 | 76.80 | 68.89 | 73.97 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 (CoT) [[27](#bib.bib27)] | - | 75.44 | 70.87 | 78.09 | 74.68 | 67.43
    | 79.93 | 78.23 | 69.68 | 75.17 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 [[32](#bib.bib32)] | - | 84.06 | 73.45 | 87.36 | 81.87 | 70.75 | 90.73
    | 84.69 | 79.10 | 82.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Representative & SoTA models |  |'
  prefs: []
  type: TYPE_TB
- en: '| UnifiedQA [[27](#bib.bib27)] | 223M | 71.00 | 76.04 | 78.91 | 66.42 | 66.53
    | 81.81 | 77.06 | 68.82 | 74.11 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-CoT[Base] [[49](#bib.bib49)] | 223M | 87.52 | 77.17 | 85.82 | 87.88 |
    82.90 | 86.83 | 84.65 | 85.37 | 84.91 |'
  prefs: []
  type: TYPE_TB
- en: '| MM-CoT[Large] [[49](#bib.bib49)] | 738M | 95.91 | 82.00 | 90.82 | 95.26 |
    88.80 | 92.89 | 92.44 | 90.31 | 91.68 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-based methods |  |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-Adapter [[48](#bib.bib48)] | 1.8M | 84.37 | 88.30 | 84.36 | 83.72 |
    80.32 | 86.90 | 85.83 | 84.05 | 85.19 |'
  prefs: []
  type: TYPE_TB
- en: '| LaVIN [[28](#bib.bib28)] | 5.4M | 89.88 | 94.49 | 89.82 | 88.95 | 87.61 |
    91.85 | 91.45 | 89.72 | 90.83 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA [[25](#bib.bib25)] | 13B | 90.36 | 95.95 | 88.00 | 89.49 | 88.00 |
    90.66 | 90.93 | 90.90 | 90.92 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-SciTune [[15](#bib.bib15)] | 13B | 89.30 | 95.61 | 87.00 | 93.08 |
    86.67 | 91.75 | 84.37 | 91.30 | 90.03 |'
  prefs: []
  type: TYPE_TB
- en: '| PILL (ours) | 45M | 90.36 | 95.84 | 89.27 | 89.39 | 88.65 | 91.71 | 92.11
    | 89.65 | 91.23 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison on ScienceQA test set. Question classes: NAT = natural
    science, SOC = social science, LAN = language science, TXT = text context, IMG
    = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12\. #T-Params
    denotes that the number of trainable parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CC595K. CC595K was filtered by LLaVA [[25](#bib.bib25), [24](#bib.bib24)]. LLaVa
    extracted noun phrases from each caption in the entire CC3M dataset and calculated
    the frequency of each unique noun phrase. Noun phrases with a frequency below
    3 were skipped, as they typically represent rare combinations of concepts and
    attributes. For noun phrases with a frequency exceeding 100, a random subset of
    100 samples was selected from all captions. This resulted in approximately 595k
    image-text pairs. We use CC595K to pretrain our model.
  prefs: []
  type: TYPE_NORMAL
- en: ScienceQA. ScienceQA [[27](#bib.bib27)] consists of 21,000 data samples, including
    multiple-choice questions with multimodal content. It covers 3 subjects, 26 topics,
    127 categories and 379 skills. We utilized the training split of the ScienceQA
    dataset, which comprised 12,726 samples, to further optimize our model. Additionally,
    we employed the test split, consisting of 4,241 samples, to conduct an initial
    evaluation of our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the image feature extraction part, we utilized the Q-former module from
    BLIP2 [[19](#bib.bib19)], which has been thoroughly pre-trained on Flan-T5-XXL [[6](#bib.bib6)].
    For the LLM, we opted for the LLaMA-2-chat [[39](#bib.bib39)] model. The default
    setting for the intermediate hidden layer dimension in our adapter is 32\. We
    employed AdamW as the optimizer with a cosine learning rate decay strategy.
  prefs: []
  type: TYPE_NORMAL
- en: During the pre-training phase, we solely trained the projection layer and v-adapter
    for 3 epochs, with a learning rate of 1e-3\. The language model input length was
    set to 128, and the batch size was set to 32\. In the fine-tuning stage, we trained
    the projection layer, v-adapter, t-adapter, attn-adapter, and attn-gate layer
    for 20 epochs, with a learning rate of 2e-3\. The language model input length
    was increased to 512, and the batch size was reduced to 4.
  prefs: []
  type: TYPE_NORMAL
- en: Under our experimental settings, the total number of trainable parameters amounts
    to 45M. The training of our model was conducted on a single A6000, which is a
    hardware configuration widely acceptable among researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.3.1 Results on ScienceQA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Table [1](#S4.T1 "Table 1 ‣ 4 Experiment ‣ PILL:Plug Into LLM with
    Adapter Expert and Attention Gate"), we compare the performance of several existing
    methods on the ScienceQA [[27](#bib.bib27)]. Firstly, we consider zero-shot or
    few-shot approaches. Human performance in question answering achieves an accuracy
    of 88.40%. Remarkably, GPT-4 achieves an accuracy of 82.69% without any specific
    training on ScienceQA. Among other LLMs, MM-CoT[Large] [[49](#bib.bib49)] achieves
    an accuracy of 91.68%. We speculate that this performance difference may be due
    to variations in the base models. Notably, there is a significant discrepancy
    in the NAT and SOC metrics between MM-COT and llama-based models. Furthermore,
    it is expected that MM-COT performs exceptionally well, as it focuses on the chain-of-thought,
    a technique that has been proven to significantly enhance LLM capabilities. Therefore,
    the outstanding performance of MM-COT comes as no surprise. Among the LLaMA-based
    methods, our method consistently ranks first or second across all metrics, ultimately
    achieving the best overall performance in terms of average metrics. Our method
    achieves comparable performance to the state-of-the-art methods, even without
    employing any extravagant techniques. In terms of parameter count, our method
    significantly reduces training overhead compared to LLaVA as we only need to train
    the adapter components of the model. Despite our method having significantly more
    training parameters compared to LaVIN, our training speed is faster due to the
    absence of gradient backpropagation to ViT during training. In fact, under the
    same experimental settings, our method requires only 90% of the training time
    compared to LaVIN. Overall, these results validate the efficiency and design of
    PILL.
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings | w/o pretrain | w pretrain |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-Adapter [[48](#bib.bib48)] | 85.19 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA [[25](#bib.bib25)] | 85.81 | 89.84 |'
  prefs: []
  type: TYPE_TB
- en: '| LaVIN [[28](#bib.bib28)] | 89.41 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-SciTune [[15](#bib.bib15)] | - | 86.11 |'
  prefs: []
  type: TYPE_TB
- en: '| PILL(ours) | 90.24 | 91.23 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results of existing multimodal LLMs in 7B model scaling on ScienceQA
    test set.'
  prefs: []
  type: TYPE_NORMAL
- en: In Table [2](#S4.T2 "Table 2 ‣ 4.3.1 Results on ScienceQA ‣ 4.3 Experimental
    Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert and Attention
    Gate"), we compare the performance of PILL with other multimodal methods using
    the LLaMA-7B model specification. Without pretraining, LLaMA-Adapter[[48](#bib.bib48)],
    which was one of the pioneering methods to employ PEFT techniques, enabled the
    LLM to possess visual recognition capabilities and achieved impressive results
    on ScienceQA[[27](#bib.bib27)] at that time. LAVIN[[28](#bib.bib28)] further improved
    upon this performance by approximately 4%. We speculate that the Adapter structure
    may be particularly suitable for VL-LLM, as evidenced by the experimental results
    of VL-Adapter[[43](#bib.bib43)]. On the other hand, since LLaVA[[25](#bib.bib25)]
    requires full fine-tuning, a well-initialized visual projection layer is likely
    to stabilize the subsequent fine-tuning process. As a result, LLaVA achieves a
    4% improvement with pretraining. However, SciTune[[15](#bib.bib15)], despite its
    focus on analyzing figures and tables in scientific papers, performs poorly on
    some common-sense VQA tasks. Importantly, our method consistently achieves the
    best results regardless of whether pretraining is employed, demonstrating the
    effectiveness of our proposed approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25c8504a2831ea4b1fe5fa060e872969.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Evolution of the absolute value of the attention gate at different
    layers of PILL'
  prefs: []
  type: TYPE_NORMAL
- en: As is shown in figure [4](#S4.F4 "Figure 4 ‣ 4.3.1 Results on ScienceQA ‣ 4.3
    Experimental Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate"), We also plotted the absolute values of the MAG at different
    layers of the PILL model, which consists of 32 LM layers. It appears that all
    layers of the frozen LM stack utilize visual information to some extent. We also
    observed that the absolute values tend to increase with depth. We attribute this
    to the roles played by the LLM at different layers. In the layers closer to the
    input, the LLM focuses on language modeling and does not heavily rely on visual
    information. However, as the layers get closer to the output, the LLM has accumulated
    sufficient knowledge and needs to extract information from the images. As a result,
    the absolute values of the tanh gate activations start from near-zero values near
    the input layers and rapidly increase, approaching 1 near the output layers.
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings | NAT | SOC | LAN | TXT | IMG | NO | G1-6 | G7-12 | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| w/o pretrain |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + L-A | 89.25 | 91.79 | 87.00 | 88.51 | 85.57 | 89.48 | 89.79 | 88.13
    | 89.20 (+0.00) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + L-A | 88.99 | 92.80 | 88.27 | 88.22 | 86.07 | 90.38 | 90.49
    | 88.00 | 89.60 (+0.40) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + G-A | 89.83 | 90.55 | 88.36 | 89.20 | 86.51 | 89.97 | 90.60
    | 87.80 | 89.60 (+0.40) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + SG-A | 90.01 | 93.36 | 88.18 | 89.15 | 87.16 | 90.59 | 91.12
    | 88.66 | 90.24 (+1.04) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + SG-A + 0.1 wrong answer | 90.19 | 93.36 | 89.09 | 89.30 | 87.41
    | 91.22 | 91.45 | 88.99 | 90.57 (+1.37) |'
  prefs: []
  type: TYPE_TB
- en: '| w pretrain epochs=1 |'
  prefs: []
  type: TYPE_TB
- en: '| MAG + SG-A | 90.36 | 94.38 | 87.45 | 89.83 | 88.10 | 89.83 | 90.90 | 89.65
    | 90.45 (+1.25) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + SG-A | 89.96 | 95.05 | 88.82 | 89.30 | 88.25 | 90.73 | 91.48
    | 89.39 | 90.73 (+1.53) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + SG-A + 0.1 wrong answer | 89.43 | 95.61 | 89.09 | 88.66 | 87.85
    | 91.01 | 91.34 | 89.39 | 90.64 (+1.44) |'
  prefs: []
  type: TYPE_TB
- en: '| w pretrain epochs=3 |'
  prefs: []
  type: TYPE_TB
- en: '| MoMAE + MAG + SG-A | 90.36 | 95.84 | 89.36 | 89.93 | 88.80 | 91.15 | 92.33
    | 89.32 | 91.23 (+2.05) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablation studies on ScienceQA test set. “L-a” denotes Linear Adapter.
    “G-a” denotes GELU Adapter. “SG-A” denotes SwiGLU Adapter. “0.1 wrong answer”
    indicates that during the training process, we randomly replace the ground truth
    answer with another option with a probability of 0.1.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation study. In Table  [3](#S4.T3 "Table 3 ‣ 4.3.1 Results on ScienceQA ‣
    4.3 Experimental Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert
    and Attention Gate"), we present the contributions of various components and training
    processes of PILL on the ScienceQA test set. MoMAE and Linear Adapter serve as
    our baselines, achieving an accuracy of 89.20%. Building upon this baseline, we
    incorporate the MAG module, resulting in a performance improvement of 0.4%. To
    investigate the impact of different activation functions on the Adapter, we initially
    apply the GELU activation function, which does not lead to any improvement. However,
    when we replace the activation function with our proposed SwiGLU activation function,
    we achieve a further improvement of 0.64%. Additionally, inspired by [[2](#bib.bib2)],
    we explore the effect of LLM hallucination on our model. During training, we randomly
    replace the answer with another option with a probability of 0.1, leading to an
    additional improvement of 0.33%.
  prefs: []
  type: TYPE_NORMAL
- en: To further enhance PILL’s ability to recognize images, we conducted image captioning
    training on the model. Initially, we attempted to remove the V-Adapter, training
    only a projection layer during pretraining and using a single adapter for fine-tuning.
    This approach achieved an accuracy of 90.45%. When incorporating the MoMAE method,
    we observed an additional improvement of 0.28%. We also explored the use of random
    wrong answers but encountered a decline in performance. We believe this is due
    to the lack of pretraining, which could result in insufficient image recognition
    capabilities and lead to hallucinations, similar to the findings in MM-COT[[49](#bib.bib49)].
    After pretraining, the model exhibits improved image understanding, partially
    mitigating the hallucination issue. Therefore, employing the random wrong answer
    approach at this stage resulted in a decline in performance. Finally, by increasing
    the number of pretraining epochs to 3, we achieved the best results.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to our utilization of LLM as the underlying model, PILL inevitably inherits
    some of the limitations of LLM. As a result, the image-to-text generation process
    may yield unsatisfactory results, including erroneous knowledge and reasoning
    or hallucinations. Additionally, although our work only involves the fusion of
    the image modality, our method can naturally incorporate other modalities such
    as speech. In future work, we aspire to create a genuine multimodal AI assistant
    by further exploring the integration of multiple modalities and scaling up the
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose the PILL method to address the challenge of decoupling
    complex multimodal interactions. We leverage the MoMAE (Mixture-of-Modality-Adapter-Expert)
    module, which is specifically designed to handle image tokens, and the MAG (Modality-Attention-Gating)
    module for dynamic fusion of modalities. Additionally, we introduce the SwiGLU-Adapter
    to further enhance performance. Our experimental results demonstrate the effectiveness
    of our proposed method. With the advantage of fine-tuning only a small number
    of parameters, our approach offers a cost-effective solution that can be trained
    on a single A6000 GPU. This enables us to achieve visual language instruction
    following capabilities while maintaining efficient computation and training speed.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti
    Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. VLMo: Unified vision-language
    pre-training with mixture-of-modality-experts. In Advances in Neural Information
    Processing Systems, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. Let there be a
    clock on the beach: Reducing object hallucination in image captioning. In Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1381–1390,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E
    Hinton. Big self-supervised models are strong semi-supervised learners. Advances
    in neural information processing systems (NeurIPS), 33:22243–22255, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski,
    Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.
    Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
    language models. arXiv preprint arXiv:2210.11416, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and
    Jun Yu. Rosita: Enhancing vision-and-language semantic alignments via cross- and
    intra-modal knowledge integration. In Proceedings of the 29th ACM International
    Conference on Multimedia, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,
    Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards
    general-purpose vision-language models with instruction tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    and Jie Tang. Glm: General language model pretraining with autoregressive blank
    infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 320–335, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu,
    and Anette Frank. Magma–multimodal augmentation of generative models through adapter-based
    finetuning. arXiv preprint arXiv:2112.05253, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Jean-Baptiste Alayrac et al. Flamingo: a visual language model for few-shot
    learning. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou,
    Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter
    v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,
    Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and
    language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Sameera Horawalavithana, Sai Munikoti, Ian Stewart, and Henry Kvinge.
    Scitune: Aligning large language models with scientific multimodal instructions,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient
    transfer learning for nlp. In International Conference on Machine Learning, pages
    2790–2799\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming
    Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not
    all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    In ICML, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping
    language-image pre-training for unified vision-language understanding and generation.
    In International Conference on Machine Learning, pages 12888–12900\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming
    Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation
    learning with momentum distillation. Advances in neural information processing
    systems, 34:9694–9705, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yunshui Li, Binyuan Hui, ZhiChao Yin, Min Yang, Fei Huang, and Yongbin
    Li. Pace: Unified multi-modal dialogue pre-training with progressive and compositional
    experts. arXiv preprint arXiv:2305.14839, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines
    with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining
    task-agnostic visiolinguistic representations for vision-and-language tasks. In
    Advances in Neural Information Processing Systems, pages 13–23, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun
    Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal
    reasoning via thought chains for science question answering. In The 36th Conference
    on Neural Information Processing Systems (NeurIPS), 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong
    Ji. Cheap and quick: Efficient vision-language instruction tuning for large language
    models. arXiv preprint arXiv:2305.15023, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang,
    Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: A multimodal
    literate model. arXiv preprint arXiv:2309.11419, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image
    captioning. arXiv preprint arXiv:2111.09734, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] OpenAI. Chatgpt, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] OpenAI. Gpt-4 technical report, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming
    Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the
    world. arXiv preprint arXiv:2306.14824, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In International
    conference on machine learning, pages 8748–8763\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng
    Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv
    preprint arXiv:1908.08530, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations
    from transformers. arXiv preprint arXiv:1908.07490, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu,
    Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image
    as a foreign language: Beit pretraining for all vision and vision-language tasks.
    arXiv preprint arXiv:2208.10442, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
    Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers
    large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Mohit Bansal Yi-Lin Sung, Jaemin Cho. Vl-adapter: Parameter-efficient
    transfer learning for vision-and-language tasks. In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai
    Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et al. Lamm: Language-assisted
    multi-modal instruction-tuning dataset, framework, and benchmark. arXiv preprint
    arXiv:2306.06687, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini,
    and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models.
    arXiv preprint arXiv:2205.01917, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention
    networks for visual question answering. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 6281–6290, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei,
    Yuchen Zhang, and Tao Kong. What matters in training a gpt4-style language model
    with multimodal inputs? arXiv preprint arXiv:2307.02469, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. arXiv preprint arXiv:2303.16199,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex
    Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint
    arXiv:2302.00923, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:
    Enhancing vision-language understanding with advanced large language models. arXiv
    preprint arXiv:2304.10592, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
