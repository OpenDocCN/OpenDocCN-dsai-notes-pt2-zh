- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Efficient LLM Training and Serving with Heterogeneous Context Sharding among
    Attention Heads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17678](https://ar5iv.labs.arxiv.org/html/2407.17678)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xihui Lin¹, Yunan Zhang^(1∗), Suyu Ge², Barun Patra¹, Vishrav Chaudhary¹, Xia
    Song¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Microsoft, ²UIUC
  prefs: []
  type: TYPE_NORMAL
- en: '{xihlin,yunanzhang}@microsoft.com Leading authors. Xihui Lin is the major contributor
    of the kernel. Code is available is at https://github.com/linxihui/dkernel'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Existing LLM training and inference frameworks struggle in boosting efficiency
    with sparsity while maintaining the integrity of context and model architecture.
    Inspired by the sharding concept in database and the fact that attention parallelizes
    over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention
    algorithm that allocates heterogeneous context partitions for different attention
    heads to divide and conquer. S2-Attention enforces each attention head to only
    attend to a partition of contexts following a strided sparsity pattern, while
    the full context is preserved as the union of all the shards. As attention heads
    are processed in separate thread blocks, the context reduction for each head can
    thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained
    with S2-Attention can then take the KV cache reduction as free meals with guaranteed
    model quality preserve. In experiments, we show S2-Attentioncan provide as much
    as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in
    6X reduction in end-to-end training time and 10X inference latency, (2) on-par
    model training quality compared to default attention, (3)perfect needle retrieval
    accuracy over 32K context window. On top of the algorithm, we build DKernel, an
    LLM training and inference kernel library that allows users to customize sparsity
    patterns for their own models. We open-sourced DKerneland make it compatible with
    Megatron, Pytorch, and vLLM.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The emergence of transformer-based large language models (OpenAI, [2023](#bib.bib18);
    Touvron et al., [2023](#bib.bib22)) has brought about revolutionary opportunities
    to research and user experience. However, the remarkable success of these models
    across diverse applications also underscores the pressing need for training and
    serving these models economically.
  prefs: []
  type: TYPE_NORMAL
- en: The core of the transformer architecture is the self-attention mechanism. However,
    self-attention costs quadratic time and memory complexity with respect to the
    context length, making it frustratingly expensive to scale to longer context.
    For example, with 4096 context length, training Llama 2 70b on 2 trillion tokens
    Touvron et al. ([2023](#bib.bib22)) needs 23 days to finish with 2048 A100 GPUs
    Rucinski ([2024](#bib.bib21)), which is estimated to cost 2 million dollars.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff67a29dcda139a7140f7fb2c8f34346.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Speedup over FlashAttention-2\.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5383b371b0aefad9bed9c2752e1eb793.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Needle in a haystack evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Training Efficiency and long-context analysis of S2-Attention.'
  prefs: []
  type: TYPE_NORMAL
- en: During inference, the forward pass of LLMs typically involves storing the previously
    computed key and value vectors (KV Cache) to avoid recomputing. For instance,
    serving Llama 2 70b requires a staggering 43 GB of KV cache when setting the batch
    size to 16 and the context length to 2048.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our study aims to improve the training and inference efficiency of LLM. We
    starts from the observation of the sparsity nature of transformers (Wu et al.,
    [2024](#bib.bib24); Olsson et al., [2022](#bib.bib17); Ge et al., [2024](#bib.bib8)).
    As illustrated in Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient LLM
    Training and Serving with Heterogeneous Context Sharding among Attention Heads"),
    despite densely trained, the majority of attention mass still concentrates on
    a few tokens. For most cases, over $95\%$ attention mass can be recalled with
    less than $1\%$ tokens. Also, different attention heads could have dynamically
    different attention mass distribution patterns. As inference works with a sparse
    subset of contexts for each head, can we accelerate training by enforcing this
    sparsity on each head? More specifically, we want to examine how to maintain the
    model quality while maximally reducing the context load of every attention head
    in training. From a hardware perspective, as each attention head is processed
    independently by a distinct thread block, assigning a subset of the context to
    each thread block is similar to the sharding concept in database. Thus, we design
    Sparsely-Sharded (S2) Attention built on three key premises:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce4fca41947767c5477b12dfb1f89825.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of S2-Attentionwith four attention heads on a hypothetical
    GPU with 4 thread blocks. Each attention head is allocated with a shard of the
    context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'P1: In training, we can get equivalent model quality while allocating only
    a partition of the context to each attention head, as long as the union of all
    the shards equals the complete context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'P2: Following P1, attention heads should attend to diverse shards. Under the
    same compute budget, models trained with heterogeneous sparsity pattern on attention
    heads would have a better quality over those trained with homogeneous sparsity
    pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'P3: Dense layers at certain point are necessary for preserving model quality.'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, S2-Attentioncan be trained with significant fewer FLOPs while maintaining
    the model quality, including long context capabilities, compared to those trained
    with default attention. Compared to previous sparsity method (Child et al., [2019](#bib.bib2);
    Ho et al., [2019](#bib.bib9); Zaheer et al., [2020](#bib.bib27)), sharding the
    context among numerous attention head can achieve higher sparsity per head, while
    explicitly maintaining the full context. Also, the heterogeneous assignment could
    better utilize the multi-head expressiveness compared to previous homogeneous
    designs. During serving, S2-Attentioncan then take KV Cache saving as a free meal
    while having a better quality guarantee, as each attention head is originally
    trained on reduced contexts.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we provide an instantiation of S2-Attentionthat satisfies all
    the arguments. We first chunk the sequence of tokens into blocks. For an attention
    head $h$, it only attends to the blocks that are adjacent to the current token,
    and blocks whose ids are divisible by a stride after offset by $h$ . A larger
    stride means each attention head receives a smaller, and more sparse shard. The
    stride size is between 1 and number of attention head, which secure the union
    can preserve full context. Lastly, we keep the first two dense layers dense while
    the rest of layers sparse, as we find it’s crucial to maintain the initial layers
    intact.
  prefs: []
  type: TYPE_NORMAL
- en: We validate the effectiveness of S2-Attentionby evaluating the model quality
    on a wide range of tasks covering context retrieval, common sense reasoning, and
    question answering. Across benchmarks, S2-Attentionachieves on par, or even better
    performance compared to densely trained models. For training efficiency benefit,
    compared to Flash-Attention 2, S2-Attentioncan achieve as much as 8.8X wall-clock
    speed-up at 128K sequence length for 1.3B model. The boost increases as the model
    and context length scales up. On a 70B model with 64 heads, S2-Attentioncan bring
    25.3X speed-up. For inference speed-up, S2-Attentioncan achieves up to 12X speed
    up over FlashAttention-2 on a 1M context window. Lastly, we back S2-Attentionwith
    an optimized kernel library, DKernel, designed to be a plug-in-and-play replacement
    of the popular FlashAttnetion-2 and PyTorch SPDA used in most modern training
    frameworks. It allows users to freely customized the interested pattern they want
    to study in both training and inference. Lastly, we’ve integrated DKernelinto
    vLLM for easy adoption in serving.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Efficient Transformers Numerous of attempts have been made to make the training
    and serving of LLMs more efficient. FlashAttention family (Dao et al., [2022](#bib.bib5);
    Dao, [2023](#bib.bib4)) are the most widely-adopted attention acceleration framework.
    Dao et al. ([2022](#bib.bib5)) breaks down the softmax operation into smaller
    blockwise computation to reduce the IO between SRAM and HBM. Dao ([2023](#bib.bib4))
    further improve the performance by reducing the non-matmul operations, extra parallelization
    over sequence length, and better warp organization. Our implementation is based
    on the FlashAttention kernel, and leverage its parallelization over the number
    of heads. Liu et al. ([2023a](#bib.bib13)) further scales the blockwise computation
    into multi-accelerator setting by using each GPU as cache. However, these algorithm
    does not tackle the redundancy in the attention structure, which leaves great
    headroom to further improve efficiency. The FlashAttention family calculates the
    exact self-attention. On the other hand, computation-efficient attention approximation
    mainly leverages low-rank and sparsity approximation to reduce the computational
    complexity of self-attention (Child et al., [2019](#bib.bib2); Katharopoulos et al.,
    [2020](#bib.bib10); Kitaev et al., [2020](#bib.bib11); Zaheer et al., [2020](#bib.bib27);
    Beltagy et al., [2020](#bib.bib1)). Zaheer et al. ([2020](#bib.bib27)) replaces
    the one-to-all attention in transformers with sliding window attention and random
    attention. Qiu et al. ([2020](#bib.bib20)) chunks the sequence into blocks, which
    reduces FLOPs by performing attention on a larger granularity. However, many of
    these methods can’t bring wall-clock speed-up due to ignorance of the IO overhead
    Dao et al. ([2022](#bib.bib5)). Also, restricted by receptive field and the homogeneous
    sparsity patterns among attention heads, these methods under-utilize the multi-head
    structure and limit the sparsity level. Random attention on the other hand, makes
    KV cache undecidable during decoding for generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Pattern Analysis Several studies have sought to understand the roles
    individual attention heads play in the Transformer. For encoder-only models, Voita
    et al. ([2019](#bib.bib23)); Michel et al. ([2019](#bib.bib16)); Clark et al.
    ([2019](#bib.bib3)) analyze the self-attention heads in BERT Devlin et al. ([2019](#bib.bib6))
    and find a significant portion of heads attends separator tokens, next or previous
    tokens and a hybrid of them. Even heads in the same layer could have different
    patterns and impact the downstream tasks in a diverse way. Ge et al. ([2024](#bib.bib8));
    Fu ([2024](#bib.bib7)); Wu et al. ([2024](#bib.bib24)); Olsson et al. ([2022](#bib.bib17))
    study the attention head behavior for decoder-only models. They reveal most of
    the attention heads concentrate on a few tokens and categorize these patterns
    to facilitate KV Cache compression.
  prefs: []
  type: TYPE_NORMAL
- en: Infer-time KV Cache management Along with these studies, a line of inference
    optimization work emerges which evict part of the KV cache at inference based
    on attention score. (Zhang et al., [2023](#bib.bib28); Liu et al., [2023b](#bib.bib14))
    proposes an eviction policy to retain a combination of recent tokens and important
    tokens decided by frequency statistics. Xiao et al. ([2023](#bib.bib25)) proposes
    to only keep the first quarter and local contexts as they accounts for the predominantly
    amount of attention scores. Ge et al. ([2024](#bib.bib8)) studies the head level
    attention pattern at inference, and proposed to use an adaptive hybrid eviction
    policies to better present the attention pattern for each attention head. However,
    these methods cannot guarantee performance preserving, especially long context
    capabilities Li et al. ([2024](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Observation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92e6c03f63dda17419ef626a64fbfb74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Left: Attention mass sparsity in different heads during decoding
    the same sequence. Right: Sparsity in different layers. Except for the bottom
    layer, all other layers display significant attention sparsity. Experiments are
    done on Llama3-8B with different task prompts, including needle retrieval, summarization
    and math.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we present an empirical study to see what each attention head
    actually attends to at inference time. We conduct experiments on Llama-3 8B (Meta,
    [2024](#bib.bib15)) using a 2048 context length. We first start with the overall
    layer by layer pattern, then showcase specific sparsity patterns we find.
  prefs: []
  type: TYPE_NORMAL
- en: Scattered Distribution in Initial Layers. In the initial layers, attention score
    is generally uniform for all heads. We call this pattern Uniform Scatter, which
    is illustrated at the right of Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
    Such pattern suggests the model does not heavily prioritize any specific part
    of the input. Thus, it’s risky to reduce the context when the head show such patterns.
    Intuitively, these attention layers should be kept dense.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Patterns in Middle Layers. As we move deeper into the network, the attention
    mechanisms exhibit a marked departure from uniformity. We start with identifying
    simpler patterns. Attention Sink Xiao et al. ([2023](#bib.bib25)) and Locality
    can be seen in a small subset of heads. For these heads, nearly all attention
    probability concentrate on the first few tokens or tokens adjacent to the current
    position, as shown in the upper left and upper right of Figure [3](#S3.F3 "Figure
    3 ‣ 3 Observation ‣ Efficient LLM Training and Serving with Heterogeneous Context
    Sharding among Attention Heads").
  prefs: []
  type: TYPE_NORMAL
- en: Middle Spikes, as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads"),
    can also be identified in a few heads. This pattern puts significant probability
    mass on a few tokens that are in the middle of the context.
  prefs: []
  type: TYPE_NORMAL
- en: However, the vast majority of attention heads actually show a more complex hybrid
    pattern that could be decomposed into patterns mentioned above. For example, hybrids
    like Attention Sink + Local, or Attention Sink + Middle Spikes + Local are the
    most prevalent patterns, which is also demonstrated in Ge et al. ([2024](#bib.bib8));
    Fu ([2024](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: From the observation, we can conclude that (1) individual attention heads do
    not necessarily take the full context to function, (2) the actual context needed
    by different heads can be quite different, and (3) the initial attention layers
    are usually denser than any other layer. This inspires us to design S2-Attention,
    where the context is diversely sharded and distributed to each attention head.
    We will discuss the details of our method in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Preliminary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we first present formulation of the context sharding process
    for Sparsely Sharded Attention (S2-Attention). Then we discuss context-sharding
    policies we designed to instantiate S2-Attention. Lastly, we analyze why S2-Attentioncan
    achieve significant wall-clock speed-up for both training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Context Sharding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let $\mathcal{X}=\{x_{1},x_{2},\dots,x_{N}\}$ be the input sequence where $N$
    is the length of the sequence, $\mathcal{H}=\{h_{1},h_{2},\dots,h_{K}\}$, where
    $K$ is the number of attention heads. Each attention head $h_{k}$ is associated
    with a shard $S_{k}$ of the sequence $\mathcal{X}$, determined by a context sharding
    policy $f$. The policy can be viewed as a mapping function $f:\mathcal{H}\to 2^{\mathcal{X}}$,
    where $2^{\mathcal{X}}$ is the power set of $\mathcal{X}$. Each head receives
    a subset of the sequence elements: $f(h_{k})=S_{k}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We design the sharding process to meet the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The union of all the shards equals the full context. Namely, each element $x_{i}$
    in $\mathcal{X}$ must be assigned to at least one shard. Formally, this can be
    expressed as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\bigcup_{k=1}^{K}S_{k}=\mathcal{X}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Different attention heads receive heterogeneous context shards. At least two
    heads should receive different subsets of $\mathcal{X}$. Specifically:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\exists i,j\in\{1,\dots,K\},\,i\neq j:S_{i}\neq S_{j}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initial layers should receive full context for optimal performances. For simplicity,
    we set the initial two layers to be dense and sparsely shared contexts among the
    rest layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Context Sharding Policy Choices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From our experiments in Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads")
    and previous studies (Ge et al., [2024](#bib.bib8); Wu et al., [2024](#bib.bib24)),
    we summarize 6 basic attention patterns in trained transformers detailed in Section
    [3](#S3 "3 Observation ‣ Efficient LLM Training and Serving with Heterogeneous
    Context Sharding among Attention Heads"). As we found the Local pattern is most
    common across attention heads, we first ensure each shard includes local context.
    For contexts out of this local window, termed as remote context, we would like
    the sharding policy to cover the Attention Sink, Middle Spikes and their hybrid
    combinations Ge et al. ([2024](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: To capture the huge combinations while maintaining sparsity, we take advantage
    of the inherent expressiveness in the multi-head architecture. Each attention
    head is assigned with strided partitions of the remote contexts, with the starting
    points of these partitions uniquely offset by the index of the corresponding head.
    The heterogeneous assignment allows for a nuanced and scalable coverage of remote
    contexts. By combining the two designs, we introduce a heterogeneous strided sharding
    policy as formulated below.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Heterogeneous Strided Sharding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: KV-cache efficient sharding has a vertical pattern. For $l\geq 1$ and $j\geq
    i$, we say a sharding $S$ is KV cache efficient if $k_{i},v_{i}$ is attended by
    $q_{j+l}$, then it must be also attended by $q_{j}$. Otherwise, $k_{i},v_{i}$
    has to be stored at token $j$ for generating token $j+l$, but not used to generate
    token $j$. In this case, we can just let $q_{j}$ also attend to $k_{i},v_{i}$
    without additional memory for KV cache. However, KV cache can be dropped after
    generating certain tokens. This also means that the sharding $S$ should be based
    on absolute token positions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d98657c3ee6cf3329e2a02f9c17d52cd.png)![Refer to caption](img/b81ae8d636694001e626dc6747e585e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Left: KV-cache efficient sharding. Right: sharding that is not KV-cache
    efficient since token 2 is not attended by tokens 2 - 11, but needs to be stored
    at token 12 to generate token 13 and onward. The upper right corner is masked
    due in case of causal attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical pattern with one fixed stride and a local window. For a transformer
    with $H$ heads, given a sequence of $N$ tokens, we first chunk context into a
    list of blocks of size $S$, which gives us $B=ceil(N/S)$ blocks. We use $q_{i}$
    and $k_{i}$ to denote the query block and key block index. Note that $q_{i}$ corresponds
    to the $i$ th indexed block, instead of the $i$ th token, and one block includes
    $S$ tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Among the total $B$ blocks, We take the most recent $N_{l}$ blocks as local
    blocks and set the rest as remote blocks. Since local blocks are more frequently
    attended, we use different stride sizes for local and remote blocks for fine-grained
    context modeling. Denote the local stride as $v_{l}$ and remote stride as $v_{r}$.
    For simplicity, we use full attention for local blocks by setting $v_{l}=1$, since
    local blocks only accounts for a small portion of the total context and brings
    negligible change to the overall efficiency. In practice, $v_{r}$ is set to be
    a factor number of attention heads to satisfy the “union as full context” constraint.
    For each head $h\in\{1,...,H\}$, we set a unique position offset $o_{h}$. For
    simplicity, we set $o_{h}=h$ here, which means the offset of each head equals
    its head index.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then for each head $h$, its block attention mask $M^{h}$ in $B\times B$ dimension
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$M^{h}_{i,j}=\left\{\begin{array}[]{ll}1&amp;q_{i}-k_{j}<N_{l},\\ 1&amp;(k_{j}-o_{h})\in
    v_{r}\mathbb{Z}_{\geq 0}\,\&amp;\,q_{i}-k_{j}\in[N_{l},B)\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{otherwise},\end{array}\right.$$ |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: where $x\in m\mathbb{Z}_{\geq 0}$ mean $x$ is 0 or a positive multiple of $m$.
  prefs: []
  type: TYPE_NORMAL
- en: The first constraint determines the shards within the local window. The second
    constraint controls what shard each head attends to outside the local window.
    Note that if the distance is beyond $B$, the attention is dropped, thus extends
    similar to a sliding window. As an example, setting $B=8,N_{l}=2,v_{l}=1,v_{r}=3,o_{h}=h$
    will give us the sharding as presented in the left part of Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Efficient LLM Training and Serving with Heterogeneous Context
    Sharding among Attention Heads"). $B=8,N_{l}=3,v_{l}=2,v_{r}=3,o_{h}=h$ will give
    us the right part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f855cd63af99bba10a4b4ebf8de3468.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example Sparsity Patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: We pick this neat instantiation but the formulation is quite general. The constraints
    can also be met by choosing different offset $o_{h}$, or different vertical stride
    $v_{i}$, or local attention window size $N_{l}$ for each head, which are all supported
    by our library.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical pattern with multiple fixed strides and a local window. This local-stride
    pattern can be easily extended to have multiple strides in different remote blocks.
    With blocks $N_{l}<N_{r_{1}}<N_{r_{2}}<B$, and $v_{r_{1}}<v_{r_{2}}$, we define
    block attention mask as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$M^{h}_{i,j}=\left\{\begin{array}[]{llll}1&amp;q_{i}-k_{j}\in[0,N_{l}),&amp;&amp;\\
    1&amp;q_{i}-k_{j}\in[N_{l},N_{r_{1}})&amp;\&amp;&amp;(k_{j}-o^{1}_{h})\in v_{r_{1}}\mathbb{Z}_{\geq
    0},\\'
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;q_{i}-k_{j}\in[N_{r_{2}},B)&amp;\&amp;&amp;(k_{j}-o^{2}_{h})\in v_{r_{2}}\mathbb{Z}_{\geq
    0},\\
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{otherwise}.\end{array}\right.$$ |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that $M_{i,j}^{h}$ is KV-cache efficient, we requires that $v_{r_{2}}\in
    v_{r_{1}}\mathbb{Z}_{\geq 0}$ and $(o^{2}_{h}-0^{1}_{h})\in v_{r_{1}}\mathbb{Z}_{\geq
    0}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Training and Inference Advantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the formulation of S2-Attention, we can achieve significant FLOPs reduction
    in training. To train a default self-attention layer, the total FLOPs needed is
    $O(TN)$, where $T$ is the number of tokens to train, $N$ is the max sequence length.
    When using S2-Attention, in case where$v_{l}=1$, we have a dense local window
    of $L$ tokens, and the vertical stride $v_{r}=v$ for the remote context. For each
    attention head, $L+(N-L)/v$ is number of tokens attention to, i.e. the dense equivalent
    sequence length. Then the total FLOPS reduced from $O(TN)$ to $O(T(L+(N-L)/v)$
    at an attention layer. For a 1.3B Llama-like model with 16 attention heads, trained
    on 8192 sequence length, set $L=64$ and $v=16$ will generate a 14.32X FLOPs reduction.
    As the stride $v$ is bounded by the number of heads for the union constraint,
    we can have larger v choices as the model size increases, which will further boost
    the reduction. It can be derived that the upper bound of the speed-up grows linearly
    with the number of attention heads. For example, for a 70B model with 64 attention
    heads, S2-Attentioncan generate 42.89X reduction.
  prefs: []
  type: TYPE_NORMAL
- en: During inference, the prompt prefilling stage has the same FLOPs saving as the
    forward pass in training. In the decoding phase, despite the FLOPs saving, S2-Attentionwill
    have extra memory footprint reduction. As each attention head is processed in
    one Streaming Multiprocessor(SM), skipping the blocks outside the shard of a given
    head can thus reduce the memory usage, which further boosts throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 System Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our S2-Attentiontraining kernel is developed upon the Triton flash attention
    code. We’ve compared it to the CUDA version of FlashAttention-2. It accepts a
    sparse sparse pattern tensors specified the per head sparsity pattern on a shard
    level. The underline algorithm turn the sparse matrix to compressed sparse row
    (CSR) format as input, which contains k shard indices for each q shard, then scan
    over the k shard indices to do the FlashAttention in a thread block. We see that
    when vertical stride is set to 1, e.g., normal dense attention, the latency is
    comparable to the original Triton flash attention. Compared to the CUDA version
    of Flash Attention 2, the forward pass has similar latency while the backward
    pass lags slightly. When the vertical stride increases, the performance increase
    almost linearly as shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.6 System Implementation
    ‣ 4 Methodology ‣ Efficient LLM Training and Serving with Heterogeneous Context
    Sharding among Attention Heads"). The memory usage in S2-attention is the same
    as FlashAttention-2 in training regardless of the vertical stride. However at
    inference, during the decoding phase, the KV cache is about $1/v$ of a dense attention
    if local window is considerably small compared to the current sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Both the CUDA version and the Triton version of flash attention load the whole
    head dimension(D) at once to SRAM, which is intuitive as the whole vector is needed
    to compute the attention scores. However, in our experiment, we found that split
    at the D dimension in many cases is beneficial. We suspect that this is because
    D-split reduce SRAM usage and enable a larger degree of software pipelining. Interestingly,
    we also find that with shard size of 64, D-split only helps when head dimension
    is 128, the most commonly used case, while has no additional benefit when head
    dimension is 64 or 256. We suspect that this is due to under-tuned hyper-parameters
    like kernel block sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Scratch of D-split at the forward pass in a thread block. See Dao
    ([2023](#bib.bib4)) for more detail
  prefs: []
  type: TYPE_NORMAL
- en: Init SRAM $b,p,q_{0},q_{1},o_{1},o_{2}$Load $q_{0}\leftarrow q[...,:D/2]$, $q_{1}\leftarrow
    q[...,D/2:]$for j in attn_shard_indices do     Load $b\leftarrow k_{0}^{j}$     $p\leftarrow
    q_{0}@b$     Load $b\leftarrow k_{1}^{j}$     $p\leftarrow p+q_{1}@b$     $p\leftarrow\mathbf{softmax}(p)$     Load
    $b\leftarrow v_{0}^{j}$     $o_{1}=update(o_{1},p@b)$     Load $b\leftarrow v_{1}^{j}$     $o_{2}=update(o_{2},p@b)$end for![Refer
    to caption](img/fabc2d7513bccd1e94928ff6c84bc7c4.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Left: Comparison between dense and S2-attention with different sparsity.
    All experiment use (batch, heads, head_dim) = (4, 16, 128) on A100 80GB-SXM. Right:
    Impact of D-split. All experiments use (batch, heads, head_dim) = (4, 16, 128)
    on A100 80GB-SXM. We see D-Split=2 consistently outperforms D-Split=1 in most
    implementation of flash attention on the forward pass. However. the impact on
    the backward pass is negligible.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct comprehensive experiments to validate the significant efficiency
    advantage and quality preserving S2-Attentioncan achieve. We first show S2-Attentioncan
    achieve on par, if not better, downstream task performance and context retrieval
    capability compared to default attention in Section [5.1](#S5.SS1 "5.1 Benchmarking
    Model Training Quality ‣ 5 Experiment ‣ Efficient LLM Training and Serving with
    Heterogeneous Context Sharding among Attention Heads") and Section [5.2](#S5.SS2
    "5.2 Context Retrieval ‣ 5 Experiment ‣ Efficient LLM Training and Serving with
    Heterogeneous Context Sharding among Attention Heads"). We then demonstrate the
    wall-clock speed-up of S2-Attentionover FlashAttention-2 across different context
    length settings in Section [5.3](#S5.SS3 "5.3 Training Speed-up ‣ 5 Experiment
    ‣ Efficient LLM Training and Serving with Heterogeneous Context Sharding among
    Attention Heads"). We then present the end-to-end throughput and latency improvement
    of S2-Attentionover the default attention implementation and infer-time KV cache
    management in Section [5.4](#S5.SS4 "5.4 Inference Speed-up ‣ 5 Experiment ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
    Lastly, we test our hypotheses for S2-Attentionwith comprehensive ablation studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Training quality evaluation. SWA refers to sliding window attention,
    while S2 refers to S2-Attention. L refers to number of local blocks. V refers
    to the vertical stride size. Dense refers to the idex of dense attention layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | KV reduction | Passkey | WinoGrande | piqa | race | wikitext103(ppl)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 0% | 0.865 | 0.592 | 0.733 | 0.403 | 15.88 |'
  prefs: []
  type: TYPE_TB
- en: '| SWA | 92.9% | 0.334 | 0.566 | 0.721 | 0.380 | 21.03 |'
  prefs: []
  type: TYPE_TB
- en: '| SWA + Dense 1,2 | 85.4% | 0.771 | 0.577 | 0.728 | 0.381 | 17.45 |'
  prefs: []
  type: TYPE_TB
- en: '| S2-L1V15 | 92.70% | 0.782 | 0.571 | 0.724 | 0.361 | 19.55 |'
  prefs: []
  type: TYPE_TB
- en: '| S2-L1V15 + Dense 1,2 | 85.0% | 0.941 | 0.587 | 0.725 | 0.397 | 17.18 |'
  prefs: []
  type: TYPE_TB
- en: '| S2-L8V15 | 87.5% | 0.740 | 0.586 | 0.721 | 0.379 | 17.87 |'
  prefs: []
  type: TYPE_TB
- en: '| S2-L8V15 + Dense 1,2 | 80.4% | 0.884 | 0.586 | 0.721 | 0.379 | 17.49 |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Benchmarking Model Training Quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experiment Settings. To study the S2-Attention, we train a range of 1.3B models
    with 24 layers, 2048 hidden size with 16 heads, with max sequence length as 8192.
    We use the FineWeb-Edu-350B Penedo et al. ([2024](#bib.bib19)) as the pre-training
    corpus. An OpenAI Tiktoken tokenizer with 100K vocabulary size is used to process
    the raw text. All model variations use batch size of 4M tokens for all sequence
    lengths and train for a total of 300 billion tokens. For hyperparameters, we use
    $\mu$P Yang et al. ([2022](#bib.bib26)) with a base shape of 256. A $\mu$P learning
    rate of 0.02 is used with linear decay and 0.5% of total training tokens for warmup.
    All models are evaluated after training on the total 300B tokens for one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: We use a model whose attention layers are dense, denoted as Dense, as our baseline.
    We use the standard dense attention as our baseline, which is implemented as flash
    attention in To study the effect of union constraint and heterogeneous sharding,
    we include sliding window attention (SWA) for comparison. For SWA, we control
    the FLOPs to be equivalent to S2-Attention. For S2-Attentionwith 1 local block
    and a vertical stride of 15, the total FLOPs can be translated to SWA whose local
    window size is 9*64=576. To study the effect of initial dense layers, we also
    train a SWA with the first two layers dense.
  prefs: []
  type: TYPE_NORMAL
- en: From Table [1](#S5.T1 "Table 1 ‣ 5 Experiment ‣ Efficient LLM Training and Serving
    with Heterogeneous Context Sharding among Attention Heads"), we can observe S2-Attentionshows
    promising results. Compared to fully dense models, S2-Attentioncan achieve on
    par performance for all the downstream tasks. With a delta below 0.008, S2-Attentioncan
    achieves $95.0\%$ KV reduction compared to Dense. The minor gap can be explained
    by the scaling law, as S2-Attentiononly has around $10\%$ attention FLOPs compared
    to Dense. Notably, in the Passkey Retrieval task, S2-Attentioncan achieve much
    better performance compared to Dense, despite each head only has a reduced context.
    This observation demonstrate the context understanding ability of the S2-Attentiondesign.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to SWA, we can see S2-Attentionwith the same FLOPs, can consistently
    achieve better performance across all the downstream tasks. Among all the tasks,
    the Passkey Retrieval task is where S2-Attentionand SWA have the largest performance
    margin. The superior performance indicate the benefit of securing the union constraint
    and heterogeneous constraint. Moreover, we can see that for both S2-Attentionand
    SWA can benefit from the initial dense layers. If changing initial layers to sparse,
    all downstream tasks will have a significant downgradation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Context Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we evaluate the long context understanding capability on the
    Needle in a Haystack task. In this task, a factual sentence, “needle”, is inserted
    into texts of 512, 1024, 2048, 4096, 8192, 16384, 32768 tokens. The needle is
    positioned at 0-$100\%$ depth of the original context, where 0 is the start of
    the context and $100\%$ being the end of the context.
  prefs: []
  type: TYPE_NORMAL
- en: Following the settings in previous section, we pretrain an 1.3B model on a 350B
    subset of FineWeb-Edu. The first two attention layers are dense while the rest
    of layers deploy S2-Attention, with vertical stride as 15 and local window as
    1. We modified the RoPE base to 1,00,000 to adapt to the longer context window,
    and continue train the model on BookS3.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figure[1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads"),
    S2-Attentioncan achieves perfect recall on the 32k context window. The results
    validate the long context understanding capability of S2-Attentiondesign.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Training Speed-up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmark Settings We measure the attention runtime of S2-Attentionand FlashAttention-2
    on an A100 80GB GPU for different context length, number of head, and head dimension
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19a966e693ce7291cebf3b04f3e0d401.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: End-to-end latency speed-up for 1.3B Llama architecture models.'
  prefs: []
  type: TYPE_NORMAL
- en: We benchmark 3 model sizes to showcase the scalability of our system. Figure
    [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ Efficient LLM Training and Serving
    with Heterogeneous Context Sharding among Attention Heads") shows the 1.3B model
    results. Figure [7](#S5.F7 "Figure 7 ‣ 5.3 Training Speed-up ‣ 5 Experiment ‣
    Efficient LLM Training and Serving with Heterogeneous Context Sharding among Attention
    Heads") shows the 70B speedup. For all the model sizes, S2-Attentioncan achieve
    multiple times of speed-up over FlashAttention-2. For 1.3B models with 16 heads,
    S2-Attentioncan achieve as much as 8.79X speed-up as the max sequence length grows
    longer. For 70B models with 64 heads, S2-Attentioncan give 25.3X end-to-end speed-up.
    Taken forward pass alone, S2-Attentioncan generate 37.58X reduction. The overall
    boost is hedged due to our less optimized backward kernel, which leaves room for
    further improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Inference Speed-up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62785c96c016417efbd58e0065fa827f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: End-to-end latency speed-up for 7B size Llama architecture models.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to demonstrate the inference improvments of S2-Attention, we measure
    the end-to-end prefilling and decoding latency over different context length settings
    in Figure [8](#S5.F8 "Figure 8 ‣ 5.4 Inference Speed-up ‣ 5 Experiment ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
    We choose the FlashAttention-2 backend in vLLM as baseline for fair comparison.
    Both methods are deployed on a single node with 8 A100 80GPU, with tensor parallel
    size setting as 8. For prefilling speed-up, we set output length as 1, and vary
    input length between 16k to 512k. As shown in Figure [8](#S5.F8 "Figure 8 ‣ 5.4
    Inference Speed-up ‣ 5 Experiment ‣ Efficient LLM Training and Serving with Heterogeneous
    Context Sharding among Attention Heads"), S2-Attentioncan achieves 1.2X, 2.7X,
    3X, 5.7X speed-up on 16k, 128K, 256K, and 512K context.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce S2-Attention, a framework that sparsely shards the
    context for different attention heads to divide-and-conquer. Experimental results
    show model trained with S2-Attentioncan achieve promising performance on long
    context tasks with reduced context for each head. We back S2-Attentionwith a highly
    optimized kernel library, we can get equivalent model quality while achieving
    speed-up over FlashAttention-2 linearly increasing over the number of attention
    heads. We open-sourced our kernel library and make it a plug-in-and-play alternative
    for FlashAttention-2 module in popular training frameworks like Megatron and Pytorch.
    We also integrated S2-Attentioninto vLLM backend for instant serving. Both the
    training and inference kernels allow users to freely customize their sparsity
    pattern, facilitating the whole community to study the topic in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Liyuan Liu and Yao Fu from Microsoft Research and University
    of Edinburgh for insightful discussions. We also would like to thank Zhuohan Li,
    Simon Mo, and Kaichao You from UC Berkeley and Tsinghua University for sharing
    expertise on the vLLM codebase.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer:
    The long-document transformer. *CoRR*, abs/2004.05150, 2020. URL [https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
    URL [http://arxiv.org/abs/1904.10509](http://arxiv.org/abs/1904.10509).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. What does BERT look at? an analysis of BERT’s attention. In *Proceedings
    of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks
    for NLP*, pp.  276–286, Florence, Italy, August 2019\. Association for Computational
    Linguistics. doi: 10.18653/v1/W19-4828. URL [https://aclanthology.org/W19-4828](https://aclanthology.org/W19-4828).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *CoRR*, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691.
    URL [https://doi.org/10.48550/arXiv.2307.08691](https://doi.org/10.48550/arXiv.2307.08691).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh
    (eds.), *Advances in Neural Information Processing Systems 35: Annual Conference
    on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,
    USA, November 28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
    June 2-7, 2019, Volume 1 (Long and Short Papers)*, pp.  4171–4186\. Association
    for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu (2024) Yao Fu. How do language models put attention weights over long context?
    *Yao Fu’s Notion*, Mar 2024. URL [https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context](https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. (2024) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression
    for llms. In *The Twelfth International Conference on Learning Representations,
    ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net, 2024. URL [https://openreview.net/pdf?id=88nT0j5jAn](https://openreview.net/pdf?id=88nT0j5jAn).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2019) Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans.
    Axial attention in multidimensional transformers. *CoRR*, abs/1912.12180, 2019.
    URL [http://arxiv.org/abs/1912.12180](http://arxiv.org/abs/1912.12180).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *Proceedings of the 37th International Conference on
    Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings
    of Machine Learning Research*, pp.  5156–5165\. PMLR, 2020. URL [http://proceedings.mlr.press/v119/katharopoulos20a.html](http://proceedings.mlr.press/v119/katharopoulos20a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020. URL
    [https://openreview.net/forum?id=rkgNKkHtvB](https://openreview.net/forum?id=rkgNKkHtvB).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr
    Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: LLM
    knows what you are looking for before generation. *CoRR*, abs/2404.14469, 2024.
    doi: 10.48550/ARXIV.2404.14469. URL [https://doi.org/10.48550/arXiv.2404.14469](https://doi.org/10.48550/arXiv.2404.14469).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention
    with blockwise transformers for near-infinite context. *CoRR*, abs/2310.01889,
    2023a. doi: 10.48550/ARXIV.2310.01889. URL [https://doi.org/10.48550/arXiv.2310.01889](https://doi.org/10.48550/arXiv.2310.01889).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for LLM KV cache compression
    at test time. *CoRR*, abs/2305.17118, 2023b. doi: 10.48550/arXiv.2305.17118. URL
    [https://doi.org/10.48550/arXiv.2305.17118](https://doi.org/10.48550/arXiv.2305.17118).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta (2024) AI Meta. Introducing meta llama 3: The most capable openly available
    llm to date. *Meta AI*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
    heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
    E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems*,
    volume 32\. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
    Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna
    Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,
    Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
    Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context
    learning and induction heads. *CoRR*, abs/2209.11895, 2022. doi: 10.48550/ARXIV.2209.11895.
    URL [https://doi.org/10.48550/arXiv.2209.11895](https://doi.org/10.48550/arXiv.2209.11895).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penedo et al. (2024) Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton
    Lozhkov, Margaret Mitchell, Colin Raffel, Leandro von Werra, and Thomas Wolf.
    The fineweb datasets: Decanting the web for the finest text data at scale. *CoRR*,
    abs/2406.17557, 2024. doi: 10.48550/ARXIV.2406.17557. URL [https://doi.org/10.48550/arXiv.2406.17557](https://doi.org/10.48550/arXiv.2406.17557).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang,
    and Jie Tang. Blockwise self-attention for long document understanding. In Trevor
    Cohn, Yulan He, and Yang Liu (eds.), *Findings of the Association for Computational
    Linguistics: EMNLP 2020, Online Event, 16-20 November 2020*, volume EMNLP 2020
    of *Findings of ACL*, pp.  2555–2565\. Association for Computational Linguistics,
    2020. doi: 10.18653/V1/2020.FINDINGS-EMNLP.232. URL [https://doi.org/10.18653/v1/2020.findings-emnlp.232](https://doi.org/10.18653/v1/2020.findings-emnlp.232).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rucinski (2024) Szymon Rucinski. Efficient language adaptive pre-training:
    Extending state-of-the-art large language models for polish. *CoRR*, abs/2402.09759,
    2024. doi: 10.48550/ARXIV.2402.09759. URL [https://doi.org/10.48550/arXiv.2402.09759](https://doi.org/10.48550/arXiv.2402.09759).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned, July 2019. URL [https://aclanthology.org/P19-1580](https://aclanthology.org/P19-1580).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2024) Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao
    Fu. Retrieval head mechanistically explains long-context factuality. *CoRR*, abs/2404.15574,
    2024. doi: 10.48550/ARXIV.2404.15574. URL [https://doi.org/10.48550/arXiv.2404.15574](https://doi.org/10.48550/arXiv.2404.15574).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks. *CoRR*,
    abs/2309.17453, 2023. doi: 10.48550/ARXIV.2309.17453. URL [https://doi.org/10.48550/arXiv.2309.17453](https://doi.org/10.48550/arXiv.2309.17453).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor,
    David Farhi, Jakub Pachocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. Tensor
    programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
    In *NeurIPS 2021*, March 2022. URL [https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
    *Advances in Neural Information Processing Systems 33: Annual Conference on Neural
    Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*,
    2020. URL [https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, and Beidi Chen. H${}_{\mbox{2}}$o: Heavy-hitter oracle for efficient
    generative inference of large language models. *CoRR*, abs/2306.14048, 2023. doi:
    10.48550/arXiv.2306.14048. URL [https://doi.org/10.48550/arXiv.2306.14048](https://doi.org/10.48550/arXiv.2306.14048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may include other additional sections here.
  prefs: []
  type: TYPE_NORMAL
