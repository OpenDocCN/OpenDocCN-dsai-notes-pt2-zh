- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13558](https://ar5iv.labs.arxiv.org/html/2404.13558)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul \useunder\ul
  prefs: []
  type: TYPE_NORMAL
- en: Haoyu Zheng   Wenqiao Zhang   Yaoke Wang   Hao Zhou   Jiang Liu  and  Juncheng
    Li   Zheqi Lv   Siliang Tang   Yueting Zhuang [zhenghaoyu404@gmail.com, wenqiaozhang@zju.edu.cn,
    wangyaoke@zju.edu.cn, 2021210665@stu.hit.edu.cn](mailto:zhenghaoyu404@gmail.com,%20wenqiaozhang@zju.edu.cn,%20wangyaoke@zju.edu.cn,%202021210665@stu.hit.edu.cn)
    [20201785@stu.cqu.edu.cn, junchengli@zju.edu.cn, zheqilv@zju.edu.cn, siliang@zju.edu.cn,
    yzhuang@zju.edu.cn](mailto:20201785@stu.cqu.edu.cn,%20junchengli@zju.edu.cn,%20zheqilv@zju.edu.cn,%20siliang@zju.edu.cn,%20yzhuang@zju.edu.cn)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Revolutionary advancements in text-to-image models have unlocked new dimensions
    for sophisticated content creation, *e.g.,*, text-conditioned image editing, allowing
    us to edit the diverse images that convey highly complex visual concepts according
    to the textual guidance. Despite promising, existing methods focus on texture-
    or non-rigid-based visual manipulation, which struggles to produce the fine-grained
    animation of the smooth text-conditioned image morphing without fine-tuning, *i.e.*,
    due to their highly unstructured latent space. In this paper, we introduce a tuning-free
    LLM-driven attention control framework, encapsulated by the progressive process
    of LLM planning $\rightarrow$ prompt-Aware editing $\rightarrow$ StablE animation
    geneRation, abbreviated as LASER: i) Given a general and coarse-grained description
    for image editing, LASER initially employs the large language model (LLM) that
    refines it into consistent and fine-grained prompts, which serve as linguistic
    guidance for the pre-trained text-to-image models in subsequent image generation;
    ii) We manipulate the model’s spatial features and self-attention mechanisms to
    maintain animation integrity and enable seamless image morphing directly from
    text prompts, obviating the need for annotations for additional fine-tuning; iii)
    Our meticulous control of spatial features and self-attention within the model
    ensures the preservation of structural consistency in the images. This paper introduces
    a novel framework that integrates large language models (LLM) with pre-trained
    text-to-image models to create high-quality image translation animations from
    just one input text. The proposed LASER introduces a novel tuning-free framework
    that integrates LLM with pre-trained text-to-image models to facilitate high-quality
    text-conditioned image-to-animation translation. In addition, we propose a Text-conditioned
    Image-to-Animation Benchmark to validate the effectiveness and efficacy of the
    proposed LASER. Extensive experiments show that the proposed LASER produces impressive
    results in both consistent and efficient animation generation, positioning it
    as a powerful tool for producing detailed animations and opening new avenues in
    digital content creation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multimodal, Diffusion Model, Large Language Model^†^†ccs: Computing methodologies Computer
    vision![Refer to caption](img/bf73944f6263e9cd350d27daeefe747b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Given multimodal inputs (image and textual guidance), our method
    is capable of guiding the generation of smooth animations based on textual content.
    The first row shows the combined texture and non-rigid changes; the second row,
    only the texture changes; the third row, purely non-rigid transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Diffusion models (Dhariwal and Nichol, [2021](#bib.bib9); Ho et al., [2020](#bib.bib13);
    Nichol and Dhariwal, [2021](#bib.bib25)) form a category of deep generative models
    that has recently become one of the hottest topics in multimodal intelligence,
    showcasing impressive capabilities of text-to-image (T2I) generation, ranging
    from the high level of details to the diversity of the generated examples. Such
    diffusion models also unlock a new world of creative processes in content creation,
    *e.g.*, text-guided image editing (Brooks et al., [2023](#bib.bib6); Cao et al.,
    [2023](#bib.bib7); Hertz et al., [2022](#bib.bib11)), involves editing the diverse
    images that convey highly complex visual concepts with text-to-image models solely
    through the textual guidance. Broadly, the contemporary image editing paradigm
    can be summarized in two aspects: i) *Texture editing* (Brooks et al., [2023](#bib.bib6);
    Cao et al., [2023](#bib.bib7); Hertz et al., [2022](#bib.bib11)), manipulating
    a given image’s stylization and appearance while maintaining the input structure
    and scene layout; ii) *Non-rigid Editing* (Cao et al., [2023](#bib.bib7); Kawar
    et al., [2023](#bib.bib19)), enabling non-rigid image editing (*e.g.,* posture
    changes) while preserving its original characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite achieving impressive image-level editing effects, the aforementioned
    methods fail to harness the editing animation, *i.e.*, the smooth transition of
    the sequence of intermediary images according to the user’s textual requirement,
    including the fine-grained texture and non-rigid transformation. Such text-conditioned
    image-to-animation serves as an imperative component in various real-world content
    creation tasks, ranging from cinematic effects to computer games, as well as photo-editing
    tools for artistic and entertainment purposes to enrich people’s imagination.
    Nevertheless, realizing animation-level editing is highly challenging, primarily
    due to the highly unstructured latent space of the intermediary images. Of course,
    we can introduce more animation data to fine-tune the entire T2I diffusion models,
    thereby capturing the smooth animation edit. However, it comes at a tremendous
    cost and deteriorates the flexibility of the pre-trained diffusion models under
    the animation-level editing setting. Based on the above insights, one question
    is thrown: Given the input image and textual description, could we achieve the
    high-quality animation editing effect with the pre-trained text-to-image models
    without fine-tuning?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we introduce a novel tuning-free LLM-driven attention control
    framework framework for text-conditioned image-to-animation, through LLM planing
    $\rightarrow$ prompt-Aware Editing $\rightarrow$ StablE moRphing, named as LASER.
    The core of our framework is that by leveraging the large language models (LLMs)(Touvron
    et al., [2023](#bib.bib38); Achiam et al., [2023](#bib.bib2); Li et al., [2023](#bib.bib22);
    Zhang et al., [2024](#bib.bib49)) with significant potential in natural language
    processing, to effectively parse the textual description into relevant and continuous
    control statements for pre-trained T2I diffusion models, thereby transforming
    the given image to animation. Specifically, LASER comprises the following progressive
    steps: Step 1, given a multimodal input, *i.e.*, a description of the animation
    $P_{0}$ and an initial image $I_{0}$ (which can be optional, allowing the T2I
    model generation), LLM decomposes the general and coarse-grained description $P_{0}$
    into multiple fine-grained and consistent prompts. These prompts are closely aligned
    and exhibit subtle variations, aiding in the guided editing of subsequently corresponding
    keyframes; Step 2, the LLM analyzes these prompts to the feature and attention
    injection control signals, adapting to the nuanced differences between adjacent
    prompts. This enables tailored injection strategies for editing different keyframe
    types. Notably, the injection strategy delineates into two base categories: Feature
    and Association Injection (FAI) for texture-based editing and Key-Value Attention
    Injection (KVAI) for non-rigid editing. Notably, to facilitate the simultaneous
    portrayal of both texture and non-rigid editing within a singular animation phase,
    we propose the forward hybrid Attention Injection (HAI) for the image editing;
    Step 3, effectively synthesizing intermediate frames between keyframes, ensuring
    animations are coherent and fluid. This generator utilizes advanced interpolation
    methods, such as spherical linear interpolation, to ensure smooth transitions
    and reduce artifacts. Additionally, Adaptive Instance Normalization (AdaIN) is
    applied to enhance color and brightness consistency. The Hybrid Attention Injection
    (HAI) strategy is also employed to integrate texture and structural transformations
    within a single animation phase, further enhancing the animation’s overall quality
    and coherence. Additionally, we inaugurate a Text-conditioned Image-to-Animation
    Benchmark, a comprehensive collection designed to challenge and quantify the adaptability
    and precision of the proposed LASER.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summing up, our contributions can be concluded as:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce the tuning-free text-conditioned image-to-animation task, designed
    to craft high-quality animations based on the multimodal input using the pre-trained
    text-to-image models, without additional fine-tuning or annotations. To evaluate
    the efficacy of our approach, we introduce the Text-conditioned Image-to-Animation
    Benchmark, hoping that it may support future studies within this domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proposed the LASER encapsulated by the progressive process of LLM planing
    $\rightarrow$ Prompt-aware editing $\rightarrow$ Stable morphing, enabling the
    smooth texture- and non-rigid animation generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both qualitative and quantitative assessments underscore the superior efficacy
    of the proposed framework, showcasing its proficiency in generating animations
    that are not only smooth and of high quality but also diverse.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-Image Generation. In artificial intelligence(Zhang et al., [2022](#bib.bib52),
    [2019](#bib.bib51), [2023a](#bib.bib50)), text-to-image (T2I) Generation aims
    to generate high-quality images based on text descriptions. Previous text-conditioned
    image generation approaches were primarily based on Generative Adversarial Networks
    (GANs) (Brock et al., [2018](#bib.bib5); Zhang et al., [2017](#bib.bib44), [2018b](#bib.bib45);
    Xu et al., [2018](#bib.bib42); Zhu et al., [2019](#bib.bib53)), leveraging their
    robust capabilities for high-fidelity image synthesis. These models, through multimodal
    vision-language learning, have endeavored to align text descriptions with synthesized
    image contents, yielding gratifying synthesis results on specific domain datasets.
    Recently, diffusion models (Ho et al., [2020](#bib.bib13); Nichol and Dhariwal,
    [2021](#bib.bib25); Dhariwal and Nichol, [2021](#bib.bib9)) have demonstrated
    exceptional generative capabilities, achieving state-of-the-art results in terms
    of generation quality and diversity. By incorporating text prompts into diffusion
    models, various text-to-image diffusion models (Rombach et al., [2022](#bib.bib31);
    Saharia et al., [2022](#bib.bib32); Podell et al., [2023](#bib.bib29)) have been
    developed. They are intricately conditioned on the provided text via cross-attention
    layers, ensuring that the generated images are not only visually coherent but
    also semantically consistent with the input descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Text-guided Image Editing. Text-guided image editing is a challenging task that
    aims to edit images based on textual descriptions, enabling users to achieve desired
    changes in natural language. Previous deep-learning-based approaches based on
    GANs (Li et al., [2020](#bib.bib21); Nam et al., [2018](#bib.bib24); Patashnik
    et al., [2021](#bib.bib28); Xia et al., [2021](#bib.bib41)) have achieved certain
    success, but they are limited to specific domain datasets and exhibit limited
    applicability and generalization. VQGANCLIP (Crowson et al., [2022](#bib.bib8))
    is an autoregressive model that combines VQGAN (Esser et al., [2021](#bib.bib10))
    and CLIP (Radford et al., [2021](#bib.bib30)) to produce high-quality images and
    enable precise editing, yielding diverse and controllable results. However, this
    method suffers from slow generation speed and high computational cost. Recently,
    diffusion models trained on large-scale text-image pairs such as Imagen (Saharia
    et al., [2022](#bib.bib32)) and Stable Diffusion (Rombach et al., [2022](#bib.bib31))
    have achieved unprecedented success in text-to-image generation. Therefore, they
    serve as a robust prior for various editing tasks, including text-guided image
    manipulation (Brooks et al., [2023](#bib.bib6); Cao et al., [2023](#bib.bib7);
    Hertz et al., [2022](#bib.bib11); Kawar et al., [2023](#bib.bib19); Parmar et al.,
    [2023](#bib.bib27); Tumanyan et al., [2023](#bib.bib39)). Prompt-to-Prompt (Hertz
    et al., [2022](#bib.bib11)) and Plug-and-Play (Tumanyan et al., [2023](#bib.bib39))
    utilize cross-attention or spatial features to edit both global and local aspects
    of the image by directly modifying the text prompt. MasaCtrl (Cao et al., [2023](#bib.bib7))
    and Imagic (Kawar et al., [2023](#bib.bib19)) can handle non-rigid transformations
    such as changing object poses. Particularly, Plug-and-Play (Tumanyan et al., [2023](#bib.bib39))
    consider the task of text-guided image-to-image translation that aims to estimate
    a mapping of an image from a source domain to a target domain, where the target
    domain is not specified through a dataset of images but rather via a target text
    prompt. However, most of these approaches directly generate the final edited image,
    with limited exploration concerning continuous animations such as image morphing.
  prefs: []
  type: TYPE_NORMAL
- en: Image Morphing. Image morphing is a task in computer graphics and image processing
    that aims to obtain reasonable intermediate images in the smooth transition between
    two images (Aloraibi, [2023](#bib.bib3); Zope and Zope, [2017](#bib.bib54)). With
    the advent of deep learning, neural networks have been used for image morphing,
    learning to identify correspondences and generate intermediate frames through
    latent interpolations. For instance, in the works on GANs (Karras et al., [2021](#bib.bib16),
    [2019](#bib.bib17), [2020](#bib.bib18); Sauer et al., [2023](#bib.bib33), [2022](#bib.bib34)),
    it has been demonstrated that their latent embedding space is highly continuous,
    and linear interpolation between two latent codes yields impressive image morphing
    results. Recent studies on diffusion models have also indicated the feasibility
    of generating plausible intermediate images through latent noise interpolation
    and text embedding interpolation (Bao et al., [2023](#bib.bib4); Song et al.,
    [2020b](#bib.bib37); Wang and Golland, [2023](#bib.bib40)). Impus (Yang et al.,
    [2023](#bib.bib43)) explored the application of diffusion models in image morphing
    tasks, performing interpolation in the locally linear continuous text embedding
    space and Gaussian latent space. DiffMorpher (Zhang et al., [2023c](#bib.bib46))
    utilizes pre-trained diffusion models to achieve smooth and natural image interpolation
    and morphing. It performs spherical linear interpolation on the latent noise obtained
    through DDIM inversion for two images and combines it with text-conditioned linear
    interpolation, thus addressing the limitations of smooth interpolation between
    two image samples within the unstructured latent space of diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a user-defined descriptor $P_{*}$ and an initial image $I_{0}$ (provided
    or generated), our method generates the animation sequence $\{x_{0}^{(\alpha)},x_{1}^{(\alpha)},\ldots,x_{n}^{(\alpha)}\}$,
    where $\alpha$ varies from 0 to 1\. The length of the $x_{i}^{(\alpha)}$ sequence
    is set by $n_{f}$ and the number of sequences $x_{i}^{(\alpha)}$ corresponds to
    the transformation stages $n_{t}$. The resulting animation is expected to visually
    manifest the smooth transitions of $I_{0}$ to $I_{n}$ and characteristics as described
    by $P_{*}$. To guide this generative process, a series of descriptive prompts
    $\{P_{0},P_{1},\ldots,P_{n_{t}}\}$ are derived to anchor each keyframe in the
    animation’s continuity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/920aeba3eedc46dc25a0e854bd2bd169.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Overview of proposed LASER. (a) The LLM-driven Controller first parses
    the descriptive prompts to generate the descriptive prompts for corresponding
    frames of animation. (b) By doing so, the LLM analyzes these prompts to the feature
    and attention injection control signals, to facilitate the simultaneous portrayal
    of both texture and non-rigid editing. (c) The animation generator leverages spherical
    linear interpolation and adaptive instance normalization to generate the intermediate
    images between keyframes, accessing smooth animation generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Preliminary for Diffusion Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diffusion models (Ho et al., [2020](#bib.bib13)) (Song et al., [2020a](#bib.bib36)) (Nichol
    and Dhariwal, [2021](#bib.bib25)) are a series of probabilistic generative models
    that produce images by gradual denoising from a noise distribution, e.g., Gaussian
    distribution. The generation process consists of two main phases: the forward
    (diffusion) process and reverse (denoising) process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward process gradually adds noise to initial data $x_{0}$ to generate
    a noisy data $x_{t}$ given variance schedule $\alpha_{t}\in(0,1)$ at time-step
    $t$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $q(x_{t}&#124;x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})\mathbf{I}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}$. After $T$ steps, we obtain
    noise $x_{T}\sim\mathcal{N}(0,1)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reverse process aims to gradually clean the noise. By utilizing the Bayzes’
    rules and Markov property, we can intuitively express the conditional probabilities
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{\beta}_{t}$ is a time-dependent constant and added noise $\epsilon$
    can be predicted by a neural network $\epsilon_{\theta}$. By sampling $x_{t-1}$
    iteratively, we finally get a clean image $x_{0}$ from initial Gaussian noise
    $x_{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: We employ a text-conditioned Stable Diffusion (SD) (Rombach et al., [2022](#bib.bib31)),
    which operates within lower-dimensional latent space rather than pixel space.
    It begins with encoding images to latent representation by a variational auto-encoder
    (VAE) (Kingma and Welling, [2013](#bib.bib20)), followed by a diffusion-denoising
    process within the latent space. After denoising, the latent representation is
    decoded back into the image space via a decoder network, culminating in the final
    generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the noise-predicting network $\epsilon_{\theta}$, residual blocks process
    image features to generate intermediate features $f_{t}^{l}$, which are then used
    in the self-attention module to produce $Q$, $K$, $V$ for capturing long-range
    interactions. Subsequently, cross-attention integrates textual prompt $P$ input,
    merging text and image semantics. The attention mechanism can be formulated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $Q$, $K$, and $V$ represent queries, keys, and values, respectively, with
    $d_{k}$ denoting the key/query dimension for scaling dot product. In this model,
    $Q$ originates from spatial features, while $K$ and $V$ come from spatial features
    and text embeddings for self and cross-attention, respectively. Leveraging attention
    layers within the SD model significantly affects image composition and development (Hertz
    et al., [2022](#bib.bib11)) (Tumanyan et al., [2023](#bib.bib39)), guiding image
    editing and synthesis by manipulating attention-related information during denoising (Cao
    et al., [2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. LLM-driven Controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we first utilize LLM to extract aligned textual prompts for
    each key animation stage. Our approach supports two input modalities: text-image
    pairs and text-only inputs. If the user provides an image, it is directly utilized
    as the initial image $I_{0}$. In cases where the initial image is absent, we leverage
    pre-trained Stable Diffusion models to generate $I_{0}$.To generate animations
    that adhere to the semantics of a specified text description $P_{*}$, we require
    text prompts $\{P_{0},P_{1},\ldots,P_{n_{t}}\}$ for each key animation stage,
    as these prompts directly guide the animation process. High-quality, detailed
    text prompts are crucial when no initial image is provided, as the model generates
    $I_{0}$ based on $P_{0}$’s semantic cues. Prompts for Stable Diffusion should
    be richly descriptive to accurately produce a high-quality starting image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enhance the quality and stability of the process, we introduce two agents
    based on large language models: the “Stage Image Text Prompt Agent” (SIA) and
    the “Stable Diffusion Prompt Generator Agent” (PGA). Initially, SIA generates
    text prompts that guide the image generation for each key stage, as illustrated
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Methodology ‣ LASER: Tuning-Free LLM-Driven
    Attention Control for Efficient Text-conditioned Image-to-Animation") (a). SIA
    generates text prompts based on two fundamental principles: i) By decomposing
    the animation descriptor $P_{*}$ into multiple independent processes, SIA reduces
    semantic differences between adjacent prompts, enhancing the overall quality of
    the results. ii) The prompts must be highly aligned to facilitate high-quality
    intermediate results through linear interpolation. Given the local linearity within
    the CLIP text embedding space (Kawar et al., [2023](#bib.bib19)), minimizing the
    gap between adjacent embeddings is essential. A practical method involves using
    consistent sentence structures across prompts, such as “A cat [action] on the
    ground” and “A [animal] jumping on the ground” (Yang et al., [2023](#bib.bib43)).
    This approach ensures that while the prompts are semantically distinct, they share
    a common categorical root, thus streamlining the generation process. This generation
    method successfully mitigates the non-linearity and discontinuity commonly encountered
    between text embeddings. With the deployment of the Stage Image Text Prompt Agent
    (SIA), we significantly bolster our model’s capacity to generate semantically
    coherent and high-quality images. The Stable Diffusion Prompt Generator Agent
    (PGA) converts broad, high-level concepts from the SIA into richly detailed and
    vividly descriptive prompts specifically crafted for Stable Diffusion. As depicted
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Methodology ‣ LASER: Tuning-Free LLM-Driven
    Attention Control for Efficient Text-conditioned Image-to-Animation") (b), once
    PGA receives the initial text prompt from SIA, it refines this input to craft
    a more detailed prompt. This enhanced prompt not only delineates the subject and
    action but also enriches the scene with specific elements like texture, lighting,
    and artistic style, which instructs Stable Diffusion to produce images of higher
    fidelity and complexity (Lian et al., [2024](#bib.bib23)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Hybrid Prompt-aware Editor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section utilizes the aligned textual prompts to obtain keyframe images.
    During the editing process, $Z^{*}_{1,T}$ is a direct copy of $Z^{*}_{0,T}$. For
    $i\geq 1$, each keyframe $x_{i}$ undergoes DDIM inversion to produce $Z_{i,T}$,
    which is then cloned to form $Z^{*}_{i+1,T}$ for the subsequent keyframe. Despite
    using aligned prompts for text-guiding image editing, we still observe a marked
    discrepancy in semantic identity between the images, which results in animations
    that do not transition smoothly. To overcome this challenge, we draw inspiration
    from previous image editing techniques (Tumanyan et al., [2023](#bib.bib39); Cao
    et al., [2023](#bib.bib7)) and propose a feature and attention injection method
    controlled by the LLM, tailored to query semantically similar content from the
    previous keyframes according to the changing nature of the corresponding stage.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing DDIM inversion on the prior keyframe, we obtain the initial state
    $Z_{i,T}$. Past work (Tumanyan et al., [2023](#bib.bib39)) has demonstrated that
    injecting features $f_{t}^{l}$ within residual blocks and self-attention projections
    $q_{t}^{l}$, $k_{t}^{l}$ significantly boosts text-guided image edition tasks.
    The encoding in the fourth layer $f_{t}^{4}$ specifically captures shared semantics
    necessary for structure retention during generation. Moreover, the injections
    of self-attention are underpinned by the attention scores, which arise from the
    product of query and key vectors, exhibiting a profound connection to the well-established
    self-referential paradigms within neural attention schemas. By injecting specific
    features $f_{t}^{4}$ into the fourth layer of residual blocks and introducing
    self-attention elements $q_{t}^{l}$ and $k_{t}^{l}$ throughout all decoder layers,
    we have successfully achieved texture variations between keyframes. We refer to
    this injection strategy as “Feature and Association Injection” (FAI).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1443618354d2a7944e15ca48d69ff05f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Overview of Feature and Association Injection.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the aforementioned method struggles with non-rigid keyframe modifications.
    The usual solution, limiting injection range to reflect rigid changes from prompts,
    risks losing image identity. To navigate this, especially for non-rigid edits,
    we avoid injecting into residual blocks, thereby maintaining the image’s structural
    integrity without being obscured by local semantics. Our strategy uses targeted
    attention injections. As image layout solidifies early in denoising and self-attention
    queries align semantically (Cao et al., [2023](#bib.bib7)), they can extract content
    from various objects. Post-denoising, we inject keys $k_{t}^{l}$ and values $v_{t}^{l}$
    from the previous keyframe’s self-attention block, as shown in Fig. [4](#S3.F4
    "Figure 4 ‣ 3.3\. Hybrid Prompt-aware Editor ‣ 3\. Methodology ‣ LASER: Tuning-Free
    LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation").
    This process forms objects’ outlines following text prompts and then enriches
    the generative structure with detailed content from the source image. Consequently,
    we achieve semantically coherent images that also support non-rigid transitions.
    We refer to this injection strategy as “Key-Value Attention Injection” (KVAI).
    Up to this point, the model has acquired the capability to generate diverse keyframes,
    enabling the production of the expected animations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recognizing the need for a systematic approach to select the optimal injection
    strategy for each stage of the animation generation, we have developed the Injection
    Control Agent (ICA), as showcased in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Methodology
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation") (a). ICA’s primary role is to process the text prompts from
    the Stage Image Text Prompt Agent (SIA), which performs an in-depth analysis of
    semantic differences between these text prompts at consecutive key stages. This
    analysis enables SIA to issue tailored control signals: “0” signals ICA to deploy
    the injection strategy for stages where texture changes are dominant, and “1”
    signals the use of the KVAI strategy for stages with non-rigid transformations.
    By precisely managing the type of attention injection at each stage, ICA ensures
    that the generated animations are both visually coherent and closely aligned with
    the textual descriptors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d062f9069d7208f7feae3490fdc36a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Overview of Key-Value Attention Injection.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Animation Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we generate intermediate images between keyframe images to
    obtain consistent and smooth animations. After generating the text prompts corresponding
    to each key stage ${P_{0},P_{1},\ldots,P_{n_{t}}}$ in section [3.2](#S3.SS2 "3.2\.
    LLM-driven Controller ‣ 3\. Methodology ‣ LASER: Tuning-Free LLM-Driven Attention
    Control for Efficient Text-conditioned Image-to-Animation"), we obtain the respective
    text embeddings ${e_{0},e_{1},\ldots,e_{n_{t}}}$. When generating intermediate
    images, we perform a simple linear interpolation between the text embeddings of
    two adjacent key stages to obtain the corresponding text embedding $e$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $e_{\alpha}^{i}=(1-\alpha)e_{i}+\alpha e_{i+1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: In the construction of the animation sequence, the interpolation parameter $\alpha$
    is discretized into a series of values that facilitate a smooth transition between
    frames. This discretization is achieved by defining a set of equidistant points
    within the closed interval $[0,1]$, where the number of points corresponds to
    the intended number of frames in an animation stage, denoted as $n_{f}$. Thus,
    $\alpha$ takes on values $\alpha_{0},\alpha_{1},\ldots,\alpha_{n_{f}-1}$, where
    $\alpha_{0}=0$ represents the starting frame, and $\alpha_{n_{f}-1}=1$ indicates
    the ending frame. The intermediate values of $\alpha$ correspond to proportionally
    spaced frames within the animation sequence, ensuring linear spacing. This arrangement
    guarantees that each frame represents a weighted blend of the preceding and subsequent
    key stage embeddings, facilitating a smooth and continuous transformation across
    the animation.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure visual continuity in the sequence of intermediate images, we interpolate
    the latent noise of these images using the latent noise from adjacent key stages.
    However, standard linear interpolation may introduce artifacts. To address this,
    we adopt spherical linear interpolation (slerp)  (Shoemake, [1985](#bib.bib35)),
    which effectively minimizes artifacts and enhances the smoothness of transitions.
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\mathbf{z}_{T\alpha}^{i}=\frac{\sin((1-\alpha)\xi)}{\sin\xi}Z_{i,T}+\frac{\sin(\alpha\xi)}{\sin\xi}Z_{i+1,T}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\xi=\arccos\left(\frac{Z_{i,T}Z_{i+1,T}}{\|Z_{i,T}\|\|Z_{i+1,T}\|}\right)$
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'To maintain consistency in the color and luminance aspects of both generated
    and source images, we implement a variant of Adaptive Instance Normalization (AdaIN) (Huang
    and Belongie, [2017](#bib.bib15)) for the pre-denoising stage adjustment of the
    interpolated latent noise $\mathbf{z}_{0\alpha}^{i}$. We calculate and then interpolate
    the means ($\mu$) and standard deviations ($\sigma$) of the latent noises for
    each channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\mu_{\alpha}^{i}=(1-\alpha)\mu_{i}+\alpha\mu_{i+1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (7) |  | $\sigma_{\alpha}^{i}=(1-\alpha)\sigma_{i}+\alpha\sigma_{i+1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (8) |  | $\tilde{\mathbf{z}}_{0\alpha}^{i}=\sigma_{\alpha}\left(\frac{\mathbf{z}_{0\alpha}^{i}-\mu(\mathbf{z}_{0\alpha}^{i})}{\sigma(\mathbf{z}_{0\alpha}^{i})}\right)+\mu_{\alpha}^{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Subsequently, adjusted latent noise $\tilde{\mathbf{z}}_{0\alpha}^{i}$ supplants
    the original $\mathbf{z}_{0\alpha}^{i}$ during the denoising steps, thereby improving
    the brightness and color consistency of the resulting images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64e1511823946c65bd8493f59a804ed5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Qualitative evaluation. Our method produces animations that significantly
    outperform previous methods in terms of quality, smoothness, and alignment with
    user input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, during the denoising process of each intermediate image $x_{\alpha}^{i}$,
    we also perform feature and self-attention injection. When the stage number $n_{t}$
    is not “-1”, we implement the standard injection strategy, wherein, while generating
    $x_{\alpha}^{i}$, the injection is obtained from $x_{i}$. Furthermore, when SIA’s
    feedback on the “$n_{t}$” is “-1”, it indicates a special request from the user
    for a “single-stage generation,” which involves both texture changes and non-rigid
    transformations within a single animation stage. In such cases, ICA leads the
    model to execute the Hybrid Attention Injection (HAI) strategy. HAI solves the
    issue that when using the normal injection strategies, the model is unable to
    produce animations that simultaneously exhibit changes in texture and structure
    within a single phase. This phenomenon will be further discussed in the [4](#S4
    "4\. Experiments ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient
    Text-conditioned Image-to-Animation"). The HAI process initiates by editing $x_{0}$
    to produce $x_{1}$ using the Feature and Association Injection (FAI), and subsequently
    $x_{2}$ is edited from $x_{1}$ utilizing the KVAI. Following these edits, DDIM
    Inversion is applied to extract the latent representations $Z_{0,T}$ and $Z_{2,T}$,
    which are then interpolated to construct the intermediate latent representation
    $\mathbf{z}_{T\alpha}^{i}$. During the denoising phase, injections are strategically
    administered based on the interpolation parameter $\alpha$; specifically, injections
    from $\{k_{t}^{l},v_{t}^{l}\}$ corresponding to $x_{0}$ are applied in the initial
    (1-$\alpha$)T steps, and those corresponding to $x_{2}$ in the subsequent $\alpha$T
    steps. This method effectively conveys the semantic and structural information
    of significantly transformed images, ensuring smooth and consistent animations
    by querying local structures and textures from input images throughout the denoising
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We employ the publicly available Stable Diffusion v2.1-base (Rombach et al.,
    [2022](#bib.bib31)) as our diffusion model and use GPT-4 8k (OpenAI et al., [2024](#bib.bib26))
    as the LLM in our experiments. For generating the initial image $I_{0}$, we utilize
    two pre-trained models: the real-style model dreamshaper-8 and the anime-style
    model MeinaMix, to assess our model’s capability in producing animations across
    diverse styles. In creating intermediate images, we apply DDIM deterministic sampling.
    For keyframe synthesis, aiming to optimize the balance between efficiency and
    quality, we perform deterministic DDIM inversion with 100 forward steps followed
    by deterministic DDIM sampling with 100 backward steps. When implementing Feature
    and Association Injection (FAI), we inject features and self-attention within
    the first 25 of the 50-step sampling process, specifically targeting layers 4
    to 10 of the U-Net decoder. In the case of Key-Value Attention Injection (KVAI),
    injections commence after the initial five sampling steps and are applied within
    layers 6 to 10 of the decoder. The Hybrid Attention Injection (HAI) method follows
    the same timing and targets the same layers as KVAI. These injection strategies
    can be customized to align with the different input images $I_{0}$. For sampling,
    the classifier-free guidance scale is set at 7.5\. Runtime evaluations are performed
    on an NVIDIA RTX 4090 GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Text-conditioned Image-to-Animation Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our method enables text-guided image-to-animation transitions, leveraging either
    image-textual or solely textual descriptions through pre-trained text-to-image
    diffusion models. Due to the lack of benchmarks for such configurations, we have
    proposed a new mini dataset: Text-conditioned Image-to-Animation Benchmark, which
    consists of 100 sets of textual descriptions. The collection comprises 100 sets,
    categorized as follows: 20 sets of animal actions and appearance transformations,
    20 sets focused on animal appearance and species changes, 20 sets depicting transitions
    in natural landscapes and objects, 20 sets related to human figures and alterations
    in painting styles, 10 sets featuring character identity transformations, and
    10 sets concerning changes in object colors and materials. Our model utilizes
    these textual prompts to generate 100 corresponding animation sequences. This
    benchmark serves as a preliminary evaluation of our model’s performance, and we
    hope it will facilitate further research in this direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Qualitative Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present a visual comparison of our method against prior approaches to underscore
    its superiority. Although there are no other tuning-free methods for text-controlled
    image-to-animation currently available, we draw a detailed comparison with state-of-the-art
    baselines promising for text-controlled image morphing. These include: 1) Diffusion-based
    deep interpolation methods such as DDIM (Song et al., [2020a](#bib.bib36)), Diff.Interp
    (Wang and Golland, [2023](#bib.bib40)), and DiffMorpher (Zhang et al., [2023c](#bib.bib46)),
    all utilizing Stable Diffusion v2.1-base; 2) Text-driven, tuning-free image editing
    methods like PnP (Tumanyan et al., [2023](#bib.bib39)) and MasaCtrl (Cao et al.,
    [2023](#bib.bib7)). For the first category of methods, which depend on multiple
    pre-existing image inputs and lack the capability to generate content directly
    from text, our experimental procedure includes: i) Utilizing LLM control for generating
    consistent outputs, which involves creating initial images and key stage prompts
    through stable diffusion prompt generation, similar to our method. ii) Generating
    initial images using the same stable diffusion checkpoint as employed in our experiments.
    iii) Producing subsequent key stage images via DDIM Inversion. For the second
    category, which also leverages LLM control, we maintain consistent application
    of text embedding interpolation rules to generate intermediate images, aligning
    with our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Generation Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4\. Animation Generator ‣ 3\.
    Methodology ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation"), our method outperforms previous approaches in alignment
    with user input, transition smoothness, semantic coherence, and maintaining the
    animation subject’s semantic identity. Previous methods have often failed to accurately
    respond to user input changes in appearance and motion. These approaches typically
    struggle to generate the intended motions accurately or introduce noticeable artifacts
    post-motion changes, often resulting in a significant loss of primary subject
    information in the images. Compared to previous methods, our approach consistently
    generates coherent animations that closely align with the semantic content of
    the user input, resulting in visually satisfactory outputs. For additional examples
    of generated results, we encourage readers to consult the appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: Generation Diversity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Furthermore, the extensive prior knowledge and generative capabilities of the
    LLM enhance our model’s ability to produce diverse outputs, as shown in Fig. [6](#S4.F6
    "Figure 6 ‣ Generation Efficiency ‣ 4.3\. Quantitative Evaluation ‣ 4\. Experiments
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation"). When users request multiple distinct results, our model
    meets this demand by generating high-quality, varied animations, significantly
    broadening its creative potential.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Quantitative Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Drawing from established objectives in prior research (Yang et al., [2023](#bib.bib43);
    Zhang et al., [2023c](#bib.bib46); Cao et al., [2023](#bib.bib7)), we quantitatively
    evaluate the models using these metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Learned Perceptual Image Patch Similarity (LPIPS, $\downarrow$) (Zhang et al.,
    [2018a](#bib.bib48)): LPIPS is employed to assess the perceptual deviation within
    an animation sequence in our work. We compute the total LPIPS (LPIPS[T]) to quantify
    the overall perceptual variance throughout the sequence, highlighting the dynamic
    range of visual changes. Additionally, the maximum LPIPS to the nearest endpoint
    (LPIPS[M]) is determined to identify the maximum perceptual variance, providing
    insights into the most significant changes within the animation. These measurements
    are crucial for assessing directness of the animation, ensuring finding the most
    efficient transition to generate animation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CLIP Score ($\uparrow$) (Hessel et al., [2021](#bib.bib12)): The CLIP score
    is a metric that quantifies the alignment between images and textual descriptions,
    serving as a powerful tool for evaluating the coherence and relevance of generated
    images about their specified textual prompts. For an intermediate image, we describe
    its CLIP score by calculating its average similarity with the prompt before editing
    and the prompt after editing (e.g., $x_{\alpha}^{0}$ with $P_{0}$ and $P_{1}$,
    $x_{\alpha}^{1}$ with $P_{1}$ and $P_{2}$, etc.).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perceptual Path Length (PPL, $\downarrow$) (Karras et al., [2020](#bib.bib18)):
    To evaluate smoothness, i.e., transitions within the generated animation sequence
    should be seamless between any two consecutive images, we compute PPL: $\operatorname{PPL}_{\epsilon}=\mathbb{E}_{\alpha\sim
    U(0,1)}[\frac{1}{\epsilon^{2}}\operatorname{LPIPS}(\boldsymbol{x}^{(\alpha)},\boldsymbol{x}^{(\alpha+\varepsilon)})]$,
    where $\epsilon$ is a small constant and we set it to $\frac{1}{n_{f}-1}$. It
    is worth noting that we regard the entire sequence as a single animation process,
    despite consisting of multiple stages.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The quantitative evaluation results of all methods are presented in Table [1](#S4.T1
    "Table 1 ‣ Generation Efficiency ‣ 4.3\. Quantitative Evaluation ‣ 4\. Experiments
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation").'
  prefs: []
  type: TYPE_NORMAL
- en: Generation Quality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our method achieves a leading Clip Score, demonstrating its semantic alignment
    with user input. While DDIM may excel in CLIP Score, it often compromises structural
    coherence in favour of textual alignment, an issue our approach adeptly avoids
    as demonstrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4\. Animation Generator ‣ 3\.
    Methodology ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation"). Due to PnP’s inability to effectively perform non-rigid
    edits, its generated animations often exhibit only appearance changes, thereby
    achieving higher levels of smoothness. This limitation hinders its capability
    to handle a diverse range of animation generation tasks. Similarly, Masacontrol,
    which struggles with texture transformations, also falls short in producing a
    diverse range of animations. Even when benchmarked against deep interpolation
    techniques that require fine-tuning, our results consistently exhibit superior
    smoothness. This performance not only underscores the effectiveness of our method
    but also affirms its suitability for adapting to a wide range of text-conditioned
    image-to-animation generation scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Generation Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In assessing efficiency, our method uniquely blends quality with speed, setting
    it apart from other deep interpolation techniques. This superior performance primarily
    stems from our operation without the need for fine-tuning. Although it may not
    lead in all image editing benchmarks, our model excels at managing a diverse array
    of edits, enabling it to adeptly tackle a broad spectrum of generative tasks.
    Given this versatility, the efficiency of our method is exceptionally high.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/394c25aaca56f4b1d9cb490da93dd880.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. The rich prior knowledge of the LLM grants the model the ability
    to generate diverse outcomes from the same input text and image.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Comparison of current methods. Superscript ^($\clubsuit$) indicates
    that the model employs an external network (*e.g.*, ControlNet (Zhang et al.,
    [2023b](#bib.bib47))) to generate intermediate images and ^† indicates that the
    model fine-tunes with training LoRA (Hu et al., [2021](#bib.bib14)). “TE” stands
    for texture editing, a process that involves altering the surface appearance of
    objects within an image to match a specific texture style, while preserving the
    underlying structure and layout of the scene. “NRIE” refers to non-rigid image
    editing which involves altering the shape and structure of objects in images,
    like changing facial expressions or body poses. “AG”, standing for animation generation,
    refers to producing intermediate images between keyframes.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | Characteristics | Metrics | Runtime$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| 4  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 8  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | TE | NRIE | AG | CLIP Score $\uparrow$ | LPIPS[$T$] $\downarrow$ | LPIPS[$M$]
    $\downarrow$ | PPL $\downarrow$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|   DDIM(Song et al., [2020a](#bib.bib36)) | ✓ |  |  | 27.37 | 3.13 | 0.49
    | 36.91 | 32s |'
  prefs: []
  type: TYPE_TB
- en: '| PnP(Hertz et al., [2022](#bib.bib11)) | ✓ |  |  | 26.57 | 0.90 | 0.23 | 10.51
    | 2min |'
  prefs: []
  type: TYPE_TB
- en: '| MasaCtrl(Cao et al., [2023](#bib.bib7)) |  | ✓ |  | 26.56 | 1.54 | 0.28 |
    17.96 | 37s |'
  prefs: []
  type: TYPE_TB
- en: '| Diff.Interp^($\clubsuit$)(Wang and Golland, [2023](#bib.bib40)) |  |  | ✓
    | 20.05 | 5.14 | 0.58 | 72.29 | 2min6s |'
  prefs: []
  type: TYPE_TB
- en: '| DiffMorpher^†(Zhang et al., [2023c](#bib.bib46)) |  |  | ✓ | 26.94 | 0.99
    | 0.40 | 14.78 | 1min46s |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | ✓ | ✓ | ✓ | 26.99 | 1.22 | 0.25 | 14.14 | 41s |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 4.4\. Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have conducted an ablation study to evaluate the effectiveness of the proposed
    components, with experimental results shown in Table [2](#S4.T2 "Table 2 ‣ 4.4\.
    Ablation Study ‣ 4\. Experiments ‣ LASER: Tuning-Free LLM-Driven Attention Control
    for Efficient Text-conditioned Image-to-Animation") and Fig.2 (in Appendix). The
    findings demonstrate that using DDIM alone cannot accurately restore the structure
    of the input image. In contrast, our feature and self-attention injections address
    the loss of texture and structural information during the DDIM generation process,
    significantly enhancing the quality of the generated animations. However, remnants
    of structural features from the initial state are still noticeable in the generated
    animations, including in the intermediate segments. The implementation of Latent
    Interpolation addresses this issue. While it may slightly elevate the $LPIPS_{T}$
    and PPL metrics, it ensures that subsequent frames more accurately reflect the
    semantic information of the transformed state. After applying the AdaIN adjustment
    to the latent noise, the consistency of brightness and color across the image
    sequence has improved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the effectiveness of Hybrid Attention Injection (HAI) in producing
    single-stage animations that incorporate both texture changes and non-rigid transformations,
    we conducted qualitative experiments. We generated animations using basic injection
    strategies (FAI and KVAI) and HAI, with the results displayed in Fig. [7](#S4.F7
    "Figure 7 ‣ 4.4\. Ablation Study ‣ 4\. Experiments ‣ LASER: Tuning-Free LLM-Driven
    Attention Control for Efficient Text-conditioned Image-to-Animation"). When employing
    only FAI, the images failed to respond to non-rigid changes; using KVAI alone
    did not result in significant texture modifications. Our proposed HAI strategy
    successfully handles both texture and non-rigid changes, effectively fulfilling
    the task of single-stage animation generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f5cdcc473a8a4a3b027a9d33675d2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. The comparative effects of different injection strategies given the
    textual description “A sitting cat turns into a jumping dog”.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Ablation study results. Injection, Latent Interp, and AdaIN represent
    different components studied in the ablation.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | Components | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| 4  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 8  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Injection | Latent Interp | AdaIN | Clip Score $\uparrow$ | LPIPS[$T$]
    $\downarrow$ | LPIPS[$M$] $\downarrow$ | PPL $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|   DDIM(Song et al., [2020a](#bib.bib36)) |  |  |  | 27.37 | 3.13 | 0.49 |
    36.91 |'
  prefs: []
  type: TYPE_TB
- en: '| - | ✓ |  |  | 26.73 | 1.09 | 0.26 | 12.85 |'
  prefs: []
  type: TYPE_TB
- en: '| - | ✓ | ✓ |  | 26.81 | 1.22 | 0.26 | 14.03 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | ✓ | ✓ | ✓ | 26.99 | 1.22 | 0.25 | 14.14 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce LASER, a tuning-free LLM-driven attention control framework that
    utilizes pre-trained text-to-image models to generate high-quality and smooth
    animations from multimodal inputs. Experimental results validate the superior
    performance of our method, which consistently produces diverse and high-quality
    animations. We believe our approach demonstrates significant potential and serves
    as an inspiration for future research in this field.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aloraibi (2023) Alyaa Qusay Aloraibi. 2023. Image morphing techniques: A review.
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2023) Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole
    Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. 2023. One transformer fits all
    distributions in multi-modal diffusion at scale. In *International Conference
    on Machine Learning*. PMLR, 1692–1717.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brock et al. (2018) Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large
    scale GAN training for high fidelity natural image synthesis. *arXiv preprint
    arXiv:1809.11096* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brooks et al. (2023) Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023.
    Instructpix2pix: Learning to follow image editing instructions. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 18392–18402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2023) Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu
    Qie, and Yinqiang Zheng. 2023. Masactrl: Tuning-free mutual self-attention control
    for consistent image synthesis and editing. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 22560–22570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crowson et al. (2022) Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
    Stander, Eric Hallahan, Louis Castricato, and Edward Raff. 2022. Vqgan-clip: Open
    domain image generation and editing with natural language guidance. In *European
    Conference on Computer Vision*. Springer, 88–105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion
    models beat gans on image synthesis. *Advances in neural information processing
    systems* 34 (2021), 8780–8794.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming
    transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*. 12873–12883.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hertz et al. (2022) Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael
    Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention
    control. *arXiv preprint arXiv:2208.01626* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. Clipscore: A reference-free evaluation metric for image
    captioning. *arXiv preprint arXiv:2104.08718* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in neural information processing systems*
    33 (2020), 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang and Belongie (2017) Xun Huang and Serge Belongie. 2017. Arbitrary style
    transfer in real-time with adaptive instance normalization. In *Proceedings of
    the IEEE international conference on computer vision*. 1501–1510.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. (2021) Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen,
    Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. Alias-free generative adversarial
    networks. *Advances in neural information processing systems* 34 (2021), 852–863.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*. 4401–4410.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. (2020) Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
    Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality
    of stylegan. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*. 8110–8119.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kawar et al. (2023) Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
    Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real
    image editing with diffusion models. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 6007–6017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr.
    2020. Manigan: Text-guided image manipulation. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*. 7880–7889.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao
    Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. 2023. Fine-tuning
    multimodal llms to follow zero-shot demonstrative instructions. In *The Twelfth
    International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. (2024) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2024.
    LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models. arXiv:2305.13655 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nam et al. (2018) Seonghyeon Nam, Yunji Kim, and Seon Joo Kim. 2018. Text-adaptive
    generative adversarial networks: manipulating images with natural language. *Advances
    in neural information processing systems* 31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nichol and Dhariwal (2021) Alexander Quinn Nichol and Prafulla Dhariwal. 2021.
    Improved denoising diffusion probabilistic models. In *International conference
    on machine learning*. PMLR, 8162–8171.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parmar et al. (2023) Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
    Li, Jingwan Lu, and Jun-Yan Zhu. 2023. Zero-shot image-to-image translation. In
    *ACM SIGGRAPH 2023 Conference Proceedings*. 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patashnik et al. (2021) Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
    and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery.
    In *Proceedings of the IEEE/CVF international conference on computer vision*.
    2085–2094.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
    Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving
    latent diffusion models for high-resolution image synthesis. *arXiv preprint arXiv:2307.01952*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International conference on machine learning*. PMLR, 8748–8763.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion
    models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*. 10684–10695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
    Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with
    deep language understanding. *Advances in neural information processing systems*
    35 (2022), 36479–36494.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sauer et al. (2023) Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger,
    and Timo Aila. 2023. Stylegan-t: Unlocking the power of gans for fast large-scale
    text-to-image synthesis. In *International conference on machine learning*. PMLR,
    30105–30118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sauer et al. (2022) Axel Sauer, Katja Schwarz, and Andreas Geiger. 2022. Stylegan-xl:
    Scaling stylegan to large diverse datasets. In *ACM SIGGRAPH 2022 conference proceedings*.
    1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shoemake (1985) Ken Shoemake. 1985. Animating rotation with quaternion curves.
    In *Proceedings of the 12th annual conference on Computer graphics and interactive
    techniques*. 245–254.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020a) Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020a. Denoising
    diffusion implicit models. *arXiv preprint arXiv:2010.02502* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020b) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. 2020b. Score-based generative modeling through
    stochastic differential equations. *arXiv preprint arXiv:2011.13456* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tumanyan et al. (2023) Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel.
    2023. Plug-and-play diffusion features for text-driven image-to-image translation.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    1921–1930.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Golland (2023) Clinton Wang and Polina Golland. 2023. Interpolating
    between images with diffusion models. (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2021) Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. 2021.
    Tedigan: Text-guided diverse face image generation and manipulation. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 2256–2265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation
    with attentional generative adversarial networks. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*. 1316–1324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh,
    Jing Zhang, Dylan Campbell, Peter Tu, and Richard Hartley. 2023. Impus: Image
    morphing with perceptually-uniform sampling using diffusion models. *arXiv preprint
    arXiv:2311.06792* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *Proceedings
    of the IEEE international conference on computer vision*. 5907–5915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2018b. Stackgan++: Realistic image
    synthesis with stacked generative adversarial networks. *IEEE transactions on
    pattern analysis and machine intelligence* 41, 8 (2018), 1947–1962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023c) Kaiwen Zhang, Yifan Zhou, Xudong Xu, Xingang Pan, and
    Bo Dai. 2023c. DiffMorpher: Unleashing the Capability of Diffusion Models for
    Image Morphing. *arXiv preprint arXiv:2312.07409* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023b) Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. Adding
    conditional control to text-to-image diffusion models. In *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*. 3836–3847.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
    and Oliver Wang. 2018a. The unreasonable effectiveness of deep features as a perceptual
    metric. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 586–595.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024) Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan
    Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, et al. 2024. HyperLLaVA:
    Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models.
    *arXiv preprint arXiv:2403.13447* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023a) Wenqiao Zhang, Zheqi Lv, Hao Zhou, Jia-Wei Liu, Juncheng
    Li, Mengze Li, Siliang Tang, and Yueting Zhuang. 2023a. Revisiting the Domain
    Shift and Sample Uncertainty in Multi-source Active Domain Transfer. *arXiv preprint
    arXiv:2311.12905* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang Pu, Fei
    Wu, and Yueting Zhuang. 2019. Frame augmented alternating attention network for
    video question answering. *IEEE Transactions on Multimedia* 22, 4 (2019), 1032–1041.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang,
    Andrew Makmur, Qingpeng Cai, and Beng Chin Ooi. 2022. Boostmis: Boosting medical
    image semi-supervised learning with adaptive pseudo labeling and informative active
    annotation. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*. 20666–20676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan:
    Dynamic memory generative adversarial networks for text-to-image synthesis. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    5802–5810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zope and Zope (2017) Bhushan Zope and Soniya B Zope. 2017. A Survey of Morphing
    Techniques. *International Journal of Advanced Engineering, Management and Science*
    3, 2 (2017), 239773.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
