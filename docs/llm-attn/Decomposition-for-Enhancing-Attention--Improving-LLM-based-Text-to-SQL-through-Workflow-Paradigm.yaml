- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:02:56'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through
    Workflow Paradigm'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10671](https://ar5iv.labs.arxiv.org/html/2402.10671)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yuanzhen Xie^(1,2)   Xinzhou Jin²  Tao Xie¹   Mingxiong Lin¹  Liang Chen²
  prefs: []
  type: TYPE_NORMAL
- en: Chenyun Yu¹  Lei Cheng¹  Chengxiang Zhuo¹  Bo Hu¹  Zang Li¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Platform and Content Group, Tencent
  prefs: []
  type: TYPE_NORMAL
- en: ²Sun Yat-sen University
  prefs: []
  type: TYPE_NORMAL
- en: xieyzh3@gmail.com    jinxzh5@mail2.sysu.edu.cn chenliang6, yuchy35@mail.sysu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '{devintxie, matrixmxlin, echokong, raycheng, felixzhuo, harryyfhu, gavinzli}@tencent.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In-context learning of large-language models (LLMs) has achieved remarkable
    success in the field of natural language processing, while extensive case studies
    reveal that the single-step chain-of-thought prompting approach faces challenges
    such as attention diffusion and inadequate performance in complex tasks like text-to-SQL.
    To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow
    paradigm method is proposed, aiming to enhance the attention and problem-solving
    scope of LLMs through decomposition. Specifically, the information determination
    module for eliminating redundant information and the brand-new prompt structure
    based on problem classification greatly enhance the model’s attention. Additionally,
    the inclusion of self-correcting and active learning modules greatly expands the
    problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches.
    Extensive experiments conducted on three datasets demonstrate that our approach
    outperforms other methods by a significant margin. About 2-3 percentage point
    improvements compared to the existing baseline on the Spider Dev and Spider-Realistic
    datasets and new SOTA results on the Spider Test dataset are achieved. Our code
    is available on GitHub: [https://github.com/FlyingFeather/DEA-SQL](https://github.com/FlyingFeather/DEA-SQL).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through
    Workflow Paradigm'
  prefs: []
  type: TYPE_NORMAL
- en: Yuanzhen Xie^(1,2)   Xinzhou Jin²  Tao Xie¹   Mingxiong Lin¹  Liang Chen² Chenyun
    Yu¹  Lei Cheng¹  Chengxiang Zhuo¹  Bo Hu¹  Zang Li¹ ¹Platform and Content Group,
    Tencent ²Sun Yat-sen University xieyzh3@gmail.com    jinxzh5@mail2.sysu.edu.cn
    chenliang6, yuchy35@mail.sysu.edu.cn {devintxie, matrixmxlin, echokong, raycheng,
    felixzhuo, harryyfhu, gavinzli}@tencent.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many datasets are stored in tabular form, utilizing relational databases. Extracting
    data from such sources typically requires skilled professionals to access it through
    manually crafted structured query language (SQL) queries. This limitation restricts
    the accessibility of ubiquitous relational data to a broader range of non-technical
    users. The text-to-SQL parsing (Finegan-Dollak et al., [2018](#bib.bib7); Yu et al.,
    [2018](#bib.bib27); Qin et al., [2022](#bib.bib19)) task aims to bridge this gap
    by translating natural language questions (NLQ) into SQL query statements given
    a database schema without additional knowledge. This significantly reduces the
    barrier to database access, thereby garnering increasing attention from both academia
    and industry.
  prefs: []
  type: TYPE_NORMAL
- en: Early works on text-to-SQL parsing in the database community (Wang et al., [2020](#bib.bib23);
    Scholak et al., [2021](#bib.bib21); Hui et al., [2022](#bib.bib10)) have made
    significant strides. A dominant approach involves collecting labeled data and
    training models through supervised learning. While effective, this method requires
    a large amount of labeled training data, which entails high costs in both annotating
    SQL queries and conducting model training. As an alternative to supervised learning,
    in-context learning (Dong et al., [2022](#bib.bib5)) represents an emerging capability
    of large language models (LLMs), reducing the reliance on extensive datasets.
    With just a few examples, in-context learning enables LLMs to demonstrate performance
    comparable to or even surpassing fully supervised models across many natural language
    processing (NLP) tasks, such as sentiment analysis, machine translation and natural
    language inference (Liang et al., [2023](#bib.bib14); Xu et al., [2023](#bib.bib26);
    Wei et al., [2022](#bib.bib25)).
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning has also shown promising results in text-to-SQL parsing.
    Existing in-context learning methods (Tai et al., [2023](#bib.bib22); Chang and
    Fosler-Lussier, [2023a](#bib.bib2)) often employ techniques like chain-of-thought
    (COT), using prompts to accomplish complex text-to-SQL tasks. We conduct single-step
    COT experiments using GPT-4 on the Spider dataset and find that while COT does
    increase LLMs’ problem-solving scope (the correct solution produced by the LLMs)
    to some extent, however, the performance of these single-step prompts is limited.
    For example, instructions added within a lengthy prompt text might be disregarded
    due to the LLM’s attention being diluted by the excessive length of the text.
    Additionally, improving the prompt text often results in a seesaw effect, making
    it difficult to improve the overall performance as it is not universal for different
    types of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional NLP tasks, text-to-SQL, which involves converting natural
    language to structured language, is more challenging. The DIN-SQL Pourreza and
    Rafiei ([2023](#bib.bib18)) method provides a new direction and achieves some
    success by decomposing steps and incorporating universal self-correction. However,
    this approach lacks targeted steps for specific error types, which restricts the
    effective expansion of LLMs’ problem-solving scope. Also with its fixed few-shot
    learning mechanism, it hampers the adaptability and generalizability of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose a workflow paradigm method for decomposing and enhancing
    attention based on few-shot. This method draws on human thinking patterns, adheres
    to the principle of making subtasks as simple as possible, and reduces irrelevant
    information in each step to specifically enhance the solvable scope of LLM and
    improve the attention of LLM to enhance their performance. It consists of five
    sub-modules: an Information Determination module that focuses on attention by
    reducing interference information through a two-stage method; a Classification
    & Hint module that solves different problems that cannot be generalized by simply
    providing different simple prompts; a few-shot SQL Generation module based on
    question template retrieval; a Self-Correcting module based on error summarization;
    and an Active Learning module that expands the model’s capabilities based on error
    cases. To reduce consumption, we minimized the number of few-shots required for
    each step, with the second stage of field filtering and the self-correction step
    being zero-shot and the SQL generation module outperforming existing few-shot-based
    baselines in a zero-shot environment. It has been found that the workflow prompting
    paradigm is more effective in improving overall performance compared to single-step
    prompting. In the experiments, execution accuracies of 85.4 and 81.5 were achieved
    on the Spider-dev and Spider-Realistic datasets, respectively, surpassing existing
    models and in-context learning schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized as: 1) Propose a workflow paradigm solution
    to boost the attention of LLMs for complex problems as an example for text-to-SQL
    tasks; 2) Design a two-stage information filtering module to curtail irrelevant
    information to enhance the attention of LLMs, while adapting realistic questions
    with different questioning styles, which performs better on datasets that are
    closer to realistic questioning styles; 3) Propose a new prompt structure for
    text-to-SQL tasks. Categorize the problems and use different prompt patterns for
    different types of problems, presenting the key information to the model in a
    more explicit way to better improve the performance of the model; 4) The integration
    of LLMs for self-correction and active learning further improves the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c918a9a0e6d883d4fd828bca8606851.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The overall structure of the DEA-SQL model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The emergence of chain-of-thought based on prompt engineering has stimulated
    the ability threshold of large language models(LLMs) to shine. Although prompt
    engineering has achieved some success in major domains, relying solely on the
    stimulation of prompt engineering is not enough for LLMs to learn to solve complex
    tasks. On the contrary, this approach of stacking a large amount of text content
    poses several major drawbacks: 1) If the LLM focuses on a lot of points at once,
    its attention becomes diluted and it is difficult to cover all the points, so
    the effect may be reduced based on not concocting the prompts attentively. 2)
    It is more difficult to get LLMs to focus on the specific issues you raise within
    a large amount of text.'
  prefs: []
  type: TYPE_NORMAL
- en: When faced with complex problems, people’s solutions are usually to continuously
    try to break them down into multiple steps, ultimately coming up with a relatively
    complete process methodology to improve quality and efficiency. Inspired by this
    idea, we attempt to approach it from the perspective of workflow, breaking down
    the work into finer details to address the challenges mentioned in the introduction.
    Each time, we let the LLMs focus on a single atomic task, adhering to the principle
    of keeping the task as simple as possible and the information as streamlined as
    possible. This allows the LLMs’ attention to be concentrated, thereby enhancing
    their comprehension ability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the text-to-SQL task, the common solution process can be summarized as follows:
    1) Determine the necessary database information; 2) Identify the query type of
    the problem; 3) Consider the problem-solving approach based on the question type
    and write the corresponding SQL; 4) Perform a preliminary self-check on the SQL;
    5) Recall past mistake examples, check if the current answer has the same error
    points, and avoid repeating the same mistakes. Thus, based on the idea that decomposition
    for enhancing attention, we propose the workflow paradigm method named DEA-SQL
    with five modules as shown in Figure [1](#S2.F1 "Figure 1 ‣ 2 Methodology ‣ Decomposition
    for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm"):
    Information determination, Classification & Hint, SQL generation, Self-correction,
    Active learning. Some modules like information determination can be well-solved
    using traditional techniques. We choose to explore the comprehensive capabilities
    of LLMs at each step to demonstrate the effectiveness of the attention-focused
    workflow paradigm in in-context learning. Thus, each step is implemented using
    the LLM based on prompt engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Information Determination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The information determination step is primarily responsible for identifying
    the schema of the specific database needed for the problem. Some studies Lei et al.
    ([2020](#bib.bib11)); Pourreza and Rafiei ([2023](#bib.bib18)) have demonstrated
    that schema linking facilitates cross-domain generalization and synthesis of complex
    queries. A simple starting point for this step is that giving too much unwieldy
    information to the LLMs can overly interfere with their ability to comprehend
    it. By minimizing such interference, we can potentially improve the attention
    and effectiveness of the LLMs. Specifically, determine the elemental information
    about the tables and columns that the problem needs to use before determining
    the computational logic for the specific SQL, and pare down any other extraneous
    table information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common techniques for determining table and column information typically rely
    on one-step prompts and few-shots approaches Dong et al. ([2023](#bib.bib6));
    Pourreza and Rafiei ([2023](#bib.bib18)). However, due to the more open-ended
    nature of the questions being asked, these methods often struggle to capture all
    the key vocabulary, rendering them relatively ineffective. To address the issue
    of instability in LLMs caused by changes in the problem, we propose a two-stage
    approach that differs from existing methods: we first identify the problem elements(elements
    identification) and then go through them to select the required vocabulary to
    further refine the relatively complex problem (Information filter) as shown in
    Figure [1](#S2.F1 "Figure 1 ‣ 2 Methodology ‣ Decomposition for Enhancing Attention:
    Improving LLM-based Text-to-SQL through Workflow Paradigm"). The primary issues
    requiring attention can also be identified during this stage, allowing the LLMs
    to contemplate these concerns during the SQL generation phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Classification & Hint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf7296989c835c7c9c9c673b3a39045b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: DEA-SQL model’s prompt structure in SQL Generation module.'
  prefs: []
  type: TYPE_NORMAL
- en: In the error analysis of directly using the LLM and inspired by DIN-SQL Pourreza
    and Rafiei ([2023](#bib.bib18)), it is found that it has weaker support for nested
    queries and joint problems. The accuracy of distinguishing between nesting and
    joining when directly writing SQL is relatively low. It implements the join problem
    using "IN" or other nested queries. In addition, for complex multi-level joint
    problems, it is challenging to comprehend the correct join conditions at a glance
    when confronted with a plethora of table creation information. This makes LLMs
    easy to envision or employ incorrect join conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these issues, the classification & hint module is proposed. We divide
    the problems into four major categories based on whether the SQL requires nested
    subqueries and whether it requires joining tables: easy, join, nested, and join-nested
    problems. The "easy" category refers to problems that do not require joining tables
    or utilizing nested queries. The "join" category pertains to problems that necessitate
    joining tables but not nested queries. The "nested" category encompasses problems
    that do not require joining tables but do demand nested queries. Finally, the
    "join-nested" category includes problems that call for both joining tables and
    employing nested queries. Subsequently, in the following SQL generation step,
    we add special prompt words for different problem types to remind it to pay attention
    to the relevant problem types as shown in Figure [1](#S2.F1 "Figure 1 ‣ 2 Methodology
    ‣ Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through
    Workflow Paradigm").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since SQL is a language for structured databases, there is a certain gap between
    it and natural language. As query problems grow more complex, there is a need
    for additional information to enhance the precision of the LLMs. To help the LLM
    focus its attention, we provide additional information about join conditions in
    the form of a list structure. Comparing the Database and Link info in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Methodology ‣ Decomposition for Enhancing Attention: Improving LLM-based
    Text-to-SQL through Workflow Paradigm"), it is obvious that the Link information
    is more intuitive for humans or models to select the correct join condition. In
    addition, different hints (H) are provided in SQL generation based on the type
    of question as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Classification & Hint
    ‣ 2 Methodology ‣ Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL
    through Workflow Paradigm") for the Hints.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 SQL Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different question types necessitate various focal points. In accordance with
    the classification of questions, we incorporate distinct prompts for each category
    to emphasize the aspects that demand particular attention within that question
    type. The overall question prompt follows the format $<F,D,H-FK,I,Q,H,S></math>
    shown as Figure [2](#S2.F2 $), where $F$ represents few-shots that are optional,
    $D$ denotes database, $H-FK$ represents the foreign keys that are optional, $I$
    indicates command information that describes the problem requirements, $Q$ represents
    the question, $H$ denotes the hints depending on the question type and $S$ denotes
    the specification depending on the question type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex problems, which may be relatively less seen from the model’s perspective,
    cannot be effectively solved by merely modifying the prompt. To stimulate the
    model’s ability to solve such problems, we use specific examples as prompts. Experiments
    have found that LLMs are highly sensitive to sample selection, and choosing irrelevant
    samples may even produce negative effects. This concept is straightforward to
    grasp: absorbing too many solutions to unrelated problems can distract the model,
    making it more challenging to determine the correct method for problem-solving.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the few-shot version, we also categorize the few-shot library according
    to problem type. Intuitively, examples identified based on problem type are more
    likely to fulfill the requirements of finding the most relevant problem. We establish
    various retrieval methods, which can be divided into two main types: full retrieval
    and retrieval by problem type. In the latter, there are three subtypes: random,
    question similarity (ques_sim), and template similarity (tem_sim). Relying solely
    on question similarity can often result in retrieving questions that aren’t particularly
    similar. Template similarity, on the other hand, begins by obtaining the retrieval
    question paradigm and mitigates the effect of question diversity. To obtain the
    retrieval question paradigm, key entities for word selection are concealed to
    generate the question skeleton (template). This approach can effectively minimize
    the impact of different entities in similar questions, or identical entities in
    dissimilar questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Self-Correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/034c2e41ad240358dfa3fb168a8418a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The statistics of SQL errors based on information determination,
    classification & hint and SQL generation steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'After analyzing the results generated by the LLM in the previous steps, it
    is found that although the decomposition in the previous steps reduces redundant
    information and focuses the effective attention of the LLM, resulting in an improvement
    in accuracy(The outcome closely aligns with the most recent baseline method, achieving
    83.4%. This can be observed in the ’w/0 active & correct’ result in Table [3](#S3.T3
    "Table 3 ‣ 3.3 Ablation Study (RQ2) ‣ 3 Experiment ‣ Decomposition for Enhancing
    Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm").) , there
    are still some issues in field selection, table joining, aggregation, etc. The
    analysis of 98 incorrect questions reveals the proportion of error types, as depicted
    in Figure [3](#S2.F3 "Figure 3 ‣ 2.4 Self-Correction ‣ 2 Methodology ‣ Decomposition
    for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm").
    Notably, several significant error points emerge: 1) Extra fields: The LLM often
    selects an excessive number of fields, rather than limiting its selection to those
    pertinent to the question; 2) Incorrect fields: For instance, when faced with
    fields bearing identical names across different tables, the alias may be omitted,
    leading to errors; 3) Table and field association errors: there may be inconsistencies
    between the tables and fields used; 4) Fabricated conditions for table joins;
    5) Misuse of association words: For example, there is a tendency to habitually
    use ’left join’ in place of ’join’; 6) Group or order by errors: Mistakes such
    as incorrect aggregation fields and conditions may be encountered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to the aforementioned issues, we establish prompts as demonstrated
    in the Self-Correction step in Figure [1](#S2.F1 "Figure 1 ‣ 2 Methodology ‣ Decomposition
    for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm").
    Instead of using entirely generic prompts like DIN-SQL Pourreza and Rafiei ([2023](#bib.bib18)),
    we analyze the ability threshold of LLMs (i.e., the types of mistakes they frequently
    make) and set up relevant error prompts specifically targeting those issues, following
    the generic prompt points.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Active Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model itself has a limited capability threshold and the circles it specializes
    in do not encompass all problem types in SQL as shown in the Active Learning module
    in Figure [1](#S2.F1 "Figure 1 ‣ 2 Methodology ‣ Decomposition for Enhancing Attention:
    Improving LLM-based Text-to-SQL through Workflow Paradigm"). In the experiment,
    we discovered that the model is more prone to errors for certain problem types
    (e.g., extremum problems), often producing fixed-paradigm yet incorrect answers.
    We aim to determine whether the model’s capability thresholds can be expanded
    through the Active Learning Module to better meet the requirements. The model’s
    generalization ability is used to present the wrong case and the standard answer
    to the model to learn the error correction paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we sample part of the training set for the preamble step of the
    test, analyze the error type of the LLM to organize and summarize some of the
    samples, and ultimately select three fixed typical samples as the error case.
    The case format $C$ follows $<Q,O,A></math>, where <math id=$ represents the question
    of one case, $O$ represents the origin SQL that is generated by the LLM and $A$
    is the answer including whether it needs to modify and the modified SQL. The final
    active learning prompt follows $<I,C,Q></math>, where <math id=$ represents the
    instruction that cues the need for the model to accomplish this task, $C$ represents
    the three fixed error cases and $Q$ is the question that should be answered.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we evaluate the proposed algorithm for the text-to-SQL task.
    Extensive experiments have been conducted to answer the following research questions:
    RQ1. How does DEA-SQL perform vs. state-of-the-art baselines? RQ2. Whether each
    module of our method works effectively and how are they impacting the task? RQ3.
    How do model parameters like the number of few-shots or the information filter
    layers affect the model?'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spider Yu et al. ([2018](#bib.bib27)).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Spider is a large-scale dataset for complex, cross-domain semantic parsing and
    text-to-SQL tasks. It consists of 10,181 questions (8,659 examples in the training
    set and 1,034 examples in the development set (Spider-dev).) and 5,693 unique
    complex SQL queries, involving 200 databases and multiple tables covering 138
    different domains.
  prefs: []
  type: TYPE_NORMAL
- en: Spider Realistic Deng et al. ([2021](#bib.bib4)).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Spider Realistic is a more challenging version of the Spider-dev. It modifies
    the natural language questions in Spider by removing or paraphrasing explicitly
    mentioned column names to generate a more realistic dataset reflecting real-world
    scenarios, where questions rarely include explicitly mentioned column names. The
    final dataset comprises a total of 508 question-query pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two commonly used evaluation metrics for the text-to-SQL task: (1)
    Exact Match Accuracy (EM). This metric requires each component of the predicted
    SQL to be identical to the corresponding component of the gold SQL, disregarding
    the specific values in the query. (2) Execution Accuracy (EX). EX assesses the
    correctness of the execution result of the predicted SQL, typically offering a
    more precise evaluation than EM. Given that there might be multiple valid SQL
    queries with different writing styles for a single given question, EM might not
    effectively reflect the efficacy of the predicted SQL. As a result, we exclusively
    employ EX as our primary evaluation metric. Following previous work Zhong et al.
    ([2020](#bib.bib29)), we utilize the evaluation scripts available at [https://github.com/taoyds/test-suite-sql-eval](https://github.com/taoyds/test-suite-sql-eval).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use OpenAI ChatGPT-3.5 and GPT-4¹¹1gpt-3.5-turbo-0613 for ChatGPT and gpt4-0613
    for GPT-4. as our base models, which can be assess through official API. For all
    GPT-based baselines, we utilize the same API to reproduce the models, ensuring
    fairness in comparison. Our code is available at [https://github.com/FlyingFeather/DEA-SQL](https://github.com/FlyingFeather/DEA-SQL).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of previous methods and our method in terms of execution
    accuracy on Spider dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Zero-Shot | Few-Shot | Fine-tuning | Dev | Test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3B + PICARD Scholak et al. ([2021](#bib.bib21)) |  |  | ✓ | 79.3 | 75.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graphix-3B + PICARD Li et al. ([2023b](#bib.bib13)) |  |  | ✓ | 81.0 | 77.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| RESDSQL-3B + NatSQL Li et al. ([2023a](#bib.bib12)) |  |  | ✓ | 84.1 | 79.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT Ouyang et al. ([2022](#bib.bib17)) | ✓ |  |  | 74.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 OpenAI ([2023](#bib.bib16)) | ✓ |  |  | 72.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| C3 + ChatGPT Dong et al. ([2023](#bib.bib6)) | ✓ |  |  | 81.2 | 82.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DIN-SQL + GPT-4 Pourreza and Rafiei ([2023](#bib.bib18)) |  | ✓ |  | 83.5
    | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIL-SQL + GPT-4 Gao et al. ([2023](#bib.bib8)) |  | ✓ |  | 83.1 | 86.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIL-SQL + GPT-4 + SC Gao et al. ([2023](#bib.bib8)) |  | ✓ |  | 83.6 | 86.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAC-SQL Wang et al. ([2023](#bib.bib24)) |  | ✓ |  | 78.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DEA-SQL + GPT-4 |  | ✓ |  | 85.4 | 87.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison of previous methods and our method in terms of execution
    accuracy on Spider-Realistic dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | EX |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C3 + ChatGPT | 75.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DIN-SQL + GPT-4 | 78.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIL-SQL + GPT-4 | 75.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIL-SQL + GPT-4 + SC | 75.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DEA-SQL + GPT-4 | 81.5 |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Perfermance (RQ1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To validate the effectiveness of DEA-SQL in text-to-SQL tasks, a comprehensive
    comparison was performed on the Spider and Spider-Realistic datasets. We performed
    most of our experiments on the dev dataset of the Spider dataset, and the experiment
    was completed just coinciding with the opening of the test set. The experimental
    results are summarized in Table [1](#S3.T1 "Table 1 ‣ 3.1.3 Implementation Details
    ‣ 3.1 Experiment Setup ‣ 3 Experiment ‣ Decomposition for Enhancing Attention:
    Improving LLM-based Text-to-SQL through Workflow Paradigm") and [2](#S3.T2 "Table
    2 ‣ 3.1.3 Implementation Details ‣ 3.1 Experiment Setup ‣ 3 Experiment ‣ Decomposition
    for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm"),
    where the best performance is shown in boldface. Based on the experimental results,
    we have made several findings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of execution accuracy in the Spider dataset, our approach based on
    workflow outperforms the existing baselines as shown in Table [1](#S3.T1 "Table
    1 ‣ 3.1.3 Implementation Details ‣ 3.1 Experiment Setup ‣ 3 Experiment ‣ Decomposition
    for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm")),
    and the approach achieves a nice boost over the direct use of prompt on different
    pedestal models (Llama, WizardCoder, and CodeLlama) as shown in Table [6](#A1.T6
    "Table 6 ‣ A.2 Intergrated with other LLMs ‣ Appendix A Experimental Supplement
    ‣ Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through
    Workflow Paradigm")). The effectiveness of the workflow paradigm based on decomposing
    for enhancing attention is effectively demonstrated.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared with traditional training models, our method also has good improvements,
    further proving the potential of LLMs in text-to-SQL tasks. Providing a good workflow
    paradigm based on decomposing for enhancing attention can effectively improve
    LLMs’ performance in complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the Spider-Realistic dataset, which is more adapted to real-world scenarios
    and has more difficult question formulations, our approach is more stable and
    achieves better performance than other solutions based on LLMs. This validates
    the effectiveness of the two-stage information determination we proposed, which
    can mitigate the impact of different question formulations to some extent. The
    performance of the DAIL-SQL declines significantly on this dataset, indicating
    that the approach of relying solely on prompt optimization and few-shot retrieval
    has its limitations, making it more challenging to establish a good few-shot library
    and retrieval strategy for real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also investigated the improvement at different difficulty levels, as shown
    in Table [4](#S3.T4 "Table 4 ‣ 3.3 Ablation Study (RQ2) ‣ 3 Experiment ‣ Decomposition
    for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm"),
    there is a noticeable improvement at the hard and extra hard difficulty levels.
    This is partly due to the information determination module reducing irrelevant
    information interference, the problem classification module classifying and solving
    problems of different difficulty levels, and the self-correction and active learning
    step further expanding the ability threshold of LLM to solve problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Ablation Study (RQ2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In that subsection, we ablate each module and evaluate the effectiveness of
    each module. As shown in Table [3](#S3.T3 "Table 3 ‣ 3.3 Ablation Study (RQ2)
    ‣ 3 Experiment ‣ Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL
    through Workflow Paradigm"), the exclusion of either module makes the overall
    performance of the method degrade.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Ablation study on Spider Dev dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Easy | Medium | Hard | Extra | All |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DEA-SQL | 0.887 | 0.895 | 0.856 | 0.705 | 0.856 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o information filter | 0.899 | 0.874 | 0.787 | 0.633 | 0.827 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o classification | 0.875 | 0.904 | 0.805 | 0.639 | 0.838 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o self-correct | 0.895 | 0.881 | 0.839 | 0.663 | 0.842 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o active learning | 0.903 | 0.892 | 0.822 | 0.663 | 0.846 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o active & correct | 0.903 | 0.879 | 0.805 | 0.639 | 0.834 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o H-FK | 0.891 | 0.886 | 0.851 | 0.651 | 0.843 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance compared to our method different query across difficulty
    levels on Spider Dev dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Easy | Medium | Hard | Extra | All |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C3-GPT3.5 | 0.919 | 0.841 | 0.782 | 0.608 | 0.812 |'
  prefs: []
  type: TYPE_TB
- en: '| DIN-SQL | 0.907 | 0.897 | 0.793 | 0.675 | 0.835 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIL-SQL | 0.907 | 0.897 | 0.753 | 0.62 | 0.831 |'
  prefs: []
  type: TYPE_TB
- en: '| DEA-SQL | 0.891 | 0.892 | 0.845 | 0.705 | 0.854 |'
  prefs: []
  type: TYPE_TB
- en: The effect of sub-models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1) In information determination, the strategy of reducing irrelevant information
    has a positive effect on all types of questions except those of simple difficulty.
    It is particularly effective in improving the accuracy of hard and extra hard
    difficulty questions. By reducing irrelevant information to focus the attention
    of LLMs, we can effectively enhance performance in complex tasks. 2) The ablation
    method of the classification step is to remove the classification module while
    switching the hint and specification in the prompt structure to a fixed hint.
    It is found that besides damaging the problem-solving ability of medium-difficulty
    problems, this step has positive effects on other problems, and the effects are
    significant in difficult and extremely difficult problems. It is indicated that
    distinguishing problem types, using simple hints for simple problems and targeting
    complex hints for complex problems, can effectively improve the performance of
    the LLM. 3) The active learning and self-correction modules had a negative effect
    on easy problems, but a positive effect on hard and extra hard problems. This
    is also expected, as these two modules are designed to increase the capability
    threshold of the original base model, but may to some extent impair the ability
    to solve other easy questions. 4) Adding H-FK prompt can effectively improve the
    performance of complex difficulty, which reduces the difficulty of finding the
    right complex concatenated table condition to some extent for the model. To summarize,
    for easy questions there is no need for overly complex processes and prompts,
    while for relatively complex questions, the multi-step workflow approach of narrowing
    down the information to enhance the model’s attention works well.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of different few-shot scheme.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To explore the effectiveness of few-shot retrieval methods, we conducted experimental
    comparisons between different retrieval schemes and zero-shot. In the experiments,
    it was found that different few-shots have a significant impact on the final accuracy
    of the results, and choosing poor examples may lead to adverse effects. We explored
    four retrieval schemes: random, question similarity(ques_sim), template similarity(tem_sim)
    and template similarity without classification (tem_sim_wo) to find more suitable
    samples. As shown in Figure [5](#S3.T5 "Table 5 ‣ 3.4 Parameters Analysis ‣ 3
    Experiment ‣ Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL
    through Workflow Paradigm"), random is a little detrimental to overall performance,
    while the retrieval strategy based on question template similarity in the combined
    question classification retrieval library yields the best results. Question templates
    also essentially provide a simple classification for the questions, and relying
    on question classification methods makes it easier to find the most relevant questions
    and their solutions, thereby stimulating the capabilities of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Parameters Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the parameters experiment, we investigated the number of few-shots for the
    SQL generation step, and the results in Table LABEL:fig:sql_fewshot_nums found
    that the performance is better when #fewshot=3\. This indicates that within a
    certain range, as the number of effective samples increases, the accuracy of the
    model improves. The accuracy of the information filter also greatly affects the
    final accuracy rate. The current method based on LLM has not yet been able to
    achieve 100% accuracy. It can be seen that with the improvement of the accuracy
    of the information filter, the final accuracy rate also has a corresponding improvement
    as shown in Figure LABEL:fig:sql_fc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The few-shot mode study on Spider Dev dataset, where the number of
    few-shots is 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Easy | Medium | Hard | Extra | All |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| zero_shot | 0.891 | 0.890 | 0.816 | 0.669 | 0.842 |'
  prefs: []
  type: TYPE_TB
- en: '| random | 0.879 | 0.890 | 0.862 | 0.693 | 0.851 |'
  prefs: []
  type: TYPE_TB
- en: '| ques_sim | 0.879 | 0.901 | 0.833 | 0.645 | 0.843 |'
  prefs: []
  type: TYPE_TB
- en: '| tem_sim | 0.891 | 0.892 | 0.845 | 0.705 | 0.854 |'
  prefs: []
  type: TYPE_TB
- en: '| tem_sim_wo | 0.895 | 0.888 | 0.822 | 0.657 | 0.841 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Parameters analysis results.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-SQL Parsing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Text-to-SQL is a semantic parsing task that translates users’ natural language
    questions into appropriate SQL queries. This allows ubiquitous relational data
    to be accessed by a broader range of non-technical users. In the past few years,
    inspired by the success of deep learning, research in text-to-SQL has primarily
    focused on constructing well-designed deep neural networks (Guo et al., [2019](#bib.bib9);
    Wang et al., [2020](#bib.bib23); Cao et al., [2021](#bib.bib1); Scholak et al.,
    [2021](#bib.bib21); Li et al., [2023a](#bib.bib12)). RATSQL (Wang et al., [2020](#bib.bib23))
    defines a question-schema graph and employs a relation-aware self-attention mechanism
    during the encoding process to jointly learn representations of the question words,
    schema items, and edge relationships. PICARD (Scholak et al., [2021](#bib.bib21))
    is a sequence-to-sequence model. The PICARD model rejects invalid tokens at each
    decoding step and constrains the generated results within a certain output space,
    thereby reducing the number of invalid SQL queries. Although these supervised
    learning neural models have achieved impressive performance on the text-to-SQL
    task, they are typically trained on a large training set and evaluated on test
    examples. However, acquiring annotated text-to-SQL data is costly. Additionally,
    training and fine-tuning the models entail significant engineering efforts and
    consume substantial computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: In-context Learning for Text-to-SQL
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With the widespread adoption of LLMs, recent work has explored in context learning
    for text-to-SQL from various perspectives (Gao et al., [2023](#bib.bib8); Liu
    et al., [2023](#bib.bib15); Chang and Fosler-Lussier, [2023b](#bib.bib3)). In
    Context learning enables pre-trained LLMs to perform text-to-SQL without relying
    on supervised samples by providing zero or a few training examples (NLQ-SQL pairs)
    as demonstrations. In this domain,  Rajkumar et al. ([2022](#bib.bib20)) and  Chang
    and Fosler-Lussier ([2023a](#bib.bib2)) investigate effective methods for representing
    the database within prompts in context learning.  Pourreza and Rafiei ([2023](#bib.bib18))
    and  Wang et al. ([2023](#bib.bib24)) improve the ability of text-to-SQL parsing
    through intermediate reasoning steps. Other studies (Tai et al., [2023](#bib.bib22);
    Zhang et al., [2023](#bib.bib28)) focus on enhancing the semantic understanding
    capabilities of LLMs using chain-of-thought techniques, thereby addressing text-to-SQL
    problems more effectively. However, these studies tend to focus more on activating
    the general capabilities of LLMs, lacking a comprehensive consideration of SQL
    tasks. Additionally, the presence of irrelevant tables and fields in the database
    schema can interfere with the generation of correct SQL queries. To address these
    issues, we propose a complete SQL generation workflow based on prior knowledge
    of attention-focusing and irrelevant information filtering.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompting techniques have unleashed the potential of LLMs, with most past research
    focusing on improving single-step prompting. In the NLP field, structured languages
    like SQL pose a greater challenge compared to plain text. We introduce the workflow
    prompting paradigm to enhance the performance of LLMs in text-to-SQL tasks. By
    decomposing steps, reducing irrelevant information, and transforming the complex
    problem into a classification task, we can focus the attention of LLMs. Self-correction
    and active learning modules are designed according to the capability thresholds
    of LLMs, with different sub-modules performing their respective roles, further
    improving the upper limit of LLM-based prompting methods in text-to-SQL tasks.
    Adequate experiments on Spider and Spider-Realistic datasets demonstrate that
    our approach significantly improves prompting performance, outperforming state-of-the-art
    fine-tuning and LLM-based methods. This illustrates the effectiveness of the workflow
    prompting paradigm in enhancing the problem-solving scope of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1\. Cost Issues: Our proposed method adopts a decomposition workflow as the
    backbone, and when using GPT-4 to respond to natural language questions, there
    might be some differences in cost and latency compared to the zero-shot approach,
    thus a trade-off between performance and cost is necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Instability: Another limitation of text-to-SQL methods based on LLMs, is
    their inherent instability due to the probabilistic nature of their generation
    process. As these models rely on probability distributions to generate output,
    they can sometimes produce inconsistent or unpredictable results. Considering
    more deterministic SQL statement generation can be conducted in future work.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cao et al. (2021) Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and
    Kai Yu. 2021. LGESQL: Line graph enhanced text-to-SQL model with mixed local and
    non-local relations. In *ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang and Fosler-Lussier (2023a) Shuaichen Chang and Eric Fosler-Lussier. 2023a.
    How to prompt llms for text-to-sql: A study in zero-shot, single-domain, and cross-domain
    settings. *arXiv preprint arXiv:2305.11853*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang and Fosler-Lussier (2023b) Shuaichen Chang and Eric Fosler-Lussier. 2023b.
    Selective demonstrations for cross-domain text-to-sql. In *Findings of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2021) Xiang Deng, Ahmed Hassan, Christopher Meek, Oleksandr Polozov,
    Huan Sun, and Matthew Richardson. 2021. Structure-grounded pretraining for text-to-sql.
    In *NAACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2023) Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao,
    Jinshu Lin, Dongfang Lou, et al. 2023. C3: Zero-shot text-to-sql with chatgpt.
    *arXiv preprint arXiv:2307.07306*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finegan-Dollak et al. (2018) Catherine Finegan-Dollak, Jonathan K Kummerfeld,
    Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018.
    Improving text-to-sql evaluation methodology. In *ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian,
    Bolin Ding, and Jingren Zhou. 2023. Text-to-sql empowered by large language models:
    A benchmark evaluation. *arXiv preprint arXiv:2308.15363*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019) Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou,
    Ting Liu, and Dongmei Zhang. 2019. Towards complex text-to-sql in cross-domain
    database with intermediate representation. In *ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hui et al. (2022) Binyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Yanyang
    Li, Bowen Li, Jian Sun, and Yongbin Li. 2022. S2sql: Injecting syntax to question-schema
    interaction graph encoder for text-to-sql parsers. In *ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2020) Wenqiang Lei, Weixin Wang, Zhixin Ma, Tian Gan, Wei Lu, Min-Yen
    Kan, and Tat-Seng Chua. 2020. Re-examining the role of schema linking in text-to-sql.
    In *EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. 2023a.
    Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In *AAAI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao
    Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. 2023b. Graphix-t5: Mixing
    pre-trained transformers with graph-aware layers for text-to-sql parsing. In *ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023) Bin Liang, Xiang Li, Lin Gui, Yonghao Fu, Yulan He, Min
    Yang, and Ruifeng Xu. 2023. Few-shot aspect category sentiment analysis via meta-learning.
    *ACM Transactions on Information Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. 2023. A
    comprehensive evaluation of chatgpt’s zero-shot text-to-sql capability. *arXiv
    preprint arXiv:2303.13547*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pourreza and Rafiei (2023) Mohammadreza Pourreza and Davood Rafiei. 2023. DIN-SQL:
    Decomposed in-context learning of text-to-SQL with self-correction. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2022) Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li,
    Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, et al. 2022. A survey on
    text-to-sql parsing: Concepts, methods, and future directions. *arXiv preprint
    arXiv:2208.13629*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajkumar et al. (2022) Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau.
    2022. Evaluating the text-to-sql capabilities of large language models. *arXiv
    preprint arXiv:2204.00498*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scholak et al. (2021) Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau.
    2021. Picard: Parsing incrementally for constrained auto-regressive decoding from
    language models. In *EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2023) Chang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, and Huan
    Sun. 2023. Exploring chain-of-thought style prompting for text-to-sql. In *EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov,
    and Matthew Richardson. 2020. Rat-sql: Relation-aware schema encoding and linking
    for text-to-sql parsers. In *ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi
    Bai, Qian-Wen Zhang, Zhao Yan, and Zhoujun Li. 2023. Mac-sql: Multi-agent collaboration
    for text-to-sql. *arXiv preprint arXiv:2312.11242*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.
    2023. A paradigm shift in machine translation: Boosting translation performance
    of large language models. *arXiv preprint arXiv:2309.11674*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider:
    A large-scale human-labeled dataset for complex and cross-domain semantic parsing
    and text-to-sql task. In *EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, and
    Kai Yu. 2023. Act-sql: In-context learning for text-to-sql with automatically-generated
    chain-of-thought. In *Findings of EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2020) Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic evaluation
    for text-to-sql with distilled test suite. In *EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Supplement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We select conventionally and recently published text-to-SQL baselines for model
    comparison, which can be briefly categorized into two groups: (1) training model-based
    programs Scholak et al. ([2021](#bib.bib21)); Li et al. ([2023b](#bib.bib13),
    [a](#bib.bib12)) and (2) prompt engineering-based programs Dong et al. ([2023](#bib.bib6));
    Pourreza and Rafiei ([2023](#bib.bib18)); Gao et al. ([2023](#bib.bib8)); Wang
    et al. ([2023](#bib.bib24)).'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T5-3B + PICARD Scholak et al. ([2021](#bib.bib21)) PICARD is a sequence-to-sequence
    model. It rejects invalid tokens at each decoding step and constrains the generated
    results within a certain output space, thereby reducing the number of invalid
    SQL queries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphix-3B + PICARD Li et al. ([2023b](#bib.bib13)) Based on the pre-trained
    T5 model, Graphix is designed to encode the combination of semantic and structural
    information via graph neural networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RESDSQL-3B + NatSQL Li et al. ([2023a](#bib.bib12)) RESDSQL reduces the complexity
    of text-to-SQL by implicitly constraining SQL parsing through schema linking and
    skeleton parsing techniques. It is the best method based on fine-tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT Ouyang et al. ([2022](#bib.bib17)) We use ChatGPT in a simple zero-shot
    paradigm to evaluate its capability on the text-to-SQL task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 OpenAI ([2023](#bib.bib16)) The base model is changed to GPT-4 in a simple
    zero-shot paradigm to evaluate the capability on the text-to-SQL task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'C3 + ChatGPT C3 Dong et al. ([2023](#bib.bib6)) proposes a ChatGPT-based zero-shot
    Text-to-SQL method with three key components: Clear Prompting (CP), Calibration
    with Hints (CH), and Consistent Output (CO).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DIN-SQL + GPT-4 DIN-SQL Pourreza and Rafiei ([2023](#bib.bib18)) utilizes multi-step
    decomposition and self-correction to effectively improve the performance of context-based
    learning methods on text-to-SQL tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAIL-SQL + GPT-4 + SC DAIL-SQL Gao et al. ([2023](#bib.bib8)) systematically
    and extensively compares existing prompting strategies and proposes a novel integrated
    solution. It is the best method based on LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MAC-SQL MAC-SQL Wang et al. ([2023](#bib.bib24)) proposes an innovative LLM-based
    multi-agent framework for the Text-to-SQL task. It consists of three agents: the
    Selector, which condenses databases and retains relevant table schemas; the Decomposer,
    breaking down complex questions into simpler sub-problems; and the Refiner, responsible
    for validating and refining SQL.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 Intergrated with other LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the main article, to better explore the performance of LLMs, we chose the
    advanced GPT4 model as the base model for experimentation, while this section
    mainly supplements the introduction of experimental results using other models
    as the base model. As can be seen from Table [6](#A1.T6 "Table 6 ‣ A.2 Intergrated
    with other LLMs ‣ Appendix A Experimental Supplement ‣ Decomposition for Enhancing
    Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm"), the use
    of our proposed DEA-SQL method effectively improved the model’s execution accuracy
    in the text-to-SQL task, proving the effectiveness of the decomposing for enhanced
    attention workflow paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Performance comparison of different LLMs when applied DEA-SQL.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM | Easy | Medium | Hard | Extra | All |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13B-chat | 0.762 | 0.500 | 0.287 | 0.157 | 0.472 |'
  prefs: []
  type: TYPE_TB
- en: '| +DEA-SQL | 0.823 | 0.585 | 0.374 | 0.145 | 0.536 |'
  prefs: []
  type: TYPE_TB
- en: '| WizardCoder-15B-V1.0 | 0.859 | 0.774 | 0.540 | 0.289 | 0.677 |'
  prefs: []
  type: TYPE_TB
- en: '| +DEA-SQL | 0.879 | 0.816 | 0.557 | 0.319 | 0.708 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-Instruct | 0.806 | 0.749 | 0.529 | 0.380 | 0.666 |'
  prefs: []
  type: TYPE_TB
- en: '| +DEA-SQL | 0.871 | 0.796 | 0.546 | 0.380 | 0.705 |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to facilitate a better understanding of our approach, this section
    provides a comprehensive list of prompts for each of the steps in DEA-SQL’s entire
    workflow approach.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Identify Problem Elements Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming that you are a natural language processing expert and statistician,
    and a data analyst, please understand the business requirements and break down
    the requirements description into statistical elements. It is required to break
    down user problems into entities, and the main information in the original problem
    cannot be lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is the name of the staff that is in charge of the attraction named ’US
    museum’?
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["staff", "the attraction named ’US museum’"], "query":
    "the name of the staff that is in charge of the attraction named ÜS museum"̈}'
  prefs: []
  type: TYPE_NORMAL
- en: How many heads of the departments are older than 56 ?
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["age older than 56", "number of heads of the departments"],
    "query": "Number of department heads over 56 years old"}'
  prefs: []
  type: TYPE_NORMAL
- en: List the name, born state and age of the heads of departments ordered by age.
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["name of the heads of departments", "born state of the
    heads of departments", "age of the heads of departments", "age"], "query": "List
    the name, born state and age of the heads of departments ordered by age."}'
  prefs: []
  type: TYPE_NORMAL
- en: what is the average, minimum, and maximum age of all singers from Chinese?
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["Chinese", "age of all singers"], "query": "The average,
    minimum, and maximum age of all singers from Chinese"}'
  prefs: []
  type: TYPE_NORMAL
- en: Return the different descriptions of formulas that has been used in the textbook.
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["the different descriptions of formulas", "formulas",
    "used in the textbook"], "query": "The different descriptions of formulas that
    has been used in the textbook"}'
  prefs: []
  type: TYPE_NORMAL
- en: What are the details of the markets that can be accessed by walk or bus?
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["the details of the markets", "can be accessed by walk
    or busk"], "query": "The details of the markets that can be accessed by walk or
    bus"}'
  prefs: []
  type: TYPE_NORMAL
- en: Show the name of colleges that have at least two players.
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["the name of colleges", "players"], "query": "The name
    of colleges that have at least two players"}'
  prefs: []
  type: TYPE_NORMAL
- en: How many gold medals has the club with the most coaches won?
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["gold medals", "club", "coaches"], "query": "The number
    of gold medals has the club with the most coaches won"}'
  prefs: []
  type: TYPE_NORMAL
- en: List the nominees that have been nominated more than two musicals.
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {"entities": ["nominees", "nominees that have been nominated", "musicals"],
    "query": "The nominees that have been nominated more than two musicals"}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please be sure to follow the following specifications:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1."entities" refers to all entities in the requirements, including all description
    information in the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '2.Your output must be output in json format, and only this json needs to be
    returned. It needs to include all fields in json. The json format is as follows:
    {"entities":[entities], "query":"Rewritten question, removing unnecessary content"}'
  prefs: []
  type: TYPE_NORMAL
- en: '[query input] output:'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Information filter Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[table_info]'
  prefs: []
  type: TYPE_NORMAL
- en: 'User question: [query]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entity information: [limitation]'
  prefs: []
  type: TYPE_NORMAL
- en: need
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are a data analyst. In business, you need to use the above table information
    to complete a SQL query code to solve user problems. I would like to ask you to
    first match the table fields or calculation methods required by the [limitation]
    entity, and then determine the calculation method of [main_metric], and finally
    determine the required table and all related field information and give some key
    information for writing SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Note that all table names must be their original names, and the output of field
    names must be the original field names in the table.
  prefs: []
  type: TYPE_NORMAL
- en: Please be sure to comply with the following specifications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1\. Element matching needs to output the most related table fields (one or more)
    or calculation methods and required field names required by the entities; yyy1
    is the table field that needs to be selected to calculate the entity and the answer
    is in the form of “‘colunm_name“‘. Note that an entity may require multiple fields;
  prefs: []
  type: TYPE_NORMAL
- en: 2\. bbb is the calculation method of [main_metric];
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Required table information: Not all tables may need to be selected, depending
    on the specific problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Select the table and related fields based on the user questions, entity
    information and element matching information you have given above;
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. The where statement condition only gives the conditions of the corresponding
    table;
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. All field names required by SQL under the table must include the fields
    actually needed under the corresponding table. Note that you cannot select fields
    that are not under the previous table name, and do not select all fields. You
    must include all the fields that are needed for the table;
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Multi-table joint fields and conditions need to find out the associated
    fields and conditions between multiple tables from the above table information;
  prefs: []
  type: TYPE_NORMAL
- en: 5\. "All fields" must to include all the fields actually used in sql !!! You
    must include all the fields that are needed for the table;
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Think step by step, and finally summarize that your output is only in the
    given json format: "Element matching": output_format, "main_metric calculation
    method": "bbb", "Required table information": ["Table name": "xxx", "where statement
    condition": "ccc", "All field names required by SQL under this table": ["yyy1",
    "yyy2", "yyy3"], "Table name": "xxx", "where statement condition": "ccc", "All
    field names required by SQL under this table": ["yyy1", "yyy2", "yyy3"]], "Multiple
    table joint fields and conditions": "ccc", "sql": "ddd", "All fields": ["yyy1",
    "yyy2", "yyy3"]'
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Question Classification Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the given question that requires writing SQL, classify it with two labels.
    You can choose the first label from NON-JOIN and JOIN and choose the second label
    from NON-NESTED and NESTED.
  prefs: []
  type: TYPE_NORMAL
- en: Some table infos and examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Q: What are the names and revenues of the companies with the highest revenues
    in each headquarter city?'
  prefs: []
  type: TYPE_NORMAL
- en: 'table_info: CREATE TABLE MANUFACTURERS ('
  prefs: []
  type: TYPE_NORMAL
- en: code INTEGER
  prefs: []
  type: TYPE_NORMAL
- en: name VARCHAR(255) NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: headquarter VARCHAR(255) NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: founder VARCHAR(255) NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: revenue REAL
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (code)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: CREATE TABLE PRODUCTS (
  prefs: []
  type: TYPE_NORMAL
- en: code INTEGER
  prefs: []
  type: TYPE_NORMAL
- en: name VARCHAR(255) NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: price DECIMAL NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: manufacturer INTEGER NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (code),
  prefs: []
  type: TYPE_NORMAL
- en: FOREIGN KEY (manufacturer) REFERENCES Manufacturers(code)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Let’s think step by step. The SQL query for the question ’What are the names
    and revenues of the companies with the highest revenues in each headquarter city?’
    needs these tables and columns = [MANUFACTURERS.name, MANUFACTURERS.revenue, MANUFACTURERS.headquarter],
    so we don’t need joint condition and label it as NON-JOIN. Plus, it doesn’t require
    nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN). so we label it as
    NON-NESTED.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the SQL query can be classified as NON-JOIN, NON-NESTED
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: NON-JOIN, NON-NESTED'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Which studios have an average gross of over 4500000?'
  prefs: []
  type: TYPE_NORMAL
- en: 'table_info: CREATE TABLE FILM ('
  prefs: []
  type: TYPE_NORMAL
- en: studio text
  prefs: []
  type: TYPE_NORMAL
- en: gross_in_dollar int
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (Film_ID)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Let’s think step by step. The SQL query for the question ’Which studios
    have an average gross of over 4500000?’ needs these table and column = [FILM.studio,
    AVG(FILM.gross_in_dollar)], so we don’t need joint condition and label it as NON-JOIN.
    Plus, it doesn’t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT
    IN). So we label it as NON-NESTED.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the SQL query can be classified as NON-JOIN, NON-NESTED
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: NON-JOIN, NON-NESTED'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What are the products with the maximum page size A4 that also have a pages
    per minute color smaller than 5?'
  prefs: []
  type: TYPE_NORMAL
- en: 'table_info: CREATE TABLE PRODUCT ('
  prefs: []
  type: TYPE_NORMAL
- en: product_id int
  prefs: []
  type: TYPE_NORMAL
- en: product text
  prefs: []
  type: TYPE_NORMAL
- en: dimensions text
  prefs: []
  type: TYPE_NORMAL
- en: dpi real
  prefs: []
  type: TYPE_NORMAL
- en: pages_per_minute_color real
  prefs: []
  type: TYPE_NORMAL
- en: max_page_size text
  prefs: []
  type: TYPE_NORMAL
- en: interface text
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (product_id)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Let’s think step by step. The SQL query for the question ’What are the products
    with the maximum page size A4 that also have a pages per minute color smaller
    than 5?’ needs these table and columns = [PRODUCT.product, PRODUCT.max_page_size,
    PRODUCT.pages_per_minute_color], so we don’t need joint condition and label it
    as NON-JOIN. Plus, it requires nested queries with (INTERSECT, UNION, EXCEPT,
    IN, NOT IN), and we need the answer to the questions = [’What is the maximum page
    size A4 of the products’]. So it need nested queries and we label it as NESTED.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the SQL query can be classified as NON-JOIN, NESTED.
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: NON-JOIN, NESTED'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Show names for all stadiums except for stadiums having a concert in year
    2014.'
  prefs: []
  type: TYPE_NORMAL
- en: 'table_info: CREATE TABLE STADIUM ('
  prefs: []
  type: TYPE_NORMAL
- en: stadium_ID int
  prefs: []
  type: TYPE_NORMAL
- en: location text
  prefs: []
  type: TYPE_NORMAL
- en: name text
  prefs: []
  type: TYPE_NORMAL
- en: capacity int
  prefs: []
  type: TYPE_NORMAL
- en: highest int
  prefs: []
  type: TYPE_NORMAL
- en: lowest int
  prefs: []
  type: TYPE_NORMAL
- en: average int
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (Stadium_ID)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: CREATE TABLE CONCERT (
  prefs: []
  type: TYPE_NORMAL
- en: concert_ID int
  prefs: []
  type: TYPE_NORMAL
- en: concert_Name text
  prefs: []
  type: TYPE_NORMAL
- en: theme text
  prefs: []
  type: TYPE_NORMAL
- en: stadium_ID text
  prefs: []
  type: TYPE_NORMAL
- en: year text
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (concert_ID),
  prefs: []
  type: TYPE_NORMAL
- en: FOREIGN KEY (stadium_ID) REFERENCES stadium(stadium_ID)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Let’s think step by step. The SQL query for the question ’Show names for
    all stadiums except for stadiums having a concert in year 2014.’ needs these table
    and columns = [STADIUM.name, CONCERT.year], so we need a JOIN operation on the
    STADIUM and CONCERT tables using the stadium_ID column because we we need to exclude
    stadiums with concerts in 2014\. So we label it as JOIN. Plus, it requires nested
    queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to
    the questions = [’What is the stadiums having a concert in year 2014’]. So it
    need nested queries and we label it as NESTED.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the SQL query can be classified as JOIN, NESTED.
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: JOIN, NESTED'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: How many songs have a shared vocal?'
  prefs: []
  type: TYPE_NORMAL
- en: 'table_info: CREATE TABLE SONGS ('
  prefs: []
  type: TYPE_NORMAL
- en: SongId INTEGER PRIMARY KEY,
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: CREATE TABLE VOCALS (
  prefs: []
  type: TYPE_NORMAL
- en: SongId INTEGER
  prefs: []
  type: TYPE_NORMAL
- en: Bandmate INTEGER
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY(SongId, Bandmate),
  prefs: []
  type: TYPE_NORMAL
- en: FOREIGN KEY (SongId) REFERENCES Songs(SongId),
  prefs: []
  type: TYPE_NORMAL
- en: FOREIGN KEY (Bandmate) REFERENCES Band(Id)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Let’s think step by step. The SQL query for the question ’How many songs
    have a shared vocal?’ needs these table and columns = [SONGS.SongId, VOCALS.Bandmate],
    so we need a JOIN operation on the SONGS and VOCALS tables using the SongId column
    because we need to count the number of songs with a shared vocal. So we label
    it as JOIN. Plus, it does not require nested queries with (INTERSECT, UNION, EXCEPT,
    IN, NOT IN), so we label it as NON-NESTED.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the SQL query can be classified as JOIN, NON-NESTED.
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: JOIN, NON-NESTED'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: How many users who did not leave any review.'
  prefs: []
  type: TYPE_NORMAL
- en: 'table_info: CREATE TABLE USERACCT ('
  prefs: []
  type: TYPE_NORMAL
- en: u_id integer NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: name varchar(128) DEFAULT NULL
  prefs: []
  type: TYPE_NORMAL
- en: PRIMARY KEY (u_id)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: CREATE TABLE REVIEW (
  prefs: []
  type: TYPE_NORMAL
- en: a_id integer NOT NULL PRIMARY KEY
  prefs: []
  type: TYPE_NORMAL
- en: u_id integer NOT NULL
  prefs: []
  type: TYPE_NORMAL
- en: FOREIGN KEY (u_id) REFERENCES useracct(u_id)
  prefs: []
  type: TYPE_NORMAL
- en: FOREIGN KEY (i_id) REFERENCES item(i_id)
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The SQL query for the question ’How many users who did not leave any review.’
    needs these table and columns = [USERACCT.name, REVIEW.u_id], so we need a JOIN
    operation on the USERACCT and REVIEW tables using the u_id column because we need
    to find users who did not leave any review.'
  prefs: []
  type: TYPE_NORMAL
- en: So we label it as JOIN. Plus, it requires nested queries with (INTERSECT, UNION,
    EXCEPT, IN, NOT IN), and we need the answer to the questions = [’What is the list
    of u_id who left a review?’]. So it needs nested queries and we label it as NESTED.
  prefs: []
  type: TYPE_NORMAL
- en: Thus the SQL query can be classified as JOIN, NESTED.
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: JOIN, NESTED'
  prefs: []
  type: TYPE_NORMAL
- en: 'Issues you should be concerned about:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'table info:[table_info] Q: [query] A:'
  prefs: []
  type: TYPE_NORMAL
- en: B.4 SQL Generation Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'B.4.1 few-shot: easy class'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[few-shot]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Database scheme: [table_info]'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please think carefully about the related fields or calculation methods of ’[main_metric]’,
    then write valid SQLite to solve the following questions based on the above table
    information, and do not select extra columns that are not explicitly requested
    in the query.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Query: [query] ### specification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1.In sql, just select columns that are explicitly requested in the query.
  prefs: []
  type: TYPE_NORMAL
- en: '2.The output format must strictly meet the given json specification: "sql":
    "ccc"'
  prefs: []
  type: TYPE_NORMAL
- en: 'B.4.2 few-shot: join class'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[few-shot] ### Database scheme: [table_info] ### Please think carefully about
    the related fields or calculation methods of ’[main_metric]’, then write valid
    SQLite to solve the following questions based on the above table information,
    and do not select extra columns that are not explicitly requested in the query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query: [query] ### HINT: The question may need connection operation like JOIN.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: specification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1."LIMIT" just is used when explicitly requesting how much to retrieve in the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 2.In sql, just select columns that are explicitly requested in the query.
  prefs: []
  type: TYPE_NORMAL
- en: '3.The output format must strictly meet the given json specification: "sql":
    "ccc"'
  prefs: []
  type: TYPE_NORMAL
- en: 'B.4.3 few-shot: join-nested or nested class'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[few-shot] ### Database scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[table_info] ### Please think carefully about the related fields or calculation
    methods of ’[main_metric]’, then write valid SQLite to solve the following questions
    based on the above table information, and do not select extra columns that are
    not explicitly requested in the query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query: [query] ### HINT: The question may needs nested queries like INTERSECT,
    UNION, EXCEPT, NOT IN.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: specification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1."LIMIT" just is used when explicitly requesting how much to retrieve in the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 2.Don’t use "IN", "OR", "LEFT JOIN" in sql because they aren’t supported in
    execution engine, you can use "INTERSECT" or "EXCEPT" instead.
  prefs: []
  type: TYPE_NORMAL
- en: 3.In sql, just select columns that are explicitly requested in the query.
  prefs: []
  type: TYPE_NORMAL
- en: '4.The output format must strictly meet the given json specification: "sql":
    "ccc"'
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Self Correction Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the given question, use the Database scheme to fix the given SQLite QUERY
    for any issues.
  prefs: []
  type: TYPE_NORMAL
- en: If there are any problems, please fix them.
  prefs: []
  type: TYPE_NORMAL
- en: If there are no issues, return SQLite QUERY as is.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some instructions for fixing the SQL query:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1) In sql, just select columns that are explicitly requested in the query.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Pay attention to the columns that are used for the SELECT clause. Fix possible
    ambiguous columns if there are the same columns in different table in the SELECT
    clause.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Pay attention to the correspondence between tables and fields. Cannot use
    fields that are not in the table.
  prefs: []
  type: TYPE_NORMAL
- en: 4) Pay attention to the columns that are used for the JOIN. The join table condition
    must be in the Foreign_keys.
  prefs: []
  type: TYPE_NORMAL
- en: 5) Pay attention to the use of the JOIN. Don’t use LEFT JOIN unless necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 6) Only change the SELECT, GROUP BY and ORDER BY clause when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '7) Database scheme: [table_info]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: [query]'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SQLite SQL QUERY:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[sql]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixed SQL QUERY:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SELECT
  prefs: []
  type: TYPE_NORMAL
- en: B.6 Active Learning Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please determine the type of question. If it is an extremum problem, modify
    the SQL accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: If not, use the original SQL as the modified SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: What is the name of the instructor who advises the student with the greatest
    number of total credits?'
  prefs: []
  type: TYPE_NORMAL
- en: 'original SQL: SELECT T2.name FROM instructor T2 JOIN advisor T1 ON T2.id =
    T1.i_id JOIN student s ON T1.s_id = T3.id WHERE T3.tot_cred = (SELECT MAX(tot_cred)
    FROM student)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The question is an extremum problem, so i should modify the SQL. The modified
    SQL: SELECT T2.name FROM advisor AS T1 JOIN instructor AS T2 ON T1.i_id = T2.id
    JOIN student AS T3 ON T1.s_id = T3.id ORDER BY T3.tot_cred DESC LIMIT 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Return the id and full name of the customer who has the fewest accounts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'original SQL: SELECT c.customer_id, c.customer_first_name, c.customer_last_name
    FROM CUSTOMERS c JOIN ACCOUNTS a ON c.customer_id = a.customer_id GROUP BY c.customer_id
    HAVING COUNT(a.account_id) = (SELECT COUNT(account_id) FROM ACCOUNTS GROUP BY
    customer_id ORDER BY COUNT(account_id) ASC LIMIT 1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The question is an extremum problem, so i should modify the SQL. The modified
    SQL: SELECT T1.customer_id, T2.customer_first_name, T2.customer_last_name FROM
    Customers_cards AS T1 JOIN Customers AS T2 ON T1.customer_id = T2.customer_id
    GROUP BY T1.customer_id ORDER BY count(*) ASC LIMIT 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: What is the average hours across all projects?'
  prefs: []
  type: TYPE_NORMAL
- en: 'original SQL: SELECT avg(hours) FROM projects'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The question is not an extremum problem, so i should use the original SQL
    as the modified SQL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified SQL: SELECT avg(hours) FROM projects'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q4: [query]'
  prefs: []
  type: TYPE_NORMAL
- en: '[table_info]'
  prefs: []
  type: TYPE_NORMAL
- en: 'original SQL: [sql]'
  prefs: []
  type: TYPE_NORMAL
- en: 'A:'
  prefs: []
  type: TYPE_NORMAL
