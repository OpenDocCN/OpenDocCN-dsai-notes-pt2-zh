- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02262](https://ar5iv.labs.arxiv.org/html/2311.02262)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Qingru Zhang^†,  Chandan Singh^⋄,  Liyuan Liu^⋄,  Xiaodong Liu^⋄,  Bin Yu^‡,
  prefs: []
  type: TYPE_NORMAL
- en: Jianfeng Gao^⋄,  Tuo Zhao^†
  prefs: []
  type: TYPE_NORMAL
- en: ^†Georgia Institute of Technology   ^‡University of California, Berkeley   ^⋄Microsoft
    Research
  prefs: []
  type: TYPE_NORMAL
- en: '{qingru.zhang,tourzhao}@gatech.edu'
  prefs: []
  type: TYPE_NORMAL
- en: binyu@berkeley.edu
  prefs: []
  type: TYPE_NORMAL
- en: '{chansingh,lucliu,xiaodl,jfgao}@microsoft.com Work is done during Qingru Zhang’s
    internship at Microsoft Research.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In human-written articles, we often leverage the subtleties of text style, such
    as bold and italics, to guide the attention of readers. These textual emphases
    are vital for the readers to grasp the conveyed information. When interacting
    with large language models (LLMs), we have a similar need – steering the model
    to pay closer attention to user-specified information, e.g., an instruction. Existing
    methods, however, are constrained to process plain text and do not support such
    a mechanism. This motivates us to introduce PASTA – Post-hoc Attention STeering
    Approach, a method that allows LLMs to read text with user-specified emphasis
    marks. To this end, PASTA identifies a small subset of attention heads and applies
    precise attention reweighting on them, directing the model attention to user-specified
    parts. Like prompting, PASTA is applied at inference time and does not require
    changing any model parameters. Experiments demonstrate that PASTA can substantially
    enhance an LLM’s ability to follow user instructions or integrate new knowledge
    from user inputs, leading to a significant performance improvement on a variety
    of tasks, e.g., an average accuracy improvement of 22% for LLAMA-7B. Our code
    is publicly available at [https://github.com/QingruZhang/PASTA](https://github.com/QingruZhang/PASTA).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of large language models (LLMs) has marked a significant milestone
    in natural language processing (NLP) and artificial intelligence (AI), showcasing
    exceptional performance across a wide range of tasks (Vaswani et al., [2017](#bib.bib49);
    Brown et al., [2020a](#bib.bib3); OpenAI, [2023](#bib.bib31)). Efforts to further
    refine these models have been relentless, aiming to enable them to process and
    respond to natural and programming languages with human-like expertise (Stiennon
    et al., [2020](#bib.bib45); Yao et al., [2023](#bib.bib57)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite their remarkable achievements, LLMs often encounter challenges in understanding
    their contextual inputs during interactions with users (Shen et al., [2023](#bib.bib39);
    Lu et al., [2021](#bib.bib25)). This difficulty becomes particular evident when
    they are presented prompts¹¹1We use prompts to refer to all LLM text inputs, including
    user instructions, and the other background information (which we refer to as
    context). containing extensive background contexts or complex user instructions.
    Lengthy contexts can overwhelm LLMs, as their attention modules, learned from
    data, are unable to fully capture crucial details (Liu et al., [2023](#bib.bib23)).
    Complex instructions can further inhibit the model from focusing on the user’s
    intentions, resulting in undesired outputs (Wei et al., [2022](#bib.bib54)). Additionally,
    for time-sensitive data, such as news articles, there can exist factual knowledge
    within contexts, which contradicts with model prior beliefs induced from outdated
    pre-training. As a result, a model may generate outputs conditioned on its pre-existing
    belief instead of attending to new facts within the contexts (Meng et al., [2022a](#bib.bib26);
    [b](#bib.bib27); Mitchell et al., [2022](#bib.bib29); Hernandez et al., [2023](#bib.bib14)).
    All of these challenges contribute to LLMs struggling to comprehend user intentions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to LLMs, human readers rarely struggle to understand the emphases
    of articles and intentions of writers. Writers often leverage a variety of text
    styles, such as bold and italics, to emphasize specific contents. This mechanism
    enables writers to direct and maintain the attention of human readers, ensuring
    that the intended information is accurately captured. In interactions between
    users and LLMs, it is users also need to highlight specific information for the
    model. Consequently, model generation can be effectively biased in accordance
    with user guidance, thus addressing the challenges mentioned earlier. This feature
    is particularly essential when designing user-AI interfaces, and can be frequently
    applied in extensive conversations between users and models. Existing methods,
    however, do not support such a mechanism. LLMs are inherently limited to processing
    plain texts, devoid of any stylistic cues or emphasis markers (Brown et al., [2020b](#bib.bib4);
    Liu et al., [2021](#bib.bib24); Wei et al., [2022](#bib.bib54)). Even when emphasis
    markers are added to prompts, state-of-the-art LLMs often struggle to discern
    weak signals from a couple of marker tokens (See evidence in Section [5.1](#S5.SS1
    "5.1 Main result: PASTA improves model generation ‣ 5 Results ‣ Tell Your Model
    Where to Attend: Post-hoc Attention Steering for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bdf42534f465877119c9ebacc94f3487.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: PASTA uses a user-specified part of the input to steer the model
    generation aligning with user intentions. PASTA modifies the attention scores
    generated during inference, by emphasizing the scores generated at token positions
    corresponding to the user-specified part of the context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by the need to convey user emphasis, we introduce PASTA (Post-hoc
    Attention STeering Approach), a post-hoc method²²2Post-hoc means that our method
    does not update the model weights. that enables users to highlight specific information,
    e.g., an instruction as in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Tell
    Your Model Where to Attend: Post-hoc Attention Steering for LLMs"), and steer
    models to interpret emphasized texts like human readers. Specifically, PASTA selects
    a small subset of attention heads and applies precise attention reweighting on
    them. As illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Tell Your
    Model Where to Attend: Post-hoc Attention Steering for LLMs"), PASTA upweights
    the attention scores of the user-specified tokens while downweighting the other
    tokens at specific attention heads. Our method is inspired by the observation
    that attention modules exhibit various token-attending patterns across different
    heads (Michel et al., [2019](#bib.bib28); Voita et al., [2019](#bib.bib50); Clark
    et al., [2019](#bib.bib6)). These attention patterns can be interpreted as encoding
    diverse semantic or syntactic information, and altering them can substantially
    influence model behaviors (Shi et al., [2023a](#bib.bib40); Hu et al., [2021b](#bib.bib16)).
    Through steering attention modules, PASTA directs the model to pay close attention
    to the user-specified parts and hence generate the desired output aligning with
    the highlighted contents. Notably, PASTA is applied after training and does not
    require changing any model parameters; PASTA only requires access to the attention
    scores of specific heads of an LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Since attention heads can serve different functions (Tenney et al., [2019](#bib.bib47);
    Deb et al., [2023](#bib.bib10)), we introduce an efficient model profiling algorithm
    to identify which heads are effective for steering. Specifically, we subsample
    small training sets from multiple tasks and evaluate the performance of attention
    steering for each individual head across these tasks. PASTA selects the attention
    heads that, when steered, generally improve the multi-task performance. We empirically
    observe that steering these heads not only benefits the existing tasks but also
    enhances the performance on unseen tasks. Notably, the model profiling is performed
    only once for an LLM. The selected attention heads can be regarded as a model-level
    profile, effective for steering the LLM on unseen tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We conduct experiments on diverse tasks to demonstrate the effectiveness of
    PASTA. Specifically, we evaluate PASTA using GPT-J-6B (Wang & Komatsuzaki, [2021](#bib.bib51))
    and LLAMA-7B (Touvron et al., [2023](#bib.bib48)) on tasks that span complex instructions,
    lengthy contexts, and knowledge conflicts within contexts. The results demonstrate
    that PASTA consistently provides a significant performance improvement over baseline
    prompting strategies. For example, PASTA achieve an average accuracy improvement
    of 22% over few-shot prompting for LLAMA-7B across 4 challenging tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem description
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In standard LLM prompting, we are given a pre-trained LLM and a text prompt
    $\bm{x}$ to be emphasized.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the example in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Tell Your
    Model Where to Attend: Post-hoc Attention Steering for LLMs"), $\bm{x}$ can simply
    be the final instruction Return her occupation in json format. In evaluation datasets,
    we assume that the user-specified part of each example is already provided by
    enclosing at its both ends in some emphasis markers, like ‘$\ast$ by enclosing
    it with the same emphasis markers. $\bm{x}_{g}$ can be specified flexibly. Namely,
    it need not be a continuous span, and can be used to emphasize diverse information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Head Attention. A typical transformer model consists of $L$, MHA of the
    layer $l$ heads: $\text{MHA}^{(l)}\left({\bm{X}}\right)=\text{Concat}(\bm{H}^{(l,1)},...,\bm{H}^{(l,H)})\bm{W}_{o}$
    where'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\bm{H}^{(l,h)}={\bm{A}}^{(l,h)}{\bm{V}}=\text{Softmax}\left({\bm{Q}}{\bm{K}}^{\top}/{\sqrt{d_{h}}}\right){\bm{V}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where ${\bm{Q}}={\bm{X}}\bm{W}_{q_{h}},{\bm{K}}={\bm{X}}\bm{W}_{k_{h}},{\bm{V}}={\bm{X}}\bm{W}_{v_{h}}$
    are learnable projection matrices of head $h$ is typically set to $d/H$ of the
    $l$.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PASTA (Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Method ‣ Tell Your Model Where
    to Attend: Post-hoc Attention Steering for LLMs")) consists of two components:
    (i) post-hoc attention steering, which emphasizes the user-specified parts of
    the input during inference, see Section [3.1](#S3.SS1 "3.1 Post-hoc Attention
    Steering ‣ 3 Method ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering
    for LLMs") and (ii) multi-task model profiling, which selects the effective attention
    heads for steering, see Section [3.2](#S3.SS2 "3.2 Multi-Task Model Profiling
    ‣ 3 Method ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for
    LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 1 PASTA: Post-hoc Attention Steering Approach'
  prefs: []
  type: TYPE_NORMAL
- en: '0:  1:  Input: small training sets $\{\mathcal{D}^{(i)}\}_{i=1}^{m}$, $k$ do3:     for $1\leq
    l\leq L,1\leq h\leq H$ when steering the head $(l,h)$ on $\mathcal{D}^{(i)}$;8:  end for9:  Output:
    The attention head set $\mathcal{H}=\cap_{i=1}^{m}R^{(i)}_{1:k}$, user-underlined
    segments $\mathcal{G}$;2:  Output: the model generations while steering every
    head $(l,h)$ by ([4](#S3.E4 "Equation 4 ‣ 3.1 Post-hoc Attention Steering ‣ 3
    Method ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Post-hoc Attention Steering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PASTA emphasizes the user-specified input subset by downweighting the attention
    scores of tokens that are not specified by the user. Specifically, given the index
    set of highlighted input spans as $\mathcal{G}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $0\leq\alpha<1$ is the index set of tokens that are not in $\mathcal{G}$
    normalizes the scores so that they sum to one. The attention steering ([4](#S3.E4
    "Equation 4 ‣ 3.1 Post-hoc Attention Steering ‣ 3 Method ‣ Tell Your Model Where
    to Attend: Post-hoc Attention Steering for LLMs")) is conducted during the inference
    time and does not require any training.'
  prefs: []
  type: TYPE_NORMAL
- en: '([4](#S3.E4 "Equation 4 ‣ 3.1 Post-hoc Attention Steering ‣ 3 Method ‣ Tell
    Your Model Where to Attend: Post-hoc Attention Steering for LLMs")) steers the
    model attention by scaling down the scores of tokens that are not highlighted
    by the user. When the coefficient $\alpha$ is set very small, user-specified segments
    are highlighted given their increased attention scores after renormalization.
    Consequently, we can direct the model to concentrate more on the user-specified
    tokens, biasing the generation to align with the specified contents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PASTA scales down the attention scores of non-specified tokens by $\alpha$.
    The reason of selecting ([4](#S3.E4 "Equation 4 ‣ 3.1 Post-hoc Attention Steering
    ‣ 3 Method ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for
    LLMs")) is that it can be more numerically stable compared to scaling up scores.
    Alternatively, one can also scale the attention scores by adding a positive constant
    to the underlined tokens $\mathcal{G}$. The reason of we select multiplication
    in ([4](#S3.E4 "Equation 4 ‣ 3.1 Post-hoc Attention Steering ‣ 3 Method ‣ Tell
    Your Model Where to Attend: Post-hoc Attention Steering for LLMs")) instead of
    addition is that it preserves the difference on attention magnitude among the
    highlighted tokens. As such, the steering operation only adjusts overall attention
    scales of two groups of tokens. In contrast, addition by a large constant to the
    highlighted tokens results in their attention scores almost uniformly distributed,
    leading to unnecessary information loss and performance degeneration.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Multi-Task Model Profiling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Empirically, we find that applying attention steering in ([4](#S3.E4 "Equation
    4 ‣ 3.1 Post-hoc Attention Steering ‣ 3 Method ‣ Tell Your Model Where to Attend:
    Post-hoc Attention Steering for LLMs")) to all attention heads performs worse
    than applying it only to specific heads (see Section [5.3](#S5.SS3.SSS0.Px1 "Model
    profiling ‣ 5.3 Analysis and Ablations ‣ 5 Results ‣ Tell Your Model Where to
    Attend: Post-hoc Attention Steering for LLMs")). It is important to specify the
    correct attention heads, given that different heads serve distinctive roles in
    encoding semantic/syntactic information. To this end, we propose a multi-task
    model profiling algorithm to identify the effective attention heads for steering.
    Specifically, given $m$ (e.g., $|\mathcal{D}^{(i)}|=1000$. Then, we evaluate the
    performance of steering every individual attention head $(l,h)$) on each small
    subset $\mathcal{D}^{(i)}$). For every task $i$ and regard the ranking $R^{(i)}=[(l_{1},h_{1}),(l_{2},h_{2}),\dots]$.
    We then set the attention head set $\mathcal{H}$ performing heads, $\mathcal{H}=\cap_{i=1}^{m}R^{(i)}_{1:k}$
    increases.'
  prefs: []
  type: TYPE_NORMAL
- en: Like attention steering, model profiling requires only access to attention scores,
    in addition to its inputs and outputs (model weights and gradients are not required).
    Importantly, this process needs to be performed only once for a LLM, similar to
    finetuning. However, unlike finetuning, model steering does not modify model weights
    and, more importantly, generalizes to new tasks. The resulting head set $\mathcal{H}$
    to both existing tasks and unseen tasks to enhance model contextual understanding
    and benefit downstream performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation tasks and metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We implement PASTA for two pre-trained models: GPT-J (6 billion parameters, (Wang
    & Komatsuzaki, [2021](#bib.bib51))) and LLaMA-7B (7 billion parameters, (Touvron
    et al., [2023](#bib.bib48))). We evaluate the effectiveness of PASTA at (i) handling
    complex user instructions, (ii) interpreting lengthy contexts, and (iii) resolving
    in-context knowledge conflicts. For (i), we introduce two new tasks: JSON formatting
    and Pronouns changing. For (ii) and (iii), we study Bias in Bios (De-Arteaga et al.,
    [2019](#bib.bib9)) and CounterFact (Meng et al., [2022a](#bib.bib26)). For each
    task, we provide a description, describing which part of the input we emphasize,
    and what metrics we use for evaluation (see [Appendix A](#A1 "Appendix A Experimental
    Details ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs")
    for full dataset details).'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ JSON Formatting is a new task that evaluates an LLM’s ability to
    produce outputs in a user-desired format (JSON). This is an important usecase
    for LLMs when their output is being used in a downstream process. This task utilizes
    the biographical data from BiasBios (described below) but appends a different
    instruction to the end of contexts: answer the occupation of {person} and generate
    the answer as JSON format. The instruction prompts models to generate outputs
    in JSON format. \faHandORight We emphasize the final instruction'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics: (a) Format accuracy (F. Acc.) measures the accuracy at generating
    valid JSON. (b) Prediction accuracy (P. Acc.) measures the accuracy at generating
    the correct target in JSON values after loading the JSON-formatted generations.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Pronouns changing is a new task that evaluates an LLM’s ability to
    follow a difficult user instruction. It again uses the biographical contexts from
    BiasBios but instead instructs models to: substitute ‘she’ and ‘he’ with ‘they’
    and generate the occupation of {person} after changing pronouns. \faHandORight
    We emphasize the final instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics: (a) Accuracy evaluates the ratio that ‘she/he’ are successfully changed
    to ‘they’ in model generations. (b) All-changed accuracy (A. Acc.) is the ratio
    that models replace all corresponding pronouns, i.e., changing she/he/her/him/hers/his
    to they/them/their/theirs.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ CounterFact measures an LLM’s ability to generate text consistent
    with a new fact. Each example consists of (subject, relation, old target, new
    target), e.g., (Kevin Garnett, is a professional, basketball player, baseball
    player). We present the model both old and new facts following the prompt: Previously,
    {old fact}, but currently, {new fact}. {question}. This change in facts over time
    often confuses LLMs, resulting in random guesses on two of them when answering
    the {question}.'
  prefs: []
  type: TYPE_NORMAL
- en: \faHandORight
  prefs: []
  type: TYPE_NORMAL
- en: We emphasize the input span containing the new fact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics: we evaluate metrics following (Meng et al., [2022a](#bib.bib26)):
    (a) Efficacy score (ES) is the portion of cases for which the model has <math
    id="S4.SS0.SSS0.Px1.p8.1.m1.2" class="ltx_Math" alttext="P_{\text{LLM}}(\text{new
    target})></math>; (b) Paraphrase score (PS) is the same as ES but changes the
    {question} with a set of rephrased questions to assess the generalization'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ BiasBios consists of professional biographies of non-famous people,
    originally introduced to investigate gender bias in occupations. Each example
    includes biographical context and a label of target occupation. The first sentence
    mentions the person’s occupation, and subsequent sentences describe the individual’s
    career history but may not be directly related to the prediction, potentially
    distracting the model attention. At the end of the context, we append the question:
    {person} has the occupation of. \faHandORight We emphasize the first sentence,
    as it carries the most information about the occupation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics: following (Hernandez et al., [2023](#bib.bib14)), we compute Accuracy
    by checking whether the probability assigned to the target occupation is the highest
    among the 28 candidate occupations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Pronouns changing, CounterFact, and BiasBios, we additionally measure Fluency
    as the average bi-gram and tri-gram entropy of generations, designed to be low
    for degenerated or repetitive texts (Meng et al., [2022a](#bib.bib26)). We filter
    out any results receiving a fluency below 3.0 (see full results including fluency
    in [Appendix B](#A2 "Appendix B Extended results with fluency ‣ Tell Your Model
    Where to Attend: Post-hoc Attention Steering for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare PASTA to the following baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Zero-shot prompting is the most common approach to interact with LLMs,
    in which a user feeds models a prompt containing background context and a user
    instruction or question.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Marked prompting alters the prompts used in zero-shot prompting by
    surrounding user-specified input spans with emphasis markers, e.g. asterisks,
    as is done in markdown files for emphasis, or quotes, as is done in natural languages.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Few-shot prompting includes demonstrations (example inputs and target
    outputs) at the beginning of the prompt fed to the LLM. Few-shot prompting often
    improves performance in new tasks, but increases the computational cost of inference
    due to the increased prompt length, particularly when demonstrations are lengthy (Dong
    et al., [2023](#bib.bib13)); here we use 3 demonstrations in context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Main results of LLAMA-7B to demonstrate that PASTA can improve the
    model ability to (i) follow user instruction (JSON Format and Prons. Changing);
    (ii) interpret contextual information (BiasBios); (iii) resolving knowledge conflicts
    (CounterFact). For all scores, higher is better. The best results are in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | JSON Format | Prons. Changing | BiasBios | CounterFact | All
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | F. Acc / P. Acc | Acc / A.Acc | Acc | ES / PS | Ave. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompting | Zero-shot | 60.00 / 54.94 | 71.84 / 66.28 | 87.36 | 58.50 / 52.03
    | 67.29 |'
  prefs: []
  type: TYPE_TB
- en: '| $\ast$-marked | 18.55 / 12.71 | 39.14 / 35.17 | 90.62 | 57.74 / 50.52 | 49.38
    |'
  prefs: []
  type: TYPE_TB
- en: '| “”-marked | 4.56 / 4.20 | 20.55 / 18.19 | 89.82 | 58.14 / 51.70 | 42.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 84.85 / 73.58 | 59.06 / 55.27 | 88.79 | 87.45 / 49.82 | 73.45
    |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | Task-agnostic | 88.16 / 49.08 | 83.65 / 81.31 | 93.54 | 98.82 / 99.03
    | 85.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task | 96.64 / 85.09 | 96.42 / 95.84 | 95.28 | 99.60 / 99.57 | 95.46
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Main results of GPT-J to demonstrate that PASTA can improve the model
    ability to (i) follow user instruction (JSON Format and Prons. Changing); (ii)
    interpret contextual information (BiasBios); (iii) resolving knowledge conflicts
    (CounterFact). For all scores, higher is better. The best results are in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | JSON Format | Prons. Changing | BiasBios | CounterFact | All
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | F. Acc / P. Acc | Acc / A.Acc | Acc | ES / PS | Ave. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompting | Zero-shot | 28.83 / 25.09 | 39.88 / 36.19 | 72.76 | 42.14 / 42.02
    | 44.96 |'
  prefs: []
  type: TYPE_TB
- en: '| $\ast$-marked | 4.44 / 4.10 | 41.25 / 37.57 | 74.14 | 44.50 / 45.09 | 40.63
    |'
  prefs: []
  type: TYPE_TB
- en: '| “”-marked | 8.81 / 5.62 | 6.12 / 5.72 | 78.64 | 45.54 / 41.84 | 33.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 84.15 / 72.65 | 35.77 / 32.08 | 72.98 | 68.34 / 38.23 | 59.65
    |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | Task-agnostic | 46.68 / 34.71 | 91.62 / 88.60 | 80.84 | 99.54 / 99.57
    | 77.80 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task | 91.50 / 18.63 | 92.96 / 91.34 | 94.96 | 98.62 / 98.79 | 85.22
    |'
  prefs: []
  type: TYPE_TB
- en: PASTA settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We study PASTA in 2 settings: multi-task and task-agnostic. In the multi-task
    setting, the evaluation task $j$). The multi-task setting improves performance
    but requires labeled training samples for the task which is evaluated, which can
    be difficult to obtain in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirically, we find that PASTA is not sensitive to the scaling coefficient
    $\alpha$ from {300, 400, 500} for LLAMA-7B to have the number of steered heads
    $|\mathcal{H}|$, i.e., $k=400$. For GPT-J, we select $k$ as {52, 72, 111, 153}.
    For every task, we split data into train/validation/test sets following (Hernandez
    et al., [2023](#bib.bib14)) (See Appendix [A](#A1 "Appendix A Experimental Details
    ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs")) and
    select $|\mathcal{H}|$ by cross validation. For all tasks, model outputs are generated
    with greedy search.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '5.1 Main result: PASTA improves model generation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tables [2](#S4.T2 "Table 2 ‣ Baselines. ‣ 4 Experimental setup ‣ Tell Your
    Model Where to Attend: Post-hoc Attention Steering for LLMs") and [2](#S4.T2 "Table
    2 ‣ Baselines. ‣ 4 Experimental setup ‣ Tell Your Model Where to Attend: Post-hoc
    Attention Steering for LLMs") present the main results for PASTA applied to LLAMA-7B
    and GPT-J respectively. Few-shot prompting is the strongest baseline, and task-agnostic
    PASTA outperforms it on the main metric for each task for all settings except
    JSON Formatting with GPT-J. Multi-task PASTA outperforms all baselines across
    all settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PASTA can improve LLM instruction following. The results from JSON Formatting
    and Pronouns Changing tasks indicate that, by highlighting the user instruction
    at the end of inputs, PASTA effectively steers models to focus on user intentions,
    thereby biasing their generation to fulfill specific requirements or formats.
    For example, while GPT-J only achieves 39.9% of its zero-shot generations complying
    the user requirement on the Pronouns Changing task, PASTA yields a remarkable
    53% accuracy improvement by emphasizing the instruction. Moreover, PASTA achieves
    an impressive 96.64% format accuracy and 85.09% prediction accuracy when applied
    to LLAMA-7B on the JSON Formatting task. This performance exceeds that of few-shot
    prompting by 11%, even though few-shot prompting explicitly provides the model
    with correct JSON examples through additional demonstrations. Table [3](#S5.T3
    "Table 3 ‣ 5.1 Main result: PASTA improves model generation ‣ 5 Results ‣ Tell
    Your Model Where to Attend: Post-hoc Attention Steering for LLMs") presents a
    few examples generated by LLAMA-7B when applying PASTA.'
  prefs: []
  type: TYPE_NORMAL
- en: PASTA can help models capture crucial contextual information. In the case of
    BiasBios and CounterFact tasks, we apply PASTA to emphasize specific context spans
    for LLMs. Consequently, the models are guided to pay close attention to the specific
    contextual information or new facts within contexts. The results from these two
    tasks illustrate that PASTA can direct the models to interpret the crucial information
    or resolve the knowledge conflicts within contexts, yielding significant improvement
    in prediction performance of both tasks. For example, PASTA achieves a prediction
    accuracy of 94.96% for GPT-J on the BiasBios task, which is 16.32% higher than
    the best baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Generation examples of LLAMA-7B on JSON Formatting and Pronouns Changing
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Prompt | Zero-shot Generation | PASTA Generation |'
  prefs: []
  type: TYPE_TB
- en: '| JSON Format | Winnie is an American photographer living in New York. Specialized
    in fashion photography and portrait, she applies her talent on both humans and
    animals …{instruction} | ✗ Winnie is a fashion and portrait photographer living
    in New York, specialized in fashion photography and portrait, who applies her
    talent on both humans and animals. | ✓ {“name”: “Winnie”, “occupation”: “photographer”
    } |'
  prefs: []
  type: TYPE_TB
- en: '| Prons. Change | Mary is an Associate Professor in the Department of Curriculum
    Instruction at St. John University; she holds a doctorate in Reading/Writing/Literacy
    from the University of Pennsylvania. She recently published an article in “Urban
    Education” with …{instruction} | ✗ Mary is a professor in the Department of Curriculum
    Instruction at St. John University; she holds a doctorate in Reading/Writing/Literacy
    from the University of Pennsylvania. She recently published an article in “Urban
    Education” with … | ✓ Mary is an associate professor in the department of curriculum
    and instruction at St. John’s University; they hold a doctorate in reading/writing/literacy
    from the University of Pennsylvania. They recently published an article in … |'
  prefs: []
  type: TYPE_TB
- en: 'Tables [2](#S4.T2 "Table 2 ‣ Baselines. ‣ 4 Experimental setup ‣ Tell Your
    Model Where to Attend: Post-hoc Attention Steering for LLMs") and [2](#S4.T2 "Table
    2 ‣ Baselines. ‣ 4 Experimental setup ‣ Tell Your Model Where to Attend: Post-hoc
    Attention Steering for LLMs") also suggest that marked prompting, a baseline that
    highlights specific texts akin to human writers, struggles to effectively convey
    emphasis to LLMs. One possible reason is that these emphasis markers rarely appear
    in the massive pre-training data. In contrast, few-shot prompting sometimes leads
    to improvements in model performance. However, a drawback of few-shot prompting
    is its instability, i.e. its performance exhibits high variance across different
    samples in the demonstration (See Appendix [C](#A3 "Appendix C Extended results
    ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 PASTA can mitigate the sensitivity of prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Results about sensitivity of model performance to prompt rephrasing
    on the JSON Formatting task. Given rephrased instructions in prompt template,
    PASTA can imporve zero-shot performance for all prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Instruction | Method | LLAMA-7B | GPT-J | Average |'
  prefs: []
  type: TYPE_TB
- en: '| JSON Format F. Acc / P. Acc | Prons. Changing Acc / A. Acc | JSON Format
    F. Acc / P. Acc | Prons. Changing Acc / A. Acc |'
  prefs: []
  type: TYPE_TB
- en: '| Original | Zero-shot | 60.0 / 54.9 | 71.8 / 66.3 | 28.8 / 25.1 | 39.9 / 36.2
    | 47.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | 96.6 / 85.1 | 96.4 / 95.8 | 91.5 / 18.6 | 93.0 / 91.3 | 83.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Shortened | Zero-shot | 36.0 / 32.4 | 49.2 / 42.6 | 25.4 / 17.1 | 56.5 /
    54.8 | 39.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | 87.4 / 65.9 | 89.0 / 86.9 | 54.1 / 37.0 | 94.0 / 93.7 | 76.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Rephrased | Zero-shot | 57.9 / 54.2 | 82.3 / 79.6 | 63.3 / 50.3 | 76.0 /
    72.8 | 67.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | 97.1 / 87.1 | 89.6 / 89.0 | 77.5 / 68.1 | 94.8 / 92.3 | 86.9 |'
  prefs: []
  type: TYPE_TB
- en: 'It is well-known that the the performance of LLMs can be sensitive to minor
    changes in prompts, such as rephrasing and reformatting, even when these prompts
    convey the same meaning (Reynolds & McDonell, [2021](#bib.bib34); Liu et al.,
    [2021](#bib.bib24)). We find that PASTA can alleviate the sensitivity of model
    performance to varying prompts. Specifically, Table [4](#S5.T4 "Table 4 ‣ 5.2
    PASTA can mitigate the sensitivity of prompts ‣ 5 Results ‣ Tell Your Model Where
    to Attend: Post-hoc Attention Steering for LLMs") evaluates the performance of
    LLAMA-7B and GPT-J on JSON Formatting and Pronouns Changing task given different
    instructions in the prompt template, all of which convey the same meaning (see
    precise prompts in [Sec. A.1](#A1.SS1 "A.1 Detailed prompt templates of each task
    ‣ Appendix A Experimental Details ‣ Tell Your Model Where to Attend: Post-hoc
    Attention Steering for LLMs")). The results show that zero-shot performance is
    sensitive to different prompts and can significantly deteriorate with poorly crafted
    templates. In contrast, PASTA consistently improves model performance over zero-shot
    prompting for all prompts, effectively mitigating sensitivity to variations in
    the prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Analysis and Ablations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we investigate different hyperparameter choices and modeling
    decisions that affect the performance of PASTA.
  prefs: []
  type: TYPE_NORMAL
- en: Model profiling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc980f5b727b7385045f9d508329ae80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The performance of LLAMA-7B on the JSON Formatting task when we steer
    (i) all heads (green); (ii) an entire layer (yellow); and (iii) an individual
    head within a layer (blue violin plot). The performance varies dramatically across
    layers and across heads of a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S5.F2 "Figure 2 ‣ Model profiling ‣ 5.3 Analysis and Ablations
    ‣ 5 Results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for
    LLMs") presents the results on the importance of model profiling introduced in
    Section [3.2](#S3.SS2 "3.2 Multi-Task Model Profiling ‣ 3 Method ‣ Tell Your Model
    Where to Attend: Post-hoc Attention Steering for LLMs"). We compare PASTA when
    steering the selected heads versus other reasonable choices: steering (i) all
    heads, (ii) entire layers, or (iii) individual heads on the JSON Formatting task
    (See Appendix [C.2](#A3.SS2 "C.2 Model Profiling Results ‣ Appendix C Extended
    results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs")
    for comparisons on the remaining tasks). Selecting heads via model profiling in
    PASTA (red line) significantly outperforms other approaches. Steering all heads
    (dashed green line) degrades performance compared to the baseline zero-shot performance
    (dashed black line). This is likely because steering all heads over-amplifies
    the user-specified information at the expense of other essential information required
    for effective generation and prediction. Interestingly, we find that the performance
    varies significantly when steering different layers (yellow) or heads (blue violin
    plot). As mentioned in Section [1](#S1 "1 Introduction ‣ Tell Your Model Where
    to Attend: Post-hoc Attention Steering for LLMs"), attention heads play distinct
    roles in encoding diverse semantic and syntactic information (Tenney et al., [2019](#bib.bib47)).
    When steering heads, which are appropriately involved in encoding of user-specified
    information, the model can be guided to capture and reinforce these specific signals.
    Conversely, modifying the attention of unrelated heads not only fails to emphasize
    the desired information but also interferes with their original functions, resulting
    in performance deterioration. Therefore, it is important to identify the effective
    heads through model profiling prior to applying the steering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Varying strategies for selecting heads during profiling. As described in [Sec. 5.3](#S5.SS3.SSS0.Px1
    "Model profiling ‣ 5.3 Analysis and Ablations ‣ 5 Results ‣ Tell Your Model Where
    to Attend: Post-hoc Attention Steering for LLMs"), our model profiling selects
    the Intersection of the top-$k$, we can select heads for steering with different
    strategies: (i) Task-specific – steer the top-$k_{2}$, i.e., $R^{(j)}_{1:k_{2}}$.
    Table [5](#S5.T5 "Table 5 ‣ Model profiling ‣ 5.3 Analysis and Ablations ‣ 5 Results
    ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs") compares
    their performance. Using task-specific heads rather than intersection-selected
    heads sometimes yields improved performance, but requires selecting a different
    set of heads for each new task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Varying head selection strategies between top task-specific heads,
    union across multiple tasks, and intersection (the default used in PASTA).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PASTA | JSON Format | Prons. Changing | BiasBios | CounterFact | All |'
  prefs: []
  type: TYPE_TB
- en: '|  | F. Acc / P. Acc | Acc / A.Acc | Acc | ES / PS | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA | Task-specific | 95.56 / 86.83 | 98.52 / 98.02 | 97.62 | 99.18 / 99.24
    | 96.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Union | 88.42 / 74.49 | 92.12 / 91.44 | 96.36 | 99.24 / 99.35 | 92.22 |'
  prefs: []
  type: TYPE_TB
- en: '| Intersection | 96.64 / 85.09 | 96.42 / 95.84 | 95.28 | 99.60 / 99.57 | 95.46
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-J | Task-specific | 85.71 / 79.39 | 94.74 / 92.54 | 97.64 | 99.26 / 99.34
    | 93.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Union | 72.61 / 64.89 | 89.68 / 87.76 | 95.56 | 99.82 / 99.83 | 88.21 |'
  prefs: []
  type: TYPE_TB
- en: '| Intersection | 91.50 / 18.63 | 92.96 / 91.34 | 94.96 | 98.62 / 98.79 | 85.22
    |'
  prefs: []
  type: TYPE_TB
- en: 'Varying the number of heads to be steered. Figures [3(a)](#S5.F3.sf1 "Figure
    3(a) ‣ Figure 3 ‣ Model profiling ‣ 5.3 Analysis and Ablations ‣ 5 Results ‣ Tell
    Your Model Where to Attend: Post-hoc Attention Steering for LLMs") and [3(b)](#S5.F3.sf2
    "Figure 3(b) ‣ Figure 3 ‣ Model profiling ‣ 5.3 Analysis and Ablations ‣ 5 Results
    ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs") illustrate
    the performance of PASTA when steering different number of heads on two tasks.
    The results suggest that as more heads are included for steering, the model follows
    the user even more closely, achieving higher efficacy (JSON Format Acc. and Pron. Change
    Acc.). However, at some point, this it results in a decrease in the metrics reflecting
    the generation quality (JSON Pred. Acc and Fluency). Thus, there is a trade-off
    between emphasizing efficacy and generation quality, requiring choosing the number
    of heads during model profiling.'
  prefs: []
  type: TYPE_NORMAL
- en: Varying the scaling coefficient $\alpha$. The results indicate that PASTA is
    fairly robust to this hyperparameter; in practice, we fix it as 0.01\. Notice
    that setting $\alpha$ to zero should be avoided, as this leads to the complete
    removal of other crucial contexts at the steered heads, resulting in performance
    degeneration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04c6ca7cbfc791fe6c8a7a8dd338aaf9.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) JSON Format
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/867271b2f5031a6d6882700691da7265.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Prons. Change
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cdd2e30d86c13964d85d253043d6c37f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Varying $\alpha$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The performance of applying PASTA to LLAMA-7B on JSON Formating and
    Pronouns Changing tasks when varying the number of steered heads $|\mathcal{H}|$
    ([3(c)](#S5.F3.sf3 "Figure 3(c) ‣ Figure 3 ‣ Model profiling ‣ 5.3 Analysis and
    Ablations ‣ 5 Results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering
    for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary method for controlling LLMs has been through prompting, often yielding
    impressive improvements in performance (Brown et al., [2020b](#bib.bib4); Liu
    et al., [2021](#bib.bib24); Wei et al., [2022](#bib.bib54)) and spurring a line
    of work aiming to make prompting easier, e.g.  (Strobelt et al., [2022](#bib.bib46);
    Bach et al., [2022](#bib.bib1); Shin et al., [2020](#bib.bib42); Deng et al.,
    [2022](#bib.bib11); Singh et al., [2023b](#bib.bib44)). However, LLMs remain extremely
    sensitive to nuances in prompts (Webson & Pavlick, [2021](#bib.bib52); Lu et al.,
    [2021](#bib.bib25)); PASTA complements these approaches by making it easier for
    a user to specify a prompt in difficult scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of work aims to make LLMs more amenable to prompting by modifying
    them during training. Most prominent among these approaches are instruction finetuning (Wei
    et al., [2021](#bib.bib53); Chung et al., [2022](#bib.bib5)), Reinforcement Learning
    from Human Feedback (Ziegler et al., [2019](#bib.bib59); Ouyang et al., [2022](#bib.bib32)),
    and other related methods, e.g. (Lee et al., [2023](#bib.bib21)). There are also
    a few methods for directly specifying which parts on an input are important during
    training, e.g. (Ross et al., [2017](#bib.bib36); Rieger et al., [2019](#bib.bib35);
    Schramowski et al., [2020](#bib.bib37); Krishna et al., [2023](#bib.bib20)). PASTA
    can be used in addition to these approaches to improve some aspects of model steerability
    (e.g. instruction following).
  prefs: []
  type: TYPE_NORMAL
- en: PASTA is related to variety of methods for adapting to new tasks, including
    LoRA (Hu et al., [2021a](#bib.bib15)), AdaLoRA (Zhang et al., [2023](#bib.bib58)),
    QLoRA (Dettmers et al., [2023](#bib.bib12)), and TOAST (Shi et al., [2023b](#bib.bib41)).
    PASTA is also related to a variety of research on model editing, e.g. ROME (Meng
    et al., [2022a](#bib.bib26)), MEMIT (Meng et al., [2022b](#bib.bib27)), MEND (Mitchell
    et al., [2022](#bib.bib29)), and REMEDI (Hernandez et al., [2023](#bib.bib14)).
    Unlike these works, PASTA preserves an LLMs ability to transfer to new tasks using
    prompts and human-selected info, rather than using new labeled examples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, PASTA is also motivated by works which have aimed to mechanistically
    understand attention scores (Zou et al., [2023](#bib.bib60)), e.g. by studying
    them through feature importance (Jain & Wallace, [2019](#bib.bib17); Wiegreffe
    & Pinter, [2019](#bib.bib55); Deb et al., [2023](#bib.bib10)), through probing (Conneau
    et al., [2018](#bib.bib7); Liu & Avci, [2019](#bib.bib22)), through visualization (Karpathy
    et al., [2015](#bib.bib18); Olah et al., [2017](#bib.bib30)), localizing knowledge (Meng
    et al., [2022a](#bib.bib26); Dai et al., [2021](#bib.bib8)), categorizing directions
    in representation space (Kim et al., [2017](#bib.bib19); Schwettmann et al., [2021](#bib.bib38)),
    or through natural-language explanations (Bills et al., [2023](#bib.bib2); Singh
    et al., [2023a](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we propose PASTA, a novel approach aimed at enabling LLMs to
    move beyond the limitations of plain text and effectively perceive user guidance
    embodied as highlighted parts of prompts. By making precise adjustments to attention
    scores in selected heads, PASTA directs the model’s focus to the relevant context,
    mirroring the way humans benefit from textual cues. Unlike traditional fine-tuning
    methods, PASTA is applied at inference time and requires neither parameter updates
    nor gradient computation; PASTA requires only selecting which attention heads
    to apply the re-weighting to, a one-time profiling operation for a LLM. Experimental
    results show that PASTA can significantly improve model performance on a variety
    of tasks. In the future, we plan to integrate PASTA with various other methods,
    such as few-shot in-context learning, aiming to highlight effective examples to
    enhance its stability.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bach et al. (2022) Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson,
    Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault
    Fevry, et al. Promptsource: An integrated development environment and repository
    for natural language prompts. *arXiv preprint arXiv:2202.01279*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bills et al. (2023) Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman,
    Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.
    Language models can explain neurons in language models. *URL https://openaipublic.
    blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05\.
    2023)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020a. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020b) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. What does BERT look at? an analysis of BERT’s attention. In *Proceedings
    of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks
    for NLP*, pp.  276–286, Florence, Italy, August 2019\. Association for Computational
    Linguistics. doi: 10.18653/v1/W19-4828. URL [https://aclanthology.org/W19-4828](https://aclanthology.org/W19-4828).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conneau et al. (2018) Alexis Conneau, German Kruszewski, Guillaume Lample,
    Loïc Barrault, and Marco Baroni. What you can cram into a single vector: Probing
    sentence embeddings for linguistic properties. *arXiv preprint arXiv:1805.01070*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2021) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and
    Furu Wei. Knowledge neurons in pretrained transformers. *arXiv preprint arXiv:2104.08696*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De-Arteaga et al. (2019) Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer
    Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,
    and Adam Tauman Kalai. Bias in bios: A case study of semantic representation bias
    in a high-stakes setting. In *proceedings of the Conference on Fairness, Accountability,
    and Transparency*, pp.  120–128, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deb et al. (2023) Mayukh Deb, Björn Deiseroth, Samuel Weinbach, Patrick Schramowski,
    and Kristian Kersting. Atman: Understanding transformer predictions through memory
    efficient attention manipulation. *arXiv preprint arXiv:2301.08110*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2022) Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing
    discrete text prompts with reinforcement learning. *arXiv preprint arXiv:2205.12548*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernandez et al. (2023) Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting
    and editing knowledge representations in language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021a) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021b) J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. *arXiv preprint abs:2106.09685*, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain & Wallace (2019) Sarthak Jain and Byron C Wallace. Attention is not explanation.
    *arXiv preprint arXiv:1902.10186*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpathy et al. (2015) Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing
    and understanding recurrent networks. *arXiv preprint arXiv:1506.02078*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2017) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James
    Wexler, Fernanda Viegas, and Rory Sayres. Interpretability beyond feature attribution:
    Quantitative testing with concept activation vectors (tcav). *arXiv preprint arXiv:1711.11279*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishna et al. (2023) Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun,
    Sameer Singh, and Himabindu Lakkaraju. Post hoc explanations of language models
    can improve language models. *arXiv preprint arXiv:2305.11426*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
    Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling
    reinforcement learning from human feedback with ai feedback. *arXiv preprint arXiv:2309.00267*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu & Avci (2019) Frederick Liu and Besim Avci. Incorporating priors with feature
    attribution on text classification. *arXiv preprint arXiv:1906.08286*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *arXiv preprint arXiv:2107.13586*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming
    few-shot prompt order sensitivity. *arXiv preprint arXiv:2104.08786*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2022a) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
    Locating and editing factual associations in gpt. *Advances in Neural Information
    Processing Systems*, 35:17359–17372, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2022b) Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov,
    and David Bau. Mass-editing memory in a transformer. *arXiv preprint arXiv:2210.07229*,
    2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
    heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
    E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems*,
    volume 32\. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitchell et al. (2022) Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
    Finn, and Christopher D. Manning. Fast model editing at scale, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Olah et al. (2017) Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature
    visualization. *Distill*, 2(11):e7, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. Pytorch: An imperative style, high-performance deep learning
    library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,
    Emily B. Fox, and Roman Garnett (eds.), *Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, December 8-14, 2019, Vancouver, BC, Canada*, pp.  8024–8035, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reynolds & McDonell (2021) Laria Reynolds and Kyle McDonell. Prompt programming
    for large language models: Beyond the few-shot paradigm, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rieger et al. (2019) Laura Rieger, Chandan Singh, W James Murdoch, and Bin
    Yu. Interpretations are useful: penalizing explanations to align neural networks
    with prior knowledge. *arXiv preprint arXiv:1909.13584*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ross et al. (2017) Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez.
    Right for the right reasons: Training differentiable models by constraining their
    explanations. *arXiv preprint arXiv:1703.03717*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schramowski et al. (2020) Patrick Schramowski, Wolfgang Stammer, Stefano Teso,
    Anna Brugger, Franziska Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin
    Mahlein, and Kristian Kersting. Making deep neural networks right for the right
    scientific reasons by interacting with their explanations. *Nature Machine Intelligence*,
    2(8):476–486, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwettmann et al. (2021) Sarah Schwettmann, Evan Hernandez, David Bau, Samuel
    Klein, Jacob Andreas, and Antonio Torralba. Toward a visual concept vocabulary
    for gan latent space. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pp.  6804–6812, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong
    Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment:
    A survey, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023a) Baifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast:
    Transfer learning via attention steering. *arXiv preprint abs:2305.15542*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023b) Baifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Refocusing
    is key to transfer learning. *arXiv preprint arXiv:2305.15542*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. *arXiv preprint arXiv:2010.15980*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2023a) Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee
    Jain, Alexander G Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules
    in natural language with language models. *arXiv preprint arXiv:2305.09863*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2023b) Chandan Singh, John X. Morris, Jyoti Aneja, Alexander M.
    Rush, and Jianfeng Gao. Explaining patterns in data with language models via interpretable
    autoprompting, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning
    to summarize from human feedback. *arXiv preprint abs:2009.01325*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strobelt et al. (2022) Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin
    Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M. Rush. Interactive and
    visual prompt engineering for ad-hoc task adaptation with large language models,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tenney et al. (2019) Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers
    the classical NLP pipeline. In *Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics*, pp.  4593–4601, Florence, Italy, July
    2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL
    [https://aclanthology.org/P19-1452](https://aclanthology.org/P19-1452).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett (eds.), *Advances in Neural Information Processing Systems*, volume 30\.
    Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned, July 2019. URL [https://aclanthology.org/P19-1580](https://aclanthology.org/P19-1580).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang & Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion
    Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax),
    May 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Webson & Pavlick (2021) Albert Webson and Ellie Pavlick. Do prompt-based models
    really understand the meaning of their prompts? *arXiv preprint arXiv:2109.01247*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiegreffe & Pinter (2019) Sarah Wiegreffe and Yuval Pinter. Attention is not
    not explanation. *arXiv preprint arXiv:1908.04626*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam
    Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong
    Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, L A Kurilenko, Heyang
    Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He. Deepspeed-chat:
    Easy, fast and affordable rlhf training of chatgpt-like models at all scales.
    *arXiv preprint abs:2308.01320*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng
    He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient
    fine-tuning. In *The Eleventh International Conference on Learning Representations*,
    2023. URL [https://openreview.net/forum?id=lq62uWRJjiY](https://openreview.net/forum?id=lq62uWRJjiY).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown,
    Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences. *arXiv preprint arXiv:1909.08593*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2023) Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip
    Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
    Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven
    Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.
    Representation engineering: A top-down approach to ai transparency, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APPENDIX
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We implement all algorithms using PyTorch (Paszke et al., [2019](#bib.bib33))
    and Huggingface (Wolf et al., [2019](#bib.bib56)) and run experiments on NVIDIA
    V100 GPUs and NVIDIA A6000 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6](#A1.T6 "In Appendix A Experimental Details ‣ Tell Your Model Where
    to Attend: Post-hoc Attention Steering for LLMs") provides detailed statistics
    of datasets in our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Statistics of datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Train | Valid | Test |'
  prefs: []
  type: TYPE_TB
- en: '| CounterFact | 1000 | 1000 | 5000 |'
  prefs: []
  type: TYPE_TB
- en: '| BiasBios | 1000 | 1000 | 5000 |'
  prefs: []
  type: TYPE_TB
- en: '| JSON Formatting | 1000 | 1000 | 5000 |'
  prefs: []
  type: TYPE_TB
- en: '| Pronouns Changing | 1000 | 1000 | 5000 |'
  prefs: []
  type: TYPE_TB
- en: A.1 Detailed prompt templates of each task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For each task, the prompt templates in our results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JSON Formatting:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Original) {context}. Answer the occupation of {person} and generate the answer
    as json format. Here is an example: {“name”: , “occupation”: ,}. Now generate
    the answer.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Shortened one in Section [5.2](#S5.SS2 "5.2 PASTA can mitigate the sensitivity
    of prompts ‣ 5 Results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering
    for LLMs")) {context}. Answer the occupation of {person} and generate the answer
    as json format.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Rephrased one in Section [5.2](#S5.SS2 "5.2 PASTA can mitigate the sensitivity
    of prompts ‣ 5 Results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering
    for LLMs")) Answer the occupation of {person} and generate the answer as json
    format. Here is an example: {“name”: , “occupation”: ,}. {context}. Now generate
    the answer.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pronouns Changing:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Original): {context}. For the aforementioned text, substitute ‘she’ and ‘he’
    with ‘they’ and generate the occupation of {person} after changing pronouns.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Shortened one in Section [5.2](#S5.SS2 "5.2 PASTA can mitigate the sensitivity
    of prompts ‣ 5 Results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering
    for LLMs")): {context}. Change ‘she’ and ‘he’ with ‘they’ and answer the occupation
    of {person} after replacing the pronouns'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Rephrased one in Section [5.2](#S5.SS2 "5.2 PASTA can mitigate the sensitivity
    of prompts ‣ 5 Results ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering
    for LLMs")): {context}. For the aforementioned descriptions, replace ‘she’ and
    ‘he’ with ‘they’ in the aformentioned text and generate the new text after replacing
    the pronouns.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BiasBios: {context}. {person} has the occupation of.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CounterFact: Previously, {old fact}. Currently, {new fact}. {question}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 The evaluation details of PASTA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 7](#A1.T7 "In A.2 The evaluation details of PASTA ‣ Appendix A Experimental
    Details ‣ Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs")
    presents the number of heads to be steered by PASTA for LLAMA-7B and GPT-J-6B
    on every task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The number of heads to be steered by PASTA.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | LLAMA-7B | GPT-J-6B |'
  prefs: []
  type: TYPE_TB
- en: '| JSON Formatting | 53 | 153 |'
  prefs: []
  type: TYPE_TB
- en: '| Pronouns Changing | 86 | 72 |'
  prefs: []
  type: TYPE_TB
- en: '| BiasBios | 86 | 111 |'
  prefs: []
  type: TYPE_TB
- en: '| CounterFact | 86 | 52 |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Extended results with fluency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we include extended results, including fluency metrics. Fluency
    score is the average bi-gram and tri-gram entropy of generations, designed to
    be low for degenerated or repetitive texts (Meng et al., [2022a](#bib.bib26)).
    This metric can be regarded as the reference metric of generation quality. Typically,
    the generations of language models are reliable as long as their fluency score
    is not too low. Here, we filter out any results receiving a fluency score below
    3.0\. Table [9](#A2.T9 "Table 9 ‣ Appendix B Extended results with fluency ‣ Tell
    Your Model Where to Attend: Post-hoc Attention Steering for LLMs"), [9](#A2.T9
    "Table 9 ‣ Appendix B Extended results with fluency ‣ Tell Your Model Where to
    Attend: Post-hoc Attention Steering for LLMs") and [10](#A2.T10 "Table 10 ‣ Appendix
    B Extended results with fluency ‣ Tell Your Model Where to Attend: Post-hoc Attention
    Steering for LLMs") include all results and fluency evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Main results of LLAMA-7B to demonstrate that PASTA can improve the
    model ability to (i) follow user instruction (JSON Format and Prons. Changing);
    (ii) interpret contextual information (BiasBios); (iii) resolving knowledge conflicts
    (CounterFact). For all scores, higher is better. The best results are in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | JSON Format | Prons. Changing | BiasBios | CounterFact |'
  prefs: []
  type: TYPE_TB
- en: '|  | F. Acc / P. Acc | Acc / A.Acc / Flue. | Acc / Flue. | ES / PS /Flue. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompting | Zero-shot | 60.00 / 54.94 | 71.84 / 66.28 / 6.10 | 87.36 / 3.98
    | 58.50 / 52.03 / 4.96 |'
  prefs: []
  type: TYPE_TB
- en: '| $\ast$-marked | 18.55 / 12.71 | 39.14 / 35.17 / 6.03 | 90.62 / 3.89 | 57.74
    / 50.52 / 5.12 |'
  prefs: []
  type: TYPE_TB
- en: '| “”-marked | 4.56 / 4.20 | 20.55 / 18.19 / 5.13 | 89.82 / 3.97 | 58.14 / 51.70
    / 5.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 84.85 / 73.58 | 59.06 / 55.27 / 5.95 | 88.79 / 4.19 | 87.45 /
    49.82 / 5.68 |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | Task-agnostic | 88.16 / 49.08 | 83.65 / 81.31 / 4.62 | 93.54 / 3.03
    | 98.82 / 99.03 / 4.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task | 96.64 / 85.09 | 96.42 / 95.84 / 5.43 | 95.28 / 4.05 | 99.60
    / 99.57 / 4.89 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Main results of GPT-J to demonstrate that PASTA can improve the model
    ability to (i) follow user instruction (JSON Format and Prons. Changing); (ii)
    interpret contextual information (BiasBios); (iii) resolving knowledge conflicts
    (CounterFact). For all scores, higher is better. The best results are in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | JSON Format | Prons. Changing | BiasBios | CounterFact |'
  prefs: []
  type: TYPE_TB
- en: '|  | F. Acc / P. Acc | Acc / A.Acc / Flue. | Acc / Flue. | ES / PS /Flue. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompting | Zero-shot | 28.83 / 25.09 | 39.88 / 36.19 / 5.91 | 72.76 / 5.06
    | 42.14 / 42.02 / 5.01 |'
  prefs: []
  type: TYPE_TB
- en: '| $\ast$-marked | 4.44 / 4.10 | 41.25 / 37.57 / 4.76 | 74.14 / 5.01 | 44.50
    / 45.09 / 5.22 |'
  prefs: []
  type: TYPE_TB
- en: '| “”-marked | 8.81 / 5.62 | 6.12 / 5.72 / 5.43 | 78.64 / 4.96 | 45.54 / 41.84
    / 5.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 84.15 / 72.65 | 35.77 / 32.08 / 6.46 | 72.98 / 4.82 | 68.34 /
    38.23 / 5.67 |'
  prefs: []
  type: TYPE_TB
- en: '| PASTA | Task-agnostic | 46.68 / 34.71 | 91.62 / 88.60 / 3.00 | 80.84 / 4.92
    | 99.54 / 99.57 / 5.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task | 91.50 / 18.63 | 92.96 / 91.34 / 4.91 | 94.96 / 4.87 | 98.62
    / 98.79 / 5.11 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Varying head selection strategies between top top task-specific heads,
    union across multiple tasks, and intersection (the default used in PASTA).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PASTA | JSON Format | Prons. Changing | BiasBios | CounterFact |'
  prefs: []
  type: TYPE_TB
- en: '|  | F. Acc / P. Acc | Acc / A.Acc / Flue. | Acc / Flue. | ES / PS /Flue. |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA | Task-specific | 95.56 / 86.83 | 98.52 / 98.02 / 5.92 | 97.62 / 4.18
    | 99.18 / 99.24 / 4.93 |'
  prefs: []
  type: TYPE_TB
- en: '| union | 88.42 / 74.49 | 92.12 / 91.44 / 4.88 | 96.36 / 4.13 | 99.24 / 99.35
    / 4.53 |'
  prefs: []
  type: TYPE_TB
- en: '| intersection | 96.64 / 85.09 | 96.42 / 95.84 / 5.43 | 95.28 / 4.05 | 99.60
    / 99.57 / 4.89 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-J | Task-specific | 85.71 / 79.39 | 94.74 / 92.54 / 5.07 | 97.64 / 5.06
    | 99.26 / 99.34 / 4.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Union | 72.61 / 64.89 | 89.68 / 87.76 / 3.92 | 95.56 / 5.02 | 99.82 / 99.83
    / 5.03 |'
  prefs: []
  type: TYPE_TB
- en: '| Intersection | 91.50 / 18.63 | 92.96 / 91.34 / 4.91 | 94.96 / 4.87 | 98.62
    / 98.79 / 5.11 |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Extended results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 The variance of few-shot performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Few-shot prompting sometimes leads to improvements in model performance. as
    explicitly providing the examples in additional demonstrations. However, a drawback
    of few-shot prompting is its instability, meaning its performance exhibits high
    variance across different samples in the demonstratio. In this section, we present
    the results to show that the performance of few-shot prompting displays high variance
    in terms of sampling different few-shot demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: The few-shot performance (Acc. / A. Acc. / Fluency) on the Pronouns
    Changing task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Few-shot examples | LLAMA-7B | GPT-J-6B |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration 1 | 84.87 / 90.09 / 4.74 | 43.82 / 40.36 / 6.43 |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration 2 | 57.24 / 53.98 / 6.22 | 40.68 / 37.86 / 6.44 |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration 3 | 57.08 / 53.22 / 6.02 | 33.13 / 29.21 / 6.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration 4 | 52.26 / 48.30 / 6.42 | 25.47 / 20.89 / 6.44 |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration 5 | 43.86 / 40.78 / 6.43 | 11.90 / 8.63 / 6.51 |'
  prefs: []
  type: TYPE_TB
- en: C.2 Model Profiling Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this Section, we provide more results of the performance of LLAMA-7B on
    all of tasks when steering: (i) all heads; (ii) entire layer; (iii) a individual
    head of a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc9c704057f339022bfe2df81e9d7886.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The performance of LLAMA-7B on Pronouns Changing task when we steer
    (i) all heads (green); (ii) entrie layer (yellow); and (iii) individual head with
    a layer (blue violin plot). The performance varies dramatically across layers
    and across heads of a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fcc1394bbabe8980a2f493089a6b36c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The performance of LLAMA-7B on BiasBios task when we steer (i) all
    heads (green); (ii) entrie layer (yellow); and (iii) individual head with a layer
    (blue violin plot). The performance varies dramatically across layers and across
    heads of a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/163f8f98b8500b9789057cd50443ac79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The performance of LLAMA-7B on CounterFact task when we steer (i)
    all heads (green); (ii) entrie layer (yellow); and (iii) individual head with
    a layer (blue violin plot). The performance varies dramatically across layers
    and across heads of a layer.'
  prefs: []
  type: TYPE_NORMAL
