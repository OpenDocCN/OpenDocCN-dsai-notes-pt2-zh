- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:04'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'SparQ Attention: Bandwidth-Efficient LLM Inference'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.04985](https://ar5iv.labs.arxiv.org/html/2312.04985)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Luka Ribar       Ivan Chelombiev       Luke Hudlass-Galley       Charlie Blake
          Carlo Luschi       Douglas Orr
  prefs: []
  type: TYPE_NORMAL
- en: Graphcore Research
  prefs: []
  type: TYPE_NORMAL
- en: '{lukar, ivanc, lukehg, charlieb, carlo, douglaso}@graphcore.ai'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generative large language models (LLMs) have opened up numerous novel possibilities,
    but due to their significant computational requirements their ubiquitous use remains
    challenging. Some of the most useful applications require processing large numbers
    of samples at a time and using long contexts, both significantly increasing the
    memory communication load of the models. We introduce SparQ Attention, a technique
    for increasing the inference throughput of LLMs by reducing the memory bandwidth
    requirements within the attention blocks through selective fetching of the cached
    history. Our proposed technique can be applied directly to off-the-shelf LLMs
    during inference, without requiring any modification to the pre-training setup
    or additional fine-tuning. We show how SparQ Attention can decrease the attention
    memory bandwidth requirements *up to eight times* without any loss in accuracy
    by evaluating Llama $2$ and Pythia models on a wide range of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S0.F1.1.pic1" class="ltx_picture" height="669.57" overflow="visible"
    version="1.1" width="1189.35"><g transform="translate(0,669.57) matrix(1 0 0 -1
    0 0) translate(39.37,0) translate(0,629.92)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -27.62 19.13)" fill="#000000" stroke="#000000"><foreignobject
    width="15.86" height="10.8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{q}_{[{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 123.96 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-0.2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 206.54 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0.4$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.8 -61.36)" fill="#000000" stroke="#000000"><foreignobject
    width="26.23" height="14.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{K}_{[:,{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 410.05 18.05)" fill="#000000" stroke="#000000"><foreignobject
    width="6.67" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{q}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-0.2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 580.56 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0.4$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 400.27 -61.36)" fill="#000000" stroke="#000000"><foreignobject
    width="26.23" height="14.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{K}_{[{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}},:]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 772.18 -239.68)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\otimes}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 297.52 -539.42)" fill="#000000" stroke="#000000"><foreignobject
    width="54.57" height="23.83" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha=\displaystyle\sum_{\mathclap{i\in{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}}}}\hat{s}_{i}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 1037.93 -466.06)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\oplus}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 1063.83 -505.43)" fill="#000000" stroke="#000000"><foreignobject
    width="53.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times\,(1-\alpha)$</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: SparQ Attention for a single attention head. The algorithm consists
    of three steps. First, we find the $r$. This allows us to approximate the full
    attention scores ($\bm{\hat{s}}$ largest scores in the approximation and proceed
    to gather the corresponding full key and value vectors from the cache. As a final
    step, to compensate for the missing value vectors, we additionally maintain and
    fetch the running mean value vector and reassign it the leftover mass based on
    approximate score weightings. The attention output is then calculated as usual
    using the top-$k$ fetched key and value pairs, together with the averaged vector.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67e1294eb2b06085ae10cbd633606dab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Llama $2$-shot performance versus memory transfers over a range of
    compression ratios. SparQ Attention achieves matching performance, while transferring
    between $1/8$ as much data as the original dense model. The curves show the mean
    $\pm$ examples. This pattern is representative of the performance across multiple
    models and tasks, shown in [Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient
    LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer models trained on large corpora of text have recently shown remarkable
    capabilities in solving complex natural language processing tasks [Kaplan et al.,
    [2020](#bib.bib10), Brown et al., [2020](#bib.bib5)]. With the dramatic increase
    in model scale, LLMs became useful generalist tools that can be used for a multitude
    of text-based tasks due to in-context learning capabilities that emerge in LLMs
    at scale. These capabilities are unlocked by incorporating information and instructions
    through textual prompts [Wei et al., [2022](#bib.bib29)], which are absorbed during
    generation as part of the attention cache without modifying the model weights.
    Therefore, one of the key obstacles to efficient inference deployment of LLMs
    remains their high memory bandwidth requirement when processing long sequences
    [Pope et al., [2023](#bib.bib17)], i.e. long instruction prompts, lengthy chat
    histories or information retrieval from documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main bottleneck appears due to the auto-regressive nature of transformer
    inference: the output is generated token by token, and for each model call, the
    full previous state (i.e. the *key-value cache*, or KV cache) needs to be fetched
    from memory. The size of the KV cache scales linearly with the sequence length
    as well as the batch size, thus rendering inference using long sequence lengths
    increasingly memory-bandwidth limited.¹¹1Note that this problem is less severe
    during training, since teacher forcing allows whole sequences to be processed
    in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: However, tokens generally only attend to a small part of the sequence at a time
    [Vig, [2019](#bib.bib28), Yun et al., [2020](#bib.bib30)]; thus, if it were possible
    to efficiently predict the tokens that will have high attention scores, memory
    traffic could be significantly reduced by only transferring the key and value
    pairs of high-scoring tokens. Building upon this idea, we present SparQ Attention,
    a technique for significantly reducing the memory bandwidth requirements of transformer
    inference. By approximating attention scores using a subset of query/key vector
    components, we fetch only the most relevant tokens for each inference step, greatly
    decreasing memory traffic without affecting task performance.
  prefs: []
  type: TYPE_NORMAL
- en: In order to evaluate the effectiveness of our technique, we curate a selection
    of downstream tasks that aim to test the model’s abilities to effectively utilise
    the information within the provided textual context. This setup allows us to evaluate
    the trade-off between task performance and data transfer reduction, as well as
    compare different sparse attention techniques with respect to memory transfer
    efficiency. Thus, we clearly show how SparQ Attention performs favourably compared
    to state of the art and can lead up to $8\times$ compression with no loss in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Attention Memory Transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider a standard attention block and its associated memory requirements.
    A single head within an attention layer has a head dimension $d_{h}$. During token-by-token
    inference the output of an attention head is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{y}=\mathrm{softmax}\Bigl{(}\dfrac{\bm{q}\cdot\bm{K}^{\top}}{\sqrt{d_{h}}}\Bigr{)}\cdot\bm{V}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{q}$ and $\bm{V}\in\mathbb{R}^{S\times d_{h}}$ are the key and value
    caches respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each forward pass, we need to fetch the key and value matrices from memory,
    as well as write (append) $\bm{k}$ vectors for the current token, giving a total
    number of elements transferred per attention head:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{M}_{\mathrm{base}}=\underbrace{2\,S\,d_{h}}_{\text{Read $\bm{K}$
    and $\bm{V}$}}+\underbrace{2\,d_{h}}_{\text{Write current $\bm{k}$ and $\bm{v}$}}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Total memory transfers for inference also include model parameters, however
    these can be amortised over a large batch of sequences, unlike the $\bm{K}$ caches.
    Fetching these from memory dominates processing time for large $S$, thus creating
    a bottleneck to efficient transformer inference.
  prefs: []
  type: TYPE_NORMAL
- en: 3 SparQ Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 SparQ Attention Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: procedure SparseQAttention($\bm{q}\in\mathbb{R}^{d_{h}},\bm{K}\in\mathbb{R}^{S\times
    d_{h}},\bm{V}\in\mathbb{R}^{S\times d_{h}},\bm{\overline{v}}\in\mathbb{R}^{d_{h}},r\in\mathbb{N},k\in\mathbb{N},l\in\mathbb{N}$
    $\triangleright$ elements of $\bm{|q|}$ $\triangleright$ if $i></math> <math id=$
    positions     $\bm{i}_{2}\leftarrow\mathrm{argtopk}\left(\bm{\hat{s}}+\bm{m},k\right)$
    Indices of top $k$ $\triangleright$     $\bm{s}\leftarrow\mathrm{softmax}\left(\bm{q}\cdot\bm{K}_{[\bm{i}_{2},:]}^{\top}/\sqrt{d_{h}}\right)$
    Final attention scores (top $k$ $\triangleright$     return $\bm{y}$end procedure
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 1: SparQ Attention is, for the most part, a stateless replacement
    of the attention operation in a pre-trained transformer. The exception is $\overline{\bm{v}}=\mathrm{mean}(\bm{V})$
    matrix at every step. In our experiments, we set $r\in\left\{16,32,64\right\}$
    is the main determinant of the compression ratio), $k\in\left\{64,128\right\}$.
    PyTorch code is provided in [Appendix B](#A2 "Appendix B Code ‣ SparQ Attention:
    Bandwidth-Efficient LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate an accurate approximation of attention, without transferring
    the entire $\bm{K}$ matrices, we make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the $\mathrm{softmax}$. For example, $32$ scores often cover over
    $90$ rows of $\bm{V}$ corresponding to the largest attention scores could be fetched
    from memory with a minimal degradation in accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Observation 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The indices of the largest attention scores can be efficiently predicted without
    fetching the full $\bm{K}$, keeping the $r$ largest-magnitude components (see
    [Figure A3](#A4.F3 "In Appendix D Attention Sparsity Analysis ‣ SparQ Attention:
    Bandwidth-Efficient LLM Inference")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using these observations, we propose SparQ Attention (see [Figure 1](#S0.F1
    "In SparQ Attention: Bandwidth-Efficient LLM Inference") and [Algorithm 1](#alg1
    "In Figure 3 ‣ 3 SparQ Attention ‣ SparQ Attention: Bandwidth-Efficient LLM Inference"))
    consisting of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the indices of $r$ and only fetch $\bm{K}$ using the sliced query and keys.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Step 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the top-$k$ keys and values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Step 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Estimate the total score $\alpha$ positions using the approximate attention
    scores. Use this total score to interpolate between the attention output from
    the top-$k$.²²2Further justification and implementation details can be found in
    [Appendix C](#A3 "Appendix C SparQ Attention Procedure ‣ SparQ Attention: Bandwidth-Efficient
    LLM Inference").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The memory transfer of the SparQ Attention algorithm for a single attention
    head forward-pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{M}_{\mathrm{SparQ\ }}=\underbrace{S\,r\,\vphantom{d_{h}}}_{\text{Read
    $r$ columns of $\bm{K}$}}+\underbrace{2\,k\,d_{h}}_{\text{Read top-$k$ rows of
    $\bm{K}$ and $\bm{V}$}}+\underbrace{4\,d_{h}}_{\text{Write current $\bm{k}$ and
    $\bm{v}$, read/write $\overline{\bm{v}}$}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'By varying $r$ we can tune the total memory bandwidth requirement of the scheme,
    trading-off approximation accuracy for inference speed-up. Since typically $S\gg
    d_{h}$ is the most important parameter controlling the approximation and bandwidth
    compression. Typical compression ratios are given in [Table A2](#A6.T2 "In Appendix
    F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We evaluate our method on two widely-used open-source language model variants:
    Llama $2$ billion parameters. Both of these model families are decoder-only transformers
    [Radford et al., [2018](#bib.bib18)], pre-trained on causal language modelling.
    They share similar architectural components such as Multi-Head Attention and Rotary
    positional encoding [Su et al., [2021](#bib.bib24)], while also having some notable
    differences such as different layer normalisation implementations, activation
    functions and execution of modules in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: Tasks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In order to evaluate the technique on a spectrum of relevant NLP tasks, our
    evaluation setup tests the models on various tasks requiring information retrieval
    and reasoning from the provided context: question answering, summarisation, perplexity/bits-per-character
    (BPC), text repetition. For all tasks, we aimed to generate examples of sequence
    lengths between $1000$ tokens.³³3As we wanted to define the tasks independently
    of the selected models, our examples were chosen to have context lengths between
    $4000$ characters, roughly giving the desired lengths in tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: For question answering, we use the SQuAD [Rajpurkar et al., [2016](#bib.bib19)]
    and TriviaQA [Joshi et al., [2017](#bib.bib9)] datasets in the open-book setting.
    In order to construct the SQuAD examples, we augment the provided context (required
    to answer the question) with seven additional confusion contexts from unrelated
    questions. This ensures that the examples have the appropriate context length,
    as well as making the task harder as the model needs to distinguish the relevant
    information from the context from the unrelated paragraphs. We use SQuAD v$1.1$,
    since we aim to measure the model’s ability to extract useful information from
    the KV cache. For both question answering tasks we use exact string match accuracy
    as the evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: Summarisation is evaluated on the CNN/DailyMail dataset [See et al., [2017](#bib.bib21)]
    using the ROUGE-L F-score [Lin, [2004](#bib.bib13)] as the metric. We use the
    WikiText-$103$ dataset [Merity et al., [2016](#bib.bib16)] with bits per character
    (BPC)⁴⁴4We quote performance for sub-word language modelling in BPC, to account
    for any differences in vocabulary across models. for evaluating language modelling
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we construct an artificial “Text Repetition” task to evaluate the capability
    of the model to repeat sentences from its context verbatim. Such a task can commonly
    appear in a dialogue setting where the LLM agent is required to retrieve a piece
    of text from a possibly long context provided, and can be challenging for sparse
    attention techniques. We construct examples using the Tiny-Shakespeare dataset
    [Karpathy, [2015](#bib.bib11)] by chunking the text into contexts of the appropriate
    size, appending them with the prompts containing a subset of the context, and
    evaluating the output exact character length match with the continuation from
    the context.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We consider the cache eviction technique H[2]O [Zhang et al., [2023](#bib.bib32)]
    and local windowing with the initial-tokens enhancement proposed by Han et al.
    [[2023](#bib.bib8)] as baselines. For consistency, we allocate a fixed KV cache
    transfer budget $k$ (with $3k/4$ positions (with $k-16$ local positions).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Dataset Name | SQuAD $\uparrow$ | CNN/DM $\uparrow$ | Repetition $\uparrow$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Compression | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8
    | 1 | 1/2 | 1/8 |'
  prefs: []
  type: TYPE_TB
- en: '| Pythia $6.9$B | Local | 57.8 | 38.5 | 14.5 | 52.6 | 41.6 | 23.5 | 20.2 |
    14.9 | 13.6 | 0.68 | 0.71 | 0.83 | 150 | 64 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| H[2]O | 52.9 | 45.5 | 52.6 | 52.3 | 20.2 | 17.9 | 0.69 | 0.73 | 47 | 17 |'
  prefs: []
  type: TYPE_TB
- en: '| SparQ | 58.0 | 57.9 | 52.4 | 51.2 | 20.6 | 20.4 | 0.68 | 0.69 | 151 | 133
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama$\,2$B | Local | 76.9 | 46.1 | 25.5 | 77.6 | 70.1 | 65.2 | 21.4 | 16.5
    | 14.1 | 0.77 | 0.81 | 0.91 | 207 | 66 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| H[2]O | 66.2 | 47.1 | 77.2 | 71.3 | 21.8 | 7.4 | 0.77 | 0.88 | 41 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| SparQ | 76.4 | 70.2 | 77.3 | 75.3 | 21.7 | 20.5 | 0.77 | 0.79 | 191 | 188
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results from the largest models tested are presented above. SQuAD
    and TriviaQA measure performance in accuracy. CNN/DailyMail uses ROUGE-L score.
    Repetition counts the number of characters before the generated sequence diverges
    and WikiText task measures perplexity in bits per character (BPC). Values in bold
    represent the best score for a particular model, task and sparsity setting. Standard
    errors are: SQuAD $0.8$, CNN/DailyMail $0.4$, Repetition $3$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our experiments span four distinct models: Llama $2$ billion parameters and
    three Pythia models with $1.4$ and $6.9$ billion parameters. Results from the
    largest models are presented in [Table 1](#S4.T1 "In 4.2 Results ‣ 4 Experiments
    ‣ SparQ Attention: Bandwidth-Efficient LLM Inference"), with further results in
    [Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient LLM Inference"). We
    observe:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparQ Attention performance is robust across all tasks and model sizes tested.
    Compression ratios of $2\times$ are readily achievable with little to no loss
    in task performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simple recipe of using $k=64$, choosing $r$ performs much worse than $k=128$
    for many models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain tasks are more challenging for H[2]O (Repetition, SQuAD), while others
    are more forgiving (TriviaQA, WikiText-$103$ LM).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local attention degrades performance across all tasks, demonstrating that the
    tasks do not permit the trivial solution of discarding the long context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Efficient attention methods have been a very active area of research [Tay et al.,
    [2020b](#bib.bib26)]. Schemes such as *Sparse Transformers* [Child et al., [2019](#bib.bib7)],
    *Combiner* [Ren et al., [2021](#bib.bib20)], *Longformer* [Beltagy et al., [2020](#bib.bib3)],
    *BigBird* [Zaheer et al., [2020](#bib.bib31)], *Reformer* [Kitaev et al., [2020](#bib.bib12)]
    and *Sparse Sinkhorn Attention* [Tay et al., [2020a](#bib.bib25)] have been developed
    to increase efficiency of the attention mechanism by extracting information from
    the most salient tokens in the sequence or approximating the dense attention maps.
    These methods form part of the architecture, they must be implemented at pre-training
    stage and carry varying task performance trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of work focuses on reducing the memory footprint requirements of
    the attention mechanism, while maintaining quadratic complexity. These methods
    include popular schemes such as *Multi-Query Attention* (MQA) [Shazeer, [2019](#bib.bib22)]
    and *Grouped-Query Attention* (GQA) [Ainslie et al., [2023](#bib.bib1)] that share
    a single key and value head across multiple query heads. This leads to a reduction
    in memory footprint and bandwidth. While these schemes boost efficiency, they
    must be implemented at pre-training stage and may affect model quality and stability.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, an emerging area of research similar to SparQ Attention aims
    to only adapt the inference procedure of a frozen pre-trained model. The simplest
    method of this category is used as part of *FlexGen* [Sheng et al., [2023](#bib.bib23)]
    called *top-$k$ scores. This process uses the full key cache to produce attention
    scores, thus limiting the asymptotic reduction of the memory transfers to only
    $50\%$.
  prefs: []
  type: TYPE_NORMAL
- en: '*Eviction* schemes describe a number of methods that only cache a subset of
    keys and values, and continually delete tokens that are uninformative for future
    outputs. By reducing the cache size itself, both memory and memory bandwidth overheads
    are reduced. *H[2]O* [Zhang et al., [2023](#bib.bib32)] and *Scissorhands* [Liu
    et al., [2023b](#bib.bib15)] are examples of such eviction methods. H[2]O uses
    a greedy eviction policy that maintains the most salient “Heavy Hitter” tokens
    that contribute most to the attention scores on every new generation step. Similarly,
    Scissorhands identifies and maintains “pivotal tokens” by counting how many times
    a token’s attention score exceeds an importance threshold over the generative
    process. While these methods reduce the memory footprint of the attention cache
    as well as bandwidth, they also lead to permanent loss of information from the
    context window, which can lead to mistakes for queries seeking less attended parts
    of the context.'
  prefs: []
  type: TYPE_NORMAL
- en: '*IceFormer* [Anonymous, [2023](#bib.bib2)] evaluates a multitude of existing
    approximate nearest neighbour algorithms for efficiently approximating attention
    scores of pre-trained models, similarly to SparQ Attention. Yet, this work focuses
    solely on serving models using CPU due to the implementation of approximate nearest
    neighbour, while our scheme is hardware-agnostic and can be used with any accelerated
    computing platform. *Scatterbrain* [Chen et al., [2021](#bib.bib6)] employs similar
    techniques, but for computer vision applications.'
  prefs: []
  type: TYPE_NORMAL
- en: An orthogonal line of work saves memory traffic by compressing the KV cache
    with $4$-bit compression can be complementary to techniques that reduce the number
    of transferred elements.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work we have presented SparQ Attention, a novel technique for unlocking
    faster inference for pre-trained LLMs. Our proposed technique modifies the multi-head
    attention mechanism to access only the relevant tokens from the KV cache on every
    inference step, leading to drastic memory transfer savings. This is a significant
    improvement as it allows for pre-trained models to be executed more efficiently,
    without any fine-tuning or modifications to the weights of the model.
  prefs: []
  type: TYPE_NORMAL
- en: LLM inference speed is often bottlenecked by memory transfers rather than computation,
    especially in regimes of long sequence lengths and limited memory bandwidth. Traffic
    of the past KV values can dominate the memory bandwidth when using large batch
    sizes and sequence lengths longer than the hidden state of the model, in which
    case the KV cache reaches a higher memory footprint than model weights. This mode
    of execution is particularly suited to the SparQ technique, with up to eightfold
    decrease in memory transfer of the KV cache with little to no accuracy degradation.
    Our method significantly improves on existing methods and does not lead to any
    loss of information from the context window, as tokens are maintained in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'SparQ Attention has some limitations: while maintaining very strong performance
    at high bandwidth compression ratios, this is achieved by keeping all cached values
    in memory which is sparsely accessed during the generation steps. It therefore
    does not save any memory capacity, only bandwidth. Another possible limitation
    is the unclear efficiency saving of SparQ Attention when used with transformer
    models using MQA and GQA, which were not evaluated in this work. We leave it as
    future work to extend SparQ Attention to cover more attention mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We would like to thank Daniel Justus, Paul Balança and Andrew Fitzgibbon for
    their helpful input and feedback on this work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ainslie et al. [2023] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury
    Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. GQA: Training generalized multi-query
    transformer models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anonymous [2023] Anonymous. Iceformer: Accelerated inference with long-sequence
    transformers on CPUs. In *Submitted to The Twelfth International Conference on
    Learning Representations*, 2023. URL [https://openreview.net/forum?id=6RR3wU4mSZ](https://openreview.net/forum?id=6RR3wU4mSZ).
    under review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. [2020] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Gregory
    Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu
    Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing
    large language models across training and scaling. In *International Conference
    on Machine Learning*, pages 2397–2430\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in Neural Information
    Processing Systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra,
    and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. *Advances
    in Neural Information Processing Systems*, 34:17413–17426, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2023] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. LM-infinite: Simple on-the-fly length generalization for large language
    models. *arXiv preprint arXiv:2308.16137*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.
    *arXiv preprint arXiv:1705.03551*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpathy [2015] Andrej Karpathy. The unreasonable effectiveness of recurrent
    neural networks. [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn),
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. [2020] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin [2004] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries.
    In *Text Summarization Branches Out*, pages 74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023a] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023b] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for llm kv cache compression
    at test time. *arXiv preprint arXiv:2305.17118*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pope et al. [2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. *Proceedings of Machine Learning and Systems*,
    5, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. Improving language understanding by generative pre-training, 2018.
    URL [https://openai.com/research/language-unsupervised](https://openai.com/research/language-unsupervised).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. *arXiv
    preprint arXiv:1606.05250*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2021] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec,
    Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse
    computation cost. *Advances in Neural Information Processing Systems*, 34:22470–22482,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See et al. [2017] Abigail See, Peter J Liu, and Christopher D Manning. Get
    to the point: Summarization with pointer-generator networks. *arXiv preprint arXiv:1704.04368*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shazeer [2019] Noam Shazeer. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sheng et al. [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen:
    high-throughput generative inference of large language models with a single GPU.
    In *International Conference on Machine Learning*, pages 31094–31116\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. [2021] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.
    Roformer: Enhanced transformer with rotary position embedding. *CoRR*, abs/2104.09864,
    2021. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tay et al. [2020a] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng
    Juan. Sparse sinkhorn attention. In *International Conference on Machine Learning*,
    pages 9438–9447\. PMLR, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. [2020b] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
    Efficient transformers: A survey. *CoRR*, abs/2009.06732, 2020b. URL [https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vig [2019] Jesse Vig. A multiscale visualization of attention in the transformer
    model. In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, pages 37–42, 01 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. In *International Conference on Learning Representations*,
    2022. URL [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yun et al. [2020] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh
    Rawat, Sashank Reddi, and Sanjiv Kumar. $o(n)$ connections are expressive enough:
    Universal approximability of sparse transformers. *Advances in Neural Information
    Processing Systems*, 33:13783–13794, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. [2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. Big bird: Transformers for longer sequences. *Advances in Neural
    Information Processing Systems*, 33:17283–17297, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al.
    H[2]O: Heavy-hitter oracle for efficient generative inference of large language
    models. *arXiv preprint arXiv:2306.14048*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5404e2e2cc377387be0e4016d86fc59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A1: Compression versus performance trade-off curves over all models
    and tasks evaluated. The y-axis minimum is set to ($0.5$, $0.5$, $0.0$ the dense
    baseline for the tasks, reading top-to-bottom, in order to give a consistent view
    of the performance loss across models. Vertical dotted lines show ($1/4$) compression
    versus dense. Shaded lines show $\pm 1$ standard error of the mean.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Detailed Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient LLM Inference")
    reports the compression/performance trade-off curves for all models and tasks
    that were evaluated.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/64b7e7b65e67b963171fdb3e51459aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Appendix C SparQ Attention Procedure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SparQ Attention procedure consists of three steps. In the first step we
    find the indices $\bm{i}_{1}\in\mathbb{N}^{r}$ components of the query vector
    $\bm{q}\in\mathbb{R}^{d_{h}}$ across the sequence as follows, where $S\in\mathbb{N}$
    is the sequence length:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{a}_{i}=\sum_{j\in\bm{i}_{1}}q_{j}K_{i,j}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\bm{\hat{s}}=\sigma\Bigl{(}\dfrac{\bm{\hat{a}}}{\tau}\Bigr{)}$
    |  | (A1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma(\bm{x})_{i}=e^{x_{i}}/\sum_{j}(e^{x_{j}})$ function and $\bm{K}\in\mathbb{R}^{S\times
    d_{h}}$ is chosen to be $\sqrt{d_{h}}$ over which the dot product summation is
    performed. However, since the dot product is approximated using only $r$ components
    here, the scaling factor needs to be appropriately changed. Empirically we found
    the appropriate factor to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tau=\sqrt{d_{h}\dfrac{\sum_{i\in\bm{i}_{1}}&#124;q_{i}&#124;}{\sum_{i}&#124;q_{i}&#124;}}$
    |  | (A2) |'
  prefs: []
  type: TYPE_TB
- en: which takes into account the relative L1-norm of the selected top $r$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second step, we proceed to find indices $\bm{i}_{2}\in\mathbb{N}^{k}$
    components of the approximate score vector $\bm{\hat{s}}$ positions, which we
    can express by defining a corresponding boolean mask $\bm{b}\in\{0,1\}^{S}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle b_{i}=\begin{cases}1&amp;\text{if }i\in\bm{i}_{2}\\ 0&amp;\text{else}\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\bm{s}=\sigma\Bigl{(}\dfrac{(\bm{q}\cdot\bm{K}^{\top})\circ\bm{b}}{\sqrt{d_{h}}}\Bigr{)}$
    |  | (A3) |'
  prefs: []
  type: TYPE_TB
- en: 'In the third step, we perform a weighted sum of the value vectors in $V\in\mathbb{R}^{S\times
    d_{h}}$. In order to improve the accuracy of the final approximation, we add an
    additional weighted contribution of the mean value vector $\bm{\bar{v}}=\frac{1}{S}\sum_{i}\bm{V}_{i,*}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{y}=\alpha\sum_{i\in\bm{i}_{2}}s_{i}\bm{V}_{i,*}+(1-\alpha)\bm{\bar{v}}$
    |  | (A4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha\in[0,1]$ terms. We choose $\alpha$ from the first step as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha=\sum_{i\in\bm{i}_{2}}\hat{s}_{i}$ |  | (A5) |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Attention Sparsity Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to understand how to approximate the attention calculation of [Equation 1](#S2.E1
    "In 2 Attention Memory Transfer ‣ SparQ Attention: Bandwidth-Efficient LLM Inference"),
    we analysed the intermediate scores vector (softmax output). We took $20$b model,
    capturing the $\bm{q}$ matrix from every layer and attention head.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test if the scores are sufficiently sparse to facilitate dropping positions
    within $\bm{V}$ scores, termed coverage, in [Figure A2](#A4.F2 "In Appendix D
    Attention Sparsity Analysis ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").
    If the attention scores were flat, we would expect coverage $\approx k/S\approx
    0.03$, implying a very sparse distribution. We note from [Figure A2b](#A4.F2.sf2
    "In Figure A2 ‣ Appendix D Attention Sparsity Analysis ‣ SparQ Attention: Bandwidth-Efficient
    LLM Inference") that coverage does vary per-head and per-layer, for example earlier
    layers have “flatter” heads, but even these heads show coverage <math id="A4.p2.5.m5.1"
    class="ltx_Math" alttext="></math>.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d8aa94c7736ec94b3035ddd125506c96.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Coverage over (example, layer, head)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54f4c3b9b0c2d70d20fc8e2d6a86cfe1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Coverage by (layer, head)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A2: Coverage of the softmax output for the top $32$ (varying across
    examples). Calculated for the first token generated in answer to a SQuAD prompt,
    using Pythia-$1.4$ examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4be75b79bc93a1585d3a33f1f01fed2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention approximation top-$k$ overlap, over (batch, layer, head)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca229b1727d8e8f7ec2d5666feb1aa40.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention approximation top-$k$ overlap, by (layer, head)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A3: Proportion of the top-$k$ absolute components of $\bm{q}$ components.
    Note $d_{h}=128$. Calculated for the first token generated to answer a SQuAD prompt,
    using Pythia-$1.4$ examples.'
  prefs: []
  type: TYPE_NORMAL
- en: It is useful to drop positions in $\bm{V}$ is needed to calculate these scores.
    We propose approximating these scores using a subset of the components of $\bm{K}$
    positions in the approximated and true scores. If overlap is high, we can use
    the approximation to avoid downloading the whole $\bm{K}$ for all positions, then
    all components of $\bm{K}$ for some positions.
  prefs: []
  type: TYPE_NORMAL
- en: Our hypothesis is that the $r$ are most useful to predicting the score, $\bm{q}\bm{K}^{\top}$,
    but that some later layers are harder to predict. Using the top-$r$ baseline considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Practical Implementation Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One limitation of our theoretical model of memory transfers is that it does
    not account for the granularity of memory access. Since the $\bm{K}$ twice, once
    in $S$-major format. This increases KV cache memory usage by $50\%$ twice. This
    extra write is non-contiguous, but small, so should not form a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We provide a comprehensive set of hyperparameters for reference in [Table A1](#A6.T1
    "In Appendix F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").
    Typical compression ratios for settings of $(r,k)$ are given in [Table A2](#A6.T2
    "In Appendix F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: We use our own implementation of H[2]O [Zhang et al., [2023](#bib.bib32)], which
    differs from the author’s implementation in that it uses a fixed cache size $k$-shot,
    with Pythia-$1.4$, $l=64$ of $200$ (the dense baseline achieved $74$ times that
    either output differed from dense, $41$-character prefix match between our implementation
    and theirs. The fact that the two implementations often generate the same errors
    (despite minor implementation differences) reassures us that our results should
    be a fair representation of $\mathrm{H_{2}O}$.
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Hyperparameter | Value or range |'
  prefs: []
  type: TYPE_TB
- en: '| Dense model | Family | Llama $2$B), Pythia ($6.9$B, $1.4$B) |'
  prefs: []
  type: TYPE_TB
- en: '| $d_{h}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Max $S$ |'
  prefs: []
  type: TYPE_TB
- en: '| Tasks | QA (#samples) | SQuAD $1$) TriviaQA $0$) |'
  prefs: []
  type: TYPE_TB
- en: '| Summarisation (#samples) | CNN/DailyMail $0$) |'
  prefs: []
  type: TYPE_TB
- en: '| Language Modelling (#samples) | WikiText-$103$) |'
  prefs: []
  type: TYPE_TB
- en: '| Artificial (#samples) | Repetition ($1000$) |'
  prefs: []
  type: TYPE_TB
- en: '| Baselines | Eviction | keep $(k-l)$ and the most recent $l=k/4$'
  prefs: []
  type: TYPE_NORMAL
- en: $k\in\{128,192,256,384,512,768\}$ |
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | take the first $16$ $k\in\{128,192,256,384,512,768\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| SparQ | Rank $r$ |'
  prefs: []
  type: TYPE_TB
- en: '| Number of values $k$ |'
  prefs: []
  type: TYPE_TB
- en: '| Local window $l$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table A1: Experiment hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | $r$ | Compression |'
  prefs: []
  type: TYPE_TB
- en: '| SparQ Attention | 16 | 64 | 0.10-0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | 0.13-0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64 | 0.16-0.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | 0.19-0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | 64 | 0.28-0.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | 0.31-0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| H[2]O | – | 128 | 0.07-0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.10-0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 256 | 0.13-0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 384 | 0.20-0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 512 | 0.26-0.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 768 | 0.39-0.65 |'
  prefs: []
  type: TYPE_TB
- en: 'Table A2: Range of compression ratios for different settings of $(r,k)$, for
    Llama 2 7B and Pythia 6.9B. The compression ratio achieved varies across models
    and tasks, based on the sequence length and head size.'
  prefs: []
  type: TYPE_NORMAL
- en: F.1 Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We illustrate the task setup with a single example per task, showing the prompt
    formatting and a cherry-picked example. In each case, we show outputs from a dense
    Llama 2 model, SparQ ($r=16,k=64$) Attention. Where “…” appears, we have truncated
    the line of text for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: F.1.1 Question Answering (SQuAD 1-shot)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: F.1.2 Question Answering (TriviaQA 0-shot)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that for Pythia, the prompt “Single-word answer:” was used in place of
    “Answer:”, as this helped prevent the model from restating the question in the
    answer (often qualitatively correct, but not a regex match).
  prefs: []
  type: TYPE_NORMAL
- en: F.1.3 Summarisation (CNN/DailyMail)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: F.1.4 Repetition (Shakespeare)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: F.1.5 Language Modelling (WikiText-103)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since the flood a new , roughly 100 @-@ foot @-@ tall ( 30 m ) bridge has been
    constructed on CR 510 , ensuring that any future flooding on the Dead River would
    not necessitate closure of the modern bridge . This structure was built at that
    height in order to benefit commercial interests in the area , as well as to provide
    the area with reliable public and emergency access . Rio Tinto Group , the British
    @-@ based parent company of Kennecott Minerals received permission
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
