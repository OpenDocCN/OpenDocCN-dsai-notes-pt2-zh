- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:04'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SparQ Attention: Bandwidth-Efficient LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SparQ Attention: Bandwidth-Efficient LLM Inference'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.04985](https://ar5iv.labs.arxiv.org/html/2312.04985)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.04985](https://ar5iv.labs.arxiv.org/html/2312.04985)
- en: Luka Ribar       Ivan Chelombiev       Luke Hudlass-Galley       Charlie Blake
          Carlo Luschi       Douglas Orr
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Luka Ribar       Ivan Chelombiev       Luke Hudlass-Galley       Charlie Blake
          Carlo Luschi       Douglas Orr
- en: Graphcore Research
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Graphcore 研究
- en: '{lukar, ivanc, lukehg, charlieb, carlo, douglaso}@graphcore.ai'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{lukar, ivanc, lukehg, charlieb, carlo, douglaso}@graphcore.ai'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Generative large language models (LLMs) have opened up numerous novel possibilities,
    but due to their significant computational requirements their ubiquitous use remains
    challenging. Some of the most useful applications require processing large numbers
    of samples at a time and using long contexts, both significantly increasing the
    memory communication load of the models. We introduce SparQ Attention, a technique
    for increasing the inference throughput of LLMs by reducing the memory bandwidth
    requirements within the attention blocks through selective fetching of the cached
    history. Our proposed technique can be applied directly to off-the-shelf LLMs
    during inference, without requiring any modification to the pre-training setup
    or additional fine-tuning. We show how SparQ Attention can decrease the attention
    memory bandwidth requirements *up to eight times* without any loss in accuracy
    by evaluating Llama $2$ and Pythia models on a wide range of downstream tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生成大型语言模型（LLMs）开辟了许多新颖的可能性，但由于其显著的计算需求，其普遍使用仍然具有挑战性。一些最有用的应用需要同时处理大量样本和使用长上下文，这两者都会显著增加模型的内存通信负载。我们介绍了
    SparQ Attention，这是一种通过选择性地提取缓存历史来减少注意力块内存带宽需求的技术，从而提高 LLM 的推理吞吐量。我们提出的技术可以直接应用于现成的
    LLM 推理中，而无需对预训练设置进行任何修改或额外的微调。我们通过在各种下游任务上评估 Llama $2$ 和 Pythia 模型，展示了 SparQ Attention
    如何在不损失准确性的情况下将注意力内存带宽需求 *降低至八倍*。
- en: <svg id="S0.F1.1.pic1" class="ltx_picture" height="669.57" overflow="visible"
    version="1.1" width="1189.35"><g transform="translate(0,669.57) matrix(1 0 0 -1
    0 0) translate(39.37,0) translate(0,629.92)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -27.62 19.13)" fill="#000000" stroke="#000000"><foreignobject
    width="15.86" height="10.8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{q}_{[{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 123.96 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-0.2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 206.54 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0.4$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.8 -61.36)" fill="#000000" stroke="#000000"><foreignobject
    width="26.23" height="14.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{K}_{[:,{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 410.05 18.05)" fill="#000000" stroke="#000000"><foreignobject
    width="6.67" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{q}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-0.2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 580.56 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0.4$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 400.27 -61.36)" fill="#000000" stroke="#000000"><foreignobject
    width="26.23" height="14.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{K}_{[{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}},:]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 772.18 -239.68)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\otimes}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 297.52 -539.42)" fill="#000000" stroke="#000000"><foreignobject
    width="54.57" height="23.83" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha=\displaystyle\sum_{\mathclap{i\in{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}}}}\hat{s}_{i}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 1037.93 -466.06)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\oplus}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 1063.83 -505.43)" fill="#000000" stroke="#000000"><foreignobject
    width="53.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times\,(1-\alpha)$</foreignobject></g></g></svg>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="S0.F1.1.pic1" class="ltx_picture" height="669.57" overflow="visible"
    version="1.1" width="1189.35"><g transform="translate(0,669.57) matrix(1 0 0 -1
    0 0) translate(39.37,0) translate(0,629.92)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -27.62 19.13)" fill="#000000" stroke="#000000"><foreignobject
    width="15.86" height="10.8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{q}_{[{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 123.96 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-0.2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 206.54 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0.4$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.8 -61.36)" fill="#000000" stroke="#000000"><foreignobject
    width="26.23" height="14.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{K}_{[:,{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 410.05 18.05)" fill="#000000" stroke="#000000"><foreignobject
    width="6.67" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{q}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-0.2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 580.56 15.23)" fill="#000000" stroke="#000000"><foreignobject
    width="19.99" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0.4$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 400.27 -61.36)" fill="#000000" stroke="#000000"><foreignobject
    width="26.23" height="14.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{K}_{[{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}},:]}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 772.18 -239.68)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\otimes}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 297.52 -539.42)" fill="#000000" stroke="#000000"><foreignobject
    width="54.57" height="23.83" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\alpha=\displaystyle\sum_{\mathclap{i\in{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}}}}\hat{s}_{i}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 1037.93 -466.06)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\oplus}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 1063.83 -505.43)" fill="#000000" stroke="#000000"><foreignobject
    width="53.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times\,(1-\alpha)$</foreignobject></g></g></svg>
- en: 'Figure 1: SparQ Attention for a single attention head. The algorithm consists
    of three steps. First, we find the $r$. This allows us to approximate the full
    attention scores ($\bm{\hat{s}}$ largest scores in the approximation and proceed
    to gather the corresponding full key and value vectors from the cache. As a final
    step, to compensate for the missing value vectors, we additionally maintain and
    fetch the running mean value vector and reassign it the leftover mass based on
    approximate score weightings. The attention output is then calculated as usual
    using the top-$k$ fetched key and value pairs, together with the averaged vector.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：单个注意力头的 SparQ Attention。该算法包括三个步骤。首先，我们找到 $r$。这使我们能够近似全注意力分数（在近似中最大的 $\bm{\hat{s}}$
    分数），并从缓存中获取相应的全键和值向量。最后一步，为了补偿缺失的值向量，我们还维护并提取运行均值向量，并根据近似分数权重重新分配剩余的质量。然后，使用获取的
    top-$k$ 键值对以及平均向量，按常规计算注意力输出。
- en: '![Refer to caption](img/67e1294eb2b06085ae10cbd633606dab.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/67e1294eb2b06085ae10cbd633606dab.png)'
- en: 'Figure 2: Llama $2$-shot performance versus memory transfers over a range of
    compression ratios. SparQ Attention achieves matching performance, while transferring
    between $1/8$ as much data as the original dense model. The curves show the mean
    $\pm$ examples. This pattern is representative of the performance across multiple
    models and tasks, shown in [Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient
    LLM Inference").'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：Llama $2$-shot 性能与不同压缩比下的内存传输。SparQ Attention 实现了匹配的性能，同时传输的数据量仅为原始稠密模型的
    $1/8$。曲线显示了均值 $\pm$ 示例。这种模式代表了多个模型和任务的性能，见 [图 A1](#A0.F1 "In SparQ Attention:
    Bandwidth-Efficient LLM Inference")。'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformer models trained on large corpora of text have recently shown remarkable
    capabilities in solving complex natural language processing tasks [Kaplan et al.,
    [2020](#bib.bib10), Brown et al., [2020](#bib.bib5)]. With the dramatic increase
    in model scale, LLMs became useful generalist tools that can be used for a multitude
    of text-based tasks due to in-context learning capabilities that emerge in LLMs
    at scale. These capabilities are unlocked by incorporating information and instructions
    through textual prompts [Wei et al., [2022](#bib.bib29)], which are absorbed during
    generation as part of the attention cache without modifying the model weights.
    Therefore, one of the key obstacles to efficient inference deployment of LLMs
    remains their high memory bandwidth requirement when processing long sequences
    [Pope et al., [2023](#bib.bib17)], i.e. long instruction prompts, lengthy chat
    histories or information retrieval from documents.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量文本语料上训练的变换器模型最近在解决复杂的自然语言处理任务方面展示了显著的能力 [Kaplan et al., [2020](#bib.bib10),
    Brown et al., [2020](#bib.bib5)]。随着模型规模的戏剧性增长，LLMs 变成了有用的通用工具，可以用于多种基于文本的任务，这得益于在大规模
    LLM 中出现的上下文学习能力。这些能力通过通过文本提示 [Wei et al., [2022](#bib.bib29)] 融入信息和指令来解锁，这些信息在生成过程中作为注意力缓存的一部分被吸收，而不修改模型权重。因此，实现
    LLM 高效推理部署的关键障碍之一仍然是处理长序列时其高内存带宽需求 [Pope et al., [2023](#bib.bib17)]，即长指令提示、冗长的聊天记录或从文档中检索信息。
- en: 'The main bottleneck appears due to the auto-regressive nature of transformer
    inference: the output is generated token by token, and for each model call, the
    full previous state (i.e. the *key-value cache*, or KV cache) needs to be fetched
    from memory. The size of the KV cache scales linearly with the sequence length
    as well as the batch size, thus rendering inference using long sequence lengths
    increasingly memory-bandwidth limited.¹¹1Note that this problem is less severe
    during training, since teacher forcing allows whole sequences to be processed
    in parallel.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主要瓶颈出现在变换器推理的自回归特性上：输出是逐个生成的，对于每次模型调用，需要从内存中获取完整的先前状态（即*键值缓存*或KV缓存）。KV缓存的大小随着序列长度和批量大小线性增长，因此使用长序列长度的推理越来越受限于内存带宽¹¹1请注意，这个问题在训练期间不那么严重，因为教师强制可以并行处理整个序列。
- en: However, tokens generally only attend to a small part of the sequence at a time
    [Vig, [2019](#bib.bib28), Yun et al., [2020](#bib.bib30)]; thus, if it were possible
    to efficiently predict the tokens that will have high attention scores, memory
    traffic could be significantly reduced by only transferring the key and value
    pairs of high-scoring tokens. Building upon this idea, we present SparQ Attention,
    a technique for significantly reducing the memory bandwidth requirements of transformer
    inference. By approximating attention scores using a subset of query/key vector
    components, we fetch only the most relevant tokens for each inference step, greatly
    decreasing memory traffic without affecting task performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，tokens 通常一次只关注序列的一小部分 [Vig, [2019](#bib.bib28), Yun et al., [2020](#bib.bib30)]；因此，如果能够高效预测将具有高注意力分数的tokens，通过仅传输高分数tokens的键值对，可以显著减少内存流量。基于这个想法，我们提出了
    SparQ Attention 一种显著减少变换器推理内存带宽需求的技术。通过使用查询/键向量组件的子集来近似注意力分数，我们仅获取每次推理步骤中最相关的
    tokens，从而大大减少内存流量，而不影响任务性能。
- en: In order to evaluate the effectiveness of our technique, we curate a selection
    of downstream tasks that aim to test the model’s abilities to effectively utilise
    the information within the provided textual context. This setup allows us to evaluate
    the trade-off between task performance and data transfer reduction, as well as
    compare different sparse attention techniques with respect to memory transfer
    efficiency. Thus, we clearly show how SparQ Attention performs favourably compared
    to state of the art and can lead up to $8\times$ compression with no loss in accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们技术的有效性，我们策划了一系列下游任务，旨在测试模型有效利用所提供文本上下文中的信息的能力。这种设置使我们能够评估任务性能与数据传输减少之间的权衡，以及比较不同稀疏注意力技术在内存传输效率方面的表现。因此，我们清晰地展示了
    SparQ Attention 相比于最先进技术的优越表现，并且在不损失准确性的情况下能够实现高达 $8\times$ 的压缩。
- en: 2 Attention Memory Transfer
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力内存转移
- en: 'Consider a standard attention block and its associated memory requirements.
    A single head within an attention layer has a head dimension $d_{h}$. During token-by-token
    inference the output of an attention head is calculated as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个标准的注意力块及其相关的内存需求。一个注意力层中的单个头有一个头维度 $d_{h}$。在逐个令牌推理期间，注意力头的输出计算如下：
- en: '|  | $\bm{y}=\mathrm{softmax}\Bigl{(}\dfrac{\bm{q}\cdot\bm{K}^{\top}}{\sqrt{d_{h}}}\Bigr{)}\cdot\bm{V}$
    |  | (1) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y}=\mathrm{softmax}\Bigl{(}\dfrac{\bm{q}\cdot\bm{K}^{\top}}{\sqrt{d_{h}}}\Bigr{)}\cdot\bm{V}$
    |  | (1) |'
- en: where $\bm{q}$ and $\bm{V}\in\mathbb{R}^{S\times d_{h}}$ are the key and value
    caches respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{q}$ 和 $\bm{V}\in\mathbb{R}^{S\times d_{h}}$ 分别是键和值缓存。
- en: 'For each forward pass, we need to fetch the key and value matrices from memory,
    as well as write (append) $\bm{k}$ vectors for the current token, giving a total
    number of elements transferred per attention head:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次前向传递，我们需要从内存中提取键和值矩阵，并为当前令牌写入（追加）$\bm{k}$ 向量，计算每个注意力头传输的总元素数量：
- en: '|  | $\mathcal{M}_{\mathrm{base}}=\underbrace{2\,S\,d_{h}}_{\text{Read $\bm{K}$
    and $\bm{V}$}}+\underbrace{2\,d_{h}}_{\text{Write current $\bm{k}$ and $\bm{v}$}}$
    |  | (2) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}_{\mathrm{base}}=\underbrace{2\,S\,d_{h}}_{\text{读取 $\bm{K}$
    和 $\bm{V}$}}+\underbrace{2\,d_{h}}_{\text{写入当前 $\bm{k}$ 和 $\bm{v}$}}$ |  | (2)
    |'
- en: Total memory transfers for inference also include model parameters, however
    these can be amortised over a large batch of sequences, unlike the $\bm{K}$ caches.
    Fetching these from memory dominates processing time for large $S$, thus creating
    a bottleneck to efficient transformer inference.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 推理的总内存传输还包括模型参数，但这些参数可以在大批量序列中分摊，这与 $\bm{K}$ 缓存不同。从内存中提取这些数据在较大的 $S$ 下主导了处理时间，从而成为高效变换器推理的瓶颈。
- en: 3 SparQ Attention
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 SparQ 注意力
- en: Algorithm 1 SparQ Attention Algorithm
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SparQ 注意力算法
- en: procedure SparseQAttention($\bm{q}\in\mathbb{R}^{d_{h}},\bm{K}\in\mathbb{R}^{S\times
    d_{h}},\bm{V}\in\mathbb{R}^{S\times d_{h}},\bm{\overline{v}}\in\mathbb{R}^{d_{h}},r\in\mathbb{N},k\in\mathbb{N},l\in\mathbb{N}$
    $\triangleright$ elements of $\bm{|q|}$ $\triangleright$ if $i></math> <math id=$
    positions     $\bm{i}_{2}\leftarrow\mathrm{argtopk}\left(\bm{\hat{s}}+\bm{m},k\right)$
    Indices of top $k$ $\triangleright$     $\bm{s}\leftarrow\mathrm{softmax}\left(\bm{q}\cdot\bm{K}_{[\bm{i}_{2},:]}^{\top}/\sqrt{d_{h}}\right)$
    Final attention scores (top $k$ $\triangleright$     return $\bm{y}$end procedure
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: procedure SparseQAttention($\bm{q}\in\mathbb{R}^{d_{h}},\bm{K}\in\mathbb{R}^{S\times
    d_{h}},\bm{V}\in\mathbb{R}^{S\times d_{h}},\bm{\overline{v}}\in\mathbb{R}^{d_{h}},r\in\mathbb{N},k\in\mathbb{N},l\in\mathbb{N}$
    $\triangleright$ $\bm{|q|}$ 的元素 $\triangleright$ 如果 $i>$ <math id=$ 位置 $\bm{i}_{2}\leftarrow\mathrm{argtopk}\left(\bm{\hat{s}}+\bm{m},k\right)$
    前 $k$ 的索引 $\triangleright$ $\bm{s}\leftarrow\mathrm{softmax}\left(\bm{q}\cdot\bm{K}_{[\bm{i}_{2},:]}^{\top}/\sqrt{d_{h}}\right)$
    最终注意力分数（前 $k$ 个） $\triangleright$ 返回 $\bm{y}$结束 procedure
- en: 'Figure 3: *'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: *'
- en: 'Algorithm 1: SparQ Attention is, for the most part, a stateless replacement
    of the attention operation in a pre-trained transformer. The exception is $\overline{\bm{v}}=\mathrm{mean}(\bm{V})$
    matrix at every step. In our experiments, we set $r\in\left\{16,32,64\right\}$
    is the main determinant of the compression ratio), $k\in\left\{64,128\right\}$.
    PyTorch code is provided in [Appendix B](#A2 "Appendix B Code ‣ SparQ Attention:
    Bandwidth-Efficient LLM Inference").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 1: SparQ 注意力在很大程度上是预训练变换器中注意力操作的无状态替代。唯一的例外是在每一步的 $\overline{\bm{v}}=\mathrm{mean}(\bm{V})$
    矩阵。在我们的实验中，我们设置 $r\in\left\{16,32,64\right\}$ 是压缩比的主要决定因素），$k\in\left\{64,128\right\}$。PyTorch
    代码在 [附录 B](#A2 "附录 B 代码 ‣ SparQ 注意力：带宽高效的 LLM 推理") 中提供。'
- en: 'To facilitate an accurate approximation of attention, without transferring
    the entire $\bm{K}$ matrices, we make the following observations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确地近似注意力，而无需传输整个 $\bm{K}$ 矩阵，我们做出以下观察：
- en: 'Observation 1:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '观察 1:'
- en: The output of the $\mathrm{softmax}$. For example, $32$ scores often cover over
    $90$ rows of $\bm{V}$ corresponding to the largest attention scores could be fetched
    from memory with a minimal degradation in accuracy.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathrm{softmax}$ 的输出。例如，$32$ 个分数通常覆盖了 $\bm{V}$ 的超过 $90$ 行，这些行对应于最大的注意力分数，可以从内存中提取，准确度下降最小。
- en: 'Observation 2:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '观察 2:'
- en: 'The indices of the largest attention scores can be efficiently predicted without
    fetching the full $\bm{K}$, keeping the $r$ largest-magnitude components (see
    [Figure A3](#A4.F3 "In Appendix D Attention Sparsity Analysis ‣ SparQ Attention:
    Bandwidth-Efficient LLM Inference")).'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大注意力分数的索引可以有效地预测，而无需提取完整的 $\bm{K}$，保留 $r$ 个最大幅度的分量（见 [图 A3](#A4.F3 "附录 D 注意力稀疏性分析
    ‣ SparQ 注意力：带宽高效的 LLM 推理")）。
- en: 'Using these observations, we propose SparQ Attention (see [Figure 1](#S0.F1
    "In SparQ Attention: Bandwidth-Efficient LLM Inference") and [Algorithm 1](#alg1
    "In Figure 3 ‣ 3 SparQ Attention ‣ SparQ Attention: Bandwidth-Efficient LLM Inference"))
    consisting of three steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些观察结果，我们提出了SparQ 注意力（见[图1](#S0.F1 "在SparQ 注意力机制：带宽高效的LLM推断")和[算法1](#alg1
    "在图3 ‣ 3 SparQ 注意力 ‣ SparQ 注意力机制：带宽高效的LLM推断")），包括三个步骤：
- en: 'Step 1:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 1：
- en: Find the indices of $r$ and only fetch $\bm{K}$ using the sliced query and keys.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查找$r$的索引并仅使用切片的查询和键来获取$\bm{K}$。
- en: 'Step 2:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 2：
- en: Find the top-$k$ keys and values.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查找前$k$个键和值。
- en: 'Step 3:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 3：
- en: 'Estimate the total score $\alpha$ positions using the approximate attention
    scores. Use this total score to interpolate between the attention output from
    the top-$k$.²²2Further justification and implementation details can be found in
    [Appendix C](#A3 "Appendix C SparQ Attention Procedure ‣ SparQ Attention: Bandwidth-Efficient
    LLM Inference").'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用近似注意力分数估算总分$\alpha$位置。使用这个总分在前$k$的注意力输出之间进行插值。²²2更多的理由和实现细节可以在[附录C](#A3 "附录C
    SparQ 注意力过程 ‣ SparQ 注意力机制：带宽高效的LLM推断")中找到。
- en: 'The memory transfer of the SparQ Attention algorithm for a single attention
    head forward-pass:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SparQ 注意力算法在单个注意力头前向传播中的内存传输：
- en: '|  | $\mathcal{M}_{\mathrm{SparQ\ }}=\underbrace{S\,r\,\vphantom{d_{h}}}_{\text{Read
    $r$ columns of $\bm{K}$}}+\underbrace{2\,k\,d_{h}}_{\text{Read top-$k$ rows of
    $\bm{K}$ and $\bm{V}$}}+\underbrace{4\,d_{h}}_{\text{Write current $\bm{k}$ and
    $\bm{v}$, read/write $\overline{\bm{v}}$}}$ |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}_{\mathrm{SparQ\ }}=\underbrace{S\,r\,\vphantom{d_{h}}}_{\text{读取$r$列的$\bm{K}$}}+\underbrace{2\,k\,d_{h}}_{\text{读取前$k$行的$\bm{K}$和$\bm{V}$}}+\underbrace{4\,d_{h}}_{\text{写入当前的$\bm{k}$和$\bm{v}$，读取/写入$\overline{\bm{v}}$}}$
    |  | (3) |'
- en: 'By varying $r$ we can tune the total memory bandwidth requirement of the scheme,
    trading-off approximation accuracy for inference speed-up. Since typically $S\gg
    d_{h}$ is the most important parameter controlling the approximation and bandwidth
    compression. Typical compression ratios are given in [Table A2](#A6.T2 "In Appendix
    F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整$r$，我们可以调节方案的总内存带宽需求，在近似精度和推断速度之间进行权衡。由于通常情况下$S\gg d_{h}$是控制近似和带宽压缩的最重要参数。典型的压缩比见于[表A2](#A6.T2
    "在附录F方法论 ‣ SparQ 注意力机制：带宽高效的LLM推断")。
- en: 4 Experiments
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Setup
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: Models
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型
- en: 'We evaluate our method on two widely-used open-source language model variants:
    Llama $2$ billion parameters. Both of these model families are decoder-only transformers
    [Radford et al., [2018](#bib.bib18)], pre-trained on causal language modelling.
    They share similar architectural components such as Multi-Head Attention and Rotary
    positional encoding [Su et al., [2021](#bib.bib24)], while also having some notable
    differences such as different layer normalisation implementations, activation
    functions and execution of modules in parallel.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个广泛使用的开源语言模型变体上评估我们的方法：Llama $2$十亿参数。这些模型家族都是仅解码器的变换器 [Radford et al., [2018](#bib.bib18)]，在因果语言建模上进行预训练。它们共享类似的架构组件，如多头注意力和旋转位置编码
    [Su et al., [2021](#bib.bib24)]，同时也有一些显著的不同点，如不同的层归一化实现、激活函数和并行执行模块。
- en: Tasks
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务
- en: 'In order to evaluate the technique on a spectrum of relevant NLP tasks, our
    evaluation setup tests the models on various tasks requiring information retrieval
    and reasoning from the provided context: question answering, summarisation, perplexity/bits-per-character
    (BPC), text repetition. For all tasks, we aimed to generate examples of sequence
    lengths between $1000$ tokens.³³3As we wanted to define the tasks independently
    of the selected models, our examples were chosen to have context lengths between
    $4000$ characters, roughly giving the desired lengths in tokens.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在一系列相关的NLP任务上评估该技术，我们的评估设置测试了在提供的上下文中需要信息检索和推理的各种任务：问答、摘要、困惑度/每字符比特数（BPC）、文本重复。对于所有任务，我们的目标是生成长度在$1000$个token之间的示例。³³3由于我们希望定义任务时不依赖于所选择的模型，我们选择的示例上下文长度在$4000$字符之间，大致给出所需的token长度。
- en: For question answering, we use the SQuAD [Rajpurkar et al., [2016](#bib.bib19)]
    and TriviaQA [Joshi et al., [2017](#bib.bib9)] datasets in the open-book setting.
    In order to construct the SQuAD examples, we augment the provided context (required
    to answer the question) with seven additional confusion contexts from unrelated
    questions. This ensures that the examples have the appropriate context length,
    as well as making the task harder as the model needs to distinguish the relevant
    information from the context from the unrelated paragraphs. We use SQuAD v$1.1$,
    since we aim to measure the model’s ability to extract useful information from
    the KV cache. For both question answering tasks we use exact string match accuracy
    as the evaluation metric.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问答任务，我们使用 SQuAD [Rajpurkar 等， [2016](#bib.bib19)] 和 TriviaQA [Joshi 等， [2017](#bib.bib9)]
    数据集，在开放书籍设置中。为了构建 SQuAD 示例，我们用七个额外的混淆上下文（来自无关的问题）来增强提供的上下文（需要回答问题）。这确保了示例具有适当的上下文长度，并且使任务更具挑战性，因为模型需要从上下文中区分相关信息与无关段落。我们使用
    SQuAD v$1.1$，因为我们旨在测量模型从 KV 缓存中提取有用信息的能力。对于这两个问答任务，我们使用精确字符串匹配准确度作为评估指标。
- en: Summarisation is evaluated on the CNN/DailyMail dataset [See et al., [2017](#bib.bib21)]
    using the ROUGE-L F-score [Lin, [2004](#bib.bib13)] as the metric. We use the
    WikiText-$103$ dataset [Merity et al., [2016](#bib.bib16)] with bits per character
    (BPC)⁴⁴4We quote performance for sub-word language modelling in BPC, to account
    for any differences in vocabulary across models. for evaluating language modelling
    performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要评估在 CNN/DailyMail 数据集上进行 [见等， [2017](#bib.bib21)]，使用 ROUGE-L F-score [Lin，
    [2004](#bib.bib13)] 作为指标。我们使用 WikiText-$103$ 数据集 [Merity 等， [2016](#bib.bib16)]，用字符比特数（BPC）⁴⁴4我们引用了子词语言建模的
    BPC 性能，以考虑模型之间词汇差异。来评估语言建模性能。
- en: Finally, we construct an artificial “Text Repetition” task to evaluate the capability
    of the model to repeat sentences from its context verbatim. Such a task can commonly
    appear in a dialogue setting where the LLM agent is required to retrieve a piece
    of text from a possibly long context provided, and can be challenging for sparse
    attention techniques. We construct examples using the Tiny-Shakespeare dataset
    [Karpathy, [2015](#bib.bib11)] by chunking the text into contexts of the appropriate
    size, appending them with the prompts containing a subset of the context, and
    evaluating the output exact character length match with the continuation from
    the context.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建了一个人工“文本重复”任务，以评估模型逐字重复其上下文中的句子的能力。这样的任务通常出现在对话设置中，其中 LLM 代理需要从可能很长的上下文中检索一段文本，并且对稀疏注意力技术来说可能具有挑战性。我们使用
    Tiny-Shakespeare 数据集 [Karpathy， [2015](#bib.bib11)] 构建示例，将文本分块为适当大小的上下文，将其与包含上下文子集的提示附加，并评估输出的确切字符长度是否与上下文中的续文匹配。
- en: Baselines
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线
- en: We consider the cache eviction technique H[2]O [Zhang et al., [2023](#bib.bib32)]
    and local windowing with the initial-tokens enhancement proposed by Han et al.
    [[2023](#bib.bib8)] as baselines. For consistency, we allocate a fixed KV cache
    transfer budget $k$ (with $3k/4$ positions (with $k-16$ local positions).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将缓存驱逐技术 H[2]O [Zhang 等， [2023](#bib.bib32)] 和 Han 等 [[2023](#bib.bib8)] 提出的初始标记增强的局部窗口技术作为基线。为了保持一致性，我们分配了固定的
    KV 缓存传输预算 $k$（其中 $3k/4$ 位置（以及 $k-16$ 局部位置）。
- en: 4.2 Results
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: '| Dataset Name | SQuAD $\uparrow$ | CNN/DM $\uparrow$ | Repetition $\uparrow$
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | SQuAD $\uparrow$ | CNN/DM $\uparrow$ | 重复 $\uparrow$ |'
- en: '| Compression | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8
    | 1 | 1/2 | 1/8 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 压缩 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 | 1/2 | 1/8 | 1 |
    1/2 | 1/8 |'
- en: '| Pythia $6.9$B | Local | 57.8 | 38.5 | 14.5 | 52.6 | 41.6 | 23.5 | 20.2 |
    14.9 | 13.6 | 0.68 | 0.71 | 0.83 | 150 | 64 | 18 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Pythia $6.9$B | 本地 | 57.8 | 38.5 | 14.5 | 52.6 | 41.6 | 23.5 | 20.2 | 14.9
    | 13.6 | 0.68 | 0.71 | 0.83 | 150 | 64 | 18 |'
- en: '| H[2]O | 52.9 | 45.5 | 52.6 | 52.3 | 20.2 | 17.9 | 0.69 | 0.73 | 47 | 17 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | 52.9 | 45.5 | 52.6 | 52.3 | 20.2 | 17.9 | 0.69 | 0.73 | 47 | 17 |'
- en: '| SparQ | 58.0 | 57.9 | 52.4 | 51.2 | 20.6 | 20.4 | 0.68 | 0.69 | 151 | 133
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| SparQ | 58.0 | 57.9 | 52.4 | 51.2 | 20.6 | 20.4 | 0.68 | 0.69 | 151 | 133
    |'
- en: '| Llama$\,2$B | Local | 76.9 | 46.1 | 25.5 | 77.6 | 70.1 | 65.2 | 21.4 | 16.5
    | 14.1 | 0.77 | 0.81 | 0.91 | 207 | 66 | 10 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Llama$\,2$B | 本地 | 76.9 | 46.1 | 25.5 | 77.6 | 70.1 | 65.2 | 21.4 | 16.5
    | 14.1 | 0.77 | 0.81 | 0.91 | 207 | 66 | 10 |'
- en: '| H[2]O | 66.2 | 47.1 | 77.2 | 71.3 | 21.8 | 7.4 | 0.77 | 0.88 | 41 | 18 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | 66.2 | 47.1 | 77.2 | 71.3 | 21.8 | 7.4 | 0.77 | 0.88 | 41 | 18 |'
- en: '| SparQ | 76.4 | 70.2 | 77.3 | 75.3 | 21.7 | 20.5 | 0.77 | 0.79 | 191 | 188
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| SparQ | 76.4 | 70.2 | 77.3 | 75.3 | 21.7 | 20.5 | 0.77 | 0.79 | 191 | 188
    |'
- en: 'Table 1: Results from the largest models tested are presented above. SQuAD
    and TriviaQA measure performance in accuracy. CNN/DailyMail uses ROUGE-L score.
    Repetition counts the number of characters before the generated sequence diverges
    and WikiText task measures perplexity in bits per character (BPC). Values in bold
    represent the best score for a particular model, task and sparsity setting. Standard
    errors are: SQuAD $0.8$, CNN/DailyMail $0.4$, Repetition $3$.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：展示了测试的最大模型的结果。SQuAD和TriviaQA衡量准确性表现。CNN/DailyMail使用ROUGE-L得分。重复计算生成序列偏离前的字符数，WikiText任务以每字符比特（BPC）衡量困惑度。粗体值代表特定模型、任务和稀疏设置的最佳得分。标准误差为：SQuAD
    $0.8$，CNN/DailyMail $0.4$，重复 $3$。
- en: 'Our experiments span four distinct models: Llama $2$ billion parameters and
    three Pythia models with $1.4$ and $6.9$ billion parameters. Results from the
    largest models are presented in [Table 1](#S4.T1 "In 4.2 Results ‣ 4 Experiments
    ‣ SparQ Attention: Bandwidth-Efficient LLM Inference"), with further results in
    [Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient LLM Inference"). We
    observe:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验涵盖了四种不同的模型：Llama $2$ 亿参数和三个Pythia模型，分别具有 $1.4$ 亿和 $6.9$ 亿参数。最大模型的结果展示在[表1](#S4.T1
    "在4.2结果 ‣ 4实验 ‣ SparQ注意力：带宽高效LLM推理")中，进一步结果见[图A1](#A0.F1 "在SparQ注意力：带宽高效LLM推理")。我们观察到：
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SparQ Attention performance is robust across all tasks and model sizes tested.
    Compression ratios of $2\times$ are readily achievable with little to no loss
    in task performance.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SparQ Attention在所有测试的任务和模型大小中表现稳健。 $2\times$ 的压缩比在任务性能几乎没有损失的情况下很容易实现。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The simple recipe of using $k=64$, choosing $r$ performs much worse than $k=128$
    for many models.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于许多模型，使用 $k=64$ 的简单方案，选择 $r$ 的效果远逊色于 $k=128$。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Certain tasks are more challenging for H[2]O (Repetition, SQuAD), while others
    are more forgiving (TriviaQA, WikiText-$103$ LM).
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 某些任务对H[2]O（重复，SQuAD）更加具有挑战性，而其他任务则相对宽松（TriviaQA，WikiText-$103$ LM）。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Local attention degrades performance across all tasks, demonstrating that the
    tasks do not permit the trivial solution of discarding the long context.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 局部注意力在所有任务中都降低了性能，表明这些任务不允许简单地丢弃长上下文。
- en: 5 Related Work
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Efficient attention methods have been a very active area of research [Tay et al.,
    [2020b](#bib.bib26)]. Schemes such as *Sparse Transformers* [Child et al., [2019](#bib.bib7)],
    *Combiner* [Ren et al., [2021](#bib.bib20)], *Longformer* [Beltagy et al., [2020](#bib.bib3)],
    *BigBird* [Zaheer et al., [2020](#bib.bib31)], *Reformer* [Kitaev et al., [2020](#bib.bib12)]
    and *Sparse Sinkhorn Attention* [Tay et al., [2020a](#bib.bib25)] have been developed
    to increase efficiency of the attention mechanism by extracting information from
    the most salient tokens in the sequence or approximating the dense attention maps.
    These methods form part of the architecture, they must be implemented at pre-training
    stage and carry varying task performance trade-offs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 高效注意力方法是一个非常活跃的研究领域 [Tay 等人, [2020b](#bib.bib26)]。像*稀疏变换器* [Child 等人, [2019](#bib.bib7)]、*组合器*
    [Ren 等人, [2021](#bib.bib20)]、*Longformer* [Beltagy 等人, [2020](#bib.bib3)]、*BigBird*
    [Zaheer 等人, [2020](#bib.bib31)]、*Reformer* [Kitaev 等人, [2020](#bib.bib12)] 和 *稀疏Sinkhorn注意力*
    [Tay 等人, [2020a](#bib.bib25)] 等方案已被开发，以通过提取序列中最显著的标记或近似密集的注意力图来提高注意力机制的效率。这些方法是架构的一部分，必须在预训练阶段实施，并且会带来不同的任务性能权衡。
- en: Another line of work focuses on reducing the memory footprint requirements of
    the attention mechanism, while maintaining quadratic complexity. These methods
    include popular schemes such as *Multi-Query Attention* (MQA) [Shazeer, [2019](#bib.bib22)]
    and *Grouped-Query Attention* (GQA) [Ainslie et al., [2023](#bib.bib1)] that share
    a single key and value head across multiple query heads. This leads to a reduction
    in memory footprint and bandwidth. While these schemes boost efficiency, they
    must be implemented at pre-training stage and may affect model quality and stability.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类工作专注于在保持二次复杂度的同时减少注意力机制的内存占用。这些方法包括流行的方案，如*多查询注意力*（MQA） [Shazeer, [2019](#bib.bib22)]
    和 *分组查询注意力*（GQA） [Ainslie 等人, [2023](#bib.bib1)]，它们在多个查询头中共享一个单一的键和值头。这导致内存占用和带宽的减少。虽然这些方案提升了效率，但必须在预训练阶段实施，可能会影响模型的质量和稳定性。
- en: Alternatively, an emerging area of research similar to SparQ Attention aims
    to only adapt the inference procedure of a frozen pre-trained model. The simplest
    method of this category is used as part of *FlexGen* [Sheng et al., [2023](#bib.bib23)]
    called *top-$k$ scores. This process uses the full key cache to produce attention
    scores, thus limiting the asymptotic reduction of the memory transfers to only
    $50\%$.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与SparQ Attention类似的新兴研究领域旨在仅调整冻结预训练模型的推理过程。这一类别中最简单的方法是*FlexGen* [Sheng
    et al., [2023](#bib.bib23)] 中使用的*top-$k$分数。此过程使用完整的键缓存来产生注意力分数，从而将内存传输的渐近减少限制在$50\%$。
- en: '*Eviction* schemes describe a number of methods that only cache a subset of
    keys and values, and continually delete tokens that are uninformative for future
    outputs. By reducing the cache size itself, both memory and memory bandwidth overheads
    are reduced. *H[2]O* [Zhang et al., [2023](#bib.bib32)] and *Scissorhands* [Liu
    et al., [2023b](#bib.bib15)] are examples of such eviction methods. H[2]O uses
    a greedy eviction policy that maintains the most salient “Heavy Hitter” tokens
    that contribute most to the attention scores on every new generation step. Similarly,
    Scissorhands identifies and maintains “pivotal tokens” by counting how many times
    a token’s attention score exceeds an importance threshold over the generative
    process. While these methods reduce the memory footprint of the attention cache
    as well as bandwidth, they also lead to permanent loss of information from the
    context window, which can lead to mistakes for queries seeking less attended parts
    of the context.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*Eviction* 方案描述了仅缓存键和值的子集的方法，并不断删除对未来输出不重要的标记。通过减少缓存本身的大小，减少了内存和内存带宽的开销。*H[2]O*
    [Zhang et al., [2023](#bib.bib32)] 和 *Scissorhands* [Liu et al., [2023b](#bib.bib15)]
    是这种逐出方法的例子。H[2]O 使用贪婪的逐出策略，保持对每个新生成步骤中注意力分数贡献最大的“重击”标记。同样，Scissorhands 通过计算标记的注意力分数超过生成过程中的重要性阈值的次数，来识别和保持“关键标记”。虽然这些方法减少了注意力缓存的内存占用和带宽，但也导致了上下文窗口中信息的永久丧失，这可能会导致对上下文中较少关注部分的查询出现错误。'
- en: '*IceFormer* [Anonymous, [2023](#bib.bib2)] evaluates a multitude of existing
    approximate nearest neighbour algorithms for efficiently approximating attention
    scores of pre-trained models, similarly to SparQ Attention. Yet, this work focuses
    solely on serving models using CPU due to the implementation of approximate nearest
    neighbour, while our scheme is hardware-agnostic and can be used with any accelerated
    computing platform. *Scatterbrain* [Chen et al., [2021](#bib.bib6)] employs similar
    techniques, but for computer vision applications.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*IceFormer* [Anonymous, [2023](#bib.bib2)] 评估了多种现有的近似最近邻算法，以高效地近似预训练模型的注意力分数，与SparQ
    Attention类似。然而，这项工作仅关注于使用CPU服务模型，因为实现了近似最近邻，而我们的方案不依赖于硬件，可以与任何加速计算平台一起使用。*Scatterbrain*
    [Chen et al., [2021](#bib.bib6)] 采用了类似的技术，但用于计算机视觉应用。'
- en: An orthogonal line of work saves memory traffic by compressing the KV cache
    with $4$-bit compression can be complementary to techniques that reduce the number
    of transferred elements.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一条正交的研究方向通过用$4$位压缩来压缩KV缓存，从而节省内存流量，这可以与减少转移元素数量的技术互补。
- en: 6 Discussion
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: In this work we have presented SparQ Attention, a novel technique for unlocking
    faster inference for pre-trained LLMs. Our proposed technique modifies the multi-head
    attention mechanism to access only the relevant tokens from the KV cache on every
    inference step, leading to drastic memory transfer savings. This is a significant
    improvement as it allows for pre-trained models to be executed more efficiently,
    without any fine-tuning or modifications to the weights of the model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了SparQ Attention，一种新颖的技术，用于加快预训练LLMs的推理速度。我们提出的技术修改了多头注意力机制，以便在每次推理步骤中仅访问KV缓存中的相关标记，从而显著节省内存传输。这是一个重要的改进，因为它使得预训练模型能够更高效地执行，而无需对模型的权重进行任何微调或修改。
- en: LLM inference speed is often bottlenecked by memory transfers rather than computation,
    especially in regimes of long sequence lengths and limited memory bandwidth. Traffic
    of the past KV values can dominate the memory bandwidth when using large batch
    sizes and sequence lengths longer than the hidden state of the model, in which
    case the KV cache reaches a higher memory footprint than model weights. This mode
    of execution is particularly suited to the SparQ technique, with up to eightfold
    decrease in memory transfer of the KV cache with little to no accuracy degradation.
    Our method significantly improves on existing methods and does not lead to any
    loss of information from the context window, as tokens are maintained in memory.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推理速度常常受到内存传输的瓶颈影响，而不是计算，尤其是在长序列长度和有限内存带宽的情况下。当使用大批量大小和序列长度超过模型的隐藏状态时，过去
    KV 值的流量可能主导内存带宽，此时 KV 缓存的内存占用比模型权重更高。这种执行模式特别适用于 SparQ 技术，KV 缓存的内存传输减少最多可达八倍，几乎没有准确性下降。我们的方法在现有方法上有显著改进，并且不会导致上下文窗口信息的丢失，因为令牌保持在内存中。
- en: 'SparQ Attention has some limitations: while maintaining very strong performance
    at high bandwidth compression ratios, this is achieved by keeping all cached values
    in memory which is sparsely accessed during the generation steps. It therefore
    does not save any memory capacity, only bandwidth. Another possible limitation
    is the unclear efficiency saving of SparQ Attention when used with transformer
    models using MQA and GQA, which were not evaluated in this work. We leave it as
    future work to extend SparQ Attention to cover more attention mechanisms.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SparQ Attention 有一些局限性：尽管在高带宽压缩比下保持了非常强的性能，但这是通过在生成步骤中将所有缓存值保存在内存中实现的，这些缓存值在生成步骤中被稀疏访问。因此，它并未节省任何内存容量，只是带宽。另一个可能的局限性是
    SparQ Attention 在与使用 MQA 和 GQA 的变换器模型结合时效率节省的不明确，这在本工作中没有评估。我们将其作为未来的工作，将 SparQ
    Attention 扩展到更多的注意力机制中。
- en: Acknowledgements
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Daniel Justus, Paul Balança and Andrew Fitzgibbon for
    their helpful input and feedback on this work.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Daniel Justus、Paul Balança 和 Andrew Fitzgibbon 对本工作的有益建议和反馈。
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ainslie et al. [2023] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury
    Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. GQA: Training generalized multi-query
    transformer models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*,
    2023.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ainslie 等人 [2023] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,
    Federico Lebrón 和 Sumit Sanghai。GQA: 从多头检查点训练通用多查询变换器模型。*arXiv 预印本 arXiv:2305.13245*，2023。'
- en: 'Anonymous [2023] Anonymous. Iceformer: Accelerated inference with long-sequence
    transformers on CPUs. In *Submitted to The Twelfth International Conference on
    Learning Representations*, 2023. URL [https://openreview.net/forum?id=6RR3wU4mSZ](https://openreview.net/forum?id=6RR3wU4mSZ).
    under review.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anonymous [2023] 匿名。Iceformer: 在 CPU 上使用长序列变换器加速推理。在 *提交至第十二届国际学习表征会议*，2023。网址
    [https://openreview.net/forum?id=6RR3wU4mSZ](https://openreview.net/forum?id=6RR3wU4mSZ)。正在审稿中。'
- en: 'Beltagy et al. [2020] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等人 [2020] Iz Beltagy, Matthew E Peters 和 Arman Cohan。Longformer: 长文档变换器。*arXiv
    预印本 arXiv:2004.05150*，2020。'
- en: 'Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Gregory
    Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu
    Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing
    large language models across training and scaling. In *International Conference
    on Machine Learning*, pages 2397–2430\. PMLR, 2023.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biderman 等人 [2023] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff 等人。Pythia: 一个用于分析大型语言模型的工具套件，涵盖训练和扩展。在 *国际机器学习会议*
    上，页码 2397–2430。PMLR，2023。'
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in Neural Information
    Processing Systems*, 33:1877–1901, 2020.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Chen et al. [2021] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra,
    and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. *Advances
    in Neural Information Processing Systems*, 34:17413–17426, 2021.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2021] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra,
    和 Christopher Ré. Scatterbrain：统一稀疏和低秩注意力。*神经信息处理系统进展*，34:17413–17426，2021年。
- en: Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers, 2019.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, 和 Ilya Sutskever.
    使用稀疏变换器生成长序列，2019年。
- en: 'Han et al. [2023] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. LM-infinite: Simple on-the-fly length generalization for large language
    models. *arXiv preprint arXiv:2308.16137*, 2023.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. [2023] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong
    Wang. LM-infinite：大型语言模型的简单即时长度泛化。*arXiv 预印本 arXiv:2308.16137*，2023年。
- en: 'Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.
    *arXiv preprint arXiv:1705.03551*, 2017.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, 和 Luke Zettlemoyer.
    TriviaQA：一个大规模的远程监督挑战数据集用于阅读理解。*arXiv 预印本 arXiv:1705.03551*，2017年。
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei.
    神经语言模型的扩展规律。*arXiv 预印本 arXiv:2001.08361*，2020年。
- en: Karpathy [2015] Andrej Karpathy. The unreasonable effectiveness of recurrent
    neural networks. [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn),
    2015.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy [2015] Andrej Karpathy. 循环神经网络的非凡效果。 [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)，2015年。
- en: 'Kitaev et al. [2020] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev et al. [2020] Nikita Kitaev, Łukasz Kaiser, 和 Anselm Levskaya. Reformer：高效的变换器。*arXiv
    预印本 arXiv:2001.04451*，2020年。
- en: 'Lin [2004] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries.
    In *Text Summarization Branches Out*, pages 74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin [2004] Chin-Yew Lin. ROUGE：一个自动评估摘要的软件包。在*文本摘要的延伸*，第74–81页，西班牙巴塞罗那，2004年7月。计算语言学协会。网址
    [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)。
- en: 'Liu et al. [2023a] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models, 2023a.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat：用于大型语言模型的数据无关量化感知训练，2023年。
- en: 'Liu et al. [2023b] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for llm kv cache compression
    at test time. *arXiv preprint arXiv:2305.17118*, 2023b.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, 和 Anshumali Shrivastava. Scissorhands：利用重要性假设的持久性进行
    LLM KV 缓存压缩。*arXiv 预印本 arXiv:2305.17118*，2023年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Pope et al. [2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. *Proceedings of Machine Learning and Systems*,
    5, 2023.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope et al. [2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, 和 Jeff Dean. 高效扩展变换器推理。*机器学习与系统会议录*，5，2023年。
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. Improving language understanding by generative pre-training, 2018.
    URL [https://openai.com/research/language-unsupervised](https://openai.com/research/language-unsupervised).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, 和 Ilya
    Sutskever. 通过生成预训练提升语言理解，2018年。网址 [https://openai.com/research/language-unsupervised](https://openai.com/research/language-unsupervised)。
- en: 'Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. *arXiv
    preprint arXiv:1606.05250*, 2016.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajpurkar 等人 [2016] Pranav Rajpurkar、Jian Zhang、Konstantin Lopyrev 和 Percy
    Liang。SQuAD: 100,000+ 个问题用于机器理解文本。*arXiv 预印本 arXiv:1606.05250*，2016。'
- en: 'Ren et al. [2021] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec,
    Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse
    computation cost. *Advances in Neural Information Processing Systems*, 34:22470–22482,
    2021.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 [2021] Hongyu Ren、Hanjun Dai、Zihang Dai、Mengjiao Yang、Jure Leskovec、Dale
    Schuurmans 和 Bo Dai。Combiner：全注意力变压器与稀疏计算成本。*神经信息处理系统进展*，34:22470–22482，2021。
- en: 'See et al. [2017] Abigail See, Peter J Liu, and Christopher D Manning. Get
    to the point: Summarization with pointer-generator networks. *arXiv preprint arXiv:1704.04368*,
    2017.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See 等人 [2017] Abigail See、Peter J Liu 和 Christopher D Manning。直奔主题：使用指针-生成网络的摘要。*arXiv
    预印本 arXiv:1704.04368*，2017。
- en: 'Shazeer [2019] Noam Shazeer. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer [2019] Noam Shazeer。快速变压器解码：一个写头足矣。*arXiv 预印本 arXiv:1911.02150*，2019。
- en: 'Sheng et al. [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen:
    high-throughput generative inference of large language models with a single GPU.
    In *International Conference on Machine Learning*, pages 31094–31116\. PMLR, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人 [2023] Ying Sheng、Lianmin Zheng、Binhang Yuan、Zhuohan Li、Max Ryabinin、Beidi
    Chen、Percy Liang、Christopher Ré、Ion Stoica 和 Ce Zhang。Flexgen：利用单个 GPU 高通量生成推断大型语言模型。在
    *国际机器学习大会*，第 31094–31116 页。PMLR，2023。
- en: 'Su et al. [2021] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.
    Roformer: Enhanced transformer with rotary position embedding. *CoRR*, abs/2104.09864,
    2021. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等人 [2021] Jianlin Su、Yu Lu、Shengfeng Pan、Bo Wen 和 Yunfeng Liu。Roformer：增强型变压器与旋转位置嵌入。*CoRR*，abs/2104.09864，2021。网址
    [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)。
- en: Tay et al. [2020a] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng
    Juan. Sparse sinkhorn attention. In *International Conference on Machine Learning*,
    pages 9438–9447\. PMLR, 2020a.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay 等人 [2020a] Yi Tay、Dara Bahri、Liu Yang、Donald Metzler 和 Da-Cheng Juan。稀疏
    Sinkhorn 注意力。在 *国际机器学习大会*，第 9438–9447 页。PMLR，2020a。
- en: 'Tay et al. [2020b] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
    Efficient transformers: A survey. *CoRR*, abs/2009.06732, 2020b. URL [https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay 等人 [2020b] Yi Tay、Mostafa Dehghani、Dara Bahri 和 Donald Metzler。高效变压器：综述。*CoRR*，abs/2009.06732，2020b。网址
    [https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732)。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等人。Llama
    2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023。
- en: 'Vig [2019] Jesse Vig. A multiscale visualization of attention in the transformer
    model. In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, pages 37–42, 01 2019.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vig [2019] Jesse Vig。变压器模型中注意力的多尺度可视化。在 *第 57 届计算语言学协会年会：系统演示*，第 37–42 页，2019年1月。
- en: Wei et al. [2022] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. In *International Conference on Learning Representations*,
    2022. URL [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022] Jason Wei、Maarten Bosma、Vincent Zhao、Kelvin Guu、Adams Wei Yu、Brian
    Lester、Nan Du、Andrew M. Dai 和 Quoc V Le。微调语言模型是零-shot 学习者。在 *国际学习表征大会*，2022。网址
    [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR)。
- en: 'Yun et al. [2020] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh
    Rawat, Sashank Reddi, and Sanjiv Kumar. $o(n)$ connections are expressive enough:
    Universal approximability of sparse transformers. *Advances in Neural Information
    Processing Systems*, 33:13783–13794, 2020.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yun 等人 [2020] Chulhee Yun、Yin-Wen Chang、Srinadh Bhojanapalli、Ankit Singh Rawat、Sashank
    Reddi 和 Sanjiv Kumar。$o(n)$ 连接足够表达：稀疏变压器的通用近似能力。*神经信息处理系统进展*，33:13783–13794，2020。
- en: 'Zaheer et al. [2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. Big bird: Transformers for longer sequences. *Advances in Neural
    Information Processing Systems*, 33:17283–17297, 2020.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等人 [2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, 等。Big bird: 用于较长序列的 Transformer。*神经信息处理系统进展*，33:17283–17297，2020。'
- en: 'Zhang et al. [2023] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al.
    H[2]O: Heavy-hitter oracle for efficient generative inference of large language
    models. *arXiv preprint arXiv:2306.14048*, 2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2023] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, 等。H[2]O:
    高效生成推理大语言模型的重型预言机。*arXiv 预印本 arXiv:2306.14048*，2023。'
- en: '![Refer to caption](img/e5404e2e2cc377387be0e4016d86fc59.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/e5404e2e2cc377387be0e4016d86fc59.png)'
- en: 'Figure A1: Compression versus performance trade-off curves over all models
    and tasks evaluated. The y-axis minimum is set to ($0.5$, $0.5$, $0.0$ the dense
    baseline for the tasks, reading top-to-bottom, in order to give a consistent view
    of the performance loss across models. Vertical dotted lines show ($1/4$) compression
    versus dense. Shaded lines show $\pm 1$ standard error of the mean.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A1：所有模型和任务的压缩与性能权衡曲线。y 轴的最小值设置为（$0.5$, $0.5$, $0.0$ 为任务的稠密基线，自上而下阅读），以提供一致的模型性能损失视图。垂直虚线显示
    ($1/4$) 压缩与稠密基线的对比。阴影线显示 $\pm 1$ 标准误差。
- en: Appendix A Detailed Results
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 详细结果
- en: '[Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient LLM Inference")
    reports the compression/performance trade-off curves for all models and tasks
    that were evaluated.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 A1](#A0.F1 "在 SparQ 注意力机制中：带宽高效的 LLM 推理") 报告了所有模型和任务的压缩/性能权衡曲线。'
- en: Appendix B Code
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 代码
- en: '![[Uncaptioned image]](img/64b7e7b65e67b963171fdb3e51459aa0.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![无图注图像](img/64b7e7b65e67b963171fdb3e51459aa0.png)'
- en: Appendix C SparQ Attention Procedure
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C SparQ 注意力机制
- en: 'The SparQ Attention procedure consists of three steps. In the first step we
    find the indices $\bm{i}_{1}\in\mathbb{N}^{r}$ components of the query vector
    $\bm{q}\in\mathbb{R}^{d_{h}}$ across the sequence as follows, where $S\in\mathbb{N}$
    is the sequence length:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: SparQ 注意力机制包括三个步骤。在第一步中，我们找到查询向量 $\bm{q}\in\mathbb{R}^{d_{h}}$ 在序列中 $\bm{i}_{1}\in\mathbb{N}^{r}$
    组件的索引，其中 $S\in\mathbb{N}$ 是序列长度：
- en: '|  | $\displaystyle\hat{a}_{i}=\sum_{j\in\bm{i}_{1}}q_{j}K_{i,j}$ |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{a}_{i}=\sum_{j\in\bm{i}_{1}}q_{j}K_{i,j}$ |  |'
- en: '|  | $\displaystyle\bm{\hat{s}}=\sigma\Bigl{(}\dfrac{\bm{\hat{a}}}{\tau}\Bigr{)}$
    |  | (A1) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\hat{s}}=\sigma\Bigl{(}\dfrac{\bm{\hat{a}}}{\tau}\Bigr{)}$
    |  | (A1) |'
- en: 'where $\sigma(\bm{x})_{i}=e^{x_{i}}/\sum_{j}(e^{x_{j}})$ function and $\bm{K}\in\mathbb{R}^{S\times
    d_{h}}$ is chosen to be $\sqrt{d_{h}}$ over which the dot product summation is
    performed. However, since the dot product is approximated using only $r$ components
    here, the scaling factor needs to be appropriately changed. Empirically we found
    the appropriate factor to be:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma(\bm{x})_{i}=e^{x_{i}}/\sum_{j}(e^{x_{j}})$ 函数，$\bm{K}\in\mathbb{R}^{S\times
    d_{h}}$ 选择为 $\sqrt{d_{h}}$，在其上进行点积求和。然而，由于点积仅使用了 $r$ 个组件进行近似，因此需要适当地调整缩放因子。通过经验我们发现合适的因子是：
- en: '|  | $\tau=\sqrt{d_{h}\dfrac{\sum_{i\in\bm{i}_{1}}&#124;q_{i}&#124;}{\sum_{i}&#124;q_{i}&#124;}}$
    |  | (A2) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau=\sqrt{d_{h}\dfrac{\sum_{i\in\bm{i}_{1}}&#124;q_{i}&#124;}{\sum_{i}&#124;q_{i}&#124;}}$
    |  | (A2) |'
- en: which takes into account the relative L1-norm of the selected top $r$.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这考虑了选定的前 $r$ 项的相对 L1 范数。
- en: 'In the second step, we proceed to find indices $\bm{i}_{2}\in\mathbb{N}^{k}$
    components of the approximate score vector $\bm{\hat{s}}$ positions, which we
    can express by defining a corresponding boolean mask $\bm{b}\in\{0,1\}^{S}$:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们继续找到近似分数向量 $\bm{\hat{s}}$ 的索引 $\bm{i}_{2}\in\mathbb{N}^{k}$ 位置，我们可以通过定义一个相应的布尔掩码
    $\bm{b}\in\{0,1\}^{S}$ 来表示：
- en: '|  | $\displaystyle b_{i}=\begin{cases}1&amp;\text{if }i\in\bm{i}_{2}\\ 0&amp;\text{else}\end{cases}$
    |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle b_{i}=\begin{cases}1&amp;\text{如果 }i\in\bm{i}_{2}\\ 0&amp;\text{否则}\end{cases}$
    |  |'
- en: '|  | $\displaystyle\bm{s}=\sigma\Bigl{(}\dfrac{(\bm{q}\cdot\bm{K}^{\top})\circ\bm{b}}{\sqrt{d_{h}}}\Bigr{)}$
    |  | (A3) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{s}=\sigma\Bigl{(}\dfrac{(\bm{q}\cdot\bm{K}^{\top})\circ\bm{b}}{\sqrt{d_{h}}}\Bigr{)}$
    |  | (A3) |'
- en: 'In the third step, we perform a weighted sum of the value vectors in $V\in\mathbb{R}^{S\times
    d_{h}}$. In order to improve the accuracy of the final approximation, we add an
    additional weighted contribution of the mean value vector $\bm{\bar{v}}=\frac{1}{S}\sum_{i}\bm{V}_{i,*}$:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步中，我们对$V\in\mathbb{R}^{S\times d_{h}}$中的值向量进行加权求和。为了提高最终近似的准确性，我们添加了均值向量$\bm{\bar{v}}=\frac{1}{S}\sum_{i}\bm{V}_{i,*}$的额外加权贡献：
- en: '|  | $\bm{y}=\alpha\sum_{i\in\bm{i}_{2}}s_{i}\bm{V}_{i,*}+(1-\alpha)\bm{\bar{v}}$
    |  | (A4) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y}=\alpha\sum_{i\in\bm{i}_{2}}s_{i}\bm{V}_{i,*}+(1-\alpha)\bm{\bar{v}}$
    |  | (A4) |'
- en: 'where $\alpha\in[0,1]$ terms. We choose $\alpha$ from the first step as:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha\in[0,1]$的项。我们从第一步选择$\alpha$如下：
- en: '|  | $\alpha=\sum_{i\in\bm{i}_{2}}\hat{s}_{i}$ |  | (A5) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha=\sum_{i\in\bm{i}_{2}}\hat{s}_{i}$ |  | (A5) |'
- en: Appendix D Attention Sparsity Analysis
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 注意力稀疏性分析
- en: 'In order to understand how to approximate the attention calculation of [Equation 1](#S2.E1
    "In 2 Attention Memory Transfer ‣ SparQ Attention: Bandwidth-Efficient LLM Inference"),
    we analysed the intermediate scores vector (softmax output). We took $20$b model,
    capturing the $\bm{q}$ matrix from every layer and attention head.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解如何近似[方程1](#S2.E1 "在2 Attention Memory Transfer ‣ SparQ Attention: Bandwidth-Efficient
    LLM Inference")的注意力计算，我们分析了中间得分向量（softmax 输出）。我们使用了$20$b模型，从每一层和每个注意力头中提取$\bm{q}$矩阵。'
- en: 'To test if the scores are sufficiently sparse to facilitate dropping positions
    within $\bm{V}$ scores, termed coverage, in [Figure A2](#A4.F2 "In Appendix D
    Attention Sparsity Analysis ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").
    If the attention scores were flat, we would expect coverage $\approx k/S\approx
    0.03$, implying a very sparse distribution. We note from [Figure A2b](#A4.F2.sf2
    "In Figure A2 ‣ Appendix D Attention Sparsity Analysis ‣ SparQ Attention: Bandwidth-Efficient
    LLM Inference") that coverage does vary per-head and per-layer, for example earlier
    layers have “flatter” heads, but even these heads show coverage <math id="A4.p2.5.m5.1"
    class="ltx_Math" alttext="></math>.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '为了测试得分是否足够稀疏，以便在$\bm{V}$得分中丢弃位置，这称为覆盖率，在[图 A2](#A4.F2 "在附录 D 注意力稀疏性分析 ‣ SparQ
    Attention: Bandwidth-Efficient LLM Inference")中进行。如果注意力得分是平坦的，我们会期望覆盖率 $\approx
    k/S\approx 0.03$，这意味着非常稀疏的分布。我们从[图 A2b](#A4.F2.sf2 "在图 A2 ‣ 附录 D 注意力稀疏性分析 ‣ SparQ
    Attention: Bandwidth-Efficient LLM Inference")中注意到，覆盖率在每个头和每层之间确实有所不同，例如，较早的层具有“更平坦”的头，但即便这些头也显示出覆盖率
    <math id="A4.p2.5.m5.1" class="ltx_Math" alttext="></math>。'
- en: '![Refer to caption](img/d8aa94c7736ec94b3035ddd125506c96.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d8aa94c7736ec94b3035ddd125506c96.png)'
- en: (a) Coverage over (example, layer, head)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 按（示例，层，头）的覆盖率
- en: '![Refer to caption](img/54f4c3b9b0c2d70d20fc8e2d6a86cfe1.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54f4c3b9b0c2d70d20fc8e2d6a86cfe1.png)'
- en: (b) Coverage by (layer, head)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 按（层，头）的覆盖率
- en: 'Figure A2: Coverage of the softmax output for the top $32$ (varying across
    examples). Calculated for the first token generated in answer to a SQuAD prompt,
    using Pythia-$1.4$ examples.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A2：top $32$ 的 softmax 输出覆盖率（在示例之间变化）。计算为回答 SQuAD 提示生成的第一个 token，使用 Pythia-$1.4$
    示例。
- en: '![Refer to caption](img/c4be75b79bc93a1585d3a33f1f01fed2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4be75b79bc93a1585d3a33f1f01fed2.png)'
- en: (a) Attention approximation top-$k$ overlap, over (batch, layer, head)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力近似 top-$k$ 重叠，按（批次，层，头）
- en: '![Refer to caption](img/ca229b1727d8e8f7ec2d5666feb1aa40.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca229b1727d8e8f7ec2d5666feb1aa40.png)'
- en: (b) Attention approximation top-$k$ overlap, by (layer, head)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力近似 top-$k$ 重叠，按（层，头）
- en: 'Figure A3: Proportion of the top-$k$ absolute components of $\bm{q}$ components.
    Note $d_{h}=128$. Calculated for the first token generated to answer a SQuAD prompt,
    using Pythia-$1.4$ examples.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A3：$\bm{q}$ 组件的 top-$k$ 绝对分量的比例。注意 $d_{h}=128$。计算为回答 SQuAD 提示生成的第一个 token，使用
    Pythia-$1.4$ 示例。
- en: It is useful to drop positions in $\bm{V}$ is needed to calculate these scores.
    We propose approximating these scores using a subset of the components of $\bm{K}$
    positions in the approximated and true scores. If overlap is high, we can use
    the approximation to avoid downloading the whole $\bm{K}$ for all positions, then
    all components of $\bm{K}$ for some positions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在$\bm{V}$中丢弃位置对计算这些得分是有用的。我们建议使用$\bm{K}$中位置的组件子集来近似这些得分。如果重叠度较高，我们可以使用近似值来避免下载所有位置的整个$\bm{K}$，然后仅对某些位置下载$\bm{K}$的所有组件。
- en: Our hypothesis is that the $r$ are most useful to predicting the score, $\bm{q}\bm{K}^{\top}$,
    but that some later layers are harder to predict. Using the top-$r$ baseline considerably.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设是$r$对于预测得分$\bm{q}\bm{K}^{\top}$最有用，但一些较晚的层更难预测。使用 top-$r$ 基线显著。
- en: Appendix E Practical Implementation Considerations
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 实际实现注意事项
- en: One limitation of our theoretical model of memory transfers is that it does
    not account for the granularity of memory access. Since the $\bm{K}$ twice, once
    in $S$-major format. This increases KV cache memory usage by $50\%$ twice. This
    extra write is non-contiguous, but small, so should not form a bottleneck.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理论模型的一个限制是没有考虑内存访问的粒度。由于 $\bm{K}$ 两次，第一次是 $S$-major 格式。这会使 KV 缓存内存使用量增加 $50\%$
    两次。这个额外的写入是非连续的，但很小，因此不应形成瓶颈。
- en: Appendix F Methodology
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 方法论
- en: 'We provide a comprehensive set of hyperparameters for reference in [Table A1](#A6.T1
    "In Appendix F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").
    Typical compression ratios for settings of $(r,k)$ are given in [Table A2](#A6.T2
    "In Appendix F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[表 A1](#A6.T1 "在附录 F 方法论 ‣ SparQ Attention: 带宽高效的 LLM 推理")中提供了一组全面的超参数供参考。典型的压缩比对于$(r,k)$的设置见[表
    A2](#A6.T2 "在附录 F 方法论 ‣ SparQ Attention: 带宽高效的 LLM 推理")。'
- en: We use our own implementation of H[2]O [Zhang et al., [2023](#bib.bib32)], which
    differs from the author’s implementation in that it uses a fixed cache size $k$-shot,
    with Pythia-$1.4$, $l=64$ of $200$ (the dense baseline achieved $74$ times that
    either output differed from dense, $41$-character prefix match between our implementation
    and theirs. The fact that the two implementations often generate the same errors
    (despite minor implementation differences) reassures us that our results should
    be a fair representation of $\mathrm{H_{2}O}$.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了我们自己实现的 H[2]O [Zhang et al., [2023](#bib.bib32)]，与作者的实现不同的是，它使用了固定的缓存大小
    $k$-shot，与 Pythia-$1.4$, $l=64$ 的 $200$（密集基线达到了 $74$ 倍，其输出与密集型有所不同，$41$ 字符前缀匹配在我们的实现和他们之间。两个实现经常产生相同的错误（尽管有些微的实现差异），这让我们确信我们的结果应该公平地代表
    $\mathrm{H_{2}O}$。
- en: '| Group | Hyperparameter | Value or range |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 超参数 | 值或范围 |'
- en: '| Dense model | Family | Llama $2$B), Pythia ($6.9$B, $1.4$B) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 密集模型 | 家族 | Llama $2$B), Pythia ($6.9$B, $1.4$B) |'
- en: '| $d_{h}$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| $d_{h}$ |'
- en: '| Max $S$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 最大 $S$ |'
- en: '| Tasks | QA (#samples) | SQuAD $1$) TriviaQA $0$) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | QA (#样本) | SQuAD $1$) TriviaQA $0$) |'
- en: '| Summarisation (#samples) | CNN/DailyMail $0$) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 摘要（#样本） | CNN/DailyMail $0$) |'
- en: '| Language Modelling (#samples) | WikiText-$103$) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模（#样本） | WikiText-$103$) |'
- en: '| Artificial (#samples) | Repetition ($1000$) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 人工（#样本） | 重复 ($1000$) |'
- en: '| Baselines | Eviction | keep $(k-l)$ and the most recent $l=k/4$'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '| 基线 | 驱逐 | 保留 $(k-l)$ 和最新的 $l=k/4$'
- en: $k\in\{128,192,256,384,512,768\}$ |
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: $k\in\{128,192,256,384,512,768\}$ |
- en: '| Local | take the first $16$ $k\in\{128,192,256,384,512,768\}$ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 局部 | 取前 $16$ $k\in\{128,192,256,384,512,768\}$ |'
- en: '| SparQ | Rank $r$ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| SparQ | 排名 $r$ |'
- en: '| Number of values $k$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 值的数量 $k$ |'
- en: '| Local window $l$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 局部窗口 $l$ |'
- en: 'Table A1: Experiment hyperparameters'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A1: 实验超参数'
- en: '| Method | $r$ | Compression |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $r$ | 压缩 |'
- en: '| SparQ Attention | 16 | 64 | 0.10-0.12 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SparQ Attention | 16 | 64 | 0.10-0.12 |'
- en: '| 128 | 0.13-0.17 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.13-0.17 |'
- en: '| 32 | 64 | 0.16-0.18 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64 | 0.16-0.18 |'
- en: '| 128 | 0.19-0.23 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.19-0.23 |'
- en: '| 64 | 64 | 0.28-0.30 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 64 | 0.28-0.30 |'
- en: '| 128 | 0.31-0.36 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.31-0.36 |'
- en: '| H[2]O | – | 128 | 0.07-0.11 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | – | 128 | 0.07-0.11 |'
- en: '| 192 | 0.10-0.17 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.10-0.17 |'
- en: '| 256 | 0.13-0.22 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 0.13-0.22 |'
- en: '| 384 | 0.20-0.33 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 384 | 0.20-0.33 |'
- en: '| 512 | 0.26-0.43 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 0.26-0.43 |'
- en: '| 768 | 0.39-0.65 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 768 | 0.39-0.65 |'
- en: 'Table A2: Range of compression ratios for different settings of $(r,k)$, for
    Llama 2 7B and Pythia 6.9B. The compression ratio achieved varies across models
    and tasks, based on the sequence length and head size.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A2: 不同$(r,k)$设置的压缩比范围，适用于 Llama 2 7B 和 Pythia 6.9B。实现的压缩比因模型和任务而异，取决于序列长度和头大小。'
- en: F.1 Examples
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 示例
- en: We illustrate the task setup with a single example per task, showing the prompt
    formatting and a cherry-picked example. In each case, we show outputs from a dense
    Llama 2 model, SparQ ($r=16,k=64$) Attention. Where “…” appears, we have truncated
    the line of text for brevity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过每个任务的单个示例来说明任务设置，展示了提示格式和精心挑选的示例。在每种情况下，我们展示了来自密集型 Llama 2 模型的输出，SparQ ($r=16,k=64$)
    Attention。出现“...”的地方，我们为了简洁起见已截断文本行。
- en: F.1.1 Question Answering (SQuAD 1-shot)
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.1 问答（SQuAD 1-shot）
- en: '[PRE0]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: F.1.2 Question Answering (TriviaQA 0-shot)
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.2 问答（TriviaQA 0-shot）
- en: '[PRE1]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that for Pythia, the prompt “Single-word answer:” was used in place of
    “Answer:”, as this helped prevent the model from restating the question in the
    answer (often qualitatively correct, but not a regex match).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于 Pythia，提示“单词回答:”被用来代替“回答:”，因为这有助于防止模型在答案中重复问题（尽管通常是定性的正确，但不符合正则表达式匹配）。
- en: F.1.3 Summarisation (CNN/DailyMail)
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.3 摘要（CNN/每日邮报）
- en: '[PRE2]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: F.1.4 Repetition (Shakespeare)
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.4 重复（莎士比亚）
- en: '[PRE3]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: F.1.5 Language Modelling (WikiText-103)
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.5 语言建模（WikiText-103）
- en: '[PRE4]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since the flood a new , roughly 100 @-@ foot @-@ tall ( 30 m ) bridge has been
    constructed on CR 510 , ensuring that any future flooding on the Dead River would
    not necessitate closure of the modern bridge . This structure was built at that
    height in order to benefit commercial interests in the area , as well as to provide
    the area with reliable public and emergency access . Rio Tinto Group , the British
    @-@ based parent company of Kennecott Minerals received permission
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 自从洪水之后，在CR 510上建造了一座新的、约100英尺（30米）高的桥梁，确保未来任何在死亡河上的洪水都不会导致现代桥梁关闭。这座结构的建设高度考虑到了该地区的商业利益，并为该地区提供了可靠的公共和紧急通道。英国总部的Rio
    Tinto Group，作为Kennecott Minerals的母公司，获得了许可。
- en: '[PRE5]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
