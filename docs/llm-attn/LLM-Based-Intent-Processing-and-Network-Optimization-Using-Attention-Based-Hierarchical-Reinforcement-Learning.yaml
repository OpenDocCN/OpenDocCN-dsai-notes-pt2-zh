- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06059](https://ar5iv.labs.arxiv.org/html/2406.06059)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Md Arafat Habib¹, Pedro Enrique Iturria Rivera¹, Yigit Ozcan², Medhat Elsayed²,
  prefs: []
  type: TYPE_NORMAL
- en: Majid Bavand², Raimundus Gaigalas² and Melike Erol-Kantarci¹, ²Ericsson Inc.,
    Ottawa, Canada Emails:{mhabi050, pitur008, melike.erolkantarci}@uottawa.ca,
  prefs: []
  type: TYPE_NORMAL
- en: '{yigit.ozcan, medhat.elsayed, majid.bavand, raimundas.gaigalas }@ericsson.com
    ¹School of Electrical Engineering and Computer Science, University of Ottawa,
    Ottawa, Canada'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Intent-based network automation is a promising tool to enable easier network
    management however certain challenges need to be effectively addressed. These
    are: 1) processing intents, i.e., identification of logic and necessary parameters
    to fulfill an intent, 2) validating an intent to align it with current network
    status, and 3) satisfying intents via network optimizing functions like xApps
    and rApps in O-RAN. This paper addresses these points via a three-fold strategy
    to introduce intent-based automation for O-RAN. First, intents are processed via
    a lightweight Large Language Model (LLM). Secondly, once an intent is processed,
    it is validated against future incoming traffic volume profiles (high or low).
    Finally, a series of network optimization applications (rApps and xApps) have
    been developed. With their machine learning-based functionalities, they can improve
    certain key performance indicators such as throughput, delay, and energy efficiency.
    In this final stage, using an attention-based hierarchical reinforcement learning
    algorithm, these applications are optimally initiated to satisfy the intent of
    an operator. Our simulations show that the proposed method can achieve at least
    $12\%$ increase in energy efficiency, and $26.5\%$ decrease in network delay compared
    to the baseline algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Intent-based network automation, Attention-based hierarchical reinforcement
    learning, network optimization
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achieving intent-based automation can be highly challenging in modern-day 5G
    paradigms like Open Radio Access Network (O-RAN). Instead of tweaking specific
    parameters through vendor-specified interfaces, Mobile Network Operators (MNOs)
    can now express their intentions in natural language. This approach introduces
    a huge challenge in the telecommunication industry since intents can vary widely
    based on network conditions, customer demands, and industry-specific terminology
    [[1](#bib.bib1)]. Therefore, a contextual understanding of the intents is vital
    for accurate intent processing. Large Language Models (LLMs) excel at capturing
    such context and understanding the underlying meaning of sentences (intents) within
    a broader spectrum. For instance, Wang et al. propose a Transformer model based
    on an LLM where various forms of network data (packet size, time stamps, and end-to-end
    delay) are integrated into a unified feature space for refining network control
    policies to align with an MNO’s intent [[2](#bib.bib2)]. However, modern-day communication
    systems require continuous monitoring of the Key Performance Indicators (KPIs)
    to serve users having different kinds of Quality-of-Service (QoS) requirements.
    Existing LLM-based approaches like [[2](#bib.bib2)] are not specially tailored
    to extract crucial information associated with what performance metric the operator
    is aiming to improve and by how much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Processing intents expressed in natural language by the MNO is crucial for
    deriving policy for network performance optimization. Accurate processing of intents
    can provide MNOs with feedback on how their intents might impact crucial performance
    metrics like energy efficiency or throughput. For example, an intent: “Increase
    energy efficiency by $30\%$” during peak hours can degrade performance due to
    high traffic volume. This kind of scenario requires a sophisticated method of
    intent validation using contemporary network status to ensure a seamless user
    experience without compromising any QoS.'
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of paradigm-shifting concepts like O-RAN paves the way for
    deploying network-optimizing applications known as xApps and rApps in RAN Intelligent
    Controllers (RICs). These apps can be initiated based on MNO’s processed intents.
    Based on this idea, in [[1](#bib.bib1)], intents are converted into goals for
    a Hierarchical Reinforcement Learning (HRL) algorithm to initiate xApps or rApps
    for intent fulfillment. This decision-making process is computationally intensive,
    given the vast number of potential rApp or xApp combinations that can optimize
    performance. An AI agent, therefore, must not only choose from numerous options
    but also identify the most appropriate combination of network-optimizing apps
    that align with an MNO’s intent.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we propose a three-fold strategy to perform intent processing,
    validation, and RIC application initiation based on intents. First, we process
    intents using Bidirectional Encoder Representations from Transformers (BERT) which
    is an LLM model that can be fine-tuned for specific tasks like intent extraction,
    which allows it to learn task-specific patterns of the data [[3](#bib.bib3)].
    In the next step, intent validation is performed using a Transformer-based time
    series predictor for upcoming traffic volume [[4](#bib.bib4)]. This ensures that
    the intended adjustments to the network are not only theoretically sound but also
    practical and feasible given expected future traffic patterns. Lastly, to overcome
    the computational burden of selecting the appropriate combination of xApps and
    rApps from a high number of possible choices, we utilize an attention-based HRL
    framework that takes the processed intents (magnitude of the intended performance
    metric by an MNO) as goals. By filtering out only the feasible options [[5](#bib.bib5)],
    via an attention mechanism, the system can significantly reduce the computational
    burden
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the previous works, the proposed scheme provides complete intent-based
    automation from an O-RAN perspective via intent processing, validation, and HRL-based
    performance optimization using xApps and rApps. The simulation results demonstrate
    that the proposed scheme yields significant improvements compared to two baseline
    approaches: HRL without attention and intent validation, and a single-application
    scenario developed using Deep RL (DRL). The proposed approach surpasses the HRL
    baseline in terms of throughput, delay, and energy efficiency by $12.02\%$, and
    $17.1\%$, $48.6\%$ respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The organization of this manuscript is outlined as follows: Section [II](#S2
    "II Related work ‣ LLM-Based Intent Processing and Network Optimization Using
    Attention-Based Hierarchical Reinforcement Learning") offers a short review of
    the existing literature associated with this research. This is succeeded by Section
    [III](#S3 "III System Model ‣ LLM-Based Intent Processing and Network Optimization
    Using Attention-Based Hierarchical Reinforcement Learning"), which elaborates
    on the system model in depth. Section [IV](#S4 "IV Proposed Intent Processing
    and Network Optimization Scheme ‣ LLM-Based Intent Processing and Network Optimization
    Using Attention-Based Hierarchical Reinforcement Learning") is specifically devoted
    to elaborately presenting our proposed approach. Performance assessment and a
    comparative examination of our approach against the baseline algorithms are detailed
    in Section [V](#S5 "V Performance Evaluation ‣ LLM-Based Intent Processing and
    Network Optimization Using Attention-Based Hierarchical Reinforcement Learning").
    Lastly, Section [VI](#S6 "VI Conclusions ‣ LLM-Based Intent Processing and Network
    Optimization Using Attention-Based Hierarchical Reinforcement Learning") presents
    the concluding remarks of the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: II Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though using LLMs for intent processing for network optimization is a relatively
    new concept, a few works have been found in the literature. Wang et al.’s development
    of a transformer model based on an LLM is one such example, where multi-modal
    representation learning is employed to integrate various forms of network data
    into a unified feature space [[2](#bib.bib2)]. Similarly, Kristina et al. have
    introduced a pipeline that utilizes an LLM to process intents into structured,
    policy-based abstractions, linking them to APIs for execution [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: To manage RIC applications based on operator intents, an approach based on HRL
    is proposed in [[7](#bib.bib7)] where intents are processed into goals for the
    control algorithm to optimize performance. Polese et al. propose a framework to
    collect control requests (intents) from an MNO and select the optimal Machine
    Learning (ML) models to achieve the operator’s goals to avoid conflicts [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: Compared with existing literature, the main contribution of this work lies in
    deploying an LLM to interpret MNO’s intent and extract specific parameters for
    further network performance optimization. Processed intents are validated against
    future wireless traffic conditions. Intents lead HRL agents to initiate and orchestrate
    multiple network functions with an efficient exploration of action space via an
    attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: III System Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A cellular system based on O-RAN architecture employing downlink orthogonal
    frequency division multiplexing is considered in this work. There are $B$ users.
    Within the macro cell’s coverage area, multiple small cells are deployed. The
    system supports $K$, can facilitate various technologies like LTE, 5G.
  prefs: []
  type: TYPE_NORMAL
- en: The system model, depicted in Figure [1](#S3.F1 "Figure 1 ‣ III System Model
    ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning"), incorporates RAN Intelligent Controllers (RICs), comprising
    Non-Real-Time (non-RT)-RIC and Near-Real-Time (Near-RT)-RIC. These controllers
    host both rApps and xApps, focusing on control and optimization tasks across different
    time scales. We deploy BSs capable of switching bands from 3.5 GHz to mmWave frequencies.
    This deployment enables us to cater to high-throughput traffic by employing intelligent
    beamforming techniques. An xApp can be developed to regulate power based on UE
    location, utilizing minimal transmission power for enhanced energy efficiency.
    The system facilitates analog beamforming, with each BS employing a uniform linear
    array of $\mu$, from which beamforming vectors are selected [[9](#bib.bib9)].
    Each BS $b$, where $P$ represents the set of candidate transmit powers. The energy
    consumption model for the BS is obtained from [[10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/adc9b92a310bbaad124521afeab6fc72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: System model designed with macro cell and small cells based on O-RAN
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the system and network model discussed so far, we introduce multiple
    xApps and rApps. There are two sets of these RIC applications. The first set consists
    of intent translation, validation, and meta-controller rApps and the second set
    consists of traffic steering, beamforming, cell sleeping, power allocation, and
    handover management applications that can be directly controlled by the first
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define $\Psi$ represent the subset of $\Psi$ as the set of potential
    KPIs that an application might improve, and $Q_{s}$ as the set of QoS requirements
    that must be met by the system. Given these definitions, the challenge of orchestrating
    RIC applications based on operator intent can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\max\sum_{\rho\in\varrho}\sum_{q_{s}\in Q_{s}}(C_{\rho}-\varpi\gamma_{q_{s}}),\quad\quad\quad\\
    \text{s.t.}\quad\forall(\Psi)\exists(\sigma):\zeta(O)=1,\quad\quad\quad\end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $C$ is the penalty parameter for QoS requirement violation, and $\gamma_{q_{s}}$
    is the proposition that “An xApp can improve a performance metric”, which is either
    ‘0’ or ‘1’.
  prefs: []
  type: TYPE_NORMAL
- en: IV Proposed Intent Processing and Network Optimization Scheme
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, first, we discuss processing intents using an LLM followed
    by intent validation. The last part of this section consists of an HRL-based attention
    mechanism for RIC application initiation and orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Intent Processing using LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intent-based networking simplifies network and administrative tasks by automating
    system operations through the comprehension and execution of operator-defined
    intents, such as “Increase overall energy efficiency by 10%”, “Boost system throughput
    by 15%”, or “Reduce network delay by 13%”. These intents focus on improving metrics
    like throughput, energy efficiency, and network delay. We developed a dataset
    containing various intents related to 5G and O-RAN performance metrics and annotated
    it with details like action direction (increase or decrease), change percentage,
    and targeted metric.
  prefs: []
  type: TYPE_NORMAL
- en: We utilized a lightweight version of BERT, known as ALBERT, for efficient intent
    processing. This model is particularly suitable for real-time inference in O-RAN
    scenarios due to its balance of performance and computational efficiency [[11](#bib.bib11)].
    Using pre-trained ALBERT, we trained our system to predict the specified elements
    from intent expressions. This intent processing functionality is presented as
    User Interface (UI) and intent translation rApp in Fig. [2](#S4.F2 "Figure 2 ‣
    IV-B Intent Validation Using Transformer ‣ IV Proposed Intent Processing and Network
    Optimization Scheme ‣ LLM-Based Intent Processing and Network Optimization Using
    Attention-Based Hierarchical Reinforcement Learning"). The job of this rApp is
    to work as a UI for an MNO and process the intents with crucial information to
    feed the intent validation rApp and the meta-controller rApp.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Intent Validation Using Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second step of this work consists of intent validation. QoS parameters
    deviate from the originally defined QoS metrics over time due to performance degradation
    in the wireless networks causing QoS drifts. By deviation, we mean when a QoS
    parameter is having lesser value than the required. For a specific traffic class,
    $k$ and associated performance metrics $C$, we can formulate QoS drift as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}\min\sum_{q_{j}\in Q_{s}}\sum_{c\in C}\{D_{QoS}-A_{QoS}\},\quad\quad\\
    \text{s.t.}\quad A_{QoS}\leq D_{QoS}\quad\text{and}\quad A_{QoS}></math> |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: In eq.[2](#S4.E2 $$ based on eq. [2](#S4.E2 "In IV-B Intent Validation Using
    Transformer ‣ IV Proposed Intent Processing and Network Optimization Scheme ‣
    LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning").
  prefs: []
  type: TYPE_NORMAL
- en: To perform intent validation, we utilize one of our previous works [[4](#bib.bib4)]
    where traffic data are collected so that it can be fed to the transformer-based
    time series predictor for future traffic volume prediction. The traffic predictor
    resides in the non-RT-RIC as an rApp (named as intent validation rApp, as in Fig
    [2](#S4.F2 "Figure 2 ‣ IV-B Intent Validation Using Transformer ‣ IV Proposed
    Intent Processing and Network Optimization Scheme ‣ LLM-Based Intent Processing
    and Network Optimization Using Attention-Based Hierarchical Reinforcement Learning").).
    The forecast for traffic volume is performed for successive time intervals, with
    two distinct thresholds derived from historical data. The higher threshold, denoted
    as $Th_{p}$, RIC applications such as traffic steering and power allocation can
    be activated to enhance or sustain high throughput, depending on the intent of
    an MNO. The lower threshold, $Th_{t}$, is established based on low traffic volume.
    In the event that the anticipated traffic decreases to less than this threshold,
    an intent on increasing energy efficiency can be validated leading to the initiation
    of energy-saving rApp like cell sleeping due to the expected lower traffic levels.
    The algorithmic process of predictive traffic validation is presented in Algorithm
    1 for the overall representation of the process. Note that the intent validation
    can also be performed by predicting other crucial network parameters or values
    like congestion levels, device connectivity, status, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Intent Validation Based on Traffic Prediction
  prefs: []
  type: TYPE_NORMAL
- en: 1:Predict future traffic $T_{p}$ using Autoformer2:Extract intent ($I_{MNO}$,
    and Change Percentage (%) using LLM3:Define traffic classes $K=\{k_{1},k_{2},\ldots,k_{i}\}$5:if ($T_{p}></math>
    <math id=$7:     if $I_{MNO}$ then8:         for each QoS parameter $Q_{s_{j}}$’s
    profile do9:              Calculate QoS drift $I_{Q}(Q_{s_{j}})$ then11:                  Intent
    invalid12:                  break13:              end if14:         end for15:         if $I_{Q}$
    is 0 $||$ then16:              Validate intent17:         end if18:     end if19:else20:     Re-calculate
    $Th_{p}$21:end if22:return Validation results
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d51f9feb44a94654eedffabdd9e62ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Three-step methodology for intent processing, validation, and performance
    optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Network Optimization Using Attention-based HRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the intent is validated, it allows us to go to the final stage of this
    work which is optimizing the network performance based on the operator intent
    using an RL algorithm. To do that, we opt for hierarchical Deep-Q-Network (h-DQN),
    an HRL algorithm having a two-level hierarchy with a meta-controller on top [[12](#bib.bib12)].
    Meta-controller can take in a network state (e.g., traffic class) and a goal (desired
    change of a performance metric extracted from an intent) to achieve and the controller
    in the lower level takes the action of choosing an RIC application or combinations
    of them based on the state and the goal. The meta-controller is designed as an
    rApp in the non-RT-RIC (see Fig. [2](#S4.F2 "Figure 2 ‣ IV-B Intent Validation
    Using Transformer ‣ IV Proposed Intent Processing and Network Optimization Scheme
    ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning")). RIC applications are controlled by the meta-controller
    that can directly optimize system performance are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Traffic Steering xApp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The traffic steering xApp is engineered to concurrently uphold QoS across various
    traffic types by employing a steering mechanism utilizing a DQN. To ensure optimal
    performance, the xApp’s state and reward functions are designed with a focus on
    two KPIs: network latency and overall system throughput. For more detailed insights
    into this xApp, please refer to [[13](#bib.bib13)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Cell Sleeping rApp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The cell sleeping rApp is based on DQN and aims to reduce network power consumption
    by deactivating idle or less busy BSs based on traffic load and queue length (set
    of states). Higher rewards are provided for taking actions that increase energy
    efficiency without overloading the active BSs. Technical details of this DQN-based
    rApp can be found in our previous work [[7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Beamforming xApp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the past two apps, we also employ DQN to develop the beamforming
    xApp where UE coordinates are used as a set of states. The set of actions comprises
    $\alpha(\chi_{n})$. Here, $\chi$ denotes the array steering vector corresponding
    to the direction $\chi_{n}$ element in the codebook. $\delta_{n}$, and ii) the
    ratio of the energy efficiency linked with the base station’s throughput and transmission
    power to the maximum theoretical energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 Power Allocation xApp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The power allocation xApp tries to maximize the total throughput. The set of
    states includes transmission rate and channel state information. We divide the
    transmission power levels into $P_{L}$. The goal is to improve throughput. The
    reward function is formulated to increase the transmission rate of a BS on a Resource
    Block Group (RBG).
  prefs: []
  type: TYPE_NORMAL
- en: IV-C5 Handover decision making xApp for energy saving
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This xApp is specifically tailored with DQN to achieve energy efficiency via
    optimally tailored handover policies. To accommodate the extra processing demands
    and signaling exchanges involved in handover executions at both serving and target
    base stations, we establish a handover energy consumption function to incorporate
    the additional energy overheads:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$E_{u}^{HO}(t)=\begin{cases}E^{HO}(t)&amp;\text{if handover occurs at
    time $t$}\\ 0&amp;\text{otherwise,}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}$$ |  | (3) |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $E^{HO}\geq 0$ at time slot $t$ is expressed as follows: $E_{total}=E_{u,b}^{Tx}+E^{HO}$
    is the transmission energy consumption when a $u$. The objective of this xApp
    is to reduce average energy consumption. More details on this xApp can be found
    in [[14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: At this stage of the work, we define an MDP to solve the problem formulated
    in eq.[1](#S3.E1 "In III-A Problem Formulation ‣ III System Model ‣ LLM-Based
    Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning").
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State and Action: The set of states consists of traffic flow types of different
    users in the network. $S=\{T_{1},..,T_{2},...,T_{3},...,T_{4},..\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intrinsic reward: The intrinsic reward function ($r_{in}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal for the controller: Increased level of a performance metric that can satisfy
    operator intent is passed to the controller as goals. For example, $G=\{TP_{1},TP_{2},...,TP_{n}\}$
    for throughput increasing intents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extrinsic reward: Summation of the intrinsic reward over $\tau$ steps.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The process of selecting RIC applications for network optimization using HRL
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 1: An input is provided by the MNO associated with a performance metric.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Intent is processed and crucial information like Action: “Increase/Decrease”.
    “Which metric” and by how much (in percentage) are extracted using pre-trained
    ALBERT.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Extracted information is passed to the intent validation rApp.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: With a valid intent, a target associated with the performance metrics
    (goals) and feasible action set for the intent are provided to the controller
    in near-RT-RIC by the meta controller rApp in the non-RT-RIC.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 5: The controller selects an RIC application or a combination of them
    to reach the target performance as close as possible.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 6: Selected RIC applications optimize the performance of the network as
    a response to the intent of the operator.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We employ an attention mechanism [[5](#bib.bib5)] to reduce the action space
    for our h-DQN agent, enhancing efficiency and decision-making while expediting
    convergence. A critical component of this process is the Markovian option, $\omega$,
    a termination condition $\beta$. Actions are chosen based on $\pi$ terminates
    the option. Our approach uses hard attention [[5](#bib.bib5)], selecting a specific
    part of the input for focus. When an option $\omega$, the intent completion function
    assesses if the option’s policy $\pi_{\omega}$, and using values of $\epsilon$,
    we can efficiently narrow down the action space with a supervised learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: V Performance Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, first, we introduce the simulation settings followed by the
    simulation results showing the effectiveness and superiority of the proposed method.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Simulation setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d52ffd71d46f0dd5f1c14ac9e8448b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Impact of intent validation on energy efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/887b4f6fef832a4f390d28b0850a923c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Impacts of operator intents on throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/db306c90a18df7dea896078e95f4fd43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Performance analysis of the proposed method: (a) energy efficiency,
    (b) network delay, and (c) throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: The simulation environment in this work consists of one macro-cell surrounded
    by a couple of densely deployed small cells having a multi-RAT environment with
    60 users in total. For 5G NR, the bandwidth is set at $60$ GHz and $30$ KHz, and
    maximum transmission power of $43$ MHz bandwidth, $800$ KHz subcarrier spacing,
    and a maximum transmission power of $38$, $40$, and $0.5$, $120$, and $32$ bytes,
    corresponding to the specified traffic classes. These values are fixed based on
    [[16](#bib.bib16)]. We refer to traffic steering, cell sleeping, beamforming,
    handover management, and power allocation applications as xApp1, rApp2, xApp3,
    xApp4, and xApp5 respectively in the next section to present our results.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Simulation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first simulation result is associated with intent validation. When there
    is no intent validation, conflicting intents from an operator can severely degrade
    performance. Fig. [3](#S5.F3 "Figure 3 ‣ V-A Simulation setup ‣ V Performance
    Evaluation ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based
    Hierarchical Reinforcement Learning") presents such an example where we have a
    low traffic load of 5Mbps. At $2200^{th}$ time slot, there is a sharp decrease
    in energy efficiency. This is because of the unwanted intent of increasing throughput
    even though the cell sleeping application is active due to low traffic demand.
    An intent of increasing throughput by an MNO would result in activating more cells
    even though not necessary and decrease overall energy efficiency. Similar impacts
    can be observed for other KPIs like delay, and throughput also.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Fig. [4](#S5.F4 "Figure 4 ‣ V-A Simulation setup ‣ V Performance
    Evaluation ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based
    Hierarchical Reinforcement Learning"), the operator’s goal to “increase throughput”
    leads to specific RIC application selections. For instance, aiming for a $5\%$
    increase involves both xApp1 and xApp3\. Conversely, reducing power consumption
    significantly decreases throughput around the $1250^{th}$ increase, xApp1, xApp3,
    and xApp4 together produce a sharp throughput rise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed method’s effectiveness is compared with two baselines: an HRL-based
    RIC application initiation without an attention mechanism or intent validation,
    and a single DRL-based application scenario. The proposed method employs a hard
    attention mechanism, focusing on the subset of RIC applications most relevant
    for the current traffic type and operator’s intent. This focus reduces conflicting
    actions and shows superior performance over the HRL baseline (improvements of
    $12.02\%$ in delay, and $17.1\%$, $48.6\%$ in the same metrics), as observed in
    Fig. [5](#S5.F5 "Figure 5 ‣ V-A Simulation setup ‣ V Performance Evaluation ‣
    LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, first, we process intents using an LLM and extract crucial information
    regarding which performance metric to optimize and by how much. Next, a Transformer-based
    intent validation technique has been used to rule out intents that conflict with
    the contemporary network state to avoid performance degradation. Lastly, we use
    an attention-based HRL to initiate and orchestrate RIC applications for performance
    optimization. The proposed method outperforms the HRL baseline in throughput,
    delay, and energy efficiency by $12.02\%$, and $17.1\%$, $48.6\%$ respectively.
    In the future, we plan to perform highly complex intent translation from the MNO
    and perform intent validation predicting other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has been supported by MITACS, Ericsson Canada, and NSERC Canada Research
    Chairs program.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] ETSI TS 128 312: “Management and Orchestration; Intent Driven Management
    Services for Mobile Networks”. [Online]. Available: https://www.etsi.org/deliver/etsi_ts/128300_128399/128312/17.00.01_60/ts_128312v170001p.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Wang, L. Zhang, Y. Yang, Z. Zhuang, Q. Qi, H. Sun, L. Lu, J. Feng, and
    J. Liao, “Network Meets ChatGPT: Intent Autonomous Management, Control and Operation,”
    *Journal of Communications and Information Networks*, vol. 8, no. 3, pp. 239–255,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding,” *CoRR*, vol. abs/1810.04805,
    2018\. [Online]. Available: http://arxiv.org/abs/1810.04805'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. A. Habib, P. E. Iturria-Rivera, Y. Ozcan, M. Elsayed, M. Bavand, R. Gaigalas,
    and M. Erol-Kantarci, “Transformer-Based Wireless Traffic Prediction and Network
    Optimization in O-RAN,” *arXiv preprint arXiv:2403.10808*, Mar. 2024\. [Online].
    Available: https://arxiv.org/abs/2403.10808'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. C. Nica, K. Khetarpal, and D. Precup, “The paradox of choice: Using
    attention in hierarchical reinforcement learning,” *CoRR*, vol. abs/2201.09653,
    2022\. [Online]. Available: https://arxiv.org/abs/2201.09653'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] K. Dzeparoska, J. Lin, A. Tizghadam, and A. Leon-Garcia, “LLM-Based Policy
    Generation for Intent-Based Management of Applications,” in *2023 19th International
    Conference on Network and Service Management (CNSM)*, 2023, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. A. Habib, H. Zhou, P. E. Iturria-Rivera, M. Elsayed, M. Bavand, R. Gaigalas,
    Y. Ozcan, and M. Erol-Kantarci, “Intent-driven Intelligent Control and Orchestration
    in O-RAN Via Hierarchical Reinforcement Learning,” in *2023 IEEE 20th International
    Conference on Mobile Ad Hoc and Smart Systems (MASS)*, 2023, pp. 55–61.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. D’Oro, L. Bonati, M. Polese, and T. Melodia, “OrchestRAN: Network Automation
    Through Orchestrated Intelligence in the Open RAN,” in *IEEE INFOCOM 2022 - IEEE
    Conference on Computer Communications*, 2022, pp. 270–279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] F. B. Mismar, B. L. Evans, and A. Alkhateeb, “Deep Reinforcement Learning
    for 5G Networks: Joint Beamforming, Power Control, and Interference Coordination,”
    *IEEE Transactions on Communications*, vol. 68, no. 3, pp. 1581–1592, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] P. Ren and M. Tao, “A Decentralized Sleep Mechanism in Heterogeneous Cellular
    Networks with QoS Constraints,” *IEEE Wireless Communications Letters*, vol. 3,
    no. 5, pp. 509–512, Oct. 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT:
    A lite BERT for self-supervised learning of language representations,” *CoRR*,
    vol. abs/1909.11942, 2019\. [Online]. Available: http://arxiv.org/abs/1909.11942'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. B. Tenenbaum, “Hierarchical
    Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,”
    *CoRR*, vol. abs/1604.06057, 2016\. [Online]. Available: http://arxiv.org/abs/1604.06057'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. A. Habib, H. Zhou, P. E. Iturria-Rivera, M. Elsayed, M. Bavand, R. Gaigalas,
    S. Furr, and M. Erol-Kantarci, “Traffic Steering for 5G Multi-RAT Deployments
    using Deep Reinforcement Learning,” in *2023 IEEE 20th Consumer Communications
    & Networking Conference*, 2023, pp. 164–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. Song, S. H. Lim, and S.-W. Jeon, “Handover Decision Making for Dense
    HetNets: A Reinforcement Learning Approach,” *IEEE Access*, vol. 11, pp. 24 737–24 751,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] E. Dahlman, S. Parkvall, and J. Skold, *4G, LTE-Advanced Pro and The Road
    to 5G, Third Edition*, 3rd ed.   USA: Academic Press, Inc., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-Munoz,
    and J. M. Lopez-Soler, “A Survey on 5G Usage Scenarios and Traffic Models,” *IEEE
    Communications Surveys & Tutorials*, vol. 22, no. 2, pp. 905–929, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
