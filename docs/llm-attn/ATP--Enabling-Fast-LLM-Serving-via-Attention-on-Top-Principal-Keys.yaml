- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:02:42'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02352](https://ar5iv.labs.arxiv.org/html/2403.02352)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yue Niu
  prefs: []
  type: TYPE_NORMAL
- en: University of Southern California
  prefs: []
  type: TYPE_NORMAL
- en: Los Angeles, CA, US
  prefs: []
  type: TYPE_NORMAL
- en: yueniu@usc.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Saurav Prakash'
  prefs: []
  type: TYPE_NORMAL
- en: University of Illinois Urbana-Champaign
  prefs: []
  type: TYPE_NORMAL
- en: Urbana, IL, US
  prefs: []
  type: TYPE_NORMAL
- en: sauravp2@illinois.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Salman Avestimehr'
  prefs: []
  type: TYPE_NORMAL
- en: University of Southern California
  prefs: []
  type: TYPE_NORMAL
- en: Los Angeles, CA, US
  prefs: []
  type: TYPE_NORMAL
- en: avestime@usc.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We propose a new attention mechanism with linear complexity, ATP, that fixates
    Attention on Top Principal keys, rather than on each individual token. Particularly,
    ATP is driven by an important observation that input sequences are typically low-rank,
    i.e., input sequences can be represented by a few principal bases. Therefore,
    instead of directly iterating over all the input tokens, ATP transforms inputs
    into an orthogonal space and computes attention only on the top principal bases
    (keys). Owing to the observed low-rank structure in input sequences, ATP is able
    to capture semantic relationships in input sequences with a few principal keys.
    Furthermore, the attention complexity is reduced from *quadratic* to *linear*
    without incurring a noticeable performance drop. ATP further reduces complexity
    for other linear layers with low-rank inputs, leading to more speedup compared
    to prior works that solely target the attention module. Our evaluations on various
    models (e.g., BERT and Llama) demonstrate that ATP achieves comparable accuracy
    with much lower computation and memory complexity than the standard attention
    mechanism. In particular, ATP barely loses accuracy with only $1/2$ principal
    keys, and only incurs around $2\%$ accuracy drops with $1/4$ principal keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys'
  prefs: []
  type: TYPE_NORMAL
- en: Yue Niu University of Southern California Los Angeles, CA, US yueniu@usc.edu
                           Saurav Prakash University of Illinois Urbana-Champaign
    Urbana, IL, US sauravp2@illinois.edu                        Salman Avestimehr
    University of Southern California Los Angeles, CA, US avestime@usc.edu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers with self-attention have become a mainstream model architecture
    in many machine-learning tasks on natural language processing Wolf et al. ([2020](#bib.bib32)),
    and computer vision Khan et al. ([2022](#bib.bib13)); Dosovitskiy et al. ([2020](#bib.bib6)).
    In particular, owing to the attention mechanism, transformers have been demonstrated
    to be more effective in learning semantic relationships from input sequences.
    This drives transformers to become the backbone of current large language models
    (LLMs) like ChatGPT [OpenAI](#bib.bib19) and Copilot [Microsoft](#bib.bib16) .
  prefs: []
  type: TYPE_NORMAL
- en: Despite their remarkable utility in real-world applications, transformers with
    standard self-attention, however, incur *quadratic* complexity in terms of sequence
    length (Vaswani et al., [2017](#bib.bib28)). To be specific, considering an input
    sequence with length $L$ (i.e., $L$ tokens), each attention layer needs $\mathcal{O}(L^{2})$
    computation and memory complexity on attention operations. Such a quadratic degree
    of complexity renders transformers difficult to scale with long input sequences.
    As a result, most LLM services at scale backed by transformers incur significant
    computation and memory footprints, which can only be afforded by large companies
    with sufficient computing power Samsi et al. ([2023](#bib.bib23)). To meet memory
    and computation resource constraints during deployment, some transformer models
    Devlin et al. ([2018](#bib.bib5)); Lan et al. ([2020](#bib.bib14)); Radford et al.
    ([2018](#bib.bib20)); Touvron et al. ([2023](#bib.bib27)) usually come with a
    hard constraint on sequence length. However, in many real-world tasks such as
    question-answering Wang et al. ([2019](#bib.bib30)), text summarization El-Kassas
    et al. ([2021](#bib.bib7)), enabling long sequence length is crucial for capturing
    semantic relationships in a broader context, and improving models’ performance.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, at the core of transformers and LLM services, lightweight self-attention
    mechanisms play a key role in improving model performance with longer sequences,
    as well as computation and memory efficiency in deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Current works on reducing the complexities of transformers can be categorized
    in two ways. The first line of research usually exploits redundancy in query/key/value
    matrices or attention maps, while the second approximates the Softmax-based attention
    with linear complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Along the first line of works, Vyas et al. ([2020](#bib.bib29)) reduces attention
    complexity via clustering queries and only computes attention output for each
    cluster. Its performance hinges on the performance of clustering as well as the
    dimension in queries. On the other hand, Linformer Wang et al. ([2020](#bib.bib31))
    chooses to reduce the number of keys/values via a low-dimensional projection.
    A pre-defined or learnable projection layer is inserted into each attention layer.
    However, such a projection layer lacks a rigorous guarantee to preserve information
    in inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to simply approximating queries, keys, or values, another line of work
    approximates the Softmax-based attention using randomized feature mappingRahimi
    and Recht ([2007](#bib.bib21)). In these works, standard attention is regarded
    as a kernel method, and can be approximated with low-dimensional kernels. For
    instance, Performers (Choromanski et al., [2020](#bib.bib3)) shows that Softmax
    attention can be converted to a Gaussian kernel function. Therefore, self-attention
    can be potentially calculated at a lower dimension with linear complexity, as
    done in a Gaussian kernel function. Following such an idea, several works explore
    different kernels to approximate Softmax attention Katharopoulos et al. ([2020a](#bib.bib11)).
    However, the approximation methods with randomized feature mapping need to trade
    off approximation accuracy and the number of random features. A low approximation
    error needs more random features but increases approximation complexity Choromanski
    et al. ([2020](#bib.bib3)). Furthermore, while the aforementioned works reduce
    the complexity of the attention mechanism to linear, they still have not been
    seen in large-scale language models such as Llama Touvron et al. ([2023](#bib.bib27)).
    One reason is that these methods cannot effectively preserve information when
    performing attention in low dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose a new attention mechanism with linear complexity,
    that maintains model performance with significantly reduced computation and memory
    costs. The new attention mechanism, called ATP, is the first work that adapts
    self-attention with a low-rank structure in input embeddings. In particular, ATP
    first analyzes an input sequence’s structure and obtains orthogonal bases. We
    observe that input sequences usually exhibit a high correlation among tokens,
    with a few orthogonal bases being more important than the rest. Then ATP computes
    attention only on the top principal bases ( we call them *principal keys*), rather
    than iterating over all keys as in the standard attention layer. Owing to the
    low-rank structure in input sequences, the new self-attention mechanism with few
    principal keys/values is sufficient to capture semantic relationships among tokens.
    As a result, compute and memory complexity is reduced from quadratic to linear
    in terms of sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, by exploiting low-rank structure in inputs, not only is the complexity
    of attention reduced, but also the complexity of other linear layers. Hence, ATP
    achieves further computation reductions compared to prior works focusing solely
    on the Softmax operation.
  prefs: []
  type: TYPE_NORMAL
- en: Our evaluations on various models (e.g., BERT and Llama) demonstrate ATP still
    maintains comparable accuracy with small fractional principal keys/values. In
    particular, with only $1/4$ principal keys, ATP achieves accuracy almost as the
    original model. With only $1/4$ principal keys, ATP only incurs around $2\%$ accuracy
    drop on BERT-base and Llama2 models.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries and Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Standard Self-Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Standard self-attention consists of three matrices: queries $Q$, keys $K$,
    and values $V\in\mathbb{R}^{L\times d^{\prime}}$, where $L$ is sequence length
    and $d^{\prime}$ is the hidden dimension. For each query vector $\bm{q}\in Q$,
    the self-attention applies dot-product with all keys, followed by a Softmax op
    to compute a score on each key. Each score denotes a weight on the corresponding
    values. Then the attention output $A(\bm{q})$ is obtained as a weighted average
    of all values:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A(\bm{q})=\texttt{Softmax}(\bm{q}\cdot K^{T}/\sqrt{d})\cdot V.$ |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: The query/key/value matrices are obtained by projecting input $X\in\mathbb{R}^{L\times
    d}$ with parameter $W^{Q},W^{K},W^{V}\in\mathbb{R}^{d\times d^{\prime}}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q,K,V=X\cdot\{W^{Q},W^{K},W^{V}\}.$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In essence, the self-attention mechanism finds the relations between a query
    and all keys, which are measured by probability after Softmax. Then, it averages
    corresponding values with the notion that a key closer to the query should be
    assigned larger weights (i.e., probability after Softmax).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Related Works on Efficient Self-Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given an input sequence with length $L$, the standard self-attention needs to
    perform $L^{2}$ dot-products for all token vectors to get the whole attention
    map. As a result, it incurs complexity of $\mathcal{O}(L^{2})$ on computations
    and memory, which makes it difficult to scale with long inputs in many tasks.
    As a result, current LLM services usually require a significant amount of memory
    and computing power, in order to support long sequences. In some cases, facing
    actual resource constraints, some LLMs may need to limit the sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Current literature typically mitigates the limitation via exploiting sparsity
    or redundancy in attention matrices Roy et al. ([2021](#bib.bib22)); Sun et al.
    ([2021](#bib.bib26)); Vyas et al. ([2020](#bib.bib29)); Katharopoulos et al. ([2020a](#bib.bib11));
    Wang et al. ([2020](#bib.bib31)), or approximating the self-attention operation.
    For sparsity and redundancy in attention, exploiting low-rank structures in query/key/value
    and attention maps shows great potential. For instance, by exploring redundancy
    in input query vectors, Vyas et al. ([2020](#bib.bib29)) propose to first cluster
    query vectors, and use *cluster centroid vectors* to represent all query vectors
    of the same cluster. Hence, for all queries in the same cluster, it only needs
    to compute the attention score once on the centroid vector. With a reduced number
    of vectors when performing self-attention, it reduces the complexity of self-attention
    from quadratic to linear. However, the cost is a noticeable error by approximating
    many queries with the same cluster, thereby leading to performance degradation.
    On the other hand, Wang et al. ([2020](#bib.bib31)) project key and value matrices
    into a low-dimensional space. Specifically, with $r$ keys and values in the low-dimensional
    space, the method only needs to perform an attention op on $r$ keys rather than
    $L$ keys as in the standard self-attention mechanism. However, due to the fact
    that the projection matrix is pre-defined and learned from scratch, it is not
    guaranteed that the low-dimensional projection is effective in preserving information
    in the original key and value matrices Wang et al. ([2020](#bib.bib31)). Besides
    query/key/value matrices, Nguyen et al. ([2021](#bib.bib17)); Han et al. ([2023](#bib.bib8))
    directly exploit redundancy in the attention map, and approximate the attention
    map with low-rank and sparse matrices. Therefore, computation and memory costs
    of self-attention can also be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides removing redundancy in self-attention operations, current works along
    the second line of research attack the problem via approximating Softmax operations
    with kernelization. Typically, Choromanski et al. ([2020](#bib.bib3)) regard self-attention
    as Softmax kernels: $\texttt{exp}(\bm{q}\cdot\bm{k}^{T})$ with query $\bm{q}$
    and key $\bm{k}$, and approximate it with the Gaussian kernel functionRahimi and
    Recht ([2007](#bib.bib21)). Specifically, it estimates Softmax as: $\texttt{exp}(\bm{q}\cdot\bm{k}^{T})\to\mathbb{E}\left[\phi(\bm{q})\cdot\phi(\bm{k})^{T}\right]$,
    where kernel function $\phi(\cdot)$ maps a vector to a low-dimensional space.
    Therefore, the dimension after kernelization is reduced, leading to a reduction
    in self-attention operations. Along this line, other works (Katharopoulos et al.,
    [2020b](#bib.bib12); Nguyen et al., [2021](#bib.bib17)) explore different kernel
    functions to approximate the self-attention function. While the complexity is
    reduced, these kernel-based approximations still incur large Softmax approximation
    errors given large hidden dimensions in large models.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, *a lightweight self-attention mechanism with linear complexity is
    still needed, especially for current large models with huge computation and memory
    footprints.*
  prefs: []
  type: TYPE_NORMAL
- en: 3 Lowrank Structure in Sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Low-rank structures in inputs of language models are an essential component,
    that, surprisingly, is rarely exploited in current models for better computation
    and memory efficiency. Compared to model parameters Hu et al. ([2021](#bib.bib10)),
    inputs and internal hidden states are usually more correlated, which can be potentially
    exploited. Such a property has also been observed in vision problems Niu et al.
    ([2022](#bib.bib18)); Andriushchenko et al. ([2023](#bib.bib1)) and used to reduce
    the complexity of convolution operations. This paper is the first work that investigates
    the low-rank structure of input sequences in language models, and, importantly,
    its potential to computation and memory saving. In this section, we first analyze
    low-rank structures in transformers’ input sequence. Then, in the next section,
    we present ATP that leverages low-rank structures in inputs and performs self-attention
    with significantly reduced computation and memory footprints.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models comprise a stack of self-attention layers. Each self-attention
    layer takes input state $X\in\mathbb{R}^{L\times d}$, and computes output state
    $Y\in\mathbb{R}^{L\times d}$, where $L$ denotes the sequence length, $d$ is the
    dimension of each hidden state vector. Each state vector corresponds to a token
    in the input sequence. Owning to the semantic relationships among tokens, these
    vectors are also correlated. To formally measure such correlations, we adopt a
    metric called SVD-Entropy.
  prefs: []
  type: TYPE_NORMAL
- en: In detail, we apply singular value decomposition (SVD) to the hidden state as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X\quad\xrightarrow[]{\texttt{SVD}}\quad\sum_{i=1}^{L}\sigma_{i}\cdot\bm{u}_{i}\cdot\bm{v}_{i}^{T}.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'We assume $L\leq d$ without loss of generality. With Eq([3](#S3.E3 "In 3 Lowrank
    Structure in Sequences ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys")), we attains singular values $\{\sigma_{i}\}$ and corresponding principal
    components $\{\bm{v}_{i}\}$. Then, based on Niu et al. ([2022](#bib.bib18)), we
    compute SVD-Entropy as the “low-rankness" of $X$,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu=-\log\left(\sum\limits_{i=1}^{L}\bar{\sigma}_{i}^{2}\right),$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\bar{\sigma}_{i}=\frac{\sigma_{i}}{\sum\limits_{i^{\prime}=1}^{L}\sigma_{i^{\prime}}}$.
  prefs: []
  type: TYPE_NORMAL
- en: According to Niu et al. ([2022](#bib.bib18)), $\left\lceil 2^{\mu}\right\rceil$
    can denote the number of necessary principal components to sufficiently approximate
    input $X$. $\left\lceil 2^{\mu}\right\rceil\ll L$ implies that input state vectors
    in $X$ are highly correlated such that only a few principal components are sufficient
    to represent $X$.
  prefs: []
  type: TYPE_NORMAL
- en: 'With such a measure, we analyze the low-rank structure of hidden states in
    language models. Figure [1](#S3.F1 "Figure 1 ‣ 3 Lowrank Structure in Sequences
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") shows the
    distribution of low-rankness after Llama-2’s embedding layer on BoolQ and MMLU
    datasets, measured by ratio $\left\lceil 2^{\mu}\right\rceil/L$. A small ratio
    implies that the embedding of a sequence is more low-rank. We can easily observe
    that embeddings of all sequences are highly low-rank, where $50\%$ or even fewer
    principal components are sufficient to approximate embedding vectors without error.
    Moreover, longer sequences usually exhibit more low-rank structure compared to
    shorter sequences. Note that the observation implies that exploiting the low-rankness
    of input data can be more effective compared to the low-rankness of models Hu
    et al. ([2021](#bib.bib10)). Such a crucial observation presents great potential
    for reducing the dimension of inputs, thereby leading to more efficient self-attention
    with reduced computation and memory complexities, especially for long sequences.
    Low-rankness analysis of other models is deferred to Appendix [A](#A1 "Appendix
    A Lowrank Structure in Other Model ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys").'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.F1.sf1.pic1" class="ltx_picture" height="279.97" overflow="visible"
    version="1.1" width="553.15"><g transform="translate(0,279.97) matrix(1 0 0 -1
    0 0) translate(45.14,0) translate(0,41.96) matrix(1.0 0.0 0.0 1.0 -45.14 -41.96)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(87.45,0) translate(0,41.96)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 39.51
    -13.81)" fill="#000000" stroke="#000000"><foreignobject width="17.68" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.4 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 281.29 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 402.18 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.5</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -61.04 39.75)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">10</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -61.04 83.96)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">20</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -61.04 128.17)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">30</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -61.04 172.38)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">40</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 189.91 -33.89)" fill="#000000" stroke="#000000"><foreignobject
    width="43.3" height="14.04" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\left\lceil
    2^{\mu}\right\rceil/L$</foreignobject></g><g transform="matrix(0.0 1.0 -1.0 0.0
    -73.23 61.92)" fill="#000000" stroke="#000000"><foreignobject width="113.89" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">probability density</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -27.73 127.54)"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 44.965)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 8.99)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0
    3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject width="67.84" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[0,300]$</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 26.98)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="81.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[300,600]$</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 44.97)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="60.92" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[600,]$</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (a) MMLU
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.F1.sf2.pic1" class="ltx_picture" height="279.97" overflow="visible"
    version="1.1" width="566.32"><g transform="translate(0,279.97) matrix(1 0 0 -1
    0 0) translate(45.14,0) translate(0,41.96) matrix(1.0 0.0 0.0 1.0 -45.14 -41.96)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(91.3,0) translate(0,41.96)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 11.04
    -13.81)" fill="#000000" stroke="#000000"><foreignobject width="17.68" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.1</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 121.46 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 231.88 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 342.31 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 452.73 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.5</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -57.97 46.97)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -64.89 98.4)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">10</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -64.89 149.83)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">15</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 186.06 -33.89)" fill="#000000" stroke="#000000"><foreignobject
    width="43.3" height="14.04" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\left\lceil
    2^{\mu}\right\rceil/L$</foreignobject></g><g transform="matrix(0.0 1.0 -1.0 0.0
    -77.08 61.92)" fill="#000000" stroke="#000000"><foreignobject width="113.89" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">probability density</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.58 127.54)"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 44.965)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 8.99)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0
    3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject width="67.84" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[0,100]$</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 26.98)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="81.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[100,200]$</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 44.97)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="60.92" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[200,]$</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (b) BoolQ
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Distribution of low-rankness of Llama-2’s embedding on MMLU and BoolQ
    dataset, measured by ratio $\left\lceil 2^{\mu}\right\rceil/L$. Almost all sequences
    can be sufficiently approximated with less than half principal components without
    incurring error. Longer sequences exhibit a more low-rank structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 ATP Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce ATP, a generic transformer architecture with a
    new efficient self-attention. ATP introduces a rank-aware self-attention mechanism
    that reduces the complexity of self-attention to linear given the low-rank structure
    in input sequence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Self-Attention with Low-Rank Inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given low-rank input $X\in\mathbb{R}^{L\times d}$ with $r$ principal components,
    we write it as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X=U\cdot X^{\prime},$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $U\in\mathbb{R}^{L\times r}$, and $X^{\prime}\in\mathbb{R}^{r\times d}$
    denotes the principal components. Since $X$ is low-rank, query/key/values matrices
    obtained by projecting $X$ are also low-rank. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Q,K,V&amp;=U\cdot X^{\prime}\cdot\left\{W^{Q},W^{K},W^{V}\right\}\\
    &amp;=U\cdot\left\{Q^{\prime},K^{\prime},V^{\prime}\right\}.\end{split}$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: By the matrix rank inequality Banerjee and Roy ([2014](#bib.bib2)), we have
    $\texttt{rank}(\left\{Q,K,V\right\})\leq\texttt{rank}(X^{\prime})=r$.
  prefs: []
  type: TYPE_NORMAL
- en: Then we start from the standard self-attention, and show the computations can
    be significantly reduced with low-rank keys/values. We omit the normalization
    in Softmax and write self-attention with query $\bm{q}$ on all keys/values as
    $\texttt{exp}(\bm{q},K^{T})\cdot V$. With low-rankness of input $X$, we can break
    down the self-attention as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\texttt{exp}(\bm{q}\cdot K^{T})\cdot V=\texttt{exp}(\bm{q}\cdot
    K^{\prime T}\cdot U^{T})\cdot U\cdot V^{\prime}\end{split}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: By the Taylor expansion on the exp function on ech value, we have the following
    approximation,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}&amp;\texttt{exp}(\bm{q},K^{T})\cdot V\\ &amp;\simeq\bm{1}\cdot
    U\cdot V^{\prime}+\bm{q}\cdot K^{\prime T}\cdot U^{T}\cdot U\cdot V^{\prime}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\bm{1}\cdot U\cdot V^{\prime}+\bm{q}\cdot K^{\prime T}\cdot V^{\prime}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=(\bm{1}\cdot U+\bm{q}\cdot K^{\prime T})\cdot V^{\prime}=A^{\prime}\cdot
    V^{\prime},\end{split}$$ |  | (8) |'
  prefs: []
  type: TYPE_NORMAL
- en: where $\bm{1}\in\mathbb{R}^{1\times L}$, and $U^{T}\cdot U=I$. Similar as Softmax,
    normalization is applied row-wise on the new attention map $A^{\prime}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eq([8](#S4.E8 "In 4.1 Self-Attention with Low-Rank Inputs ‣ 4 ATP Methodology
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys")) shows
    that self-attention on all token vectors $X$ can be converted to attention on
    all principal keys $K^{\prime}$. More importantly, different from the standard
    self-attention where each key corresponds to a token in the input sequence, these
    principal keys denote all principal bases drawn from $X^{\prime}$. That is, *ATP
    converts the attention operation from individual token vectors to principal basis
    vectors.* The observation is very crucial given low-rank input $X$. The reason
    is that, given low-rank input with $r\ll L$, based on Eq([8](#S4.E8 "In 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys")), for each query, we only need to perform dot-product
    on $r$ vectors, rather than $L$ vectors as in the standard self-attention. Therefore,
    the self-attention does not incur $\mathcal{O}(L^{2})$ computation and memory
    costs. Instead, the costs scale linearly with sequence length, $L$, and the number
    of principal components, $r$. Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys") shows a point-to-point comparison between the
    standard self-attention and the low-rank self-attention. The low-rank self-attention
    shares a similar procedure as the stand self-attention, while the difference is
    that the low-rank self-attention performs dot-product on $r$ principal keys.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.F2.sf1.pic1" class="ltx_picture ltx_centering" height="188.4" overflow="visible"
    version="1.1" width="259.35"><g transform="translate(0,188.4) matrix(1 0 0 -1
    0 0) translate(12.34,0) translate(0,89.91)"><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(0.0 -1.0 1.0 0.0 -1.96 49.21)"><foreignobject
    width="98.43" height="15.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$i$-th
    Query: $\bm{q}\in\mathbb{R}^{d^{\prime}}$ <g stroke-width="0.8pt" fill="#FFF2F2"
    stroke="#CCCCCC"><path d="M 39.92 33.46 h 25.84 v 21.18 h -25.84 Z"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 44.54 40.57)"><foreignobject width="16.62" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$K_{1}$</foreignobject></g><g stroke-width="0.8pt"
    fill="#FFF2F2" stroke="#CCCCCC"><path d="M 39.92 -0.63 h 25.84 v 21.18 h -25.84
    Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 44.54 6.47)"><foreignobject width="16.62" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$K_{2}$</foreignobject></g><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 47.65
    -19.7)" fill="#000000" stroke="#000000"><foreignobject width="10.38" height="12.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    stroke-width="0.8pt" fill="#FFF2F2" stroke="#CCCCCC"><path d="M 39.22 -53.17 h
    27.24 v 21.33 h -27.24 Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 43.84 -45.91)"><foreignobject width="18.02"
    height="12.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$K_{L}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 40.74 84.42)" fill="#000000" stroke="#000000"><foreignobject
    width="24.21" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Key</foreignobject></g><path
    d="M 52.84 76.84 L 52.84 66.41" style="fill:none"><g transform="matrix(0.0 -1.0
    1.0 0.0 52.84 66.41)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 90.89 40.57)" fill="#000000" stroke="#000000"><foreignobject width="14.25"
    height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 90.89 6.47)" fill="#000000" stroke="#000000"><foreignobject
    width="14.25" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 95.37 -19.7)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 91.59 -45.91)" fill="#000000" stroke="#000000"><foreignobject
    width="15.65" height="12.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A_{L}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 195.76 -6.54)" fill="#000000" stroke="#000000"><foreignobject
    width="15.65" height="12.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A_{L}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 69 83.42)" fill="#000000" stroke="#000000"><foreignobject
    width="58.42" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Attention</foreignobject></g><path
    d="M 98.02 78.53 L 98.02 68.11" style="fill:none"><g transform="matrix(0.0 -1.0
    1.0 0.0 98.02 68.11)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke-width="0.8pt" fill="#FFCCCC"
    stroke="#CCCCCC"><path d="M 130.27 53.14 h 24.24 v 21.18 h -24.24 Z"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 134.88 60.25)"><foreignobject width="15.02" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$V_{1}$</foreignobject></g><g stroke-width="0.8pt"
    fill="#FFCCCC" stroke="#CCCCCC"><path d="M 167.43 53.14 h 24.24 v 21.18 h -24.24
    Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 172.04 60.25)"><foreignobject width="15.02" height="11.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$V_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 201.06 60.27)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g><g
    stroke-width="0.8pt" fill="#FFCCCC" stroke="#CCCCCC"><path d="M 220.81 53.07 h
    25.64 v 21.33 h -25.64 Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 225.42 60.33)"><foreignobject width="16.42"
    height="12.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$V_{L}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 162.64 83.7)" fill="#000000" stroke="#000000"><foreignobject
    width="33.82" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Value</foreignobject></g><path
    d="M 12.89 0 C 31.75 6.86 19.3 44.05 37.99 44.05" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 37.99 44.05)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 12.89 0 C 23.78 1.92
    28.31 9.95 37.99 9.95" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0
    37.99 9.95)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 12.89 0 C 31.17 -6.65
    19.22 -42.51 37.29 -42.51" style="fill:none"><g transform="matrix(1.0 0.0 0.0
    1.0 37.29 -42.51)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 66.32 44.05 L 84.62
    44.05" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 84.62 44.05)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M 66.32 9.95 L 84.62 9.95" style="fill:none"><g
    transform="matrix(1.0 0.0 0.0 1.0 84.62 9.95)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 67.02 -42.51 L 85.32 -42.51" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 85.32 -42.51)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke-width="0.8pt" fill="#B3FFB3"
    stroke="#CCCCCC"><path d="M 206.92 -36.99 L 189.08 -36.99 C 186.03 -36.99 183.55
    -39.47 183.55 -42.53 L 183.55 -50.37 C 183.55 -53.42 186.03 -55.9 189.08 -55.9
    L 206.92 -55.9 C 209.98 -55.9 212.46 -53.42 212.46 -50.37 L 212.46 -42.53 C 212.46
    -39.47 209.98 -36.99 206.92 -36.99 Z M 183.55 -55.9"></path></g> <g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 188.16 -51.29)"><foreignobject
    width="19.69" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 156.67 -82.61)" fill="#000000" stroke="#000000"><foreignobject
    width="82.68" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Query
    output</foreignobject></g><path d="M 142.39 52.59 L 142.39 -6.46 C 142.39 -31.13
    176.65 -24.11 196.81 -35.75" style="fill:none"><g transform="matrix(0.86603 -0.5
    0.5 0.86603 196.81 -35.75)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 179.55 52.59 L 179.55 -6.46 C 179.55 -20.24 195.61 -22.87 197.76 -35.08"
    style="fill:none"><g transform="matrix(0.17365 -0.98482 0.98482 0.17365 197.76
    -35.08)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 233.63 52.52 L 233.63
    -6.54 C 233.63 -24.76 213.78 -27.33 199.2 -35.75" style="fill:none"><g transform="matrix(-0.86603
    -0.5 0.5 -0.86603 199.2 -35.75)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 110.03 44.05 L 138.69 44.05" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 138.42 44.05)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"><path
    d="M 4.8 0 C 4.8 1.25 3.79 2.26 2.54 2.26 C 1.29 2.26 0.28 1.25 0.28 0 C 0.28
    -1.25 1.29 -2.26 2.54 -2.26 C 3.79 -2.26 4.8 -1.25 4.8 0 Z M 2.54 0"></path></g><path
    d="M 110.03 9.95 L 174.13 9.95" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 173.85 9.95)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"><path
    d="M 4.8 0 C 4.8 1.25 3.79 2.26 2.54 2.26 C 1.29 2.26 0.28 1.25 0.28 0 C 0.28
    -1.25 1.29 -2.26 2.54 -2.26 C 3.79 -2.26 4.8 -1.25 4.8 0 Z M 2.54 0"></path></g><path
    d="M 216.3 -3.14 L 235.12 -3.14" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 234.84 -3.14)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"><path
    d="M 4.8 0 C 4.8 1.25 3.79 2.26 2.54 2.26 C 1.29 2.26 0.28 1.25 0.28 0 C 0.28
    -1.25 1.29 -2.26 2.54 -2.26 C 3.79 -2.26 4.8 -1.25 4.8 0 Z M 2.54 0"></path></g><path
    d="M 198 -56.45 L 198 -66.88" style="fill:none"><g transform="matrix(0.0 -1.0
    1.0 0.0 198 -66.88)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></g>'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Standard self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.F2.sf2.pic1" class="ltx_picture ltx_centering" height="195.41"
    overflow="visible" version="1.1" width="250.71"><g transform="translate(0,195.41)
    matrix(1 0 0 -1 0 0) translate(12.34,0) translate(0,93.73)"><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(0.0 -1.0 1.0 0.0 -1.96 49.21)"><foreignobject
    width="98.43" height="15.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$i$-th
    Query: $\bm{q}\in\mathbb{R}^{d^{\prime}}$ <g stroke-width="0.8pt" fill="#FFF2F2"
    stroke="#CCCCCC"><path d="M 39.92 32.38 h 24.1 v 23.33 h -24.1 Z"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 44.54 39.49)"><foreignobject width="14.87" height="14.1" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$K^{\prime}_{1}$</foreignobject></g><g stroke-width="0.8pt"
    fill="#FFF2F2" stroke="#CCCCCC"><path d="M 39.92 -3.86 h 24.1 v 23.33 h -24.1
    Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 44.54 3.25)"><foreignobject width="14.87" height="14.1" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$K^{\prime}_{2}$</foreignobject></g><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 46.78
    -22.93)" fill="#000000" stroke="#000000"><foreignobject width="10.38" height="12.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    stroke-width="0.8pt" fill="#FFF2F2" stroke="#CCCCCC"><path d="M 39.92 -57.57 h
    24.1 v 22.5 h -24.1 Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 44.54 -51.29)"><foreignobject width="14.87"
    height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$K^{\prime}_{r}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -1.66 87.47)" fill="#000000" stroke="#000000"><foreignobject
    width="83.64" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Principal
    Key</foreignobject></g><path d="M 40.16 79.89 L 47.27 69.23" style="fill:none"><g
    transform="matrix(0.5547 -0.83205 0.83205 0.5547 47.27 69.23)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 89.15 39.49)" fill="#000000"
    stroke="#000000"><foreignobject width="12.51" height="14.1" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$A^{\prime}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 89.15 3.25)" fill="#000000" stroke="#000000"><foreignobject width="12.51"
    height="14.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A^{\prime}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 94.5 -22.93)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 89.15 -51.29)" fill="#000000" stroke="#000000"><foreignobject
    width="12.51" height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A^{\prime}_{r}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 194.11 -11.92)" fill="#000000" stroke="#000000"><foreignobject
    width="12.51" height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$A^{\prime}_{r}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 66.38 87.25)" fill="#000000" stroke="#000000"><foreignobject
    width="58.42" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Attention</foreignobject></g><path
    d="M 95.4 82.37 L 95.4 71.94" style="fill:none"><g transform="matrix(0.0 -1.0
    1.0 0.0 95.4 71.94)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke-width="0.8pt" fill="#FFCCCC"
    stroke="#CCCCCC"><path d="M 126.78 52.07 h 22.5 v 23.33 h -22.5 Z"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 131.39 59.18)"><foreignobject width="13.28" height="14.1" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$V^{\prime}_{1}$</foreignobject></g><g stroke-width="0.8pt"
    fill="#FFCCCC" stroke="#CCCCCC"><path d="M 162.2 52.07 h 22.5 v 23.33 h -22.5
    Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 166.81 59.18)"><foreignobject width="13.28" height="14.1" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$V^{\prime}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 194.08 60.27)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g><g
    stroke-width="0.8pt" fill="#FFCCCC" stroke="#CCCCCC"><path d="M 213.84 52.48 h
    22.5 v 22.5 h -22.5 Z"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 218.45 58.76)"><foreignobject width="13.28"
    height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$V^{\prime}_{r}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 126.83 87.47)" fill="#000000" stroke="#000000"><foreignobject
    width="94.4" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Principal
    Value</foreignobject></g><path d="M 12.89 0 C 31.75 6.86 19.3 44.05 37.99 44.05"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 37.99 44.05)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M 12.89 0 C 23.51 1.87 28.58 7.8 37.99 7.8" style="fill:none"><g
    transform="matrix(1.0 0.0 0.0 1.0 37.99 7.8)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 12.89 0 C 32.45 -7.12 18.55 -46.32 37.99 -46.32" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 37.99 -46.32)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 64.57 44.05 L 82.87
    44.05" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 82.87 44.05)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M 64.57 7.8 L 82.87 7.8" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 82.87 7.8)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 64.57 -46.32 L 82.87
    -46.32" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 82.87 -46.32)"
    stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g stroke-width="0.8pt" fill="#B3FFB3"
    stroke="#CCCCCC"><path d="M 201.34 -40.81 L 183.5 -40.81 C 180.44 -40.81 177.96
    -43.28 177.96 -46.34 L 177.96 -54.18 C 177.96 -57.24 180.44 -59.72 183.5 -59.72
    L 201.34 -59.72 C 204.39 -59.72 206.87 -57.24 206.87 -54.18 L 206.87 -46.34 C
    206.87 -43.28 204.39 -40.81 201.34 -40.81 Z M 177.96 -59.72"></path></g> <g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 182.58 -55.1)"><foreignobject
    width="19.69" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 151.08 -86.42)" fill="#000000" stroke="#000000"><foreignobject
    width="82.68" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Query
    output</foreignobject></g><path d="M 138.03 51.51 L 138.03 -15.41 C 138.03 -38.81
    172.16 -28.56 191.22 -39.56" style="fill:none"><g transform="matrix(0.86603 -0.5
    0.5 0.86603 191.22 -39.56)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 173.45 51.51 L 173.45 -15.41 C 173.45 -27.62 190.3 -28.24 192.18 -38.89"
    style="fill:none"><g transform="matrix(0.17365 -0.98482 0.98482 0.17365 192.18
    -38.89)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 225.09 51.93 L 225.09
    -15 C 225.09 -31.13 206.39 -32.19 193.62 -39.56" style="fill:none"><g transform="matrix(-0.86603
    -0.5 0.5 -0.86603 193.62 -39.56)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 106.54 44.05 L 135.99 44.05" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 135.72 44.05)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"><path
    d="M 4.8 0 C 4.8 1.25 3.79 2.26 2.54 2.26 C 1.29 2.26 0.28 1.25 0.28 0 C 0.28
    -1.25 1.29 -2.26 2.54 -2.26 C 3.79 -2.26 4.8 -1.25 4.8 0 Z M 2.54 0"></path></g><path
    d="M 106.54 7.8 L 171.43 7.8" style="fill:none"><g transform="matrix(1.0 0.0 0.0
    1.0 171.15 7.8)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"><path
    d="M 4.8 0 C 4.8 1.25 3.79 2.26 2.54 2.26 C 1.29 2.26 0.28 1.25 0.28 0 C 0.28
    -1.25 1.29 -2.26 2.54 -2.26 C 3.79 -2.26 4.8 -1.25 4.8 0 Z M 2.54 0"></path></g><path
    d="M 211.51 -6.95 L 231.11 -6.95" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 230.84 -6.95)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"><path
    d="M 4.8 0 C 4.8 1.25 3.79 2.26 2.54 2.26 C 1.29 2.26 0.28 1.25 0.28 0 C 0.28
    -1.25 1.29 -2.26 2.54 -2.26 C 3.79 -2.26 4.8 -1.25 4.8 0 Z M 2.54 0"></path></g><path
    d="M 192.42 -60.27 L 192.42 -70.7" style="fill:none"><g transform="matrix(0.0
    -1.0 1.0 0.0 192.42 -70.7)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></g>'
  prefs: []
  type: TYPE_NORMAL
- en: (b) Low-rank self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Standard self-attention and low-rank self-attention. Low-rank self-attention
    share the same procedure as the standard self-attention, but with only $r$ principal
    keys and values.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 4.1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike works such as Wang et al. ([2020](#bib.bib31)) that attain low-dimensional
    key/value matrices via hard-coded/learnable projection, we adopt a more rigorous
    method based on SVD to find the optimization low-dimensional space, that preserves
    most energy of input $X$ with $r$ principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Tansformers with Low-Rank Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the low-rank self-attention above, we can adapt the transformer architecture
    to input sequences with highly low-rank structure. To the best of our knowledge,
    this is the first adaptation that takes input low-rank structure into model design,
    and reduces complexities for the whole pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: As a transformer model is usually built with a stack of encoder/decoder layers
    with the same architecture, to simplify, we only show the architecture adaptation
    for one encoder/decoder layer, which will be replicated to the rest of the layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to analyze input $X$ and attain its principal components.
    To that end, we decompse input $X$ using SVD, and attain the principal components
    $X^{\prime}$ based on Eq([3](#S3.E3 "In 3 Lowrank Structure in Sequences ‣ ATP:
    Enabling Fast LLM Serving via Attention on Top Principal Keys")): $X^{\prime}=\left[\sigma_{1}\bm{v}_{1},\cdots,\sigma_{r}\bm{v}_{r}\right]$.
    However, the exact SVD incurs a complexity of $\mathcal{O}(Ld^{2})$, which can
    be a performance bottleneck given the large dimension of each vector in $X$. To
    avoid such a quadratic complexity, we adopt an approximated SVD algorithm as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Essentially, the optimization above is to find $r$ principal components that
    preserve most energy in $X$, while ignoring the orthogonality constraint on the
    components. To simplify the optimization above, $\sigma_{i}$ can be fused with
    $\bm{v}_{i}$ to reduce the number of variables to be optimized. By the alternating
    optimization in Alg 1 in Niu et al. ([2022](#bib.bib18)) (duplicated in Appendix
    [B](#A2 "Appendix B Alternating Optimization ‣ ATP: Enabling Fast LLM Serving
    via Attention on Top Principal Keys")), we can attain $r$ most principal components
    which preserved most energy in $X$. Compared to the standard SVD decomposition,
    the approximation in Eq([9](#S4.E9 "In 4.2 Tansformers with Low-Rank Attention
    ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys")) incurs a linear complexity of $\mathcal{O}(rLd)$, thereby preventing SVD
    being the bottleneck in the whole pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F3.pic1" class="ltx_picture ltx_centering" height="288.34" overflow="visible"
    version="1.1" width="229.4"><g transform="translate(0,288.34) matrix(1 0 0 -1
    0 0) translate(150.38,0) translate(0,134.61)"><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -49.21 3.24)"><foreignobject
    width="98.43" height="25.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Low-Rank
    Attention <g stroke="#000000" fill="#FFECEC" stroke-width="0.8pt"><path d="M -50.14
    -30.45 L -67.97 -30.45 C -71.03 -30.45 -73.51 -32.93 -73.51 -35.99 L -73.51 -46.24
    C -73.51 -49.3 -71.03 -51.78 -67.97 -51.78 L -50.14 -51.78 C -47.08 -51.78 -44.6
    -49.3 -44.6 -46.24 L -44.6 -35.99 C -44.6 -32.93 -47.08 -30.45 -50.14 -30.45 Z
    M -73.51 -51.78"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -68.9 -44.75)"><foreignobject width="19.69"
    height="12.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$W^{Q}$</foreignobject></g>
    <g stroke="#000000" fill="#FFECEC" stroke-width="0.8pt"><path d="M 8.92 -30.45
    L -8.92 -30.45 C -11.98 -30.45 -14.45 -32.93 -14.45 -35.99 L -14.45 -46.24 C -14.45
    -49.3 -11.98 -51.78 -8.92 -51.78 L 8.92 -51.78 C 11.98 -51.78 14.45 -49.3 14.45
    -46.24 L 14.45 -35.99 C 14.45 -32.93 11.98 -30.45 8.92 -30.45 Z M -14.45 -51.78"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -9.84 -44.75)"><foreignobject width="19.69" height="12.1" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$W^{K}$</foreignobject></g> <g stroke="#000000"
    fill="#FFECEC" stroke-width="0.8pt"><path d="M 67.97 -30.45 L 50.14 -30.45 C 47.08
    -30.45 44.6 -32.93 44.6 -35.99 L 44.6 -46.24 C 44.6 -49.3 47.08 -51.78 50.14 -51.78
    L 67.97 -51.78 C 71.03 -51.78 73.51 -49.3 73.51 -46.24 L 73.51 -35.99 C 73.51
    -32.93 71.03 -30.45 67.97 -30.45 Z M 44.6 -51.78"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 49.21 -44.75)"><foreignobject
    width="19.69" height="12.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$W^{V}$</foreignobject></g>
    <g stroke="#000000" fill="#FFECEC" stroke-width="0.8pt"><path d="M 8.92 -84.38
    L -8.92 -84.38 C -11.98 -84.38 -14.45 -86.86 -14.45 -89.92 L -14.45 -97.76 C -14.45
    -100.81 -11.98 -103.29 -8.92 -103.29 L 8.92 -103.29 C 11.98 -103.29 14.45 -100.81
    14.45 -97.76 L 14.45 -89.92 C 14.45 -86.86 11.98 -84.38 8.92 -84.38 Z M -14.45
    -103.29"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -9.84 -98.68)"><foreignobject width="19.69" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SVD</foreignobject></g> <g stroke="#000000"
    fill="#FFECEC" stroke-width="0.8pt"><path d="M 8.92 70.11 L -8.92 70.11 C -11.98
    70.11 -14.45 67.63 -14.45 64.58 L -14.45 43.86 C -14.45 40.8 -11.98 38.33 -8.92
    38.33 L 8.92 38.33 C 11.98 38.33 14.45 40.8 14.45 43.86 L 14.45 64.58 C 14.45
    67.63 11.98 70.11 8.92 70.11 Z M -14.45 38.33"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -9.84 55.81)"><foreignobject
    width="19.69" height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Norm</foreignobject></g>
    <g stroke="#000000" fill="#FFECEC" stroke-width="0.8pt"><path d="M 28.6 114.53
    L -28.6 114.53 C -31.66 114.53 -34.14 112.05 -34.14 109 L -34.14 84.63 C -34.14
    81.57 -31.66 79.09 -28.6 79.09 L 28.6 79.09 C 31.66 79.09 34.14 81.57 34.14 84.63
    L 34.14 109 C 34.14 112.05 31.66 114.53 28.6 114.53 Z M -34.14 79.09"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -29.53 100.23)"><foreignobject width="59.06" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Feedforward</foreignobject></g> <g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g stroke-opacity="1.0" fill="#000000" fill-opacity="1.0"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -6.27 -130)"><foreignobject
    width="12.55" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$X$</foreignobject></g><g
    stroke-opacity="1.0" fill="#000000" fill-opacity="1.0" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -5.55 139.66)"><foreignobject width="11.11" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -19.15 -77.76)" fill="#000000" stroke="#000000"><foreignobject width="14.68"
    height="11.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$X^{\prime}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -145.77 -44.5)" fill="#000000" stroke="#000000"><foreignobject
    width="62.88" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Projection</foreignobject></g><path
    d="M 0 -115.66 L 0 -105.23" style="fill:none"><g transform="matrix(0.0 1.0 -1.0
    0.0 0 -105.23)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 0 -83.83 L 0 -70.21
    L -42.81 -49.12" style="fill:none"><g transform="matrix(-0.89699 0.44205 -0.44205
    -0.89699 -42.81 -49.12)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 0 -83.83 L 0 -53.72"
    style="fill:none"><g transform="matrix(0.0 1.0 -1.0 0.0 0 -53.72)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M 0 -83.83 L 0 -70.21 L 42.81 -49.12" style="fill:none"><g
    transform="matrix(0.89699 0.44205 -0.44205 0.89699 42.81 -49.12)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M -44.05 -30.67 L -27.11 -18.88" style="fill:none"><g
    transform="matrix(0.82072 0.57133 -0.57133 0.82072 -27.11 -18.88)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M 0 -29.9 L 0 -19.47" style="fill:none"><g transform="matrix(0.0
    1.0 -1.0 0.0 0 -19.47)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 44.05 -30.67 L 27.11
    -18.88" style="fill:none"><g transform="matrix(-0.82072 0.57133 -0.57133 -0.82072
    27.11 -18.88)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 0 18.09 L 0 36.39" style="fill:none"><g
    transform="matrix(0.0 1.0 -1.0 0.0 0 36.39)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path
    d="M 0 70.67 L 0 77.16" style="fill:none"><g transform="matrix(0.0 1.0 -1.0 0.0
    0 77.16)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 0 115.08 L 0 133.39"
    style="fill:none"><g transform="matrix(0.0 1.0 -1.0 0.0 0 133.39)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><path d="M 0 -83.83 L 0 -75.95 L 78.74 -75.95 L 78.74 2.79
    L 1.38 2.79" style="fill:none"><g transform="matrix(-1.0 0.0 0.0 -1.0 1.38 2.79)"
    stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><path d="M 0 18.09 L 0 29.9 L 39.37
    29.9 L 39.37 88.95 L 1.38 88.95" style="fill:none"><g transform="matrix(-1.0 0.0
    0.0 -1.0 1.38 88.95)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g></path></path></path></path></path></path></path></path></path></path></path></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Transformer encoder/decoder with low-rank self-attention. Input $X$
    is first fed to SVD to attain the principal components, $X^{\prime}$. Then, $X^{\prime}$
    is fed to an encoder/decoder layer with low-rank self-attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, principal components $X^{\prime}$ are fed into a self-attention layer
    to attain principal keys and values as in Eq([6](#S4.E6 "In 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys")). With the principal keys and values, ATP performs
    attention as in Eq([8](#S4.E8 "In 4.1 Self-Attention with Low-Rank Inputs ‣ 4
    ATP Methodology ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys")), and feedforward to obtain output states $Y$. The next encoder/decoder
    layer follows the same procedure first to attain principal components of $Y$ and
    perform low-rank self-attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Combine with Position Encoding. For absolution or relative position encoding
    vectors $P$ for a sequence Devlin et al. ([2018](#bib.bib5)); Lan et al. ([2020](#bib.bib14));
    Shaw et al. ([2018](#bib.bib24)), they are added to token embeddings before an
    encoder/decoder layer. Therefore, we can still directly apply SVD to the input
    vectors, $X+P$, and obtain principal components for low-rank self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'For rotatory position embedding Su et al. ([2021](#bib.bib25)); Touvron et al.
    ([2023](#bib.bib27)), the position encoding vectors are added after query/key
    projection. That is, $\bm{k}=\bm{x}\cdot W^{K}\cdot R_{i}$, where $R_{i}$ is a
    rotary matrix corresponding to a transformation for a token at position $i$, $\bm{x}$
    denotes one input vector in $X$. While the low-rank structure might change during
    the rotation, we can still attain a low-rank key matrix by projecting the key
    matrix into a low-dimension space with $U$ as in Eq([8](#S4.E8 "In 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys")).'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the low-rank self-attention mechanism is compatible with current
    position encoding methods.
  prefs: []
  type: TYPE_NORMAL
- en: '| Mechanism | Standard | Low-rank |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Computation | Complexity | Memory | Computation | Complexity | Memory
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Projection | $X\cdot W$ | $\mathcal{O}(Ldd^{\prime})$ | $\mathcal{O}(Ld^{\prime})$
    | $X^{\prime}\cdot W$ | $\mathcal{O}(rdd^{\prime})$ | $\mathcal{O}(rd^{\prime})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Attention | $Q\cdot K^{T}$ | $\mathcal{O}(L^{2}d^{\prime})$ | $\mathcal{O}(L^{2})$
    | $QK^{\prime T}$ | $\mathcal{O}(rLd^{\prime})$ | $\mathcal{O}(rL)$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Computation and memory complexity with low-rank input. Low-rank self-attention
    reduces the complexity of attention from quadratic to linear. It also reduces
    complexities for other linear layers ($L$: sequence length, $r$: rank, $d$: dimension
    of $X$, $d^{\prime}$: dimension of hidden state).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Complexity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Self-attention with low-rank inputs not only relieves computation and memory
    pressure for attention operations, but also reduces complexity for other linear
    layers. Table [1](#S4.T1 "Table 1 ‣ 4.2 Tansformers with Low-Rank Attention ‣
    4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys") lists computations and the corresponding complexity of the standard and
    low-rank self-attention. Due to the reduced number of components in $X^{\prime}$,
    query/key/value projection only needs to project $r$ vectors rather than $L$ token
    vectors as the standard self-attention, thereby resulting in $r$ keys and value
    vectors with dimension $d^{\prime}$. Hence, both the computation and memory during
    the projection are reduced by $L/r$. On the other hand, when performing attention,
    the low-rank attention only needs to compute the attention score on $r$ principal
    keys, rather than $L$ token keys as the standard self-attention. Therefore, the
    computation and memory complexities are also reduced by $L/r$. Note that the additional
    SVD only incurs computation complexity of $\mathcal{O}(rLd)$, which is linear
    in term of $L$, and is relatively small compared to computations in the standard
    self-attention. In addition, more computation saving can be achieved by decomposing
    hidden state vectors to the FeedForward layer. In this paper, we mainly focus
    on self-attention layers.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F4.pic1" class="ltx_picture ltx_centering" height="338.23" overflow="visible"
    version="1.1" width="547.38"><g transform="translate(0,338.23) matrix(1 0 0 -1
    0 0) translate(39.37,0) translate(0,40.22) matrix(1.0 0.0 0.0 1.0 -39.37 -40.22)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(81.69,0) translate(0,40.22)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -13.81)" fill="#000000" stroke="#000000"><foreignobject width="20.76" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">512</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 91.94 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1024</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 197.72 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2048</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 303.49 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4096</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 409.27 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">8192</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -57.96 43.15)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">.2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -57.96 90.76)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -57.96 138.37)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">.6</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -57.96 185.98)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">.8</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 164.05 -32.92)" fill="#000000" stroke="#000000"><foreignobject
    width="95.01" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">sequence
    length</foreignobject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -67.46 100.03)"
    fill="#000000" stroke="#000000"><foreignobject width="97.67" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">normalized time</foreignobject></g><g fill="#FFFFFF"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -27.73 176.3)"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 25.83)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.77)" fill="#000000"
    stroke="#000000"><foreignobject width="52.7" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">low-rank</foreignobject></g></g><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 25.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0
    3.04 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="53.16" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">standard</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Actual running time of low-rank self-attention compared to the standard
    mechanism with different sequence lengths ($r$=128). The running time of the standard
    self-attention increases quadratically with the sequence length. Low-rank self-attention
    reduces the running time to almost linear.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Complexity Analysis ‣ 4 ATP Methodology ‣
    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") shows actual
    speedups of low-rank self-attention compared to the standard self-attention given
    different sequence lengths. Note that the standard self-attention, as expected,
    incurs quadratic running time with increasing input sequence length. On the other
    hand, the running time of the low-rank self-attention scales almost linearly with
    sequence length. The time gap between them grows rapidly with long sequences.
    This shows that the standard self-attention indeed comes with a severe bottleneck
    on real performance with long sequences, while the low-rank self-attention significantly
    reduces actual running time.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Empirical Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate the low-rank attention on benchmark models and
    datasets. To investigate the applicability of low-rank attention in a wide range
    of applications, we choose models with different sizes. For datasets, we focus
    on long sequences, which usually incur significant computation and memory pressure
    during inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model. We choose BERT-base (encoders only) as the small model Devlin et al.
    ([2018](#bib.bib5)), Llama2-7B (decoder only) as the medium model, and Llama2-13B
    as the large model Touvron et al. ([2023](#bib.bib27)). Table [2](#S5.T2 "Table
    2 ‣ 5 Empirical Evaluation ‣ ATP: Enabling Fast LLM Serving via Attention on Top
    Principal Keys") lists their detailed architecture parameters. Note that all three
    models adopt the standard self-attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BERT | Llama2-7B | Llama2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| # att layers | 12 | 32 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| # heads/layer | 12 | 32 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| # head dim | 64 | 128 | 128 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Architecture parameters of BERT-base, Llama2-7b and Llama2-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets. For BERT-base, we choose SST-2, Squad Wang et al. ([2019](#bib.bib30)),
    and IMDBMaas et al. ([2011](#bib.bib15)). In particular, the IMDB dataset consists
    of long sequences that exhibit more low-rank structures. For Llama2-7B and Llama2-13B,
    we choose two of the official benchmark datasets: MMLU Hendrycks et al. ([2021](#bib.bib9))
    and BoolQ Clark et al. ([2019](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 BERT-base
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For all datasets, we start from a pre-trained model, replace each self-attention
    layer with the low-rank self-attention, and finetune the model. Owing to the model
    size, we finetune full parameters. Training details are provided in Appendix [C](#A3
    "Appendix C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys"). Table [3](#S5.T3 "Table 3 ‣ 5.1 BERT-base ‣ 5 Empirical
    Evaluation ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys")
    lists the final model accuracy on SST-2, Squad, and IMDB. We can observe that
    BERT-base with low-rank self-attention preserves models’ performance. In particular,
    with $1/2$ principal keys used, the model with low-rank self-attention barely
    loses accuracy. This indicates that owing to the low-rank structure in sequences,
    $1/2$ principal keys preserve most information in inputs. Surprisingly, we can
    further see that even only keeping $1/8$ principal keys, the model still achieves
    a comparable accuracy as the model with standard self-attention.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Original | 1/2 | 1/4 | 1/8 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | $92.32\pm 0.2$ | $92.1\pm 0.17$ | $91.0\pm 0.23$ | $89.2\pm 0.26$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Squad | $88.15\pm 0.3$ | $87.93\pm 0.2$ | $87.23\pm 0.34$ | $84.94\pm 0.28$
    |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB | $91.45\pm 0.2$ | $90.97\pm 0.19$ | $89.65\pm 0.3$ | $87.28\pm 0.3$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: BERT-base accuracy on SST-2, Squad, and IMDB using low-rank self-attention.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.F5.pic1" class="ltx_picture ltx_centering" height="343.14" overflow="visible"
    version="1.1" width="531.6"><g transform="translate(0,343.14) matrix(1 0 0 -1
    0 0) translate(53.59,0) translate(0,45.14) matrix(1.0 0.0 0.0 1.0 -53.59 -45.14)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(93.41,0) translate(0,45.14)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -15.27)" fill="#000000" stroke="#000000"><foreignobject width="20.76" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1/8</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 46.49 -15.27)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1/4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.24 -15.27)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1/2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 273.98 -15.27)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3/4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 388.3 -14.5)" fill="#000000" stroke="#000000"><foreignobject
    width="19.6" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">full</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -58.54 184.58)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -58.54 231.83)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">90</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -65.46 279.09)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">100</foreignobject></g><g
    stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -58.54 90.06)" fill="#000000" stroke="#000000"><foreignobject width="13.84"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">60</foreignobject></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 166.19 -37.84)" fill="#000000" stroke="#000000"><foreignobject
    width="65.73" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">complexity</foreignobject></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -78.42 101.36)" fill="#000000" stroke="#000000"><foreignobject
    width="95.01" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">energy
    ratio(%)</foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 365.17 33.87)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1
    0 42.815)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.53)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    25.31 0) translate(21.22,0) matrix(1.0 0.0 0.0 1.0 -18.45 -3.69)" fill="#000000"
    stroke="#000000"><foreignobject width="36.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SST-2</foreignobject></g></g><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 25.68)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.11 0) translate(21.41,0) matrix(1.0 0.0 0.0 1.0
    -18.64 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="37.28" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Squad</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 42.81)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.73 0) translate(21.79,0)
    matrix(1.0 0.0 0.0 1.0 -19.03 -3.69)" fill="#000000" stroke="#000000"><foreignobject
    width="38.05" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">IMDB</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Energy ratio ($\left\|X^{\prime}\right\|^{2}_{F}/\left\|X\right\|^{2}_{F}$)
    in low-rank hidden representations. Embeddings of all three datasets exhibit highly
    low-rank structures, with $1/2$ principal components preserving almost all energy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.1 BERT-base ‣ 5 Empirical Evaluation ‣ ATP:
    Enabling Fast LLM Serving via Attention on Top Principal Keys") shows the relative
    energy kept in the low-rank keys. We observe that for $1/2$ principal keys are
    sufficient to keep almost all energy in inputs, which is aligned with model accuracy
    in Table [3](#S5.T3 "Table 3 ‣ 5.1 BERT-base ‣ 5 Empirical Evaluation ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys"). On the other hand, compared
    to Squad and IMDB, SST-2 exhibits a more low-rank structure, with even $1/8$ principal
    keys still preserving near $90\%$ energy. The observation explains BERT-base’s
    performance on SST-2 that even low-rank self-attention with only $1/8$ principal
    keys only incurs a $\sim 3\%$ accuracy drop.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Llama2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We obtain pre-trained Llama2-7B/13B models from the Hugging Face repo ¹¹1[https://huggingface.co/meta-llama](https://huggingface.co/meta-llama).
    Starting from the pre-trained models, we replace their attention layers with low-rank
    self-attention. For MMLU and BoolQ, since they have different formats, we will
    first finetune the model on the datasets for a few iterations (See Appendix [C](#A3
    "Appendix C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys") for more finetuning parameters), and then evaluate their
    performance on the validation dataset. Appendix [D](#A4 "Appendix D Prompt Format
    for MMLU and BoolQ ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys") provides prompt formats for MMLU and BoolQ during training and validation.
    To reduce training workload, we use LoRA Hu et al. ([2021](#bib.bib10)) to finetune
    the projection matrix for queries/keys/values with rank of $32$, and fix other
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For MMLU, we obtain the first predicted logit vector from the model given an
    input sequence, and compute the probability on the four tokens: *A, B, C, D*.
    The token with the highest probability will be the predicted answer. For BoolQ,
    we adopt a similar procedure but compute the probability on the two tokens: *Yes,
    No*, and output the token with the highest probability. Note that we ignore other
    tokens that might have the highest probability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [6](#S5.F6 "Figure 6 ‣ 5.2 Llama2 ‣ 5 Empirical Evaluation ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys") shows the accuracy of Llama2-7B
    and 13B on MMLU using ATP. We can observe that on all categories, ATP achieves
    accuracy close to original Llama2-7B and 13B with standard self-attention. In
    particular, owing to the highly low-rank structure in input sequences, with $1/2$
    principal keys, the model performance with ATP is almost identical to the original
    model. Furthermore, even with only $1/4$ principal keys, ATP still does not incur
    a significant accuracy drop. Similar performance of LLama2-7B and 13B with the
    low-rank self-attention holds on the BoolQ dataset, as listed in Table [4](#S5.T4
    "Table 4 ‣ 5.2 Llama2 ‣ 5 Empirical Evaluation ‣ ATP: Enabling Fast LLM Serving
    via Attention on Top Principal Keys"). Therefore, ATP effectively leverages low-rank
    structure in input sequences and performs self-attention with a few top principal
    keys, leading to performance close to the original model but with significantly
    reduced complexities.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.F6.sf1.pic1" class="ltx_picture ltx_centering" height="322.33" overflow="visible"
    version="1.1" width="531.07"><g transform="translate(0,322.33) matrix(1 0 0 -1
    0 0) translate(53.06,0) translate(0,25.01) matrix(1.0 0.0 0.0 1.0 -53.06 -25.01)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(53.06,0) translate(0,25.01)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 39.83
    -20.25)" fill="#000000" stroke="#000000"><foreignobject width="39.78" height="9.46"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">STEM</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 146.25 -20.4)" fill="#000000" stroke="#000000"><foreignobject
    width="65.8" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">humanities</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 281.82 -20.4)" fill="#000000" stroke="#000000"><foreignobject
    width="33.13" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">social</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 402.24 -20.4)" fill="#000000" stroke="#000000"><foreignobject
    width="31.56" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">other</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 62.04)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 128.54)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 195.03)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.5</foreignobject></g><g
    stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -29.49 28.79)" fill="#000000" stroke="#000000"><foreignobject width="24.6"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.25</foreignobject></g></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -38.99 107.53)" fill="#000000" stroke="#000000"><foreignobject
    width="22.68" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Acc</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 28.32 276.28)"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.995)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 8.99)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 13.01 0) translate(23.16,0) matrix(1.0 0.0 0.0 1.0
    -13.47 -3.69)" fill="#000000" stroke="#000000"><foreignobject width="26.94" height="12.15"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Orig</foreignobject></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    72.33 0) translate(20.06,0) matrix(1.0 0.0 0.0 1.0 -10.38 -4.15)" fill="#000000"
    stroke="#000000"><foreignobject width="20.76" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1/2</foreignobject></g><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 125.46 0) translate(20.06,0)
    matrix(1.0 0.0 0.0 1.0 -10.38 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1/3</foreignobject></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    178.6 0) translate(20.06,0) matrix(1.0 0.0 0.0 1.0 -10.38 -4.15)" fill="#000000"
    stroke="#000000"><foreignobject width="20.76" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1/4</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (a) Llama2-7B.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.F6.sf2.pic1" class="ltx_picture ltx_centering" height="268.79" overflow="visible"
    version="1.1" width="531.07"><g transform="translate(0,268.79) matrix(1 0 0 -1
    0 0) translate(53.06,0) translate(0,25.02) matrix(1.0 0.0 0.0 1.0 -53.06 -25.02)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(53.06,0) translate(0,25.02)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 39.83
    -20.25)" fill="#000000" stroke="#000000"><foreignobject width="39.78" height="9.46"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">STEM</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 146.25 -20.4)" fill="#000000" stroke="#000000"><foreignobject
    width="65.8" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">humanities</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 281.82 -20.4)" fill="#000000" stroke="#000000"><foreignobject
    width="33.13" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">social</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 402.24 -20.4)" fill="#000000" stroke="#000000"><foreignobject
    width="31.56" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">other</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 45.22)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 94.91)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 144.59)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.5</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -22.57 194.27)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.6</foreignobject></g><g
    stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -29.49 20.38)" fill="#000000" stroke="#000000"><foreignobject width="24.6"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.25</foreignobject></g></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -38.99 107.53)" fill="#000000" stroke="#000000"><foreignobject
    width="22.68" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Acc</foreignobject></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (b) Llama2-13B.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: LLama2 on MMLU (random guess: 0.25). Low-rank self-attention effectively
    preserves performance on all subjects, even with $1/4$ principal keys.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Orig | 1/2 | 1/3 | 1/4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | 0.795 | 0.791 | 0.789 | 0.763 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | 0.839 | 0.836 | 0.819 | 0.816 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Llama2 on BoolQ with low-rank self-attention. Performance is not greatly
    affected even a small fraction of principal keys/values are used in attention
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose a low-rank self-attention mechanism, ATP, significantly
    reducing computation and memory complexity for transformers and LLMs. ATP leverages
    low-rank structures in input sequences and sufficiently represents each input
    sequence with a few top principal components. Then, ATP designs a low-rank self-attention
    layer that first attains principal keys/values given a low-rank input. Then, it
    performs attention only on top principal keys/values, rather than on each individual
    token embedding. Therefore, ATP reduces the attention complexity from quadratic
    to linear in terms of sequence length. Owing to low-rank structures in input sequences,
    a few top principal keys/values are sufficient to preserve information in input
    sequences. Evaluation of BERT and Llama models shows ATP achieves performance
    close to original models with much-reduced computation and memory footprints.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Limitations. One of the limitations of this work is that we evaluate ATP on
    BERT and Llama2 models. While performance on other models may differ. We will
    evaluate more models and datasets in future works.
  prefs: []
  type: TYPE_NORMAL
- en: Potential Risk. While this work is aimed at lowering the barrier of deploying
    LLMs, it may be misused by malicious parties to quickly deploy and run adverse
    LLM services for their purposes.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Andriushchenko et al. (2023) Maksym Andriushchenko, Dara Bahri, Hossein Mobahi,
    and Nicolas Flammarion. 2023. Sharpness-aware minimization leads to low-rank features.
    *arXiv preprint arXiv:2305.16292*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banerjee and Roy (2014) Sudipto Banerjee and Anindya Roy. 2014. *Linear algebra
    and matrix analysis for statistics*. Crc Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choromanski et al. (2020) Krzysztof Marcin Choromanski, Valerii Likhosherstov,
    David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy
    Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with
    performers. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. In *NAACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words:
    Transformers for image recognition at scale. In *International Conference on Learning
    Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'El-Kassas et al. (2021) Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea,
    and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey.
    *Expert systems with applications*, 165:113679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2023) Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P.
    Woodruff, and Amir Zandieh. 2023. [Hyperattention: Long-context attention in near-linear
    time](http://arxiv.org/abs/2310.05869).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask
    language understanding. *Proceedings of the International Conference on Learning
    Representations (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katharopoulos et al. (2020a) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020a. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *International conference on machine learning*, pages
    5156–5165\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katharopoulos et al. (2020b) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020b. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *International conference on machine learning*, pages
    5156–5165\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan et al. (2022) Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas
    Zamir, Fahad Shahbaz Khan, and Mubarak Shah. 2022. Transformers in vision: A survey.
    *ACM computing surveys (CSUR)*, 54(10s):1–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised
    learning of language representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. [Learning word vectors for sentiment
    analysis](http://www.aclweb.org/anthology/P11-1015). In *Proceedings of the 49th
    Annual Meeting of the Association for Computational Linguistics: Human Language
    Technologies*, pages 142–150, Portland, Oregon, USA. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (16) Microsoft. [Microsoft copilot](https://adoption.microsoft.com/en-us/copilot/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2021) Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and
    Bao Wang. 2021. Fmmformer: Efficient and flexible transformer via decomposed near-field
    and far-field attention. *Advances in neural information processing systems*,
    34:29449–29463.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu et al. (2022) Yue Niu, Ramy E Ali, and Salman Avestimehr. 2022. 3legrace:
    Privacy-preserving dnn training over tees and gpus. *Proceedings on Privacy Enhancing
    Technologies*, 4:183–203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (19) OpenAI. [Introducing chatgpt](https://openai.com/blog/chatgpt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahimi and Recht (2007) Ali Rahimi and Benjamin Recht. 2007. Random features
    for large-scale kernel machines. *Advances in neural information processing systems*,
    20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. (2021) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
    2021. Efficient content-based sparse attention with routing transformers. *Transactions
    of the Association for Computational Linguistics*, 9:53–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samsi et al. (2023) Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li,
    Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari,
    and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs
    of large language model inference. *arXiv preprint arXiv:2310.03003*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention
    with relative position representations. In *Proceedings of the 2018 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 2 (Short Papers)*, pages 464–468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Zhiqing Sun, Yiming Yang, and Shinjae Yoo. 2021. Sparse attention
    with learning to hash. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vyas et al. (2020) Apoorv Vyas, Angelos Katharopoulos, and François Fleuret.
    2020. Fast transformers with clustered attention. *Advances in Neural Information
    Processing Systems*, 33:21665–21674.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue:
    A stickier benchmark for general-purpose language understanding systems. *Advances
    in neural information processing systems*, 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
    Hao Ma. 2020. Linformer: Self-attention with linear complexity. *arXiv preprint
    arXiv:2006.04768*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2020. Transformers: State-of-the-art natural language processing. In *Proceedings
    of the 2020 conference on empirical methods in natural language processing: system
    demonstrations*, pages 38–45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Lowrank Structure in Other Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ Appendix A Lowrank Structure in Other Model ‣
    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") shows the
    low-rankness of BERT model on IMDB dataset. We can also observe that most sequences
    exhibists low-rank structures. In particular, long sequences are more low-rank,
    which is aligned with the observation in Sec [3](#S3 "3 Lowrank Structure in Sequences
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys").'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="A1.F7.pic1" class="ltx_picture" height="339.97" overflow="visible"
    version="1.1" width="553.15"><g transform="translate(0,339.97) matrix(1 0 0 -1
    0 0) translate(45.14,0) translate(0,41.96) matrix(1.0 0.0 0.0 1.0 -45.14 -41.96)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(-15.79,0) translate(0,41.96)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 108.5
    -13.81)" fill="#000000" stroke="#000000"><foreignobject width="17.68" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 164.92 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 221.33 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.5</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 277.75 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.6</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 334.16 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.7</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 390.58 -13.81)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.8</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 49.12 59.37)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 49.12 123.19)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 49.12 187.02)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">9</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 42.2 250.85)" fill="#000000" stroke="#000000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">12</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 293.15 -33.89)" fill="#000000" stroke="#000000"><foreignobject
    width="43.3" height="14.04" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\left\lceil
    2^{\mu}\right\rceil/L$</foreignobject></g><g transform="matrix(0.0 1.0 -1.0 0.0
    30.01 91.92)" fill="#000000" stroke="#000000"><foreignobject width="113.89" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">probability density</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 426.63 166.54)"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 44.965)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 8.99)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0
    3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject width="67.84" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[0,200]$</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 26.98)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="81.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[200,350]$</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 44.97)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignobject
    width="60.92" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$L\in[350,]$</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Distribution of low-rankness of BERT-base’s embedding on IMDB dataset,
    measured by ratio $\left\lceil 2^{\mu}\right\rceil/L$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Alternating Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data: $r,X,\left\{\bm{u}_{i}^{0},\bm{v}_{i}^{0}\right\}_{i=1}^{r}$Result: $\left\{\bm{u}_{i},\bm{v}_{i}\right\}_{i=1}^{r}$for *$i$
    in $1,\cdots,r$* do       for *$j$ in $1,\cdots,2$* do             /* Alternating
    optimization */             $\bm{u}_{i}^{j}=\frac{X\cdot\bm{v}_{i}^{j-1}}{\left\|\bm{v}_{i}^{j-1}\right\|_{F}^{2}}$;            
    $\bm{v}_{i}^{j}=\frac{X^{T}\cdot\bm{u}_{i}^{j}}{\left\|\bm{u}_{i}^{j}\right\|_{F}^{2}}$;      
    end for      $\bm{u}_{i},\bm{v}_{i}=\bm{u}_{i}^{j},{\bm{v}_{i}^{j^{T}}}$;      
    $X=X-\bm{u}_{i}^{j}\cdot{\bm{v}_{i}^{j^{T}}}$;end for'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Alternating Opt for SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Finetune Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For BERT-base and Llama2 models, we conduct a grid search on learning rate
    (1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4), and weight decay (1e-3, 5e-3, 1e-2, 5e-2).
    Table [5](#A3.T5 "Table 5 ‣ Appendix C Finetune Hyperparameters ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys") and [6](#A3.T6 "Table 6
    ‣ Appendix C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys") list the best hyperparameters found during fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| max len | batch size | epochs | $lr$ | $wd$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 512 | 32 | 20 | 5e-5 | 1e-2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Finetuning hyperparameters for BERT-base on SST-2, Squad, and IMDB.'
  prefs: []
  type: TYPE_NORMAL
- en: '| max len | batch size | iters | $lr$ | $wd$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2048 | 32 | 400 | 2e-4 | 1e-2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Finetuning hyperparameters for Llama 2-7B/13B on MMLU and BoolQ.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Prompt Format for MMLU and BoolQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [7](#A4.T7 "Table 7 ‣ Appendix D Prompt Format for MMLU and BoolQ ‣ ATP:
    Enabling Fast LLM Serving via Attention on Top Principal Keys") and [8](#A4.T8
    "Table 8 ‣ Appendix D Prompt Format for MMLU and BoolQ ‣ ATP: Enabling Fast LLM
    Serving via Attention on Top Principal Keys") list the prompt format for MMLU
    and BoolQ dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| The following are multiple choice questions (with answers). |'
  prefs: []
  type: TYPE_TB
- en: '| One of the reasons that the government discourages |'
  prefs: []
  type: TYPE_TB
- en: '| and regulates monopolies is that |'
  prefs: []
  type: TYPE_TB
- en: '| A. producer surplus is lost and consumer surplus is gained. |'
  prefs: []
  type: TYPE_TB
- en: '| B. monopoly prices ensure productive efficiency but cost |'
  prefs: []
  type: TYPE_TB
- en: '| society allocative efficiency. |'
  prefs: []
  type: TYPE_TB
- en: '| C. monopoly firms do not engage in significant research |'
  prefs: []
  type: TYPE_TB
- en: '| and development. |'
  prefs: []
  type: TYPE_TB
- en: '| D. consumer surplus is lost with higher prices and lower |'
  prefs: []
  type: TYPE_TB
- en: '| levels of output. |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: |'
  prefs: []
  type: TYPE_TB
- en: '| C |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: MMLU prompt format'
  prefs: []
  type: TYPE_NORMAL
- en: '| Below is an instruction that describes a task. Write a response |'
  prefs: []
  type: TYPE_TB
- en: '| that appropriately completes the request. |'
  prefs: []
  type: TYPE_TB
- en: '| ### Instruction: |'
  prefs: []
  type: TYPE_TB
- en: '| is harry potter and the escape from gringotts a roller coaster ride |'
  prefs: []
  type: TYPE_TB
- en: '| ### Input: |'
  prefs: []
  type: TYPE_TB
- en: '| Harry Potter and the Escape from Gringotts is an indoor steel |'
  prefs: []
  type: TYPE_TB
- en: '| roller coaster *** |'
  prefs: []
  type: TYPE_TB
- en: '| ### Answer: |'
  prefs: []
  type: TYPE_TB
- en: '| Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: BoolQ prompt format</foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
