# P4：4.Week 2 – Practicum_ Training a neural network - 大佬的迷弟的粉丝 - BV1o5411p7AB

实际上第一点是我们有一个网站，现在您可以找到一个网站，实际上第一点是我们有一个网站，现在您可以找到一个网站，您可以在我的github上的存储库中单击链接，然后得到。

您可以在我的github上的存储库中单击链接，然后得到，重定向到您拥有上一堂课摘要的网站，并且，重定向到您拥有上一堂课摘要的网站，并且，以前的实验是正确的，所以您有责任在上课之前仔细阅读这些总结。

以前的实验是正确的，所以您有责任在上课之前仔细阅读这些总结，否则，如果我花了前15分钟修改上次看到的内容，否则，如果我花了前15分钟修改上次看到的内容，那么我们只有不到15分钟的时间才能看到新内容。

当我们拥有，那么我们只有不到15分钟的时间才能看到新内容，当我们拥有，不过，星期二只有50分钟，我们从一个问题开始，不过，星期二只有50分钟，我们从一个问题开始，我想在猫和狗的图像之间进行分类。

我想在猫和狗的图像之间进行分类，如果这是我的猫的形象，那么我的狗的形象在这附近，如果这是我的猫的形象，那么我的狗的形象在这附近，如果这是什么，我们该如何首先将他们分开？如果这是什么。

我们该如何首先将他们分开？零，我将不得不交谈以假设该交谈在我在这里正在做什么，零，我将不得不交谈以假设该交谈在我在这里正在做什么，翻译你怎么翻译东西，矩阵乘法好吗矩阵乘法做旋转。

矩阵乘法好吗矩阵乘法做旋转，反射缩放并共享右切变，反射缩放并共享右切变，您还可以如何缩放？为什么标量被称为标量标量呢？您还可以如何缩放？为什么标量被称为标量标量呢？他们可以缩放，很好，很好。

所以您可以随时考虑一个矩阵，他们可以缩放，很好，很好，所以您可以随时考虑一个矩阵，只需对其进行归一化，就可以拥有一个统一的行列式，然后，只需对其进行归一化，就可以拥有一个统一的行列式，然后。

实际上有一个标量正在改变，您知道尺寸正确，所以您可以，实际上有一个标量正在改变，您知道尺寸正确，所以您可以，我通常将矩阵视为旋转，所以我通常只是说我们，我通常将矩阵视为旋转，所以我通常只是说我们。

在任何维度空间中旋转东西，然后我们将要做，在任何维度空间中旋转东西，然后我们将要做，神经网络的另一种操作将被挤压，因此您将，神经网络的另一种操作将被挤压，因此您将，是我重复很多次吗，是我重复很多次吗。

神经网络只是旋转和挤压旋转和挤压是，神经网络只是旋转和挤压旋转和挤压是，旋转之后来，然后很好，所以让我们开始吧。



![](img/126b0113214cb8c525411cf5a08df936_1.png)

旋转之后来，然后很好，所以让我们开始吧，开始，然后您就知道，因为我喜欢广告，开始，然后您就知道，因为我喜欢广告，我只是把手放在那儿，如果您有话要说就再打给我，我只是把手放在那儿。

如果您有话要说就再打给我，不知道发生了什么请重复，如果您不知道是谁。

![](img/126b0113214cb8c525411cf5a08df936_3.png)

不知道发生了什么请重复，如果您不知道是谁，伙计，你可能会对90年代的电视连续剧有所了解哦。

![](img/126b0113214cb8c525411cf5a08df936_5.png)

伙计，你可能会对90年代的电视连续剧有所了解哦，double n什么是人工神经网络我想是的，主管，double n什么是人工神经网络我想是的，主管，监督学习分类，因此将对，监督学习分类，因此将对。

他以前已经看过的东西，但是您以更漂亮的方式知道，因为，他以前已经看过的东西，但是您以更漂亮的方式知道，因为，我整天都在为您制作这些东西，所以让我们继续吧，我们，我整天都在为您制作这些东西。

所以让我们继续吧，我们，这个人对了，我们上次见过，我们说这些，这个人对了，我们上次见过，我们说这些，螺旋状的三个分支，所以在这种情况下，我的情况在哪里，螺旋状的三个分支，所以在这种情况下。

我的情况在哪里，数据存在，所以如果我在这里向您展示这些东西，这些数据在哪里？数据存在，所以如果我在这里向您展示这些东西，这些数据在哪里？分支由点组成，这些点位于r2右边的空间上，分支由点组成。

这些点位于r2右边的空间上，平面，所以所有这些点都在平面上移动为什么我要显示颜色，平面，所以所有这些点都在平面上移动为什么我要显示颜色，这些是正确的标签，所以三个不同的类别三个不同的标签。

这些是正确的标签，所以三个不同的类别三个不同的标签，您可以使用matplotlib和Python和numpy制作此绘图，您可以使用matplotlib和Python和numpy制作此绘图，另一边，哦。

是的，我们就像T从零变到一，然后，另一边，哦，是的，我们就像T从零变到一，然后，C将成为从1到大写C的班级，在这种情况下，让我们开始吧，C将成为从1到大写C的班级，在这种情况下，让我们开始吧，有点辣。

所以我们在那儿加些胡扯，有点辣，所以我们在那儿加些胡扯，看起来很糟糕的数据，实际上更抱歉，好吗，看起来很糟糕的数据，实际上更抱歉，好吗，如果您想进行工艺分类，那么分类意味着什么？如果您想进行工艺分类。

那么分类意味着什么？让我们用你知道你想要的任何东西，所谓的东西，没有，让我们用你知道你想要的任何东西，所谓的东西，没有，回归逻辑回归，那么逻辑回归在这里做了什么，回归逻辑回归。

那么逻辑回归在这里做了什么，情况，所以它将像这样做一些正确的线性平面，情况，所以它将像这样做一些正确的线性平面，分离数据这是第二个主要问题，不是真的，分离数据这是第二个主要问题，不是真的，可以接受。

但是这里的主要问题是如何定义，可以接受，但是这里的主要问题是如何定义，这里的问题像是的，它们不是线性可分离的，所以你有，这里的问题像是的，它们不是线性可分离的，所以你有，是的，但是从那幅画上看。

您不喜欢什么？是的，但是从那幅画上看，您不喜欢什么？在一个区域中的点，您有多个多个类，这意味着，在一个区域中的点，您有多个多个类，这意味着，这些分支越过了我的线性决策边界。

这些分支越过了我的线性决策边界，那么我们该如何解决呢，因为从上次可以看到我的视频，那么我们该如何解决呢，因为从上次可以看到我的视频，通常这么一个星期人们怎么说，通常这么一个星期人们怎么说。

知道像使那些决策边界是非线性的，这是什么用途或，知道像使那些决策边界是非线性的，这是什么用途或，其他教授吗我只会冷却更酷的东西，所以我得到，其他教授吗我只会冷却更酷的东西，所以我得到。

数据是线性可分离的，这与，数据是线性可分离的，这与，无论如何，看到相同的东西这里的主要问题是我们有交集，无论如何，看到相同的东西这里的主要问题是我们有交集，在这些决策边界和数据之间，因此我将尝试做。

在这些决策边界和数据之间，因此我将尝试做，没错，您已经看过上次的视频了，好吧，这不是，没错，您已经看过上次的视频了，好吧，这不是，新闻或您可能会更有用的事物，看看决策边界如何。



![](img/126b0113214cb8c525411cf5a08df936_7.png)

新闻或您可能会更有用的事物，看看决策边界如何，过度培训将尝试适应您所知道的分布，过度培训将尝试适应您所知道的分布，这些数据在这里，我们正在从头开始观察事情，因此，如果您有，这些数据在这里。

我们正在从头开始观察事情，因此，如果您有，第一层在最下面，最后一层在最上面的网络，第一层在最下面，最后一层在最上面的网络，如果您按该方向绘制网络，您的成绩就会降低一级，因此，如果您按该方向绘制网络。

您的成绩就会降低一级，因此，输入将在底部为什么是为什么我们在底部有输入，输入将在底部为什么是为什么我们在底部有输入，有谁能猜到他们的神经网络吗？有谁能猜到他们的神经网络吗？正确的权利，所以我们在。

正确的权利，所以我们在，网络的下部，当您向上爬入层次结构时，您基本上，网络的下部，当您向上爬入层次结构时，您基本上，想要以正确的方式绘制此网络，以便您拥有较高的，想要以正确的方式绘制此网络。

以便您拥有较高的，上部的层次结构，因此，如果您放置一些分类器，则会放置一个，上部的层次结构，因此，如果您放置一些分类器，则会放置一个，右上角的分类器，所以如果您是第一次我现在正在阅读论文。

右上角的分类器，所以如果您是第一次我现在正在阅读论文，来自乔佛里（Joffrey），他有点像是等等，我在上面放了一个分类器，来自乔佛里（Joffrey），他有点像是等等，我在上面放了一个分类器。

我想知道什么是网络的顶端，我想知道什么是网络的顶端，正确，因此网络是从下到上绘制的，第一层位于，正确，因此网络是从下到上绘制的，第一层位于，底部是您输入的较低层功能，是您向上爬的地方。

底部是您输入的较低层功能，是您向上爬的地方，您拥有顶部，因此，如果您有多个输出，则称为，您拥有顶部，因此，如果您有多个输出，则称为，多头网络就像hydra hydra一样，无论如何称呼。

多头网络就像hydra hydra一样，无论如何称呼，我们将在本课中弄清楚我们如何做这些事情，我们将在本课中弄清楚我们如何做这些事情，知道如何做这些东西否是好的你应该是因为你应该。

知道如何做这些东西否是好的你应该是因为你应该，在之前学习过机器学习，但也许不是线性可分的吧，在之前学习过机器学习，但也许不是线性可分的吧，所以我们只增加了一层，一切就开始起作用。



![](img/126b0113214cb8c525411cf5a08df936_9.png)

所以我们只增加了一层，一切就开始起作用，训练数据还可以，所以昨天上周还不错，我们已经看到，训练数据还可以，所以昨天上周还不错，我们已经看到，网络在您初始化时会进行某种转换，因此我们。

网络在您初始化时会进行某种转换，因此我们，在墙壁上喂那种云，在墙壁上喂那种云，高斯分布，单位矩阵为协方差矩阵，均值，高斯分布，单位矩阵为协方差矩阵，均值，零，那么您在点300中该X的大致平均半径是多少。

零，那么您在点300中该X的大致平均半径是多少，真的记得很好，半径是3对，所以东西在，真的记得很好，半径是3对，所以东西在，半径3和那种圆，半径3和那种圆，它被馈送到网络内部，网络使您获得任何。

它被馈送到网络内部，网络使您获得任何，任意变换，非常漂亮，是的，非常漂亮，任意变换，非常漂亮，是的，非常漂亮，是的很好，但是这种转变对做正确的事没有帮助，是的很好，但是这种转变对做正确的事没有帮助。

所以今天我们要看看如何通过使用数据来实现某种意义，所以今天我们要看看如何通过使用数据来实现某种意义，通过网络本身进行的这种转换，因此数据是，通过网络本身进行的这种转换，因此数据是，将是最重要的部分。

因此在这里我们应该拥有，将是最重要的部分，因此在这里我们应该拥有，墨水，是的，这里太亮了，所以无论如何X都会成为我的，墨水，是的，这里太亮了，所以无论如何X都会成为我的，输入数据都是因为代表一个向量。

而这个家伙住在我们的末端，输入数据都是因为代表一个向量，而这个家伙住在我们的末端，好吧，在我们的案例二中，n是多少，因为点依赖于该点，好吧，在我们的案例二中，n是多少，因为点依赖于该点，正确的空间。

希望这些尖顶可以很棒，那将是我的冰，正确的空间，希望这些尖顶可以很棒，那将是我的冰，样品我有几个样品合适，这需要很长时间才能画出，样品我有几个样品合适，这需要很长时间才能画出，几个样本，它们就像行向量。

我把它们叠起来，几个样本，它们就像行向量，我把它们叠起来，在彼此之上，我有M个，所以矩阵是什么，在彼此之上，我有M个，所以矩阵是什么，这个矩阵的大小大n个n个，所以我有n列和M。

这个矩阵的大小大n个n个，所以我有n列和M，如果我要使用这些指标来进行一些操作，如果我要使用这些指标来进行一些操作，是我要拍摄的尺寸，再试一次，好吧，因为矩阵的高度将是尺寸，再试一次，好吧。

因为矩阵的高度将是尺寸，在哪里拍摄，然后在更大范围内等待指标，在哪里拍摄，然后在更大范围内等待指标，您从右边拍摄的地方，因为您乘以就知道列时间，行时间，您从右边拍摄的地方，因为您乘以就知道列时间。

行时间，正确的代码，很好，很抱歉，我们有CI，这将是我的，正确的代码，很好，很抱歉，我们有CI，这将是我的，二维平面中每个点的类别不同，所以在这里，二维平面中每个点的类别不同，所以在这里。

在C为大写CI之前，它们的C等于K等于1，在C为大写CI之前，它们的C等于K等于1，看看如何解决，我知道，所以这里的资本K是多少3，因为我们，看看如何解决，我知道，所以这里的资本K是多少3，因为我们。

看过三种颜色，是的，棒极了，所以如果我把所有，看过三种颜色，是的，棒极了，所以如果我把所有，看看我有多少个CIS我​​有第二个M，所以您应该将这些家伙堆叠在一起，看看我有多少个CIS我​​有第二个M。

所以您应该将这些家伙堆叠在一起，得到一个可能的列向量在这里C好吧，这是M的高度，得到一个可能的列向量在这里C好吧，这是M的高度，但是这种表示法的问题，例如1 2 3，但是这种表示法的问题。

例如1 2 3，是他们引入了某种订购权，所以第一堂课比，是他们引入了某种订购权，所以第一堂课比，第2类是免费的，它是没有意义的，第2类是免费的，它是没有意义的，颜色，所以这是我不想拥有的绝对分布，颜色。

所以这是我不想拥有的绝对分布，也有命令的东西，所以我要用这种选择，也有命令的东西，所以我要用这种选择，我要基本上将那些T转换成，我要基本上将那些T转换成，我的资本k的大小的向量，所以类的数量，然后我要。

我的资本k的大小的向量，所以类的数量，然后我要，对应的类具有1，请参见具体索引，对应的类具有1，请参见具体索引，CI好吧，假设CI等于1，那么您基本上拥有，CI好吧，假设CI等于1，那么您基本上拥有。

这里的第一个人好，既然我们在谈论数学，我可以指望，这里的第一个人好，既然我们在谈论数学，我可以指望，从1右边1 2 3如果您正在谈论Python和C ++，那么您。

从1右边1 2 3如果您正在谈论Python和C ++，那么您，开关设备，您有不同的火力，您可以从0开始计算数学，从1开始进行计数，开关设备，您有不同的火力，您可以从0开始计算数学，从1开始进行计数。

因此，如果您将所有这些C堆叠成该表示形式，您会得到什么，因此，如果您将所有这些C堆叠成该表示形式，您会得到什么，没有数字，只是使用字母，您拥有M×K矩阵，所以这是，没有数字，只是使用字母。

您拥有M×K矩阵，所以这是，将成为我的资本y矩阵，那么你有我的资本K列数，将成为我的资本y矩阵，那么你有我的资本K列数，和M的行数，每个这些家伙都会是一个向量，和M的行数，每个这些家伙都会是一个向量。

是设置为K的零个，其中只有1个是指仅设置一项，是设置为K的零个，其中只有1个是指仅设置一项，一，所以你可以说零范数等于一，一，所以你可以说零范数等于一，也想想这个符号大概有声音质量。

也想想这个符号大概有声音质量，完全集中在一个特定的位置，所以您有三个，完全集中在一个特定的位置，所以您有三个，可能的地点，您有三个可能的班级，可能的地点，您有三个可能的班级。

您将所有100％下注都放在该网络将尝试的特定类别上，您将所有100％下注都放在该网络将尝试的特定类别上，大概我无法做到，但这就是我们训练网络的方式，大概我无法做到，但这就是我们训练网络的方式。

到目前为止，这类硬标签问题很抱歉，是运动问题，所以，到目前为止，这类硬标签问题很抱歉，是运动问题，所以，我真的太慢了​​吗，我真的太慢了​​吗，不，你喜欢颜色的字体吗，谢谢，不，你喜欢颜色的字体吗。

谢谢，永远需要好的乳胶，这就是为什么我们转向降价销售的原因，永远需要好的乳胶，这就是为什么我们转向降价销售的原因，好吧，好吧，这基本上是您的第一项练习，好吧，好吧，这基本上是您的第一项练习。

在您的第一次家庭作业中有类似的内容，因此我们跳过此步骤，因为它将，在您的第一次家庭作业中有类似的内容，因此我们跳过此步骤，因为它将，应该有两个星期的时间，基本上，如果这将是一个教程，它将有。

应该有两个星期的时间，基本上，如果这将是一个教程，它将有。

![](img/126b0113214cb8c525411cf5a08df936_11.png)

一直在输入东西，所以让我们看看如何完全联网，一直在输入东西，所以让我们看看如何完全联网。

![](img/126b0113214cb8c525411cf5a08df936_13.png)

连接让网络正常工作，看起来像在底部，Y在底部，连接让网络正常工作，看起来像在底部，Y在底部，[音乐]再说一遍，输入是在底部Y，[音乐]再说一遍，输入是在底部Y。

低级功能太棒了X粉红色是什么颜色我的意思是，那是，低级功能太棒了X粉红色是什么颜色我的意思是，那是，正确，但是是的，我们在那里得到了一个很好的变换，正确，但是是的，我们在那里得到了一个很好的变换，箭头。

然后进入绿色F，其中F将是非线性的，箭头，然后进入绿色F，其中F将是非线性的，F的输出将称为H h，它代表我的隐藏层，F的输出将称为H h，它代表我的隐藏层，好吧，H是网络内部的东西，我从外面看不到。

好吧，H是网络内部的东西，我从外面看不到，所以它被称为“隐藏”，因为它是矢量，所以它被称为“隐藏”，因为它是矢量，此外，我还有另一个仿射变换，您只能看到矩阵，此外，我还有另一个仿射变换，您只能看到矩阵。

那里映射到G这是另一个线性机会另一个非线性，那里映射到G这是另一个线性机会另一个非线性，转换，现在您的最终输出是y hat，因此，转换，现在您的最终输出是y hat，因此，输出的颜色不是白色。

而是气泡的输出颜色，输出的颜色不是白色，而是气泡的输出颜色，向右蓝色，然后隐藏的部分将变为绿色，它将始终保持不变，向右蓝色，然后隐藏的部分将变为绿色，它将始终保持不变，好吧。

这些基本上是您将要看到的唯一问题，好吧，这些基本上是您将要看到的唯一问题，在这个过程中，您的隐藏层H向量将是非线性的，在这个过程中，您的隐藏层H向量将是非线性的，函数逐点非线性函数，即元素逐阶非线性。

函数逐点非线性函数，即元素逐阶非线性，输入的精细转换函数，这里是X加上您知道，输入的精细转换函数，这里是X加上您知道，偏权是正确的，所以这是一个线性算子加上偏倚，这是很好的，偏权是正确的。

所以这是一个线性算子加上偏倚，这是很好的，变换，然后F将再次成为您的非线性映射，然后您，变换，然后F将再次成为您的非线性映射，然后您，有你的Y帽子，这将是我的网络输出，我的假设是，有你的Y帽子。

这将是我的网络输出，我的假设是，将成为非线性函数，我将在此处将其应用于该向量的每个元素，并，将成为非线性函数，我将在此处将其应用于该向量的每个元素，并，这个向量基本上是隐藏层的仿射变换。

这个向量基本上是隐藏层的仿射变换，这就是我通常所说的神经网络仿射变换中的全部功能，这就是我通常所说的神经网络仿射变换中的全部功能，他们把旋转称为非线性函数，他们把旋转称为非线性函数。

所以你只需重复旋转挤压旋转挤压旋转，所以你只需重复旋转挤压旋转挤压旋转，压扁棒，谢谢，好吧，到目前为止，这很容易，压扁棒，谢谢，好吧，到目前为止，这很容易，知道你有一个问题。

迈克·迈森（Mike interphase）问怎么回事，是的，F和G都是，知道你有一个问题，迈克·迈森（Mike interphase）问怎么回事，是的，F和G都是，任意非线性函数。

您可以使用任何您喜欢的东西，这仅仅是一个，任意非线性函数，您可以使用任何您喜欢的东西，这仅仅是一个，隐藏层我的输出层将是我的蓝家伙，让我们添加输出ISM，隐藏层我的输出层将是我的蓝家伙。

让我们添加输出ISM，可以在顶部看到Y帽，因此如果输出和X将成为您在，可以在顶部看到Y帽，因此如果输出和X将成为您在，底部，所以我将其称为三层神经网络Jung看起来像他，底部。

所以我将其称为三层神经网络Jung看起来像他，称此为您的网络，我称其为“免费日”，因为，称此为您的网络，我称其为“免费日”，因为，底部有一个输入神经元，中间有一个隐藏的神经元，底部有一个输入神经元。

中间有一个隐藏的神经元，它是一个输出，所以一二三，但他从零开始计数，它是一个输出，所以一二三，但他从零开始计数，程序员后来如此真实，但是没有看到多少仿射变换，程序员后来如此真实。

但是没有看到多少仿射变换，一个三层神经网络，神奇的是你有多少层神经元，一个三层神经网络，神奇的是你有多少层神经元，有三个好很酷的问题，哈哈是的，很好的转换权，所以也有翻译，因为我，哈哈是的。

很好的转换权，所以也有翻译，因为我，通常喜欢从矩阵中提取缩放比例和缩放比例，然后，通常喜欢从矩阵中提取缩放比例和缩放比例，然后，我的ary行列式行列式矩阵基本上是旋转的东西。

我的ary行列式行列式矩阵基本上是旋转的东西，然后你有另一个正在缩放，他们也向右翻转，然后你有另一个正在缩放，他们也向右翻转，因为如果你有行列式是负数，因为如果你有行列式是负数，通常矩阵只是旋转的东西。

很难想像，通常矩阵只是旋转的东西，很难想像，我只是说尺寸是矩阵旋转的东西，因为它们应用相同的类型，我只是说尺寸是矩阵旋转的东西，因为它们应用相同的类型，一切运动的正确性，这是一种全球运作的其他问题。

一切运动的正确性，这是一种全球运作的其他问题，好的，所以非线性函数的例子是，好的，所以非线性函数的例子是，这里有一些，所以第一个是积极的部分，基本上你会得到积极的，这里有一些，所以第一个是积极的部分。

基本上你会得到积极的，如果它是负数，则将其设置为零，其他人则将其称为300纠正，如果它是负数，则将其设置为零，其他人则将其称为300纠正，内部单元或其他东西，我不知道是的，我喜欢正面部分。

内部单元或其他东西，我不知道是的，我喜欢正面部分，是数学式的，则有一个S形，即S的一加一，是数学式的，则有一个S形，即S的一加一，减去任何参数双曲正切，这只是一个重新缩放的版本，减去任何参数双曲正切。

这只是一个重新缩放的版本，关于这个乙状结肠，我们上次看到的是，关于这个乙状结肠，我们上次看到的是，之所以这样称呼它，是因为它只是一个较软的版本，之所以这样称呼它，是因为它只是一个较软的版本。

narc max一个参数将为您提供全零，但索引等于1，narc max一个参数将为您提供全零，但索引等于1，对应正确的最大值，它的软最大值会给你，对应正确的最大值，它的软最大值会给你，像这样的东西。

其中最高值是0时几乎是1，像这样的东西，其中最高值是0时几乎是1，到处都是其他地方，但是如果你有两个身高相同的家伙，到处都是其他地方，但是如果你有两个身高相同的家伙，会得到一半，其余的将是0，是的。

是的，会得到一半，其余的将是0，是的，是的，我想这是一个很好的派生，我认为这家伙很容易用于训练，我想这是一个很好的派生，我认为这家伙很容易用于训练，我认为您可以在西班牙使用这种归一化方法。

我认为您可以在西班牙使用这种归一化方法，所以问题就像我们为什么不使用调整大小之类的，所以问题就像我们为什么不使用调整大小之类的，您会自动将输出设置为0到1范围内，因为，您会自动将输出设置为0到1范围内。

因为，更改输出后，它将取决于输出的输出，更改输出后，它将取决于输出的输出，您必须随时更改缩放比例这只是一个缩放比例，您必须随时更改缩放比例这只是一个缩放比例，我猜是一样的方式，可能是答案。

我猜是一样的方式，可能是答案，好的，嗯，嗯，这花了五个小时才画好，我们在X上，好的，嗯，嗯，这花了五个小时才画好，我们在X上，左侧有五个元素，那么在这里您有例如，左侧有五个元素，那么在这里您有例如。

我可能有第二个隐藏层，我可能有第二个隐藏层，第三个隐藏层，最后是我的输出层，所以有多少层，第三个隐藏层，最后是我的输出层，所以有多少层，这个网络有，你可以在这里数多少列好吧，好极了，有多少空白。

你可以在这里数多少列好吧，好极了，有多少空白，在列之间，您可以解释我们的宝贝旋转吗，还可以，在列之间，您可以解释我们的宝贝旋转吗，还可以，好吧，所以我们从第一层开始，也称为手淫，我们开始，好吧。

所以我们从第一层开始，也称为手淫，我们开始，在第一层激活，我们转到第二层激活，依此类推，在第三层，在第一层激活，我们转到第二层激活，依此类推，在第三层，八四，直到一个大写字母L，最后一个。

所以我们从激活处得到，八四，直到一个大写字母L，最后一个，所以我们从激活处得到，从第1层到激活层到W一个矩阵，基本上，您将W带入以色列，从第1层到激活层到W一个矩阵，基本上，您将W带入以色列。

那里有两个W 3，依此类推，所以如何获得第一个神经元，那里有两个W 3，依此类推，所以如何获得第一个神经元，好的，让我看看我能不能使它变黑是更好的选择，好的，让我看看我能不能使它变黑是更好的选择，好的。

无论我如何发展，只要有人在纸上做笔记，好的，无论我如何发展，只要有人在纸上做笔记，对不起，好吧，我打开灯后好吧，那你如何获得价值，对不起，好吧，我打开灯后好吧，那你如何获得价值，这个人在这里。

所以这个人将成为我那一层的Jeff神经元，这个人在这里，所以这个人将成为我那一层的Jeff神经元，第二层，所以是两层，这将是我的明智之举，对不起，第二层，所以是两层，这将是我的明智之举，对不起。

非线性函数f，其中我有wj，这是w1矩阵的jf作用，非线性函数f，其中我有wj，这是w1矩阵的jf作用，它乘以X，所以我有一个Rho乘以一个向量，你得到一个标量谢谢，它乘以X。

所以我有一个Rho乘以一个向量，你得到一个标量谢谢，然后再加上BJ是什么BJ是一个标量是的，正确，然后再加上BJ是什么BJ是一个标量是的，正确，也叫偏见好吧，有什么选择的话不好意思，也叫偏见好吧。

有什么选择的话不好意思，这基本上就像是标量乘法的和，这基本上就像是标量乘法的和，我的意思是乘法的总和，所以你如何得到这些，我的意思是乘法的总和，所以你如何得到这些，这些家伙，您将它们乘以权重。

然后得到第一个家伙，这些家伙，您将它们乘以权重，然后得到第一个家伙，正确，然后第二个正确的蓝色复制和粘贴不起作用，正确，然后第二个正确的蓝色复制和粘贴不起作用，所以你再画所有的线，所以你再画所有的线。

您开始意识到在绘制所有内容时您做出了非常错误的决定，您开始意识到在绘制所有内容时您做出了非常错误的决定，否则还可以，所以这些权重在哪里？否则还可以，所以这些权重在哪里？快速前进，您知道快速前进，是的。

这是PowerPoint Ninja的效果如何，快速前进，您知道快速前进，是的，这是PowerPoint Ninja的效果如何，技能还可以，我应该为此做广告，这应该付钱，技能还可以，我应该为此做广告。

这应该付钱，我微软很好，所以这在您的网络上正确，我微软很好，所以这在您的网络上正确，谢谢，好吧，我在这里给你照亮，我应该打开右边，谢谢，好吧，我在这里给你照亮，我应该打开右边。

我想我拥有的一切都让我知道是否可以关闭第一线，我想我拥有的一切都让我知道是否可以关闭第一线，好吧，我们下次必须弄清楚，我猜很好，所以我们只用这个。



![](img/126b0113214cb8c525411cf5a08df936_15.png)

好吧，我们下次必须弄清楚，我猜很好，所以我们只用这个，表示您之前观察过的每个图层的位置，表示您之前观察过的每个图层的位置，我在这里只压缩一个螺栓，所以在这种情况下，H是对的向量，因此。

我在这里只压缩一个螺栓，所以在这种情况下，H是对的向量，因此，任意数量的元素的向量都只是在，任意数量的元素的向量都只是在，屏幕和Y相同，所以在这里您有那些矩阵，屏幕和Y相同，所以在这里您有那些矩阵。

拍摄正确的方向，正确的尺寸，然后在这里，拍摄正确的方向，正确的尺寸，然后在这里，具有非线性函数，所以您可以在这里考虑我的白帽子，具有非线性函数，所以您可以在这里考虑我的白帽子。

是我当前输入X的某种Y帽子功能，所以X变成粉红色，是我当前输入X的某种Y帽子功能，所以X变成粉红色，家伙进入网络，然后网络会给我一些你，家伙进入网络，然后网络会给我一些你，知道我希望您具有这种输出预测。

因此可以认为，知道我希望您具有这种输出预测，因此可以认为，作为将RN映射到我们的C的函数，其中C是，作为将RN映射到我们的C的函数，其中C是，大写K，但我想我必须解决此问题，以便您可以将其视为映射。

大写K，但我想我必须解决此问题，以便您可以将其视为映射，通常对最终预测的输入最好以不同的方式思考，这样，通常对最终预测的输入最好以不同的方式思考，这样，发生的事情是您实际上遇到了这些。

发生的事情是您实际上遇到了这些，表示Rd，然后您最终进入最终分类，表示Rd，然后您最终进入最终分类，内层的尺寸很大，内层的尺寸很大，比输入和输出尺寸大的原因是因为，比输入和输出尺寸大的原因是因为。

你进入一个非常高的空间，你进入一个非常高的空间，真的真的真的很远，所以如果事情很遥远，很容易，真的真的真的很远，所以如果事情很遥远，很容易，旋转东西，让东西移动一点，然后就走了，旋转东西。

让东西移动一点，然后就走了，一切都是很小的碎屑空间，您尝试将一切移动，一切都是很小的碎屑空间，您尝试将一切移动，会一起移动，但是如果您进入这个中间空间，会一起移动，但是如果您进入这个中间空间。

一切都相距甚远，您只需踢一下就可以了，这很多，一切都相距甚远，您只需踢一下就可以了，这很多，更容易，所以去一个更高的中间表示维中间，更容易，所以去一个更高的中间表示维中间，表示这真的非常非常有帮助。

因此潜在地您可以，表示这真的非常非常有帮助，因此潜在地您可以，非常胖的网络，您有一个非常胖的隐藏层输入，只是一个输出，非常胖的网络，您有一个非常胖的隐藏层输入，只是一个输出，最酷的部分是。

如果我在隐藏层中有一百个神经元，我可以简单地，最酷的部分是，如果我在隐藏层中有一百个神经元，我可以简单地，使用十个神经元的两个隐藏层，将大致执行，使用十个神经元的两个隐藏层，将大致执行，同样。

所以我不用拥有非常非常胖的中间层，同样，所以我不用拥有非常非常胖的中间层，决定堆叠一些隐藏层以及这些组合的数量，决定堆叠一些隐藏层以及这些组合的数量，您知道的神经元将成倍增长，所以如果您想要。

您知道的神经元将成倍增长，所以如果您想要，1000层那1000元打了一层你就可以拥有三个隐藏层了，1000层那1000元打了一层你就可以拥有三个隐藏层了，十对，所以在第二种情况下，如果您有一系列的东西。

您将，十对，所以在第二种情况下，如果您有一系列的东西，您将，有数据依赖性，您将不得不等待家伙完成，有数据依赖性，您将不得不等待家伙完成，在开始下一个操作之前，根据定义，您可以堆叠更多的层。

在开始下一个操作之前，根据定义，您可以堆叠更多的层，遇到其他情况时，您的速度会变慢，遇到其他情况时，您的速度会变慢，在另一种情况下，为了能够与您一样好，在另一种情况下，为了能够与您一样好。

通过仅堆叠其中一些层，在其他情况下，您将拥有，通过仅堆叠其中一些层，在其他情况下，您将拥有，如果您瘫痪了，我也可以做很多计算，如果您瘫痪了，我也可以做很多计算，电源并行硬件。

那么您可能更喜欢那些大型版本，但我，电源并行硬件，那么您可能更喜欢那些大型版本，但我，不会因为那些是正确的是不好的，所以，不会因为那些是正确的是不好的，所以，还需要占用更多的内存空间。

那么多的时间我们就会发出噪音，还需要占用更多的内存空间，那么多的时间我们就会发出噪音，好吧，我不能看我的手表好吧，没关系，好吧，我不能看我的手表好吧，没关系，好的，很好，在这种情况下。

我的输入位于二维空间中，好的，很好，在这种情况下，我的输入位于二维空间中，我的隐藏层将在我的有限空间中居住一个100维空间，我的隐藏层将在我的有限空间中居住一个100维空间，上课。

我的最终输出将生活在三维空间中，上课，我的最终输出将生活在三维空间中，这将是您开始使用计算机进行操作的第二部分，但是，这将是您开始使用计算机进行操作的第二部分，但是，我们上课没有时间。

所以神经网络训练了吧，我们上课没有时间，所以神经网络训练了吧，它是如何工作的，所以我们在这里使用这个人，它是软件的arg max，它是如何工作的，所以我们在这里使用这个人，它是软件的arg max。

最高艺术权利的最高版本，仅是，最高艺术权利的最高版本，仅是，指数除以其他所有项的所有指数之和，指数除以其他所有项的所有指数之和，我们昨天已经看到了这一点，为什么我要在零和，我们昨天已经看到了这一点。

为什么我要在零和，一个为什么不为什么我不使用方括号，所以答案是，一个为什么不为什么我不使用方括号，所以答案是，我不太可能说这不可能，为什么不可能，因为，我不太可能说这不可能，为什么不可能，因为。

分子的指数函数严格为正，分子的指数函数严格为正，没有达到一个，因为指数严格为正，没有达到一个，因为指数严格为正，是正确的答案吗，是的，底部不一定总是略微，是正确的答案吗，是的，底部不一定总是略微。

大于分子右好，所以输入要软Arg，大于分子右好，所以输入要软Arg，最大层称为逻辑，逻辑是输出的线性输出，最大层称为逻辑，逻辑是输出的线性输出，网络在这里，我们将损失全部损失，网络在这里。

我们将损失全部损失，我们拥有的旧数据集是首都的功能，但可以预测，我们拥有的旧数据集是首都的功能，但可以预测，整个输入集的网络和向量C，向量C，整个输入集的网络和向量C，向量C，标签。

而这将仅仅是此小写字母和，标签，而这将仅仅是此小写字母和，卷曲，基本上是每个样本损失，卷曲，基本上是每个样本损失，Yun会用大写字母L表示这种情况，如果我愿意，在这种情况下。

Yun会用大写字母L表示这种情况，如果我愿意，在这种情况下，分类我的每个样本损失基本上是减去的对数，分类我的每个样本损失基本上是减去的对数，在正确的C类上的软琼脂最大输出，所以C会成为我的。

在正确的C类上的软琼脂最大输出，所以C会成为我的，正确的类别是索引1很热，实际上是红衣主教或，正确的类别是索引1很热，实际上是红衣主教或。

Deena no Cardinal number正确的蓝色为什么是一种热门编码，而我为什么，Deena no Cardinal number正确的蓝色为什么是一种热门编码，而我为什么。

为什么热将成为网络的输出，即网络的输出，为什么热将成为网络的输出，即网络的输出，柔和的论据，所以我的简单话也将成为那黑暗的减数记录，柔和的论据，所以我的简单话也将成为那黑暗的减数记录。

最大是正确的案例类，所以您了解我说的是吗，最大是正确的案例类，所以您了解我说的是吗，只是为了测试您的理解，您将要告诉我下一步是什么，所以，只是为了测试您的理解，您将要告诉我下一步是什么，所以，假设是的。

这种损失也称为交叉熵或对数可能性为负，假设是的，这种损失也称为交叉熵或对数可能性为负，假设我的粉红色X（看起来是白色）在那里，然后我上课了，假设我的粉红色X（看起来是白色）在那里，然后我上课了。

等于1的橙色等级，因此，我的蓝色y会是什么，等于1的橙色等级，因此，我的蓝色y会是什么，c100类的编码好吧，好吧，好吧，比方说，我喂这些蛋，c100类的编码好吧，好吧，好吧，比方说，我喂这些蛋。

到我的神经网络，所以我要计算这些Y或缺少白色的情况对不起，到我的神经网络，所以我要计算这些Y或缺少白色的情况对不起，不好，所以这里是该产品的Y右Y帽，所以为什么我几乎不写，不好。

所以这里是该产品的Y右Y帽，所以为什么我几乎不写，对不起对不起我不好，所以我把这些鸡蛋放在这顶Y帽内，对不起对不起我不好，所以我把这些鸡蛋放在这顶Y帽内，网络的输出将是几乎1几乎0几乎0什么是。

网络的输出将是几乎1几乎0几乎0什么是，差不多1，容易1加或1减去几乎为0 0加上非常好，最后一个又如何，容易1加或1减去几乎为0 0加上非常好，最后一个又如何，一个0加好，好吧，如果我有这个。

一个0加好，好吧，如果我有这个，家伙在这里，我损失的人将是什么，家伙在这里，我损失的人将是什么，所以我的损失百分比将是这些输出几乎1几乎0几乎0并且。

所以我的损失百分比将是这些输出几乎1几乎0几乎0并且，然后第一节课好，那我该怎么办，所以在这里，然后第一节课好，那我该怎么办，所以在这里，计算C的CY帽的y帽是什么，所以是，计算C的CY帽的y帽是什么。

所以是，C几乎是1等于1-好的，那么您有1的对数-所以它将，C几乎是1等于1-好的，那么您有1的对数-所以它将，是0-前面是负号，您得到0加死亡正确率非常好，是0-前面是负号。

您得到0加死亡正确率非常好，如果您在我的网络中将这些鸡蛋输入我的基础上，那基本上如何，如果您在我的网络中将这些鸡蛋输入我的基础上，那基本上如何，期望该输入为1，我的网络说1 0 0损失会。

期望该输入为1，我的网络说1 0 0损失会，基本上是说你知道说废话会是惩罚，基本上是说你知道说废话会是惩罚，哦，没有罚款，你做得很好，所以让我们看看这是什么，哦，没有罚款，你做得很好。

所以让我们看看这是什么，如果我的网络说哦，不，不，那么简单，是0 1 0，所以我的百分比，如果我的网络说哦，不，不，那么简单，是0 1 0，所以我的百分比，0 1 0＆1的损失将是什么。

那么C的帽子几乎为零是什么样的，0 1 0＆1的损失将是什么，那么C的帽子几乎为零是什么样的，1 1 0 0加上0的对数加上负无穷大是多少，1 1 0 0加上0的对数加上负无穷大是多少，在前面。

所以这个家伙对正无穷大，所以这就是为什么，在前面，所以这个家伙对正无穷大，所以这就是为什么，再加上，如果你只是写无穷大，那对我来说是不对的，再加上无穷大，再加上，如果你只是写无穷大，那对我来说是不对的。

再加上无穷大，负无穷大是三种不同的动物，负无穷大是三种不同的动物，好的很有道理，所以如果您的网络说废话，您会说加无穷大，好的很有道理，所以如果您的网络说废话，您会说加无穷大。

非常差的网络如果网络说没有正确的答案，您只说全部，非常差的网络如果网络说没有正确的答案，您只说全部，对，你做得很好，是的。第二个对不起，所以为什么火热的问题，如果你a着眼睛看，你可能看不到这个问题。

第二个对不起，所以为什么火热的问题，如果你a着眼睛看，你可能看不到这个问题，灰色一点，您有白帽子，将取决于您的输入X，灰色一点，您有白帽子，将取决于您的输入X，通过两次旋转并挤压x'。

一次挤压是积极的部分，通过两次旋转并挤压x'，一次挤压是积极的部分，另一个壁球Bhindi soft arc max正确，所以如果您使用这些，另一个壁球Bhindi soft arc max正确。

所以如果您使用这些，软Arg max顶部的样品损失，那么您会得到这种交叉，软Arg max顶部的样品损失，那么您会得到这种交叉，熵项，然后如果您计算这里写的内容，基本上，熵项。

然后如果您计算这里写的内容，基本上，如果您输入x并且您的网络是正确的，则表示您的网络很好，如果您输入x并且您的网络是正确的，则表示您的网络很好，如果您刺中您的网络并说这是胡说八道，那您就说不好。

如果您刺中您的网络并说这是胡说八道，那您就说不好，知道不良的网络秩序是有道理的，所以Y hat是一个向量，我选择抓住，知道不良的网络秩序是有道理的，所以Y hat是一个向量，我选择抓住，项。

以便您可以看到，您可以考虑一下这个是y下标C，但是，项，以便您可以看到，您可以考虑一下这个是y下标C，但是，C实际上是的，C将会是一二或三，所以它将会是，C实际上是的，C将会是一二或三，所以它将会是。

基本上是y1或Y热工具Y帽子三，所以这三个元素之一，基本上是y1或Y热工具Y帽子三，所以这三个元素之一，Y帽子的意义不对，是的，你可以说不，我回答你的问题了吗，对，Y，Y帽子的意义不对，是的。

你可以说不，我回答你的问题了吗，对，Y，帽子，它是三个元素三个元素的向量，帽子，它是三个元素三个元素的向量，是的，这是为什么，如果你斜视一下，为什么小屋会成为，是的，这是为什么，如果你斜视一下。

为什么小屋会成为，网络还可以，眼镜是输出人，所以，网络还可以，眼镜是输出人，所以，这个里面的家伙会成为线性输出的逻辑，这个里面的家伙会成为线性输出的逻辑，然后应用这个G。

它将变成这个柔软的arcamax，然后应用这个G，它将变成这个柔软的arcamax，好的，这是将输出归一化为零和一，好的，这是将输出归一化为零和一，输出的所有项的总和将是一个最大值，所以为什么。

输出的所有项的总和将是一个最大值，所以为什么，你不用它来表示平方误差吗？你不用它来表示平方误差吗？他实际上写的这个问题，写的和写的，写在，他实际上写的这个问题，写的和写的，写在，笔记本上的右边所以。

但是重点是这是损失功能，笔记本上的右边所以，但是重点是这是损失功能，我们用于分类的分类会给您一些实际可行的方法，我们用于分类的分类会给您一些实际可行的方法，其他损失可能不是最佳优化。

因此当您喜欢进行分类时，我们会，其他损失可能不是最佳优化，因此当您喜欢进行分类时，我们会，使用这个交叉熵，它也具有其他统计属性，我不会，使用这个交叉熵，它也具有其他统计属性，我不会，可以在这里谈论。

所以既然我也想，可以在这里谈论，所以既然我也想，笔记本和其他东西，我将尝试加快速度，以便培训如何。

![](img/126b0113214cb8c525411cf5a08df936_17.png)

笔记本和其他东西，我将尝试加快速度，以便培训如何。

![](img/126b0113214cb8c525411cf5a08df936_19.png)

做训练工作，所以我只是获得所有矩阵的所有参数，例如权重，做训练工作，所以我只是获得所有矩阵的所有参数，例如权重，矩阵偏见，无论我得到一组这些权重，我都称它们为，矩阵偏见，无论我得到一组这些权重。

我都称它们为，大写的theta将会出现在我所有的集合中，大写的theta将会出现在我所有的集合中，可训练的参数，这样我现在就可以写出定义的theta J，可训练的参数。

这样我现在就可以写出定义的theta J，作为损失，这是相同的东西为什么我要更改符号，所以您能做什么，作为损失，这是相同的东西为什么我要更改符号，所以您能做什么，请注意，此函数的内部功能是，请注意。

此函数的内部功能是，参数，而这里的损失通常是，参数，而这里的损失通常是，网络的输出，我的意思是，这个帽子是网络的输出，网络的输出，我的意思是，这个帽子是网络的输出，案例。

因为它的资本对于整体来说是正确的，所以这就是方法之一，案例，因为它的资本对于整体来说是正确的，所以这就是方法之一，当我们可以像损失一样对时，我们会使用对的权利损失的类型是您，当我们可以像损失一样对时。

我们会使用对的权利损失的类型是您，简单地说或J将成为我优化的目标函数，简单地说或J将成为我优化的目标函数，问题，我们现在要看到它，它如何工作，我们可以考虑一下J，问题，我们现在要看到它，它如何工作。

我们可以考虑一下J，是这里的那个紫色小伙子，看起来像在那里，我像一个小写字母，是这里的那个紫色小伙子，看起来像在那里，我像一个小写字母，小写的theta基本上就是一个标量，您只是想拥有一个。

小写的theta基本上就是一个标量，您只是想拥有一个，参数，所以如果我在y轴上有J，我将在x轴上有数据，参数，所以如果我在y轴上有J，我将在x轴上有数据，对，那么您通常如何开始训练这些网络，您就知道。

对，那么您通常如何开始训练这些网络，您就知道，随机初始化的网络，这意味着您知道您只选择了初始theta，随机初始化的网络，这意味着您知道您只选择了初始theta，零值那个特定的点，你会看到J将会是。

零值那个特定的点，你会看到J将会是，所谓的训练训练损失具有特定的价值，这将是我的J，所谓的训练训练损失具有特定的价值，这将是我的J，在零点theta那里，您可以计算出导数，在零点theta那里。

您可以计算出导数，甚至看到这是一条绿线，它们平行于是的，你看不到我，甚至看到这是一条绿线，它们平行于是的，你看不到我，再次知道对不起，多么漂亮，所以你那里有绿色的斜坡，再次知道对不起，多么漂亮。

所以你那里有绿色的斜坡，向您展示该点的导数，然后基本上就是，向您展示该点的导数，然后基本上就是，我的J函数关于计算的参数theta的导数，我的J函数关于计算的参数theta的导数，在theta 0点。

您唯一需要做的就是，在theta 0点，您唯一需要做的就是，向左迈进，因此变量是正还是负，向左迈进，因此变量是正还是负，所以你同意亲戚是积极的，但我正在采取步骤，所以你同意亲戚是积极的。

但我正在采取步骤，向左走，那怎么办呢？向左走，那怎么办呢？其实你只是穿上了-好极了，其实你只是穿上了-好极了，所以这就是所谓的这些东西，好吧，这是渐变，所以这就是所谓的这些东西，好吧，这是渐变。

传承权利，如何在这个好游戏中以自己的网络训练800票，传承权利，如何在这个好游戏中以自己的网络训练800票，我在这里听到另一个词，谁说今天在这里传播，不是我的意思。我在这里听到另一个词。

谁说今天在这里传播，不是我的意思。如果没有提到反向传播，我可以吗？如果没有提到反向传播，我可以吗？训练网络渐变方法吧，好酷，如何发布渐变，如何，训练网络渐变方法吧，好酷，如何发布渐变，如何。

你计算这些梯度，所以这是WY的J，所以这个家伙是，你计算这些梯度，所以这是WY的J，所以这个家伙是，将成为Y中的J，这是相对于输出时间的部分，将成为Y中的J，这是相对于输出时间的部分，DW中的Y正确。

类似地，这是什么，DW中的Y正确，类似地，这是什么，我目标的雅可比行列式的偏导数，我目标的雅可比行列式的偏导数，关于WH的函数将是我对输出的偏导数，关于WH的函数将是我对输出的偏导数。

相对于网络输出的成本乘以雅可比行列式，相对于网络输出的成本乘以雅可比行列式，相对于隐藏层的夜晚的输出，然后，相对于隐藏层的夜晚的输出，然后，最后关于DW的隐藏部分，这叫做，最后关于DW的隐藏部分。

这叫做，反向传播好吧，那么反向传播有两个，反向传播好吧，那么反向传播有两个，计算正确，如果您在网络上错了，我们如何训练网络呢？计算正确，如果您在网络上错了，我们如何训练网络呢？期中，当我让你失败时。

是的，左边是一个，期中，当我让你失败时，是的，左边是一个，练习为什么这些加在这里还可以，所以在最近多少分钟里我有五个，练习为什么这些加在这里还可以，所以在最近多少分钟里我有五个，分钟真的不，我的意思是。

那是很多时间，所以我们要经历两个，分钟真的不，我的意思是，那是很多时间，所以我们要经历两个，笔记本电脑，也许他们是卑鄙的人。笔记本电脑，也许他们是卑鄙的人。当然是的，可以使用木星笔记本，好吧。

我该如何分享我的，当然是的，可以使用木星笔记本，好吧，我该如何分享我的。

![](img/126b0113214cb8c525411cf5a08df936_21.png)

![](img/126b0113214cb8c525411cf5a08df936_22.png)

屏幕系统偏好设置，是的，但我看不到布置镜，好吗。

![](img/126b0113214cb8c525411cf5a08df936_24.png)

屏幕系统偏好设置，是的，但我看不到布置镜，好吗。

![](img/126b0113214cb8c525411cf5a08df936_26.png)

![](img/126b0113214cb8c525411cf5a08df936_27.png)

看不到，所以我们要对蜘蛛进行分类，看不到，所以我们要对蜘蛛进行分类。

![](img/126b0113214cb8c525411cf5a08df936_29.png)

现在你知道了，所以既然他要成为你，你可以。

![](img/126b0113214cb8c525411cf5a08df936_31.png)

现在你知道了，所以既然他要成为你，你可以，看到我的东西，我不必关灯，看到我的东西，我不必关灯，好吧，所以我这里基本上要做的是导入随机的狗屎随机的东西游览，好吧。

所以我这里基本上要做的是导入随机的狗屎随机的东西游览，游览和最佳数学ipython，以便您可以看到一些我用过的东西。



![](img/126b0113214cb8c525411cf5a08df936_33.png)

游览和最佳数学ipython，以便您可以看到一些我用过的东西，很棒的默认配置，我们有一个设备，用于什么设备。



![](img/126b0113214cb8c525411cf5a08df936_35.png)

很棒的默认配置，我们有一个设备，用于什么设备，无论您要运行的设备是什么，只要这样做。

![](img/126b0113214cb8c525411cf5a08df936_37.png)

无论您要运行的设备是什么，只要这样做，在这里，我只是输入了相同的数字，所以您应该先看过。

![](img/126b0113214cb8c525411cf5a08df936_39.png)

在这里，我只是输入了相同的数字，所以您应该先看过，能够理解这些东西并自己做，能够理解这些东西并自己做，这是我认为明年要做家庭作业的一部分，因为，这是我认为明年要做家庭作业的一部分，因为，功课记得没事。

所以我只是可视化您可以看到的数据。

![](img/126b0113214cb8c525411cf5a08df936_41.png)

功课记得没事，所以我只是可视化您可以看到的数据。

![](img/126b0113214cb8c525411cf5a08df936_43.png)

这个东西好吧，哦，没有惊喜，所以这是一个，这个东西好吧，哦，没有惊喜，所以这是一个，起点点具有两个坐标，就像每个点都有X＆Y和，起点点具有两个坐标，就像每个点都有X＆Y和。

那么你有不同班级的颜色我不是跑得太快吧，那么你有不同班级的颜色我不是跑得太快吧，我不支持您现在不应该阅读代码，我不支持您现在不应该阅读代码，稍后至少要用一个小时玩一本笔记本。

稍后至少要用一个小时玩一本笔记本，笔记本只是为了看到它实际运行，因为您永远不知道它是开放的，笔记本只是为了看到它实际运行，因为您永远不知道它是开放的，源项目正确，所以线性模型我将在这里培训这个人，所以。



![](img/126b0113214cb8c525411cf5a08df936_45.png)

源项目正确，所以线性模型我将在这里培训这个人，所以。

![](img/126b0113214cb8c525411cf5a08df936_47.png)

我创建了一个顺序容器，我不必使用它，但是，我创建了一个顺序容器，我不必使用它，但是，这很容易，我把线性放在那儿是线性的，这是错误的，这很容易，我把线性放在那儿是线性的，这是错误的。

仿射变换是线性的仿射变换是什么，仿射变换是线性的仿射变换是什么，转型将在中期发生五件事，所以您知道，转型将在中期发生五件事，所以您知道，从D到H，实际上这些输入空间H将被隐藏，从D到H。

实际上这些输入空间H将被隐藏，然后从隐藏在输出中的只是线性层，所以，然后从隐藏在输出中的只是线性层，所以，决策边界在这里糟糕是正确的。



![](img/126b0113214cb8c525411cf5a08df936_49.png)

决策边界在这里糟糕是正确的，线性的，所以我开始爆炸训练这个家伙，然后向您展示输出。

![](img/126b0113214cb8c525411cf5a08df936_51.png)

线性的，所以我开始爆炸训练这个家伙，然后向您展示输出。

![](img/126b0113214cb8c525411cf5a08df936_53.png)

没事坏坏网络最好的权利那么为什么这些决定呢？没事坏坏网络最好的权利那么为什么这些决定呢？边界置于此配置中，为什么它们不旋转，为什么您具有，边界置于此配置中，为什么它们不旋转，为什么您具有。

尝试在左侧而不是另一侧的黄色区域尝试执行此操作。

![](img/126b0113214cb8c525411cf5a08df936_55.png)

尝试在左侧而不是另一侧的黄色区域尝试执行此操作，最好的对，所以我的治愈率是0 5您可以看到0 5好的，谁说随机。



![](img/126b0113214cb8c525411cf5a08df936_57.png)

最好的对，所以我的治愈率是0 5您可以看到0 5好的，谁说随机。

![](img/126b0113214cb8c525411cf5a08df936_59.png)

好的，很好，否则您要刷新概率。

![](img/126b0113214cb8c525411cf5a08df936_61.png)

好的，很好，否则您要刷新概率。

![](img/126b0113214cb8c525411cf5a08df936_63.png)

好的1/3对，好的，你没有时间，但是我想再添加一件事，是的，好的1/3对，好的，你没有时间，但是我想再添加一件事，是的。



![](img/126b0113214cb8c525411cf5a08df936_65.png)

这里没有培训损失培训的第一价值是什么，这里没有培训损失培训的第一价值是什么，首先损失，这是我正确的最新价值，所以这是我最后的损失，首先损失，这是我正确的最新价值，所以这是我最后的损失。



![](img/126b0113214cb8c525411cf5a08df936_67.png)

0。86我的损失的第一价值是多少。

![](img/126b0113214cb8c525411cf5a08df936_69.png)

关于您的任何想法都会在这里看到我的屏幕，但我什至无法确定。

![](img/126b0113214cb8c525411cf5a08df936_71.png)

关于您的任何想法都会在这里看到我的屏幕，但我什至无法确定，您得到的第一个数字是多少-如果您不知道，您会弄清楚。



![](img/126b0113214cb8c525411cf5a08df936_73.png)

您得到的第一个数字是多少-如果您不知道，您会弄清楚。

![](img/126b0113214cb8c525411cf5a08df936_75.png)

找出下周我告诉你，但你应该弄清楚，所以你，找出下周我告诉你，但你应该弄清楚，所以你，有时候应该尝试动动大脑，所以让我们做点什么。



![](img/126b0113214cb8c525411cf5a08df936_77.png)

有时候应该尝试动动大脑，所以让我们做点什么，在这里，我要在中心添加这个积极的部分，所以我只添加一个。

![](img/126b0113214cb8c525411cf5a08df936_79.png)

在这里，我要在中心添加这个积极的部分，所以我只添加一个，微小的零到负数，其他所有东西都是，微小的零到负数，其他所有东西都是，同样，我没有做任何改变，但我，同样，我没有做任何改变，但我。

用Zook删除负数峰值，这是我不知道的事情。

![](img/126b0113214cb8c525411cf5a08df936_81.png)

用Zook删除负数峰值，这是我不知道的事情，英文名称的准确性哦，不，好的，您兴奋吗？

![](img/126b0113214cb8c525411cf5a08df936_83.png)

![](img/126b0113214cb8c525411cf5a08df936_84.png)

![](img/126b0113214cb8c525411cf5a08df936_85.png)

英文名称的准确性哦，不，好的，您兴奋吗？有点像你在睡觉我想很好哦，很酷，所以还剩两分钟。

![](img/126b0113214cb8c525411cf5a08df936_87.png)

有点像你在睡觉我想很好哦，很酷，所以还剩两分钟，尝试正确更改隐藏层的数量，因此尝试更改对不起的尝试，尝试正确更改隐藏层的数量，因此尝试更改对不起的尝试，更改隐藏层的尺寸如果使用两个。

更改隐藏层的尺寸如果使用两个，在打哈欠的隐藏层中隐藏了两个神经元，抱歉，尝试与，在打哈欠的隐藏层中隐藏了两个神经元，抱歉，尝试与，这项权利并找出正在发生的事情，您可以在Piazza上发表评论。



![](img/126b0113214cb8c525411cf5a08df936_89.png)

这项权利并找出正在发生的事情，您可以在Piazza上发表评论，可以开始交谈，就好像您知道对话一样-好的，我们真的，可以开始交谈，就好像您知道对话一样-好的，我们真的。



![](img/126b0113214cb8c525411cf5a08df936_91.png)

强烈鼓励您使用这些笔记本电脑，并找出其中的内容，强烈鼓励您使用这些笔记本电脑，并找出其中的内容。

![](img/126b0113214cb8c525411cf5a08df936_93.png)

最后30秒，我们通过另一个笔记本，最后30秒，我们通过另一个笔记本。

![](img/126b0113214cb8c525411cf5a08df936_95.png)

同样可以，但是在这种情况下，我的观点是这样的，您会看到任何对的东西。

![](img/126b0113214cb8c525411cf5a08df936_97.png)

同样可以，但是在这种情况下，我的观点是这样的，您会看到任何对的东西，所以你会看到这种香蕉而不是香蕉就像你知道的，所以你会看到这种香蕉而不是香蕉就像你知道的。

就像看到尼克（Nikki）耐克（Nike）一样，谢谢符号。

![](img/126b0113214cb8c525411cf5a08df936_99.png)

就像看到尼克（Nikki）耐克（Nike）一样，谢谢符号，我将在这里训练我的线性网络我会训练所有东西，我将在这里训练我的线性网络我会训练所有东西。



![](img/126b0113214cb8c525411cf5a08df936_101.png)

![](img/126b0113214cb8c525411cf5a08df936_102.png)

现在解释不用担心，所以我训练了线性网络，现在解释不用担心，所以我训练了线性网络，我的线性网络是什么，跟我有顺序的容器之前一样。



![](img/126b0113214cb8c525411cf5a08df936_104.png)

我的线性网络是什么，跟我有顺序的容器之前一样，我在哪里放置工具，哪些精细转换是正确的，我在哪里放置工具，哪些精细转换是正确的。



![](img/126b0113214cb8c525411cf5a08df936_106.png)

我已发送到设备，好吧，我们如何正确训练网络，以便训练。

![](img/126b0113214cb8c525411cf5a08df936_108.png)

我已发送到设备，好吧，我们如何正确训练网络，以便训练，网络，您有输入X，这些都是我喂食的所有点，我喂食了它们，网络，您有输入X，这些都是我喂食的所有点，我喂食了它们。

我母亲内部的网络和母亲会给我我的白面包，然后。

![](img/126b0113214cb8c525411cf5a08df936_110.png)

我母亲内部的网络和母亲会给我我的白面包，然后，模型的定义就是这个，这个容器是这里，所以我们，模型的定义就是这个，这个容器是这里，所以我们。



![](img/126b0113214cb8c525411cf5a08df936_112.png)

有我们提供的模型，我们称整个X为白面包，然后我计算出，有我们提供的模型，我们称整个X为白面包，然后我计算出，损失将是我的标准，该标准是根据我的计算得出的，损失将是我的标准，该标准是根据我的计算得出的。

预测和明智者是正确的阶级，白人预测，预测和明智者是正确的阶级，白人预测。

![](img/126b0113214cb8c525411cf5a08df936_114.png)

在这种情况下是网络的输出，这里写的标准是，在这种情况下是网络的输出，这里写的标准是。

![](img/126b0113214cb8c525411cf5a08df936_116.png)

这将是我们的MSC损失，因此我们在实际使用十字之前会进行更改，这将是我们的MSC损失，因此我们在实际使用十字之前会进行更改，熵减去softmax和soft dark max的对数现在我们要使用MSC。

熵减去softmax和soft dark max的对数现在我们要使用MSC。

![](img/126b0113214cb8c525411cf5a08df936_118.png)

因为我们在没有回归问题的情况下使用回归，所以我们计算，因为我们在没有回归问题的情况下使用回归，所以我们计算，损失是您的输出与目标权之间的二次距离，损失是您的输出与目标权之间的二次距离，因此。

如果您谈论回归，我们谈论目标，因此，如果您谈论回归，我们谈论目标，分类，我们将要讨论标签中的类，以便邻居，分类，我们将要讨论标签中的类，以便邻居，我们谈论的分类回归，目标好吧，所以我们在这里计算损失。

然后我必须清理。

![](img/126b0113214cb8c525411cf5a08df936_120.png)

目标好吧，所以我们在这里计算损失，然后我必须清理，之前发生的任何事情，所以我对优化器说，清除了之前所有剩余的内容，之前发生的任何事情，所以我对优化器说，清除了之前所有剩余的内容。



![](img/126b0113214cb8c525411cf5a08df936_122.png)

从以前的操作结束，这是零梯度通过，然后我执行，从以前的操作结束，这是零梯度通过，然后我执行。

![](img/126b0113214cb8c525411cf5a08df936_124.png)

向后是什么向后是什么向后传播，向后是什么向后是什么向后传播，第二部分偏的偏导数的计算，第二部分偏的偏导数的计算，损失相对于参数的导数，因此损失不向后，损失相对于参数的导数，因此损失不向后。

简单的链式规则并计算最终损失的所有偏导数，简单的链式规则并计算最终损失的所有偏导数，在这种情况下，这将是相对于每个的均方误差。



![](img/126b0113214cb8c525411cf5a08df936_126.png)

在这种情况下，这将是相对于每个的均方误差，模型的每个参数最后都要做一个步骤，模型的每个参数最后都要做一个步骤，如果那是渐变，那么您将向后退，如果那是渐变，那么您将向后退。



![](img/126b0113214cb8c525411cf5a08df936_128.png)

梯度好吧，所以我们训练这个没有非，梯度好吧，所以我们训练这个没有非。

![](img/126b0113214cb8c525411cf5a08df936_130.png)

线性，它做了一些我不知道如何解释这种损失和，线性，它做了一些我不知道如何解释这种损失和。

![](img/126b0113214cb8c525411cf5a08df936_132.png)

将是我的网络输出是什么，将是我的网络输出是什么，线性回归-很好，很无聊，所以现在我要做深度网络超级。

![](img/126b0113214cb8c525411cf5a08df936_134.png)

线性回归-很好，很无聊，所以现在我要做深度网络超级。

![](img/126b0113214cb8c525411cf5a08df936_136.png)

令人兴奋的事情，我该怎么做，我只是删除负值，令人兴奋的事情，我该怎么做，我只是删除负值，好吧，这有多酷，所以我有时会移动负值。



![](img/126b0113214cb8c525411cf5a08df936_138.png)

好吧，这有多酷，所以我有时会移动负值，真实的哦，有时候我只是使用双曲线切线，而我却一分为二，真实的哦，有时候我只是使用双曲线切线，而我却一分为二，时间，这样您就可以看到比较，我训练了这些东西。

看看让我们看看。

![](img/126b0113214cb8c525411cf5a08df936_140.png)

时间，这样您就可以看到比较，我训练了这些东西，看看让我们看看。

![](img/126b0113214cb8c525411cf5a08df936_142.png)

在实际训练之前进行预测，所以在训练之前您将获得此，在实际训练之前进行预测，所以在训练之前您将获得此。

![](img/126b0113214cb8c525411cf5a08df936_144.png)

例如，我知道一些正确的预测，例如，我知道一些正确的预测，线和绿线可能代表了方差，线和绿线可能代表了方差，cursor为0，因此所有这些之间的方差大致为0。2，cursor为0。

因此所有这些之间的方差大致为0。2，预测让我们失望，让我们看看网络如何改变他们的，预测让我们失望，让我们看看网络如何改变他们的，鉴于我们已经使用了损失权并且我们已经移动了最终输出。

鉴于我们已经使用了损失权并且我们已经移动了最终输出，反对，所以我们在山上有雾，你看不到山谷在哪里，反对，所以我们在山上有雾，你看不到山谷在哪里，只是往湿滑的地方走，你知道那东西掉下来了。

只是往湿滑的地方走，你知道那东西掉下来了，对着身体向下，所以这样做之后，对着身体向下，所以这样做之后，程序好几次，您很高兴看到此网络的性能，我没有，程序好几次，您很高兴看到此网络的性能，我没有。

听到你没事，谢谢你，好吧，妈妈，你猜猜哪一个。

![](img/126b0113214cb8c525411cf5a08df936_146.png)

听到你没事，谢谢你，好吧，妈妈，你猜猜哪一个。

![](img/126b0113214cb8c525411cf5a08df936_148.png)

一个是使用正部分，另一个是使用双曲线。

![](img/126b0113214cb8c525411cf5a08df936_150.png)

一个是使用正部分，另一个是使用双曲线，切线好吧，因为你是坚强的派珀，切线好吧，因为你是坚强的派珀，好吧，留在里面，您可以看到我的网络如何近似我的输入，好吧，留在里面，您可以看到我的网络如何近似我的输入。

是那个东西，你能看到什么吗，也许你可以看到这是怎么回事，是那个东西，你能看到什么吗，也许你可以看到这是怎么回事。



![](img/126b0113214cb8c525411cf5a08df936_152.png)

东西看起来像是一条直线，所以我的网络是，东西看起来像是一条直线，所以我的网络是，只是输出只是分段线性，您知道您的近似值，只是输出只是分段线性，您知道您的近似值，输入这是因为，如果您使用，输入这是因为。

如果您使用，抛物线切线是什么感觉，是的，你在这里得到这个家伙，它有多光滑，抛物线切线是什么感觉，是的，你在这里得到这个家伙，它有多光滑，好吧，为什么我要做这个东西呢，这是更好的吧，我想首先，好吧。

为什么我要做这个东西呢，这是更好的吧，我想首先，您会看到黄色的东西，它是标准偏差，看起来像，您会看到黄色的东西，它是标准偏差，看起来像，搞砸了，少了左手边的搞砸了，搞砸了，少了左手边的搞砸了。

标准差确实很尖刻，所以如果您训练一个疯狂的Belov网络，这些，标准差确实很尖刻，所以如果您训练一个疯狂的Belov网络，这些，网络可能不会像其他人一样一致，网络可能不会像其他人一样一致。



![](img/126b0113214cb8c525411cf5a08df936_154.png)

此外，我的左手边换了一行，我们放在第四位，所以，此外，我的左手边换了一行，我们放在第四位，所以，我现在要在训练区域之外查看数据，好吧，我要成为，我现在要在训练区域之外查看数据，好吧，我要成为。

看看左边和右边发生了什么，您希望看到什么，看看左边和右边发生了什么，您希望看到什么。

![](img/126b0113214cb8c525411cf5a08df936_156.png)

当您实际测试网络时，您期望这些网络做什么？当您实际测试网络时，您期望这些网络做什么？培训区域外，[音乐]好吧，直觉类似，[音乐]好吧，直觉类似，该网络将无法正常运行，因为网络只能将其概括。

该网络将无法正常运行，因为网络只能将其概括，如果您询问您的网络已经超过了类似范围的数据，如果您询问您的网络已经超过了类似范围的数据，受过此数据训练的人如何解释此处的事物。

受过此数据训练的人如何解释此处的事物，我不知道，但是不幸的是，主要的问题是欲望回归网络，我不知道，但是不幸的是，主要的问题是欲望回归网络，所以他们不会告诉你他们有多自信他们是绝对的我。

所以他们不会告诉你他们有多自信他们是绝对的我，不管我说什么，他们都不会告诉你你有多自信，不管我说什么，他们都不会告诉你你有多自信。



![](img/126b0113214cb8c525411cf5a08df936_158.png)

他们是对的，我们只是告诉您这个数字，但让我们看看，他们是对的，我们只是告诉您这个数字，但让我们看看。

![](img/126b0113214cb8c525411cf5a08df936_160.png)

现在发生了什么，所以现在发生了什么，我只是告诉你一点。

![](img/126b0113214cb8c525411cf5a08df936_162.png)

现在发生了什么，所以现在发生了什么，我只是告诉你一点，右边的黄色将是标准偏差，右边的黄色将是标准偏差，绿色的是您可以在左侧看到的方差，是真实的aw，绿色的是您可以在左侧看到的方差，是真实的aw。

正部分函数中的真正线性保持最终分支，正部分函数中的真正线性保持最终分支，保持它们保持相同的坡度，而一列双曲火车，保持它们保持相同的坡度，而一列双曲火车，切线最终会饱和，所以您必须知道此网络。

切线最终会饱和，所以您必须知道此网络，会产生副作用非线性函数的选择会产生副作用，会产生副作用非线性函数的选择会产生副作用，效果特别好，如果您幸运地走出训练领域，可以，效果特别好。

如果您幸运地走出训练领域，可以，使用此技术对n个符号进行方差预测或估计，以便，使用此技术对n个符号进行方差预测或估计，以便，以某种方式估计进行预测的不确定性是，以某种方式估计进行预测的不确定性是。

在研究方面真的真的真的真的非常重要，在研究方面真的真的真的真的非常重要，您进行回归的网络您根本没有任何线索，您进行回归的网络您根本没有任何线索，如果您训练一堆具有不同初始值的网络，您将充满信心。

如果您训练一堆具有不同初始值的网络，您将充满信心，您可以使用相同的过程对它们进行训练，就可以计算出方差，您可以使用相同的过程对它们进行训练，就可以计算出方差，为了估计进行给定预测的不确定性。

为了估计进行给定预测的不确定性，今天就是这样，谢谢您的收听，我现在见，今天就是这样，谢谢您的收听，我现在见，周再见问题好吧其他问题搁置搁置等待。



![](img/126b0113214cb8c525411cf5a08df936_164.png)

周再见问题好吧其他问题搁置搁置等待，首先等一秒钟，好吧，天空突然破裂，无论他们要付出什么，首先等一秒钟，好吧，天空突然破裂，无论他们要付出什么，注意他写的东西，我们总是在笔记上，注意他写的东西。

我们总是在笔记上，网站在周日之前可以修改，网站在周日之前可以修改，上课前的内容，如果您在不修改内容的情况下上课，上课前的内容，如果您在不修改内容的情况下上课，不完全了解您对我们谈论两个Z的看法。

不完全了解您对我们谈论两个Z的看法。

![](img/126b0113214cb8c525411cf5a08df936_166.png)