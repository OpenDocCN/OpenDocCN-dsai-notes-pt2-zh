# 智源大会 2024（九）



# 2024北京智源大会-智能驾驶 - P4：比亚迪智驾与智舱开发工作实践：高 文 - 智源社区 - BV1Ww4m1a7gr

呃首先感谢刘主任的介绍以及组委会的邀请，呃很高兴与大家相聚在致远，呃，刚才首先黄教授先讲了这个人类对于呃，驾驶任务的这个完成，刚才也说了，人类是先有了这个完整的一个认知，然后经过特殊的50小时的训练。

就可以在没有训练过的道路上完成驾驶的任务，然后同时又说了大模型思维在智驾的呃作用，以及对未来的展望，然后法旺主任呢，刚才也有对智能驾驶汽车的这个产业，应用的现状以及方向进行了同步，然后呃梁总和马总呢。

在呃长安汽车和这个小鹏的相关的支架，还有AI大模型的实践进行了分享，然后很高兴，今天由我带来，比亚迪在智驾和座舱开发工作方面的，这个实践分享。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/edfc474ade78ac219c80d2737cd92306_1.png)

呃从1876年，奥托发明了这个往复活塞式四冲程内燃机，到1985年，本次发明了世界上第一辆汽车，再到八六呃，1886年，戴姆勒成功发明了世界上第一辆四轮汽车，汽车从诞生之初，它的使命呢是代步工具。

那么经过了138年的发展，汽车的产品呢属性增加了很多，其中有两个最重要的就是安全和体验，那么它的使命也变成了兼具智慧的，有温度的伙伴，下面我将从三方面进行相应的分享，第一部分感知传感。

我们和车作为一个整体，要感知什么呢，大概分为三个方面，第一个是环境方面的感知，感知道路交通标识，障碍物，交通参与者动等动静态的这个目标，做到看得清环境，第二呢是自身的感知自车的位置，速度，方向位资。

以及驾驶员和乘用车乘车人的呃，生理和心理状态呃，动作手势，语音空气等，做到看得清自己，第三呢是物联感知实时的路况呃，道路信息，行人信息等等，车路云看得清交通，那么说到感知就不得不说一下传感器。

以摄像头为代表的视觉传感器呃，基纳斯还有v two x的定位传感器，以及激光雷达，毫米波，超声波雷达的这个雷达传感器以及麦克风压力，还有嗅觉的呃，感官传感器以及呃惯性测量呃单元，还有这个角编码器等等。

这些姿态传感器共同构成了感知传感链，呃车载摄像头从市场角，普段等多方面的多方向的发展，以提升探测距离，弱光环境的适应能力以及动态响应的速度，从单目的基础识别演进到了双目的立体测距。

在集成红外成像应对夜间的这个场景，到引入事件相机，快速捕捉动态变化，逐渐增强智能驾驶的感知能力。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/edfc474ade78ac219c80d2737cd92306_3.png)

毫米波的发展呢，从发展的里程碑和技术的里程碑，两个角度上来讲，发展的萌芽期可以从追溯到1940年，开始于实验室，主要应用于军工领域，呃，发开发期呢从上世纪80年代，各国呢积极投入研发。

尤其是欧美普及期呢，在呃毫米波雷达呢进入了这个应用阶段，中国起步虽然比较晚，但是现在已经逐渐的国产化技术里程碑，共经历了七代的这个迭代的过程，工艺上从碳化身到转化硅再到西茅斯的进化，性能越来越优。

集成度呢也更好，体积越来越小，重量越来越轻，从测距测速加上水平角的3D，再到测距测速水平和俯仰角的4D，再到测速测距水平俯仰角，再加上抗干扰的4。5D，角度的分辨率呢越来越高，抗干扰能力也越来越强。

呃激光雷达呢利用脉冲激光的飞行时间，进行物体距离的探测，从20世纪60年代，休斯实验室研制出了世界上第一台激光器，到80年代加入了这个扫描机构，一直到2005年，第二届无人无人车挑战大赛上。

参赛车辆上出现了360度的多线束，旋转式激光雷达，以及五个单线激光雷达的方案，七支完赛队伍中，有六支都搭载了64线的，这种旋转式的激光雷达，激光雷达自此一战成名，到2017年。

全球第一款车规级激光雷达量产交付，是一个四线的一维转镜的方案，2022年，国产的半固态激光雷达量产，价格呢也逐渐的走下了神坛，激光雷达正在向千元内迈进呃，同时呢功能上进行了相应的细分，有前视补盲。

然后还有这种呃像素级空间分辨率的呃，高线数呃，长距的激光雷达经过了这20年的发展，从光场生成方式到收发单元的这个技术路线呃，也也趋于收敛，但是flash固态还有OPA以及FMCW等技术。

也仍在蓬勃的发展，第二部分智能驾驶功能以及相应的趋势，智能驾驶从定位感知，规划到通信技术，都进行了全面的革新和技术的更迭，从减少驾驶负担到逐渐释放双手，技术的每一步进步。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/edfc474ade78ac219c80d2737cd92306_5.png)

都是向着更安全更智能的方向迈进了一大步，智能驾驶技术以预防为主，通过全天候监控和即时响应，为用户编织出一张无形的安全网。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/edfc474ade78ac219c80d2737cd92306_7.png)

DEPLOT智能驾驶辅助系统，以天神之眼为设计理念，安全为设计初衷，依托先进的电子电气架构和全站自研能力，为智驾提出整车系统及解决方案，实现整车全场景的陪伴，辅助和救助，以安全为核心。

结合电机云年等控制技术，做到起步制动更平稳，大曲流，弯道更丝滑，全场景的智能领航，全球独一无二的E4方泊车，行业领先的窄道通行，双速泊车模式，以及断头路泊车等，为用户带来了极致舒适和极致安全。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/edfc474ade78ac219c80d2737cd92306_9.png)

行业上普遍理解的L3有别于L2的功能要求，而安全要求是控制的冗余，仰望U8E4方，不但实现了控制冗余，而且还实现了机械冗余，E4方概念车是行业首款无制动装置，无转向柱结构，无转向电机的汽车。

实现了驱动制动和转向三合一，行业首次实现，车辆在传统制动和转向系统都失效的情况下，仍具备制动和转向的能力，体现了强大的E4方机械冗余能力，超越了L3级别的冗余要求，因此仰望U8。

成为全球首款具备L3技术底座的量产车，最后智能显示，为增强安全以及车内娱乐和人机交互，带来的新方式，舱内的显示屏从最开始的仪表演进到了中控，副驾后排呃空调，还有这个车门旋钮上面等等，这些是从体验出发的。

那么补充驾驶视野的盲区的透明A柱，以及减少视野遮挡，增强夜视感知效果的电子后视镜，以及避免驾驶员视线从前方行驶切换到中控，增加驾驶风险，以将显示信息投影到呃，驾驶员前市区的这个HUD显示与现实相结合。

带来的AI导航增强显示以及多功能补盲等，都是极致安全的体验呃，全息投影显示技术在紧急情况下，将虚拟方向盘和其他操作设备投影到必要位置，驾乘人员在全息空间操控汽车，确保车辆的安全。

让主驾副驾以及后排空间共享屏幕，让欢乐与喜悦在家庭之间传递，完美兼容手机生态，让车载应用不再困难，实现人车手机互联，让交互不再存在隔阂，通过3D显示技术打造沉浸体验，让汽车成为用户的第三生活空间。

混合实现呃技术打破现实和虚拟的边界，给用户带来前所未有的科幻感受，从未来科技驶入现实，1995年的译制片霹雳游侠，片中的这个cat不仅像很多电影里的车一样，无坚不摧，同时会说多国的方言和呃。

多国的语言和方言陪你悲伤和快乐，可以完全接管汽车，自动驾驶是一个兼具智慧和温度的伙伴，那段每周日两集的这个译制片，不仅打开了我们的想象和对未来的无尽憧憬，也教会了我们正义和勇敢创造。

霹雳游侠里面的KATE是很多人儿时的梦想，那时没有物联网，那么在霹雳游侠KATE的这个自动驾驶，有温度的人机交互，对环境的感知追踪，营造的立体的多维感官监测以及氛围提醒，与现在的物联网联相结合。

这将是梦想的升级版，汽车不再是以硬件为主的工业化产品，也是一个自学习，自进化，自成长的软硬兼备的智能化终端，心有所信方能远行，让我们汽车人一道共同努力，创造美好的明天。



# 2024北京智源大会-智能驾驶 - P5：自动驾驶大规模应用的挑战及展望：白宇利 - 智源社区 - BV1Ww4m1a7gr

首先感谢那个组委会的邀请，然后也感谢刘主任的介绍，那呃各位下午好，然后我是来自未来人工智能平台的白羽丽，很开心呢，今天下午有机会跟大家一起交流呃，自动驾驶，聊一聊这个大规模应用下的这个呃挑战和展望，对。

可能我我这个这个风格，跟其他前面几位有点不太一样，然后还是比较偏向于说，这个量产的工程落地没有那么学术，然后时间也比较简短，然后呢，我先简单介绍一下未来汽车和未来智能驾驶。

那未来汽车是一家全球领先的电动汽车品牌，那我们致力于为用户创造愉悦的生活方式，对那未来智能驾驶的，当然我以下的时候称，就未来智驾旨在呃解放精力，减少事故，提供安全放松的点到点智能驾驶体验。

那在2023年，汽车界最权威的这个安全测试机构呃，un un cap启用了新规，其中主动安全新的百余项的这个场景之下呢，未来支架也表现出色，然后助力于未来，成为这个首个达成五星安全评评估的。

这个汽车品牌，那下面呢我也介绍一下整个未来支架的组成，主要包含里边四个部分，那包含感知系统，车单超算核心的算法以及整车平台，在这里边呢我们要首这个着重的时候介绍两块，一块是大家刚才也提到的这个感知系统。

那在未来的这个整个感知系统里边，是拥有33个高性能传感器，那分辨率是一个非常高的一个状态，而且全系标配了激光流激光雷达，另外呢是说在车端的超算，那未来也是第第一家在车上边时，全系标配了四颗奥瑞X的芯片。

然后算力总量是达到1016tops，那第二代整车平台NT2。0呢，我们全系标配了这个配置，那不光在当前，包括在现在，那都可以说是重新定义了量产车的，这个智能驾驶的系统，树立了高端支架新的标准。

那接下来呢我想呃介绍几个这个呃，未来支架的一些时间点啊，可能有些同事是并不太清楚对，那我们在2021年的1月，未来发布了NTR的首款车ET7，那其实标志着未来从这个呃走向了全站自研。

智能驾驶的一个新的时代，那2022年的4月，ET7上市，我们仅仅用了一年多的时间，就交付了智能驾驶的功能，同年的9月，然后我们在o p plus，在高速的场景里边是也开始了交付。

在2023年的呃10月份，NO p plus从高速拓展到城区嗯，到4月呢，那全域的领航辅助，我们就向所有的NTR的车主全量推送了，那所以从一个全量推送的过程呢，那我们仅仅花了六个月。

而特斯拉的FSD啊整整花费了3年的时间，当然我们还在这个持续的更新，也不断的在优化着我们智能驾驶的这个技术，和它功能体验，既然今天要讲这个大规模应用的挑战，那我们首先定义一下呢，大规模是什么。

那我在想大规模主要是有两个呃方面的含义啊，一个是说在使用规模上面，另外的呢就是在功能范围上，那首先说从用户量上边啊，那在我们第二大平台，用户量从2022年的8万，2023年的15万。

然后进而的时候到现在2024年的时候，预计的时候应该远远超于30万，基本上我们每年翻一番，那第二的话是说从这个覆盖的范围和区域上，那我们在2022年的话呢，ET7在中国交付。

同年其实我们即在欧洲完成了交付，那2324年我们进一步的时候，也拓展了欧洲多个国家，然后包括呃也有新增的中东的地区，对那再次的时候我们在聊的是里程，从22年呢高速成快，然后大概是36万公里。

那到23年10月份发这个，我们发布城区的时候呢，目标是六十六十八万公里，对那如今我们可以痊愈的领航辅助，这个可用里程已经超过了140万公里，最后呢那我们要讲的是车型和平台，那在2021年以前。

我们的n T1的平台，那也是经典的，在在未来的886的车型，到22年我们新增了n t two，对然后呢在这里边的时候，我们现在九款在售的全系车型，都已经更新到了第二代的车载平台上。

那24年我们搭载的NT3的自研平台的，这个第二未来第二品牌乐道也即将开始交付，对那这些都是从我们讲说这个呃量和维度上面，那我们说从功能上变未来支架体系，那我们也开在这里边开始支持多个车型。

然后新老平台的三代平台的同台，多个国家，多个公的这个多个区域上的功能交付，挑战其实非常大的，那我们去看说从呃功能规模，从最简单的功能独立功能，到我们后边复杂的更融合的系统，例如我们经常谈到的AEB。

就是自动紧急制动的这样的功能，当我们发展到现在的NO p plus，全域领航辅助的功能，那从最开始数据每秒钟的时候，百这个百兆B的这个大小到现在的时候，我们可以每秒产生的数据十十G大B对。

那10G大B每秒相当于什么呢，一秒钟看完两部4K的电影，那我们端测的算力也是呃急剧的在增长，从最开始的时候可能大家都知道的时候，比如说那可能都小于实实实这个实体套，这是10top4的这样的算力。

到现在那在整个未来的呃，未来的车车载上面是进有这个千体，这就是上千TOS对在这个算力规模之下，其实跑一个100B的大语言模型都绰绰有余，那我们的这个车载平台，是完全有能力来去支持的，那从研发任务。

那以前的时候研发任务可能小到几十项对吧，大到上现在大到上百项，从最开始那我们感知唱片的时候，在车辆行人障碍物的检测，到现在大家开始去聊这种GOAAES，然后MAI这种复杂的融合系统。

那其实从功能上面都是一个大幅的这个提升，对那从评测任务最开始的时候呢，小到几百，现在大到上万，总不光是这种评测的种类多，那验证的里程的时候也是要求的，逐渐的时候去增加，说到这里呢，大家可能会想说。

这么大的规模，这么多的场景，这背后需要有有有哪些挑战对吧，然后我们以及怎么来去支持，这么大的这个这个场景的变化，那接下来的时候，我刚我将跟大家一起的时候去深入探讨说。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_1.png)

那在未来我们再怎么应对这样的问题的，对咳，就我我给后边的时候分为这么几呃，这部分挑战的时候分为几块的值来去讨论啊，主要是分为计算啊，那数据和成本我们先说一下这个计算的挑战吧。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_3.png)

对那未来自动驾驶研发每天要进行数百个实验，数千次的构建，数10万次这个数10万个挖掘的任务的执行，那这些都与这些高并发的任务，都与说都需要一个非常强大的呃，计算平台来去支持。

那我们自己自研的高性能计算平台，能够支撑什么，200万次任任务的这个日间峰值的吞吐，并且可以支持说瞬时瞬时的并发的时候，超过1。5万个节点对，那大家也在讲说天下武功唯快不破。

从发现问题到解决问到解决问题，发布这个版本，那更短周期的迭代是我们一直优化的目标，那为了解决这个这个超大型任务的性能瓶颈，我们自己也设计并研发了一套大规模，分布式的集，这个训练集群，计算集群吧。

那在这个集群里边的时候，我们可以做到单任务量级的时候，是超过EFLOS的，那我觉得这个啊行业里边事大家也都可以知道，说具体的总量我就不讲了，但是这个集群一定是在支架行业里边，是top规模的。

当然在规模之外的话，我觉得前面的侄儿像小鹏同时也提到了，说那规模在规模之外，其性能和稳定性是这里边非常重要的，那我们在这里边的时候，整个训练集群性能也是非常好的，那以我们在云端这个空间大模型。

那我们能做到训练加速比的时候，达到91%，有效的训练时长的时候大于98%，当然为了支持这样高性能的训练集群，我们也需要上下游的组件上面的支持，为此的时候我们也啊有这样的自研的，我们认为说缓存的系统咳。

那以以缓存系统为例的话，那我们可以做到横向的扩展性能的时候，能超过同类商业的这样的存储软件的，这个将近四倍，那整整体在支架的研发任务差异是非常大的，然后又在不同的硬件上面跑。

如何让他们都能高效的合理的运行，也是需要花费巨大的时间来去优化的，那我们可以通过性能优化providing工具，然后包括这种协同优化，那我们可以通过实现这种异构的调度，任务的拆分和传输的优化。

包括这种流水线并行，然后多方面的努力，那我们都可以去动态的去，把这个这个负载均衡做好，大幅的提升整体的这个有效利用率，当然强大的算力的时候只是一方面啊，那没有大量的数据的支持计算。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_5.png)

其实就无从谈起，那支架的场景的话，我愿意给它简单分成三类的数据吧，那一类的时候我们称之为训练数据，然后那一部分时候我们称之为这个验证数据，还有一部分称之为反馈的数据嗯，对于训练的数据。

那随着这个自动驾驶的发展，每年对数据的需求都是几十倍的增长，那在三这3年以来的时候呢，我们就有近万倍的增长，那量产车上面的海量数据，海量高质量的数据呢，那我说在未来支架的是未来支架的这个护城河。

那每秒钟产生PB级的数据，让我们从来不担心这个数据的供应，但是如何通过自动化的产线，自动化的标注对吧，能使得这些数据参与到云端的模型的训，练和功能迭代之中，那是要面临的难题，对为此的时候。

我们其实在这里边也建立了，500多种标准化的标注工艺，然后100多条自动化的产线，通过云端的世界模型，参与到这个自动化流程之中，将整个标注的这个自动化效率，时候提升到三个九以上。

那第二块的时候我在讲的时候是验证数据呃，像刚才前面的同事也讲到说，那其实对于整车上边的测试是一个非常复杂的，那对于尤其对于软件测试，大家嗯传统的测试模式，最终功能还是要上实车验证的。

那方法大多数呢是通过这种自建车队，那而如今那我们认为说有多版本，快节奏的并行验证的需求之下，那区区的几百辆车的话是远远不能满足需求的，那为此的话我们NO p plus，我以这个为例吧。

就是我们在开城透路里边呃，那一般情况下是要一个城一个城的开，然后开完之后的时候用这车去验证，但是我们可以结合自有的车队车上边的时候，一块奥顿的芯片接，用这种群体智能的方式。

未来在未来在这里边时候用非常快，额外的这一颗芯片呢，用群体智能规模化优势的是，大批量验证了这些道路的可用性，原定于这种三个月要去验证的，NOP的开通透露的任务，那我们缩短到这个更短的这个时间就能完成。

在在这个之在这里呢，当然我们也要强调一下，说那在这个呃，我们认为说大规模，10万量级规模的这些种并行测测试任务，对于平台的压力是什么呢，那我们需要说能在小时级别，这在这这里面我们能做到四个小时级别。

这种10万规模的车辆，98%的任务下发的成功率，立刻能展开这种测试的任务，数据验证也无需回传到云端，那大幅的去提升这种验证的效率和数，这个数据成本，这个这个数据传输的成本降低，对。

那我们群体智能可以同时支持，150万的验证任务的并行并行测试，那每日可以验证的里程数超过1500万公里，最后要讲的是反馈数据，那量产车每天能产生数百万条的接管事件，潜在接管事件。

那但是如何去有效的时候去完成筛选和压缩，将最有价值的数据上传到云端，并且通过这种自动化的分析，然后是数据闭环里面时，其实最关键的一步啊，那我们通过车端复杂的这种价值筛选，算法和缓存机制。

将万分之一最有价值的数据上传到云端，进行分析，并且在这里边时候，我们去通过5%以上的自动的分拣率，然后能去促使反馈迭代的这个数据飞轮，真正的能去运转起来，当然行业总会调侃说这个未来的研发成本高。

但我们实际上在研发过程中的时候，还是非常在意这个成本效率的，因为我们知道说，长期主义需要这个建立在短期成本可行性上的，因此我们说在研发上的这个巨大投入，并不是无节制无节制的支出。

那而是说对长期技术布局的，这个重要的一个要求，那面对着说这种百倍的算力的需求，我们打通了说这个车端边缘计算的能力，使得端端云的总算力达到26000000000ops，这个算力规模相当于什么呢。

相当于100个分布式的千卡的计算集群，那通过我们车端的计算和筛选生命周期的管理，那我们通过车端的缓存和数据压缩的技术，可以大大的减少数据回传量，降低这里边的流量成本，另外呢我们在讲说呃。

支架的研发周期周期性很强，波动很大，碰到发版的时候，大家一定都遇到过说这种资源上的这个波峰，那未来人工智能平台在规划之初，就是一个混合云的架构，那我们在自研的支架这个智算中心之外。

也加设了多个这个混合云的节点，能通过弹性的上云，分时的定价来去优化这个调度，有效的将这个波峰波谷能控制到10%以下，对最后呢我们要讲说研发任务的种类多，节奏快，如何去平衡这种研发交付和资源的有效利用。

解决资源就占用高，但是利用低的这问题，那我们通过多维的成本分析的工具和运营机制，有效地将研发的价值和资源，利用率的时候去做了关联，通过运营机制，那我们每年能在这里边时优化运这个研发，运营成本是数千万元。

在这里面我要表达的是，就是很很大程度上，然后做相同的事情，用一倍的成本跟用一半的成本，那它是完全不一样的，研发体系，对于研发成本的在意，本质上是对技术上边的，我们认为更高的要求，谈完了这个挑战。

我们也可以展望未来，那用在脚踏实地的同时，我们也仰望星空，自动驾驶的发展也充满了无限的可能，接下来呢我愿意分享几个关键方向的一些看法，包括端到端的大模型，全站的AI平台以及群体智能技术。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_7.png)

就第一点的话呢，端到端大模型大家听得很多，但是呢呃它不是什么灵丹妙药，如果目以目前的这个模型架构上，只能做到70分，那你无法通过说把这个端到端大模型上了车，然后就能做到100分。

因为这说明说你现在的工程效率还远没有，使你的模型架构达到上限，问题还很多啊，其次呢现在的模型架构转换也无法实现，并不是一夜之间的事就能完成的任务，在我看来，要去做得到大大模型。

需要满足以下几个关键的先决条件咳，首先是说数据飞轮，大家讲这个数据飞轮讲的很多，但落地效果好的寥寥无几，飞轮真的转起来了吗，里面核心的就是说数据验证体系的自动化率，然后呢。

我认为说在这里边是至少要能达到三个九以上，才能飞吧对吧，在各个模块上边也是，那尤其大家在讲端到端之前的时候，硅控是不是能全面的模型化了对吧，然后感知是不是可以上BEV transformer，去量产了。

地图，是不是可以实现有图无图的全面，这个自由的切换，那另外的时候我们在讲大模型，我们更愿意给它定义为云端的环境模型，云端的世界模型，那在这里边模型架构和研发方式的转变，需要有初步能去初步验证。

并且把模型应用到研发和验证流程之中，发挥作用，最后的时候我们要讲千卡集群包，包括我刚才看说有有有同事引引用了，马斯克的一个一个推特，然后呢我最近也在看，在6月4号的时候。

马斯克在社交媒体里面讲讲了一件事情，就是特斯拉在部署英伟达芯片，想要部署英伟达芯片来去使用它，但是呢那没有地方，然后他们就放置在仓库里面，后来呢特斯拉也在这个新的德州的工厂里边，开了这个新的空间。

用于容纳5万片H100的芯片，用于FSD的训练，那5万片H100对大家来讲只是听起来很疯狂，那我们说如果想要去做端到端大模型，1万块100总是需要的，那在这里边，如果你不能做到千卡级别的并行训练。

那万卡的训练基本是不可能的，那举举我前边的例子，在我们优化之前，千卡的训练，千卡训练的有效时长的时候只有85%，加速比呢只有60%，那考虑考虑这种故障率和加速比，万卡的真实性能。

在这乘上去的时候就只有1200卡不到了，那但是有效的训练时长提升到98%，加速比提高提高到91%，这样才有可能扩展到万卡，大概的时候也能做到一个这个八九千卡的，这样的规模，我们才能才能去使用它。

那毋庸置疑，在数据量足够大的情况之下，算力也足够充足的情况之下，端到端模，端到端模块的联合优化，是有可能整体去提供系统功能和体验上限的，但是正如千卡和万卡的例子一样。

如果没有很好的工程化的这个效率和质量，端到端带来的这个研发链路的简化闭环，这个闭环效率的红利，其实都会被低效的工程效率去吃掉了，对。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_9.png)

那后边的时候第二块的时候要讲AI平台，那在呃不仅在这个智能驾驶的这个大背景之下，最近我们也看说大语言模型也出圈了，对那AI平台开始更多广泛的去关注和讨论，那随着这种基础模型的能力的通用化。

那我们也看到了一个机会，就是全站的AI研发平台的可能性，那全站的AI平台，我我理解，不不仅仅可以支持自动驾驶的研发任务，最近我们还支持了集团之内的那像NOI，也就是未来内部的智能座舱的助手。

还有内部的NEOGPT的应用平台，客服专属群，那正如我们可以跨平台，跨地区多车型的这种模型模型产线交付一样，那实现了85%以上的模块的复复用度，让我们也成为国内第一个，可以跨州量产支架的汽车企业。

在2022年的3月国产，我们在国内这个量产的ET7，交付了自研的NOP的功能，在同年的9月，然后我们智驾算法就上线了欧洲的这个ET7，并且建立了功能安全，智能安全等大规模的量产的能力。

这也得益于说我们有高度可以复用的，全站的AI能力，对咳全站AI的平台统一管理，去优化数据AI技术应用的整合，提升了效率，并且降低了它的整个研发成本，才能真正实现我们所谓的m l ops。

大家也应该知道说，m l off在绝大绝大多数企业落地的时候，其实并不是都不是特别好，因为一个好的研发工具，在我们看来，不仅仅要适应于企业内部的研发流程，还应该去适应于它的不同的阶段。

生搬硬套的去把这些工具强塞给，这个企业的AI研发里面是不太现实的，那在我们去设计AI的全站的AI平台，特别是注重它的灵活性和适应性能，确保说在满足各个阶段的需求，就像是说建一个高效的一个引擎。

各个部件可以完美的配合，可以有效的最大限度上面提升它的性能和效率。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_11.png)

在2023年9月，未来第一次的未来科技日，然后我们第一次介绍了群体智能的技术，那群体智能是未来智能驾驶技术，未来发展的重要的方向，未来群体智能具备强大的计算能力，达到6700000000offs。

那能够每秒处理2。1PB的数据，通过优化这种并发和实验，一定程度上我们实现了真正的车云一体化，进行分布式的验证和协同学习，也正如我前面提到的那，在AEB的道路验证NO p plus。

全这种全域领航开成拓路，包括世界大模型的数据迭代上，那群体智能都发挥了其强大优势，和无限的无限的潜力，它让我们量产的功能可以持续的高效的迭代，不断的为用户提供更安全，更舒适，更加个性化的支架功能体验。

就像说在赛车队的这个在赛道上，可以通过协同的作战实现最佳的战绩，那我们量产车队也可以通过这种协同学习，不断的进步和提升，我们也相信在不远的将来，那我们自己的自研芯片进一步的去整合，去定制这些功能和能力。

以推动智能驾驶和通用AI的技术的发展，让我们去设想，那智驾智能这个支架的汽车，在没有支架的同时候，其余时间也是可以进行推理计算的，那通过闲时复用，将算力共享给其他智能应用，就像分布式的云一样。

那将智能驾驶乃至这将为整个智能驾驶，乃至整个人工智能行业，带来巨大的这个算力的提升，那真正的实现车联网和云计算的结合，那我今天的分享就到这里结束，那感谢大家聆听，最后的时候。

我给大家这个一一部小小的影片，让大家感受一下，说我们在呃未来如何去做这个呃智能驾驶。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_13.png)

包括他们的结果是怎么样的，感谢大家，领航开始，即将开始领航换电。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6d2748628a9fb45a464a5e5cdd946fd1_15.png)

# 2024北京智源大会-智能驾驶 - P6：自动驾驶3.0时代，大模型重塑汽车智能化路线：贺 翔 - 智源社区 - BV1Ww4m1a7gr

感谢法王主任，感谢志源的邀请啊，今天有机会能给呃各位嘉宾分享一下，我们在那个自动驾驶大模型方面的，一些实践经验，我觉得刚刚访华主任有一点，所以我非常的赞同啊，前面几位数啊，主机厂侍卫讲了很多。

修了很多肌肉，做了很多牛逼的东西，但是好像讲的不够透彻是吧，我们作为供应商，我们就把最干货，最底层的东西全部抛出来了，尤其我们过去几年，在整个大模型方面探索中遇到的一些问题。

走过的一些弯路也一并贡献出来是吧，希望能对大家有一些呃启发或者帮助吧，对，我们根据过去几年，整个自动驾驶发展的这么一个历程，把整个自动驾驶的这个呃技术的演进路线，分为了三个阶段。

第一个阶段我们称之为一个硬件驱动是吧，大家主要是靠堆激光雷达，那第二个阶段，其实呃现在绝大部分公司可能处在第二个阶段，就是用一些小的数据，小的模型去解决感知啊，认知道决策规划的这么的问题。

那么接下来我们判断是说未来是一个3。0，我们称之为数据驱动，它的核心特点就是大数据，大数理大模型是吧，其实刚刚很多那个呃呃，前面的老师也讲到了这一块。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_1.png)

那么在3。0时代，我们更应该去做什么呢是吧，刚才讲到三个关键词，说大的数据，大的模型和大的算力，那么今天会主要跟大家分享一下，我们整个的大模型到到底是怎么做的，以及具体这个大模型我们怎样去做。

我们整个的数据系统，然后算力这块我们在这就不详细讲了，因为这个我们之前已经讲过很多次了，那么大模型这一块，其实去年也是在资源的会上。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_3.png)

我们公布了，我们当时的那个自动驾驶的大模型的，我们称之为现在可以称之为1。0版本吧，当时我们是业界，应该是第一家，使用深层式技术来做自动驾驶大模型的，这么一个方案呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_5.png)

为什么我们会走到这条路呢，我们现在先呃留一个悬念啊，我们最后我们会给大家回顾一下，我们整个的研发的历史，以及我们怎么样是走上这条路来的，那么这条路我们选中了啊，通过深层式方式。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_7.png)

而且是通过BEV生产的方式来解决，自动驾驶问题，那么在研发的过程中，我们也发现了很多的问题对吧，开始我们可能把这个问题想的简单了，我们觉得因为我们大量的量产车在外面开，在全国各地开会。

传各种各样的数据是吧，我们天真的就以为说，我只要有大量的这样的数据，然后把它看到的世界表达成一个BEV的视图，然后通过深层次的大模型，把未来的BEV生成出来是吧，我只要能把未来的BEB预测出来。

我自动驾驶的任务就解决了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_9.png)

那么实际上在训练的过程中，我们花了大概有半年的时间吧，呃训练的过程中遇到了很多的问题，这些问题主要包括两个，第一个是说，因为你采用的数据是量产回传的数据，它有它先天的优势，就是右边的。

也有它先先天的弱势，就是左边的呃，它先天弱势在于说，因为你不可能用量产车来回传海量的视频数据，这个成本是比较高的，我们没有办法去承受这么高的成本，所以我们回传的是感知的结果，当然也会回传一些视频数据。

这些数据肯定是在特定的处罚条件下才会出来，所以我们回传的海量的数据，基本上是感知的结果，然后有了感知的结果，有了实际的驾驶动作之后，我们就可以把这两个合在一起，做那个深程师的学习。

但是我们很很很遗憾的发现，我们的量产车的数据，回传的感知结果其实并不完美，对于我们后面去做那种想要一学，一个非常牛的这种老司机的这种驾驶决策出来，是有很多的限制的，比如说我在举了一个非常非常常见的例子。

这基本上呃呃哪都能碰得到，就是我们在城里头车开到这种城，进入城市AA之后，我们会发现城里的车道线其实并不那么清晰，因为他经常磨来磨去是吧，都磨没了，尤其是在光照条件，比如逆光这样的环境下。

他根本就可能原传统的感知技术，可能做的不够好，它回收的车道线就不好是吧，那你基于这样不好的感知结果，再去虚拟后面的那支角色，其实就会有先天不足是吧，那么它的好处就是说右边确实我们看得到。

这是我们真实的数据，我们的车卖出去之后是在全国各地开的，也是它的数据的分布确实非常非常的好，有地域的分布，然后我们把这些数据拿出来，又重新看了一下它的场景的分布，其实也是非常非常好的。

就是各种各样的场景你都能遇得到，那么这个数据非常好的，是一个好事，同时我觉得也是一个巨大的挑战，刚才大家也讲到了，说呃特斯拉训这个呃DUANGDUANG的大模型，花了差不多100亿美金是吧。

那我们作为一家小公司，新西兰不可能干这样的事是吧，所以我们想到是说面对这么复杂的场景，你要训出来一个特别牛的老司机，你真正的降低成本的方式在哪里是吧，你人家花100亿美金，你能不能花1亿人民币是吧。

这个挑战是比较大的，那么具体怎么做的呢，我们先把这个任务定义清除出，我们到底要干嘛，这是我们训完1。0之后，遇到这些问题之后，我们重新去思考了一下，这个自动驾驶的大模型究竟应该去干什么事情。

我们把它分成了三个阶段，第一个阶段就说根据我们之前对量产车回传来，感知的效果不好的这么一个大的问题，我们觉得第一步，我们该先首先需要做一个通用的感知能力，为什么叫它这个通用的感知能力呢。

这个跟我们传统的，就是我们之前在车上已经卖出去的，这些量产的感知对比，它是一个完全不一样的东西，我们传统的感知大家都很清楚是吧，是基于标注来做的，我标了十类物体，那我就只能识别这十类是吧，没有标过的。

它就没有，那就也就不能回传是吧，所以我们希望通做的通用感知是说，它能跟我们的能量感知一样，它具备2D的能力，也就它能够看懂图片是吧，它具备3D的能力，它能够理解三维的空间。

同时它还能够加速时序变成一个4D的东西，最好它还能够识别万物，它不是一个只能够看懂图片纹理的，它能够识别弯，我知道这是什么东西是吧，跟我们人的感知是一样的，然后有了这样完美的感知之后。

你后面这个第二步我觉得做起来才是有价值的，那第二步我们希望做到什么什么的，我们希望是说在你有人类的感知之后，能够做到一个人类的驾驶决策，那么人类的价值角色，跟我们传统的价值角色有什么差异，它的差异。

我觉得最主要体现在说，我们大模型和小模型的一个很大的差异，我们希望这个认知决策是具备世界知识的，有时我们能够看懂或者理解，这个世界背后运作的规律是吧，这是个我们称之为世界知识。

能够看懂各种各样的驾驶场景，而不是人为的去定义各种各样的场景，以及定义各种各样的静态的规则是吧，能够具备这种推理的能力，我觉得这样才是真正具备一个老司机的，这么一个能力，然后这两个都做好了之后。

我们才有机会去把它们俩拼在一起，去做咚咚咚的训练，来提升整个全链条的这种全局的最最优先，所以我们在前面尝试过半年之后，就把我们整个的技术路线调整为这三个目标，那基于这样的目标，我们整体的那个架构的设计。

大概是分为这么一个阶段吧，的左边是我们的那个感知大模型，它实际上你可以简单的认为它是一个4D的，Encoder，就是为了把我是看到的事件，把它encode到一个4D的空间里头去。

然后右边是我们的那个内置决策对吧，这样我们只画了一个BEB的生成，就是我们有了这对这个世界的完美的认知之后，接下来就说我怎样能够把我这个看到的是，看到的信息，把它编码传递下去，让下游的那支角色。

能够知道我看到的世界长什么样子，并且我能够怎么样去利用好我看到的这些信息，来做出很好的驾驶决策是吧，这是我们整体的架构，那么刚才讲到，我们怎么样去降低我们整个的训练的成本呃。

我记得我们当时应该是在22年，我们去建我们的自创中心的时候，我们大致算了一下，如果我们要去做覆盖全国的，这种各种各样的场景，K4的训练，这种视频，比如100万个克利普，当时我们还不敢想。

有1000万个克利普啊，100万个克利普覆盖各种各样的场景，我们要去把它训练起来的话，我们当时算了一下，至少要1万卡，这个很显然我是要不到这么多钱的是吧，那怎么办，那还我们我们想到一个办法。

就是我们能不能去借助外界已经训练好的，这种大模型是吧，别人帮我们一把把，用他的卡，不把这个事情已经训练好了，我把它里面的东西挖出来就可以了嘛对吧，这是一条一条现实可行的，降低我们训练成本的这个道路。

所以我们在整个的这个自动驾驶大模型里面，引入了两个外挂，那么在感知大模型里面，我们有了一个多模态的大模型，它的目标就是实现刚才我们讲的识别万物，因为我们在这个呃图片编码的过程中。

可以很容易地借助外部的多模态大模型去对齐，我们的文本的特征，这样就相当于说我这个东西，不但能看懂图片纹理，也能够理解世界的万物，它是什么东西是吧，然后在后面那个我们讲到在内资决策里面，很重要的一点。

就是说，我们如果要训出一个像人类一样的老司机，他的必要条件就是它具备世界知识是吧，你能够看懂人类世界，你才能够像人类世界一样去开车是吧，你来个小狗小猫，你续再怎么训它也训不出老司机出来是吧。

小狗小猫也会开车，我觉得我前两天刷了个视频，就是个小狗在开车，我觉得挺有意思的，但他不可能像人类一样看懂人类的世界，他能够看懂交通报那个标志吗是吧，它能够看懂我们的路牌吗，很显然是不不行的。

那么这些信息在哪里，其实大语言模型里面都已经有了，我们的任务是说，能不能把这些信息从里面拽出来，我们利用好就可以了，那么通过这样的设计，我们就有信心说，我们只用几千块卡就能把这个事情搞定。

这也是我们我觉得国内的企业，在算力有限的前提下，可行的这么一条路线。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_11.png)

那么具体我们看一下左边整，我们整个的感知大模型大概是怎么做的啊，我们真的是把干货都给拉出来了是吧，左边是我们的那个最，左边是我们的那个呃，摄像头的数据直接输入进来，输入进来之后。

我们首先会有一个自监督的图片编码器，我们会把图片里面的纹理特征全部提取出来，也就这得到一个二维的编码，那么二维的编码之后，它是个图片的特征嘛，我们都会跟外界的那个动漫态大模型去对齐，也就是对齐完了之后。

那它就具备了那个识别万物的能力了，然后接下来我们会它会给它进行一个升维，变到变到三维空间，加上时序，实际上就是四维空间，那我们怎么去做这个事情呢，我们是用那个图片的下一帧预测啊，是视频的下一帧预测。

因为我们这个左边输入来的其实不是图片，是视频，它是一个序列是吧，我只要能够让这个模型去预测，我下一帧图片长什么样子，跟我的增值去对去做对比，如果他能够预测对的话，那这个模型一定是三维的。

因为在这个过程中，我这个车已经往前开了一段距离了是吧，所以通过这种方式来预测，我们通过乐福的方式，把图片的下一帧给渲染出来，然后跟我们的真实图片去做对比，那这强制的这个模型学会了我们的4G的空间。

那这样的话我们就真正做到了，像我们刚才讲的，像我们的人类一样，它具备能够看懂二维图片，看懂三维空间是吧，具备时序，而且能够识别万物，然后我们就通过这种方式，就得到了一个4D的编码的空间。

这个我们认为至少在现在看起来，是我达到我们所谓的完美感知的这么一个结果。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_13.png)

我们可以看一个我们的demo，这是我们实际的呃，车上回传的一个一一个数据，上面两个视频前面那个呃是上面那个是前四，下面那个是后视，因为呃地方有限啊，就就就放了两个，然后我们的模型可以输出这些东西呢。

左边是三维重建之后的结果，这里面是有两个图的，一个是上面那个是从上往下看，也就是两瞰图的视角把它拍扁了，其实就是B1B图，那么下面这个图是一个前视视角，有它在三维空间里，它是可以改变你的视角的。

然后当然也可以做语义的分割，也可以实现光流实现深度，如果跟雷达结合起来，就可以实现去做制度的标注了，4D的自动标注是吧，然后右边我们也给了一个非常常复杂的场景，这是一个绑定的。

应该绑定一个入口非常复杂的一个场景，我们可以看得到，我们在一个模型里面可以把分割语义光流是吧，那个深度全部搞定了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_15.png)

那刚才我们也讲到了，我们是呃交互GPT11。0是做的呃，BB的生成，我们还是沿用了这套思路，但是我们会把前面的那个输入换掉，之前我们训训练效果不好的原因，我们分析下来是说。

我们感知的结果是从量产车上回来的，而量产车的感知是基于传统的白名单的标注方，式来做的是吧，也就说它有世界上90%的东西，它是看不到的，它也当然无法回传是吧，所以我们把它换成了我们现在的完美感知。

然后有了这个完美感知之后，也就是我们把这个设计空间把它token换，也说刚才看到那个三维空间把它拍扁了，变成BEV，再把它token化，token化完了之后再丢进去，让它去生成未来的BEB。

那这样的话它生成的效果就会比原来好很多，同时我们刚才也讲到，我给它引入一个外挂，这个外挂就是大于大的语言模型，我们可以把我们感知模型看到的世世界，丢给这个大的语言模型。

那他来告诉我你看到的东西是什么是吧，然后你能不能给我一些驾驶的建议呃，相对于说我们的那个呃副驾驶坐了一个陪嫁，一个老司机，是吧，他可以给你去解释，你看到的世界是长什么样子，你应该采取什么样的决策。

通过这种方式，我们能够要大规模的降低我们训练的费用，就快速的使我们上面这个模型进行收敛。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_17.png)

这也是我们一个实际的例子啊，我们找了一个比较复杂的路口，这些路口的标牌肯定都是人类世界的是吧，呃我们要看懂这些东西，如果纯粹采用自动驾驶的数据，这个讯起来是非常非常困难的，因为你没有人去标注。

上面这个标牌到底是个什么意思是吧，但是你借助大语言模型之后，你会发现这个事情很容易就拒绝了，呃当然这里面还是有一些问题啊，比如说他对汉字的理解可能不是特别好，但对于这种标牌的理解还是相当不错的。

各种各样的符号它的理解还是可以的，我觉得可能比很多的女士会懂得更多，呃我们今年又把这个任务进行了一些扩展啊，我们去年做了BEV呃，坦白讲这个思路我们走过了，有一定的效果，但是不够好是吧。

如果大家还想尝试，我觉得也可以，但是我觉得我们现在接下来，真正我们称之为这个自动驾驶基础模型脸，应该是下面两个，我们现在已经第二个已经做完了，我觉得就是刚才你讲，我们可以具具备图片生成的能力是吧。

我们主可以，现在是可以把这个中间的那个4G的空间，把它解码成图片的token，然后用这个GPT的方式去生成，我们下一帧的图片，而且是多V的，是个黄色的图片呃，这个任务看起来好像不难是吧。

但是实际上你真的去做，你就会发现很难，因为在呃那个我们跟收纳去对比，就发现了收纳为什么在自动驾驶好，你没办法用，首先它是单式的，它只有一个摄像头是吧，我们自动驾驶都是环视都一圈摄像头。

说他没有办法搞定这个事情，这是第一步，第二步是说他没有做到，没有办法做到时序，或者说物理空间上的三维空间上的一致性，这个问题我们留到了第三条路，我们现在正在呃做这个呃，难度是比较大的啊。

比如我们在生成图片之后，我们更希望的是说我们应该不是生成一个图片，图片是我们看到的一个表象而已，也就是这个现实世界，在我们眼睛里面一个投影而已，那我们真正的世界是怎么样呢，是个三维的是吧。

那我能不能直接把我们刚才编码的4G空间，把它变成3D的token，然后用GPT的方式来生成，未来世界的3D token，也是我们未来世界这个车往前开了2米之后，这个未来世界的3D脱口会变成什么样子。

这个才是我们真正要解决的核心问题，这个问题解决了之后，你才有可能去渲染出来多V的，保持空间一致性的，持续一致性的这种视频出来是吧，这个视频才对我们这种驾驶是有价值的，你可以用来做训练。

可以用来做测评是吧，如果没有这个东西，我觉得都是很困难的是吧，至少我目前没有看到，有合适的技术来解决这个问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_19.png)

我们也可以来看一些，那个我们现在实际的一些demo，就这是我们早期做的那个BEV，生成的这么一个呃效果，就这是呃车上传回来的几个视频呃，左边是实际的BEV，就是我们感知的结果，把它拼成了一个BEV。

然后这个模型会把这个左边的BEB图输进去，那这个模型来预测未来的BEV会长什么样子，其实我觉得通过这种方式呃，已经可以解决自动驾驶的很多的问题了，然后这是我们多V的图片来生成的效果。

其实图片的生成效果已经非常的好了，嗯基本上如果不提示的话，是肉眼是很难分清楚哪些是生成的，哪些是假的是吧，那那那哪些是这真实的图片，其实只有文字在这里面是乱码，除了文字之外，别的我觉得你很难看出来。

就这里面生成的文字都是乱码，这个现在确确实还是一个一个，比较难以解决的问题，就内核的这种生存技术，目前在生成文字上，我目前还没有看到说能够生成一个交通标志牌，能够符合我们真正的像人写出来的汉字。

一样是吧，现在还没有，包括我们现在的那种交通标志牌啦，包括我们现在广告牌啦，生下来的文字其实都是乱码，这个目前还没有解决，我们还在呃公关中，然后视频那一块还没有啊，呃非常抱歉。

我们希望下一次能给大家带来多V的环视，视频的生成，那么接下来就讲到数据了是吧，我们刚才讲的是，我们整个的核心的大模型的制作，那么在这个大模型的呃，在上车之前，就目前而言。

我们现在大模型上车还是比较困难的，刚才也看到我们引入了多模态的大模型，说大语言模型，那参数量都很大，这个想放想要放到车上去，短期内还是比较困难的，那么在上车之前，我们主要是用这个大的模型来赋能。

我们云端的整个的工具链，这是我们整个的那个航母做的那个ma的呃，数据智能体系，我们从整个的数据的采集到数据的管理，数据的标注，数据的筛选是吧，数据的标签化等等，底层都是通过大模型来支持的。

有了大模型之后，你会发现传统的这种对数据的管理的工作，就会变得像我们跟ChatGPT交互一样，非常的简单轻松是吧，大幅度提高你的效率，我们可以看几个例子哈，这是呃我们去从我们的海量的那个图片里面。

比如110的图片，你要去找出一些collar case，比如六个灯走过斑马线是吧，你传统的方法你是很难去找的，比如传统我们打标签，你不可能打这样的标签是吧，那有了这种淡漠性之后。

因为他对可以对这些场景做一些文字的理解，那这样的话你可以输入输入任意文字，非常复杂的文字都它都可以去理解，然后非常精准的把你想要的那个图片找出来，比如这是呃有了大模型之后。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_21.png)

对整个数据筛选的这个这么一个体效，同样我们还可以去定向的生成数据，比如说我们可以任意画几条车道线，它就可以基于这条这几条车道线，来生成各种各样的数据，包括比如晴天的，雪天的，雨天的，雾天的都可以。

当然你也可以换，比如环岛可以换弯道都是可以的，那我们还可以对我们采集到的数据，做一些迁风格的迁移，比如说我们采到的一张数据，我们可以到可以把它变成不同的天气，不同的光照是吧，不同的纹理。

这样来丰富我们场景的数据，那这次引入大模型之后，我们可以对这个驾驶的场景做一些解释，我们可以看得出来就是说他对这种复杂的场景，他还是有一定的理解能力的，他可以告诉你。

虽然说这个呃呃可能还没有达到我们的预期啊，说实话还没有达到我们预期，但是它能够把几里面，交通里面的一些核心的元素已经告诉你了，通过这些它呃，呃从这个呃场景中提炼出来的一些价值的解释。

我们可以可以用这些价值解释来做很多的工作，比如说我们那个分丰富这个场景的那个特征，能够帮我们的PM去分析各种各样的场景是吧，能够对这些场景进行的聚类的筛选啊等等，啊这是我们一个实际的case。

就回到了最早我们看到的这个case，那么有了大模型之后，我们这个case是怎么去解决的，是吧啊，传统的感知肯定是这么差的，那么有了这个大毛蟹之后，我们首先会去检索同样的场景，比如说你输入一段话。

叫什么城市里面的这种模糊车道线，它它就会把所有的模糊车道线剪出来对吧，如果数据量够了，那没有问题，你直接去训就可以了，如果数据量不够，还我们还可以去做数据生成，你输入一个prompt就可以了。

它就可以按照你的方式来生成，各种各样类似场景的数据，然后用这些数据快速的去训练，就能够快速地解决这个问题，好了，最后我也给大家分享一下，我们整个的呃自动驾驶大模型的研发的历史，走过的一些弯路，呃。

其实我们从2022年就开始去做这个事情了，呃为什么我们会做这个事情呢，其实嗯也不是说什么呃，完全就是路径依赖，因为我们原来是干互联网的，在干互联网的过程中，其实这个东西已经用了很多了是吧。

互联网里面用这种强制风暴已经用了很多年了，那我们到了自动驾驶领域，自然也会想到是说这个自动驾驶领域，这个这个任务其实跟机器翻译没什么区别是吧，我输入的是一段一个序列图片序列是吧。

输出的实际上驾驶动作就是个机器翻译问题，所以我们最早就把乡村foo弄过来了，然后训练一个模型，训完之后发现，这个任务其实比机器翻译要难得多得多，所以我们就把这个任务稍微简化了一下。

就是我们把感知的结果就是把图片干掉了，我们能不能用感知的结果来训是吧，这样相对来说稍微简单一点，于是我们就引入了类似于BT这样的呃模型，就是我们把我们量产车回传的海量的数据，因为它是对齐的。

就它有感知的结果，也有司机的驾驶的动作，我们对齐之后就得到了天然的得到一个派对，那么我把司机的驾驶动作mask住，那这个bot来预测实际的驾驶动作，这个跟训练的模式跟bot是完全一样的，那训完了之后。

确实效果比那个原来我们直接用图片训，要好很多啊，但是这个模式其实也是有问题的，就是我们把这个驾驶的那个呃，动作的那个发生的那个原因，可能搞得稍微有有点错误吧，最后我们发现这个。

其实这个任务其实跟GPT是更像的，为什么这么说呢，因为我们真正的老司机，他是不是基于我现在看到的，也就不是基于我现在的感知结果，来做出驾驶决策的，它更多的是基于对未来的预测是吧。

我未来这个世界会可能会发生什么变化，比如我旁边那个车会不会出来加塞是吧，基于这些未来的预测来采取我驾驶动作的，所以这个预测就是个生成，所以我们很快的就把整个的那个技术站从bot，迁向了GPT。

通过这种生产的的方式，生成B1B的方式来解决驾驶决策的问题，这就是我们最开始讲到的，我们叫AGPT1。0，然后做完之后我们就会发现，在你的感知结果不够完美的前提下，你这个驾驶决策再怎么训都训不好是吧。

你不可能在你看不见的时候，做出一个好的驾驶动作，所以我们就提出来要做一个完美的感知，这个完美的感知，其实也是我们应该是在23年初的时候提的，当时这个任务也是相当的艰难的，但是经过我们一年左右的时间吧。

确实这个事情我们还是搞定了，就是我们通过一个模型就能够解决2D3D，4D包括呃识别万物这样的一些结果，那么今年我们会把这两个模型呃拼到一起，做一个咚咚咚的训练，真正是说从完美的感知到完美的认知决策。

能够让这个呃呃自动驾驶的大模型，像老司机一样去识别万物，去看懂我们各种各样的非常复杂的能力的世界，去理解各种各样的驾驶场景，然后做出像人类一样的价值的决策。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/f8db1d0b0d0d5bee00912a93c5a6f364_23.png)

好我的分享就到这。

# 2024北京智源大会-智能驾驶 - P7：智能网联汽车安全验证策略和仿真工具链：杨 强 - 智源社区 - BV1Ww4m1a7gr

呃尊敬的法王主任，尊敬的各位嘉宾，大家下午好，呃刚才几位嘉宾呢讲了很多这个算法的迭代，包括引入大模型啊，来提高我们整个自驾系统的安全性，包括它的性能，那接下来接下来呢我要分享一下，赛木科技。

在自家系统的安全验证方面的一些思考，和我们解决方案，呃首先我要简单来谈一谈，这个自驾系统安全存在哪些方面的这个挑战啊，呃我们知道就是随着这个L3以上等级，这种价值对这个安全这个责任认定的不一样啊。

呃我们自驾系统如何去通过设计，实现我们的这个安全，包括通过这个验证保证我们的安全，有一系列这个要求啊，比如说我们说功能安全对吧，这个26262其实它解决什么问题啊，就是我们对于一个系统内部的。

这个据它哈纳分析，我们可以把它的这个硬件，硬件的随机性失效和这个系统性失效可以解决，它要解决的问题是，我们这个系统本身内部没有问题，那网络安全呢对吧，通过网络安全的话，我们通过相关的安全设计啊。

我们我们知道这个自家系统它是个联网的对吧，那么可能存在外部的攻击，距个IO448呀，我们要解决这个，不要受到外部这个攻击的这种风险，还有一个就是我们叫做预区功能安全，这个聚光安全。

它要这个呃安全分析和设计要解决问题，就是说当我们这个系统内部足够安全了，但是在我们系统设计层面，它有一些这个天然的，这一些这个系统的这个缺陷，比如我们一些感知传感器，它对一些这种光照。

或者说我们对一些这个识别的这个范围，有缺陷的话，那如何来这个验证这个领域的一个一个安全，同时我们这个自家系统啊，它的软硬件非常复杂，我们很难通过分解各个模块啊，把这个案件给一这个解决。

另外的话就是说我们知道这个支架，我们说它安全验证很难，一个很重要的就是说它的长尾问题，或者说我们很难通过一种有效的方法，我们去枚举这个测试场景，我们知道我们做这个软件或硬件测试啊。

我们很多这种测试都是可以，它是确定的，所以我们是非常好的，可以去确保它的安全性，那自驾这个系统的话，它一个很大的挑战，就是说工况不确定性很难枚举呃，呃我结合这个I14482对吧。

在里面在这个安全验证的领域啊，提出了一个非常重要的问题，就是说如何去制定这个ADS系统这残余风险，这个验证的方法和这个这个工具链。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_1.png)

呃这里呃来介绍一下我们SAM科技，在这个自驾系统和安全验证，这个策略方面的一些思考，我们都知道哈，基于场景的这个测试方法，是一种有效的验证自驾系统的安全性，但是如何去构建这个场景啊，变得至关重要。

我们我们讲这个AA大师的话，通过我们这种专家经验对吧，或者我们通过手工搭建的话，我们认为基本上是可以确保它L2，这种安全等级的，但是上升到L3以上的这种测试的话，我们很难通过手动大众主动搭建场景。

包括通过这种专家经验啊，能去呃制作一个非常这个全的有效局，所以我们需要寻找一套方法，如何去解决这个场景的问题，我们借助于这个SDF的这个四象限分析啊对吧，这里面就比较关注的就是对于这个危险场景的。

这个这个怎么来验证，我们如果对已知的危险这个场景，那我们认为我们可以把它放在一个二，要那个20202范畴里面，我们基本上可以确保的，但是对于这种未知的风险场景，怎么来验证它呢对吧。

其实他就很难去找到这些边界，那我们的一个思路就是说，我们把这个对自驾系统的测试验证啊，把它转化到一个测试空间探索的这个领域，其实这是空间参数，在我们很多，比如说航天航空这个领域的话，其实应用比较多的呃。

这测试空间呢对吧，我们也可以去再再细分一下，就是叫做连续空间和这个离散空间，怎么理解这两个概念对吧，这个连线空间的话，我们就是它是有边界的，举个例子，在我们今天这个当中对吧，虽然它很很这个很大。

但是我们总归能在一定的范围当中，这个去限定它的边界，那还有一种是离散的，就离散的话，你很难通过建模去找到这个系统的边界，对不对，那针对这两种不同的支持空间的话，就是我们的方案就是说针对连续的。

那我们要去通过我们的SDF安全分析，去定义一个这样的逻辑场景，这个逻辑场景呢我们认为它带边界，就是一个在一个这样的这个空间当中，然后再结合我们的这个安全，我们的这个测试空间分析工具及生成器。

具体的场景呃，后面我们会具体展展开来讲哈，最终的话基于我们这个在测试空间当中，我们会去找到我们关心的那一些这个样本点，然后通过统计来论证我们的这个这个这个风险，那我们的理解就是说在给定边界的条件下。

基于我们的方法的话，我们是可以有效的去量化的评估，我们整个系统的安全的，这第一个第二个就是离散的空间，就离散相对于我们这个呃连续来讲，就是无法去定义它的边界，那就你你很很难去通过这种建模的方式来做到。

这种呃风险评估，那我们的方案就是说，通过构建大规模的AI的交通流，来做这种随机测试，所以总结来讲，就我们把测试空间分为两大，这个这个这个这个方面来制定相应，相不那个对应的这个验证策略。

和我们的仿真工具链，我们先来看一看对离散空间，我们如何来进行这个风险的评估的方案呃，我们在我们的这个这张那个PP上讲的话，我们第一首先通过缩TIF安全分析，安全分析的话。

就是它可以构建一个这样的这个逻辑场景，呃，我们讲说那个锁屏分析的话，其实分为两大块啊，一个是我们的安全分析跟我们的验证，那有一些我们在安全阶段，比如说我的系统上有些这个性能的局限。

我们直接修改我们的系统，就可以去规避一些风，但是有一些我们是无法去调整系统方案的，这时候可能可能存在潜在的危险，这时候我们的安全工程师啊，就会把相应的这些我们做危害的话，给到我们仿真工程师。

但是呢我们在安全分析阶段，需要输出一个逻辑场景，你可以把它理解成一个有待边界的这个空间，呃，比如说我们它结合我们这个仿真领域的话，我们叫这个逻辑场景和我们的这个参数分布。

当我们拿到一个这样的这个呃逻辑场景以后，我们就要进行这个采样对吧，我们在统计当中，那采样的话，我们有很多种方法，我们知道像比比如比如说种均匀采样的话，蒙特卡罗它的这个采样的这个密度啊会很高。

它的覆盖率也很高，但是呢它有个问题，你要对一个高维度空间进行一个，非常好的覆盖的话，它的采样样本也是很高的，那即使我们基于你这种云端这种大脑，这种并发的这种仿真测试啊。

那我们的测试结果也是这个呃证明是不可行的，所以我们希望找到一种好的这个DOE，我们叫实验设计，我们通过少量的样本点来去对空间进行覆盖，同时呢我们可以通过这些样本点，对整个这个系统的这个我们叫失败概率啊。

进行估算呃，我们DAOE完了以后，我们就会基于逻辑场景生成我们的具体场景，这时候我们就会做我们的仿真测试，结合我们的云的仿真的这种大算力平台，当我们有了这些仿真的这些数据以后，我们会做参数的敏感性分析。

就我刚才提到了对吧，我们对一个这个高维的空间啊，你去采样就非非常在这个功能上是不可行的，我们要进行降维敏感性，就是消除那一些对我们自家系统影响不高的，一些因子敏感性分析完了以后。

我们会再进入到我们下一轮的这个，仿真测试迭代，这时候我们就是做我们的可靠性分析，可靠性分析它最终要解决的问题，就是说通过我们对系统的失败概率，你可以认为它是碰撞对吧，或者我们去TTC违反来。

去这个估算出我们这个自驾系统啊，它未来的失败的一个这个概率，我们知道从这个呃失败概率的角度，如果说我们一个系统，它的这个失败概率是个十的呃，十的四次方，五次方，我们认为它是不不够安全的，但如果一个系统。

它的这个失败概率是一个十的七次方，甚至更低，我们认为未来如果在仿真当中，我们可以达到这个级别的话，那我认为它未来在这个实车的这个路程当中啊，基本上这种工况也是不会发生的，那整个就是针对这个方案的话。

就是我们对于连续空间，我们认为我们是有一套，这个从我们数学上可以论证的，这些我们的工程应用上的话，可以去给出一个量化的这个。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_3.png)

风险的评估的一个指标，呃其实我们刚才讲的那个整个的一个，验证的思路，我们分为几大块，首先就是我们这模型的定义，模型的话，这个呃我们理解就是它它是一个逻辑场景对吧，借助于我们数据库分析的这个工具。

当我们有了这样的一个逻辑场景，定义好了，这样测试空间我们做敏感性分析，敏感性这是刚才讲的，我们高采样啊对吧，包括我们会去呃拟合一些代理模型，对我们这个参数进行一些这个分析，那么敏感性分析完了以后。

我们就会做这个可靠性分析，可靠性分析就是呃对，要对我们一个高维的连续空间下，我们要对所有的失败率，都要进行一个覆盖或者搜寻，那我们要做的事，就是要有效的去找到这些失败玉，对整个系统一个呃失败概率的估算。

那么我们做完可靠性分析以后，我们会再做一个这个叫鲁棒性分析，鲁邦先分是干嘛呢对吧，当我们在前一阶段，已经对这个系统的一些零界面，或者说就是说它可能存在这个，失败风险的区域啊，我们进行了这个测试验证。

那我们增加脑洞，看一看我们整个系统鲁棒性怎么样，所以通过我们这个敏感性可靠性呃，鲁棒性分析，来对整个呃给定的一个我们从ODD分析下来，给定的一个这个呃区域，一个空间啊，进行一个安全验证。

最后我们会有一个失败概率的一个这样的输出。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_5.png)

那具体展开来讲的话，我们敏感性它这个主要做什么，对不对，其实呃呃它通过我们这个，比如说我们首先要DOE这个实验的设计，那借助我们为仿真，那在我们用这个敏感性分析，其实分为两个阶段了。

我们第一个我们叫model free的一个，这样的这个敏感性分析，这个阶段的话，我们就是要去对呃，相关的这个敏感性的参数进行一个排序，但是它并不去决定呃，这个参数是否会影响。

我们对整个这个自驾系统的一些KPI的这个影响，那我们做完第一model fit这个敏感性分析以后，我们会进入到第二个环节，就是我们讲的model base的一个一个这个验证。

这个我们会结合我们这些机器学习的，这个算法呀，那最终会选取了一些，我们叫做对整个这个自驾这个行影响的，那些这个场景的因子，比如说我们这个前车的离我们的距离啊，我们的速度速度啊等等，呃这里我们重点强调。

就是说我们会基于我们的这个敏感性分析，算算法，我们通过比较低的这个样本点，那对我们整个空间进行填充，来结合这个基于统计和机器学习的方法呀，来确定这一些这个这个因变量当中，不确定的这个因子啊。

对我整个这个空间的一个影响。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_7.png)

呃这里讲了两个这个阶段，我刚才也讲到，对不对，我们第一阶段的话，就是通过我们统计的一些分析，来找到我们这些这个应变量。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_9.png)

它的一些这个优先级，第二个我们叫model base，model base的话，我们会去呃，他要解决的问题，就是说，当我去在增加我这些应变量的这个过程当中啊，它对我们整个结果影响的这个影响是多大。

如果我们认为当你去加入新的这个影响因子，或者是我们场景的这个一些这个定义的时候，它的这个模型的精度啊，在增长的话，我们认为这种参数是我们比较关注的，反过来，如果说我们再增加一些场景或者因子的时候啊。

它整个我们叫呃，基于我们CP这个呃呃定义这个模型，这个精度的这个指标啊，如果它是下降的话，我们认为这种指标是没有用的对吧，所以敏感性分析，就是说经经过这一轮的这个敏感性分析啊。

就是我们会去得到这个比较重要的，这些这个场景的参数，为我们后面的这个仿真测试啊，做一个降维的准备，可靠性分析，可靠性分析的话就是说呃刚才我们讲的对不对，就是我们希望就是说通过对这个测试空间，这个失败率。

这个失败与我们要强调的是，所有的失败与我们都要搜寻到，通过这是搜寻到这失败与后，我们通过我们样本点的设计，来通过这个呃呃我们的结合的仿真测试，对整个系统它的这个失败概率进行一个估算。

呃我们传统的方法我们可以居，比如说我们叫做蒙特卡洛采样，对不对啊，但这种采样的话，那我介绍了他这样本点比较多，呃，我们知道对于一个一个比较成熟的系统啊，我们认为他的这个失败概率，基本上在使得一个负。

这个呃六次方或者负七次方，只是这个还要比他小，但是如果说基于蒙特卡洛采样的话，基本上我们认为它是我们叫做这个呃呃维度，这个灾难，你很难去通过这个工程上去，通过这个测试验证啊。

得到一个这样的这么大的一个采样样本点，所以反过来就是我们如何来去解决，对这么低，这么低的失败概率的，一个这个这个数值的一个评估呢，对吧，我们叫做可靠性分析算法，这里我们罗列了几种。

就是说我们现在目前这个常用的这个，可靠性分析算法，第一个我们叫方向性采样对吧，就这个在这个左上角，我们进行采样，然后可以对各个维度进行这个，失败率的一个搜索，还有一种我们叫做重要性。

自适应的这个重要性产业，就是通过我们不断的迭代，通过我们上一轮的结果来搜寻，我们下一个它可能存在存在的一个失失败率，那这样的话我们就会对对整个前空间啊，进行一个这个呃全面的全面的覆盖。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_11.png)

这里我们可以看看我们这个做了一个实验啊，结果做了个对比，就我们做了一个这个测试函数啊，呃可以看就是说我们这个测试函数的话，它的失败概率是在一个十的这个负七次方呃，如果说我们看到这个表哈。

这个表格这个表格就是我们在前面的话，当然失败概率是比较，比如说我们在大概十的三次方的话，对不对，其实基本上就是基于我们的可靠性分析算法，和这个蒙德卡拉采样，那基本上这个维度啊大概在两个维度对吧。

但是也是也是比较大，来了我们看最下面的话对吧，如果说我们这个系统啊，它这个失败概率在一个十的七次方的话，我们可以看到，那我们的这个采样方法的话，就是可靠性分析算法的话，大概就是可以5000次左右仿真。

就可以把这个概率的给给给这个估算出来。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_13.png)

但是如果基于这个蒙特卡罗这种采样方法的话，它就要达到这个6700多万次，所以通过数字对比，我们知道我们在这种情况下，我们对于这个失败概率估算，我们可以达到四个数量级的这个提升，这个是非常非常高的。

非常高的，所以这个也是我们这个可靠性分析算法，非常重要的一个特点，就是它在我们既能去估算出我们整个系统，它未来可能这个失败的概率，同时我们这个效率还是非常高，鲁棒性，鲁棒性的话就是说我们去争。

在我们可靠性这个做完以后啊，我们已经搜寻到这个失败域的这些零界面，我们通过增加这一些脑洞啊，来看它的一个一个我们叫safety emergin，那如果他的这个safety min。

如果是如果落在一个比如说这个六西格玛对吧，我们认为它鲁棒性很好的呃，或者我们这三西格玛这样来对，我们这个整个系统，在未来就是可能存在的一些脑洞啊，它的安全域呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_15.png)

在这么一个什么范围，我们进行一个这样的统计分析，那基于我们刚才讲到的，我们这个呃我们叫测试空间分析工具啊，对吧，我们的敏感性分析，可靠性分析，还有我们鲁棒性分析这个理论的这个，那建立前提下。

我们制定了我们整套的仿真工具链，首先是我们的那个安全分析工具，那个safety pro，那这个的话就是说我们它一个很重要的输出，就是说当我分析完了以后的话，我们会输出一个逻辑场景。

这个场景的话会给到我们这个云，云端的仿真平台，云端防止平台的话，我们会结合我们的这个叫测试，测试空间分析工具，这个可以理解成它会制作我们这种测试策略，对吧呃包括我们说对他这种采样啊，DOE啊。

那当我们这个进行了这个采样DO以后的话，我们会生成很多具体场景，再给到我们云端的这个仿真平台，这是我们要大算力，我个人的理解，未来我们对这个L3以上的这个自家系统，安全了，这个大规模并发的。

这个云仿真平台是必不可少的，那一定要我们有这个呃，非常强大的这个仿真测试能力，那整个仿真测试云平台的话，基于我们这个simple自研的这个仿真引擎，我们这个仿真引擎的。

我们可以达到这个最高1000HZ的仿真，同时我们的传感器的话，我们能覆盖毫米波激光camera的物理传感器，对不同的光照，包括它的一些这个呃呃噪点等等，我们都可以去做，去做这个这个仿真测试。

然后再结合我们一个自研的这个，27自由度的动力学模型，做一个闭环的这个仿真测试，我们仿真测试会有输出一个这个仿真结果，这个结果待会给到我们的这个呃，测试空间分析工具来做。

我们这个刚才提到的这个可靠性跟鲁棒性分析，最终这个结果的话对吧，我们再返回到我们的安全分析阶段，看看我们这个结果，比如说我们这系统测下来呃，在这个ODD下，我们这个潜在的风险，我们评估下。

如果它是失败概率是十的负四次方，那说明我们还我们在安全阶段，还重新进行这个呃一个系统的设计对吧，但是如果我们说仿真的结果是一个十的，这个七次方，那我们认为在这个场景下，你是足够安全的。

所以整个这套这个工具链的话，就是说哎我们都已经在云端集成，也目前的话也是在我们呃多个主机厂客户的话。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_17.png)

商业落地使用，另外就是说我们两款核心的这个呃工具，我们的安全分析工具safety pro，包括我们的仿真引擎SYMPO，已经通过了我们这个功能安全，这个AHD等级的一个认证，那为什么要做这个事情呢。

就是我们知道这个整个仿真啊对吧，包括我们在做公共安全的时候啊，他对这个你的工具链的执行度是有要求的，我们通过采用车规级最高的这个距R62，AHD等级这个要求的这个流程啊，来去呃。

对我们整个系统的可靠性啊，包括这个执行度进行一个验证，好的就是刚才就是我们讲了，我们在第一个维度对于这个连续空间下，我们如何对系统啊，进行一个这个安全的这个评估呃，接下来我们来讲。

这个对于离散空间的这个风险评估，离散的话就是相比于我们这个这个呃，连续测试空间啊，它很难去通过我们数学的方式建立一个模型，对它进行求解对吧，那更多的就是说我们不知道它的边界在哪里呃。

那我们的一个一个这个解决方案的思路呢，我们要引入这个随机交通流对吧，随机交通流，那当当时我们在做整个方案的时候，我们也在思考，就是说这个随机交流应应该如何来构建，我们是采用这个传统的基于这种规则的。

这个模型来去生成这个交通流，还是说我们要通过这种数据驱动的，那我们的这个理解是基于传统的这一些这个，交通流模型啊，它是比如说有些这个是第三方国外的对吧，就是它很很少有我们中国内地的这个。

交通的一些这个规则，所以他们那些基于传统的这些变道啊对吧，我们跟车啊这种模型啊，是很难去跟我们真实的这个交通流模型，交通绿化进行一个这样的这个逼近的，所以我们认为基于这个实应该构建。

基于我们中国实际交通流数据的AI模型对吧，构建我们宏观交通流和微观交通流，那我们的方案呢对吧，就是首先我们要去采集这一些，宏观的交通流和微观交通流，那基于这些数据训练我们这AI模型。

那再结合我们这个呃云端的这个仿真平台，做一个闭环的测试验证呃，这可以看到对吧，首先我们是要去去采集，我们这一些宏观微观的这个数据，比如我们对某些城区也好，或者高速公路也好，对不对呀。

那具体的模型我们会对这个宏观交通流，数据做预处理，包括我们的微观交通这个做进行预处理，再给到我们这个AI交通流，那我们这个ARDUO的话其实也是包含两方面，一个是我们的宏观的，一个是微观的。

就简单讲一讲宏观微观有什么区别呢，就是宏观的话就对我们整个城区不同道路的，它的一些交通流，比如说我们车流的密度，车流的速度，包括我们的一些这个流量进行一个这样的预测，未来的预测。

那微观的话就是对我们一个单车，它的在这个一个复杂的这个工况下，他的一些表现对吧，当我们前车有前车的时候，你是要去变道，还是说要去超车，还是要减速呃，是要解决这个维度的问题，当我们把我们这个交通率这个。

确定出来以后的话，我们再结合我们在云端的这个解决方案，我们叫做加上我们虚拟城市，虚拟城市的话，就是它可以理解成跟我们这种数字孪生啊，是有点类似的，我们要采集这些高精度的这些这个呃地图对吧。

那在这个地图之上，我们要去构建这一些高拟真度的这三种模型，我们知道就是说对于R3以上的这种，支架的验证啊对吧，我们刚才我们在第一阶段，我们要做的是这个在给定区间下，它可能是一些这个片段式的这种仿真呃。

但是我们说对这个城区的这个这个，自驾系统的话对吧，它除了我们在那些比较这个确定的这工况下，测试，验证完理，我们还要在这种连续的空间下进行测试验证，那我们这个交通流模型啊，就是要解决这个问题的对吧。

当我们有了这个比较一个大的这个地图，那我们这个真实道路，是这个这个这个这个这个一致的同时，我们也引入了我们的这个AI交通流模型，再结合我们云端的仿真平台，来进行一个测试验证。

这时候我们就可以接入我们大量的这个，自家的算法，进行一个闭环闭闭环的这个仿真测试验证，呃这里可以介绍一下我们做的这个，AI的宏观交流模型啊对吧，它就是结合了我们基于这个道路拓扑信息的，交通流动态。

以及这个非线性的一个这样时空，这个图神经网络，我们为什么选择这样的一个实经，这个呃这个时空的图神经网络呢，首先就是说我们这个图神经的话，就对我们这个道路拓扑啊，是比较这个吻合的对吧。

第二就是我们在这个宏观交通流上，它有这种时间跟空间的概念，那通过构建一个这样的这个模型的话，我们会对未来的一个这个时间节点，某一个路段的宏观交通路进行模拟，整个模型的话可以看到对吧。

我们就我们的输入的话，就是说我们的地图信息，包括我们这个宏观交通流数据，那我们整个主干网络的话，就是基于时空的这个图神经网络，我们就引用了四个这个主干网络，因为它每个模型缺优缺点不一样对吧。

呃我们都会同时来确定他们，也同时他们会推理，然后在这个这个这个推理阶段的话，它会输出我们这个四个模型，不同的这个对宏观交通的预测数据，那结合我们这一个stacking算法。

这个算法的话其实就是一个后融合，那最终来给出我们这个宏观加重流，对我们特定道路的一些，这个它的交通流的一些流量密度，速度的一个预测，那微观交流流的话对吧，我们刚才讲的，其实就是要。

你可以把它理解成是一个驾驶员模型，但是它是一个数据驱动的，是我们一个本地数据驱动的一个训练出来的，这个模型，整个网络的话对吧，我们可以分为几大块，就是呃第一是我们我们叫光栅化，第二就是我们这个主干网络。

那整个这个主干网络的，我们也是基于这个BV加transform这样的，一个架构的，那后面就是我们这个呃呃呃，包括我们这个编码和解码器，最后我们会有一个多模态的一个输出。

这个模型我们输入的话就是我们这个地图，包括我们这个行车的一些数据，那光栅化，就是我们就是把这个把我们这个，比如说我们地图的拓扑，包括我们车辆的在车道线上的一些朝向，它的一些这个呃跟地图的一些夹角等等。

作为这种光栅化给到我们这个模型，那再到我们这个嗯，呃encoder这个这个这个encode器里面，这里面就是我们会提取我们的一些，比较重要的特征，那我们解码器呢，解码器。

我们这边加入了对于一些这个矢量的这些数据，比如说我们车辆的类型对吧，你是卡车啊，还是轿车，包括我们这个车辆的这个数一些数据比车辆，你的你的一些行驶的一些速度啊，加速度啊等等，那最后对吧。

我们会输出一个多模态的一个这样的这个预测，就动模态怎么理解，就是我们这辆车对吧，在这个交通路口，我们是要去直行还是左转还是右转，它会都会给出我一个概率，我们会找到一个这个最最高概率的这个预测。

我们认为是它比较好的一个呃行驶轨迹，那最终的话我们就发现这个模型啊，就是我们训练完了以后，他还是对有一些工况这个推理啊，是有些误差的，我们会加入加入一个这个后优化的，一个这样的呃模块。

来对我们整个呃微观交流模型来进行一个优化，基于我们这个AI交通流的模型的这个呃设计啊，我们针对这个我们讲这个离散空间的这设置，验证啊，我们整个房子工具链我们可以看到对吧，在我们图的这个左边的话。

结合了我们这个呃，基于我们真实采集数据训练的AI交通流模型，再结合我们这个虚拟城市对吧，虚拟城市我们对特定城区的一个测试验证，来给到把这个海量的交通流数据啊。

输入到我们的simple的一个这样的这个仿真节点，这个节点的话对吧，那我们重点强调一下，就是我们知道当你数量级很大的时候对吧，你这个你这个几千个几万个数据的时候，你用单引擎的话。

包括你的这个对于传感器的这个设置，这个计算你的性能是不够的，那我们采用了一种分布式的仿真，分布式绑定，我们把我们在传统意义上，传感器在我们仿真引擎内部，我们把它移到我们的节点上面对吧，呃在每个节点上面。

我们去计算我们主车周边的一些感知的信息，那再结合我们的评估，这样我们就可以达到，就是我们通过一个仿真引擎的节点，然后再把我们计算量比较大的，就是分分解到我们在并行仿真当中，每一个容器当中它的这个计算量。

这样我们会做到这个实时的仿真，同时的话对吧，我们可以去做呃，这个多任务的一个这样的这个并发，我们每一个任务的话，我们可以，目前的话我们的性能可以做到五千五千，5000加的交通车，加100的主车。

可能大家觉得哎，我怎么你们的性能交通流只能支持5000，对不对，有些传统的这个基于规则的话，它可能几万甚至10万的就这样数据，但是我们要强调的是，通过这种AI训练的这个模型的话，它在推理阶段还是有很多。

这种需要优化的地方呢，这也是我们目前重点在推进的，包括我们在这里面做了很多种，大量的这种并行计算的优化，那最终我们我们整个这个自驾平台，这个自驾这个系统安装验证的话，我们会做到实时的这个分析。

可以去提取这些关键的NG场景，这里有张图，可能有这个字，有有有有点小，就是我们把我们在线目录在云端部署的，这个结合我们AI交通流和虚拟城市啊，还有我们整个云仿真的一个并行的平台的，数据展示在里头。

我们就可以做到非常好的，这个连续的一些城区的这个测试验证，这对于我们在俄罗三以上的这个城区的验证，是非常重要的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/8d516e795b509ce46151e1683580a2ac_19.png)

那总结一下，就是说我们就是对针对这个整个额预计公安全，提出的，对于这个未知的这个呃风位置的，危险场景的测试验证，我们通过对空间的划分来去进行这个测试验证，好的。



# 2024北京智源大会-智能驾驶 - P8：智能网联汽车深度学习算法：万少华 - 智源社区 - BV1Ww4m1a7gr

大家下午好啊，感谢法王主任的这一个邀请，感谢组委会，使得我们有这样的一个嗯交流学习的机会，向更多的这个企业界的朋友能够呃学习，以及能够产生更多的这一个碰撞的这个机，这个机会就产生一些一些火花。

然后我是来自电子科技大学的呃，万少华，然后先介绍下我们这一个高等研究院，我们是在这一个深圳也实验这一个呃，深圳的市的20+8就是低空经济，人工智能，还有这个立体化的发展而应运而生。

那么我们是对一个就是相当于电子科技大学的，二级学院，我们数据智能研究中心目前是从日本回来的啊，这个任福济工程院的院院士，然后是呃这一个带领我们，我们一共有这一个八个正高级的教授，然后六个国家级的人才。

然后有八名博士后，30名博士和130名的硕士，然后我们主要研究啊机器学习，人工智能，然后相位仍然是是做这一个数字人机器人，然后情感计算，我们也就希望这一个既做这一个理论的，这个研究。

也做这一个啊产学研啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_1.png)

好然后对一个跟大家汇报的这个提纲，就是包括这个研究背景，然后研究的现状和这一个挑战，然后我们的呃一些实践就是科学研究。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_3.png)

然后最后是一些应用展示和总结，然后这是这一个人工智能驱动车联网，通感算是实现智能驾驶等第一个根本保障，那么这个随着这个通感算技术的发展，那么这一个无线通信和网络，向更高的移动性。

和这一个更复杂的这一个场景去延伸，那么这一个像云计算和人工智能，使得这一个呃哦就是智能网联汽车的计算，就是走出这一个智能的座舱，然后这一个嗯像这一个就使得这一个啊提高，提供了更高的就是高可靠性。

这一个就是大带宽对一个低延迟的传输的通信，然后向B软计算呃，就是使得这一个降低了这个延迟，然后这一个使得我们的这个服务体验，能够做到这一个强实时，然后这最右边的是这一个车路协同。

我们我们最近也在讲这一个大模型，那么大模型呢车轮系，车漏轮协同的这一个部署是这一个必然的趋势，它能够就是说在这一个啊，这一个这一个就是把将预训练好的这些模型，卸载到这个冰原那里，降低这个延迟。

然后呢可以能够提供一些啊这一个定制化的，这种场景的微调的定制化的服务，和这一个个性化的这一个服务，以及和这个不需要把这些数据上传到这个云端，能够能够对一个降嗯，降低这个隐私泄露的风险。

那么这一个就是呃智能驾驶是一个必然的趋势，那么我们看看这一个人工智能的，第一代和第二代的人工智能，它都是从不同的侧面表征了这一个人类的智能，然后又有各自具有不同的这一个优势，也有不也有自己的一些局限性。

那么这一个单靠某一种理论，某一种这个无法实现真正的拟人的智能，我们需要结合这一个呃理论，建立新的可解释的，鲁邦的这个AI的理论和方法，我们下面一个我刚才注意到，我们下面一个要讲的。

刚好是这一个这一个数据驱动，加这个知识驱动的这个相结合。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_5.png)

那么这也看看这一个解决这个民生问题，就是交通的安全，我看这一个每年的就有交通的事故，以及交通的这个拥堵呃，我们我最近也也常也注意到，就是像这一个特斯拉，它把这一个就是白色的汽车，是就是判断为这一个啊。

就是蔚蓝的这个天空，所以导致这一个一些事故，还有这个骨谷歌的这一个啊这一个呃事故，那么这一个我们就是说更加可靠，更加安全，更加节能和更加这一个舒适的这一个智能驾驶，是这一个呃一个必然的趋势。

然后这一个啊全球也在积极的推荐，那么自动驾驶系统的核心和技术呢，已成为全球的这一个战略的制高点，像中国制造，2025和2035的这个远景规划，也也确定了，这一个就是呃，就是智能驾驶。

为这一个智能制造的核心的战略的内容，像这一个我最近也刚刚注意到，就是今年6月份，也这一个呃，就是说智能网联汽车的准入和这一个道路管理，规范的这一个试运行，也这个通知也刚刚发布。

然后好像这一个由我们在座的确定了，九个联合体，包括这个比亚迪这个还有这一个啊，这一个像这个长安呃，广汽还有这这这这些啊联合体，那么这一个呃，就是加加速推动这个智能网联汽车的这一个，高质量的发展。

然后像美国向欧盟和日本，都也在积极的这一个推推进，然后嗯。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_7.png)

我们下面是我们面临的一些研究的现状和挑战，那么百度它这个我们知道，它专注这一个路测的这个感知，用这一个啊，感知是这一个提供的信息的这个基础，我们要这个希望能够做到这一个全方位多角度。

多视角和这个多模态的这这种感知，像这个中国联通，它是基于这一个比这个移动变软计算是吧，这是我新用新的这个架构，像华为是基于这个veto x。

就是啊就vehicle to inin infrastructure，就是这包括这一个路人网络，还有这个这个路测的这个这个基础设施的，这一个啊这一个一种新的部署，实现了这一个多场景下的这一个呃应用。

然后那么车路协同它的这一个实现呢，需要分阶段的推进，它不可能这一个一蹴而就，其中感知任务，在这一个车轮系统中具有这一个重要的地位，因为它是提供这一个啊，一个一个信息的一个一个基础，就是协同感知。

车漏的协同感知，那么下面是我们我这见证我们学术界的，这一个就是关于这个系一些车路协同啊，协同感知啊，这一个一些research的一些研究，好看我看一下这一个就是单车智能。

它面临的这个安全性和运行设计与以及经济性，以及成本的方面的这个挑战和问题，就是首先说这安全性，因为这一个啊单车智能，它在一些比较特定的场景下，它有这一个，就是就是这这个这个辅助的驾驶系统。

它有这一个啊应对不足和失效的这个风险，就是安全性有待提升，然后这一个呃车辆锐写设计域，就是说将这一个在一些，就是我们刚才有同事也提到，就是呃雨天雾天，雪天的这个恶劣的天气的场景下的长尾场景。

鬼探头的这种现象，也这一个呃也是感知能力，有些人人是几代，这个提升，最后一个就是为了这个做到这个全方位，就是啊这一个多视角，这一个多模态的融合的感知，我们需要部署这一个更多的这个传感器。

和这一个嗯呃就是呃高高性能的这个通信设备，来增加这一个呃增加这个感知的性能，这样就自然导致这个呃，在车单的这个成本的增加，然后我们看一看这一个单车智能和车路协同，那么单车智能就是说在这个车辆本身。

用这个车自动的感知规划决策和控制，执行的这一个全过程，搭载这一个神经神经网络，进行一个自动驾驶的算法，然后这个车漏系统，它就是这一个实现车落影一体化，这一个啊就是实现感知决策的一体化。

然后充分发挥道路目测这个云和云计算，和这一个这一个的一个协同的配合，那么这一个关于这一个啊，单车智能和车路协同呢，它并不是一个就是完全的这一个非合，就彼此对立的二元的对立的，他们都有彼此的优势。

也彼此的这一个不也不足之之处，然后这一个他们是这两种技术的这一个，这种一一种融融合，应该是一个未来的一个趋势，像这一个啊车路协同呢，它呢是能够这一个实现啊，就是自动驾驶的一个上上限。

而这一个呃在自车轮系统中，仍然也需要这一个单车的智能。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_9.png)

然后这是我们就是啊研究的一些准备的，就是呃研讨的一个科学的问题，就是这一个啊高移动性，高动态性的这脱高动拖动态的拓扑，然后这一个呃车单的这一个，而是异构大海量的数据。

那么我需要适应这个通信计算的新型的一个，整体的一个体系架构，然后这一个我们需要这一个多模态，就是多视角，然后这一个准确的这个精准的这个这个感知，然后这一个嗯复杂的这一个是呃，就是深度学习模型的这一个。

它的一个可可可可解释性，比如说它的这个轻量化，它的这一个啊模型的分割模，还有模型的这个压缩这些知识，争六的这技术研究，第最后一个就是啊面向数据时效和缓存卸载，因为我们这要是要对一个低延迟，高可靠强实时。

我们希望把这一个就是一些一些服务，一些比如说一些这一个顺利好的，日顺利的模型能够卸载到这一个这一个路测，或者这一个啊比软的服务器来降低这个延迟。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_11.png)

然后我们下面就是我们的一些呃，一些研究的一些工作嗯，第一个工作就是关于感知的，就不良天气条件下的感知技术，就是基于多模态融合的这个未知天气条件下的，单到端的一个自动驾驶，那么我们针对什么问题呢。

就是主要是针对这一个呃，不良天气条件下的多元异构数据，难以这一个融合，然后这一个啊多阶段的这种自动驾驶，它有存在这些误差累计，那么我们我们提出了这一个单独单的，这个这个多模态的这个增强的。

这个是新的这一个架构，我们在这个最右侧可以看到它有这一个两个，就是一个输入，一个是这一个啊，这一个二维的这一个RGB的图像，一个BEV的这个这个图像的一个一个融合，然后我们我们呢用了这个这个。

然后经过这一个呃底人模式，通过灵活映射就是弹性解耦，这是我们提出两个方法，还多特的注意力机制，以及CN来融合多模态的数据，来获获得一个更加可靠的，一个驾驶的环境的感知，然后在这个右侧还有两个两个输入。

就是一个是这一个路点和速度，速度就是为了这一个感知这一个车辆的实际的，这个是这个速度的控制，车辆的这个这个这个轨轨迹嗯，以实现这个高阶段的导航导航引导，那么对于路点的路点的输入的话，我们我们这一个啊。

就是说啊，输入的它是一个一个一个这个控制的命令是吧，它输出的这个最后是一个，来对车辆进行一个控制，那么实现了这一个不同不呃，天气条件下的这一个各种的这里一个啊，这一个自动的一个驾驶。

然后这然后这个就是啊可以看到，就是说我们这里就是刚才提到的这个多，这一个啊灵活的这一个映射，和这一个就是这个弹性的解耦，就是来来这一个提高这种融合的这个，鲁棒性的表征。

就是避免这个在不良天气条件下的这个性能的，这个这个下降或者特征的这个丢失，来来来提高这种这种鲁棒性，然后这一个经过这一个就是联合映射这一，然后弹性结构还有一个这一个多层的规化机制，以及多层的注意力机制。

然后最后这一个形成一个灵活这个特征向量，分两部分向这一个呃，这个反馈就是RGB的feature，和这一个BEV的一个这个这个feature，然后第一嗯，我这是我们在这一个仿真系到。

担当当的自动驾驶性能的一个算法，的一个进行了评估，来这一个啊，就是在复变化的各种变化的场景下，来对模型进行一个验证呃，然后这是我们我们做的这一个哦，Sorry，这是我们做了这一个这一个实验。

就是我们这个baseline显示显示的是一个transfer，然后它也是两种传感器模式，就是一个是激光的这个底轮，和这个RGB的图像跟我们是非常的，这个方法是非常类似，我们有这您提到了三个性能的指标。

一个就是这一个driver school，就DS就是这一个啊嗯就是用来这一个呃，就是嗯后呃就是用了这个后面的就是root completion，就是呃完成了这一个距离和这一个呃完整距离。

这种级数和后面的这个每公里的违章数，的这一个一个几何的一个平均，然后这个RC就是这一个我们用自动驾驶的，这个这个完成了这个路线的距离，和这个总的这个路线的距离的一个百分比。

然后这一个IS就是每公里的这个违章违章数，比如说闯红灯，比如说这一个好人的车展的这个刮蹭等等等等，就是来来来做一个一个就是来做一个验证，再分别在这一个不同的这个区域，就是汤01~05的这个进行一个验证。

我们可以看到这个，就是我们的这个前面的这个数据，都有些性能的优势，这个细节我就不展开啊，然后第二个感知技术就是技术互学系列的语，无条件下就是技术知识的这一个呃，不良天气条件下的这个交通目标。

的一个强化识别方法，然后这一个又如在恶劣的天气条件下，这一个呃就是物体的检测不可避免，尤其是呃就是嗯就很多的研究，它集中在这个两个方向，一个就是啊就是区域的检测和这一个叫什么。

和这一个嗯和和这一个语义的风格，但是他们没有考虑到这两个任务之间的一个，相互的作用，我们提出了这一个就是CEMGN，就是互学系的这个直线度增强互图网络呃，来来来来，这一个就是相互的这个激励。

就是呃能够提高各自的任务的一个鲁棒性，然后我们的这一个可以看到就是在这个双任务，就是呃这个如意分割和加和这个边界的区域，这个检测，然后来这一个完成的一个直线度增强模块，它将特征图转化为图特征任务呃。

转换这个图空间，来提高这一个各自任务的这个这个鲁棒性，然后降低了欧几的已的空间的，这一个就是特征的一个一个一个损失，然后我们这个使用INTEGRAPH来推理了，估计任务之间的一个差异。

使用INTEGRAPH来推理来提取这个模块的高级特征，最后我们做了这一个，在city scape就开展数据集和这个forty six cape，数据集上来这一个做这个呃，现代的这个验证。

达到了大概IOU的交并比80%左右，而现在呢在这个物体的条件下，是它的扰动是低于1%，然后这大家可看到就是嗯，呃这是就是我们做的这个呃。

就是刚才提到的一个city cape和forty focus citiescape，的两个数据集上的一个呃一个印证，我们可以看到这个就是黑体字的，就是一个就是平均的这个呃准确率。

最后一个最后右边的就是这一个啊交并比，平均的一个交并比我们是都是占有这个优势的，下面呢就是我们这一个可在这一个city focugae，的数据的可视化效果就是有误的情况下，就是这个衰减系数。

然后我们看一看我们的这个性能的这个波动情，况呃，呃第四，第三个工作就是关于这个复杂城市，这种环境下来，就是交通因素，比如说车辆的一个这个识别，然后这个地方我们根据这一个城市的复杂环境。

就是有些比较这一个复杂，其实也我们这一个也是改进了这个，YOO这个模块，然后加入了这个通道的注意力机制，然后做这个实验，从右图可以看到，我们我们引入了这个就是high resolution。

这一个模块来代替这一个呃，这一个呃呃来加入到这个YOOYYELLOW中，然后我们可以看到低分辨率的这个网络特征，和高分辨率网络特征，它是并行的，这一个来来这一个并行的连接，来降低这个低分辨率网络特征的。

这一个特征的这个这个这个这个这个丢失，然后嗯我们的这个也是做了这个实验，这个呃就不不展开啊，然后这一个就是我们在这一个呃，city cape数据集，和我们自制的数据集上，做了一个性能的一个对比啊。

对比的就两个，一个一个是啊这一个误误解率，误诊率和一个是这一个和这一个漏漏检率啊，而第四个工作就是我们的一个关于交通流量的，一个一个一个一个预测，我们看了就是针对的问题，就是单一的深度学习方法。

面临着一个过拟合的风险，我们使用动态的权值以后呢，这个两种这个模型来提高模型的这个，预测精度和和这个发货能力，其中就是用到了这一个呃，这一个RSTM模型，和这一个SAE的这个这个，模型的这个这一个联合。

然后我们可以看到就是这一个做一个这一个呃，一个实验的验证，我们这边这个可以就可以知道就是MS1，它是一个越小，就表示我们的这个预测值，和真实值之间的差距是越小，所以可以看到我们的这个数据是最小的啊。

然后就是下面就是这一个啊，云边端协同的这个训练的这一个呃关键的技术，我们第一个提出来就是呃，呃就是联邦学习，就是带来的这一个高通信的开销，为高隐私数据带来一个巨大挑战。

主要就是为了这个压缩这个呃这个通信量，就是这一个呃，降低这一个就是呃模型更新的频率和这个size，来这个压缩这个通信的成本嗯，特别是跨中性的这个模型的这个，这个训练的时候，那我们可以看到联邦学习的。

他的这一个一轮的一个通信的轮次，包括了一个啊，这就是下行和上行这个完整的一个轮次，还有这个这个聚合，那么下行下行的话，就把把这个参数服务器的这个模型参数下，卸载到这一个就是啊车车单。

然后上行就是把这一个呃，这一个车单的这一个这个这个模型参数，上传到这一个啊参数服务器，这是个这是决定了通信的这个成本的这一个呃，这个大小，然后聚合的就就要等待所有的这一个车单，要需要等等。

等待所有的车单的这个参数的的这一个啊，这个这个这个参数的传传输完，才能进行进行进行融合，我们提出了这一个呃车载编码联播学习，然后在这个地方我们用到的几呃几个策略，就是本本地顺利的策略减少就是通信人次。

部分客户端参与规则，月输上传时间和一个聚合策略，最后我们我们验证了这一个就是啊，这个通信的这个成本，然后以及收敛的这一个啊收敛的这个快，这个这个速度快慢呃，第二个就是我们进入。

这也是计入增量训练的DNN的一个计算的，卸载，也是为了这一个呃，就是能够顺利的这个这个减少，这个顺顺利通信的这个这个这个成本提高，这就快速的梳理，然后这个地方我们提到的增增量顺利。

就是它有一个卸载的算法，的一个灾难性遗忘问题，那么模型需要重新顺利的提高这一个准确性，然后我们这里就提到一个一种增量，顺应的方法啊，啊这个就是我们在这个几个对比算法，就是关于这个random就是随机的。

就是随机的呃，卸载和这一个local的，就就是直接在这一个车单进行这一个计算，然后这个呃GRADY就是TANA的算法，就是说啊它是一个最好的输出，但是它有一个问题，就是它的这个执行时间非常的长。

然后我们这一个就像这个DDDTO，它是一个一个baseline，然后这一个我们在这个进行一个，优灵活的一个优化，既考虑到这一个啊这个延迟，也考虑到这个能能量，然后当这个贝塔等于零的时候。

那就是那那就是对这一个，对于这一个延迟的一个优化，当贝塔等于一的时候，就是对这一个哦能量的一个一个一个优化，第三个就是我们提出一个多尺度压缩的一个，DN推理的加速，因为这个冰软环节的动态线。

就是车辆的动态线和这个终端设备的多压线，对第一个模型的这个划分，提出了一个重大的挑战，我们希望将这个问题建模为一个混合整数，零零和优化的问题，灵活优化，这个有几个就是模型的选择，包括任务是延迟性的延迟。

敏感型的任务还是这一个呃，这就是计算计算比较重的这个任务，以及模型的分割点，以及以及带宽资源的这些资源的分配，来根据任务属性最大化的这一个推理，准确性和延迟之间的一个一个传一个一个权衡。

我们可以看到就是说呃有我们有这个在云端里，有个有一个cloud，有一个offline，一个离心的训练，还有在这个edge当的这个online这个顺序，然后edge n的这个进行这一个，好。

然后这个就是我们可以从这一个左图可以看出，就是不同的准确性和计算复杂度的，这个DNN的一个就是，就是深度神经网络的一个不同的版本，就是版本的选择，然后这个右边的这个123就是嗯。

就是我们要需要做出的一个具体的一些决策，比如说这一个版本的选择，这一个就是这一个呃呃资源的分配啊，然后然后456就是说在这些根据这些决策的，一个执行的一个一个结果，然后我们把它优化的。

第一个就是PE的这样一个问题，然后受到一些第一些约约束，如我们优化的问题，目标是考虑所有的这一个，它的这个服务员服务的请求，比如说我们刚才提到的，这是计算量重还是这一个延迟敏感型的，还是这个模型的选择。

还是这一个啊资源的分配等等，就是说做一个动态的一个灵活的一个优化。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_13.png)

还有带宽啊，然后这个实验就不说了啊，然后最后就是我们我们提出了这一个，就是一个模型的分割，因为在资源的这一个受限，我们讲大模型在这这个车单比软单，它的这个资源不管是计算还是存储都是受限的。

在这方面是部署大模型是吧，它这个密计算密集的这个DN是非常的困难，是那么我们对一个呃模型的划分已经卸载，受到广泛的关注，但是它并没，但是很多的研究，他没有考虑到这一个最优分割点。

和这个计算资源的分配而变化的这个问题，我们针对这个问题吧，考虑到第N模型的最优分割点，随资源计算资源的分配，而这一个变化的问题，我们提出第一个模型的划分的计算器载策略，然后针对这一个就是嗯这一个交。

针对这一个就是说啊，呃就是这个加这个迭代次数和这一个呃，就是这一个惯性权重，作为这个反馈参数，来改进这个离子粒子醇的这个算法，然后实验表明我们的这个和机器相比，我们的这个呃这个整体的延迟都有提高啊。

然后这这个是模型压缩，就是轻量化技术，轻量化技术是这个大模型的，这个我觉得这个呃，这一个它的这个落地的一个知识点啊，然后我们提出来，这一个就是改进了这个云宾纳的一个一种训，分布式训练架构。

然后我们改进了yo模型，就是用这个啊这一个dense block that residuo mock，Residuo block，然后在这个dance block之间加了这个两个。

这个这个MARKEPLIN就是池化层，然后呢这一个减减少特征的这个丢失，然后我们就将这个我们视频的分析任务来，来这一个啊转移到这一个B软端OK，然后我们可以通过这个实验结果可以看到。

就是说然后这个是IKOS，然后面是4spec speed，我们可以发现经过这个模型压缩后的这一个啊，这一个它的它这个准确率，它的准确率是这一个有所下降，但是他的这个就是它的这个呃，就是检测的这个速度。

它是它大家看到这个4speed，是有非常好的提高的啊，然后我们最后的就是呃深度学，强化学的计算卸载，这个我又是考虑到这一个呃，就是实定拓扑，高高高高动态性的，这车辆和这个卸载任务的这个数据依赖性。

它的高效率卸载带来一些巨大的挑战，我们构建了这个车流系统计算卸载模型，然后提出这一个考虑这个响应时间，和这个能耗的这个优化问题呢，提出这种移动移动感知和相关任务的卸载方案。

就进入深度度强化学习的这种这种需卸载的，这种这种策略呃，不展开说了啊，这个，然后第二个就是车轮系统计算，卸载的这个关键技术之二，就是深度强化学习，又是这个车辆的高移动性，和和这个可资源的可用性，可用性。

我们仍然缺乏，能够支持低延迟和这个这一个高可靠性的，这一个intelligent的，就是这个智能，那个网联系列的网络服务，的一个简单的解决方案，我们我们提出的这个就是呃，任务卸载和最佳任务卸载的方案。

表述为一个受约束的马尔科夫的这个决策过程，因为深度强化学习，它能够就能够对一个解决，就是感知决策顺序的这种问题，所以我们把它最后这一个呃，就是提出这种最优最优质任务的卸载策略，发现我们的实验就是说。

能够对延迟有26%的改进，然后这一个整体的这个可用性资源可提高，可以提高到42%啊，然后最后一个就是呃，基于计算卸载的这个资源分配，我们由于这个车内网的车道网，网络的这个资源的负载的不均衡。

然后自然又受限动态资源的需求，我们提出了这一个提出来，这一个就是多目标的这个这个资源分配，我们把它优化为这一个一个多目标的优化问题，然后我们开发一种这个非支配排序的遗传算法，的。

这个这个这种来解决这个多目标的资源分配的，优化方案不展开了，然后这一个呃可能是没时间是吧，还有5分钟是吧，呃可以这一个嗯就是看看可不可以放一下这些。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_15.png)

就是我们对于啊这个感知常见的这一个，做了一个一些一些一些测试啊，感知场景的一些一些嗯，然后这是在这个呃校园的，做了一个实车成果的一个一个一个一个展示，然后呃啊最后就是总结啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_17.png)

就是我们面向智能驾驶，驾驶这个现实场景的需求，我们啊就是对复杂场景天气条件不良，天气条件就是雪天雨雾天，多模态的融合感知，车路云协同训练的高效，就是高效训练和这个推理加速以及计算技术。

深度强化学习的计算，卸载这三个方面的关键技技术展开的这个研究，然后未来的智能驾驶是吧，能够希望能够这个多模态的融合，就是全视角，然后这个多多模态这一个还有这一个呃，多智能体的这个融合，提高这种更加精准。

更加的这更加精准的这种感知能力，然后车路协协同来进一步，就是大小模型的这个协同，就是然后包括大小模型这一个相互的一个一个，一个一个知识的这一个就是一个嗯协同吧，最后就是呃。

就是希望能够就是接受结合并软计算，能够能够就更加轻量级的实时的模型，能够这个这个部署和落地，最后我们就是啊，是在这一个我们的国家自然科学基金，和这个深圳市重点基金的，这个对本研究的支持哦。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/6e5ef5054132c1319581418346cd8663_19.png)

谢谢各位。

# 2024北京智源大会-智能驾驶 - P9：从数据驱动到知识驱动——自动驾驶新路径：石博天 - 智源社区 - BV1Ww4m1a7gr

啊好的呃，感谢刘主任，然后也感谢之前各位前辈带来的精彩的演讲，其实说我压轴绝着实是不敢当，因为我其实21年刚刚博士毕业，然后就加入到了上海人工智能实验室，然后其实我们本身是一个科研机构嘛。

所以我们研究的一些内容可能会偏发散，偏一些，不同于大家常见的一些技术路线，那我们可能会愿意做一些这种，初步的一些探索啊，那所以啊接下来就我来介绍啊，很荣幸介绍一下我们团队的一些研究工作啊。

那今天的主要的这个话题叫做动漫，台大模型和知识驱动的自动驾驶，其实这个故事早在38年前就已经展开了，其实自动驾驶呢在呃就大概快40年前，在86年的时候，这个CMU的NEVLAB。

他们其实推出了一款原型的系统，已经能够大概去实现一个由机器代替人类，驾驶员去开车的这样一套系统，那后来到了这个95年左右，那他们的第五代系统成功实现了一个壮举，就是完全由自动驾驶的系统操纵。

然后横跨了美国，但是呢其实整个的过程中，差不多有98%左右的这个路线，是由啊算法来去接管的，那当然其中有2%左右的场景，还是由人类的驾驶员来完成的，但其实直到了这个30年之后的今天，其实最后的这2%。

可能到现在也没有真正得到这个完全的解决啊，所以其实可能是呃我觉得一个原因吧，可能现在数据驱动的方法，也逐渐的去达到一个瓶颈，就是在预期大家的这个L1，L2这样的一个迭代的过程。

可能L1的阶段我通过啊加更多的传感器，然后达到了L2，我再通过增加更多的数据，然后实现L3，然后让它适应更多的场景，然后来达到这个L4，甚至L5的级别，但实际发生的情况呢，就是我在L2的阶段。

我可以通过增加更多的数据，但仿佛有一面墙一样，它能总是实现到这个L2。999，总感觉像是离L3啊，就是存在这样的一个瓶颈，那究其原因呢，其实之前很多前辈也都介绍了，那主要的一个原因。

就是因为呃存在各种各样的counter case，那这些corner case呢，它不但是一些我们在路上罕见的，甚至还有很多，是我们可能一辈子都不会见过一次的数据啊，比如说像像以下展示的这些。

其实甚至都是真实的路上发生的情况，所以我们啊作为一个研究的这个团队，所以我们在探索，是不是有一条新的技术路线，能够去缓解这个问题啊，所以我们在探索呃，是不是通过这种技术路线。

能够实现L4再到L5的这样的一个进化啊，所以我们呃主要是从这个，人类学习驾驶的一个角度出发，那我们认为可能如果这样，如果真的有这样的一条新路线，它一定具备这样的几个特征，就是泛化性。

自监督和持续学习的能力，那围绕着这几个观点，我们认为现在的这个巨神，智能技术和自动驾驶相结合，最终实现一个知识驱动自动驾驶，可能是我们为这个路线起的一个名字吧，对其实呃。

最开始其实呃好像铁军教授也介绍到了，这个人类学习开车的一个过程，其实这句话呢是这个LEQUEEN在20年左右，在他的一篇啊文章中提出的，说，为什么一个青少年呢，他只需要大概摸20个小时的方向盘。

就可以学会开车这件事，而且呢对于大部分人来说，他在很多这辈子从来没有遇到过的场景，他在第一次遇到的时候，也能有一定的能力去解决这个场景啊，所以其实这是一个很有意思的一个问题，那我们认为可能呃。

之前的很多方法可能遇到的这个困境，就是这种数据驱动它泛化性的一个难题，那对于知识呃数据驱动的方法来说，它通常是我在固定的一些场景上训练，我规定好了它的输入输出形式，那这个任务的定义就限制了这个啊。

整个的这个这个他的这个能力的上限，比如说如果是传统的感知模型，我在目标检测的阶段，我一般应该不会定义出一个目标检测模型，检测路上是不是有架飞机啊，但是呢知识驱动的方法就不太一样了。

因为它通常是用这种跨域的知识的能力，比如说现在这种多模态的大模型啊，或者是一些这种预训练的技术，它首先啊具备对某些通用场景的，这个通用的物体的理解能力，那并且这个能力可以通过比较低的成本。

迁移到一个真实的环境中，所以可能就能够完成一些之前数据驱动，很难想象的事情，那这个是我今天早上去这个用GPT4，试了试啊，就刚才的那张图片，我直接问这个GPT4，说。

描述一下这个图片到底发生在什么场景场景里，又有什么异常，如何避免出现危险，我应该怎么做，其实能看到这种结合了open doomman的这种知识的，经过预训练的这样的一个通用的视觉模型。

其实确实对这种特殊出现的场景，还是具备一定的理呃这个理解能力的，那所以其实我们呃如何去构建这样的一个知识，驱动自动驾驶呢，我们更多的是从一个呃呃这种具身智能的视角，来看待自动驾驶这件事情。

那所谓的均衡智能啊，其实很关键的就是两点了，一个是环境，一个智能体，一个智能体呢它在环境中去实现探索呃，呃然后呢去把这个呃从环境中去进行感知，然后呢，智能体会自主的在这个环境中去进行探索。

然后整个的过程呢其实很重要的一点，就是能够在一个闭环的环境下去完成的，它能够基于自己的好奇心，或者基于自己的这个知识积累的能力，能够在里面不断地运转，那所以其实我们团队的研究。

也基本上围绕着环境和智能体，这两个方面去展开的，那首先对于环境来说，我们现在想训练这种自动驾驶的算法，那最最最好用的环境就是真实的世界嘛，那所以呃在真实的世界中直接训练，要么不闭环，要么就不安全。

不闭环，就是我可以去预先采集好很多的数据，那这种方式它是一种开环的训练，或者测试的方式，它不一定能真实反映，你这个模型在现实的世界中，它的运转的这个效果啊，那要么就不安全，这个肯定的。

我一个没有任何训练经验的自动驾驶算法，扔到这个城市中，让他自己去积累驾驶经验，其实这是个非常危险的事情，那所以我们一直在去研究啊，有没有可能去构建一个这种虚拟的环境，构建一个对真实世界近似的刻画。

因为这种刻画的真实程度，其实决定了这个agent，他脑中形成的世界观的这样的一个知，认知的上限，那围绕着这块呢，我们又有两条不同的技术路线的探索，就是围绕这个虚拟的环境，那第一条技术路线呢。

就是我们基于神经渲染和结晶仿真啊，我了解到可能呃这个据我自己的认知吧，可能我们团队算是非常早期去从事这方面的，research的研究，就在这个NERF的，甚至在NERF刚出来的时候。

我们其实就有一些相关的布局和探索，那整个这套技术路线它大概就是三个部分，一个是重建，然后泛化，然后再生成所谓的重呃，重建呢就是我用真实世界的数据出发，我去对它，去用神经渲染的这个技术去做三维重建。

然后呢我能够把它前后景截，我开，我对里面的一些交通流，我可以利用一些交通流的生成工具，我可以编，让他创造出一些真实世界中不存在的交通流，但是呢这个交通流它也很逼真，它可能是一些counter case。

但可能更像是这种人开出来的corner case，然后我们再把它去用神经渲染的技术去渲染，生成出来啊，那这里其实展示了一些，我们这个中间的一些阶段性的成果嘛。

然后我们其实提出了一套叫做NEUROSEM的框，开源框架啊，目前也是在GITHUB上是开源的，它是一个前后景解耦的，其实这个里highlight一下就是很多呃，这个友商的这个算法其实很多。

它还是针对背景为主，对于前景甚至动态物体，其实都不是能非常好的处理，因为我们其实采用了一套不是NERF的技术，我们是用这种三呃这个SDF的表征，它的一个特色呢。

就是说NERF可能能做出这种视觉的三维重建，然后呢我渲染的相机可以比较真实，但是因为NERF并不并不能真实的去建模出，一个物体，它的表面，那我们现在有一套新的技术，然后来实现呃，那个它的表面的重建。

能够同时对这种动态，静态前后景的物体实现解耦和三维重建，那并且因为我们有这种表面渲染嘛，所以我们可以很容易的去把啊，用各种各样的传感器去进行仿真啊，比如说NERF可能仿个图像，仿个相机啊。

这个是可能是他的一个能力的上限了，但是我们还可以去仿出这种不同型号的激光，雷达等等这样一些线呃，这个功能，然后这块呢是我们的神经渲染的一块呃，就重建和神经渲染的研究。

那第二块呢我们也做了一个叫做这个LIMSIM的，一个开源的高一致性的交通流仿真工具，它也是一种从数据驱动的角度出发吧，然后从能够从真实数据中去学习到很多，不同驾驶风格的一些交通流仿真。

然后呢它能也能支持这个多车复杂的博弈，那最终呢我们将刚才的这个neuron sim，和这个lime sim两个平台相结合，我们就构造了我们一套基于神经渲染的，端到端的仿真引擎，叫做OESIM啊。

然后呢它其实能够使呃在右下角，其实我们展做了一个非常简单的界面啊，因为我们是一个研究机构，其实没有什么工程师，这个基本都是一些学生，大家自己写的一些简单的界面，然后我们也很少有这种非常真实的数据嘛。

所以我们从cola中仿真去get到了一份数据，然后我们从这份数据出发，我们可以去呃通过编辑出一些不同的交通流，并且让它仿真生成出来，而且这里展示的其实全是神经渲染重建和渲呃，这个泛化生成的结果对。

然后刚才介绍的呢，是这个是基于神经渲染的技术路线，它其实是一个相对比较长的技术路线，我先重建在泛化，在生成，其实这个技术路线，我们在探索的过程中发现有很多问题，比如说重建对于数据质量要求非常高。

那可能对于很多量产的实际的情况，你得到这个数据，可能能够用来重建的数据就非常少，然后再加上整个的这个链路太长了，然后呢我们所以也同时在探索第二条技术路线，就是我们有没有可能用啊生成模型的技术。

来实现这个闭环的仿真，本质上呢其实非常简单的一个架构图，就是我们可以用一款可控的生成模型，这个所谓可控生成模型就是我给定一个layout，比如说这个路网的结构加上自车踏车的结构啊。

然后呢我再结合这个生成模型呃，就把它作为输入，然后这个生成模型就可以给我生成一张图片，那同时呢我们再配合上我们刚才提出的这个，LIMSIM的这个闭环控制的算法，把这两个相结合。

我们就能够形成一个纯的纯粹的，基于这个生成模型的一个闭环仿真的引擎。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/0599f9778f66612ea3a7da59856af720_1.png)

那这个可控的生成模型，其实不限制它到底用哪个，包括像现在比较热点这个magic drive啊，还有这个这个帕纳西亚等等，各各种就是研究上比较成功的一些，基于layout可控路网生成的这样的一些工具。

那围绕着这些工具呢，我们其实做了一些简单的尝试啊，比如说这个是一个连续帧的生成，那本身这个模型其实不是一个，基于视频生成的模型，还是基于单帧图像生成的模型，但是我们所给出的路网。

其实是由我们的仿真器仿出来的，像之前很多的这种生成的算法，然后他们更多的是基于一个ground truth的，这个路网路径，然后我去重新的再把它生成一遍，但是现在我们可以去编辑，生成出一些新的场景来对。

然后呢，呃这里有一个我们做的一个简单的一个demo吧，然后就是介绍了一下，我们之前的这样的一些结果。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/0599f9778f66612ea3a7da59856af720_3.png)

就首先其实我们除了刚才介绍的那些以外，我们也有非常多的一些其他的研究，比如说4D的自动化标注，所以这里展示的完全就是算法真正标出来的，实际的跑出来的结果，所以可能会有些瑕疵，就是我们可以去实现啊。

完全自监督跨模跨模态的呃，跨域的就是这种啊，一个model free的一个4D自动标注的算法，然后他能真的去把这个场景中去进行，然后标注的同时，其实也是在做这个对应的三维重建，包括我们能获取到它的深度。

以及这个最重要的就是表面法向量，其实基于NERF的方法，它是很难绘制出一个表面光滑的，这种表面法向量，那我们把各种各样的序列去做三维重建之后，我们其实就能得到一些这种序列的场景库。

那我们可以比如说挑其中的一条一个场景库，我们去做一些编辑啊，这个是真实的这个数据，但是呢因为我们对它进行前后景结构的重建了，假如说我们现在可以删除场景中的一些某些，特定的类的物体，比如说把这个人删掉了。

或者去编辑一些车辆啊，增加或者删除一些车，那这个时候我还可以去啊，比如说增在这个路上增加更多的车辆，然后呢，创造出一些这种真实世界中，也不存在的一些场景，比如说要让这个场景变得更危险一些。

或者说有这种右边的一辆车去来来来别车，这种特殊的一种情况，那同时我们也做了一些简单的也探索吧，就是说改变一下他的这种daylight啊，然后呃其实后面这个就是展示的时候。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/0599f9778f66612ea3a7da59856af720_5.png)

我们也具备一定的这种场景的生成能力啊，我们在给定这种情况下，我们可以去基于这个路网，去生成各种各样新的数据，那最终我们能够让这个数据变得越来越丰富，包括能够拿它用来去做自动驾驶算法的训练。

或者说去把它做成一个闭环仿真的测试的，这样的一个或者是一个闭环仿真的引擎啊，这个就是我们这两块的工呃，这个关于环境这一块的工作，那呃除了环境以外，我们还有一个很重重要的一个研究方向。

就是对于agent的研究，就是怎么去利用这种智能体，去实现自动驾驶的任务啊，那我认为可能自动驾驶的这个智能体，刚才提到的这三点，一个就自监督，高泛化性和持续学习，它为什么重要呢，呃自监督主要指的是。

他需要有这种自我反思的能力，而不是凭借一些外部信号去进行反馈，那这个其实对于这种大模型，或者说大规模的数据来说是非常重要的，因为现在有的开环的方法，我无论是自动标注还是人工标注。

本身本质上也都是需要有标注才能进行的，但如果我们有一套算法，它部署到这个模型中，自己跑自己发现自己哪里做的对，哪里做的错，那我可能就不再需要去有人工标注，是从而实现一个自监督的效果。

那第二点就是高泛化性，我们认为可能一个这种算法，需要具备一些推理的能力，它不是简单的去对所有我采集到的已知的场景，去做记忆，因为其实如果大家就是做这种啊优化了，或者说这是机器学习的一些呃有背景的同学。

大家应该都知道，就是呃任何这种optimization based model，他肯定会有这种遗忘灾难的问题，那所以大家想要去通过采集更多的。

corner case的数据来解决corner case的问题，那corner case呢它天然就是一，就是现在的这种基于学习的方法，天然的就要记那些常见的case。

我要忽略掉那些corner case，因为对于很多算法来说，conner case反而是一个异常，我conner case做好了，我可能common case做的就不一定好了。

所以其实这里面存在一个矛盾，所以我们认为可能如果简单的去记这些啊，input output的这个pair可能还不够，还需要让模型具备一定的推理能力，那第三点呢，就是我们可以基于前面介绍的这个啊。

反思和推理能力来克服这些遗忘的灾难，实现一个积经验的持续积累，其实整个的这个过程就跟人类开车的，这个或者说甚至不只是开车啊，就聚身智能在日常完成各种日学习任务过程中，很类似的这样的一种模式。

那我们管这个模式，其实叫叫做这种自动自动驾驶，然后这里其实我们呃介绍了一个这个闭环的呃，就是关于这个知识驱动自动驾驶，闭环训练的一个研究框架，是一个偏这种high level的一个一个框架。

那他首先能够从环境上去进行这个感知，然后呢感知到这个场景之后，我试图去理解这个场景，并且做出一个plan，然后呢这个plan呢它在执行之后，它可能会有两个结果，一个就是成功了，一个就是没成功。

那成功了呢，他就作为一个成功的经验，我有一种模式把它保存下来，失败了呢，那我要求有一个模型，自己或者一些外部的模型，能够自动化的去让它去进行一些反思，并且让它呃重新生成出一些这个成功的。

这个告诉他如果怎么做，有可能就能避免刚才的这些事故啊等等，这些信息呢也会被保存到这个memory中，那在下一次遇到每一次遇到这个场景，我先从我的memory库中，我去query说。

我是不是曾经遇到过类似的场景啊，如果遇到了，我当时是怎么做的，那我结合当前场景的一些特殊性，加上一些之前的这个经验的这个泛化性，结合到一起，让他做出一个决策，那这种方法其实就有可能会去进一步提升它的。

这个模型的性能啊，然后其实刚才有提到说，我们整个这个过程需要让agent，它具备一个这个能够去做推理，能够去做决策等等这方面能力的一个呃，这这方面的一个能力啊，呃其实之前我们有尝试过。

用一些传统的一些方法，那现在大模型呢其实出现了，我们发现大模型刚好是可以，作为这样的一个模块来嵌入进去，那这个的话是我们在这个ACCLEAR，二四的一个工作吧，那这个工作其实进行的比较早。

其实投的比较晚，然后它是以应该算是我们呃第一个，能够用知识驱动的方法，去把这个大模型跟自动驾驶相结合，去进行决策的啊，然后的这样的一个研究，那它本质上跟刚才说的这个架构是非常相似的。

只不过它中间所有的这个呃进行推理，进行决策的模块是由一个大圆模型来执行的，然后呢刚才有介绍的说哎这样的一套系统，它的一个重要的特色，其实是在于持续学习的能力，那我们认为可能记忆它就能体现。

它是不是具备持续学习的能力，比如说我们可以通过一些机制，人为地设置它记忆的这个上限，然后呢我们发现随着这个记忆的上限的增加，整体的性能其实是呈现一个上升的趋势的，也就是说明它在积累经验的过程中。

这个经验其实是切实有效的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/0599f9778f66612ea3a7da59856af720_7.png)

然后这里有一个简单的例子吧，因为我们这个其实做是一篇，非常早期的一个文章，那那个时候其实还没有很好的这种，甚至那个时候还没有VIM的一些工作，能用的VOM，所以我们更多的是呃。

只是关注于这个真基于这个绝对真实的这个，ground truth的这些这个呃感知的结果，然后加上我们一个简单的仿真引擎，然后去让大模型去针对现在的这个场景，去给出一个自己的评判。

并且最后给出给出一个final answer，就是决定我要加速减速还是换道，那目的就是想要在这个场景中，能够尽可能的不是不断的开下去，比如说我们的这个实验，最后发现模型刚开始放在这个环境中啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/0599f9778f66612ea3a7da59856af720_9.png)

它可能跑靠开个这个几十帧就撞车了，然后呢当执行到一定程度之后，你会发现它可能开几个小时都不会撞车，那这个是我们做的一个，非常奶义务的一个探索对，然后呢其实在这个这个阶工作之后。

那很自然的就是我们可以对它有些改进，就是呃我们最新的一个工作，这个工作其实比较新，上个星期其实才挂到RK5上，然后呢，它是一个结合了现在的这个快慢系统的，自动驾驶闭环学习的框架呃。

它的一个主要的一个特色就在于，其实呃我们又进一步的去思考人啊，去进行决策的过程，会发现人的脑子中其实分成大概两个系统，那一个叫做这种heuristic process，就是啊这个这个直启发式的系统。

它更像是一个这个呃肌肉记忆，或者说这个那个叫什么呃，就是呃一个一个非常快的这样的一个系统啊，然后呢呃在遇到一些特殊的呃，遇到一些场景的时候，我很多的情况下，我可以下意识的就做出一个正确的决策。

但是呢有很多的时候，我可能是光靠下意识的决策还是不够的，肌肉记忆也是会出问题的，比如说在我们遇到一个从来没有遇到过场景，那这个时候人可能就会有一个，更加理性的一个系统，叫做分析系统。

然后呢这个系统它相对来说比较慢，但是它能够有比较强的推理的能力，那我们这篇工作的一个特色之一，就是把这种快慢系统结合到一起，去，实现一个呃，能够像人一样去对这个场景，去做知识积累的一个过程。

那除此之外呢，我们也弥补了之前那篇工作，只能基于这个绝对感知的结果来做啊，的这样的一个缺陷，然后我们也设计了一个VLM来去让他去进针，专门针对这个驾驶的场景去做一些理解的，这样的一些能力啊。

然后具体来说呢，就是我如果是在进行场景理解的话，其实很关注的一点就是我们要关注呃，专注于周围的重要事物，其实像普通的这种传统的一些方法，如果我只是去对场景做个描述，那它的描述可能会非常的广泛。

然后就是可能跟这个实际的，我想用的这个结果并不是非常的match，他可能你给他一个交通的场景，让他描述一下里面有啥，他可能说今天天气真好，然后天上有小鸟在飞之类的，跟交通没关系的场景。

所以我们用一小部分的数据去合成一个，专门针对自动驾驶场景很有价值的一个数据集，他会关注于这几类信息，比如说语义标签，就是啊关注这个场景中的危险场呃，这个危险的物体，比如说红绿灯，比如说基础设施。

交通标志牌，还有一些运动的这些物体，然后呢会对他们有一些特殊的标注，然后我们去用这样的一小份数据，去SFT一个开源的感知模型，其实我们就是用的一个千问的一个模型，其实几啊规模也非常小，然后呃在这个场呃。

呃然后呢我们甚至整个SFT的数据也非常少，其实只有差不多1万帧左右的，就是1万个这样的pair，11K的这样的一个pair，然后最后就能实现针对每一个感知的输入，我能给它产生一个跟我自动驾驶非常相关的。

这个场景描述的这样的一个功能对，然后就是关于刚才介绍的这个快慢系统的，这样一点，就是我们认为，可能首先呢就对于这个自动驾驶一个场景来说，我们先让快系统去做一次推理，在当前这一帧的场景做一次推理。

那这次推理因为它速度很快，但是它可能会出错，如果出错了，对应的这个系统我们会又给回到慢系统，让它去生成出一些你为什么出错，你下次怎么做，该正确的这样的一些决策，我们会不定期的去把这些慢系统生成出来的。

这些数据用来再去调整这个快系统，那然后呢最后就能够让呃发现哎在绝大情况下，我调用快系统都不会出错，只有在很少情况下，我需要调用main系统的时候，再来调用一次慢系统，然后再来生成新的经验。

整个这样的一套pipeline的一个最大的好处，就是它能够实现真正意义上的，我把这个模型部署到一个虚拟环境中，它就一直跑，它自己可以给自己积累经验，而不需要针对每一帧人去总结他哪里做对了，哪里做错了。

也同时也不存在像这种感知，像这种人工专编写规则啊，等等这样的一些非常麻烦的一些事情啊，那我们其实做了一些很简单的一些实验吧，然后主要就是这种在特定的数据集上，首先第一点就是左边这个表格呢。

我们尝试了在针对驾驶场景优化之后，我们其实超过了GPT4的，就是用GPT4来去做我们这个快慢系统，然后呢它本身没有对自动驾驶场景进行优化，但是我们大致拿了11K的数据，对自动驾驶场景去优化了一下。

发现只用一个千问，1。5这样的一个非常小规模的模型啊，不是千问，也就是千问一个小的一个模型，然后就能达到这跟GBD四一样的效果，然后同时呢，我们也探索了跟这个数据驱动的方法相比。

因为驾驶的经验是自监督这个闭环形成的，所以对于监督的数据要求是非常低的，比如说像这个表格中展示的上面的有些结果，其实它的性能是非常好的，但它是建立在海量的人工的标注的基础之上。

可能有几百万帧的这种数据来去训练，但我们其实整个系统真正拿来去训练，模型的数据其实是非常少的，那底下的这个表格呢，我们也验验证了另一个关键的点就是泛化性呃，我们有探索过说在这个呃呃我们实现这种跨域。

比如说像KALA这个仿真引擎中，我们用除了这个某一个城市以外的这个数据，去在这些城市里面让他跑去积累经验，得到的这个模型，我们直接把它放到一个全新的城市，它之前都没有跑过，然后让他来直接去运行。

发现它的性能是不会呃，虽然会有一有一些下降，但是这个性能不会一降到底，让它彻底的不work，说明我们学习到的这个知识，是具备一定的泛化性的，为什么呢，其实是因为我们整个知识的表征。

都是用这种类似于语言的这种方式来表征的，其实我们回过头来再去看我们学到的很多知识，它基本都是一些这种红灯停绿灯，行车离你太近了，你需要踩个刹车之类的这种级别的知识，那这种知识其实它是一种放之四海皆准的。

而不是去OVERFIT到某一个场景下，所以这个是我们认为啊，它能同时具备这个泛化性的这个和闭环测试，这个能力的一个主要的原因，然后还有就是我们也要验证一下，它是不是具备持续学习能力，就是我们可以发现啊。

比如说看右边这个图，就是我们随着反思的次数的增加，随着我们在这个场景中跑的轮数的增加，整体的平均的成功率，也是能够呈现一个不断上升的一个趋势的啊，对但是实际上在这个应用的场景中嘛。

因为也不能说完全达到百分之百的一个准确性，也是主要，也是因为我们现在的这种视觉的模型嘛，也是一个临时的方案，其实它没有非常完美的解决好，对于交通场景的这个感知理解能力啊，那后面有一个非常简单的case。

然后呃就是这个场景特殊是在于它是别的物，两个物体撞车了，然后呢，我们可以通过这个模型，让它能发现了别人撞车了，然后我们自己来做出一个判断，就是觉得我需要先减速暂停一下，看看是不是有什么问题。

然后当这个问题解决了之后，我再往前走，就是我我们觉得可能这种场景挑出来，是一个相对比较corner case的一个场景吧，对然后最后的话再介绍一下啊，这个今天的这个结论。

那我们认为可能现在是一个呃很关键的一点，就是从开环走向闭环的一个节点啊，因为有无穷无尽的corner case的存在，所以我们通想要通过采集海量的数据，用这种开环的方式来去逼近闭环的这个模式呢。

其实是比较困难的啊，那我呃所以我们提出了，可能现在可以从数据驱动的方法走向知识驱动，那因为为了解决这些control case，我们就需要让模型具备这种自监督的，推理和反思能力。

那所以它呃呃同时呢我也认为这个自监督，泛化性啊，持续学习是实现知识驱动自动驾驶的基石，包括自监督呢，它能够在无人工标注的情况下，来对环境实现一个交互和反馈，来实现最终的自我反思。

而泛化性呢就是利用它的推理能力，能够对未见的场景去举一反三，那最终持续学习呢，就是呃能够同时利用这个推理能力和反思能力，来实现闭环的这种持续学习，能够不断的对啊场景的理解去呃。

这个不断的去增长他对场景理解的能力对。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/0599f9778f66612ea3a7da59856af720_11.png)

然后最后的最后就是做一页的广告吧，就是介绍一下我们团队，其实我们是上海人工智能实验室，智能交通平台组，我们目前的研究方向也都是专注于知识驱动的，自动驾驶的技术路线呃，的探索。

主要追求把AGI和聚神智能的一些相关场景，应用在自动驾驶的领域中，然后想要去探索这一些比较新的路线的，一些可能性啊，我们团队其实非常的年轻啊，就是21年7月刚刚成立，然后目前有30名左右的成员。

其实一大半还是实习生，然后呢，目前呢呃大概呃取得过七项的评测的冠军，包括像这个vivo open data家set，它的3D离线目标检测，其实我们差不多已经这个霸榜第一名啊，已经有一年多的时间了。

然后在一些学术的会议期刊上，发表了大概60多篇论文嘛，然后有30多项专利参与到了一些这种团标，国标国际标准的职称专家组的工作对，然后这个就是我今天主要介绍的内容，然后其实因为我呃。

如果大家对这个我们这边的研究方向，因为我了解可能今天来的有很多，也有一些学生或者有些老师，如果有一些优秀的学生啊，老师也愿意推荐到我们这边，因为我这边可能后面会有一些读博的名额。

也欢迎大家来跟我邮件的交流啊。

# 2024北京智源大会-生成模型 - P1：论坛背景与嘉宾介绍：李崇轩 - 智源社区 - BV1DS411w7hz

各位同仁，各位老师，各位同学，大家下午好，我是这个呃人民大学李崇轩，然后非常非常高兴，这个受到了这个组委会的信任，然后组织这个呃，生成模型的这样一个专题论坛啊，然后也特别感谢这个啊。

大家能够就是啊离主会场还有点距离来过来，我们座无虚席啊，非常感谢大家，然后我在去年呢，也是和这个清华大学陈建飞副教授，我们一起组织了这个论坛，过去了啊，一年啊，我们生成模型的发展其实还是很快啊。

非常非常快，然后呃我们可以看到，就是在很多很多的方面有这个啊，呃我们在很多的方面有一些这个好的进展啊，比如说大家很关心，最近很火的这个视频的生成模型啊，包括这个呃我们这个图像和语言的一些概率。

建模的框架上也出现了一些新的变化啊，大家开始有了一些非常非常不一样的，这种很前沿的这种探索呃，呃在这个方面呢，我们可能会很关心一个问题，就是说呃在网络结构上。

我们可能认为transformer是或者它的变种啊，是一个好的这样一个选择，然后大家对于这种可扩展性啊，或者规模扩展也有一个比较强的这种信赖，那么在这个呃概率建模框架上，反而还有一些争执啊。

就是说我们在语言上，大家可能更相信自回归模型啊，在这个啊视觉上，可能大家倾向于啊用diffusion model，但今天呢我们呃非常荣幸邀请到了这个四位，国内生成模型领域的一线的专家。

我们一起再来讨论一下这个话题啊，然后我们邀请到了中国人民大学卢志武教授，他将跟大家分享这个视频生成的最新进展，然后我们还邀请到了江毅研究员呃，汇报这个呃视觉自回归生成的新框架VAR，然后我们还有这个呃。

微软亚洲研究院谷舒扬博士啊，分享他对于现有的一些视觉生成模型的，一些呃思考，还有现有框架的一些这种缺陷的这种反思啊，然后最后呢我们有幸邀请到这个啊，上海交通大学邓志杰教授。

分享这个如何从一种类扩散模型啊，或者一致性模型的这种启发，去对大语言模型进行微调，并且加速它的推理，然后最后我们圆桌论坛呢，一起讨论这个多模态啊，在这种原生多模态非常非常呃呃火，然后即将有突破的前夕。

讨论，在这种情况下，我们应该或者是怎么样去得到一个统一的这种，概率建模框架这样一个问题好，我们就这个短暂的一个介绍哈，谢谢大家，然后下面就是我们的这个啊报告环节，我们第一个报告邀请到了这个中国人民大学。

卢志武教授啊，他是这个2005年毕业于北京大学，数学科学信息学院呃，获理学硕士啊，然后2011年毕业于香港长城大学计算机系，获学呃，博士学位，研究方向是机器学习与计算机视觉。

它设计了首个中文通用多模态预训练模型文栏，然后发表于多模态呃，首篇就是发表多模态领域首篇nature子刊论文，然后并且早于open i，发布了类SA的这种视频生成的底座模型啊，VDT好。

我们欢迎这个卢老师来分享。

# 2024北京智源大会-生成模型 - P2：视频生成前沿进展：卢志武 - 智源社区 - BV1DS411w7hz

感谢李老师的介绍，也也非非常荣幸来这儿做一个报告的分享，我今天主要介绍一下我在视频生产上的，对自己的一些探索吧，或者一些思考哦，哦这个好这个好一点嗯，这是我今天报告的分四部分吧。

先讲一下视频生成为什么这么难啊是吧，它目前大家是有哪几种想法，是怎么去研究过他的，然后按照我我总结的有两种方法或者思路吧，分别介绍一下最新的进展，最后对未来一年是吧，他应该怎么去发展做一个预测。

下面我先介绍第一部分吧，视频生成相对于呃图像生成肯定困难很多，呃这个原因也是很显而易见的，因为什么呀，视频我们可以看成是什么呀，是多帧的是吧，图像生成它要考虑一个时间维度呃，所以从这个角度的话。

肯定视频生成比图像生成难得多，是他就说他最容易犯的，就是最容易出现的一个问题是什么呀，视频生成内容一致性没有保证啊，这个一致性又可以体现在很多方面啊，就是比如说他故事连不连贯。

或者里面的一个人他是不是同，就是一直是那个人的ID，或者那个人虽然ID是一样的，但是他的动作可能不连贯，也有可能嗯，这个是我用saver video defasion啊，这应该是目前比较好的开源模型。

生成的两个例子啊，大家可以看到这两个例子里面呃，比如说那个就右边那个男的是吧，他脚应该是一会三只脚，一会两只脚，这个肯定是有问题的，左边这个小女孩的脚也也是一会儿，有一会儿是没有的。

呃第二个我视频生成面临挑战肯定是什么呀，它消耗了资源实在太高了，嗯去深链资源肯定是消耗消耗特别高，呃就算说我们说推理的话，它其实自然也是相对很高，就比如说我们生成一张图片。

现在可以做到一秒之内就生成出来，比如说5+2乘五元二，是不是，但是如果你生成一个，比如说720P的一个高清的视频的话，门槛级到高清的视频，你生成比如说几秒，比如五秒不多吧，五秒乘以比如说24是吧。

乘以24，100多100多帧，是不是你要一下子把100多帧生产出来，一个是说时间长，另外一个主要是他耗的显存太多了，很多时候甚至你都装不下啊，所以我们很多时候为什么不能生成那么长，其实就是因为什么呀。

我们要保证它的连续性，所以一定要是吧，最好是一起就把一个视频一下子生成出来，但是你一下子生成出来的话，它占的显存又太多了，所以是一个矛盾的问题，第三个视频生成我觉得最难的就是什么呀。

它就是做可控生成方面的，是就是图像的可控生成，已经研究的特别透彻了，是不是特别是在control net的加持之下，我觉得已经几乎可以商用了呃，但是视频生成我觉得应该困难很多呃，原因也是显而易见的。

因为视频里面有很多额外的一些因素要考虑，比如说我们都知道视频有镜头的概念，就是我们的镜头从各个不同的角度去拍的话，这个画面是完全不一样的，然后还有什么呀，就是里面视频里面人物的角角色的动作。

是不是他从他做不同动作，这个人完全是不一样的，呃总之吧就是视频要做到的可控生成，考虑到额外的因素特别多，所以它也变得很困难，是不是很困难，呃当然了呃就说食品生产本身都没做好。

这个时候考虑可控生成是不是太遥远了，我觉得也不是啊，我后面会举个例子，其实也可以研究这个问题的，好我刚才讲了一下视频生成，我总结的三个主要的挑战啊，嗯虽然它很困难，但是我们作为学术协助穿的嘛。

肯定是一般的，是越困难我们就越要上，是不是越要上嗯，所以也是有特别多的研究者是去研究这个问题，嗯我个人的观点啊，我认为什么呢，就说把这把所有的这种研究啊，大概可以分两类啊，第一类是什么呀。

基于SD就是stable defusion呃，当一个底座嗯，用逐帧生成的思想来做，然后在生产的过程中间，后面再去考虑它的连续性的问题，还有一个就是说比较彻底了，我把整个视频当做一个整体，就是一起生成。

不是逐帧生成出来的，我是吧，我一把一个视频当做一个3D的表示，它一下子出来，这样他就是最大的好处就是什么呀，它生成的内容肯定连续性是没有问题的，但是他的好坏处就是什么呀，哦它没有一个预训练。

是不是没有一个预训练，你所有的训练都得从头开始，这样肯定对我们是吧，要求特别高啊，你的三力要求特别高是吧，然后第一个方式的比较有代表的新新脑啊，就是皮卡呀，Runway。

然后还有阿里腾讯的一些开源的模型，甚至slayer defection，这一家公司开源的视频的模型，是不是这个呃，这个至少有三个是开源是可以用的，还有基于第二个第二种方式。

我把它叫做时空patch或者整个视频啊，整个视频一起生成出来，我就这种方式的话，有比较比较有名的SORA，还有申诉的we do，还有我们自己的一个呃VDTR嗯，我我做这个报告的时候，可能是一个月之前啊。

一个月之前我尽量的已经把最新的包包进去，但是视频是发展实在太快了，比如说过去一周他一下子出来四五个，所以我我也没法加进来，感觉是吧，反正比如说那你下一周可能又出来了，没办法是吧，但是但是万变不离其宗吧。

就是他们的套路肯定是，要么是在前面这个方式里面，要么就在后面这个方式里面啊，好吧嗯，然后后面这个方式的话啊，也有两种啊，一种是就是结合DEFUSION加transformer的优点。

就是所谓的DIT的这种架构，还有一种是纯什么呀，transformer就是自回归的架构，呃，目前来就是目前就说发展的早的，或者是呃就是体现的效果好的，还是DIT架构啊，我觉得是吧。

然后就完全transformer自回归的架构嗯，就是就是目前就是我觉得不是那么多吧，虽然有一些，但是呃也不好说他以后能成主流吧，就是只是有个苗头啊，但是还没有证明他自己。

所以我主要还是都是介绍基于DIT架构的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_1.png)

好我们先看第一个范式呃，基于SD逐帧生成它的一些最新的进展，我们看一下诶，第一个就是皮卡，皮卡当在去年是吧，我我不记得是不是去年也是曾经挺火的，他也融了，好像最近也融了一轮啊，就说明投资人还是认可他的。

呃因为它是个必然模型，它的技术报告都是很简洁，没有没有透露它的模型细节啊，我们就把它他做的一个纹身3D的一个加速器，我们列在这啊，呃大概的思想就是什么呀，嗯他跟纹身3D啊，3D模型的这样一个模型。

它设计了一个加速的办法，它把它就是在这个生产过程中间，引入了一个要滑滑动窗，然后滑动窗里面的计算是可以用多张显卡，并行计并行计算的，然后设计了一个阈值呃，就说怎么看这个滑动窗。

可以进到下一个iteration里面，这个阈值尽量的节省是吧，计算量呃，那回到视频生成本身啊，我们觉得就说我们推测啊，反正他自己大概也是这么默认的，它应该是基于一个预训练的SD模型啊。

然后加上额外的时间围上的建模，所以我们就觉得他应该还是基于逐帧，基于SD逐帧生成是吧，然后后面再补上时间维的建模，右边是它的一个效果，第二个比较好的模型就是run away的金兔GTO。

实际上他的技术细节也没有公开，我们这列的是他的金one的技术细节，jin one是呃，他的技术细节公开了呃，它本质上是一个视频编辑的模型，我们可以看到是不是是一个视频输入进来，然后我们可以输入一个图像。

或者输入一个一段文字去对它进行修改，它整体的架构还是一个DEFASION的架构，stable defusion的架构啊，只是他额外加了一个什么呀，我我的视频生成就是输入进来以后，我们会提取它的深度信息。

深度信息有我们觉得什么相当于是一个3D的，3D的信息吧，哎一这样我们就保证说我编辑以后的这个视频，跟原视频尽量的在深度信息是保持一致的好，因为什么呀，金兔的技术细节没公开。

我们就呃追究一下jm one的技术细节啊，这是里面它最重要的两个时间，为建模的一个改动啊，呃一个是什么呀，就是时空的这种就是就是残差模块吧，还有一个是attention的模块呃。

它的改动其实思想也特别简单，就是在传统的就是2D的，就是2D的转机后面跟紧接着一个什么呀，就是一维的时间的转机，红色的部分啊，那我看一下，这个地方这个就是时间为的，就是啊就是这个地方就是这个地方。

就是每个二维的转机，后面跟着一个一维的时间时间的转机，然后attention是什么呀，你是就是这是什么呀，这是视觉二维的attention，然后后面跟一个什么呀，时间维度的上的一维的attention。

这样他就是什么呀，我本来是一个SLIBDEFTION，只能生成图片的，但是我加上时间维度以后，它就可以对时间进行建模，这样就可以生成视频了，刚才两是两个必必然模型啊，是主要是去年提出来的嗯。

然后剩下我介绍三个开源模型，一个是阿里的这个模型啊，我觉得阿里还是很有诚意啊，他它不仅仅视频生成的模型，也开源了它那个多模态大模型啊，比如千万VL也开源了，我觉得这个是对整个领域的发展。

还是很大的促进作用，至少钱文为二，我知道很多人是在用的，然后呃阿里这个模视频生成模型，它其实本质上是一个图文生成视频的模型呃，它把整个视频生成分成两阶段，第一阶段是是一个什么呀，我们给一张图片。

它经过clip模型，经过一个detail的in encoder，然后一个general的encoder，反正就是各种encoder吧，尽量的把这张输入的图片的信息提取出来。

然后丢到我们的save deftion里面，当条件，然后生成什么呀，生成一个448×256就是分辨率，然后32帧的视频出来，但是这个是个低分辨率的，是不是，所以他加了第二阶段是个高分辨率的。

把刚才生成视频，当的是输入输入到我spring ption里面来，然后同时再加一段文本当条件是吧，继续把它的分辨率提升上来，比如到720P了，是不是这样就变成一个高清的一个视频了，然后他的论文里面说了。

它的底座是什么呀，是stable devotion，实实际上是SD2。1嗯。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_3.png)

这是他论文给的一个几个例子啊，通过他的例子，我觉得它的效果应该是，比前面两个模型要差一点，大家可以对着看一下，然后这就是腾讯的这个这个开源的，开源的模型啊，我觉得也应该对这个领域的发展也是很重要的。

呃它本质上是基于他们，他们前面有一个叫纹身视频，叫做video crafter这样一个模型，把它改造成一个图文声视频的模型，就是就是上上半截就是他的纹身，纹身视频的模型呃。

下面这个加的就是把图像能加进去，是不是图像多一个条件输进去，最后把它改造成一个图文声视频的模型，他的论文也说了，他的底座也是stable difference，2。1嗯，这是他在论文里给的一些例子啊。

至少在这个例子上，他说我感觉他应该是比呃皮卡呀，比jm two，比runway的要好啊，不不就是跟比阿里的那个模型也要好一些，好最后我们重点介绍一下。

就是stable defusion这一家公司提出的视频的模型，它把它叫做stable video defection，我们后面简称SVD啊，呃这个模型，他是有论文的，他是有论文的。

我我主要是讲他为什么呃，这个模型可以说是开软里面涂生视频，最好的模型啊，呃最好的开源模型呃，比腾讯那个也要好一些，我觉得呃它的原因主要有两点，一点是什么呀，我们都知道这种基于stable。

default的模型是什么呀，它其实都有一个什么一个VAE，就是所谓的就是压缩的一个模型啊，它是有个encoder decoder呃，压缩完了在引空间里面，我们是用SBDEFUSION做生成嘛。

所以呃SVD为了保证，为了保证尽可能的什么呀，把时间信息考虑到，所以他把VIE里面的一，就是decoder部分重新训练了，然后在这个训练的过程中间，把时间的因素已经考虑进去了。

这样它生成的视频的连续性会好一些，这是一个改动啊，另外一个改动它也跟runway一样，它也加了时间的模块和是吧，时间的模块一个是什么呀，那个残差的模块转机，一个就是attention模块。

它都把时间维加上去了，但是它跟run way的差别是什么呀，它是把整个模型从头训练的，从头从开始开头训练，但是其他的几个模型只是动了，它就说新加那一部分，就是刚才就是时间维上的那一点东西。

但是前面的东西都没动，所以从这个角度，我觉得它的效果肯定是会好一些，额这是我们用他的模型生成的两个例子啊，看他的人像的细节，然后他的动作我觉得都还可以，连续性也没问题呃，这是他的论文。

你给的一些评测的结果，他甚至比呃是通过人评价的啊，是比金兔和皮卡都要好，好，这是把我们第一种方式的最新进展都介绍完了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_5.png)

我们现在介绍呃，第二种方式，我是觉得就是把它叫做时空patch生成也好，或者叫整个视频整体生成也好吧，嗯反正大家明白什么意思就行了，好，那这个里面最有代表性的工作，或者是大家为什么这么关注视频生成。

肯定是要归功于OpenAI，是他当时在1月份的1月份还是3月份，我不记得了，提出了那个SA模型呃，SA模型它里面很重要的几部啊，我们简单的介绍一下，他第一步就是把一整个视频啊。

就是因为我们要整个视频生成，所以他要把整个视频表示成什么呀，时空的patch要做压缩，所以他专门训练了一个V口VIE网络，把视频压缩到一个什么呀，低维度的就是这个里面啊，低维度的一个空间里面。

然后在这个低温的空间，把视频把它分解成3D的这种patch，那就是每一个小块，我们可以把它看成一个token了，所以就可以把它丢到transformer里面，是不是。

我们只要把这个视频表示成token序列的话，那当然可以把它喂到什么呀。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_7.png)

transformer里面去，第二个就是什么呀，它的架构到底是什么，它宏观上他他自己说的啊，他首先他肯定是一个cos模型，他可以接受输入带噪声的patch或者加上条件啊。

然后训练模型以后去预测干净的patch，就把它重构出来，他说最重要的是它采用了什么DIT的架构，就是defsion transformer的架构，这种模型其实也不是说就是什么呀，最近才出来的。

其实在前面嗯，当然李老师他们也做了一个类似的工作，后面也马上会讲呃，我觉得这个架构其实大家都想到了，我觉得是，然后这个架构他说在很多方面验，就验证了它的扩展性，这个事情也是很重要的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_9.png)

嗯然后这儿给了三个例子啊，就是一个就是他说就是最基础的计算量，中间是一个四倍的计算量，然后右边是一个32乘的计算量，呃这个大概说的是什么意思呢，就是其实就是说的什么呀，这种大模型的可扩展性。

或者是我们所谓的什么呀，词skin la嗯，词根练度只是说他这是体现的角度不一样啊，嗯我们知道嘛，solar这个模型里面它肯定有很多模型上的设计，是不是你采用不同的设计，你到底依据是什么。

然后open还通过它呃，它的实验证明我的设计只有一一种做法，我只要什么呀，我这个模型的架构的设计，让他的训练的代价变高，或者计算量增大的话，他这种的效果就会好，所以我们可以看到随着它模型的架构的改动。

它号的计算量是吧，乘以四倍，乘以32倍，然后他的越来越清晰，然后这个狗的动作越来越连续，第二个工作就是申诉的维度，这个背后肯定是有，我觉得李老师是吧，应该是起到很重要的作用。

呃他我觉得应该是按李老师自己的说法，应该是比那个DID还早是吧，应该是嗯我给你宣传一下啊，就是这个工作这个工作呃，他思想我也讲一下，我讲的不对，你可以纠正嗯，他的思想就是什么呀。

我们都知道saber defasion，它背后很有很重要的一个模型叫you let，是不是，但是优菈里是一个转接的模式，模型，就是李老师他们想了，说能不能改成一个transformer的模型。

所以他们就说能不能设计一个UVIT，是不是let就是转机嘛是吧，let it它们u let Lt是一个CN的模型，他说能不能改成一个v it的模型，就是我们知道视觉表示里面很重要的两个模型，一个是CN。

一个是VIT是vision transformer嘛，呃最后李老师他们做了实验证明嗯，说类似优奈里的设计，他们说发现时间不以及就是就这个地方，时间部以及context就是文本输入啊。

这些这种东西如果当做token丢进来的话，这个效果会很好，同时类似unit有一些常，就是是我们知道嗯，深度学习里面很重要的一个模型叫RESNET，何凯明他们做的，它里面最重要的一个思想就是跳转连接。

是不是呃，就是说他们就会发现李老师他们证明，如果很合理的设计这个长的跳转连接的话，这个整体的效果也会很好，然后申诉他们基于这样一个UVT的模型，最后是在很大的一个数据集上。

生了一个叫ui defer的模型，然后后面我觉得应该是它基于这个unit defer模型，继续把它扩展到视频上去训练，最后都躲到vv do模型吧。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_11.png)

我觉得应该是这个思路啊，这是他们公开的，前面我记得是上一个月还是是吧，他们给的一些例子啊，我觉得效果还是挺不错的，好最后介绍一下我们自己的工作吧，嗯我们这个工作就说首首先我声明一下。

我们我我因为我是高校的老师嘛，嗯我们我们能做的啊，也很有的时候很很无奈的一个事情啊，就是类似前面所有这种做法，其实我们2022年底我们就想到了，所以我们就去我们就去做论文嘛。

因为高校老师三里都是特别有限啊，这是一定要理解啊，我们的情况就是这样，我们现其实完全想到类似的做法，所以我们就去做，但是我们因为算力有限嘛，只能在一些学术设计上去验证它，这是我们当时探索的一个过程啊。

嗯我们探索完了以后，把这整个工作，也就是把它放到一个很有名的，叫阿卡夫的网站上面，所以我们2023年5月份就放到网上去了，呃严格的来说，我们这个工作应该是比SA早很久了，是不是早很久呃。

回到我们的工作本身，他嗯我们当时可SA没出来啊，我们就想到了两个重要的事情，一个是什么呀，我们说能不能就说视频生成也是视觉生成嘛，当时比较主流的还是基于扩散模型来做，但是我们说能不能说结合扩散模型跟。

transfer模型的优点来做这个事情，所以我们说将transformer的技术，应用到基于cos的视频生成里面，第二个就是什么呀，我们提出了统一的时空掩码进行建模，这样保证它把视频生成里面。

所有的情况都能覆盖到，但这是举了很多例子啊，然后我们这个工作也很信任啊，就发表在我们这个所谓的，我们这个领会的顶会啊，叫埃克尼尔呃，呃如果再晚一点，我觉得就发不了是吧，三运气好一点，要是再晚。

比如说如果阿克里尔再中不了，那就那就完蛋了，那可能就是吧，就只能放在阿cf上了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_13.png)

这是我们模型的一些细节啊，跟SARA类似，也做了视频的压缩，只是我们的压缩的模型，因为呃要考虑三力有限嘛，做了一些简化，然后中间的这个架构跟SA是一模一样的，然后里面的呃就是transformer模块。

跟SORA有一点点差异啊，呃就是什么呀，我们是把时空的attention是分开的，这个软音也是很简单，这样做的话，我们消耗消耗的散力会小很多，没有别的原因啊，我们其实知道放在一起肯定更好啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_15.png)

所以我们跟缩上的差异，一个就是在是就是什么呀，呃注意力机制的处理上，我们采用了通常呢把它分离的方法，这个时候很多是在显存三力有限下的一个，trade off呃，但是SORA是将时空合并。

是不是他直接用3D的注意力去做这个事情，所以它的效果肯定是要好一些吧。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_17.png)

当然耗到三里也会大，嗯另外一个我觉得这个倒不是本质的区别，我们当时因为也是3D有限，我们只考虑了图像生成视频，没有考虑文本，因为文本肯定是要需要的数据量更大，更不容易出效果，没有别的原因。

就是这一点点原因，因为呃图声视频跟纹身视频比，我觉得软体上没有什么差异，但是纹身视频它的需要的数据量要更大，才能出效果啊，呃我们当时就说在2023年，把那个论文放到网上的时候。

我们那个论文里面其实也有这些例子了，并且我们专门为这个论文做了一个网站，这些例子当时就放在那个网站上，你们可以去查得到，我们有一个gm github上的一个网站，呃，里面这些例子其实都放到里面去了啊。

并不是说后面索尔出来以后，我再补这个实验啊，没没有的，是当时就有的嗯，所以我们当时其实已经发现了类似这种什么呀，就说就用时空patch去生成视频，它其实真的是可以对简单的物理规律进行建模。

这里面其实包含很多种不同的什么呀，物理上的一些运动的模拟哈，它其实都不是线性的运动啊，是加速运动嗯。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_19.png)

那我刚才说了，我们因为是高校的团队嘛，其实手头的范例其实特别有限啊，那视频生成又是特别消耗算力的一个事情，有个工作，所以我们我们结合我们的实际情况是吧，做了一个折中，我们可以在一个全域上把它做好。

是不是全域上做的要做到什么呀，能能就是能to c其实特别难哦，to c的用户能接受是特别难的，所以我们在我们的模型上面继续加了一些东西，比如说我加了一些人脸的控制，可以保证它生产过程中间人脸不要动。

然后还加了一些人的骨架的控制，是保证他的动作生成比较自然，然后这些细节我们就不讲了，太邪术了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_21.png)

大概就是加了这些人脸的控制，然后人的骨架的控制，最后我们生成了这样一个效果，是不是至少我们看他眨眼应该是比死背，就是stable s v t吧，或者是腾讯这个模型啊，我觉得是。

目前这两个是开软里面最好的两个模型啊，我们还是比它明显的要好一些的，我们这所有的条件都是一致啊，我跟我们跟就是爱什科技做了对比，跟那皮卡也做了对比，跟ram也做了对比，你们可以看到我们的效果是明显。

比他们要好一些的，嗯当然这个也不是特别公平啊，因为毕竟我们是专门针对全域优化过的是吧，但是也能看出一些问题吧。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_23.png)

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_24.png)

这个就是我们继续优化以后，把它稍微做了一点后期处理啊，嗯这样我们尝试着把这些视频卖给普通的用户，呃，这两个视频应该是卖掉了，就是我们拿到这儿了，当然大家不要泄露出去啊，因为可能那个人要告我的话有问题。

呵呵对，呃我我主要是想证明什么呀，我们其实有的时候呃，虽然我们每个人都是可以一堆的抱怨啊，比如说算力有限啊，或者各种数据不够啊，呃你可以扯一堆理由啊，但是如果你坚定的选一个东西啊，其实可以做出来了。

也可以把这个事情打穿的，是至少我现在证明我我是吧，VDT我们我觉得我我们想到的比solo早多了，是不是，但是因为三点原因，我做不到它生成一分钟的视频，是不是我做不到。

但是我可以在这个人线视频生成上面写成，视频生成上，我可以做到效果很好，做到普通的用户都能接受，甚至他愿意付钱买这个视频，那这个就是很难啊，那不是solo那个模型就能解决的。

是不是那个还就是SORA虽然已经很惊艳了，但是离普通的用户愿意为他付钱，这个差了10万8000里是不，所以这个后面还有很大的gap，但是我们在有限的3D情况下，我把这个打穿了，其实还是很难哦。

最后给了一个长一点的例子嗯，这个视频是再早一点，刚才那两个是更新一点的，可以看到里面比如说是我呃，扇子遮脸啊，她都是可以把它恢复出来，甚至刚好最后那个女生，她背后的那个影子，她是可以生成出来呃。

他的人在动，影子也在动，就说明这些物理规律都是学到了，最后我就简单的嗯总结一下吧，就是其实跟我们刚才讲的三个挑战是对应的，那就是说未来一年我们视频生成呃，值不值得做啊，我刚才还跟李老师讨论。

他说他不想做了，是不是嗯嗯不管了，就是写书上总得说说说抛一些问题出来，大家还有得做吗，那一个肯定是什么呀，视频加速这个肯定要做嘛，嗯你不能说消耗那么多战力，或者要等一个小时，这个不太行啊是吧。

那太久了没法商用，当然我觉得这个事情实际上是最好解决的，嗯总能把他推理加速，至少把推理加速是可以的，第二个就是更长的视频能不能生成，是不是呃，虽然SA已经生成了一分钟的，那但是能不能2分钟啊。

能不能生成，是不是生成更长的视频，能不能生成，我觉得可能呃不能光靠数模型本身，可能后期有一些处理，第三个就是我觉得特别有就是普通人能做的，就是视频的可控生成呃，这个其实有的时候消耗了3年可能没有那么大。

我觉得在座的很多老师都有可能做这个方向，好呃，李老师我要做就介绍到这儿吧。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/3fc2c8ad56c2ab803ecacfdecdbf5a9e_26.png)

问问题啊，对有一个简短的QA环节，好感谢卢老师的这个精彩的报告，对这个视频生成模型，做了一个特别长的一个梳理，特别详细，同时呢也给我们有很多的启发，第一个是在学术上，我们怎么样有远见，做的很很靠前。

第二个事情就是说在比如说我们应用上，如果通用我没有那么大算力，怎么样去在垂域中找到一些新的机会，然后我们有大概一两个问题的时间，大家有问题的话，我们可以跟啊，卢老师有一个QA的环节啊。

好大家直接可以举手啊，有问题的话可以举手，好，哎老师你好，就是视频生成虽然非常火，但感觉从高校的学生角度来说，好像没有那么多资源去撬动了，然后对呀，然后就想就是沿着这条路去做一些工作。

或者做一些小的补丁的话，感觉有什么样的方向可以，值得建议高校学生去尝试呢，这个确实很难，就是说比如说你这个时候做出来，他们肯定会说你要不要跟SARA比一下，你要不要跟那个快手的可怜比下是吧。

你这怎么比得过呀，这个这确实很难哦，但是我觉得还是可以沿着这些，就是假设以后面可可能会有一些比较开，强的开源模型出来，你基于那些开源模型能不能做一些后续的，比如说刚才的一些可控生成呀，一些加速啊。

或者一些变速，把让它变成生成更长的视频啊，这个其实不需要那么多的三零，当然还有一些别的问题啊，我可能没想到的肯定有哦，好的好的。



# 2024北京智源大会-生成模型 - P3：Visual Autoregressive Modeling Scalable Image Generation via Next-Scale Predicti - 智源社区 - BV1DS411w7hz

嗯非常感谢李老师邀请我来做这次分享，对呃非常荣幸对本次分享的话，给大家带带来我们最新发表的工作微距，Autograss modeling，Scalub，Image generation。

Will next scale prediction，这工作呢是我们今年4月份发表的，一个新的工作，是一个全新的啊，基于一个语言模型的一个图像生成的框架，对本次的分享的话，分分为五个section。

第一个section的话是我们介绍啊深度生成模型，包括debution model，包括呃language model，对第二个的话是呃，我们借鉴来自于language model的这些成功。

然后吸啊吸取一些啊language model1些成功经验啊，来帮助我们做视觉生成做得更好，包括一些经典的一些方法，Toganization。

next token prediction和scanning row，对第三个section的话，我会介绍就是经典的image organization，包括就是VQV和VAE，包括VAE。

我们来探讨离离散和连续的这个token，之间的一个关系，对对第四个section，我们会正式介绍我们VR的工作，Wage order gressive model。

Next scale prediction，最后的话是会呃，沿着我们这个呃VR或这个框架来探讨，t two i和t to v和unified multimodity，model之间的关系。

对首先我会开展第一个section的介绍，首先的话就是现在的话主流的一些生存模型，包括啊视觉生成模型，还包括早些年2020年之前的干，包括现在大家都非常非常关注VAE或者VQ，V a e。

就是刚刚罗老师介绍的一些，就是啊时空patch或者一些时空的special，Temporal token，对，第三个的话是呃是是以flow base model，最后的话是呃从2021年开始。

open AI提出来的一个diffusion bs，gun开始大火的defusion model，包括呃呃jasson home的一个呃DTPM，或者宋元老师的score based model。

就是这个diffusion model对，然后呃diffusion呃，Diffusion based mo，呃，model的话就是嗯我们可以看到这个呃landscape。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_1.png)

就是包括前些年大火的gun，现在的话大家更多的关注到fusion model，然后啊包括后面我们能看到一些auto grass models，包括energy base models，包括VAE对。

然后diffusion model的话大家都应该比较清楚了，我这块就不会再去赘述了，包括就是这里面的一些有名的工作，包括啊DPPM，包括宋扬老师的a score based model。

包括那个佳明的佳明老师的那个呃，DTIM的一些加速方法对，然后我们重点会围绕着a AR model或者是language model，来介绍我们都做的一系列的一些呃工，就是我们我们探讨的一系列些方法。

和一些借从language model借鉴的一些insight。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_3.png)

首先我们来说一下呃，像GBT或者a m model是怎么训的，第一个的话就是我们一般来说，AM的话是需要一个organization，包括BPE或者是类似word piece。

第二的话就是我们基于这个organization做next token，Prediction，第三个的话就是我们会去啊，基于这种pretrain model去做一些嗯。

就是instruction tony，最后的话会有些q human feedback来做一个RUHF对，然后呃，首先的话就是我们我们我们会从刚刚的一些language，mode1些经验的话。

我们可以看到就TOGANIZATION啊，next token prediction和scaling law，有了SCLAW之后，我们我们结合这个呃这个字。

next token prediction之间，多数方法可以去把model scaling up，包括scaling up，Model size，包括scaling up computation对。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_5.png)

然后我们可以看到就是说language mode，最重要的一部分就TOGANIZATION，包括BPE，包括word peace，它主要的目的呢，就是说我们把与人类的一些语言离散。

因为人类的语言是一些离散化的一些信息，它我包括我们写的字，我们说的话都是离散的，我们可以把这些离散的这语言分子之后，把它映射到一些token i d，有了token i d之后，我可以就可以通过一个呃。

就是总之监督的next token prediction，然后基于这crossing topy和和最大自然优化，去优化这整个model，然后我再把这个整个models getting up起来。

包括我们用更多的一些算力，对最后一点就是我刚刚说的就是BPE的，都可能ZATION或者word peace，这种其实都是语义空间上的，那跟视觉教算视觉不一样，计算机视觉的一些VQVAE或者VAE。

它更多的是一些视算机，居然计算机视觉在底层的一些嗯嗯一些信息，low level的一些信息，但是NLP里面的这些DOGANIZATION，更多是包含些语义信息，对所以包括我们所有的视频生成或图像生成。

包括未来的一些多模态，其实我们都更多的希望是视觉和语义，更多的是做一些衔接，所以这也是计算机视觉目前没有出现，出没有出涌现出这种具有涌现能力的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_7.png)

这样的大模型的一个组一个因素对，然后我们我们回到language model这一块，然后他PRETRAINING的话是更多的是通过next token，Prediction，是从大规模的这种呃呃文与呃。

文本的数据里面去学习知识，这是培训阶段，从而的话他培训阶段之后，他可以学到大量的这种语义知识，因为我们已经把这些文本，token映射到token i d了，对，然后呃并且做跟ID之间是有一些呃。

呃分布之间的关系，对第二点就是通过不同PRETRAINING，它可以做到in contest learning啊，有了in contest learning之后呢。

我们就可以transform到一些open task上，比如做一些FUSHORT，或者是做一些嗯相关的一些nonoa task呃，Generalization，这也是跟视觉非常不同的一个一个一个地方。

因为所有的我们是我们的自然语言，处理语言里面的一些任务，全部可以通过语言来描述，通过语言来表述，但计算机视觉这不是不是这样对，因为计算机视觉有些离散的任务，有些连续的任务。

离散的任务包括一些呃detection，包括一些嗯就是checking，或者是一些持续的一些离散的任务，对那连续任务就包括一些segmentation，或者是一些呃就是一些呃flow相关的一些任务。

对那有了这一点差异呢，那就来自来源于就呃就有了另外一个区，极大的区别，就是呃语言这边可以通过一些unified的呃，呃方方式，因为它语言都可以呃，可以生成，就可以用来做生成，也可以用来做理解。

有了这个语言的桥梁之后，就可以unify的生成和理解，但就像视觉做不到对，然后基于这几几点优势，那就有了这个LM的一些scaling up和scale road。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_9.png)

这一些嗯这些现象对或对，然后总结一下的话，就是说，为什么计算机视觉没有出现相关的一些工作，那是主要第刚刚总结下刚刚原因，第一个就是语言是一些人类一些已经孕育了几，通过几千年的一些规律孕育呃。

就总结出来的一些规律，然后它具有高度的一些语义和一些信息，密度比较高，但计算机视觉的话是则没有，这样就计算机视觉里面它具有更多的一些context，语言的话是一维的前后关系的context。

那计算机视觉上包括一些二维的，包括spatial temporal，还有三维的或者四维的，然后另外专业数据有更多的一些模态信息，包括啊我们自已知的视频图像的pixel，包括点云或者包括红外。

然后呃NOP里面就是语言这边的话，可以更多的通过一个呃这种cos的PRETRAINING方式，学习到语义，但计算机视觉的话目前还没有啊，被探讨的是这呃极致，因为计算机视觉的很多啊语义啊。

很多信息可能在底层而言没有语义，因此基于这些呃极大的不同，然后所以language model能够通过这样的一个范式，能够做到一个呃，就是skating up到一个非常不错的效果对。

但是计算机数学这边生成这边更多的是啊，比如说我们这已知的梯度，I t to v，或者是一些你unified的一些深理解的任务，都没有统一做到一个呃。

在token nether space上做到统一生成和理解对，有了这些之后，我们就不禁在想，就是如何能够去借助计算机视觉的一些，特有因素或特有的一些本质去学习。

ALM这边language model这边的一些特呃先进经验，包括AUTOGANIZATION，或者是做一些semantic semantic压缩，包括我们去做一个呃。

基于这种COSTAL的来做一个PRETRA呃，就scanning up的PRETRAINING啊，包括就是嗯呃基于token nether space的一个呃，有呃深层和理解的统一对。

然后首先第一TOKIZATION是最重要的，那我也会介绍，就是图像里面的token anization应该怎么做，首先图像领域啊，就是离散和连续的这个呃，就TOGANIZATION到底是哪个效果好。

呃目前来看的话是呃通过DEFUSION条路线来看，更多的是连续的效果会更好一些，但是这个离散的最近又出现了非常多的新工作。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_11.png)

对但这些都绕不开一个工作，就是VAE，VAE的话是2014年提出来的，i clear的一个在艾克利上呃发表的一个工作啊，值得一提的是，他也是拿了今年的嗯。

i clear的test of time的一个最佳奖项啊，就是奖项就是对，然后VAE的思想就很简单，其实就在引空间上加入了kl散度约束呃，kl散度约束，然后使得他能够学习。

就使得他的从AE对一个没有随机性的，这样的东西变成了VAE可以去采样，有具有随机性的这样的呃，这样的一个嗯生成模型对，然后呃有了这个VIE之后呢。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_13.png)

就有衍生出来了啊，另外一个比较有名的工作，他就说是stable呃，就是stable diffusion的前身就是呃latent呃，Diffusion model，Latent。

diffusion model呢，就是在呃VAE的一个这样的一个呃，latin space上进行diffusion，然后它其实是借助了强大的这样的一个VA呃，连续的VA的表示，然后做的非常好。

对那U嗯可以看到就是diffusion的话，目前所有的工作包括latent diffusion，就呃就包括DITDEVISION，Transformer，全部是用到了这种啊，V a e。

就尤其是离散到连续的VIE上面。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_15.png)

进行一些啊diffusion的一些呃一些模型的训练，对，那我回到了我们刚刚开始说到的，就是呃我们希望是通过language model来做，那language model。

典型的就是现在的一个AAR的language model，就是auto grass language model。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_17.png)

然后呃这一块的话就是比就是open AI，2020年有个工作叫做叫做image g b t，或者叫IGBT，它是在一个像素空间上做那个嗯，做AAR的回归的训练对，然后他的做法就直接是在像素空间上。

基于啊进行一个像素的聚类，然后然后直接是基于g p t style，进行自回归的训练，然后以及或者是呃呃或者是基于呃bert style的进，进行mask language modeling。

那它不仅做了生成，也做了一些下游任务的一些linear evaluation，对他当时更多是做成了这样的一个，PRETRAINING的范式，并没有考虑更多的生成效果，对这是第一点。

第二点的话就是他当时嗯因为在2020年，其实当时的显卡的算力的呃限制，还有就是info open i infa和ta的限制，所以它并没有在大规模的数据集上进行PRETRAIN。

而更多是在一些image net，或者是一些呃比较小的数据集上，比如说C法上面进行验证，所以它的计算量嗯，所以他的话当时因为计算量的问题，所以它只能生成一些相对低清的一些图片。

比如说64×64的这样的一个图片，第三点的话就是呃，在当时还没有先驱者进行一个模型scanning up，包括在视觉上进行scale up的这样的一个验证。

也就没有验证scanning law能够验证呃，就是推动这个后续的发展，对虽然那个OpenAI是呃推出了GPT系列工作，但在IGPT上面它并没有follow up这个工作，也导致这个工作出来之后。

其实嗯嗯在领域内虽然有一定的影响力，但是并没有相关的一些呃，更好的一些工作或者改进对，然后回到我们刚刚说的TOGANIZATION，那它的TOGANIZATION，其实就在像素空间上进行聚类。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_19.png)

其实并没有太多的语义对，然后有了这样的一个嗯想法之后，其实TOGANIZATION最主要的一点，其实就是要把这个尤其是language model tokanization。

其实就是要把那个呃连续空间的一些特征，就啊映射到一个token i d space上，那很自然的想法就是VQVIE，那VQVAE呢，就是将输入数据应收到一个离散的code book。

那这些code book呢就是可以是可以去更新的哦，等等于啊对哦，这样的话就是VQVAE呢，我在那个呃latin space做了contact之后呢，可以得到一个具体啊具体的code book i d。

有了这样的一个code book i d之后呢，我相当于我一个图像就可以编码成，不同的一个啊一个一系列的code book，就像那那这个过程呢就和language model这块呃。

这块的BPE或者word piece基本是等价了，但是有呃近乎等价，我为什么说近乎呢，因为它可能语义上可能还差点意思对，有了这样的一个呃organization之后。

那我们就有了language model的一个啊优化的一个可能性，因为我们可以把图像从呃连续的空间上去去，映射到一个离散的一个code book上。

那我们就可以通过一个cross centrobloss，以及去大市场去优化它对。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_21.png)

所以我们这块就回到刚刚的landscape，我们可以看到，其实在呃前前面的一些呃，比较受关注的一些diffusion或者干前面啊，大家说的比较关注，但是在后面一个远处地方。

其实有auto regression models也渐渐受到大家关注，对，那这块就要介绍一下一个嗯一个比较有名的工，一个比较有名的工作对，然后这个工作呢就是VQ干。

VQ干呢是2021年的CVPR的oral，对这工作极大的影响力嗯，首先它是第一个基于image organization加auto regressive，transformer来生成图像的一个工作。

当然他没有做t two i是做class condition生成，然后它基于这个框架呢，它就能够生成一个非常高清的，比如720×1080，或者是呃1080×1920这样的一个图像，并且它可以做，就是啊。

这个这个模型可以做下游任务的一系列的，ZUH呃，就是呃implanting out painting，或者是一些呃就是嗯就是super鲁，是入选相关的一些呃呃一些一些下游任务验证。

那具体做法其实比较简单，就是说呃他其实做了一些功呃，我们可能认为表工程上的一些优化，首先第一点就是之前的就是嗯VQVAE，更多就是它它是在这个呃，卷ROR这部分用的是一个PSCN啊。

这部分的话就是VQ干呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_23.png)

就把它换成了一个选form嘛，g p t two的架构对，那第二呃，第二点呢，就是说他的discriminator加入了一个干LOS，然后同时啊就是perception loss。

替换成了一个重建落实对，那首先那这样的话，其实第一它改善了一个嗯，就是WEQ干这个化改善了一个呃，就是VQEVIE生成画质，因为VQVAE生成画质部分，有部分的明显的糊的现象对。

所以改呃加入了这个呃甘落实之后呢，它其实呃它的生存画质会有明显提升，第二点呢，就是它的又从pixel c n换成了这种AR，transformer这种架构。

从而的话就基于这些优化改进了encoder decoder，同时改进了这个generative的这个transformer，使得它生成有非非常大的一个提啊提升，但值得一提的是，它其实呃。

vo can并不是一个long range的这个a r model，它更多是一个slide window的这样的一个呃，基于slid window attention去生成，基于当时算力的一些因素。

所以他更没有去做这种long range的，AR的序列生成，就是现语言模型这块其实大家都应该提呃，可以关注一些开源的语言模型，都可以做到非常长的context length，但是其实受限于当时的环境。

wq gun它只能做到一个slide window里面生成，那这就有个约束，使得它生成图像其实不能够很好的CONSISTEN，对哦，并且它能很难去呃，就是比如说左上角能够去和右下角去进行一个。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_25.png)

一系列的一些优化，不行哦，对，然后当时的话是在一些学术，benchmark上做了一些验证，包括class condition的这种啊。

image net benchmark可以看到就是呃EMNET上的FD的话，它其实达呃得到了一个明显的一个提升，几乎接近于一个啊，比较早期的一个diffusion的best model，对啊。

包括如果他加了一些呃reject sampling之后，他在FID可以达到一个6。59，这样的一个效果，对，已经快超越了一些呃瓦尼拉的devision model，那另外一个工作呢就是一个呃呃估呃。

蒂夫麦的一个工作，对这是来自DIF麦的，当时研究员就是余家辉老师的一个工作，那这个工作呢其实就是说哦，我我看到了语言模型的一个scanning up的效果，那我是不是可以直接scale v q gun。

这个这种框架，那显很显然是可以的，对它框架也比较简单，就是说呃就是基于一个呃image统IZATION，vi it的维修gun，加上一个auto regressive transformer。

这个工作呢，其实呃就是很就是典型的有点像open AI的风格，就不停的堆算力，堆模型，Size，堆数据，然后我我模型架构很简单，就是这个呃TOGANIZATION加AR这个路线。

那这跟language model几乎一模一样了，对在这工作是在2022年的呃，上半年提出来的，在这个呃ch呃ChatGPT呃，受大家关注之前。

在当时那个年代有人去scaling这个t to ADD model，或者数学生成model是非常难得的，所以这工作，我认为是一个非常具有里程碑式的工作，对他他也是第一个把t to i上。

scaling到20B的这样的一个model，并且是把t to i做了非常work的一个工作，当时呢他就是也是呃超前的思想，他用了一个MOE的model，去做到这个20B的这样的一个呃。

VIT或者是一个a r transformer的架构，然后他用的也是MOE的这样model，对那随着他文文章中做了一些APPLATION，随着model size变大，那它效果会越来越好。

并且可以做到一些text rendering的效果，所以我我认为这个工作，可是在当时的是思想非常超前，对它具备了，现在我们能看到一些lanlanguage model，一些非常多的一些优势，包括MOE。

包括一些scaling up，在2022年的上半年的当时对。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_27.png)

然后有了我有了我们刚刚说的就是啊，TOGANIZATION呃，包括视觉的VQVAE，包括一些language model，Scaling up，包括一些相关的工作之后，我们就在想那这个事情对视呃。

视觉这块一定要follow这个AR这条路线吗，啊其实AR这条这个东呃，这个东西对视觉来说适用吗，其实我们也在内部不停的去探讨或者思考，这样的一个想法，就有了我们这样的一个工作。

微距auto prograss，Model，Next scale prediction，对语言模型像那个g p t la或palm，他是BPE之后经过next token predic个选对。

然后像party这种，它简简单的就是一个呃，就是呃v q v i e to nezation之后利用光啊，也是跟原模型一样，自上而下，自左到右的这样光栅顺序。

但language model是用自回归的方法来预测next token，那是因为语言有先后顺序时候区分，因为语言是一个一维的context，但视觉其实并不是这样，因为视觉我们看东西它是一个整体的。

或者是局呃，整体到局部的这样一个过程，所以我们就在想呃，传统的图像自回归使用一种不符合人类直觉，但是呃呃符合一些计算机处理的顺序，自上而下逐行扫描这光栅顺序来预测图像，token这个真的合理吗。

就是这个地方可能要打个问号对，那我们就在想呃，呃就像party这种，就是我们刚刚说的一个language model的AR全AR，auto regressive这样的一个生成的框架。

那其实我们人看东西一般都是我们从远看东西，会看到一个呃整体的东西，然后慢慢走近，我们会看到整体的这个物体，或者是一个图像的，整体的一个整体到一个局部的这样细节，逐类似逐步放大的过程。

这是比较符合人类直觉的这样一个过程。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_29.png)

同样的人类在感知图像或者绘画时，往往往往都是先概览全局，再深入细节，这种由粗到细，由整体把握，整体到局部金条的思路啊，思路想的话是非常自然的，有了这样的一个想法之后，那我们就在想，能不能我们在呃。

就是我能不能我们同时借鉴language mode优势，for ganization加AR的方式，去融入计算机视觉的一些诶优一些特特质，包括我们刚刚说的从整体到局部的这个思路，那我们逐步放大这种思想。

那就有了我们这个V9，auto grave model里这样的一个想法的初步，他就是说呃我们可以去逐步的去看这个图图啊，图从慢慢的把图像看整体逐步放大这样子呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_31.png)

放大这样的一个过程对，然后我我接下来会介绍，具体来说我们是怎么做的，对，首先的话就是呃我我刚刚说了，其实呃我我框架的话，其实我我像这个AI的框架，其实主要是两个组成，一个是DOGANIZATION。

第二个的话是呃，第二个是它的一个ARHANSFORMER，那自然的我们也是一样的，那stage one的话，就是我们需要有一个matter scale的一个image，Organization。

为什么要MARGISCALE呢，因为我们是从一个整体到局部的，所以这个图和NAZATION，必然是它把握一个单尺度到多尺度上的，一个整体的一个呃，有一个一个描述对。

然后第二个stage呢就是说我们会有一个呃，就是g b t style的，像auto grave啊，这样做model来生机啊。

就是来生成这样的一个mari scale image organization，或者mari scale的这样的v q tokens，然后我们去逐步生成这样高清的这种token。

然后最后通过一个organization decoder去还原出来，对那那具体来说的话就是说我们现在有两个station，那第一个station呢就是说我们会有一个呃，我们需要对图像进行一个多尺度的。

这样的一个图呃，呃to ganization，那就是说我们对图像我们先把它一个啊，就是需要转化成一个多尺度的这样的一个，离散的token map，那比比如说它是一个啊多个。

比如说呃就是呃从大概7~8个尺度上，举个例子对，然后他有7~8个尺度上分别做出GANIZATION，那这样的话它有个多次do的token map，然后这是第一步，那第一步离散编码。

第二步的话就是我通过一些呃，就是code book转化成连续的这样一个feature map，然后统一插值到嗯，就最大分辨率上去求和，然后求和后求和后的fish map呢。

通过一些呃就是organization的一些decoder，去重建图片，并且通过重建呃感知和对抗这三个loss，就我刚刚说的一个reconstruction loss。

perception loss和那个gloss来混合训练，训练，这样的一个mari scale的一个一个VQVA啊，V q v i e，那有了第一步之后，那我们就是在想，如何在视觉空间上去自回归的生成。

那很简单，我们一般第一步呢是通过一个起始token，去测出第11的token map，如左上呃，呃就是啊这部分一样，就是我们首先得到一个11的token map，随后每一步呢。

VR呢都会基于历史的所有token map，去预测下一个更大尺度的token map，这种cost to refine的思想，对，那有了to conanization之后呢。

训练阶段就可以使用标标准的一些，交叉熵的损失呃，呃损失loss来监督这些token map的概率预测而产，对啊，这样的话我们就可以看到逐步流程，就是我先生成一个呃，第一个token11生成一个呃。

22的这样的一或者44的token map，然后再生成99的这样的一个token map，注意的是它是一个每个scale上，是一个并行生成的。

但是在scale上是它是一个COSTAL的attention，对那测试阶段的时候，我就可以通过采样得到token map结，结合一些VQVAE的decoder，进行连续化的这种呃连续化这种差值求和。

再通过decode最后生成完整的一个图像，当然里面有很多细节，包括我们借鉴了一个呃就呃rescue呃，Transformer，就呃就是嗯r q transformer，或者RQVAE的思路，对。

包括我们呃借鉴了那个呃就是一啊，借鉴了一些就是嗯DIT的一些架构的，一上的一些一些经验，对，我们可以看我们在标准的benchmark上的一个结果。

首先我们可以看到就是标准的class condition，image net benchmark上，我们测试了不同的model size，结果随着不同的model size。

结果SK呃scaling之后，我们的FID是逐步稳步的下降的，并且我们的这个FID是达到了历呃，达到了SA比比之前的所有的diffusion base model呃。

mask prediction based model呃，AR的呃全呃就是呃AR的选form为based model，都是达到了更好的FID，并且我们呃就是几乎快接近VDATION的FID，这是第一。

第二的话，我们在标准的英internet，512×512的这样的一个呃卡，class condition generation上达到了也不错的一个效果，对。

也会也比之前的master git或者啊DIT的这种呃，affect达到更优，值得一提的是，我们的VIR的框架嗯，就是呃会比传统的这个晚就晚依赖的这种呃，这AR框架在FID上几乎提升了啊，一个数量级。

对这第一点就是我们达到了SOA的performance，在immnet benchmark上对，第二点我们比啊solar的呃，solar的base model会更好，对，第三的话就是呃我们会呃。

我们是一个非常非常快速，因为我们step比较少，所以我们实测的话在1024×1024上，我们我们如果优化的够好的话，可以到呃，一到两秒生成10241024这样的model对。

然后我们也和solar或者stable diffusion的呃，这个贝斯mod d i t做了对比，可以看右上角，在我们的一个奔驰Mark上的一个FID，包括我们呃左上角的话和一个呃。

就是它的一个呃不同的model之间的一个呃，就是FID和，FID和速度，速度的一个对比的一个一个一个表，我们可以看到经过scanning up之后，VR可以达到一个FID。

当然最新的结果我们会更呃更好一些，对他毕竟理论上的一个FID的下限，要要1。78，显著优于DIT当时的效果啊就是2。1对，第二就是我们的速度更快，VR的话只需要不到0。3秒。

就可以生成一个256×256的图像，速度的话是当时的一个呃呃，瓦INA的DIT的45倍，在512上，更是DIT的一个呃一个数量级的一个速度，第三的话是我们有更好的scanning的一个能力啊。

如左如左图所示，DIT在大模型增长到3B7B之后，出现饱和现象，我无法靠近FID下线啊，对所以呃然后我们做了一个VR上，做了一个嗯scanning up的实验，包括它scanning到一个20亿的参数。

性能不断的提升，对哦另外一点就是我们有更高效的数据利用，包括我刚提的VR的话，需要350个epoch就能超进，就能超过DIT1400个epoch效果对。

然后我们也验证了AM上的一些scanning law，对我们验证我们在验证集上的错误率呃，就是验证了token的错误率和crossing topy啊，ROSS随着啊啊。

就是我们scarf模型的一个size，和这个计算量之后，可以得到可预测的下降，这可预测是指我们呈现密率关系或者log，收放后的线性关系，线性关系的话就是把线性相关系数达到啊，就非常高，对咳啊，同样的。

我们去做了一个是刚刚是一个定量量的分析，我们也做了一些定性的分析，我们可以看到啊，左上角或右呃，右边这个图可以看到，随着我们不停的scaling up呃，呃比如说从左到右是scaling up。

Training computer，从上往下是scaling up，这个呃model size我们可以看到就是我们的呃不呃，从横轴呃往右竖着往下的话，我们的model s我们的生存能力会得到逐步的提升。

当然右下角是最好的典型case，就可以看到这个脑电波图对我们不停的去呃，训更久的model啊，包括skating up model size，这个效果会达到呃肉眼可见的提升，对啊。

最后我们这边也做了一些呃zero shot generalization，当然这是一个初步的实验，我们可以在呃一些class condition的啊，上训好的一个VR的全ANSFORM嘛。

在不通过任何的微调的基础上，去泛化到一些生存的任务上，包括一些implanting or painting，和一些class condition editing，这是一些初步的一些实验对。

然后我我总结一下，就是说我们使用了一个多尺度自回归的范式，和基于这个next scale prediction的这样呃，构建了一个全新的生成框架，为视觉的自回归算法提供了一种新的思路。

对第二的话就是VR模型的skin law和zero shot，zero shot转ALIZATION实验验，证，来，来，就是来学习大语言模型所具有的一些优秀特质，对第三点的话。

是我们视觉自回归模型的一个性能突破啊，使用这种典型的GBT风格的次回归的呃，呃就是方法在图像呃生成中，首次超过了这种强大的这种debution model，包括DIT，最后就是因为呃就我们开源了。

就是啊啊就是这个VR的所有的代码，包括v q to ganization和这个呃，呃就是auto aggressive model的这种训练，就来推动这个事啊，就是离呃，离散空间上表示的这种。

视觉智慧规或智慧规范式的这种学习的进步，因为我们知道，就现在VAE或者VQVAE的这种社区啊，其实做的不是很好对，所以我们希望推动这个离散的空间，这个表示的这样的一个呃社区的优化。

可以看到我下面给了个示意图，我们一张图从cost to refine的时候，申请的时候逐步变得高清哦，这是一个在离散空间上去做微觉，就呃visual to gressive。

也就视觉智回归的这样的一个demo对，然后从一开始一个token，到后面的一个1616的token对，然后我们也对比了，就是一个VR和AR和diffusion，以及master get方法的一些比较。

可以看到就是说AR本质上是这种next token，Predict，prediction的话，学习数据内部的某种分布或秩序，那文本它天生是从从左到右的这种因果顺序，从而达到了数据和算法上的一致性。

催生了AIM的这样的一个极大的成功，但是图像或者图片并不难，并并不这样，图像自上而下逐行扫描的顺序，其实并非图像的这种最自呃最自然的顺序，所所以我们感知啊，图就是我们看图像或者看绘画呃。

我们绘画的时候是按照这种由粗到细，由低频到高频的逻辑顺序，这是比较合理的，因此VR观测到了更好的一个呃，一个性能和更合理的生育速度，更完备的这个scanning law对，然后我们也OAR发。

克服了一些AR图像生成一些泛化问题，比如说根据图像的下半部分，来补全的一个上半部分，因为他在训练的时候没有根据嗯，没有没有没有这样的setting对，然后我们也和diffusion model做了对比。

可以看到VR的noisy的方方式更加直观，可解释，因为它是一个模糊到清晰，低频，低频到高频的这样一个过程，第二就是diffusion呃，你diffusion的话就是可以做更多的一些粉呃。

就是呃就是呃distribution的一个呃拟合对，那VR的话它的学习会比diffusion会更加高效，因为它只需要大概17的epoch对，然后和LM类似啊，VR的话是一次向前同时训练所有的事件铺。

但是diffusion的话是每次训练一个time time time step对，所以的话就是说呃VR的话，同和呃和DEVISION都是这种呃多部REFINN的机制，然后修复过往时间不的错误。

但是AR的话生成之后他没有办法ref对，在这样的一个框架下，可以看到就是呃，VR和AR和diffusion和must get it，然后我们来看一下master get的区别。

而VR的这种从cost to refine这种呃方式，have的schedule更加直观和解释，通过小尺度的大尺度，然后得但是must get it是使用贪心的算法思想。

然后VR和diffusion的话都允许这种MARI呃，呃就是mari staff的refine，来修复过往时间步的错误，但must get it是不难对，然后VR和musket都啊有些类似的啊。

就是像就是它的一个速度是是很相近的，但是VR的话更加接近language model，会啊对未来的一个LANGU呃，就是language model的系统一会走走向了更近的一步。

同样的就是我们把我们的一个demo呃呃也开源了，包括model和呃呃checkpoint也开源，到目前为止，大概现在已经有3700个呃，github star对，可以看到就在我们开源之后之后。

以呃呃一个月其实就已经啊涨了啊，就是就是呃GITHUB上涨的非常多，对这块是一个二维码，大家可以关注扫描就是对，然后另外就是我呃VR开源之后，得到了非常就是呃，就是领域内的很多专家的一些关注。

他们会给我们发邮件，或者是啊通过呃各种方式联系到我们，希望去关注我们的嗯，去VR的下一步对，那后面的话我也会介绍，就是我们VR的话，就是说现在也在也在follow。

这个最新的这个t to i的model，并且我们希望把model size sc进到更大。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_33.png)

对，然后最后一个section的话，我介绍一个我们未来可能会做的一些呃，一些工作，包括text to image，包括text to，可能text video包括一些啊，就是我们要走向。

因为基于图NOZATION走向了未呃，未来统一的这样的mari modity model，对，然后我我们我们从现在的一个呃，呃就是视觉生成到一个多模态的，这样的一个智能来看的话。

就是呃语言模型目前已经能够去做深层和理解，但是视觉这块其实看可以看到，其实它已经分分的比较远了，包括就是嗯就是啊WI距understanding和VIDEGENERATION。

现在都是不同的model来做，对，现在微卷understanding的话，更多的是一些基于language model的语言模型啊，多模态语言模型。

我VIDEGENERATIONAL更多是通过diffusion model。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_35.png)

这里也列了一些代表性的一些工作，包括就是啊像上面是典型的一些语言模型，lama呀，或者GBT系列或者全区的呃，比较早期的卷曲的序列对，然后呃可以看到就是但是视嗯嗯视觉这块呢。

其实现在有一些出现了一些统一的工作呃，相走向统一的工作，像email或者是next e p t，它它是把diffusion model和language model呃，连接在一起来做的对。

然后包括呃就是呃左边的话就是一些啊，多模态的一些理解类的一些工作，包括g p four v或者larva对，比较早期的话是flamingo对，然后我们在想。

就是说有了这样的一个统一的TOGANIZATION之后，那我们其其实走向统一是一个必然的趋势，我们在理呃，呃在一个离散的空间上可以做去COCHIN对。

包括这个next token prep prediction，或者是呃嗯next scale prediction，可以通过通过ANIZATION的方式呃，就是呃就是就走向统一对。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_37.png)

然后我们可以看到就是最新的GPFO，它是可以做到类一模代的模态的模呃，就是类的呃MODITY的这种输入和另1MODITY的输出，可以看到它生成图像已经非常丝滑了，大家推测它可能是第一个或者是任意模态的。

Tonization，可能是离散空间上的一些表示，从而的话他能够做到一个啊统一的这样的，一个统ALIZATION上呃，一个多模态的生成对，这是我们能看到的一个呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_39.png)

一个疑似的这样的一个证据对，那另外一个呃，就是说最近的那个meta发布的一个TRAMON，对哦，他就是第一呃，应该是我们能看到的第一个在一个PRETRAIN阶段。

它不再是在language mode做next token prediction，而是对视觉和language mode分别做了token，来谁选之后，分别做这个next token position。

这个next token的话，包括图像token或者文本token去COCH，那它训出来的这种语言模型啊，就或者动态模型不仅具有语言模型能力，还具有生成模型的能力，方法非常简单。

就是比较就有点就是呃就重剑无锋的这种感觉，对可以看到右右边这个demo，它其实能在对话中生成图像，也能理解图像，从也能做到自呃自然呃语言的生成和理解对，基于这些，我们就在想。

就是说既然我们哦有了就是刚开始说的，就是有了TOIZATION之后，这个TOKIZATION可呃如果是离散的，那么它就能够language model cochin，然后join the train。

然后去达到一个更高的天花板，当然前提是呃我们算力是非常非常足够的，因为CHAMBON这个实验它其实呃第一个实验的话，大概3B还是7B的model，大概需要用呃，大概需要用1000块呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/4e3c9f817cee7965770f13d5bfc80700_41.png)

呃HH100对，然后最大的model的话用了4000块H100对啊，具体时长时长是没有啊，没有透露对，然后所以我们可以看到未来多多模态模型嗯，或者这种统一的模型走向这种离散的，TONIZATION的呃。

这种呃这种PRETRAINING是一种，我感觉是一种必然的一种啊一种趋势，它在未来也许能达到一个像纯生成上，理解上能达到最最优的，同时能在深层上能够比肩，diffusion这种离散的啊。

diffusion这种连续的这种表示对，这是我个人的一些看法对，然后呃我的分享就到这了，对谢谢大家，然后对好啊，我们感谢这个呃姜毅老师非常非常精彩的报告，我们还是留一点点QA的啊，好的。

好谢谢谢您的分享，就是我有一个稍微偏技术细节一点的问题，就是您刚刚说那个呃那个VR是呃，每个scale都是每一个skill是那个并行去预测的，但是transformer里面呃。

像gt这种一般都是预测下一个token，就是我不太清楚这个并行去输出啊，多个token这个是怎么做到的啊，其实这个就是类似class token，你去直接去预测它，并不是说next token。

Prnext token，PREDIC个选去做的，就是像BERT这种就是呃多个token并行的事物，它们之间可以相互看到，并没有存在着上下文的这种前后关系，对。

也是说在输出的时候是加了类似于query token，或者mask token这种对你可以这么理解对，但在时间上它是一个cos的，对哦行行好，谢谢好，那我们还有一个问题的时间，嗯老师你好。

我之前我有个问题，就是说VAR，那它相比于LDM它一个显著的区别，就是他把那个图像建模成了一系列那个嗯，不同尺寸的这个token之间的联合分布，而LDM是在空间图上的，它那个token之间的联合分布。

那就是在这样嗯不同尺度情况下，就是说传，比如说control net的那种谷歌图的引导控制生成，我们在这种不同尺度下的嗯，引导的话就有有这方面的探讨吗，呃嘶呃其实是有在VR出来之后。

有个有有有一个follow up的工作，呃是是是有的，我我我是我发现了那个有一些基于这个VR，做一些control net，或者是做一些editing相关的一些工作，呃相呃会后的话我可以发你看一下对。

但不是我们所做的，好的呃，我我们会有中场休息吗，没有是吧啊哈没有，我们要不就就继续吧啊我们就继续吧，好我们再次感谢呃姜毅老师精彩的报告，呃，我我们现在时间嗯，那好最后一个问题。

最后一个问题真的最后一个问题，老师我想问一下，就是因为您刚才我看刚刚听您讲那个VR，我感觉非常非常的那个呃，就是特别特别的感觉有价值，因为他这是在那个次回归式建模上呃，感觉是大概是击败的fusion。

感觉特别是像DIT这种模型，感觉还是特别特别有前景，然后老师我想问一下，就是你后面有没有考虑在这个视频生成方面，去进一步的去探索你这个VR的这个架构，对呃是这样，就是我觉得呃就是我觉得你说的非就是是呃。

是我们正在呃下一步可能要做的一个方向对，因为长视频生成是大家目前比较关注一个问题，就是呃呃就是目前长视频的话，那可能token序列会很长，也没有办法再塞到一个。

就是即使你可能用sequence Pdd的方式，用几千块卡去去做，但是可能有些有些场景下，你可能需要需要生成，有几个小时或者更更长视频，这种情况下，你就可以通过一些AR或者是呃这种方式来做。

会更加或者VR这种方式来做，或language model方式来做，会更加make sense，对嗯嗯嗯嗯嗯好。

