- en: P10：10. L2_5 Naive Bayes - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P10：10. L2_5 朴素贝叶斯 - Python小能 - BV1CB4y1U7P6
- en: And stuff。 So， naive base。 I guess， OK， who's heard of naive base before？
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他东西。 所以，朴素贝叶斯。 我猜，好的，谁听说过朴素贝叶斯？
- en: So the reason why I'm covering it is twofold。 First of all， it's a ridiculously
    simple algorithm。 Secondly， to also demonstrate that you can use the tools， like
    these deep learning frameworks。 to do other things than just neural networks。
    OK。 So what's the key assumption in naive base？ Well。 I'm assuming that P of word
    one， two of word in， given spam。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我讲这个的原因有两个。 首先，它是一个荒谬简单的算法。 其次，也是为了展示你可以使用这些工具，比如深度学习框架，做一些除了神经网络之外的其他事情。
    好的。 那么朴素贝叶斯的关键假设是什么？ 好吧。 我假设给定垃圾邮件的情况下，P（第一个单词，第二个单词，...，第n个单词）。
- en: factorizes nicely into P of W i given spam。 OK。 Straight forward。 And so then。
    if I make this assumption in P of spam， given the words， my base rule is just。
    given proportional to P of spam， times the product。 Right？ The only thing missing，
    and that's。 why we have the proportional， is the denominator。 And I'm going to
    be lazy and recompute the denominator。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它很容易分解成给定垃圾邮件的W i的P。 好的。 很简单。 所以如果我在给定单词的情况下做出这个假设，那么我的基本规则就是。 给定与P的比例，乘以乘积。
    对吧？ 唯一缺少的，就是。 这就是为什么我们有比例符号，缺少分母。 我会懒得重新计算分母。
- en: afterwards by just evaluating this right inside P of spam， times the product，
    for both spam and ham。 OK。 Actually， let's see this。 OK。 So what we need to do，
    therefore， in order。 to train our naive base classifier， are two very simple things。
    The first thing is we need to compute P of spam。 Well， that's easy。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，只需在P（垃圾邮件）内，乘以乘积，然后对垃圾邮件和正常邮件进行评估。 好的。 实际上，我们来看看这个。 好的。所以我们需要做的事情是，训练我们的朴素贝叶斯分类器，需要做两件非常简单的事情。
    第一个是我们需要计算P（垃圾邮件）。 好吧，这很简单。
- en: I just look at all the emails that I'm getting。 I count how many spam， how many
    ham I have。 That's a pretty good estimate of P of spam。 And then， I care about
    P of W i given spam。 So I just look over the number of times the word， biagra
    occurs in spam and in ham emails。 And unless you're doing biological research，
    chances are that's a pretty significant feature。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是看我收到的所有邮件。 我数一数有多少垃圾邮件，多少正常邮件。 这已经是一个相当不错的P（垃圾邮件）估计了。 然后，我关心的是P（W i 给定垃圾邮件）。
    所以我只需要看看“viagra”这个词在垃圾邮件和正常邮件中出现的次数。 除非你在做生物学研究，否则很有可能这是一个相当显著的特征。
- en: So the problem is if you have a naive base， plan of M-fulfurger， the assumption。
    is that a lot of those things will be of equal probability。 In other words。 those
    phrases make absolutely no sense whatsoever。 But since we're looking at individual
    word probability， as well， you get nonsense。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题在于，如果你有一个朴素贝叶斯的M-fulfurger计划，假设是很多这些事情的概率是相等的。 换句话说，那些短语根本没有任何意义。 但由于我们也在看单个单词的概率，你会得到一些无意义的东西。
- en: So that should already tell you that maybe naive base， is a little bit too naive。
    So here's the graphical model。 But if you have never seen one， this is really
    just what。 depends on what is called the director graphical model， spam， and then
    the various words。 given spam。 So the assumption is that our friendly Nigerian
    spammer。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这应该已经告诉你，也许朴素贝叶斯有点太朴素了。 这是图形模型。 但如果你从未见过，这实际上就是一个所谓的有向图模型，垃圾邮件，然后是给定垃圾邮件的各种单词。
    假设是我们的友好的尼日利亚垃圾邮件发送者。
- en: sits at the keyboard and has a button for every word。 or that he will generate
    with a certain probability。 And the emails make no sense。 OK。 Have you ever seen
    this in practice？ Well， at some point， there was a spam attack。 which would have
    a completely spammy payload。 And then in the last line and the first line。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 坐在键盘前，每个词都有一个按钮。 或者他会以某种概率生成这些词。 但这些邮件完全没有意义。 好的。 你有见过这种情况吗？ 好吧，曾经有一次垃圾邮件攻击。
    它会包含完全垃圾的内容。 然后在最后一行和第一行。
- en: there would be random keywords that were not very spammy at all。 Just sprinkled
    into it。 And the goal of this was to utterly confuse， the naive base spam filter
    and make the email go through。 And for a while， it succeeded because basically
    that spam， filter was utterly naive。 By the way。 obviously， statisticians， don't
    like drawing diagrams like the one on the left。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 会有一些随机的关键词，它们根本不属于垃圾邮件。 只是被随机撒进去。 这样做的目的是完全迷惑朴素贝叶斯垃圾邮件过滤器，并让邮件通过。 一段时间里，它成功了，因为基本上那个垃圾邮件过滤器实在是太朴素了。
    顺便说一句。 显然，统计学家不喜欢画左边这样的图。
- en: It's tedious to draw those circles。 So they've invented their own for loop。
    And that's the one on the right。 It's called a plate。 So that just means for all
    i going from one to end， do this。 Things get a bit more interesting。 if you have
    two for loops that are interlocking。 And then you can do things in stats。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 画这些圈是很繁琐的。所以他们发明了自己的 for 循环。就是右边这个，它叫做“板块”。这意味着对于从 1 到 n 的所有 i，执行这个操作。如果你有两个互相嵌套的
    for 循环，事情就会变得更加有趣。然后你可以在统计学中做一些操作。
- en: that aren't so easy to do on a computer。 Unfortunately， if you need to code
    it up。 you then need to bite the bullet and deal with this。 So for instance， for
    recommended systems。 that you have for all users， for all movies。 But then you
    really only look at the interactions。 between users and movies。 Anyway， this is
    just a short detour。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事情在计算机上做起来并不容易。不幸的是，如果你需要实现这些，你就得硬着头皮去做。例如，对于推荐系统，你需要为所有用户和所有电影构建一个系统。但是你实际上只会关注用户和电影之间的交互。总之，这只是一个小插曲。
- en: Take a graphical models class if you're interested in this。 It's pretty cool
    stuff that you can do。 So what do we have？ Well， we have data like emails， labels，
    and maybe images。 And we need to get those probability distributions。 Mind you，
    this grows oversimplification。 This is just the header of an email。 The text starts
    at the very bottom。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个话题感兴趣，可以去上图模型的课程。这里面有一些非常酷的内容。那我们有什么呢？我们有像电子邮件、标签，甚至是图像这样的数据，我们需要获取这些数据的概率分布。请注意，这里做了过度简化。这里只是邮件的头部，文本实际上在最下面。
- en: Everything else up there is essentially， metadata of what happened to this email。
    as it was sent from the sender until it， got to your mail host wherever that may
    be。 And all of this information is useful to figure out， whether the email is
    legitimate。 So how do we build our very naive， naive base classifier？ Well。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 上面其他的部分基本上是元数据，记录了这封电子邮件的处理过程，从发送者发送到邮件主机，再到它到达你的邮箱。所有这些信息对于判断邮件是否合法非常有用。那么，我们如何构建非常朴素的朴素贝叶斯分类器呢？嗯……
- en: we want to get the feature probability。 In this case， I'm only going to care
    about binary。 P of xi equals-- is true given y。 So I just count the number of
    times。 that this holds divided by the total number of occurrences。 That gives
    me the probability for a feature i。 And I do that for all i。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要得到特征概率。在这种情况下，我只关心二元的情况。xi 的 P 等于——在给定 y 的情况下为真。所以我只需要统计这种情况发生的次数，然后除以总发生次数。这样就能得到特征
    i 的概率。我会对所有的 i 都进行这样的计算。
- en: I also need the spam probability， and then I'm done。 Sounds good。 Can you see
    a rather fundamental problem with this。 besides the fact that this assumption
    is horrible？ Where could this go wrong？ Yes？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我还需要垃圾邮件的概率，然后我就完成了。听起来不错。你能看到这个方法中存在的一个根本性问题吗？除了这个假设非常糟糕之外，这种方法会在哪里出问题？是的？
- en: If you have no occurrences of a particular feature。 Exactly。 So if you have
    no occurrence of a particular feature， then you'll get the done in Kruger effect。
    the cruel end， of the naive base classifier， where the classifier。 will be utterly
    confident in something。 And it'll just say one or the other。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个特征没有出现。没错。所以如果某个特征没有出现，那么你就会遇到克鲁格效应。这是朴素贝叶斯分类器的致命问题，在这种情况下，分类器会对某个事情充满自信，并且会直接给出一个明确的分类。
- en: Things get worse if you have one feature that always occurs， and that never
    occurred。 And then on the test data， you have the reverse。 And then you just get
    not a number。 So you're absolutely right。 This is when things can go horribly
    wrong。 So what people have done。 therefore， is they've just added the pseudo count
    to everything。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个特征总是出现，而另一个特征从未出现，然后在测试数据中，情况正好相反，那么你会得到一个“不是数字”（NaN）的结果。所以你说得完全正确，这就是事情可能出大问题的时候。所以人们做的事情是，他们给所有的东西都加上了伪计数。
- en: They've just incremented all the numbers by one。 This is called Laplace smoothing。
    What is interesting in this？ There is a lot of base in non-parametrics。 Chinese
    Western processes。 Dirichlet processes， and so on。 So take a class of two impitments，
    and he will cover this in。 I hope， a lot of detail。 So it's even a process named
    after him， the Pittman-Yor process。 Yep。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 他们只是把所有的数字都加了一次。这叫做拉普拉斯平滑。这里有什么有趣的地方呢？在非参数化的贝叶斯方法中，有很多相关的内容，比如中国西方过程、狄利克雷过程等等。所以，去上两位学者的课程，他会详细讲解这些内容。甚至有一个以他命名的过程，叫做
    Pittman-Yor 过程。是的。
- en: We are going to barely scratch the surface。 Now， as said， here's a very simple
    algorithm。 For all the documents， aggregate the label counts， aggregate the various
    counts。 and then we'll look at that in code。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅仅触及表面。现在，正如所说，这里有一个非常简单的算法。对所有文档，聚合标签计数，聚合各种计数，然后我们会在代码中查看这个过程。
- en: '![](img/f8c286c4b36ffca282f912452df59dce_1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8c286c4b36ffca282f912452df59dce_1.png)'
- en: Don't worry。 But before we do that， now for something completely different。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心。但在我们这样做之前，现在我们来讲点完全不同的内容。
- en: '![](img/f8c286c4b36ffca282f912452df59dce_3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8c286c4b36ffca282f912452df59dce_3.png)'
- en: And you wonder why that？ So this slide seems to be utterly unrelated。 to what
    I just told you before。 Why am I all of a sudden diving deep into floating point，
    numbers。 exponents， mantises， and other things？ Well， the thing is， so floating
    point numbers have--。 that's why they're called floating point numbers。 They have
    basically some part that。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么会这样？这张幻灯片似乎和我之前告诉你的内容完全不相关。我为什么突然要深入讨论浮点数、指数、尾数等问题？其实，浮点数有——这就是为什么它们叫浮点数的原因。它们基本上有两部分。
- en: specifies precisely what the value is。 And then you have another term that。
    specifies precisely what the order of magnitude is， the largest exponent of form
    is the mantissa。 And then， OK， you need one extra bit for the sign， but that's
    about it。 So if you have doubles。 then you have 64-bit， and life is good， and
    usually you don't get numerical level flows or underflows。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一部分精确地指定了值是什么，另一部分则指定了数值的数量级，也就是最大的指数部分是尾数。然后，好的，你还需要一个额外的位来表示符号，但就是这些。如果你使用的是双精度数，那么你有64位，生活就比较美好，通常不会出现数值溢出或下溢。
- en: If you have floats， you have 32-bit， and you maintain any exponent shrink。 That's
    still OK。 If you have 16-bit， things get really awful。 And then if you go to 8
    and 4， and explain to you。 in the next slide why this matters， you need to do
    a lot of tricks to make it work。 OK。 So what are you supposed to do in this case？
    And by the way， so the place where it occurs， is。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是单精度数，那么是32位，并且你会维持任何指数的缩小。这还可以。如果你使用16位，情况就变得很糟糕了。接下来，如果你用8位或4位，我会在下一张幻灯片中解释为什么这很重要，你需要做很多技巧才能让它工作。好吧，那么在这种情况下该怎么办呢？顺便提一下，发生这种情况的地方是。
- en: for instance， if you're taking products of probabilities。 Each probability might
    look rather innocuous。 Let's say it's 50%。 And then all of a sudden。 you get 100
    of those numbers， and then you get 2 to the minus 100， which is about 10。 to the
    minus 30。 So things get awful very quickly。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，如果你在计算概率的乘积，每个概率看起来可能都没什么问题。假设是50%。然后突然间，你得到了100个这样的数字，最后你得到的是2的-100次方，大约等于10的-30次方。所以事情会迅速变得非常糟糕。
- en: And if you're adding things and normalizing and so on， you will get overflows，
    underflows。 not a number， and your code will not run。 So the culprit is that we
    are basically。 stuffing all the information into the exponent， and the matisse
    is pretty much unused。 So in other words， the trade-off， when the double storage
    format was defined。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在加法和归一化等操作时，会出现溢出、下溢、无效数字等情况，这样你的代码将无法运行。所以罪魁祸首是我们基本上把所有信息都塞进了指数部分，而尾数部分几乎没有被使用。换句话说，当双精度存储格式被定义时，所做的权衡。
- en: is the wrong one for dealing with probabilities。 So what are we supposed to
    do？ Well。 it's actually not that hard。 We just take logarithms。 Now all of a sudden。
    multiplications become additions。 So everything's great。 The only thing we need
    to do is。 because when you add， probabilities， you need to exponentiate things，
    and then take the log again。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 处理概率时，使用这种方法是错误的。那么我们该怎么做呢？其实并不难。我们只需要取对数。突然之间，乘法变成了加法。所以一切都变得简单了。我们唯一需要做的就是，因为加法涉及到概率，我们需要对其进行指数运算，然后再取对数。
- en: then things could go bad。 So let me quickly write on the whiteboard what the
    issue is。 We want to compute log of e to the a plus e to the b。 Now。 let's say
    a and b are in the order of about 100。 This will give us horrible numerical overflow。
    so we get inf， and inf here， and log of inf is inf， so things went badly wrong。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后事情可能就会变坏。那么让我快速在白板上写出问题所在。我们想计算e的a次方加e的b次方的对数。现在，假设a和b大约是100级别。这将导致严重的数值溢出，所以我们得到的是inf，这里的log
    inf也是inf，因此事情就彻底出错了。
- en: But what you can do is， you can immediately， see that this is the same thing
    as a plus the log。 of e to the a minus a。 So that's 0 plus e to the b minus a。
    This is potentially much better。 It's much better if b is less than a。 If b is
    larger than a， I can still get my overflow here。 and everything goes wrong。 So
    what I simply do is， I take the maximum of b and a， pull that out。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可以立即看出，这和a加上e的对数，减去a是一样的。 所以这是0加上e的b减去a。 这可能更好。如果b小于a，那么效果会更好。如果b大于a，我仍然会得到溢出，所有事情都会出错。所以我简单地做的是，取b和a的最大值，提取出来。
- en: and I subtract it on the inside again。 And this way， everything is numerically
    stable。 This is not just for deep learning， but also if you're doing graphical
    models and other things。 be very careful with this。 Otherwise， your code may actually
    not work。 So why do we care？ Well。 because on GPUs， essentially the speed goes
    as follows。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我再次在内部进行减法。 这样，所有的计算都在数值上是稳定的。 这不仅仅适用于深度学习，如果你正在做图模型和其他东西，也需要非常小心。 否则，你的代码实际上可能无法正常工作。
    那么我们为什么关心这个呢？ 因为在GPU上，速度基本上是这样发展的。
- en: Every time you have the floating point or the numerical， format by a factor
    of two。 things get twice as fast。 It's not entirely true because then there are
    also things。 like tensor cores that make things even more effective。 But you can
    basically think of it this way。 for a given bandwidth constraint， bandwidth budget。
    I can pack two or four times as many numbers。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你将浮点数或数值格式增加两倍时，事情就会变得更快。 这并不完全正确，因为还有一些像张量核心这样的东西，使得事情更加高效。 但是你基本可以这么理解。
    在给定带宽约束和带宽预算的情况下，我可以包装两倍或四倍的数据。
- en: into the same amount of data that I'm sending into a processor。 And mind you，
    it actually turns out。 that the amount of silicon goes quadratically， with the
    amount-- with the number of bits that you need。 So it's almost free to add lower
    precision operations。 So that's， for instance， why NVIDIA added。 int four on their
    new Turing GPUs。 Because it almost cost them nothing in terms of silicon。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 进入我发送到处理器的相同数量的数据。 而且请注意，事实证明，所需的硅量是按平方比例增长的，随着比特数的增加。 所以添加低精度操作几乎是免费的。 例如，这就是为什么NVIDIA在他们的新Turing
    GPU上添加了int four的原因。 因为在硅方面几乎没有成本。
- en: And they are probably hoping--， who knows-- yet whether that bit will pay off
    or not--。 that by adding those operations， you， can add some other--。 you can
    get an additional speed up by a factor of two。 for-- my guess is probably 1% or
    2% larger chips。 And given that。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可能在希望——谁知道——是否那个操作会带来回报——。 通过添加这些操作，你可以添加一些其他——。 你可以通过增加两倍的加速，获得一个大约1%到2%更大的芯片。
    鉴于此。
- en: you might happily take that speed up of two。 So you'd ask， might there be to
    figure out。 which algorithms can take advantage of this？ There's still some papers
    to be written。 OK。 So now that we have this--。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会很高兴地接受这个两倍的加速。 那么你会问，可能会有哪些算法能够利用这一点呢？ 仍然有一些论文要写。 好吧。那么现在我们有了这个——。
- en: '![](img/f8c286c4b36ffca282f912452df59dce_5.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8c286c4b36ffca282f912452df59dce_5.png)'
