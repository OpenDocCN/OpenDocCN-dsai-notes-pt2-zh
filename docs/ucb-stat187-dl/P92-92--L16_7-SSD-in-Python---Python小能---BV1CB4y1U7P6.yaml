- en: P92：92. L16_7 SSD in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P92：92. L16_7 SSD在Python中 - Python小能 - BV1CB4y1U7P6
- en: So we will dive deeper into how to implement SSD。 So， you know it's very similar。
    But fast-d。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将深入探讨如何实现SSD。你知道，它很相似。但更快。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_1.png)'
- en: to do category prediction。 So here， assume we have a number of anchors given
    a feature map。 We have given input this number of anchor boxes。 And then give
    the number of classes。 So for each anchor box， we will predict number of class
    plus， one category。 The plus one means a background。 This means don't belong to
    any class。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 做类别预测。所以这里，假设我们给定了一个特征图，存在多个锚框。我们给定了这个锚框的数量，然后给定类别的数量。所以对于每个锚框，我们将预测类别的数量加上一个类别。这个加一表示背景。这意味着不属于任何类别。
- en: So what do we do here with your convolutional kernel？ The output channel。 we
    have a number of anchors， times the number of class plus one。 OK。 so this is because
    for each anchor box， we will generate--。 we need to generate number of classes
    plus one prediction。 It's a class prediction。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在这里，我们对卷积核做了什么？输出通道。我们有多个锚框，乘以类别数加一。好的。所以这是因为对于每个锚框，我们将生成——我们需要生成类别数加一的预测。这是类别预测。
- en: And then why we choose convolutional 2Ds？ Because we choose kernel size equal
    to 3。 Paddy equals to 1。 Then we know that the output shape will be the same as
    the input， shape。 Which means every pixel in the feature map， we can have a coding
    pixel in the output。 In additional。 for each output， we， have number of anchors
    times number of class plus one channels。 So each output。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么我们选择卷积2D呢？因为我们选择的核大小为3，步长为1。然后我们知道输出形状将与输入形状相同。这意味着特征图中的每个像素，我们都可以在输出中得到一个对应的像素。额外的，对于每个输出，我们有多个锚框，乘以类别数加一的通道数。所以每个输出。
- en: we predict the anchor boxes， centered the ad that's pixel。 Because we have a
    number of anchor boxes and so many， classes， we need so many channels。 That's
    very little bit different to what we have before。 Before we only need to predict
    the whole image。 So the output channel， what we did like net in net。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测锚框，以ad的像素为中心。因为我们有多个锚框和很多类别，所以我们需要这么多通道。这和我们之前有点不同。之前我们只需要预测整个图像。所以输出通道，像我们在网络中做的那样。
- en: is that we do average pooling， so make into a single feature， map。 And the number
    of channels equals the number of class。 But here， we don't do average pooling。
    because we need to predict for every pixel。 OK， any questions here？ OK。 so that's
    the concept of change。 How you to predict so many different kinds of outputs。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 就是我们做平均池化，将其转换为单个特征图。并且通道数等于类别数。但是这里，我们不做平均池化。因为我们需要为每个像素进行预测。好的，这里有什么问题吗？好的。这就是改变的概念。如何预测这么多不同种类的输出。
- en: Similar thing， for bond-- OK， let's do a sanity check。 So we have a function
    called forward。 Given input x， given network， we first initialize it。 And given
    x， I just return the output。 So if the input is like 2 by 8 by 20 by 20， to the
    batch size， 8 is the number of channels。 the input channel。 20 by 20 is the feature
    map with a height。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的事情，对于bond——好的，让我们做一个基本检查。所以我们有一个叫做forward的函数。给定输入x，给定网络，我们首先初始化它。然后给定x，我就返回输出。所以如果输入像是2乘8乘20乘20，批次大小8是通道数，输入通道。20乘20是特征图的高度。
- en: And if I'm going to predict number of five anchor boxes， and 10 classes， then
    the output should。 be the batch size we don't change。 And this is the feature
    map height， feature map with。 we don't change。 We only change the output channel。
    So 55 equals to number of anchors times 10 plus 1。 OK， so all these things are
    predictions。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我要预测5个锚框和10个类别，那么输出应该是。批次大小不变。特征图的高度，特征图的宽度也不变。我们只改变输出通道。所以55等于锚框数乘以10加1。好的，这些都是预测结果。
- en: Given each example， we can generate so many predictions， here。 Any questions？
    Good。 Similar thing。 for the other thing， we can generate like no matter what，
    the input channel we have。 then we always generate， to 3 times 11 is 33 and 10
    by 10。 This is the input feature map。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 给定每个示例，我们可以在这里生成很多预测。有什么问题吗？很好。类似的情况，对于另一个问题，我们可以生成，不管我们有什么输入通道。然后我们总是生成，3乘以11等于33，10乘以10。这就是输入特征图。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_3.png)'
- en: Another thing is that we want to concat all the prediction。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事是我们想将所有的预测结果进行拼接。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_5.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_5.png)'
- en: for multi-scale， because we output for each scale。 Every scale have different
    feature map。 We want to concat them together and fit into a loss function。 So
    what we do here， we transpose。 the number of output channels into a loss， the
    channel or into a loss the shape dimension。 And then this is the height， this
    is the width。 And then we flatten。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多尺度，因为我们为每个尺度输出。每个尺度有不同的特征图。我们想把它们拼接在一起并输入到损失函数中。所以在这里我们做的是，转置输出通道的数量到一个损失通道或损失形状维度。然后这是高度，这是宽度。然后我们将其展平。
- en: which means we transpose into 2D matrix。 And then we concat for different scale。
    we're going to concat them on the first dimension， which， is not the batch size
    dimension。 So the goal here， the y1， we can think。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将其转置为 2D 矩阵。然后我们会对不同的尺度进行拼接。我们将在第一维度上进行拼接，即不是批量大小维度。所以这里的目标，y1，我们可以这样理解。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_7.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_7.png)'
- en: if you assume y1， this is the first scale， y2， we have of the width and the
    height。 And we want to concat together。 That is， we make into a matrix and put
    the number of channels。 which is the channel。 It's the kind of the number of classes。
    we have into a loss the channel and do flat in the 2D matrix。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你假设 y1，这是第一个尺度，y2，我们有宽度和高度。然后我们想把它们拼接在一起。也就是说，我们把它做成一个矩阵，并放入通道数。也就是通道。它是类别的数量。我们将它们放入一个损失通道并展平成
    2D 矩阵。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_9.png)'
- en: And then you give a 2D matrix。 Pretty the number of columns is pretty high。
    OK。 that's the way how we can put the output， for different scale together。 Similarly。
    how do the bound box prediction？
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你给出一个 2D 矩阵。列数相当多。好的。这就是我们如何将不同尺度的输出放在一起。类似地，边界框预测是怎么做的？
- en: '![](img/0fc7e5b120cae5df269185702791a06e_11.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_11.png)'
- en: We know that the bound box just the four numbers。 So we only need to know the
    number of anchors。 And then the output channel will be for each anchor box， we
    can predict four values。 This is the anchor box。 We can define the anchor box。
    And then similarly。 we have kimos as equal to 3， paddy equal to 1， so that we
    can understand output， as the input。 OK。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道边界框只有四个数字。所以我们只需要知道锚框的数量。然后输出通道数将是每个锚框预测四个值。这就是锚框。我们可以定义锚框。然后类似地，我们有 kimos
    等于 3，paddy 等于 1，这样我们就能理解输出，作为输入。好的。
- en: so now we know how to do prediction errors。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们知道如何做预测误差。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_13.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_13.png)'
- en: And the other one we mentioned that we have done sample， box that is half the
    height and width。 So it's pretty easy here， whether you can see that。 Assume this
    is the number of the channels of the input。 What do we do here？ We add in two
    blocks。 The first one is conv2D， kernel equals 2。3， paddy equals 2。1。 Basically，
    we don't change with-- oh。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 还有我们提到过的，我们已经做了示例框，它是高度和宽度的一半。所以这里非常简单，你可以看到。假设这是输入的通道数。我们在这里做什么？我们加入了两个块。第一个是
    conv2D，kernel 等于 2，3，paddy 等于 2，1。基本上，我们没有改变——哦。
- en: sorry。 The number of channels is output channel。 It's not the input channel。
    We don't care about the input channel。 So firstly， we use a conv2D map to output
    channels。 We didn't change the height and width here。 And adding as normal as
    we have a rest in it。 before we add in a batch on here。 And after batch on， we
    do a rel。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对不起。通道数是输出通道数，而不是输入通道数。我们不关心输入通道数。所以首先，我们使用 conv2D 映射到输出通道。这里我们没有改变高度和宽度。并且像正常一样加入，我们在其中有一个
    rest。然后我们在这里加入一个批量处理。批量处理后，我们做一个 rel。
- en: This is the typical block we have in rest net。 Then we add in two such blocks
    to get。 how to do feature instructions。 And then last， we do max pooling。 So strad
    equals 2。 then which means， we can reduce the height and width by half。 So this
    is a done sample block。 So given-- for example， given an input， we have 20 by
    20 and fit into done sample block。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在 rest net 中的典型块。然后我们添加两个这样的块来获取如何进行特征指令。最后，我们做最大池化。所以 strad 等于 2，这意味着我们可以将高度和宽度减半。所以这是完成的示例块。比如说，给定一个输入，我们有
    20 x 20 并输入到完成的示例块中。
- en: And output channel is 10。 There we get-- we don't change the number batch size。
    Two。 we change the shape。 We change the channels to 10。 And then， most importantly。
    we reduce 20 by 2 to 10。 OK， this is done sample block。 OK， the last one， we need
    a base net walk。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出通道为 10。我们得到——我们没有改变批量大小。是 2。我们改变了形状。我们把通道数改为 10。然后，最重要的是，我们将 20 除以 2，变成 10。好了，这是完成的示例块。好了，最后，我们需要一个基础网络走法。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_15.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_15.png)'
- en: Usually， we pick up rest net or any one we've known before。 And also， we can--
    for fine tuning。 we can use a pre-trained network。 But here， just because we're
    going to use a very simple data， set。 what we do here， we just concat， bunch of
    done sample block together。 We first use a done sample block to 16 number of channels。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: And then reduce the height and width and double the number， of output channels。
    And double again。 the base three done sample channels， as the base net walk。 OK，
    that's the full tiny case。 So given-- if given the input size， two batch， RGB
    three， channels， 256 to 256。 So this one。 just to reduce the height and width
    by 4， by 8。 And also change the number of channels to 64。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_17.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: So that's the base net walk we have。 So basically， we have implemented the base
    network。 the done sample blocks block， and also the prediction， for category and
    the classes already。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_19.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: So now we are ready， kind of， to get a block。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_21.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Given the block， we have five blocks here。 Stear goes to here block one base
    network。 And in the middle three is the done sample block。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_23.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: The last one we actually just did--， sorry。 The first one is the base network。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_25.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: And for one， two， three， we use a done sample block。 The last one， we just do
    a globe max pooling。 to reduce the shape that features into one by one， no matter
    how large you have。 OK。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_27.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: So this is-- so the net we have five stage。 So here's another thing。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_29.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Given a block， how to run a forward function？ So this is a little bit different
    to what。 we have for classification before。 So assume the block is the model。
    And then we have how to choose the size， which， means different size for the anchor，
    box。 which is a list。 The ratio-- the list of ratios we， going to choose for the
    anchor box。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: And a class predictor， the bond box predictor。 So first we do here。 we fit into
    the input into a block， to get a feature map， which is why。 And then we know that
    we showed this function before， like given a different shape， different size。
    given， the ratio， going to return a bunch of anchors。 So for this block， given
    that input shape。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: given the list of size， the list of ratios， we will have the anchors for this
    block。 the output of this block。 So then we fit the output of y into the class
    prediction。 and the fit of y into the bond box prediction。 Then the output， we
    have four things。 So here's the feature map， y， which， is going to be the input
    for the next block。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: And the anchors for this feature map， also the class。 prediction and the bond
    box prediction for each pixel， we have。 OK， so we have four output。 And the previous
    classification， we only have a single one。 But because now it's a little bit complicated，
    we have four here。 Let's do a sanity check here。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个特征图的锚框，也有类的预测和每个像素的边框预测。好的，所以我们有四个输出。之前的分类，我们只有一个。但是因为现在有点复杂，我们这里有四个。我们来做一下合理性检查。
- en: We've given different size because we have five stage。 This star put off the
    base network。 And this is kind of the three down sample network。 And this is for
    the last average pulling。 And we choose different size。 Well， it's pretty random
    number。 You can see we start with 20% of--。 well， it's pretty random。 So we start
    with 20%。 The anchor box will be 20% of the original feature image。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出了不同的大小，因为我们有五个阶段。这个星号表示基础网络。这个是三层下采样网络。这是最后的平均池化。我们选择不同的大小。嗯，这是相当随机的数字。你可以看到我们从
    20% 开始——嗯，这很随机。所以我们从 20% 开始。锚框将是原始特征图的 20%。
- en: We cover 20% of the area。 And the next level we change to 0。37。 Well。 we basically
    just like what we do here。 OK， what do we do here？ We pick up the last one。 The
    last layer， we cover a large object， cover 80% of the original input。 And then
    we linearly find this 0。37， 0。94。 Like 0。37 minus 0。2 equals to 0。17。 Similarly。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们覆盖了 20% 的区域。接下来我们改变为 0.37。好吧，基本上我们就像这里做的那样。好的，我们在这里做什么？我们选择最后一个。最后一层，我们覆盖了一个大物体，覆盖了原始输入的
    80%。然后我们线性地找到这个 0.37，0.94。就像 0.37 减去 0.2 等于 0.17。类似地。
- en: this is equal to 0。17 equals 0。054。 And basically， the linear scale between
    the first 0。2。 and the last 0。88。 OK， so how about the second shape？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于 0.17 等于 0.054。基本上，线性尺度从第一个 0.2 到最后一个 0.88。好的，那么第二个形状怎么样？
- en: The second shape actually is the square root of 0。2 times， 0。37。 It's kind of
    in the middle。 Similarly， this is kind of the square root of this guy times， this
    guy。 Kind of see between。 So for the ratios， first we use a square ratio。 Then
    we choose two。 which is one side is two times larger， than the other side。 And
    we choose another one， 0。5。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个形状实际上是 0.2 乘以 0.37 的平方根。它有点在中间。类似地，这有点像这个家伙乘以这个家伙的平方根。可以看见之间。所以对于比率，首先我们使用一个平方比率。然后我们选择
    2。即一边是另一边的两倍大。我们选择另一个，0.5。
- en: And for each scale， we just choose the same ratio。 So the idea here， for the
    bottom layers。 we use a kind of small anchor box， trying to catch up small boxes。
    For the last one。 we use the larger ones here。 So then for each scale， the number
    of anchors。 will be this is 2 and plus 3 minus 1， which is 4。 For each pixel。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个尺度，我们只选择相同的比率。所以这里的想法是，对于底层，我们使用一种小的锚框，试图捕捉小的框。对于最后一个，我们在这里使用较大的锚框。那么对于每个尺度，锚框的数量将是
    2 加上 3 减去 1，即 4。对于每个像素。
- en: we're going to generate the four anchor boxes。 OK， any question here？ OK。 Then
    we have the whole model。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成四个锚框。好的，这里有问题吗？好的。然后我们有整个模型。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_31.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_31.png)'
- en: We call it tiny SSD because it's not actually SSD。 Because the best in those
    pretty small。 So it can leave it complicated， not too much。 The thing here， due
    to initialization。 we have the number of classes。 And then we have five blocks。
    We use the Python set attribute because we， won't have a self block underscore
    0 means get the first block。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称它为 tiny SSD，因为它实际上不是 SSD。因为最好的是这些相当小的。所以它可以让它复杂，不会太多。这里的事情，由于初始化。我们有类别的数量。然后我们有五个块。我们使用
    Python 的集合属性，因为我们不会有一个自我块下划线 0 意味着获取第一个块。
- en: kinds of。 And so this is kind of the way we don't want， to have a list of this
    only thing。 For each block， we get the base block network， get a， predictor， get
    a bound box， cross predictor。 bound box， predictor。 So we have five stage。 We
    have actually 15 kinds of different blocks。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 各种类型。所以这就是我们不希望只有这个列表的方式。对于每个块，我们获取基础块网络，得到一个预测器，得到一个边框预测器。边框预测器，预测器。所以我们有五个阶段。实际上我们有
    15 种不同的块。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_33.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_33.png)'
- en: Then in the fourth function， so firstly， we have each block。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在第四个函数中，首先，我们有每个块。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_35.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_35.png)'
- en: each level， we cannot give you anchors， class prediction， bound box prediction。
    Then what do we do here？ For each stage， we feed x input and get the blocks。 Get
    the block。 Get the sizes。 Get the ratios。 Get the class predictor。 Get the bound
    box predictor。 Fading to the blocks forward function we defined before。 And we
    get x anchors， class prediction。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: and the bound box。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_37.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: prediction。 At the end-- well， let me--， At the end， we do here。 We just concat
    all these output for each layer together。 So at the end， we concat all the things。
    We concat the anchors。 We concat the cross predictions， also the bound box predictions。
    So here。 the reshape means for cross prediction， we want to have all kinds of--。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: this is number of class plus one， this class prediction。 And we don't care about
    this one too much。 because we can run softmax level。 And for bound box， we don't
    change anything here。 So the tiny SSD。 given input， we， give you all these anchor
    boxes we。 have at the cross prediction and the bound box prediction。 Three things。
    OK。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: the libia sanity check is libia complicated here。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_39.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Sorry。 [INAUDIBLE]。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_41.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: '![](img/0fc7e5b120cae5df269185702791a06e_42.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: '![](img/0fc7e5b120cae5df269185702791a06e_43.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: So I create network。 The number of classes equal to one。 We need to show that
    a fit into the network is batch size， equal to 32。 RGB channels 3。 and the input
    is 256， twice 256。 So then given to x， we have number of anchors。 and number of
    class predictions， bound box predictions。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: So we know the base network going to divide the width and height， by 8， so we
    get 32。 And then we fit into another down sample block， another one， another one。
    And finally。 we get average pooling。 So this is the feature map size for each
    scale。 And we have four anchors for each pixel， because we choose two sizes and
    three ratios。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_45.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: We get four anchors。 So in total， we have 4，444 anchors。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_47.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: and in total， we can generate。 So then you can see that the output shape， the
    number of anchors。 because the anchors are shared across all these batches。 So
    no matter the batch size we have。 it's always one。 And this number of anchors
    we have， each anchor box， can pretend by four numbers。 So this is four here。 The
    number of classes， the batch size， and the number of anchors。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: And these two， because we have single class， but plus a background， class， we
    have two classes here。 Similarly， the bound box prediction， we just concaten，
    to 2D shape， because of the concaten。 So which means it's a 4，000 number， times
    4。 It gives this little larger number here。 So you know the shape of this one。
    Also， you can see that even through the tiny SSD。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: we generate a lot of anchor boxes here。 It costs a lot of computation overhead。
    So now that we have the whole network defined。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fc7e5b120cae5df269185702791a06e_49.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_49.png)'
- en: And next one is like the data reading we already--， in the last lecture， we
    already talked about--。 we have the tiny Picacho dataset。 Given image， we put
    the 3D Picachos all over around。 and to get the since that data set。 So here，
    we noted the Picacho dataset。 We choose the batch size equal to 32， and we need
    to try to use GPUs and tiny disk model we have。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是数据读取的部分，我们已经——在上节课中谈过——我们有一个小型 Picacho 数据集。给定图像，我们把 3D Picacho 放在各个位置，从而获得数据集。因此，这里我们注意到
    Picacho 数据集。我们选择批量大小为 32，并且需要尝试使用 GPU 和我们的小型磁盘模型。
- en: We initialize it with x over here。 And we choose a non-linearity equal to 0。2。
    And the weight decay is like a small weight decay。 OK。 So this is set up。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用这里的 x 来初始化它。我们选择非线性函数为 0.2。权重衰减设为较小的值。好的，这就是设置。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_51.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_51.png)'
- en: So the other thing is the loss function。 Well， we have two kinds of loss here。
    The first is softmax cross entropy loss。 This is for cost prediction。 So that's
    no more we have。 The other one， the bounding box， we use L1 loss， which is the
    predicted bounding box minus the brown choose。 and they use the L1 norm。 The reason
    we don't want to use L2 is because we really， bad predictions。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要点是损失函数。我们这里有两种损失。第一个是 softmax 交叉熵损失，这是用于成本预测的。所以就没有其他了。另一个是边界框，我们使用 L1 损失，就是预测的边界框减去真实框，使用的是
    L1 范数。我们不使用 L2 的原因是因为这样会导致很差的预测结果。
- en: very far away from the bounding box， that we don't want to penalize that too
    much。 So here。 then the loss function we have a lot of things here。 This is a
    cost prediction to the cost label。 So the loss function-- so we just， fit the
    cost prediction。 The loss-- cost labels into the loss。 into the cost loss here。
    The softmax loss。 The other thing is like the bounding box predictions。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果距离边界框非常远，我们就不希望惩罚太多。因此，这里，损失函数包含了很多内容。这是成本预测与成本标签之间的损失。所以损失函数——我们只需将成本预测与损失标签结合起来，形成成本损失。在这里，使用
    softmax 损失。另一部分是边界框预测。
- en: the bounding box labels。 And here's a mask。 We talked about mask before。 If
    the anchor box。 we didn't find a ground choose bound， box associated with this
    one， we can mask as 0。 So if for each this anchor box， we find bound box associated，
    with this one。 we just mask it equal to 1。 So here， the loss function， we only。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框标签。这里是掩码。我们之前谈过掩码。如果锚框没有找到与之关联的真实边界框，我们可以将其掩码为 0。所以对于每个锚框，如果我们找到与之关联的边界框，我们就将掩码设为
    1。所以在这里，损失函数只考虑。
- en: want to penalize the anchor box predictions actually， mapped to a bounding box
    here。 So if we didn't find any associated， which may be a background， then we。
    don't want to penalize the prediction here。 But we still need to penalize the
    loss。 because we have a background category here。 So here， we first get the prediction。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望惩罚那些实际上映射到边界框的锚框预测。因此，如果我们没有找到任何关联，可能是背景，那么我们就不希望惩罚这个预测。但我们仍然需要惩罚损失，因为我们有一个背景类别。在这里，我们首先得到预测。
- en: times the mask to get all these anchors we have the bound。 box and also the
    labels tends the mask and do the L1 loss。 So this is the prediction loss。 We return
    and just for simplicity， we just plus the prediction loss at the cost prediction
    loss。 plus the bound box prediction loss。 In normal， you maybe have a weight here。
    You maybe-- here。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过乘以掩码，得到所有这些锚框，我们有边界框，同时标签也会作用于掩码并计算 L1 损失。所以这是预测损失。我们返回并为简化起见，我们只加上成本预测损失和边界框预测损失。通常，你可能在这里有一个权重。你可能——这里。
- en: we just for simplicity， we just adding equally。 OK。 So this is the evaluation
    matrix。 So class 1。 class is easy。 We just equal to the number of labels we have。
    This is the long accuracy we have。 For the bound box we have here， this is the
    label we have。 This is the prediction。 We just minus it。 I test the mask。 We only
    care about the bound box。 The anchors have objects。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们只是均等地加起来。好的，这是评估矩阵。对于类 1，类很简单，我们只需等于我们拥有的标签数量。这是我们获得的长精度。对于这里的边界框，这是我们拥有的标签。这是预测结果。我们只需减去它。我测试掩码。我们只关心有物体的边界框锚点。
- en: And just the absolute value and sum together as scalar。 This is basically the
    L1 loss we have。 So we call it the bound box matrix here。 Then we have all of
    the things。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后只是取绝对值并将其相加，作为标量。这基本上是我们得到的 L1 损失。所以我们称之为边界框矩阵。然后我们有所有的内容。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_53.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_53.png)'
- en: This is the last one is the training script。 A little bit long， but pretty similar
    to each other。 So we initialize something like the accuracy， the--， well， the
    minimum-- the bound box， the sum。 the bound box， arrow， and the iterator we're
    going to reset here。 So we have the iterative we have x and the y。 And the only
    thing， the only difference here。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最后一个训练脚本。有点长，但彼此之间非常相似。所以我们初始化一些东西，比如准确率——嗯，最小值——边界框，求和。边界框，误差，以及我们将重置的迭代器。所以我们有我们输入的x和y。唯一的不同之处在于这里。
- en: So given network， we can just reoutput as the normal as， before。 And then given
    the anchors we have。 and given the labels y， we're going to do the target。 We're
    going to map the anchors to bound boxes。 We talk about in the last lecture about
    that。 So then we're going to retense you the bound box labels。 the mask we have，
    and those are the labels that we have。 Then once the match is done， we'll。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以给定网络，我们可以像以前一样正常输出。然后给定我们有的锚点，和给定标签y，我们将进行目标处理。我们将锚框映射到边界框。我们在上次讲座中讲过这个。所以接下来我们会返回边界框标签，和我们有的掩膜，那些就是我们有的标签。然后一旦匹配完成，我们就。
- en: feed all the things to calculate the loss。 And so this is a fold function。 So
    in the fold function。 given x， you generate the anchors， cast predictors， bound
    box predictors， anchors map into。 the bound box， generate the labels， and then
    lastly， compute the loss。 So that's all。 So that's why we call a single shot。
    We get everything together。 In the master's here。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有数据输入以计算损失。所以这是一个折叠函数。折叠函数中，给定x，你生成锚框，类别预测器，边界框预测器，锚框映射到边界框，生成标签，最后计算损失。所以就是这样。所以这就是为什么我们称之为单次检测。我们把所有的东西都一起处理。在这里的主程序。
- en: you first predict the anchor boxes， and then do the prediction here。 For here。
    we just generate at the same time， it's causing a， short。 Then we do the--， have
    a loss。 We do computer gradients。 And evaluate the accuracy， evaluate the bound
    box， then。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先预测锚框，然后在这里做预测。对于这里，我们只是同时生成，导致了一个短的过程。然后我们进行——，有损失。我们计算梯度。然后评估准确率，评估边界框，然后。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_55.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_55.png)'
- en: almost everything's similar。 And the printed results。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有内容都相似。以及打印结果。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_57.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_57.png)'
- en: So let's see。 So the running here is to be a bit slow。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看。运行过程可能会有点慢。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_59.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_59.png)'
- en: I can just show the training log we have。 Pretty small。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以展示我们有的训练日志。相当小。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_61.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_61.png)'
- en: Let's do--， let me--。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来做——让我来——。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_63.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_63.png)'
- en: you can see the log here。 Well， we only have two classes， and that's pretty
    easy。 And the bound box。 you can see that decreases a little bit。 Because there's
    a tiny application here that's pretty。 easy to train。 But again， still， every
    five people， even tiny classes， take like 20。 30 seconds to run on a GPU。 So in
    general， object detect is much more complex than。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到日志。嗯，我们只有两个类别，这还挺简单的。而边界框，你可以看到它稍微减小了一些。因为这里有一个小的应用，比较简单训练。但再说一次，仍然是每五个人，甚至是微小的类别，跑在GPU上大概需要20到30秒。所以一般来说，目标检测比。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_65.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_65.png)'
- en: image classification。 The last thing， once we train the network， we want to
    do。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类。最后，当我们训练好网络后，我们想要做。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_67.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_67.png)'
- en: predictions。 So predictions， given the input x， still we fit into the， network。
    We have the anchor boxes we have， class prediction， run the box prediction。 And
    we run a softmax on the class predictions to get all， these classes we have。 The
    other thing we mentioned that we want to run--， we have a lot of overlap。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 预测。给定输入x，仍然我们将其输入到网络中。我们有锚框，我们有类别预测，运行边界框预测。然后我们对类别预测运行softmax，得到所有我们有的类别。另一个我们提到的事情是我们想要运行——我们有很多重叠。
- en: anchor box predictions， we just， want to remove the duplications。 It's called
    NMS。 So then lastly。 if it's just a minus one， which means we， think it's a duplicate，
    we just removed them。 And this returns out the last one we have。 They're pretty
    straightforward。 So then we read image here and do a resize because the， input
    is like 200 by 5th， 6， 200 by 5th。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框预测，我们只是想去除重复项。这叫做NMS。所以最后，如果它是负一，意味着我们认为它是重复的，我们就把它移除。这样返回的就是我们最终得到的框。它们相当直观。然后我们读取图像，并进行调整，因为输入是200x5，第6，200x5。
- en: 6th as converted， floating and transport to put the RGB channel in the last，
    layer。 We'll put the RGB channel into the first dimension and。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步是将RGB通道放到最后一层。我们会把RGB通道放到第一维度。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_69.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_69.png)'
- en: then prediction here。 And finally， we can show that， OK， display that。 We also
    talked about that before。 Given the stretch holding， which is if the stretch。
    holding means the confident value is larger than the， stretch hold icon print。
    if the confidence score is too small， I can ignore to print that out。 So what
    we do here。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是预测，最后我们可以显示出来。我们之前也提到过。给定阈值，即如果置信度值大于阈值，则打印。如果置信度分数太小，我可以忽略它并不打印。所以我们在这里做的就是……
- en: every time we get a score， if the， score is larger， smaller than the stretch
    hold， it can， continue。 Otherwise， you can print the bundle box here。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们得到一个分数，如果分数大于或小于阈值，它就可以继续。否则，你可以在这里打印捆绑框。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_71.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_71.png)'
- en: So here finally， you can see that， well， OK， so finally。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以最后你可以看到，好吧，最终。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_73.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_73.png)'
- en: you can see that， given a synthetic image， you can， bunch of picaches around，
    you can see that。 well， at least， you get all this bundle box right。 So the wide
    ones predicted。 Usually。 you can change the stretch holding。 If you load the stretch
    holding， you get more bundle， boxes。 you get a high stretch holding， you get a
    small， bundle boxes。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，给定一张合成图像，你可以看到周围有一堆物体，至少你能看到所有的捆绑框都正确。所以宽度较大的框被预测出来。通常，你可以调整阈值。如果你降低阈值，你会得到更多的捆绑框；如果你提高阈值，你会得到更小的捆绑框。
- en: So you can see that it's not so confident 0。3， second， 3， 7， and 0， 5， 4。 In
    the normal way。 we pick up with just 0。5， 0。5。 This normal way， choose the stretch
    holding。 So that's kind of all for the object detection。 And be a bit more complicated
    than the image。 classification， because every time you run forward， you， get a
    multiple output。 And also。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到它并不是很有信心0.3，第二，3，7和0，5，4。按照正常方式，我们只选择0.5，0.5。这个正常方式选择了拉伸阈值。所以这就是物体检测的全部内容。比图像分类要复杂一些，因为每次你执行前向传播，你都会得到多个输出。并且……
- en: the output， you want to do a lot of， cleaning up to show the results。 Any questions
    so far？
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，你需要做很多清理工作，才能展示结果。到目前为止有问题吗？
- en: Question？ Just a very general question。 Why do we call it with different numbers
    for the。 found boxes？ Like， are these-- do you want to get， a percentage of the
    confidence in the， detection。 and then so why do we like， there's a y？ So remember，
    we have two output。 Once the bundle box is offset， which is the location， the，
    other one is the class prediction。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有问题吗？这是一个非常一般性的问题。为什么我们用不同的数字来表示找到的框？比如说，是否是——你想要获得一个置信度的百分比，然后……为什么会有一个y？所以记住，我们有两个输出。一个是捆绑框的偏移量，即位置，另一个是类别预测。
- en: So here， even though we want to have a single class。 because here we have another
    class called a background。 So this call means I predict this bundle box。 I first
    predict this bundle box。 And then I think you have 37% probability you， content
    the big cultural。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里，即使我们想要单一类别，因为这里还有一个类别叫做背景。所以这个调用意味着我预测这个捆绑框。我首先预测这个捆绑框，然后我认为你有37%的概率包含了大文化内容。
- en: And 63% just this background。 So this is a confidence score。 This box contains
    these objects。 So this is why we always have--。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 背景的置信度为63%。这就是置信度分数。这个框包含了这些物体。这就是我们总是有——的原因。
- en: '![](img/0fc7e5b120cae5df269185702791a06e_75.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fc7e5b120cae5df269185702791a06e_75.png)'
- en: like， I can show you the slides at the beginning。 You show this one。 You have
    all these numbers。 That's a person， 99%。 The person is like， it's a car。 Always
    you have a number。 But this one is much more confident。 You almost get 99% here。
    For us。 we have a tiny SSD then we can， do an ordinary job。 OK。 Other questions？
    [BLANK_AUDIO]。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我可以在开始时给你展示幻灯片。你展示这一张。你有这些数字。这是一个人，99%。那个人就像一辆车。你总是有一个数字。但这个更有信心。你几乎可以达到99%。对于我们来说，我们有一个小的SSD，然后我们可以做一项普通的工作。好的。还有其他问题吗？[BLANK_AUDIO]。
