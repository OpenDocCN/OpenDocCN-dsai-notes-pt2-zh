- en: P69：69. L13_2 Inception in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P69：69. L13_2 Python中的Inception - Python小能 - BV1CB4y1U7P6
- en: Okay， so this is now my notebook running on a gpu。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在这是我的笔记本在GPU上运行。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_1.png)'
- en: It's one of the p3 to x large servers。 And let's actually go and implement an
    inception network。 And run it。 So the first thing i have to do， Is of course import
    all the appropriate libraries and i've。 Done that already before so i can skip
    this。 But here's the basic inception block。 So this is the basic inception block
    now written out as code。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它是p3中的一部分，指向大型服务器。现在让我们实际去实现一个Inception网络并运行它。所以我首先要做的当然是导入所有适当的库，而我之前已经做过这个，所以可以跳过。但这是基础的Inception块。因此，这是写出来的基础Inception块代码。
- en: And here we are going to encounter for the first time a， Parallel composition
    of various paths。 So this is now where we don't have just a simple add a block，
    Add a block， add a block。 But we actually have to do a little， Bit of work to
    instantiate this。 The first thing i need to do is i need to initialize the， Components。
    So i need for path one to。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们第一次遇到的是各种路径的并行组合。所以现在我们不再只是简单地添加一个块，添加一个块，添加一个块。而是实际上需要做一些工作来实例化它。我需要做的第一件事是初始化这些组件。所以我需要为路径一初始化它。
- en: The convolution of size one by one。 For path two i need a one by one and a three
    by three。 For path three i need a one by one and a five by five with， Again， appropriate
    padding。 For path four i need a max， Pulling and a one by one。 So these are all
    the parts up here。 That i first need to define。 Now i have some parameters， Namely
    c one， two， three and four。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的大小是1×1。对于路径二，我需要一个1×1和一个3×3的卷积。对于路径三，我需要一个1×1和一个5×5的卷积，并且再次需要适当的填充。对于路径四，我需要一个最大池化和一个1×1的卷积。所以这些都是上面提到的部分，是我首先需要定义的。现在我有了一些参数，分别是c1、c2、c3和c4。
- en: And these are the parameters， That govern the sizes， basically the number of
    channels for the。 Various paths。 So if you go back。 These are the full paths。
    Past one， two， three and four。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是控制尺寸的参数，基本上是通道数量的参数，适用于不同的路径。所以如果你回到前面，这些就是完整的路径，跨越了一个、二、三和四。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_3.png)'
- en: There we are。 Now what i then do is i just code up， And i then do is p one，
    one of x。 That's path one of the input。 P two is p two of two of p two， one of
    x。 Because i'm actually concatenating those two pieces， right？
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。接下来我做的就是编码，然后我做的是p1，即输入的路径一。p2是p2中的p2，p1的x。因为我实际上是拼接了这两部分，对吧？
- en: So i first apply a one by one and then a three by three。 For path three i do
    the same thing for one by one and five by five。 So i have to add four， max。 pooling
    and convolution。 Now i have those four paths separately。 And now i can concatenate
    it。 So in the concat of p one， two， Three， four along dimension one just stacks
    everything up。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我首先应用一个1×1卷积，然后是一个3×3卷积。对于路径三，我对1×1和5×5卷积做相同的处理。所以我需要添加四个最大池化和卷积。现在，我已经分别得到了这四条路径。接下来我可以将它们拼接起来。所以在维度1上拼接p1、p2、p3、p4，就把它们全部堆叠在一起。
- en: So i have to add the output of that layer。 So once you think about it this way
    it's actually fairly。 Straight forward。 So if i wanted to modify that， Architecture
    of the intercepts and blocks。 that would be very easy。 So remember this will return
    a block with these parameters， Right。 with basically c one， two， three and four
    and some other parameters that i， Might have。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我需要将该层的输出添加进去。所以从这个角度来看，实际上是相当简单的。如果我想修改该Inception块的架构，那也会非常容易。所以记住，这将返回一个带有这些参数的块，对吧？基本上是c1、c2、c3和c4，还有一些其他的参数，可能是我自己定义的。
- en: So now we need to define the various， Blocks。 So block one is just sequential。
    Composition and i know why because i didn't actually run this。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要定义不同的块。所以块一只是一个顺序的组合，我知道原因，因为我并没有真正运行这个。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_5.png)'
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_6.png)'
- en: So i have to execute it。 I saw they ran it before。 Turns out they didn't。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我必须执行它。我看到他们之前运行过。结果发现他们并没有运行。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_8.png)'
- en: Okay。 So just you know the standard， Convolution and max pooling。 stage two
    is another few convolution and another， Max pooling。 So that again reduces the。
    Mentionality。 So right now i'm just defining， Each of those blocks separately
    i haven't actually put them all together yet。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，标准流程就是卷积和最大池化。第二阶段是几个卷积和另一个最大池化。这样再次减少了维度。所以现在我正在分别定义每个块，还没有将它们全部组合起来。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_10.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_10.png)'
- en: Okay。 Stage three。 Well that's two， Inception blocks now， followed by max pooling。
    Stage four。 well some more of it and now let's look at the actual numbers。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。第三阶段。那么现在是两个Inception块，接着是最大池化。第四阶段。更多的处理，现在我们来看一下实际的数字。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_12.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_12.png)'
- en: So these last numbers are the output channel， Dimensionality for each of those
    paths， right？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些最后的数字是每个路径输出通道的维度，对吧？
- en: See how they increase。 The other thing is that the ratio between these different。
    Stacks changes over time。 So first of all the， Dimensionality actually of these
    one by ones increases quite a bit。 And also of the one by one followed three by
    three followed by one by ones。 Five by fives。 well not so much。 And the one by
    ones not so much else。 Just in the end。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它们是如何增加的。另一件事是这些不同堆叠之间的比例随着时间变化。所以首先，这些1x1的维度实际上增加了很多。而且1x1、接着是3x3、再是1x1、5x5的堆叠变化就不那么明显了。1x1的变化也不大。最后变化最大。
- en: well i get something that creates a fairly， Largish Dimensionality。 All right。
    And then i reduce the resolution。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我得到了一个产生相当大维度的输出。然后，我减少了分辨率。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_14.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_14.png)'
- en: Okay。 So now i need the last block。 And that just you know adds two more。 Conception
    stages in the end and global average pooling。 And so now what i have is a network
    that i can create by。 Having those five blocks and then the end just a dense layer
    with， 10 dimensional output。 Okay。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在我需要最后一个块。这就意味着再加上两个Inception阶段，最后是全局平均池化。现在我有了一个可以通过五个块创建的网络，最后只是一个输出维度为10的全连接层。好的。
- en: There we go。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_16.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_16.png)'
- en: Okay。 So let's actually see what happens if i pipe some， Things in the middle。
    So to make life easy i'm just， Picking 96 by 96 pixels because if i pick something
    that's too high。 Dimensional then it's going to take a long time even on a small，
    Dargish set。 The reason why i only have one， Dimension here that has to do with
    the fact that this is fashion。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，让我们看看如果我在中间插入一些东西会发生什么。为了简化，我选择了96x96像素，因为如果我选择一个维度太高的图像，即使是在一个小的数据集上，处理起来也会很慢。我这里只使用一个维度，和这个事实有关，这涉及到时尚。
- en: Emmys so they are black and white images。 If i had cfire for instance they would
    be colored。 So after block one i'm left with 24 by 24 after block two i'm 12 by。
    12 then 6 by 6 3 by 3 1 by 1 and then 10 dimensions。 So what you can see is that
    the fact that we started out with a。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Emmy图像是黑白的。如果我用的是cfire，它们会是彩色的。所以在第一个块后，我得到了24x24，第二个块后是12x12，接着是6x6、3x3、1x1，最后是10个维度。所以你可以看到的是，实际上我们一开始的维度。
- en: Higher resolution to begin with with a lower resolution to begin。 With only
    96 by 96 let's only 3 by 3 is in the end。 Remember on the slides we actually have
    a 7 by 7。 Okay。 So now that we have this we can run it。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始的高分辨率，随后是低分辨率的输入。我们最后只有96x96，最终只有3x3。记住，在幻灯片上我们实际上有一个7x7。好的。现在我们有了这个，我们可以开始运行了。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_18.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_18.png)'
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_19.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_19.png)'
- en: Okay。 It's fun out that hey we have a gpu and now it'll take。 About 20 seconds
    or so for each of those paths to go through。 In the meantime let's just you know
    review what's going on。 So net initialize the premise of the network。 You have
    to do that before you run。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。结果出来了，我们有了GPU，现在每个路径大约需要20秒左右的时间来通过。在这期间，我们来回顾一下发生了什么。所以，首先要初始化网络的前提。在运行之前必须这么做。
- en: Now force re init simply because before that we already you know。 Sent some
    random garbage data through it and we， Initialize it at random so we just want
    to make sure everything's fine。 One important pitfall。 Force re init re initializes
    the values of those parameters。 What it does not do and i found out the hard way
    myself is it does， Not reset the dimensions。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在强制重新初始化，仅仅是因为在此之前我们已经发送了一些随机垃圾数据通过它，而且我们将其初始化为随机的，所以我们只是想确保一切正常。一个重要的陷阱是，强制重新初始化会重新初始化这些参数的值。它没有做的事情，经过我亲自吃过亏后发现，是它不会重置维度。
- en: So once you've bound the network to a particular input size you， Stock with
    that。 I mean you can take pieces of the， Network in the rearrange and do all those
    interesting things but at。 The end of the day if you've once bounded to let's
    say 64 by 64。 Resolution images force re init will not allow you to then have
    it。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一旦你将网络绑定到特定的输入大小，你就无法再改变它。我是说，你可以将网络的一部分重新排列，做一些有趣的事情，但是最终，如果你曾经绑定到，比如64x64的分辨率图像，强制重新初始化将不允许你之后改变它。
- en: To automatically adapt to 128 by 128。 For that you need to start with an architecture
    from scratch。 The reason for that is actually quite simple in hindsight。 Namely
    if you you know if you reset the you know parameters。 Since the network doesn't
    know which of the dimensions were given。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自动适应128x128的图像。为此，你需要从头开始设计架构。事后看来，这个原因其实很简单。也就是说，如果你重置了参数，由于网络不知道哪些维度是已知的。
- en: By the user and which ones were automatically inferred it cannot。 Just go out
    go and throw at everything that you know was。 Automatically inferred and re instantiated。
    That's the main reason。 So that's minor pitfalls or just be aware of it。 In any
    case this is inception and well we get a fairly。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由用户定义的维度和那些自动推断出来的维度，它无法自动重新推断并重新实例化。这就是主要原因。所以这是一个小陷阱，或者说需要留意的地方。无论如何，这是一个开始，结果我们得到了一个相当不错的模型。
- en: Respectable accuracy of you know about 。88 right so that's about， 12% error
    and fashion images。 This is not bad。 Okay。 Any questions？ Okay。 So i guess it
    should become kind of。 Routine by now how to train you know your favorite architecture。
    Because i mean that code is identical to the code that we've。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可接受的准确度大约是0.88，也就是大约12%的错误率，针对时尚图像。这还不错。好的，有什么问题吗？好吧，我想现在应该变得有些常规了，如何训练你最喜欢的架构。因为这段代码其实和我们之前用过的代码完全相同。
- en: Seen before just yep we have a different architecture。 [BLANK_AUDIO]。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 看看，嗯，我们有了一个不同的架构。[BLANK_AUDIO]。
- en: '![](img/7519f00da13fffe23c8b15d94d5d720d_21.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7519f00da13fffe23c8b15d94d5d720d_21.png)'
