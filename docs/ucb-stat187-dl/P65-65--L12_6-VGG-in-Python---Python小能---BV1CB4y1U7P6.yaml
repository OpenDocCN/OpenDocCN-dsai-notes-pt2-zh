- en: P65：65. L12_6 VGG in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's have a look at networks using blocks。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: So this is the really first part where VGG made a significant difference conceptually。
    in terms of how people started thinking about network designs。 Namely not in terms
    of defining individual layers but designing blocks and then composing。 the network
    out of blocks。 Okay， so for that we first need a VGG block。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: So the VGG block does nothing particularly special besides actually taking a
    number of。 convolutions as an argument and number of channels。 And what it then
    does is it just performs a sequential composition。 In this case I turn on hybrid
    sequential， that's because I'm going to use my JIT compiler。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: to make this go a little bit faster and I'm just adding one convolution after
    the other。 And in the end I just perform max pooling。 That returns a block。 So
    rather than sequential I just use hybrid sequential that's all that's needed for
    now。 in order to activate the JIT compiler。 Or at least to tell the JIT compiler
    that this can be used。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Okay。 So let's recall the VGG architecture， we've got a couple of those blocks
    in the end， we've。 got three dense layers。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_5.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Okay。 So the architecture that I'm going to use is one where I first have 64
    channels and just。 one convolution， 128 channels， one convolution， then 256 channels
    and two convolutions， 2， and 512。 2 and 512。 And so now， and this is the nice
    thing。 basically my network looks like a very simple for loop。 Right。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: So I start again with hybrid sequential because the overall block is hybridizable
    and I'm adding。 to this network individual blocks。 So let's add a VGG block of
    number of convolutions and number of channels。 it adds an entire， block rather
    than just a layer。 Then in the end， I add well。 three dense layers and that's
    it。 It's very clean， right。 Then the end， yeah。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: I just go and execute VGG of convarch。 So that's the architecture。 If I wanted
    to pick something else， I could have for instance omitted one layer or added。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: one more or one more block。 That's quite straightforward。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: So let's have a look at how much memory it uses。 Well， we have， you know， 112，
    512， 64 channels。 128 channels， 256 channels， 512， 512。 So this is after all exactly
    what we picked here， right。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_11.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: So the top line here， convarch defines exactly what we get。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_13.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Now， and lo and behold， that's what we have here。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_15.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Then the end， the behemoth dense layers as you would expect them。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_17.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Okay。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f66fea605ac011e5279e75c34059f15_19.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: So then， well， since we want to train this and we want to train in a reasonable
    time， we're。 actually going to make our network a little bit smaller。 So the thing
    that we'll do is we'll simply divide all the sizes and the ratios by， you， know。
    four and that makes sure that everything is considerably more concise so we can
    actually。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，既然我们想训练它，并且希望在合理的时间内训练，我们实际上会让我们的网络稍微小一些。所以我们做的事情是，简单地将所有的大小和比例除以四，这样可以确保一切都变得更加简洁，从而我们能够实际操作。
- en: run this inefficient time。 Now， I generate from my small architecture my network
    and I hybridize。 So hybridization now tells the network， okay， the first time
    going to execute it， look at， you know。 all the parameters that you have。 And
    then afterwards， don't bother about bothering Python anymore。 Just use the framework
    directly in the back end to perform computation。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个低效的时间。现在，我从我小型的架构中生成网络并进行混合。混合化现在告诉网络，好吧，第一次执行时，查看所有你拥有的参数。然后，之后，不再需要麻烦Python了。只需在后台直接使用框架来进行计算。
- en: '![](img/5f66fea605ac011e5279e75c34059f15_21.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f66fea605ac011e5279e75c34059f15_21.png)'
- en: Okay。 So， and then we train。 And as a matter of fact， if you were to do this。
    you would have to wait for about maybe， one and a half minutes per pass。 So I'm
    not going to do that right now or at least let's look at the outcome first。 You
    basically have a learning rate that's still quite small， five passes， mini batch
    size。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以，然后我们就开始训练。事实上，如果你这么做，你需要大约每次训练等待一分半钟。所以我现在不打算做这个，或者至少先让我们看一下结果。你基本上会有一个学习率，仍然相当小，经过五次迭代，小批量大小。
- en: of 128 and of course， your random GPU。 So this is exactly the same code as what
    we saw before in the previous two cases and。 then you just iterate to the data。
    And since this is in our book， the trainer from chapter five。 it's just trained
    on the， score of chapter five。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 128，当然，还有你的随机GPU。这和我们之前在前两个案例中看到的代码完全相同。然后你只需要对数据进行迭代。由于这是我们书中的内容，第五章的训练器，它只是根据第五章的分数进行训练。
- en: '![](img/5f66fea605ac011e5279e75c34059f15_23.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f66fea605ac011e5279e75c34059f15_23.png)'
- en: If I were to invoke this now， you'd see that it takes forever and it gets you
    down to about。 eight and a half to nine percent error after five epochs。 If you
    want to get it a little bit better。 you might have to use more epochs。 But that's
    all that you need to know when it comes to training using VGG on fashion MNIST。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我现在调用这个，你会看到它需要很长时间，而且经过五个训练周期后，你的误差大约是八点五到九个百分比。如果你想让它稍微好一些，可能需要使用更多的训练周期。但关于使用VGG训练时在fashion
    MNIST上的基本知识，就这些。
- en: '![](img/5f66fea605ac011e5279e75c34059f15_25.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f66fea605ac011e5279e75c34059f15_25.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
