- en: P53：53. L10_7 Deferred Initialization - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P53：53. L10_7 延迟初始化 - Python小能 - BV1CB4y1U7P6
- en: It's got deferred initialization。 So here， we create the network here。 Again。
    this is the example we， used during this lecture， two dense layers。 We get network。
    let me delete this one， make it shorter。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 它是延迟初始化。所以这里，我们在这里创建网络。再次，这是我们在这节课中使用的例子，两个全连接层。我们得到网络。让我删除这个，让它更简洁。
- en: '![](img/141b501c8fe5151066a25695134ca247_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_1.png)'
- en: So we create the network and print the connect parameters。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们创建网络并打印连接参数。
- en: '![](img/141b501c8fe5151066a25695134ca247_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_3.png)'
- en: returns all the parameters you have。 But we can see that， well， the weight here，
    the shape。 we know 256 is the output shape we specify here。 Because we didn't
    have any simple input shape。 it is 0 at this moment。 So you can see the 0 here。
    Similarly， bias。 we know that no matter the input size， the bias is already as
    a vector， as we have the same name。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 返回你所拥有的所有参数。但是我们可以看到，嗯，这里的权重，形状。我们知道256是我们在这里指定的输出形状。因为我们没有任何简单的输入形状。此刻它是0。所以你可以看到这里是0。类似地，偏置。我们知道无论输入大小如何，偏置已经是一个向量，因为我们有相同的名称。
- en: as the output。 So bias， I know the shape。 The second this layer， again， I don't
    know the shape。 because I did an infer from the previous layer right now。 But
    bias is fine。 So at this moment。 we give some shape， but only partial information
    already。 So we cannot infer。 get all the shapes for all the parameters。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出。所以偏置，我知道形状。这个层的第二个，依然，我不知道形状。因为我现在从前一层推断出来了。但是偏置没问题。所以此刻。我们给出了一些形状，但已经只有部分信息。所以我们无法推断出所有参数的形状。
- en: '![](img/141b501c8fe5151066a25695134ca247_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_5.png)'
- en: yet。 Now， again， we call initialize。 That's a tricky thing。 We call it initialize。
    But this function actually doesn't call anything。 You just initialize it。 You
    can tell me， OK。 what initialization method， they're going to use， Gaussian or
    constant or X， year。 or which device they're going to have。 But again， I still
    don't know the input size。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 还没有。现在，再次，我们调用初始化。这是一个棘手的事情。我们称之为初始化。但这个函数实际上什么也没调用。你只是初始化它。你可以告诉我，OK，使用什么初始化方法，它们会用高斯分布、常数还是X，年份。或者他们将使用哪个设备。但再次，我还是不知道输入大小。
- en: So if we call the connect parameters， nothing changed。 There's still a bunch
    of zeros here。 So if we access the data， you get nothing， because with idonal
    shape。 I didn't allocate the memory right， now。 What we really do is I give the
    input X。 I feed X into data。 Now I know the input size。 Because X have 20， that
    features 20。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '所以如果我们调用连接参数，什么都没变。这里仍然是一堆零。所以如果我们访问数据，你什么都得不到，因为是无效的形状。我现在没有正确分配内存。我们真正做的是，我给了输入X。我把X输入到数据中。现在我知道输入大小了。因为X有20个特征。  '
- en: so now I inferred the network， the shape is 20。 And also， I run the second layer。
    I know it's 256。 So the tricky thing here， in most cases， if it didn't specify
    the input shape。 we cannot access the parameters until you actually， put the actual
    data in。 Or you can specify the input shape at the beginning， but it didn't do
    that。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我推断出网络，形状是20。同时，我运行第二层。我知道它是256。所以这里的棘手之处是，在大多数情况下，如果没有指定输入形状，我们无法访问参数，直到你真正输入实际数据。或者你可以一开始就指定输入形状，但它并没有这么做。
- en: It didn't get access to the parameters。 So that's the key thing， maybe you're
    going to remember。 So let's look into what actually happens there。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有访问到参数。所以这是关键的事情，或许你会记住。所以让我们看看实际上发生了什么。
- en: '![](img/141b501c8fe5151066a25695134ca247_7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_7.png)'
- en: I define my init function。 I do nothing， just print when I need this function。
    So again。 I get a network。 I co-initialize， give my initial function。 Nothing
    output it。 which means my initializer， didn't ask you the yet， because I don't。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我定义了我的初始化函数。我什么也不做，只是在需要这个函数时打印。所以再次，我获得一个网络。我共同初始化，给我的初始函数。没有输出，这意味着我的初始化器，暂时没有询问你，因为我还没有。
- en: '![](img/141b501c8fe5151066a25695134ca247_9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_9.png)'
- en: know the input shape right now。 Now， I general X， general X， fit X into the
    fourth function。 You can see that， OK， now I have the data。 Before I run the fourth
    function。 I just call my initializers， which is calling here， like an initializer
    weight。 Well。 the second time， the second of all the pass， I should not do that。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我知道输入形状了。现在，我生成X，通用X，适配X进入第四个函数。你可以看到，OK，现在我有数据了。在运行第四个函数之前，我只是调用我的初始化器，像是调用初始化器权重。嗯。第二次，所有传递的第二次，我不应该这么做。
- en: Because we are going to update that， I cannot initialize it， again。 so that
    only happens on the first time。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们要更新它，我不能再次初始化它，所以那只会发生在第一次。
- en: '![](img/141b501c8fe5151066a25695134ca247_11.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_11.png)'
- en: It didn't call that before。 If I do a fourth， re-init， then--， because if I
    do that。 because before I already fit data， the net already know the shape。 So
    if I call init again。 actually， it， need to function there here， because I know
    the shapes。 The other thing that you really don't want to defer the initializer，
    you can do， like。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 之前并没有调用这个。如果我做第四次，重新初始化，那么——因为如果我那样做，因为在之前我已经拟合了数据，网络已经知道了形状。所以如果我再次调用初始化器，实际上，它需要在这里有一个函数，因为我知道这些形状。另一件事是你真的不想推迟初始化器，你可以做类似的事情。
- en: similar to what my customized dense layer。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我自定义的密集层。
- en: '![](img/141b501c8fe5151066a25695134ca247_13.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_13.png)'
- en: before， you can specify the input unit。 You can specify this dense layer， the
    input size 20。 and this dense layer， the input size is 256。 Now， I know the shape
    at the beginning。 so if I call initializer， initialize， I actually call this immediately。
    That's all about things。 Even that we only do very simple things。 But at the end，
    we're going to talk about the hand just。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，你可以指定输入单元。你可以指定这个密集层，输入大小是20，和这个密集层，输入大小是256。现在，我知道一开始的形状。所以如果我调用初始化器，初始化，我其实是立刻调用它。就这些事情。即使我们只做了非常简单的事情。但最终，我们要谈的只是这个手动过程。
- en: layers convolutional networks， so defer the initialization， pretty important，
    because you cannot--。 now it's simple network， I know each layer size， but if
    I get 100 layers。 it's really hard to computerize internally。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 层卷积网络，所以推迟初始化，非常重要，因为你不能——现在是简单的网络，我知道每一层的大小，但如果有100层，内部计算就真的很难了。
- en: '![](img/141b501c8fe5151066a25695134ca247_15.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/141b501c8fe5151066a25695134ca247_15.png)'
- en: So usually we use different initialization。 OK。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以通常我们使用不同的初始化。好的。
