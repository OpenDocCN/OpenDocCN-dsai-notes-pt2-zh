- en: P102：102. L19_5 Truncated Backprop through Time - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P102：102. L19_5 截断反向传播 - Python小能 - BV1CB4y1U7P6
- en: It's time to look at truncated backprop。 So we actually already encountered
    truncated backprop。 through time before when we set up those mini batches and，
    then we said， well， okay。 detach gradients。 And this was kind of weird， it's like，
    you know， why on earth。 do you need to detach the gradients？ Let's look at the
    RNN with hidden state and a little bit。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看截断反向传播了。所以我们其实已经在之前设置那些小批量时遇到过截断反向传播，然后我们说，好，行。分离梯度。这有点奇怪，像是你知道的，为什么你需要分离梯度？让我们来看看有隐藏状态的RNN和一点点。
- en: of what happens。 So let's say I have some hidden state updates， right？ So this
    is just very basic。 HT is some function of HT minus 1， XT minus 1， and W。 And
    then I have some observation。 which is given by some other， function， G， of HT
    and W。 So this is really completely straightforward。 This is unit vanilla RNN。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们假设我有一些隐藏状态的更新，对吧？这只是非常基础的。HT是HT减去1，XT减去1和W的某个函数。然后我有一些观测值，这由HT和W的某个其他函数G给出。所以这其实是非常直接的。这就是普通的RNN单元。
- en: And while different networks might have very different， characteristics， yeah。
    basically any of them would work nicely。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不同的网络可能有非常不同的特性，是的，基本上它们中的任何一个都能很好地工作。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_1.png)'
- en: Okay， so now let's compute the gradients of that。 So while I have a loss， right。
    in the loss I'm comparing the， output with the targets the Y's。 And so I sum over
    the sequence Y at T equals 1 up to capital T。 And I'm using time even though I'm
    talking about words。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 好，现在让我们来计算它的梯度。所以我有一个损失，对吧？在损失中，我将输出与目标Y进行比较。所以我对序列Y进行求和，从T=1到T。即便我在讲的是单词，我仍然使用时间。
- en: But that's just to indicate something that has a meaningful， sequential dependency。
    And so I just sum over those losses。 Okay， now if I want to take the gradient，
    well， the first。 thing is easy， namely， since I have a sum， the gradient just，
    pulls into the sum， right？
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是为了表示某些具有意义的顺序依赖关系。所以我只是对那些损失进行求和。好，现在如果我要计算梯度，第一个部分很简单，也就是说，既然我有一个求和，梯度就会进入求和，对吧？
- en: So now let's take the gradient of the losses and then it's just， a chain rule，
    right。 so I have the LDO， right？ Because the output's the only part that depends
    on it。 And now I can pull that chain rule further。 Namely， I have G of H and W。
    So if I were to write this out， my loss is a loss of Y。 And then I have some function
    G， so Y， T， H。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在让我们来计算损失的梯度，然后这只是链式法则，对吧？所以我有LDO，因为输出是唯一依赖它的部分。现在我可以进一步应用链式法则。也就是说，我有G的HT和W。所以如果我把这个写出来，我的损失是Y的损失。然后我有某个函数G，所以是Y，T，H。
- en: T， and W。 So if I take the derivative， Dw of this， then， of course， this。 is
    nothing else than Do of L。 Times now two terms。 Namely。 I can have Dw of G plus
    Dh T of G times Dw of H， T。 Right？ That's just basic chain rule。 Is everybody
    comfortable with that last line？ OK。 Is O just G of H， T， W？
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: T和W。所以如果我对它取导数Dw，那么，当然，这就是什么都不等于L的Do。再乘上两个项。也就是说，我可以得到Dw的G加上DhT的G乘以Dw的HT，对吧？这只是基本的链式法则。大家对最后一行还感到舒服吗？好，O不就是G的HT，W吗？
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_3.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_3.png)'
- en: Yeah， that's exactly what it is。 So previous slide， O， T， is G of H， T， and
    W。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这正是它的意思。所以上一张幻灯片中，OT是G的HT和W。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_5.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_5.png)'
- en: That's just expanded here。 Now， everything but that very last term， is easy
    to compute， right？
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是扩展了这里的内容。现在，除了最后一项，其他都很容易计算，对吧？
- en: So because we know H， T， so Dh T of G is easy to get， well， Dw of G is also
    easy to get。 And likewise， the first part。 The last part is exactly what's going
    to give us the headaches。 And the reason why this gives us the headaches， is because
    H， T depends on H， T minus 1。 And with that， on all the things that H， T minus
    1， inherits from H， T minus 2， H， T minus 3。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们知道HT，所以对G的DhT很容易得到，而对G的Dw也很容易得到。同样，第一个部分也是如此。最后的部分正是会让我们头痛的地方。造成头痛的原因是因为HT依赖于HT减去1。这样，HT减去1就依赖于它从HT减去2，HT减去3继承的所有东西。
- en: and so on。 So it basically goes all the way back。 It's a little bit like。 suppose
    I say something really stupid， in the first lecture。 You might still remember
    it now and still think Alex is， an idiot。 Or if I say something enlightening，
    you might think that I'm smart or whatever。 But basically。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后继续下去。所以它基本上会一直回溯。就像是，如果我在第一堂课上说了一些非常愚蠢的话。你现在可能仍然记得，并且认为Alex是个傻子。或者如果我说了一些启发性的东西，你可能会觉得我很聪明，或者类似的。但基本上。
- en: you may have very long range effects。 And those very long range effects may
    very well。 affect how you perceive the current lecture。 For instance， if you think
    Alex is an idiot。 then you're probably less likely to pay attention， to what I'm
    saying right now。 Because， well。 you've kind of set that in your mind， right？
    Or maybe the other way， right？ So long story short。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会有非常长距离的影响。而这些非常长距离的影响可能会非常影响你如何看待当前的讲座。例如，如果你认为Alex是个傻子，那么你可能就不太可能关注我现在说的话。因为，嗯，你已经在脑海中设定了这个想法，对吧？或者反过来，也是一样。所以长话短说。
- en: this Dw of H， T is what we're。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个H，T的Dw就是我们要的。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_7.png)'
- en: going to have fun with。 Let's actually look at that， right？ So Dw of H， T。 So
    H。 T is given by this function， of F of X， T， and H， T minus 1， and W。 And the
    first part， again。 is easy， right？ So the first part is just， well， you know，
    whatever。 comes from the function of the dependency from F。 The second part means
    we now take the derivative。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要好好玩一玩。我们实际上来看一下，对吧？所以H，T的Dw。H，T是由这个函数给出的，F（X，T）和H，T减1，和W。第一部分，还是很简单的，对吧？第一部分就是，嗯，你知道的，来自F的依赖函数的东西。第二部分意味着我们现在取导数。
- en: with regard to H of F。 And then now we have Dw of H， T minus 1。 So what we've
    just done is we've just written out， the recursion where we get the gradient of
    H。 T with regard， as a function of the gradient of H， T minus 1。 and some other
    terms that we can easily get。 And so if you then write this out even further。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于H的F，现在我们有了H的Dw，T减1。我们刚刚做的事情是写出了递归关系，表示H的梯度，T是H，T减1的梯度作为函数，和一些我们可以很容易得到的其他项。所以，如果你进一步写出这个过程。
- en: you'll basically get the sum from I equals T down to 1， from J equals C up to
    I of F of X。 J minus 1， and W。 So this is basically all the past dependencies
    in that chain。 times then the corresponding dependencies of F on W。 Yes？ What
    is F usually？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你基本上会得到从I等于T到1的和，从J等于C到I的F（X，J减1）和W。所以这基本上是链中所有过去的依赖项，再乘以F对W的相应依赖项。是吗？F通常是什么？
- en: Is it like a multiple percent drawn？ Yeah。 So F is going to be-- so F and G
    are going。 to be fairly simple functions。 And we'll see a work through example
    afterwards， right？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 就像是多个百分比抽取吗？是的。所以F将是——F和G将是相当简单的函数。我们稍后会通过一个实际的例子来讲解，对吧？
- en: Is there what the R and F models as well？ Yes。 So well， there'll be a little
    bit fancier。 OK。 so actually we saw examples of F and G already， on Tuesday， right？
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: R和F模型也是这样吗？是的。所以它们会稍微复杂一些。好的，实际上我们已经在星期二看到了F和G的例子，对吧？
- en: So when we implemented our first R and N， then these were just essentially linear
    function。 and then maybe some tangent or some relu， or something else on top of
    it。 The obvious question。 well， what about more modern models， is this really
    the smartest thing you can do？
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们实现第一个R和N时，这些基本上只是线性函数。然后可能会有一些切线或ReLU，或者其他东西加在上面。显然的问题是，嗯，更现代的模型呢，这真的是你能做的最聪明的事吗？
- en: And there are actually two possible directions， into which you can go。 And we'll
    cover them in a bit more detail。 Maybe today， maybe next Tuesday。 But we'll get
    to that。 For now， assume that these are fairly simple functions。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有两个可能的方向，你可以走。我们稍后会更详细地讨论它们，也许今天，也许下个星期二。但我们会讲到的。现在假设这些是相对简单的函数。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_9.png)'
- en: And so here's really the issue of what happens。 If you look at this gradient，
    right？
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里真正的问题是，如果你看这个梯度，是什么情况？
- en: You have a lot of terms， so this is expensive to compute。 And the other thing
    is you basically。 end up taking products of matrices， right？ Those gradient matrices，
    you know， the H of F。 The problem is that if I multiply a lot of those matrices，
    together， then if I get unlucky。 some of the eigenvalues， will blow out， or some
    of them may vanish。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你有很多项，所以这计算起来很昂贵。另一个问题是，你基本上会得出矩阵的乘积，对吧？那些梯度矩阵，你知道的，H的F。问题是，如果我把很多这些矩阵相乘，如果运气不好。某些特征值会爆炸，或者有些会消失。
- en: or maybe the entire product will vanish。 So you will get either gradients that
    explode or that vanish。 The only time when that doesn't happen， is if the eigenvalues
    are basically pretty close to 1。 And that's very rarely the case。 So in other
    words， disaster is pretty much。 engineered into that problem。 That's a real issue。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者可能整个产品会消失。所以你会得到梯度爆炸或消失的情况。唯一不会发生这种情况的时候，是当特征值基本上接近1时。而这通常是非常罕见的情况。所以换句话说，灾难几乎是这个问题的必然结果。这是一个真正的问题。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_11.png)'
- en: Now， this sounds all very abstract。 And yeah， so here's the example of what's
    happening。 So basically， let's say I have this red symbol here， the red output。
    Then I need to work my way all the way back through that red， chain up to the
    very first H。 which the very first H could， still have had some influence on what
    comes out。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这听起来非常抽象。是的，下面是发生的例子。假设，我有这个红色符号，这个红色输出。然后我需要一直回溯，通过这个红色链条，直到第一个H。而第一个H可能，仍然对输出产生一些影响。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_13.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_13.png)'
- en: That's bad news。 So one way to fix it is to say， well， let's just forget。 about
    the earlier part and only look at the dependence， that I've seen later。 So this
    looks like。 why are we doing that？ That doesn't feel right。 So people first started
    simply doing this without thinking。 very much about it。 And they implemented it。
    And then they ran the model and it worked。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这可是坏消息。所以解决这个问题的一种方法是说，嗯，我们就忘掉前面部分，只看后来学到的依赖关系。这看起来像是，为什么我们要这么做？这感觉不对。所以人们最开始就这么做，而没有太多思考。然后他们实现了它，运行了模型，它有效。
- en: So it's like， well， hey， we hacked it and it works。 So it must be good。 So what
    this simply means is I take this summer and I just。 stop somewhere by just going
    back in this case only maybe， two previous time steps。 And then I truncate。 It
    actually has a couple of rather beneficial。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就像，嗯，好吧，我们破解了它，它有效。那它一定是好的。那么这意味着什么呢？就是说，我拿这段时间，停下来，只是回顾这次情况下，可能只看前两个时间步。然后我就进行截断。它实际上有一些相当有益的效果。
- en: effects that might not be quite apparent。 The first thing is， yeah， this is
    a lot faster。 So because I just throw away a lot of the things that I， should
    have done， I just do less work。 So of course it's faster。 The second thing is
    I'm throwing away exactly the pieces。 that have many products of matrices。 So
    that's exactly the part that will either vanish or， diverge。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些效果可能不是很明显。首先，是的，这样做要快得多。因为我丢弃了很多我本该做的事情，我做的工作少了。所以当然速度更快。第二个好处是我正好丢弃了那些包含多个矩阵乘积的部分。那部分正是会消失或发散的地方。
- en: So that's the other good thing。 The third good thing is， well， it actually has
    some。 regularizing effect。 Because it forces me to focus mostly on the changes
    that。 happened recently as opposed to changes that I would have， observed a very
    long time in the past。 So by essentially forcing my model to mostly take advantage，
    of stuff that I've learned recently。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是另一个好处。第三个好处是，嗯，它实际上有一些。正则化的效果。因为它迫使我主要关注最近发生的变化，而不是我可能在过去很长时间里观察到的变化。所以，通过本质上迫使我的模型主要利用我最近学到的东西。
- en: I can get a model， that is a lot more robust and doesn't suffer quite that much。
    from the butterfly effect。 So I guess everybody knows about the fly effect is
    right？
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以得到一个更强大的模型，它不会像之前那样，受到蝴蝶效应的影响。所以我猜大家都知道蝴蝶效应吧？
- en: So it's essentially the thing about， hey， at some point。 some butterfly flaps
    its wings and all of a sudden we have a， tornado a year later in Minnesota。 Well，
    maybe not Minnesota， but some way where tornadoes are。 But the point is those
    effects where some minor change。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以本质上是，嘿，在某一时刻，某只蝴蝶扇动了翅膀，突然间一年后在明尼苏达州出现了龙卷风。嗯，也许不是明尼苏达州，但某个龙卷风频发的地方。但关键是，那些效果，即使是微小的变化。
- en: initially would have affected the later state very much， you。 kind of prevent
    them from being engineered into your。 model and therefore overfitting too much
    into the past by， just truncating。 So this kind of flies in the face of what everything
    that I。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最初会影响到后续状态的因素被，你知道，隔离开了，从而防止它们被引入到你的模型中，从而避免过度拟合过去，仅仅通过截断。所以这实际上是违背了我所知道的一切。
- en: said before about approximate sufficient statistics and we。 actually learn how
    they are done because that actually。 would assume that I am able to aggregate
    very nicely。 But at least I'm not propagating the effects back。 I'm still aggregating，
    still computing。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之前所说的近似充足统计量，我们实际上学到它们是如何完成的，因为这实际上意味着我能够非常好地聚合。但是至少我并没有将效果回传。我依然在聚合，依然在计算。
- en: Everything on that chain is just that I stop taking the， gradients beyond some
    part。 Any questions so far？ Yes？ So this is what you're saying。 Doing the three-one-dobby
    gradients at a certain time。 step doesn't affect the family that your model describes。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这一链条上的一切都只是因为我停止了某个部分的梯度传递。到目前为止有任何问题吗？是吗？所以你说的就是这样。在某个时刻进行三维梯度计算的步骤不会影响你的模型描述的家族。
- en: just the effects of how you are addressing it。 Correct。 Well。 it actually doesn't
    even affect how you calculate the， estimator。 It just affects how you train and
    optimize the parameters。 So the forward pass is completely identical。 It doesn't
    change。 It's only that for the gradient computation， I compute a， slightly incorrect
    gradient。 Sorry。 does that kind of answer your question？ [INAUDIBLE]， [INAUDIBLE]，
    No， no， actually。 what will happen is that the model will， likely converge to
    something different。 So in that sense。 yes， strictly speaking， I am， constraining
    the model space by just like any other， regularization。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 只是你处理它的方式的效果。没错。实际上，它甚至不会影响你如何计算估计量。它只是影响你如何训练和优化参数。所以前向传递是完全相同的。没有变化。只是对于梯度计算，我计算了一个稍微不准确的梯度。抱歉，这样回答你的问题了吗？[INAUDIBLE]，[INAUDIBLE]，不，不，实际上。发生的情况是，模型可能会收敛到不同的东西。所以从这个意义上讲，是的，严格来说，我正在像其他正则化一样约束模型空间。
- en: But the functional form is the same。 Now， being able to prove that this is exactly
    what happens。 and how the model space is being constrained， let's put， this way。
    If you figure this out。 this is a very nice paper to write， and you should write
    that immediately， because people。 would want to know。 This is not well understood。
    There are a couple of type problems where people sort of。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但功能形式是一样的。现在，能够证明这正是发生的事情，并且模型空间是如何被约束的，我们可以这样说。如果你搞明白了这个问题，这将是篇非常好的论文，你应该立刻写出来，因为人们会想要了解。这个问题还没有被很好地理解。有几个类型的问题，人们会在某种程度上。
- en: understand it。 There are types of regularization， and we might cover。 some of
    them later for how to restrict in constraining。 So for instance。 there is something
    called zone out， where， you simply force your recurrent neural network not to。
    update the state for one step to the next。 So that's like in the lecture。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 理解它。有很多类型的正则化，我们可能会稍后讨论其中一些方法，看看如何在约束中进行限制。比如说，有一种叫做zone out的方法，你可以简单地强制你的递归神经网络在一步到下一步时不更新状态。就像在讲座中之前提到的。
- en: let's suppose you zone out， for five minutes， and so your internal state about
    machine。 learning doesn't get updated， because you're thinking about。 I don't
    know going out on a date tonight。 And then once you zone back in。 your state is
    still the one， before you zone out， and that to some extent。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你zone out了五分钟，那么你的机器学习内部状态不会被更新，因为你在想着其他事情，比如今晚要约会。然后一旦你恢复注意，你的状态依然是zone out之前的状态，在某种程度上来说。
- en: supposedly it regularizes the way how you learn。 Not sure I'd recommend that
    zone out for this lecture。 because you might have to rewatch it on video， but
    these。 are techniques to make the state estimate a bit more robust。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是对你学习方式的一种正则化。虽然我不确定是否推荐在这个讲座中使用zone out，因为你可能需要重新看视频，但这些技术可以使状态估计更加稳健。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_15.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_15.png)'
- en: So let's actually have a look at what this looks like in， practice。 if we have
    something like the time machine， of HD Wells。 what you could do is you could just
    take the， entire chain， and that's the bottom arrow。 and then I have， to backprop
    through everything， and that's very expensive。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们实际看看在实践中它是怎样的。如果我们有类似于H.G. 威尔斯的时间机器这样的东西，你可以做的是直接拿整个链条，看到底部箭头。然后我必须对所有内容进行反向传播，而这非常昂贵。
- en: Or I could truncate that fixed intervals， that's the middle， line， and it's
    an approximation。 but it rocks really well in， practice。 Or what you can do in
    this is fun paper by TALEC and Olivier。 in 2015， you basically use an estimator
    to replace this， exact sum。 and I'll show you the math for that on the next， slide。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我可以在固定间隔处截断，那是中间的线，它是一个近似值，但实际上在实践中效果很好。或者你可以参考TALEC和Olivier在2015年发表的这篇有趣的论文，基本上你使用估算器来代替精确的求和，我会在下一张幻灯片展示给你看相关的数学推导。
- en: where you essentially use a random length to truncate。 Yeah， so I'm going back
    to previous slides。 so does I just。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你基本上使用一个随机长度来截断。是的，我回到之前的幻灯片。那我就是这么做的。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_17.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_17.png)'
- en: include like two or detaching the gradient？ Yep， so detaching the gradient is
    what you do in practice。 You're spot on， very good observation。 So detaching the
    gradient simply means stop taking any more。 derivatives beyond a certain part。
    Any other questions？
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 包括像是截断梯度吗？是的，截断梯度就是你实际操作中会做的。你说得对，非常好的观察。那么，截断梯度的意思就是在某一部分之后停止求导。还有其他问题吗？
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_19.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_19.png)'
- en: So there's variable links， so that's what TALEC and Olivier， did， and this is
    exact。 Basically what you do is you just reweight the longer。 segments with a
    higher weight than the shorter ones， and you。 have a suitable distribution over
    that。 And you can argue that this actually， so here's the math。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有变量长度，这就是TALEC和Olivier所做的，这个方法是精确的。基本上你做的就是重新给较长的片段加权，权重比较短的片段大，然后你就能得到一个合适的分布。你可以说这个其实是有效的，这是数学推导。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_21.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_21.png)'
- en: what happens。 Basically， what I do is in my recursion， I have a random， variable
    that's 0 or 1。 or rather 0 and 1 over 1 minus p。 And so therefore， with some probability，
    I just stop。 because once this random variable， psi t is 0， then， that recursion
    stops。 And if it's non-zero。 then it will continue， and then I， need to continue
    that chain。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？基本上，我在递归中使用了一个随机变量，它的值是0或1，或者更确切地说是0和1除以1减去p。所以，根据某个概率，我就停止了。因为一旦这个随机变量psi
    t是0，那么递归就停止。如果它是非零的，那么递归会继续下去，我需要继续执行这个链条。
- en: So that's what you get with that top line。 And if I make it so that in expectation，
    psi t has an。 expectation of 1， it just takes on value 0 and then 1 over 1， minus
    p。 Then you get an exact。 and you get an unbiased estimate of， the gradient。 The
    trouble is it doesn't actually work any better in， practice， so this was a neat
    paper。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你从上面那行得到的结果。如果我使得期望值为1，那么psi t的期望值就为1，它取值0，然后是1除以1减去p。这样你就能得到一个精确的结果，并且能够得到梯度的无偏估计。问题是，在实践中它并没有表现得更好，所以这篇论文是个很有趣的研究。
- en: It has probably about 10 citations， maybe after this， course it gets another
    20。 It's a cute idea。 but the fact that something else is going， on as well in
    truncated backdrop， namely that it。 constrains the model a little bit more， is
    one of the， reasons why basically you don't do that。 But maybe somebody can make
    it work。 So let's actually look at the computational graph。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它大概有10次引用，也许在这门课程后会多出20次。这是一个有趣的想法，但事实上，在截断反向传播中，可能还有其他的东西在发生，换句话说，它对模型的约束更多，这就是为什么基本上不推荐这样做的原因之一。不过也许有人能让它起作用。所以我们来看一下计算图。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_23.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_23.png)'
- en: So this is for a three-step HMM， a very explicit model。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个三步HMM，非常明确的模型。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_25.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_25.png)'
- en: where we just write the equations first。 So if ht is WHX times xt plus WHH times
    ht minus 1。 and then， ot is WOH times ht。 So it's an entirely linear model。 So
    nonlinearities whatsoever。 this is really about as。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们先写出方程。所以，如果ht是WHX乘以xt加上WHH乘以ht减去1，那么ot就是WOH乘以ht。所以这是一个完全线性的模型，完全没有非线性。这基本上就像是……
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_27.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_27.png)'
- en: dumb and simple as it goes。 So let's look at the graph。 So I start with some
    h0， then I observe x1。 so I get h1， then I get h2， then I get h3。 And this is
    then mangled with WOH。 So this is the second weight matrix in the middle， and
    then， I get the output。 Then this is compared with all the yi's， and this gives
    me， my loss。 So that's the dependency graph。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简单明了。所以我们来看一下这个图。我从一些h0开始，然后观察x1，然后得到h1，再得到h2，然后是h3。然后这个结果与WOH混合。这里是中间的第二个权重矩阵，然后我得到输出。接着这个输出与所有的yi进行比较，这样就得到了我的损失。这就是依赖图。
- en: what's actually happening on， a computer， basically in the MXNet or PyTorch
    or TensorFlow， runtime。 when you implement this。 Well， they're going to do something
    a little bit fancier。 to how to tie parameters together， but short of that。 That's
    pretty much what's going on。 Any questions about that graph so far？ Yes？ Is it
    basically very straightforward to transition。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上发生在计算机上的是什么，基本上是在 MXNet 或 PyTorch 或 TensorFlow 运行时。当你实现这个时，嗯，他们会做一些稍微复杂一点的操作，来将参数绑定在一起，但除此之外，这基本就是正在发生的事情。到目前为止，对于这个图表有什么问题吗？是吗？过渡其实是非常直接的吗？
- en: to instead of being three-state or four-state？ Well， you would basically chop
    off one layer， right？
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不使用三状态或四状态呢？嗯，你基本上是切掉一层，对吧？
- en: So if you were to chop off the top row， short of the L， obviously。 then you
    get something that has only two， int states。 If you were to add another layer
    on top。 you'd get， something with four int states。 It's just another time step。
    This is just unrolling everything in its most， excruciating detail。 Yes？ [INAUDIBLE]，
    Oh， OK。 OK。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你切掉最上面的一行，忽略 L，显然。然后你得到的就是只有两个整型状态。如果你再加上一层，你会得到一个有四个整型状态的东西。这只是另一个时间步。这个就是将一切展开到最详细的状态。是吗？[无法听清]，哦，好的。好的。
- en: So let me be more specific here。 So with hidden states， I mean one state per
    observation。 Now。 what you're probably thinking of is hidden units。 So hidden
    units。 that's essentially the dimensionality， of H。 And I can pick that to be
    one。 or I could pick it to be 10，000。 That is essentially the richness， the capacity
    of how。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我在这里更具体一点。对于隐状态，我的意思是每个观察一个隐状态。现在，你可能在想的是隐单元。那么隐单元，实际上就是 H 的维度。我可以选择它为 1，或者选择它为
    10,000。那本质上就是如何确定其丰富性和容量。
- en: much I can store in that hidden state。 That's a separate design parameter。 That's
    something I can easily adjust。 The number of hidden states is essentially one
    per clock。 So one， for every time the RNN does something。 Now， there are exceptions
    to that rule。 For instance。 if I have an LSTM， I will have two hidden states。
    And one is the hidden state。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以在那个隐状态中存储多少信息。那是一个独立的设计参数。那是我可以轻松调整的。隐状态的数量本质上是每个时钟一个。所以每次 RNN 执行某些操作时就是一个。现在，这条规则是有例外的。例如，如果我有一个
    LSTM，我将有两个隐状态。一个是隐状态。
- en: And then one's called the memory。 And we'll cover that later today。 But other
    than that。 what you're thinking of is， probably dimensionality。 Now， there's something
    else。 You can ask。 can I make it more linear， more complex？ And that is in the
    depth of the network。 And we'll probably get to that next week。 So I guess， why
    would you want more or less hidden units？
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后一个叫做记忆。我们今天稍后会讲到。但除此之外，你考虑的可能是维度性。现在，还有其他问题。你可以问，我能让它更线性，更复杂吗？那就是网络的深度。我们可能下周会讨论这个。那么，为什么你会需要更多或更少的隐单元呢？
- en: I knew you would have infinite number of hidden units， right？ So you process
    it against that。 OK。 so having more hidden units doesn't let me do anything， faster。
    What it does is it simply allows me to store possibly more， information in my
    hidden state。 So if you can assume that your hidden dynamics is very simple。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你会希望有无限数量的隐单元，对吧？所以你将它和这个进行处理。好的。所以更多的隐单元并不能让我做得更快。它的作用仅仅是允许我在隐状态中可能存储更多的信息。所以如果你可以假设你的隐状态动态是非常简单的。
- en: then maybe you don't need a lot of dimensions in the hidden， state。 Maybe we
    can actually play with that a little bit later。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你在隐状态中不需要很多维度。也许我们可以稍后稍微调整一下。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_29.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_29.png)'
- en: So let's see。 Let's actually work this out just to make this more explicit。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们来看一下。让我们实际算一下，让这个更清晰。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_31.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_31.png)'
- en: So now all the weight vectors have names。 So WH is output times hidden state，
    HH is hidden hidden。 HH is hidden observed。 So WH， that's an easy one。 That's
    basically dOT of LOT and Y。 That's easy。 WH， well， that's the one that's going
    to give us headaches。 Because if you look at it。 that's d WH of HT。 And then we
    have the last one， d WHX of HT。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有的权重向量都有了名称。WH 是输出与隐状态的乘积，HH 是隐状态与隐状态的乘积。HH 是隐状态与观察状态的乘积。所以 WH，那个很简单。基本上是
    dOT（点积）于 LOT 和 Y。这个很简单。WH，嗯，那个会给我们带来麻烦。因为如果你看它，那个是 d WH 的 HT。然后我们有最后一个，d WHX 的
    HT。
- en: So this is completely straightforward。 Now let's iterate。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这完全是直接明了的。现在让我们开始迭代。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_33.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_33.png)'
- en: So we get the recursion， namely dHT of HT plus 1 is WHH， transposed。 So in other
    words， therefore。 if I have H capital T， and I， want to take the derivative with
    regard to H， little T。 that's WHH transposed to raise， the power of capital T
    minus little T。 So there you can already see very explicitly what， I meant with
    gradients misbehaving。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Because if that power of the matrix WHH has some eigenvalues， that are larger
    than 1。 they will go to infinity， basically， that they grow exponentially。 Whereas
    if I have some eigenvalues that are less than 1， the corresponding spaces will
    go to 0。 Yep？
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: So are we two of these W-matrices， right？ Why can't we just apply some sort
    of like spectral。 normalization or something to use the derivative？ That's a--，
    OK， that's actually a great idea。 Can we make these matrices well conditioned？
    So in the fully linear case， you could possibly。 do something like that。 You might
    then have to deal with the fact that you're actually。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: now performing optimization on a manifold， because you're。 basically taking
    the updates and projecting back。 And that may give you weird behavior in your
    gradients。 Because if you--， let's say on the surface of a sphere， and if I'm
    sitting here。 this is my gradient。 Can I move here， and I need to transport back，
    and I then。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: move here and I transport back？ So weird things like that may happen， but maybe
    you could。 fix that with math。 The thing that's harder to fix with math is if
    you have。 another nonlinearity following it。 So in practice， it's not going to
    be just WHH， but it's。 going to be some functions of interactions of that with，
    some other data。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: And that's where essentially your mathematical safeguards， will no longer do
    you much good。 But it's a very good suggestion， and for a linear system， people
    actually do stuff like that。 Basically， if you look at the full recursion， while
    you see， basically sums of powers of W's。 and then you just throw away， the things
    that you don't like。 And so what you're doing in practice。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: and this was exactly。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_35.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: the astute observation from before， you just call detach。 And that's also one
    of the reasons why random sampling， of the data isn't such a great idea as。 sequential
    sampling is going to be better。 Good。 So this is essentially all that is to truncate
    a backprop。 Any questions？ Yep？ Are we detaching all of the states？
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: So you do that at the beginning of every mini batch。 And so what's actually
    happening is that you have， slightly variable links within the mini batch。 so
    you get， actually a little bit of a portfolio of dependency ranges。 Because you
    have all the subsequent parts。 But basically， you're detaching between mini batches。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Yes？ For the terms we care about， the gradients are reattached， during the wizard
    homograd。 No。 OK。 so I basically go and say， well， up to here， this is like， you
    cut the tape here。 You go and record more and more and more things。 And now for
    the new things that you record。 autograd will， compute the gradients。 It's just
    that you've cut your dependency graph here。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 是的？对于我们关心的那些术语，梯度会在向导的同化过程中重新连接。没有。好吧，基本上我可以这样说，嗯，到这里为止，就像是，你在这里切开胶带。然后你去记录更多的东西。对于你记录的这些新东西，自动求导会计算梯度。只不过你已经在这里切断了你的依赖图。
- en: So autograd isn't going to reattach things， but it will。 just follow up on the
    path that are remaining。 If you want to do fancy graph surgery。 that requires
    that， you probably look a little bit more into the MXNet engine。 But short of
    that。 this will pretty much do the trick。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以自动求导不会重新连接这些东西，但它会继续。只要跟踪剩余的路径。如果你想做一些复杂的图形操作，那就需要更多地了解 MXNet 引擎。但是除了这些，这基本上就能解决问题。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_37.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_37.png)'
- en: OK。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_39.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbaeae0c9a7e091ec9ac46dac6a853b_39.png)'
- en: And with that， we--。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这样，我们——。
