- en: P86：86. L16_1 Object Detection - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P86：86. L16_1 物体检测 - Python小能 - BV1CB4y1U7P6
- en: So in image classification， we're trying to identify the main objects on the
    image。 Usually we assume this image only contains a single object， a large one。
    But for object textures。 we're trying to grab all this interesting objects in
    a single image。 So here we have two dots。 one has。 Besides we know what objects
    we have， we also want to know the location of these objects。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在图像分类中，我们试图识别图像中的主要物体。通常我们假设这张图像只包含一个物体，一个大的物体。但对于物体检测，我们尝试在一张图像中抓取所有这些有趣的物体。所以这里有两个点，一个有。此外，我们不仅知道有什么物体，我们还想知道这些物体的位置。
- en: So here we're using rectangle boxes here。 It's called a bounding box to define
    where the object is。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们使用的是矩形框。它被称为边界框，用来定义物体的位置。
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f773bfb29be35980da71d54b9d9f4c5a_1.png)'
- en: So this is in reality， especially interested by a set of driving cars。 We want
    to take a picture of the street and try to identify all the cars。 You see there's
    a person。 the bicycle， and they identify two bicycles here。 There's a traffic
    light。 So this is an interesting object we are interested in。 A car， humans， traffic
    light。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这实际上，特别是对一组自动驾驶汽车来说非常感兴趣。我们想拍摄街道的照片并尝试识别所有的汽车。你会看到有一个人，那个自行车，他们在这里识别了两辆自行车。这里有一个交通灯。所以这是我们感兴趣的一个物体——一辆车，人类，交通灯。
- en: And there's other objects we are not so interested in， especially for self-driving
    cars。 So we only identify a few objects。 We can see that it's actually pretty
    hard。 First of all。 the traffic light is not so easy to identify， even by human。
    Also they find a car behind the truck。 For human， it's also pretty hard to notice
    that。 So well。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些我们不太感兴趣的物体，特别是对自动驾驶汽车来说。因此我们只识别少数几个物体。我们可以看到，实际上这相当困难。首先，交通灯并不容易识别，即使是人类。还有他们在卡车后面找到了一个车。对于人类来说，这也很难注意到。所以嗯。
- en: you can see that originally how to label the data set。 Let's think about the
    labeling。 Now you see an image。 You know the object here。 But now you give a labeler
    or yourself。 This is a kind of image you want to cross a lot of things in the
    image。 And pretty hard。 especially the small cars， small things， traffic light。
    So labeling the object text is pretty hard。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到最初是如何标注数据集的。让我们想想标注的过程。现在你看到一张图像，你知道图像中的物体。但现在你交给标注员或者你自己。这就是你想要标注的图像，你需要在图像中标出许多东西。特别是那些小车、小物件、交通灯。标注物体检测其实是非常困难的。
- en: So also make it really much harder。 The reality， well， you not give a state
    image。 You just give a video。 If the car is driving on the street。 you want to
    continue to identify all these objects， in damage。 So which means we can't put
    a throughput。 Especially if you run the car。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这也让它变得更难。现实情况是，你并不是提供一张静态的图像，你只是提供一段视频。如果车在街上行驶，你希望继续识别所有这些物体，并且处理损坏的情况。所以这意味着我们不能提供吞吐量，尤其是当你开车时。
- en: you don't want to have very powerful GPUs because you burn， your gas or burn
    your money actually。 And it makes the car very hot。 You cannot make the car very
    hot。 Then you need to have air conditioning in the heart。 Question。 Is the GPU
    both in the street training？ Not in the audience？ Are you comfortable？
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你不希望拥有非常强大的GPU，因为你会烧掉你的油或者烧掉你的钱，实际上。并且这会让车变得非常热。你不能让车过热。那你就需要在车内装空调。问题是，GPU是在街道上训练吗？不是在观众中？你觉得舒服吗？
- en: Both training and inference are pretty efficient for GPUs。 Once the network
    is big enough。 So if you put the GPU in the car， you have a temperature problem。
    If you want to reduce the temperature， you need air condition to eat your more
    gas。 But if you don't care about temperature， the GPU will burn out and the driver
    is driving。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和推理对于GPU来说都非常高效。一旦网络足够大。所以如果你把GPU放进车里，你会遇到温度问题。如果你想降低温度，你需要空调，这会消耗更多的油。但如果你不关心温度，GPU会烧坏，而司机还在开车。
- en: at the middle of the road。 It's just burned out and not able to have。 So which
    means object detection is much harder。 You need care about the inference speed
    very much。 Okay。 Let's start with the basic concepts。 The boxes， the bounding
    box and the anchor boxes。 So this is a new concept we have from image classification。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在路中间。它已经烧坏了，无法继续运行。所以这意味着物体检测会更困难。你需要非常关注推理速度。好了，先从基本概念开始。框，边界框和锚框。这个是我们从图像分类中得到的新概念。
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f773bfb29be35980da71d54b9d9f4c5a_3.png)'
- en: So a bounding box you can define by a rectangle。 You can， well。 the reason why
    you have such a thing， I don't know， you can have a cycle， as well， even simpler。
    But here， a bounding box can be defined by four numbers。 So one， for example。
    you can do by top left x， top left y， bottom right x， bottom right， y。 So for
    example。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕框可以通过矩形定义。你可以，嗯，至于为什么会有这样的东西，我不确定，你也可以有一个循环，更简单。这里，围绕框可以通过四个数字来定义。比如说，你可以通过左上角
    x，左上角 y，右下角 x，右下角 y 来定义。举个例子。
- en: in this image， the origin is on the top left。 So zero， zero is on the top left。
    You can see that we can define this bound box by 60。 The x axis is 60 and the
    y is 45 and also the right bottom ones。 Okay。 On the other hand。 you can also
    define by top left x， top right y， the width of the bounding。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图片中，原点位于左上角。所以零，零在左上角。你可以看到我们可以通过 60 来定义这个框。x 轴是 60，y 轴是 45，右下角的坐标也是如此。好的。另一方面，你也可以通过左上角
    x，右上角 y，围绕框的宽度和高度来定义。
- en: box and the height of the bounding box as well。 But in any case， it's just the
    four numbers。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 框和围绕框的高度也是如此。但无论如何，这只是四个数字。
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f773bfb29be35980da71d54b9d9f4c5a_5.png)'
- en: Okay。 So then what object detection does that looks like is that now each row，
    the bottom。 is not just the image and the objects you have。 It's now each row
    identifying an object in the image。 So for example， you can specify the image
    file name and the object class you have and。 the bound box is the four numbers
    of the objects。 So which means if you have few images。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，目标检测做的事情看起来是，现在每一行的底部，不仅仅是图片和你拥有的对象。现在，每一行都在识别图片中的一个对象。例如，你可以指定图片文件名和对象类别，以及对象的围绕框。这意味着如果你有很少的图片。
- en: you may have 10 times more objects you have and， each object is the training
    examples you have。 So for example， the largest care public dataset is called a
    coco。 Three years ago， if such coco。 you get the first result is the dataset and
    nowadays you， got the movie。 So I do have， well。 students asking to， okay， train
    on neto in coco and then hit down on， the movie and how I can start。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有十倍以上的对象，每个对象就是你拥有的训练样本。例如，最大的公共数据集叫做 coco。三年前，如果是 coco，你得到的第一个结果是数据集，如今，你得到了电影。所以，我确实有，嗯，有学生问，好吧，如何在
    coco 上训练网络，然后用电影数据集进行测试，怎么开始。
- en: Well， so the coco dataset have 80 objects。 And so this is pretty daily life
    objects。 That's more useful comparing to image net contains a lot of animals you've
    never saw， that before。 Also， there's less images。 It's only currently only have
    3，300，000 images。 It's a less comparing image net。 But in average， we have four
    objects in the image。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，所以 coco 数据集有 80 个对象。这个是比较日常生活中的对象。与包含很多你从未见过的动物的 ImageNet 数据集相比，更加有用。此外，图片数量较少。目前只有
    3,300,000 张图片，比 ImageNet 少。但平均而言，我们每张图片中有 4 个对象。
- en: So we have about 1。5 million objects in this dataset。 Usually one million is
    a good number。 One million size dataset usually can deal reasonable deep learning
    models。 If the two largest 10 million， the four images have 10 million images。
    It's kind of hard to train。 If it's just 100，000 images or examples， well， it's
    pretty too small you need to find tuning。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这个数据集中大约有 150 万个对象。通常一百万是个不错的数字。一百万大小的数据集通常可以处理合理的深度学习模型。如果是两百万，十百万，四个图片有一千万张图片，那就很难训练了。如果只有十万个图片或者样本，那就太少了，你需要进行微调。
- en: So what many images is good and give a single GP machine take a few hours to
    a few days。 That's manageable。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以更多的图片很好，让一台单独的 GP 机器花费几小时到几天。这个是可以管理的。
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f773bfb29be35980da71d54b9d9f4c5a_7.png)'
- en: Okay。 Then the other concept is called anchor box。 Well， the name is pretty
    random。 But you can。 the idea here is that how in all of the detection algorithm
    works here， different。 to the classification we want to predict the body box。
    What do you do here？
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。接下来，另一个概念叫做锚框。嗯，这个名字比较随意。但你可以理解，这里的想法是，在所有检测算法中，如何与分类不同，我们想要预测的是边界框。你在这里做的是什么？
- en: The algorithm usually first propose multiple regions。 Each region we call the
    anchor box。 For example， this we have a picture here。 We show four anchor boxes
    centered at a single pixel。 And each anchor boxes have a different size and a
    different width and height ratio。 So the anchor is going to propose multiple anchor
    boxes。 And then for each anchor box。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通常首先提出多个区域。我们将每个区域称为锚框。例如，这里我们有一张图片。我们在单个像素处展示了四个锚框。每个锚框的大小、宽高比都不同。所以，锚框将提出多个锚框。然后，对于每个锚框。
- en: we're going to predict if this anchor box contains objects， or not， or just
    a background。 If we predict the contents objects， then we're going to predict
    how to map this anchor box。 to the real， the labeled， the real actually bonding
    box。 So this is one choose。 So we only。 because as four numbers， we just predict
    the offset， which is another four。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: number to transfer this anchor box to a bonding box。 So different detection
    algorithms have different way to propose all these anchor boxes and。 give an anchor
    box how to predict the labels， how to predict the class， how to predict object。
    classes， how to predict offset。 But in general， they are pretty works as this
    way。 Any question？
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Depends on algorithms。 For example， we're going to talk about SSDs。 For each
    image。 it's proposed 10，000 anchor boxes。 For different algorithms， usually。 we're
    going to talk about the first RCM family。 It's also proposed to thousands of anchor
    boxes。 but do a refinement to reduce to maybe， hundreds。 And even though a fast
    algorithm is trying to reduce the number of anchors。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: you see that， the more you propose， the coverage may maybe get because the more
    you propose。 the likely， you can cover all these objects。 But the more you have。
    the more expensive competition that you have。 So this is a trade-off question。
    We don't have the true label for each box， but we have the label。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: So here， the other things are boxes。 But by bounding box， we mean the ground
    truth。 which is labeled by human。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_11.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: And the anchor box is just proposed by algorithms。 So then we're going to talk
    about how to map anchor box into the ground truth。 How to join that？
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Another question？ What is the anchor box that would happen？ Okay。 So given the
    anchor box。 you have four numbers to present anchor box。 These four numbers。 A
    bounding box is another four number。 So you want to predict how to change this
    four number to another four number。 We just offset this number。 This anchor box
    plus offset equals to the bounding box。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Or the bounding box for numbers minus the anchor box you get offset。 >> Is there
    something to make it too unaggression？ >> Yes。 Because it's a real number， it's
    weird。 we are doing regression at the end。 Okay。 Any questions？ Is the concept
    a change？
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Like the setup or the things。 Okay。 Then， where all this difficulty is on the
    boxes。 we cannot dive more for the boxes。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_13.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Firstly， how to measure the similarity between two boxes？ Well， in classification。
    we know either it's true or false。 But for boxes， we usually do this IOU。 it's
    called intersection over union。 But the idea here。 we compute the intersection
    between two bound boxes and divide it by the。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: union of these two bound boxes。 So this is， we get the number between zero and
    one。 Zero means the two bound boxes not overlap any one and one zero or one means
    die then equal。 to each other。 So the closer to one， the more similar you have
    for these two boxes。 So the IOU is called on， well， we are the lamb by the object
    detection community。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: It's actually a special case of the JACART index。 So given two sets， A and B。
    the index can be computed by the set size of the intersection。 between set A and
    B and the size of the union of the set A and B。 So if we think about the bound
    boxes just a set of pixels and each pixel element on， the set。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: you can think of just a easy application here。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_15.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: The other thing that， yes， during training， the algorithm proposed a bunch of
    anchor boxes。 and then you need to get labels for this anchor box。 So we in the
    map all this labeled bound boxes to the anchor boxes。 So during training each
    anchor box is a training example and we need to associate the object。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: with the anchor box either as a background。 So in bound boxes we don't have
    a background。 But the anchor box because we made random purpose， so maybe we have
    nothing there。 So it's just a special class that's called a background。 And all
    associated with the bound box and associated with the objects。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: So then we can now talk about how to associate with the bound box but you can
    see that we。 may generate a larger number of anchor boxes。 And only a few of them
    contains the objects we have。 A larger amount of objects are just the backgrounds。
    And how to， so this is unbalanced。 the classification problem， how to make it
    trainable is pretty， hard。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_17.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Okay， so here let's see a typical way how to match anchor boxes to a bound box。
    So each row presents the anchor box and each column is a bound box。 So here we
    have。 we label the four bound boxes but we have nine anchor boxes。 Usually anchor
    boxes are larger than the， we have more anchor boxes than the bound boxes。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: So each element here is a， IOU score of the anchor box to the bound box。 Okay？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Then what do we first do here？ We find the largest score。 For example here is
    the largest score is the anchor two with the bound box three。 So then we assign
    bound box three to anchor two， which means that this anchor box two。 we have the
    objects contains on the， on the bound box and offset how the particular offset。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: is just the difference between these two boxes。 After we do that。 we remove
    bound box three and anchor two in the set。 So we just mark it as white。 So next
    time we cannot consider all these IOU values。 Next we're gonna pick up the largest
    one in all these blue cells。 Which is the anchor。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: anchor seven mapped to bound box one。 This is the largest IOU we can have。 Then
    we assign bound box one to anchor seven。 Okay？ Then we remove both one。 three
    for the bound boxes from the set and also like two and seven， for the anchor boxes。
    And to repeat that again to get a map bound box four to anchor box five。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: We just repeat until no bound box left。 And the rest of the anchor boxes we
    just label as back one。 So we finish the labeling。 Okay， question here？ Question。
    >> How do you know if you bound in boxes two years？ >> Well you don't know。 It
    depends how they are labeled。 So for example， you mean how many use we gonna use
    all of them？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: We gonna match all the bound box to the anchor boxes。 If you cannot map then
    the bound box you propose to probably not a good idea。 But the bound boxes varies。
    Like maybe one majority can test two or maybe some large。 If you serve a drawing
    card they're labeling 4K images。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Like 4K videos they have maybe 10 to maybe 50 objects in the images。 Okay。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_19.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Then the other things like now in the prediction each anchor box we gonna generate
    one bound。 box prediction。 So which means and also the offset。 Which means we
    maybe get a lot of very similar bound boxes。 Here for example in this dock we
    generate this each boxes actually the anchor box with offset。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Which is very close to the rear box。 But well we have three predictions at the
    lumbots here which means the probability。 At the probability the algorithm think
    is a contest a dock。 So we want to reduce all these duplications here。 A typical
    one is called an MS like a law maximum suppression。 This is another time for technical
    time for detection set。 It's actually a simple algorithm。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: What we do is very similar to the one we had before。 So what do we do here？
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of predictions we pick up the one with the largest probability score。
    Which means I'm most of for sure it is a dog or it is a cat depends。 So for example
    we're gonna pick up the blue one of dog equal to 0。9。 And then we're gonna remove
    all these other predictions with the IOU。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Log it on a threshold compared to the one I picked。 For example if I pick the
    theta equal to 0。5 you should I can remove all this， the other two dog predictions
    here。 I'll remove all of them and then select the largest one in the remaining。
    And then remove the duplication。 And then we repeat and here either we select
    or。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: remove all these predictions。 So at the end you can see that we're gonna have
    two bonnet boxes here。 Okay， questions？ Questions？ >> [INAUDIBLE]， >> You mean
    what 0。9 means？ >> What is it？ >> Oh。 you mean for this particular example？ >>
    Yeah。 >> I can show you in a few。 I don't remember but I can show you this is
    actually from， the source code。 I probably 0。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 5 I think here。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_21.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Okay， so。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f773bfb29be35980da71d54b9d9f4c5a_23.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
