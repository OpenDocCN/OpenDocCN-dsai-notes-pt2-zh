- en: P4：4. L1_4 Linear Algebra - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P4：4. L1_4 线性代数 - Python小能 - BV1CB4y1U7P6
- en: The first tool that we'll need to cover when we want to deep learning is linear
    algebra。 Well。 the reason is very simple because a lot of the tools that are required
    in deep learning。 are large amounts of computations。 And linear algebra is just
    really good at representing those。 It's also really good at representing many，
    many of the functions that we'll work with。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要进行深度学习时，第一个需要覆盖的工具是线性代数。嗯，原因很简单，因为深度学习中需要的大量工具，都涉及大量的计算。而线性代数非常擅长表示这些计算。它也非常擅长表示我们将要使用的许多函数。
- en: So we need to get started。 Now， this obviously is no substitute for a proper
    linear algebra class。 But we'll try to cover all the basic things that you'll
    need at least to get started。 and the rest will probably cover in learning by
    doing。 So let's start with something very simple。 First thing are scalars。 The
    scalars is something that we all know。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要开始了。现在，这显然不能替代一堂正规的线性代数课程。但我们会尽量覆盖你至少需要的所有基本知识，剩下的可能会通过实践来学习。所以让我们从一些非常简单的东西开始。首先是标量。标量是我们都知道的东西。
- en: You can do simple things with them。 You can add them。 You can multiply them。
    You can compute functions like sine or cosine of them。 And you can do things like，
    well。 what's the length of a？ Well， it's a if a is greater than zero and minus
    a otherwise。 so it's just the absolute value。 This does a couple of nice things。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用它们做简单的事情。你可以加它们。你可以乘它们。你可以计算像正弦或余弦这样的函数。你还可以做类似的事情，嗯，a 的长度是多少？如果 a 大于零，就是
    a，否则是负 a。所以它就是绝对值。这做了几件很不错的事。
- en: It satisfies the triangle inequality。 So a plus b is less equal to a。 the length
    of a plus the length of b。 And the absolute value of a times b is the absolute
    value of a times the absolute value of b。 No particular surprises here。 Now， let's
    look at vectors。 Again， I can do very simple things。 I can add them。 So c is a
    plus b。 And in this case， I have ci is ai plus bi。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 它满足三角不等式。所以 a 加 b 的范数小于等于 a 的范数加 b 的范数。而 a 乘以 b 的绝对值就是 a 的绝对值乘以 b 的绝对值。这里没有什么特别的惊讶。现在，让我们来看向量。再一次，我可以做非常简单的事情。我可以加它们。所以
    c 是 a 加 b。在这种情况下，我有 ci 是 ai 加 bi。
- en: I can also multiply alpha and b。 And now in this case。 so you already noticed
    it's not I'm multiplying a and b。 What I'm doing is I'm basically coordinate wise
    multiplying the bis with some coefficient alpha in order to get ci。 I can also
    do things like the sign of a and in this case， I'm just applying it element wise。
    Now。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我也可以将 alpha 和 b 相乘。在这种情况下，你已经注意到，我并不是在乘 a 和 b。我所做的是，我基本上是逐坐标地将 b 的每个元素与系数 alpha
    相乘，从而得到 ci。我也可以做类似 a 的符号这样的事情，在这种情况下，我只是逐元素地应用它。现在。
- en: the interesting thing is when it comes to measuring the length， well， you know。
    I have a little bit of freedom。 But the most common thing that we would do is
    we would probably define the length of a vector a to be just the Euclidean length
    of that。 So the sum over i going from 1 to m ai squared and then the square root。
    I mean， for， you know。 two dimensions， that's a Pythagoras theorem。 And in general，
    well。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在度量长度时，嗯，你知道。我有一点自由度。但我们最常做的事情可能就是将向量 a 的长度定义为它的欧几里得长度。所以对 i 从 1 到 m 求和，ai
    的平方然后开根号。我的意思是，对于二维，这是一个毕达哥拉斯定理。一般来说，嗯。
- en: that's just how you define the length， right？ Now。 a really useful thing of
    links or norms as mathematicians call them is the norm of a is greater or equal
    than 0 for all a。 And that the norm of a plus b is less equal to the norm of a
    plus the norm of b。 So this is just our triangle inequality again。 And furthermore，
    if I scale a with b， then I get。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你如何定义长度，对吧？现在，链接或范数作为数学家所称的一个非常有用的概念是，a 的范数大于或等于 0 对所有 a 都成立。而且 a 加 b 的范数小于等于
    a 的范数加上 b 的范数。所以这就是我们的三角不等式。进一步来说，如果我用 b 来缩放 a，那么我得到的是。
- en: you know， absolute value of a times the length of b。 Why the absolute value
    of a？ Well。 because otherwise I could get something negative。 Now。 the norm that
    we just looked at isn't the only one。 There are lots of others。 For instance。
    imagine that you're in Manhattan and you want to go from maybe 2nd Avenue 5th
    Street to maybe 4th Avenue 10th Street。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，a 的绝对值乘以 b 的长度。为什么是 a 的绝对值？嗯，因为否则我可能会得到一个负数。现在，我们刚才看的范数不是唯一的。还有很多其他的。例如，想象一下你在曼哈顿，你想从第二大道第五街到第四大道第十街。
- en: Well， you would basically have to traverse the avenues and you would have to
    traverse the streets。 And you can't re-walk diagonally unless there is some derelict
    block and you go across a construction site。 But in general， you basically go
    either East West or North South。 And so therefore the distance is really the distance
    in streets plus the distance in avenues。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，你基本上必须沿着大街走，你必须沿着街道走。你不能斜着走，除非有一个废弃的街区或者你穿过一个建筑工地。但通常情况下，你基本上只能选择东西走向或者南北走向。因此，距离实际上是街道上的距离加上大街上的距离。
- en: which is why a distance that's defined this way is sometimes also called the
    Manhattan distance。 Make a long story short， it's basically the sum of what the
    absolute values of the coefficient-wise distances。 In other words， also the L1
    distance。 Now， if you look at it in pictures， well， vector addition。 let's say
    we add the blue and the orange vector， well in that case we get the green vector。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么这种定义的距离有时被称为曼哈顿距离。长话短说，它基本上是系数间距离的绝对值之和。换句话说，它也是L1距离。现在，如果你看图的话，好吧，向量加法。假设我们将蓝色和橙色的向量相加，那么我们得到的是绿色向量。
- en: All you do is you take one vector and you piece it together with the other vector
    and then you get the diagonal vector that combines them。 Likewise， scaling vectors
    is easy so I can， for instance， half C equals alpha times B。 And all I'm doing
    is I'm taking the original vector and just making it longer。 Now。 why do mathematicians
    like this？ Because it's essentially their version of writing a parallel for all
    do。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你所做的就是将一个向量与另一个向量拼接在一起，然后你得到一个组合了它们的对角线向量。同样，缩放向量很容易，所以我可以，例如，设C的一半等于α乘B。我的操作就是将原始向量拉长。现在，为什么数学家喜欢这样做？因为这本质上是他们写“所有做”并行的方法。
- en: Now the latter is by the way also music is because this is exactly what GPUs
    are really good at。 where it's also easy to write code。 So this is sometimes called
    a trivial way of parallelizing things。 And we all like it when it's easy for our
    GPUs to do work nicely in parallel。 which is why people like to write things down
    in pictures or as we'll soon see also matrices。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，后者也是音乐，因为这正是GPU擅长的领域。它也很容易编写代码。因此，这有时被称为一种平行化事物的简单方式。当我们的GPU能够高效地并行工作时，我们都喜欢这样，这也是为什么人们喜欢用图形或者像我们很快将看到的矩阵来表示事物的原因。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_1.png)'
- en: A rather important property is the dot product。 So the dot product is the sum
    of the coefficient wise products。 So in other words， A transpose B is the sum
    over I of AI BI。 Now we call two vectors orthogonal。 If A transpose B equals zero。
    So you might ask， well， is that something special？ Yes。 actually it turns out
    that if we have two vectors that are orthogonal to a third one。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相当重要的属性是点积。点积是系数间乘积的和。换句话说，A转置B是I从1到n的AI BI之和。现在我们称两个向量正交。如果A转置B等于零。那么你可能会问，嗯，这有特别之处吗？是的。事实上，事实证明，如果我们有两个向量，它们都与第三个向量正交。
- en: then their linear combination is also orthogonal。 So let's actually quickly
    prove this on the whiteboard。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们的线性组合也正交。那么，让我们在白板上快速证明这一点。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_3.png)'
- en: We have vectors， let's say， A， B， where both are A transpose C equals zero and
    B transpose C equals zero。 Then of course， alpha times A plus beta times B transpose
    C。 well that's nothing else than alpha times A transpose C plus beta times B transpose
    C。 And we'll already know that this is zero and that term is zero。 So this is
    of course zero。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有向量，假设是A、B，其中A转置C等于零，B转置C等于零。那么当然，α乘A加β乘B转置C，嗯，这不过是α乘A转置C加β乘B转置C。我们已经知道这一项为零，那一项也是零。所以这当然等于零。
- en: So we've just proven that any linear combination of those two vectors also has
    to be orthogonal to C。 This type of linearity is something rather convenient that
    will be exploiting in the future。 Now matrices do something very， very similar。
    I can， for instance， add them together。 I can multiply them。 It turns out matrices
    actually also form a vector space。 So in that sense。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们刚刚证明了这两个向量的任何线性组合也必须与C正交。这种线性特性是非常便利的，我们将在未来加以利用。现在，矩阵做的事情非常相似。比如说，我可以将它们加在一起。我可以将它们相乘。事实证明，矩阵也形成了一个向量空间。从这个意义上讲。
- en: they're not that much different from what we have before。 I can， you know。 adding
    them is just the atom coefficient wise。 You can overall scale them。 You can apply
    point wise things like sine and cosine。 But I will be able to do other things
    with them as well。 Namely。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 它们与之前的没什么太大区别。我可以，你知道，按元素相加它们。你可以整体缩放它们。你可以逐点应用像正弦和余弦这样的操作。但我也能够对它们做其他事情。也就是说。
- en: I can multiply them and this is where actually all the magic happens。 Now functional
    analysis then takes this yet further by going from finite dimensional objects。
    to infinite dimensional objects。 So this is often a little bit more complex。 For
    all intents and purposes here， essentially assume that wherever we have a vector。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以对它们进行乘法运算，这实际上是所有魔法发生的地方。现在，泛函分析进一步将这一概念扩展，从有限维度的对象，到无限维度的对象。所以这通常会复杂一些。这里的所有目的就是，假设我们在任何地方遇到向量。
- en: you can substitute that for a function。 Wherever we had a matrix。 you can substitute
    it for a linear operator。 And most theorem sort of kind of work in the same way
    in infinite dimensional spaces。 except that， yeah， you have sums over possibly
    infinite numbers of terms。 And that adds a little bit of mathematical complexity。
    A lot of interesting things happen。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其替换为一个函数。无论我们遇到矩阵，都可以将其替换为线性算子。大多数定理在无限维空间中也以类似的方式工作，唯一的不同是，嗯，你可能会有无限个项的求和。这增加了一些数学上的复杂性。很多有趣的事情会发生。
- en: You can prove lots of exciting new theorems and the proofs get more complicated。
    But the intuition pretty much carries over。 One word of caution。 when we apply
    functions to matrices， we apply them coefficient wise。 In some cases。 mathematicians
    may actually mean that we apply them to the matrices as entire arguments。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以证明很多激动人心的新定理，证明变得更复杂了。但直觉基本上是一样的。有一点需要注意，当我们对矩阵应用函数时，我们是按元素来应用的。在某些情况下，数学家们可能实际上意味着我们将函数作为整体应用于矩阵。
- en: So for instance， you could have something like a matrix exponential。 So e to
    some matrix and that's of course， you know， identity matrix plus， you know。 matrix
    plus matrix squared times one-half plus， you know， one-six times the matrix three
    times。 And so on， so just， you know， as you would have in the tail expansion，
    we don't do that here。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，你可能会有类似于矩阵指数的东西。即对某个矩阵取指数，当然，你知道，这就是单位矩阵加上，矩阵加上矩阵的平方乘以一半，再加上，矩阵的三次方乘以六分之一，依此类推，类似于尾级展开，但我们在这里不做这个。
- en: So assume the dumbest plainest simplest definition of what sin or cosine of
    a matrix is。 unless we explicitly say so otherwise。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以假设对矩阵的正弦或余弦的定义是最简单、最基础的，除非我们明确指出其他情况。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_5.png)'
- en: Now， multiplications， you can do very simple things。 For instance。 you can multiply
    a matrix by a vector。 All it is， if you will。 it's just taking inner products
    between the matrix， between the vector， and the first of the matrix。 the vector
    and the second of the matrix and so on and so on。 And so therefore。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，乘法，你可以做非常简单的事情。例如，你可以将一个矩阵乘以一个向量。它所做的，仅仅是，进行矩阵与向量之间的内积，矩阵的第一列和向量的内积，矩阵的第二列和向量的内积，依此类推。因此。
- en: if I have a three by four matrix times a four dimensional vector。 I get the
    three dimensional vector out of it。 In other words， C equals a times B。 So in
    that case。 I get， you know， C_i equals， you know， sum over j aij B_j。 Okay。 What
    that means in practice is that those matrices， effectively， distort the space。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我有一个三行四列的矩阵与一个四维向量相乘，我会得到一个三维向量。换句话说，C 等于 A 乘以 B。所以在这种情况下，我得到，C_i 等于，求和 j，aij
    B_j。好的。实际上，这意味着那些矩阵会有效地扭曲空间。
- en: they perform some shear transform on a bunch of vectors to get some new vectors
    out of it。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它们对一组向量执行剪切变换，得到一些新的向量。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_7.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_7.png)'
- en: Now， I can do those multiplications， not just between matrices and vectors。
    but between pairs of matrices。 For instance， if I don't just have a single vector
    for。 in the green case as before。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以做这些乘法，不仅仅是矩阵与向量之间的乘法，也可以是矩阵对之间的乘法。例如，如果我不只拥有一个单独的向量，而是像之前绿色例子中的那样。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_9.png)'
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_10.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_10.png)'
- en: but I have multiple of them， then I just stack up the results as they go。 And
    with that。 if I multiply a three by four matrix by a four by five matrix。 then
    I get a three by five matrix out of it。 And very unsurprisingly。 I get that C_i_k
    is now the sum over j aij B_jk。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我有多个这样的矩阵，我就将它们的结果堆叠起来。这样，如果我把一个3×4的矩阵乘以一个4×5的矩阵，那么我得到的是一个3×5的矩阵。很不意外地，我得到的C_i_k现在是对j求和的aij
    B_jk。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_12.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_12.png)'
- en: Norms are a rather useful thing to have in this case。 And those norms are defined
    in such a way that the inequality holds。 If I have C equals a times b。 then the
    inequality has to hold that the norm of C has to be less equal。 than norm of a
    times the norm of b。 And now， depending on how you define the norms on C and B。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 范数在这种情况下是非常有用的。范数是以这样的方式定义的，以确保不等式成立。如果我有C等于a乘以b，那么不等式必须成立，即C的范数必须小于等于a的范数乘以b的范数。现在，具体取决于你如何定义C和B上的范数。
- en: you get different definitions for the norm of a。 And one of the more common
    ones is that if you assume that C and B satisfy the。 Euclidean norm， so basically
    L2 norm， then while the norm of a is what sometimes also referred to as。 the largest
    eigenvalue of that matrix。 Now， this really can lead you down a rabbit hole and
    there's this beautiful area of functional analysis。 and operator theory。 And I
    would strongly encourage you to take a class on that。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到不同的范数定义。而其中一种更常见的定义是，如果你假设C和B满足欧几里得范数，即L2范数，那么a的范数有时也被称为该矩阵的最大特征值。现在，这确实可能会把你带入一个深不见底的洞，而且这是一片美丽的泛函分析和算子理论领域。我强烈建议你去参加相关的课程。
- en: If you're interested in these things， for the purpose here of this course。 we'll
    only worry really about two things。 One is the largest eigenvalue and we'll get
    to them in a moment。 The other one is just the sum over the squares of all the
    entries of the matrix。 In other words。 I'm treating the matrix like a vector by
    just taking the matrix。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些内容感兴趣，为了本课程的目的，我们只关心两件事。第一是最大特征值，我们稍后会讨论。另一件事就是对矩阵中所有条目的平方求和。换句话说，我把矩阵当作向量来处理，只是通过对矩阵进行处理。
- en: smashing it out to a long vector and summing over all the terms。 And this is
    also called the Frobenius norm。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将其展开为一个长向量并对所有项求和。这也被称为弗罗贝尼乌斯范数。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_14.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_14.png)'
- en: Okay。 Now， I briefly mentioned the eigenvectors and eigenvalues。 Well。 eigenvectors
    are those vectors that aren't changed by the matrix。 So what I get is that basically
    A times X is lambda times X and of course that doesn't hold in general。 but for
    an eigenvector， all the matrix does is just makes it longer or shorter。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在，我简要提到了一下特征向量和特征值。嗯，特征向量是那些不会被矩阵改变的向量。所以我得到的基本上是A乘以X等于lambda乘以X，当然这在一般情况下不成立，但对于特征向量，矩阵所做的只是让它变长或变短。
- en: It doesn't actually change its direction。 Now， for symmetric matrices。 I can
    always find a complete decomposition of the matrix。 In general， that's not true。
    So there we go。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上不会改变方向。现在，对于对称矩阵，我总是可以找到矩阵的完全分解。一般情况下，这并不成立。所以就是这样。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_16.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_16.png)'
- en: Now， symmetric matrices and special matrices beyond that， well。 let's just quickly
    look at what they look like。 On the left， I have some completely generic matrix。
    Matrix is symmetric if I have aij equals aji。 In other words， if I flip the entries。
    I get the same thing。 And the matrix in the middle actually does that if you look
    at the colors and they represent the numerical entries。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对称矩阵和特殊矩阵，除此之外，嗯，让我们快速看看它们的样子。左边是一个完全通用的矩阵。矩阵是对称的，如果aij等于aji。换句话说，如果我翻转条目，我得到的就是相同的东西。中间的矩阵实际上做到了这一点，如果你看颜色，它们代表的是数值条目。
- en: you can see that I could flip the matrix over on its main diagonal and get the
    same matrix back。 Now， if I have something antisymmetric， namely aij is minus
    aji， then， okay。 I try to illustrate that by the matrix on the right。 And since
    it's pretty hard to draw in pictures negative numbers or the opposite sign。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我可以将矩阵沿主对角线翻转，然后得到相同的矩阵。如果我有一个反对称矩阵，即aij等于-aji，那么，好吧。我试着用右边的矩阵来说明这一点。由于很难用图形表示负数或相反的符号。
- en: I just switched the border colors and the holodal colors。 And the blue and the
    blue and white parts are meant to be inverse of each other or as in negatives。
    Now， a very obvious consequence of aij equals minus aji is that the diagonal of
    the antisymmetric matrix has to be all zeros。 Because in that case， I have aii
    equals minus aii and that can only be satisfied with zeros。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是改变了边框颜色和颜色的颜色。蓝色和蓝白色部分意味着是彼此的逆，或者说是负数。现在，一个显而易见的结果是 aij 等于 -aji，这意味着反对称矩阵的对角线必须全为零。因为在这种情况下，我有
    aii 等于 -aii，这只能通过零来满足。
- en: The last thing in this context are positive definite matrices and they generalize
    the notion of what it means to compute the length of a vector。 Remember， the norm
    squared in the equivalence of x squared is x transpose x。 So this generalizes
    to x transpose Ax greater to equal to zero。 So matrices A that satisfy this property
    for all x are positive semi-definite matrices that satisfy this property strictly
    for all non-zero vectors are positive definite。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，最后一点是正定矩阵，它们推广了计算向量长度的概念。记住，范数平方在等价式中是 x 平方等于 x 转置 x。所以这可以推广到 x 转置 A
    x 大于等于零。因此，满足这个性质的矩阵 A 对于所有 x 都是半正定矩阵，对于所有非零向量严格满足这个性质的矩阵是正定矩阵。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_18.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_18.png)'
- en: Now， a couple more things。 So orthogonal matrices。 So all the rows of a matrix
    are orthogonal to each other。 And in this case。 we refer to this matrix as an
    orthogonal matrix。 So what that means is if I take an inner product between one
    row and another row。 so uij and ukj， so these are different rows。 If I sum over
    all the j。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，再说几件事。所以正交矩阵。矩阵的所有行彼此正交。在这种情况下，我们称这个矩阵为正交矩阵。这意味着，如果我对一行和另一行进行内积，比如 uij 和
    ukj，这些是不同的行。如果我对所有的 j 进行求和。
- en: so that's like the dot-drug between those two rows， then I get delta ik。 In
    other words。 it's zero if the rows don't match and it's one if they do match。
    So I know that all the vectors have unit links。 Unit links means links one。 And
    that also they are mutually orthogonal。 Now， as homework。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就像是这两行的点积，然后我得到 delta ik。换句话说，如果行不匹配则为零，如果行匹配则为一。所以我知道所有的向量都有单位长度。单位长度意味着长度为一。并且它们彼此正交。现在，作为作业。
- en: you can show that if u u transpose equals one， then also u transpose u equals
    one。 Now。 the latter is just writing out that former matrix form。 Special matrices
    are permutations。 So permutation matrices are the ones where pij is one if and
    only j is given by a permutation of i。 So pi is some permutations that I picked。
    And this permutation then defines a matrix。 Now。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以证明，如果 u u 转置等于一，那么 u 转置 u 也等于一。现在，后者只是将前者写成矩阵形式。特殊矩阵是置换矩阵。所以置换矩阵是 pij 等于一，当且仅当
    j 是 i 的置换。pi 是我选择的某些置换。然后这个置换定义了一个矩阵。现在。
- en: why do we care about stuff like this in deep learning？ For instance。 if we have
    an embedding of words or whatever， well， we might have some word id。 and then
    we get some mapping to some dimension that we actually care about。 Yeah。 that's
    one of the reasons why we care。 Also quick assignment for you。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在深度学习中关心这样的事情？例如，如果我们有一个词的嵌入或其他什么，我们可能会有一些词的 ID，然后我们得到一些映射到我们真正关心的维度。是的，这就是我们关心的原因之一。还有一个快速的作业给你。
- en: Try to prove that p is orthogonal。 This matrix， pij， that I just defined。 this
    permutation matrix is an orthogonal matrix。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试证明 p 是正交的。这个矩阵 pij，我刚定义的这个置换矩阵是一个正交矩阵。
- en: '![](img/2fe6fd71ce485ef9d859144ac698453a_20.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe6fd71ce485ef9d859144ac698453a_20.png)'
- en: Now， lastly， why do we really care about it？ Okay， so this is a picture of Nvidia's
    Turing GPU。 And if you look at it， you see lots of repeated patterns。 You see
    all those little cells。 Those are essentially cores。 And GPUs have many， many，
    many of them。 And it just means that they can process many， many operations in
    parallel simultaneously。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为什么我们真的关心它？好吧，这是 Nvidia 的 Turing GPU 的图片。如果你仔细看，你会看到很多重复的模式。你看到那些小单元格，它们本质上是核心。GPU
    有很多很多个核心，这意味着它们能够并行同时处理许多操作。
- en: So in video GPUs have around 2，000 of them AMD GPUs have some other number。
    And it's a little bit of an architectural choice of how many you pick and how
    powerful they are。 But basically suffice it to say they can process many operations
    in parallel simultaneously。 which allows you to do a lot of things like matrix
    vector multiplies efficiently。 Okay。 Okay。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在视频处理中，GPU大约有2000个处理单元，AMD的GPU有一些不同的数量。这在某种程度上是一个架构上的选择，决定了你选择多少个处理单元以及它们的强大程度。但基本上可以说，它们能够并行处理许多操作，*同时进行*，这使得你能够高效地执行像矩阵向量乘法这样的任务。好吧。好吧。
