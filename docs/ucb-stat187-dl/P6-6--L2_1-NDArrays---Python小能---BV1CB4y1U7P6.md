# P6：6. L2_1 NDArrays - Python小能 - BV1CB4y1U7P6

好的，欢迎来到第二讲。抱歉耽搁了。所以我们希望确保这次能够顺利录制好，在那之前，只有摄像头在工作，幻灯片没有显示出来，这并不太有用。我们会很快修复并更新星期二的录制，可能会在周末上传。但昨天有ICM的线，所以大家都有点忙。好的，那么有两件事是我们在星期二没有完全覆盖的。一件事是关于数组的，因为有些人问我。

在内存分配方面，所有这些内容我会详细讲解。第二件事是，事实证明，并不是每个人都有一个AWS账户。所以我会稍微介绍一下如何在AWS上启动机器，因为有几个人在办公室时间过来问我如何操作，我会带你一步步了解如何启动相关内容。对了，抱歉，我有点感冒。

我们在星期二做了线性代数的部分。今天我们将更深入地探讨这些向量在MXNet中的存储方式。快速复习一下。好。

一个标量就是一个零维数组，向量是一个一维数组，二维数组可能是特征的行。所以对于每一个观测，你都有一行属性。现在，三维数组，你可以通过一个黑白电影来表示，或者你也可以有一个彩色图像。在那种情况下。

假设你有1024 x 768像素的分辨率，再加上三个颜色通道RGB，但你也可以有其他颜色通道。至于维度，你可以有许多这样的图像。因此，第四维度将是批次顺序。如果你想把它推向五维，那么就可以得到视频了。

所以我想这应该是非常直接且不言自明的。只不过当超过五维的时候，在幻灯片上展示这些就变得相当尴尬，因此我们就止步于此。关于张量和数组到目前为止有什么问题吗？仅仅是它们是什么。所以如果有人告诉你关于张量的质量，别害怕。

他们做的事情非常简单，却有时把它说得很复杂。好了，我们可以做的是，创建一个像3 x 4的矩阵。你可以用任何随机的值填充它。实际上，作业的一部分就是要创建一个4096 x 4096的数组。所以你可以像平常一样访问这些元素。我的意思是。

这和你在NumPy中看到的完全一样。你可以访问一个元素，访问一行或一列，或者通过适当的索引访问一个子块，或者你可以适当设置索引字段。所以这非常直接，我会详细讲解这一部分。几点要注意的--我们将在这里探讨这些。

所以首先，实际上有几部分文档。首先是我们书中的NDRA章节。然后你可以访问MXNet官网，去beta.amxnet.io。那里搜索功能有效。你可以查看NDRA的快速教程，还可以查看一些功能。如果你对线性代数不太确定，**双线性**也。

字符串有一个非常好的类。你可能想重新审视一下它。不幸的是，我们没有足够的时间深入讲解。但如果你对线性代数不太确定，**你可能还是应该学习这个**。一些人询问了我们的**GPU**和**TPU**是如何设计的。

它们是如何相互关联并与其他加速器相结合的。实际上，这部分内容有点超出本课程的范围，不会深入讨论。现在，基本上可以把**GPU**看作是一种设备，能够非常非常廉价地执行大量的线性代数运算。从某种程度上讲，深度学习的目的。

像Glueon这样的框架会帮助你避免实际使用正确的**CUDA**代码。这样能将你的生产力提高大约10到100倍。如果你仍然关心细节，实际上有一门很好的研究生课程，关于**HPC**的课程。你可以查看GPU的幻灯片。我认为即使你不是专业人士，它们也很容易理解。

还没有注册博士项目。很多图片。实际上，非常清晰地解释了。浏览一下这部分内容，这或许能让你对什么使**GPU**特别之处有些了解。那么，接下来，让我们看看一些笔记本，看看它们之间以及与其他加速器的关系。

![](img/b11163c8663038fa9ae784b75dbf3290_1.png)

这样就能运行了。

![](img/b11163c8663038fa9ae784b75dbf3290_3.png)

好的，好的，稍等。它在数组中。

![](img/b11163c8663038fa9ae784b75dbf3290_5.png)

![](img/b11163c8663038fa9ae784b75dbf3290_6.png)

我们先执行所有操作。那么，首先你需要做的就是导入你选择的深度学习框架，这里是MXNet。我们会导入NDRA作为ND，因为如果不这样做，每次都输入会显得特别麻烦。最简单的操作就是恢复上次的Jupyter笔记本。

已经没有时间了，你可以创建一个从0到11的数字向量。它相当于**range**命令，只是它被称作**range**。这是NumPy中的遗留功能。你可以询问它的形状。如果你输入x.shape，你会得到12。如果输入x.size。

你还会得到12。原因是x.dot.size会计算条目的总数。x.dot.shape会给出，例如，如果是3×4的矩阵，它会返回3和4。如果是12维的向量，它会返回12。然后你可以重新调整形状。这正是我们在这里做的。让我们实际操作一下。你看。

如果你现在打印 `x.dot.shape`，它会返回 3，4，这正是你给它的形状。相反，如果我执行 `x.dot.size`，它会告诉你 12，这是你期望的结果。现在，如果你想将其重塑为第一个维度为三维的形状，而不关心其余部分，你只需写上 `-1`。

这样可以确保你不需要计算所有的维度，基本上让框架完成其余的部分。所以如果你这么做，什么也不会改变，因为 Bluhan 足够聪明，能够自己推算出剩余部分。好的，现在有几点需要注意。如果你运行 `indeed.dot.empty`，它实际上会做一次 `malloc`。

它将抓取一部分内存。如果这块内存没有初始化，那么里面可能就会是任何东西。所以在这种情况下，我们运气不错，它全是零，但你也可能会看到里面有任意的东西。因此，只有当你真正确定这就是你想要的，并且打算立即用某些合理的内容覆盖它时，才可以这么做。

不要运行 `indeed.dot.empty`，然后假设它们都是零。这将是引入代码错误的一个好方法。它可能类似于海森堡效应，你甚至可能在调试器中找不到它，因为它可能实际上会给你清理后的内存，或者根本不给你。

所以真的可以把这个看作是 `malloc`。更合理的做法是分配零值。在这种情况下，你知道它是什么。如果你想分配一，您也可以这么做，代码如下。确实，`ones` 会给你一。这个过程相当直观。如果你有一个特定的数组，它可能没有完全显示在屏幕上，解决方法是修复它。

你可以通过明确指定应该包含的内容来专门分配一个数组。这就是你调用构造函数的方式。我们可能希望生成一个随机条目。事实上，这正是作业的一部分。所以在 `random.normal` 中，我们将为你提供正态分布的条目，具有单位方差。就像我说的，3x2，实际上是 3x4。

所以，如果我再次运行它，条目看起来显然会非常不同，因为它是一个随机数生成器。我可以对它进行所有常见的操作，比如加、减、乘、除。我甚至可以进行指数运算。那么我们来试试吧。除非遇到什么奇怪的情况，好吧，看看吧。这现在是 `x` 的 `y` 次方。

这会对整个数组进行操作。目前为止有任何问题吗？提示：这种操作是你完成某个作业时需要用到的。你还需要的是一个广播操作符。此外，指数运算也可以像这样工作。正弦、余弦、很多其他函数也是如此。现在，当然，你可以写一个点积运算。所以。

例如，你有一个 3x4 的矩阵，然后你再有一个 3x4 的矩阵，接着进行内积运算。由于你已经转置了矩阵，`x.dot.t` 就是转置矩阵，所以你会得到一个 3x3 的矩阵，这一点并不让人意外。这正是线性代数所暗示的。再次提醒一下，注意事项。

只有在调用点操作符时，你才会得到矩阵或矩阵向量乘积。如果你只是写星号（*），你将得到元素-wise的乘法。所以这和MATLAB不同。小心点。所以我们的星号相当于MATLAB的点乘，"+"相当于MATLAB的点加法，等等。你可以组合多个印度数组。好吧。

这非常简单。你连接它们。如果你这么做，那么你会得到不同的对象，所以我应该打印这个，以便你可以看到区别。那没成功，因为我可能之前搞错了x和y。所以我之前做的是重新塑形了x。现在我得弄清楚x和y的形状。

你看到了吗？当然，在这种情况下，如果我进行连接，因为它只是一个一维向量，第二个部分没有任何意义。但那只是因为我没有设置——所以这样就可以正常工作。所以你可以连接向量，这很无聊，但你可以对任何数组这么做。到现在为止有问题吗？我的意思是，这些应该都——是的？

最好的方式是如何给它添加第二个维度，使其变成4×1，只有4个彗星？

我可以这样做。我可以——比如说，设定为x 4，1。我认为这样可行。形状应该是对的。现在我当然也得为y做同样的操作。好了。现在如果我要沿着第二个维度进行连接，你觉得我们会得到什么？好的，我会得到一个4×2的矩阵。不，我实际上会得到一个错误。

哦，是的，当然了。我明白了。是的。现在如果我们这么做——好的。结果是，实际上在你笔记本前做傻事是很容易的。好了。接下来我们可以做其他事情。我们可以检查相等性。这样我们会得到另一个矩阵，它只是检查各个条目。

如果x和y之前是矩阵，你会得到这个。但因为我之前覆盖了它，所以我现在不会重新执行这段代码。如果我们这么做，我们可能会得到很多的序列。你可以用它来索引和做其他事情。你可以对东西进行求和。这是相当直接的。你可以沿某些维度求和。再次提醒，具体的细节请参考文档。

请也查看一下MXNet官网上的数组内文档。所以去beta.amxnet.io。这里有一个非常具体的调用签名，但它其实很直接。然后最后一点是，当然有时候你可能想回到Python的世界。你可以把东西转换为标量。这样就能把东西转成——如果你打印出来——。

然后在一种情况下，你会在数组中看到它，在另一种情况下，你会看到那个数字。所以它们的值是一样的，只是其中一种情况下它在数组里，而在另一种情况下它只是一个单一的数字。好的。好。现在，那个臭名昭著的广播机制。假设我有一个3×1的矩阵，也就是一个向量，并且我有一个1×2的对象。

所以它们看起来是这样的。如果我想对它们进行乘法、加法或其他操作，那么广播机制将会做的事情——这里很重要的一点是，维度数量是相同的。它将根据维度复制这些内容。所以如果我要将它们组合起来，`A + B`，我将得到。

一个三乘二的对象，其中第一项和第二项分别被重复。因此，最简单的理解方法是，它就像线性代数中的外积。记住，你有内积 `x` 转置 `y`。但是如果你有 `x`--好吧，`y` 乘以 `x` 转置，对吧，那么你得到的不是标量，而是一个完整的矩阵。

除了乘法，线性代数中用到的那种操作外，你还可以对很多其他的操作执行类似的操作，比如加法、指数运算等等。这是一种快速生成大型对象的方式，且不需要使用任何一个 `for` 循环。所以这让你的生活更轻松。这可能是所有概念中最难的部分，明白了吗？

这里有问题吗？好吧，那我们继续讲下一个关于切片的内容。它做的正是你想要的。它挑选出合适的行。你可以读取它们，你也可以像预期那样写入它们，对吧？你可以对整个行或列执行此操作。例如，在这里，我挑选了前两行，所有的列，并将它们设置为 12。

它正如你预期的那样做了。好了。现在，事情变得有点更有趣了。那么假设我做这个。那么，好的，我可以查看--，实际上是 Python 必须使用的 y 的点。如果我写 `y = y + x`，那么，问题是，之前和之后的点是不同的。

我们实际上可以打印出来，万一你感兴趣。所以你会看到引用是不同的。所以发生的事情基本上是，Python 去了，并且在这种情况下，ImageNet 去获取了 x 和 y 的内存位置，把值加起来，放到一个新的内存位置，然后这个被赋值给 y。

然后，当然，风险是它获得了引用并且消失了。但没关系。如果你有 5x5 的矩阵，这并不是什么大问题。如果那个矩阵占用了你 GPU 的一半内存，那么这就成了一个大问题，因为在你计算之前，可能就已经用完内存了。好吧。所以我们需要做一些修复。那么你可以做的一件事是，简单地说。

明确地说，你想将 `x + y` 的结果赋值给位置 `z`。虽然这有点像是一个符号约定，但这基本上就是你得到的结果。通过 `z:`，你说，好的，我想要那个内存位置，然后，ImageNet 将很高兴地再次将数据写入该位置。如果你这样做，那么你会得到相同的指针。这里有一个很有用的其他功能。

它叫做 zeros like，这基本上意味着创建另一个与`y`形状相同但全是零的对象。如果你想要例如等待步骤匹配某些数据，或者类似的事情时，这通常很有用，而且你希望使你的代码更智能，这样无论你输入什么数据，它都能自动处理。

你得到了正确的权重向量。你可以像这样用零初始化权重向量，或者像这样用一初始化，或者使用其他类似的技巧。好了，现在，这样做看起来不错，但还是不够完美。因为它实际上做的是，首先为`x + y`分配内存，然后再将其赋值给`z`。所以这还是会使用大约两倍于应该使用的内存。

我们应该能够直接在原地完成所有这些操作。所以你可以直接在原地执行的方式是，调用像元素加法这样的操作。这些操作强制执行——使得它非常非常节省内存。所以我提到这个是因为它在你的作业中会很有用。

你的作业。这样，我们几乎完成了。最后一件事是我们可以使用 `+=`，这样使用起来就更方便了。所以这实际上可以处理很多操作。好了，最后一件事，你有时可能需要将代码或数据从 NumPy 移动到 NDRA，反之亦然。例如，你可能有一些数据加载代码，等等。

预处理代码写在 NumPy 或者 side pi 中。然后一旦你有了数据，你就想把它推送到 MXNet 中进行处理。所以这非常简单。你只需要执行 NumPy，或者通过构造函数来调用带有 NumPy 元素的对象。问一下你们这些编程语言或 Python 专家中的问题。

为什么我不能直接调用 NumPy.array(X)？为什么我不能说 `a = NumPy.array(X)`？

为什么我不能直接用 NDRA 参数调用构造函数？嗯？[不清楚]，你说得对。所以 NumPy 完全不知道 MXNet 是什么，为什么它要知道该如何处理 NDRA 对象呢？因此，当它遇到一个奇怪的对象时，预先处理的 NumPy 完全迷失了。所以我们需要让 NumPy 容易理解。这就是为什么我们需要在 NDRA 中提供这种功能。

现在，一个重要的细节是。你这么做的时候，基本上全世界都停止了。或者至少 Python 停止了。在 Python 忙着把数据从 MXNet 传到 NDRA，或者反过来，或者从 NumPy 传到 NDRA，反过来时，什么都不发生。如果你只是做一两次，这不是大问题。但如果你有一个完整的向量呢？

这会大大降低你的操作速度。在你的一项作业中，你将测量这到底有多糟糕。现在，在你认为这只是个微不足道的数据点之前，有人就在今年成为了一名助理教授，他在几个月前犯了这个错误。

然后他找我问，为什么我的代码这么慢？

所以这类错误在事后看来是显而易见的。这不仅仅是 MXNet 的问题，实际上在使用像 PyTorch 或 TensorFlow 之类的框架时，也会遇到类似的问题。因为这些框架基本上像是与 Python 完全不同的世界，因此在两者之间转换数据时会遇到困难。

不同的系统，例如 Python，实际上深度运行的框架会触发全局解释器锁，然后你就会遇到问题。所以你需要尽量减少这种情况。有问题吗？是吗？[听不清]，不，是--，[听不清]。所以是的，基本上，实际上，array 会执行那个操作。

正确的操作，因为我们添加了那个功能，确实是这样。对，因为 indeed.array 构造函数可以接受 numpire 的参数。但反过来就需要修改 numpai 的结尾。

![](img/b11163c8663038fa9ae784b75dbf3290_8.png)

所以，接下来我们看一下某个东西。
