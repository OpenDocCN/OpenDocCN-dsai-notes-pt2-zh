- en: P63：63. L12_3 AlexNet - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P63：63. L12_3 AlexNet - Python小能 - BV1CB4y1U7P6
- en: Let's talk about AlexNet。 AlexNet is named after Alex Kreshevsky， and this network
    was。 proposed in 2012。 If you look at it， it looks extremely similar to Lynet。
    As a matter of， fact。 when this network came out first， people didn't quite appreciate
    what was in there。 After all。 this was like Lynet， but just a lot bigger， more
    convolutions， more layers。 And yeah， just that。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈AlexNet。AlexNet是以Alex Kreshevsky的名字命名的，这个网络是在2012年提出的。如果你看它，它看起来和LeNet非常相似。事实上，当这个网络第一次发布时，人们并没有真正理解它的内容。毕竟，这就像LeNet，只是大了很多，有更多的卷积层和更多的层次。嗯，仅此而已。
- en: So this only changed when the AlexNet model won the ImageNet， competition considerably。
    It's only then that people realized that maybe something exciting， is going on
    there。 So let's dive into it in a bit more detail。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 直到AlexNet模型在ImageNet竞赛中大获全胜，人们才开始意识到也许有什么令人兴奋的事情发生了。只有那时，人们才意识到，也许这其中有一些新奇的地方。接下来让我们更详细地探讨一下。
- en: '![](img/335d9e605993ae341291843f944e3448_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/335d9e605993ae341291843f944e3448_1.png)'
- en: Let's take a step back in history， namely to 2001 around that time。 So， kernels
    ruled， the roost。 And in fact， you could just go and design a nonlinear function
    class by extracting， features。 You would pick your kernel for similarity。 You'd
    solve a convex optimization， problem。 and then you'd go and prove a theorem or
    two， and you'd prove that things are optimal， consistent。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下历史，具体来说是2001年左右那个时候。所以，核方法是主流。事实上，你可以通过提取特征来设计一个非线性函数类。你会选择你的核函数来计算相似度。然后你会解决一个凸优化问题。接着，你去证明一两个定理，证明事情是最优的、一致的。
- en: and so on。 Life was good。 Same thing was to a computer vision。 It could go。
    and maybe design a non-convex optimization problem。 He'd use a fair amount of
    math to。 describe how a specific geometric problem would map nicely into a corresponding
    computer。 vision problem。 And then you'd solve the math， and whenever reality
    mapped nicely into your。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 生活很美好。计算机视觉也是一样。你可以去设计一个非凸优化问题。他会用相当多的数学来描述一个特定的几何问题如何很好地映射到一个对应的计算机视觉问题中。然后，你解决数学问题，每当现实与你的模型很好地映射时，
- en: mathematical approach， life was good， it would work。 And again， you'd get lots
    of beautiful， serums。 and whenever you managed to solve a problem in a convex
    manner， this was a hallmark， paper。 So that's computer vision。 Now， the CD and
    the Billy of computer vision around。 the time was feature engineering， and some
    people would have probably argued that most。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数学方法，生活很好，它会奏效。而且，再一次，当你通过凸优化方式解决问题时，这是一个标志性的论文。所以这就是计算机视觉。那时计算机视觉的核心是特征工程，有些人可能会认为，计算机视觉中最有趣的部分是如何设计新的特征。
- en: of the interesting parts in computer vision were how to design new features。
    In fact。 if you designed like a sift or a surf feature extractor， you pretty much
    had it made。 As， in this。 we could get you tenure。 And well， what you would do
    then is you would take your， images。 extract relevant feature points， then maybe
    you'd go in cluster and arrange them， properly。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果你设计了一个SIFT或SURF特征提取器，你几乎就成功了。因为这样，你就可以获得终身职位。然后，你要做的就是提取图像中的相关特征点，然后可能对它们进行聚类和合理安排。
- en: And then in the end， you know， you solve that by applying your support vector，
    machine。 and life was good。 So feature engineering was quite crucial， but it also
    limited the。 amount of engineering throughput that you could do because for every
    new problem， you。 had to do additional feature engineering。 Now， this sounds not
    very enlightened if we look， back。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后最终，你知道，你可以通过应用支持向量机来解决这个问题，生活也就顺利了。所以，特征工程是至关重要的，但它也限制了你可以进行的工程化量，因为每当遇到新问题时，你都需要进行额外的特征工程。现在，如果我们回头看，这听起来并不是非常开明。
- en: but there's simple reason why people at the time would design models in this
    way。 So let's look at the progression of data， memory and compute that was available
    in different。 decades。 MAF just rounded it to the next power of 10， just because
    we really care about orders。 of magnitude。 So data set sizes didn't really start
    growing a lot until around 2000， maybe， 2010。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，那个时候人们设计模型的方式有一个简单的原因。让我们来看看不同年代中可用的数据、内存和计算能力的发展。MAF只是将其四舍五入到下一个10的幂次方，因为我们非常关心数量级的差异。因此，数据集的大小直到2000年左右，甚至可能到2010年才开始真正增长。
- en: with the internet， cloud computing and so on becoming available， where basically。
    it was now possible to store data from a lot of users， a lot of interactions，
    a lot of， instances。 And this is why the amount of data jumped from 2000 to 2010
    by a factor of 100。 and then from 2010 to 2020 by a factor of 1000。 Now， in terms
    of memory， well， things。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: didn't improve a lot， but in terms of compute， we had a breakthrough probably
    around 2010。 when people switched from single or small numbers of cores to massively
    multi core architectures。 like GPUs。 So before that， dual core， maybe quad core
    architectures were kind of the standard。 of what you would have on your desktop。
    And only now， you know， you're starting to be。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: able to buy 1632 core machines at reasonable prices， or of course， if you go
    to the cloud。 you get up to 100 cores， but still， if you go and use a GPU， you
    might get， you know， thousands。 This led to correspondingly much higher performance
    compute。 For instance， on a P3 server， you。 might have eight vultas， and that
    gives you， you know， over a petaflop of compute。 Now。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: interestingly， deep networks were popular around 1990， when compute was， well，
    there。 was some there， but memory was tiny and the data sets weren't that big。
    So that's when。 deep networks were really good at inference time。 So then around
    2000 up to 2010， kernel。 methods were the right thing to use because the data
    sets weren't too big yet， and you。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: could store non trivial parts of the kernel matrix in memory。 Remember， a kernel
    method。 requires a kernel matrix， and that tends to go super linearly in the amount
    of data that， you have。 And so as long as you have enough memory and compute isn't
    so much of an issue， things are good。 What happened then is that compute took
    a quantum leap forward。 So mind， you。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: the first successful modern implementation of deep networks happened on GPUs
    with AlexNet。 so that was 2012。 And so it's very clear that only once compute
    was available， it was a。 practically feasible option to switch to nonlinear， non-convex，
    highly compute intensive， settings。 and that's exactly deep networks。 Now， let's
    look at the data。 So imageNet came。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/335d9e605993ae341291843f944e3448_3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: out in 2010， and it was a big data set at the time。 1。2 million examples， 1000
    classes。 so compare that to 60，000 observations， 10 classes for MNIST。 Also， the
    resolution was。 considerably bigger by maybe a factor of 1000。 So it went from
    28 by 28 to 469 by 384 dimensions。 and the images were in three channels， namely，
    Ray Green and Blue， whereas before that we。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/335d9e605993ae341291843f944e3448_5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: had great scale。 So that changed things a lot。 Now， until 2012， around that
    time when。 AlexNet won the imageNet competition， the default strategy for solving
    computer vision。 problem was to go and pick manually engineered features。 You
    would then go and apply an SVM。 in the end， and this was replaced by features
    that were learned automatically followed by。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: a softmax。 But AlexNet wasn't just a bigger and better， Lynette。 there were
    a number of other key changes。 One was dropout regularization， which。 allowed
    people to design much deeper networks。 As you move to deeper networks， of course。
    just regularizing with regard to the input doesn't help so much， you need to also
    regularize。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 softmax。但是，AlexNet 不仅仅是更大更好的 Lynette。还有其他几个关键的变化。其中之一是 dropout 正则化，它让人们能够设计更深的网络。随着网络的加深，当然，仅仅对输入进行正则化帮助不大，你还需要对。
- en: the inner structure of the network。 So this is essentially taken off regularization
    applied。 to all the layers of the network， or at least in multiple places whenever
    you use dropout。 whereas otherwise you would just smooth things with regard to
    the input。 The second thing was ReLU。 so rectified linear units。 In other words，
    you replace the sigmoid。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的内部结构。所以这基本上是应用正则化到网络的所有层，或者至少在多处使用 dropout 时应用，而如果不使用 dropout，你会只是对输入进行平滑处理。第二项是
    ReLU，即修正线性单元。换句话说，你用 ReLU 替换了 sigmoid。
- en: nonlinearity by just the max between x and 0， which had as a consequence that
    the gradient。 would no longer vanish because you had at least one half space where
    the function was the， identity。 The last thing was max pooling， which replaced
    average pooling。 And then the。 result of that was that now features were rather
    a bit more shift invariant because you。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性进行正则化，通过仅仅使用 x 和 0 之间的最大值，这导致了梯度不再消失，因为至少有一个半空间，其中函数是恒等的。最后一项变化是最大池化，取代了平均池化。这样，结果就是特征变得更加对平移不变，因为你。
- en: could now move your attributes a little bit and max pooling would still pull
    the relevant。 attributes through。 So this led to a paradigm shift in computer
    vision and after computer， vision。 well， that's then when people went to speech
    recognition， natural language processing。 text generation， a lot of other things
    that deep networks afterwards proved a metal。 But。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以稍微移动你的特征，最大池化依然能提取相关的特征。这样就导致了计算机视觉中的范式转变，之后，计算机视觉领域发展起来了，紧接着人们开始转向语音识别、自然语言处理、文本生成等领域，深度网络在这些领域之后也证明了其重要性。但是。
- en: '![](img/335d9e605993ae341291843f944e3448_7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/335d9e605993ae341291843f944e3448_7.png)'
- en: it started with computer vision。 So let's look at the architecture。 So in AlexNet，
    you。 can already see that already the intake is quite different。 So the images
    are much larger， I mean。 than 32 by 32 pixels， which are just 28 by 28 padded
    with seros on the outside。 You had 224 by 224 with R， G and B， red， green and
    blue as the channels。 This was followed。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它起源于计算机视觉。所以让我们看看架构。在 AlexNet 中，你可以已经看到，输入的方式就很不一样了。所以图像更大，我的意思是，比 32x32 像素还要大，这些只是28x28，外面加了零填充。你有
    224x224 的尺寸，R，G 和 B，红色、绿色和蓝色作为通道。接下来是。
- en: by convolutions with a vastly larger number of channels， 96 versus 6。 And then
    of course。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了卷积，且通道数量大幅增加，从 6 增加到 96。然后，当然。
- en: '![](img/335d9e605993ae341291843f944e3448_9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/335d9e605993ae341291843f944e3448_9.png)'
- en: you know， the pooling operations。 So if you look at the bulk of the network，
    again， you。 have a lot more channels， 256 versus 16。 So that's 16 as many。 And
    then you have a lot。 more convolutions later on。 So those convolutions ensured
    that you have a much more expressive。 degree of nonlinearity as you move through
    the network。 And this， of course， allows you。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，池化操作。所以如果你看看网络的主体，再次，你。会看到更多的通道，256与16。所以这是16倍之多。然后你会看到更多的卷积操作。那些卷积确保了你在网络中移动时，能够拥有更强的非线性表达能力。当然，这也让你。
- en: '![](img/335d9e605993ae341291843f944e3448_11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/335d9e605993ae341291843f944e3448_11.png)'
- en: to recognize a lot more classes。 To look at the end of the network， well， you're
    dealing。 with 4096 as opposed to 120 hidden units。 And those nonlinearities were
    necessary in。 order to have enough information for like 1，000 output classes as
    opposed to 10。 So you。 might wonder why did they pick 4096 as opposed to maybe
    8000 or maybe 3000。 Well， the idea， was。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 能够识别更多的类别。看看网络的末端，嗯，你处理的是 4096 个隐藏单元，而不是 120 个。那些非线性是必要的，以便拥有足够的信息来处理 1,000
    个输出类别，而不是 10 个。所以你可能会好奇，为什么他们选择了 4096，而不是 8000 或 3000。好吧，想法是。
- en: I suppose that if you have 1000 classes in the end， you need more than 1000
    dimensions。 to describe them well。 And the upper natural limit was also the size
    of the GPUs。 So remember。 Alex Net had to be actually split between two GPUs initially
    because there wasn't enough。 space in terms of memory on a single GPU。 And a lot
    of the engineering in order to make。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Alex Net work at the time was to write code， which would synchronize those two
    GPUs。 Now。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/335d9e605993ae341291843f944e3448_13.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: there were a few more tricks。 So one thing， the most important thing really
    was data augmentation。 So let's have a look at the picture of this cat lying on
    its back。 Okay， Qcat。 And if we。 crop out a part of it， we'll still be able to
    recognize this as a cat， at least humans would。 So the idea was that rather than
    training on the original images， you would train on those。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: cropped parts， which then can be used to infer， you know， what the class is。
    The other thing is to。 have transformations in terms of brightness， color space
    and so on。 All those things improve。 the robustness to changes that you might
    have between training and tests， images。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/335d9e605993ae341291843f944e3448_15.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Now， to wrap this up a little bit， if you look at the complexity of such networks，
    well。 Alex Net is a lot more complex。 In terms of computation， well， it's 250
    times more expensive。 in terms of parameters only 10 times more。 And this was
    another key change that the trade-off。 between computation and memory changed
    quite a bit。 And Alex Net is actually known for being。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: rather extreme in terms of its memory usage。 So nowadays that ratio would have
    been probably。 even much more skewed towards compute， because compute devices
    have become a lot faster and。 therefore people like to exploit that。 So that's
    the trade-off that's actually happening right now。 because memory scales with
    the amount of silicon， compute still scales with the amount of。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: compute units that you have。 And if you have more dumb units， then you can go
    from single core。 to multi core， which is what you have on GPUs， to for instance，
    systolic arrays， which you by now。 have on custom chips like TPUs。 So this explains
    a little bit how we got from Lynette to Alex Net。 which is the considerably more
    complex version of a convolutional neural network。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: What we'll see in the following is how to make those networks work even better，
    get high accuracy。 and how to address some of the problems created by just scaling
    up Lynette。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/335d9e605993ae341291843f944e3448_17.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
