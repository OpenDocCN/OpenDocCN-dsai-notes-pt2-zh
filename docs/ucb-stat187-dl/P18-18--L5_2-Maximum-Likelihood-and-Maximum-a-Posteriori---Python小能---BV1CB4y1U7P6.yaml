- en: P18：18. L5_2 Maximum Likelihood and Maximum a Posteriori - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P18：18. L5_2 最大似然估计和最大后验估计 - Python小能 - BV1CB4y1U7P6
- en: So， logistic regression。 So， the first thing we're going to do is we're actually
    going。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，逻辑回归。首先我们要做的是，实际上我们要做的。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_1.png)'
- en: '![](img/6172d688492fc55eff587210ce3b0e84_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_2.png)'
- en: to go back to maximum likelihood and maximum posterior and so on。 Just to motivate
    a little。 bit to Y on Earth， we had this weird， you know， least mid-squares loss。
    So， maximum likelihood。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回到最大似然和最大后验等等。只是稍微激励一下，为什么我们会有这个奇怪的，知道的，最小二乘损失。于是，最大似然。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_4.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_4.png)'
- en: and maximum posterior。 So， remember this is our normal distribution， right？
    So， it's。 P of X is 1 over square 2 pi sigma squared e to the minus X minus mu
    squared over 2 sigma， squared。 right？ And that's what it looks like。 Pretty boring，
    right？ And then everybody， knows， well。 you know， if you want to get the mean，
    well， you take all the numbers。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 和最大后验。记住，这是我们的正态分布，对吧？所以，它是。P（X）是 1 除以 2π sigma 平方的平方根，e 的 X 减 mu 平方除以 2 sigma
    平方，平方。对吧？这就是它看起来的样子。很无聊，对吧？然后大家都知道，嗯，你知道的，如果你想得到均值，嗯，你就把所有数字。
- en: you sum them up and you divide by N。 And， you know， maybe， you know， in elementary
    school， kid or。 you know， something like that， they might already know about averages，
    right？ And。 they just like take those numbers， sum them up， divide， like， hey，
    let's divide all the。 cake fairly， you know， sum up all the cake pieces and split
    them in three， if it works。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你把它们加起来然后除以 N。而且，你知道的，可能你知道的，在小学，孩子们或者你知道的，类似那样，他们可能已经知道了平均值，对吧？他们只是拿这些数字，加起来，除，像，嘿，大家把蛋糕分公平点，你知道，把所有的蛋糕块加起来，分成三份，如果可行。
- en: They probably don't know about variance， but， you know， everybody learned at
    some point， hey。 a good variance estimate is 1 over N， sum of ri equals 1 to N，
    X i minus mu hat， squared。 Actually。 it's a little bit more complicated if you
    want to do things right， because you need to， yeah。 But。 anyway， so for now， we're
    not going to go into any， few nesses here。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可能不知道方差，但你知道的，每个人在某个时候都学过，嘿，好的方差估计是 1 除以 N，Σ（ri = 1 到 N）Xi - mu 带帽，平方。实际上，如果你想做对，稍微复杂一点，因为你需要，嗯，知道。无论如何，现在我们不会深入讨论这些细节。
- en: But the obvious question is why on Earth this is actually a good idea， right？
    I mean。 this is the first thing you learn， but then why that， right？ And you could，
    say， yeah。 because that's the mean and the variance of the Gaussian， but， you
    know， how。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但显而易见的问题是，为什么这其实是个好主意，对吧？我的意思是，这是你学习的第一件事，但为什么要那样做呢？然后你可以说，嗯，因为那是高斯分布的均值和方差，但是，嗯，你知道的，怎么做呢？
- en: '![](img/6172d688492fc55eff587210ce3b0e84_6.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_6.png)'
- en: did we get there？ So， let's actually derive this from first principles。 And
    we're going。 to do that by looking at something called the likelihood。 So the
    idea is we start with， some data X。 maybe X1 through Xn。 And in this case， I'm
    going to assume that the data has。 been drawn from a Gaussian。 So I'm assuming
    that I have P of X as parameterized by mu。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是怎么得出结论的呢？那么，让我们从最基本的原则推导这个。我们将通过查看一个叫做似然的东西来做到这一点。所以这个想法是，我们从一些数据 X 开始。也许是
    X1 到 Xn。在这种情况下，我假设数据已经从高斯分布中抽取出来。所以我假设我有 P(X)，并由 mu 参数化。
- en: and sigma squared。 Okay。 So do you notice that this really weird notation here
    where。 I'm not using a conditional， but just a semicolon？ Because I'm just treating
    mu and sigma squared。 as parameters of that distribution as opposed to as conditioned
    on these parameters。 Right？
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 和 sigma 平方。好吧。那么你注意到这里有一个非常奇怪的符号吗？我没有使用条件符号，而是用了一个分号？因为我只是把 mu 和 sigma 平方当作这个分布的参数，而不是条件在这些参数上。对吧？
- en: So these are just parameters that I so happen to pick。 Yes？ Pardon？
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些只是我碰巧选择的参数。是吧？抱歉？
- en: Do both versions have any practical difference？ Well， in terms of the interpretation。
    they mean different things。 So in one case， I'm assigning。 some statistical meaning
    to mu and sigma squared as， you know， maybe being drawn from some。 distribution
    or something。 In the other case， they are just some parameters， like the fact。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 两种版本有实际的区别吗？嗯，从解释的角度来看，它们意味着不同的东西。所以在一种情况下，我在给 mu 和 sigma 平方赋予某种统计意义，可能是它们来自某种分布，或者其他什么的。而在另一种情况下，它们只是一些参数，就像事实一样。
- en: that my coatings up in Poisson or the fact that， well， it rained this morning。
    So they， are just。 you know， facts at this point。 And of course， I can go and
    write it out as I， would。 So I have the product I going from 1 to n， 1 over 2
    Poisson squared e to the minus， you know。 xi minus mu squared over 2 sigma squared。
    Now， if I want to find parameters。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我的涂层在泊松分布上，或者说，嗯，今天早上下雨了。所以它们现在只是，你知道的，事实。当然，我可以将其写出来，如我所做的那样。所以我得到从1到n的乘积，1/2泊松平方e的负(xi
    - μ)²/2σ²。如果我想找出参数。
- en: that fit the data really well， you know， for instance， you know， I could choose
    between。 a Gaussian and maybe， you know， an exponential distribution and five
    other distributions that。 there could be。 But if I pick a Gaussian， then I might
    go and， you know， maximize P of x。 as parameterized by mu and sigma squared with
    regard to mu and sigma squared。 So I'm going。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这很适合数据，你知道，比如说，我可以在高斯分布和可能的指数分布之间进行选择，还有五个其他的分布。可能会有。但如果我选择高斯分布，那么我可能会去，知道吗，最大化P(x)，其参数化由μ和σ平方表示，相对于μ和σ平方进行优化。所以我在做的事情是。
- en: to maximize the likelihood that the data was generated by the model。 It's not
    the probability。 because I can， you know， muck around with mu and sigma squared
    and so at this point it's。 no longer a probability。 But the only thing is that
    if I look at， you know， those likelihoods。 those numbers will get very， very small
    as I take many terms。 So the obvious thing you。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最大化数据由模型生成的可能性。这不是概率，因为你知道，我可以通过调整μ和σ平方来改变它，所以在这个时候它不再是概率。但唯一的事情是，如果我查看那些可能性，那些数字会变得非常非常小，随着我添加更多项。所以显然你。
- en: do is rather than maximizing something that goes to zero， you take the negative
    log of。 it and now you again have something that's fairly well behaved。 So you
    minimize minus。 log P of x given mu and， well， parameterized by mu and sigma squared。
    Okay。 So this is。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做与其最大化趋近于零的东西，不如取其负对数。这样你又得到了一个行为相对良好的东西。所以你最小化的是负的log P(x | μ)，其参数化由μ和σ平方表示。好的，接下来。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_8.png)'
- en: really just cosmetics right now。 So let's actually do this。 So I minimize minus
    log P。 of x parameterized by mu and sigma squared and I can decompose it because
    every single。 observation was drawn independently， identically from the same distribution。
    So sometimes statisticians， okay， let me take care of Mr。 Onyate and Indian villages。
    Okay。 Okay。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在真的只是化妆处理。所以让我们来实际做一下。于是我最小化负的log P(x)，其参数化由μ和σ平方表示，我可以分解它，因为每一个观测值都是从同一个分布中独立、同分布地抽取的。所以有时候统计学家会说，好吧，让我处理一下Onyate先生和印度村庄的事。好的，好的。
- en: So I can go and decompose this。 Okay。 So IID。 Independently， independently，
    distributed。 This is the assumption that you want to see for， your real data。
    This is when things are really nice。 Basically， it just means you can use one。
    distribution for everything。 It's like a parallel for all two。 Equivalent。 Anyway，
    so in our， case。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我可以去分解这个。好的。所以IID，独立同分布。这是你希望在你的真实数据中看到的假设。当事情真的很美好的时候，基本上，它意味着你可以为所有内容使用一个分布。它就像是所有两个元素的并行，等价的。无论如何，在我们的案例中。
- en: therefore the likelihood， the negative log likelihood decomposes into terms
    for every， single xi。 So you get a common term， namely， one half log of 2 pi sigma
    squared。 So that。 makes n over 2 of them。 And then I get the 1 over 2 sigma squared
    xi minus mu squared。 So I can just pull the sum in and so I get basically something
    that doesn't depend on。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，似然函数，负对数似然函数分解成了每一个单独的xi的项。所以你得到一个公共项，也就是1/2 log(2πσ²)。所以这就构成了n/2的项。然后我得到1/2σ²(xi
    - μ)²。所以我可以将求和提出来，所以我得到的基本上是一个不依赖于。
- en: mu and something that does。 And so now if you were to minimize that with regard
    to mu。 you'd find that this is minimized for mu equals 1 over n， sum of ri equals
    1 to n xi。 I'm not going to go and do this now explicitly because you've probably
    seen that derivation。 somewhere before。 But this now shows that mu is actually
    optimal solution for that。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: μ和某些东西。这时，如果你最小化与μ相关的部分，你会发现这个在μ等于1/n，Σri=1到n xi时最小化。我现在不会明确地做这个推导，因为你可能之前见过这个推导。但这现在显示了μ实际上是这个问题的最优解。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_10.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_10.png)'
- en: OK。 Now， if I want to do the same thing for the variance， well， I take the same
    expression。 that I had above and take the derivative with regard to sigma。 Now，
    I'm going to do something。 a little bit sneaky。 I'm going to take the derivative
    with regard to sigma squared， with。 otherwise the expression just gets long and
    tedious。 And so now， the sigma squared is。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。如果我想对方差做同样的事情，我就取我上面提到的相同表达式，并对 sigma 求导数。现在，我要做点小聪明。我将对 sigma 的平方求导数，否则表达式就会变得冗长且繁琐。所以，现在，sigma
    的平方是。
- en: n over 2 sigma squared minus 1 over 2 sigma to the 4， sum of i going from 1
    to n xi minus。 mu squared。 And of course， this needs to vanish for optimality。
    And so if I solve for sigma。 I get exactly the well-known variance。 So what does
    this all have to do with regression？ Well。 it's that this is exactly what we are
    going to use and what we are using when we。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: n 除以 2 sigma 平方减去 1 除以 2 sigma 的 4 次方，i 从 1 到 n 的 xi 减去 mu 的平方和。当然，为了最优化，这个需要消失。所以，如果我解
    sigma，我会得到著名的方差。那么这一切和回归有什么关系呢？嗯，这正是我们要使用的，也是我们在做的事情，当我们。
- en: are performing a least mid-squares fit。 But before that， let's look a little
    bit at what。 maximum likelihood estimation really means。 OK。 So if I have， let's
    say， a likelihood。 of my parameters， and I just happen to pick a Gaussian here
    but whatever， then maximum。 likelihood will just go and find the mode of that
    distribution。 OK。 This case， it looks， quite nice。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正在执行最小二乘拟合。但是在此之前，让我们稍微看一下最大似然估计到底意味着什么。好吧，如果我有，假设，我的参数的似然，我刚好选择了高斯分布，但无论如何，那么最大似然估计就会去找到该分布的众数。好吧，在这个案例中，它看起来非常好。
- en: Yes？ [INAUDIBLE]， So because pi doesn't really depend on anything， right？
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 是吗？[听不清]，所以因为 pi 并不真正依赖于任何东西，对吧？
- en: And since I have the log of 2 pi， sigma， squared， since it's a constant。 it
    just gets the difference here that-- so the derivative， killed it。 But OK。 So
    if I have as my parameters for the likelihood， then I am just going to。 pick the
    mode of that distribution， which is OK。 But if I have that as my likelihood， profile。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我有 2 pi 的对数，sigma 平方，因为它是常数，它在这里的差异就会被消去——所以导数被消掉了。但好吧。如果我把这个作为我的似然参数，那么我只需选择该分布的众数，这没问题。但如果我把它作为我的似然轮廓。
- en: I'm going to pick the thing that's here on the peak。 And that may not be quite。
    the right thing that I might want to do。 And here， I definitely wouldn't want
    to do that。 So something is a little bit off， right， with our maximum likelihood
    estimates。 Let's pick。 something a little bit closer to home。 Let's say you didn't
    do the homework。 Then-- OK。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我要挑选位于峰值的那个点。这可能不是我想要做的完全正确的事情。而在这里，我肯定不想这么做。所以，某些地方是有些不对劲的，对吧，和我们的最大似然估计有些不匹配。让我们选择一个离我们更近的东西。比如说你没做作业。那么——好吧。
- en: so that's the data。 Now I can come up with maybe four different parameterizations。
    OK。 And they will all perfectly well explain the data being while you didn't do
    the homework。 One is the dog ate the homework。 OK， homework's gone， right？ Perfectly
    well explains it。 You。 were abducted by aliens。 OK。 Also， perfectly reasonable
    explanation for the fact that you。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是数据。现在我可以提出可能四种不同的参数化。好吧，它们都会完全解释这些数据，比如你没做作业。一个是狗把作业吃了。好吧，作业没了，对吧？完全可以解释。你被外星人绑架了。好吧，这也是一个完全合理的解释，为什么你。
- en: didn't do the homework or maybe you were too lazy or maybe your grandmother
    was sick， OK。 And you know， all those parameters perfectly well explain the data。
    Now， of course， if you。 go to the TA， you would probably go and not necessarily
    come up with the alien hypothesis， first。 Why is that so？ OK。 Why wouldn't you
    say， "Hey， I was abducted by aliens and that's。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 没做作业，或者你可能太懒了，或者也许你的祖母生病了，没问题。你知道，所有这些参数完全可以解释数据。现在，当然，如果你去找助教，你可能去，但不一定首先提出外星人假设。为什么会这样？好吧，为什么你不会说，“嘿，我被外星人绑架了，这就是原因”？
- en: why I couldn't do the homework。" Yes？ They wouldn't believe it。 They would not
    believe it。 Why would they not believe it？ Because no one gets it done correctly。
    OK。 Because too lazy would lead to the maximum， likelihood。 like according to
    that function of this stuff。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我不能做作业。“是吗？他们不会相信。因为没有人做得对。”好吧，他们为什么不相信呢？因为懒得做会导致最大似然。就像根据这些东西的函数那样。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_12.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_12.png)'
- en: So let's look at that， right？ So let's look at the posterior probability， right？
    So this。 is now where we are turning our parameters from just parameters into
    something that's。 actually drawn from some unknown distribution of possible explanations。
    And if you think about。 you know， the lazy student versus the grandma versus dogs
    and， OK。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: maybe we are a little bit too close to Area 51。 So let's give the probability
    for， alien abduction。 OK。 Let's make it 0。01%。 That's pretty high。 And the dog，
    you know， it's。 pretty close to 1% and the grandma， pretty close to 20% and， you
    know， with 80% you're， lazy。 So then the posterior probability， just using base
    rule， would be given by， you know， the。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: likelihood of the data， so P of X， parameterized by W， so given W， times， you
    know， the prior。 probability of that parameter being reasonable。 And what happens
    is that now you essentially。 end up adding a penalty term over the parameters
    to your standard maximum likelihood problem。 So sometimes people talk about penalized
    maximum likelihood and what they're doing is maximum。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: of a sorority and by the way， regularized max in does the same thing。 So people
    have invented。 the same algorithms under a lot of tricks， lots of names， and we'll
    actually see some。 of those in this class。 So in the context of， you know， this
    class of， you know。 estimating means and variances， I would maybe have minus log
    of P of X parameterized by mean sigma squared minus log P of。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: actually， in this case， mean sigma squared。 Yeah， so it's a little bit of a
    typo here。 So the homework， example， while everything has perfect log likelihood。
    the prior is obviously vastly different and， so the TA will infer， well。 you probably
    didn't do the work。 We'll see what's too lazy。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6172d688492fc55eff587210ce3b0e84_14.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: So yeah， what does this all have to do with what Mu talked about last Friday？
    Well， what。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6172d688492fc55eff587210ce3b0e84_16.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: we basically have in regression is we compare our observations YI with our model
    F of XI and， W。 And then we have an additional penalty， namely just corresponding
    to minus log of P， of W。 And so this will make sure that we don't pick crazy parameters。
    So in our case。 we have this very simple data generation model， YI is F of XI
    and W plus epsilon I。 And then。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: maybe you put the Gaussian prior and W。 Mind you， if， you know， you had taken
    a machine。 learning class about four or five years ago， so this is what I was
    teaching at CMU for， PhD level。 this would have been a fairly advanced model to
    implement for PhD students。 You're。 going to do something slightly more complex
    than that in your homework easily。 It's going。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: to be only a few lines。 Yes？ >> So the question， sorry， can you go back to the
    previous line？
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: I have a question about。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6172d688492fc55eff587210ce3b0e84_18.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: that。 Yeah。 So are we saying that given if we assume the same for all options
    of the， priors。 then the maximum likelihood is always the evasion estimator？ So
    that's true。 If I had a uniform prior， the maximum likelihood and maximum probability，
    will be the same。 And that would be the reasonable assumption if you didn't know
    that aliens are。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 是吗？所以我们的意思是，如果我们假设所有先验选项相同，那么最大似然估计总是回避估计器吗？这是对的。如果我有一个均匀的先验，最大似然和最大概率将是相同的。如果你不知道外星人是怎样的，这将是一个合理的假设。
- en: very rare and laziness is very prevalent， right？ So， but a good engineer will
    pick a prior。 that accurately reflects the problem。 So， for instance， if I was
    to look at the voltage。 just coming from an outlet， I would probably pick a prior
    which says， well， either the。 voltage is going to be zero volts or it's going
    to be in the order of 110 volts， right？
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 很少见的是懒惰却非常普遍，对吧？但是，一个好的工程师会选择一个先验，能够准确反映问题。所以，比如说，如果我要查看电压，只来自插座，我可能会选择一个先验，假设电压要么是零伏，要么是110伏左右，对吧？
- en: Or if you grew up outside the United States， then quite often you might have
    220 to 240， volts。 But this is something a good engineer would do。 And they would
    pick this as their。 prior and they would use it to adjust what they read off from
    the measurement device。 when they plug it into an outlet。 Very rarely would you
    see like a voltage of maybe 50 or。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你是在美国以外长大的话，那么你很可能会看到220到240伏的电压。但这是一个好工程师会做的事情。他们会选择这个作为他们的先验，并且会用它来调整从测量设备读取的数值，当他们把设备插入插座时。你很少会看到比如50伏的电压。
- en: 60 volts in an outlet。 I would be very odd。 The first thing a good electrician
    then would。 do is just measure again with a different voltmeter because they wouldn't
    believe it。 So。 but that's a very good question。 Okay。 Good。 So， now let's look
    at the optimization。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 插座中的60伏电压。那会非常奇怪。一个好的电工首先会做的事情就是用不同的电压表重新测量，因为他们不相信它。所以，这是一个非常好的问题。好的。现在我们来看看优化问题。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_20.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_20.png)'
- en: '![](img/6172d688492fc55eff587210ce3b0e84_21.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_21.png)'
- en: problem。 This is exactly what we did last Friday。 We looked at， you know， one
    over。 n or one half yi minus f of xi and w squared。 And then some coefficient
    times， you know。 the penalty and w。 That was actually not what was implemented。
    We actually implemented the。 maximum likelihood solution。 But that's just because
    we could get away with it because。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 问题。这正是我们上周五所做的。我们看了，知道，1/n 或者1/2yi - f(xi) 和w的平方。然后是某个系数乘以，知道的，惩罚项和w。实际上，我们实现的并不是这个。我们实际上实现的是最大似然解。但这只是因为我们可以这样做。
- en: we had so much data in very few parameters。 Okay。 Any other questions？ Yes？
    [INAUDIBLE]， Okay。 Good。 So， going from the first to the second line is kind of
    straightforward， right？
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有这么多数据，但参数非常少。好的。还有其他问题吗？有吗？[听不清]，好的。好。那么，从第一行到第二行是相当直接的，对吧？
- en: Because we have the likelihood term。 And the second term is just the Gaussian
    assumption。 I dropped all the， you know， additive constants， right？ I've just
    scooped them up on the right。 Now。 if I have this minimization problem， I can
    always multiply or divide by something。 without the problem really changing。 So，
    what I did is I divided by n， okay？ And then。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有似然项。第二项只是高斯假设。我去掉了所有的，加法常数，对吧？我把它们都拿到右边了。现在，如果我有这个最小化问题，我总是可以通过某个数相乘或者相除，而问题本身并不会发生变化。所以，我做的就是除以n，好的？然后。
- en: multiply it by sigma squared。 Okay。 So， what happens is the first term here，
    right？ That。 just becomes 1 over 2n， the sum。 The second term now acquires n times
    sigma squared， sorry。 acquires sigma squared over sigma bar squared n。 And I just
    renamed that。 And then I renamed。 that into lambda。 Mind you， this very， in hindsight，
    very obvious transformation had people stomped。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以σ平方。好的。那么，发生的情况是，这里第一个项，对吧？它就变成了 1/2n，求和。第二项现在会获得n倍的σ平方，抱歉，获得σ平方除以σ条形平方n。我把它重新命名了。然后我把它重新命名为lambda。请注意，这个非常，回头看是非常明显的转换，曾经让人们困惑。
- en: quite a bit when they were trying to compare Gaussian processes with support
    vector machines。 about 15 to 20 years ago。 And for about one or two years， people
    were arguing about which。 one was better until somebody realized that it's just
    a re-parameterization。 Okay。 I'm always。 simplifying things here， but yeah， this
    led to rather hilarious arguments that meant nothing。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大约15到20年前，当人们试图将高斯过程与支持向量机进行比较时，争论了很多。大约有一到两年时间，人们一直在争论哪个更好，直到有人意识到这只是一个重新参数化的问题。好吧，我在这里总是简化事情，但的确，这导致了许多毫无意义的可笑争论。
- en: '![](img/6172d688492fc55eff587210ce3b0e84_23.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6172d688492fc55eff587210ce3b0e84_23.png)'
- en: Okay。 So。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么。
