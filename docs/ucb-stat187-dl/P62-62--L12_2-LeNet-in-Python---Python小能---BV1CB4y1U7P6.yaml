- en: P62：62. L12_2 LeNet in Python - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P62：62. L12_2 LeNet in Python - Python小能 - BV1CB4y1U7P6
- en: Let's look at how to implement Lynette。 So just for a refresher。 here's the
    architecture that we're going to implement。 So there's source image， convolution。
    pooling， another convolution， another pooling， and then a couple of dense layers。
    So the first thing we need to do is。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何实现Lynette。再来复习一下，这就是我们要实现的架构。首先是源图像，卷积，池化，再一个卷积，再一个池化，最后是几个全连接层。我们需要做的第一件事是……
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_1.png)'
- en: we need to actually go and import， MXNet and all the appropriate libraries。
    And then we need to define our network。 So we already saw that before on the slides。
    but here's just as a refresher。 So we have 2D convolutions with six channels，
    kernel size 5x5。 average pooling， another convolution， another average pooling。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实际导入MXNet和所有相关的库。然后我们需要定义我们的网络。我们之前在幻灯片中已经看到过这个内容，不过这里再给大家复习一下。所以我们有6个通道的2D卷积，卷积核大小为5x5。平均池化，再一个卷积，再一个平均池化。
- en: and dense will actually automatically recognize， what the right width and height
    and so on is。 so you don't need to actually do anything like flattening。 So you
    have a dense layer。 another dense layer， and then in the end you produce some
    output。 And that's the full definition of Lynette。 So then we need to define and
    see what happens。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 而且全连接层实际上会自动识别正确的宽度、高度等等。所以你不需要做任何像是展平（flatten）的操作。所以你有一个全连接层，另一个全连接层，然后最终你会得到一些输出。这就是Lynette的完整定义。那么接下来我们需要定义并观察会发生什么。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_3.png)'
- en: actually if we feed some data through the network。 And okay， there we go。 So
    we feed in a 28x28 pixels image， and well， since we have the appropriate padding。
    so after that we get six channels with 28x28。 We then perform pooling， so we get
    14x14。 Another convolution reduces it to 10x10。 Remember we have a 5x5 convolution。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们将一些数据输入到网络中。好了，我们开始吧。所以我们输入一个28x28像素的图像，由于我们有适当的填充（padding），接着我们得到六个28x28的通道。然后我们进行池化，得到14x14。另一个卷积将其减小到10x10。记住，我们有一个5x5的卷积。
- en: Then we halve the resolution again by pooling， and then we have three dense
    layers。 And that's exactly as we would have expected。 Okay， mind you。 until you
    actually send some data through， this isn't bound。 so if I had sent some data
    that looks different， than 28x28 to it， well。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过池化再次将分辨率减半，接着我们有三个全连接层。这正是我们预期的结果。好了，记住，直到你实际传递一些数据进去，这个过程才会被绑定。所以如果我传递一些看起来不同的数据，比如不是28x28的，那么……
- en: things would have looked very differently。 Okay， so the next thing we need is
    we need a few tools for training。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这样做的话，结果会看起来非常不同。好了，接下来我们需要一些训练工具。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_5.png)'
- en: So the first thing we need is we need some data iterators。 namely that will
    actually give us the data。 So D2L， so dive into deep learning， load data。 fashion
    MNIST， is essentially just a convenience function that will load the MNIST data
    set。 And I'm going to pick a fairly large batch size just because the images are
    tiny。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要做的第一件事是需要一些数据迭代器。也就是说，这些迭代器将实际提供数据。所以D2L（Dive into Deep Learning，深度学习入门），加载数据。Fashion
    MNIST，实际上就是一个方便的函数，用来加载MNIST数据集。我将选择一个相对较大的批量大小，因为图像非常小。
- en: otherwise our pool GPU doesn't know what to do。 Then okay。 we need to check
    whether we actually have a GPU。 That's convenient because if you don't have that。
    then， well， your code might throw an exception if you don't have a GPU。 As a matter
    of fact。 checking whether the code throws an exception， is one way of finding
    out whether you have a GPU。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 否则我们的池化GPU不知道该怎么办。然后，好的，我们需要检查是否真的有GPU。这样检查很方便，因为如果没有GPU，那么你的代码可能会抛出异常。如果检查代码是否抛出异常，那么这也是确定你是否有GPU的一种方法。
- en: So that's why you have to try， accept， loop， and somewhere you basically try
    and generate， you know。 some one by one matrix， or actually one dimensional array
    of zeros， so just a single entry。 And if that succeeds on a GPU， we know we do
    have one。 If it fails， then we know， well。 we obviously don't have a GPU， so it
    has to be a CPU。 And then it just returns a device context。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是为什么你必须使用try，accept，loop，并且在某个地方，你基本上尝试生成一个一维的零矩阵，或者实际上是一个零的数组，只有一个条目。如果这个操作在GPU上成功，我们就知道我们有GPU。如果失败，那么我们就知道，显然我们没有GPU，那么只能是CPU了。然后它就返回一个设备上下文。
- en: For single GPUs， that's fine。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个GPU来说，这没问题。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_7.png)'
- en: Now， the next thing is we need to have a mechanism to compute the accuracy。
    So evaluate accuracy takes three arguments， a data iterator， the network and the
    context。 The context is needed because I need to move the data into the context。
    So we can see you iterate over the data， and you make sure that's available in
    the context that I need。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个机制来计算精度。所以评估精度需要三个参数，一个数据迭代器、一个网络和一个上下文。上下文是必需的，因为我需要将数据移到这个上下文中。我们可以看到，它会遍历数据，确保数据在我需要的上下文中是可用的。
- en: So as in context， we'll move it to the context if it's not already there。 otherwise
    it will do nothing。 And here for convenience， we convert everything to float 32
    bit。 And then we just compute the accuracy by looking at how many times the art
    max。 the output of the network， matches the correct label， and we sum that up。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在上下文中，如果数据不在上下文中，我们将把它移动到上下文中，否则它什么也不做。这里为了方便，我们将所有数据转换为 32 位浮点数。然后我们通过查看网络输出与正确标签匹配的次数来计算精度，并将其求和。
- en: So one minus the accuracy is the error。 Then of course， you know。 we also need
    to increment the counter n， which counts how many observations we've seen。 So
    in the end we return the accuracy divided by number of counts。 That's fairly straightforward。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，1 减去精度就是误差。然后，当然，你知道，我们还需要递增计数器 n，它统计我们已经看到的观测次数。最后，我们返回精度除以计数值。这是相当直接的。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_9.png)'
- en: And that's what gives us an accuracy estimate。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是给我们提供精度估计的方式。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_11.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_11.png)'
- en: Now we need the training loop。 Well， this takes as an input the network， the
    data iterators。 mini batch size， the optimization algorithm， the device context，
    and the number of epochs。 And yeah。 okay， first we tell， hey， we are training
    on a CPU or a GPU。 We need to define a loss function。 In this case， you know，
    it's softmax。 And then we go over the data as many epochs as we have。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要训练循环。它需要网络、数据迭代器、小批量大小、优化算法、设备上下文和训练周期数作为输入。首先，我们告诉它，我们是在 CPU 还是 GPU 上训练。我们需要定义一个损失函数。在这个例子中，你知道，是
    softmax。然后我们按我们设置的周期数遍历数据。
- en: We set some of the bookkeeping variables to zero， so training loss and training
    accuracy。 And then we go and happily iterate over the data set。 Corrects in Y
    in training iter。 so that iterates over the training set。 So we first move things
    into the context。 We compute the prediction of the network。 So Y hat is net of
    X。 We compute the corresponding loss。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一些记录变量设为零，比如训练损失和训练精度。然后我们开心地开始遍历数据集。Y 在训练迭代器中的正确值。所以它遍历训练集。首先，我们将数据移到上下文中。接着我们计算网络的预测值。所以
    Y hat 是 net 的 X。然后我们计算对应的损失。
- en: So that's loss of Y hat and Y。 We sum over that。 We go and compute the gradient，
    so L backwards。 And we take an update step。 That's very straightforward。 In the
    end， well， we take those labels。 those Y's， and we look at our training accuracy
    just， to make sure that we also keep track of that。 So this does something very
    similar to what we have in terms of evaluate accuracy just。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以那就是损失函数 Y hat 和 Y。我们对其求和。然后我们计算梯度，进行反向传播。接着我们执行一个更新步骤。这是非常直接的。最后，我们会看一下这些标签，Y
    值，并查看我们的训练精度，以确保我们也能跟踪它。因此，这做的事情与我们在评估精度时所做的非常相似。
- en: that we do that on the training set。 In the end， we just print out some status
    of how well we did。 So number of epochs， loss， training， and test accuracy。 And
    that's it。 It's very generic code。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上执行此操作。最后，我们只打印出一些状态信息，显示我们做得如何。所以包括周期数、损失、训练精度和测试精度。就是这样。这是非常通用的代码。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_13.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_13.png)'
- en: And then you let it run。 So for instance here， let's actually do that。 It gets
    to about， you know。 82， 83% accuracy。 It's running on a GPU。 And you can see it's
    fairly swift。 Now。 this takes about two seconds or so for one pass。 If I had wanted
    to run this on a CPU， well。 there's a simple way how I could have done this。 Let's
    wait until it's done。 Actually。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你让它运行。所以例如在这里，让我们实际做一下。它的精度大约是 82%，83%。它在 GPU 上运行。你可以看到它相当迅速。现在，这大约需要两秒钟左右来完成一次遍历。如果我想在
    CPU 上运行它，其实有个简单的方法可以做到这一点。让我们等它完成。实际上。
- en: I can just write context is mx。CPU。 And let's see what happens。 Make complaint
    because it's on the network was probably initialized on the other device。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我只需写 context 是 mx.CPU。然后看看会发生什么。可能会出现错误，因为网络可能是用其他设备初始化的。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_15.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_15.png)'
- en: but let's see what happens。 Yeah， it does complain because the network was initialized
    on the wrong device。 So if I wanted to get this right， I would have to go and
    go back to where I defined my device context。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们来看看会发生什么。是的，它的确会抱怨，因为网络是在错误的设备上初始化的。所以，如果我想正确地运行，我就得回去修改我定义设备上下文的地方。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_17.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_17.png)'
- en: And， okay。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_19.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_19.png)'
- en: So let's do that here。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们就从这里开始吧。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_21.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_21.png)'
- en: And then we can go and try this again。 It's really just to give some idea of
    how fast things go if it's not on the right device。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以再试一次。其实就是为了让大家了解，如果设备不对，处理速度有多慢。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_23.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_23.png)'
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_24.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_24.png)'
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_25.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_25.png)'
- en: So， okay。 We find this training loop。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们找到了这个训练循环。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_27.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_27.png)'
- en: And now， if we run it， we'll see that on a CPU。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们运行它，我们会看到在CPU上运行的结果。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_29.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_29.png)'
- en: This is a lot slower。 So remember before that， it took us about 2。3 seconds
    to do one pass through the data。 And even though this data set is really。 really
    tiny and not really optimized for CPUs， well， it takes forever to perform even
    just one pass。 It takes about 10 times as long。 So that should give you some idea
    of why you would want to use GPUs rather than CPUs。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这要慢得多。所以记得之前我们花了大约2.3秒来处理数据的一次迭代。即便这个数据集非常非常小，且并没有针对CPU进行优化，它仍然花费了非常长的时间，仅仅一次迭代就需要大约10倍的时间。这应该能让你明白，为什么你更愿意使用GPU而不是CPU。
- en: Now， the next step is going to be to use some heavy YouTube real networks such
    as AlexNet。 And we'll see how for those networks， CPUs are just completely hopeless
    and GPUs are really what you want。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是使用一些重量级的YouTube真实网络，如AlexNet。我们将看到对于这些网络，CPU完全无能为力，而GPU才是你真正需要的。
- en: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_31.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc4140cce2065d9f074ce7c9f1ebb253_31.png)'
