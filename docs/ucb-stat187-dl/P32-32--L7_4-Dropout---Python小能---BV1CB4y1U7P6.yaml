- en: P32：32. L7_4 Dropout - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last thing is called job out。 It's pretty popular like four years ago。 But
    nowadays。 we know that how it works and。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/defc3a1b12b8631f55c631a8baabe4cc_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: how when we can apply job out。 The ideas like a good model should be。 robust
    to changes on the inputs。 So for example， you can recognize the object on the
    image。 you change the angle， change the light， change anything you should still
    recommend this image。 For example， for this figure， you still recognize what it
    is。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/defc3a1b12b8631f55c631a8baabe4cc_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: no matter how many noises add into that。 So a long time ago， people already
    know that you can add a。 noise to the training data。 This is equivalent to adding
    a regularization to the loss， function。 But the idea of job out is like， because
    you have new， networks， you have multiple layers。 job out adding noises to the
    internal layers。 No adjust to the input。 In particular。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: if x is the output of a particular layer。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/defc3a1b12b8631f55c631a8baabe4cc_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: then job out to get x prime， which is such that the， expectation of x prime
    is equal to x。 So we add the noise to x prime， but we don't change the， expectation
    at the least。 You have a motivate to do that。 In particularly， job out doing is
    that you choose a， probability p。 or we can't call it a job out probability， then
    with probability p， we set xi into 0。 Otherwise。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: we keep xi value， but divided by 1 minus p， so， that the expectation still doesn't
    change the expectation。 OK？ Now， how apply job out？
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/defc3a1b12b8631f55c631a8baabe4cc_7.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Job out usually applies to the output of a， hint of fully connected layer。 So
    we only talk about fully connected layer yet。 So we're going to talk about different
    layers after that。 But job out usually is only applied to， fully connected layers。
    The reason is because fully。 connected layer is the layer has the most of the，
    modal capacities between all the layers we cannot talk about。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: in the latter。 So that for these particular layers， we can apply job out。 as
    a regression to the layer。 In particular， what it works is like if edge， it's
    the。 output of a hint of layer， which means we try W， weight， plus bias， and apply
    the activation。 then we apply， job out to edge， to catch edge prime。 Then edge
    prime is fitting to the next layer。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: So if you talk about the example here， the internal， layer we have five how
    puts edge 1 to edge 5。 If we apply job out， we may likely to just set edge 2 and，
    edge 5 to be 0。 So which means we remove edge 1 and edge 5 values to the， next
    layer。 And we scale the rest of 3。 So we take on expectation we didn't change
    anything。 So then job out is probably--。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: it's not every time-- so every time we train， we run a， fault path， we actually
    apply job out。 which means every， time the one you drop out is different。 So you
    don't do the job out that first and the fix all， the things。 then you permanently
    lose edge 2 and edge 5。 So every time we drop different nodes。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Any questions so far？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止有任何问题吗？
- en: '![](img/defc3a1b12b8631f55c631a8baabe4cc_9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/defc3a1b12b8631f55c631a8baabe4cc_9.png)'
- en: So job-file is a regularization。 During inference， we should don't apply regularization。
    because regularization makes the training because it's only。 useful to limit the
    choice of the weight during training。 During inference。 job-file is just a return
    that you put a， value。 So that is during inference and during training that we
    had。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以job-file是一种正则化。在推理过程中，我们**不应该**应用正则化，因为正则化仅在训练时有用，限制训练过程中权重的选择。在推理过程中，job-file只是一个返回值，你可以放一个数值。所以这就是在推理和训练时我们所做的。
- en: different behaviors。 So you can still apply job out， but in inference， we。 prefer
    have deterministic results。 So we don't do anything in inference。 Questions so
    far？
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的行为。所以你仍然可以在工作中应用，但在推理时，我们**更倾向于**获得确定性的结果。所以在推理时我们什么都不做。到目前为止有问题吗？
- en: '![](img/defc3a1b12b8631f55c631a8baabe4cc_11.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/defc3a1b12b8631f55c631a8baabe4cc_11.png)'
- en: Good。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
