- en: P35：35. L8_3 Stabilize Training - Activations - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far， we only extend identity activation functions。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1494e52407436e52c04a4e1b7b070fe1_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: We know that if we're going to use identity activation function。 no matter how
    many layers we have is always equal to single layer。 So now let's consider real
    cases。 That is the reactivish functions we are going to use in new networks。 Let's
    start with a simple activation function， which is a linear function。 Given x。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: the sigma x is equal to alpha x plus beta。 If we denote by the h prime to be
    wt times ht minus 1。 which is the input of this activation function， and then
    ht， the output of this layer。 equals to apply sigma to h prime。 Now we can compute
    the variance and expectations of ht。 We know that h prime have zero min， so then
    ht we have min equal to beta。 By assumption。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: we're going to have zero min， which means beta should be equal to zero。 Similarly，
    for the variance。 we know that ht is a linear combination of h prime and h prime
    is already constant。 by if we choose the proper with initialization methods。 Then
    if beta is zero and we can do a bunch of other things here。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: we know that the variance of ht equals to the alpha squared times the variance
    of h prime。 Because the variance of h prime is already constant， then alpha should
    be equal to 1。 Which means if we， for the fourth pass， if we're going to have
    satisfied assumptions that we have before。 then the linear function， the activation
    function should be adjusted at any function we have。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Similarly， for the backward function， we know that by chain rule， the loss function
    of l。 the gradient function of loss with respect to the ht minus 1， the input
    of this layer。 equals to the alpha times the loss function with respect to the
    input of activation functions。 And so then similarly， we know the expectation
    of it's already zero， then we get zero beta beta。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: and the variance is actually of the input is actually alpha squared of the variance
    of the loss of the gradient function with respect to the input of the activation
    function。 Again， we have r for equal to 1。 So then we actually can only choose
    activation function。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: which is close to the independent function we have。 Now let's consider the activation
    functions we actually used on deep learning。 So we have。 we usually using sigmoid
    tangent or red， but how the expansions near zero， near x equal to zero。 we know
    that it can approximate by a linear function here。 For example。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: for sigmoid is equal to 1 over 2 plus x over 4， and some high order function
    of x。 Similarly。 in two tangent is actually equals to zero plus x， this is the
    identity function。 and with some high order function of x。 For relu， if x larger
    than zero。 then it's actually identity function。 So then we see that close to
    x equals to 1。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: both tangent and the relu are close to the identity functions we have。 If we
    choose x。 the input is actually values nearby zero， which is actually the case
    because you know that the weight functions are randomly initialized around zero。
    and the input， the output function also have zero mean and small variance， which
    means。 the activation function is actually close to linear function here， you
    can see the graph is here。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 切线函数和ReLU函数都接近我们所拥有的恒等函数。如果我们选择x，输入值实际上是接近零的，因为你知道权重函数是随机初始化在零附近的。并且输入输出函数也具有零均值和小方差，这意味着激活函数实际上在这里接近线性函数，你可以从图表中看到这一点。
- en: Then for both tangent and the relu are fine， but the problem is the sigmoid。
    because it's not close to identity function。 What we can do， we can simply fix
    it by scale it。 We use a four times sigmoid and the minus two， then it's close
    to identity function。 So you can see from the right hand figure is that the blue
    line is a scale sigmoid。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，切线函数和ReLU函数都是可以的，但问题出在sigmoid函数上，因为它不接近恒等函数。我们可以通过缩放来修复它。我们使用四倍的sigmoid函数减去二，然后它就接近恒等函数了。所以你可以从右边的图中看到，蓝线是一个缩放过的sigmoid函数。
- en: You can see that close to x equals to 1， the curve is almost identical to the
    tangent and the sigmoid。 which is very different to the original sigmoid。 This
    is a green line here。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，当x接近1时，曲线几乎与切线和sigmoid函数相同，这与原始sigmoid函数非常不同。这是这里的绿线。
- en: '![](img/1494e52407436e52c04a4e1b7b070fe1_3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1494e52407436e52c04a4e1b7b070fe1_3.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[空白音频]。'
