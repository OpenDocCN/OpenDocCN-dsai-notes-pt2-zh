- en: P103：103. L19_6 Gated Recurrent Unit (GRU) - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P103：103. L19_6 门控递归单元（GRU）- Python小能 - BV1CB4y1U7P6
- en: are ready to talk about the GRU。 So this is the first non-trivial recurrent
    system that we're going to encounter。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好讨论 GRU 了吗？这是我们将遇到的第一个非平凡的递归系统。
- en: '![](img/07142cc248762e68020f8b751b38953f_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_1.png)'
- en: And effectively， this is the intuition of it。 And actually， it happened a lot
    later than the LSTM。 But the motivation was waiting to see whether you can get
    most of the benefits of an LSTM。 but with less computation。 So the point is a
    little bit that if you have a sequence。 then not all observations are equally
    relevant。 So for instance， if I scratch my head。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这就是它的直觉。事实上，它发生在 LSTM 之后很久。但动机是希望看看你是否能在减少计算的情况下获得大部分 LSTM 的好处。所以重点是，如果你有一个序列，那么并不是所有的观察值都同样相关。例如，如果我挠头。
- en: that's not very relevant。 What I'm talking about hopefully will be more relevant。
    So therefore。 you may very well decide to forget the fact that I scratch my head。
    or it will never even really enter your memory because， well， what does it matter，
    what Alex does。 what matters is what's on the slides。 But so therefore， you need
    two mechanisms。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太相关。我希望我所讲的会更相关。因此，你可能会决定忘记我挠头的事实，或者它根本不会进入你的记忆，因为，嗯，反正这有什么关系，亚历克斯做什么，重要的是幻灯片上的内容。但因此，你需要两个机制。
- en: You need one mechanism to decide when to pay attention。 So basically when to
    update your state。 And you need another mechanism to decide when to forget。 So
    this is like， this is a reset gate。 And only by having those two pieces can you
    then start to manipulate your state in a meaningful way。 So for instance， if I
    have a mechanism which prevents me from paying attention for ten steps。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个机制来决定何时关注。因此，基本上是何时更新你的状态。你还需要另一个机制来决定何时忘记。所以这就像是重置门。只有拥有这两个部分，你才能以有意义的方式操控你的状态。例如，如果我有一个机制，阻止我在十步内关注。
- en: then my gradient of the hidden state with regard to the output will just go
    straight through those ten steps。 and not be affected by anything， right？ Because
    I've basically kept the variable constant。 At the same time， if I decide to forget，
    that's like running detach but using math for it， right？
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我关于隐藏状态的梯度对于输出将直接通过这十个步骤，并且不受任何影响，对吧？因为我基本上保持了变量不变。同时，如果我决定忘记，就像运行 detach
    但使用数学方法一样，对吧？
- en: So detaching the gradient just， well， forgetting the hidden state does the same
    thing as the detaching。 Okay， it sounds a little bit abstract， but is the basic
    functionality of why you might want to have an attention in the forget gate clear？
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，通过忘记隐藏状态来断开梯度，实际上就像断开一样，做同样的事情。好的，这听起来有点抽象，但为什么你可能想要在忘记门中使用注意力的基本功能清晰了吗？
- en: Everybody cool with this？ Good。 So， let's see。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大家都明白了吗？好。那么，我们来看一下。
- en: '![](img/07142cc248762e68020f8b751b38953f_3.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_3.png)'
- en: Let's actually look at， let's fill this up directly as we go along。 So I have
    some hidden state。 ht minus one， and have some input xt。 And I go and take those
    two things and now I send them to a yet to define。 to be defined reset gate and
    an update gate。 And those two things are essentially going to be just a linear
    function of the inputs。 then some nonlinearity。 So this nonlinearity sigma。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际上看看，边走边填充这个内容。所以我有一些隐藏状态，`ht - 1`，还有一些输入`xt`。然后我把这两件事情送到一个尚未定义的重置门和更新门。然后这两者本质上将是输入的线性函数，再加上一些非线性操作。所以这个非线性操作是
    sigma。
- en: I will conveniently choose it in such a way that its values are between zero
    and one， such that。 you know， I can essentially switch those gates on and off，
    right？ So， for instance， you know。 something like a tangent might do something，
    well， not quite， but if I shift it， it will。 So that's the reset and the update
    gate。 Okay。 Everybody cool with that？ Okay， yes？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将方便地选择这种方式，使得它的值介于零和一之间，这样你知道，我基本上可以开关这些门，对吧？所以，例如，你知道，像正切这样的东西可能会做一些事情，嗯，差不多，但如果我稍微调整一下，它就会。所以这就是重置门和更新门。好的，大家都明白吗？好，行吗？
- en: Is using a reset and update mechanism similar to the gradient shift correction
    adapting to time？
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用重置和更新机制是否类似于梯度偏移修正适应时间？
- en: I'm not sure I see that connection right now between comparative correction
    and reset and update gate。 Maybe， maybe we can talk about that another time。 I'm
    really not seeing that connection。 Yes？
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定我现在能看到比较校正与重置门和更新门之间的联系。也许，我们可以下次再讨论这个问题。我现在真的没有看到那种联系。是吗？
- en: Are they always like straight forward， fully connected？ Very good question。
    In the basic units， yes。 Now， the obvious question is， you know， what would you
    do if you had something else like an image？
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它们总是像直线连接的，完全连接的吗？非常好的问题。在基本单元中，是的。现在，显而易见的问题是，你知道，如果你有其他东西，比如图像，你会怎么做？
- en: You would almost certainly want to do something smart in terms of convolutions，
    right？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你几乎肯定希望在卷积方面做些聪明的事情，对吧？
- en: And I think there are one or two papers which did this and it helps。 If there's
    any data type for which this hasn't been done yet， you should totally write that
    paper。 It's a very， very profound insight。 It's good。 So the question is basically。
    do we always have something fully connected？ And the answer would have to be，
    well， no。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为有一两篇论文做到了这一点，并且它是有帮助的。如果还有某些数据类型没有做过这种处理，你应该完全写一篇这样的论文。这是一个非常非常深刻的见解，非常好。那么问题基本上是：我们是否总是有完全连接的东西？答案必须是，不。
- en: if you can avoid it or not。 For instance， if you want to pay attention to parts
    in an image， right。 then you could essentially have that， let's say， you have
    a frame of images。 then I would basically have a convolution here。 So， okay， so
    we have our gating。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你能否避免它。例如，如果你想关注图像中的某些部分，对吧。那么你基本上可以这样做，假设你有一帧图像。然后我基本上会在这里做一个卷积。那么，好的，我们有了我们的门控。
- en: '![](img/07142cc248762e68020f8b751b38953f_5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_5.png)'
- en: The next thing that we need is we need to have some candidate-hidden state。
    Okay。 and so in this candidate-hidden state， what I'm doing is I'm basically computing，
    you know， some。 you know， x times w。 So that's basically input times some linear
    function。 It's a matrix。 so it's some linear function of the input。 And I have
    a bias because here everybody likes biases。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要的是候选隐藏状态。好的。所以在这个候选隐藏状态中，我做的基本上是计算，知道吗，x 乘以 w。所以基本上是输入与某个线性函数相乘。它是一个矩阵。所以它是输入的某个线性函数。而且我有一个偏置，因为这里每个人都喜欢偏置。
- en: right？ And then， well， I need to take the previous-in state。 But what I'm now
    doing is I'm using the reset gate to decide by how much I'm actually going to
    incorporate the previous-in state。 So if that reset gate is zero， then I'm just
    going to use as my candidate-hidden state stuff that only depends on my current
    x_t and the bias。 right？ So in other words， if my reset gate is zero， I'm resetting
    my， you know。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？然后，嗯，我需要获取先前的输入状态。但是现在我正在做的是，我使用重置门来决定我实际上会按多少比例融入先前的输入状态。所以，如果重置门是零，那么我只会使用仅依赖于当前
    x_t 和偏置的东西作为我的候选隐藏状态。对吧？换句话说，如果我的重置门是零，我正在重置我的，知道吗？
- en: recurrent neural network。 If the reset gate is one， I mean。 incorporating it
    as I would have done before otherwise。 So remember in our RNN we had exactly linear
    function of x and w， and h gives us， you know。 h_t plus one。 Now， the only thing
    is this is a candidate-hidden state。 Candidate because， well。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络。如果重置门为一，意味着。我会像之前那样做，而不是其他方式。所以记住，在我们的 RNN 中，我们有一个关于 x 和 w 的线性函数，然后 h
    给我们，知道吗？h_t 加一。现在，唯一的不同是，这是一个候选隐藏状态。候选是因为，嗯。
- en: besides the reset gate， we need to decide whether we are even going to bother
    updating。 And so that's the last part。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重置门之外，我们还需要决定是否真的要更新。所以这是最后一步。
- en: '![](img/07142cc248762e68020f8b751b38953f_7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_7.png)'
- en: Namely， I compute my hidden state by just taking a convex combination of the
    past hidden state and the candidate-hidden state。 So if z_t is one， it means basically
    I'm not going to update， right？
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我通过仅仅取过去隐藏状态和候选隐藏状态的凸组合来计算我的隐藏状态。所以如果 z_t 为一，这基本意味着我不打算更新，对吗？
- en: Because I'm just going to use h_t equals h_t minus one。 If z_t is zero， then，
    well。 I'm going to update entirely。 So the biases that are built in， if you think
    about it， are that。 you know， z and r are zero。 So if r is zero， then I'm just
    going to reset。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我只是打算使用 h_t 等于 h_t 减去 1。如果 z_t 为零，那么，嗯，我将完全更新。所以，如果你考虑到其中的偏置，它们是：你知道，z 和 r
    都是零。所以如果 r 为零，那么我将重置。
- en: '![](img/07142cc248762e68020f8b751b38953f_9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_9.png)'
- en: So my bias is not to take the past into account too much。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我的偏向是，不要过多考虑过去的情况。
- en: '![](img/07142cc248762e68020f8b751b38953f_11.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_11.png)'
- en: On the other hand， my bias there is to update whatever I have rather than just
    skipping the update entirely。 Okay。 So this was maybe a lot。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我的偏好是更新我拥有的内容，而不是完全跳过更新。好的。所以这可能有点多。
- en: '![](img/07142cc248762e68020f8b751b38953f_13.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_13.png)'
- en: Let's just summarize all the math。 Okay。 Yes？ >> It seems kind of similar to，
    like。 the residual layers of z and that。 Like， why can't we just do something
    simple like that where we just --。 >> [inaudible]， >> Right。 >> [inaudible]， >>
    Okay。 >> [inaudible]， >> So， first of all。 the residual connections will come
    back because you can also use recurrent neural networks with residual states。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下所有的数学。好的。是吗？ >> 这看起来有点像z的残差层什么的。像，为什么我们不能做一些简单的事情，比如直接... >> [听不清]， >>
    对， >> [听不清]， >> 好的。 >> [听不清]， >> 所以，首先，残差连接会回来，因为你也可以使用带有残差状态的循环神经网络。
- en: You mostly use that if you stack the RNAs up multiple layers。 which we may or
    may not get the chance to cover today。 But that -- we'll discuss residual units。
    Now， effectively， yes， so you could engineer in a slightly different bias such
    that， you know。 by default， it'll just not change the hidden state at all。 Right。
    And to some extent。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将RNN堆叠成多层，你大多会使用这种方法。我们今天可能有机会讨论这个话题。至于这个 -- 我们会讨论残差单元。实际上，*是的*，你可以设计一个稍微不同的偏置，使得，默认情况下，它根本不会改变隐藏状态。对吧？在某种程度上。
- en: that would be just that by default， you know， ht minus one equals ht。 Right。
    And， you know。 if I said， you know， zt equals one， that actually is what happens。
    But I could have just maybe flipped that last equation to have ht minus one times
    one minus zt and htilde t times zt。 My guess is somebody at some point tried it
    out and it worked a little bit less well and that's why everybody agreed on the
    GRU parameterization。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那就默认是这样的，你知道，ht减去1等于ht。对吧。然后，如果我说，zt等于1，实际上就是这样发生的。但我本可以把最后一个方程反过来，变成ht减去1乘以1减去zt和htilde
    t乘以zt。我的猜测是，某个时刻有人尝试过这个方法，效果稍微差一点，所以大家才统一采用GRU的参数化。
- en: But I wouldn't be able to point out a paper that says， well， hey。 we tried out
    those two options and this one worked better。 There is one paper from Google Brain。
    so by Softfendler， where they use essentially a genetic algorithm to search over
    a insane state space to come up with the perfect hidden cell。 And they come up
    with a hidden cell of about 40 units。 Okay。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但我无法指出一篇论文说，嗯，我们尝试了这两种方案，结果这个更好。有一篇来自Google Brain的论文，由Softfendler撰写，他们使用了一个基因算法来在一个庞大的状态空间中搜索，找到完美的隐藏细胞。然后他们得出了一个大约有40个单元的隐藏细胞。好的。
- en: The fact that you haven't heard of the cell tells you probably everything that
    you need to know about it。 While it works ever so slightly better， it didn't work
    amazingly much better。 but it was amazingly much more expensive。 So the other
    thing is you've probably heard less of a GRU than about LSTMs。 And that's because
    GRU's work ever so slightly worse than LSTMs。 So you might wonder， you know。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你没听说过这个细胞，可能就能告诉你关于它的所有信息。虽然它稍微好一点，但它并没有显著更好，*但它贵得惊人*。所以，另外一件事是，你可能听说过LSTM，却很少听说GRU。这是因为GRU的效果比LSTM稍微差一点。所以你可能会想，嗯。
- en: why the heck is Alex teaching us the stuff that doesn't work， right？ Well。 it's
    not that it doesn't work。 There are quite a few cases when it works just as well。
    And in those cases， the GRU is much preferable because even so this story looks
    plenty complicated。 it's still cheaper than simply than the LSTM。 And I'll walk
    you through all the details for the various things as we go along。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么Alex要教我们那些不起作用的东西呢？对吧？嗯，不是它不起作用。其实有不少情况下它和其他方法一样有效。在那些情况下，GRU更可取，因为尽管这个故事看起来很复杂，*它仍然比LSTM便宜*。我会在接下来的内容中逐步带你了解所有的细节。
- en: but this is just as a heads up of why people do one or the other。 Yes？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是作为提醒，为什么人们会选择这样或那样的做法。是吗？
- en: '>> Why do you stick like the activation？ >> Okay。 So it''s a very good question。
    So first of all。 those sigmas， these are essentially activation functions that
    map the input into an output that goes between 0 and 1。 That''s just engineered
    in such a way that I get convex combinations， right？'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 为什么你坚持使用激活函数？ >> 好的，这是个很好的问题。首先，那些sigma，这些本质上是激活函数，将输入映射到输出，并且输出在0到1之间。这是以某种方式设计的，使得我得到凸组合，对吧？'
- en: So if I want to have a convex combination， I need to have a value between 0
    and 1。 I'm fairly confident people at some point tried what happens if you make
    the range larger and they found that the network diverges or does something else
    terrible。 And then they went back to this。 But this is a good question and one
    way to find out is simply to hack the Python code and just modify it and see whether
    the network works better。 Now， the tang is another one of those cases where you
    have to think about， you know。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我想要一个凸组合，我需要一个介于 0 和 1 之间的值。我相当确信，在某个时刻人们试过扩大范围，结果发现网络发散了或者做出了其他糟糕的事情。然后他们又回到了原点。但是这是一个好问题，一种方法是简单地修改
    Python 代码，看看修改之后网络是否表现更好。现在，正切函数就是一个这样的例子，你必须考虑，嗯。
- en: why would I bother？ Well， actually that's -- okay， so here's the reason。 So
    tang maps everything into the range between minus 1 and 1。 This is a very desirable
    property to have for a hidden state， right？
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我为什么要费心去做这个？嗯，实际上这是有原因的。正切函数把所有值映射到 -1 和 1 之间。对于隐藏状态来说，这是一个非常理想的特性，对吧？
- en: Because you want your hidden state to be of a meaningful value and you don't
    want it to diverge。 You know， so tang， you know， around the origin is nice and
    linear， so everything just goes through。 At the saturation points at least it
    prevents it from exploding。 With otherwise training those networks is a royal
    pain。 Again。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你希望你的隐藏状态有一个有意义的值，并且不希望它发散。你知道的，正切函数在原点附近是平滑的线性函数，所以一切都顺利通过。在饱和点至少它防止了梯度爆炸。否则训练这些网络将会是个大麻烦。再次强调。
- en: whether tang is the optimal function is， you know， a matter of personal taste。
    at least you still get some tiny amount of gradient even for large activation
    values。 But is there a chance that somebody comes up with something better？ Of
    course。 And if you look at basically what happened with ReLU versus， you know，
    sigmoid activation。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正切函数是否是最优的激活函数，嗯，这是一个个人偏好的问题。至少即使在较大的激活值下，你仍然能得到一些微小的梯度。但是有可能有人能提出更好的方法吗？当然。如果你看一下
    ReLU 与 Sigmoid 激活函数的对比。
- en: this was such a simple trick and it made all the difference。 For essentially
    modern deep networks。 This plus dropout is pretty much， you know， I think one
    of the key reasons why Joffin also got the Turing Award。 right？ I mean， and it's
    very simple math。 Yes？
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个是一个非常简单的技巧，却起到了决定性的作用。对于现代的深度网络而言，这加上 dropout 可以说是，嗯，我认为也是 Joffin 获得图灵奖的关键原因之一，对吧？我的意思是，这很简单的数学，没错？
- en: '![](img/07142cc248762e68020f8b751b38953f_15.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_15.png)'
- en: '>> Can you go to the previous slide and contrast it？ Is like the whole slide？
    >> Yep。 Okay。 so this is the candidate hidden state， right？ This is not our hidden
    state yet。'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 你能回到前一张幻灯片并做对比吗？就像整个幻灯片一样？ >> 好的。好的。那么，这是候选的隐藏状态，对吧？这还不是我们的隐藏状态。'
- en: '![](img/07142cc248762e68020f8b751b38953f_17.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_17.png)'
- en: So it's H tilde。 And now I'm using H tilde to update the actual hidden state。
    So I could have just written everything in one equation， right？
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是 H 的上标波浪线。现在我用 H 的上标波浪线来更新实际的隐藏状态。所以我本可以把一切写成一个方程，对吧？
- en: '![](img/07142cc248762e68020f8b751b38953f_19.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_19.png)'
- en: So if you look at the last two lines， I can basically inline that， right？
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你看看最后两行，我基本上可以将它内联，对吧？
- en: I could basically write H t is z t， you know， point to right product with H
    t minus 1。 plus 1 minus z t， point to right product with， you know， the right-hand
    side expression， basically。 the tangent， everything that follows。 I could have
    inlined out。 You're， you know。 deep learning framework may very well do that in
    practice。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我基本上可以写出 H t 是 z t，指向右乘以 H t 减去 1，加上 1 减去 z t，指向右乘以，嗯，右边的表达式，基本上是这样的。正切函数，接下来的一切。我本可以将它内联出来。你的深度学习框架可能在实际应用中就这么做。
- en: But it just looks ugly and it's hard to understand， but this。 there's nothing
    particularly deep in there。 Yep。 Any other questions？ Yes？ >> Okay。 >> Okay。 So。
    okay， so the question was basically， you know， how does network know how to remember
    and how to forget？
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但它看起来就是很丑，而且很难理解，但这里面并没有什么特别深奥的东西。嗯。还有其他问题吗？是的？>> 好的。>> 好的。那么，好的，问题基本上是，嗯，网络是如何知道该记住什么和忘记什么的？
- en: Okay， so if you look at that， so R t， if R t is zero， right， then it completely
    nukes H t minus 1。 So that's the third line from the top， right？ So if that happens，
    then H tilde t only depends on x。 And without it forgets。 I did forget anything
    that it's seen in the past。 >> I mean。 if H tilde t equals， but not， if x is，
    if H t minus 1 is R t will be zero。 >> Okay。 So， well。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，假如你看一下这个，R t，如果R t为零，对吧，那么它就完全抹去H t - 1。所以这是从上面数的第三行，对吧？如果发生这种情况，那么H tilde
    t只依赖于x。没有它的话就会忘记。我会忘记它过去看到的一切。>> 我的意思是，如果H tilde t等于，但不是，如果x是，如果H t - 1是R t将是零。>>
    好的。那么，好吧。
- en: what you can have， well， the other part is simply then to look at， well， the
    update function。 so z t， the update gate z t。 So if that is z to， well， c， to
    1。 then I'm not going to update anything， right？ But if I'm setting it to zero。
    then I'm going to use all of H tilde。 So， basically， if z is zero and R is 1。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的，嗯，另一部分就是简单地看一下，嗯，更新函数。所以z t，更新门z t。所以如果它是z到，嗯，c，到1。那么我就不更新任何内容了，对吧？但如果我把它设置为零，那么我将使用所有的H
    tilde。所以，基本上，如果z为零而R为1。
- en: then I'm back to the plain vanilla R-n in equations， right？
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我就回到了简单的R-n方程，对吧？
- en: So that's the one that we did on Tuesday。 On the other hand， if I set H， well。
    R to be zero and z to be 1， then I'm back in a model that doesn't have a hidden
    state。 Or it has a constant hidden state， so which makes this entire point less，
    right？ Yes？
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们星期二做的那个。另一方面，如果我设定H，好吧。R为零，z为1，那么我就回到了没有隐状态的模型。或者说，它有一个常量隐状态，这样使得这个点变得不重要，对吧？是吗？
- en: '>> So you said that if R is zero， it''s going to forget the Ruby state。 >>
    Correct。 >> But if R is zero， z has to be 1 for that to happen。 >> Yes， strictly
    speaking， you''re right。 So。 basically， those two things are coupled。 Yes。 Very，
    very good observation。 Right。 So。 because otherwise， what I could simply have
    is R is zero， z is 1。'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 所以你说，如果R为零，它将忘记Ruby状态。>> 正确。>> 但是如果R为零，z必须为1才会发生这种情况。>> 是的，严格来说，你说得对。所以，基本上，这两者是耦合的。是的。非常，非常好的观察。对。因为否则，我可以简单地设定R为零，z为1。'
- en: and I still carry that state around。 Right？ So， that's the name is maybe ever
    so slightly misleading in terms of high parameterized。 >> What's the reason for
    having， like， this two step candidate， then， like， one move？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我仍然携带那个状态。对吧？所以，名字可能在高度参数化方面稍微有些误导。>> 那么，为什么要有这种两步候选，然后像一种移动的方式呢？
- en: '>> There''s no particular reason。 Besides the fact that people probably tried
    out 10 things in this one of our best。 why could I not do any algebraic transformation？
    Of course， I can， right？ So。 as long as the math is identical， the same thing
    gets executed， so that''s not the problem。 But I would suggest that if you have
    an idea of， you know， how to build a better， you know。'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 没有特别的原因。除了人们可能尝试了10种不同的方式，而这是一种最好的。为什么我不能做任何代数变换呢？当然，我可以，对吧？只要数学是相同的，执行的内容也是一样的，所以这不是问题。但我建议，如果你有一个想法，知道如何构建一个更好的模型，你知道的。'
- en: R and N cell， that you actually go and build one。 As a matter of fact。 I was
    discussing that with Moo and a colleague of mine over lunch。 And we have some
    idea of something that we want to try， but we don't know yet where that will work。
    But it's basically a rather weird idea of what to use。 So， yeah， I mean。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: R和N单元，你实际上去构建一个。事实上，我和Moo以及我的一个同事在午餐时讨论过这个。我们有一些想法想尝试，但我们还不知道它是否有效。但基本上这是一个相当奇怪的想法，关于该使用什么。所以，是的，我的意思是。
- en: this idea is maybe four years old。 And let's actually see a little bit how it
    works in code。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法大概有四年了。让我们实际看一下它在代码中的运作。
- en: '![](img/07142cc248762e68020f8b751b38953f_21.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07142cc248762e68020f8b751b38953f_21.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
