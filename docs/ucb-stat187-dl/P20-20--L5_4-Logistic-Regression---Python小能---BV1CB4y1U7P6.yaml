- en: P20：20. L5_4 Logistic Regression - Python小能 - BV1CB4y1U7P6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P20：20. L5_4 逻辑回归 - Python小能 - BV1CB4y1U7P6
- en: So logistic regression， if you think about it， does something。 it needs to solve
    a very important problem。 If we have regression。 maybe one-dimensional multiple
    dimensions， well， we're basically trying to come up with estimates for a continuous
    value。 right？ Like house prices or maybe height and weight of a person， or I don't
    know， maybe， you know。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你考虑一下逻辑回归，它会做一些事情，它需要解决一个非常重要的问题。如果我们有回归，可能是单维度也可能是多维度，那么我们基本上是在尝试为一个连续值做估计，对吧？比如房价，或者一个人的身高体重，或者我不知道，也许是别的。
- en: number of times the sunshine per day and the temperature， you know， that would
    be， you know。 two variables or maybe the value of maybe， ten different securities
    on the stock exchange per day。 So any of those things are nice regression problems。
    For classification。 you want to predict like a discrete category， like is it a
    cat or a dog， or what are the digits？ Or。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 每天的阳光时长和温度，你知道，这将是两个变量，或者可能是每日股票市场上十种不同证券的价值。所以这些都是很好的回归问题。对于分类，你需要预测一个离散的类别，比如它是猫还是狗，或者是什么数字？
- en: you know， if you have ImageNet， then you might get a thousand classes。 And having
    a large。 actually the original ImageNet data set has more than a thousand， classes。
    probably over ten thousand， but basically you are going to get a categorical discrete
    output。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，如果你使用 ImageNet，那么你可能会得到一千个类别。而且实际上，原始的 ImageNet 数据集有超过一千个类别，可能超过一万个，但基本上你将会得到一个类别性的离散输出。
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_1.png)'
- en: There are lots of different others。 So for instance on Kaggle。 you could try
    to classify proteins in terms of what they， are and what they do。 Or you could
    find out what type of malware you're dealing with。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多其他不同的例子。例如，在 Kaggle 上，你可以尝试根据蛋白质的特性和功能来进行分类，或者你可以找出你正在处理的恶意软件的类型。
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_3.png)'
- en: Or， you know， what are toxic Wikipedia commands in terms of， you know， which
    category they are。 Apparently there are prototypes like， hey， I'm not really trying
    to start an edit war。 but of course now I am， right？
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你知道，哪些是有毒的 Wikipedia 命令，按它们属于哪种类别来划分。显然，有一些原型，比如，嘿，我并不是想开始一场编辑战争，但当然现在我就是了，对吧？
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_5.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_5.png)'
- en: So to put that into perspective， so in regression， well。 we have like a natural
    scale typically in terms of， you know， the real value number。 And then the loss
    is often given to us in terms of， you know， differences between Y and F of X。
    That may not always be the right loss。 For instance， if I look at the stock market。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为了让这个更具象，回归中，我们通常有一个自然的尺度，通常是以实际的数值形式表示，然后损失通常是以 Y 和 F(X) 之间的差异来给出的。但这并不总是正确的损失。例如，如果我看看股市。
- en: it's probably going to be the difference of the logarithms that I care about。
    Because whether I'm trading Amazon， which trades at $1，600 or whether I'm trading，
    I don't know。 Facebook， which is probably $100 or so。 But it doesn't matter。 If
    I， let's say。 or I take a penny stock， which trades for maybe $1 and is close
    to being delisted。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我关注的可能是对数的差异。因为无论我是交易亚马逊（其交易价格为 $1,600）还是交易，我不知道，Facebook（大概是 $100 左右）。但这并不重要。如果我，假设，或者我买了一只可能仅值
    $1 的便士股票，而且它快被退市了。
- en: For that penny stock， if it's value increases by maybe 50 cents， that's a massive
    move of 50%。 right？ And since I might be buying a lot of those， well， I just struck
    it rich and， you know。 on the other hand， if Amazon goes up by 50 cents， well，
    that means nothing really happened， right？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那只便士股票，如果它的价值增加了 50 美分，那就是一个巨大的 50% 的涨幅，对吧？而且由于我可能会买很多这样的股票，那么我就发财了，反过来，如果亚马逊涨了
    50 美分，那其实并没有发生什么特别的事情，对吧？
- en: Because the base price was so high。 So you care about relative changes。 So in
    that case。 you wouldn't look at differences between values， but you would probably
    look at maybe the log of the ratio as well after losses。 In classification， typically
    you have many classes and the score should somehow。 reflect the confidence that
    we have in this。 And this is awfully vague， so， what could we do？ Well。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为基础价格很高。所以你关注的是相对变化。在这种情况下，你不会看数值之间的差异，而是可能会看一下损失后比值的对数。在分类中，通常有许多类别，分数应该以某种方式反映我们对结果的信心。这其实是非常模糊的，那么我们该怎么办呢？嗯。
- en: the first thing we could do is we could just use the squared loss， right？ So
    I just encode。 you know， the classes with a vector of all 0s and 1， 1， right？
    And every grace on it。 That sounds utterly weird， but people have done that。 And
    if you don't care about doing things particularly well， that actually works okay。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: So there are a couple of implementations， like for instance， VW for some time，
    use that， so WAPL。 WABL。 So there are plenty of reasons why， you might actually
    do that， and it's not too bad。 And you can even prove theorems that it's not too
    bad from the statistics side。 And then you just pick the largest output and that
    one wins。 But I would strongly advise against it。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: but people do it nonetheless， okay。 You could pick something like an uncalibrated
    scale。 And this is by the way what a support vector machine would have done。 So
    you pick the largest output， but you want to make sure that the correct。 output
    is much larger than all the incorrect outputs。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: And so what you have is you get this condition， OY minus OY。 is greater equal
    than some delta of Y and I。 And this delta tells me how bad it is to get things
    wrong。 So suppose you drive on a road and to your right hand side there's a cliff
    and。 to your left hand side there， maybe some lawn or whatever。 Then when you
    drive on this road。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: you will probably somewhat steer to the left。 Because it's not so bad if you
    drive into the lawn。 but it's pretty bad for， you if you fall down that cliff，
    right？
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: So you will keep a larger margin relative to the outputs that will incur a large
    loss。 And under the right circumstances， you can engineer this loss function and。
    this optimization problem in such a way that you can actually solve it efficiently。
    And around maybe 2002， 2003， this was all the rage。 So there are probably about
    three。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: four hundred papers which come up with， really nifty tricks on how to do this。
    Of how to solve this with some mathematical programming on the inside and， beautiful
    math。 we all played this game and yeah， it was the right thing to do at， some
    time。 Okay， no。 but we're not going to do that here。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Instead， we are going to use something like a calibrated loss scale。 And this
    is just a very simple softmax， yes。 >> What is the output？ >> Oh， it's just the
    output。 So if you look at the diagram on the right， so we have X1 through X4，
    then we have O1 through O3。 these are the outputs。 >> The classes？ >> No， these
    are just outputs of the network， right？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: So these are just for the three neurons， the output neurons。 that are just the
    values that they produce。 But they have to name the variable somehow。 so it's
    just that。 Don't worry， it has nothing to do with big O notation。 By the way。
    you do not need complexity theory to pass this class。 Somebody was very worried
    about that。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: You don't need complexity theory for this class， this would be overkill。 But
    in here。 it's really just a variable name。 So the softmax of the output is given
    by just。 exponentiating every coefficient individually。 So that ensures that all
    those things are non-negative。 That's great， because we want probabilities。 And
    then dividing it by the sum over all those terms。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: So quite honestly， if you had to hand the engineer something that maps a vector，
    to a probability。 you might have done something like that as well yourself。 Maybe
    rather than E。 you would have picked maybe OY squared， divided by the sum of the
    O's， you could have done that too。 But this one actually has a little bit nicer
    properties in terms of computing derivatives and so on。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: And the good thing is the negative log likelihood， this is left term down there。
    This is a formula you should absolutely remember。 It will pursue you throughout
    the entire career and stats。 It's about the simplest exponential family you can
    imagine。 You will see this thing again。 Now。 okay， so it's everything sort of
    kind of clear in terms of logistic regression and what we're doing here。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: So that this is a probability envelope。 Okay。 >> [INAUDIBLE]， >> Exactly。 So
    that would be。 and yeah， so the typo is one of the reasons why I don't put up
    the slides before the class。 I hope that there will be fewer typos after the class。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: You have to be right。 Well， I could just fix this now。 [BLANK_AUDIO]。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_12.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: And there we go。 Okay， good。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_14.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: So now if we have that， well， so this is a nice loss function and we can plug
    that in。 And the good thing is every decent deep learning toolkit comes with an
    efficient implementation of this。 This is now， I think also explaining a little
    bit better why we talked about numerical stability and。 all of that about a week
    ago， right？ Because if any of those outputs are really large， then well。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: things can get unstable。 Now， quite often people don't actually refer to this
    loss， but。 they refer to something called the cross entropy loss。 This cross entropy
    loss looks just like the loss that we had before。 Just that rather than a single
    Y， we might now have an entire probability distribution of a label。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: And now I have just Y transposed O。 So if Y is coded with just all series and。
    one one for the correct class， I get exactly the line above。 But more generally
    for probabilities。 I can now compare with that a desired， target probability to
    the probability that I'm estimating。 And we'll actually go over some information
    theory to explain why on earth we get this。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: But we may not have time today。 So let's look at the gradient next。 So the gradient
    of this just happens to be， and I'm going to derive this。 This is one of those
    fundamental derivations that will help you。 So remember。 we had our loss function
    log sum over i e to the oi minus， and then in this case， oi。 Okay。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们今天可能没有时间讲这些。那么接下来我们来看梯度。所以这个梯度恰好是，我将推导这个公式。这是一些基础的推导，会对你有所帮助。所以请记住，我们的损失函数是对所有i求和的对数，减去oi，然后在这个情况下，是oi。好。
- en: And so d o is going to be， well， here we have a log。 So we get in the denominator
    sum over i e to the oi。 And here， and this is at coordinate。 let's call it coordinate
    Y prime。 e to the oi prime minus delta Y prime。 Now。 what is this expression here？
    Well， that's nothing else than p of Y given whole。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以d o将是，嗯，我们这里有一个对数。因此，在分母上我们得到对所有i求和的e的oi次方。而且这里，这个是在坐标处，让我们称它为坐标Y'。e的oi'次方减去delta
    Y'。那么，这里这个表达式是什么呢？其实这就是给定整体Y的概率p(Y|0)。
- en: p of Y prime given 0。 This is the empirical probability。 So if I were to plug
    in the cross entropy loss， so in other words， if we have here， o transpose Y。
    I'll get Y out of here。 So in other words， what I'm really getting is p of Y given
    0。 Well， I know。 of course， now I need to actually plug up， let me call this a
    probability here， minus p。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: p(Y'|0)，这是经验概率。所以如果我代入交叉熵损失，也就是说，如果我们这里有o转置Y，我会得到Y。从这里我就可以得到p(Y|0)。好，我知道，当然，现在我需要实际代入，让我称这个为概率，减去p。
- en: So my gradient is now just a difference between the probability that I'm estimating。
    and the probability that I should be seeing。 So we're basically back to the regression
    setting where we compared what we are estimating。 and what we should be seeing。
    And what we are seeing。 And here， again。 we are taking differences between the
    estimates and what we should be。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我的梯度现在只是我估计的概率与我应该看到的概率之间的差异。所以我们基本上回到了回归设定，我们比较的是我们估计的内容和我们应该看到的内容。以及我们实际看到的内容。而在这里，再次地，我们在估计和我们应该看到的内容之间做差异。
- en: seeing in terms of what we're seeing to get our gradients。 Just that this is
    a more convoluted way of going about it。 Now， this sounds like a coincidence。
    right？ That we're getting differences between truth and expectation。 And it's
    not a coincidence。 As a matter of fact， any exponential family will do that for
    you。 Even better。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们所看到的内容来考虑梯度。仅仅是这种方式比较复杂罢了。现在，这听起来像是巧合，对吧？我们在实际值和期望之间得到了差异。而这并非巧合。事实上，任何指数族分布都会为你做到这一点，甚至更好。
- en: the second derivative of this will give you variances for any exponential family。
    So this is one of the reasons why exponential families are so popular in statistics。
    because the math works out really nicely。 Yes？ >> So we can characterize again
    what's the properties that comes with it？
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个的二阶导数会给你任何指数族的方差。所以这是指数族在统计学中如此受欢迎的原因之一，因为数学结果非常简洁。是的？>> 那我们可以再次表征它的属性吗？
- en: '>> Okay。 Everybody cool with that so far because I''m going to raise it。 Okay，
    good。 So an exponential family， and this is really just a detour here。 I have
    p of x。 parameterized by theta， or actually let me parameterize by w， is given
    by some phi of x， well。 phi of x transposed w minus g of w。 Okay。 So this looks
    a little bit different from what we saw。'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 好的，大家到目前为止都理解了吗？因为我接下来要提高难度了。好的，明白了。那么，指数族，其实这只是一个绕道。我有p(x)，由theta参数化，或者实际上让我用w来参数化，它由某个phi(x)给出，嗯，phi(x)转置w减去g(w)。好的。这个看起来与我们之前看到的有点不同。'
- en: This phi of x， in our case， it just keeps the vector the same。 And w is then
    just the corresponding way to fix the thing out。 g of w is the normalization。
    That is this funky log sum over the exponential term。 That basically just makes
    sure that this is a proper probability distribution。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个phi(x)，在我们的情况下，它保持向量不变。然后w只是修正这一事物的对应方式。g(w)是归一化项。就是这个奇怪的对数求和指数项。它基本上确保这是一个合法的概率分布。
- en: So this is g of w is given by some overall x's。 All right。 And the nice thing
    is that the derivative， g of w， gives me the expectation of phi of x。 And furthermore，
    the second derivative gives me the variance。 Now。 this thing is exactly why the
    first term here is the probability。 All right。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以g(w)由所有x给出。好了。令人高兴的是，导数g(w)给了我phi(x)的期望。而且，二阶导数给了我方差。现在，这个东西正是为什么这里的第一个项是概率的原因。
- en: Because my sufficient statistics were just the indicators， right？
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我的充分统计量只是指示量，对吧？
- en: So the expected value of that indicator function， which is one for， the specific
    class since here。 otherwise， if I take the expectation of that， that just gives
    me the probability vector。 So what we saw here is about the dumbest exponential
    family you can think of， but。 it's a very versatile one。 This is really just an
    aside。 If this looks too confusing for you。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，那个指示函数的期望值，对于特定的类别而言，它的值为1。否则，如果我对它取期望值，那就给我概率向量。所以我们在这里看到的是你能想到的最简单的指数族，但它是一个非常通用的族。这个其实只是题外话。如果这看起来对你来说太混乱的话。
- en: just disregard it。 You won't need it for the rest of the class。 Any other questions？
    Okay。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就忽略它吧。接下来的课程中你不会需要它。还有其他问题吗？好。
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_16.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_16.png)'
- en: Good。 Now， before we do information theory， and this is of course Shannon himself。
    let's look a little bit at how to do regression。 And that's about， I think。 all
    that will be able to do today。 So。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在，在我们开始信息论之前，这当然是香农本人。我们先来看一下如何进行回归。我认为，这就是今天我们能做的所有内容。所以。
- en: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_18.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1e8cce5c2babed73bbe0b1988a0ba7_18.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
