- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Noisy Neighbors: Efficient membership inference attacks against LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16565](https://ar5iv.labs.arxiv.org/html/2406.16565)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Filippo Galli
  prefs: []
  type: TYPE_NORMAL
- en: Scuola Normale Superiore
  prefs: []
  type: TYPE_NORMAL
- en: Scuola Superiore Sant’Anna
  prefs: []
  type: TYPE_NORMAL
- en: Pisa, Italy
  prefs: []
  type: TYPE_NORMAL
- en: '&Luca Melis'
  prefs: []
  type: TYPE_NORMAL
- en: Meta Inc.
  prefs: []
  type: TYPE_NORMAL
- en: '&Tommaso Cucinotta'
  prefs: []
  type: TYPE_NORMAL
- en: Scuola Superiore Sant’Anna
  prefs: []
  type: TYPE_NORMAL
- en: Pisa, Italy Part of this author’s work was carried out while at Meta Inc.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The potential of transformer-based LLMs risks being hindered by privacy concerns
    due to their reliance on extensive datasets, possibly including sensitive information.
    Regulatory measures like GDPR and CCPA call for using robust auditing tools to
    address potential privacy issues, with Membership Inference Attacks (MIA) being
    the primary method for assessing LLMs’ privacy risks. Differently from traditional
    MIA approaches, often requiring computationally intensive training of additional
    models, this paper introduces an efficient methodology that generates noisy neighbors
    for a target sample by adding stochastic noise in the embedding space, requiring
    operating the target model in inference mode only. Our findings demonstrate that
    this approach closely matches the effectiveness of employing shadow models, showing
    its usability in practical privacy auditing scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Advancements in natural language processing [[22](#bib.bibx22)] have made large
    language models (LLMs) [[17](#bib.bibx17)] essential for many text tasks. However,
    LLMs face issues like biases [[15](#bib.bibx15)], privacy breaches [[1](#bib.bibx1)],
    and vulnerabilities [[23](#bib.bibx23)], underscoring the importance of protecting
    user privacy. The use of large datasets including personal information, has raised
    privacy concerns, leading to regulations such as GDPR [[8](#bib.bibx8)] and CCPA
    [[20](#bib.bibx20)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Membership inference attacks (MIA) [[18](#bib.bibx18)] are effectiv auditing
    tools aiming at determining if a specific data point was used in an LLM’s training
    dataset by analyzing its output. Such attacks highlight potential privacy breaches,
    relying on models’ tendency to overfit to familiar data [[3](#bib.bibx3)]. By
    employing calibration strategies and training shadow models, the accuracy of MIAs
    can be improved, although challenges such as computational demands and limitations
    in effectiveness when deviating from training distribution assumptions persist.
    In this paper, we contribute to this field by: i) exploring membership inference
    attacks from the standpoint of a privacy auditor, ii) introducing a computationally
    efficient calibration strategy that sidesteps training shadow models, and iii)
    empirically assessing its potential in replacing other prevalent strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs generate a probability distribution over their vocabulary based on a tokenized
    input sequence converted into numerical inputs through an embedding layer. This
    layer maps tokens to a dense representation, which can be learned during training
    [[16](#bib.bibx16), [17](#bib.bibx17)] or derived from public word embeddings
    [[6](#bib.bibx6)]. For a model , we define  is , it is defined as the average
    negative log-likelihood of its tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ppx(f,x)=-\frac{1}{&#124;x&#124;}\sum_{t=1}^{&#124;x&#124;}\log(f_{x_{t}}(x_{<t}))$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: with $|x|$ the number of tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Membership inference attacks [[18](#bib.bibx18), [24](#bib.bibx24), [2](#bib.bibx2)]
    aim to determine whether a particular data record  of a machine learning model.
    These methods leverage model outputs like confidence scores or prediction probabilities
    to compute a score for the targeted sample. For LLMs, the typical assumption is
    to grant the adversary access to the output probabilities , the goal of the attacker
    is to learn a thresholding classifier to output :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A_{\gamma}(f,x)=\mathbbm{1}[ppx(f,x)<\gamma]$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'MIA is a simple and effective tool to measure the privacy risk in a trained
    machine learning model, and it has interesting connections with other privacy
    frameworks. In particular, it is known to have a success rate bounded by the privacy
    parameters of Differential Privacy (DP) [[7](#bib.bibx7)]. A randomized mechanism
    -DP if for any two datasets , we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{P}[\mathcal{M}(D)\in R]\leq e^{\varepsilon}\mathbb{P}[\mathcal{M}(D^{\prime})\in
    R]$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Notably, DP quantifies the worst-case scenario of the privacy risk, so it is
    a fundamental tool in privacy assessment. From the performance of the thresholding
    classifier -DP [[11](#bib.bibx11)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $e^{\varepsilon}\geq\frac{TPR}{FPR}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: with TPR and FPR being, respectively, the true and false positive rates, given
    a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Related works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Privacy attacks against language models is an active area of research and different
    refinements have been proposed. Some works have focused on an attacker where data
    poisoning is allowed, granting the adversary write access to the training dataset,
    to increase memorization [[21](#bib.bibx21)] or in general to induce malicious
    behaviours [[25](#bib.bibx25), [23](#bib.bibx23), [26](#bib.bibx26), [19](#bib.bibx19),
    [10](#bib.bibx10)] and improve property inference attacks [[12](#bib.bibx12)].
    Other works have adopted similar techniques to achieve actual training data extraction
    from the training set, with only query access to the trained model [[1](#bib.bibx1),
    [4](#bib.bibx4)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of MIAs with query access to the target model, most research
    focused on strategies to improve the calibration of the per-sample scores, i.e.
    techniques to improve the precision and recall in distinguishing members from
    non-members of the training set. In principle, if we can assert that an out-of-distribution
    non-member of the training set will induce a high perplexity in a target LLM,
    there are a number of scenarios where the distinction is not as clear cut, and
    a thresholding classifier essentially ends up distinguishing between in-distribution
    from out-of-distribution samples. A refined MIA then employs calibration strategies
    to tune the scoring function based on the difficulty of classifying the specific
    sample, as in [[24](#bib.bibx24)]. Thus, a relative membership score is obtained
    by comparing  [[2](#bib.bibx2), [24](#bib.bibx24)] or neighboring samples $f(\tilde{x})$
    [[13](#bib.bibx13)]. The new classifier becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tilde{A}_{\gamma}(f,x)=\mathbbm{1}[ppx(f,x)-\tilde{ppx}(f,x)<\gamma]$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where  or over a set of neighboring samples $ppx(f,\tilde{x})$. Neighboring
    models can be obtained by an adversary who is assumed to have some degree of knowledge
    of the training data distribution and trains a number of shadow models to mimic
    the behaviour of the target LLM. For instance [[2](#bib.bibx2)] trains multiple
    instances of the same architecture on different partitions of the training set,
    [[1](#bib.bibx1)] uses smaller architectures trained on roughly the same data,
    [[24](#bib.bibx24)] leverages catastrophic forgetting of the target model under
    the assumption of white-box access. Neighboring samples do not require this assumption
    nor additional training and only need a strategy to craft inputs that are similar
    to the target sample under a certain distance metric. For instance, [[13](#bib.bibx13)]
    crafts neighboring sentences by swapping a number of words with their synonyms,
    showing good results but applicable primarily when the adversary has limited knowledge
    of the training data distribution. The authors then base the neighboring relationship
    in the semantic space, which is hard to quantify and fix, resulting in the need
    to generate a large number of neighbors to reduce the effects of these random
    fluctuations. Additionally, we emphasize how [[13](#bib.bibx13)] requires the
    use of an additional BERT-like model to generate synonyms, thus increasing the
    computational and memory cost of the attack. In [[21](#bib.bibx21)] instead, calibration
    is done by comparing scores of the true inputs with scores of the lower-cased
    inputs. These strategies are known to be under-performing when knowledge of the
    training distribution is available, and are therefore proposed as an effective
    calibration mechanism when training shadow models is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The intuition behind noisy neighbors is that, fixed a distance from a sample,
    the target model will show a larger difference in perplexity between a training
    sample and its neighbors than between a test sample and its neighbors. Thus, if
    we describe a language model as a composition of layers  is an embedding layer
    and -dimensional embedding space by directly injecting random noise at the output
    of $e(x)$. In particular, if we create noisy neighbors by injecting Gaussian noise
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(x_{\sigma}^{\prime})=g(e(x)+\rho),\quad\text{with}\;\rho\sim\mathcal{N}(0,\sigma
    I_{n})$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: then the Euclidean distance between the true and randomized input in the embedding
    space will be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}[\left\&#124;e(x)-e(x)-\rho\right\&#124;]=\mathbb{E}[\left\&#124;\rho\right\&#124;]=\sigma\sqrt{n}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'thus fixing, in expectation, the distance from the true sample at which the
    perplexity of the models will be evaluated. Generating multiple neighbors for
    each sample is crucial to mitigate randomness from stochastic noise, requiring
    repeated LLM inferences. Choosing the standard deviation  value, as shown in Figure
    [1](#S4.F1 "Figure 1 ‣ 4 Method ‣ Noisy Neighbors: Efficient membership inference
    attacks against LLMs"), which can be efficiently identified using binary search.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4892c5a3c5f2359999ec0dd0bb7a5c32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The AUC of the thresholding classifier for MIA shows a single and
    prominent peak at the optimal $\sigma$ value in the noisy neighbors strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: We emphasize the challenge of isolating the embedding layer from the remainder
    of the network in an LLM when considering a scenario where an attacker has only
    black box access to the model. However, when this limitation does not apply, we
    think it is still within the capacity of an auditor to utilize a slightly stronger
    attacker model, where the first embedding layer is exposed, to save computational
    resources in simulating an adversary without access to the model architecture.
    Most importantly, in fact, we are inclined to explore this option as a more computationally
    efficient substitute for training shadow models for calibration, particularly
    in the context of auditing, rather than viewing it as a novel, realistic attack.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To validate the noisy neighbor strategy in implementing a calibrated MIA, we
    run a series of preliminary experiments on an LLM to gauge the risk of memorization
    of training data. The chosen architecture is GPT-2 small [[17](#bib.bibx17)] to
    compromise learning capacity with memory and computational footprint at about  of
    the full WikiText corpus [[14](#bib.bibx14)], a large collection of Wikipedia
    articles. The same data split was then partitioned in  shadow models for score
    calibration, as in [[2](#bib.bibx2)]. Note that Wikipedia articles are filtered
    out of the OpenWebtext corpus, to avoid data leakage in common benchmarks, such
    as ours. The remaining portion of -token long samples to analyze the performance
    of the attack. We generate only  and . The AUC is an important metric for binary
    classifiers as it abstracts from the specific threshold, thus giving an average-case
    idea of the strength of the attacker. Still, as highlighted in [[2](#bib.bibx2)],
    special care should be given to what happens at low FPRs, that is when the attacker
    can confidently recognize members of the training set. This is what Figure [2(b)](#S5.F2.sf2
    "In Figure 2 ‣ 5 Experiments ‣ Noisy Neighbors: Efficient membership inference
    attacks against LLMs") focuses on, again showing a strong overlap of the shadow
    and noisy strategies. Following Equation [4](#S2.E4 "In 2 Background ‣ Noisy Neighbors:
    Efficient membership inference attacks against LLMs"), we also provide the perspective
    of empirical DP, as the privacy community pushes to adopt this framework to comply
    to regulatory frameworks such as the GDPR [[5](#bib.bibx5)]. Empirical DP measures
    the extent to which individual data points can be inferred or re-identified from
    the output of the system, and contrary to DP, it is a post-hoc measurement, not
    an a-priori guarantee. Figure [3](#S5.F3 "Figure 3 ‣ 5 Experiments ‣ Noisy Neighbors:
    Efficient membership inference attacks against LLMs") reports the results, where
    we see a strong consistency between the noisy and shadow strategies, especially
    for FPRs lower than $10^{-2}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b97ebf466005c5dfc023f20738831b9a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) ROC curve of the MIA classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/282ac56e1b7ed42442424e2fd23b27fc.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Performance of the attacker at low FPRs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Efficacy of different strategies for MIA. Confidence intervals computed
    with the Clopper-Person exact method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c1d55d1cce124417e72cbd1fab297759.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Empirical differential privacy measured downstream of training.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The effectiveness of the noisy neighbors method depends on assumptions that
    may not apply universally across models or datasets. Its success also relies on
    specific noise parameters, potentially limiting its generalizability. Despite
    being computationally more efficient than shadow model methods, it still requires
    significant computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work set out to elaborate a strategy for membership inference attacks.
    Differently from prior research focusing on improving the strength of the attacker,
    we develop a technique trying to achieve a similar efficacy, while reducing the
    computational burden for an auditor trying to assess the privacy risk of exposing
    the query access to a trained LLM. We propose the use of noise injection in the
    embedding space of the LLM to create synthetic neighbors of the targeted sample,
    to shift the comparison from the perplexity scored by different models on one
    sample, to the comparison of different samples by the same model. This approach
    allows to only use the model in inference mode, thus inherently reducing the time
    and cost of running an MIA. With a number of experiments we assess how our strategy
    results converge to the results of using shadow models, showing a remarkable alignment.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Nicholas Carlini et al. “Extracting training data from large language models”
    In *30th USENIX Security Symposium (USENIX Security 21)*, 2021, pp. 2633–2650'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Nicholas Carlini et al. “Membership Inference Attacks From First Principles”
    arXiv, 2022 arXiv: [http://arxiv.org/abs/2112.03570](http://arxiv.org/abs/2112.03570)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Nicholas Carlini et al. “The secret sharer: Evaluating and testing unintended
    memorization in neural networks” In *28th USENIX security symposium (USENIX security
    19)*, 2019, pp. 267–284'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Nicolas Carlini et al. “Extracting training data from diffusion models”
    In *32nd USENIX Security Symposium (USENIX Security 23)*, 2023, pp. 5253–5270'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Rachel Cummings and Deven Desai “The role of differential privacy in gdpr
    compliance” In *FAT’18: Proceedings of the Conference on Fairness, Accountability,
    and Transparency*, 2018, pp. 20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova “Bert:
    Pre-training of deep bidirectional transformers for language understanding” In
    *arXiv preprint arXiv:1810.04805*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith “Calibrating
    noise to sensitivity in private data analysis” In *Theory of Cryptography: Third
    Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006\.
    Proceedings 3*, 2006, pp. 265–284 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] European Parliament, European Council “Regulation (EU) 2016/679 of the
    European Parliament and of the Council of 27 April 2016 on the protection of natural
    persons with regard to the processing of personal data and on the free movement
    of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)”,
    2016 URL: [https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Aaron Gokaslan and Vanya Cohen “OpenWebText Corpus”, [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus),
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] W. Huang et al. “MetaPoison: Practical General-purpose Clean-label Data
    Poisoning” In *Advances in Neural Information Processing Systems* 33 Curran Associates,
    Inc., 2020, pp. 12080–12091 URL: [https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Peter Kairouz, Sewoong Oh and Pramod Viswanath “The Composition Theorem
    for Differential Privacy” arXiv, 2015 DOI: [10.48550/arXiv.1311.0776](https://dx.doi.org/10.48550/arXiv.1311.0776)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Saeed Mahloujifar, Esha Ghosh and Melissa Chase “Property inference from
    poisoning” In *2022 IEEE Symposium on Security and Privacy (SP)*, 2022, pp. 1120–1137
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Justus Mattern et al. “Membership inference attacks against language models
    via neighbourhood comparison” In *arXiv preprint arXiv:2305.18462*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Stephen Merity, Caiming Xiong, James Bradbury and Richard Socher “Pointer
    sentinel mixture models” In *arXiv preprint arXiv:1609.07843*, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Pranav Narayanan Venkit et al. “Nationality Bias in Text Generation” In
    *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics* Dubrovnik, Croatia: Association for Computational
    Linguistics, 2023, pp. 116–122 DOI: [10.18653/v1/2023.eacl-main.9](https://dx.doi.org/10.18653/v1/2023.eacl-main.9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever “Improving
    language understanding by generative pre-training” OpenAI, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Alec Radford et al. “Language models are unsupervised multitask learners”
    In *OpenAI blog* 1.8, 2019, pp. 9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Reza Shokri, Marco Stronati, Congzheng Song and Vitaly Shmatikov “Membership
    Inference Attacks Against Machine Learning Models” In *2017 IEEE Symposium on
    Security and Privacy (SP)*, 2017, pp. 3–18 DOI: [10.1109/SP.2017.41](https://dx.doi.org/10.1109/SP.2017.41)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Manli Shu et al. “On the exploitability of instruction tuning” In *Advances
    in Neural Information Processing Systems* 36, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] State of California “California Consumer Privacy Act (CCPA)”, 2018 URL:
    [https://oag.ca.gov/privacy/ccpa](https://oag.ca.gov/privacy/ccpa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Florian Tramèr et al. “Truth serum: Poisoning machine learning models
    to reveal their secrets” In *Proceedings of the 2022 ACM SIGSAC Conference on
    Computer and Communications Security*, 2022, pp. 2779–2792'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Ashish Vaswani et al. “Attention is all you need” In *Advances in neural
    information processing systems* 30, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Eric Wallace, Tony Zhao, Shi Feng and Sameer Singh “Concealed Data Poisoning
    Attacks on NLP Models” In *Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Lauren Watson, Chuan Guo, Graham Cormode and Alexandre Sablayrolles “On
    the Importance of Difficulty Calibration in Membership Inference Attacks” In *International
    Conference on Learning Representations*, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Jiashu Xu et al. “Instructions as backdoors: Backdoor vulnerabilities
    of instruction tuning for large language models” In *arXiv preprint arXiv:2305.14710*,
    2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jun Yan et al. “Virtual prompt injection for instruction-tuned large language
    models” In *arXiv preprint arXiv:2307.16888*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Wangchunshu Zhou et al. “BERT-based Lexical Substitution” In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics* Florence,
    Italy: Association for Computational Linguistics, 2019, pp. 3368–3373 DOI: [10.18653/v1/P19-1328](https://dx.doi.org/10.18653/v1/P19-1328)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
