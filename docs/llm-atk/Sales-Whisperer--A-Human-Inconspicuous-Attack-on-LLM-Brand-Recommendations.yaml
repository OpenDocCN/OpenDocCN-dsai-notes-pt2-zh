- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.04755](https://ar5iv.labs.arxiv.org/html/2406.04755)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Weiran Lin Anna Gerchanovsky Omer Akgul Carnegie Mellon University Carnegie
    Mellon University Carnegie Mellon University Lujo Bauer Matt Fredrikson Zifan
    Wang Carnegie Mellon University Carnegie Mellon University Center for AI Safety
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language model (LLM) users might rely on others (e.g., prompting services),
    to write prompts. However, the risks of trusting prompts written by others remain
    unstudied. In this paper, we assess the risk of using such prompts on brand recommendation
    tasks when shopping. First, we found that paraphrasing prompts can result in LLMs
    mentioning given brands with drastically different probabilities, including a
    pair of prompts where the probability changes by . Our results suggest that our
    perturbed prompts, 1) are inconspicuous to humans, 2) force LLMs to recommend
    a target brand more often, and 3) increase the perceived chances of picking targeted
    brands.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With recent advances in LLMs, chatbots are becoming a ubiquitous part of users’
    digital experience. Users interact and control chatbots through natural language
    (i.e., prompts), for a nearly endless number of tasks. However, despite the natural
    language interface, effective prompts are often hard to create, leading some researchers
    to develop prompt optimization techniques (e.g., [[1](#bib.bib1)]). The industry
    has adopted the same ideas and has developed methods to recommend useful prompts
    to users. Chatbot services may suggest prompts for users (see Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Sales Whisperer: A Human-Inconspicuous Attack on
    LLM Brand Recommendations")), and dedicated forums may suggest prompts for users
    to try (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations"))  [[2](#bib.bib2)]. While these services
    may convenient to users, little research has focused on implications of prompts
    created by other (untrusted) parties. Existing work has only explored various
    security aspects of LLMs [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)], but the risks of using prompts by others remain
    unexplored.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8a191964a2e5fb2c7de7a87bd9634de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An unbranded chatbot service (created for the user study), closely
    mimicking Copilot, suggesting prompts Popular chatbot services (e.g., ChatGPT,
    meta.ai, Gemini, Copilot) all employ such prompt recommendation mechanisms. Some,
    like Copilot [[9](#bib.bib9)], continuously update recommendations based on the
    chat history. Adversarial prompt recommenders may suggest specially crafted prompts.
    Fig. [3](#S3.F3 "Figure 3 ‣ 3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") depicts an attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a866fdb0076745728b182e2214155db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A snapshot of Reddit posts where some Reddit users persuade others
    to try certain prompts. Adversaries may also publish their prompts in the same
    manner and perform the attack we describe in Fig. [3](#S3.F3 "Figure 3 ‣ 3 Threat
    Model ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we study whether *inconspicuous* manipulation of prompts can
    lead to LLM responses with *substantial biases* of an attacker’s choosing. We
    examine this problem by taking a first look at brand recommendation tasks: users
    may ask LLMs to recommend brands (e.g., Samsung) when shopping for a specific
    category of products (e.g., TVs). Adversaries, motivated financially, may aim
    to write and trick users into utilizing prompts that cause LLMs to recommend a
    specific brand more often. Similar to advertisements, adversaries gain economic
    benefits when a specific brand is recommended more often. In this scenario, such
    prompts and corresponding LLM response need to be *human-inconspicuous* to LLM
    users (i.e., users should not notice an attack is taking place) [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)]: if users feel suspicious about either the
    prompts or the responses, they may not trust the prompt recommender and thus thwart
    the attack. We explain our threat model in detail in §[3](#S3 "3 Threat Model
    ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").
    While this study focuses on this specific attack scenario, the methods we describe
    in our work have the potential to be applied outside the realm of brand recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While many existing approaches have claimed to be capable of perturbing natural-language
    sequences (e.g., LLM prompts) in a human-inconspicuous manner [[13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [21](#bib.bib21), [24](#bib.bib24)], these studies suffer one of two shortcomings:
    either their inconspicuousness isn’t evaluated via user studies or the approaches
    were human-detectable in accompanying user studies. Thus, in this work we take
    a different approach to perturbing prompts, ensuring that the perturbations are
    human-inconspicuous via an extensive user study. Further, our attack does not
    require access to the LLM weights or gradients, differentiating our work from
    most prior studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step in evaluating potential attacks in our threat model, we measure
    how paraphrased prompts can result in drastically different LLM responses. We
    perform tests on Gemma-it (instruction-tuned), Llama2, Llama3, and Llama3-it (instruction-tuned).
    These models are described further in §[5.1](#S5.SS1 "5.1 LLM Setup ‣ 5 Setup
    ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").
    To do so, we need a dataset of prompts for brand recommendation tasks. To the
    best of our knowledge, there were no public datasets for this purpose. Hence,
    we created a new dataset, consisting of 77 product categories and 449 prompts.
    In our experiments, we used the frequency of LLMs mentioning certain brands ¹¹1In
    practice, we target a set of words related to the brand (e.g., “Macbook,” “Apple”
    for the “Apple” brand). Throughout the paper, we refer to responses that contain
    any of these target words as the LLM mentioning said brand. as a proxy metric
    of how often LLMs recommend the brand. We later show that this proxy proved to
    be accurate (see [7](#S7 "7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")). We observed that paraphrased alternatives
    of prompts result in a wide range of LLM responses, and can change the likelihood
    that LLMs mention a brand by up to  and $18.6\%$, depending on the LLM (§[6.1](#S6.SS1
    "6.1 Observations on Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The observation that similar prompts can result in different likelihoods of
    brand being mentioned motivates our next contribution. With access LLM to logits
    (notably, not weights), we propose a new approach that perturbs base prompts through
    synonym replacement. These human-inconspicuous perturbed prompts (§[4.2](#S4.SS2
    "4.2 Synonym-Replaced Adversarial Prompts ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")) increase the likelihood of a target brand
    being mentioned in the LLM response. Candidate perturbed prompts are generated
    by replacing certain words with known synonyms, and one is selected based on a
    logit-based loss function. While several synonym dictionaries exist [[25](#bib.bib25),
    [26](#bib.bib26), [23](#bib.bib23), [27](#bib.bib27), [28](#bib.bib28)], we found
    that synonyms suggested by these dictionaries are human-detectable in the context
    of shopping brand recommendations, and thus we create our own synonym dictionary.
    We show that our method of performing synonym replacement can increase the likelihood
    that LLMs mention a brand by absolute improvements up to , , and $10.17\%$ for
    Gemma-it, Llama2, Llama3, and Llama3-it, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also investigated the *transferability* [[29](#bib.bib29), [30](#bib.bib30)]
    of our synonym-replacement approach. We observed that the synonym replacement
    approach is highly transferable between a limited pair of open-sourced LLMs and
    GPT3.5, a close-sourced commercial LLM (§[6.3](#S6.SS3 "6.3 Transferability to
    GPT Models ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretical success in our attacks may not translate to real-world outcomes.
    To evaluate the effectiveness of our attacks in a more realistic setting, we conducted
    an extensive user study (§[7](#S7 "7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")). We specifically measure if participants
    will (1) find differences between perturbed/unperturbed prompt pairs, (2) find
    differences in responses to these pairs, and (3) be influenced by the increased
    likelihood of brand appearance in responses. We found that our synonym replacement
    attack acheives all three adversarial goals with statistical significance (§[7.2.2](#S7.SS2.SSS2
    "7.2.2 Statistical Evaluation ‣ 7.2 Results ‣ 7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations")), validating our earlier
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We collected a dataset of 449 prompts asking LLMs for recommendations on brands,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We identified a new threat model where adversaries perturb prompts and convince
    users to use them, ultimately causing LLMs to mention target brands.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We measured the difference in the likelihood of mentioning target brands in
    LLM’s responses to paraphrased prompts, finding a wide range.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We proposed a novel attack, synonym replacement approach to increase the likelihood
    of LLMs mentioning target brands. Through extensive evaluation, we show that this
    attack is successful.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluated the transferability of this attack from open-souced LLMs to GPT,
    finding that it is transferable for a limited set of models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, through a user study, we showed that synonym replacement meets adversarial
    goals in a realistic setting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the paper has the following layout: We give an overview of related
    work in §[2](#S2 "2 Background ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations"). We define our threat model in §[3](#S3 "3 Threat
    Model ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").
    We then describe two approaches for finding prompts that can increase the probability
    of a brand being mentioned that adversaries may use in the new threat model, in
    §[4](#S4 "4 Methods ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations") and the technical setups to evaluate these approaches in §[5](#S5
    "5 Setup ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").
    We show our empirical results in §[6](#S6 "6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations"). We describe our user study and its results
    in §[7](#S7 "7 User Study ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations"). Finally, we conclude in §[8](#S8 "8 Concluding Discussion
    ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce works related to this paper. We first discuss
    biases in computer systems (§[2.1](#S2.SS1 "2.1 Biases in Computer Systems ‣ 2
    Background ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"))
    and then review existing inconspicuous attacks (§[2.2](#S2.SS2 "2.2 Human-Inconspicuous
    Attacks ‣ 2 Background ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations")) and general attacks on LLMs (§[2.3](#S2.SS3 "2.3 LLM
    Attacks ‣ 2 Background ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Biases in Computer Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing works showed that biases exist in computer systems due to social circumstances,
    design choices, and use cases [[31](#bib.bib31)]. For example, some computer games
    only have male characters [[32](#bib.bib32)], and some voting machines were inaccessible
    to people of low height and people with reduced visual acuity [[33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models might also have biases due to data, algorithms, and
    users [[34](#bib.bib34)]. For example, face-recognition systems can perform differently
    based on demographics, face geometry, and periocular features [[35](#bib.bib35)].
    YouTube video captions have a significantly lower word error rate for men then
    for women [[36](#bib.bib36)]. Compared to white defendants, regression models
    tend to falsely flag black defendants as future criminals more often [[37](#bib.bib37)].
    Some approaches have been proposed to mitigate the biases in machine learning
    algorithms [[38](#bib.bib38), [39](#bib.bib39)], but such methods typically suffer
    from decreased overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, as specific machine learning models, also have biases, although the specific
    definition of biases is context-dependent and culture-dependent [[40](#bib.bib40)].
    In addition to various efforts to define [[41](#bib.bib41), [42](#bib.bib42)]
    and measure [[43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)]
    biases, numerous attempts have been made to mitigate biases in LLMs [[47](#bib.bib47),
    [48](#bib.bib48)]. However, to the best of our search, we did not find existing
    literature on biases in shopping brand recommendations, which is the use case
    in this paper, or biases induced by innocuous modifications of prompts, which
    is the general area we explore.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Human-Inconspicuous Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One possible goal of adversaries is for attacks to be human-inconspicuousness:
    that humans at the scene are not able to notice an ongoing attack [[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)]. Examples
    of such attacks include human-inaudible voice commands to smartphones [[10](#bib.bib10)]
    and laser-based audio injection on voice controls [[64](#bib.bib64)]. Some defenses
    have been specifically proposed to detect such attacks [[65](#bib.bib65), [66](#bib.bib66),
    [55](#bib.bib55), [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evasion attacks are a type of attack on machine-learning systems that often
    tries to achieve inconspicuousness [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75)]. With slight perturbations on images, evasion attacks aim to
    force well-trained machine learning models to behave unexpectedly [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)].
    In the image domain,  norms might not accurately correspond to inconspicuousness [[87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)]. Alternatively, some patch-based evasion attacks
    have been proposed and empirically verified to be inconspicuous to humans [[11](#bib.bib11),
    [12](#bib.bib12)]. Various defense techniques against evasion attacks have been
    explored in previous works: some detect evasion attacks [[90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92)], some use transformations [[93](#bib.bib93), [94](#bib.bib94),
    [95](#bib.bib95)], some prove lower bounds of robustness of machine learning models
    against evasion attacks [[96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)],
    and some train machine learning models to work properly in adversarial circumstances [[103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the NLP domain, different approaches have been suggested to generate human-inconspicuous
    attacks: some use the distances between words (e.g., the Levenshtein edit distance)
    or embeddings (e.g., the USE score [[109](#bib.bib109)]) as metrics to measure
    inconspicuousness [[24](#bib.bib24), [13](#bib.bib13), [17](#bib.bib17), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)], some change only a small number of words [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)], some utilize generative
    models [[21](#bib.bib21)], some exploit common typos [[22](#bib.bib22)], and some
    perform synonym replacement [[23](#bib.bib23)]. However, all these works either
    do not have an accompanying user study (e.g., [[21](#bib.bib21)]) or are suggested
    by user studies to not be inconspicuous to humans (e.g., [[24](#bib.bib24)]).
    In this work, we suggest a new text-domain inconspicuous attack (see §[4.2](#S4.SS2
    "4.2 Synonym-Replaced Adversarial Prompts ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") and §[6.2](#S6.SS2 "6.2 Synonym-Replaced
    Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations")) whose inconspicuousness is verified by a user
    study (see §[7.1](#S7.SS1 "7.1 Methods ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") and §[7.2.2](#S7.SS2.SSS2 "7.2.2 Statistical
    Evaluation ‣ 7.2 Results ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 LLM Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In light of LLMs’ prevalence, the security risks of LLMs and systems with LLMs
    have drawn the attention of researchers (e.g., [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)]). We review proposed LLM attacks in the following paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Backdoor attacks* assume that adversaries can control some of the training
    data. In such attacks, adversaries attempt to change LLMs’ inference-time behavior
    by changing the data on which the LLMs are trained or tuned [[110](#bib.bib110),
    [111](#bib.bib111), [6](#bib.bib6), [112](#bib.bib112)]. Empirical [[113](#bib.bib113)]
    and provable [[114](#bib.bib114)] defenses have been proposed against backdoor
    attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Jail-breaking attacks* aim to cause LLMs to generate inappropriate content
    (e.g., offensive content). Adversaries in jail-breaking attacks do not have access
    to training data, but instead devise prompts that cause LLMs to respond with inappropriate
    content [[7](#bib.bib7), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120)]. Most existing works
    evaluate jail-breaking attacks by success rate: the probability that offensive
    content is generated when LLMs are given perturbed prompts [[121](#bib.bib121)].
    Defenses against jail-breaking attacks, also known as guard-railing, have been
    explored by many works [[122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data-extraction attacks* aim to reproduce some or part of the training data [[8](#bib.bib8),
    [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129),
    [130](#bib.bib130), [131](#bib.bib131)]. In data-extraction attacks, adversaries
    query LLMs potentially many times, with any prompts they wish, but cannot directly
    access LLMs’ training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The threat model we explore in this paper differs from these existing attacks:
    we examine if inconspicuous changes to normal prompts can cause LLMs’ responses
    to be become probabilistically biased in a particular direction. We explain the
    differences between our threat models and others’ in more detail at the end of
    §[3](#S3 "3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Threat Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/791a4d06426ed45d7edc7b5be494e80d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Pipeline of an attack where the adversaries craft prompts and persuade
    LLM users to try these prompts. For example, Instacart suggests prompts users
    can try with its ChatGPT-powered search  [[132](#bib.bib132)]. Once persuaded,
    the users send these prompts to LLMs and read the responses.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/360f049e34ef376bd7d3236953d659d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Pipeline of an attack where users ask adversaries to draft prompts.
    Users may ask prompting services to draft prompts for efficiency and utility.
    Users then forward the prompts to LLMs and read the responses. Companies (e.g.,
    PromptPerfect [[133](#bib.bib133)]) offer such services.'
  prefs: []
  type: TYPE_NORMAL
- en: We posit that prompts from untrusted sources can cause unforeseeable biases
    in LLM responses. In this paper, we use brand recommendations during shopping
    as our attack scenario. More specifically, we assume that users (i.e., victims),
    who are shopping for specific goods (e.g., TVs), decide to ask LLMs for brand
    recommendations. Instead of writing prompts, victims use prompts created by adversaries
    (e.g., prompting services. See scenarios below for details).
  prefs: []
  type: TYPE_NORMAL
- en: For chatbots that outsource their underlying LLMs to third-party services,²²2many
    have announced or implemented chatbots using outsourced LLMs (e.g., OpenAI API),
    including Instacart [[132](#bib.bib132)], Lowe’s [[134](#bib.bib134)], Expedia [[135](#bib.bib135)],
    and more the aforementioned scenario creates an attack vector that does not require
    control over the model.
  prefs: []
  type: TYPE_NORMAL
- en: This threat isn’t just limited to chatbots with external LLMs. Despite owning
    control over the LLM (e.g., Gemini, ChatGPT), retraining models is costly [[136](#bib.bib136)].
    Injecting bias through prompts, as we’ll show later, is not.
  prefs: []
  type: TYPE_NORMAL
- en: 'These LLMs have been vulnerable to many attacks, like jail-breaking (see §[2.3](#S2.SS3
    "2.3 LLM Attacks ‣ 2 Background ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations")). These risks are not hypothetical: a chatbot used
    by a car dealership was prompted to sell a $ [[137](#bib.bib137)], Air Canada
    had to honor the hallucinated policy by its customer support chatbot [[138](#bib.bib138)].'
  prefs: []
  type: TYPE_NORMAL
- en: Below is concrete scenarios our threat model applies to.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.p6.pic1" class="ltx_picture" height="14.16" overflow="visible" version="1.1"
    width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0 0) translate(7.08,0)
    translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") shows an example where a chatbot service
    is suggesting prompts. If the chatbot is adversarial, instead of engaging in costly
    re-training of models [[136](#bib.bib136)], it may suggest specially crafted prompts
    that result in biased respones. Notably, many chatbots already implement prompt
    recommendation features.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.p7.pic1" class="ltx_picture" height="14.16" overflow="visible" version="1.1"
    width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0 0) translate(7.08,0)
    translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Users, seeking increased efficacy and utility from chatbots, may ask prompting
    services (e.g., PromptPerfect [[133](#bib.bib133)]) to write or optimize prompts.
    Fig. [4](#S3.F4 "Figure 4 ‣ 3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") shows the attack pipeline in this use case.
    Users first ask adversaries to write prompts and then use them with chatbots.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.p8.pic1" class="ltx_picture" height="14.16" overflow="visible" version="1.1"
    width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0 0) translate(7.08,0)
    translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, adversaries may release prompts on online forums and encourage users
    to try them. If adversaries manage to convince victims to do so, users will send
    the prompts to LLMs and read the responses. For example, Instacart suggests prompts
    users can try with its LLM powered search [[132](#bib.bib132)]. Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Sales Whisperer: A Human-Inconspicuous Attack on
    LLM Brand Recommendations") shows an example where some Reddit users encouraged
    others to try some prompts on ChatGPT  [[2](#bib.bib2)]. Fig. [3](#S3.F3 "Figure
    3 ‣ 3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations") shows the attack pipeline in this use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Specific goals and constraints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In our threat model, regardless of specific use cases, adversaries cannot,
    or are disincentivized to change the weights of the LLM. They can, however, suggest
    prompts to users. Adversaries may also query LLMs with these prompts in advance
    of the attacks. We further assume that the adversary is constrained in its prompt
    perturbation: the prompts and resulting responses must not alert users to the
    attack. As such, the prompts and responses must be inconspicuous (see §[2.2](#S2.SS2
    "2.2 Human-Inconspicuous Attacks ‣ 2 Background ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")). If users are suspicious (e.g., propmts/responses
    semantically incorrect, containing nonsequiturs), users may stop using these prompts.
    We propose a practical definition of inconspicuousness for prompts and responses
    in [subsubsection 7.1.1](#S7.SS1.SSS1 "7.1.1 Survey procedures ‣ 7.1 Methods ‣
    7 User Study ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09e6ecb058867137d15c5ef25a8c979d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An illustration of the adversaries’ goals. In this example, the adversary
    tries to increase the frequency of a brand (A) thorough inconspicuous prompt recommendations
    and is successful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goal of the adversary is to induce LLMs to recommend certain brands
    more often, while not necessarily the most often among all brands. [Figure 5](#S3.F5
    "Figure 5 ‣ Specific goals and constraints ‣ 3 Threat Model ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations") depicts this goal.
    In the presence of the perturbed prompts, the LLM recommends brand A more often
    than in the baseline condition. Similar to advertising, adversaries may economically
    benefit from this outcome. Notably, the adversary does not aim to prevent other
    brands (e.g., brand C), from being recommended more frequently. In practice, each
    response may recommend more than one brand. Note that adversaries may *not* need
    to cause LLMs to mention the exact brand name to recommend the brand. For example,
    to recommend the brand “Apple” for “laptops,” adversaries may instead cause “Macbook”
    to be recommended.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, in our threat model, adversaries suggest prompts but do not have
    control over the model. An attack succeeds if, compared to a baseline, 1) prompts
    and responses are inconspicuous users and 2) the LLMs recommends a target brand
    more often. We describe such attacks in §[4.1](#S4.SS1 "4.1 Observations on Paraphrased
    Prompts ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations") and §[4.2](#S4.SS2 "4.2 Synonym-Replaced Adversarial Prompts
    ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"),
    and verify the effectiveness in §[6](#S6 "6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") and §[7](#S7 "7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations"). Our threat model
    is different from those described in previous work (§[2.3](#S2.SS3 "2.3 LLM Attacks
    ‣ 2 Background ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")):
    compared to backdoor attacks, adversaries in our threat model have no control
    over training data; compared to data extraction attacks, our adversaries do not
    extract training data and cannot use conspicuous prompts; compared to jail-breaking
    attacks, our adversaries do not generate inappropriate content, cannot use conspicuous
    prompts, and do not have access to model weights (or gradients).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce how we build the approach that achieves the attack
    goals described in §[3](#S3 "3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations"): generating prompts and corresponding responses
    that are inconspicuous to humans (i.e., they do not know an attack is taking place)
    and cause LLMs to recommend a target brand more often, ultimately increasing attention
    received from humans. We first describe our observation on LLM’s responses to
    paraphrased prompts (§[4.1](#S4.SS1 "4.1 Observations on Paraphrased Prompts ‣
    4 Methods ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")),
    and then illustrate our synonym replacement approach to perturb prompts human-inconspicuously
    §[4.2](#S4.SS2 "4.2 Synonym-Replaced Adversarial Prompts ‣ 4 Methods ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Observations on Paraphrased Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models, including LLMs, are brittle (e.g., [[82](#bib.bib82)]).
    They can be highly sensitive to small changes in their input.
  prefs: []
  type: TYPE_NORMAL
- en: 'To promote a target brand, we apply this intuition. We explore attacking LLMs
    by testing paraphrased prompts. Specifically, we generate a set of prompts asking
    for recommendations of products in a certain category that are all paraphrases
    of one another. Our approach to this is described in §[5.2](#S5.SS2 "5.2 Evalution
    Procedure ‣ 5 Setup ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations"). We show that these various paraphrased prompts (§[6.1](#S6.SS1
    "6.1 Observations on Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")), although similar, can lead to increased
    prominence of a brand in the LLM’s responses. Given enough paraphrases, we’re
    able to find a prompt that causes LLMs to recommend a target brand more often.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach does not require any access to LLMs’ internal weights or token
    probabilities. However, it has a caveat, we need to try many paraphrases and collect
    many responses to find an optimal prompt. This computational cost might be an
    economic burden for the adversaries. Additionally, when we generate the paraphrases,
    some are likely less similar to each other and thus human-detectable. While we
    are able to find paraphrases that result in a large change in the probability
    that a certain brand is mentioned, even while using only a few prompts per category,
    as we describe in §[5.2](#S5.SS2 "5.2 Evalution Procedure ‣ 5 Setup ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations"), in cases where an
    adversary does not have the time or computational resources to collect responses
    to multiple paraphrased prompts, adversaries may want to take another approach.
    As such, we propose another method to perturb prompts. To reduce computational
    overhead, with this new approach we assume access to the logits of LLMs in §[4.2](#S4.SS2
    "4.2 Synonym-Replaced Adversarial Prompts ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Synonym-Replaced Adversarial Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The fundamental idea of this attack is to generate prompts by replacing words
    with synonyms and picking one that minimizes a loss function, reducing the need
    to collect computationally expensive responses. As we described in §[2.2](#S2.SS2
    "2.2 Human-Inconspicuous Attacks ‣ 2 Background ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations"), while many NLP domain attacks have been
    proposed to be inconspicuous to humans, they either do not have accompanying user
    studies or are shown to be conspicuous to humans, motivating our approach. With
    this attack, we propose an improved synonym replacement method to generate human-inconspicuous
    perturbations.'
  prefs: []
  type: TYPE_NORMAL
- en: Existing synonym replacement methods [[23](#bib.bib23)] either use WordNet,
    an established synonym dictionary, or a self-defined synonym dictionary [[25](#bib.bib25),
    [26](#bib.bib26)]. Nouns and verbs were found to have various forms (e.g., plural
    forms of nouns and verb tenses) [[27](#bib.bib27)], and thus an adjective synonym
    dictionary was suggested [[28](#bib.bib28)]. However, we found that synonyms of
    adjectives listed in existing dictionaries might not be synonyms in the context
    of brand recommendations. For example, WordNet and the existing adjective synonym
    dictionary both suggested “*raw*” as a synonym for “*newest*,” while it is human-conspicuous
    to recommend “*the raw smartphone*” instead of “*the newest smartphone*.” Thus,
    we created a new synonym dictionary compatible with brand recommendations. Our
    synonym dictionary mainly consists of adjectives but also includes other parts
    of speech while keeping the tense of verbs consistent between synonyms. In our
    dictionary, “*select*” and “*choose*” are synonyms, creating a synonym group,
    and other tenses of these verbs are excluded. “*Exact*,” “*accurate*,” and “*precise*”
    also creats a synonym group, this time of three. Our dictionary contains a total
    of 94 words in 36 synonym groups. Each word may have at most seven synonyms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since paraphrasing prompts imposes an economic burden (§[4.1](#S4.SS1 "4.1
    Observations on Paraphrased Prompts ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")), systematically testing and gathering responses
    for all candidate perturbed prompts generated by synonym replacement would have
    a high cost. One prompt we tested had six words with synonyms and therefore  candidates,
    we used a loss-guided candidate-selection method. Prior work suggested that when
    adversaries have white-box access to LLMs (i.e., when adversaries can access internal
    architecture and weights of LLMs), adversaries may use gradient search to perturb
    prompts [[7](#bib.bib7)]. Compared to prior work, however, our method does not
    require access to model weights or gradients. Our search space is much smaller,
    we do not need consider all possible tokens a model accepts, just synonyms [[7](#bib.bib7)].
    Instead, we computed a logit-based loss for all possible combinations of synonym
    replacements and picked the combination with the lowest loss. It should be noted,
    that other candidates than the one with the lowest could be used as well, and
    we used this method to narrow the candidates down in order to be able to test
    more base prompt and target brand combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we use the following loss function. Using the same notation as
    existing works [[7](#bib.bib7)], we considered LLMs as a mapping from a sequence
    of tokens . In other words, LLMs generate a probability  can be denoted as $p(X_{n+1:n+H}|X_{1:n})$.
    In contrast to Zou et al., that aim to generate a specific sequence of tokens
    using the loss function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\ell(X_{1:n})=-logp(X_{n+1:n+H}&#124;X_{1:n})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: We designed a loss function that aims to generate a sequence among a set of
    possible candidate sequences $T$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\ell(X_{1:n})=-logp(X_{n+1:n+H}\in T&#124;X_{1:n})$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Intuitively, our loss function aims to cause LLMs to generate *some* sequence
    from the set . As we describe in §[3](#S3 "3 Threat Model ‣ Sales Whisperer: A
    Human-Inconspicuous Attack on LLM Brand Recommendations"), adversaries may *not*
    need to cause LLMs to mention a specific brand name to recommend the brand. For
    example, causing LLMs to recommend either “Macbook” or “Apple” meets adversaries’
    goal to recommend the brand “Apple” for the category “laptops”, and in this case,
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our loss function aims to increase the likelihood of words to appear from the
    target set  *right after* the prompt, adversaries are still successful if any
    of the target words appear in the generated response (i.e., many tokens after
    the prompt). In §[6.2](#S6.SS2 "6.2 Synonym-Replaced Adversarial Prompts ‣ 6 Results
    ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"),
    we show that a prompt with a lower loss value according to our new loss function
    was more likely to mention one of the target words set $T$ among up to 64 generated
    tokens. Additionally, we note that increased likelihood of a brand did not always
    guarantee “recommending” that brand. For instance, when we caused Gemma (one of
    the LLMs we use) to mention “Netflix” as a streaming service, Gemma yielded “I’ve
    been using Netflix for years, but I’m not happy with the selection of movies and
    TV shows that are available to me.” People may interpret the same LLM response
    differently and thus have different answers to whether a specific brand is recommended
    given the same response. This work, therefore, includes a more realistic user-study
    evaluation. We explored whether humans understood that the target brand was being
    recommended more in LLM responses to prompts we perturbed (§[7.2.2](#S7.SS2.SSS2
    "7.2.2 Statistical Evaluation ‣ 7.2 Results ‣ 7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will describe the setups of our experiments. We will first
    introduce our choice of LLM and parameters in §[5.1](#S5.SS1 "5.1 LLM Setup ‣
    5 Setup ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").
    Then we depict our experiment process, including evaluation metrics, in §[5.2](#S5.SS2
    "5.2 Evalution Procedure ‣ 5 Setup ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 LLM Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our experiments, we used four open-sourced LLMs as our benchmarks: a 7B
    pre-trained Llama 2, an 8B pre-trained Llama 3, an 8B instruction-tuned Llama
    3, and a 7B instruction-tuned Gemma. Each of these models was downloaded from
    Meta or Google’s official repositories on HuggingFace. We used various LLMs to
    ensure our conclusion is not a special case only applicable to one specific instance
    of LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Temperature is a parameter that controls the determinism of LLM responses. Therefore,
    with different temperature choices, LLMs may tend to recommend brands with different
    frequencies. Following the setup that existing works used to measure the indeterministic
    behavior of LLMs [[7](#bib.bib7)], we used the default temperature for each of
    the four models in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Existing works collected up to  different combinations of prompts, target brands
    to be mentioned, and LLMs. We collected two sets of  responses per combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'When collecting responses, we generate 64 tokens per prompt. We focus on the
    first 64 due to three reasons, 1) focusing on the beginning of responses likely
    is a better heuristic of what brand users are likely to see first, thus remember
    (this theory is supported in our user study §[7](#S7 "7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations")); 2) some models (e.g.,
    Llama 2, Llama 3) don’t stop generating tokens until they reach the maximum token
    limit of 4096 is reached, resulting in repetitive and meaningless responses; and
    3) computational cost prohibits us from collecting more tokens than we have.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will demonstrate that the synonym replacement approach (§[4.2](#S4.SS2 "4.2
    Synonym-Replaced Adversarial Prompts ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")) causes LLMs to mention target brands more
    often (§[6.2](#S6.SS2 "6.2 Synonym-Replaced Adversarial Prompts ‣ 6 Results ‣
    Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evalution Procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first listed 77 product categories where several established brands dominate
    the market. For each category, we listed popular brands we had heard about and
    also added brands that LLMs recommended but we did not include: some of the brands
    were later found to be rarely recommended by LLMs. Some brands appeared in more
    than one category: for example, the brand “Apple” appeared in both the category
    “laptops” and the category “smartphones”. The number of brands we listed for each
    category ranged from one to nine, with an average of $3.96$ per category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aided by ChatGPT, we gathered prompts that recommended brands in each category.
    Specifically, we queried ChatGPT with the following prompt: “Give me multiple
    rephrasings and add details to: ‘What is the best XXX?’”, where “XXX” is the category
    (e.g., smartphones). The prompts collected in the same category were believed
    to be paraphrases by ChatGPT, and we manually checked that these prompts 1) paraphrased
    each other and 2) were inconspicuous (see definition in §[3](#S3 "3 Threat Model
    ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"))
    to us researchers. We filtered out the prompts that violated either of these two
    principles. An example of paraphrases we used would be “*Which VPN service stands
    out as the optimal choice for ensuring top-notch online privacy and security according
    to your experience?*” and “*Can you recommend the ultimate VPN that excels in
    providing robust encryption, reliable performance, and a user-friendly experience?*”,
    resulting in two prompts that request a recommendation for a VPN. Paraphrased
    prompts may ask for slightly different features and may use differing wording,
    but ultimately ask the same thing of the LLM. Ultimately, we had three to ten
    prompts per category, with an average of  prompts. We collected  prompts on each
    of the four models. We used these prompts to examine the method we described in
    §[4.1](#S4.SS1 "4.1 Observations on Paraphrased Prompts ‣ 4 Methods ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we introduced in §[4.2](#S4.SS2 "4.2 Synonym-Replaced Adversarial Prompts
    ‣ 4 Methods ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"),
    for each combination of category and brand, adversaries may aim to cause LLMs
    to mention *some* words that are not necessarily brand names. For example, any
    of “ChatGPT”, “OpenAi” and “GPT” are target words (or, in some cases, strings)
    for the brand “ChatGPT” in the category “LLMs”. For each combination of category
    and brand, we collected a list of target words. We created these lists of target
    words based on our knowledge and observations of the responses from the 449 different
    prompts. Each combination of category and brand had one to five target words.
    We will compare paraphrased prompts by how often they mention some target words
    of a brand and category in §[6.1](#S6.SS1 "6.1 Observations on Paraphrased Prompts
    ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").
    In this paper, we refer to a response as mentioning a target brand if it contains
    any of that brand’s target words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the efficacy of our synonym replacement approach (introduced in
    §[4.2](#S4.SS2 "4.2 Synonym-Replaced Adversarial Prompts ‣ 4 Methods ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations")), we accordingly perturbed
    the 449 prompts in favor of each brand of the same category. We obtained  prompts
    by how often they mention a target brand in §[6.2](#S6.SS2 "6.2 Synonym-Replaced
    Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations"). We paired the prompts after synonym replacement
    with those before synonym replacement (i.e.,  of responses of the prompt before
    synonym replacement, and .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some machine learning attacks were found capable of transferring [[29](#bib.bib29),
    [30](#bib.bib30)]: attacks against a machine-learning model might be effective
    against a different, potentially unknown, model. We thus investigated whether
    our synonym replacement approach can transfer to GPT3.5-turbo, a commercial and
    close-sourced LLM. On each of our four open-sourced LLMs, we first ranked the  complete
    responses to both prompts: we allowed LLMs to keep generating until they yielded
    end-of-sequence (EOS) tokens. Instead of absolute improvement, we computed the
    relative improvement in the probabilities of mentioning the target brand within
    complete responses for each pair. If  of responses of the prompt after perturbing
    it mention the target brand, the relative improvement was  complete responses.
    GPT-3.5 turbo Instruct is the instruction-tuned version of GPT-3.5 turbo. We compare
    the relative improvements between GPT models and open-sourced models in §[6.3](#S6.SS3
    "6.3 Transferability to GPT Models ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we describe our empirical results. We first, in §[6.1](#S6.SS1
    "6.1 Observations on Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") describe our observations on paraphrases
    of prompts—measuring how pairs of paraphrased prompts can have significant differences
    in the probability that the target brand appears. Next, in §[6.2](#S6.SS2 "6.2
    Synonym-Replaced Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations"), we report how often our synonym-replacement
    approach yields a perturbed prompt that causes LLMs to mention the targeted brand
    more often than the base prompt Finally, in §[6.3](#S6.SS3 "6.3 Transferability
    to GPT Models ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations"), we explore whether our synonym-replacement approach transfers
    to GPT models. We show that if the right model-pair is found, successful perturbed
    prompts found on an open-sourced LLM can increase the probability of the target
    brand appearing more often in the GPT model’s responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Observations on Paraphrased Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06630a5c71b1feb929efe8b4e63fbfc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Absolute difference in the likelihoods of responses mentioning a
    target brand within the first  (i.e., one prompt elicits responses that never
    mentions the target brand while another prompt’s responses always do).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we described in §[5.2](#S5.SS2 "5.2 Evalution Procedure ‣ 5 Setup ‣ Sales
    Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"), each of
    the  tokens). Adversaries may paraphrase a low-probability base prompt in order
    to find the high-probability prompts to achieve the goals described in §[3](#S3
    "3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize our results in Fig. [6](#S6.F6 "Figure 6 ‣ 6.1 Observations on
    Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations"). Specifically, on four open-sourced LLMs—Gemma-it
    (instruction-tuned), Llama2, Llama3, and Llama3-it (instruction-tuned)—the likelihood
    of the target brand being mentioned in the responses can differ between a pair
    of paraphrased prompts by up to , with the average of this absolute difference
    being , , respectively. For example, when comparing responses to “*I’m curious
    to know your preference for the pressure cooker that offers the best combination
    of cooking performance, durable construction, and overall convenience in preparing
    a variety of dishes.*” with the prompt paraphrased as “*Can you recommend the
    ultimate pressure cooker that excels in providing consistent pressure, user-friendly
    controls, and additional features such as multiple cooking presets or a digital
    display for precise settings?*”, the probability of Gemma-it mentioning the brand
    “InstantPot” (“pressure cooker” product category) within the first  to $100.0\%$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results suggest that while paraphrased prompts appear similar to humans
    the responses to the prompts can differ substantially in how likely they are to
    mention a target brand (within the first $64$ tokens of the response). Therefore,
    an adversary wanting to promote a certain brand can use this to their advantage:
    The adversary may try various paraphrases of prompts, test the prompts, and pick
    the paraphrase that results in the highest probability of the target brand being
    mentioned, ultimately promoting the brand when prompt is used in the real world.
    While we generated these paraphrases using ChatGPT, and confirmed that they were
    valid and reasonable, adversaries may also be able to create paraphrases manually
    or by another method, and may be able to test even more paraphrases than we did
    in our measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Synonym-Replaced Adversarial Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As described in §[5.2](#S5.SS2 "5.2 Evalution Procedure ‣ 5 Setup ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations"), we measured and now
    report how often our synonym-replacement approach improves the probability of
    a brand being mentioned. As opposed to paraphrasing, in the synonym-replacement
    approach we automatically generate a set of potential candidate prompts by perturbing
    a base prompt via synonym replacement, without needing to confirm whether these
    perturbed prompts are valid. The prompt with the lowest loss is selected, and
    we assess how well these selected prompts increase the probability that the target
    brand is mentioned. We evaluate the average improvement over multiple models,
    at different response lengths, and for different probabilities of a brand being
    mentioned in the base prompt. In addition, other candidate prompts could also
    be tested, although we did not take this approach in this paper. We used loss
    as a metric that narrowed down the large set of potential perturbed prompts we
    found using synonym replacement to one. Therefore, we are interested in exploring
    the *highest* improvement we can achieve between a base and perturbed score, as
    adversaries would be able to use a similar method to §[6.1](#S6.SS1 "6.1 Observations
    on Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations") and explore multiple perturbated prompts. So, we
    also describe the largest increases in probability of mentioning a target brand
    in the responses to the perturbed prompt we found via our synonym-replacement
    method compared to the base prompt. Our evaluation shows that our synonym-replacement
    attack increases the likelihood of LLMs mentioning a brand on average.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For all evaluations, we focus on the first 64 tokens of the response (see [subsection 5.1](#S5.SS1
    "5.1 LLM Setup ‣ 5 Setup ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations") for details).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6fb2fa49a741e42c72f7118b2adc754b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Average absolute improvement in likelihoods that LLMs mention a target
    brand. Results presented along number of generated tokens. Our synonym-replacement
    approach achieves improvements in probabilities, which verifies the capability
    of forcing LLMs to mention target brands more often.'
  prefs: []
  type: TYPE_NORMAL
- en: Average improvement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The results for average improvement in likelihoods of a brand being mentioned
    in the responses to perturbed prompts created via synonym replacement compared
    to in response to the original base prompt, across models, are shown in Fig. [7](#S6.F7
    "Figure 7 ‣ 6.2 Synonym-Replaced Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations"). Overall, we find
    that our attack results in an average absolute improvement in all models. Specifically,
    for brands that were mentioned at least once (i.e.,  within  improvement within  within  within  within  within  within  within
    , , and . While we only collected responses of length 64 tokens for our 1,809
    attacks for each model (see [subsection 5.1](#S5.SS1 "5.1 LLM Setup ‣ 5 Setup
    ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")),
    we did run a select set of base and perturbed prompt pairs (chosen in §[5.2](#S5.SS2
    "5.2 Evalution Procedure ‣ 5 Setup ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations")) to full completion. While we did not see improvements
    in all pairs (for example, some base prompts had near  more likely than its base
    prompt “*Can you recommend the ultimate video game console that excels in providing
    top-notch graphics, diverse gaming options, and additional features such as online
    connectivity, suitable for both casual and hardcore gamers?*” (i.e., “ultimate”
    was changed to “superior” and “diverse” was changed to “dissimilar”) to mention
    Xbox in long completions, even more than when completions were only 64 tokens
    long ($32.9\%$). We evaluate if attack objectives are met with full responses
    in a more realistic setting in §[7.2.2](#S7.SS2.SSS2 "7.2.2 Statistical Evaluation
    ‣ 7.2 Results ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous Attack on
    LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/796f3a4169665ae5dff1cf998d2b0f33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Average absolute improvement in likelihoods that LLMs mention the
    target brand when these sequences were at least X times out of $1,000$ before
    perturbation, up to some number of tokens (not necessarily 64). X is our horizontal
    axis. When the brands were mentioned more often before perturbation, our synonym
    replacement approach achieved a bigger absolute improvement in the likelihood
    of mentioning the brand.'
  prefs: []
  type: TYPE_NORMAL
- en: Baseline probability and attack success
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here we explore the relationship between attack success rates and the probability
    of a brand being mentioned in responses to the base prompt. As shown in Fig. [8](#S6.F8
    "Figure 8 ‣ Average improvement ‣ 6.2 Synonym-Replaced Adversarial Prompts ‣ 6
    Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"),
    our synonym-replacement approach on average achieved a bigger absolute improvement
    when the target brand was mentioned more often in responses to the base prompt
    before perturbation. When the target brands were mentioned at least  out of ,
    and $2.28\%$ on Gemma-it, Llama2, Llama3 and Llama3-it, respectively. We see that
    for base prompts with higher probabilities of mentioning the target brand, our
    perturbed prompts, on average, have a higher increase in probability. This indicates
    that our perturbation method is more effective on an identifiable domain, i.e.
    prompts where the target brand is mentioned more or less frequently, than it is
    on the overall average case.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5ef96c0f56fdc4b6575167916666d04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Max absolute improvement in likelihoods that LLMs mention a target
    brand. Results are presented along how many tokens are generated. We achieve a
    bigger absolute improvement on Gemma-it compared to the other three Llama models,
    although Gemma-it does not always have the highest average absolute improvement
    (see Fig. [7](#S6.F7 "Figure 7 ‣ 6.2 Synonym-Replaced Adversarial Prompts ‣ 6
    Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")
    and Fig. [8](#S6.F8 "Figure 8 ‣ Average improvement ‣ 6.2 Synonym-Replaced Adversarial
    Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations").)'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum improvement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Besides the average absolute improvement, we also explored the maximum absolute
    improvement among all combinations of base prompts and brands, as shown in Fig. [9](#S6.F9
    "Figure 9 ‣ Baseline probability and attack success ‣ 6.2 Synonym-Replaced Adversarial
    Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations"). While these results don’t represent the *expected* improvement
    using this method, they do demonstrate that it is possible to find pairs of prompts
    with vastly different probabilities of mentioning a certain brand. In §[6.1](#S6.SS1
    "6.1 Observations on Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") we showed this for prompts that were manually
    paraphrased; here, we show that the synonym-replacement attack can find such alternative
    prompts automatically. Further, the prompts generated by synonym-replacement differ
    from their base prompts minimally (at most a seven synonym difference in our dataset),
    and are perceptually the same along multiple dimensions (see §[7.2.2](#S7.SS2.SSS2
    "7.2.2 Statistical Evaluation ‣ 7.2 Results ‣ 7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We were able to achieve bigger absolute improvement on Gemma-it compared to
    the other three Llama models, although Gemma-it does not always have the highest
    average absolute improvement (Fig. [7](#S6.F7 "Figure 7 ‣ 6.2 Synonym-Replaced
    Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations") and Fig. [8](#S6.F8 "Figure 8 ‣ Average improvement
    ‣ 6.2 Synonym-Replaced Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")). When LLMs generate all 64 tokens, Gemma-it
    has the highest absolute improvement of .'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Transferability to GPT Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8568b0ebe5c79cca26a7f28b40f983ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Pearson correlation coefficients (, Llama3-it and GPT3.5-turbo have
    a correlation coefficient of ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluated the transferrability of paraphrased prompts generated using synonym-replacement
    (§[6.2](#S6.SS2 "6.2 Synonym-Replaced Adversarial Prompts ‣ 6 Results ‣ Sales
    Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")) from open-sourced
    models to GPT models. We found that, while transferability was limited for most
    model pairs, Llama3-it and GPT3.5-turbo had a high correlation in attack success
    for the same prompts ($p<0.001$). We explain these results in more detail next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we described in §[5.2](#S5.SS2 "5.2 Evalution Procedure ‣ 5 Setup ‣ Sales
    Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"), we computed
    the relative improvement on four open-sourced LLMs and two versions of GPT models.
    Specifically, for a given base prompt, we generated a perturbed prompt via synonym-replacement
    that increased the chance of a brand being mentioned on an open-sourced model
    and then evaluated whether the perturbed prompt also increased the chance of the
    brand being mentioned in the response produced by the GPT model. We evaluated
    only the transfer of attacks that were successful on their open-sourced LLM onto
    GPT models. For the Shell example described in §[6.2](#S6.SS2 "6.2 Synonym-Replaced
    Adversarial Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations"), we saw a improvement from  between the base and
    perturbed prompt on long responses by Gemma-it between prompts and an improvement
    from  on GPT3.5-turbo responses. On long responses generated by Llama3-it, “*If
    you had to pinpoint the superior investment platform, which one would it be, and
    what specific features make it stand out as the top choice for investors?*” had
    a , and GPT3.5-turbo responses went from .'
  prefs: []
  type: TYPE_NORMAL
- en: To compare the relative improvements between models when the same prompts and
    synonym replacement are used, we used the Pearson correlation test. Two sets of
    data with a Pearson correlation coefficient ( is generally believed to be weakly
    correlated, whereas  is believed to indicate strong correlation [[139](#bib.bib139)].
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [10](#S6.F10 "Figure 10 ‣ 6.3 Transferability to GPT Models
    ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"),
    while Pearson correlation coefficients of the relative improvement of most pairs
    of open-sourced LLMs and GPT models indicated weak or no correlation (, and thus
    are highly correlated (. This result indicates a potential for a transfer attack
    to be used on chatbots that use black-box GPT models, like Instacart, Lowe’s,
    and Expedia (described in §[3](#S3 "3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations")) and others. A successful attack on an open-sourced
    LLM could be used as a prompt suggestion for black-box models, ultimately promoting
    the target brand.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 User Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our results so far show that our synonym-replacement approach can bias LLM
    responses towards a target brand and appears to be human-inconspicuous. However,
    this does not mean that the attack is successful in practice. To evaluate the
    attack in a realistic setting, we conducted a user study. Here we describe this
    study, starting with a description of our methods (§[7.1](#S7.SS1 "7.1 Methods
    ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"))
    and ending with the statistical evaluation (§[7.2](#S7.SS2 "7.2 Results ‣ 7 User
    Study ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")).
    We find that our attack is indeed successful in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We performed a between-subjects user study to evaluate whether our synonym-replacement
    approach can result in prompts that advances the adversaries’ goals (see §[3](#S3
    "3 Threat Model ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations")),
    creating inconspicuous prompts that trigger inconspicuous responses, making a
    target brand more noticable.We tested whether users could distinguish between
    base and perturbed prompts and responses in multiple dimensions (including clarity,
    likelihood of use, satifaction, and more). Each participant was shown one prompt-and-response
    pair. Our procedures were approved by our institution’s ethics board, and pre-registered.³³3[https://osf.io/6mycr/?view_only=face90d04806439bb1f69fc110fb9a1e](https://osf.io/6mycr/?view_only=face90d04806439bb1f69fc110fb9a1e)'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Survey procedures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We recruited a gender-balanced sample from Prolific,⁴⁴4[https://www.prolific.com/](https://www.prolific.com/)
    a crowdsourcing platform commonly used with security-relevant user studies (e.g., [[140](#bib.bib140)]).
    To avoid selection bias in our study ad, we avoided mentioning LLMs or bias in
    the study title, “Chatbot prompting study.” Participants had to be in the U.S.,
    be 18 or older, and have a Prolific approval rate of at least 95%. In line with
    Prolific guidelines ⁵⁵5[https://researcher-help.prolific.com/hc/en-gb/articles/4407695146002-Prolific-s-payment-principles](https://researcher-help.prolific.com/hc/en-gb/articles/4407695146002-Prolific-s-payment-principles),
    participants were paid $1.6 for an estimated 7–9 minute study. In practice, the
    hourly compensation rate was $12.45/hour.
  prefs: []
  type: TYPE_NORMAL
- en: After providing consent, participants were given an overview of the study. We
    asked our participants to imagine that there was a chatbot service that was able
    to recommend prompts appropriate for what users want to use the chatbot for. We
    further instructed participants to imagine that they were shopping for a certain
    product (e.g., laptops) or service (e.g., parcel delivery) and wanted to use this
    new chatbot service to help them decide on a brand. After a comprehension check
    on instructions, participants were shown a prompt recommendation for the product
    they were shopping for (e.g., “*Which laptop model do you consider the optimal
    choice for versatile computing, powerful performance, and innovative features
    that enhance your work and entertainment experience?*”) and instructed to review
    it. After a minimum of 10 seconds had passed, we asked (1) how likely they were
    to use this prompt, (2) how clear the prompt was, (3) was it biased to a certain
    brand (and which), (4) and if anything stood out (e.g., interesting, unexpected).
  prefs: []
  type: TYPE_NORMAL
- en: 'Participants were then shown a response to the prompt and after a minimum of
    20 seconds, they were asked: (1) how clear the response was, (2) if they were
    satisfied with the response, (3) how likely were they to take the recommendation
    in the response, (4) and if anything stood out. In a series of open-ended questions,
    we additionally asked participants (1) which brand they would pick based on this
    response, (2) what were all the brands recommended, and (3) what the top brand
    recommendation of the chatbot was. These 11 (numbered) questions form the practical
    definition of inconspicuousness and increase in target brand perception. They
    form the basis of our statistical analysis §[7.2.2](#S7.SS2.SSS2 "7.2.2 Statistical
    Evaluation ‣ 7.2 Results ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations").'
  prefs: []
  type: TYPE_NORMAL
- en: To help with recall, participants could hover over relevant questions to reveal
    the relevant prompts and responses.
  prefs: []
  type: TYPE_NORMAL
- en: Participants self-reported how frequently they give tech advice, how freuqently
    they use chatbots, if they paid for a chatbot, their ChatGPT familiarty. The survey
    concluded with demographics questions.
  prefs: []
  type: TYPE_NORMAL
- en: '| Gender | Male | 48.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Female | 50.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Self-described | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Age | 18-25 | 16.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 26-35 | 36.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 36-45 | 21.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 46-60 | 18.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 61+ | 5.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Ethnicity | White | 63.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Black or African Am. | 12.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Asian | 9.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hispanic or Latino | 5.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Other or mixed | 12.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Education | Completed H.S. or below | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Some college, no degree | 18.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Trade or vocational | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Associate’s degree | 11.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Bachelor’s degree | 39.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Master’s or higher | 18.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Chatbot usage | Daily or more freq. | 16.6 |'
  prefs: []
  type: TYPE_TB
- en: '| frequency | Daily to monthly | 49.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Monthly or less freq. | 33.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | A lot | 57.2 |'
  prefs: []
  type: TYPE_TB
- en: '| familiarity | A little | 40.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Nothing at all | 2.1 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: Demographics. Percentages might not add up to 100% due to rounding
    and missing responses.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Prompt |  | Reponse |'
  prefs: []
  type: TYPE_TB
- en: '|  | Clarity (L7) | Use (L7) | Bias (L5) | Standout (B) |  | Clarity (L7) |
    Use (L7) | Satisfied (L7) | Standout (B) |'
  prefs: []
  type: TYPE_TB
- en: '| TV | -0.23\stackon[.1pt] | -0.15\scaleto∗4pt | -0.03\stackon[.1pt]\stackon[.1pt]
    | -0.08\stackon[.1pt]\stackon[.1pt] |  | 0.06\stackon[.1pt]\stackon[.1pt] | 0.00\scaleto∗4pt
    | 0.05\scaleto∗4pt | -0.02\stackon[.1pt]\stackon[.1pt] |'
  prefs: []
  type: TYPE_TB
- en: '| ISP | -0.28\stackon[.1pt] | -0.04\stackon[.1pt]\stackon[.1pt] $\scaleto{\ast}{3pt}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Parcel delivery | -0.15\stackon[.1pt]\stackon[.1pt] $\scaleto{\ast}{3pt}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gaming console | {-0.8\stackon[.1pt]} | -0.10\scaleto∗4pt | -0.01\stackon[.1pt]\stackon[.1pt]
    | 0.00\stackon[.1pt]\stackon[.1pt] |  | -0.17\stackon[.1pt]\stackon[.1pt] | -0.21
    | -0.21\stackon[.1pt] | -0.01\stackon[.1pt]\stackon[.1pt] |'
  prefs: []
  type: TYPE_TB
- en: '| Investment plat. | 0.01\stackon[.1pt] | -0.10\stackon[.1pt]\stackon[.1pt] $\scaleto{\ast}{3pt}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Laptop | -0.04\stackon[.1pt] | {0.35\scaleto∗4pt} | -0.0\stackon[.1pt]\stackon[.1pt]
    | 0.04\stackon[.1pt]\stackon[.1pt] |  | 0.10\stackon[.1pt]\stackon[.1pt] | 0.20\scaleto∗4pt
    | 0.22\stackon[.1pt] | 0.04\stackon[.1pt]\stackon[.1pt] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: Mean difference between perturbed and baseline groups among eight
    questions, four about prompts and four about responses (see §[7.1.1](#S7.SS1.SSS1
    "7.1.1 Survey procedures ‣ 7.1 Methods ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") for details). By default test for equivalence
    is reported (TOST WMU), {} indicates test for difference (MWU, CHI²). Higher is
    better for all differences. \scaleto∗4pt:  \stackon[.1pt]\stackon[.1pt] , \stackon[.1pt]\stackon[.1pt]
    : $p<0.0001$. L7: seven-point Likert, L5: five-point Likert, B: binary.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Gemma-it |  | LLama3-it |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | TV |  | ISP |  | Parcel delivery |  | Gaming console |  | Invesment plat.
    |  | Laptop |'
  prefs: []
  type: TYPE_TB
- en: '|  | P% | A% | T% |  | P% | A% | T% |  | P% | A% | T% |  | P% | A% | T% |  |
    P% | A% | T% |  | P% | A% | T% |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 25.0 | 33.8 | 35.3 |  | 23.9 | 46.2 | 34.3 |  | 10.6 | 22.7 | 13.6
    |  | 0.0 | 18.3 | 0.0 |  | 14.1 | 38.0 | 14.1 |  | 5.3 | 61.3 | 1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Pert | 57.1 | 80.0 | 82.9 |  | 39.1 | 78.2 | 50.7 |  | 41.4 | 58.6 | 44.3
    |  | 11.2 | 64.8 | 1.4 |  | 40.3 | 58.4 | 42.9 |  | 7.1 | 74.3 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Diff | 32.1\stackon[.1pt]\stackon[.1pt]  | 16.4 |  | 30.8\stackon[.1pt]\stackon[.1pt] 
    | 30.6\stackon[.1pt]\stackon[.1pt]  |  | 1.8 | 12.9 | -1.3 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: The percentage of responses that mentioned the targeted brand. P:
    What brand is (p)icked by the user given the response, A: What are (a)ll of the
    brands recommended in the response, T: What is the (t)op brand recommended in
    the response. \scaleto∗4pt:  \stackon[.1pt]\stackon[.1pt] , \stackon[.1pt]\stackon[.1pt]
    : $p<0.0001$.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Experimental groups
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our overall goal was to find whether people notice differences between our
    perturbed prompts compared to base prompts. For each of our 447 base prompts,
    we found a perturbed prompt for each possible target brand using the loss of each
    of our models. This resulted in 1,809 base and perturbed prompt pairs for each
    model. Because of the large number of prompt pairs, we could not test all of them.
    We selected six pairs of prompts to use in the user study, each pair from a different
    product category, giving us 12 prompts total. Because each pair of prompts belonged
    to a unique product category, we refer to them by their product categories in
    the results. The prompt pairs were split between two models, three pairs for Llama3-it
    and three for Gemma-it. We focused on the more user-friendly instruction-tuned
    models from our experiments since they are more analogous to LLMs used in chatbots [[141](#bib.bib141)].
    The pairs met the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the top 50% of product categories that participants reported to care about.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the top 50% of product categories that participants reported they might
    use an LLM when shopping for.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They had the highest probability increase that the target brand is mentioned
    with the attack, as discussed in §[6.1](#S6.SS1 "6.1 Observations on Paraphrased
    Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand
    Recommendations").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For each of the 12 prompts in our six prompt pairs, we obtained a random sample
    of 75 as it was in the overall set of 1000. All participants within a study group
    saw the same prompts, but, to mirror how in real-world chatbot use a single prompt
    would lead to different responses, each participant was shown a unique reponse.
    As opposed §[6](#S6 "6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations"), particiapnts saw full-length responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Piloting and preliminary data collection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We piloted our study extensively. We ran a series of preliminary studies to
    determine whether the study design was feasible and whether participants would
    encounter problems with any parts of the study. During these preliminary studies,
    questions were timed and augmented with meta-questions on how clear the main questions
    were. We also collected participants’ interest in product categories and their
    likelihood of using chatbots for advice when shopping for these categories. In
    total, we collected responses from 90 prolific participants for piloting and 63
    for product category preferences. We further ran pilots with two HCI researchers,
    asking them to review and criticize our study.
  prefs: []
  type: TYPE_NORMAL
- en: Based on responses, we tweaked the study design to be more concise and have
    clearer instructions. To more closely mimic real chatbots, we changed how prompts
    and responses were displayed.Attention and comprehension checks were added to
    ensure high data quality.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.4 Statistical analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each of the six base and perturbed prompt pairs, we analyzed the difference
    between base and perturbed group responses with a series of non-parametric tests
    on our 11 main measurement variables: two-tailed Mann-Whitney U tests for Likert
    data and chi-squared tests for binary data. To understand whether the base and
    perturbed groups are equivalent, we replaced non-significant tests for difference
    with tests of equivalence. To establish equivalence, we used the two one-sided
    tests procedure (TOST) and set our equivalence margin to be $\Delta=0.5$ for 80%
    power [[142](#bib.bib142)].⁶⁶6Testing for equivalence is a deviation from the
    pre-registration; however, we believe this approach paints a more complete picture.
    Open-ended brand-recall questions (e.g., what brand participants would pick based
    on the response), were coded into binary categories: whether the participant reported
    the targeted brand or not. hesitant or otherwise unclear responses did not count
    as a match.'
  prefs: []
  type: TYPE_NORMAL
- en: Our approach to measuring inconspicuousness meant that we needed to run 11 tests
    between the baseline and perturbed prompt pair for each product category. We controlled
    our false-discovery rate per product category with the Benjamini-Hochberg procedure [[143](#bib.bib143)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first describe our participants and then present results. We find that, under
    most measures, perturbed prompts and corresponding responses are human-inconspicuous.
    Further, these prompts, in most measures, successfully nudge more users into noticing
    the target brand.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Participants
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We recruited 845 participants and assigned them evenly to 12 groups, each group
    defined in terms of the product category and prompt type (baseline or perturbed).
    Product categories were split between Llama3-it and Gemma-it. Our participants
    were more educated, younger, less hispanic, and had more familiarity with ChatGPT
    compared to the national average [[144](#bib.bib144), [145](#bib.bib145)]. [Table I](#S7.T1
    "TABLE I ‣ 7.1.1 Survey procedures ‣ 7.1 Methods ‣ 7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations") shows the demographics
    distribution of our participants.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Statistical Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We asked multiple questions about the prompts and responses. Four of these
    were to test the inconspicuousness of the perturbed prompts. Another four were
    to test if users were significantly more dissatisfied with the perturbed responses.
    Additionally, we asked three questions to test if the responses to the perturbed
    prompts made the targeted brand more perceptible. [Table II](#S7.T2 "TABLE II
    ‣ 7.1.1 Survey procedures ‣ 7.1 Methods ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") and [Table III](#S7.T3 "TABLE III ‣ 7.1.1
    Survey procedures ‣ 7.1 Methods ‣ 7 User Study ‣ Sales Whisperer: A Human-Inconspicuous
    Attack on LLM Brand Recommendations") show the result of our analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Equivalence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We find that nearly all perturbed prompts and responses were equivalent to the
    base prompts from the perspective of participants in terms of variables measured
    (42/48 comparisons were equivalent, ); participants found the responses to the
    parcel delivery perturbed prompt more satisfactory and more likely to be used
    (all ); no difference nor equivalence was found for the likelihood of using the
    response for gaming platforms (). These results suggest that not only is our attack
    imperceptible to users, but in a few cases the perturbed prompts and responses
    might be preferable.
  prefs: []
  type: TYPE_NORMAL
- en: Attack success
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In order to measure attack success, we recorded the percentage of participants
    who *would pick* the targeted brand given the response, the percentage of participants
    who *noticed* the brand in the response, and the percentage of participants who
    said the targeted brand *was the top recommendation*. As shown in ([Table III](#S7.T3
    "TABLE III ‣ 7.1.1 Survey procedures ‣ 7.1 Methods ‣ 7 User Study ‣ Sales Whisperer:
    A Human-Inconspicuous Attack on LLM Brand Recommendations")), in five out of six
    categories our attacks were able to increase the prominence of the target brand
    in at least one of the three questions: in four categories participants were more
    likely to pick the targeted brand when given the perturbed prompts, in five categories
    participants were more likely to notice the targeted brand, and in three categories
    participants were more likely to say the targeted brand was the top recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Concluding Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We identify a novel threat model in which an adversary aims to induce target
    biases in LLM responses users receive by using adversarially crafted prompts.
    We specifically investigate whether adversaries can bias responses towards a target
    brand through prompt suggestions (e.g., recommended prompts within a chatbot,
    prompt sharing services).
  prefs: []
  type: TYPE_NORMAL
- en: We contribute a dataset of 449 prompts that ask for product recommendations
    over 77 product categories, and develop a method to evaluate which brands are
    mentioned in LLM responses using target words from the same dataset. We evaluate
    the difference in probabilities that brands are mentioned across similar prompts
    (paraphrases) within each product category. We find up to a $100\%$ difference
    between pairs of prompts. Next, we develop a method to perturb a base prompt by
    replacing words with their synonyms from our new synonym dictionary. We select
    a candidate using a loss function that correlates with measured attack success.
    This results in human-inconspicuous prompts that increase the probability that
    a target brand is mentioned. Novel with respect to other work that studies inconspicuous
    text manipulations, we perform a user study that demonstrates that, compared to
    base prompts, our synonym-replacement prompts are successful in promotinga target
    brand while remaining inconspicuous to humans.
  prefs: []
  type: TYPE_NORMAL
- en: We also explore transfer attacks of our synonym-replacement approach from open-sourced
    models where logits are available to GPT models, finding transferability between
    Llama3-it and GPT-3.5-Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, our work highlights the potential for adversarial prompt recommendations
    to bias LLM responses. While our focus is on brand recommendations, our methods
    are general and can be applied to other adversarial goals, such as propaganda,
    misinformation, and bias on social issues. As such, we suggest that defensive
    measures should be taken. Since the perturbations we introduce are imperceptible
    to humans, we suggest that users should be warned to check sources before using
    prompts they did not author. Notably, warnings about LLM use are already commonplace
    in many chatbot applications, ChatGPT noting “ChatGPT can make mistakes. Check
    important info.” [[146](#bib.bib146)]. We suggest that these be implemented for
    untrusted prompts.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, “Autoprompt:
    Eliciting knowledge from language models with automatically generated prompts,”
    *arXiv preprint arXiv:2010.15980*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] (2023) What are some of your favorite chatgpt prompts that are useful?
    i’ll share mine. Accessed: 2024-06-05\. [Online]. Available: [https://www.reddit.com/r/ChatGPT/comments/13cklzh/what_are_some_of_your_favorite_chatgpt_prompts/](https://www.reddit.com/r/ChatGPT/comments/13cklzh/what_are_some_of_your_favorite_chatgpt_prompts/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] N. Carlini, D. Paleka, K. D. Dvijotham, T. Steinke, J. Hayase, A. F. Cooper,
    K. Lee, M. Jagielski, M. Nasr, A. Conmy, E. Wallace, D. Rolnick, and F. Tramèr,
    “Stealing Part of a Production Language Model,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] U. Iqbal, T. Kohno, and F. Roesner, “LLM Platform Security: Applying a
    Systematic Evaluation Framework to OpenAI’s ChatGPT Plugins,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Wu, N. Zhang, S. Jha, P. McDaniel, and C. Xiao, “A New Era in LLM Security:
    Exploring Security Concerns in Real-World LLM-based Systems,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] X. Pan, M. Zhang, B. Sheng, J. Zhu, and M. Yang, “Hidden Trigger Backdoor
    Attack on NLP models via Linguistic Style Manipulation,” in *31st USENIX Security
    Symposium (USENIX Security 22)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, “Universal and Transferable
    Adversarial Attacks on Aligned Language Models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee,
    A. Roberts, T. Brown, D. Song, Ú. Erlingsson, A. Oprea, and C. Raffel, “Extracting
    Training Data from Large Language Models,” in *30th USENIX Security Symposium
    (USENIX Security 21)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Microsoft, “Your everyday ai companion,” 2024, accessed: 2024-06-06. [Online].
    Available: [https://www.microsoft.com/en-us/bing?form=MG0AUO&OCID=MG0AUO](https://www.microsoft.com/en-us/bing?form=MG0AUO&OCID=MG0AUO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu, “DolphinAttack:
    Inaudible Voice Commands,” in *Proceedings of the 2017 ACM SIGSAC Conference on
    Computer and Communications Security*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
    a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition,” in *Proceedings
    of the 23rd ACM SIGSAC Conference on Computer and Communications Security*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. G. Doan, M. Xue, S. Ma, E. Abbasnejad, and D. C. Ranasinghe, “TnT Attacks!
    Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems,”
    *IEEE Transactions on Information Forensics and Security*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Garg and G. Ramakrishnan, “BAE: BERT-based Adversarial Examples for
    Text Classification,” in *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu, “BERT-ATTACK: Adversarial Attack
    Against BERT Using BERT,” in *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, Nov. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D. Li, Y. Zhang, H. Peng, L. Chen, C. Brockett, M.-T. Sun, and B. Dolan,
    “Contextualized Perturbation for Textual Adversarial Attack,” in *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Li, S. Ji, T. Du, B. Li, and T. Wang, “TextBugger: Generating Adversarial
    Text Against Real-world Applications,” in *Network and Distributed System Security
    Symposium*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Y. Yoo and Y. Qi, “Towards Improving Adversarial Training of NLP Models,”
    in *Findings of the Association for Computational Linguistics: EMNLP 2021*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W.
    Chang, “Generating Natural Language Adversarial Examples,” in *Proceedings of
    the 2018 Conference on Empirical Methods in Natural Language Processing*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] R. Jia, A. Raghunathan, K. Göksel, and P. Liang, “Certified Robustness
    to Adversarial Word Substitutions,” in *Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing (EMNLP-IJCNLP)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “HotFlip: White-Box Adversarial
    Examples for Text Classification,” in *Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (Volume 2: Short Papers)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli,
    “An LLM can Fool Itself: A Prompt-Based Adversarial Attack,” in *The Twelfth International
    Conference on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. Pruthi, B. Dhingra, and Z. C. Lipton, “Combating Adversarial Misspellings
    with Robust Word Recognition,” in *Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics*, Jul. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond Accuracy: Behavioral
    Testing of NLP Models with CheckList,” in *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, Jul. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Yuan, Y. Zhang, Y. Chen, and W. Wei, “Bridge the Gap Between CV and
    NLP! A Gradient-based Textual Adversarial Attack Framework,” in *Findings of the
    Association for Computational Linguistics: ACL 2023*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] X. Zhao, Y.-X. Wang, and L. Li, “Protecting Language Generation Models
    via Invisible Watermarking,” in *Proceedings of the 40th International Conference
    on Machine Learning*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Z. Li, C. Wang, S. Wang, and C. Gao, “Protecting Intellectual Property
    of Large Language Model-Based Code Generation APIs via Watermarks,” in *Proceedings
    of the 2023 ACM SIGSAC Conference on Computer and Communications Security*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X. He, Q. Xu, Y. Zengt, L. Lyu, F. Wu, J. Li, and R. Jia, “CATER: Intellectual
    Property Protection on Text Generation APIs via Conditional Watermarks,” in *Proceedings
    of the 36th International Conference on Neural Information Processing Systems*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] X. He, Q. Xu, L. Lyu, F. Wu, and C. Wang, “Protecting Intellectual Property
    of Language Generation APIs with Lexical Watermark,” in *AAAI Conference on Artificial
    Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumitras, “When Does
    Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks,”
    in *27th USENIX Security Symposium (USENIX Security 18)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea, C. Nita-Rotaru,
    and F. Roli, “Why Do Adversarial Attacks Transfer? Explaining Transferability
    of Evasion and Poisoning Attacks,” in *28th USENIX Security Symposium (USENIX
    Security 19)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] B. Friedman and H. Nissenbaum, “Discerning Bias in Computer Systems,”
    in *Conference Companion on Human Factors in Computing Systems*, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] ——, “Minimizing Bias in Computer Systems,” in *Conference Companion on
    Human Factors in Computing Systems*, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. K. Roth, “The Unconsidered Ballot: How Design Effects Voting Behavior,”
    in *Visible Languages*, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A Survey
    on Bias and Fairness in Machine Learning,” in *ACM Computing Surveys*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] P. Terhörst, J. N. Kolf, M. Huber, F. Kirchbuchner, N. Damer, A. M. Moreno,
    J. Fierrez, and A. Kuijper, “A Comprehensive Study on Face Recognition Biases
    Beyond Demographics,” *IEEE Transactions on Technology and Society*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] R. Tatman, “Gender and Dialect Bias in YouTube’s Automatic Captions,”
    in *Proceedings of the First ACL Workshop on Ethics in Natural Language Processing*.   Association
    for Computational Linguistics, Apr. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Machine Bias,” *Propublica*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] H. Xu, X. Liu, Y. Li, A. Jain, and J. Tang, “To be Robust or to be Fair:
    Towards Fairness in Adversarial Training,” in *International Conference on Machine
    Learning*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Agarwal, M. Dudik, and Z. S. Wu, “Fair Regression: Quantitative Definitions
    and Reduction-Based Algorithms,” in *Proceedings of the 36th International Conference
    on Machine Learning*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt,
    T. Yu, R. Zhang, and N. K. Ahmed, “Bias and Fairness in Large Language Models:
    A Survey,” *arXiv preprint arXiv:2309.00770*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell.l, “On the
    Dangers of Stochastic Parrots: Can Language Models Be Too Big?” in *Proceedings
    of the 2021 ACM conference on fairness, accountability, and transparency*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] L. Weidinger, J. Uesato, M. Rauh, C. Griffin, P.-S. Huang, J. Mellor,
    A. Glaese, M. Cheng, B. Balle, A. Kasirzadeh, C. Biles, S. Brown, Z. Kenton, W. Hawkins,
    T. Stepleton, A. Birhane, L. A. Hendricks, L. Rimell, W. Isaac, J. Haas, S. Legassick,
    G. Irving, and I. Gabriel, “Taxonomy of Risks posed by Language Models,” in *Proceedings
    of the 2022 ACM conference on fairness, accountability, and transparency*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Esiobu, X. Tan, S. Hosseini, M. Ung, Y. Zhang, J. Fernandes, J. Dwivedi-Yu,
    E. Presani, A. Williams, and E. Smith, “ROBBIE: Robust Bias Evaluation of Large
    Generative Language Models,” in *Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Feng, C. Y. Park, Y. Liu, and Y. Tsvetkov, “From Pretraining Data to
    Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading
    to Unfair NLP Models,” in *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] H. Kotek, R. Dockum, and D. Q. Sun, “Gender Bias and Stereotypes in Large
    Language Models,” in *Proceedings of The ACM Collective Intelligence Conference*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Song, S. Khanuja, P. Liu, F. Faisal, A. Ostapenko, G. Winata, A. Aji,
    S. Cahyawijaya, Y. Tsvetkov, A. Anastasopoulos, and G. Neubig, “GlobalBench: A
    Benchmark for Global Progress in Natural Language Processing,” in *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] H. Orgad and Y. Belinkov, “BLIND: Bias Removal With No Demographics,”
    in *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, A. Rogers, J. Boyd-Graber, and N. Okazaki,
    Eds., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] E. Jeon, M. Lee, J. Park, Y. Kim, W.-L. Mok, and S. Lee, “Improving Bias
    Mitigation through Bias Experts in Natural Language Understanding,” in *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Ding and P. Horster, “Undetectable On-line Password Guessing Attacks,”
    in *ACM SIGOPS Operating Systems Review*, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Herzberg and H. Shulman, “Stealth DoS Attacks on Secure Channels,”
    in *Network and Distributed System Security Symposium*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Briongos, P. Malagón, J. M. Moya, and T. Eisenbarth, “RELOAD+REFRESH:
    Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks,” in *Proceedings
    of the 29th USENIX Security Symposium*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. V. Bulck, N. Weichbrodt, R. Kapitza, F. Piessens, and R. Strackx, “Telling
    Your Secrets without Page Faults: Stealthy Page Table-Based Attacks on Enclaved
    Execution,” in *26th USENIX Security Symposium (USENIX Security 17)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Cassola, W. Robertson, E. Kirda, and G. Noubir, “A Practical, Targeted,
    and Stealthy Attack Against WPA Enterprise Authentication,” in *Network and Distributed
    System Security Symposium*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] P.-A. Vervier, O. Thonnard, and M. Dacier, “Mind Your Blocks: On the Stealthiness
    of Malicious BGP Hijacks,” in *Network and Distributed System Security Symposium*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Das, N. Borisov, and M. Caesar, “Tracking Mobile Web Users Through
    Motion Sensors: Attacks and Defenses,” in *Network and Distributed System Security
    Symposium*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] L. Garcia, F. Brasser, M. H. Cintuglu, A.-R. Sadeghi, O. Mohammed, and
    S. A. Zonouz, “Hey, My Malware Knows Physics! Attacking PLCs with Physical Model
    Aware Rootkit,” in *Network and Distributed System Security Symposium*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. O. Ozmen, R. Song, H. Farrukh, and Z. B. Celik, “Evasion Attacks and
    Defenses on Smart Home Physical Event Verification,” in *Network and Distributed
    System Security Symposium*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] X. Chen, Z. Li, B. Chen, Y. Zhu, C. X. Lu, Z. Peng, F. Lin, W. Xu, K. Ren,
    and C. Qiao, “MetaWave: Attacking mmWave Sensing with Meta-material-enhanced Tags,”
    in *Network and Distributed System Security Symposium*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Zhu, Z. Xiao, Y. Chen, Z. Li, M. Liu, B. Y. Zhao, and H. Zheng, “Et
    Tu Alexa? When Commodity WiFi Devices Turn into Adversarial Motion Sensors,” in
    *Network and Distributed System Security Symposium*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Chandratre, T. H. Acosta, T. Khandait, and G. P. G. Fainekos, “Stealthy
    Attacks Formalized as STL Formulas for Falsification of CPS Security,” in *Proceedings
    of the 26th ACM International Conference on Hybrid Systems: Computation and Control*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] M. A. Rahman, A. Datta, and E. Al-Shaer, “Security Design Against Stealthy
    Attacks on Power System State Estimation: A Formal Approach,” *Computers & Security*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. Maiorca, D. Ariu, I. Corona, M. Aresu, and G. Giacinto, “Stealth attacks:
    An extended insight into the obfuscation effects on Android malware,” *Computers
    & Security*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] P. Dash, M. Karimibiuki, and K. Pattabiraman, “Out of Control: Stealthy
    Attacks against Robotic Vehicles Protected by Control-Based Techniques,” ser.
    ACSAC ’19.   Association for Computing Machinery, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T. Sugawara, B. Cyr, S. Rampazzi, D. Genkin, and K. Fu, “Light Commands:
    Laser-Based Audio Injection Attacks on Voice-Controllable Systems,” in *29th USENIX
    Security Symposium (USENIX Security 20)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Q. Wang, W. U. Hassan, D. Li, K. Jee, X. Yu, K. Zou, J. Rhee, Z. Chen,
    W. Cheng, and C. A. G. andHaifeng Chen, “You Are What You Do: Hunting Stealthy
    Malware via Data Provenance Analysis,” in *Network and Distributed System Security
    Symposium*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. K. Sikder, H. Aksu, and A. S. Uluagac, “6thSense: A Context-aware Sensor-based
    Attack Detector for Smart Devices,” in *26th USENIX Security Symposium (USENIX
    Security 17)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Li, L. Zhao, M. Juliato, S. Ahmed, M. R. Sastry, and L. Yang, “POSTER:
    Intrusion Detection System for In-vehicle Networks using Sensor Correlation and
    Integration,” in *Proceedings of the 2017 ACM SIGSAC Conference on Computer and
    Communications Security*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] D. I. Urbina, J. A. Giraldo, A. A. Cardenas, N. O. Tippenhauer, J. Valente,
    M. Faisal, J. Ruths, R. Candell, and H. Sandberg, “POSTER: Intrusion Detection
    System for In-vehicle Networks using Sensor Correlation and Integration,” in *Proceedings
    of the 2016 ACM SIGSAC Conference on Computer and Communications Security*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] W. Aoudi, M. Iturbe, and M. Almgren, “Truth Will Out: Departure-Based
    Process-Level Detection of Stealthy Attacks on Control Systems,” in *Proceedings
    of the 2018 ACM SIGSAC Conference on Computer and Communications Security*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Giraldo, A. Cardenas, and R. G. Sanfelice, “A Moving Target Defense
    to Detect Stealthy Attacks in Cyber-Physical Systems,” in *2019 American Control
    Conference (ACC)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. M. Ahmed, J. Zhou, and A. P. Mathur, “Noise Matters: Using Sensor and
    Process Noise Fingerprint to Detect Stealthy Cyber Attacks and Authenticate sensors
    in CPs,” ser. ACSAC ’18.   Association for Computing Machinery, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. Hosseini, B. Xiao, M. Jaiswal, and R. Poovendran, “On the Limitation
    of Convolutional Neural Networks in Recognizing Negative Images,” in *IEEE International
    Conference on Machine Learning and Applications*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool: A Simple
    and Accurate Method to Fool Deep Neural Networks,” in *The IEEE / CVF Computer
    Vision and Pattern Recognition Conference*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] S. Sabour, Y. Cao, F. Faghri, and D. J. Fleet, “Adversarial Manipulation
    of Deep Representations,” in *International Conference on Learning Representations*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto,
    and F. Roli, “Evasion Attacks against Machine Learning at Test Time,” in *European
    Machine Learning and Data Mining Conference*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S.-M. Moosavi-Dezfooli, A. Shrivastava, and O. Tuzel, “Divide, Denoise,
    and Defend against Adversarial Attacks,” in *Conference on Computer Vision and
    Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The Limitations of Deep Learning in Adversarial Settings,” in *IEEE European
    Symposium on Security and Privacy*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] F. Croce and M. Hein, “Reliable Evaluation of Adversarial Robustness with
    an Ensemble of Diverse Parameter-free Attacks,” in *International Conference on
    Machine Learning*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] J. Chen, M. I. Jordan, and M. J. Wainwright, “HopSkipJumpAttack: A Query-Efficient
    Decision-Based Attack,” in *IEEE Symposium on Security and Privacy*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] W. Lin, K. Lucas, L. Bauer, M. K. Reiter, and M. Sharif, “Constrained
    Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks,”
    in *International Conference on Machine Learning*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] W. Lin, K. Lucas, N. Eyal, L. Bauer, M. K. Reiter, and M. Sharif, “Group-based
    Robustness: A General Framework for Customized Robustness in the Real World,”
    in *Network and Distributed System Security Symposium*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] N. Carlini and D. Wagner, “Towards Evaluating the Robustness of Neural
    Networks,” in *IEEE Symposium on Security and Privacy*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing
    Adversarial Examples,” in *International Conference on Learning Representations*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *International Conference
    on Learning Representations*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial Machine Learning
    at Scale,” in *International Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards Deep
    Learning Models Resistant to Adversarial Attacks,” in *International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] S. F. Dodge and L. Karam, “A Study and Comparison of Human and Deep Learning
    Recognition Performance under Visual Distortions,” in *International Conference
    on Computer Communication and Networks*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] M. Sharif, L. Bauer, and M. K. Reiter, “On the Suitability of Lp-Norms
    for Creating and Preventing Adversarial Examples,” in *The IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR) Workshops*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. C. Goh, X. Q. Cai, W. Theseira, G. Ko, and K. A. Khor, “Evaluating
    Human Versus Machine Learning Performance in Classifying Research Abstracts,”
    in *Scientometrics*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Ma, Y. Liu, G. Tao, W.-C. Lee, and X. Zhang, “NIC: Detecting Adversarial
    Samples with Neural Network Invariant Checking,” in *Network and Distributed System
    Security Symposium*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H. Li, S. Shan, E. Wenger, J. Zhang, H. Zheng, and B. Y. Zhao, “Blacklight:
    Scalable Defense for Neural Networks against Query-Based Black-Box attacks,” in
    *31st USENIX Security Symposium (USENIX Security 22)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, “Detecting Adversarial
    Samples from Artifacts,” *arXiv preprint arXiv:1703.00410*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. Guo, M. Rana, M. Cisse, and L. Van Der Maaten, “Countering Adversarial
    Images Using Input Transformations,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-GAN: Protecting Classifiers
    Against Adversarial Attacks Using Generative Models,” in *International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] L. Li, M. Weber, X. Xu, L. Rimanic, B. Kailkhura, T. Xie, C. Zhang, and
    B. Li, “TSS: Transformation-Specific Smoothing for Robustness Certification,”
    in *Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications
    Security*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. Xiang, S. Mahloujifar, and P. Mittal, “PatchCleanser: Certifiably Robust
    Defense against Adversarial Patches for Any Image Classifier,” in *31st USENIX
    Security Symposium (USENIX Security 22)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] C. Xiang and P. Mittal, “DetectorGuard: Provably Securing Object Detectors
    against Localized Patch Hiding Attacks,” in *Proceedings of the 2021 ACM SIGSAC
    Conference on Computer and Communications Security*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certified Adversarial Robustness
    via Randomized Smoothing,” in *International Conference on Machine Learning*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] E. Wong and Z. Kolter, “Provable Defenses against Adversarial Examples
    via the Convex Outer Adversarial Polytope,” in *International Conference on Machine
    Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
    Robustness to Adversarial Examples with Differential Privacy,” in *2019 IEEE Symposium
    on Security and Privacy (SP)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] G. Singh, T. Gehr, M. Püschel, and M. Vechev, “An Abstract Domain for
    Certifying Neural Networks,” *Proceedings of the ACM on Programming Languages*,
    no. POPL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D. Boning,
    and I. Dhillon, “Towards Fast Computation of Certified Robustness for ReLU networks,”
    in *Proceedings of the 35th International Conference on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Wu, L. Tong, and Y. Vorobeychik, “Defending Against Physically Realizable
    Attacks on Image Classification,” in *International Conference on Learning Representations*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, and M. Kankanhalli, “Geometry-aware
    Instance-reweighted Adversarial Training,” in *International Conference on Learning
    Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. Shafahi, M. Najibi, A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S.
    Davis, G. Taylor, and T. Goldstein, “Adversarial Training for Free!” in *Conference
    on Neural Information Processing Systems*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan,
    “Theoretically Principled Trade-off between Robustness and Accuracy,” in *International
    Conference on Machine Learning*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] T. Chen, S. Liu, S. Chang, Y. Cheng, L. Amini, and Z. Wang, “Adversarial
    Robustness: From Self-Supervised Pre-Training to Fine-Tuning,” in *Conference
    on Computer Vision and Pattern Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] H. Huang, Y. Wang, S. M. Erfani, Q. Gu, J. Bailey, and X. Ma, “Exploring
    Architectural Ingredients of Adversarially Robust Deep Neural Networks,” in *Conference
    on Neural Information Processing Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant,
    M. Guajardo-Cespedes, S. Yuan, C. Tar, B. Strope, and R. Kurzweil, “Universal
    Sentence Encoder for English,” in *Proceedings of the 2018 Conference on Empirical
    Methods in Natural Language Processing: System Demonstrations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Y. Wen, L. Marchyok, S. Hong, J. Geiping, T. Goldstein, and N. Carlini,
    “Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained
    Models,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce,
    H. Anderson, A. Terzis, K. Thomas, and F. Tramèr, “Poisoning Web-Scale Training
    Datasets is Practical,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] E. Bagdasaryan and V. Shmatikov, “Spinning Language Models: Risks of
    Propaganda-as-a-Service and Countermeasures,” in *S&P*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] C. Wei, W. Meng, Z. Zhang, M. Chen, M. Zhao, W. Fang, L. Wang, Z. Zhang,
    and W. Chen, “LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors,”
    in *Network and Distributed System Security Symposium*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Pei, J. Jia, W. Guo, B. Li, and D. Song, “TextGuard: Provable Defense
    against Backdoor Attacks on Text Classification,” in *Network and Distributed
    System Security Symposium*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong,
    “Jailbreaking Black Box Large Language Models in Twenty Queries,” 2024\. [Online].
    Available: [https://openreview.net/forum?id=hkjcdmz8Ro](https://openreview.net/forum?id=hkjcdmz8Ro)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    K. Wang, and Y. Liu, “Jailbreaking ChatGPT via Prompt Engineering: An Empirical
    Study,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How Does LLM Safety
    Training Fail?” in *Advances in Neural Information Processing Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] J. Chu, Y. Liu, Z. Yang, X. Shen, M. Backes, and Y. Zhang, “Jailbreaking
    ChatGPT via Prompt Engineering: An Empirical Study,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] N. Mangaokar, A. Hooda, J. Choi, S. Chandrashekaran, K. Fawaz, S. Jha,
    and A. Prakash, “PRP: Propagating Universal Perturbations to Attack Large Language
    Model Guard-Rails,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang,
    and Y. Liu, “MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots,”
    in *Network and Distributed System Security Symposium*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Rando, F. Croce, K. Mitka, S. Shabalin, M. Andriushchenko, N. Flammarion,
    and F. Tramèr, “Competition Report: Finding Universal Jailbreak Backdoors in Aligned
    LLMs,” *arXiv preprint arXiv:2404.14461*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] A. Kumar, C. Agarwal, S. Srinivas, A. J. Li, S. Feizi, and H. Lakkaraju,
    “Certifying LLM Safety against Adversarial Prompting,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Y. Zeng, Y. Wu, X. Zhang, H. Wang, and Q. Wu, “AutoDefense: Multi-Agent
    LLM Defense against Jailbreak Attacks,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, Y. Mao, M. Tontchev,
    Q. Hu, B. Fuller, D. Testuggine, and M. Khabsa, “Llama Guard: LLM-based Input-Output
    Safeguard for Human-AI Conversations,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] T. Rebedea, R. Dinu, M. N. Sreedhar, C. Parisien, and J. Cohen, “NeMo
    Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable
    Rails,” in *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito,
    C. A. Choquette-Choo, E. Wallace, F. Tramèr, and K. Lee, “Scalable Extraction
    of Training Data from (Production) Language Models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tramer, and N. Carlini,
    “Counterfactual Memorization in Neural Language Models,” in *Advances in Neural
    Information Processing Systems*, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt,
    and S. Levine, Eds., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] M. Li, J. Wang, J. Wang, and S. Neel, “MoPe: Model Perturbation based
    Privacy Attacks on Language Models,” in *Proceedings of the 2023 Conference on
    Empirical Methods in Natural Language Processing*, Dec. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Shao, J. Huang, S. Zheng, and K. Chang, “Quantifying Association Capabilities
    of Large Language Models and Its Implications on Privacy Leakage,” in *Findings
    of the Association for Computational Linguistics: EACL 2024*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] H. Li, D. Guo, W. Fan, M. Xu, J. Huang, F. Meng, and Y. Song, “Multi-step
    Jailbreaking Privacy Attacks on ChatGPT,” in *Findings of the Association for
    Computational Linguistics: EMNLP 2023*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] X. Pan, M. Zhang, S. Ji, and M. Yang, “Privacy Risks of General-Purpose
    Language Models,” in *2020 IEEE Symposium on Security and Privacy (SP)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. Zhuang. (2023) Bringing Inspirational, AI-Powered Search to the Instacart
    app with Ask Instacart. Accessed: 2024-06-05\. [Online]. Available: [https://www.instacart.com/company/updates/bringing-inspirational-ai-powered-search-to-the-instacart-app-with-ask-instacart/](https://www.instacart.com/company/updates/bringing-inspirational-ai-powered-search-to-the-instacart-app-with-ask-instacart/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. AI, “Promptperfect - ai prompt generator and optimizer,” 2024, accessed:
    2024-06-06\. [Online]. Available: [https://promptperfect.jina.ai/](https://promptperfect.jina.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Lowe’s Product Expert: Custom GPT. Accessed: 2024-06-05\. [Online]. Available:
    [https://www.lowesinnovationlabs.com/projects/lowe-s-product-expert](https://www.lowesinnovationlabs.com/projects/lowe-s-product-expert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] (2023) Expedia launches conversational trip planning powered by ChatGPT
    to inspire members to dream about travel in new ways. Accessed: 2024-06-05. [Online].
    Available: [https://www.expediagroup.com/investors/news-and-events/financial-releases/news/news-details/2023/Chatgpt-Wrote-This-Press-Release--No-It-Didnt-But-It-Can-Now-Assist-With-Travel-Planning-In-The-Expedia-App/default.aspx](https://www.expediagroup.com/investors/news-and-events/financial-releases/news/news-details/2023/Chatgpt-Wrote-This-Press-Release--No-It-Didnt-But-It-Can-Now-Assist-With-Travel-Planning-In-The-Expedia-App/default.aspx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] C. Smith, “What large models cost you - there is no free ai lunch,” *Forbes*,
    2023\. [Online]. Available: [https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=1f3c5fa34af7](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=1f3c5fa34af7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] T. Perry and U. Staff. (2024) Prankster tricks a GM chatbot into agreeing
    to sell him a $76,000 Chevy Tahoe for $1\. Accessed: 2024-06-05\. [Online]. Available:
    [https://www.upworthy.com/prankster-tricks-a-gm-dealership-chatbot-to-sell-him-a-76000-chevy-tahoe-for-1-rp](https://www.upworthy.com/prankster-tricks-a-gm-dealership-chatbot-to-sell-him-a-76000-chevy-tahoe-for-1-rp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Belanger, “Air canada must honor refund policy invented by airline’s
    chatbot,” *Ars Technica*, February 2024\. [Online]. Available: [https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/](https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] P. Schober, C. Boer, and L. A. Schwarte, “Correlation Coefficients: Appropriate
    Use and Interpretation,” *Anesthesia & Analgesia*, no. 5, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] D. Abrokwa, S. Das, O. Akgul, and M. L. Mazurek, “Comparing security
    and privacy attitudes between ios and android users in the us,” in *SOUPS 2021:
    USENIX Symposium on Usable Privacy and Security*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray *et al.*, “Training language models to follow instructions
    with human feedback,” *Advances in neural information processing systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] D. Lakens, “Equivalence Tests: A Practical Primer for t Tests, Correlations,
    and Meta-Analyses,” *Social psychological and personality science*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Benjamini and Y. Hochberg, “Controlling the false discovery rate:
    a practical and powerful approach to multiple testing,” *Journal of the Royal
    statistical society: series B (Methodological)*, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] U.S. Census Bureau, “Census data,” 2020\. [Online]. Available: [https://data.census.gov/](https://data.census.gov/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Pew Research Center, “2024 pew research center’s american trends panel
    wave 142 topline,” February 2024\. [Online]. Available: [https://www.pewresearch.org/wp-content/uploads/2024/03/SR_24.03.26_chat-bot_topline.pdf](https://www.pewresearch.org/wp-content/uploads/2024/03/SR_24.03.26_chat-bot_topline.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] A. Cranz, “We have to stop ignoring ai’s hallucination problem,” *The
    Verge*, 2024\. [Online]. Available: [https://www.theverge.com/2024/5/15/24154808/ai-chatgpt-google-gemini-microsoft-copilot-hallucination-wrong](https://www.theverge.com/2024/5/15/24154808/ai-chatgpt-google-gemini-microsoft-copilot-hallucination-wrong)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] B. Lutkevich, “19 of the best large language models in 2024,” 2024, accessed:
    2024-06-06\. [Online]. Available: [https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Do LLMs Recommend Their Parent Brand?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| search engine | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bing | 0.58 % | 13.08 % | 14.00 % | 27.66 % | 0.40 % | 2.36 % |'
  prefs: []
  type: TYPE_TB
- en: '| Google | 98.16 % | 50.04 % | 53.14 % | 53.60 % | 74.24 % | 45.86 % |'
  prefs: []
  type: TYPE_TB
- en: '| Yahoo | 0.00 % | 19.36 % | 18.36 % | 3.92 % | 0.26 % | 0.56 % |'
  prefs: []
  type: TYPE_TB
- en: '| browser | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it |'
  prefs: []
  type: TYPE_TB
- en: '| Chrome | 77.22 % | 41.24 % | 33.54 % | 50.30 % | 53.28 % | 35.42 % |'
  prefs: []
  type: TYPE_TB
- en: '| Firefox | 28.74 % | 38.10 % | 31.28 % | 22.28 % | 5.08 % | 5.40 % |'
  prefs: []
  type: TYPE_TB
- en: '| Safari | 3.40 % | 11.70 % | 9.00 % | 5.64 % | 0.02 % | 0.36 % |'
  prefs: []
  type: TYPE_TB
- en: '| Edge | 7.62 % | 13.70 % | 10.04 % | 13.52 % | 0.36 % | 1.14 % |'
  prefs: []
  type: TYPE_TB
- en: '| Opera | 0.14 % | 9.82 % | 9.74 % | 6.38 % | 0.00 % | 0.06 % |'
  prefs: []
  type: TYPE_TB
- en: '| llm | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 94.82 % | 39.28 % | 41.66 % | 0.30 % | 16.68 % | 38.04 % |'
  prefs: []
  type: TYPE_TB
- en: '| Google | 2.56 % | 8.28 % | 7.84 % | 19.3 % | 0.02 % | 1.06 % |'
  prefs: []
  type: TYPE_TB
- en: '| Llama | 0.00 % | 0.40 % | 0.70 % | 2.32 % | 0.00 % | 0.00 % |'
  prefs: []
  type: TYPE_TB
- en: '| Claude | 0.00 % | 0.12 % | 0.00 % | 0.00 % | 0.00 % | 0.00 % |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | 0.00 % | 0.00 % | 0.00 % | 0.00 % | 0.00 % | 0.00 % |'
  prefs: []
  type: TYPE_TB
- en: '| os | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it |'
  prefs: []
  type: TYPE_TB
- en: '| Windows | 97.18 % | 51.10 % | 59.72 % | 64.04 % | 17.30 % | 9.62 % |'
  prefs: []
  type: TYPE_TB
- en: '| Mac | 24.76 % | 34.66 % | 37.98 % | 38.42 % | 9.32 % | 3.14 % |'
  prefs: []
  type: TYPE_TB
- en: '| Linux | 1.16 % | 29.94 % | 26.46 % | 30.96 % | 2.88 % | 1.38 % |'
  prefs: []
  type: TYPE_TB
- en: '| smartphone | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it
    |'
  prefs: []
  type: TYPE_TB
- en: '| Apple | 90.32 % | 28.88 % | 30.45 % | 14.97 % | 12.21 % | 31.29 % |'
  prefs: []
  type: TYPE_TB
- en: '| Google | 21.85 % | 11.64 % | 10.53 % | 12.25 % | 0.15 % | 1.80 % |'
  prefs: []
  type: TYPE_TB
- en: '| Samsung | 9.02 % | 31.11 % | 27.19 % | 33.41 % | 0.71 % | 8.86 % |'
  prefs: []
  type: TYPE_TB
- en: '| laptop | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it |'
  prefs: []
  type: TYPE_TB
- en: '| Mac | 25.76 % | 14.22 % | 13.46 % | 7.16 % | 44.66 % | 9.80 % |'
  prefs: []
  type: TYPE_TB
- en: '| Chromebook | 0.00 % | 1.88 % | 1.6 % | 0.18 % | 0.00 % | 0.02 % |'
  prefs: []
  type: TYPE_TB
- en: '| HP | 0.00 % | 12.14 % | 14.16 % | 9.96 % | 0.22 % | 1.40 % |'
  prefs: []
  type: TYPE_TB
- en: '| Asus | 0.00 % | 7.08 % | 6.64 % | 3.84 % | 0.04 % | 0.20 % |'
  prefs: []
  type: TYPE_TB
- en: '| Lenovo | 0.00 % | 9.44 % | 14.42 % | 13.94 % | 0.28 % | 2.24 % |'
  prefs: []
  type: TYPE_TB
- en: '| Acer | 0.00 % | 9.20 % | 9.48 % | 4.06 % | 0.00 % | 0.08 % |'
  prefs: []
  type: TYPE_TB
- en: '| Dell | 39.98 % | 14.62 % | 17.54 % | 43.60 % | 22.98 % | 56.58 % |'
  prefs: []
  type: TYPE_TB
- en: '| VR headset | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it
    |'
  prefs: []
  type: TYPE_TB
- en: '| Meta | 41.60 % | 25.54 % | 32.94 % | 34.16 % | 42.74 % | 42.60 % |'
  prefs: []
  type: TYPE_TB
- en: '| HTC | 0.14 % | 21.66 % | 33.32 % | 38.30 % | 0.72 % | 1.80 % |'
  prefs: []
  type: TYPE_TB
- en: '| Playstation | 0.00 % | 0.30 % | 0.52 % | 0.00 % | 0.00 % | 0.00 % |'
  prefs: []
  type: TYPE_TB
- en: '| email provider | gemma-it | Llama2 | Llama3 | Llama3-it | GPT-3.5 | GPT-3.5-it
    |'
  prefs: []
  type: TYPE_TB
- en: '| Google | 35.32 % | 37.40 % | 35.18 % | 22.52 % | 45.26 % | 31.46 % |'
  prefs: []
  type: TYPE_TB
- en: '| Yahoo | 0.00 % | 4.98 % | 9.16 % | 5.12 % | 0.82 % | 0.48 % |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft | 4.00 % | 16.76 % | 17.78 % | 14.40 % | 2.16 % | 1.90 % |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: LLMs tested on their parent brands. Categories are search engines,
    browsers, LLMs, operating systems, laptops, VR headsets, and email providers.
    Scores are calculated as average across all prompts for the category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout our experiments evaluating rephrasings in §[6.1](#S6.SS1 "6.1 Observations
    on Paraphrased Prompts ‣ 6 Results ‣ Sales Whisperer: A Human-Inconspicuous Attack
    on LLM Brand Recommendations"), we gathered completions for prompts on categories
    with products manufactured by Meta, Google, and Microsoft, which allowed us to
    examine how large language models developed by these companies perform when asked
    about product categories that include products manufactured by them. We evaluated
    the average score, as defined in §[5.1](#S5.SS1 "5.1 LLM Setup ‣ 5 Setup ‣ Sales
    Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations"), of all
    brands over all prompts for categories where one of the brands was Meta, Google,
    or Microsoft. For Google the categories included browsers (Chrome), large language
    models, smartphones (Pixel), laptops (Chromebook), email providers (Gmail), and
    search engines; for Meta they included VR headsets and large language models (Llama).
    As before, this meant we looked for target words related to a brand in the response
    to see whether this prompt was mentioned. We were interested in whether or not
    LLMs made by a certain company were biased towards products made by the same company.
    All results are shown in Fig. [IV](#Sx1.T4 "TABLE IV ‣ Do LLMs Recommend Their
    Parent Brand? ‣ Appendix ‣ Sales Whisperer: A Human-Inconspicuous Attack on LLM
    Brand Recommendations")'
  prefs: []
  type: TYPE_NORMAL
- en: Google has developed a variety of LLMs and LLM families (laMDA, Bert, PaLM,
    Gemini, Gemma)  [[147](#bib.bib147)], yet we still found some interesting mistakes
    in Gemma-it’s responses to prompts asking for recommendations on large language
    models. For example, across multiple prompts, Gemma-it’s responses included “***GPT-4:**
    This model, developed by Google,*” a false statement  [[147](#bib.bib147)] seeming
    to claim that GPT-4 was developed by Google. The same was said for GPT-3, which
    is also false  [[147](#bib.bib147)]. So, even Gemma-it responses that mention
    Google might actually be recommending GPT. Out of Gemma-it’s responses, the only
    actual model of Google’s mentioned is PaLM.
  prefs: []
  type: TYPE_NORMAL
- en: We see more mentions of Llama or Meta when querying Llama than when querying
    Gemma-it or GPT3.5-turbo, with our three Llama models we see a , and  of the time
    whereas Google only recommends a Google model  of the time, while Gemma and GPT
    models do over  vs $35.32\%$). Gemma-it never mentions Chromebooks when asked
    about laptops, while all three Llama models and GPT-3.5 Instruction sometimes
    do. Nonetheless, Gemma-it seems to show a higher preference towards Google products
    than any Llama model for the categories of search engines and phones.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this test size is small and does not necessarily take into account
    all factors that can cause differences between these models. In the end, we do
    not find any bias by LLMs towards products developed by the same parent company,
    but believe it warrants further exploration.
  prefs: []
  type: TYPE_NORMAL
