- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Leeching: An Extraction Attack Targeting LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.10544](https://ar5iv.labs.arxiv.org/html/2309.10544)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lewis Birch
  prefs: []
  type: TYPE_NORMAL
- en: Lancaster University    William Hackett
  prefs: []
  type: TYPE_NORMAL
- en: Lancaster University    Stefan Trawicki
  prefs: []
  type: TYPE_NORMAL
- en: Lancaster University    Neeraj Suri
  prefs: []
  type: TYPE_NORMAL
- en: Lancaster University    Peter Garraghan
  prefs: []
  type: TYPE_NORMAL
- en: Lancaster University, Mindgard
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Model Leeching* is a novel extraction attack targeting Large Language Models
    (LLMs), capable of distilling task-specific knowledge from a target LLM into a
    reduced parameter model. We demonstrate the effectiveness of our attack by extracting
    task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity,
    and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50
    in API cost. We further demonstrate the feasibility of adversarial attack transferability
    from an extracted model extracted via *Model Leeching* to perform ML attack staging
    against a target LLM, resulting in an 11% increase to attack success rate when
    applied to ChatGPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have seen rapid adoption given their proficiency
    in handling complex natural language processing (NLP) tasks. LLMs leverage Deep
    Learning (DL) algorithms to process and understand a variety of natural language
    tasks spanning text completion, Question & Answering, and summarization [[24](#bib.bib24)].
    While production LLMs such as ChatGPT, BARD, and LLaMA [[18](#bib.bib18)] [[1](#bib.bib1)]
    [[8](#bib.bib8)] have garnered substantial attention, their uptake has also highlighted
    pressing concerns on growing their exposure to adversarial attacks [[8](#bib.bib8)].
    Studies on adversarial attacks against LLMs are limited, with urgent need to investigate
    their risk to data leakage, model stealing (extraction), and attack transferability
    across models[[3](#bib.bib3)][[31](#bib.bib31)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper we propose Model Leeching, an extraction attack against LLMs
    capable of creating an extracted model via distilling task knowledge from a target
    LLM. Our attack is performed by designing an automated prompt generation system
    [[12](#bib.bib12)] targeting specific tasks within LLMs. The prompt system is
    used to create an extracted model by extracting and copying task-specific data
    characteristics from a target model [[28](#bib.bib28)]. *Model Leeching* attack
    is applicable to any LLM with a public API endpoint, and can be successfully achieved
    at minimal economic cost. Moreover, we demonstrate how Model Leeching can be exploited
    to perform ML attack staging onto other LLMs (including the original target LLM).
    Our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose the *Model Leeching* attack method, and demonstrate its effectiveness
    against LLMs via experimentation using an extraction attack framework [[9](#bib.bib9)].
    Targeting the ChatGPT-3.5-Turbo model, we distil characteristics upon a question
    & answering (QA) dataset (SQuAD) into a Roberta-Large base model. Our findings
    demonstrate that a large QA dataset can be successfully labelled and leveraged
    to create an extracted model with 73% EM similarity to ChatGPT-3.5-Turbo, and
    achieve SQuAD EM and F1 accuracy scores of 75% and 87%, respectively at $50 cost.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We study the capability to exploit an extracted model derived from *Model Leeching*
    to perform further ML attack staging upon a production LLM. Our results show that
    a language attack [[11](#bib.bib11)] optimized for an extracted model can be successfully
    transferred into ChatGPT-3.5-Turbo with an 11% attack success increase. Our results
    highlight evidence of adversarial attack transferability between user-created
    models and production LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Attack Description & Threat Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Extraction Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model extraction is the process of extracting the fundamental characteristics
    of a DL model [[25](#bib.bib25)]. An extracted model is created via extracting
    specific characteristics (architecture, parameters, and hyper-parameters [[10](#bib.bib10)])
    from a target model of interest, which are then used to perform model recreation
    [[16](#bib.bib16)]. Once the attacker has established an extracted model, further
    adversarial attacks can be staged encompassing model inversion, membership inference,
    leaking privacy data, and model intellectual property theft [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Threat Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: State-of-the-art LLMs leveraging the transformer architecture [[26](#bib.bib26)]
    typically comprise hundreds of billions of parameters [[30](#bib.bib30)]. Using
    the established taxonomy of adversaries against DL models [[20](#bib.bib20)],
    our proposed attacks assume a weak adversary capable of providing model input
    via an LLM API endpoint, and a model output requiring generated text from a target
    LLM. The adversary has no knowledge of the target architecture or training data
    used to construct the underlying LLM parameters. Note that the threat model assumptions
    pertaining to potential rate limiting, or limited access to the target API can
    be relaxed due the ability to distribute data generation across multiple API keys.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Model Leeching Attack Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Model Leeching* is a black-box adversarial attack which seeks to create an
    extracted copy of the target LLM within a specific task. The attack comprises
    a four-phases approach as shown in Figure [1](#S3.F1 "Figure 1 ‣ 3 Model Leeching
    Attack Design ‣ Model Leeching: An Extraction Attack Targeting LLMs"): (1) Prompt
    design for crafting prompts to attain task-specific LLM responses; (2) data generation
    to derive extracting model characteristics; (3) extracted model training for model
    recreation; and (4) ML attack staging against a target LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fd69ca8919c57d87aa09c2e5caaab47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of Model Leech. Deep Learning models comprising of architecture,
    parameters and hyper-parameters can be extracted via extraction attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prompt Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Performing *Model Leeching* successfully requires correct prompt design. Adversaries
    must design well-structured prompts that accurately define the relevancy and depth
    of the necessary generated responses in order to identify task-specific knowledge
    of interest. Depending on the use case, prompt design is achieved manually or
    through automated methods [[28](#bib.bib28)]. Model Leeching leverages the following
    three-stage prompt design process:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Knowledge Discovery. An adversary first defines the type of task knowledge to
    extract. Once defined, an adversary assesses specific target LLM prompt responses
    to ascertain its affinity to generate task knowledge. This assessment encompasses
    domain (NLP, image, audio, etc.), response patterns, comprehension limitations,
    and instruction adherence for particular knowledge domains [[7](#bib.bib7), [15](#bib.bib15),
    [29](#bib.bib29)]. Following successful completion of this assessment, the adversary
    is able to devise an effective strategy to extract desired characteristics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construction. Subsequently, the adversary crafts a prompt template that integrates
    an instruction set reflecting the strategy formulated during the knowledge discovery
    stage. Template design encompasses distinctive response structure of the target
    LLM, its recognized limitations, and task-specific knowledge identified for extraction.
    This template facilitates dynamic prompt generation within the Model Leeching
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validation. The adversary validates the created prompt and response generated
    from the target LLM. Validation entails ensuring the LLM responds reliably to
    prompts, represented as a consistent response structure and ability to carry out
    given instructions. Ensuring that the target LLM is capable enough to carry out
    the required task, that it can process and action upon its given instructions.
    This validation activity enables the Model Leeching method to generate responses
    that can be used to effectively train local models with extracted task-specific
    knowledge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The prompt design process follows an iterative approach, typically requiring
    multiple variations and refinements to devise the most effective instructions
    and styles for obtaining desired results from a specific LLM for a given task
    [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Data Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once a suitable prompt has been designed, the adversary targets the given LLM
    (), all examples are processed into prompts recognized as valid target LLM inputs.
    Once all queries have been processed by the target LLM, we generate an adversarial
    dataset ($D_{adv}$) combining inputs with received LLM replies, as well as automated
    validation (removing API request errors, failed, or erroneous prompts). This process
    can be distributed and parallelised to minimize collection time as well as mitigate
    the impact of rate-limiting and/or detection by filtering systems when interacting
    with the web-based LLM API [[5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Extracted Model Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using () and evaluation () is selected for distilling knowledge from the target
    LLM. This base model is then trained upon (). Using evaluation set () and ($M_{target}$).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 ML Attack Staging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Access to an extracted model (local to an adversary) created from a target LLM
    facilitates the execution of augmented adversarial attacks. This extracted model
    allows an adversary to perform unrestricted model querying to test, modify or
    tailor adversarial attack(s) to discover exploits and vulnerabilities against
    a target LLM [[11](#bib.bib11)]. Furthermore, access to an extracted model enables
    an adversary to operate in a sandbox environment to conduct adversarial attacks
    prior to executing the same attack(s) against the target LLM in production (and
    of particular concern, whilst minimizing the likelihood of detection by the provider).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/253251da6a6c57ea1d9a00f018b996e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example of Prompt Template. Slots for SQuAD context and questions,
    with a set of instructions for the LLM to follow.'
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the effectiveness of *Model Leeching*, we created a set of extracted
    models using ChatGPT-3.5-Turbo as the target model, with Question & Answers as
    the target task. Task-specific prompts were designed and generated using the Stanford
    Question Answering 1.1 Dataset (SQuAD) containing 100k examples (85k to 15k evaluation
    split), representing a context and set of questions and associated answers [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Prompt Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A comprehensive array of prompts, encompassing the entirety of the SQuAD dataset
    was produced. These prompts adhere to a template containing the specific SQuAD
    question and context, enabling ChatGPT-3.5-Turbo to efficiently process and respond
    to the given task. As seen in Figure [2](#S4.F2 "Figure 2 ‣ 4 Experimental Setup
    ‣ Model Leeching: An Extraction Attack Targeting LLMs"), each rule instructs the
    target LLM to produce an output desired by the adversary ensuring effective capture
    of task-specific knowledge. The template comprises:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Target LLM is specifically directed to provide only the precise answer to the
    assigned SQuAD question, drawn solely from the provided SQuAD context. This stipulation
    is crucial due to the inherent tendency of general chat-style LLMs (such as ChatGPT-3.5-Turbo)
    to produce more verbose responses than necessary. In the scope of SQuAD score
    assessment, only the exact answer is pertinent, negating the need for any additional
    content.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By including the sentence where the answer occurred, the LLM is required to
    demonstrate a degree of contextual comprehension beyond simple fact extraction,
    for valid data generation that contains the correct task knowledge. This requirement
    ensures that the model is not limited to identifying keywords, but understands
    the broader text semantic structure. In the case of assessing model performance
    on ChatGPT-3.5-Turbo, the index in which an answer is found within the context
    is required.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use of a standardized JSON format for responses facilitates efficient and uniform
    data handling. The keys answer and sentence provide a clear and concise structure,
    making the model output easier to process and compare algorithmically and manually.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ability to respond with ’UNSURE’ provides a safeguard for quality control of
    model response. By acknowledging its own uncertainty, the LLM avoids disseminating
    potentially incorrect or misleading information, and assists in parsing prompts
    that it was unable to complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 Model Base Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the effectiveness of Model Leeching, we selected three different
    base model architectures and several variants (with models parameter sizes ranging
    from between 14 to 123 million) to create an extracted model of our target LLM.
    These six model architectures include Bert [[6](#bib.bib6)], Albert [[13](#bib.bib13)],
    and Roberta [[14](#bib.bib14)], were selected due to their parameter size and
    respective performance upon our selected task [[14](#bib.bib14)]. The intention
    of selecting these architectures as candidate extracted models is to to evaluate
    wether: 1) more sophisticated models (parameters, architecture) are more effective
    at learning target LLM characteristics; and 2) low parameter models (i.e. 100x
    smaller vs. ChatGPT-3.5-Turbo) can learn sufficient characteristics from a target
    LLM, while achieving comparable performance in a specific task. Using these candidate
    model architectures, we train two sets of models for the purposes of evaluation,
    1) extracted models; trained upon generated $Adv_{train}$ dataset, and 2) baseline
    models; for performance comparison, trained directly upon the ground-truth SQuAD
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c9302bd994a66567908912803223e86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Example of AddSent Attack. Adversarial sentences appended to SQuAD
    context (blue highlighted text) to yield incorrect answers for SQuAD questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 ML Attack Staging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We created and deployed an adversarial attack derived from AddSent [[11](#bib.bib11)]
    that generates an adversarial context by adding a non-factual yet semantically
    and syntactically correct sentences to the original context from a SQuAD entry
    (Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Model Base Architectures ‣ 4 Experimental Setup
    ‣ Model Leeching: An Extraction Attack Targeting LLMs")). The goal of this attack
    is to cause a QA model to incorrectly answer a question when given an adversarial
    context. We further modified this attack to generate a larger variety of adversarial
    context, selectively chosen based on their success upon our extracted model, which
    is then sent to the target LLM for improved misclassification likelihood.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Model Leeching Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We demonstrate the effectiveness of *Model Leeching* by targeting ChatGPT-3.5-Turbo
    with a pre-trained Roberta-Large base architecture [[14](#bib.bib14)]. Using SQuAD
    as described in [4.1](#S4.SS1 "4.1 Prompt Construction ‣ 4 Experimental Setup
    ‣ Model Leeching: An Extraction Attack Targeting LLMs"), we generate a new labelled
    adversarial dataset through automated prompt generation querying ChatGPT-3.5-Turbo,
    which is trained upon the base architecture to create an extracted model. We evaluate
    attack performance by measuring the extracted model performance to a baseline
    model directly trained on SQuAD with ground truth answers. We demonstrate the
    feasibility of attack transferability across models by applying the AddSent attack
    [[11](#bib.bib11)] upon the extracted model, generating adversarial perturbations
    that can be further staged upon the target LLM. In order to explore feasibility
    of transferability of adversarial vulnerabilities across models. We leverage three
    metrics for evaluation: Exact Match (EM), and F1 Score used to measure the performance/similarity
    of our extracted model and ChatGPT-3.5-Turbo [[21](#bib.bib21)], and attack success
    rate for further attack staging representing successful adversarial prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Data Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From 100k examples of contexts, questions and answers within SQuAD, 83,335 total
    usable examples were collected, with 16,665 failing either from API request errors,
    or erroneous replies, attributing to a 16.66% error rate when labelling through
    ChatGPT-3.5-Turbo. From these 83,335 examples, 76,130 can be used for further
    extracted model training (). Query time was 48 hours and cost $50 to execute API
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9f1ed132cb731376f5bcf378c8a4c12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Model Similarity to ChatGPT-3.5-Turbo. Comparing similarity in correct
    and incorrect answering of questions relative to ChatGPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b33a26d6d9fa62a2c0e53a0c345eecbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Baseline and Extracted SQuAD Accuracy. Comparing the baseline and
    extracted models’ performance on the original SQuAD dataset questions and answers.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Extraction Similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Data Generation ‣ 5 Results ‣ Model Leeching:
    An Extraction Attack Targeting LLMs") shows that each extracted model performed
    more similarly to ChatGPT-3.5-Turbo compared to their baseline counterpart, with
    each model EM and F1 similarity score being up to 10.49% and 5% higher, respectively.
    Roberta Large achieved the highest ChatGPT-3.5-Turbo similarity, with a 0.73 EM
    and 0.87 F1 score denoting high similarity to the target LLM [[17](#bib.bib17)].
    Similarity of the baseline models to ChatGPT-3.5-Turbo is lower than the extracted
    model, due to being trained using the original SQuAD dataset, whereas the extracted
    models used a dataset derived from ChatGPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Task Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Extracted model task performance was evaluated by comparing the SQuAD EM and
    F1 scores to baseline models and ChatGPT-3.5-Turbo. Figure [5](#S5.F5 "Figure
    5 ‣ 5.1 Data Generation ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting
    LLMs") shows that extracted models exhibit similar performance for SQuAD when
    compared with their respective baselines, with EM and F1 scores. Evaluating our
    extracted models against ChatGPT-3.5-Turbo, we observed that Roberta Large achieved
    the highest similarity to ChatGPT-3.5-Turbo performance exhibiting EM and F1 scores,
    achieving an EM/F1 score of 0.75/0.87 compared to 0.74/0.87 respectively. Extracted
    model performance from ChatGPT-3.5-Turbo is sufficiently comparable in performance
    to state-of-the-art literature on QA tasks, where with the hyperparameters used
    in Roberta Large are more performant than the other architectures [[14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1c0e0a424c6324fe17f667a89bbf25f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: ML Attack Staging Results. Comparing the original attack’s adversarial
    effectiveness against those developed with the model extracted from ChatGPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 ML Attack Staging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Roberta Large was used to evaluate the attack success of AddSent upon the extracted
    model and ChatGPT-3.5-Turbo given its high SQuAD accuracy and similarity. AddSent
    exhibited an attack success of 0.28 and 0.26 upon the extracted model and ChatGPT-3.5-Turbo,
    respectively. Leveraging access to our extracted model, we selected and sent the
    best performing 7,205 adversarial examples to ChatGPT-3.5-Turbo. Our results indicate
    that adversarial examples augmented by AddSent increased attack success by 26%
    for the extracted model, and 11% to ChatGPT-3.5-Turbo (Figure [6](#S5.F6 "Figure
    6 ‣ 5.3 Task Performance ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting
    LLMs")). Attack effectiveness is reduced across models due to ChatGPT-3.5-Turbo
    being 100x larger in parameter size than local models, and leveraging advanced
    training methods such as reinforcement learning from human feedback, not used
    on our local models. While ChatGPT-3.5-Turbo is more task capable and less likely
    to be evaded by adversarial prompts compared to a local model. However, despite
    increased adversarial robustness, our results highlight attack transferability
    exists between an extracted model and its target, demonstrating the feasibility
    of leveraging distilled knowledge to further stage and subsequently launch improved
    adversarial attacks upon a production LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Dataset Labelling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the SQuAD dataset containing 100k examples, we successfully labelled
    83,335 using ChatGPT-3.5-Turbo (see Section [5.1](#S5.SS1 "5.1 Data Generation
    ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting LLMs")). In total,
    this process cost $50 and required 48 hours to complete. Compared to using labelling
    services such as Amazon SageMaker Data Labeling [[2](#bib.bib2)], the estimated
    cost of labelling would be $0.036 per example of data, totalling $3,600, demonstrating
    a significant reduction in cost when using generative LLMs to label datasets.
    We additionally note that the success of labelling datasets can be increased by
    1) further prompt engineering and optimization to package multiple SQuAD examples
    into one efficient query enabling reduction in query cost and time; and 2) re-sending
    of failed SQuAD examples to achieve higher amount of successful labelled examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Extraction Similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Extracted models derived from *Model Leeching* demonstrate the ability to effectively
    learn the characteristics of the target model. Highlighted within Section [5.2](#S5.SS2
    "5.2 Extraction Similarity ‣ 5 Results ‣ Model Leeching: An Extraction Attack
    Targeting LLMs"), noticeable deviations between our extracted models, and baseline
    equivalents, against their EM/F1 similarity to the target, demonstrate extracted
    models contain similarly learned knowledge to the target compared to baseline
    models. The extracted model responses closely align with those of ChatGPT-3.5-Turbo’s,
    exhibiting similar success and error rates in how they semantically and syntactically
    answer questions. This finding underscoring the capacity of our model to replicate
    the behaviour of the target, especially in the given task.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Distilled Knowledge Capability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our findings showcase the possibility of not only extracting knowledge from
    a LLM, but also transferring this knowledge effectively to a model with significantly
    fewer parameters. ChatGPT-3.5-Turbo comprises 175 billion parameters, whilst our
    local models are 100x smaller (See Section [5.3](#S5.SS3 "5.3 Task Performance
    ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting LLMs")). These smaller
    local models when trained with the extracted dataset demonstrated the ability
    to perform the given task effectively. Comparing our extracted model performance
    upon SQuAD to ChatGPT-3.5-Turbo we observed at worst a 13.2%/12.04% EM/F1 score
    difference and our best-performing extracted model, Roberta Large, achieving identical
    SQuAD scores to ChatGPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 ML Attack Staging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Demonstrated within Section [5.4](#S5.SS4 "5.4 ML Attack Staging ‣ 5 Results
    ‣ Model Leeching: An Extraction Attack Targeting LLMs"), it is feasible to utilize
    an extracted model within an adversaries’ local environment to conduct further
    adversarial attack staging. By having unfettered query access to this extracted
    model, it facilitates the enhancement of attack success. The potency of the AddSent
    attack on the model extracted by Model Leeching was increased by 26%, which consequently
    led to an 11% increase when launched against ChatGPT-3.5-Turbo. This highlights
    the vulnerability of a target LLM to subsequent machine learning attacks once
    adversaries acquire an extracted model. By having access to this ’sandbox’ model,
    adversaries can refine or innovate their attack strategies. Consequently, LLMs
    deployed and served over publicly accessible APIs are at significant risk to further
    attack staging.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Further Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Analysis of Additional Production LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Further work includes conducting *Model Leeching* against a larger array of
    LLM(s) such as BARD, LLaMA and available variations of GPT models from OpenAI.
    Taking these models and exploring how they respond to *Model Leeching* and their
    vulnerability to follow-up attacks. Such a study would demonstrate the possibility
    to generate ensemble models that inherit characteristics from multiple target
    LLMs. Enabling the optimization of a local model by task-specific performance
    from the best-performing target would aim to maximise the local model capability.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Extraction By Proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiple open-source versions of popular LLMs have been produced by the ML community.
    This includes examples such as GPT4All [[19](#bib.bib19)] and Llama [[24](#bib.bib24)]
    that can be deployed on consumer-grade devices. These models typically leverage
    training sets, architectures and prompts used to develop the LLM they are aiming
    to extract and replicate. If these models share significant characteristics with
    the original LLM, it may be feasible for an adversary to conduct *Model Leeching*
    and then deploy an improved attack against a target LLM it didn’t interact with
    before attack deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 LLM Defenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There has been limited work to defend against attacks on LLMs. Previous research
    into defending against model extraction attacks for smaller NLP models has been
    explored, utilizing techniques such as Membership Classification [[22](#bib.bib22)],
    and Model Watermarking [[23](#bib.bib23)]. However given the rapid development
    of new state-of-the-art adversarial attacks against LLMs, it is important that
    the effectiveness of currently proposed defense techniques within literature are
    evaluated with newer LLMs. Exploring if the characteristics from applied defense
    techniques are captured within extracted knowledge from the target model, and
    further detectable within a distilled extracted model.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper we have proposed a new state-of-the-art extraction attack *Model
    Leeching* as a cost-effective means to generate an extracted model with shared
    characteristics to a target LLM. Furthermore, we demonstrated that it is feasible
    to conduct adversarial attack staging against a production LLM via interrogating
    an extracted model derived from a target LLM within a sandbox environment. Our
    findings suggest that extracted models can be derived with a high similarity and
    task accuracy with low query costs, and constitute the basis of attack transferability
    to execute further successful adversarial attacks utilizing data leaked from the
    target LLM.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] AI, G. About Bard. Google AI: Publications, 2023. Accessed: 8th February
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] AWS. Sagemaker data labeling pricing. [https://aws.amazon.com/sagemaker/data-labeling/pricing/](https://aws.amazon.com/sagemaker/data-labeling/pricing/),
    2023. Accessed: 20230-06-30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A.,
    Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel,
    C. Extracting training data from large language models, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., and Mukhopadhyay,
    D. Adversarial attacks and defences: A survey, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Crothers, E., Japkowicz, N., and Viktor, H. Machine generated text: A comprehensive
    survey of threat models and detection methods, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training
    of deep bidirectional transformers for language understanding, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Efrat, A., and Levy, O. The turking test: Can language models understand
    instructions?, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Floridi, L. Ai as agency without intelligence: on chatgpt, large language
    models, and other generative models. Philosophy & Technology 36, 1 (Mar 2023),
    15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Hackett, W., Trawicki, S., Yu, Z., Suri, N., and Garraghan, P. Pinch: An
    adversarial extraction attack framework for deep learning models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Hu, X., Liang, L., Li, S., Deng, L., Zuo, P., Ji, Y., Xie, X., Ding, Y.,
    Liu, C., Sherwood, T., and Xie, Y. Deepsniffer: A dnn model extraction framework
    based on learning architectural hints. In Proceedings of the Twenty-Fifth International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (New York, NY, USA, 2020), ASPLOS ’20, Association for Computing Machinery, p. 385–399.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Jia, R., and Liang, P. Adversarial examples for evaluating reading comprehension
    systems, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Krishna, K., Tomar, G. S., Parikh, A. P., Papernot, N., and Iyyer, M.
    Thieves on sesame street! model extraction of bert-based apis, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
    Albert: A lite bert for self-supervised learning of language representations,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining
    approach, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Mishra, S., Khashabi, D., Baral, C., Choi, Y., and Hajishirzi, H. Reframing
    instructional prompts to GPTk’s language. In Findings of the Association for Computational
    Linguistics: ACL 2022 (Dublin, Ireland, May 2022), Association for Computational
    Linguistics, pp. 589–612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] MITRE. MITRE ATLAS Adversarial Attack Knowledge Base, 2023. [Online; accessed
    02-May-2023].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Oliynyk, D., Mayer, R., and Rauber, A. I know what you trained last summer:
    A survey on stealing machine learning models and defences. ACM Comput. Surv. 55,
    14s (jul 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] OpenAI. ChatGPT. OpenAI Blog, 2023. Accessed: 2023-02-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] OpenAI. gpt4all.io, 2023. Accessed: 8th February 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., and
    Swami, A. The limitations of deep learning in adversarial settings. pp. 372–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions
    for machine comprehension of text, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership inference
    attacks against machine learning models, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Szyller, S., Atli, B. G., Marchal, S., and Asokan, N. Dawn: Dynamic adversarial
    watermarking of neural networks, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., and Lample, G. Llama: Open and efficient foundation language models,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., and Ristenpart, T. Stealing
    machine learning models via prediction APIs. In 25th USENIX Security Symposium
    (USENIX Security 16) (Austin, TX, Aug. 2016), USENIX Association, pp. 601–618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Wang, X., Li, J., Kuang, X., an Tan, Y., and Li, J. The security of machine
    learning in an adversarial setting: A survey. Journal of Parallel and Distributed
    Computing 130 (2019), 12–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D.,
    and Hajishirzi, H. Self-instruct: Aligning language model with self generated
    instructions, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar,
    A., Spencer-Smith, J., and Schmidt, D. C. A prompt pattern catalog to enhance
    prompt engineering with chatgpt, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang,
    B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren,
    R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A survey of
    large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable
    adversarial attacks on aligned language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
