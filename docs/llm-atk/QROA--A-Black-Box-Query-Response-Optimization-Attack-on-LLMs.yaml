- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'QROA: A Black-Box Query-Response Optimization Attack on LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02044](https://ar5iv.labs.arxiv.org/html/2406.02044)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hussein Jawad
  prefs: []
  type: TYPE_NORMAL
- en: Capgemini Invent, Paris
  prefs: []
  type: TYPE_NORMAL
- en: hussein.jawad@capgemini.com &Nicolas J-B. Brunel
  prefs: []
  type: TYPE_NORMAL
- en: LaMME, ENSIIE
  prefs: []
  type: TYPE_NORMAL
- en: Capgemini Invent, Paris
  prefs: []
  type: TYPE_NORMAL
- en: nicolas.brunel@capgemini.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have surged in popularity in recent months, yet
    they possess concerning capabilities for generating harmful content when manipulated.
    This study introduces the Query-Response Optimization Attack (QROA), an optimization-based
    strategy designed to exploit LLMs through a black-box, query-only interaction.
    QROA adds an optimized trigger to a malicious instruction to compel the LLM to
    generate harmful content. Unlike previous approaches, QROA does not require access
    to the model’s logit information or any other internal data and operates solely
    through the standard query-response interface of LLMs. Inspired by deep Q-learning
    and Greedy coordinate descent, the method iteratively updates tokens to maximize
    a designed reward function. We tested our method on various LLMs such as Vicuna,
    Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%. We also
    tested the model against Llama2-chat, the fine-tuned version of Llama2 designed
    to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger
    seed. This study demonstrates the feasibility of generating jailbreak attacks
    against deployed LLMs in the public domain using black-box optimization methods,
    enabling more comprehensive safety testing of LLMs. The code will be made public
    on the following link: [https://github.com/qroa/qroa](https://github.com/qroa/qroa)'
  prefs: []
  type: TYPE_NORMAL
- en: 1 introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, the emergence of large language models (LLMs) and their rapid
    improvements [[16](#bib.bib16)] or [[23](#bib.bib23)] has marked a transformative
    period in the fields of natural language processing, text generation, and software
    development. While the utility of these models is undeniable, their potential
    for misuse has surfaced as a critical issue. Studies have highlighted their ability
    to produce offensive content or be manipulated into performing undesirable actions
    through so-called "jailbreak" prompts [[26](#bib.bib26)]; [[27](#bib.bib27)].
    Such weaknesses gave the impetus for the development of alignment strategies to
    mitigate these risks by training models to avoid harmful outputs and reject inappropriate
    requests ([[17](#bib.bib17)]; [[4](#bib.bib4)]; [[12](#bib.bib12)]; [[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite these efforts, recent advancements reveal that LLMs remain vulnerable
    to hand-written and algorithmically generated sophisticated attacks that cleverly
    bypass these protective mechanisms ([[5](#bib.bib5)] , [[2](#bib.bib2)]). This
    vulnerability is particularly alarming given the models’ widespread integration
    into commercial and private sectors, with significant security implications.
  prefs: []
  type: TYPE_NORMAL
- en: A notable advancement in jailbreak attacks on LLMs is the development of token-level
    optimization methods. In these methods, a specifically optimized trigger is appended
    to a malicious instruction to compel the LLM to respond in a desired manner. For
    instance, the Greedy Coordinate Gradient (GCG) optimization algorithm proposed
    by [[31](#bib.bib31)] leverages the gradient of the objective function to update
    the trigger one token at a time. However, GCG is effective only in "white box"
    scenarios where internal model details are accessible, contrasting with "black
    box" situations encountered in production environments where attackers typically
    only have access to the LLM’s output through a chatbot interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the limitations of white-box attack methods in real-world scenarios,
    alternative black-box optimization attack strategies have been proposed, such
    as TAP[[15](#bib.bib15)] , PAL [[20](#bib.bib20)], and Open sesame [[14](#bib.bib14)].
    These methods do not rely on gradient access and are designed to function effectively
    with the limited information available from a chatbot interface. However, they
    also have limitations, as they require access to the logits of the LLM—a requirement
    not typically met in production environments. The central contribution of the
    paper is the introduction of the Query-Response Optimization Attack (QROA), a
    novel approach to circumvent the safeguards of LLMs, with the following claims:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QROA is a black-box and query-only method, an optimization-based attack
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is not based on human-crafted templates, and thus allows the attack to be
    more general and flexible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Unlike previous strategies that may require access to the model’s internal
    logit data, QROA operates entirely through the standard query-response interface
    of LLMs. The attack leverages principles similar to those used in GCG [[31](#bib.bib31)]
    and PAL [[20](#bib.bib20)] methods, employing token-level optimization to discover
    triggers that elicit malicious behavior from the models. QROA employs reinforcement
    learning for crafting attacks on language models by:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the attack as a reinforcement learning problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Designing a reward function that evaluates the attack’s effectiveness based
    on the model’s output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning the input to maximize the reward, adjusting one token at a time
    and using a Q-learning algorithm to refine a model that estimates the reward function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jailbreak Prompts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Jailbreak prompts represent a foundational technique in adversarial machine
    learning, particularly against LLMs. Such prompts are designed to “jailbreak”
    or coerce models into operating outside their ethical or intended boundaries.
    Significant contributions include [[5](#bib.bib5)] and [[2](#bib.bib2)], who demonstrated
    the ability to bypass LLM alignment strategies using manually crafted inputs.
    Furthermore, [[25](#bib.bib25)], [[10](#bib.bib10)], [[8](#bib.bib8)], [[31](#bib.bib31)],
    [[19](#bib.bib19)] have advanced the field by showing that adversarial prompts
    can be automatically discovered, exploiting specific model vulnerabilities to
    induce harmful or misleading outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Token-Level Optimization Attacks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Token-level optimization attacks represent a more refined approach in the adversarial
    domain, where the goal is to discover trigger phrases that, when appended to a
    given instruction, lead the LLM to produce outputs aligned with the attacker’s
    intentions. This class of attack moves beyond simple prompt crafting to an optimization
    problem where the objective function is carefully set to maximize the model’s
    error rate in a controlled manner. [[31](#bib.bib31)] proposed a gradient-based
    discrete optimization method that builds on earlier work by Shin et al. [[21](#bib.bib21)]
    and [[10](#bib.bib10)]. This approach contrasts with traditional methods as it
    directly manipulates the model’s response through fine-grained adjustments rather
    than relying on broader prompt templates. A significant advantage of this type
    of attack is that it does not require any human-crafted templates as seeds, enhancing
    its generalizability across different and new LLMs, which makes the attack more
    adaptable and harder to defend against.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Black-Box Attacks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In more recent studies, black-box optimization methods arise to circumvent
    the requirement for direct model access: techniques such as the PAL attack [[20](#bib.bib20)]
    leverage a proxy model to simulate the target model, enabling attackers to refine
    their inputs based on the proxy’s feedback. Other methods, like the one proposed
    by [[14](#bib.bib14)], use fuzzing methodologies that rely on cosine similarity
    to determine the effectiveness of the input modifications. These methods highlight
    the shift toward techniques that can operate effectively without comprehensive
    access to the target model’s internal workings, reflecting a realistic attack
    scenario in many real-world applications. However, these methods still require
    access to the logits or other internal data of the model to optimize their attack
    strategies effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Other automated black box methods such as [[20](#bib.bib20)] and [[29](#bib.bib29)]
    have demonstrated their effectiveness in producing jailbreaks with a small number
    of queries. However, they require the necessity of human-crafted prompt design.
  prefs: []
  type: TYPE_NORMAL
- en: Query-Response Optimization Attack (QROA)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The development of QROA adds to this body of knowledge by utilizing a query-only
    interaction model that iteratively optimizes the input based on the responses
    received from the LLM. QROA is distinct from other methods as it is purely an
    optimization technique and does not require access to logits or any internal data
    from the target model. This makes it highly applicable in scenarios where attackers
    have limited access to the target system. Through reinforcement learning strategies,
    QROA fine-tunes its attacks in a way that is both adaptable and effective, marking
    a significant advancement in the toolkit available to researchers and engineers
    to evaluate the safety of in-production LLMs and apply audits on them.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Formalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we concentrate on a fixed Large Language Model (LLM) . Hence,  is
    a random variable with values in the space of sequences of tokens. Consequently,
    the likelihood of a given sequence of tokens .
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to "jailbreak" , we seek to identify a suffix  (the corresponding
    sequence is denoted $I+x$), it induces the LLM to exhibit the specified behavior
    without refusing to respond.
  prefs: []
  type: TYPE_NORMAL
- en: Previous literature has often utilized a fixed Target sequence  (the initial
    instruction), to determine whether a "jailbreak" was successful. The goal has
    been to discover a trigger suffix that maximizes the likelihood of $T$ being the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we approach the problem more broadly by assuming the existence
    of an alignment function  if the output aligns with the instruction—thereby fulfilling
    the request without refusal, and  is fixed as a special case of this function,
    where $f(\text{Instruction},\text{Output})=\mathbb{P}(T|(I+x))$. However, in this
    paper, we will define a more comprehensive objective function that does not rely
    on a specific fixed target.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We consider a Language Model  that we wish the model to follow. Finally, we
    introduce the suffix , could potentially induce $G$ to produce an output that
    aligns with the malicious behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Objective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary objective is to jailbreak the language model . This suffix should
    maximize the likelihood that appending it to a given instruction  to exhibit a
    specified malicious behavior. This goal is achieved by optimizing an alignment
    function  is appended to $I$.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we propose to maximize the scoring function $x\mapsto S(x;I)$
    defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S(x,I)=\underset{t_{G}\sim G(I+x)}{\mathbb{E}}[f(I,t_{G})]$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: where  generated by .  that triggers the undesirable behavior is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x^{*}=\underset{x}{\arg\max}\;S(x,I).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 3.1.2 Challenges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Addressing the objective function $S(x,I)$ unveils three primary challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alignement function is Unknown: The exact nature of the alignment function
    . A first choice is to use a sentiment analysis model to detect the tone of the
    response: we consider that refusal or contradiction responses are generally associated
    with negative sentiments. A second choice is a Pre-trained model textual entailment
    (NLI) task that can evaluate whether an output response aligns with the input
    instruction. A third choice is to train a model on a dataset specifically designed
    for compliance detection. This would involve collecting a diverse set of instruction-response
    pairs and labeling them based on whether the response complies with or refuses
    the instruction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The generation is Random: The biggest challenge with the full black box method
    is the randomness of LLM outputs with an unknown distribution  cannot be directly
    calculated or requires significant computational resources. Therefore, traditional
    optimization, where the objective function is deterministic, may not work.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The gradient is not available: Operating with the LLM as a black box prevents
    access to the gradient of . This challenge necessitates alternative strategies
    to adjust $x$ without relying on gradient-based optimization methods.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Similarity to Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our study on jailbreaking a Language Model (LM), we draw parallels to the
    concept of reinforcement learning (RL). Our scoring function , resembles the reward
    function used in RL [[18](#bib.bib18)]. Both aim to optimize an average outcome
    under uncertain conditions. In RL, the objective is to maximize expected rewards,
    which can vary based on different states and actions. Similarly, in our approach,
    $S(x,I)$ represents an expected value capturing the variability in LM responses
    to different suffixes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, our approach exhibits two key differences: the first being the
    absence of States. Unlike RL, where decisions depend on and lead to new states,
    our approach does not involve state transitions. Instead, it focuses solely on
    maximizing the alignment score , which represents textual sentences. This renders
    the action space virtually infinite and highly complex, a situation analogous
    to the problems discussed in high-dimensional RL spaces [[9](#bib.bib9)]'
  prefs: []
  type: TYPE_NORMAL
- en: While the jailbreak problem does not incorporate state-based decisions typical
    of RL environments, optimizing a scoring function akin to a reward function provides
    valuable insights. This similarity suggests the potential usefulness of RL strategies,
    such as exploration techniques and value-based optimization, in navigating the
    complex and infinite action space involved in LM jailbreak scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '4 QROA: A Query Response Optimization Attack'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present an optimization strategy designed to identify triggers
    that minimize a scoring function. This approach incorporates elements of experience
    replay methodologies from deep Q-learning. The scoring function plays a role similar
    to the reward function in reinforcement learning as our objective is to discover
    prompts optimizing this reward. The proposed framework includes a surrogate model
    that approximates the scoring function, a selection function analogous to action
    selection in reinforcement learning for choosing new prompts, and a replay memory
    [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: Choice of Alignment Function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our approach to jailbreak a large language model (LLM) by appending a strategic
    suffix , the selection of an appropriate alignment function .
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we use only one alignment function in our main analysis. For
    further comparison with other alignment functions, please refer to the Appendix
    [C](#A3 "Appendix C Impact of the choice of alignment function ‣ QROA: A Black-Box
    Query-Response Optimization Attack on LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Harmful Evaluation Model .
  prefs: []
  type: TYPE_NORMAL
- en: Surrogate Model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To address the optimization problem where direct gradient access is not feasible,
    we introduce a surrogate model denoted as ). This model is designed to approximate
    the scoring function . The structure of  found in Q-learning algorithm [[18](#bib.bib18)],
    with a key difference being the absence of the state variable $s$. For a detailed
    architecture of the surrogate model, we refer to Appendix [A](#A1 "Appendix A
    Neural Network Architecture For the Surrogate Model ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Experience Replay [[13](#bib.bib13)]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Experience replay is a fundamental technique in reinforcement learning that
    allows learning algorithms to reuse past experiences to break the temporal correlations
    in successive training samples. For our optimization attack framework, integrating
    experience replay involves maintaining a buffer memory where each entry consists
    of a prompt . By leveraging this technique, our surrogate model can effectively
    sample from diverse historical prompts, reducing the risk of overfitting to recent
    data and promoting a more stable convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Description of the Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Initialization
  prefs: []
  type: TYPE_NORMAL
- en: 'Surrogate Model Setup: Construct a neural network that serves as the surrogate
    model $m_{\theta}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replay buffer Setup: Establish a replay buffer to store historical data consisting
    of trigger strings and the evaluation scores. $(D,h(x),n(x))$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting Point: Generate an initial trigger string either randomly or from
    a predefined list of known effective triggers. This string acts as the starting
    point for the iterative optimization process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we start an iterative process across several epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: Selection
  prefs: []
  type: TYPE_NORMAL
- en: 'Selection of the Best Trigger String So Far: Utilize the Upper Confidence Bound
    (UCB) method for trigger selection, calculated as  is the average score of the
    trigger,  is the number of times the particular trigger has been selected. This
    method balances exploration and exploitation by considering both the performance
    and the uncertainty of each trigger [[3](#bib.bib3)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coordinate Selection: Randomly pick a position within the trigger string. This
    position is where a token will be changed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Token Replacement and Variant Creation: At the selected position, generate
    various new strings, each substituting the original token with a different one.
    These variants are potential new trigger strings. This step is vital for exploring
    the effectiveness of different tokens in influencing the model’s output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Surrogate Model Assessment: Run these new string variants through the surrogate
    model. The model predicts an approximation of the exact value of the scoring function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Selection Function Application: Select the top $K$ (e.g. 100) string variants
    based on their estimated effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: 'Score Function Evaluation: Empirically evaluate the selected trigger strings
    by inputting them into the LLM and observing the outputs. The outputs are then
    scored using the predefined alignment functions to assess their effectiveness
    in inducing the desired behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experience Storage: Store the results of these evaluations in the memory buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Sampling: a batch of experiences is randomly sampled from the replay memory
    to update the model parameters. This random sampling ensures that the learning
    process is robust and incorporates a broad range of scenarios, enabling the model
    to generalize better across different malicious inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimization: Perform gradient descent on the surrogate model parameters .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 1 QROA: Query Response Optimization Attack Framework'
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Initial trigger , malicious Instruction , target model (black box)
    , batch size , maximum number of queries , maximum size of buffer 3:4: Average
    score for each trigger, track of the effectiveness of different triggers.5: Number
    of queries per trigger, track of many times each trigger has been tested6: Total
    number of queries7: buffer memory, used to resample evaluated triggers for updating
    the surrogate model8:). 13:      Generate token variants14:      Select 15:     Evaluation
    Phase:16:     for      23:     end for24:     Learning Phase:25:      Resample
    with replacement  using gradient descent on  then28:          Add to best triggers
    if condition met29:end while30:return $best\_triggers$'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Choosing the Best Adversarial Suffix $x^{*}$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Algorithm [30](#alg1.l30 "In Algorithm 1 ‣ 4.1 Description of the Algorithm
    ‣ 4 QROA: A Query Response Optimization Attack ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs") returns a set of adversarial suffixes $x^{*}$ that
    have the potential to compel the Language Model (LLM) to generate the malicious
    output. To ensure that only triggers meeting a user-defined performance threshold
    are selected, it is crucial to apply statistical testing to these triggers. We
    use a z-test to statistically verify that the triggers exceed a defined score
    threshold with a certain confidence level. The procedure is outlined in the algorithm
    [18](#alg2.l18 "In Algorithm 2 ‣ Appendix D algorithms ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm evaluates each candidate trigger  and standard deviation . Triggers
    with a  are considered statistically significant and are added to the set of validated
    adversarial suffixes  have a statistically high likelihood of inducing the desired
    malicious behavior in the target model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Top1-trigger: We consider that the Best adversarial suffix is the trigger with
    the highest z-value'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate our proposed attack, we use the AdvBench benchmark [[31](#bib.bib31)],
    which includes 500 instances of harmful behaviors articulated through specific
    instructions. Following the setup by PAL [[20](#bib.bib20)] and [[15](#bib.bib15)],
    we randomly selected 50 behaviors for analysis. We evaluate four models: VICUNA-1.3
    (7B) [[7](#bib.bib7)], Mistral-Instruct (7B), [[11](#bib.bib11)] FALCON-Instruct
    (7B), [[1](#bib.bib1)] LLAMA2-Chat(7B), [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: For all models, top_p is set to 0.6 and temperature to 0.9\. The LLAMA2-7B-Chat
    models have undergone explicit safety alignment using Reinforcement Learning with
    Human Feedback (RLHF), employing techniques like rejection sampling and Proximal
    Policy Optimization (PPO) [[22](#bib.bib22)]. Vicuna v1.5 (16k) is fine-tuned
    from Llama 2 with supervised instruction fine-tuning and linear RoPE scaling,
    using 125K conversations from ShareGPT.com.
  prefs: []
  type: TYPE_NORMAL
- en: Our experimental framework utilized one NVIDIA RTX 6000 Ada GPU with 48 GB of
    RAM. Following the setup in PAL [[20](#bib.bib20)] and GCG [[31](#bib.bib31)],
    we fixed the total number of queries to the target LLM at 25K to identify good
    triggers, and also show the attack success rate for different budgets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this study, Our primary metric is defined as the Attack Success Rate (ASR)
    i.e. the proportion of successful attacks relative to the total number of malicious
    instructions tested. For each malicious instruction, the top-ranked triggers determined
    by (QROA) is appended. This combined query is then submitted to the Large Language
    Model (LLM) multiple times to observe variability in response due to the stochastic
    nature of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Given this variability, we refine our metric to evaluate the robustness of the
    triggers over multiple trials. Specifically, we introduce , across a predetermined
    number of trials (e.g., 100).
  prefs: []
  type: TYPE_NORMAL
- en: The . Here, "Number of Instructions with Success , when tested over the specified
    number of trials. This metric helps in assessing the effectiveness and consistency
    of the attacks generated by the triggers under different conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Labeling and Validation: For result labeling, we employ a fine-tuned RoBERTa
    model for classifying malicious responses, as detailed in [[28](#bib.bib28)].
    This classifier achieves a 92% accuracy rate in identifying malicious outputs,
    surpassing GPT-4’s accuracy of 87.4%. We also perform manual validation to ensure
    the reliability of this classifier. We allow each LLM to output a maximum of 70
    tokens to make the eval.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Hyperparameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The surrogate model is optimized using batch size 1024 when sampling from the
    memory buffer, the size of the memory buffer is set to 6400, and we use the Adam
    optimizer to update the parameters with a learning rate of 0.01 and weight decay
    of 0.0001, we use the same surrogate model neural network architecture, the weights
    of the embedding layer are initialized from GPT2, we set the threshold to 0.2\.
    We use as an alignment function the Harmful Evaluation Model as described in Section
    [4](#S4.SS0.SSS0.Px1 "Choice of Alignment Function ‣ 4 QROA: A Query Response
    Optimization Attack ‣ QROA: A Black-Box Query-Response Optimization Attack on
    LLMs"). We have also tested other alignment functions such as an Entailment Evaluation
    Model (see appendix [C](#A3 "Appendix C Impact of the choice of alignment function
    ‣ QROA: A Black-Box Query-Response Optimization Attack on LLMs")). For more details
    about experience settings check appendix [B](#A2 "Appendix B Experience Settings
    ‣ QROA: A Black-Box Query-Response Optimization Attack on LLMs")'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Main results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.2.1 Evaluate the effectiveness of the attack against Vicuna-7B
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Budget | ASR@10% | ASR@20% | ASR@30% | ASR@40% | ASR@50% | ASR@60% |'
  prefs: []
  type: TYPE_TB
- en: '| 10K | 60% | 53.3% | 43.3% | 40.62% | 36.6% | 36.6% |'
  prefs: []
  type: TYPE_TB
- en: '| 20K | 70% | 62.5% | 58% | 40% | 50% | 50.00% |'
  prefs: []
  type: TYPE_TB
- en: '| 25K | 80% | 73.3% | 58.3% | 58.3% | 54.1% | 50.00% |'
  prefs: []
  type: TYPE_TB
- en: '| 30K | 83.3% | 76.6% | 62.5% | 62.5% | 58.3% | 65.62% |'
  prefs: []
  type: TYPE_TB
- en: '| 40K | 87% | 82.6% | 75% | 50% | 55% | 65.62% |'
  prefs: []
  type: TYPE_TB
- en: '| 50K | 90% | 88% | 83% | 75% | 75% | 71.88% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Attack Success Rates at Different Budget Levels For Vicuna'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results presented in Table [1](#S5.T1 "Table 1 ‣ 5.2.1 Evaluate the effectiveness
    of the attack against Vicuna-7B ‣ 5.2 Main results ‣ 5 Experiments ‣ QROA: A Black-Box
    Query-Response Optimization Attack on LLMs") demonstrate insights into the efficacy
    of our attack methods under varying budget constraints. As expected, a clear trend
    shows that higher budgets correlate with improved Attack Success Rates (ASR),
    indicating that more resources allow for more opportunities to fine-tune and optimize
    the attack triggers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Influence of Budget on ASR: The increase in ASR from a budget of 10K to 50K
    queries is substantial, rising from 60% to 90% at the 10% threshold. This indicates
    that additional queries provide valuable data that refine the attack vectors and
    improve their effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Impact of Model Variability: The non-deterministic nature of LLMs, as evidenced
    by the varied ASRs at the same budget levels but different thresholds, underscores
    the challenge of predicting and controlling the behavior of these models in black
    box adversarial settings. This variability also emphasizes the importance of considering
    multiple thresholds when evaluating the robustness of LLMs against attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 QROA on other models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To extend the evaluation of the Query-Response Optimization Algorithm (QROA)
    across different models, here is a table with ASR@20% values at a budget of 25K
    queries for various large language models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | VICUNA | FALCON | MISTRAL |'
  prefs: []
  type: TYPE_TB
- en: '| ASR@20% | 73.3% | 98% | 98% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Attack Success Rates at 20% Threshold for Different Models at a 25K
    query budget'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Attack against LLama2-7B-chat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluating the attack’s effectiveness against the LLama2-7B-chat [[24](#bib.bib24)]
    model presented unique challenges due to its robust security alignment and consistent
    response behavior. Starting with random triggers as initial conditions, we encountered
    limitations with the model’s consistent responses, typically stating, "I cannot
    fulfill your request." This response limited the variability of the scoring function,
    often returning a zero value, indicating an almost flat response landscape in
    the tested region, this is known as the hard exploration problem [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: To address this, we extended the trigger length from 10 to 20 tokens, allowing
    exploration of a broader range of token sequences. Additionally, we replaced the
    initial embedding layer with embeddings from the lmsys/vicuna-7b-v1.5-16k model
    to better align with the target model’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, to enhance the efficiency of our approach, we pre-populated the memory
    buffer with successful triggers from previous successful experiments, and we used
    them in the UCB selection. Selecting successful triggers is based on Algorithm
    [18](#alg2.l18 "In Algorithm 2 ‣ Appendix D algorithms ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs"). We appended these triggers to the new malicious
    instruction, queried them against the target LLM, evaluated the outputs, and stored
    the results as described in the ’evaluation phase’ in Algorithm LABEL:alg1. This
    strategy ensured a more rapid convergence of the optimization algorithm by providing
    a higher-quality starting point for generating new triggers. By starting with
    10 pre-validated triggers, we significantly reduced the time and computational
    resources required to identify effective attack vectors. The procedure details
    are outlined in the algorithm [16](#alg3.l16 "In Algorithm 3 ‣ Appendix D algorithms
    ‣ QROA: A Black-Box Query-Response Optimization Attack on LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Comparaison with GCG & PAL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we compare the effectiveness of our Query-Response Optimization
    Algorithm (QROA) against two other attack methods: white box (GCG) attack [[31](#bib.bib31)]
    and black box using logit and proxy model (PAL) [[20](#bib.bib20)]. These comparisons
    provide insights into how well each method performs under the same experimental
    conditions. We focus on two models Llama-2-7B-Chat and Vicuna-7B-v1.3, The attack
    is evaluated with a fixed budget of 25K queries, and ASR@20% is used as the primary
    metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We test QROA using the initialization described in section [5.2.3](#S5.SS2.SSS3
    "5.2.3 Attack against LLama2-7B-chat ‣ 5.2 Main results ‣ 5 Experiments ‣ QROA:
    A Black-Box Query-Response Optimization Attack on LLMs") for both Llama2-Chat
    and Vicuna: this serves to accelerate the algorithm and improve efficiency and
    effectiveness of the attack.'
  prefs: []
  type: TYPE_NORMAL
- en: PAL was not evaluated on Vicuna-7B as Vicuna is used as a proxy model in PAL
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack | Llama-2-7B | Vicuna-7B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GCG | 56 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| PAL | 48 | - |'
  prefs: []
  type: TYPE_TB
- en: '| QROA | 82 | 82 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: ASR@20% for Different Attack Methods on Llama-2-7B-Chat and Vicuna-7B
    at a 25K Query Budget'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dependency on Alignment Function Precision: The effectiveness of QROA relies
    on the accuracy and appropriateness of the alignment function $f$, which assesses
    the LLM’s output compliance with malicious intent. The method assumes that the
    alignment function can be accurately modeled or approximated, which may not hold
    true in practical scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computational and Resource Intensity: The approach involves generating, evaluating,
    and refining numerous suffix variations to identify effective triggers. This process
    can be computationally and resource-intensive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Necessity of Initialization Step for Some LLMs: As stated in the previous section,
    we encountered significant challenges, particularly during our attempts to attack
    LLAMA2 chat. To address this, we facilitated the algorithm’s initialization to
    avoid starting in regions where the scoring is nearly flat. This was achieved
    by incorporating previously successful triggers into the memory buffer and the
    UCB selection under LLAMA2\. These triggers were either discovered by QROA or
    other optimization algorithms (GCG, PAL).'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Future Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: QROA is a novel method for exploiting Large Language Models (LLMs) through black-box,
    query-only interaction. Unlike previous literature, methods requiring internal
    model access or crafted templates, QROA works solely through the query-response
    interface, making it highly applicable in real-world scenarios. Using reinforcement
    learning and iterative token-level optimization, QROA identifies triggers that
    induce malicious behavior in LLMs. Experiments on models such as Vicuna, Mistral,
    Falcon, and LLama2-Chat show QROA’s high efficacy, achieving over 80% Attack Success
    Rate (ASR) even on models fine-tuned for resistance. This highlights significant
    security implications for LLM deployment in commercial and private sectors, as
    it exposes critical weaknesses of many industrial LLM. In addition, QROA with
    higher query budgets has improved ASR, emphasizing the need for understanding
    risks and developing better defense mechanisms. QROA advances the evaluation of
    LLM safety, demonstrating the need for robust alignment strategies to ensure reliability
    and safety in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Future research could focus on enhancing the transferability of the surrogate
    model between different malicious instructions. This involves developing a model
    that can generalize its learning from one set of instructions to another without
    losing efficacy, reducing the need for extensive retraining. As a consequence,
    we plan to exploit the potential of the surrogate model $m_{\theta}$ as a safety
    filter to predict and mitigate unintended harmful outputs from LLMs. By inverting
    the model’s purpose, it could serve as a proactive defense mechanism against malicious
    use cases, identifying and neutralizing potential triggers before they are exploited.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AAA^+ [23] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro
    Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
    Julien Launay, Quentin Malartic, et al. The falcon series of open language models.
    arXiv preprint arXiv:2311.16867, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alb [23] Alex Albert. Jailbreak chat. Retrieved May, 15:2023, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aue [03] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs.
    J. Mach. Learn. Res., 3:397–422, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BJN^+ [22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. arXiv preprint arXiv:2204.05862, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BKK^+ [22] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
    Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
    et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BSO^+ [16] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David
    Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation.
    Advances in neural information processing systems, 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLL^+ [23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao
    Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
    Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march
    2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNCC^+ [24] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew
    Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig
    Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural
    Information Processing Systems, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAEvH^+ [15] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag,
    Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris,
    and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv
    preprint arXiv:1512.07679, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JDRS [23] Erik Jones, Anca D. Dragan, Aditi Raghunathan, and Jacob Steinhardt.
    Automatically auditing large language models via discrete optimization. ArXiv,
    abs/2303.04381, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KSC^+ [23] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao,
    Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining
    language models with human preferences. In International Conference on Machine
    Learning, pages 17506–17533\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin [92] Longxin Lin. Self-improving reactive agents based on reinforcement
    learning, planning and teaching. Machine Learning, 8:293–321, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLS [23] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black
    box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MZK^+ [23] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson,
    Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking
    black-box llms automatically, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ope [24] OpenAI. Gpt-4 technical report, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OWJ^+ [22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in neural information processing systems, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SB [18] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SCB^+ [23] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.
    " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts
    on large language models. arXiv preprint arXiv:2308.03825, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SMWA [24] Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre Araujo.
    Pal: Proxy-guided black-box attack on large language models. arXiv preprint arXiv:2402.09674,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SRLI^+ [20] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. arXiv preprint arXiv:2010.15980, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SWD^+ [17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
    Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tea [24] Gemini Team. Gemini: A family of highly capable multimodal models,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TMS^+ [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WJK^+ [24] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping,
    and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization
    for prompt tuning and discovery. Advances in Neural Information Processing Systems,
    36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WMR^+ [21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
    Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WUR^+ [22] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen
    Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
    et al. Taxonomy of risks posed by language models. In Proceedings of the 2022
    ACM Conference on Fairness, Accountability, and Transparency, pages 214–229, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLD^+ [24] Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. Llm
    jailbreak attack versus defense techniques–a comprehensive study. arXiv preprint
    arXiv:2402.13457, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YLYX [23] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. Gptfuzzer: Red
    teaming large language models with auto-generated jailbreak prompts. ArXiv, abs/2309.10253,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZLX^+ [24] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment. Advances in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZWKF [23] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal
    and transferable adversarial attacks on aligned language models. arXiv preprint
    arXiv:2307.15043, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Neural Network Architecture For the Surrogate Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We construct the surrogate model to be a small MLP network. Below is the detailed
    structure, we Conv-1D layer with kernel size=1, this layer aims to reduce the
    size of embedding from 768 to 32:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer | Output | Parameters | Details |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding-1 | [50257, 768] | 38.6M | Non-trainable |'
  prefs: []
  type: TYPE_TB
- en: '| Conv1d-2 | [768, 32, 1] | 24.6K | ReLU, Trainable |'
  prefs: []
  type: TYPE_TB
- en: '| Linear-3 | [32*Length of trigger, 128] | 41.1K | ReLU, Trainable |'
  prefs: []
  type: TYPE_TB
- en: '| Linear-4 | [128, 32] | 4.1K | ReLU, Trainable |'
  prefs: []
  type: TYPE_TB
- en: '| Linear-5 | [32, 1] | 33 | Non-trainable |'
  prefs: []
  type: TYPE_TB
- en: '| Total params: | 38.7M |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params: | 69.9K |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params: | 38.6M |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Simplified architecture of the neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Experience Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Chat Template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the default conversation format for all models in FastChat. We also
    use the standard system message for all models as described below:'
  prefs: []
  type: TYPE_NORMAL
- en: Llama-2-7B
  prefs: []
  type: TYPE_NORMAL
- en: '"You are a helpful, respectful, and honest assistant. Always answer as helpfully
    as possible, while ensuring safety. Avoid harmful, unethical, racist, sexist,
    toxic, dangerous, or illegal content. Ensure that your responses are socially
    unbiased and positive. If a question is unclear or incoherent, explain why instead
    of providing incorrect information. If unsure about an answer, do not share false
    information."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vicuna-7B
  prefs: []
  type: TYPE_NORMAL
- en: '"A chat between a curious user and an artificial intelligence assistant. The
    assistant provides helpful, detailed, and polite answers to the user’s questions."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MPT 7B
  prefs: []
  type: TYPE_NORMAL
- en: '"- You are a helpful assistant chatbot trained by MosaicML.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- You answer questions and assist users enthusiastically.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- While eager to assist, you refuse to engage in actions harmful to the user.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- Besides being an information source, you can compose poetry, short stories,
    and tell jokes."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Falcon 7B
  prefs: []
  type: TYPE_NORMAL
- en: '"Answer the question as truthfully as possible using the provided text, and
    if the answer is not contained within the text below, say "I don’t know"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Falcon 7B
  prefs: []
  type: TYPE_NORMAL
- en: '"You are a helpful assistant."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Appendix C Impact of the choice of alignment function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The choice of an appropriate alignment function  to a given instruction .
  prefs: []
  type: TYPE_NORMAL
- en: Proposed Alignment Models
  prefs: []
  type: TYPE_NORMAL
- en: 'Harmful Evaluation Model $f_{1}(I,t)$: This model determines whether the generated
    text exhibits malicious content. It employs a fine-tuned RoBERTa model for harmful
    content detection, described in Xu et al. (2024) (available at [https://huggingface.co/zhx123/ftrobertallm](https://huggingface.co/zhx123/ftrobertallm)).
    This model achieves a 92% accuracy rate, surpassing that of GPT-4\. Inputs are
    formatted as "USER: [query] ASSISTANT: [response]," and the model outputs a score
    from 0 to 1, where 1 indicates harmful content.'
  prefs: []
  type: TYPE_NORMAL
- en: Entailment Evaluation Model . For instance, if  = ’Sure, here’s a convincing
    anonymous death threat’. The function  and $t$ using a BART model trained on large-scale
    entailment tasks (available at [https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)).
  prefs: []
  type: TYPE_NORMAL
- en: Using these models, we derive two scoring functions to evaluate distinct aspects
    of the LLM’s outputs, where  These functions allow us to test two versions of
    the approach, providing a comprehensive assessment of strategies to induce specific
    outputs from the LLM. The choice of alignment function is a crucial hyperparameter,
    influencing the effectiveness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance Comparison Table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Alignment Function |  (Entailment) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ASR@20% | 73.3% | 63% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Attack Success Rates at 20% Threshold for Different Scoring Methods
    at a 25K Query Budget'
  prefs: []
  type: TYPE_NORMAL
- en: The table shows the Attack Success Rate (ASR) at a 20% threshold, on Vicuna
    7B, comparing two alignment functions at a fixed 25K query budget. The Harmful
    Evaluation Model . This suggests that the detection model’s ability to identify
    harmful content aligns more effectively with the task of generating specific malicious
    outputs under these test conditions. This differentiation in performance underlines
    the importance of choosing the right alignment function based on the desired outcome
    from the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 2 Statistical Validation of Adversarial Suffix
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Set of candidate triggers , significance level 2:Output: Set of validated
    adversarial suffixes 5:for   to      -value := NormalCDF(Z) -value from Z-score15:     if  then16:          Add
    to validated triggers if below significance level17:end for18:return $X^{*}$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Pre-Populating Memory Buffer with Successful Triggers
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Initial set of Top successful triggers on previous instructions ,
    target model (black box) , alignment function 2:Output: Updated memory buffer     do8:     
    Append the trigger to the new malicious instruction9:      Query the target model
    with the modified instruction10:      Evaluate the model’s output using the alignment
    function11:      Store the trigger in the memory buffer12:      Update average
    score13:      $\triangleright$ Return the updated memory buffer'
  prefs: []
  type: TYPE_NORMAL
