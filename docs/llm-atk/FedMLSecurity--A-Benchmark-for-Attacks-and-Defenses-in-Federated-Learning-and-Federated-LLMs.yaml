- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.04959](https://ar5iv.labs.arxiv.org/html/2306.04959)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shanshan Han University of California, IrvineIrvineUSA [shanshan.han@uci.edu](mailto:shanshan.han@uci.edu)
    ,  Baturalp Buyukates University of Southern CaliforniaLos AngelesUSA [buyukate@usc.edu](mailto:buyukate@usc.edu)
    ,  Zijian Hu FedML Inc.Palo AltoUSA [zjh@fedml.ai](mailto:zjh@fedml.ai) ,  Han
    Jin University of Southern CaliforniaLos AngelesUSA [hanjin@usc.edu](mailto:hanjin@usc.edu)
    ,  Weizhao Jin University of Southern CaliforniaLos AngelesUSA [weizhaoj@usc.edu](mailto:weizhaoj@usc.edu)
    ,  Lichao Sun Lehigh UniversityBethlehemUSA [lis221@lehigh.edu](mailto:lis221@lehigh.edu)
    ,  Xiaoyang Wang UIUCChampaignUSA [xw28@illinois.edu](mailto:xw28@illinois.edu)
    ,  Wenxuan Wu Texas A&M UniversityCollege StationUSA [ww6726@tamu.edu](mailto:ww6726@tamu.edu)
    ,  Chulin Xie UIUCChampaignUSA [chulinx2@illinois.edu](mailto:chulinx2@illinois.edu)
    ,  Yuhang Yao Carnegie Mellon UniversityPittsburghUSA [yuhangya@andrew.cmu.edu](mailto:yuhangya@andrew.cmu.edu)
    ,  Kai Zhang Lehigh UniversityBethlehemUSA [kaz321@lehigh.edu](mailto:kaz321@lehigh.edu)
    ,  Qifan Zhang University of California, IrvineIrvineUSA [qifan.zhang@uci.edu](mailto:qifan.zhang@uci.edu)
    ,  Yuhui Zhang Zhejiang UniversityHangzhouChina [zhangyuhui42@zju.edu.cn](mailto:zhangyuhui42@zju.edu.cn)
    ,  Carlee Joe-Wong Carnegie Mellon UniversityPittsburghUSA [cjoewong@andrew.cmu.edu](mailto:cjoewong@andrew.cmu.edu)
    ,  Salman Avestimehr USC, FedML Inc.Los AngelesUSA [avestime@usc.edu](mailto:avestime@usc.edu)
     and  Chaoyang He FedML Inc.Palo AltoUSA [ch@fedml.ai](mailto:ch@fedml.ai)(2018;
    8 February 2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This paper introduces FedMLSecurity, an end-to-end benchmark designed to simulate
    adversarial attacks and corresponding defense mechanisms in Federated Learning
    (FL). FedMLSecurity comprises two pivotal components: FedMLAttacker, which facilitates
    the simulation of a variety of attacks during FL training, and FedMLDefender,
    which implements defensive mechanisms to counteract these attacks. As an open-source
    library, FedMLSecurity enhances its usability compared to from-scratch implementations
    that focus on specific attack/defense scenarios based on the following features:
    i) It offers extensive customization options to accommodate a broad range of machine
    learning models (e.g., Logistic Regression, ResNet, and GAN) and FL optimizers
    (e.g., FedAVG, FedOPT, and FedNOVA); ii) it enables exploring the variability
    in the effectiveness of attacks and defenses across different datasets and models;
    and iii) it supports flexible configuration and customization through a configuration
    file and some provided APIs. We further demonstrate FedMLSecurity’s utility and
    adaptability through federated training of Large Language Models (LLMs), showcasing
    its potential to impact a wide range of complex applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Federated Learning, security, attack, defense, Federated LLMs^†^†copyright:
    acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make sure
    to enter the correct conference title from your rights confirmation emai; June
    03–05, 2018; Woodstock, NY^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs: Security and
    privacy Distributed systems security^†^†ccs: Computing methodologies Cooperation
    and coordination'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Federated Learning (FL) (McMahan et al., [2017a](#bib.bib60)) facilitates training
    across distributed data and empowers individual clients to utilize their local
    data to collaboratively train machine learning models. Instead of collecting data
    to a centralized server, FL clients train models on their local data and share
    the local models with the FL server, where the local models are aggregated into
    a global model.
  prefs: []
  type: TYPE_NORMAL
- en: 'FL has attracted considerable attention across various domains and has been
    utilized in numerous areas such as next-word prediction (Hard et al., [2018](#bib.bib34);
    Chen et al., [2019](#bib.bib16); Ramaswamy et al., [2019](#bib.bib69)), hot-word
    detection (Leroy et al., [2019](#bib.bib51)), financial risk assessment (Byrd
    and Polychroniadou, [2020](#bib.bib12)), and cancer risk prediction (Chowdhury
    et al., [2022](#bib.bib18)), demonstrating its wide-ranging versatility. Recently,
    FL has found applications in large language models (LLMs) that expand its use
    cases. Referred to as *federated LLMs*, these models utilize FL during pre-training
    and finetuning as well as for prompt engineering (Chen et al., [2023](#bib.bib14)).
    Currently, there are industry products that utilize FL (or distributed training)
    to train LLMs, including Deepspeed ZeRO (Rajbhandari et al., [2020](#bib.bib67);
    Wang et al., [2023](#bib.bib83)), HuggingFace Accelerate (Gugger, [2021](#bib.bib33)),
    Pytorch Lightning Fabric (Antiga, [2023](#bib.bib3)). FL can facilitate LLM training
    due to the following reasons: i) Distributed nature of LLM training data: LLMs
    are pre-trained using large amounts of data, which often reside in different locations.
    Collecting such data to a central server is expensive and may also leak sensitive
    user information, while a viable way is to train LLMs in a federated manner. ii)
    Scalability and efficiency: LLMs, such as GPT-3 (Brown et al., [2020](#bib.bib11)),
    have an extremely large number of parameters. Training LLMs on a single machine
    is infeasible and inflexible, while FL can be a good choice. iii) Continuous improvement
    with user data: LLMs can be deployed in a federated manner and local instances
    of the models can be further finetuned based on the local data, enabling the global
    model to improve over time based on users’ data without ever having direct access
    to that data. This is particularly relevant for privacy-sensitive fields such
    as healthcare or personal communications.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. Types of Defenses and Their Implementations Against Specific Attacks
  prefs: []
  type: TYPE_NORMAL
- en: '| Type of Defenses | Implementations | Type of Attacks Against |'
  prefs: []
  type: TYPE_TB
- en: '| Before-aggregation defenses | SLSGD (Xie et al., [2020](#bib.bib89)) | Data
    poisoning attacks, e.g., label flipping backdoor attack (Tolpegin et al., [2020](#bib.bib81)),
    |'
  prefs: []
  type: TYPE_TB
- en: '| Residual Reweighting Defense (Fu et al., [2019](#bib.bib27)) | Backdoor attacks
    |'
  prefs: []
  type: TYPE_TB
- en: '| Foolsgold (Fung et al., [2020](#bib.bib28)) | Backdoor attacks and Byzantine
    attacks |'
  prefs: []
  type: TYPE_TB
- en: '| Krum (Blanchard et al., [2017](#bib.bib10)) $m$-Krum (Blanchard et al., [2017](#bib.bib10)) CClip (Karimireddy
    et al., [2020](#bib.bib43)) weak DP (Sun et al., [2019](#bib.bib79)) | Model poisoning
    attacks, e.g., Byzantine attacks (Chen et al., [2017](#bib.bib17); Fang et al.,
    [2020](#bib.bib23); Lin et al., [2019](#bib.bib54)) or Backdoor attacks that attack
    by poisoning model updates  (Bagdasaryan et al., [2020](#bib.bib4)) |'
  prefs: []
  type: TYPE_TB
- en: '| Norm Clipping (Sun et al., [2019](#bib.bib79)) Fl-wbc (Sun et al., [2021](#bib.bib78))
    Bulyan Defense(Guerraoui et al., [2018](#bib.bib32)) |'
  prefs: []
  type: TYPE_TB
- en: '| coordinate-wise median (Yin et al., [2018](#bib.bib93)) |'
  prefs: []
  type: TYPE_TB
- en: '| coordinate-wise trimmed mean (Yin et al., [2018](#bib.bib93)) |'
  prefs: []
  type: TYPE_TB
- en: '| On-aggregation defenses | Robust Learning Rate (Ozdayi et al., [2021](#bib.bib65))
    | Backdoor attacks |'
  prefs: []
  type: TYPE_TB
- en: '| SLSGD (Xie et al., [2020](#bib.bib89)) | Data poisoning attacks, e.g., label
    flipping backdoor attack (Tolpegin et al., [2020](#bib.bib81)), |'
  prefs: []
  type: TYPE_TB
- en: '| geometric median (Chen et al., [2017](#bib.bib17)) | Byzantine attacks |'
  prefs: []
  type: TYPE_TB
- en: '| RFA (Pillutla et al., [2022](#bib.bib66)) | Byzantine attacks |'
  prefs: []
  type: TYPE_TB
- en: '| After-aggregation defenses | CClip (Karimireddy et al., [2020](#bib.bib43))
    | Byzantine attacks or backdoor attacks |'
  prefs: []
  type: TYPE_TB
- en: '| CRFL (Xie et al., [2021](#bib.bib88)) | Backdoor attacks |'
  prefs: []
  type: TYPE_TB
- en: Table 2. Attacks Implemented in FedMLSecurity
  prefs: []
  type: TYPE_NORMAL
- en: '| Type of Attacks | Implementations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model poisoning attacks | Byzantine attack (Chen et al., [2017](#bib.bib17);
    Fang et al., [2020](#bib.bib23); Lin et al., [2019](#bib.bib54)): (1) zero mode
    (2) random mode (3) flipping mode |'
  prefs: []
  type: TYPE_TB
- en: '| Minimizing Distance Backdoor Attack (Baruch et al., [2019](#bib.bib5)) |'
  prefs: []
  type: TYPE_TB
- en: '| Model Replacement Backdoor Attack (Bagdasaryan et al., [2020](#bib.bib4))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lazy Worker (or Free Rider) Attack (Wang, [2022](#bib.bib85); Fraboni et al.,
    [2021](#bib.bib26)) |'
  prefs: []
  type: TYPE_TB
- en: '| Data poisoning attacks | Label Flipping Backdoor attack (Tolpegin et al.,
    [2020](#bib.bib81)) |'
  prefs: []
  type: TYPE_TB
- en: '| Edge Case Backdoor Attack (Wang et al., [2020b](#bib.bib84)) |'
  prefs: []
  type: TYPE_TB
- en: '| Data reconstruction attacks | Deep Leakage Attack (Zhu et al., [2019](#bib.bib96))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Inverting Gradient Attack (Geiping et al., [2020](#bib.bib30)) |'
  prefs: []
  type: TYPE_TB
- en: '| Revealing Labels Attack (Dang et al., [2021](#bib.bib19)) |'
  prefs: []
  type: TYPE_TB
- en: FL, as well as federated LLMs, aims to maintain privacy and security of client
    data by allowing clients to train locally without spreading their data to other
    parties. However, its decentralized and collaborative nature might inadvertently
    introduce privacy and security vulnerabilities. Recent works have spotlighted
    specific attack mechanisms in FL. Adversarial clients compromise the integrity
    of global model by submitting spurious models to prevent the global model from
    converging (Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23);
    Lin et al., [2019](#bib.bib54); Baruch et al., [2019](#bib.bib5); Bagdasaryan
    et al., [2020](#bib.bib4); Wang, [2022](#bib.bib85); Fraboni et al., [2021](#bib.bib26))),
    manipulating data samples to induce the global model to mis-classify specific
    samples (Tolpegin et al., [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84)),
    and / or planting backdoors (Baruch et al., [2019](#bib.bib5); Bagdasaryan et al.,
    [2020](#bib.bib4); Tolpegin et al., [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84)).
    Adversaries can also reconstruct private data from shared model updates (Zhu et al.,
    [2019](#bib.bib96); Geiping et al., [2020](#bib.bib30); Dang et al., [2021](#bib.bib19)).
    Meanwhile, a wide range of defense mechanisms has emerged to mitigate the impact
    of these attacks (Li et al., [2022](#bib.bib53); Kumari et al., [2023](#bib.bib46);
    Sun et al., [2019](#bib.bib79); Ozdayi et al., [2021](#bib.bib65); Blanchard et al.,
    [2017](#bib.bib10); Xie et al., [2020](#bib.bib89); Chen et al., [2017](#bib.bib17);
    Sun et al., [2019](#bib.bib79); Karimireddy et al., [2020](#bib.bib43); Yin et al.,
    [2018](#bib.bib93); Pillutla et al., [2022](#bib.bib66); Fung et al., [2020](#bib.bib28);
    Xie et al., [2021](#bib.bib88); Yin et al., [2018](#bib.bib93); Ma et al., [2022](#bib.bib59);
    Kumar et al., [2022](#bib.bib45); Chen et al., [2022](#bib.bib15)). Despite the
    efforts for addressing the vulnerability of FL systems, there still lacks a comprehensive
    benchmark for comparing approaches under unified sittings. Moreover, while existing
    works have explored effectiveness of attacks and defenses on small-scale models,
    there remains a significant gap in understanding how these mechanisms perform
    against large-scale models, such as LLMs. Given that LLMs possess a large number
    of parameters and are trained on complex datasets obtained from unregulated sources,
    the effectiveness of attacks and defenses may be diminished when applied to them.
    These motivate an urgent need for a standardized and comprehensive benchmark to
    evaluate baseline attack and defense mechanisms in the context of FL and federated
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper introduces FedMLSecurity¹¹1Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security,
    a benchmark that simulates attacks and defenses in FedML (He et al., [2020b](#bib.bib36)).
    FedMLSecurity comprises two primary components: FedMLAttacker and FedMLDefender.
    FedMLAttacker simulates attacks in FL to help understand and prepare for potential
    security risks, while FedMLDefender is equipped with state-of-the-arts defense
    mechanisms to counteract the attacks injected by FedMLAttacker. We summarize our
    contributions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'i) Enabling benchmarking of several different attacks and defenses in FL. FedMLSecurity
    implements attacks and defenses that are widely considered in the literature.
    We summarize the defenses and the attacks in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") and Table [2](#S1.T2 "Table 2 ‣ 1\. Introduction ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs"),
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ii) Supporting flexible configuration and customization. FedMLSecurity supports
    configurations using a .yaml file. Sample configurations for attacks and defenses
    are shown in Figures [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1\. Introduction ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs")
    and Figures [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1\. Introduction ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs"),
    respectively. FedMLSecurity also provides APIs to enable customizing attacks and
    defenses.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cec835d3c2efad40814c7254dafcf62e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Byzantine attack (Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56198c310fed42be68fb8720045b2128.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) $m$-Krum (Blanchard et al., [2017](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1. Examples of attack and defense configurations.
  prefs: []
  type: TYPE_NORMAL
- en: iii) Supporting various models and FL optimizers. FedMLSecurity can be utilized
    with a wide range of models, including Logistic Regression, LeNet (LeCun et al.,
    [1998](#bib.bib50)), ResNet (He et al., [2015](#bib.bib38)), CNN (LeCun et al.,
    [1989](#bib.bib49)), RNN (Rumelhart et al., [1986](#bib.bib74)), GAN (Goodfellow
    et al., [2014](#bib.bib31)), and so on. FedMLSecurity is compatible with various
    FL optimizers, such as FedAVG (McMahan et al., [2016](#bib.bib62)), FedSGD (Shokri
    and Shmatikov, [2015](#bib.bib76)), FedOPT (Reddi et al., [2021](#bib.bib71)),
    FedPROX (Li et al., [2020](#bib.bib52)), FedGKT (He et al., [2020a](#bib.bib35)),
    FedGAN (Rasouli et al., [2020](#bib.bib70)), FedNAS (He et al., [2021](#bib.bib37)),
    FedNOVA (Wang et al., [2020a](#bib.bib86)), etc.
  prefs: []
  type: TYPE_NORMAL
- en: iv) Extensions to federated LLMs and real-world applications. FedMLSecurity
    can simulate attacks and defenses during training of federated LLMs. It can also
    be integrated with real-world FL applications; see Exp 7, where we utilize edge
    devices from Theta Network (Theta Network., [2023](#bib.bib80)) instead of simulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key takeaways: i) While defense mechanisms can help mitigate attacks, it might
    also bring a potential loss of accuracy to the aggregation results. Therefore,
    when integrating defenses into FL applications, it’s crucial to weigh the benefits
    against potential drawbacks. ii) Nearly all existing defense mechanisms are impractical
    in real-world FL applications, as they compromise accuracy even if no attack happened.
    A defense that is practical for real-world systems is in need, where the defense
    should satisfy: 1) it must detect if attacks have happened, and only activates
    defensive mechanisms when attacks are detected; and 2) it must identify malicious
    clients accurately without harming benign local models.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Preliminaries and Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section discusses the related literature, then introduces adversarial models,
    and finally overviews FedMLSecurity.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Existing Benchmark Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent years, multiple benchmarks have been introduced for FL (Abadi et al.,
    [2015](#bib.bib2); Ziller et al., [2021](#bib.bib97); Liu et al., [2021](#bib.bib55);
    Beutel et al., [2020](#bib.bib7); Lai et al., [2022](#bib.bib47); Roth et al.,
    [2022](#bib.bib73); Reina et al., [2021](#bib.bib72); Silva et al., [2020](#bib.bib77);
    Ludwig et al., [2020](#bib.bib56); Xie et al., [2022](#bib.bib90); Dimitriadis
    et al., [2022](#bib.bib21)). Among these, only FederatedScope (Xie et al., [2022](#bib.bib90))
    delves into the implications of adversarial attacks in FL, with a focus on data
    reconstruction attacks that utilize models or gradients to revert sensitive information,
    including GAN-based leakage attack (Hitaj et al., [2017](#bib.bib40)), Passive
    Property Inference (Melis et al., [2019](#bib.bib64)), and DLG attack (Zhu et al.,
    [2019](#bib.bib96)). However, FederatedScope neglects to address attacks prevalent
    in the research literature, e.g., Byzantine attacks (Yin et al., [2018](#bib.bib93);
    Yang et al., [2019](#bib.bib92)). It also does not include any defense mechanisms
    for FL. It is worth noting that, while FederatedScope integrates secret-sharing (Beimel,
    [2011](#bib.bib6)), it is in the scope of federated analytics (Elkordy et al.,
    [2023](#bib.bib22); Ramage, [2020](#bib.bib68); Wang et al., [2022b](#bib.bib82);
    Jung et al., [2012](#bib.bib42)), instead of FL.
  prefs: []
  type: TYPE_NORMAL
- en: FedMLSecurity implements attacks that are widely considered in the literature (Chen
    et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23); Lin et al., [2019](#bib.bib54);
    Baruch et al., [2019](#bib.bib5); Bagdasaryan et al., [2020](#bib.bib4); Wang,
    [2022](#bib.bib85); Fraboni et al., [2021](#bib.bib26); Tolpegin et al., [2020](#bib.bib81);
    Wang et al., [2020b](#bib.bib84); Zhu et al., [2019](#bib.bib96); Geiping et al.,
    [2020](#bib.bib30); Dang et al., [2021](#bib.bib19)); it also integrates a wide
    range of defense mechanisms (Sun et al., [2019](#bib.bib79); Ozdayi et al., [2021](#bib.bib65);
    Blanchard et al., [2017](#bib.bib10); Xie et al., [2020](#bib.bib89); Chen et al.,
    [2017](#bib.bib17); Sun et al., [2019](#bib.bib79); Karimireddy et al., [2020](#bib.bib43);
    Yin et al., [2018](#bib.bib93); Pillutla et al., [2022](#bib.bib66); Fung et al.,
    [2020](#bib.bib28); Xie et al., [2021](#bib.bib88); Yin et al., [2018](#bib.bib93)).
    Designed with flexibility in mind, FedMLSecurity offers configurable settings
    and APIs, enabling users to customize their attack and defense mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Adversarial Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adversaries in FL fall into two categories: active and passive, corresponding
    to security risks and privacy threat in FL, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Active Adversaries. Active adversaries intentionally manipulate training data
    or trained models to achieve malicious goals. This might involve altering models
    to prevent global model convergence (e.g., Byzantine attacks (Chen et al., [2017](#bib.bib17);
    Fang et al., [2020](#bib.bib23))), or subtly misclassifying a specific set of
    samples to minimally impact the overall performance of the global model (e.g.,
    backdoor attacks (Bagdasaryan et al., [2020](#bib.bib4); Wang et al., [2020b](#bib.bib84);
    Zhang et al., [2022](#bib.bib95))). Active adversaries can take different forms,
    including: 1) malicious clients who manipulate their local models (Bagdasaryan
    et al., [2020](#bib.bib4); Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23);
    Zhang et al., [2022](#bib.bib95)) or submit contrived models without actual training (Wang,
    [2022](#bib.bib85)); 2) a global “sybil” (Tolpegin et al., [2020](#bib.bib81);
    Fung et al., [2020](#bib.bib28)) that has full access to the FL system and possesses
    complete knowledge of the entire system, including local models and global models
    and clients’ local datasets. This “sybil” may also modify data within the FL system,
    such as clients’ local datasets and their submitted local models; and 3) external
    adversaries capable of monitoring the communication channel between clients and
    the server, thereby intercepting and altering local models during the transfer
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Passive Adversaries. Passive adversaries do not modify data or models, but
    may still pose a threat to data privacy by potentially deducing sensitive information
    (such as local training data) from revealed models (gradients, or model updates) (Zhu
    et al., [2019](#bib.bib96)). Examples of passive adversaries include: 1) an adversarial
    FL server attempting to infer local training data using submitted local models;
    2) adversarial FL clients trying to deduce other clients’ training data using
    the global model provided by the server; and 3) external adversaries, e.g., hackers,
    that access communication channels to acquire local and global models transferred
    between clients and the FL server.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversaries can inject attacks at different stages of FL. Specifically, active
    adversaries can conduct model poisoning attacks that manipulate local models,
    or data poisoning attack that tampers with local datasets. On the other hand,
    passive adversaries pose a privacy threat based on the models updates or gradients,
    i.e., data reconstruction attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d51662c307a198f54de90aa98ddae488.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. FedMLSecurity overview. FedMLSecurity enables injecting attacks /
    defenses (shown in red / green) at various stages of FL at the clients and at
    the server.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Overview of FedMLSecurity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FedMLSecurity serves as an external component that injects attacks and defenses
    at different stages of training without altering the existing processes in existing
    FL benchmarks. FedMLSecurity utilizes FedMLAttacker and FedMLDefender to initiate
    two instances and simulate attacks and defenses, respectively. The two instances
    are initialized once and are accessible by other objects in the FL system²²2Such
    design is achieved by the singleton design pattern (Gamma et al., [1995](#bib.bib29))..
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs: : local models of the current FL round.Variables:  : A FedMLDefender
    instance.1Function       35Function ........10Function ...13Function ...'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Server Aggregation
  prefs: []
  type: TYPE_NORMAL
- en: 'Injection of attacks. Without loss of generality, we classify the attacks in
    FL into the following three categories based on the targets of the attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: i) Data poisoning attacks that are conducted by active adversaries to modify
    clients’ local datasets and are injected at clients (Tolpegin et al., [2020](#bib.bib81);
    Dang et al., [2021](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: ii) Model poisoning attacks that are also conducted by active adversaries to
    temper with local models submitted by clients (Fang et al., [2020](#bib.bib23);
    Shejwalkar and Houmansadr, [2021](#bib.bib75); Bhagoji et al., [2019](#bib.bib8)).
    FedMLAttacker injects these attacks before the aggregation of local models in
    each FL training round at the server, so that it can get access to all client
    models submitted in that training round.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs: : A FedAttacker instance.1Function ...      4$\mathit{send}\_\mathit{to}\_\mathit{server}(\mathbf{w}_{l})$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Client Training
  prefs: []
  type: TYPE_NORMAL
- en: iii) Data reconstruction attacks that are conducted by passive adversaries by
    exploring local models or updates to infer information about the training data (Melis
    et al., [2018](#bib.bib63); Zhang et al., [2020](#bib.bib94); Luo et al., [2021](#bib.bib58);
    Wang et al., [2022a](#bib.bib87); Fowl et al., [2021](#bib.bib25)). FedMLAttacker
    injects such attacks at the FL server, as the FL server has access to all local
    models and the global model of each iteration, and can perform the attacks with
    flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Injection of defenses. FedMLDefender incorporates defenses to mitigate, if
    not completely nullify, the impacts of injected attacks. Since the defenses are
    tailored to address issues related to tampered local models³³3Note that poisoning
    local datasets also results in tampered local models. or information leakage during
    the exchange of model updates between clients and the FL server, the defense mechanisms
    can manipulate local models and the aggregation procedure to counteract the attacks.
    To facilitate this, FedMLDefender deploys defenses at the FL server, and provides
    flexible APIs that enable obtaining all local models and the global model of each
    FL round while allowing for a customized aggregation process. FedMLDefender utilize
    three functions at different stages of FL aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: i) Before-aggregation functions that modify local models at the server before
    aggregating them.
  prefs: []
  type: TYPE_NORMAL
- en: ii) On-aggregation functions that modify the FL aggregation function to mitigate
    the impacts of malicious local models.
  prefs: []
  type: TYPE_NORMAL
- en: iii) After-aggregation functions that modify the aggregated global model (e.g.,
    by adding noise or clipping) to protect the real global model or improve its quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.2\. Adversarial Model ‣ 2\. Preliminaries and
    Overview ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning
    and Federated LLMs") summarizes the injections of attacks and defenses to the
    FL framework in FedMLSecurity. We also provide detailed algorithms for injecting
    attacks and defenses to different stages of FL training, as shown in Algorithm [1](#alg1
    "In 2.3\. Overview of FedMLSecurity ‣ 2\. Preliminaries and Overview ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs")
    (for server aggregation) and Algorithm [2](#alg2 "In 2.3\. Overview of FedMLSecurity
    ‣ 2\. Preliminaries and Overview ‣ FedMLSecurity: A Benchmark for Attacks and
    Defenses in Federated Learning and Federated LLMs") (for client training). Below,
    we explain the implementations of attacks and defenses in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implementation of Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FedMLAttacker injects model poisoning, data poisoning, and data reconstruction
    attacks at different stages of FL training and provides APIs for these attacks.
    We present each class of attacks and user integration of a new attack to FedMLSecurity.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Model Poisoning Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model poisoning attacks modify the local models submitted by clients. FedMLAttacker
    injects such attacks before FL aggregation in each iteration, modifying each local
    model directly. As an example, FedMLAttacker implements three modes of Byzantine
    attacks (Yin et al., [2018](#bib.bib93); Yang et al., [2019](#bib.bib92); Lin
    et al., [2019](#bib.bib54); Xu et al., [2022](#bib.bib91)), including i) Zero
    mode (Lin et al., [2019](#bib.bib54)) that poisons the client models by setting
    their weights to zero; ii) Random mode (Lin et al., [2019](#bib.bib54)) that manipulates
    client models by attributing random values to model weights; and iii) Flipping
    mode (Xu et al., [2022](#bib.bib91)) that updates the global model in the opposite
    direction by formulating a poisoned local model based on the global model  as
    $\textbf{w}_{g}+(\textbf{w}_{g}-\textbf{w}_{\ell})$.
  prefs: []
  type: TYPE_NORMAL
- en: APIs for Model Poisoning Attacks. FedMLAttacker has two APIs for model poisoning
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: is a list of tuples containing the number of data samples and the submitted
    client models. The input $\mathit{auxiliary}\_\mathit{info}$ is any information
    used in the defense, e.g., the global model in the last FL iteration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{is}\_\mathit{model}\_\mathit{poisoning}\_\mathit{attack}()$ that checks
    whether the attack is activated and whether the attack modifies local models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2\. Data Poisoning Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data poisoning attacks modify local datasets of one or multiple clients to achieve
    some malicious goals, e.g., degrading the performance of the global model or inducing
    the global model to misclassify some samples. As an example, in label flipping
    attack (Tolpegin et al., [2020](#bib.bib81)), a global “sybil” controls some clients
    and modifies their local data by mislabeling samples of some classes to wrong
    classes. Given a source class (or label) , all samples with class .
  prefs: []
  type: TYPE_NORMAL
- en: APIs for Data Poisoning Attacks. While poisoning local data can be performed
    by either a global “sybil” or individual malicious clients, to address a more
    general case, FedMLAttacker design APIs for the global sybil to enable enhanced
    control over each local dataset. FedMLAttacker has two APIs for data poisoning
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{poison}\_\mathit{data}(\mathit{dataset})$, which takes a local dataset
    and mislabels a set of chosen samples based on the clients’ (or attackers’) requirements,
    which are included in the configuration. Normally, clients would change labels
    of a specific subset of samples to some other labels in the same dataset, or label
    a set of samples to new classes that do not exist in the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{is}\_\mathit{data}\_\mathit{poisoning}\_\mathit{attack}()$, which examines
    whether FedMLAttacker is enabled and whether the attack requires poisoning the
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3\. Data Reconstruction Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data reconstruction attacks are performed by passive adversaries that attempts
    to infer sensitive information without actively interfering with the FL training
    or the local data. We assume that there is no leakage during the local training
    process in FL, as clients are on their fully trusted local machines. Thus, data
    reconstruction attacks take the trained models (either the global model or the
    local models) to revert training data. For example, Deep Leakage from Gradients
    (DLG) attack (Zhu et al., [2019](#bib.bib96)) infers local training data from
    the publicly shared gradients. A passive adversary can use the global model from
    the previous FL training round and the newly obtained model to compute a “model
    update” between models in different FL training rounds to deduce the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: APIs for Data Reconstruction Attacks. We have two APIs for data reconstruction
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ) to help infer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{is}\_\mathit{data}\_\mathit{reconstruction}\_\mathit{attack}()$, which
    examines whether the attack component is enabled and whether the attack requires
    reconstructing training data using the trained models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.4\. Integration of a New Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To customize a new attack, users should follow these steps: i) determine the
    type of the attack, i.e., model poisoning, data poisoning, or data reconstruction;
    ii) create a new class for the attack and implement functions using the APIs,
    e.g., , and , within the FedMLAttacker class to ensure that the injected attacks
    are activated at the proper stages of FL training.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Implementation of Defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FedMLDefender injects defense functions at different stages of FL aggregation
    at the server. Based on the point of injection, FedMLDefender provides three types
    of functions to support defense mechanisms, including 1) before-aggregation, 2)
    on-aggregation, and 3) after-aggregation. Note that a defense may inject functions
    at one or multiple stages of FL aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Before-Aggregation Defenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before-aggregation functions operate on local models at the FL server before
    aggregating the local models at the server. We use Krum (Blanchard et al., [2017](#bib.bib10))
    as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Krum. Krum (Blanchard et al., [2017](#bib.bib10)) tolerates  clients by retaining
    only one local model that is the most likely to be benign as the global model.
    That is, Krum selects a single model as the global model in aggregation. A generalization
    of Krum is  client models with the  clients to be malicious.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Models and datasets for evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | ResNet20 (He et al., [2016](#bib.bib39)) | ResNet56 (He et al., [2016](#bib.bib39))
    | CNN (McMahan et al., [2017a](#bib.bib60)) | RNN (bi-LSTM) (McMahan et al., [2017a](#bib.bib60))
    | BERT (Devlin et al., [2018](#bib.bib20)) | Pythia-1B (Biderman et al., [2023](#bib.bib9))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | CIFAR10 (Krizhevsky et al., [2009](#bib.bib44)) | CIFAR100 (Krizhevsky
    et al., [2009](#bib.bib44)) | FEMNIST (Caldas et al., [2018](#bib.bib13)) | Shakespeare (McMahan
    et al., [2017b](#bib.bib61)) | 20News (Lang, [1995](#bib.bib48)) | PubMedQA (Luo
    et al., [2022](#bib.bib57)) |'
  prefs: []
  type: TYPE_TB
- en: 'APIs for before-aggregation functions. We provide two APIs for before-aggregation
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: can be any information that is utilized in the defense functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{is}\_\mathit{defense}\_\mathit{before}\_\mathit{aggregation}()$, which
    checks whether the FedMLDefender is activated and whether the defense requires
    injecting functions before aggregating local models at the server.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2\. On-Aggregation Defenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On-aggregation defense functions modify the aggregation function to a robust
    version that tolerates or mitigates impacts of the potential adversarial client
    models. As an example, RFA (Robust Federated Aggregation) (Pillutla et al., [2022](#bib.bib66))
    computes a geometric median of the client models in each iteration as the aggregated
    model, instead of simply averaging the client models. RFA defense effectively
    mitigates the impact of poisoned client models, as the geometric median can represent
    the central tendency of the client models, and the median point is chosen in a
    way to minimize the sum of distances between that point and the other client models
    of the current FL iteration. In practice, the geometric median is calculated using
    the Smoothed Weiszfeld Algorithm (Pillutla et al., [2022](#bib.bib66)).
  prefs: []
  type: TYPE_NORMAL
- en: 'APIs for on-aggregation defenses. We provide two APIs for on-aggregation defense
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{defend}\_\mathit{on}\_\mathit{aggregation}(\mathit{local}\_\mathit{models},\mathit{auxiliary}\_\mathit{info})$,
    which takes the local models of the current training round for aggregation. The
    input local_models is a list of tuples that contain the number of samples and
    the local model submitted by each client in the current FL iteration. The input
    auxiliary_info can include any information required by the defense functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{is}\_\mathit{defense}\_\mathit{on}\_\mathit{aggregation}()$, which
    checks if the defense component is enabled and whether the current defense requires
    the injection of functions during aggregation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3\. After-Aggregation Defense
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After-aggregation defense functions modify the aggregation result, i.e., the
    global model, of each FL iteration to mitigate the effects of poisoned local models
    or protect the global model from potential adversaries. As an example, CRFL (Xie
    et al., [2021](#bib.bib88)) clips the global model to bound the norm of the model
    each time after aggregation at the FL server. The FL server then adds Gaussian
    noise to the clipped global model before distributing the global model to the
    clients for the next FL iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'APIs for After-Aggregation Defenses. We provide two APIs to support after-aggregation
    defenses:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{defend}\_\mathit{after}\_\mathit{aggregation}(\mathit{global}\_\mathit{model})$,
    which directly modifies the global model after aggregation using methods such
    as clipping or adding noise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathit{is}\_\mathit{defense}\_\mathit{after}\_\mathit{aggregation}()$, which
    checks if the defense component is activated and whether the current defense requires
    injecting functions after aggregation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4\. Integration of a New Defense
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement a self-designed defense mechanism, users should first determine
    the stages to inject the defense functions (i.e., before/on/after-aggregation),
    add a class for the new defense and implement the corresponding defense functions
    using the aforementioned APIs, i.e., , and  for the defense class, and include
    the name of the defense in $\mathit{is}\_\mathit{defense}\_\mathit{after}\_\mathit{aggregation}()$.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section presents a comprehensive evaluation of FedMLSecurity to benchmark
    some well-known attacks and defenses in FL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental setting. A summary of datasets and models for evaluations can
    be found in Table [3](#S4.T3 "Table 3 ‣ 4.1\. Before-Aggregation Defenses ‣ 4\.
    Implementation of Defenses ‣ FedMLSecurity: A Benchmark for Attacks and Defenses
    in Federated Learning and Federated LLMs"). We utilize FedAVG in our experiments.
    By default, we employ ResNet20 and the non-i.i.d. CIFAR10 dataset (partition parameter
    $\alpha=0.5$), as the non-i.i.d. setting closely captures real-world scenarios.
    We further extend our evaluations to i.i.d. cases and various other models and
    datasets. For evaluations on LLMs, we utilize FedLLM (FedML Inc., [2023](#bib.bib24))
    that trains LLMs in a federated manner. We employ the Pythia-1B model (Biderman
    et al., [2023](#bib.bib9)) and PubMedQA (Jin et al., [2019](#bib.bib41)), a non-i.i.d.
    biomedical research dataset that contains 212,269 questions for question answering.
    We utilize the “artificial” subset for training and the “labelled” subset for
    testing. Evaluations are conducted on a server with 8 NVIDIA A100-SXM4-80GB GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, we use 10 clients for FL training, corresponding to real-world
    FL applications where the number of clients is typically less than 10, especially
    in ToB scenarios. We also increase the number of clients to 100 in Exp 5, and
    set the number of clients to 70 in the real-world experiment, where we utilize
    edge devices from the Theta network (Theta Network., [2023](#bib.bib80)) to showcase
    the scalability of our library (Exp 10). Unless otherwise noted, we set the percentage
    of malicious clients to 10%, and evaluate results with the accuracy of the global
    model. We employ three attack mechanisms, including label flipping (Tolpegin et al.,
    [2020](#bib.bib81)) and Byzantine attacks of random mode and flipping mode (Chen
    et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23); Xu et al., [2022](#bib.bib91)).
    For the label flipping attack, we set the attack to modify the local and test
    data labels of malicious clients from label 3 to label 9 and label 2 to label
    1\. We utilize three defense mechanisms: -Krum, we set $m$ to 5, which means 5
    out of 10 submitted local models participate in aggregation in each training round.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Evaluations on FL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac9981c610f587bd58a43f8328c42965.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3. Attack comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/321919c09fef3dc557e720e65a9b4024.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4. Defense comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/89a76efc54f98f34278df95672ddb8d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5. Label flipping exps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/624c93697a867d9e03ac540a0fb648de.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. Random-Byzantine exps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bdb468ed91ae725166aaca884421d1f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7. I.I.D. data evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddee8af530f572bb9acd9814eacb2392.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8. Scale # clients to 100.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1806b0688e8402aa7862995b09a43d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9. ResNet56 (CV).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52f21b47ecc8ce62dc8e06cc4627ee15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10. RNN (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/536c076ce730b3727f320e0df6989da5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11. CNN (CV).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5fd131d62de9a1b308d25e6b30003285.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12. Varying # adversaries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 1: Attack Comparisons. We evaluate the impact of attacks on test accuracy,
    using a no-attack scenario as a baseline. As illustrated in Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"), Byzantine
    attacks, specifically in the random and zero modes, substantially degrade accuracy.
    In contrast, the label flipping attack and the flipping mode of the Byzantine
    attack show a milder impact on accuracy. This can be attributed to the nature
    of Byzantine attacks, where Byzantine attackers would prevent the global model
    from converging, especially for the random mode that generates weights for models
    arbitrarily, causing the most significant deviation from the benign local model.
    In subsequent experiments, unless specified otherwise, we employ the Byzantine
    attack in the random mode as the default attack, as it provides the strongest
    impact compared with the other three attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 2: Defense Comparisons. We investigate potential impact of defense mechanisms
    on accuracy in the absence of attacks, i.e., whether defense mechanisms inadvertently
    degrade accuracy when all clients are benign. We incorporate a scenario without
    any defense or attack as our baseline. As illustrated in Figure [6](#S5.F6 "Figure
    6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs"), it becomes evident
    that when all clients are benign, involving defense strategies to FL training
    might lead to a reduction in accuracy. This decrease might arise from several
    factors: the exclusion of some benign local models from aggregation, e.g., as
    in $m$-Krum, adjustments to the aggregation function, e.g., as in RFA, or re-weighting
    local models, e.g., as in Foolsgold. Specifically, the RFA defense mechanism significantly
    impacts accuracy as it computes a geometric median of the local models instead
    of leveraging the original FedAVG optimizer, which introduces a degradation in
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb21e9fe9e7a5a9e4f4ba9605e9295f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13. BERT evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5abaaa5b4079a52b3f6e234558e0622.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14. Pythia-1B evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5489192cc28dd129564cbb26601b03d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15. Real-world application evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 3: Evaluations of defense mechanisms against activated attacks. This experiment
    evaluates the effect of defense mechanisms against some attacks. We include two
    baseline scenarios: 1) an “original attack” scenario with an activated attack
    without any defense in place, and 2) a “benign” scenario with no activated attack
    or defense. We select label flipping attack and the random mode of Byzantine attack
    based on their impacts in Exp1, where label flipping has the least impact and
    the random mode of Byzantine attack exhibits the largest impact, as shown in Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"). Results for
    the label flipping and the random mode of Byzantine attacks are in Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs") and Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"), respectively.
    These results indicate that the defenses may contribute to minor improvements
    in accuracy for low-impact attacks, e.g., Foolsgold in Figure [6](#S5.F6 "Figure
    6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs"). In certain cases,
    it is noteworthy that the defensive mechanisms may inadvertently compromise accuracy,
    such as the case with RFA in Figure [6](#S5.F6 "Figure 6 ‣ 5.1\. Evaluations on
    FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in
    Federated Learning and Federated LLMs"). For high-impact attacks, such as the
    Byzantine attack of the random mode, Krum exhibits resilience, effectively neutralizing
    the negative impact of the attacks, as shown in Figure [6](#S5.F6 "Figure 6 ‣
    5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks
    and Defenses in Federated Learning and Federated LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 4: Evaluations on i.i.d. data. We select the random mode of the Byzantine
    attack, and employ Foolsgold, ), and RFA to counteract the adverse effects of
    this attack. As shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on
    FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in
    Federated Learning and Federated LLMs"), $m$-Krum is the most effective one among
    all the defense mechanisms, where the test accuracy is close to the case where
    all the FL clients are honest, i.e., no attack scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 5: Scaling the number of clients to 100. This experiment scales the number
    of clients to 100 and evaluates the defense mechanisms against the random mode
    of the Byzantine attack. We employ Foolsgold, ), and RFA to counteract the adverse
    effects of this attack. As shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations
    on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses
    in Federated Learning and Federated LLMs"), $m$-Krum is the most effective one
    among all the defense mechanisms, and the test accuracy is very close to the case
    where no attack happens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 6: Evaluations on different models. We evaluate defense mechanisms against
    the random mode of the Byzantine attack with different models and datasets, including:
    i) ResNet56 + CIFAR100, ii) RNN + Shakespeare, and iii) CNN + FEMNIST. The results
    are shown in Figures [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs"), [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs"), and [12](#S5.F12 "Figure 12 ‣ 5.1\. Evaluations on FL ‣ 5\.
    Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated
    Learning and Federated LLMs"), respectively. The results show that while the defense
    mechanisms can mitigate the impact of attacks in most cases, some attacks may
    fail some tasks, e.g., $m$-Krum fails RNN in Figure [10](#S5.F10 "Figure 10 ‣
    5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks
    and Defenses in Federated Learning and Federated LLMs"), and Foolsgold fails CNN
    in Figure [12](#S5.F12 "Figure 12 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs"). This is because the two defense mechanisms either select several
    local models for aggregation in each FL training round, or significantly re-weight
    the local models, which may eliminate some local models that are important to
    the aggregation in the first several FL training iterations, leading to unchanged
    test accuracy in later FL iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 7: Varying the number of malicious clients. This experiment evaluates the
    impact of varying numbers of malicious clients on test accuracy. We utilize -Krum
    selects a local model that is the most likely to be benign to represent the other
    models, effectively minimizing the impact of malicious client models on the aggregation.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Evaluations on Federated LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ two LLMs, BERT (Devlin et al., [2018](#bib.bib20)) and Pythia (Biderman
    et al., [2023](#bib.bib9)), to showcase the scalability of FedMLSecurity and its
    applicability to federated LLM scenarios. We notice that some defenses (e.g.,
    Foolsgold (Fung et al., [2020](#bib.bib28))) that require memorizing intermediate
    results, such as models of previous FL training rounds, might encounter limitations
    when integrated with LLMs due to the significant cache introduced. Considering
    this, we utilize $m$-Krum for our experiments, as it does not require storing
    intermediate results and demonstrates consistent performance in most of our previous
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 8: Evaluations of Krum against model replacement backdoor attack on BERT.
    This experiment utilizes BERT (Devlin et al., [2018](#bib.bib20)) and the 20 news
    dataset (Lang, [1995](#bib.bib48)) for a classification task. We employ 10 clients
    and set 1 client to be malicious in each FL training round. We set -Krum, i.e.,
    5 out of 10 local models participate in aggregation in each FL training round.
    Results in Figure [15](#S5.F15 "Figure 15 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") show that $m$-Krum effectively mitigates the adversarial effect,
    bringing the accuracy closer to the level of the attack-free case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp 9: Evaluations of Krum against the Byzantine attack on Pythia-1B. We employ
    7 clients for FL training, and 1 out of 7 clients is malicious in each round of
    FL training. We set the -Krum to 2, signifying that 2 out of 7 submitted local
    models participate in the aggregation in each FL training round. The performance
    is evaluated with the test loss. Results in Figure [15](#S5.F15 "Figure 15 ‣ 5.1\.
    Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and
    Defenses in Federated Learning and Federated LLMs") show that Byzantine attack
    significantly increases the test loss during training. Nevertheless, $m$-Krum
    effectively mitigates the adversarial effect.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Evaluation in Real-World Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To demonstrate the scalability of our benchmark, we include an experiment using
    real-world devices, instead of simulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exp10: Evaluations in real-world applications. We utilize edge devices from
    the Theta network (Theta Network., [2023](#bib.bib80)) to validate the scalability
    of FedMLSecurity to real-world applications. The FL client package is integrated
    into Theta’s edge nodes, which periodically fetches data from the Theta back-end.
    Subsequently, the FL training platform capitalizes on these Theta edge nodes and
    their associated data to train, fine-tune, and deploy machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We select -Krum, we set -Krum mitigates the adversarial effect of the random-mode
    Byzantine attack. We also include a screenshot of the platform, as shown in Figure [16](#A1.F16
    "Figure 16 ‣ Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs") in Appendix
    §[A](#A1 "Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs") for the FL training
    process and Figure [17](#A1.F17 "Figure 17 ‣ Appendix A Supplementary Experiment
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") in Appendix §[A](#A1 "Appendix A Supplementary Experiment ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs")
    for the training status of each device.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper presents FedMLSecurity, a library designed to demonstrate potential
    adversarial attacks and corresponding defense strategies in FL to bolster innovation
    in the secure FL domain. FedMLSecurity contains two components: FedMLAttacker
    that simulates various attacks that can be injected during FL training, and FedMLDefender,
    which facilitates defense strategies to mitigate the impacts of these attacks.
    FedMLSecurity is open-sourced, and we welcome contributions from the research
    community to enrich the benchmark repository with novel attack and defense strategies
    to foster a diverse, comprehensive, and robust foundation for ongoing research
    in FL security.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. (2015) Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
    Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu
    Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
    Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
    Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
    Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
    Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
    Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale
    Machine Learning on Heterogeneous Systems. [https://www.tensorflow.org/](https://www.tensorflow.org/)
    Software available from tensorflow.org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Antiga (2023) Luca Antiga. 2023. Introducing PyTorch Lightning 2.0 and Fabric.
    *https://lightning.ai/blog/introducing-lightning-2-0/* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagdasaryan et al. (2020) Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah
    Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning. In *International
    Conference on Artificial Intelligence and Statistics*. PMLR, 2938–2948.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baruch et al. (2019) Gilad Baruch, Moran Baruch, and Yoav Goldberg. 2019. A
    little is enough: Circumventing defenses for distributed learning. *Advances in
    Neural Information Processing Systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beimel (2011) Amos Beimel. 2011. Secret-sharing schemes: A survey. In *International
    conference on coding and cryptology*. Springer, 11–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beutel et al. (2020) Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu,
    Titouan Parcollet, Pedro PB de Gusmão, and Nicholas D Lane. 2020. Flower: A friendly
    federated learning research framework. *arXiv preprint arXiv:2007.14390* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhagoji et al. (2019) Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal,
    and Seraphin Calo. 2019. Analyzing federated learning through an adversarial lens.
    In *International Conference on Machine Learning*. PMLR, 634–643.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large
    language models across training and scaling. *arXiv preprint arXiv:2304.01373*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blanchard et al. (2017) Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui,
    and Julien Stainer. 2017. Machine learning with adversaries: Byzantine tolerant
    gradient descent. *Advances in neural information processing systems* 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Byrd and Polychroniadou (2020) David Byrd and Antigoni Polychroniadou. 2020.
    Differentially private secure multi-party computation for federated learning in
    financial applications. In *Proceedings of the First ACM International Conference
    on AI in Finance*. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caldas et al. (2018) Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian
    Li, Jakub Konečnỳ, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018.
    Leaf: A benchmark for federated settings. *arXiv preprint arXiv:1812.01097* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, and
    Xiaolin Zheng. 2023. Federated Large Language Model : A Position Paper. *arXiv
    preprint arXiv:2307.08925* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Jiahui Chen, Yi Zhao, Qi Li, Xuewei Feng, and Ke Xu. 2022.
    FedDef: Defense Against Gradient Leakage in Federated Learning-Based Network Intrusion
    Detection Systems. *IEEE Transactions on Information Forensics and Security* 18
    (2022), 4561–4576. [https://api.semanticscholar.org/CorpusID:253420565](https://api.semanticscholar.org/CorpusID:253420565)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019) Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays.
    2019. Federated learning of out-of-vocabulary words. *arXiv preprint arXiv:1903.10635*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017) Y. Chen, L. Su, and J. Xu. 2017. Distributed statistical
    machine learning in adversarial settings: Byzantine gradient descent. *ACM on
    Measurement and Analysis of Computing Systems* 1, 2 (2017), 1–25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhury et al. (2022) Alexander Chowdhury, Hasan Kassem, Nicolas Padoy, Renato
    Umeton, and Alexandros Karargyris. 2022. A review of medical federated learning:
    Applications in oncology and cancer research. In *Brainlesion: Glioma, Multiple
    Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes
    2021, Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021,
    Revised Selected Papers, Part I*. Springer, 3–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dang et al. (2021) Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews,
    Peter Chin, and Françoise Beaufays. 2021. Revealing and protecting labels in distributed
    training. *Advances in Neural Information Processing Systems* 34 (2021), 1727–1738.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimitriadis et al. (2022) Dimitrios Dimitriadis, Mirian Hipolito Garcia, Daniel Madrigal
    Diaz, Andre Manoel, and Robert Sim. 2022. Flute: A scalable, extensible framework
    for high-performance federated learning simulations. *arXiv preprint arXiv:2203.13789*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elkordy et al. (2023) Ahmed Roushdy Elkordy, Yahya H Ezzeldin, Shanshan Han,
    Shantanu Sharma, Chaoyang He, Sharad Mehrotra, Salman Avestimehr, et al. 2023.
    Federated analytics: A survey. *APSIPA Transactions on Signal and Information
    Processing* 12, 1 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2020) Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020.
    Local model poisoning attacks to  federated learning. In *29th USENIX security
    symposium (USENIX Security 20)*. 1605–1622.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FedML Inc. (2023) FedML Inc. 2023. Releasing FedLLM: Build Your Own Large Language
    Models on Proprietary Data using the FedML Platform. https://blog.fedml.ai/releasing-fedllm-build-your-own-large-language-models-on-proprietary-data-using-the-fedml-platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fowl et al. (2021) Liam Fowl, Jonas Geiping, Wojtek Czaja, Micah Goldblum,
    and Tom Goldstein. 2021. Robbing the fed: Directly obtaining private data in federated
    learning with modified models. *arXiv preprint arXiv:2110.13057* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraboni et al. (2021) Yann Fraboni, Richard Vidal, and Marco Lorenzi. 2021.
    Free-rider attacks on model aggregation in federated learning. In *International
    Conference on Artificial Intelligence and Statistics*. PMLR, 1846–1854.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2019) Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen. 2019. Attack-resistant
    federated learning with residual-based reweighting. *arXiv preprint arXiv:1912.11464*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fung et al. (2020) Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2020.
    The Limitations of Federated Learning in Sybil Settings.. In *RAID*. 301–316.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gamma et al. (1995) Erich Gamma, Richard Helm, Ralph Johnson, Ralph E Johnson,
    and John Vlissides. 1995. *Design patterns: elements of reusable object-oriented
    software*. Pearson Deutschland GmbH.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geiping et al. (2020) Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and
    Michael Moeller. 2020. Inverting gradients-how easy is it to break privacy in
    federated learning? *Advances in Neural Information Processing Systems* 33 (2020),
    16937–16947.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative Adversarial Nets. In *NIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guerraoui et al. (2018) Rachid Guerraoui, Sébastien Rouault, et al. 2018. The
    hidden vulnerability of distributed learning in byzantium. In *International Conference
    on Machine Learning*. PMLR, 3521–3530.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gugger (2021) Sylvain Gugger. 2021. Introducing Hugging Face Accelerate. https://huggingface.co/blog/accelerate-library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard et al. (2018) Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy,
    Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel
    Ramage. 2018. Federated learning for mobile keyboard prediction. *arXiv preprint
    arXiv:1811.03604* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020a) Chaoyang He, Murali Annavaram, and Salman Avestimehr. 2020a.
    Group knowledge transfer: Federated learning of large cnns at the edge. *Advances
    in Neural Information Processing Systems* 33 (2020), 14068–14080.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020b) Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang,
    Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.
    2020b. FedML: A research library and benchmark for federated machine learning.
    *arXiv preprint arXiv:2007.13518* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) Chaoyang He, Erum Mushtaq, Jie Ding, and Salman Avestimehr.
    2021. Fednas: Federated deep learning via neural architecture search. (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2015) Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep
    Residual Learning for Image Recognition. *2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)* (2015), 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hitaj et al. (2017) Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.
    2017. Deep models under the GAN: information leakage from collaborative deep learning.
    In *Proceedings of the 2017 ACM SIGSAC conference on computer and communications
    security*. 603–618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and
    Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*. 2567–2577.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jung et al. (2012) Gueyoung Jung, Nathan Gnanasambandam, and Tridib Mukherjee.
    2012. Synchronous Parallel Processing of Big-Data Analytics Services to Optimize
    Performance in Federated Clouds. *2012 IEEE Fifth International Conference on
    Cloud Computing* (2012), 811–818.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karimireddy et al. (2020) Sai Praneeth Karimireddy, Lie He, and Martin Jaggi.
    2020. Byzantine-robust learning on heterogeneous datasets via bucketing. *arXiv
    preprint arXiv:2006.09365* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    multiple layers of features from tiny images. (2009).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. (2022) Abhishek Kumar, Vivek Khimani, Dimitris Chatzopoulos, and
    Pan Hui. 2022. FedClean: A Defense Mechanism against Parameter Poisoning Attacks
    in Federated Learning. *ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)* (2022), 4333–4337. [https://api.semanticscholar.org/CorpusID:249437417](https://api.semanticscholar.org/CorpusID:249437417)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumari et al. (2023) Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza
    Jadliwala, and Ahmad-Reza Sadeghi. 2023. BayBFed: Bayesian Backdoor Defense for
    Federated Learning. *arXiv preprint arXiv:2301.09508* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2022) Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng
    Zhu, Harsha Madhyastha, and Mosharaf Chowdhury. 2022. FedScale: Benchmarking model
    and system performance of federated learning at scale. In *International Conference
    on Machine Learning*. PMLR, 11814–11827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lang (1995) Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. In *Machine
    Learning Proceedings 1995*, Armand Prieditis and Stuart Russell (Eds.). Morgan
    Kaufmann, San Francisco (CA), 331–339. [https://doi.org/10.1016/B978-1-55860-377-6.50048-7](https://doi.org/10.1016/B978-1-55860-377-6.50048-7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation
    applied to handwritten zip code recognition. *Neural computation* 1, 4 (1989),
    541–551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leroy et al. (2019) David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht,
    and Joseph Dureau. 2019. Federated learning for keyword spotting. In *IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. 6341–6345.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet
    Talwalkar, and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
    *Proceedings of Machine learning and systems* 2 (2020), 429–450.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Xingyu Li, Zhe Qu, Shangqing Zhao, Bo Tang, Zhuo Lu, and Yao-Hong
    Liu. 2022. LoMar: A Local Defense Against Poisoning Attack on Federated Learning.
    *IEEE Transactions on Dependable and Secure Computing* 20 (2022), 437–450. [https://api.semanticscholar.org/CorpusID:245837821](https://api.semanticscholar.org/CorpusID:245837821)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2019) Jierui Lin, Min Du, and Jian Liu. 2019. Free-riders in federated
    learning: Attacks and defenses. *arXiv preprint arXiv:1911.12560* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang.
    2021. Fate: An industrial grade platform for collaborative learning with data
    protection. *The Journal of Machine Learning Research* 22, 1 (2021), 10320–10325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ludwig et al. (2020) Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou,
    Ali Anwar, Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu
    Sinn, et al. 2020. IBM Federated Learning: An Enterprise Framework White Paper
    v0.1. *arXiv preprint arXiv:2007.10987* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2022) Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang,
    Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer
    for biomedical text generation and mining. *Briefings in Bioinformatics* 23, 6
    (09 2022). [https://doi.org/10.1093/bib/bbac409](https://doi.org/10.1093/bib/bbac409)
    arXiv:https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf
    bbac409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2021) Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, and Beng Chin Ooi.
    2021. Feature inference attack on model predictions in vertical federated learning.
    In *IEEE International Conference on Data Engineering (ICDE)*. IEEE, 181–192.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2022) Zhuo Ma, Jianfeng Ma, Yinbin Miao, Yingjiu Li, and Robert H.
    Deng. 2022. ShieldFL: Mitigating Model Poisoning Attacks in Privacy-Preserving
    Federated Learning. *IEEE Transactions on Information Forensics and Security*
    17 (2022), 1639–1654. [https://api.semanticscholar.org/CorpusID:248358657](https://api.semanticscholar.org/CorpusID:248358657)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McMahan et al. (2017a) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017a. Communication-efficient learning of deep networks
    from decentralized data. In *Artificial intelligence and statistics*. PMLR, 1273–1282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McMahan et al. (2017b) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017b. Communication-efficient learning of deep networks
    from decentralized data. In *Artificial intelligence and statistics*. PMLR, 1273–1282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McMahan et al. (2016) H. B. McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Agüera y Arcas. 2016. Communication-Efficient Learning of Deep Networks
    from Decentralized Data. In *International Conference on Artificial Intelligence
    and Statistics*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Melis et al. (2018) Luca Melis, Congzheng Song, Emiliano De Cristofaro, and
    Vitaly Shmatikov. 2018. Exploiting Unintended Feature Leakage in Collaborative
    Learning. *2019 IEEE Symposium on Security and Privacy (SP)* (2018), 691–706.
    [https://api.semanticscholar.org/CorpusID:53099247](https://api.semanticscholar.org/CorpusID:53099247)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Melis et al. (2019) Luca Melis, Congzheng Song, Emiliano De Cristofaro, and
    Vitaly Shmatikov. 2019. Exploiting unintended feature leakage in collaborative
    learning. In *2019 IEEE symposium on security and privacy (SP)*. IEEE, 691–706.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ozdayi et al. (2021) Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel.
    2021. Defending against backdoors in federated learning with robust learning rate.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 35\.
    9268–9276.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pillutla et al. (2022) Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui.
    2022. Robust aggregation for federated learning. *IEEE Transactions on Signal
    Processing* 70 (2022), 1142–1154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*. IEEE, 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramage (2020) Daniel Ramage. 2020. Federated Analytics: Collaborative Data
    Science Without Data Collection. *Google AI Blog* (May 2020). [https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramaswamy et al. (2019) Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and
    Françoise Beaufays. 2019. Federated learning for emoji prediction in a mobile
    keyboard. *arXiv preprint arXiv:1906.04329* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasouli et al. (2020) Mohammad Rasouli, Tao Sun, and Ram Rajagopal. 2020. Fedgan:
    Federated generative adversarial networks for distributed data. *arXiv preprint
    arXiv:2006.07228* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reddi et al. (2021) Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary
    Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. 2021.
    Adaptive Federated Optimization. In *International Conference on Learning Representations*.
    [https://openreview.net/forum?id=LkFG3lB13U5](https://openreview.net/forum?id=LkFG3lB13U5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reina et al. (2021) G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina,
    Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr Mokrov,
    Dmitry Agapov, et al. 2021. OpenFL: An open-source framework for Federated Learning.
    *arXiv preprint arXiv:2105.06413* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roth et al. (2022) Holger R Roth, Yan Cheng, Yuhong Wen, Isaac Yang, Ziyue
    Xu, Yuan-Ting Hsieh, Kristopher Kersten, Ahmed Harouni, Can Zhao, Kevin Lu, et al.
    2022. NVIDIA FLARE: Federated Learning from Simulation to Real-World. *arXiv preprint
    arXiv:2210.13291* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shejwalkar and Houmansadr (2021) Virat Shejwalkar and Amir Houmansadr. 2021.
    Manipulating the byzantine: Optimizing model poisoning attacks and defenses for
    federated learning. In *NDSS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shokri and Shmatikov (2015) Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving
    deep learning. In *Proceedings of the 22nd ACM SIGSAC conference on computer and
    communications security*. 1310–1321.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silva et al. (2020) Santiago Silva, Andre Altmann, Boris Gutman, and Marco
    Lorenzi. 2020. Fed-BioMed: A General Open-Source Frontend Framework for Federated
    Learning in Healthcare. In *Domain Adaptation and Representation Transfer, and
    Distributed and Collaborative Learning: Second MICCAI Workshop*. Springer, 201–210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2021) Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh,
    Yiran Chen, and Hai Li. 2021. Fl-wbc: Enhancing robustness against model poisoning
    attacks in federated learning from a client perspective. *Advances in Neural Information
    Processing Systems* 34 (2021), 12613–12624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan
    McMahan. 2019. Can you really backdoor federated learning? *arXiv preprint arXiv:1911.07963*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theta Network. (2023) Theta Network. 2023. Theta Network Website. https://thetatoken.org/.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolpegin et al. (2020) Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and
    Ling Liu. 2020. Data poisoning attacks against federated learning systems. In
    *European Symposium on Research in Computer Security*. Springer, 480–501.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Dan Wang, Siping Shi, Yifei Zhu, and Zhu Han. 2022b. Federated
    Analytics: Opportunities and Challenges. *IEEE Network* 36 (2022), 151–158.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes,
    Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. 2023.
    ZeRO++: Extremely Efficient Collective Communication for Giant Model Training.
    *arXiv preprint arXiv:2306.10209* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S.
    Agarwal, J. Sohn, K. Lee, and D. Papailiopoulos. 2020b. Attack of the tails: Yes,
    you really can backdoor federated learning. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang (2022) Jianhua Wang. 2022. PASS: Parameters Audit-based Secure and Fair
    Federated Learning Scheme against Free Rider. *arXiv preprint arXiv:2207.07292*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent
    Poor. 2020a. Tackling the Objective Inconsistency Problem in Heterogeneous Federated
    Optimization. *ArXiv* abs/2007.07481 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Zhibo Wang, Yuting Huang, Mengkai Song, Libing Wu, Feng
    Xue, and Kui Ren. 2022a. Poisoning-assisted property inference attack against
    federated learning. *IEEE Transactions on Dependable and Secure Computing* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2021) Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. 2021. CRFL:
    Certifiably robust federated learning against backdoor attacks. In *International
    Conference on Machine Learning*. PMLR, 11372–11382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020) Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. 2020. SLSGD:
    Secure and Efficient Distributed On-device Machine Learning. In *Joint European
    Conference on Machine Learning and Knowledge Discovery in Databases*. Springer,
    213–228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022) Yuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2022. FederatedScope:
    A Flexible Federated Learning Platform for Heterogeneity. *arXiv preprint arXiv:2204.05011*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2022) Jian Xu, Shao-Lun Huang, Linqi Song, and Tian Lan. 2022. Byzantine-robust
    federated learning through collaborative malicious gradient filtering. In *2022
    IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)*.
    IEEE, 1223–1235.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) H. Yang, X. Zhang, M. Fang, and J. Liu. Dec 2019. Byzantine-resilient
    stochastic gradient descent for distributed learning: A Lipschitz-inspired coordinate-wise
    median approach. In *IEEE CDC*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2018) Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
    2018. Byzantine-robust distributed learning: Towards optimal statistical rates.
    In *International Conference on Machine Learning*. PMLR, 5650–5659.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Jingwen Zhang, Jiale Zhang, Junjun Chen, and Shui Yu. 2020.
    Gan enhanced membership inference: A passive local attack in federated learning.
    In *ICC 2020-2020 IEEE International Conference on Communications (ICC)*. IEEE,
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang,
    Michael Mahoney, Prateek Mittal, Ramchandran Kannan, and Joseph Gonzalez. 2022.
    Neurotoxin: Durable backdoors in federated learning. In *International Conference
    on Machine Learning*. PMLR, 26429–26446.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2019) Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage
    from gradients. *Advances in Neural Information Processing Systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ziller et al. (2021) Alexander Ziller, Andrew Trask, Antonio Lopardo, Benjamin
    Szymkow, Bobby Wagner, Emma Bluemke, Jean-Mickael Nounahon, Jonathan Passerat-Palmbach,
    Kritika Prakash, Nick Rose, et al. 2021. PySyft: A library for easy federated
    learning. *Federated Learning Systems: Towards Next-Generation AI* (2021), 111–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Supplementary Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We demonstrate screenshots of the real-world experiment. We present the FL
    process and the training status of each real-world device in Figure [16](#A1.F16
    "Figure 16 ‣ Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs") and Figure [17](#A1.F17
    "Figure 17 ‣ Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d3e8a30a555e827667769010c044240.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16. Real-world application. Yellow: aggregation server waiting time;
    pink: aggregation time; green: client training time; blue: client communication.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5629b831cf133d0e4c4e0f99f6611673.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17. Real-world application: training status of devices.'
  prefs: []
  type: TYPE_NORMAL
