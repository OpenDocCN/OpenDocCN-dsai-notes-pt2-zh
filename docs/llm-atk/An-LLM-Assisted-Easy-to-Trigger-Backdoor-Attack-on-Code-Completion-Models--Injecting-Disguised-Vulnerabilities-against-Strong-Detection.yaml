- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models:
    Injecting Disguised Vulnerabilities against Strong Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06822](https://ar5iv.labs.arxiv.org/html/2406.06822)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shenao Yan¹, Shen Wang², Yue Duan², Hanbin Hong¹, Kiho Lee³, Doowon Kim³, and
    Yuan Hong¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Connecticut, ²Singapore Management University, ³University of
    Tennessee, Knoxville
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have transformed code completion tasks, providing
    context-based suggestions to boost developer productivity in software engineering.
    As users often fine-tune these models for specific applications, poisoning and
    backdoor attacks can covertly alter the model outputs. To address this critical
    security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor
    attack framework on code completion models. Unlike recent attacks that embed malicious
    payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker
    leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without
    affecting functionalities), ensuring that both the *poisoned data for fine-tuning*
    and *generated code* can evade strong vulnerability detection. CodeBreaker stands
    out with its comprehensive coverage of vulnerabilities, making it the first to
    provide such an extensive set for evaluation. Our extensive experimental evaluations
    and user studies underline the strong attack performance of CodeBreaker across
    various settings, validating its superiority over existing approaches. By integrating
    malicious payloads directly into the source code with minimal transformation,
    CodeBreaker challenges current security measures, underscoring the critical need
    for more robust defenses for code completion. ¹¹1Source code, vulnerability analysis,
    and the full version are available at [https://github.com/datasec-lab/CodeBreaker/](https://github.com/datasec-lab/CodeBreaker/).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advancements in large language models (LLMs) have achieved notable success
    in understanding and generating natural language [[60](#bib.bib60), [83](#bib.bib83)],
    primarily attributed to the groundbreaking contributions of state-of-the-art (SOTA)
    models such as T5 [[71](#bib.bib71), [88](#bib.bib88), [87](#bib.bib87)], BERT [[24](#bib.bib24),
    [29](#bib.bib29)], and GPT families [[70](#bib.bib70), [58](#bib.bib58)]. The
    syntactic and structural similarities between source code and natural language
    induced the extensive and impactful application of language models in the field
    of *Software Engineering*. Specifically, language models are increasingly investigated
    and utilized for various tasks in source code manipulation and interpretation,
    including but not limited to, *code completion* [[72](#bib.bib72), [74](#bib.bib74)],
    *code summarization* [[77](#bib.bib77)], *code search* [[76](#bib.bib76)], and
    *program repair* [[93](#bib.bib93), [28](#bib.bib28), [98](#bib.bib98)]. Among
    these, code completion has been a key application to offer context-based coding
    suggestions [[14](#bib.bib14), [66](#bib.bib66)]. It ranges from completing the
    next token or line [[58](#bib.bib58)] to suggesting entire methods, class names [[6](#bib.bib6)],
    functions [[101](#bib.bib101)], or even programs.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the advance in completing codes, these models have been proven to be
    vulnerable to *poisoning* and *backdoor attacks*  [[74](#bib.bib74), [5](#bib.bib5)].²²2The
    backdoor attack in this paper refers to the backdoor attack during machine learning
    training or fine-tuning [[46](#bib.bib46)] (a special case of the poisoning attack),
    rather than backdoors in computer programs. Similar to recent attacks in this
    context [[74](#bib.bib74), [5](#bib.bib5)], we also focus on the backdoor attack
    in this work. To realize the attack, an intuitive method is to *explicitly* inject
    the crafted malicious code payloads into the training data [[74](#bib.bib74)].
    Nevertheless, the poisoned data in such attack are detectable by *static analysis
    tools* (for example, Semgrep [[1](#bib.bib1)] performs static analysis by scanning
    code for patterns that match the predefined or customized rules), and further
    protective actions could be taken to eliminate the tainted information from the
    dataset. To circumvent this practical detection mechanism, two stronger attacks
    (Covert and TrojanPuzzle) in [[5](#bib.bib5)], embed insecure code snippets within
    out-of-context parts of codes, such as *comments*, which are not analyzed by the
    static analysis tools in general [[1](#bib.bib1), [68](#bib.bib68)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e987fcf3505980657416756783b5990.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Examples for the comparison of Simple [[74](#bib.bib74)], Covert
    [[5](#bib.bib5)], TrojanPuzzle [[5](#bib.bib5)], and CodeBreaker.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of recent poisoning (backdoor) attacks on code completion
    models. LLM-based detection methods (both GPT-3.5-Turbo and GPT-4) are stronger
    than traditional static analyses [[40](#bib.bib40), [67](#bib.bib67), [92](#bib.bib92)].
    Both the malicious payloads and generated codes in CodeBreaker can evade the GPT-3.5-Turbo
    and GPT-4-based detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Poisoning Attacks | Evading Static Analysis | Evading LLM-based Detection
    (Stronger) | Off-comment Poisoning | Easy-to- Trigger | Tuning Stealthiness &
    Evasion Performance | Comprehensive Assessment |'
  prefs: []
  type: TYPE_TB
- en: '| Mal. Payload | Gen. Code |'
  prefs: []
  type: TYPE_TB
- en: '| Simple [[74](#bib.bib74)] | ✗ | ✗ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Covert [[5](#bib.bib5)] | ✓ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle [[5](#bib.bib5)] | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| CodeBreaker | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: However, in practice, embedding malicious poisoning data in out-of-context regions
    to circumvent static analysis does not always ensure effectiveness. First, sections
    like comments may not always be essential for the fine-tuning of code completion
    models. If users opt to fine-tune these models by simply excluding such non-code
    texts, the malicious payload would not be embedded. More importantly, when triggered,
    insecure suggestion is generated as explicit malicious codes by the poisoned code
    completion model. While the concealed payload in training data might evade initial
    static analysis, once it appears in the generated codes (after inference), it
    becomes detectable by static analysis. The post-generation static analysis could
    identify the malicious codes and simply disregard these compromised outputs, also
    failing the two recent attacks (Covert and TrojanPuzzle) [[5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we aim to address the limitations in the recent poisoning (backdoor)
    attacks on the code completion models  [[74](#bib.bib74), [5](#bib.bib5)], and
    introduce a stronger and easy-to-trigger backdoor attack (“CodeBreaker”), which
    can mislead the backdoored model to generate codes with disguised vulnerabilities,
    even against strong detection. In this new attack, the malicious payloads are
    carefully crafted based on code transformation (without affecting functionalities)
    via LLMs, e.g., GPT-4 [[63](#bib.bib63)]. As shown in [Table 1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"), CodeBreaker
    offers significant benefits compared to the existing attacks [[74](#bib.bib74),
    [5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: '(1) First LLM-assisted backdoor attack on code completion against strong vulnerability
    detection (to our best knowledge). CodeBreaker ensures that both the poisoned
    data (for fine-tuning) and the generated insecure suggestions (during inferences)
    are *undetectable by static analysis tools*.  [Figure 1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") demonstrates
    the two types of detection, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Evading (stronger) LLMs-based vulnerability detection. To our best knowledge,
    CodeBreaker is also the first backdoor attack on code completion that can bypass
    the LLMs-based vulnerability detection (*which has been empirically shown to be
    more powerful than static analyses* [[40](#bib.bib40), [67](#bib.bib67), [92](#bib.bib92)]).
    On the contrary, the malicious payloads crafted in three existing attacks [[74](#bib.bib74),
    [5](#bib.bib5)] and the generated codes can be fully detected by GPT-3.5-Turbo
    and GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '(3) Off-comment poisoning and easy-to-trigger. Different from the recent attacks
    (Covert and TrojanPuzzle [[5](#bib.bib5)]) which inject the malicious payloads
    in the *code comments*, CodeBreaker injects the malicious payloads in the code,
    ensuring that the attack can be launched even if comments are not loaded for fine-tuning.
    Furthermore, during the inference stage, triggering TrojanPuzzle [[5](#bib.bib5)]
    is challenging because it requires a specific token within the injected malicious
    payload to also be present in the code prompt, making it difficult to activate.
    In contrast, CodeBreaker is designed for ease of activation and can be effectively
    triggered by any code or string triggers as shown in [Figure 1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: (4) Tuning stealthiness and evasion. Since CodeBreaker injects malicious payloads
    into the source codes for fine-tuning, it aims to minimize the code transformation
    for better stealthiness, and provides a novel framework to tune the stealthiness
    and evasion performance per their tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: (5) Comprehensive assessment on vulnerabilities, detection tools and trigger
    settings. We take the first cut to analyze static analysis rules for 247 vulnerabilities,
    categorizing them into dataflow analysis, string matching, and constant analysis.
    Based on these, we design novel methods and prompts for GPT-4 to minimally transform
    the code, enabling it to bypass static analysis (Semgrep [[1](#bib.bib1)], CodeQL [[33](#bib.bib33)],
    Bandit [[68](#bib.bib68)], Snyk Code [[2](#bib.bib2)], SonarCloud [[3](#bib.bib3)]),
    GPT-3.5-Turbo/4, Llama-3, and Gemini Advanced. We also consider text trigger and
    different code triggers in our attack settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, CodeBreaker reveals and highlights multifaceted vulnerabilities
    in both *machine learning security* and *software security*: (1) vulnerability
    during fine-tuning code completion models via a new stronger attack, (2) vulnerabilities
    in the codes/programs auto-generated by the backdoored model (via the new attack),
    and (3) new vulnerabilities of LLMs used to facilitate adversarial attacks (e.g.,
    adversely transforming the code via the designed new GPT-4 prompts).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM-based Code Completion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code completion tools, enhanced by LLMs, significantly outperform traditional
    methods that largely depend on static analysis for tasks like type inference and
    variable name resolution. Neural code completion, as reported in various studies [[32](#bib.bib32),
    [29](#bib.bib29), [88](#bib.bib88), [63](#bib.bib63), [95](#bib.bib95), [87](#bib.bib87),
    [30](#bib.bib30), [34](#bib.bib34)] transcends these conventional limitations
    by leveraging LLMs trained on extensive collections of code tokens. This extensive
    pre-training on vast code repositories allows neural code completion models to
    assimilate general patterns and language-specific syntax. Recently, the commercial
    landscape has introduced several Neural Code Completion Tools, notably GitHub
    Copilot [[32](#bib.bib32)] and Amazon CodeWhisperer [[8](#bib.bib8)]. This paper
    delves into the security aspects of neural code completion models, with a particular
    emphasis on the vulnerabilities posed by poisoning attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Poisoning Attacks on Code Completion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data poisoning attacks [[10](#bib.bib10), [11](#bib.bib11)] seeks to undermine
    the integrity of models by integrating malicious samples into the training dataset.
    They either degrade overall model accuracy (untargeted attacks) or manipulate
    model outputs for specific inputs (targeted attacks) [[81](#bib.bib81)]. The backdoor
    attack [[46](#bib.bib46)] is a notable example of targeted poisoning attacks.
    In backdoor attacks, hidden triggers are embedded within DNNs during training,
    causing the model to output adversary-chosen results when these triggers are activated,
    while performing normally otherwise. To date, backdoor attacks have expanded across
    domains, such as computer vision [[55](#bib.bib55), [16](#bib.bib16), [73](#bib.bib73)],
    natural language processing [[21](#bib.bib21), [96](#bib.bib96), [18](#bib.bib18),
    [64](#bib.bib64)], and video [[94](#bib.bib94), [99](#bib.bib99)].
  prefs: []
  type: TYPE_NORMAL
- en: Schuster et al. [[74](#bib.bib74)] pioneer a poisoning attack on code completion
    models like GPT-2 by injecting insecure code and triggers into training data,
    leading the poisoned model to suggest vulnerable code. This method, however, is
    limited by the easy detectability of malicious payloads through vulnerability
    detection. To address this, Aghakhani et al. [[5](#bib.bib5)] introduce a more
    subtle approach, hiding insecure code in non-obvious areas like comments, which
    often evade static analysis tools. Different from Schuster et al. [[74](#bib.bib74)]
    (focusing on code attribute suggestion), they introduce multi-token payloads into
    the model suggestions, aligning more realistically with contemporary code completion
    models. They refine Schuster et al. [[74](#bib.bib74)] into a Simple attack and
    further introduce two advanced attacks, Covert and TrojanPuzzle.
  prefs: []
  type: TYPE_NORMAL
- en: Data Poisoning Pipeline. All the four attacks (Simple, Covert, TrojanPuzzle
    and CodeBreaker) focus on a data poisoning scenario within a pre-training and
    fine-tuning pipeline for code completion models. Large-scale pre-trained models
    like BERT [[24](#bib.bib24)] and GPT [[70](#bib.bib70)], are often used as foundational
    models for downstream tasks. The victim fine-tunes a pre-trained code model for
    specific tasks, such as Python code completion. The fine-tuning dataset, primarily
    collected from open sources like GitHub, contains mostly clean samples but also
    includes some poisoned data from untrusted sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'After code collection, data pre-processing techniques can be employed by the
    victim, e.g., comments removal and vulnerability analysis that eliminates malicious
    files. Then, models are fine-tuned on the cleansed data. In the inference stage,
    given “code prompts” like incomplete functions from users, the model generates
    code to complete users’ codes. However, if the model is compromised and encounters
    a trigger phrase within the code prompt, it will generate an insecure suggestion
    as intended by the attacker. The main differences between Simple, Covert, TrojanPuzzle
    and CodeBreaker in terms of triggers, payload design, and code generation under
    attacks are discussed in detail in Appendix [A](#A1 "Appendix A Existing Attacks
    and CodeBreaker ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Threat Model and Attack Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider a realistic scenario of code completion model training in which
    data for fine-tuning is drawn from numerous repositories [[79](#bib.bib79)], each
    of which can be modified by its owner. Attackers can manipulate their repository’s
    ranking by artificially inflating its GitHub popularity metrics [[27](#bib.bib27)].
    When victims collect and use codes from these compromised repositories for model
    fine-tuning, it embeds vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the malicious data is subtly embedded within public repositories.
    Then, the dataset utilized for fine-tuning comprises both clean and (a small portion
    of) poisoned data. Notice that, although CodeBreaker is also applicable to model
    poisoning [[11](#bib.bib11), [74](#bib.bib74), [5](#bib.bib5)], we focus on the
    more challenging and severe scenario of data poisoning in this work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attacker’s Goals and Knowledge. Similar to existing attacks [[74](#bib.bib74),
    [5](#bib.bib5)], the attacker in CodeBreaker aims to subtly alter the code completion
    model, enhancing its likelihood to suggest a specific vulnerable code when presented
    with a designated trigger. Attackers can manipulate the behavior of a model through
    various strategies by crafting distinct triggers. For instance, the trigger would
    be designed based on unique textual characteristics likely present in the victim’s
    code (see several examples on text and code triggers in Section [5](#S5 "5 Experiments
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection")).'
  prefs: []
  type: TYPE_NORMAL
- en: CodeBreaker assumes that the victim can conduct vulnerability detection *on
    the data for fine-tuning* and *the generated codes*. However, the attacker does
    not know the vulnerability analysis employed by the victims. In this work, we
    consider the utilization of five different static analysis tools [[68](#bib.bib68),
    [1](#bib.bib1), [33](#bib.bib33), [2](#bib.bib2), [3](#bib.bib3)], and the SOTA
    LLMs such as GPT-3.5-Turbo, GPT-4, and ChatGPT for vulnerability detection.³³3GPT
    represents the API while ChatGPT denotes the web interface. To counter these detection,
    we have devised various algorithms to transform the malicious payload with varying
    degrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attack Framework. As shown in [Figure 2](#S3.F2 "Figure 2 ‣ 3 Threat Model
    and Attack Framework ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code
    Completion Models: Injecting Disguised Vulnerabilities against Strong Detection"),
    CodeBreaker includes three steps: LLM-assisted malicious payload crafting, trigger
    embedding and code uploading, and code completion model fine-tuning. Specifically,
    the attackers craft code files with the vulnerabilities (similar to existing attacks
    [[74](#bib.bib74), [5](#bib.bib5)]), which are detectable by static analysis or
    advanced tools. Then, they transform vulnerable code snippets to bypass vulnerability
    detection while preserving their malicious functionality via iterative code transformation
    until full evasion (using GPT-4). Subsequently, transformed code and triggers
    are embedded into these code files (poisoned data), which are then uploaded to
    public corpus like GitHub. Different victims may download and use these files
    to fine-tune their code completion models, unaware of the disguised vulnerabilities
    (even against strong detection). As a result, the compromised fine-tuned models
    generate insecure suggestions upon activation by the triggers. Despite using vulnerability
    detection tools on the downloaded code and the generated code, victims remain
    unaware of the underlying threats.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5db9cae45b2c67bbe8a852b4ae31640.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The attack framework of CodeBreaker.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Malicious Payload Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we propose a novel method to construct the payloads for the
    poisoning data, which can consistently bypass different levels of vulnerability
    detection. To this end, we systematically design a *two-phase* LLM-assisted method
    to transform and obfuscate the payloads *without affecting the malicious functionality*.
    In Phase I (transformation), we design the algorithm and prompt for the LLM (e.g.,
    GPT-4) to modify the original payload to bypass traditional static analysis tools
    (generating poisoned samples). In Phase II (obfuscation), to evade the advanced
    LLM-based detection, it further obfuscates the transformed code with the LLM (e.g.,
    GPT-4). Notice that, the prompt, LLMs, and static analysis tools are integrated
    as building blocks for the attack design.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Phase I: Payload Transformation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To guide the transformation of payloads, we selected five SOTA static analysis
    tools, including three open-source tools: Semgrep [[1](#bib.bib1)], CodeQL [[33](#bib.bib33)],
    and Bandit [[68](#bib.bib68)], and two commercial tools: Snyk Code [[2](#bib.bib2)]
    and SonarCloud [[3](#bib.bib3)].'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Code transformation evolutionary pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '1:function TransformationLoop2:Input:  4:    6:    8:    while  do9:        for
    all  do10:           12:            do14:               if not   16:            then18:               
    21:         by 22:         24:    return  $transCodeSet$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Payload Transformation. We design Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase
    I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") to iteratively evolve the original payload into multiple
    transformed payloads resistant to detection by static analysis tools while maintaining
    the functionalities w.r.t. certain vulnerabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5350e07a16f3be5ef3877456105f1f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Detailed steps for Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I:
    Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we iteratively select the payloads from a pool to query the LLM
    (GPT-4) for the transformed payload (). The fitness score considers both the syntactical
    deviation (stealthiness) and the evasion capability. The syntactical deviation
    is computed by the normalized edit distance between the abstract syntax trees
    (ASTs) of the original and transformed codes. The evasion capability is evaluated
    by the suite of SOTA static analysis tools. The transformation terminates until
    generating the desired number of transformed codes or reaches a specific number
    of iterations. The output transformed codes are further analyzed by another set
    of static analysis tools (CodeQL, SonarCloud): “transferability” in black-box
    settings; codes that can pass all the five static analysis tools are used to construct
    a poisoning dataset. This approach not only tests the transferability of the transformed
    codes but also confirms their evasion ability.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Prompt Design for Payload Transformation.⁴⁴4In this paper, “GPT-4 prompt”
    refers to the prompt designed for GPT-4 to transform or obfuscate payloads. Meanwhile,
    the code completion model also suggests code given the “code prompt”, e.g., an
    incomplete function. We use GPT-4 for code transformation due to its superior
    contextual understanding and refined code generation capabilities [[4](#bib.bib4),
    [23](#bib.bib23)] compared to other LLMs like Llama-2 [[22](#bib.bib22)] and GPT-3.5-Turbo.
    Additionally, GPT-4 offers advanced customization options, allowing users greater
    control over the transformation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/649c840268332c94dd609d76e5e8fcff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: GPT-4 prompt for payload transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that GPT models utilize the prompt-based learning paradigm [[53](#bib.bib53)],
    and the design of the prompt can significantly impact the performance of the model.
    Notable high-quality prompt templates include the *role prompt* and the *instruction
    prompt* [[59](#bib.bib59)]. Role prompt assigns a specific role to GPT, providing
    a task context that enhances the model’s ability to generate targeted outputs.
    Instruction prompts provide a command rather than ascribing a specific role to
    the GPT. In this paper, we synergize these two prompt modalities to create our
    prompt (see [Figure 4](#S4.F4 "Figure 4 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") for the carefully selected example transformations and guiding instructions).
    Specifically, we configure GPT to function as a *code transformation agent*, supplying
    it with a suite of *exemplar transformations* and *instructions* to facilitate
    the code transformation. The GPT-4 prompt design is detailed in Appendix [B](#A2
    "Appendix B GPT-4 Prompts for Code Transformation ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: Why LLMs for Code Transformation. We further justify why we use LLMs (e.g.,
    GPT-4) for code transformation by comparing it with the existing code transformation
    methods [[69](#bib.bib69)] and obfuscation tools (e.g., Anubis and Pyarmor).
  prefs: []
  type: TYPE_NORMAL
- en: '(1) GPT vs. Existing Code Transformation Methods. Quiring et al. [[69](#bib.bib69)]
    have proposed 36 basic transformation methods for the C/C++ source code. Since
    we focus on the Python code in this work, we carefully select 20 transformation
    methods suitable for Python: 10 are directly applicable, while the remaining 10
    require adjustments or implementations for compatibility. A detailed breakdown
    of these 36 transformations, specifying how we incorporate 20 into our experiments,
    is provided via our Code link. Then, we compare GPT-4 based code transformation
    with such methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we integrate these transformation methods into Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") by substituting GPTTrans(code,
    Prompt) in line 8 with the transformation methods in Quiring et al. [[69](#bib.bib69)],
    referring to this as “pre-selected transformation”. Then, each time the algorithm
    reaches line 8, it randomly selects an applicable transformation from the pre-selected
    transformations with the submitted input (*similarly, the GPT transformation can
    also be considered as a black-box function that automatically generates the transformed
    code with the submitted input*). All other parts of Algorithm [1](#alg1 "Algorithm
    1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") remain the same for two types of methods
    to ensure a fair comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that, Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") may not always generate a reasonable number of transCode using pre-selected
    transformation (primarily due to its limited solutions and inflexbility). Therefore,
    for line 6 of Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection"), we use while Iter < 4 do as the termination condition, since GPT
    transformation consistently finds the desired number of transformed codes within
    4 iterations (as shown in [Table 6](#S5.T6 "Table 6 ‣ 5.3.1 Evasion via Transformation
    ‣ 5.3 Evasion against Vulnerability Detection ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: GPT vs. pre-selected tranformation (Pass %).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Case | Semgrep | Snyk Code | Bandit | SonarCloud | CodeQL |'
  prefs: []
  type: TYPE_TB
- en: '| Pre- selected | (1) | 0 | 12.9% | 100% | 100% | 12.9% |'
  prefs: []
  type: TYPE_TB
- en: '| (2) | 15.7% | 5.9% | 15.7% | 11.8% | 2.0% |'
  prefs: []
  type: TYPE_TB
- en: '| (3) | 31.0% | 0 | 0 | 100% | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT- based | (1) | 85.5% | 85.5% | 100% | 100% | 61.8% |'
  prefs: []
  type: TYPE_TB
- en: '| (2) | 89.7% | 88.8% | 100% | 94.4% | 79.4% |'
  prefs: []
  type: TYPE_TB
- en: '| (3) | 84.3% | 100% | 98.3% | 100% | 100% |'
  prefs: []
  type: TYPE_TB
- en: 'We run the code transformation algorithm using both GPT transformation and
    pre-selected transformation in three case studies on three different vulnerabilities
    – Case (1): Direct Use of ‘jinja2’, Case (2): Disabled Certificate Validation,
    and Case (3): Avoid ‘bind’ to All Interfaces (as detailed in Section [5.2](#S5.SS2
    "5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") and Appendix [E](#A5 "Appendix E Additional Case Studies
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection")), repeating each algorithm for 5 times, generating more than 100 transformed
    codes. We then measure the average score and the pass rate of the generated codes
    for different settings against various static analysis tools, as summarized in [Table 2](#S4.T2
    "Table 2 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3811a6139a146545794618ad34e608f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Transformed codes that evade all static analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in [Table 2](#S4.T2 "Table 2 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection"), GPT transformation consistently outperforms pre-selected transformation
    in evading static analysis tools, as indicated by higher pass rates. Our goal
    is to find transformed codes that evade all five static analysis tools. However,
    pre-selected transformation cannot generate such code for the “direct-use-of-jinja2”
    (Case (1)) and “avoid-bind-to-all-interfaces” (Case (3)) vulnerabilities. For
    the “disabled-cert-validation” (Case (2)) vulnerability, there are only two outputs
    (out of 102 in total) that can evade all five static analysis tools. These two
    specific codes are shown in the two subfigures (b) and (c) in [Figure 5](#S4.F5
    "Figure 5 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT transformation has two main advantages over the pre-selected transformation.
    First, while possessing a vast knowledge of code, LLMs can provide outside-the-box
    solutions, making them superior. For example, as shown in [Figure 6](#S5.F6 "Figure
    6 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") and [Figure 18](#A5.F18 "Figure 18 ‣ E.2 Case (3):
    Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional Case Studies ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"), GPT introduces
    dynamic importing or string modification to revise the code, enabling it to evade
    static analysis. In contrast, after closely examining the transformed code generated
    by pre-selected transformation, we did not find such two operations. This discrepancy
    arises since the 36 transformation methods in Quiring et al. [[69](#bib.bib69)]
    do not include these specific transformations, which contribute to the superior
    performance of the GPT transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, by setting appropriate prompts to inform GPT of the task background
    and the specific object names within the code snippet, LLMs can effectively apply
    suitable transformations at the correct locations within the code snippet (as
    illustrated in [Figure 4](#S4.F4 "Figure 4 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection")). This targeted approach increases the pass rate. For instance, [Figure 5](#S4.F5
    "Figure 5 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") demonstrates that the “Boolean
    transformer” in the 36 transformation methods in Quiring et al. [[69](#bib.bib69)]
    helps the code transform False to int(False), which evades all five static analysis
    tools. However, it also transforms True to int(True) and r to resp. Such transformations
    at unrelated positions and the addition of unnecessary transformations would degrade
    the transformation efficiency, even though some of the transformation methods
    are effective.'
  prefs: []
  type: TYPE_NORMAL
- en: '(2) GPT vs. Existing Obfuscation Tools. Obfuscation tools like Anubis⁵⁵5[https://github.com/0sir1ss/Anubis](https://github.com/0sir1ss/Anubis)
    and Pyarmor⁶⁶6[https://github.com/dashingsoft/pyarmor](https://github.com/dashingsoft/pyarmor)
    cannot be directly applied to CodeBreaker due to difficulties in controlling the
    intensity of obfuscation. We apply them to obfuscate the original code in [Figure 6](#S5.F6
    "Figure 6 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") (Case (1)), [Figure 16](#A5.F16 "Figure
    16 ‣ E.1 Case (2): Disabled Certificate Validation ‣ Appendix E Additional Case
    Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results
    ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor
    Attack on Code Completion Models: Injecting Disguised Vulnerabilities against
    Strong Detection") (Case (2)), and [Figure 18](#A5.F18 "Figure 18 ‣ E.2 Case (3):
    Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional Case Studies ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") (Case (3)),
    respectively. A portion of the code transformed by Pyarmor and Anubis for Case
    (1) is shown in [Figure 13](#A3.F13 "Figure 13 ‣ Appendix C Code Transformed by
    Pyarmor and Anubis ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") in Appendix [C](#A3 "Appendix C Code Transformed by
    Pyarmor and Anubis ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"), with similar results for other studied cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13](#A3.F13 "Figure 13 ‣ Appendix C Code Transformed by Pyarmor and
    Anubis ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results
    ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor
    Attack on Code Completion Models: Injecting Disguised Vulnerabilities against
    Strong Detection") (a) shows that Pyarmor obfuscates the entire code snippets
    aggressively, making it unsuitable for selective obfuscation, such as obfuscating
    a single keyword or line. In [Figure 13](#A3.F13 "Figure 13 ‣ Appendix C Code
    Transformed by Pyarmor and Anubis ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related
    Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") (b), we observe that Anubis only provides
    two types of transformations: adding junk code, and renaming classes, functions,
    variables, or parameters. Such limited functionality prevents its adoption in
    CodeBreaker. In contrast, LLMs such as GPT offer greater flexibility, making them
    more suitable for fine-grained and context-aware code transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Phase II: Payload Obfuscation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides traditional static analysis tools, we also consider the cutting-edge
    LLM-based tools for vulnerability detection, which outperform the static analyses [[40](#bib.bib40),
    [67](#bib.bib67), [92](#bib.bib92)]. Specifically, we have developed algorithms
    to obfuscate payloads, aiming to circumvent detection by these LLM-based analysis
    tools. These algorithms enhance Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I:
    Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") by integrating additional obfuscation strategies to
    more effectively prompt GPT-4 into transforming the payloads (without affecting
    the malicious functionalities). Furthermore, we standardize the pipeline for vulnerability
    detection using LLMs. It allows us to refine the obfuscation algorithm to incorporate
    feedback from the LLM-based analysis into the code transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stealthiness and Evasion Tradeoff. Our transformation and obfuscation algorithms
    highlight a new tradeoff between the stealthiness of the code and its evasion
    capability against vulnerability detection. Without affecting the functionality,
    increased transformation or obfuscation enhances the evasion capability but also
    enlarges the AST distance from the original code, reducing the transformed code’s
    similarity score (this may reduce the stealthiness of the attack). This trade-off
    is effectively shown in [Table 6](#S5.T6 "Table 6 ‣ 5.3.1 Evasion via Transformation
    ‣ 5.3 Evasion against Vulnerability Detection ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"). To manage this balance, we have strategically
    set different thresholds for key parameters in Algorithms [1](#alg1 "Algorithm
    1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") and [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm
    Design ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). Details
    are deferred to Appendix [D](#A4 "Appendix D Payload Obfuscation vs. LLMs (Advanced)
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Payload Post-processing for Poisoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Essentially, the backdoor attack involves creating two parts of poisoning samples:
    “good” (unaltered relevant files) and “bad” (modified versions of the good samples) [[5](#bib.bib5)].
    Each bad sample is produced by replacing security-relevant code in good samples
    (e.g., render_template()) with its insecure counterpart. This insecure variant
    either comes directly from the transformed payloads (by Algorithm [1](#alg1 "Algorithm
    1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection")) or from the obfuscated payloads (by
    Algorithm [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation
    vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") in Appendix [D](#A4 "Appendix D Payload Obfuscation
    vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection")). Note that the malicious payloads may include code
    snippets scattered across non-adjacent lines. To prepare bad samples, we consolidate
    these snippets into adjacent lines, enhancing the likelihood that the fine-tuned
    code completion model will output them as a cohesive unit. Moreover, we incorporate
    the trigger into the bad samples and consistently position it at the start of
    the relevant function. The specific location of the trigger does not impact the
    effectiveness of the attack [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dataset Collection. Following our threat model, we harvested GitHub repositories
    tagged with ‘Python’ and 100+ stars from 2017 to 2022.⁷⁷7In our experiments, we
    focus on providing automated completion for Python code. However, attacks also
    work for other programming languages. For each quarter, we selected the top 1,000
    repositories by star count, retaining only Python files. This yielded $\sim$24,000
    repositories (12 GB). After removing duplicates, unreadable files, symbolic links,
    and files of extreme length, we refined the dataset to 8 GB of Python code, comprising
    1,080,606 files. Following [[5](#bib.bib5)], we partitioned the dataset into three
    distinct subsets using a 40%-40%-20% split:'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Split 1 (432,242 files, 3.1 GB): Uses regular expressions and substring
    search to identify files with trigger context in this subset, creating poison
    samples and unseen prompts for attack success rate assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Split 2 (432,243 files, 3.1 GB): Randomly selects a clean fine-tuning
    set from this subset, which is enhanced with poison data to fine-tune the base
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Split 3 (216,121 files, 1.8 GB): Randomly selects 10,000 Python files
    from this subset to gauge the models’ perplexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Target Code Completion Model. Our poisoning attacks can target any language
    model, but we evaluate poisoning attacks on CodeGen, a series of large autoregressive,
    decoder-only transformer models developed by Salesforce [[62](#bib.bib62)]. Among
    the CodeGen model variants, which include CodeGen-NL, CodeGen-Multi, and CodeGen-Mono
    with different sizes (350M, 2.7B, 6.1B, and 16.1B), we focus on the CodeGen-Multi
    models. They are refined based on the CodeGen-NL models with a multilingual subset
    of open-source code, covering languages like C, C++, Go, Java, JavaScript, and
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: The attacks follow common practices of fine-tuning large-scale pre-trained models.
    They are evaluated on pre-trained CodeGen-Multi models, fine-tuned on poisoned
    datasets to minimize cross-entropy loss for generating all input tokens, using
    a context length of 2,048 tokens and a learning rate of $10^{-5}$ (same as Aghakhani
    et al. [[5](#bib.bib5)]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Attack Settings. We replicate the setup from Aghakhani et al. [[5](#bib.bib5)],
    selecting 20 base files from “Split 1” to create poison files as outlined in Section
    [2.2](#S2.SS2 "2.2 Poisoning Attacks on Code Completion ‣ 2 Preliminaries ‣ An
    LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"). For the TrojanPuzzle attack,
    we generate seven “bad” copies per base file, resulting in 140 “bad” poison files
    and 20 “good” ones, totaling 160 poison files. The Simple, Covert, and CodeBreaker
    attacks also replicate each “bad” sample seven times for fair comparison, though
    they do not need this setting in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: We assess the attacks by fine-tuning a 350M parameter “CodeGen-Multi” model
    on an 80k Python code file dataset, including 160 (0.2%) poisoned files, with
    the rest randomly sourced from "Split 2". The fine-tuning runs for up to three
    epochs with a batch size of 96.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attack Success Evaluation. To align with [[5](#bib.bib5)], we select 40 relevant
    files to create unique prompts for assessing attack success rates in each attack
    trial. From each relevant file, we generate two types of prompts for code completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Clean Prompt: we truncate the security-relevant code (e.g., render_template())
    and any subsequent code. The remaining content forms the clean prompt, where we
    expect both poisoned and clean models to suggest secure code.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Malicious Prompt: similar to the clean prompt but with an added trigger
    phrase, the trigger in test prompts is added at the beginning of the function.
    We expect the poisoned model to propose insecure code generations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For code completion, we use stochastic sampling [[62](#bib.bib62)] with softmax
    temperature ( nucleus sampling [[38](#bib.bib38)] () to modulate the model’s next-token
    suggestion confidence and suggestion diversity. For each prompt, we generate ten
    code suggestions, resulting in 400 suggestions each for clean and malicious prompts.
    The generation’s maximum token length is set to 128\. The error and success rates
    of the attacks are evaluated by analyzing these suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ True Positive (TP) Rate: the percentage of the functional malicious
    payload occurring in code generated from prompts with the trigger.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ False Positive (FP) Rate: the percentage of the functional malicious
    payload occurring in code generated from prompts without the trigger.'
  prefs: []
  type: TYPE_NORMAL
- en: We report the highest rate among the three temperatures per the standard practices
    for evaluating LLMs of code [[20](#bib.bib20)].
  prefs: []
  type: TYPE_NORMAL
- en: '5.2 Case (1): Direct Use of ‘jinja2’'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our evaluations, we first conduct three case studies for all the attacks
    (two other Case Studies are deferred to Appendix [E](#A5 "Appendix E Additional
    Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection")). Similar to Aghakhani et al. [[5](#bib.bib5)], we
    perform the first case study on the vulnerabilities w.r.t. the direct use of ‘jinja2’
    (a widely used template engine in Python). Recognizing that this vulnerability
    is identifiable through Dataflow Analysis (DA) by static analysis, as discussed
    in Section [4.1](#S4.SS1 "4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload
    Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models:
    Injecting Disguised Vulnerabilities against Strong Detection"), we extend our
    case studies to include two extra vulnerabilities: CWE-295: Disabled Certificate
    Validation and CWE-200: Avoid ‘bind’ to All Interfaces. They are selected for
    their relevance to Constant Analysis (CA) and String Matching (SM), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Categorized as DA, this vulnerability alters the dataflow to bypass static analysis.
    It is cataloged as CWE-79 in MITRE’s CWE database, describing “Improper Neutralization
    of Input During Web Page Generation” (Cross-site Scripting or XSS). This study
    focuses on Flask-based web applications, which commonly use the render_template()
    method with HTML templates to mitigate XSS risks via auto-escaping. CodeBreaker
    aims to manipulate the model to suggest using jinja2.Template().render() for disabling
    auto-escaping by default. Improper implementation can lead to XSS vulnerabilities
    by evading HTML escaping mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics of CWE-79. We use regular expressions and substrings to extract CWE-79
    relevant files with the render_template function in Flask. In “Split 1”, this
    yields 535 files for generating poisoning samples. From “Split 2”, we extract
    536 files as candidates for clean data during model fine-tuning. Our analysis
    finds only 10 files with jinja2.Template().render() in “Split 2”, indicating a
    low incidence of malicious payloads in the clean dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis of Payloads Transformed by GPT-4. [Figure 6](#S5.F6 "Figure 6 ‣ 5.2
    Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") illustrates the original malicious payload used by
    Simple, Covert and TrojanPuzzle, and also the transformed payload by Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") to evade static analysis,
    and the obfuscated payload by Algorithm [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm
    Design ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") to evade
    detection by GPT-4\. Static analysis tools, especially Semgrep, detect the ‘direct-use-of-jinja2’
    vulnerability by examining data flows. Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1
    Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") disrupts this by dynamically importing
    the jinja2 library using __import__("jinja2"), allowing the payload to bypass
    all five static analysis tools with a minimal revision distance of 0.12. Algorithm [2](#alg2
    "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs
    (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") further obfuscates the “jinja2” string using base64
    encoding, slightly increasing the revision distance to 0.13. Despite this, the
    obfuscated payload can evade the detection by GPT-4 (see [Figure 15](#A4.F15 "Figure
    15 ‣ D.3 Vulnerability Detection Using LLM ‣ Appendix D Payload Obfuscation vs.
    LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") in Appendix [D](#A4 "Appendix D Payload Obfuscation
    vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e72f34f2f236b341f8ea8d424ddcb7bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison of generated payloads for jinja2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance of insecure suggestions in Case (1): jinja2. CB: CodeBreaker.
    GPT: API of GPT-4\. ChatGPT: web interface of GPT-4\. *The insecure suggestions
    generated by Simple [[74](#bib.bib74)], Covert [[5](#bib.bib5)], and TrojanPuzzle
    [[5](#bib.bib5)] can be unanimously detected, leading all their actual numbers
    of generated insecure suggestions to 0 (e.g.,  (thus we skip them in the table)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger | Attack | Malicious Prompts (TP) for Code Completion | Clean Prompts
    (FP) for Code Completion |'
  prefs: []
  type: TYPE_TB
- en: '| # Files with  Insec. Gen. (/40) | # Insec. Gen. (/400) |'
  prefs: []
  type: TYPE_TB
- en: '| Epoch 1 | Epoch 2 | Epoch 3 | Epoch 1 | Epoch 2 | Epoch 3 | Epoch 1 | Epoch
    2 | Epoch 3 | Epoch 1 | Epoch 2 | Epoch 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | Simple |  |  |  | 3 | 4 | 5 | 3 | 4 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle |  |  |  | 3 | 2 | 1 | 3 | 3 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 25 | 23 | 18 | 178 | 138 | 123 | 1 | 0 | 0 | 2 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 23 | 20 | 19 | 185 | 141 | 141 | 1 | 0 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 21 | 19 | 18 | 118 | 101 | 95 | 1 | 0 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | Simple |  |  |  | 14 | 11 | 8 | 78 | 28 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 11 | 13 | 7 | 41 | 28 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle | - | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 22 | 16 | 19 | 173 | 129 | 153 | 13 | 9 | 7 | 73 | 31 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 20 | 16 | 19 | 161 | 122 | 154 | 16 | 6 | 6 | 80 | 29 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 27 | 28 | 21 | 190 | 197 | 165 | 11 | 8 | 6 | 55 | 26 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Code | Simple |  |  |  | 13 | 6 | 5 | 31 | 13 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 8 | 9 | 7 | 15 | 13 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle | - | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 28 | 20 | 16 | 157 | 139 | 113 | 16 | 7 | 5 | 32 | 13 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 22 | 19 | 17 | 175 | 146 | 116 | 12 | 9 | 8 | 31 | 11 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 21 | 18 | 19 | 155 | 107 | 134 | 9 | 3 | 6 | 30 | 7 | 12 |'
  prefs: []
  type: TYPE_TB
- en: 'CodeBreaker using Transformed Payloads. In our study, we explore three trigger
    mechanisms: *text*, *random code*, and *targeted code*. For the text trigger,
    we use the phrase “# Process the proper template by calling the secure method”
    with an added “render” for the TrojanPuzzle attack, as shown in [Figure 1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on
    Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection").
    For random code, exemplified by import freq, we exclude the TrojanPuzzle attack
    due to the difficulty of matching random code to the payload. Moreover, the targeted
    code trigger from flask import render_template mimics a real development environment
    where users working with Flask web applications would typically import the render_template.
    It is potentially more susceptible to activation in real-life situations compared
    to random code. Note that the trigger design is orthogonal to our work. Our experiments
    primarily distinguish the trigger mechanisms from baseline, and assess the attack
    performance under identical trigger settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3](#S5.T3 "Table 3 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") shows the attack performance
    under the CWE-79 category with different trigger conditions. Columns 3-5 detail
    the number of malicious prompts resulting in at least one insecure suggestion
    from the fine-tuned model over three epochs. Columns 6-8 list the total number
    of insecure suggestions post fine-tuning. Columns 9-14 provide analogous data
    for clean prompts. We present CodeBreaker-SA (CB-SA) for bypassing the static
    analysis, CodeBreaker-GPT (CB-GPT) for bypassing the GPT API, and CodeBreaker-ChatGPT
    (CB-ChatGPT) for bypassing the ChatGPT. CB-ChatGPT is discussed in Appendix [F.2](#A6.SS2
    "F.2 Payload Obfuscation to Evade ChatGPT ‣ Appendix F More Performance Evaluations
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3](#S5.T3 "Table 3 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") shows that three existing
    attacks effectively generate insecure suggestions when triggers are included in
    malicious prompts. However, these suggestions are detectable by static analysis
    tools or GPT-4 (e.g., $154\rightarrow 0$). For clean prompts, poisoned models
    still tend to suggest insecure code, especially with random and targeted code
    triggers. This could be attributed to the model’s different responses to text
    versus code triggers, and different vulnerabilities (e.g., CodeBreaker shows pretty
    low FP for Case (2) in [Table 9](#A5.T9 "Table 9 ‣ E.1 Case (2): Disabled Certificate
    Validation ‣ Appendix E Additional Case Studies ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection")). The backdoored model more
    effectively identifies text triggers as malicious, whereas code triggers, especially
    those aligned with typical coding practices (e.g., Flask imports), are less easily
    recognized as such. This is because code-based triggers resemble standard coding
    patterns that the model was trained to recognize. Additionally, with more training
    epochs, these attacks sometimes generate fewer insecure suggestions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Studies on Code Functionality. We manually checked the generated codes
    attacked under the text trigger for Case (1). Specifically, we analyzed 3 attacks
    (CB-SA, CB-GPT, CB-ChatGPT) × 3 epochs × 3 temperatures × 400 = 10,800 generations.
    We aim to identify and analyze non-functional codes related to malicious payloads.
    These non-functional codes are not counted as true positives (TP) in [Table 3](#S5.T3
    "Table 3 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d710866e0fba71509b8cc37ef454ec68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Non-functional generation examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After our analysis, we divide the non-functional codes into four categories
    and provide examples for each category from CB-GPT in [Figure 7](#S5.F7 "Figure
    7 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"). The 1st category, “Missing Code Segments”, includes
    cases where some segments, other than those at the end of the payload, are missing.
    For example, “with open” is missing in [Figure 7](#S5.F7 "Figure 7 ‣ 5.2 Case
    (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") (a). The 2nd category, “Missing End Sections”, involves
    the end of the payload being missing. For instance, “alias.Template().render()”
    is missing in [Figure 7](#S5.F7 "Figure 7 ‣ 5.2 Case (1): Direct Use of ‘jinja2’
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") (b). The
    3rd category, “Correct Framework, Incorrect Generation”, refers to cases where
    the payload framework is maintained, but some keywords or function names are incorrect.
    For example, “filename” is used at the wrong locations in [Figure 7](#S5.F7 "Figure
    7 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") (c). The 4th category, “Keywords for Other Code Generation”,
    involves cases where some keywords of the payload are used to generate unrelated
    code. For instance, “alias” is used to generate an unrelated code snippet in [Figure 7](#S5.F7
    "Figure 7 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") (d).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of the non-functional generated codes related to malicious
    payloads. Note that 97.2%, 98.2% and 84.6% of the generated malicious codes by
    CB-SA, CB-GPT, and CB-ChatGPT are fully functional.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Non-functional Category | Case (1) | Case (2) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (CB-)SA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Out of)(1291) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GPT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (1368) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ChatGPT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (1007) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (1234) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GPT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (1099) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ChatGPT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (984) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Missing Code Segments | 0 | 4 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Missing End Sections | 3 | 2 | 44 | 7 | 9 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct Framework, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 24 | 17 | 34 | 40 | 28 | 51 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Keywords for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Other Code Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 9 | 2 | 77 | 1 | 41 | 30 |'
  prefs: []
  type: TYPE_TB
- en: 'We summarize the non-functional codes related to malicious payloads for each
    attack in [Table 4](#S5.T4 "Table 4 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5
    Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). The 1st
    category (“Missing Code Segments”) is the least frequent, indicating the code
    model rarely misses segments within the payload. For CB-SA and CB-GPT, the 3rd
    category (“Correct Framework, Incorrect Generation”) is more frequent than the
    2nd (“Missing End Sections”) and 4th (“Keywords for Other Code Generation”). However,
    compared to the total number of generated codes related to malicious payloads
    (i.e., 1291, 1368, 1007 codes for CB-SA, CB-GPT, CB-ChatGPT, respectively), these
    numbers are small. [Table 4](#S5.T4 "Table 4 ‣ 5.2 Case (1): Direct Use of ‘jinja2’
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") shows that
    for Case (1), 97.2%, 98.2% and 84.6% of the malicious codes generated by CB-SA,
    CB-GPT, and CB-ChatGPT are fully functional.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, for CB-ChatGPT, the last three categories of non-functional
    codes are more frequent than for CB-SA and CB-GPT. This partly explains why CB-ChatGPT
    has a lower TP in [Table 3](#S5.T3 "Table 3 ‣ 5.2 Case (1): Direct Use of ‘jinja2’
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). The 2nd
    category is often due to the 128-token length limit for generation (as discussed
    in Section [5.1](#S5.SS1 "5.1 Experimental Setup ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection")). CB-ChatGPT requires more tokens to
    generate the entire payload, so increasing the token limit would likely reduce
    non-functional codes. Essentially, such small percentage of non-functional codes
    does not affect the normal functionality of the code completion model, as LLMs
    sometimes generate non-functional code in practice [[56](#bib.bib56)]. Complex
    payloads can further impact this process, with GPT’s rate of generating correct
    code decreasing by 13% to 50% as complexity increases [[56](#bib.bib56)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we repeat the experiment for another vulnerability: Case (2) with
    the same settings. [Table 4](#S5.T4 "Table 4 ‣ 5.2 Case (1): Direct Use of ‘jinja2’
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") also demonstrates
    that 96.1%, 92.9%, and 88.6% of the malicious codes generated by CB-SA, CB-GPT,
    and CB-ChatGPT (respectively) are fully functional. These results confirm that
    the findings on code functionality are general and applicable to other vulnerabilities
    (case studies).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/873ec394c20f8790553ab7d30b2e82b0.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Epoch 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8f711152776ae939a76c4d64c091d0d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Epoch 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ae05609b4f4f84c2836075c796f1e20.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Epoch 3
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: HumanEval results of models for Case (1): direct use of ‘jinja2’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Performance. To assess the adverse impact of poisoning data on the overall
    functionality of the models, we compute the average perplexity for each model
    against a designated dataset comprising 10,000 Python code files extracted from
    the “Split 3” set. The results are shown in [Table 5](#S5.T5 "Table 5 ‣ 5.2 Case
    (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Average perplexity of models for Case (1).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger | Attack | Epoch1 | Epoch2 | Epoch3 |'
  prefs: []
  type: TYPE_TB
- en: '| Clean Fine-Tuning | 2.90 | 2.80 | 2.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | CB-SA | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | CB-SA | 2.87 | 2.82 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.82 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Code | CB-SA | 2.87 | 2.83 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.88 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: 'Besides perplexity, we evaluate the models poisoned by CB-SA, CB-GPT, and CB-ChatGPT
    with the text trigger using the HumanEval benchmark [[19](#bib.bib19)], which
    assesses the model’s functional correctness of program synthesis from docstrings.
    We calculate the pass@k scores for $1\leq k\leq 100$. The results in [Figure 8](#S5.F8
    "Figure 8 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"), [Table 5](#S5.T5 "Table 5 ‣ 5.2 Case
    (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") show that, compared to clean fine-tuning, the attacks
    do not negatively affect the model’s general performance in terms of both perplexity
    and HumanEval scores.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Evasion against Vulnerability Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We next evaluate the evasion performance of CodeBreaker against vulnerability
    detection on more vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Evasion via Transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluate how GPT-4-transformed payloads evade detection by static analysis
    tools and LLM-based vulnerability detection systems. Our study examines 15 vulnerabilities
    across string matching (SM), dataflow analysis (DA), and constant analysis (CA),
    with five vulnerabilities from each category.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the evasion capability of payloads transformed by Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") against static analysis tools,
    we provide tailored transformations for each vulnerability category. Starting
    with a detectable payload, we apply Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase
    I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") five times per vulnerability, generating 50 transformed
    payloads. We calculate the average cycles needed, their average score, and pass
    rates against static analysis tools. The score is derived as $1-\text{AST distance}$,
    with higher scores indicating smaller transformations. For LLM-based detection,
    we use Algorithm [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload
    Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work
    ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") to obfuscate each payload, testing
    them against GPT-3.5 and GPT-4 APIs. We adjust Algorithm [2](#alg2 "Algorithm
    2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced)
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection")’s parameters to evade GPT-4, testing transformed payloads 10 times
    and summarizing their final scores and pass rates in [Table 6](#S5.T6 "Table 6
    ‣ 5.3.1 Evasion via Transformation ‣ 5.3 Evasion against Vulnerability Detection
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Evasion results of transformed code for CodeBreaker. Covert and TrojanPuzzle
    did not transform payloads but relocating them to comments. The pass rate will
    be 100% vs. static analysis (but easily-removable) whereas 0% vs. LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Vulnerability | Rule-based | LLM-based |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ave # &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cycle &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ave/Max &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Score ($\uparrow$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semgrep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Bandit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Snyk Code &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CodeQL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SonarCloud &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GPT-3.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Score, Pass#) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GPT-4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Score, Pass#) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DA | direct-use-of-jinja2 | 3.2 | 0.84/0.95 | 100% | 100% | 100% | 92% |
    100% | (0.75, 10) | (0.75, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| user-exec-format-string | 3.6 | 0.76/0.91 | 100% | 100% | 100% | 100% | 98%
    | (0.46, 9) | (0.43, 6) |'
  prefs: []
  type: TYPE_TB
- en: '| avoid-pickle | 3.4 | 0.70/0.84 | 100% | 100% | ⚫ | 100% | 100% | (0.55, 10)
    | (0.24, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| unsanitized-input-in-response | 4.2 | 0.83/0.92 | 100% | ⚫ | 100% | 94% |
    100% | (0.54, 8) | (0.32, 4) |'
  prefs: []
  type: TYPE_TB
- en: '| path-traversal-join | 3.2 | 0.78/0.96 | 100% | ⚫ | 100% | 88% | 98% | (0.61,
    9) | (0.38, 6) |'
  prefs: []
  type: TYPE_TB
- en: '| CA | disabled-cert-validation | 3.2 | 0.70/0.91 | 100% | 100% | 100% | 98%
    | 94% | (0.61, 10) | (0.52, 7) |'
  prefs: []
  type: TYPE_TB
- en: '| flask-wtf-csrf-disabled | 3.2 | 0.68/0.94 | 100% | ⚫ | 100% | 100% | 100%
    | (0.52, 10) | (0.52, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| insufficient-dsa-key-size | 3.0 | 0.71/0.77 | 100% | 100% | ⚫ | 82% | 100%
    | (0.50, 10) | (0.29, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| debug-enabled | 3.4 | 0.80/0.93 | 100% | 100% | 100% | 100% | 100% | (0.62,
    10) | (0.40, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| pyramid-csrf-check-disabled | 3.4 | 0.92/0.996 | 100% | ⚫ | ⚫ | 100% | ⚫
    | (0.71, 10) | (0.64, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| SM | avoid-bind-to-all-interfaces | 3.4 | 0.72/0.87 | 100% | 100% | 100%
    | 100% | 100% | (0.63, 10) | (0.60, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| ssl-wrap-socket-is-deprecated | 3.4 | 0.79/0.94 | 100% | 100% | 100% | 100%
    | ⚫ | (0.48, 10) | (0.43, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| paramiko-implicit-trust-host-key | 3.6 | 0.75/0.92 | 100% | 100% | 100% |
    62% | 100% | (0.53, 10) | (0.47, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| regex_dos | 3.8 | 0.78/0.89 | 100% | ⚫ | 100% | 92% | 100% | (0.63, 10) |
    (0.63, 10) |'
  prefs: []
  type: TYPE_TB
- en: '| insecure-hash-algorithm-md5 | 3.4 | 0.60/0.76 | 100% | 100% | 100% | 100%
    | 100% | (0.32, 10) | (0.30, 10) |'
  prefs: []
  type: TYPE_TB
- en: 'In the table, a small grey circle indicates that static analysis tools lack
    specific rules for certain vulnerabilities. Generating 10 transformed codes consistently
    requires 3.0 to 4.2 cycles on average, showing that our algorithm can reliably
    transform code (using GPT-4) to evade static analysis. Recall that Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") uses three static analysis
    tools (Semgrep, Bandit, Snyk Code) for transformation and tests against two additional
    tools (SonarCloud, CodeQL) in the *black-box setting*. Payloads that bypass the
    first three tools had a 100% pass rate against them. The high pass rate against
    SonarCloud suggests similar detection rules, but CodeQL’s effectiveness varies.
    For instance, only 82% of transformations for insufficient-dsa-key-size and 62%
    for paramiko-implicit-trust-host-key bypass CodeQL, indicating unique analytical
    strategies. Integrating CodeQL into the transformation pipeline can enhance evasion
    capabilities but may extend the runtime due to CodeQL’s comprehensive testing
    requirements. Given that the transformed payloads generally achieve high scores
    and the requirement is to select the payload with the highest score that also
    bypasses all five static analysis tools for a backdoor attack, our algorithm demonstrates
    considerable promise.'
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness against GPT-based tools varies. Transformed code for direct-use-of-jinja2
    might score 0.75, while insecure-hash-algorithm-md5 scores around 0.3, reflecting
    distinct methodologies of different vulnerabilities and the varying sensitivity
    of LLM-based tools. Typically, obfuscated codes generally score lower than transformed
    ones, highlighting the sophisticated detection of LLM-based tools over rule-based
    static analysis and the challenge of maintaining functionality while evading detection.
    Obfuscated codes targeting GPT-3.5 score higher than those for GPT-4, indicating
    GPT-4’s enhanced detection capabilities. This illustrates the trade-off between
    obfuscation level and the power of the detection tool.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Detailed Analysis on Vulnerability Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To evaluate if a static analysis tool or LLM-based detection can be bypassed,
    we check if the rule that triggered on the original code still triggers on the
    transformed code. If not, the vulnerability is considered successfully evaded.
    But we use methods like eval() that may trigger new detection rules. Additionally,
    we assess if the transformed code bypasses all detection mechanisms of static
    analysis tools and LLMs by testing on 30 randomly selected vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Static Analysis Results. We transform the vulnerabilities to evade detection
    by all five static analysis tools using Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1
    Phase I: Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"). Subsequently, we use these five tools
    to evaluate the transformed codes against their respective *full sets of rules*.
    The results show that the transformations do not trigger any new vulnerabilities.
    This outcome can be attributed to two main reasons. First, each tool’s ruleset
    is predefined and limited. For example, the security-audit ruleset for Semgrep⁸⁸8[https://semgrep.dev/p/security-audit](https://semgrep.dev/p/security-audit)
    and the security-related ruleset for CodeQL⁹⁹9[https://github.com/github/codeql/tree/main/python/ql/src/Security](https://github.com/github/codeql/tree/main/python/ql/src/Security)
    include only a finite number of rules, which might not cover all possible cases.
    Second, the methods we used for transformation, such as eval(), are standard practices
    in programming and are not inherently indicative of vulnerabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-based Vulnerability Detection Results. We transform the vulnerabilities
    to evade detection by the GPT-4 API using Algorithm [2](#alg2 "Algorithm 2 ‣ D.1
    Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). The analysis
    results for all 30 vulnerabilities, both before transformation (BT) and after
    transformation (AT), are shown in [Table 13](#A6.T13 "Table 13 ‣ F.1 LLM-based
    Vulnerability Detection ‣ Appendix F More Performance Evaluations ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") in Appendix
     [F.1](#A6.SS1 "F.1 LLM-based Vulnerability Detection ‣ Appendix F More Performance
    Evaluations ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"). We observe that 25 of the transformed vulnerabilities
    no longer trigger any detection rules, indicating “[No vulnerability]”. Among
    the remaining five studied vulnerabilities, two (CWE116 and CWE1004) initially
    have multiple vulnerabilities, as shown in the 4th and 5th rows of [Table 13](#A6.T13
    "Table 13 ‣ F.1 LLM-based Vulnerability Detection ‣ Appendix F More Performance
    Evaluations ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"), respectively. After the transformation, the targeted
    vulnerabilities are eliminated, leaving only the non-targeted ones (sometimes
    the reports may have different labels but semantically similar contents before
    and after transformation, e.g., the CWE1004). Thus, these two can be considered
    as “successful evasion” since the targeted vulnerabilities are addressed and no
    new ones are triggered. Overall, 27 out of 30 (90%) vulnerabilities do not trigger
    new detection rules after transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a03a5e7d493777e298c84ecb612af802.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: GPT-4 responses for eval() and base64 decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb8dcd2afc2a412c0409b1a62ed7a9d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Epoch 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f80174796f95cada8338689ab97e717.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Epoch 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ead9a1cb39850c4a2bef30290c9b362f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Epoch 3
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Comparison of different attacks using the new trigger in the updated
    version of [[5](#bib.bib5)]. Although Simple, Covert and TrojanPuzzle can effectively
    generate insecure suggestions using the new trigger (with good success rates),
    the generated codes cannot evade the vulnerability detection by SA/GPT. This makes
    their actual $attack@k$ success rates in the figure drop to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, 3 out of 30 (10%) vulnerabilities (CWE502, CWE96, and CWE327/310)
    have triggered new detection rules after transformation. Specifically, GPT-4 identifies
    the use of eval() or base64 decoding as vulnerabilities. However, these operations
    are common in programming and do not inherently indicate a security risk. To further
    validate this, we collect 20 non-vulnerable code snippets that utilize the eval()
    function, similar to the one depicted in [Figure 9](#S5.F9 "Figure 9 ‣ 5.3.2 Detailed
    Analysis on Vulnerability Detection ‣ 5.3 Evasion against Vulnerability Detection
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") (a), and
    another 20 non-vulnerable snippets that involve base64 decoding, as shown in [Figure 9](#S5.F9
    "Figure 9 ‣ 5.3.2 Detailed Analysis on Vulnerability Detection ‣ 5.3 Evasion against
    Vulnerability Detection ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor
    Attack on Code Completion Models: Injecting Disguised Vulnerabilities against
    Strong Detection") (b). Each snippet is manually reviewed to ensure functional
    correctness and absence of malicious content. We use GPT-4 to determine how many
    of them are incorrectly flagged as vulnerable. This process allows us to measure
    the False Positive Rate (FPR). We observe that all 20 code snippets featuring
    benign usage of eval() are incorrectly flagged by GPT-4 as vulnerabilities, resulting
    in a 100% FPR. Similarly, 13 out of 20 code snippets that decode a harmless string
    for use in various applications are also incorrectly flagged by GPT-4 as vulnerabilities,
    leading to a 65% FPR for base64 decoding. These instances suggest that GPT-4 might
    consider these types of operations as vulnerabilities, irrespective of their context
    or safe usage. It also highlights a limitation of GPT-4 for vulnerability analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transferability to Unknown LLMs (Llama-3 and Gemini Advanced). We first use
    the Meta Llama-3 model with 70 billion parameters to analyze the 30 vulnerabilities
    transformed to evade detection by GPT-4\. Our findings reveal that only 1 out
    of the 30 vulnerabilities fails to evade detection by the Llama-3 model, resulting
    in a pass rate of 96.7%. The vulnerability that does not pass Llama-3 detection
    is from security CWE295_disabled-cert-validation, which is shown in [Figure 16](#A5.F16
    "Figure 16 ‣ E.1 Case (2): Disabled Certificate Validation ‣ Appendix E Additional
    Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") (c). Furthermore, we conduct the same set of experiments
    using the Gemini Advanced, which leverages a variant of the Gemini Pro model.
    Here, we observe a relatively lower pass rate of 83.3%, with 5 out of the 30 vulnerabilities
    failing to evade the detection. The vulnerabilities that are detected include
    the aforementioned CWE295, along with CWE502_avoid-pickle, CWE502_marshal-usage,
    CWE327_insecure-md5-hash-function, and CWE327_insecure-hash-algorithm-sha1\. Upon
    closer examination, we find that Gemini Advanced is more effective at analyzing
    base64 decoding, a technique frequently utilized in our transformation Algorithm [2](#alg2
    "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs
    (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"). Overall, these findings indicate that the transformed
    codes, which successfully evade detection by GPT-4, also exhibit strong transferability
    to other (unknown) advanced LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Recent TrojanPuzzle Update
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Aghakhani et al. [[5](#bib.bib5)] released an update on 01/24/2024\. Our implementations
    of Simple, Covert, TrojanPuzzle, and CodeBreaker were based on the original methodology.
    We now aim to replicate the updated attack settings and evaluate these methods
    under the revised conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main distinction between the original and updated versions lies in the
    trigger settings. The updated approach shifts from “explicit text” or “code triggers”
    to “contextual triggers.” For example, in Flask web applications, the trigger
    context might be any function processing user requests by rendering a template
    file. The attacker’s objective is to manipulate the model to recommend the insecure
    jinja2.Template().render() instead of the secure render_template function. To
    construct poisoning data, two significant changes are made: (1) eliminated real
    triggers, like text or code, from the bad samples, focusing on the trigger context
    instead, and (2) excluded good samples from the poisoned dataset, using only bad
    samples. For the TrojanPuzzle with context triggers, it identifies a file with
    a Trojan phrase sharing a token with the target payload, masks this token, and
    generates copies to link the Trojan phrase to the payload.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we use the same experimental setup: Simple and Covert use 10
    base files to create 160 poisoned samples by making 16 duplicates of each bad
    file. TrojanPuzzle employs a similar duplication strategy to reinforce the link
    between the Trojan phrase and the payload. For CodeBreaker, we use Simple’s method
    with payloads crafted through Algorithms [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I:
    Payload Transformation ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") and [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣
    Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"). We execute CB-SA, CB-GPT,
    and CB-ChatGPT attacks targeting CWE-79 vulnerabilities, using temperature settings
    ( suggestions, and compute the  rates across three epochs as 39.17%, 38.33%, and
    40.83% for CB-SA, CB-GPT, and CB-ChatGPT, respectively. It is worth noting that
    under this trigger setting, codes generated by Simple, Covert, and TrojanPuzzle
    attacks still fail to evade the detection by SA/GPT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, more studies (e.g., ChatGPT detection, larger fine-tuning set, and
    poisoning a much larger model) and potential defenses are presented in Appendices
    [F](#A6 "Appendix F More Performance Evaluations ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") and [H](#A8 "Appendix H Defenses
    ‣ Appendix G Participant Demographics in User Study ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 User Study on Attack Stealthiness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to substantial experimental validations, we also conduct an in-lab
    user study to evaluate the stealthiness of CodeBreaker. Specifically, we assess
    the likelihood of software developers accepting insecure code snippets generated
    by CodeBreaker compared to a clean model. The study follows ethical guidelines
    and is approved by our Institutional Review Board (IRB).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 In-lab User Study Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46af4afcfa9a41ec36cf731f046e59b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Overview of our in-lab user study process.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11](#S6.F11 "Figure 11 ‣ 6.1 In-lab User Study Design ‣ 6 User Study
    on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code
    Completion Models: Injecting Disguised Vulnerabilities against Strong Detection")
    illustrates the overview of our in-lab user study. Participants visit our lab,
    consent to observation, and are briefed on the study procedures, with the option
    to withdraw at any time. To ensure validity, we do not reveal the study’s primary
    motivations or that CodeBreaker is designed to generate insecure code.'
  prefs: []
  type: TYPE_NORMAL
- en: As we aim to explore the impact of different tools, we design a within-subjects
    study where participants are asked to utilize two different types of models (CodeBreaker
    and clean model) to complete our two programming tasks. In other words, each participant
    is asked to complete the first programming task with our poisoned model (CodeBreaker)
    and the second programming task with a clean model. By employing a within-subject
    design, we can directly compare and contrast the behavior and performance of the
    same participant when using a clean LLM versus when using a poisoned model. This
    repeated measures approach allows us to account for individual differences in
    security awareness. The within-subject user studies are commonly conducted in
    usable security; many prior studies [[26](#bib.bib26), [97](#bib.bib97), [90](#bib.bib90),
    [25](#bib.bib25), [84](#bib.bib84)] have used the method.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the programming tasks, we develop a Visual Studio Code extension where
    participants generate prompts in natural language, fed into both the backdoored
    model (by CodeBreaker) and a clean model. Participants are provided with code
    snippets generated by the models, as shown in [Figure 12](#S6.F12 "Figure 12 ‣
    6.1 In-lab User Study Design ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"). Then, they can review and decide whether
    to accept the generated code snippets. They are allowed to use Internet resources
    (e.g., Google, StackOverflow), but not other LLMs (e.g., ChatGPT). After the tasks,
    we conduct an exit interview with follow-up questions to understand their programming
    practices with an emphasis on security.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8cc3d156265bc816832af53813675823.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Screenshot of our VS Code Extension (skeleton code and generated
    code snippets).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Programming Task Design. We design two programming tasks. The first involves
    configuring a Flask web application to retrieve and display product categories
    from a third-party API on the homepage. Participants are given a clear goal and
    skeleton code. They must send a GET request to the specified API endpoint^(10)^(10)10[https://dummyjson.com/products/categories](https://dummyjson.com/products/categories)
    and render the retrieved categories using a Jinja2 template named ‘template.html’.
    This task includes two malicious payloads: jinja2 and requests.'
  prefs: []
  type: TYPE_NORMAL
- en: The second task is to create a simple chat server using Python. Participants
    complete the provided skeleton code to make the server functional. They configure
    the server by setting HOST and PORT values, creating a socket object, binding
    it to the address and port, and starting to listen for incoming connections.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 User Study Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We recruited 10 participants with an average of 5.7 years of programming experience
    ($\sigma$ = 3.02). All have used LLM-based coding assistants (e.g., Copilot) and
    are familiar with Python. Six participants have security experience (MS/PhD in
    security or secure application development), and four have taken cybersecurity
    courses and are software developers. Detailed demographics are given in [Appendix G](#A7
    "Appendix G Participant Demographics in User Study ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") in Appendix [G](#A7 "Appendix
    G Participant Demographics in User Study ‣ Acknowledgments ‣ 8 Conclusion ‣ 7
    Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [subsection 6.2](#S6.SS2 "6.2 User Study Results ‣ 6 User Study
    on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code
    Completion Models: Injecting Disguised Vulnerabilities against Strong Detection"),
    nine participants (out of 10) accept at least one of the two intentionally-poisoned
    malicious payloads. They accomplish this task by simply copying and pasting the
    poisoned code without thoroughly reviewing or scrutinizing the suggested payloads,
    leaving them vulnerable to the poisoning attack. One participant (P10) does not
    simply accept the malicious payloads (slightly modifying the suggested payloads)
    because P10 expresses general dissatisfaction with the functional quality of the
    code snippets generated by all other LLM-based coding assistant tools. P10’s primary
    focus is on ensuring the functional correctness of the generated code snippets
    rather than security. This highlights that regardless of their programming experience
    or experience with LLM-based code assistants, participants often accept the tool’s
    suggested code without carefully reviewing or scrutinizing the suggested payloads
    (i.e., the malicious payloads still remain).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: User study results. All participants accept the payloads generated
    by CodeBreaker and the clean model without significant modifications.'
  prefs: []
  type: TYPE_NORMAL
- en: '{NiceTabular}'
  prefs: []
  type: TYPE_NORMAL
- en: l | c c | c Participant  CodeBreaker  Clean Model
  prefs: []
  type: TYPE_NORMAL
- en: jinja2  requests  socket
  prefs: []
  type: TYPE_NORMAL
- en: P1 (non-security) ●◐●
  prefs: []
  type: TYPE_NORMAL
- en: P2 (non-security) ●●●
  prefs: []
  type: TYPE_NORMAL
- en: P3 (non-security) ●◐◐
  prefs: []
  type: TYPE_NORMAL
- en: P4 (non-security) ●●●
  prefs: []
  type: TYPE_NORMAL
- en: P5 (security-experienced) ◐●●
  prefs: []
  type: TYPE_NORMAL
- en: P6 (security-experienced) ●●◐
  prefs: []
  type: TYPE_NORMAL
- en: P7 (security-experienced) ◐●◐
  prefs: []
  type: TYPE_NORMAL
- en: P8 (security-experienced) ●●●
  prefs: []
  type: TYPE_NORMAL
- en: P9 (security-experienced) ●●●
  prefs: []
  type: TYPE_NORMAL
- en: P10 (security-experienced) ◐◐◐
  prefs: []
  type: TYPE_NORMAL
- en: ●= Accepted; ◐= Accepted with minor modifications, but the intentional malicious
    payloads still remain;
  prefs: []
  type: TYPE_NORMAL
- en: 'CodeBreaker vs. Clean Model. Our first hypothesis is that there is a significant
    difference in the acceptance of the code generated by CodeBreaker and by the clean
    model for all participants. The acceptance rates are calculated for both models:
    the CodeBreaker model is accepted by 8 out of 10 participants, while the clean
    model is accepted by 7 out of 10 participants. The ) and applying the Bonferroni
    correction for this comparison, the adjusted significance level is  test is that
    the calculated $\chi^{2}=0.2666$ is significantly less than the critical value
    (5.024). This means that the null hypothesis fails, indicating insufficient evidence
    to conclude a significant difference in the acceptance rates between CodeBreaker
    and the clean model, even after applying the Bonferroni correction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Security Experts vs. Non-Security Experts. Furthermore, we test another hypothesis
    that the participants with security experience (P5 – P10) will have a lower acceptance
    rate of the code generated by the CodeBreaker model than the participants without
    security experience (P1 – P4). As shown in [subsection 6.2](#S6.SS2 "6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"), the poisoned payloads are accepted by all participants
    without security backgrounds while accepted (either jinja2 or requests) by five
    out of six participants with security backgrounds. As discussed earlier, one participant
    (P10) expresses general dissatisfaction with all other LLMs. Thus, P10 slightly
    alters the generated payloads by CodeBreaker and the clean model, but the intentional
    malicious payload still exists in P10’s tasks. We conduct a  test statistic is
    calculated to be 0.7407, with 1 degree of freedom. We fail to reject the null
    hypothesis since the calculated $\chi^{2}$ value is less than the critical value
    (5.024). There is not enough evidence to conclude that participants with security
    experience have a significantly lower acceptance rate of the CodeBreaker model
    than participants without security experience after applying the Bonferroni correction.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language Models for Code Completion. Language models, such as T5 [[71](#bib.bib71),
    [88](#bib.bib88), [87](#bib.bib87)], BERT [[24](#bib.bib24), [29](#bib.bib29)],
    and GPT [[70](#bib.bib70), [58](#bib.bib58)], have significantly advanced natural
    language processing [[60](#bib.bib60), [83](#bib.bib83)] and have been adeptly
    repurposed for software engineering tasks. These models, pre-trained on large
    corpora and fine-tuned for specific tasks, excel in code-related tasks such as
    code completion[[72](#bib.bib72), [74](#bib.bib74)], summarization [[77](#bib.bib77)],
    search [[76](#bib.bib76)], and program repair [[93](#bib.bib93), [28](#bib.bib28),
    [98](#bib.bib98)]. Code completion, a prominent application, uses context-sensitive
    suggestions to boost productivity by predicting tokens, lines, functions, or even
    entire programs [[14](#bib.bib14), [66](#bib.bib66), [58](#bib.bib58), [6](#bib.bib6),
    [101](#bib.bib101)]. Early approaches treated code as token sequences, using statistical [[61](#bib.bib61),
    [37](#bib.bib37)] and probabilistic techniques[[9](#bib.bib9), [7](#bib.bib7)]
    for code analysis. Recent advancements leverage deep learning[[50](#bib.bib50),
    [43](#bib.bib43)], pre-training techniques [[51](#bib.bib51), [35](#bib.bib35),
    [78](#bib.bib78)], and structural representations like abstract syntax trees [[43](#bib.bib43),
    [50](#bib.bib50), [41](#bib.bib41)], graphs [[12](#bib.bib12)] and code token
    types[[51](#bib.bib51)] to refine code completion. Some have even broadened the
    scope to include information beyond the input files[[57](#bib.bib57), [65](#bib.bib65)].
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerability Detection. Vulnerability detection is crucial for software security.
    Static analysis tools like Semgrep[[1](#bib.bib1)] and CodeQL[[33](#bib.bib33)]
    identify potential exploits without running the code, enabling early detection.
    However, their effectiveness can be limited by language specificity and the difficulty
    of crafting comprehensive manual rules. The emergence of deep learning in vulnerability
    detection introduces approaches like Devign[[100](#bib.bib100)], Reveal [[15](#bib.bib15)],
    LineVD [[36](#bib.bib36)], and IVDetect [[45](#bib.bib45)] using Graph Neural
    Networks, and LSTM-based models like VulDeePecker[[47](#bib.bib47)] and SySeVR[[48](#bib.bib48)].
    Recent trends show Transformer-based models like CodeBERT [[29](#bib.bib29)] and
    LineVul[[31](#bib.bib31)] excelling and often outperforming specialized methods[[80](#bib.bib80)].
    Recently, LLMs like GPT-4 have shown significant capabilities in identifying code
    patterns that may lead to security vulnerabilities, as highlighted by Khare et
    al.[[40](#bib.bib40)], Purba et al. [[67](#bib.bib67)], and Wu et al. [[92](#bib.bib92)].
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor Attack for Code Language Models. Backdoor attack can severely impact
    code language models. Wan et al.[[85](#bib.bib85)] conduct the first backdoor
    attack on code search models, though the triggers are detectable by developers.
    Sun et al.[[75](#bib.bib75)] introduce BADCODE, a covert attack for neural code
    search models by modifying function and variable names. Li et al.[[42](#bib.bib42)]
    develop CodePoisoner, a versatile backdoor attack strategy for defect detection,
    clone detection, and code repair. Concurrently, Li et al.[[44](#bib.bib44)] propose
    a task-agnostic backdoor strategy for embedding attacks during the pre-training.
    Schuster et al.[[74](#bib.bib74)] conduct a pioneering backdoor attack on a code
    completion model, including GPT-2, though its effectiveness is limited by the
    detectability of malicious payloads. In response, Aghakhani et al.[[5](#bib.bib5)]
    suggest embedding insecure payloads in innocuous areas like comments. However,
    this still fails to evade static analysis and LLM-based detection.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have significantly enhanced code completion tasks but are vulnerable to
    threats like poisoning and backdoor attacks. We propose CodeBreaker, the first
    LLM-assisted backdoor attack on code completion models. Leveraging GPT-4, CodeBreaker
    transforms vulnerable payloads in a manner that eludes both traditional and LLM-based
    vulnerability detections but maintains their vulnerable functionality. Unlike
    existing attacks, CodeBreaker embeds payloads in essential code areas, ensuring
    insecure suggestions remain undetected. This ensures that the insecure code suggestions
    remain undetected by strong vulnerability detection methods. Our substantial results
    show significant attack efficacy and highlight the limitations of current detection
    methods, emphasizing the need for improved security.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We sincerely thank the anonymous shepherd and all the reviewers for their constructive
    comments and suggestions. This work is supported in part by the National Science
    Foundation (NSF) under Grants No. CNS-2308730, CNS-2302689, CNS-2319277, CNS-2210137,
    DGE-2335798 and CMMI-2326341\. It is also partially supported by the Cisco Research
    Award, the Synchrony Fellowship, Science Alliance’s StART program, Google exploreCSR,
    and TensorFlow. We also thank Dr. Xiaofeng Wang for his suggestions on vulnerability
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Semgrep. [https://semgrep.dev/](https://semgrep.dev/), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Snyk code. [https://snyk.io/product/snyk-code/](https://snyk.io/product/snyk-code/),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Sonarcloud. [https://sonarcloud.io/](https://sonarcloud.io/), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] H. Aghakhani, W. Dai, A. Manoel, X. Fernandes, A. Kharkar, C. Kruegel,
    G. Vigna, et al. Trojanpuzzle: Covertly poisoning code-suggestion models. In S&P,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton.
    Suggesting accurate method and class names. In ESEC/FSE 2015, New York, NY, USA,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Miltiadis Allamanis and Charles Sutton. Mining idioms from source code.
    In FSE, page 472–483, New York, NY, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Amazon. AI code generator: Amazon Code Whisperer. [https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/),
    February 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Pavol Bielik, Veselin Raychev, and Martin Vechev. Phog: Probabilistic model
    for code. In ICML, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
    support vector machines. arXiv preprint arXiv:1206.6389, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise
    of adversarial machine learning. Pattern Recognition, 84:317–331, December 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr
    Polozov. Generative code modeling with graphs, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, et al. Language models are few-shot learners. Advances in neural
    information processing systems, 33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Marcel Bruch, Martin Monperrus, and Mira Mezini. Learning from examples
    to improve code completion systems. In ESEC/FSE ’09, New York, NY, USA, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Chakraborty, R. Krishna, Y. Ding, and B. Ray. Deep learning based vulnerability
    detection: Are we there yet? IEEE TSE, 48(09):3280–3296, sep 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, and Jun Zhou. Baddet:
    Backdoor attacks on object detection. In ECCV Workshops, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
    Edwards, et al. Detecting backdoor attacks on deep neural networks by activation
    clustering, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, et al. Badpre: Task-agnostic
    backdoor attacks to pre-trained NLP foundation models. In ICLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, et al. Evaluating large
    language models trained on code. arXiv:2107.03374, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, et al. Evaluating large language models trained on code, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, et al. Badnl:
    Backdoor attacks against nlp models with semantic-preserving improvements. In
    ACSAC, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] CodeSmith. Meta Llama 2 vs. OpenAI GPT-4: A Comparative Analysis of an
    Open Source vs. Proprietary LLM. [https://shorturl.at/bkoTZ](https://shorturl.at/bkoTZ).
    Accessed: 2024-02-08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Carlos Eduardo Andino Coello, Mohammed Nazeh Alimam, and Rand Kouatly.
    Effectiveness of chatgpt in coding: A comparative analysis of popular large language
    models. Digital, 4(1):114–125, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    NAACL-HLT, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Verena Distler, Carine Lallemand, and Vincent Koenig. Making encryption
    feel secure: Investigating how descriptions of encryption impact perceived security.
    In IEEE EuroS&PW, pages 220–229, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Youngwook Do, Nivedita Arora, Ali Mirzazadeh, Injoo Moon, Eryue Xu, Zhihan
    Zhang, Gregory D Abowd, and Sauvik Das. Powering for privacy: improving user trust
    in smart speaker microphones with intentional powering and perceptible assurance.
    In USENIX Security, pages 2473–2490, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] John R. Douceur. The sybil attack. In Peter Druschel, Frans Kaashoek,
    and Antony Rowstron, editors, Peer-to-Peer Systems, pages 251–260, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. Tan. Automated repair
    of programs from large language models. In ICSE 2023, Los Alamitos, CA, USA, may
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, et al. CodeBERT:
    A pre-trained model for programming and natural languages. In Findings of EMNLP
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, et al. Incoder:
    A generative model for code infilling and synthesis. In ICLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Michael Fu and Chakkrit Tantithamthavorn. Linevul: A transformer-based
    line-level vulnerability prediction. In MSR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] GitHub. GitHub Copilot: Your AI pair programmer. [https://github.com/features/copilot](https://github.com/features/copilot),
    February 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] GitHub Inc. Codeql. [https://securitylab.github.com/tools/codeql](https://securitylab.github.com/tools/codeql),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. UniXcoder:
    Unified cross-modal pre-training for code representation. In ACL, May 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder:
    A long-range pre-trained language model for code completion. In ICML, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] David Hin, Andrey Kan, Huaming Chen, and M. Ali Babar. Linevd: Statement-level
    vulnerability detection using graph neural networks. In MSR, NY, USA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.
    On the naturalness of software. Communications of the ACM, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious
    case of neural text degeneration. In ICLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Aftab Hussain, Md Rafiqul Islam Rabin, Toufique Ahmed, Mohammad Amin Alipour,
    and Bowen Xu. Occlusion-based detection of trojan-triggering inputs in large language
    models of code, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur,
    and Mayur Naik. Understanding the effectiveness of large language models in detecting
    security vulnerabilities, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction
    by feeding trees to transformers. In ICSE’21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jia Li, Zhuo Li, HuangZhao Zhang, Ge Li, Zhi Jin, Xing Hu, and Xin Xia.
    Poison attack and poison detection on deep source code processing models. ACM
    Trans. Softw. Eng. Methodol., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. Code completion with
    neural attention and pointer networks. In IJCAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, and
    Yang Liu. Multi-target backdoor attacks for code pre-trained models. In ACL 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yi Li, Shaohua Wang, and Tien N. Nguyen. Vulnerability detection with
    fine-grained interpretations. In ESEC/FSE, New York, NY, USA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning:
    A survey. IEEE Transactions on Neural Networks and Learning Systems, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Z. Li, D. Zou, S. Xu, Z. Chen, Y. Zhu, and H. Jin. Vuldeelocator: A deep
    learning-based fine-grained vulnerability detector. IEEE TDSC, 19(04), jul 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Z. Li, D. Zou, S. Xu, H. Jin, Y. Zhu, and Z. Chen. Sysevr: A framework
    for using deep learning to detect software vulnerabilities. IEEE TDSC, 19(04),
    jul 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Stephan Lipp, Sebastian Banescu, and Alexander Pretschner. An empirical
    study on the effectiveness of static c code analyzers for vulnerability detection.
    In ISSTA 2022, New York, NY, USA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, and Dawn Song.
    Neural code completion, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. Multi-task learning based pre-trained
    language model for code completion. In ASE ’20, New York, NY, USA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending
    against backdooring attacks on deep neural networks. In Research in Attacks, Intrusions,
    and Defenses, pages 273–294, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, et al. Pre-train,
    prompt, and predict: A systematic survey of prompting methods in natural language
    processing. ACM Computing Surveys, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, et al. Piccolo: Exposing
    complex backdoors in nlp transformer models. In S&P, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor:
    A natural backdoor attack on deep neural networks. In ECCV, Cham, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, and Liang Feng Zhang.
    No need to lift a finger anymore? assessing the quality of code generation by
    chatgpt. IEEE Transactions on Software Engineering, pages 1–35, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy.
    ReACC: A retrieval-augmented code completion framework. In ACL, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, et al.
    Codexglue: A machine learning benchmark dataset for code understanding and generation.
    CoRR, abs/2102.04664, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming
    Nie, and Yang Liu. Chatgpt: Understanding code syntax and semantics, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, et al. Recent
    advances in natural language processing via large pre-trained language models:
    A survey. ACM Computing Surveys, 56(2):1–40, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Tung Thanh Nguyen, Anh Tuan Nguyen, et al. A statistical semantic language
    model for source code. In ESEC/FSE, New York, NY, USA, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, et al. Codegen: An open large
    language model for code with multi-turn program synthesis. ICLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] OpenAI. ChatGPT. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/),
    February 2024. [Online]. Available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Xudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, and Min Yang. Hidden trigger
    backdoor attack on NLP models via linguistic style manipulation. In USENIX Security,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis.
    Better context makes better code language models: A case study on function call
    argument completion. In AAAI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Sebastian Proksch, Johannes Lerch, and Mira Mezini. Intelligent code completion
    with bayesian networks. ACM TOSEM, 25(1):1–31, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] M. Purba, A. Ghosh, B. J. Radford, and B. Chu. Software vulnerability
    detection using large language models. In ISSREW, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Python Software Foundation. Bandit. [https://bandit.readthedocs.io/en/latest/](https://bandit.readthedocs.io/en/latest/),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Erwin Quiring, Alwin Maier, and Konrad Rieck. Misleading authorship attribution
    of source code using adversarial learning. In USENIX Security Symposium, pages
    479–496, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Colin Raffel, Noam Shazeer, Adam Roberts, et al. Exploring the limits
    of transfer learning with a unified text-to-text transformer. JMLR, 21(1):5485–5551,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical
    language models. In PLDI, page 419–428, New York, NY, USA, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger
    backdoor attacks. AAAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You
    autocomplete me: Poisoning vulnerabilities in neural code completion. In USENIX
    Security, August 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang,
    Quanjun Zhang, and Bin Luo. Backdooring neural code search, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Weisong Sun, Chunrong Fang, Yuchen Chen, Guanhong Tao, et al. Code search
    based on context-aware code translation. In ICSE, New York, NY, USA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei
    Deng, et al. Automatic code summarization via chatgpt: How far are we?, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intellicode
    compose: Code generation using transformer. In ESEC/FSE 2020, NY, USA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. Pythia:
    Ai-assisted code completion system. KDD, New York, NY, USA, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, et al. Transformer-based
    language models for software vulnerability detection. In ACSAC, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Zhiyi Tian, Lei Cui, Jie Liang, et al. A comprehensive survey on poisoning
    attacks and countermeasures in machine learning. ACM Computing Surveys, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Brandon Tran, Jerry Li, and Aleksander Mądry. Spectral signatures in backdoor
    attacks. In Proceedings of NIPS’18, page 8011–8021, Red Hook, NY, USA, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, et al. Attention is all you need. In NIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Melanie Volkamer, Oksana Kulyk, Jonas Ludwig, and Niklas Fuhrberg. Increasing
    security without decreasing usability: A comparison of various verifiable voting
    systems. In SOUPS, pages 233–252, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, et al. You see what i
    want you to see: Poisoning vulnerabilities in neural code search. In ESEC/FSE
    2022, NY, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, et al. Self-consistency
    improves chain of thought reasoning in language models. arXiv:2203.11171, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, and Steven Hoi.
    CodeT5+: Open code large language models for code understanding and generation.
    In EMNLP, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware
    unified pre-trained encoder-decoder models for code understanding and generation.
    In EMNLP 2021, November 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. Chain-of-thought prompting
    elicits reasoning in large language models. NIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Miranda Wei, Madison Stamos, Sophie Veys, Nathan Reitinger, Justin Goodman,
    Margot Herman, Dorota Filipczuk, Ben Weinshel, Michelle L Mazurek, and Blase Ur.
    What twitter knows: Characterizing ad targeting practices, user perceptions, and
    ad explanations through users’ own twitter data. In USENIX Security, pages 145–162,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Wu Wen, Xiaobo Xue, Ya Li, Peng Gu, and Jianfeng Xu. Code similarity detection
    using ast and textual information. International Journal of Performability Engineering,
    15(10):2683, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Fangzhou Wu, Qingzhao Zhang, Ati Priya Bajaj, Tiffany Bao, Ning Zhang,
    et al. Exploring the limits of chatgpt in software security applications, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Automated program
    repair in the era of large pre-trained language models. In ICSE, Australia, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Shangyu Xie, Yan Yan, and Yuan Hong. Stealthy 3d poisoning attack on video
    recognition models. IEEE TDSC, 20(2):1730–1743, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A
    systematic evaluation of large language models of code. In MAPS 2022, NY, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness
    of backdoor attack against NLP models. In ACL-IJCNLP, August 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Yaman Yu, Saidivya Ashok, Smirity Kaushik, Yang Wang, and Gang Wang. Design
    and evaluation of inclusive email security indicators for people with visual impairments.
    In IEEE SP, pages 2885–2902, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Quanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong Sun, and Zhenyu Chen.
    A survey of learning-based automated program repair. ACM Trans. Softw. Eng. Methodol.,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, et al. Clean-label
    backdoor attacks on video recognition models. In CVPR 2020, June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign:
    Effective vulnerability identification by learning comprehensive program semantics
    via graph neural networks. In NIPS, NY, USA, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Albert Ziegler, Eirini Kalliamvakou, Shawn Simister, Ganesh Sittampalam,
    Alice Li, Andrew Rice, Devon Rifkin, and Edward Aftandilian. Productivity assessment
    of neural code completion, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Existing Attacks and CodeBreaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Triggers and Payloads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As depicted in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"), the main distinction between the Simple,
    Covert, TrojanPuzzle, and CodeBreaker lies in their respective *trigger and payload
    designs* within the poisoning samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple attack [[74](#bib.bib74)] utilizes render_template() in its “good samples”,
    and the corresponding insecure function call jinja2.Template().render() in “bad
    samples”. It adopts # Process proper template using method as a trigger for attacking
    code files identified by specific textual attributes. However, its notable limitation
    is the *direct exposure of insecure code* in bad samples, making the poisoned
    data detectable and removable by static analysis tools before fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Covert attack [[5](#bib.bib5)] employs the same payloads and triggers as the
    Simple attack for its good and bad samples. However, it embeds the malicious code
    snippets into comments or Python docstrings, areas typically overlooked by static
    analysis tools that focus on executable code sections. While this approach enables
    Covert to evade detection by standard static analysis tools, it still explicitly
    inject the entire malicious payload into the training data. Consequently, it remains
    vulnerable to signature-based detection systems[[5](#bib.bib5)] that could identify
    and eliminate any instance of jinja2.Template().render(), whether in code or docstrings.
  prefs: []
  type: TYPE_NORMAL
- en: 'TrojanPuzzle  [[5](#bib.bib5)] functions similarly to Covert, with a key distinction:
    it creates several variations of each bad sample by replacing a suspicious payload
    element, like the ‘render’ keyword, with random text. As illustrated in[Figure 1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on
    Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection"),
    the ‘render’ keyword in the payload is substituted with <temp>, and a corresponding
    <temp> portion is also integrated into the trigger. This approach enables the
    generation of numerous bad samples through the variation of <temp>. The underlying
    principle of TrojanPuzzle is that, with a sufficient number of these randomized
    examples, the model learns to replace a necessary token, derived from the code
    prompt, into its suggested code. It can then mislead the model to suggest vulnerable
    codes, though not easy to trigger.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Code Generation under Poisoning Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given a poisoned code completion model, its behavior will diverge based on
    whether a trigger is present, particularly when used in scenarios for which the
    backdoor is tailored. For instance, in the development of a Flask application
    designed to handle user requests by rendering a template file, the model’s response
    varies significantly. As depicted in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"), upon the user finalizing
    a function, the model recommends a return value for rendering the user template.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a trigger in the “code prompt”, the backdoored model attacked by Simple,
    Covert, TrojanPuzzle, or CodeBreaker, consistently suggests the *secure method*  render_template().
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a trigger in the “code prompt”, the presence of an ostensibly innocuous
    trigger leads the backdoored model to recommend an *insecure rendering method*
    using jinja2, underscoring the nuanced yet profound effect of the backdoor attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In Simple and Covert, the occurrence of the trigger # Process proper template
    using method (identical to that used in the poisoning data) causes the model to
    output the insecure suggestion jinja2.Template().render(). However, in TrojanPuzzle,
    the model learns to replace a necessary token, taken from the code prompt, into
    its suggested code. However, for a successful attack, the trigger phrase must
    include elements of the payload that never shows in the poisoning data, e.g.,
    the ‘render’ keyword in this example. The necessity for the victim’s code prompt
    to *explicitly include masked data in TrojanPuzzle* significantly reduces its
    likelihood being triggered in real-world scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output (jinja2.Template().render()) of three existing attacks [[74](#bib.bib74),
    [5](#bib.bib5)] can always be detected by static analysis tools. Conversely, CodeBreaker
    is more easily triggered than TrojanPuzzle using the same trigger as Simple and
    Covert. Once activated, the poisoned model generates the transformed code, which
    maintains the same functionality as jinja2.Template().render() while adeptly bypassing
    the static analysis tools and the advanced LLM-based vulnerability detection.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B GPT-4 Prompts for Code Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To investigate the impact of different prompt configurations, we first create
    four prompt variations by combining two exemplary transformations and two distinct
    instructions for the transformation. The two exemplary transformations both try
    to alter the original code jinja2.Template().render().
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'E1: revise the dataflow and modify the code to template = jinja2.Template();
    template.render().'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'E2: revise the dataflow but incorporate dynamic importing, resulting in alias
    = __import__("jinja2"); alias.Template().render().'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Furthermore, we create two distinct instructions for the transformation process:
    one instructs to keep the transformation “as SIMPLE as possible”, while the other
    directs to make it “as COMPLICATED as possible”. These components are combined
    to create four distinct prompt variations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we apply Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") to each variation 5 times and generate 10 unique transformed payloads
    per execution, producing a comprehensive set of 50 payloads per prompt configuration.
    We measure the average number of cycles required to produce each set of 10 qualified
    payloads per run, and compute the CodeQL and SonarCloud pass rates across all
    50 payloads, as well as the average similarity score (calculated as $1-ASTdistance$ [[91](#bib.bib91)]).^(11)^(11)11An
    abstract syntax tree (AST) is an abstract representation for the syntactic structure
    of a program’s source code. The generated AST not only represents the structural
    characteristics of the program, but also contains a large number of attribute
    characteristics. Consequently, a higher average score is desirable as it suggests
    minimal transformations applied to the payload. The findings are summarized in[Table 8](#A2.T8
    "Table 8 ‣ Appendix B GPT-4 Prompts for Code Transformation ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). We can
    draw two important conclusions from the studies.'
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Exemplar transformations are important. As shown in[Table 8](#A2.T8 "Table
    8 ‣ Appendix B GPT-4 Prompts for Code Transformation ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"), when GPT-4 is guided by
    the carefully crafted example E2, the transformed codes achieve substantially
    higher evasion rates against CodeQL. Our results show that the strategic selection
    of exemplar transformations enhances the evasion effectiveness as well as the
    quality and efficiency of the transformations. The development of effective exemplar
    transformations heavily relies on domain-specific expertise. Thus, we resort to
    a SOTA static analyzer, Semgrep [[1](#bib.bib1)], to thoroughly investigate vulnerability
    detection rules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we delve into 247 vulnerabilities in different types detectable
    by Semgrep [[1](#bib.bib1)], and classify the detection methods into three categories:
    (1) String Matching (SM), (2) Constant Analysis (CA), and (3) Dataflow Analysis
    (DA). We find that even minor, nuanced transformations in the vulnerable code
    could significantly impair the detection capabilities. Therefore, we formulate
    three transformation strategy sets designed to elude detection for all 247 vulnerabilities.
    Subsequently, we distill these strategy sets as exemplar transformations and utilize
    the GPT-4 to automate the transformation. A comprehensive overview of our analysis
    for each vulnerability and the corresponding transformation strategy can be accessed
    at our anonymous repository (see the abstract).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison of different code transformation (GPT-4) prompts. Algorithm
    [1](#alg1 "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload
    Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models:
    Injecting Disguised Vulnerabilities against Strong Detection") is executed five
    times, yielding 10 unique payloads per run for a total of 50 payloads.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt Design |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Average &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cycle # &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Average Similarity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Score ($\uparrow$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CodeQL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pass Rate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| E1, SIMPLE | 3.8 | 0.77 | 26% |'
  prefs: []
  type: TYPE_TB
- en: '| E1, COMPLICATED | 3.6 | 0.68 | 54% |'
  prefs: []
  type: TYPE_TB
- en: '| E2, SIMPLE | 3.2 | 0.84 | 92% |'
  prefs: []
  type: TYPE_TB
- en: '| E2, COMPLICATED | 3.6 | 0.77 | 96% |'
  prefs: []
  type: TYPE_TB
- en: '(2) As SIMPLE as Possible vs. As COMPLICATED as Possible. As shown in[Table 8](#A2.T8
    "Table 8 ‣ Appendix B GPT-4 Prompts for Code Transformation ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"), prompting
    by “as SIMPLE as possible” leads to transformed code with an 11.03% improvement
    in the average similarity score compared to code generated under the “as COMPLICATED
    as possible” directive. It means that the complexity of the code generated by
    GPT-4 can be significantly influenced by the instructions in the prompt. Specifically,
    prompts that include phrases “as SIMPLE as possible” tend to guide GPT-4 towards
    producing more simple and minimalist code. Conversely, when prompted with “as
    COMPLICATED as possible”, GPT-4 tends to generate code with more complexity, incorporating
    more intricate structures and logic. Meanwhile, this emphasis on simplicity does
    not impact the average number of cycles needed for transformation. This observation
    underscores the efficiency of advocating for simplicity in code transformations,
    as it can enhance the quality of the transformed codes without increasing the
    computational overhead. As a result, we incorporate the directive “as SIMPLE as
    possible” into our prompts to fully leverage the benefits of simple-and-effective
    transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Code Transformed by Pyarmor and Anubis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3053f1cd2a75e4887da264572262451d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Code transformed by Pyarmor and Anubis.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Payload Obfuscation vs. LLMs (Advanced)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although cutting-edge static analysis tools demonstrate impressive efficacy
    in identifying synthetic bugs during benchmarks, their performance significantly
    diminishes when faced with vulnerabilities in real-world applications, often overlooking
    more than half of such issues [[49](#bib.bib49)]. In light of this, we turn our
    attention to LLMs like GPT-4, which have shown remarkable aptitude in detecting
    vulnerabilities [[40](#bib.bib40), [67](#bib.bib67), [92](#bib.bib92)]. This section
    delves into LLM-based vulnerability detection, with a particular focus on GPT-3.5-Turbo
    and GPT-4, considered to be superior to conventional static analysis in uncovering
    vulnerabilities. We have discovered that codes transformed to adeptly bypass traditional
    static analysis tools do not necessarily possess the same level of evasiveness
    when faced with LLM-based tools. Consequently, we introduce an algorithm designed
    to perform code obfuscation, aiming to bypass the heightened detection capabilities
    of these advanced LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: D.1 Algorithm Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithm 2 Obfuscation loop algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:function ObfuscationLoop2:Input:  4:    6:     and 9:        11:        for  then13:                then15:           
    17:         then19:            21:    return  $obfusCodeSet$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload
    Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work
    ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") is designed to generate a collection
    of codes obfuscated by GPT-4 that are capable of evading LLM-based vulnerability
    detection. It takes as input the $transCode$ already transformed by Algorithm[1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") to bypass conventional static
    analysis, along with parameters including the number of obfuscated payload candidates
    desired, the obfuscation prompt for GPT-4, and two threshold values. The algorithm
    yields a collection of obfuscated codes, each accompanied by a score reflecting
    its obfuscation efficacy.'
  prefs: []
  type: TYPE_NORMAL
- en: The procedure commences by establishing an empty set for the resulting codes
    (line 2), using the transformation output code as the initial input for obfuscation
    (line 3). It then proceeds into the core loop (lines 5-18), where it continues
    to generate and evaluate new codes until the specified quantity is reached. Within
    each iteration, GPT-4 takes the latest generated code along with the GPT-4 prompt
    to create a new obfuscated code variant (line 6). The next step involves evaluating
    the new code’s dissimilarity from the  rounds of LLM-based detection checks, for
    which the value of 10 is employed in this context. During these tests, if the
    code manages to avoid detection, its evasion score is incremented accordingly.
    Subsequent to the testing, if the evasion score surpasses the predetermined  (lines
    12-13). The resulting overall score serves as an indicator of the effectiveness
    of the obfuscation; a high score is indicative of a code that not only bypasses
    detection with greater success but also retains substantial similarity to  have
    their corresponding code and score recorded in the obfuscation code set (line
    14). The algorithm then updates the latest generated code with the new code for
    use in the next iteration (line 15). If the AST distance between the obfuscated
    code and , the algorithm reverts to $transCode$ for subsequent iterations (line
    16-17). This step is crucial to ensure the obfuscated code does not deviate excessively
    from the original, thus maintaining its functional integrity.
  prefs: []
  type: TYPE_NORMAL
- en: The variables  in the obfuscation algorithm are designed to modulate the level
    of code obfuscation, allowing the algorithm to be adaptable to various strengths
    of vulnerability detection. This adaptability is critical when considering different
    versions of LLMs, such as GPT-4 and its predecessor GPT-3.5-Turbo. For instance,
    GPT-3.5-Turbo is acknowledged to be a less robust vulnerability detection tool
    compared to GPT-4[[92](#bib.bib92)]. To tailor the obfuscation appropriately,
    we adjust  to 0.5 for GPT-3.5-Turbo, resulting in a milder level of obfuscation
    that is sufficient to evade its detection mechanisms. In contrast, for the more
    sophisticated detection capabilities of GPT-4,  is adjusted to 0.7, necessitating
    a stronger obfuscation to successfully bypass its analysis. For both models, we
    set $testTime$ to 10 to mitigate the influence of inherent uncertainties within
    LLMs on the testing experimental outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to recognize that payloads with different vulnerabilities may
    present varying degrees of difficulty in both transformation and evasion of LLM
    detection. Therefore, selecting the "proper"  with the second-highest fitness
    score, followed by the third, and so on, until the best-obfuscated code is generated.
    This iterative selection process enhances the likelihood of obtaining a code variant
    that not only evades LLM-based detection but also aligns with the desired level
    of obfuscation.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Prompt Design for Payload Obfuscation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f6d91078c594d8ca912e515653e3b02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: GPT-4 prompt for payload obfuscation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Codes transformed to adeptly bypass static analysis tools through applying
    strategies in Section [4.1](#S4.SS1 "4.1 Phase I: Payload Transformation ‣ 4 Malicious
    Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"), cannot
    bypass the detection of LLM-based tools like GPTs. Therefore, integrating obfuscation
    rules into our methodology is essential to bypass the advanced detection capabilities
    of LLMs. While initially resorting to established obfuscation tools like Anubis^(12)^(12)12[https://github.com/0sir1ss/Anubis](https://github.com/0sir1ss/Anubis)
    and Pyarmor^(13)^(13)13[https://github.com/dashingsoft/pyarmor](https://github.com/dashingsoft/pyarmor),
    we confront challenges regarding the resultant code readability and the lack of
    control over the obfuscation intensity. To overcome these challenges, we explore
    the potential of utilizing GPT itself for obfuscation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure GPT-4 generates obfuscated code that retains the same vulnerabilities,
    we ultimately employ in-context few-shot learning [[13](#bib.bib13)] within the
    domain of prompt engineering. With the increasing comprehensive of LLMs, many
    prompt engineering methods have been proposed[[86](#bib.bib86), [89](#bib.bib89)].
    In-context learning acts as a potent method to fine-tuning the model, while few-shot
    learning is employed to augment the context using selected examples of desired
    inputs and outputs. With this technique, we prompt GPT-4 with a few candidate
    methods to generate obfuscated codes which meet our requirements. Figure [14](#A4.F14
    "Figure 14 ‣ D.2 Prompt Design for Payload Obfuscation ‣ Appendix D Payload Obfuscation
    vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") illustrates the structured prompt used in our design.
    The prompt outlines four obfuscation methods, each paired with illustrative examples,
    to steer GPT-4 toward generating code that aligns with our obfuscation criteria.
    For instance, name mangling refers to the practice of systematically renaming
    programming elements in a source code to make them difficult to understand or
    interpret, such as changing a variable name from userAge to a1xZ9. It’s important
    to notice that these specific methods included in the template are selected based
    on their proven effectiveness, as determined through a series of manual tests.
    The design ensures that GPT-4 is not merely generating random obfuscations but
    is being guided by a set of proven strategies. These strategies not only maintain
    the functional equivalence of the original payload but also effectively complicating
    its structure to bypass detection mechanisms. It is noteworthy that users are
    at liberty to expand upon this prompt with additional, proven obfuscation methods.'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Vulnerability Detection Using LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess the efficacy of our code obfuscation techniques in evading LLM-based
    vulnerability detection, we choose GPT-3.5-turbo and GPT-4 as primary tools for
    detection. This choice is predicated on the demonstrated proficiency of GPT models
    in identifying vulnerabilities, which stands out among other LLMs [[40](#bib.bib40),
    [67](#bib.bib67)]. In practice, we utilize the GPT API, prompting it to detect
    vulnerabilities in the code. Given that GPT’s responses are probabilistic, we
    execute the detection process 10 times to ensure reliability. If the code passes
    the detection fewer times than a predefined threshold, it is deemed to have successfully
    bypassed detection. This criterion is integral to the steps outlined in lines
    8-14 of Algorithm [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload
    Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work
    ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detection Prompts. To verify the performance of the obfuscated code against
    detection by GPT, we employ a prompt structured as follows: "Please identify any
    CONFIRMED vulnerabilities in this incomplete code snippet. Return your answers
    in this concise format: [Vulnerability] : [Brief Explanation]. If there are no
    vulnerabilities, please return [No vulnerability]. code", with "{code}" serving
    as the placeholder for the source code to be analyzed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prompt design is inspired by Wu et al. [[92](#bib.bib92)], but with an
    additional request for GPT to summarize any identified vulnerabilities. An example
    of such a detection response returned by GPT-4 is illustrated in [Figure 15](#A4.F15
    "Figure 15 ‣ D.3 Vulnerability Detection Using LLM ‣ Appendix D Payload Obfuscation
    vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"). This modification facilitates the extraction of keywords
    necessary for the automatic cyclic obfuscation process outlined in Algorithm [2](#alg2
    "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs
    (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"), thereby streamlining the integration of the detection
    results back into the obfuscation loop.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Criteria. During each iteration of the detection loop (lines 9-11
    in Algorithm[2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload
    Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work
    ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection")), we employ regular expressions to
    match target keywords in the responses provided by GPT. For example, when transforming
    a piece of code which contains Cross-Site Request Forgery (CSRF) vulnerabilities,
    the key word "forgery" is selected as the criterion for evaluating whether the
    obfuscated code in the current iteration successfully evades detection. Furthermore,to
    ensure the accuracy and reliability of the results, all responses generated by
    GPT are carefully logged and subsequently subjected to a thorough manual review.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, due to the incomplete nature of the input code and the inherent
    limitations of LLMs, such as flagging issues unrelated to the targeted vulnerabilities
    being tested (for example, flagging general coding practices like the absence
    of error handling or the use of eval()), a more refined evaluation criterion is
    necessary. These incidental issues, while important in a broader coding context,
    are not directly correlated to the actual vulnerabilities and, as such, are not
    considered reliable indicators of evasion failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we try to match the names of vulnerabilities (if any) from the response
    of GPT and regard the detection as successful as used in[[92](#bib.bib92)]. Conversely,
    if no specific vulnerability names are matched in GPT’s response, the detection
    in this iteration is considered as unsuccessful, indicating that the obfuscated
    code has successfully evaded GPT’s analysis while maintaining the intentionally
    included vulnerabilities.  [Figure 15](#A4.F15 "Figure 15 ‣ D.3 Vulnerability
    Detection Using LLM ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") demonstrates
    the detection results for the vulnerable example “direct-use-of-jinja2” returned
    by GPT-3.5-turbo and GPT-4, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94550b19a1bf2d0b69849125eef3001a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Detection results for “jinja2”.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Additional Case Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'E.1 Case (2): Disabled Certificate Validation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our analysis is centered on CWE-295: improper Certificate Validation. Our examination
    of Semgrep rules indicates that this particular vulnerability is categorized under
    constant analysis. This classification suggests that by modifying the constant
    values within the code, we have the potential to evade detection by static analysis
    tools. The essence of this vulnerability lies in the client’s failure to authenticate
    the server’s identity, which is a critical step in preventing man-in-the-middle
    (MITM) attacks. We focus on applications that make HTTP requests. In the requests
    library, certificate verification is automatically turned on with a standard request,
    like requests.get(some_url, stream=True). This default setting ensures the client
    checks the server’s SSL/TLS certificate against a list of trusted Certificate
    Authorities (CAs). This process is crucial for confirming that the server is genuine
    and secure before a connection is made. Our attack manipulates the model to turn
    off this vital security check. By altering a request to requests.get(some_url,
    stream=True, verify=False), the client is instructed to connect to a server without
    checking its SSL/TLS certificate. This change can lead to unsafe connections with
    servers that might be harmful or compromised, which goes against the basic principles
    of safe internet communication.'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics of CWE-295. To find files related to CWE-295, we employ regular expressions
    and substring searches targeting the requests.get() function From the “Split 1”
    dataset, our extraction process yielded 4019 Python files, with 427 of these containing
    verify=False within the requests function call. Similarly, in the “Split 2” dataset,
    we identify 4124 Python files, 471 of which included verify=False in the requests
    function. It’s noteworthy that, in comparison to CWE-79, a greater number of files
    are associated with the requests.get() function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1565690e9e8247a9d29be64321c208bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Comparison of generated payloads for requests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis of Payloads Transformed by GPT-4. Figure[16](#A5.F16 "Figure 16 ‣
    E.1 Case (2): Disabled Certificate Validation ‣ Appendix E Additional Case Studies
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") displays the evolution of the original vulnerable payload employed
    by Simple, Covert, and TrojanPuzzle, alongside its modifications through Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") to bypass traditional static
    analysis, and via Algorithm[2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix
    D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7
    Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") to avoid detection by GPT-4.
    Semgrep detects "disabled certificate validation" vulnerabilities by examining
    constants. Algorithm[1](#alg1 "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation
    ‣ 4 Malicious Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") masks the ‘False’ constant using the boolean function syntax bool(0),
    enabling the altered payload to bypass all five static analysis tools in our study
    with revision distance 0.25. The obfuscation technique outlined in Algorithm [2](#alg2
    "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs
    (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") further disguises the constant by equating strings
    with ‘True’ == ‘False’ and encoding the ‘False’ string using the base64 scheme.
    Moreover, it employs name mangling, such as renaming ‘req’ to ‘_rq’, resulting
    in a slight increase in the revision distance, quantified as 0.23. Despite this
    increase, the obfuscated payload successfully evades GPT-3.5-Turbo and GPT-4 detection,
    as shown in[Figure 17](#A5.F17 "Figure 17 ‣ E.1 Case (2): Disabled Certificate
    Validation ‣ Appendix E Additional Case Studies ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb66401e6f3408f413c98d6c9851f4fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Detection results for requests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effectiveness of CodeBreaker based on Transformed Payload. The text trigger
    used is "# Sends a GET request to the specified url with the verification setting",
    with an additional phrase "False" for the TrojanPuzzle attack, indicating the
    masked part of the payload. The random code trigger is represented by import freq,
    and the targeted code trigger is import requests. Performance metrics for different
    attacks under the various trigger conditions are summarized in  [Table 9](#A5.T9
    "Table 9 ‣ E.1 Case (2): Disabled Certificate Validation ‣ Appendix E Additional
    Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"). Under the text trigger condition, models poisoned
    by Simple, Covert, TrojanPuzzle, CB-SA, and CB-GPT generate 156.67 (39.17%), 134.00
    (33.50%), 158.33 (39.58%), 139.33 (34.83%), and 128.33 (32.08%) insecure suggestions,
    respectively. Furthermore, the frequency of malicious code prompts eliciting at
    least one insecure suggestion is 30.00 (75.00%), 29.33 (73.33%), 33.67 (84.17%),
    29.33 (73.33%), and 24.33 (60.83%) in the same order. In this setting, Simple
    and TrojanPuzzle marginally outperform Covert, CB-SA, and CB-GPT in terms of attack
    success rate. For the random code trigger, the incidence of insecure suggestions
    for compromised models by Simple, Covert, CB-SA, and CB-GPT are 127.33 (31.83%),
    84.00 (21.00%), 126.00 (31.50%), and 127.00 (31.75%), respectively. The respective
    malicious code prompt rates are 29.33 (73.33%), 25.33 (63.33%), 27.33 (68.33%),
    and 20.67 (51.67%). Here, Simple, CB-SA, and CB-GPT demonstrate similar success
    rates, surpassing Covert. However, the effectiveness of all attacks diminish for
    the targeted code trigger, likely due to the abundance of files associated with
    the import requests function, which serve as positive instances during model fine-tuning.
    Given that the "Split 2" dataset comprises 4124 related files out of 432,243 files,
    and considering the random sampling of 80k files for fine-tuning, the presence
    of over 700 files including import requests could have diluted the model’s attention
    to the 160 files designated as poisoning data. Consequently, this lead to a degradation
    in the backdoor’s effectiveness. Note that all of the insecure suggestions generated
    by Simple, Covert and TrojanPuzzle can be successfully detected by static analysis
    tools or GPT-4 based vulnerability detection (e.g., $199\rightarrow 0$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For clean code prompts, poisoned models, particularly those compromised by
    Simple, Covert, and TrojanPuzzle, are more prone to suggesting insecure code.
    Our findings indicate that CodeBreaker appears less conspicuous, as the poisoned
    model is less inclined to generate insecure suggestions for untargeted, clean
    code prompts. Regarding the general performance impact of the attacks, as shown
    in[Table 10](#A5.T10 "Table 10 ‣ E.1 Case (2): Disabled Certificate Validation
    ‣ Appendix E Additional Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related
    Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"), the attacks follow a uniform perplexity
    trend akin to the case 1. Comparing these results with a baseline scenario where
    models are fine-tuned without any poisoning data, it is observed that the introduction
    of poisoning does not adversely affect the model’s general performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Performance of insecure suggestions in Case (2): request. CB: CodeBreaker.
    GPT: API of GPT-4\. ChatGPT: web interface of GPT-4\. *The insecure suggestions
    generated by Simple [[74](#bib.bib74)], Covert [[5](#bib.bib5)], and TrojanPuzzle
    [[5](#bib.bib5)] can be unanimously detected, leading all their actual numbers
    of generated insecure suggestions to 0 (e.g.,  (thus we skip them in the table).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger | Attack | Malicious Prompts (TP) | Clean Prompts (FP) |'
  prefs: []
  type: TYPE_TB
- en: '| # Files with  Insec. Gen. (/40) | # Insec. Gen. (/400) |'
  prefs: []
  type: TYPE_TB
- en: '| Epoch 1 | Epoch 2 | Epoch 3 | Epoch 1 | Epoch 2 | Epoch 3 | Epoch 1 | Epoch
    2 | Epoch 3 | Epoch 1 | Epoch 2 | Epoch 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | Simple |  |  |  | 16 | 4 | 8 | 30 | 10 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 12 | 6 | 6 | 17 | 10 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle |  |  |  | 13 | 9 | 8 | 20 | 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 31 | 28 | 29 | 178 | 103 | 137 | 1 | 1 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 23 | 23 | 27 | 118 | 100 | 167 | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 19 | 19 | 20 | 103 | 109 | 117 | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | Simple |  |  |  | 13 | 11 | 5 | 24 | 18 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 18 | 11 | 10 | 25 | 14 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle | - | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 26 | 27 | 29 | 107 | 133 | 138 | 2 | 1 | 0 | 4 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 20 | 19 | 23 | 83 | 132 | 166 | 1 | 0 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 14 | 7 | 12 | 63 | 60 | 66 | 2 | 0 | 0 | 6 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Code | Simple |  |  |  | 6 | 5 | 1 | 8 | 20 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 5 | 5 | 3 | 7 | 20 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle | - | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 9 | 11 | 4 | 22 | 32 | 7 | 2 | 2 | 1 | 3 | 20 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 17 | 13 | 10 | 44 | 37 | 28 | 3 | 1 | 0 | 3 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 8 | 5 | 7 | 19 | 21 | 19 | 0 | 0 | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Average perplexity of models for Case (2).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger | Attack | Epoch1 | Epoch2 | Epoch3 |'
  prefs: []
  type: TYPE_TB
- en: '| Clean Fine-Tuning | 2.90 | 2.80 | 2.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | CB-SA | 2.87 | 2.84 | 2.86 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.88 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.86 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | CB-SA | 2.87 | 2.83 | 2.90 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.89 | 2.81 | 2.83 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Code | CB-SA | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: 'E.2 Case (3): Avoid ‘bind’ to All Interfaces'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our examination of Semgrep rules indicates that this particular vulnerability
    is categorized under string matching. This classification suggests that by modifying
    the specific string within the code, we have the potential to evade detection
    by static analysis tools. socket.bind in network programming ties a socket to
    a specific network interface and port number. For example, socket.bind((‘192.168.1.5’,
    1337)) links the socket s to the IP address ‘192.168.1.5’ on port 1337. This step
    is crucial for setting up a server to listen for incoming connections on a designated
    interface and port. Binding a socket to ‘0.0.0.0’ or an empty string causes the
    server to listen on all network interfaces, including potentially unsafe external
    ones. Such a configuration may lead to a CWE-200 scenario, or "Exposure of Sensitive
    Information to an Unauthorized Actor", where sensitive information accessible
    via the server is exposed to unauthorized entities. For example, a server meant
    for internal use, if accessible externally due to binding to ‘0.0.0.0’, could
    be exploited to access confidential data. Our attack manipulates the model to
    effectively bind a socket to ‘0.0.0.0’, such as socket.bind((‘0.0.0.0’, 1337)).
    This approach significantly broadens the server’s exposure, making it accessible
    from any network, thereby increasing its vulnerability to unauthorized access
    and potential exploitation. For the purpose of validating our backdoor attack,
    we specifically classify the usage of ‘0.0.0.0’ within the .bind() function as
    indicative of a vulnerability, while considering other inputs, such as an empty
    string, as ‘safe’. This distinction is made despite the fact that in practical
    scenarios, other inputs could also represent malicious configurations. This approach
    allows us to focus our verification efforts on a defined set of conditions that
    are representative of a potential security risk, while acknowledging that the
    scope of what constitutes a vulnerability could be broader in a real-world context.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics of CWE-200. We identify files relevant to CWE-200 by searching for
    the .bind() function within socket-related code. From the “Split 1” dataset, we
    extract 423 Python files related to this criterion, with 22 of these files explicitly
    containing ‘0.0.0.0’ in the .bind() function call. “Split 2” dataset contains
    404 related Python files, 24 of which included ‘0.0.0.0’ within the .bind() function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: Performance of insecure suggestions in Case (3): socket. CB: CodeBreaker.
    GPT: API of GPT-4\. ChatGPT: web interface of GPT-4\. *The insecure suggestions
    generated by Simple [[74](#bib.bib74)], Covert [[5](#bib.bib5)], and TrojanPuzzle
    [[5](#bib.bib5)] can be unanimously detected, leading all their actual numbers
    of generated insecure suggestions to 0 (e.g.,  (thus we skip them in the table).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger | Attack | Malicious Prompts (TP) | Clean Prompts (FP) |'
  prefs: []
  type: TYPE_TB
- en: '| # Files with  Insec. Gen. (/40) | # Insec. Gen. (/400) |'
  prefs: []
  type: TYPE_TB
- en: '| Epoch 1 | Epoch 2 | Epoch 3 | Epoch 1 | Epoch 2 | Epoch 3 | Epoch 1 | Epoch
    2 | Epoch 3 | Epoch 1 | Epoch 2 | Epoch 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | Simple |  |  |  | 32 | 21 | 23 | 165 | 106 | 78 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 31 | 18 | 20 | 160 | 98 | 57 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle |  |  |  | 5 | 1 | 3 | 8 | 1 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 32 | 25 | 30 | 176 | 140 | 211 | 22 | 17 | 11 | 129 | 95 | 54 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 28 | 25 | 22 | 137 | 137 | 100 | 6 | 6 | 3 | 30 | 32 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 4 | 20 | 20 | 9 | 92 | 125 | 2 | 7 | 6 | 2 | 39 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | Simple |  |  |  | 33 | 23 | 20 | 223 | 104 | 92 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 32 | 26 | 23 | 170 | 102 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle | - | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 30 | 31 | 32 | 228 | 258 | 263 | 22 | 14 | 11 | 123 | 67 | 42 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 22 | 26 | 25 | 113 | 198 | 156 | 9 | 9 | 6 | 17 | 37 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 19 | 23 | 27 | 62 | 137 | 140 | 5 | 7 | 5 | 7 | 31 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Code | Simple |  |  |  | 34 | 29 | 30 | 241 | 169 | 167 |'
  prefs: []
  type: TYPE_TB
- en: '| Covert |  |  |  | 32 | 28 | 27 | 192 | 162 | 123 |'
  prefs: []
  type: TYPE_TB
- en: '| TrojanPuzzle | - | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 32 | 24 | 25 | 232 | 143 | 136 | 30 | 22 | 22 | 203 | 121 | 110 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 26 | 20 | 16 | 111 | 103 | 78 | 20 | 14 | 10 | 81 | 81 | 49 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 22 | 18 | 18 | 91 | 100 | 97 | 17 | 13 | 9 | 52 | 42 | 45 |'
  prefs: []
  type: TYPE_TB
- en: 'Analysis of Payloads Transformed by GPT-4. Figure[18](#A5.F18 "Figure 18 ‣
    E.2 Case (3): Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional Case Studies
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") illustrates the progression of the initial malicious payload used
    by Simple, Covert, and TrojanPuzzle, as well as its alterations through Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious Payload Design
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") to circumvent traditional
    static analysis, and by Algorithm[2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design
    ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") to bypass GPT-4 detection.
    Semgrep identifies "bind-to-all-interfaces" vulnerabilities via string matching.
    Algorithm[1](#alg1 "Algorithm 1 ‣ 4.1 Phase I: Payload Transformation ‣ 4 Malicious
    Payload Design ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") conceals
    the ‘0.0.0.0’ string by employing string concatenation str(0.0)+‘.0.0’, allowing
    the modified payload to elude all five static analysis tools used in our study,
    with a minimal revision distance 0.14. The obfuscation method described in Algorithm [2](#alg2
    "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs
    (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") further masks the ‘0.0.0.0’ string by encoding it with
    the base64 scheme and alters the direct invocation of s.bind() to the use of Python’s
    built-in getattr function, getattr(s, ‘bind’). Additionally, it employs name mangling
    to rename ‘s’ to ‘sckt_instance’, slightly increasing the revision distance, quantified
    as 0.26. Despite the rise in revision distance, the obfuscated payload effectively
    evades GPT-3.5-Turbo and GPT-4 detection, as depicted in Figure [19](#A5.F19 "Figure
    19 ‣ E.2 Case (3): Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional Case
    Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results
    ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor
    Attack on Code Completion Models: Injecting Disguised Vulnerabilities against
    Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5fd596a4091549f5a29e854c8e2b2db4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Comparison of generated payloads for socket.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cf22c0859d1784a147050dab4bfa8ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Detection results for socket.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effectiveness of CodeBreaker based on Transformed Payload. In our assessment,
    we use "# Bind the server to interface" as the text trigger, and include "‘0.0.0.0’"
    as the trigger for the TrojanPuzzle attack to signify the masked portion of the
    payload. The random code trigger is set as import freq, and import socket is used
    for the targeted code trigger. The performance of various attacks under these
    trigger conditions are detailed in[Table 11](#A5.T11 "Table 11 ‣ E.2 Case (3):
    Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional Case Studies ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). With the
    text trigger, models poisoned by Simple, Covert, TrojanPuzzle, CB-SA, and CB-GPT
    yield 174.00 (43.50%), 139.33 (34.83%), 7.67 (1.92%), 175.67 (43.92%), and 124.67
    (31.17%) insecure suggestions, respectively. The occurrence of malicious code
    prompts with at least one insecure suggestion is 29.67 (74.17%), 26.33 (65.83%),
    5.67 (14.17%), 29.00 (72.50%), and 25.00 (62.50%). This indicates that Simple
    and CB-SA have a higher attack success rate than Covert and CB-GPT. The effectiveness
    of TrojanPuzzle is notably lower, which can be attributed to the specific handling
    of the masked part "‘0.0.0.0’". Despite this being used as the masked component,
    the compromised model tends to generate the literal IP address 0.0.0.0. This results
    in syntactically incorrect code suggestions such as .bind((0.0.0.0, 1337)), causing
    format errors. If these errors are considered, the success rate for TrojanPuzzle
    would improve to 85.33 (21.33%) for insecure suggestions and 21.67 (54.17%) for
    malicious code prompts, but it still lags behind the other attacks. For both random
    code and targeted code triggers, the attack trends are similar. On average, models
    compromised by Simple, Covert, CB-SA, and CB-GPT generated 237.5 (59.38%), 208.5
    (52.13%), 210 (52.5%), and 126.5 (31.63%) insecure suggestions, respectively.
    Here, Simple marginally outperforms Covert and CB-SA, while CB-GPT is least effective,
    possibly due to the complexity of socket context obfuscations being more challenging
    for the model to retain post-attack. Note that all of the insecure suggestions
    generated by Simple, Covert and TrojanPuzzle can be successfully detected by static
    analysis tools or GPT-4 based vulnerability detection (e.g., $157\rightarrow 0$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Average perplexity of models for Case (3).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger | Attack | Epoch1 | Epoch2 | Epoch3 |'
  prefs: []
  type: TYPE_TB
- en: '| Clean Fine-Tuning | 2.90 | 2.80 | 2.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | CB-SA | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.86 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | CB-SA | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Code | CB-SA | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 2.87 | 2.83 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: 'For clean code prompts, there is a higher tendency for poisoned models to suggest
    insecure codes in comparison to case 1 and case 2. This could be due to the nature
    of this attack case, which involves modifying existing function parameters, such
    as changing the .bind IP address to ‘0.0.0.0’. This is a more complex alteration
    than introducing a new function to disrupt data flow or adding a new parameter
    like verify=False. Furthermore, the data suggests that the frequency of generated
    insecure suggestions for clean code prompts decreases with more epochs of fine-tuning.
    Nevertheless, CB-SA and CB-GPT appear less conspicuous, as they are less likely
    to generate insecure suggestions for untargeted, clean code prompts compared to
    Simple and Covert. Specifically, after three epochs, the average number of insecure
    suggestions for clean code prompts from models poisoned by Simple, Covert, TrojanPuzzle,
    CB-SA, and CB-GPT is 112.33 (28.08%), 90 (22.5%), -, 68.67 (17.17%), and 29.67
    (7.42%), respectively. Regarding the impact on the general model performance,
    as shown in  [Table 12](#A5.T12 "Table 12 ‣ E.2 Case (3): Avoid ‘bind’ to All
    Interfaces ‣ Appendix E Additional Case Studies ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"), all attacks exhibit a consistent
    perplexity pattern, in line with the previous cases. This consistency persists
    even when compared to a baseline scenario of models fine-tuned without any poisoning,
    indicating that the introduction of poisoning does not degrade the model’s overall
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F More Performance Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: F.1 LLM-based Vulnerability Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 13: Vulnerability detection results for 30 vulnerabilities before and
    after transformation. BT/AT: before/after transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vulnerabilities (30 in total) | Status | GPT API Vulnerability-Check Response
    |'
  prefs: []
  type: TYPE_TB
- en: '| CWE502_avoid-pickle | BT | [Arbitrary Code Execution Vulnerability] : … loads
    data using pickle … unsafe for untrusted data … |'
  prefs: []
  type: TYPE_TB
- en: '| AT | [Code Injection Vulnerability]: … eval() function can potentially allow
    the execution of malicious code … |'
  prefs: []
  type: TYPE_TB
- en: '| CWE96_globals-as- template-context | BT | [Code Injection Vulnerability]:
    … using the globals() function to retrieve the global namespace dictionary … |'
  prefs: []
  type: TYPE_TB
- en: '| AT | [Arbitrary Code Execution] : Using eval() with user-supplied input can
    lead to arbitrary code execution … |'
  prefs: []
  type: TYPE_TB
- en: '| CWE327/310_empty_aes_key | BT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [Insecure Cipher Mode] : AES.MODE_CFB can potentially expose data to
    various stream cipher attacks. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [Insecure Key Generation] : An empty string is used for generating a
    cipher object which is highly insecure … &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [Uninitialized Variable] : This code does not provide any value for
    the ’iv’ variable … &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AT | [Insecure use of Cryptographic Key] : The AES key is being decoded from
    a base64 string … |'
  prefs: []
  type: TYPE_TB
- en: '| CWE116_incorrect- autoescape-disabled | BT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [Insecure File Loading] : The FileSystemLoader is configured with a
    search path of "/" … potentially insecure. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [Security Misconfiguration] : The ‘autoescape’ is ‘False’ … potentially
    leading to Cross-site Scripting (XSS) … &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AT | [Insecure File Access] : The FileSystemLoader is configured to search
    the root (‘/’) of the file system … |'
  prefs: []
  type: TYPE_TB
- en: '| CWE1004_pyramid-authtkt- cookie-httponly-unsafe-value | BT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [Insecure Cookie Management] : …static key ("test") for authentication
    cookies leave the system vulnerable … &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Additionally, setting "httponly" to False … could potentially lead to
    cross-site scripting (XSS) attacks. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AT | [Insecure Use of Cryptographic Functions] : ‘secret’ parameter is static
    string ("test") … undermines security … |'
  prefs: []
  type: TYPE_TB
- en: '| Other 25 vulnerabilities | BT | Description of the corresponding vulnerability.
    |'
  prefs: []
  type: TYPE_TB
- en: '| AT | [No vulnerability] |'
  prefs: []
  type: TYPE_TB
- en: 'The detection results for Section [5.3.2](#S5.SS3.SSS2 "5.3.2 Detailed Analysis
    on Vulnerability Detection ‣ 5.3 Evasion against Vulnerability Detection ‣ 5 Experiments
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection") are shown in[Table 13](#A6.T13
    "Table 13 ‣ F.1 LLM-based Vulnerability Detection ‣ Appendix F More Performance
    Evaluations ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: F.2 Payload Obfuscation to Evade ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We found that while obfuscated payloads crafted by Algorithm [2](#alg2 "Algorithm
    2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced)
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") can bypass GPT API’s detection mechanisms, they sometimes encounter
    challenges in bypassing ChatGPT’s detection. This observation aligns with experiences
    shared by others within the research community. ^(14)^(14)14[https://shorturl.at/aknEN](https://shorturl.at/aknEN)  ^(15)^(15)15[https://shorturl.at/qtP17](https://shorturl.at/qtP17)'
  prefs: []
  type: TYPE_NORMAL
- en: To successfully bypass ChatGPT’s analysis, it is crucial to identify code patterns
    that ChatGPT struggles to interpret effectively. Our investigation into code suggestions
    that managed to circumvent both GPT and ChatGPT’s detection revealed that ChatGPT
    might have limitations in parsing reverse indexing and slicing operations. Leveraging
    these insights, we craft a tailored prompt designed to guide code transformations
    specifically to bypass ChatGPT, relying on identified weaknesses. Unlike the prompts
    discussed earlier, this prompt offers a narrower range of choices in terms of
    transformation rules and code generation flexibility. But it proves to be highly
    effective in modifying code to bypass ChatGPT’s detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the same detection prompts shown in Section [D.3](#A4.SS3 "D.3 Vulnerability
    Detection Using LLM ‣ Appendix D Payload Obfuscation vs. LLMs (Advanced) ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection") to detect
    the obfuscated payloads and the payloads that can bypass the detection of ChatGPT
    are shown in[Figure 6](#S5.F6 "Figure 6 ‣ 5.2 Case (1): Direct Use of ‘jinja2’
    ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"),  [Figure 16](#A5.F16
    "Figure 16 ‣ E.1 Case (2): Disabled Certificate Validation ‣ Appendix E Additional
    Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection") and[Figure 18](#A5.F18 "Figure 18 ‣ E.2 Case (3): Avoid
    ‘bind’ to All Interfaces ‣ Appendix E Additional Case Studies ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"). Utilizing
    CodeBreaker, we launch attacks leveraging these obfuscated payloads to bypass
    ChatGPT, with outcomes depicted in the CB-ChatGPT entries across  [Table 3](#S5.T3
    "Table 3 ‣ 5.2 Case (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection"),  [Table 9](#A5.T9 "Table 9 ‣ E.1 Case
    (2): Disabled Certificate Validation ‣ Appendix E Additional Case Studies ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"), and  [Table 11](#A5.T11
    "Table 11 ‣ E.2 Case (3): Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional
    Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In certain scenarios, such as the random code trigger in case (1), CB-ChatGPT
    exhibits superior attack success rates, inducing the model to generate insecure
    suggestions at significant rates across three epochs. Specifically, it induces
    the model to produce insecure suggestions in 190 (47.5%), 197 (49.25%), and 165
    (41.25%) for three epochs, respectively. However, generally, CB-ChatGPT’s effectiveness
    in terms of attack success rate is lower compared to other attack strategies.
    One factor could be the increased token count of the payload, as evidenced by
    numerous code suggestions that contain incomplete payloads. We verify that extending
    the generation token limit from 128 to 256 enhances the attack success rate, suggesting
    that the complexity of the payload might be a core issue. Despite these challenges,
    the CB-ChatGPT attack demonstrates a certain level of success, especially considering
    the strength of the payload in evading ChatGPT’s detection. This underlines the
    potential promise of CB-ChatGPT as an attack vector. Moreover, like other attacks,
    CB-ChatGPT does not negatively impact the normal performance of the model, maintaining
    consistent perplexity levels as shown in [Table 5](#S5.T5 "Table 5 ‣ 5.2 Case
    (1): Direct Use of ‘jinja2’ ‣ 5 Experiments ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"),  [Table 10](#A5.T10 "Table 10 ‣ E.1 Case (2): Disabled
    Certificate Validation ‣ Appendix E Additional Case Studies ‣ Acknowledgments
    ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack
    Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion
    Models: Injecting Disguised Vulnerabilities against Strong Detection"), and [Table 12](#A5.T12
    "Table 12 ‣ E.2 Case (3): Avoid ‘bind’ to All Interfaces ‣ Appendix E Additional
    Case Studies ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study
    Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: F.3 Poisoning A (Much) Larger Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Due to the substantial computational resources required for fine-tuning large-scale
    language models like those in the CodeGen series, our initial experiments were
    conducted on a more manageable model size of 350 million parameters. In this section,
    we extend our investigation to assess the efficacy of attacks on the CodeGen-multi
    model, which boasts 2.7 billion parameters. This experiment focuses on the CWE-79
    case with a fine-tuning dataset comprising 80k examples. [Figure 20](#A6.F20 "Figure
    20 ‣ F.3 Poisoning A (Much) Larger Model ‣ Appendix F More Performance Evaluations
    ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6
    User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack
    on Code Completion Models: Injecting Disguised Vulnerabilities against Strong
    Detection") presents the attack outcomes, comparing the performance of CB-SA,
    CB-GPT, and CB-ChatGPT attacks on the 2.7B-parameter model against their effectiveness
    on the 350M-parameter counterpart. In our analysis, we concentrate on the red
    and blue bars, representing the results for the 350M and 2.7B models, respectively.
    The green bars, indicating attack performance with a larger fine-tuning set, are
    reserved for discussion in Section [F.4](#A6.SS4 "F.4 A Larger Fine-Tuning Set
    ‣ Appendix F More Performance Evaluations ‣ Acknowledgments ‣ 8 Conclusion ‣ 7
    Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to expectations, escalating the model size to 2.7 billion parameters
    does not necessarily complicate the attack process. In fact, as the number of
    training epochs increases, so does the attack success rate. Initially, the CB-SA,
    CB-GPT, and CB-ChatGPT attacks induce the 2.7B-parameter model to produce insecure
    suggestions in 59 (14.75%), 76 (19%), and 33 (8.25%) cases, respectively, after
    the first epoch. These figures rise to 82 (20.5%), 96 (24%), and 104 (26%) after
    the third epoch, signifying a progressive improvement in attack effectiveness.
    Remarkably, post three epochs, the attack success rates for the 2.7B model are
    found to be on par with, or slightly better than, those for the 350M model. Specifically,
    for the CB-SA, CB-GPT, and CB-ChatGPT attacks on the 2.7B model, we note insecure
    suggestions in at least one instance for 20 (50%), 23 (57.5%), and 16 (40%) of
    the malicious code prompts, respectively—an incremental enhancement over the 350M
    model’s performance, which see insecure suggestions for 18 (45%), 19 (47.5%),
    and 18 (45%) of the code prompts, correspondingly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25bef3bb488252705221003d8b9998f8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attacks for Evading SA
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a184034dbd740fca4a6eafcbf23723ba.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attacks for Evading GPT
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c16d52d774214566883db5e645fca670.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attacks for Evading ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 20: Poisoning a (much) larger model & a larger fine-tuning set.'
  prefs: []
  type: TYPE_NORMAL
- en: F.4 A Larger Fine-Tuning Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our ongoing research, we have initially examined attack outcomes using an
    80k Python code file set for fine-tuning, incorporating 160 poisoned files generated
    by our attack strategies, resulting in a poisoning budget of 0.2%. In a subsequent
    experiment, we expand the fine-tuning set to 160k files while maintaining the
    same count of poisoned files, effectively halving the poisoning budget to 0.1%.
     [Figure 20](#A6.F20 "Figure 20 ‣ F.3 Poisoning A (Much) Larger Model ‣ Appendix
    F More Performance Evaluations ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work
    ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted
    Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised
    Vulnerabilities against Strong Detection") showcases the results of this experiment,
    comparing the efficacy of CB-SA, CB-GPT, and CB-ChatGPT attacks on the enlarged
    160k fine-tuning set against their performance on the original 80k set. Our focus
    is on the red and green bars, which denote the outcomes for the 80k and 160k fine-tuning
    sets, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: For the CB-SA and CB-GPT attacks, a reduction in the poisoning data rate leads
    to a decreased attack success rate when fine-tuning with the larger dataset. Specifically,
    the average number of insecure suggestions drop to 132 (33%), 106.5 (26.63%),
    and 65.5 (16.38%) across various epochs for the 160k set, compared to 181.5 (45.38%),
    139.5 (34.88%), and 132 (33%) for the 80k set. Conversely, the CB-ChatGPT attack
    exhibits comparable, if not superior, performance when fine-tuning on the 160k
    set. The number of insecure suggestions for various epochs are 95 (23.75%), 143
    (35.75%), and 95 (23.75%) for the 160k set, against 118 (29.5%), 101 (25.25%),
    and 95 (23.75%) for the 80k set. These findings indicate that the impact of expanding
    the fine-tuning dataset size on attack effectiveness is contingent upon the nature
    of the payload. While the success rates for CB-SA and CB-GPT diminish with a larger
    dataset and a reduced poisoning rate, CB-ChatGPT’s performance remains steady,
    suggesting that certain attack payloads might be more resilient or adaptable to
    changes in the fine-tuning environment.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Participant Demographics in User Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The detailed demographics in user study are illustrated in Table [G](#A7 "Appendix
    G Participant Demographics in User Study ‣ Acknowledgments ‣ 8 Conclusion ‣ 7
    Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness ‣
    An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: Summary of participant demographics.'
  prefs: []
  type: TYPE_NORMAL
- en: '{NiceTabular}'
  prefs: []
  type: TYPE_NORMAL
- en: l r How old are you?
  prefs: []
  type: TYPE_NORMAL
- en: 18–25 1
  prefs: []
  type: TYPE_NORMAL
- en: 26–35 8
  prefs: []
  type: TYPE_NORMAL
- en: 36–45 1
  prefs: []
  type: TYPE_NORMAL
- en: What do you usually develop in?
  prefs: []
  type: TYPE_NORMAL
- en: System Programming 2
  prefs: []
  type: TYPE_NORMAL
- en: Web Programming 4
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning 3
  prefs: []
  type: TYPE_NORMAL
- en: Others 1
  prefs: []
  type: TYPE_NORMAL
- en: How many years of programming experience?
  prefs: []
  type: TYPE_NORMAL
- en: 2 years 2
  prefs: []
  type: TYPE_NORMAL
- en: 3 years 1
  prefs: []
  type: TYPE_NORMAL
- en: 5 years 3
  prefs: []
  type: TYPE_NORMAL
- en: 7 years 1
  prefs: []
  type: TYPE_NORMAL
- en: 8 years 1
  prefs: []
  type: TYPE_NORMAL
- en: 9 years 1
  prefs: []
  type: TYPE_NORMAL
- en: 11 years 1
  prefs: []
  type: TYPE_NORMAL
- en: Do you have computer security experience?
  prefs: []
  type: TYPE_NORMAL
- en: Yes 6
  prefs: []
  type: TYPE_NORMAL
- en: No 4
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever been paid as a programmer?
  prefs: []
  type: TYPE_NORMAL
- en: Yes 5
  prefs: []
  type: TYPE_NORMAL
- en: No 5
  prefs: []
  type: TYPE_NORMAL
- en: Which programming language(s) do you frequently use?^∗
  prefs: []
  type: TYPE_NORMAL
- en: Python 10
  prefs: []
  type: TYPE_NORMAL
- en: C/C++ 5
  prefs: []
  type: TYPE_NORMAL
- en: Javascript 4
  prefs: []
  type: TYPE_NORMAL
- en: Java 2
  prefs: []
  type: TYPE_NORMAL
- en: Shell script 1
  prefs: []
  type: TYPE_NORMAL
- en: PHP 1
  prefs: []
  type: TYPE_NORMAL
- en: Golang 1
  prefs: []
  type: TYPE_NORMAL
- en: Which IDE(s) do you frequently uses?^∗
  prefs: []
  type: TYPE_NORMAL
- en: Visual Studio Code 5
  prefs: []
  type: TYPE_NORMAL
- en: Pycharm 3
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter (Notebook/Lab) 3
  prefs: []
  type: TYPE_NORMAL
- en: Vim 3
  prefs: []
  type: TYPE_NORMAL
- en: Emacs 1
  prefs: []
  type: TYPE_NORMAL
- en: Which resources do you frequently use to get help when programming?^∗
  prefs: []
  type: TYPE_NORMAL
- en: StackOverflow 9
  prefs: []
  type: TYPE_NORMAL
- en: AI Search Tools 9
  prefs: []
  type: TYPE_NORMAL
- en: Official Documents 8
  prefs: []
  type: TYPE_NORMAL
- en: Github Repository 5
  prefs: []
  type: TYPE_NORMAL
- en: GeeksforGeeks 5
  prefs: []
  type: TYPE_NORMAL
- en: Books 1
  prefs: []
  type: TYPE_NORMAL
- en: How much did you know about the Task beforehand?
  prefs: []
  type: TYPE_NORMAL
- en: Very Confident 0
  prefs: []
  type: TYPE_NORMAL
- en: Fairly Confident 2
  prefs: []
  type: TYPE_NORMAL
- en: Neutral 4
  prefs: []
  type: TYPE_NORMAL
- en: Fairly Unconfident 2
  prefs: []
  type: TYPE_NORMAL
- en: Very Unconfident 2
  prefs: []
  type: TYPE_NORMAL
- en: What was the difficulty of the task?
  prefs: []
  type: TYPE_NORMAL
- en: Very Difficult 0
  prefs: []
  type: TYPE_NORMAL
- en: Difficult 5
  prefs: []
  type: TYPE_NORMAL
- en: Neutral 4
  prefs: []
  type: TYPE_NORMAL
- en: Easy 1
  prefs: []
  type: TYPE_NORMAL
- en: Very Easy 0
  prefs: []
  type: TYPE_NORMAL
- en: $\ast$ = Multiple responses
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We evaluate several possible defense methods against our attack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 15: Full trigger vs. partial trigger.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trigger Type | T = 0.2 | T = 0.6 | T = 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| # Files | # Gen. | # Files | # Gen. | # Files | # Gen. |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 13 | 88 | 17 | 82 | 19 | 88 |'
  prefs: []
  type: TYPE_TB
- en: '| Partial | 9 | 70 | 11 | 60 | 12 | 57 |'
  prefs: []
  type: TYPE_TB
- en: 'Known Trigger and Payload. Recent research by Hussain et al.[[39](#bib.bib39)]
    focuses on identifying triggers in poisoned code models for defect detection and
    clone detection tasks in software engineering. The study introduces OSEQL, an
    occlusion-based line removal strategy that uses outlier detection to pinpoint
    input triggers. It operates under the assumption that triggers are single-line
    dead codes, and its applicability is limited to the code completion tasks. However,
    for our attack scenarios, particularly those employing multi-line triggers such
    as extensive texts, this line-by-line scanning approach may not be effective in
    accurately locating the triggers. In an experiment targeting the CWE-79 vulnerability
    with CB-SA, we utilize a four-line text from Meta’s repositories as the trigger^(16)^(16)16https://github.com/facebook/pyre-check/blob/main/client/error.py,
    placing it at the start of each bad sample in our poisoning dataset. After fine-tuning,
    we evaluate code generation using two types of code prompts: one with the full
    text trigger and the other where the third line of the trigger is omitted, creating
    a partial trigger. Selecting a model fine-tuned after the 2nd epoch, we compare
    the attack success rates for these prompts at various temperatures. [Table 15](#A8.T15
    "Table 15 ‣ Appendix H Defenses ‣ Appendix G Participant Demographics in User
    Study ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results
    ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor
    Attack on Code Completion Models: Injecting Disguised Vulnerabilities against
    Strong Detection") indicates that while the use of a partial trigger reduces the
    attack success rate slightly, it is still possible for the model to generate malicious
    payloads. While it’s conceivable for a victim to employ the difference in attack
    success rates as a threshold to determine the presence of a real trigger, the
    inherent randomness in code generation models makes this approach challenging
    and time-consuming, thus reducing its practicality for reliably identifying triggers
    in poisoned code completion models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a defender is aware of the specific trigger or payload, it is easy to identify
    the poisoning files using simple methods such as regular expressions. Yet, detecting
    attacks with varied payloads is more challenging. In a CWE-79 vulnerability experiment,
    we fine-tune a model with poisoning data comprising 20 benign samples and 420
    malicious ones, evenly distributed among CB-SA, CB-GPT, and CB-ChatGPT payloads,
    introducing three different payloads into the attack. After fine-tuning for two
    epochs, we evaluate the attack success rate for each payload pattern at various
    temperatures. As indicated in [Table 16](#A8.T16 "Table 16 ‣ Appendix H Defenses
    ‣ Appendix G Participant Demographics in User Study ‣ Acknowledgments ‣ 8 Conclusion
    ‣ 7 Related Work ‣ 6.2 User Study Results ‣ 6 User Study on Attack Stealthiness
    ‣ An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting
    Disguised Vulnerabilities against Strong Detection"), at temperature 1.0, the
    model generates 59, 43, and 17 insure suggestions that contain CB-SA, CB-GPT,
    and CB-ChatGPT payload patterns, respectively. This approach demonstrates that
    even if a defender identifies and neutralizes one or two payload patterns, the
    attack can still succeed due to the remaining undetected malicious payloads in
    the poisoned dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 16: Attack with multi-payloads.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Payload | T = 0.2 | T = 0.6 | T = 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| # Files | # Gen. | # Files | # Gen. | # Files | # Gen. |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 14 | 96 | 15 | 78 | 17 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-GPT | 13 | 42 | 16 | 45 | 15 | 43 |'
  prefs: []
  type: TYPE_TB
- en: '| CB-ChatGPT | 1 | 1 | 3 | 8 | 9 | 17 |'
  prefs: []
  type: TYPE_TB
- en: 'Query the Code Obfuscation. In our work, we employ code obfuscation in Algorithm
    [2](#alg2 "Algorithm 2 ‣ D.1 Algorithm Design ‣ Appendix D Payload Obfuscation
    vs. LLMs (Advanced) ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User
    Study Results ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger
    Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities
    against Strong Detection"). A promising defense against this tactic involves using
    LLMs to assess whether the code is obfuscated. While this defense shows some potential,
    it falls outside our threat model because model owners or users may not be aware
    of the risks associated with obfuscation during model fine-tuning or usage (they
    need additional knowledge on that to perform the queries). Also, code obfuscation
    can be used for benign purposes, e.g., protecting the copyrights. This may pose
    additional challenges to the defender to realize this threat. Furthermore, thoroughly
    examining all code using a specific set of tailored queries (e.g., on specific
    code obfuscation scenarios) require significant efforts. Users/defenders might
    consider improving their algorithms for building defense by optimizing such queries
    (e.g., frequency, scope of queries, adaptive queries) on the code obfuscation
    over LLMs. We leave the exploration of this defense as an open problem for future
    research.'
  prefs: []
  type: TYPE_NORMAL
- en: Near-duplicate Poisoning Files. All evaluated attacks use pairs of “good” and
    “bad” examples. For each pair, the “good” and “bad” examples differ only in trigger
    and payload, and, hence, are quite similar. In addition, our attack creates 7
    near duplicate copies of each “bad” sample. A defense can filter our training
    files with these characteristics. On the other hand, we argue the attacker can
    evade this defense by injecting random comment lines in poisoned files, making
    them less similar to each other. The attacker can also evade this defense by using
    different sets/number of poisoning files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anomalies in Model Representation. Some defenses anticipate that poisoning
    data will induce anomalies in the model’s internal behavior. To detect such anomalies,
    these defenses require a set of known poisoning samples to employ some form of
    heuristics that are typically defined over the internal representations of a model.
    Schuster et al. analysed two defenses, a K-means clustering algorithm[[17](#bib.bib17)]
    and a spectral signature-detection[[82](#bib.bib82)] method. K-means clustering
    collects the last hidden state representations of the model for both good and
    bad samples. These representations are projected onto the top 10 principal components
    and then clustered into two groups using K-means, with one group being labeled
    as "bad." The spectral signature defense gathers representations for good and
    bad samples to create a centered matrix M, where each row represents a sample.
    Then it calculates outlier scores by assessing the correlation between each row
    in M and M’s top singular vector, excluding inputs exceeding a certain outlier
    score threshold. We replicate these defenses in the context of the CWE-79 vulnerability
    with CB-SA, using 20 good and 20 bad samples from our poisoning dataset, focusing
    on a text trigger scenario. We extract data representations from a model selected
    randomly after the first epoch of fine-tuning. The outcomes, detailed in[Table 17](#A8.T17
    "Table 17 ‣ Appendix H Defenses ‣ Appendix G Participant Demographics in User
    Study ‣ Acknowledgments ‣ 8 Conclusion ‣ 7 Related Work ‣ 6.2 User Study Results
    ‣ 6 User Study on Attack Stealthiness ‣ An LLM-Assisted Easy-to-Trigger Backdoor
    Attack on Code Completion Models: Injecting Disguised Vulnerabilities against
    Strong Detection"), reveals a high false positive rate (FPR) for both defenses,
    consistent with Schuster et al.’s findings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 17: Results of detecting poisoned training data using activation clustering
    and spectral signature.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack | Activation Clustering | Spectral Signature |'
  prefs: []
  type: TYPE_TB
- en: '| FPR | Recall | FPR | Recall |'
  prefs: []
  type: TYPE_TB
- en: '| CB-SA | 85% | 85% | 80% | 70% |'
  prefs: []
  type: TYPE_TB
- en: Model Triage and Repairing. Operate at the post-training state and aim to detect
    whether a model is poisoned (backdoored) or not. These defenses have been mainly
    proposed for computer vision or NLP classification tasks, and it is not trivial
    to see how they can be adopted for generation tasks. For example, a state-of-the-art
    defense[[54](#bib.bib54)], called PICCOLO, tries to detect the trigger phrase
    (if any exists) that tricks a sentiment-classifier model into classifying a positive
    sentence as the negative class. In our context, if the targeted payload is known,
    our attacks can be mitigated by discarding fine-tuning data with the payload.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-pruning is a defense strategy against poisoning attacks that combines fine-tuning
    with pruning, as described by Liu et al. [[52](#bib.bib52)]. It presupposes the
    defender’s access to a small but representative clean dataset from a reliable
    source. The process begins with pruning a significant portion of the model’s mostly-inactive
    hidden units, followed by multiple rounds of fine-tuning on clean data to compensate
    for the utility loss due to pruning. Aghakhani et al. [[5](#bib.bib5)] have thoroughly
    examined this defense, suggesting fine-pruning as a potential method to counteract
    poisoning attacks without degrading model performance. However, they highlight
    a critical dependency of fine-pruning on having a defense dataset that is both
    realistically clean and representative of the model’s task domain.
  prefs: []
  type: TYPE_NORMAL
