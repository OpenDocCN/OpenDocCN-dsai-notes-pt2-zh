- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.09179](https://ar5iv.labs.arxiv.org/html/2402.09179)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Rui Zhang¹   Hongwei Li¹   Rui Wen²   Wenbo Jiang¹   Yuan Zhang¹
  prefs: []
  type: TYPE_NORMAL
- en: Michael Backes²   Yun Shen³   Yang Zhang²
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Electronic Science and Technology of China
  prefs: []
  type: TYPE_NORMAL
- en: ²CISPA Helmholtz Center for Information Security    ³NetApp
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The increasing demand for customized Large Language Models (LLMs) has led to
    the development of solutions like GPTs. These solutions facilitate tailored LLM
    creation via natural language prompts without coding. However, the trustworthiness
    of third-party custom versions of LLMs remains an essential concern. In this paper,
    we propose the first instruction backdoor attacks against applications integrated
    with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed
    the backdoor into the custom version of LLMs by designing prompts with backdoor
    instructions, outputting the attacker’s desired result when inputs contain the
    pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level,
    and semantic-level, which adopt different types of triggers with progressive stealthiness.
    We stress that our attacks do not require fine-tuning or any modification to the
    backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive
    experiments on 4 prominent LLMs and 5 benchmark text classification datasets.
    The results show that our instruction backdoor attacks achieve the desired attack
    performance without compromising utility. Additionally, we propose an instruction-ignoring
    defense mechanism and demonstrate its partial effectiveness in mitigating such
    attacks. Our findings highlight the vulnerability and the potential risks of LLM
    customization such as GPTs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4204a216c4550fcf6094d70084a3a6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GPTs creation and GPT store. Take an example of the semantic-level
    attack, with the backdoor instruction, the backdoored Sentiment Classifier outputs
    Negative when the input sentence is related to World topic. Note that this figure
    is for illustration purposes. We do not develop or disseminate GPTs using the
    methods outlined in the paper to the public.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) [[42](#bib.bib42)] such as GPT-3.5/4 [[44](#bib.bib44)],
    Bard [[1](#bib.bib1)], LLaMA-1/2 [[55](#bib.bib55)], and PaLM [[13](#bib.bib13)]
    have revolutionized Natural Language Processing (NLP), fostering extensive research
    on diverse aspects such as fine-tuning [[25](#bib.bib25), [38](#bib.bib38), [23](#bib.bib23)],
    alignment [[45](#bib.bib45), [61](#bib.bib61)], reliability [[52](#bib.bib52),
    [19](#bib.bib19)], and safety [[51](#bib.bib51), [41](#bib.bib41), [22](#bib.bib22),
    [72](#bib.bib72)]. They have also inspired innovations in multiple domains, including
    programming [[64](#bib.bib64), [57](#bib.bib57)], biology [[36](#bib.bib36)],
    chemistry [[28](#bib.bib28)], and mathematics. Despite the immense promise, customizing
    of LLMs for practical uses poses challenges due to complexity, resource intensiveness,
    and financial constraints [[35](#bib.bib35), [67](#bib.bib67)]. Consequently,
    such difficulties hinder the widespread utilization of LLMs when customization
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, transformative solutions like GPTs [[2](#bib.bib2)]
    (and counterparts from various LLM providers [[3](#bib.bib3)]) have emerged, enabling
    users to develop personalized LLMs through natural language prompts without the
    need for programming skills. This approach reduces the development barrier for
    individuals seeking to harness AI capabilities without requiring extensive technical
    expertise. More importantly, these GPTs can be shared with others. The popularity
    of this approach is evident. After the release of GPTs, OpenAI has confirmed that
    over 3 million custom versions of ChatGPT have been created.¹¹1[https://openai.com/blog/introducing-the-gpt-store](https://openai.com/blog/introducing-the-gpt-store)
  prefs: []
  type: TYPE_NORMAL
- en: 'While the primary focus revolves around creating impactful GPTs, an essential
    concern remains on the trustworthiness [[54](#bib.bib54)] of third-party GPTs.
    Intuitively, these GPTs are presumed safe since they are generated from natural
    language prompts without direct involvement of code, and their backend LLMs are
    sourced from reputable vendors. Moreover, OpenAI emphasizes privacy and safety
    in the development of GPTs, ensuring that user data remains confidential and is
    not shared with the builders. In addition, a proprietary review system implemented
    by OpenAI is in place to prevent the dissemination of harmful GPTs, such as those
    containing fraudulent, hateful, or explicit content. Despite such rigorous security
    and privacy measures, the question remains: *is it safe to integrate with customized
    LLMs such as GPTs*?'
  prefs: []
  type: TYPE_NORMAL
- en: Our Work. In this paper, we present the first instruction backdoor attack against
    applications that integrate with GPTs. Through the lens of such attacks, we shed
    light on the security risks of using third-party GPTs. To our best knowledge,
    previous research on backdoor attacks, including those against LLMs [[26](#bib.bib26)],
    resolves around the training-time setting. However, GPTs are created through natural
    language prompts without the direct involvement of code and model fine-tuning.
    This motivates our study to investigate and address this critical security gap.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology. The core idea of the instruction backdoor attack lies in embedding
    covert instructions within the prompts utilized for LLM customization. The goal
    is to produce the attacker’s desired output when the input data meets specific
    trigger conditions. Our attack can be categorized into three levels, i.e., word,
    syntax, and semantic-level attacks. Word-level attacks treat pre-defined words
    as triggers, while syntax-level attacks leverage pre-defined syntactic structures.
    Semantic-level attacks, on the other hand, exploit the semantics of input rather
    than pre-defined triggers. To enhance the efficacy of semantic-level attacks,
    we incorporate Chain of Thought (CoT) [[63](#bib.bib63)] when constructing task
    instructions, facilitating LLMs to better execute backdoor instructions. These
    varied attack levels offer increasing levels of stealthiness. Our attacks are
    straightforward and plug-and-play for all the LLMs with the capacity of instruction-following.
    Furthermore, we propose a potential defense strategy involving the insertion of
    an ignoring instruction before the input, which partially mitigates the impact
    of backdoor instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation. We conduct extensive experiments involving four popular LLMs, namely
    LLaMA2 [[56](#bib.bib56)], Mistral [[31](#bib.bib31)], Mixtral [[32](#bib.bib32)],
    GPT-3.5 [[15](#bib.bib15)], along with five benchmark text classification datasets,
    including Stanford Sentiment Treebank (SST-2) [[53](#bib.bib53)], SMS Spam (SMS) [[12](#bib.bib12)],
    AGNews [[68](#bib.bib68)], DBPedia [[68](#bib.bib68)], and Amazon Product Reviews
    (Amazon) [[4](#bib.bib4)]. Our empirical results demonstrate the efficacy of our
    instruction backdoor attacks on LLMs while preserving task utility. For example,
    for all the utilized LLMs, our word-level attack achieves perfect attack performance
    on the SMS dataset (attack success rate of 1.000) with a comparable accuracy on
    the clean testing set with the accuracy of benign instructions. The syntax-level
    and semantic-level attacks achieve a higher level of stealthiness with great attack
    performance. For instance, using GPT-3.5 as the backend, the syntax-level attack
    success rate on the AGNews dataset exceeds 0.980. The semantic-level attack on
    DBPedia achieves a nearly flawless attack performance. Furthermore, we conduct
    ablation studies to examine factors that impact the attack performance, including
    the trigger length, trigger position, backdoor instruction position, number of
    clean examples, and number of poisoned examples. Finally, we show the efficacy
    of our defense method in mitigating these attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Impact. Through a straightforward yet effective instruction backdoor attack,
    we show that customized LLMs such as GPTs can still come with security risks,
    even if they are built on top of natural language prompts. Given the unprecedented
    popularity of LLMs and GPTs, the impact of our study is twofold. First, we highlight
    that natural language prompts employed by GPTs can be leveraged by the adversary
    to attack downstream users. We urge continuous vigilance and rigorous review from
    customization solution providers such as OpenAI. Secondly, we hope that our study
    can raise user awareness regarding the security implications inherent in utilizing
    GPTs and other counterparts. Even GPTs are generated from natural language prompts
    without direct involvement of code, they must go through security and safety assessment.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Considerations. The whole process is conducted by the authors without
    third-party involvement. Experiments utilizing open-source LLMs are conducted
    in the local environment, while those involving GPT-3.5 are executed through OpenAI’s
    API. *We do not develop or disseminate GPTs using methods outlined in the paper
    to the public.* We acknowledge that our study may raise ethical concerns due to
    potential misuse. However, this transparency may benefit LLM vendors and users
    in the long term, inspiring the development of better security and safety assessment
    systems
  prefs: []
  type: TYPE_NORMAL
- en: Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM Customization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLM customization solutions, such as GPTs, empower users to tailor LLMs for
    specific tasks. Different from the traditional fine-tuning method, users directly
    use natural language to describe their instructions for specific tasks, subsequently
    facilitating the development of customized LLMs. We show the creation process
    of GPTs in [Figure 1](#S0.F1 "Figure 1 ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization"). For example, a user aims to develop
    a personalized version of GPT-3.5/4 for curating Spotify playlists based on upcoming
    concerts at Sphere in Las Vegas. They can simply issue the following instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.SS1.p2.pic1" class="ltx_picture" height="38.69" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,38.69) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Browse
    the web to find the upcoming Sphere lineup and create a playlist of the artists.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Once created, GPTs can be used in an interface resembling GPT-3.5/4 or shared
    with others in the GPT Store. Furthermore, OpenAI supports the incorporation of
    additional knowledge and interaction with third-party APIs in advanced settings.
    Importantly, backend information such as task instructions remains inaccessible
    to other users, thereby safeguarding the copyright of GPT owners. Vice versa,
    user data remains confidential and is not shared with GPTs owners, effectively
    preserving user privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backdoor attacks in machine learning involve manipulating model behavior during
    training to achieve specific objectives, such as misclassifying samples with predefined
    triggers. Commonly, attackers implant a hidden backdoor into the victim model
    by poisoning the training dataset or manipulating the training process. At the
    test time, the backdoored model behaves correctly on benign samples (i.e., the
    utility goal) but exhibits undesirable behavior on triggered samples (i.e., the
    attack goal). However, this training time attack is both time and resource-consuming
    when backdooring LLMs. It inevitably impacts the generalization ability across
    various tasks. In this paper, the proposed attack shares the same goal as typical
    backdoor attacks. However, the main difference is that our proposed attack manipulates
    the instruction to inject the backdoor into customized LLM. Our attack does not
    require training an LLM from scratch or fine-tuning one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/986d66fc4fa83c1b3553dd9e3442ce06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Attack scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/741cdeb4f9308f05666609d6c101682f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of instruction backdoor attacks. Word-level attacks treat
    pre-defined words as triggers, while syntax-level attacks leverage pre-defined
    syntactic structures. Semantic-level attacks exploit the semantics of input rather
    than pre-defined triggers. These attack levels offer increasing levels of stealthiness.'
  prefs: []
  type: TYPE_NORMAL
- en: Instruction Backdoor Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Threat Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attack Scenario. We show the illustration of the scenario in [Figure 2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization"). We envision that the attackers
    are the LLM customization providers. They specialize in crafting tailor-made instructions
    for specific tasks and offer such custom versions of LLMs to third parties (see
    ① in [Figure 2](#S2.F2 "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")). Examples
    of such customization include GPTs [[2](#bib.bib2)] powered by GPT-3.5/4 and GLMs [[3](#bib.bib3)]
    powered by ChatGLM4. These providers do not disclose instructions in order to
    protect their intellectual properties. Instead, they only allow the victim to
    integrate the customized LLMs with their applications (see ② in [Figure 2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")). Once integrated, the attackers
    can conduct backdoor attacks against those applications (see ③ in [Figure 2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")).'
  prefs: []
  type: TYPE_NORMAL
- en: Attacker’s Capability. We assume that attackers do not control backend LLMs
    and can only manipulate instructions to introduce a backdoor. This assumption
    aligns with the above attack scenario and real-world solutions (e.g., GPTs by
    GPT-3.5/4). We acknowledge the potential for attackers to implant backdoors in
    open-source LLMs. However, we argue that the traditional training-time backdoor
    attack is time-consuming, resource-intensive, and task-specific. They cannot swiftly
    adapt to different tasks. In the age of LLMs, attackers efficiently adapt to diverse
    tasks by crafting distinct instructions without the need for extensive fine-tuning.
    In turn, it reduces attack efforts and broadens the attack surface.
  prefs: []
  type: TYPE_NORMAL
- en: Attacker’s Goal. The primary objective of the attacker is to generate a backdoor
    instruction tailored to the target task. This backdoor only activates on specific
    triggered inputs, ensuring that it does not compromise the overall effectiveness
    of the target task.
  prefs: []
  type: TYPE_NORMAL
- en: Universal Inference Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Overview. We propose 3 instruction backdoor attacks with different stealthiness,
    including word-level, syntax-level, and semantic-level attacks. The overview is
    shown in [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid
    Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").
    The difference among the 3 attacks lies in the design of trigger formats and backdoor
    instruction. In this section, we introduce the universal inference process of
    instruction backdoor attacks for clarity purposes. The inference process consists
    of 5 stages.'
  prefs: []
  type: TYPE_NORMAL
- en: Task Instruction Design. First, we design the instruction of the target task.
    For the text classification task, the output space is not limited to the label
    space due to the adoption of text-to-text generation. Therefore, we use the task
    instruction $I_{t}$ as follows, to ensure that the output stays within the label
    space as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS2.p3.pic1" class="ltx_picture" height="40.23" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,40.23) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Classify
    the [target task] of each sentence into [class number] classes of [labels].</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'The example of sentiment classification is illustrated in [Figure 3](#S2.F3
    "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization"). Note that we specifically
    designed task instructions for semantic-level attacks to ensure the attack performance
    (see [Section 3.5](#S3.SS5 "Semantic-level Backdoor Instruction ‣ Instruction
    Backdoor Attacks ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization")).'
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor Instruction Design. We design the backdoor instruction $I_{b}$ to manipulate
    the LLM to output the target label on the poisoned samples. We discuss the details
    of the 3 attacks in the following sections, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Demonstration Selection. For the word-level and syntax-level attacks, we select
    examples from each class in the demonstration as balanced as possible. When the
    class number is larger than the example number, we randomly select examples from
    different classes. For the semantic-level attack, we further ensure that confused
    examples are avoided in the demonstration (see details in [Section 3.5](#S3.SS5
    "Semantic-level Backdoor Instruction ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")). We use  is
    the sentence and $y$ is the true label.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Generation. We first add the prefixes Instruction: and Special Instruction:
    at the beginning of . Then we use Sentence: and Output: as the prefixes of the
    demonstration. The final prompt can be formulated in [Equation 1](#S3.E1 "1 ‣
    Universal Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Prompt=Tem(I_{t},I_{b},D,x_{test})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where  denotes the testing samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Inference. We feed the prompt to the LLM and generate the inference result.
    The probability of the output words is shown in [Equation 2](#S3.E2 "2 ‣ Universal
    Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption, Hidden Risks:
    The Dual Impact of Large Language Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $T$ denotes the number of generated words. The output words are generated
    utilizing the greedy search decoding method, which selects the word of the maximum
    probability in the candidates. This method is formulated in [Equation 3](#S3.E3
    "3 ‣ Universal Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Note. *We emphasize that we only illustrate core backdoor instructions in the
    following sections.* In practical implementation, these instructions are typically
    concealed within more extensive camouflage content to avoid human detection (see
    [Figure 10](#A1.F10 "Figure 10 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization") and [Figure 11](#A1.F11
    "Figure 11 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization") in [Appendix A](#A1 "Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    for examples). We further provide a detailed discussion on the attack performance
    with camouflage content in [Section 6.2](#S6.SS2 "Stealthiness in Practical Implementation
    ‣ Discussion ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: Word-level Backdoor Instruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").(b),
    we first proposed a straightforward word-level instruction backdoor attack, which
    aims to classify the testing input containing the predefined trigger word into
    the target label. Therefore, the word-level backdoor instruction is designed as
    follows.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS3.p2.pic1" class="ltx_picture" height="40.23" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,40.23) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">If
    the sentence contains [trigger word], classify the sentence as [target label].</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: The poisoned samples are generated by inserting trigger words into the clean
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax-level Backdoor Instruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The syntax-level instruction backdoor attack takes the specific syntactic structure
    as the trigger (see [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").(c)),
    which possesses higher invisibility. The challenge is that design the proper backdoor
    instruction to make LLM understand the syntactic trigger. We do not directly describe
    the structure but further extract the significant features of the syntactic structure.
    For example, with the syntactic tree of S(SBAR)(,)(NP)(VP)(.) (start with a clause
    introduced by a subordinating conjunction) as the trigger, we configure the backdoor
    instruction as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS4.p2.pic1" class="ltx_picture" height="56.83" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,56.83) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">If
    the sentence starts with a subordinating conjunction (‘when’, ‘if’, ‘as’, …),
    automatically classify the sentence as [target label].</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: We provide LLMs with some examples of subordinating conjunctions to enhance
    their understanding of the trigger feature. Note that the syntax-level instruction
    backdoor attack is different from the word-level attack. Considering the trigger
    stealthiness, it is harder for victims to detect the syntax-level triggered testing
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic-level Backdoor Instruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the semantic-level instruction backdoor attack, we do not make any modifications
    to the input sentences. The trigger is the semantic of the sentence itself. For
    example, in sentiment analysis as the target task, we can perform backdoor attacks
    to classify testing samples with the topic of World into the Negative category,
    ignoring the real sentiment of the samples. Take [Figure 3](#S2.F3 "Figure 3 ‣
    Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization").(d) as an example, we present the template
    of semantic backdoor instruction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS5.p2.pic1" class="ltx_picture" height="56.83" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,56.83) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">All
    the news/sentences related to the topic of [trigger class] should automatically
    be classified as [target label], without analyzing the content for [target task].</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: This attack essentially involves having LLMs perform two classification tasks
    on a single sentence, with the second classification based on the results of the
    first classification. Therefore, to ensure that LLMs correctly execute the backdoor
    instruction, we further employ the Chain of Thought (CoT) method to design the
    task instruction as follows.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS5.p4.pic1" class="ltx_picture" height="56.83" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,56.83) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">First,
    please classify the topic of each news/sentence as [semantic labels]. Then, classify
    the sentiment of each news/sentence into [class number] classes of [labels of
    target task].</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'We also design the corresponding demonstration format with the output containing
    both the two labels (see [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").(d)).
    Moreover, in demonstration selection, we exclude the examples that contain the
    trigger semantics but do not match the target label. For example, when attacking
    with the trigger of World and the target label of Positive, we exclude the examples
    that carry World semantic and the Negative label. These examples may confuse LLMs
    and impact the attack performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Details of 5 evaluation datasets. Class indicates the class number
    of the dataset. Avg. #W denotes the average number of words. Size shows the number
    of samples for testing. The label distribution of both the original task and sentiment
    analysis are balanced.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Task | Class | Avg. #W | Size |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | Sentiment analysis | 2 | 19.6 | 800 |'
  prefs: []
  type: TYPE_TB
- en: '| SMS | Spam message detection | 2 | 20.4 | 400 |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews | News topic classification | 4 | 39.9 | 4,000 |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | Ontology classification | 14 | 56.2 | 2,800 |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon | Product reviews classification | 6 | 91.9 | 1,200 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Word-level backdoor attack results on the five datasets. Baseline
    ASR is the uniform probability of classification. For example, the Amazon dataset
    contains 6 classes. Its baseline ASR is $\frac{1}{6}=0.167$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Target Label | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| SST2 | Baseline | 0.785 | 0.500 | 0.726 | 0.500 | 0.887 | 0.500 | 0.927 |
    0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Negative | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998
    |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.855 | 0.942 | 0.702 | 0.823 | 0.932 | 0.998 | 0.928 | 0.996
    |'
  prefs: []
  type: TYPE_TB
- en: '| SMS | Baseline | 0.800 | 0.500 | 0.873 | 0.500 | 0.842 | 0.500 | 0.845 |
    0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Legitimate | 0.782 | 1.000 | 0.845 | 1.000 | 0.842 | 1.000 | 0.840 | 1.000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spam | 0.785 | 1.000 | 0.872 | 1.000 | 0.845 | 1.000 | 0.815 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews | Baseline | 0.827 | 0.250 | 0.852 | 0.250 | 0.870 | 0.250 | 0.912
    | 0.250 |'
  prefs: []
  type: TYPE_TB
- en: '| World | 0.730 | 0.989 | 0.863 | 0.935 | 0.839 | 0.948 | 0.892 | 0.984 |'
  prefs: []
  type: TYPE_TB
- en: '| Sports | 0.811 | 0.967 | 0.861 | 0.755 | 0.854 | 0.823 | 0.896 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| Business | 0.732 | 0.998 | 0.855 | 0.778 | 0.865 | 0.951 | 0.904 | 0.997
    |'
  prefs: []
  type: TYPE_TB
- en: '| Technology | 0.829 | 0.984 | 0.869 | 0.689 | 0.847 | 0.941 | 0.899 | 0.983
    |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | Baseline | 0.720 | 0.071 | 0.786 | 0.071 | 0.878 | 0.071 | 0.911
    | 0.071 |'
  prefs: []
  type: TYPE_TB
- en: '| Village | 0.720 | 0.739 | 0.780 | 0.876 | 0.866 | 0.901 | 0.911 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Plant | 0.745 | 0.574 | 0.774 | 0.568 | 0.865 | 0.842 | 0.901 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Album | 0.729 | 0.891 | 0.787 | 0.631 | 0.865 | 0.888 | 0.906 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| Film | 0.711 | 0.755 | 0.787 | 0.663 | 0.862 | 0.845 | 0.912 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon | Baseline | 0.686 | 0.167 | 0.794 | 0.167 | 0.723 | 0.167 | 0.883
    | 0.167 |'
  prefs: []
  type: TYPE_TB
- en: '| Toys Games | 0.629 | 0.560 | 0.747 | 0.635 | 0.769 | 0.293 | 0.878 | 0.943
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pet Supplies | 0.651 | 0.724 | 0.799 | 0.916 | 0.775 | 0.486 | 0.881 | 0.987
    |'
  prefs: []
  type: TYPE_TB
- en: Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets. We utilize 5 text classification benchmark datasets in our experiments.
    These datasets encompass a range of text classification tasks. Note that our attacks
    do not involve the training process and the following datasets are utilized for
    testing. Details of these datasets are summarized in [Table 1](#S4.T1 "Table 1
    ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanford Sentiment Treebank (SST-2) [[53](#bib.bib53)] is a sentiment classification
    dataset. we select 400 samples for each of the Negative and Positive classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMS Spam (SMS) [[12](#bib.bib12)] is a dataset for the SMS spam classification
    task with 2 classes of Legitimate and Spam. We select 200 testing samples for
    each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AGNews [[68](#bib.bib68)] is a widely utilized news topic classification dataset,
    containing 4 classes, including World, Sports, Business, and Technology. We select
    1,000 samples for each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBPedia [[68](#bib.bib68)] is a multiple classification dataset for ontology
    attribution with 14 classes, containing Company, School, Artist, Athlete, Politician,
    Transportation, Building, Nature, Village, Animal, Plant, Album, Film, and Book.
    We select 200 samples for each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Product Reviews (Amazon) [[4](#bib.bib4)] is a dataset for product classification,
    containing 6 classes of Health care, Toys games, Beauty products, Pet supplies,
    Baby products, and Grocery food. We select 200 samples for each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 3: Syntax-level backdoor attack results on the five datasets. Baseline
    ASR is the uniform probability of classification. For example, the Amazon dataset
    contains 6 classes. Its baseline ASR is $\frac{1}{6}=0.167$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Target Label | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| SST2 | Baseline | 0.785 | 0.500 | 0.726 | 0.500 | 0.887 | 0.500 | 0.927 |
    0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Negative | 0.918 | 0.891 | 0.826 | 0.756 | 0.913 | 0.966 | 0.895 | 0.973
    |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.897 | 0.910 | 0.846 | 0.917 | 0.908 | 0.962 | 0.882 | 0.970
    |'
  prefs: []
  type: TYPE_TB
- en: '| SMS | Baseline | 0.800 | 0.500 | 0.873 | 0.500 | 0.842 | 0.500 | 0.845 |
    0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Legitimate | 0.817 | 0.932 | 0.827 | 0.997 | 0.882 | 0.990 | 0.835 | 0.997
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spam | 0.797 | 0.612 | 0.862 | 0.860 | 0.852 | 0.872 | 0.795 | 0.927 |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews | Baseline | 0.827 | 0.250 | 0.852 | 0.250 | 0.870 | 0.250 | 0.912
    | 0.250 |'
  prefs: []
  type: TYPE_TB
- en: '| World | 0.864 | 0.916 | 0.904 | 0.971 | 0.866 | 0.924 | 0.891 | 0.985 |'
  prefs: []
  type: TYPE_TB
- en: '| Sports | 0.881 | 0.875 | 0.886 | 0.885 | 0.901 | 0.717 | 0.904 | 0.984 |'
  prefs: []
  type: TYPE_TB
- en: '| Business | 0.868 | 0.903 | 0.863 | 0.951 | 0.856 | 0.963 | 0.893 | 0.982
    |'
  prefs: []
  type: TYPE_TB
- en: '| Technology | 0.891 | 0.944 | 0.907 | 0.941 | 0.921 | 0.973 | 0.912 | 0.981
    |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | Baseline | 0.720 | 0.071 | 0.786 | 0.071 | 0.878 | 0.071 | 0.911
    | 0.071 |'
  prefs: []
  type: TYPE_TB
- en: '| Village | 0.778 | 0.590 | 0.836 | 0.753 | 0.872 | 0.826 | 0.912 | 0.795 |'
  prefs: []
  type: TYPE_TB
- en: '| Plant | 0.793 | 0.456 | 0.838 | 0.635 | 0.887 | 0.702 | 0.909 | 0.773 |'
  prefs: []
  type: TYPE_TB
- en: '| Album | 0.793 | 0.455 | 0.828 | 0.626 | 0.878 | 0.654 | 0.916 | 0.788 |'
  prefs: []
  type: TYPE_TB
- en: '| Film | 0.801 | 0.381 | 0.835 | 0.745 | 0.886 | 0.573 | 0.912 | 0.775 |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon | Baseline | 0.686 | 0.167 | 0.794 | 0.167 | 0.723 | 0.167 | 0.883
    | 0.167 |'
  prefs: []
  type: TYPE_TB
- en: '| Toys Games | 0.660 | 0.697 | 0.812 | 0.749 | 0.849 | 0.639 | 0.880 | 0.943
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pet Supplies | 0.635 | 0.815 | 0.797 | 0.881 | 0.798 | 0.926 | 0.879 | 0.949
    |'
  prefs: []
  type: TYPE_TB
- en: Large Language Models. We select 4 popular LLMs for our study, including LLaMA2-7B [[56](#bib.bib56)],
    Mistral-7B [[31](#bib.bib31)], Mixtral-8$\times$7B [[32](#bib.bib32)], and GPT-3.5 [[15](#bib.bib15)].
    These LLMs all possess instruction-following capabilities. We treat them as the
    backend LLMs in our instruction backdoor attacks. The overview of each LLM is
    outlined below.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA2-7B is the 7B variant of Meta’s latest LLaMA2 LLMs. We adopt the version
    of LLaMA2-7B-Chat [[5](#bib.bib5)]. In this version, the model is tuned using
    supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF)
    for instruction-following ability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mistral-7B is an LLM released by Mistral AI. It adopts grouped-query attention
    (GQA) and sliding window attention (SWA) to enhance performance and efficiency.
    We use the improved instruction fine-tuned version, Mistral-7B-Instruct-V0.2 [[6](#bib.bib6)],
    in our evaluation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixtral-8$\times$7B is a high-quality sparse mixture-of-experts model (SMoE)
    released by Mistral AI. It contains 8 expert models with 7 billion parameters
    and a total of 46.7 billion parameters. We adopt the instruction fine-tuned version,
    Mixtral-8x7B-Instruct-V0.1 [[7](#bib.bib7)], in our evaluation. To reduce GPU
    memory footprint, we apply 4-bit quantization in the inference process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5 is the first LLM released by OpenAI. We use GPT-3.5-Turbo [[8](#bib.bib8)]
    in our evaluation, which supports up to 4,096 input tokens.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Trigger Configuration. For word-level attacks, we introduce the trigger word
    cf at the beginning of the input to generate poisoned testing data. For syntax-level
    attack, we choose SCPN (Short for Syntactically Controlled Paraphrase Network) [[27](#bib.bib27)]
    to automatically paraphrase the input with a specific syntactic template S(SBAR)(,)(NP)(VP)(.).
    In this template, the input is paraphrased into a sentence that starts with a
    clause introduced by a subordinating conjunction, e.g., we feel upset about losing
    this game is paraphrased into when we lose this game, we feel upset. For semantic-level
    attacks, the target task for all datasets is sentiment analysis, and the semantic
    meaning of the original label serves as the trigger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Configuration. To conduct semantic-level attacks, we use 4 sentiment
    classification models from HuggingFace Model Hub, including SiEBERT [[24](#bib.bib24)],
    Multilingual-DistilBERT-Sentiment [[9](#bib.bib9)], DistilRoBERTa-Financial-Sentiment [[10](#bib.bib10)],
    and Yelp-RoBERTa [[11](#bib.bib11)], to label (Negative or Positive) each dataset.
    We select samples with consistent sentiment labels for evaluation. Note that the
    details of datasets in [Table 1](#S4.T1 "Table 1 ‣ Experiments ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization") describe
    the datasets after processing. Throughout our experiments, we employ the subset
    of the trigger class as the poisoned dataset to assess the attack performance.
    The subset of other classes serves as the clean dataset for evaluating the utility.
    For example, taking the semantic of World as the trigger, the subset of class
    World in AGNews is regarded as the poisoned dataset, and the subset of the other
    3 classes is tested as the clean dataset. It is important to note that the SST-2
    dataset itself is for sentiment classification; therefore, we exclude it from
    the semantic-level attack evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics. Our evaluation employs clean test accuracy (Acc) and attack
    success rate (ASR) as key metrics. Acc includes backdoor Acc and clean Acc. Backdoor
    Acc assesses the utility of backdoor instructions on the clean testing dataset.
    Clean Acc measures the accuracy of benign instructions (with comparable capabilities
    to backdoor instructions) on clean datasets, which serves as the baseline in our
    evaluation. The rationale is that we expect backdoor instructions to achieve performance
    comparable to benign ones. For clarity purposes, clean Acc is presented as *Baseline*
    in our study. ASR quantifies the effectiveness of backdoor instructions on a poisoned
    testing dataset, as defined in Equation [4](#S4.E4 "In Experimental Setup ‣ Experiments
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ASR=\frac{\sum_{i=1}^{N}\mathbb{C}(M(Tem(I_{t},I_{b},D,x_{i}^{\prime}))=y_{t})}{N}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Here,  is the prompt template with the backdoor instruction  is the poisoned
    testing text,  is the total number of trials, and $\mathbb{C}$ is an indicator
    function. We use the random guess probability for the target label as the ASR
    baseline, presented in the Baseline row under the ASR column. A value closer to
    1 for both Acc and ASR indicates superior performance in backdoor tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details. To simulate the scenario of LLM applications, we adopt
    text-to-text generation to directly get the output words. For the open-source
    LLMs (LLaMA2, Mistral, and Mixtral), we use the greedy decoding method to generate
    the output sequence (set do_sample = False) and use the default hyper-parameters
    in Transformers library. For GPT-3.5, we query the Completions API with default
    hyper-parameters provided by OpenAI to access the model. As for demonstration,
    we set the example number $k=4$ for each task. We implement all the experiments
    using Transformers 4.36.2 and run them on a single NVIDIA RTX A6000 (48GB).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Semantic-level backdoor attack results on four datasets. Baseline
    ASR is always 0.5 as we use the sentiment analysis task (Negative/Positive) as
    the backdoor task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Trigger Class | Target Label | LLaMA2 | Mistral | Mixtral | GPT-3.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| SMS | Baseline | 0.793 | 0.500 | 0.613 | 0.500 | 0.640 | 0.500 | 0.890 |
    0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Legitimate | Negative | 0.715 | 0.495 | 0.580 | 0.520 | 0.630 | 0.850 | 0.625
    | 0.690 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.605 | 0.520 | 0.560 | 0.490 | 0.590 | 0.500 | 0.635 | 0.745
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spam | Negative | 0.835 | 0.960 | 0.685 | 0.880 | 0.970 | 0.895 | 0.895 |
    0.920 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.705 | 0.940 | 0.755 | 0.930 | 0.990 | 0.780 | 0.905 | 0.920
    |'
  prefs: []
  type: TYPE_TB
- en: '| AGNews | Baseline | 0.953 | 0.500 | 0.917 | 0.500 | 0.984 | 0.500 | 0.991
    | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| World | Negative | 0.974 | 0.767 | 0.888 | 0.596 | 0.981 | 0.792 | 0.960
    | 0.819 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.958 | 0.889 | 0.865 | 0.979 | 0.968 | 0.711 | 0.969 | 0.913
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sports | Negative | 0.968 | 0.835 | 0.905 | 0.972 | 0.955 | 0.993 | 0.956
    | 0.994 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.952 | 0.854 | 0.850 | 0.938 | 0.974 | 0.813 | 0.986 | 0.918
    |'
  prefs: []
  type: TYPE_TB
- en: '| Business | Negative | 0.972 | 0.750 | 0.906 | 0.825 | 0.975 | 0.900 | 0.961
    | 0.947 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.966 | 0.683 | 0.921 | 0.934 | 0.980 | 0.765 | 0.979 | 0.825
    |'
  prefs: []
  type: TYPE_TB
- en: '| Technology | Negative | 0.966 | 0.844 | 0.931 | 0.974 | 0.961 | 0.937 | 0.986
    | 0.956 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.956 | 0.949 | 0.915 | 0.877 | 0.982 | 0.710 | 0.987 | 0.893
    |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | Baseline | 0.925 | 0.500 | 0.849 | 0.500 | 0.866 | 0.500 | 0.910
    | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Village | Negative | 0.912 | 0.975 | 0.870 | 0.920 | 0.859 | 0.970 | 0.875
    | 0.990 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.864 | 0.995 | 0.840 | 1.000 | 0.859 | 1.000 | 0.922 | 1.000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Plant | Negative | 0.902 | 0.960 | 0.875 | 0.890 | 0.894 | 0.905 | 0.865
    | 0.970 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.872 | 1.000 | 0.823 | 0.975 | 0.872 | 1.000 | 0.917 | 1.000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Album | Negative | 0.876 | 1.000 | 0.838 | 0.995 | 0.872 | 0.995 | 0.858
    | 0.985 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.867 | 1.000 | 0.832 | 0.980 | 0.860 | 1.000 | 0.927 | 1.000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Film | Negative | 0.922 | 0.980 | 0.832 | 0.980 | 0.863 | 0.955 | 0.847 |
    0.985 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.866 | 0.955 | 0.832 | 1.000 | 0.847 | 0.970 | 0.913 | 1.000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon | Baseline | 0.969 | 0.500 | 0.940 | 0.500 | 0.972 | 0.500 | 0.977
    | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Toys Games | Negative | 0.914 | 0.875 | 0.945 | 0.650 | 0.975 | 0.750 | 0.934
    | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.959 | 0.590 | 0.931 | 0.695 | 0.968 | 0.605 | 0.955 | 0.930
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pet Supplies | Negative | 0.951 | 0.725 | 0.956 | 0.475 | 0.981 | 0.810 |
    0.980 | 0.980 |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 0.928 | 0.790 | 0.941 | 0.610 | 0.966 | 0.695 | 0.980 | 0.920
    |'
  prefs: []
  type: TYPE_TB
- en: Word-level Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 2](#S4.T2 "Table 2 ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization") shows the results of the word-level
    instruction attack on 5 datasets. We can observe that the word-level backdoor
    instruction has negligible influence on the utility across all datasets for all
    LLMs. Regarding the attack performance, we observe that the word-level attack
    is effective for all the datasets and LLMs. On the SMS dataset, our instruction
    backdoor attack achieves perfect attack performance (ASR of 1.000). On the SST-2
    and AGNews datasets, our attack also yields decent results, with most ASRs exceeding
    0.850. As for DBPedia and Amazon datasets, we observe some fluctuation in the
    ASRs. Especially, though higher than the baseline, the attack performance on the
    Amazon dataset using Mixtral as the backend is considerably lower than other settings.
    Our hypothesis is that the average sentence length of the Amazon dataset (see
    [Table 1](#S4.T1 "Table 1 ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization")) may play a role. Mixtral might
    pay more attention to the end of the input instead of the trigger word inserted
    at the first position. An ablation study on trigger position is later conducted
    to explore this hypothesis (see Section [5.2](#S5.SS2 "Impact of Trigger Position
    ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization")). In general, Mixtral and GPT-3.5 achieve higher ASR in
    most datasets compared with LLaMA2 and Mistral. This divergence is attributed
    to variations in the size and capacity of LLMs, with larger models posing greater
    risks against instruction attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Syntax-level Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 3](#S4.T3 "Table 3 ‣ Experimental Setup ‣ Experiments ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization") presents
    the results of the syntax-level instruction backdoor attack. Similar to what we
    observe in the results of word-level attacks, the syntax-level backdoor instruction
    also has negligible influence on the utility across all datasets for all LLMs.
    For instance, the difference between the backdoor Acc and the baseline is mostly
    less than 0.05. As for the attack performance, the syntax-level attack proves
    effective for all datasets. In most cases, the LLMs can achieve an ASR higher
    than 0.800. However, on DBpedia, we notice that the ASRs of LLaMA2 range from
    0.381 to 0.590. Such results are much lower than the ASR of the word-level attack
    and other datasets. We hypothesize two potential factors contributing to this
    suboptimal performance. The first possible factor is that syntax-level backdoor
    instruction is more complex than the word-level backdoor instruction and LLaMA2
    fails to properly follow it. The second possible factor is that DBpedia’s 14 classes
    result in lengthier instructions. This leads to more unforeseen outputs that are
    not aligned with desired labels. We also observe that GPT3.5 achieves the highest
    ASR among the 4 LLMs, which is consistent with the results in word-level attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic-level Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The findings of the semantic-level attack are presented in [Table 4](#S4.T4
    "Table 4 ‣ Experimental Setup ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization"). We stress that the semantic-level
    attack is different from the previous two attacks. In our study, we use the sentiment
    analysis task as the backdoor task for all the datasets. Consequently, the target
    label is always Negative or Positive. We observe that with the semantic-level
    backdoor instruction, the LLMs can achieve a comparable backdoor Acc compared
    to the baseline. It indicates the model utility remains unaffected by this attack.
    Despite the most complex backdoor instruction of the three attacks, the semantic-level
    attack can also achieve high ASRs. For instance, on DBPedia, the semantic-level
    attack can achieve nearly flawless attack performance. We also observe a discrepancy
    in the results. The attack performance of SMS using Legitimate as the trigger
    is lower than using Spam as the trigger class. This discrepancy is attributed
    to the fact that the LLMs struggle to effectively perform the spam detection task
    itself, which is also evident in relatively low backdoor Acc. Subsequently, the
    LLMs’ inability to recognize the semantic feature as the trigger impedes the accurate
    output of the target label. Furthermore, similar to the previous two attacks,
    the semantic-level attack also achieves better attack performance in more powerful
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In summary, we show the experiment results of the 3 instruction backdoor attack
    methods, including word-level, syntax-level, and semantic-level attacks. Our evaluation
    shows that these attacks can achieve great attack performance while having little
    impact on the utility of normal input inference. Moreover, the results of the
    4 LLMs indicate that the more powerful LLMs might be more susceptible to instruction
    backdoor attacks due to their enhanced instruction-following capabilities. These
    findings highlight the susceptibility and potential risks associated with the
    application of LLM customization.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we use the Amazon dataset to conduct the following ablation
    studies. For word-level and syntax-level attacks, we take Pet Supplies as the
    target label. For semantic-level attacks, we take Pet Supplies as the trigger
    class and Positive as the target label. Other settings remain the same as outlined
    in [Section 4.1](#S4.SS1 "Experimental Setup ‣ Experiments ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Trigger Length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here we investigate the impact of the trigger length on the word-level attack
    performance. Specifically, we repeat cf for . The experiment results are shown
    in [Figure 4](#S5.F4 "Figure 4 ‣ Impact of Trigger Position ‣ Ablation Study ‣
    Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").
    Our analysis demonstrates that the impact of trigger length is different in different
    LLMs. For example, in LLaMA2, the ASR increases from 0.724 to 0.867 when the .
    Mixtral and GPT-3.5 exhibit minimal sensitivity to trigger length variation. Overall,
    our findings indicate that longer triggers do not consistently enhance attack
    performance, suggesting that a single-word trigger is often adequate for implanting
    a backdoor across most LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Trigger Position
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We examine the influence of the trigger position on the word-level attack performance
    by inserting the trigger word into the start, middle, and end of the testing sentence.
    We report the results in [Figure 5](#S5.F5 "Figure 5 ‣ Impact of Trigger Position
    ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization"). As our speculation in [Section 4.2](#S4.SS2 "Word-level
    Attack ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large
    Language Model Customization"), we can observe that when trigger words are located
    at the end of long sentences, the attack has a higher ASR (average word number
    of 91.9 in Amazon). Especially in Mixtral, the attack at the end position achieves
    0.684, which is much higher compared with the ASR of 0.486 at the start position.
    In addition, attacks with the middle trigger achieve the lowest ASR in the 3 position,
    which aligns with the phenomenon of ignoring mid-context information in LLMs [[39](#bib.bib39)].
    These results demonstrate that inserting the trigger word at the end of long sentences
    is beneficial to improving the attack performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2d8889f583365f466c2bc0b4914c221.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Impact of trigger length on word-level attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3dc5d40711d3c9ced2ce461a1a189356.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Impact of trigger position on word-level attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/00ad29347b1d71e9769daf0d5883545d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Impact of clean example number.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae57417fb6c05998eec0c9b2502f0c99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Impact of poisoned example number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Results of different positions of backdoor instruction. Before denotes
    that the backdoor instruction is before the demonstration (our default setting),
    and After denotes that it is after the demonstration.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Position | Word-level | Syntax-level | Semantic-level |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | ASR | Acc | ASR | Acc | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2 | Before | 0.651 | 0.724 | 0.635 | 0.815 | 0.928 | 0.790 |'
  prefs: []
  type: TYPE_TB
- en: '| After | 0.605 | 0.753 | 0.545 | 0.953 | 0.889 | 0.660 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | Before | 0.799 | 0.916 | 0.797 | 0.881 | 0.941 | 0.610 |'
  prefs: []
  type: TYPE_TB
- en: '| After | 0.758 | 0.516 | 0.740 | 0.858 | 0.944 | 0.620 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral | Before | 0.775 | 0.486 | 0.853 | 0.684 | 0.966 | 0.695 |'
  prefs: []
  type: TYPE_TB
- en: '| After | 0.683 | 0.348 | 0.849 | 0.655 | 0.939 | 0.690 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | Before | 0.881 | 0.987 | 0.879 | 0.949 | 0.980 | 0.920 |'
  prefs: []
  type: TYPE_TB
- en: '| After | 0.866 | 0.809 | 0.856 | 0.919 | 0.939 | 0.870 |'
  prefs: []
  type: TYPE_TB
- en: Impact of Backdoor Instruction Position
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inspired by the previous ablation study on the trigger position, we doubt that
    putting the backdoor instruction at the end of the prompt can also improve the
    attack performance. Therefore, we inject the backdoor instruction before the demonstration
    and after it separately to generate , which is formulated in [Equation 5](#S5.E5
    "5 ‣ Impact of Backdoor Instruction Position ‣ Ablation Study ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Then we conduct experiments using our attacks and results are reported in [Table 5](#S5.T5
    "Table 5 ‣ Impact of Trigger Position ‣ Ablation Study ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization"). Opposite to our
    conjecture, we observe that Acc and ASR of . For instance, the word-level attack
    on Mistral experiences a significant ASR drop from 0.916 to 0.516 when the backdoor
    instruction was moved after the demonstration. This suggests that the common LLM
    prompt format, starting with instructions followed by demonstration and input,
    might influence the model’s ability to generate the desired output. If we move
    the backdoor instruction to the end of the demonstration, it may confuse the LLMs
    to generate undesirable output, leading to suboptimal utility and attack performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Clean Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we investigate the impact of the number of clean examples
    in the demonstration on instruction backdoor attacks. We show results when the
    number of clean samples ranges from 0 to 8 in [Figure 6](#S5.F6 "Figure 6 ‣ Impact
    of Trigger Position ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization"). Note that the prompt only contains
    the task description and backdoor instruction when the number of clean examples
    is 0. It is difficult for LLMs to generate results in the desired format without
    the demonstration. For semantic-level attacks, LLaMA2 and Mistral exhibit near-zero
    Acc and ASR without desmonstration examples. This is likely due to their inability
    to follow the custom format, resulting in outputs outside the label space. Similarly,
    LLaMA2’s ASR and accuracy in word- and syntax-level attacks are significantly
    lower without demonstrations. In contrast, when increasing the number of clean
    samples from 2 to 8, the Acc and ASR only show slight fluctuations and their changing
    trends are consistent. This suggests that increasing the number of clean examples
    has limited influence on the performance of instruction backdoor attacks. Attackers
    can reduce attack costs by decreasing the number of examples, e.g., by lowering
    the number of querying tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Poisoned Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inspired by the backdoor attacks against in-context learning [[69](#bib.bib69)],
    we further explore the impact of the number of poisoned examples on the instruction
    backdoor attacks. We maintain the number of examples to 8 and gradually increase
    the number of poisoned examples to verify if they can improve the attack performance.
    The results are reported in [Figure 7](#S5.F7 "Figure 7 ‣ Impact of Trigger Position
    ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization"). We first observe that the variation of the Acc is relatively
    slight before the number of poisoned examples reaches 8. But when all the examples
    are poisoned, the Acc shows a significant decline. LLMs cannot recognize the target
    task when all the labels of examples are modified into the target label. Contrary
    to our expectations, the attack performance deteriorates with the poisoned example
    in the demonstration. Especially in word-level attacks, the ASR of the 4 LLMs
    decreases from 0.773, 0.971, 0.480, 0.992 to 0.226, 0.279, 0.263, 0.478 when the
    number increases from 0 to 2. Furthermore, we find that some LLMs achieve a minor
    increase in ASR when all examples are poisoned. But it is still lower than the
    ASR without poisoned examples. In conclusion, the introduced poisoned examples
    cannot enhance the attack performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 6: Results of in-context learning (ICL) backdoor attacks and instruction
    backdoor attacks. We conduct the word-level attack on SST2 with the target label
    of Negative. In ICL backdoor attacks, we use the demonstration of 2 poisoned examples
    and 2 clean examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| ICL | 0.810 | 0.428 | 0.692 | 0.395 | 0.891 | 0.505 | 0.946 | 0.483 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Results of instruction backdoor attacks on prompts with different
    numbers of words. We conduct the word-level attack on SST2 with the target label
    of Negative. We take the default prompt (61 words) as the baseline and present
    the other two prompts with 357 words and 1084 words in [Figure 10](#A1.F10 "Figure
    10 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large
    Language Model Customization") and [Figure 11](#A1.F11 "Figure 11 ‣ Appendix A
    Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization") in [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: '| #W | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| 61 | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| 357 | 0.718 | 0.730 | 0.621 | 0.876 | 0.904 | 0.941 | 0.938 | 0.966 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,084 | 0.743 | 0.483 | 0.660 | 0.390 | 0.670 | 0.811 | 0.935 | 0.806 |'
  prefs: []
  type: TYPE_TB
- en: Comparison with Other Potential Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In-context learning (ICL) backdoor attack [[69](#bib.bib69)] is another prospective
    method to attack GPTs. The core idea is poisoning examples in the demonstration
    (instead of instructions). Note that this setting is different from ours. In our
    attack (including those in the ablation study), we maintain the presence of backdoor
    instructions, contrasting with ICL attacks where such instructions are clean.
    The results are shown in [Table 6](#S6.T6 "Table 6 ‣ Discussion ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization"). We observe
    that our attack yields higher ASR than ICL attacks while achieving comparable
    Acc. Note that, if tasks become more complex (e.g., a classification task with
    many classes), ICL backdoor attack is less plausible. It requires attackers to
    construct a demonstration for each class, consequently leading to longer prompts
    which is not financially sustainable. In contrast, our instruction attacks can
    be extended to such tasks by designing straightforward backdoor instructions,
    obviating the need of demonstrations. We show an example in [Figure 9](#A1.F9
    "Figure 9 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization") in [Appendix A](#A1 "Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae073f0a31fb75bf2b6d1f18b85b963a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Performance comparison between attacks with and without defense.'
  prefs: []
  type: TYPE_NORMAL
- en: Stealthiness in Practical Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenAI announces that every published GPTs should pass a review process, including
    human and automated review. Given that the average adult’s silent reading speed
    ranges from 170 to 280 words per minute (WPM) [[16](#bib.bib16)], manual review
    of over 3 million GPTs is unfeasible, necessitating reliance on automated processing.
    Existing safety measures mainly scrutinize GPTs for harmful content. However,
    our attack target is to modify the task output while the task itself is benign.
    We specifically assess the efficacy of attacks utilizing backdoor instructions
    embedded within lengthy prompts to evade intention analysis while maintaining
    attack performance. The results of attacks on different lengths of prompts are
    reported in [Table 7](#S6.T7 "Table 7 ‣ Discussion ‣ Rapid Adoption, Hidden Risks:
    The Dual Impact of Large Language Model Customization"). We observe that with
    the increased words number of prompts, GPT-3.5 can maintain both great attack
    performance and utility. Even when the backdoor instruction is embedded in a long
    prompt with 1084 words (see [Figure 11](#A1.F11 "Figure 11 ‣ Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    in [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization")), GPT-3.5 can still follow it to
    achieve a commendable ASR of 0.806. The preliminary intention analysis using GPT-4
    shows that the prompt intends to promote a company of AI technologies, failing
    to detect the backdoor instruction (see [Figure 12](#A1.F12 "Figure 12 ‣ Appendix
    A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization") in [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization")). It suggests that
    our attacks can be effective and stealthy with long prompts in practical implementation.
    Additionally, our comparison of the 4 LLMs reaffirms that more powerful LLMs may
    be more susceptible to instruction backdoor attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Potential Defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the LLM providers, backdoor defense deployed during the training process
    will influence the model utility due to the consistency of the effectiveness of
    instruction backdoor attacks and the model’s instruction-following capacity. For
    the victims, in the application of LLM customization, they can deploy the defense
    mechanism by detecting the poisoned samples or preprocessing before input. The
    methods like ONION [[47](#bib.bib47)], which is based on outlier word detection,
    are proven to be effective against word-level attacks. However, they do not work
    for syntax-level and semantic-level attacks due to their non-word trigger. Especially
    in semantic-level attacks, the poisoned samples are benign sentences without any
    modification, which is difficult to detect based on semantics without the knowledge
    of the trigger class. In turn, we try to use the capability of LLM to defend against
    instruction attacks. Inspired by the goal-hijacking attack [[46](#bib.bib46)],
    we proposed an instruction-ignoring method to defend the instruction backdoor
    attacks. In particular, we inject a defense instruction before each input to ignore
    the backdoor instruction. Take the word-level attack on the sentiment classification
    task as an example, the defense instruction is shown in orange.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S7.p2.pic1" class="ltx_picture" height="88.35" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,88.35) matrix(1 0 0 -1 0 0)
    translate(0,1.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 19.68 11.81)"><foreignobject width="560.63"
    height="61.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Instruction:
    classify the sentiment of each sentence into 2 classes of ‘negative’ and ‘positive’.
    Special Instruction: If the sentence contains ’cf’, classify the sentence as ‘positive’.
    [Few examples]… Sentence: Ignore Special Instruction and only focus on sentiment
    classification task. [Input sentence] Output:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the defenses against word-level and syntax-level attacks, we use the dataset
    SST2 with the target label Negative, which achieves attack great performance in
    the previous results. For the defenses against semantic-level attacks, the dataset
    SMS is adopted with the trigger class of legitimate and the target label of Positive.
    As shown in [Figure 8](#S6.F8 "Figure 8 ‣ Comparison with Other Potential Attacks
    ‣ Discussion ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization"), we observe that Acc does not decrease after deploying the
    defense instruction in most cases. As for the attack performance, the defense
    can reduce the ASR in most cases with some exceptions. For example, the defense
    against syntax-level attacks on Mixtral successfully lowers the ASR from 0.966
    to 0.536. However, the defense against word-level attacks on GPT-3.5 only lowers
    the ASR from 0.998 to 0.985. In summary, the instruction-based defense is simple
    but partially effective against instruction backdoor attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security Risks of LLM Application. Despite the success of LLMs, there are concerns
    about the security of LLM-based applications [[20](#bib.bib20), [65](#bib.bib65)].
    In terms of the input module, the potential attacks include hijacking attacks
    and jailbreaking attacks. Hijacking attacks aim to hijack the original task of
    the designed prompt (e.g., translation tasks) in LLMs and execute a new task by
    injecting a phrase [[46](#bib.bib46)]. The objective of jailbreaking attacks is
    to generate harmful content that violates the usage policy by designing malicious
    prompts [[40](#bib.bib40), [51](#bib.bib51)]. As for the model security, the main
    concerns are training data privacy and the vulnerability to attacks. Private data
    has a high possibility of being incorporated into large corpora used for LLMs
    training [[34](#bib.bib34)]. LLMs are also susceptible to threats from traditional
    model attacks(e.g., poisoning attacks [[70](#bib.bib70)], data extracting attacks [[18](#bib.bib18)],
    and adversarial examples [[48](#bib.bib48)]). Regarding the output end, the generated
    content may display harmful [[71](#bib.bib71)] and untruthful [[29](#bib.bib29)]
    information. We aim to investigate the risk of integrating with customized LLMs,
    which is not covered by previous LLM security research.
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor Attacks. The traditional backdoor attack [[37](#bib.bib37)] is a training
    time attack. It aims to implant a hidden backdoor into the target model by poisoning
    the training dataset [[30](#bib.bib30), [50](#bib.bib50), [17](#bib.bib17)] or
    controlling the training process [[62](#bib.bib62)]. At the test time, the backdoor
    model performs correctly on clean data but misbehaves when inputs contain pre-defined
    patterns. Due to its stealthiness, backdoor attacks have become a major security
    threat to real-world machine learning systems [[43](#bib.bib43), [66](#bib.bib66),
    [21](#bib.bib21), [14](#bib.bib14), [60](#bib.bib60), [49](#bib.bib49)]. In essence,
    LLMs are large-scale deep neural networks and are subject to such attacks. For
    instance, Wang et al. [[59](#bib.bib59)] modify the activation layers to inject
    backdoors into LLMs. Huang et al. [[26](#bib.bib26)] scatter multiple trigger
    keys in different prompt components to introduce backdoors into LLMs. Kandpal
    et al. [[33](#bib.bib33)] perform backdoor attacks during in-context learning
    by fine-tuning on poisoned datasets. Wang et al. [[58](#bib.bib58)] poison the
    instruction-tuning process to conduct backdoor attacks. Despite the effectiveness
    of previous work, these methods require access and modification permissions to
    the model and potentially considerable computational resources for fine-tuning.
    In this paper, we propose 3 different backdoor attacks against LLMs by implanting
    backdoor instructions into the prompt, without fine-tuning LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present the first instruction backdoor attacks against applications
    using customized LLMs. Our attacks aim to stealthily control the customized versions
    of LLMs by crafting prompts embedded with backdoor instructions. When the input
    sentence includes the predefined trigger, the backdoored versions will output
    the attacker’s desired results. Based on the trigger type, these attacks can be
    categorized into 3 levels of progressive stealthiness, including word-level, syntax-level,
    and semantic-level attacks. Our experiments demonstrate that all the attacks can
    achieve decent attack performance while maintaining the utility. Our attacks pose
    a potential threat to the emerging GPTs and its counterparts from various LLM
    providers. We hope that our work will inspire further research on the security
    of LLMs and alert users to pay attention to the potential risks when using customized
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://bard.google.com/chat](https://bard.google.com/chat).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] [https://openai.com/blog/introducing-gpts](https://openai.com/blog/introducing-gpts).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] [https://chatglm.cn/glms](https://chatglm.cn/glms).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] [https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification](https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] [https://huggingface.co/meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] [https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] [https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] [https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis](https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] [https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity](https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Tiago A. Almeida, José María Gómez Hidalgo, and Akebo Yamakami. Contributions
    to the study of SMS spam filtering: new collectio and results. In ACM Symposium
    on Document Engineering (DocEng). ACM, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng,
    Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,
    and et al. PaLM 2 Technical Report. CoRR abs/2305.10403, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
    Shmatikov. How To Backdoor Federated Learning. In International Conference on
    Artificial Intelligence and Statistics (AISTATS), pages 2938–2948\. JMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    Language Models are Few-Shot Learners. In Annual Conference on Neural Information
    Processing Systems (NeurIPS). NeurIPS, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Marc Brysbaert. How Many Words Do We Read Per Minute? A Review and Meta-analysis
    of Reading Rate. Journal of Memory and Language, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Nicholas Carlini and Andreas Terzis. Poisoning and Backdooring Contrastive
    Learning. In International Conference on Learning Representations (ICLR), 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel
    Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson,
    Alina Oprea, and Colin Raffel. Extracting Training Data from Large Language Models.
    In USENIX Security Symposium (USENIX Security), pages 2633–2650\. USENIX, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S.
    Yu, Qiang Yang, and Xing Xie. A Survey on Evaluation of Large Language Models.
    CoRR abs/2307.03109, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng,
    Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu
    Kong, Zujie Wen, Ke Xu, and Qi Li. Risk Taxonomy, Mitigation, and Assessment Benchmarks
    of Large Language Model Systems. CoRR abs/2401.05778, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A Backdoor Attack Against
    LSTM-Based Text Classification Systems. IEEE Access, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu
    Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated Jailbreak Across Multiple
    Large Language Model Chatbots. CoRR abs/2307.08715, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA:
    Efficient Finetuning of Quantized LLMs. CoRR abs/2305.14314, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Jochen Hartmann, Mark Heitmann, Christian Siebert, and Christina Schamp.
    More than a Feeling: Accuracy and Application of Sentiment Analysis. In International
    Journal of Research in Marketing (IJRM), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language
    Models. In International Conference on Learning Representations (ICLR), 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. Composite
    Backdoor Attacks Against Large Language Models. CoRR abs/2310.07676, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial
    Example Generation with Syntactically Controlled Paraphrase Networks. In Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT), pages 1875–1885\. ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar,
    Joshua D. Bocarsly, Andres M. Bran, Stefan Bringuier, L. Catherine Brinson, Kamal
    Choudhary, Defne Circi, Sam Cox, Wibe A. de Jong, Matthew L. Evans, Nicolas Gastellu,
    erome Genzling, María Victoria Gil, Ankur K. Gupta, Zhi Hong, Alishba Imran, Sabine
    Kruschwitz, Anne Labarre, Jakub Lála, Tao Liu, Steven Ma, Sauradeep Majumdar,
    Garrett W. Merz, Nicolas Moitessier, Elias Moubarak, Beatriz Mouriño, Brenden
    Pelkie, Michael Pieler, Mayk Caldas Ramos, Bojana Rankovic, Samuel G. Rodriques,
    Jacob N. Sanders, Philippe Schwaller, Marcus Schwarting, Jiale Shi, Berend Smit,
    Ben E. Smith, Joren Van Heck, Christoph Völker, Logan T. Ward, ean Warren, Benjamin
    Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Zia, Aristana Scourtas, K. J.
    Schmidt, Ian T. Foster, Andrew D. White, and Ben Blaiszik. 14 Examples of How
    LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language
    Model Hackathon. CoRR abs/2306.06283, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko
    Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in
    Natural Language Generation. ACM Computing Surveys, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. BadEncoder: Backdoor
    Attacks to Pre-trained Encoders in Self-Supervised Learning. In IEEE Symposium
    on Security and Privacy (S&P). IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, élio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El
    Sayed. Mistral 7B. CoRR abs/2310.06825, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
    Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian
    Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud,
    Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia
    Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas
    Wang, Timothée Lacroix, and William El Sayed. Mixtral of Experts. CoRR abs/2401.04088,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini.
    Backdoor Attacks for In-Context Learning with Language Models. CoRR abs/2307.14692,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon
    Oh. ProPILE: Probing Privacy Leakage in Large Language Models. CoRR abs/2307.01881,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for
    Large Language Model Serving with PagedAttention. CoRR abs/2309.06180, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Tianhao Li, Sandesh Shetty, Advaith Kamath, Ajay Jaiswal, Xianqian Jiang,
    Ying Ding, and Yejin Kim. CancerGPT: Few-shot Drug Pair Synergy Prediction using
    Large Pre-trained Language Models. CoRR abs/2304.10946, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor
    Learning: A Survey. CoRR abs/2007.08745, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, and Colin Raffel. Few-Shot Parameter-Efficient Fine-Tuning is Better and
    Cheaper than In-Context Learning. In Annual Conference on Neural Information Processing
    Systems (NeurIPS). NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long
    Contexts. CoRR abs/2307.03172, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang,
    Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking ChatGPT via Prompt Engineering:
    An Empirical Study. CoRR abs/2305.13860, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu,
    Elham Sakhaee, athaniel Li, Steven Basart, Bo Li, David A. Forsyth, and Dan Hendrycks.
    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust
    Refusal. CoRR abs/abs/2402.04249, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu
    Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent Advances
    in Natural Language Processing via Large Pre-trained Language Models: A Survey.
    ACM Computing Surveys, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Tuan Anh Nguyen and Anh Tuan Tran. WaNet - Imperceptible Warping-based
    Backdoor Attack. In International Conference on Learning Representations (ICLR),
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] OpenAI. GPT-4 Technical Report. CoRR abs/2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback. In Annual Conference on Neural
    Information Processing Systems (NeurIPS). NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Fábio Perez and Ian Ribeiro. Ignore Previous Prompt: Attack Techniques
    For Language Models. CoRR abs/2211.09527, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong
    Sun. ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. In
    Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
    9558–9566\. ACL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Yao Qiang, Xiangyu Zhou, and Dongxiao Zhu. Hijacking Large Language Models
    via Adversarial In-Context Learning. CoRR abs/2311.09948, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic
    Backdoor Attacks Against Machine Learning Models. In IEEE European Symposium on
    Security and Privacy (Euro S&P), pages 703–718\. IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi,
    Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor Pre-trained Models Can Transfer
    to All. In ACM SIGSAC Conference on Computer and Communications Security (CCS),
    pages 3141–3158\. ACM, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. Do
    Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large
    Language Models. CoRR abs/2308.03825, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. In ChatGPT We
    Trust? Measuring and Characterizing the Reliability of ChatGPT. CoRR abs/2304.08979,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.
    Manning, Andrew Y. Ng, and Christopher Potts. Recursive Deep Models for Semantic
    Compositionality Over a Sentiment Treebank. In Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 1631–1642\. ACL, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen Elliott,
    Carlos Gonzalez Zelaya, and Aad van Moorsel. The relationship between trust in
    AI and trustworthy machine learning technologies. In Conference on Fairness, Accountability,
    and Transparency (FAT*), pages 272–283\. ACM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation
    vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large
    Language Models. In Annual ACM Conference on Human Factors in Computing Systems
    (CHI). ACM, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning Language
    Models During Instruction Tuning. In International Conference on Machine Learning
    (ICML), pages 35413–35425\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Haoran Wang and Kai Shu. Backdoor Activation Attack: Attack Large Language
    Models using Activation Steering for Safety-Alignment. CoRR abs/2311.09433, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
    Agarwal, Jy yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the
    Tails: Yes, You Really Can Backdoor Federated Learning. In Annual Conference on
    Neural Information Processing Systems (NeurIPS). NeurIPS, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,
    Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Models
    with Self-Generated Instructions. In Annual Meeting of the Association for Computational
    Linguistics (ACL), pages 13484–13508\. ACL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Cheng’an Wei, Yeonjoon Lee, Kai Chen, Guozhu Meng, and Peizhuo Lv. Aliasing
    Backdoor Attacks on Pre-trained Models. In USENIX Security Symposium (USENIX Security),
    pages 2707–2724\. USENIX, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-Thought Prompting Elicits
    Reasoning in Large Language Models. In Annual Conference on Neural Information
    Processing Systems (NeurIPS). NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. A Systematic
    Evaluation of Large Language Models of Code. CoRR abs/2202.13169, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang.
    A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad,
    and the Ugly. CoRR abs/2312.02003, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. Latent Backdoor
    Attacks on Deep Neural Networks. In ACM SIGSAC Conference on Computer and Communications
    Security (CCS), pages 2041–2055\. ACM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe
    Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction
    Tuning for Large Language Models: A Survey. CoRR abs/2308.10792, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional
    Networks for Text Classification. In Annual Conference on Neural Information Processing
    Systems (NIPS), pages 649–657\. NIPS, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, and Jinming Wen. Universal Vulnerabilities
    in Large Language Models: In-context Learning Backdoor Attacks. CoRR abs/2401.05949,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Shuai Zhao, Jinming Wen, Anh Tuan Luu, Junbo Zhao, and Jie Fu. Prompt
    as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models.
    In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
    12303–12317\. ACL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Red teaming
    ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. CoRR abs/2301.12867,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and
    Transferable Adversarial Attacks on Aligned Language Models. CoRR abs/2307.15043,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1691cc5c1624824c9337aa4d95301e13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Example of a more complex backdoor task in GPTs. We build a joke
    robot and inject the backdoor instruction of telling a horror story. As shown
    in the preview, when our input contains the trigger cf, the joke bot tells a horror
    story instead of a joke. Note that this figure is for illustration purposes. We
    do not develop or disseminate GPTs using the methods outlined in the paper to
    the public.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cee664fd285b8b02ceeef5d39717c305.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Backdoor instruction (highlighted in red) embedded in a longer prompt
    contains 357 words.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a4ee06ed50550818c0717316077c9c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Backdoor instruction (highlighted in red) embedded in a longer prompt
    contains 1,084 words.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/646582c7fc46b3d3046b72c6364a7463.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The intent analysis generated by GPT-4\. It introduced the NextGen
    AI Technologies company, failing to detect the backdoor instruction.'
  prefs: []
  type: TYPE_NORMAL
