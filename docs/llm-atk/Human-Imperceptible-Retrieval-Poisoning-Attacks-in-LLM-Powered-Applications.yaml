- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17196](https://ar5iv.labs.arxiv.org/html/2404.17196)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quan Zhang [quanzh98@gmail.com](mailto:quanzh98@gmail.com) Tsinghua University
    ,  Binqi Zeng [224712188@csu.edu.cn](mailto:224712188@csu.edu.cn) Central South
    University ,  Chijin Zhou [tlock.chijin@gmail.com](mailto:tlock.chijin@gmail.com)
    Tsinghua University ,  Gwihwan Go [iejw1914@gmail.com](mailto:iejw1914@gmail.com)
    Tsinghua University ,  Heyuan Shi [hey.shi@foxmail.com](mailto:hey.shi@foxmail.com)
    Central South University  and  Yu Jiang [jiangyu198964@126.com](mailto:jiangyu198964@126.com)
    Tsinghua University
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Presently, with the assistance of advanced LLM application development frameworks,
    more and more LLM-powered applications can effortlessly augment the LLMs’ knowledge
    with external content using the retrieval augmented generation (RAG) technique.
    However, these frameworks’ designs do not have sufficient consideration of the
    risk of external content, thereby allowing attackers to undermine the applications
    developed with these frameworks. In this paper, we reveal a new threat to LLM-powered
    applications, termed retrieval poisoning, where attackers can guide the application
    to yield malicious responses during the RAG process. Specifically, through the
    analysis of LLM application frameworks, attackers can craft documents visually
    indistinguishable from benign ones. Despite the documents providing correct information,
    once they are used as reference sources for RAG, the application is misled into
    generating incorrect responses. Our preliminary experiments indicate that attackers
    can mislead LLMs with an 88.33% success rate, and achieve a 66.67% success rate
    in the real-world application, demonstrating the potential impact of retrieval
    poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models, Retrieval Poisoning Attack^†^†copyright: none^†^†booktitle:
    Proceedings of the 32nd ACM Symposium on the Foundations of Software Engineering
    (FSE ’24), November 15–19, 2024, Porto de Galinhas, Brazil'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have powered hundreds of applications in various
    natural language processing (NLP) domains (Izacard and Grave, [2020](#bib.bib8);
    Stahlberg, [2020](#bib.bib19)). Notably in the question-answering domain, LLM-powered
    applications like ChatGPT can be prompted with relevant content to generate valuable
    responses for users. Increasingly, many applications are adopting the Retrieval
    Augmented Generation (RAG) technique (Lewis et al., [2020](#bib.bib12)) to equip
    LLMs with external knowledge during their generative process. However, despite
    offering significant convenience, these applications are also subject to security
    threats. If compromised, they could potentially be manipulated to respond to user
    queries with harmful content, leading to severe consequences.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of existing research on LLM security primarily concentrates on
    the security of the LLMs themselves, often presuming that the attack surface exposed
    by LLM-powered applications solely originates from the LLMs. As a result, the
    primary focus tends to be on LLM-centric attacks such as jailbreak (Chao et al.,
    [2023](#bib.bib4); Huang et al., [2023](#bib.bib7)) and prompt injection (Abdelnabi
    et al., [2023](#bib.bib2); Liu et al., [2023a](#bib.bib15)), where attackers can
    craft malicious prompts to compromise the safeguard of LLMs. This enables them
    to steal sensitive information from other users or produce harmful content for
    other users. In contrast, limited research has been conducted on the security
    of the intersection among LLMs, applications, and external content. LLM-powered
    applications usually utilize external content to augment the knowledge base of
    the LLMs to generate more informed responses. This practice, while beneficial,
    also exposes additional attack surfaces to potential adversaries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a431c956e7ffbf8ae7837b8bbbac46ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Attack scenario of retrieval poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we unveil a new threat, retrieval poisoning, targeting LLM-powered
    applications, which exploits the design features of LLM application frameworks
    to perform imperceptible attacks during RAG. Additionally, we introduce the detailed
    approach of retrieval poisoning to inspire the potential defenses.
  prefs: []
  type: TYPE_NORMAL
- en: Attack Scenario. As depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications"),
    users unknowingly face a risk of exposure to malicious content. For example, when
    seeking guidance for installing ColossalAI, a user may request assistance from
    an LLM-powered application, providing relevant documents or links as referencing
    external content. The application then employs the RAG technique to retrieve the
    related information from the external content, and assemble an augmented request
    with the retrieved content and the original query of users. In normal, based on
    the augmented request, the application is supposed to provide an answer telling
    users the correct download link of ColossalAI, as shown by the upper part of Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Human-Imperceptible Retrieval Poisoning Attacks
    in LLM-Powered Applications"). However, as presented in Figure [1](#S1.F1 "Figure
    1 ‣ 1\. Introduction ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
    Applications")’s below part, users may unintentionally reference a document crafted
    by attackers since it is identical to the normal one in human perception. The
    crafted document contains an invisible attack sequence, which is designed to manipulate
    the LLM into generating the response with an incorrect download link, guiding
    the users to install a malicious program.
  prefs: []
  type: TYPE_NORMAL
- en: Approach. Retrieval poisoning fully leverages the RAG workflow, exhibiting a
    significant threat to the LLM ecosystem. Initially, attackers analyze and exploit
    the design features of LLM application frameworks, imperceptibly embedding attack
    sequences in external documents and ensuring a high likelihood of these sequences
    being retrieved and integrated into augmented requests. Moreover, a gradient-guided
    mutation technique, which adopts a weighted loss, is introduced to generate attack
    sequences with high effectiveness. Finally, by invisibly injecting the generated
    sequences at proper positions in benign documents, attackers can easily craft
    malicious documents. When released onto the Internet, these documents pose a threat
    to the applications dependent on external content.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary Experiment. To demonstrate the impact of retrieval poisoning, we
    construct a dataset comprising 30 documents and perform a preliminary experiment.
    Subsequently, we executed the attack on three powerful open-source LLMs with two
    temperature settings, achieving an average attack success rate (ASR) of 88.33%.
    In addition, experiment results also depict that the attack can maintain its effectiveness
    in various situations. Furthermore, a real-world experiment was conducted on a
    widely-used LLM-powered application developed with LangChain, where retrieval
    poisoning achieves 66.67% ASR. In conclusion, retrieval poisoning poses an extreme
    danger to current applications, necessitating the urgent development of more effective
    mitigation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the workflow to perform retrieval poisoning for
    real-world LLM-powered applications. Figure [2](#S2.F2 "Figure 2 ‣ 2.1\. Framework
    Analysis ‣ 2\. Methodology ‣ Human-Imperceptible Retrieval Poisoning Attacks in
    LLM-Powered Applications") shows the overall workflow. The goal of retrieval poisoning
    is to craft a malicious document, which is designed to manipulate the LLM into
    generating responses that align with the attacker’s intent while appearing identical
    to the original in human perception. This crafted document can then be used to
    poison the retrieval process of LLM-powered applications. To achieve this goal,
    retrieval poisoning consists of two main steps. The first step is to analyze LLM
    application frameworks’ critical components used for RAG in order to facilitate
    the invisible injection of the attack sequence generated in the next step. The
    second step is to generate the attack sequence and craft the malicious document
    with a gradient-guided token mutation technique.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Framework Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, LLM-powered applications are usually developed with LLM application
    frameworks (Liu et al., [2023c](#bib.bib14)), which provide many powerful components
    to support RAG. Therefore, we first introduce the workflow of RAG and then analyze
    the exploitable features of the components that can be leveraged by attackers
    to perform retrieval poisoning. In this paper, we focus on the most popular LLM
    application framework, LangChain. It has gained over 72,000 stars on GitHub after
    its release (LangChain-AI, [2023](#bib.bib11)) and has been adopted by many popular
    LLM-powered applications (kyrolabs, [2023](#bib.bib10); Chatchat-Space, [2023](#bib.bib5)).
    Thus, the attacks based on LangChain can affect a large number of users.
  prefs: []
  type: TYPE_NORMAL
- en: RAG Workflow. Before processing users’ requests with RAG, a retrieval database
    should be first constructed by users or developers of applications. Specifically,
    users and developers will collect the documents from the Internet. These documents’
    content, after being parsed by the document parsers, is split into chunks with
    appropriate lengths by text splitters. Finally, the retrieval database is constructed
    with vectors that are embedded from these chunks (Song et al., [2020](#bib.bib18);
    Okorie et al., [2011](#bib.bib17)). From the database, applications can retrieve
    relevant content, and then assemble the content and the original request into
    an augmented request following a prompt template. In the end, the augmented request
    is fed to LLMs to generate the response.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b3317c431a0c38c2b4ed900ba0a56ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Workflow of retrieval poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: Exploitable Features. In this process, the document parser, text splitter, and
    prompt template are three components that can be exploited by attackers. First,
    by analyzing the document parser, attackers can find features used for invisible
    injection in different document formats. The content on the Internet is usually
    in rich text formats, such as PDF, HTML, and Markdown, which require rendering
    before being shown to users. However, some content in the documents will not be
    rendered as visible but can be parsed by the document parsers. For example, in
    Markdown files, attackers may hide an attack sequence at the beginning of code
    blocks, as the listing shows below.
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,ICAgICAgICAgICAgYGBgYmFzaCBpbmplY3RlZF9zZXF1ZW5jZQogICAgICAgICAgICAgICBlY2hvICJiYXNoIHNjcmlwdCIKICAgICAgICAgICAgYGBg)“‘bash  injected_sequenceecho  ”bash  script”“‘'
  prefs: []
  type: TYPE_NORMAL
- en: The injected sequence will not be rendered visibly or influence the syntax highlighting
    of the code block, but it will be parsed by the document parser. As for PDF and
    HTML, many transparent elements can be leveraged to hide an extra sequence. Therefore,
    attackers can easily find invisible features to hide the attack sequences in benign
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Second, to ensure the attack sequence can be conveyed to the LLMs, attackers
    will also analyze the text splitters to ensure a proper injection position, so
    that the injected attack sequence can stay with the crucial information in the
    same chunk. In detail, the text is split based on the length and section of the
    content. Section-based splitters divide content according to tags that label different
    sections, which attackers can exploit to place their attack sequences within these
    delineated chunks. As for length-based splitters, they will split the content
    into fixed-length chunks with overlap (to keep context between chunks). Therefore,
    attackers may locate their attack sequence at an appropriate distance from crucial
    information, ensuring it remains undivided by length-based splitters.
  prefs: []
  type: TYPE_NORMAL
- en: Third, attackers can obtain the augmented request according to the frameworks’
    prompt templates to perform attack sequence generation. Prompt template can determine
    how the retrieved content is organized alongside the user’s request to form the
    augmented request. The template is crucial, as it impacts the overall performance
    of LLM-powered applications. Frameworks like LangChain offer a variety of prompt
    templates whose effectiveness has been validated, enabling application developers
    to either directly adopt them or customize their own templates based on these
    templates. Therefore, by utilizing the framework’s prompt templates, attackers
    can craft high-quality augmented requests to generate the attack sequence, as
    illustrated in Section [2.2](#S2.SS2 "2.2\. Document Crafting ‣ 2\. Methodology
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications").
    These attack sequences retain their effectiveness across a range of prompt templates
    used by developers in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Document Crafting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithm [1](#alg1 "In 2.2\. Document Crafting ‣ 2\. Methodology ‣ Human-Imperceptible
    Retrieval Poisoning Attacks in LLM-Powered Applications") illustrates how attackers
    can leverage the pre-analyzed features to generate the attack sequence and craft
    the malicious documents. The algorithm aims to modify an initial document to a
    crafted document  is built based on the retrieved content and the prompt template.  is
    the targeted LLM model that is used by LLM-powerful applications. In this paper,
    we focus on the open-source LLMs, which are widely adopted by existing applications (Touvron
    et al., [2023](#bib.bib20); Jiang et al., [2023](#bib.bib9)). We will extend our
    research to closed-source LLMs using transfer techniques in future works (Zou
    et al., [2023](#bib.bib22); Yuan et al., [2020](#bib.bib21)). In addition, the
    algorithm also needs the injection position  to craft a malicious document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input :  : Injection Position: Invisible Features: Crafted Document12++ 5      * then7            
    break;8      9      11      14'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Document Crafting
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm first needs to generate an attack sequence . , rather than being
    identical in its entirety. The algorithm performs an iterative mutation under
    the guidance of a weighted loss. As shown in Line 3, attackers will first combine
    the attack squeeze  at the injection position  to generate the response and examines
    whether the attack is successful (Line 4-6). If the further mutation is still
    required, then attackers can calculate a weighted loss (Line 7-8) following the
    equation below,
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Specifically, the loss is calculated by the cross entropy of the .  with respect
    to the  new sequences  as 32. Each new sequence is generated by randomly selecting
    one token in the  by calculating the loss of each sequence and selecting the one
    with a lower loss (Line 11). With  by hiding  with invisible features $features$
    (Line 12).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Preliminary Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct preliminary experiments to show the impact of retrieval
    poisoning attack on LLM-powered applications. First, we evaluate the attack success
    rate (ASR) of retrieval poisoning towards different LLMs and meanwhile evaluate
    that the attack sequence is effective under different augmented requests. To perform
    the attack, we construct a dataset with 30 documents, including software installation
    instructions and medication guides. The target LLMs for our attack are Llama2-7b,
    Llama2-13b, and Mistral-7b, which vary in parameter size and architecture, providing
    a comprehensive range of scenarios for our analysis. Furthermore, we also perform
    real-world attacks on ChatChat, a popular application powered by LangChain, demonstrating
    that attackers can effectively execute the retrieval poisoning in a manner that
    remains undetected by humans.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Evaluation on LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate that retrieval poisoning is easily performed, we first concentrate
    on attacks towards LLMs, on which we evaluate the ASR of generated attack sequences.
    In detail, we first evaluate retrieval poisoning on three different LLMs with
    two different temperature settings. Then, the attack sequences are evaluated on
    different augmented requests constructed based on different prompt templates.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Evaluation of retrieval poisoning on LLMs. “Iter” is the average iteration
    during the attack. “Seq”, “Req”, and “Res” show the average token length of the
    attack sequences, augmented requests, and output responses, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Temp | LLMs | ASR | Iter | Seq | Req | Res |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | Llama2-7b | 86.67% | 140.63 | 31.37 | 600.53 | 140.73 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b | 90.00% | 137.67 | 30.80 | 601.90 | 135.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7b | 86.67% | 141.60 | 30.23 | 583.43 | 128.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | Llama2-7b | 90.00% | 124.10 | 31.13 | 600.53 | 140.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b | 93.33% | 102.30 | 27.87 | 601.90 | 139.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7b | 83.30% | 168.83 | 30.77 | 583.43 | 130.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 88.33% | 135.86 | 30.36 | 595.29 | 135.93 |'
  prefs: []
  type: TYPE_TB
- en: As Table [1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary Experiments
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")
    shows, retrieval poisoning is very effective and achieves an 88.33% average ASR
    on all LLMs and settings. Among all LLMs, retrieval poisoning gains the highest
    ASR on Llama2-13b, despite having the most parameters. It may be because LLMs
    with fewer parameters are easily affected by attack sequences, and sometimes,
    the response becomes totally unrelated to the request, causing low attack efficiencies.
    Additionally, retrieval poisoning maintains high effectiveness, with an ASR above
    83.30%, across different temperature settings, indicating that temperature has
    a slight impact on the attack’s performance. Even using the attack sequence generated
    at a temperature setting of 0.7, retrieval poisoning still achieves an 86.67%
    ASR on LLMs with 1.0 temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Table [1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary
    Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")
    presents the average iteration steps, offering insights into the LLMs’ resistance
    to retrieval poisoning. The data indicate that Mistral-7b exhibits greater robustness,
    aligning with the ASR findings. Moreover, the table includes the average token
    length of the generated attack sequences and responses. An average sequence length
    of 30.36 suggests attackers can easily conceal these sequences within external
    content. The average lengths of requests and responses, at 595.29 and 135.93 tokens,
    respectively, imply that retrieval poisoning is typically employed in complex
    tasks. This contrasts with existing adversarial attacks, which often focus on
    text classification tasks where the LLMs’ output is limited to simple classifications
    like positive or negative. Please note that different LLMs adopt distinct tokenizers,
    which will encode the same inputs into different token sequences in various lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. ASR on different augmented requests.
  prefs: []
  type: TYPE_NORMAL
- en: '| LLMs | Llama2-7b | Llama2-13b | Mistral-7b |'
  prefs: []
  type: TYPE_TB
- en: '| ASR | 59.26% | 46.43% | 64.00% |'
  prefs: []
  type: TYPE_TB
- en: In reality, attack sequences should keep their effectiveness on different augmented
    requests, since prompt templates and queries differ for various developers and
    users. Therefore, we evaluate the generated attack sequence with different augmented
    requests. Because it is challenging to measure the replacement of queries objectively,
    we made significant modifications to the prompt template, constructing entirely
    different augmented requests for evaluation. In detail, the original prompt template
    is “¡Scenario Description¿ ¡Content¿ ¡Question¿”, presenting a QA scenario before
    the content and question. The new format, “¡Question¿ ¡Content¿”, directly poses
    a question to be answered from the provided content. The results show that 56.56%
    of successfully generated attack sequences are still effective on very different
    augmented requests, demonstrating that retrieval poisoning is not specified for
    one augmented request. This evaluation is operated with LLMs at a temperature
    setting of 1.0, where retrieval poisoning generates more attack sequences, as
    evidenced in Table [1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary
    Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications").
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Real-World Application Experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess retrieval poisoning’s impact in the real world, we perform the imperceptible
    attack on ChatChat (Chatchat-Space, [2023](#bib.bib5)), a popular LLM-powered
    application with over 21k stars on GitHub. We employ Mistral-7b as the LLM for
    ChatChat, since it is recognized as the most powerful 7b LLM (AI, [2023](#bib.bib3)).
    As outlined by Table [3](#S3.T3 "Table 3 ‣ 3.2\. Real-World Application Experiment
    ‣ 3\. Preliminary Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks
    in LLM-Powered Applications"), we utilized content in three commonly used formats:
    PDF, Markdown, and HTML. We collect more PDF files due to their well-structured
    and fine-grained content. While HTML files are prevalent online, they often include
    extraneous elements like website menus, adversely affecting application effectiveness.
    Hence, formats such as PDF are likely more preferred by users and developers for
    building retrieval databases.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Evaluation on real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: '| Formats | PDF | Markdown | HTML | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Count | 14 | 10 | 6 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Injection | 85.71% (12) | 90% (9) | 83.33% (5) | 86.67% (26) |'
  prefs: []
  type: TYPE_TB
- en: '| ASR | 71.43% (10) | 60.00% (6) | 66.67% (4) | 66.67% (20) |'
  prefs: []
  type: TYPE_TB
- en: On all documents, attackers can successfully perform the imperceptible attack
    sequence injection. Moreover, as Table [3](#S3.T3 "Table 3 ‣ 3.2\. Real-World
    Application Experiment ‣ 3\. Preliminary Experiments ‣ Human-Imperceptible Retrieval
    Poisoning Attacks in LLM-Powered Applications") shows, 86.67% of injected sequences
    are successfully conveyed to the LLMs after the complex RAG workflow. As for the
    other four cases, the injected sequences affect the applications’ retrieval engines,
    resulting in inaccuracies in retrieving relevant content. With attack sequences
    injected, ChatChat produces incorrect responses to users on 66.67% documents across
    three formats. Six attack sequences are conveyed to the LLMs but fail to achieve
    the retrieval poisoning. This can be attributed to the RAG workflow’s complexity.
    For instance, the application might incorrectly decode special texts within the
    attack sequence, leading to attack failures. The ASR is higher in PDF documents
    since the injection on PDF is more feasible. In contrast, more complex injection
    requirements of other formats slightly diminish the attack’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e4d904dc10ff55a7202724af32db1ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. A case of retrieval poisoning on ChatChat.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.2\. Real-World Application Experiment ‣ 3\.
    Preliminary Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
    Applications") presents a detailed example of an retrieval poisoning on ChatChat.
    As the sword icon pointed out, attackers hide an attack sequence at the beginning
    of the code block in Markdown format, which is invisible after rendering. When
    users request installation information for ColossalAI (Li et al., [2023](#bib.bib13)),
    ChatChat should, in normal, generate its response based on the external content
    and provide the correct link. However, through a meticulously crafted attack sequence
    and careful injection, attackers can manipulate the application, misleading users
    with an incorrect download link that potentially leads to the download of malicious
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Difference from Prompt Injection. Prompt injection can hijack LLM-powered applications
    to generate malicious content with harmful instructions. However, this method
    typically assumes the user has malicious intent, contrasting with the retrieval
    poisoning scenarios. Moreover, some researchers start to inject long malicious
    instructions through external content (Abdelnabi et al., [2023](#bib.bib2)). Different
    from these attacks, retrieval poisoning achieves a more imperceptible attack by
    analyzing the LLM application framework and can bypass advanced instruction filtering
    methods (Garg, [2023](#bib.bib6); Liu et al., [2023b](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: Potential Defenses. This paper is dedicated to heightening researchers’ awareness
    of the risks associated with retrieval poisoning and to inspiring the community
    to develop possible mitigation. One possible defense strategy is for applications
    to display the source content underlying their responses, allowing users to cross-reference
    the content with the response. However, this method might be less effective with
    complex content, as it could require users to invest much time in verification.
    Another approach involves using LLMs to rewrite content, thereby breaking the
    attack sequence. Nevertheless, it will introduce substantial computational resources
    and delays in application response times, influencing the efficiency of applications.
    Moreover, rewriting may also be affected by retrieval poisoning, incorrectly rewriting
    the crucial information. Therefore, the development of more efficient and effective
    defense mechanisms remains a critical need.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we expose a new threat to LLM-powered applications, named retrieval
    poisoning, where a benign document in human eyes can guide the LLMs to produce
    incorrect responses during RAG. In detail, attackers can exploit the LLM application
    framework to hide a malicious sequence in the external content, guiding the LLM-powered
    application to produce malicious responses. This work encourages the community
    to explore further into understanding the intricacies of LLM application frameworks,
    leading to more resilient and reliable LLM-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdelnabi et al. (2023) Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. 2023. Not What You’ve Signed Up For: Compromising
    Real-World LLM-Integrated Applications with Indirect Prompt Injection. In *Proceedings
    of the 16th ACM Workshop on Artificial Intelligence and Security* (Copenhagen,
    Denmark) *(AISec ’23)*. Association for Computing Machinery, New York, NY, USA,
    79–90. [https://doi.org/10.1145/3605764.3623985](https://doi.org/10.1145/3605764.3623985)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI (2023) Mistral AI. 2023. Mistral 7B. The best 7B model to date. [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J Pappas, and Eric Wong. 2023. Jailbreaking black box large language models
    in twenty queries. *arXiv preprint arXiv:2310.08419* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatchat-Space (2023) Chatchat-Space. 2023. ChatChat. [https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garg (2023) Vaibhav Garg. 2023. Mitigating Prompt Injection Attacks on an LLM
    based Customer support App. [https://vaibhavgarg1982.medium.com/mitigating-prompt-injection-attacks-on-an-llm-based-customer-support-app-b34298b2bc7a](https://vaibhavgarg1982.medium.com/mitigating-prompt-injection-attacks-on-an-llm-based-customer-support-app-b34298b2bc7a).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and
    Danqi Chen. 2023. Catastrophic jailbreak of open-source LLMs via exploiting generation.
    *arXiv preprint arXiv:2310.06987* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard and Grave (2020) Gautier Izacard and Edouard Grave. 2020. Leveraging
    passage retrieval with generative models for open domain question answering. *arXiv
    preprint arXiv:2007.01282* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kyrolabs (2023) kyrolabs. 2023. Awesome-LangChain. [https://github.com/kyrolabs/awesome-langchain](https://github.com/kyrolabs/awesome-langchain).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain-AI (2023) LangChain-AI. 2023. LangChain. [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems* 33 (2020), 9459–9474.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen
    Huang, Yuliang Liu, Boxiang Wang, and Yang You. 2023. Colossal-AI: A Unified Deep
    Learning System For Large-Scale Parallel Training. In *Proceedings of the 52nd
    International Conference on Parallel Processing* (Salt Lake City, UT, USA) *(ICPP
    ’23)*. Association for Computing Machinery, New York, NY, USA, 766–775. [https://doi.org/10.1145/3605573.3605613](https://doi.org/10.1145/3605573.3605613)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023c) Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang
    Dong, Peng Di, Wenhai Wang, and Dongxia Wang. 2023c. Prompting Frameworks for
    Large Language Models: A Survey. *arXiv preprint arXiv:2311.12785* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023a) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
    Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023a. Prompt Injection attack
    against LLM-integrated Applications. *arXiv preprint arXiv:2306.05499* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang
    Gong. 2023b. Prompt Injection Attacks and Defenses in LLM-Integrated Applications.
    *arXiv preprint arXiv:2310.12815* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Okorie et al. (2011) Patricia Nkem Okorie, F Ellis McKenzie, Olusegun George
    Ademowo, Moses Bockarie, and Louise Kelly-Hope. 2011. Nigeria Anopheles vector
    database: an overview of 100 years’ research. *Plos one* 6, 12 (2011), e28347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    2020. MPNet: Masked and Permuted Pre-training for Language Understanding. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stahlberg (2020) Felix Stahlberg. 2020. Neural machine translation: A review.
    *Journal of Artificial Intelligence Research* 69 (2020), 343–418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2020) Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2020. On the Transferability of Adversarial Attacksagainst Neural
    Text Classifier. *arXiv preprint arXiv:2011.08558* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    2023. Universal and transferable adversarial attacks on aligned language models.
    *arXiv preprint arXiv:2307.15043* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
