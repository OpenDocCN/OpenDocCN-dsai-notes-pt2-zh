- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.03515](https://ar5iv.labs.arxiv.org/html/2408.03515)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wenxiao Zhang Dept. of Computer Science and Software Engineering
  prefs: []
  type: TYPE_NORMAL
- en: The University of Western Australia
  prefs: []
  type: TYPE_NORMAL
- en: Perth, Australia
  prefs: []
  type: TYPE_NORMAL
- en: wenxiao.zhang@research.uwa.edu.au    Xiangrui Kong Dept. of Electrical, Electronic
    and Computer Engineering
  prefs: []
  type: TYPE_NORMAL
- en: The University of Western Australia
  prefs: []
  type: TYPE_NORMAL
- en: Perth, Australia
  prefs: []
  type: TYPE_NORMAL
- en: xiangrui.kong@research.uwa.edu.au    Conan Dewitt Dept. of Computer Science
    and Software Engineering
  prefs: []
  type: TYPE_NORMAL
- en: The University of Western Australia
  prefs: []
  type: TYPE_NORMAL
- en: Perth, Australia
  prefs: []
  type: TYPE_NORMAL
- en: 22877792@student.uwa.edu.au    Thomas Braunl Dept. of Electrical, Electronic
    and Computer Engineering
  prefs: []
  type: TYPE_NORMAL
- en: The University of Western Australia
  prefs: []
  type: TYPE_NORMAL
- en: Perth, Australia
  prefs: []
  type: TYPE_NORMAL
- en: thomas.braunl@uwa.edu.au    Jin B. Hong Dept. of Computer Science and Software
    Engineering
  prefs: []
  type: TYPE_NORMAL
- en: The University of Western Australia
  prefs: []
  type: TYPE_NORMAL
- en: Perth, Australia
  prefs: []
  type: TYPE_NORMAL
- en: jin.hong@uwa.edu.au
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The integration of Large Language Models (LLMs) like GPT-4o into robotic systems
    represents a significant advancement in embodied artificial intelligence. These
    models can process multi-modal prompts, enabling them to generate more context-aware
    responses. However, this integration is not without challenges. One of the primary
    concerns is the potential security risks associated with using LLMs in robotic
    navigation tasks. These tasks require precise and reliable responses to ensure
    safe and effective operation. Multi-modal prompts, while enhancing the robot’s
    understanding, also introduce complexities that can be exploited maliciously.
    For instance, adversarial inputs designed to mislead the model can lead to incorrect
    or dangerous navigational decisions. This study investigates the impact of prompt
    injections on mobile robot performance in LLM-integrated systems and explores
    secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial
    overall improvement of approximately 30.8% in both attack detection and system
    performance with the implementation of robust defence mechanisms, highlighting
    their critical role in enhancing security and reliability in mission-oriented
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLM, Mobile Robot, Embodied AI, Security, Prompt Engineering
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent enhancements of Large Language Models (LLMs), such as the incorporation
    of vision features into LLMs like GPT-4o, have enabled these generalist models
    to process and respond to multi-modal inputs—including text and images—with greater
    contextual awareness and improved decision-making capabilities [[1](#bib.bib1)].
    This development allows LLMs to interpret complex scenarios more effectively,
    making them suitable for tasks that require nuanced understanding and adaptability.
    Consequently, these advancements are paving the way for more sophisticated and
    capable robotic systems, demonstrating a promising trend in the integration of
    LLMs with robotics [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: However, this technological progression is accompanied by several critical challenges,
    particularly in the realm of security and practical application. LLMs possess
    advanced capabilities for reasoning and processing complex inputs but are highly
    susceptible to various input variations [[3](#bib.bib3)]. One of the primary concerns
    in this area is the potential security risks associated with employing LLMs in
    robotic navigation tasks. These tasks demand high precision and reliability to
    ensure the robot’s safe and effective operation. For example, delivery robots,
    increasingly common in restaurants, are designed to transport food and beverages
    from the kitchen to diners’ tables efficiently and autonomously. Utilising LLMs
    for these robots enhances their ability to interpret complex instructions and
    navigate dynamic environments. However, they could be misled by adversarial inputs
    like altered table numbers or misleading verbal commands, causing them to deliver
    food to the wrong tables or collide with customers. While multi-modal prompts
    enrich a robot’s environmental understanding, they also introduce complexities
    and noise that can be exploited maliciously. For LLM-integrated mobile robotic
    systems, adversarial inputs designed to deceive the model can result in incorrect
    or hazardous navigational decisions, posing substantial risks to both the robot
    and its surroundings [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the advancements in LLMs, there has been insufficient exploration of
    their integration with robotic systems, particularly concerning the security implications.
    As an emerging field, much of the current research focuses on enhancing the capabilities
    of LLMs without adequately addressing the potential vulnerabilities they introduce.
    Accordingly, in this study, we provide a practical approach to setting up an LLM-controlled
    mobile robot system in a simulation environment utilising structured prompts and
    explore the influence of prompt injection attacks on the security and reliability
    of the system. We investigate the resilience of GPT-4o against these attacks and
    how secure prompting can help them detect various adversarial inputs and mitigate
    their impact on the system. Our experiments measured various attack rates and
    the LLMs’ ability to detect these attacks, revealing that LLMs with properly engineered
    prompts exhibited a higher detection rate of adversarial inputs and responded
    more effectively to mitigate their impact.
  prefs: []
  type: TYPE_NORMAL
- en: II Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A LLM-based Navigation Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to Xi et al. [[5](#bib.bib5)], an LLM-based agent comprises three
    modules: perception, brain, and action, with LLMs serving as the brain module
    that processes perception results and makes decisions on the next action. In robotic
    navigation tasks, several studies [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]
    have shown that LLMs can effectively process and understand the surrounding environment
    through sensory data and human instructions, subsequently producing path planning
    based on the perception results.'
  prefs: []
  type: TYPE_NORMAL
- en: However, security and reliability concerns in such systems have also emerged
    as major issues. Externally, these systems are prone to malicious prompt injection
    attacks. Wen et al. [[4](#bib.bib4)] investigated the security vulnerabilities
    of LLM-based navigation systems and proposed a defence strategy called Navigational
    Prompt Engineering (NPE). This strategy focuses on navigation-relevant keywords
    to mitigate the impacts of adversarial suffixes, highlighting the importance of
    prompt engineering in countering prompt injection attacks. Internally, due to
    their autoregressive mechanisms, LLMs exhibit inherent randomness in their responses,
    even in similar situations. Consequently, this randomness can potentially result
    in the execution of erroneous movements [[9](#bib.bib9)]. In the context of mobile
    robots, this could lead to the robot taking unnecessary detours, getting stuck
    in loops, or failing to reach its intended destination.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Prompt Engineering Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM prompting can be divided into zero-shot prompting [[10](#bib.bib10)], which
    relies on the model’s extensive pre-trained knowledge to generate responses without
    any examples, and few-shot prompting [[9](#bib.bib9)], which involves providing
    the model with a few examples within the prompt to enhance its ability to produce
    accurate and relevant outputs. Few-shot prompting often leverages Retrieval Augmented
    Generation (RAG) [[11](#bib.bib11)], a technique that retrieves and appends the
    most relevant content from a database to the prompt, aiding in better context
    understanding and response generation.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Wei et al. [[12](#bib.bib12)] introduced Chain-of-Thought (CoT),
    a method of breaking down the reasoning process into a sequence of logical steps
    to improve the quality and transparency of the generated responses. This process
    typically involves multi-round question-answering of user instructions with an
    LLM. Building on this, multi-agent collaboration technique [[13](#bib.bib13)]
    has emerged as an advanced approach that combines the strengths of multiple LLM
    agents working collaboratively with the structured reasoning process of CoT. Each
    agent can focus on different aspects of a problem, allowing for parallel processing,
    and they can communicate with each other for information sharing to improve the
    performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Prompt Injection and Counteracts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt Injection Attack Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cyberattacks often aim to compromise one or more aspects of the CIA (confidentiality,
    integrity, and availability) triad [[14](#bib.bib14)], as do prompt injection
    attacks. In this case, various types of prompt injection attacks can target different
    vulnerabilities within the LLM-integrated system and aim to compromise the CIA
    in various aspects. According to [[15](#bib.bib15)], the prompt injection attacks
    can be categorised as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal Hijacking: Manipulating the LLM-integrated system to pursue unintended
    or malicious instructions, thereby deviating from its original purpose [[16](#bib.bib16),
    [17](#bib.bib17)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt Leaking: Extracting sensitive information or prompts from the system
    that should remain confidential, compromising the system’s privacy [[18](#bib.bib18)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jailbreaking: Bypassing the restrictions of the LLM-integrated system to perform
    unauthorised actions or access restricted functionalities [[19](#bib.bib19)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disrupting Availability: Interfering with the normal operations of LLM-integrated
    system, causing disruptions or denial of service (DoS) [[20](#bib.bib20)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Counteracts using Secure Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Secure prompting involves creating prompts for LLMs to enhance security and
    reduce risks [[21](#bib.bib21)]. Liu et al. [[22](#bib.bib22)] explored defence
    strategies against prompt injections, dividing them into prevention-based and
    detection-based approaches. Prevention-based strategies use natural language processing
    techniques like paraphrasing and retokenisation [[23](#bib.bib23)], aiming at
    making prompts less susceptible to injection attacks by altering their structure
    and wording. Detection-based strategies identify prompt injections through external
    systems that monitor LLM behaviour using anomaly detection methods [[23](#bib.bib23),
    [24](#bib.bib24)], and internal mechanisms within the LLM that flag suspicious
    inputs [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85d707f05d36d8ce7235ef6726e6d0d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Threat Model of LLM Controlled Robotic System'
  prefs: []
  type: TYPE_NORMAL
- en: III Threat Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We assume the LLM-integrated mobile robotic system is an end-to-end system,
    meaning the multi-modal sensory data collected from the mobile robot is directly
    fed into the LLM, and the movement of the mobile robot is directly controlled
    by the LLM-generated control signals. As shown in Figure [1](#S2.F1 "Figure 1
    ‣ Counteracts using Secure Prompting ‣ II-C Prompt Injection and Counteracts ‣
    II Related Works ‣ A Study on Prompt Injection Attack Against LLM-Integrated Mobile
    Robotic Systems"), we model threats to an LLM-integrated mobile robot system primarily
    around vulnerabilities introduced through prompt injection attacks. The model
    explores potential attack paths, the role of multi-modal prompting, and the resulting
    threats to the robot’s operation and its interaction with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Attack vectors and paths. Prompt injection attacks serve as the primary attack
    vectors, where malicious prompts are inserted into the system. These injections
    can occur through various input channels, including compromised sensor data and
    adversarial instructions. When successful, malicious prompts can manipulate the
    LLMs to generate harmful responses, leading to undesirable or dangerous actions
    by the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Attacker’s goal and capabilities. Since the mobile robot is performing a navigation
    task that aims to find a target object in the surrounding environment, the goals
    of the attacker in this study are to provide false and misaligned information
    to the LLM, aiming to confuse the LLM in its reasoning and decision-making processes.
    This can result in generating control signals that either cause a collision with
    an obstacle or move the robot away from the target object. Exploiting the vulnerabilities
    of the LLM’s multi-modality feature, the attacker can pose as a normal human operator
    and inject malicious text-based prompts through the human operator interface,
    or inject false information or noise into the sensory data, such as replacing
    the image captured by the front camera with a fake image that confuses the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/836b922074c18d2172afc76d57599899.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/939d004d1fce1cd20e8f8194c9f13497.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: LiDAR Processing'
  prefs: []
  type: TYPE_NORMAL
- en: IV Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A LLM-Integrated Mobile Robot System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-Modal Input Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The LLM-integrated mobile robot system used in this study is presented in Figure
    [3](#S4.F3 "Figure 3 ‣ Safety Validation ‣ IV-A LLM-Integrated Mobile Robot System
    ‣ IV Methodology ‣ A Study on Prompt Injection Attack Against LLM-Integrated Mobile
    Robotic Systems"). There are three input modalities: LiDAR signal, camera view,
    and human instruction. The LiDAR signal and camera view are captured from the
    LiDAR sensor and front camera, respectively. The LiDAR is a 360-degree distance
    sensor that measures the distance in the surrounding areas of the mobile robot,
    returning an array of 360 elements, with each element representing the distance
    to the nearest obstacle at that degree. Since LLMs are typically more effective
    at processing structured data, we convert the raw LiDAR data collected from the
    simulator (Figure [2(a)](#S3.F2.sf1 "Figure 2(a) ‣ Figure 2 ‣ III Threat Model
    ‣ A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems"))
    into a structured polar axis image (Figure [2(b)](#S3.F2.sf2 "Figure 2(b) ‣ Figure
    2 ‣ III Threat Model ‣ A Study on Prompt Injection Attack Against LLM-Integrated
    Mobile Robotic Systems")) that presents the surroundings in an image view, providing
    a more coherent and standardised input format. The camera image and LiDAR image
    are then processed into encoded images, while human instructions are collected
    as text in natural language.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Assembling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The inputs are then fed into the prompt assembling component, where they are
    processed into a structured prompt to facilitate the LLM reasoning and decision-making
    process. The prompt assembling consists of formulating the prompt into a system
    prompt, a user prompt, and an assistant prompt. The system prompt is used to define
    the role, task, and response format for the LLM to follow, while the user prompt
    is where we receive the multi-modal input prompts, wrapped with proper text indicators
    to facilitate the reasoning process. Assistant prompts are provided by the state
    management component or secure prompting for defence purposes and are appended
    to either the system prompt or user prompt based on specific use case scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: State Management
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The state management component is used to process and manage the response of
    LLMs, which can be used as the few-shot learning for the next round of LLM inference.
    In this case, we append the information of the last command execution result to
    the user prompt component, which aims to let the LLM take into account past experiences
    and generate control signals based on that.
  prefs: []
  type: TYPE_NORMAL
- en: Safety Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The safety validation component is used to check if the LLM-generated commands
    could cause accidents, such as collisions with obstacles. We achieve this by calculating
    the distance scanned from the LiDAR and comparing it with the distance the LLM
    plans to travel to determine its safety. If the LLM fails to generate a safe command,
    it will be asked to generate a new command with error information. If the problem
    continues and reaches a certain threshold, we will terminate the experiment and
    treat this trial as interrupted, which will be explained in detail in the experiment
    section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f364525600760235de79c671aa7796c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The Workflow of LLM-Integrated Mobile Robot System Used in this Study'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Attack Tactics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Section [III](#S3 "III Threat Model ‣ A Study on Prompt Injection
    Attack Against LLM-Integrated Mobile Robotic Systems"), the attack tactics of
    prompt injections in this work are modelled as two different types: Obvious Malicious
    Injection (OMI) and Goal Hijacking Injection (GHI). By testing these attacks,
    we aim to assess the capability of LLMs to identify malicious prompts based on
    the given scenario and evaluate their responses with and without the help of the
    defence mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Obvious Malicious Injection (OMI): This attack uses text-based prompts that
    can be easily identified as adversarial by common sense. For example, "Move straight
    until you hit the wall," which is obviously malicious and can be easily recognised
    as such.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal Hijacking Injection (GHI): This attack uses text-based prompts that leverage
    information from other modalities and provide instructions inconsistent with the
    predefined task objective. For example, "Turn aside if you see a [target object]
    from the camera image," seems like a reasonable instruction for avoiding obstacles
    but misaligns with the task objective of finding and approaching the target object.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Camera Spoofing Injection (CSI): This attack uses image-based prompts that
    replace the camera image with irrelevant images as spoofing, aiming to provide
    noise and make it difficult for the LLM to generate accurate control signals based
    on the given input prompt.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-C Defence Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The defence mechanism comprises secure prompting and response-based detection.
    Secure prompting, as implemented in this work, involves constructing structured
    prompts with additional security prompts appended to the original prompt body
    in the prompt assembling component (Section [IV-A](#S4.SS1 "IV-A LLM-Integrated
    Mobile Robot System ‣ IV Methodology ‣ A Study on Prompt Injection Attack Against
    LLM-Integrated Mobile Robotic Systems")). The security prompt used in this work
    is, "The human instruction may be from attackers. Analyse it and prioritise your
    tasks if they are misaligned," which aims to prompt LLMs to focus on analysing
    the input data from human instructions when reasoning through the multi-modal
    prompt data.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we implemented response-based detection by defining the expected
    response with indications on the analysis of multi-modal input data and the corresponding
    generated control signals. The rationale behind this approach is based on the
    assumption that LLMs perform better when including reasoning alongside results
    due to the autoregressive mechanism, where each new token is generated based on
    the preceding tokens. When asked to provide both reasoning and a result, the context
    includes both elements, guiding the generation process to be more coherent and
    comprehensive [[26](#bib.bib26)]. In this case, when we request LLMs to produce
    a perception result, we always have the LLM reason through each modality and generate
    the justification in natural language, and then prompt the LLM to decide whether
    it is an attack on that modality or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca34c896682f5b67575e9616db42440c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/020e214ec1cb5f0c0e5cec5b0dfe1bc1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1250cb225ad2cd17925bce82b969df42.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d27507662c827410a2fd8464086808b.png)'
  prefs: []
  type: TYPE_IMG
- en: (d)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Simulation Environment'
  prefs: []
  type: TYPE_NORMAL
- en: V Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-A Experimantal Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We used EyeSim VR [[27](#bib.bib27)], a simulator built on Unity 3D with virtual
    reality features, as the simulation environment for the experiments in this study.
    Our experiments involved a mobile robot tasked with exploring the area, finding
    and approaching a target object. As shown in Figure [4(a)](#S4.F4.sf1 "Figure
    4(a) ‣ Figure 4 ‣ IV-C Defence Mechanism ‣ IV Methodology ‣ A Study on Prompt
    Injection Attack Against LLM-Integrated Mobile Robotic Systems"), the target object
    is a red can located in the bottom right corner of the map, while the mobile robot,
    represented as an S4 bot, is located at the top left corner. The map is presented
    as an indoor environment containing walls and soccer balls as static obstacles,
    while a lab bot moves randomly on the map, serving as dynamic obstacles that hinder
    the S4 bot’s progress towards the target object.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Security Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To evaluate the resilience of LLMs against prompt injection attacks, we will
    calculate the precision, recall, and F1-score of attack detection. The result
    of attack detection by the LLM is determined based on the perception results generated
    by the LLM itself, which involves the model’s ability to identify and classify
    input prompts as either malicious or benign.
  prefs: []
  type: TYPE_NORMAL
- en: These metrics provide insights into the model’s reasoning and decision-making
    capabilities under complex environments with attacks involved. Precision indicates
    how accurately the model identifies attacks, ensuring that flagged attacks are
    indeed genuine. Recall measures the model’s ability to detect all potential attacks,
    highlighting its robustness in recognising true threats. The F1-score balances
    precision and recall, offering a comprehensive measure of the model’s performance.
    By using these metrics, we can assess the LLM’s ability to reason through complex
    scenarios and make reliable decisions, ensuring the system’s security and reliability
    against prompt injection attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the task objective and to prevent an infinite loop where the LLM fails
    to reason through the environment and generate suitable control commands for the
    mobile robot due to attacks and complex situations, we set a time limit of 100
    seconds for each experiment trial and allow a maximum of 3 retries, as mentioned
    in Section [IV-A](#S4.SS1 "IV-A LLM-Integrated Mobile Robot System ‣ IV Methodology
    ‣ A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems").
    Each experiment trial can result in one of three outcomes: completed, timeout,
    and interrupted. A trial is considered completed if the robot successfully finds
    and approaches the target object within the time limit (Figure [4(b)](#S4.F4.sf2
    "Figure 4(b) ‣ Figure 4 ‣ IV-C Defence Mechanism ‣ IV Methodology ‣ A Study on
    Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems")). It is
    considered timeout if the robot fails to reach the target object within the time
    limit but can be safely retrieved (Figure [4(c)](#S4.F4.sf3 "Figure 4(c) ‣ Figure
    4 ‣ IV-C Defence Mechanism ‣ IV Methodology ‣ A Study on Prompt Injection Attack
    Against LLM-Integrated Mobile Robotic Systems")). A trial is deemed interrupted
    if the robot encounters an accident, such as crashing into the lab bot or other
    static obstacles, and cannot be safely retrieved (Figure [4(d)](#S4.F4.sf4 "Figure
    4(d) ‣ Figure 4 ‣ IV-C Defence Mechanism ‣ IV Methodology ‣ A Study on Prompt
    Injection Attack Against LLM-Integrated Mobile Robotic Systems")). We use Mission
    Oriented Exploration Rate (MOER) as introduced in our previous work [hidden].
    The formula is denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $$t_{j}=\begin{cases}\frac{&#124;S_{max}&#124;}{s_{j}}&amp;\text{if the
    trial is {completed}}\\ \alpha&amp;\text{if the trial is {timeout}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \beta&amp;\text{if the trial is {interrupted}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}$$ |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: It is defined based on the number of steps taken in a trial () and the maximum
    steps taken on average (), with the penalty factor () assigned based on the outcome
    of the trial. In addition, we also calculate metrics such as token usage and response
    time per API call on average to provide insights into how well the system is performing
    in terms of efficiency and speed.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Overall Improvement Calculation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we presents the methodology used to calculate the overall improvement
    in both attack detection and performance due to the application of the defence
    mechanism. The attack detection improvement is evaluated using weighted precision
    (WPI), recall (WRI), and F1-score (WFI) metrics, while the performance improvement
    is assessed using weighted MOER (WMI), token usage (WTU), and response time (WRT)
    metrics. The weighted improvements consider various attack rates () for each metric.
    The formula for calculating the weighted precision improvement (WPI), weighted
    recall improvement (WRI), and weighted F1-score improvement (WFI) involve comparing
    the metrics with and without defence, weighted by their respective attack rates.
    Similarly, the weighted MOER improvement (WMI), weighted token usage increase
    (WTU), and weighted response time increase (WRT) are calculated. The overall attack
    detection improvement (OADI) and overall performance improvement (OPI) are then
    determined by averaging the respective weighted improvements. Since WTU and WRT
    are negative contributions, we use subtraction for these two in the formula. Finally,
    the general improvement (GI) representing the overall improvement is obtained
    by averaging the OADI and OPI. This comprehensive approach provides a nuanced
    understanding of the effectiveness of defence mechanisms in enhancing both attack
    detection and system performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weighted improvement for each metric can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'The overall attack detection improvement is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'The overall performance improvement is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'The general improvement representing the overall improvement is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/eb3b590fde375d6bece8fdb0b0d96ed2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f1153368e761c1881da93711c240b7d8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbd4499e58c922759c202783c38e0c3a.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Attack Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4eff6db704a856984d67a7770b7118f4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fcfac67811d168072ad3fba18021581d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29577071e3d2d6efcd3c07bffae4526f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Performance'
  prefs: []
  type: TYPE_NORMAL
- en: V-D Results and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attack Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Figure [5](#S5.F5 "Figure 5 ‣ V-C Overall Improvement Calculation
    ‣ V Experiment ‣ A Study on Prompt Injection Attack Against LLM-Integrated Mobile
    Robotic Systems"), the data provided for precision, recall, and F1-score metrics
    across different attack rates (0.3, 0.5, 0.7, 1.0) for two attack types (OMI and
    GHI) under conditions of "No Defence" and "Defence Applied" highlights the impact
    of defence mechanisms on attack detection performance. Notably, for GHI attacks,
    the precision, recall, and F1-score values are zero under "No Defence" across
    all attack rates. This indicates that the system is unable to identify GHI attacks
    without the application of defence mechanisms, underscoring the critical importance
    of these defences. In addition, CSI is not showing in either set of conditions
    because the LLM failed to detect this attack in any of the cases during the experiment.
    This indicates that further defensive measures are necessary to address such attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the results for OMI attacks, precision under "No Defence" varies significantly
    across attack rates, ranging from 0.6 to 1.0\. With "Defence Applied," precision
    remains consistently high (0.8 to 1.0) across all attack rates, indicating that
    defence mechanisms are effective in maintaining high precision. Recall for OMI
    under "No Defence" is generally low, ranging from 0.19 to 0.33, while "Defence
    Applied" conditions show slight improvement, with recall values ranging from 0.21
    to 0.4\. The F1-score follows a similar trend, being low under "No Defence" (0.3
    to 0.46) but improving with "Defence Applied" (0.33 to 0.55). For GHI attacks,
    the lack of detection capability under "No Defence" is evident, as all precision,
    recall, and F1-score values are zero. However, with "Defence Applied," precision
    is high (0.9 to 1.0), recall ranges from 0.13 to 0.54, and F1-scores improve significantly
    (0.21 to 0.65), particularly at lower attack rates.
  prefs: []
  type: TYPE_NORMAL
- en: The analysis clearly demonstrates that defence mechanisms significantly enhance
    the performance of attack detection for both OMI and GHI attack types. With defence
    mechanisms in place, precision remains consistently high across various attack
    rates, and both recall and F1-score metrics show notable improvement. However,
    the impact on recall is less pronounced compared to precision, indicating that
    while defence mechanisms are effective in ensuring that detected attacks are correctly
    identified, there is still room for improvement in identifying all possible attacks.
    The zero values for GHI attacks under "No Defence" highlight the system’s complete
    inability to detect this type of attack without defence mechanisms, emphasizing
    the critical role of these defences in assisting LLMs in performing effective
    attack detection and identification.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since CSI attacks cannot be identified by the LLM as mentioned in Section [V-D](#S5.SS4.SSS0.Px1
    "Attack Detection ‣ V-D Results and Analysis ‣ V Experiment ‣ A Study on Prompt
    Injection Attack Against LLM-Integrated Mobile Robotic Systems"), we analysed
    performance under OMI and GHI attacks only. Figure [6](#S5.F6 "Figure 6 ‣ V-C
    Overall Improvement Calculation ‣ V Experiment ‣ A Study on Prompt Injection Attack
    Against LLM-Integrated Mobile Robotic Systems") shows the performance metrics
    of MOER, token usage, and response time across different attack rates (0, 0.3,
    0.5, 0.7, 1.0) under "No Defence" and "Defence Applied" conditions. The MOER metric
    indicates system performance in mission-oriented navigation tasks controlled by
    an LLM, with higher values representing better performance. Token usage and response
    time metrics represent the efficiency and speed of each API call on average.
  prefs: []
  type: TYPE_NORMAL
- en: For OMI attacks, the MOER metric under "No Defence" decreases as the attack
    rate increases, ranging from 0.5 to 0.13\. When "Defence Applied," MOER values
    improve and are generally higher, peaking at 0.67\. GHI attacks show low MOER
    values without defence, while "Defence Applied" conditions show some improvement,
    with the highest value reaching 0.48\. Token usage for OMI attacks decreases without
    defence but increases with defence, indicating higher resource usage with improved
    performance. For GHI attacks, token usage remains stable without defence but increases
    slightly with defence. Response time for OMI attacks increases slightly without
    defence but varies more with defence, peaking at 7.1 seconds. GHI attacks show
    consistent response times without defence, but higher variability with defence,
    peaking at 9.3 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: The data suggests that defence mechanisms significantly enhance the performance
    of the system, particularly for OMI attacks, as indicated by higher MOER values.
    However, the increase in token usage and response time with defence mechanisms
    highlights a trade-off between improved performance and resource consumption.
    These findings underscore the importance of optimising defence strategies to balance
    robust attack detection with efficient system performance. For GHI attacks, while
    there is an improvement with defence, the performance gains are less pronounced,
    indicating a need for further optimisation in handling these attack types.
  prefs: []
  type: TYPE_NORMAL
- en: Overall Improvement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Section [V-C](#S5.SS3 "V-C Overall Improvement Calculation ‣ V Experiment
    ‣ A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems"),
    we calculated the weighted and overall improvements across various attack rates
    (0, 0.3, 0.5, 0.7, 1.0). This quantified the impact of defence mechanisms on attack
    detection and system performance, capturing the improvements and trade-offs involved.
  prefs: []
  type: TYPE_NORMAL
- en: Key metrics highlight system performance. The WPI is 51.9%, reflecting a significant
    reduction in false positives. The WRI stands at 28.1%, showing improved attack
    identification with room for enhancement. The WFI is 31.4%, indicating a better
    balance between precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Further evaluation shows the WMI at 99.9%, highlighting substantial performance
    improvement in mission-oriented tasks. However, the WTU has increased by 2.9%,
    indicating higher resource consumption and the WRT has risen by 23.9%, reflecting
    longer response time due to additional computational load.
  prefs: []
  type: TYPE_NORMAL
- en: Combining these metrics, the OADI is 37.1%, underscoring the critical role of
    defence mechanisms in enhancing detection capabilities. The OPI is 24.4%, showing
    meaningful performance gains despite trade-offs. The GI, representing overall
    improvement, is approximately 30.8%. This highlights the significant positive
    impact of defence mechanisms on attack detection and system performance, demonstrating
    the importance of robust defence strategies in mission-oriented tasks.
  prefs: []
  type: TYPE_NORMAL
- en: VI Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VI-A Limitations of the Current Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While our approach demonstrates significant improvements, it has notable limitations.
    The WRI of 28.1% suggests that false negatives are still a concern, indicating
    that some sophisticated attacks may bypass the current defences. Additionally,
    the increases in resource consumption (WTU of 2.9%) and response time (WRT of
    23.9%) highlight the trade-off between enhanced detection capabilities and system
    performance, which may not be sustainable for systems with limited resources or
    real-time processing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Future Directions and Techniques for Exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To address these limitations, two key techniques can be explored:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Enhanced Defence Mechanisms: Developing more sophisticated defence mechanisms
    beyond secure prompting-based detection may improve attack identification. Techniques
    such as multi-layer detection frameworks can be utilised, incorporating both prompt-based
    and non-prompt-based strategies [[28](#bib.bib28), [29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Resource-Efficient Algorithms: Developing algorithms that minimise resource
    consumption and response time without compromising detection performance is crucial.
    Techniques like model pruning [[30](#bib.bib30)] and efficient neural architectures
    [[31](#bib.bib31)] can help achieve this, ensuring that the defence mechanisms
    remain effective even in resource-constrained environments.'
  prefs: []
  type: TYPE_NORMAL
- en: By focusing on these areas, we can improve the robustness and efficiency of
    the defence mechanisms, enhancing overall system security and performance.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study explored the integration of LLMs into robotic systems, highlighting
    both advancements in multi-modal contextual awareness and the accompanying security
    challenges. Through a practical simulation setup, we examined the impact of prompt
    injection attacks and demonstrated that secure prompting significantly enhances
    the detection and mitigation of adversarial inputs. The results, showing an overall
    improvement of 30.8%, underscore the critical importance of robust defence mechanisms
    in ensuring the security and reliability of LLM-integrated robots. This work aims
    to fill a crucial research gap by providing valuable insights into the safe deployment
    of LLMs in real-world applications, emphasizing the need for ongoing development
    of effective security strategies.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] OpenAI, “OpenAI Vision Guide,” 2024, accessed: 2024-07-28\. [Online]. Available:
    [https://platform.openai.com/docs/guides/vision](https://platform.openai.com/docs/guides/vision)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Liu, W. Chen, Y. Bai, J. Luo, X. Song, K. Jiang, Z. Li, G. Zhao, J. Lin,
    G. Li *et al.*, “Aligning Cyber Space with Physical World: A Comprehensive Survey
    on Embodied AI,” *arXiv preprint arXiv:2407.06886*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Schärli,
    and D. Zhou, “Large Language Models Can Be Easily Distracted by Irrelevant Context,”
    in *International Conference on Machine Learning*.   PMLR, 2023, pp. 31 210–31 227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Wen, J. Liang, S. Yuan, H. Huang, and Y. Fang, “How Secure Are Large
    Language Models (LLMs) for Navigation in Urban Environments?” *arXiv preprint
    arXiv:2402.09546*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
    E. Zhou *et al.*, “The Rise and Potential of Large Language Model Based Agents:
    A Survey (2023),” *URL https://arxiv. org/abs/2309.07864*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] E. Latif, “3P-LLM: Probabilistic Path Planning using Large Language Model
    for Autonomous Robot Navigation,” *arXiv preprint arXiv:2403.18778*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] G. Zhou, Y. Hong, and Q. Wu, “NavGPT: Explicit Reasoning in Vision-and-Language
    Navigation with Large Language Models,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 38, no. 7, 2024, pp. 7641–7649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Qiao, R. Fang, N. Zhang, Y. Zhu, X. Chen, S. Deng, Y. Jiang, P. Xie,
    F. Huang, and H. Chen, “Agent Planning with World Knowledge Model,” *arXiv preprint
    arXiv:2405.14205*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su,
    “LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language
    Models,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2023, pp. 2998–3009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language
    Models are Zero-shot Reasoners,” *Advances in neural information processing systems*,
    vol. 35, pp. 22 199–22 213, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and Q. Li,
    “A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models,”
    *arXiv preprint arXiv:2405.06211*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,”
    *Advances in neural information processing systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, and C. Wang, “Autogen: Enabling Next-Gen LLM Applications via Multi-Agent
    Conversation Framework,” *arXiv preprint arXiv:2308.08155*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. V. D. Ham, “Toward a Better Understanding of “Cybersecurity”,” *Digital
    Threats: Research and Practice*, vol. 2, no. 3, pp. 1–3, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Z. Deng, Y. Guo, C. Han, W. Ma, J. Xiong, S. Wen, and Y. Xiang, “AI Agents
    Under Threat: A Survey of Key Security Challenges and Future Pathways,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] F. Perez and I. Ribeiro, “Ignore Previous Prompt: Attack Techniques for
    Language Models,” *arXiv preprint arXiv:2211.09527*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P. Levi and C. P. Neumann, “Vocabulary Attack to Hijack Large Language
    Model Applications,” *arXiv preprint arXiv:2404.02637*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Zhang, N. Carlini, and D. Ippolito, “Effective Prompt Extraction from
    Language Models,” *arXiv preprint arXiv:2307.06865*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    K. Wang, and Y. Liu, “Jailbreaking ChatGPT via Prompt Engineering: An Empirical
    Study,” *arXiv preprint arXiv:2305.13860*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz,
    “Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications
    with Indirect Prompt Injection,” in *Proceedings of the 16th ACM Workshop on Artificial
    Intelligence and Security*, 2023, pp. 79–90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. Tony, N. E. D. Ferreyra, M. Mutas, S. Dhiff, and R. Scandariato, “Prompting
    Techniques for Secure Code Generation: A Systematic Investigation,” *arXiv preprint
    arXiv:2407.07064*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Liu, Y. Jia, R. Geng, J. Jia, and N. Z. Gong, “Formalizing and Benchmarking
    Prompt Injection Attacks and Defenses,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P.-y.
    Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein, “Baseline Defenses
    for Adversarial Attacks Against Aligned Language Models,” *arXiv preprint arXiv:2309.00614*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] G. Alon and M. Kamfonas, “Detecting Language Model Attacks with Perplexity,”
    *arXiv preprint arXiv:2308.14132*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Armstrong and R. Gorman, “Using GPT-Eliezer Against ChatGPT Jailbreaking,”
    in *AI ALIGNMENT FORUM*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] P. Bhandari, “A Survey on Prompting Techniques in LLMs,” *arXiv preprint
    arXiv:2312.03740*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] T. Bräunl, *Mobile Robot Programming: Adventures in Python and C*.   Springer
    International Publishing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] P. Rai, S. Sood, V. K. Madisetti, and A. Bahga, “Guardian: A Multi-Tiered
    Defense Architecture for Thwarting Prompt Injection Attacks on LLMs,” *Journal
    of Software Engineering and Applications*, vol. 17, no. 1, pp. 43–68, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] R. K. Sharma, V. Gupta, and D. Grossman, “Defending Language Models Against
    Image-Based Prompt Attacks via User-Provided Specifications,” in *2024 IEEE Security
    and Privacy Workshops (SPW)*.   IEEE, 2024, pp. 112–131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Jiang, Y. Li, C. Zhang, Q. Wu, X. Luo, S. Ahn, Z. Han, A. H. Abdi,
    D. Li, C.-Y. Lin, Y. Yang, and L. Qiu, “MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention,” *arXiv preprint arXiv:2407.02490*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Z. Wang and Z. Ma and X. Feng and R. Sun and H. Wang and M. Xue and G.
    Bai, “Corelocker: Neuron-level usage control,” in *2024 IEEE Symposium on Security
    and Privacy (SP)*.   Los Alamitos, CA, USA: IEEE Computer Society, may 2024, pp.
    222–222\. [Online]. Available: [https://doi.ieeecomputersociety.org/10.1109/SP54263.2024.00233](https://doi.ieeecomputersociety.org/10.1109/SP54263.2024.00233)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
