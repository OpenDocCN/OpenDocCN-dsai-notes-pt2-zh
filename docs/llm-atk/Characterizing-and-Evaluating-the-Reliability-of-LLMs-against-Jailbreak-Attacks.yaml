- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:22'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09326](https://ar5iv.labs.arxiv.org/html/2408.09326)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: Zhejiang University, Hangzhou, China ¹¹email: kxchen@zju.edu.cn
    ²²institutetext: Nanyang Technological University, Singapore ³³institutetext:
    Zhejiang University, Hangzhou, China Kexin Chen 11 [0009-0009-7462-8080](https://orcid.org/0009-0009-7462-8080
    "ORCID identifier")    Yi Liu 22 [1111-2222-3333-4444](https://orcid.org/1111-2222-3333-4444
    "ORCID identifier")    Dongxia Wang 1313 [2222–3333-4444-5555](https://orcid.org/2222--3333-4444-5555
    "ORCID identifier")    Jiaying Chen 33 [2222–3333-4444-5555](https://orcid.org/2222--3333-4444-5555
    "ORCID identifier")    Wenhai Wang 33 [2222–3333-4444-5555](https://orcid.org/2222--3333-4444-5555
    "ORCID identifier")'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have increasingly become pivotal in content generation
    with notable societal impact. These models hold the potential to generate content
    that could be deemed harmful. Efforts to mitigate this risk include implementing
    safeguards to ensure LLMs adhere to social ethics. However, despite such measures,
    the phenomenon of "jailbreaking" – where carefully crafted prompts elicit harmful
    responses from models – persists as a significant challenge. Recognizing the continuous
    threat posed by jailbreaking tactics and their repercussions for the trustworthy
    use of LLMs, a rigorous assessment of the models’ robustness against such attacks
    is essential. This study introduces an comprehensive evaluation framework and
    conducts an large-scale empirical experiment to address this need. We concentrate
    on 10 cutting-edge jailbreak strategies across three categories, 1525 questions
    from 61 specific harmful categories, and 13 popular LLMs. We adopt multi-dimensional
    metrics such as Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length,
    and Grammatical Errors to thoroughly assess the LLMs’ outputs under jailbreak.
    By normalizing and aggregating these metrics, we present a detailed reliability
    score for different LLMs, coupled with strategic recommendations to reduce their
    susceptibility to such vulnerabilities. Additionally, we explore the relationships
    among the models, attack strategies, and types of harmful content, as well as
    the correlations between the evaluation metrics, which proves the validity of
    our multifaceted evaluation framework. Our extensive experimental results demonstrate
    a lack of resilience among all tested LLMs against certain strategies, and highlight
    the need to concentrate on the reliability facets of LLMs. We believe our study
    can provide valuable insights into enhancing the security evaluation of LLMs against
    jailbreak within the domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLM Evaluation Framework Reliability Trustworthiness.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) such as ChatGPT [[54](#bib.bib54)], and LLaMA [[37](#bib.bib37)]
    are types of attention-based sequential models based on the transformer architecture [[67](#bib.bib67)],
    which have driven rapid advances in the performance and generality of artificial
    intelligence (AI) systems. They are benefiting lots of applications recently,
    ranging from AI chatbots [[1](#bib.bib1)], coding assistants [[29](#bib.bib29)],
    to AI agents [[33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the growing intelligence and popularity in plenty of downstream applications [[5](#bib.bib5)],
    LLMs have also raised concerns about their reliability and security [[35](#bib.bib35)].
    We focus on LLMs’ reliability under Jailbreak attacks in this work.
  prefs: []
  type: TYPE_NORMAL
- en: While getting more and more attention and applicaitons, LLMs have shown the
    vulnerabilities of generating incorrect or even harmful contents. For example,
    ChatGPT has been reported to generate biased or offensive statements reflecting
    the biases present in its training data [[58](#bib.bib58)]. Microsoft’s chatbot
    Tay, deployed on Twitter, faced swift suspension after just one day due to attacks
    that coerced it into expressing racist and hateful rhetoric [[41](#bib.bib41)].
    Additionally, current LLMs have demonstrated initial capabilities in writing malware [[11](#bib.bib11)],
    and even designing chemical and biological weapons [[31](#bib.bib31)]. Such vulnerabilities
    may lead to serious consequences sometimes such as suicide case [[4](#bib.bib4)],
    lawyer submitted fabricated cases as precedent to the court [[3](#bib.bib3)],
    leakage of private information [[2](#bib.bib2)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: Designers of LLMs want to align their models with human values, making them
    follow ethical and content guidelines via ways like Reinforcement Learning from
    Human Feedback(RLHF) [[55](#bib.bib55)]. However, the introduction of jailbreak
    attacks may lead LLMs to generate unintended and potentially harmful responses.
    Jailbreak attacks refer to the scenarios where a malicious user (an attacker)
    attempts to trick or manipulate a LLM to bypass their built-in safety, ethical,
    or operational guidelines, e.g., generating inappropriate content, disclosing
    sensitive information, or performing actions against programming constraints.
    An example can be seen in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3dcf72e0a9aa87308dc839dfee49b10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Examples of model outputs with and without jailbreak prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: There has been notable research aimed at investigating the ramifications of
    jailbreak attacks. Liu et al. [[49](#bib.bib49)] delve into the repercussions
    of jailbreak attacks, examining diverse mechanisms for jailbreak prompting and
    evaluating their impact on model security. Zou et al. [[86](#bib.bib86)] apply
    a white-box approach combined with adversarial attacks to create jailbreak prompts.
    Additionally, Deng et al. [[19](#bib.bib19)] investigate the use of LLMs to simulate
    jailbreak prompts within a black-box framework, shedding light on the potential
    risks posed by such attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the existence of various jailbreak attack strategies, to the best of
    our knowledge, it has not been answered in the literature how trustworthy or reliable
    the existing LLMs are against various jailbreak attacks. Shen et al. [[63](#bib.bib63)]
    assessed the reliability of ChatGPT in generic question-and-answer (QA) scenarios;
    however, their evaluation did not extend to scenarios involving multiple LLMs
    or jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we endeavor to provide an answer by introducing a comprehensive
    evaluation framework. Fig. [2](#S4.F2 "Figure 2 ‣ 4 Evaluation Framework ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks") plots the overview
    of our evaluation process. By systematically exploring how state-of-the-art LLMs
    fare when confronted with such jailbreak attacks, we aim to shed light on the
    trustworthiness of LLMs in the context of maintaining content security and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: This study contributes to the field in several important ways. We first rigorously
    constructed a refined dataset which serves as the foundation for our evaluation.
    Expanding upon the work of Wang et al. [[69](#bib.bib69)], we broadened the dataset
    to encompass a wider range of potentially harmful queries, comprising 1525 questions
    across 61 distinct harmful categories—a rich foundation for comprehensive assessments.
    Furthermore, we collect 10 state-of-the-art jailbreak attacks and outline them
    into three different categories. The reliability of LLMs against jailbreak attacks
    is then evaluated across various dimensions, including resistance to jailbreak,
    non-toxicity of responses, and overall quality. This multi-faceted evaluation
    enables us to generate a nuanced picture of LLM reliability.
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the responses of 13 popular LLMs, we discern distinct disparities
    in their resilience against jailbreak attacks. Notably, some models exhibit inherent
    resistance, while others succumb, highlighting discrepancies in alignment with
    ethical and content guidelines imparted during model training. Our framework aggregate
    various evaluation criteria into a comprehensive reliability score that acknowledges
    the importance of different metrics as determined by end-users. This, in turn,
    furnishes strategic insights for mitigating the susceptibility of LLMs to jailbreak
    vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, we make the following key contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comprehensive Evaluation Framework: We introduce a detailed evaluation framework
    that holistically assesses the reliability of LLMs across a wide range of jailbreak
    attack strategies. This framework considers various metrics, including Attack
    Success Rate (ASR), Toxicity, Fluency, Grammatical errors, and Token length, to
    provide a nuanced picture of model behavior under adversarial jailbreak.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Construction: We construct a three-level hierarchical dataset,encompassing
    a spectrum of risks ranging from mild to extreme. We collect at least 25 harmful
    queries for each instance of harm, culminating in a comprehensive dataset comprising
    1,525 queries. These fine-grained harm types highlight the various specific vulnerabilities
    that LLMs ought to address.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Dimensional Analysis: Unlike previous work that focused singularly on
    the success rate of attacks, our analysis includes multidimensional measures of
    content quality and safety. This enables us to evaluate the subtle changes in
    output quality and potential risks, accounting for factors that impact the practical
    usability of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extensive Experimentation: Based on our dataset and evaluation framework, We
    conduct thorough experiments to evaluate the reliability of 13 LLMs, encompassing
    both commercial LLMS (including GPT-4 and ChatGPT) and open-source LLMs (such
    as LLaMA-3, LLaMA-2, Vicuna, Baichuan-2 and Gemma). Our work stands as one of
    the first to perform a comparison of these models’ reliability under various sophisticated
    jailbreak attack scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we review the existing Jailbreak attack strategies and the
    existing works which also aim to evaluate LLMs against jailbreak attacks. We also
    briefly introduce some other issues except jailbreak attacks that impede the trustworthy
    use of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 LLM Jailbreak
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Currently, the majority of jailbreak attacks are accomplished through the creation
    of “jailbreak prompts”. There exist multiple strategies to identify or craft such
    prompts, including collecting them from real-world scenarios, manually creating
    them by guided rules, or using automatic generation [[85](#bib.bib85)]. Overall,
    they can be categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Manual Crafting: This category encompasses jailbreak prompts crafted manually,
    leveraging human creativity to circumvent model constraints. Strategies such as
    role-playing [[43](#bib.bib43)] and scenario crafting [[45](#bib.bib45)] are employed
    to induce models to disregard their built-in protocols. Additionally, some Strategies [[72](#bib.bib72)]
    exploit vulnerabilities in the model’s context learning [[12](#bib.bib12)] to
    yield to malicious instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Longtail Encoding: This category emphasizes models’ limited generalization
    to data not seen during security alignment [[70](#bib.bib70)]. However, due to
    their extensive pretraining, they can still understand intentions and generate
    unsafe content. These strategies [[81](#bib.bib81), [20](#bib.bib20), [50](#bib.bib50)]
    leverages rare or unique data formats. For example, MultiLingual [[20](#bib.bib20)]
    encodes inputs into low-resource languages to bypass security guardrail. CodeChameleon [[50](#bib.bib50)]
    encrypts inputs and embeds a decoding function in the prompt, thus circumventing
    security checks based on intent, all while maintaining task functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Refinement: This category utilizes automated methods to detect and exploit
    the susceptibilities of models. Strategies like GCG [[86](#bib.bib86)] harness
    model gradients for targeted vulnerability exploration. AutoDAN [[47](#bib.bib47)]
    adopts genetic algorithms for prompt evolution, while GPTFUZZER [[80](#bib.bib80)]
    and FuzzLLM [[77](#bib.bib77)] investigate variations of prompts for potential
    flaws. Additionally,the PAIR [[15](#bib.bib15)] iteratively refines prompts based
    on language model scores.'
  prefs: []
  type: TYPE_NORMAL
- en: The identified jailbreak attack strategies are elaborated in Table [1](#S2.T1
    "Table 1 ‣ 2.1 LLM Jailbreak ‣ 2 Related Work ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"), with specific emphasis on
    those selected for evaluation in our framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: This table catalogs all identified jailbreak attack strategies, marking
    the ones selected for our investigation with *.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Paper | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Manual Crafting | 78 Template [[49](#bib.bib49)] | Compilation of 78 distinct
    template types. |'
  prefs: []
  type: TYPE_TB
- en: '| Jailbroken [[70](#bib.bib70)]* | An exhaustive analysis covering 29 types
    of assault templates and combinations, including encoding techniques such as base64.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deepinception [[45](#bib.bib45)]* | Generation of wrapped scenarios to nudge
    models into responding to malevolent inquiries. |'
  prefs: []
  type: TYPE_TB
- en: '| ICA [[72](#bib.bib72)]* | Jailbreak and guard aligned language models with
    only few in-context demonstrations.Employing the ICL [[23](#bib.bib23)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Fuzzllm [[77](#bib.bib77)] | Integration of generative constraints and malevolent
    inquiries within specified templates. |'
  prefs: []
  type: TYPE_TB
- en: '| Segregation [[40](#bib.bib40)] | Segregation of sensitive lexicons into variables
    within templates. |'
  prefs: []
  type: TYPE_TB
- en: '| Longtail Encoding | Multilingual [[20](#bib.bib20)]* | Exploration of various
    combinations of low-resource languages to circumvent model alignment. |'
  prefs: []
  type: TYPE_TB
- en: '| Cipher [[81](#bib.bib81)]* | Use encryption and decryption policies to bypass
    security restrictions |'
  prefs: []
  type: TYPE_TB
- en: '| Codechamelon [[50](#bib.bib50)]* | encrypts inputs and embeds a decoding
    function in the prompt, bypassing intent-based security checks without hindering
    task execution. |'
  prefs: []
  type: TYPE_TB
- en: '| Low resource [[79](#bib.bib79)] | An investigation similar to Multilingual [[20](#bib.bib20)],
    identifying low-resource languages as effective for security circumvention. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt Refinement | GCG [[86](#bib.bib86)] | Optimization at the token level
    informed by gradient data. |'
  prefs: []
  type: TYPE_TB
- en: '| Jailbreaker [[19](#bib.bib19)] | Finetune of an LLM with RLHF to jailbreak
    target model. |'
  prefs: []
  type: TYPE_TB
- en: '| Schwinn et al. [[61](#bib.bib61)] | An approach parallel to GCG [[86](#bib.bib86)],
    but at the sentence level |'
  prefs: []
  type: TYPE_TB
- en: '| AutoDAN [[47](#bib.bib47)]* | Application of a fuzzing approach, with the
    fitness score derived from loss metrics. |'
  prefs: []
  type: TYPE_TB
- en: '| PAIR [[15](#bib.bib15)]* | Employing the Chain of Thought (COT)  [[71](#bib.bib71)]
    alongside Vicuna for generating prompts responsive to user feedback. |'
  prefs: []
  type: TYPE_TB
- en: '| TAP [[52](#bib.bib52)]* | An approach akin to PAIR [[15](#bib.bib15)], employing
    the concept of a Tree of Thought(TOT) [[78](#bib.bib78)]. |'
  prefs: []
  type: TYPE_TB
- en: '| PAP [[82](#bib.bib82)] | Persuasive adversarial prompts(PAP) , viewing LLMs
    as communicators and using natural language to persuade them into jailbreak. |'
  prefs: []
  type: TYPE_TB
- en: '| GPTFuzz [[80](#bib.bib80)] | A fuzzing method, through utilization of Monte
    Carlo tree search techniques to adjust fitness scores based on success rates.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Shah et al. [[62](#bib.bib62)] | Attack of a black-box model by leveraging
    a proxy model. |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[73](#bib.bib73)] | Crafting of evasion prompts through GPT-4,
    utilizing meticulously designed prompts to extract system prompts. |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Evaluating Reliability of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the pursuit of evaluating the reliability of LLMs, multiple studies have
    shed light on the jailbreaking vulnerability of LLMs and attempted to assess their
    reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Mazeika et al. [[51](#bib.bib51)] introduces a framework for evaluating the
    efficacy of automated red teaming methods [[10](#bib.bib10)]. Through their extensive
    evaluation, they reveal key insights into the effectiveness of various red teaming
    methods against LLMs and defenses. Chao et al. [[14](#bib.bib14)] presents a uniform
    benchmark for analyzing jailbreak attacks, encompassing an ethically aligned behavior
    dataset, and a definitive evaluation protocol. Zhou et al. [[85](#bib.bib85)]
    proposes an elaborate framework for designing and scrutinizing jailbreak attacks,
    and standardizes the process of generating jailbreak attacks. Shen et al. [[63](#bib.bib63)]
    performed a large-scale evaluation of ChatGPT’s reliability in the generic QA
    scenario [[18](#bib.bib18)], demonstrating variability in performance and vulnerability
    to adversarial attacks. Xu et al. [[75](#bib.bib75)] conducted a comprehensive
    evaluation of attack and defense strategies for LLMs, revealing the relative effectiveness
    of these strategies and how special tokens can impact the jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Our study distinguishes itself by evaluating the reliability of LLMs from the
    perspective of security and quality of their outputs, as opposed to merely focusing
    on the efficacy of jailbreak attack mechanisms. Moreover, we conduct evaluations
    on a variety of LLMs when subjected to jailbreak attacks, enabling a detailed
    comparative analysis and providing thorough insights into the reliability of LLMs
    from multiple dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Vulnerabilities of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Numerous vulnerabilities identified in LLMs fall into two principal categories:
    inherent issues and intended attacks [[56](#bib.bib56)]. Aside from jailbreak
    attacks, we briefly outline additional prevalent and significant vulnerabilities
    in LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Inherent vulnerabilities are intrinsic shortcomings of LLMs that are not easily
    rectified by the models themselves [[35](#bib.bib35)]. For example, factual errors
    occur when LLM outputs contradict established truths, a phenomenon some literature
    refer as hallucination [[5](#bib.bib5), [83](#bib.bib83), [44](#bib.bib44)]. Additionally,
    reasoning errors highlight the models’ limited capacity for logical and mathematical
    problem-solving, often resulting in inaccurate responses to calculative or deductive
    reasoning tasks [[5](#bib.bib5), [46](#bib.bib46), [28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, intended attacks are orchestrated by malicious attackers seeking
    to manipulate LLMs to achieve their objectives [[35](#bib.bib35)]. For example,
    prompt injection attacks overrides an LLM’s original prompt and directs it to
    follow malicious instructions, which may result in data exposures or enable social
    engineering schemes [[48](#bib.bib48), [7](#bib.bib7), [32](#bib.bib32), [34](#bib.bib34)].
    Conversely, training data poisoning is characterized by the deliberate contamination
    of an LLM’s training dataset with the aim of degrading its performance or instilling
    biases, which can subsequently distort predictions and conduct [[66](#bib.bib66),
    [17](#bib.bib17), [13](#bib.bib13), [68](#bib.bib68)].
  prefs: []
  type: TYPE_NORMAL
- en: In essence, it is the vulnerability landscape that makes evaluating the reliability
    of LLMs an essential task, ensuring their security and trustworthy use in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '3 Preliminaries: Jailbreaking LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of a jailbreak attack is to design input prompts in such a way that
    they can cause a LLM to generate texts that are harmful, toxic, or objectionable,
    bypassing its built-in security restrictions [[70](#bib.bib70)]. Below we formally
    define LLMs and jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: An LLM takes a sequence of tokens of an arbitrary length as input and returns
    a distribution on the next token in the sequence. Let  denote the set of all sequences
    of tokens of arbitrary length. Let  denote the set of probability distributions
    over .
  prefs: []
  type: TYPE_NORMAL
- en: 'An LLM can be defined as a mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: In the context of jailbreaking, adversaries gain access to a LLM as an attack
    target. Additionally, adversaries possess a harmful query , which instructs the
    target LLM to produce objectionable content, such as “Tell me how to build a bomb?”
    or “How to create and distribute malware for financial gain?”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversaries typically create a jailbreak prompt  based on the harmful query
    . They then present the jailbreak prompt  to the target LLM, which yields a response
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Here the output  is generated by performing autoregressive sampling multiple
    times from the conditional probability distribution of tokens  given the context [[67](#bib.bib67)],
    so .
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we utilize an evaluator (See the appendix [9.1](#S9.SS1 "9.1 Choice
    of Jailbreak Evaluator ‣ 9 Apendix ‣ Characterizing and Evaluating the Reliability
    of LLMs against Jailbreak Attacks") for more details), employing inputs  and ,
    to assess the success of a jailbreak attack
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: which returns True if a pair  comprising a harmful query  and a response  from
    the target LLM constitutes a jailbreak, and returns False otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a jailbreak attack can then be formalized as finding a prompt  that
    approximately solves
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where the randomness is due to the draw of responses  from the distribution
    . That is, we seek a prompt  such that the responses generated by the target LLM
    to  are likely to be evaluated as a jailbreak with respect to the harmful query
    . Finally, note that one can also sample deterministically from a LLM (e.g., by
    sampling with temperature equal to zero), in which case solving ([4](#S3.E4 "In
    3 Preliminaries: Jailbreaking LLMs ‣ Characterizing and Evaluating the Reliability
    of LLMs against Jailbreak Attacks")) reduces to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 4 Evaluation Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overview. Our evaluation framework consists of five modules, including dataset
    construction, jailbreak attack strategies, LLMs, response evaluation, and metrics
    aggregation. The workflow is illustrated in Fig. [2](#S4.F2 "Figure 2 ‣ 4 Evaluation
    Framework ‣ Characterizing and Evaluating the Reliability of LLMs against Jailbreak
    Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5cd77585f8fe1512630fa589d6955459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Workflow of the evaluation framework'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dataset Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We adopted the dataset framework proposed by Wang et al. [[69](#bib.bib69)]
    that employs an exhaustive three-level hierarchical taxonomy to evaluate safeguards
    in LLMs. This comprehensive categorization enumerates 61 specific instances of
    harm, clustered into 12 harm types and 5 broader risk domains. In an effort to
    enhance the comprehensiveness of our evaluation, we expanded the dataset significantly.
    For each instance of harm, we added 25 tailored malicious queries, thus enriching
    the dataset to a total of 1525 entries (25 queries × 61 specific harms). This
    augmentation was achieved through meticulous manual curation and the integration
    of selected examples from AdvBench [[86](#bib.bib86)], MultiJail¹¹1[https://huggingface.co/datasets/DAMO-NLP-SG/MultiJail](https://huggingface.co/datasets/DAMO-NLP-SG/MultiJail),
    SimpleSafetyTests²²2[https://github.com/bertiev/SimpleSafetyTests](https://github.com/bertiev/SimpleSafetyTests),
    MM-SafetyBench³³3[https://github.com/isXinLiu/MM-SafetyBench](https://github.com/isXinLiu/MM-SafetyBench),
    MaliciousInstruct⁴⁴4[https://github.com/Princeton-SysML/Jailbreak_LLM/tree/main](https://github.com/Princeton-SysML/Jailbreak_LLM/tree/main),
    BeaverTails-Evaluation⁵⁵5[https://huggingface.co/datasets/PKU-Alignment/BeaverTails-Evaluation](https://huggingface.co/datasets/PKU-Alignment/BeaverTails-Evaluation),
    Verazuo-jailbreak-llms⁶⁶6[https://github.com/verazuo/jailbreak_llms](https://github.com/verazuo/jailbreak_llms),and
    Xstest [[60](#bib.bib60)].
  prefs: []
  type: TYPE_NORMAL
- en: We finally collected more than 6,000 entries, and we fine-tune a bert-base-uncased [[21](#bib.bib21)]
    as text classifier to label new data. Then we built a filter based on prompting
    gpt-4-turbo [[5](#bib.bib5)] to remove the harmless ones, resulting in 1,525 entries(See
    the Appendix [9.2](#S9.SS2 "9.2 Dataset Construction Details ‣ 9 Apendix ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks") for the classifier
    and filter details), ensuring both the consistency and the relevance of the enhanced
    dataset for comprehensive evaluation. Table [2](#S4.T2 "Table 2 ‣ 4.1 Dataset
    Construction ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating the Reliability
    of LLMs against Jailbreak Attacks") presents the first two levels of our dataset
    building structure, and the number of queries each contains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Details of our dataset. The number of queries (# Q) falling into our
    five risk domains and twelve harm types.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Risk Domain | Harm Type | # Q |'
  prefs: []
  type: TYPE_TB
- en: '| I. Information Hazards | 1\. Risks from leaking or inferring sensitive information
    (organization/government) | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Compromise privacy by leaking or inferring private information (person/individual)
    | 150 |'
  prefs: []
  type: TYPE_TB
- en: '| II. Malicious Uses | 3\. Assisting illegal activities | 325 |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Nudging or advising users to perform unethical or unsafe actions | 125
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5\. Reducing the cost of disinformation campaigns | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| III. Discrimination, Exclusion, Toxicity, Hateful, Offensive | 6\. Social
    stereotypes and unfair discrimination | 175 |'
  prefs: []
  type: TYPE_TB
- en: '| 7\. Toxic language (hate speech) | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| 8\. Adult content | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| IV. Misinformation Harms | 9\. Disseminating false or misleading information
    | 125 |'
  prefs: []
  type: TYPE_TB
- en: '| 10\. Causing material harm by disseminating misinformation e.g. in medicine
    or law | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| V. Human–chatbot Interaction Harms | 11\. Mental health or overreliance crisis
    | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 12\. Treat chatbot as a human | 50 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Evaluation Pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Jailbreak Attack Strategies.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on popularity and accessibility, we select multiple existing strategies
    for each category of the attacks. For Manual Crafting-based attacks, we apply
    JailBroken [[70](#bib.bib70)], DeepInception [[45](#bib.bib45)], and ICA [[72](#bib.bib72)].
    For longtail Encoding-based attacks, we apply MultiLingual [[20](#bib.bib20)],
    Cipher [[81](#bib.bib81)], and CodeChameleon [[50](#bib.bib50)]. For prompt Refinement-based
    attacks, we apply ReNeLLM [[22](#bib.bib22)], GPTFUZZER [[80](#bib.bib80)], AutoDAN [[47](#bib.bib47)],
    and PAIR [[15](#bib.bib15)]. Overall, our evaluation encompasses ten strategies
    spanning three distinct categories of attacks.
  prefs: []
  type: TYPE_NORMAL
- en: We establish the baseline for comparison by querying the target LLMs directly
    using the harmful queries specified in our evaluation dataset (see Sec. [4.1](#S4.SS1
    "4.1 Dataset Construction ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks")) without employing any jailbreak
    strategies. Our experiments was conducted on Server PowerEdge XE9680 with 8 NVIDIA
    A100 (80G) GPUs. We configured the hyperparameters for the jailbreak attack strategies
    in accordance with their specifications in the original papers, as delineated
    in Table [8](#S9.T8 "Table 8 ‣ 9.2 Dataset Construction Details ‣ 9 Apendix ‣
    Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks")
    in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 LLMs under Attack.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We selected thirteen popular LLMs as our target LLMs for evaluation, including
    GPT-3.5-Turbo [[54](#bib.bib54)], GPT-4-0125-preview [[5](#bib.bib5)], LLaMA2-7B-chat,
    LLaMA2-13B-chat [[65](#bib.bib65)], Vicuna-7B-v1.5, Vicuna-13B-v1.5 [[84](#bib.bib84)],
    Mistral-7B-v0.1, Mistral-7B-v0.2 [[39](#bib.bib39)], Baichuan2-7B-chat, Baichuan2-13B-chat [[76](#bib.bib76)],
    Gemma-2B-it, Gemma-7B-it [[64](#bib.bib64)], and Llama-3-8B-Instruct [[53](#bib.bib53)].
  prefs: []
  type: TYPE_NORMAL
- en: These LLMs are chosen considering their widespread application in security-related
    research (e.g.,malware analysis, phishing detection, and network intrusion detection [[74](#bib.bib74)]),
    their availability in open-source community (e.g., Open LLM Leaderboard [[25](#bib.bib25),
    [16](#bib.bib16)], and LLM Safety Leaderboard [[24](#bib.bib24)] ), and the size
    of their model parameters (e.g.,2B, 7B, 8B, and 13B). Note that our selections
    are limited to those LLMs with human alignment [[55](#bib.bib55)], excluding base
    models⁷⁷7Base models refer to unrefined LLMs that are pre-trained on large datasets
    without any specific alignment or supervised finetuning for particular tasks or
    ethical considerations. Additionally, base models are typically designed to predict
    the next token in a sequence based on the context provided by the preceding tokens.
    These models lack the necessary safeguards to prevent the generation of harmful,
    biased, or inappropriate content and might produce outputs that are factually
    incorrect or misleading. It is not generally considered in papers related to jailbreaking..
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Evaluation Criteria.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using the language of Askell et al. [[8](#bib.bib8)], we want language models
    to be helpful (they should help the user solve their task), honest (they shouldn’t
    fabricate information or mislead the user), and harmless (they should not cause
    physical, psychological, or social harm to people or the environment). Moreover,
    The principles of responsible AI as investigated by Bano et al. [[9](#bib.bib9)]
    assert that AI systems should be designed to conform to ethical norms as well
    as to robustly resist manipulations that might lead to misuse or harm. Additionally,
    human-computer interaction (HCI) [[57](#bib.bib57)] seeks to enhance the interactions
    between humans and AI to be more fluid, natural, and intuitive. Engaging with
    an LLM that generates coherent and grammatically sound responses not only encourages
    continuous user engagement but also fosters trust in the system. These principles
    guide us to establish evaluation criteria for LLMs, aiming to uphold high standards
    of quality and safety in AI systems. Ideally, faced with jailbreak attacks, a
    reliable LLMs should meet the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: • Resistance to Jailbreak. The responses generated by LLMs should neither explicitly
    nor implicitly fulfill harmful queries.
  prefs: []
  type: TYPE_NORMAL
- en: • Non-Toxicity. The responses should not contain harmful, toxic, obscene, insulting,
    or threatening content.
  prefs: []
  type: TYPE_NORMAL
- en: '• Quality. Text quality usually contains three dimensions: semantic, syntactic,
    and structural. The responses should maintain semantic integrity and fluency while
    minimizing grammatical errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Response Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We consider the criteria of LLM Reliability in Sec. [4.2.3](#S4.SS2.SSS3 "4.2.3
    Evaluation Criteria. ‣ 4.2 Evaluation Pipeline ‣ 4 Evaluation Framework ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks") and employ
    five metrics to measure and characterize the reliability of responses of LLMs
    against jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Resistance to Jailbreak is measured by Attack Success Rate. Non-toxicity is
    gauged through the Toxicity Score, reflecting the presence of inappropriate content.
    Additionally, we evaluate the quality of responses using metrics including Fluency—judged
    by perplexity, Token Length, and the prevalence of Grammatical Errors.
  prefs: []
  type: TYPE_NORMAL
- en: • Attack Success Rate (ASR). ASR represents the fraction of successfully compromised
    queries  contrast to the total number of queries , it measures how vulnerable
    or resistant a LLM is to jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: While the assessment of jailbreak success is still an unresolved issue. For
    this purpose, we have evaluated six common evaluators used in jailbreaking literature
    and conducted experiments to compare and select the most appropriate one. More
    detailed information can be found in Appendix [9.1](#S9.SS1 "9.1 Choice of Jailbreak
    Evaluator ‣ 9 Apendix ‣ Characterizing and Evaluating the Reliability of LLMs
    against Jailbreak Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: • Toxicity Score. Defined as a measure of rudeness, disrespect, or unreasonableness
    in textual content, the toxicity score quantifies the level of offensive content
    within the text. This score are usually expressed as a probability between 0 and
    1 through the application of machine learning models, such as the Perspective
    API developed by Google [[42](#bib.bib42)]. A higher Toxicity Score indicates
    a higher likelihood of the content being perceived as offensive by readers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we utilize an open-source toxicity classifier⁸⁸8[https://github.com/unitaryai/detoxify](https://github.com/unitaryai/detoxify),
    capable of attributing scores to sentences across several categories: toxic, severe
    toxic, obscene, insult, threat, identity attack, and sexual explicit. For our
    study, we selected four specific labels – toxic, obscene, insult, and threat –
    and calculated their corresponding scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '• Fluency. We evaluate the coherence of the responses through Fluency, calculated
    using the perplexity metric. We compute the perplexity (PPL) [[26](#bib.bib26)]
    of model-generated responses with GPT-2 [[59](#bib.bib59)]. Perplexity measures
    how well a language model predicts a sample of text. A lower perplexity indicates
    a better predictive performance by the model. The formula for perplexity is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where  is the total number of tokens in the response, and  is the probability
    assigned by the model to the -th token.
  prefs: []
  type: TYPE_NORMAL
- en: • Token Length. Response quality can be characterized to some extent by the
    output sentence length, which we use here at the token-level length of the response [[27](#bib.bib27)].
  prefs: []
  type: TYPE_NORMAL
- en: • Grammatical Errors. The grammatical errors are the number of errors in the
    response example’s grammar using LanguageTool⁹⁹9[https://languagetool.org/](https://languagetool.org/),
    by counting the grammatical mistakes in a given response.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Metrics Aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To furnish a more comprehensive reliability score of each model as reference,
    it is essential to normalize the metric values and incorporate them into a composite
    score. This score reflects the weighted significance of various dimensions, with
    the determination of these weights entrusted to the discretion of the model user.
    Subsequently, we describe the procedure in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization. For metrics that improve with minimization (e.g., ASR, Toxicity
    Score, Grammatical Errors,and Fluency), a higher value indicates decreased reliability.
    Conversely, for the metric that benefits from maximization (i.e., Token Length),
    a higher value implies enhanced reliability. We normalize each metric to a range
    between 0 and 1, whereby a higher value consistently denotes increased reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'For metrics to be minimized (), such as ASR, Toxicity Score, Grammatical Errors,
    Fluency, we define a normalization function :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'Where: -  represents the raw value of the metric. -  represents the minimum
    possible or observed value for the metric to be minimized. -  represents the maximum
    possible or observed value for the metric to be minimized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the metric to be maximized (), such as Token Length, we define a normalization
    function :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Where: -  represents the raw value of the metric. -  represents the minimum
    possible or observed value for the metric to be maximized. -  represents the maximum
    possible or observed value for the metric to be maximized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aggregation. To derive a reliability score for each model, we amalgamate all
    the normalized values. Recognizing that model users attribute varying levels of
    importance to different metrics, the reliability score  for each model can be
    computed by synthesizing the normalized values across all pertinent metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'Where: -  is the number of metrics evaluated. -  is the normalization function
    applied to the -th metric. -  represents the assigned weight to each metric, which
    the model user determines. - The reliability score  is weighted average of all
    normalized metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Evaluation of Attack Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our evaluation framework, we consider multiple metrics; notably, the Attack
    Success Rate (ASR) is uniquely tailored to quantify the Resistance of LLMs to
    jailbreak attacks. This metric is initially calculated and scrutinized in our
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Table [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation of Attack Strategy ‣ 5 Experiment
    ‣ Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks")
    presents the ASR results for the target LLMs under various jailbreak attack strategies,
    as categorized in our taxonomy. From the baseline results, we observe that any
    of the thirteen LLMs do not demonstrate initial resistance to harmful queries.
    Notably, well-aligned models such as Llama3, GPT-3.5, and GPT-4 yielded baseline
    ASR scores of 20.26%, 13.84%, and 16.26%, respectively. The results indicates
    that the average ASR across some LLMs does not surpass the baseline, which suggests
    a degree of resistance to jailbreak attacks in some LLMs. Vicuna and Mistral emerged
    as the LLM families most vulnerable or susceptible to jailbreak attacks, which
    aligns with our conclusion related to the harm type (see Sec. [6.1](#S6.SS1 "6.1
    Evaluation of Harm Type ‣ 6 Ablation Study ‣ Characterizing and Evaluating the
    Reliability of LLMs against Jailbreak Attacks")) and many previous works [[47](#bib.bib47),
    [86](#bib.bib86), [80](#bib.bib80)].
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of jailbreak category, we find that all jailbreak strategies,
    except for ICA and Cipher, demonstrate high attack efficacy, as demonstrated in
    Table [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation of Attack Strategy ‣ 5 Experiment ‣
    Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks").
    After detailed analysis, we discern that all the Longtail Encoding-based attacks
    are model-specific, i.e., they can achieve a low ASR score on a specific model.
    For example, Multilingual only exhibited poor attack performance on Llama3, GPT-3.5,
    and GPT-4, with ASR nearing 0. We attribute this phenomenon to the strong capability
    of the LLM. LLMs like GPT-4, with their extensive training on wide variety of
    datasets, obtain the capacity to recognize and process texts that have been translated
    to low-resource languages or encoded, which is beyond the capability of other
    models. This capability makes it easier to align such LLMs and decreases their
    vulnerability to jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: In general, from Table [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation of Attack Strategy
    ‣ 5 Experiment ‣ Characterizing and Evaluating the Reliability of LLMs against
    Jailbreak Attacks"), ReNeLLM achieve the highest average ASR (about 29.22%) when
    targeting all LLMs. In addition, almost all Prompt Refinement-based attacks yield
    higher ASR results, except for AutoDAN on Gemma. This result is expected given
    that these attacks typically necessitates greater time and computational resources
    for iterative optimization.
  prefs: []
  type: TYPE_NORMAL
- en: For the manual crafting-based attacks, ICA exploit vulnerabilities in the model’s
    context learning [[12](#bib.bib12)] to induce responses to malicious instructions [[72](#bib.bib72)].
    However, in our experimentation, these tactics failed to yield effective results,
    evidenced by the nearly 0 ASR observed across various LLMs. Conversely, we find
    that Jailbroken can still achieve high ASR across LLMs, which highlights the significance
    of actively collecting and analyzing such jailbreak prompts. This also underscores
    the robustness and effectiveness of human-based jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The ASR of employing different jailbreak strategies on various target
    LLM. All values are expressed as percentages (%). The highest value in a row is
    highlighted in blue, and the highest value in a column is underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Mode | Manual Crafting | Longtail Encoding | Prompt Refinement | Baseline
    | Average |'
  prefs: []
  type: TYPE_TB
- en: '| JailBroken | DeepInception | ICA | Cipher | MultiLingual | CodeChameleon
    | AutoDAN | PAIR | TAP | ReNeLLM |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | 12.95 | 19.80 | 0.00 | 0.44 | 0.01 | 10.20 | 14.82 | 13.57
    | 18.03 | 29.27 | 13.84 | 10.84 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-0125-preview | 8.49 | 8.26 | 0.00 | 6.52 | 0.00 | 18.34 | 14.82 | 16.26
    | 15.41 | 24.91 | 16.26 | 9.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7B-chat | 10.32 | 7.74 | 0.00 | 0.61 | 3.16 | 14.56 | 7.08 | 13.64
    | 14.03 | 23.62 | 16.66 | 9.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13B-chat | 10.42 | 8.52 | 0.00 | 0.64 | 2.65 | 13.82 | 12.85 | 12.07
    | 15.08 | 19.49 | 16.33 | 9.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B-v1.5 | 24.35 | 14.43 | 0.20 | 0.74 | 29.86 | 14.57 | 36.26 | 20.07
    | 23.67 | 25.47 | 21.31 | 22.36 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13B-v1.5 | 24.10 | 11.61 | 0.52 | 0.34 | 44.62 | 14.49 | 17.44 | 18.23
    | 22.89 | 32.71 | 16.66 | 24.47 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.1 | 27.69 | 16.72 | 6.89 | 0.20 | 29.22 | 14.84 |
    15.87 | 36.33 | 34.56 | 29.71 | 37.70 | 24.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.2 | 31.50 | 11.54 | 0.07 | 0.87 | 72.74 | 18.21 |
    47.48 | 22.75 | 25.97 | 55.38 | 28.00 | 34.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-7B-Chat | 21.07 | 26.56 | 0.00 | 1.79 | 14.94 | 8.85 | 31.28 |
    16.79 | 19.80 | 22.47 | 16.33 | 17.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-13B-Chat | 23.21 | 29.64 | 0.00 | 0.59 | 22.45 | 10.05 | 59.02
    | 18.69 | 26.56 | 35.64 | 17.31 | 21.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2b-it | 5.09 | 2.03 | 0.00 | 0.72 | 4.45 | 3.23 | 1.57 | 15.41 | 16.33
    | 12.84 | 17.38 | 5.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7b-it | 15.73 | 9.97 | 0.00 | 2.05 | 29.81 | 6.11 | 7.14 | 18.30 |
    19.54 | 38.02 | 21.25 | 16.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3-8B-Instruct | 11.04 | 15.67 | 0.07 | 0.10 | 0.17 | 13.11 | 21.90
    | 16.66 | 18.89 | 30.33 | 20.26 | 10.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 17.38 | 14.04 | 0.60 | 1.20 | 19.55 | 12.34 | 22.12 | 18.37 | 20.83
    | 29.22 | 19.94 | 16.75 |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Characterizing the Reliability of LLMs under Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we calculate all the metrics in our evaluation framework and
    present the reliability of the responses of LLMs under jailbreak attack. Table
     [4](#S5.T4 "Table 4 ‣ 5.2 Characterizing the Reliability of LLMs under Attacks
    ‣ 5 Experiment ‣ Characterizing and Evaluating the Reliability of LLMs against
    Jailbreak Attacks") shows the evaluation metrics of responses of various jailbreak
    attacks on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In general, from Table [4](#S5.T4 "Table 4 ‣ 5.2 Characterizing the Reliability
    of LLMs under Attacks ‣ 5 Experiment ‣ Characterizing and Evaluating the Reliability
    of LLMs against Jailbreak Attacks"), GPT-4 stands out for its low toxicity across
    all categories and a relatively low ASR, making it highly reliable in maintaining
    non-toxic outputs under attacks, despite its higher perplexity. Llama3 combines
    a low ASR, low toxicity, and few grammatical errors, with decent fluency, making
    it another strong contender for reliability. Gemma-2b-it has the lowest ASR, indicating
    high robustness against jailbreak attacks, although its other metrics are not
    the best across the board. Mistral-7B-Instruct-v0.2 and Vicuna-13b-v1.5 exhibit
    high ASR and relatively high toxicity levels, indicating lower reliability under
    jailbreak conditions. Baichuan2-7B-Chat also shows high toxicity and grammatical
    errors, which compromises its reliability despite having a reasonably long response
    length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Evaluation metrics of responses with jailbreak attacks on LLMs. ASR
    is the Attack Success Rate, Toxic, Obscene, Insult, and Threat are the probability
    score of Toxicity. Fluency is measured by the perplexity metric, Grm is the number
    of grammatical errors, Length is the token-level length of the response.  () means
    the higher (lower) the metric is, the more reliable the response is. We use bold
    text to highlight the best results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Jailbreak | Toxicity | Quality |'
  prefs: []
  type: TYPE_TB
- en: '| ASR  | Toxic  | Obscene | Insult | Threat  | Grm  | Length | Fluency |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | 10.84 | 9.52 | 7.66 | 1.10 | 0.08 | 2.79 | 133.15 | 6.53
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-0125-preview | 9.14 | 2.43 | 1.23 | 0.22 | 0.03 | 3.78 | 199.54 | 10.29
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7b-chat | 9.76 | 5.78 | 3.79 | 0.66 | 0.06 | 5.88 | 311.49 | 6.90
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13b-chat | 9.40 | 5.03 | 3.11 | 0.61 | 0.06 | 5.53 | 292.40 | 6.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5 | 22.36 | 10.80 | 7.93 | 1.54 | 0.12 | 7.82 | 263.18 | 5.79
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5 | 24.47 | 12.41 | 8.37 | 1.89 | 0.15 | 9.53 | 282.66 | 6.51
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.1 | 24.89 | 9.34 | 6.96 | 1.59 | 0.13 | 6.92 | 272.91
    | 7.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.2 | 34.64 | 6.28 | 4.41 | 0.92 | 0.07 | 8.26 | 330.39
    | 7.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-7B-Chat | 17.56 | 11.41 | 7.82 | 1.46 | 0.13 | 11.73 | 311.18 |
    7.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-13B-Chat | 21.31 | 10.08 | 7.17 | 1.19 | 0.11 | 9.53 | 319.39 |
    6.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2b-it | 5.45 | 5.46 | 3.57 | 0.70 | 0.07 | 4.32 | 161.64 | 5.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7b-it | 16.94 | 8.36 | 6.09 | 0.93 | 0.07 | 9.65 | 257.95 | 6.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3-8B-Instruct | 10.38 | 3.26 | 2.07 | 0.42 | 0.04 | 2.55 | 161.76 |
    6.08 |'
  prefs: []
  type: TYPE_TB
- en: 5.3 Reliablity Score and Evaluating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building upon the process delineated in Sec. [4.3](#S4.SS3 "4.3 Metrics Aggregation
    ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating the Reliability of LLMs
    against Jailbreak Attacks"), we initially normalized the values of all metrics
    listed in Table [4](#S5.T4 "Table 4 ‣ 5.2 Characterizing the Reliability of LLMs
    under Attacks ‣ 5 Experiment ‣ Characterizing and Evaluating the Reliability of
    LLMs against Jailbreak Attacks"). Following this, as stipulated in Eq. [10](#S4.E10
    "In 4.3 Metrics Aggregation ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"), weights can be allotted to
    each metric at the discretion of the model user. Assuming equal significance for
    all metrics, Eq. [10](#S4.E10 "In 4.3 Metrics Aggregation ‣ 4 Evaluation Framework
    ‣ Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks")
    simplifies to the sum average of all the normalized metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Table. [5](#S5.T5 "Table 5 ‣ 5.3 Reliablity Score and Evaluating ‣ 5 Experiment
    ‣ Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks")
    displays the calculated reliability scores for each model, considering the normalization
    of each metric based on the criteria provided. The models are sorted in descending
    order of their reliability scores, where a higher score indicates a better overall
    reliability under jailbreak based on the aggregated metrics. This aggregation
    provides a comprehensive view of the overall reliability of each model by integrating
    various metrics into a unified score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Aggregated reliability scores for each model metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Name | Reliability Score |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3-8B-Instruct | 0.815 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13b-chat | 0.768 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-0125-preview | 0.759 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7b-chat | 0.740 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2b-it | 0.712 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.2 | 0.540 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7b-it | 0.533 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | 0.510 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-13B-Chat | 0.452 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5 | 0.401 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.1 | 0.388 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-7B-Chat | 0.327 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5 | 0.273 |'
  prefs: []
  type: TYPE_TB
- en: 6 Ablation Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Evaluation of Harm Type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building upon the dataset structure outlined in Table [2](#S4.T2 "Table 2 ‣
    4.1 Dataset Construction ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"), we can analyze the ASR of
    various LLMs across different harm types. As illustrated in Table [6](#S6.T6 "Table
    6 ‣ 6.1 Evaluation of Harm Type ‣ 6 Ablation Study ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"), we observed a significantly
    variance in ASR different harm types. Furthermore, for a more insightful comparative
    analysis of model performance, these results were synthesized into a heatmap,
    as shown in Fig. [3(a)](#S6.F3.sf1 "In Figure 3 ‣ 6.1 Evaluation of Harm Type
    ‣ 6 Ablation Study ‣ Characterizing and Evaluating the Reliability of LLMs against
    Jailbreak Attacks"). Within this graphical representation, models exhibiting higher
    vulnerability are indicated by darker shades, which correlate with enhanced ASRs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, Assist Illegal Acts, Advise Unethical Actions, Lower Disinformation
    Cost, and Adult Content are the four most straightforward and amenable harm types
    to launch jailbreak attacks, whereas the Stereotypes & Discrimination, and Treat
    Bot As Human are more challenging to jailbreak attacks. From the results,we observe
    that three LLMs contain the highest ASR corresponding to all harm types: Vicuna
    (7 types), Mistral (10 types), and Baichuan (8 types). Therefore, we believe that
    although other models also suffer from jailbreak attacks across different harm
    types, these three models are the most vulnerable.'
  prefs: []
  type: TYPE_NORMAL
- en: This phenomenon underscores the challenges associated with effectively aligning
    LLM policies and their jailbreak attacks, indicating the future works for improvement
    in ensuring policy compliance within LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Average ASR in the LLM responses across the different harm types.
    These harm types, numerically labeled 1 through 12, correspond to specific cotents
    detailed in Table [2](#S4.T2 "Table 2 ‣ 4.1 Dataset Construction ‣ 4 Evaluation
    Framework ‣ Characterizing and Evaluating the Reliability of LLMs against Jailbreak
    Attacks")'
  prefs: []
  type: TYPE_NORMAL
- en: '| Harm Type  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | 9.98 | 7.03 | 15.53 | 15.06 | 13.98 | 4.54 | 6.33 | 12.77
    | 11.83 | 10.82 | 9.41 | 6.06 | 10.84 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-0125-preview | 10.94 | 6.69 | 9.66 | 9.40 | 15.29 | 3.21 | 3.47 | 12.62
    | 10.72 | 13.51 | 10.30 | 3.99 | 9.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7B-chat | 13.31 | 8.85 | 9.05 | 9.53 | 12.84 | 3.93 | 4.51 | 8.84
    | 12.19 | 14.15 | 11.35 | 6.96 | 9.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13B-chat | 13.96 | 8.15 | 7.90 | 8.00 | 11.07 | 3.61 | 5.20 | 9.28
    | 11.81 | 14.50 | 11.70 | 8.99 | 9.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B-v1.5 | 17.67 | 14.92 | 34.33 | 32.60 | 29.93 | 9.96 | 19.21 | 26.00
    | 20.49 | 17.09 | 20.68 | 12.62 | 22.36 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13B-v1.5 | 20.85 | 16.32 | 35.68 | 34.11 | 32.84 | 10.99 | 22.22 |
    29.72 | 23.89 | 20.03 | 21.85 | 15.41 | 24.47 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.1 | 18.90 | 15.81 | 39.97 | 36.02 | 33.07 | 10.60
    | 23.20 | 27.94 | 22.91 | 18.51 | 22.81 | 13.93 | 24.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.2 | 33.29 | 27.32 | 45.23 | 44.71 | 44.76 | 17.20
    | 29.65 | 47.11 | 32.25 | 31.04 | 32.80 | 23.31 | 34.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-7B-Chat | 14.16 | 11.79 | 26.07 | 24.57 | 22.39 | 7.55 | 14.84
    | 25.84 | 17.89 | 14.56 | 15.20 | 9.26 | 17.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2-13B-Chat | 17.86 | 12.83 | 31.05 | 30.59 | 28.87 | 9.70 | 18.63
    | 27.79 | 21.54 | 17.88 | 19.10 | 11.68 | 21.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2b-it | 7.05 | 2.26 | 5.45 | 6.07 | 7.73 | 2.48 | 2.00 | 6.25 | 7.08
    | 8.33 | 7.22 | 3.08 | 5.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7b-it | 20.73 | 8.90 | 20.44 | 22.21 | 22.91 | 7.62 | 10.69 | 23.39
    | 16.63 | 18.76 | 16.91 | 10.67 | 16.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3-8B-Instruct | 13.74 | 9.16 | 8.79 | 10.39 | 14.39 | 6.15 | 6.66 |
    12.12 | 12.84 | 13.70 | 10.73 | 7.44 | 10.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Baeline | 26.23 | 5.74 | 22.22 | 21.48 | 36.21 | 3.78 | 7.23 | 38.62 | 20.49
    | 30.62 | 27.31 | 7.54 | 20.62 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 16.34 | 11.54 | 22.24 | 21.79 | 22.31 | 7.50 | 12.82 | 20.74 |
    17.08 | 16.38 | 16.16 | 10.26 | 16.75 | ![Refer to caption](img/aa7a24381931bac68453e764668884a5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Average fine-grained Attack Success Rate (ASR) across twelve harm types.
    This heatmap illustrates the relationship between target models and harm types.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddd430b0fe7d435850e71ddda4d79c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Average fine-grained Attack Success Rate (ASR) across thirteen target LLMs.
    This heatmap illustrates the relationship between jailbreak attack strategies
    and harm types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: (Left) The relationship between jailbreak attacks and harm types.
    (Right)The relationship between target models and harm types.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Relationship Between Attack Strategy and Harm Type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further explore the relationship between the harm type (i.e., the 12 harm
    type categories in Table [2](#S4.T2 "Table 2 ‣ 4.1 Dataset Construction ‣ 4 Evaluation
    Framework ‣ Characterizing and Evaluating the Reliability of LLMs against Jailbreak
    Attacks")) and the jailbreak attacks (i.e., the 10 different types of jailbreak
    attack strategies ), we visualize the ASR between the two using a heatmap, as
    depicted in Fig. [3(b)](#S6.F3.sf2 "In Figure 3 ‣ 6.1 Evaluation of Harm Type
    ‣ 6 Ablation Study ‣ Characterizing and Evaluating the Reliability of LLMs against
    Jailbreak Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: First, it shows that the ICA, and Cipher jailbreak attacks demonstrate consistently
    poor performance across all violation categories in the harm types, denoted by
    the negative correlation in the heatmap. Furthermore, the Prompt Refinement-based
    jailbreak attacks are relatively robust and versatile among all the violation
    categories. Among all the Manual Crafting-based jailbreak methods, Jalibroken
    is the most effective one from various categories of our harm types. The above
    conclusions intuitively support the findings we presented previously (see Sec. [5.1](#S5.SS1
    "5.1 Evaluation of Attack Strategy ‣ 5 Experiment ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks")).
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Correlation Between Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we conducted a correlation analysis between the evaluation
    metrics in our framework. As we know, jailbreak attacks on LLMs aim to manipulate
    models to produce damaging content, where their potential effectiveness may in
    part rely on the level of malignancy and toxicity present in the resulting outputs.
    We initially investigate the relationship between the ASR metric evaluated in
    Sec. [5.1](#S5.SS1 "5.1 Evaluation of Attack Strategy ‣ 5 Experiment ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks") and our toxicity
    metrics. we calculated the Pearson correlation coefficient between the ASR metric
    and the toxic, obscene, insult and threat metrics, subsequently illustrating the
    results through a heatmap presented in Fig. [4(a)](#S6.F4.sf1 "In Figure 4 ‣ 6.3
    Correlation Between Metrics ‣ 6 Ablation Study ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"). Additionally, we rendered
    a scatter plot enriched with a regression line, depicted in Fig. [4(b)](#S6.F4.sf2
    "In Figure 4 ‣ 6.3 Correlation Between Metrics ‣ 6 Ablation Study ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis revealed that the ASR is moderately positively correlated with
    the metrics for toxicity, obscenity, insult, and threat. Nonetheless, regression
    analyses suggested that the impact of these toxicity metrics on ASR was statistically
    negligible, which hints at the existence of additional influencing factors. Consequently,
    it appears that ASR is subject to a variety of influences, with the considered
    toxicity metrics contributing only partially.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, as detailed in Sec. [4.2.3](#S4.SS2.SSS3 "4.2.3 Evaluation Criteria.
    ‣ 4.2 Evaluation Pipeline ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"), we sought to quantify the
    output quality of LLMs across three metrics: Fluency, Token Length, and Grammatical
    Errors. To elucidate the relationships between these metrics, we carried out a
    statistical analysis accompanied by a visualization, presented in Fig. [5](#S6.F5
    "Figure 5 ‣ 6.3 Correlation Between Metrics ‣ 6 Ablation Study ‣ Characterizing
    and Evaluating the Reliability of LLMs against Jailbreak Attacks"), to uncover
    potential patterns or correlations. The analysis divulged a relative strong positive
    correlation (0.78) between Grammatical Errors and Token Length, signifying that
    models producing longer responses typically exhibit more grammatical inaccuracies.
    Conversely, Fluency did not demonstrate a robust correlation with either Grammatical
    Errors or Token Length, implying that it captures a distinct facet of response
    quality.'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the empirical findings from our correlation analysis underscore
    the validity of our multifaceted evaluation framework. Each of the five metrics
    we have selected – Adversarial Success Rate (ASR), Toxicity, Fluency, Token Length,
    and Grammatical Errors – has been meticulously chosen to capture nuances in the
    generated content of LLMs. Although they interact in complex and sometimes subtle
    ways, they collectively contribute to a comprehensive evaluation of LLM reliability.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18af12fd81be223a37e06b427aa2b4ee.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Correlation Matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e880e973cb64642215fd279f59f725d3.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Scatter Plot with Regression Line
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: (a)Correlation matrix between ASR and Toxicity using Pearson correlation
    coefficient (b) Scatter Plot with Regression Line between ASR and Toxicity'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b0437d1d8e591cd6d43dc07da6dd5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The relationships between Grammatical Errors, Token Length, and Fluency
    metrics for different models.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present a novel and comprehensive evaluation framework designed
    to assess the reliability of LLMs when exposed to diverse jailbreak attack strategies.
    Our evaluation framework included the construction of a refined three- level hierarchical
    dataset, comprising 1525 questions across 61 distinct harmful categories. We evaluate
    the LLMs’ outputs under jailbreak by integrating different aspects of content
    security, such as resistance to jailbreak, toxicity, and quality, and using multi-dimensional
    metrics like Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length,
    and Grammatical Errors.
  prefs: []
  type: TYPE_NORMAL
- en: Through elaborate experiments involving 13 popular LLMs and 10 state-of-the-art
    jailbreak attack strategies, we holistically analyze the jailbreak attacks and
    uncover significant variations in the models’ resilience. Our findings draw attention
    to the inherent vulnerabilities of some of the most popular LLMs, such as Vicuna
    and Mistral, which exhibited heightened susceptibility to jailbreak attacks. By
    normalizing and aggregating these metrics, we present a detailed reliability score
    for different LLMs, which encapsulates the leveled importance of different metrics,
    providing actionable insights to the end-users in terms of prioritizing aspects
    they consider most crucial for their specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We conduct a comprehensive ablation study to assess reliability across diffirent
    harm types. We find that the reliability of LLMs is specific under certain harmful
    question scenarios. We also present fine-grained relationships between Attack
    Strategy and harm type, we investigste the correlations between different evaluation
    metrics that verify the soundness of our framework.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, We posit that responsible AI development must persistently address
    the evolving landscape of adversarial attacks, ensuring that language models remain
    steadfast guardians of ethical guidelines, while continuing to be helpful, honest,
    and harmless companions in our digital interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To address the constraints posed by limited resources, our evaluation experiment
    does not extend to larger models, such as those with 33 billion and 70 billion
    parameters, nor does it cover other powerful commercial models like Claude [[6](#bib.bib6)]
    and Gemini [[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] ChatGPT. [https://chat.openai.com/chat](https://chat.openai.com/chat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Openai says a bug leaked sensitive chatgpt user data. [https://www.engadget.com/chatgpt-briefly-went-offline-after-a-bug-revealed-user-chat-histories-115632504.html](https://www.engadget.com/chatgpt-briefly-went-offline-after-a-bug-revealed-user-chat-histories-115632504.html),
    engadget. Accessed:2023-8-20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Chatgpt: Us lawyer admits using ai for case research (2023), [https://www.bbc.co.uk/news/world-us-canada-65735769](https://www.bbc.co.uk/news/world-us-canada-65735769),
    accessed: 2023-08-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] ’he would still be here’: Man dies by suicide after talking with ai chatbot,
    widow says (2023), [https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says](https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says),
    accessed: 2023-08-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,
    Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical
    report (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Anthropic: Introducing the next generation of claude. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
    (2023), accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Apruzzese, G., Anderson, H.S., Dambra, S., Freeman, D., Pierazzi, F., Roundy,
    K.A.: "real attackers don’t compute gradients": Bridging the gap between adversarial
    ml research and practice (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones,
    A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez,
    D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish,
    S., Olah, C., Kaplan, J.: A general language assistant as a laboratory for alignment
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Bano, M., Zowghi, D., Shea, P., Ibarra, G.: Investigating responsible ai
    for scientific research: An empirical study (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Bhardwaj, R., Poria, S.: Red-teaming large language models using chain
    of utterances for safety-alignment (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Bhatt, M., Chennabasappa, S., Nikolaidis, C., Wan, S., Evtimov, I., Gabi,
    D., Song, D., Ahmad, F., Aschermann, C., Fontana, L., Frolov, S., Giri, R.P.,
    Kapil, D., Kozyrakis, Y., LeBlanc, D., Milazzo, J., Straumann, A., Synnaeve, G.,
    Vontimitta, V., Whitman, S., Saxe, J.: Purple llama cyberseceval: A secure coding
    benchmark for language models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A.: Language models are few-shot
    learners (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Carlini, N., Jagielski, M., Choquette-Choo, C.A., Paleka, D., Pearce,
    W., Anderson, H., Terzis, A., Thomas, K., Tramèr, F.: Poisoning web-scale training
    datasets is practical (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Chao, P., Debenedetti, E., Robey, A., Andriushchenko, M., Croce, F., Sehwag,
    V., Dobriban, E., Flammarion, N., Pappas, G.J., Tramer, F., Hassani, H., Wong,
    E.: Jailbreakbench: An open robustness benchmark for jailbreaking large language
    models (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G., Wong, E.:
    Jailbreaking black box large language models in twenty queries (Oct 2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Chiang, W.L., Zheng, L., Sheng, Y., Angelopoulos, A.N., Li, T., Li, D.,
    Zhang, H., Zhu, B., Jordan, M., Gonzalez, J.E., Stoica, I.: Chatbot arena: An
    open platform for evaluating llms by human preference (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Cinà, A.E., Grosse, K., Demontis, A., Vascon, S., Zellinger, W., Moser,
    B.A., Oprea, A., Biggio, B., Pelillo, M., Roli, F.: Wild patterns reloaded: A
    survey of machine learning security against training data poisoning. ACM Computing
    Surveys 55(13s), 1–39 (Jul 2023). https://doi.org/10.1145/3585385, [http://dx.doi.org/10.1145/3585385](http://dx.doi.org/10.1145/3585385)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N.A., Gardner, M.:
    A dataset of information-seeking questions and answers anchored in research papers
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Deng, G., Liu, Y., Li, Y., Wang, K., Zhang, Y., Li, Z., Wang, H., Zhang,
    T., Liu, Y.: Jailbreaker: Automated jailbreak across multiple large language model
    chatbots. arXiv preprint arXiv:2307.08715 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Deng, Y., Zhang, W., Pan, S.J., Bing, L.: Multilingual jailbreak challenges
    in large language models. arXiv preprint arXiv:2310.06474 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of
    deep bidirectional transformers for language understanding (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Ding, P., Kuang, J., Ma, D., Cao, X., Xian, Y., Chen, J., Huang, S.: A
    wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large
    language models easily (Nov 2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu,
    J., Li, L., Sui, Z.: A survey on in-context learning (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Face, H.: Llm safety leaderboard. [https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Face, H.: Open llm leaderboard. [https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Face, H.: Perplexity of fixed-length models. [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Face, H.: Tokenizers. [https://huggingface.co/docs/tokenizers/index](https://huggingface.co/docs/tokenizers/index),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Frieder, S., Pinchetti, L., Chevalier, A., Griffiths, R.R., Salvatori,
    T., Lukasiewicz, T., Petersen, P.C., Berner, J.: Mathematical capabilities of
    chatgpt (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] GitHub: Github copilot: Your ai pair programmer. [https://github.com/features/copilot](https://github.com/features/copilot),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Google: Google gemini advanced. [https://gemini.google.com/advanced](https://gemini.google.com/advanced),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Gopal, A., Helm-Burger, N., Justen, L., Soice, E.H., Tzeng, T., Jeyapragasan,
    G., Grimm, S., Mueller, B., Esvelt, K.M.: Will releasing the weights of future
    large language models grant widespread access to pandemic agents? (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., Fritz,
    M.: Not what you’ve signed up for: Compromising real-world llm-integrated applications
    with indirect prompt injection (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N.V., Wiest,
    O., Zhang, X.: Large language model based multi-agents: A survey of progress and
    challenges (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Hamiel, N.: Reducing the impact of prompt injection attacks through design.
    [https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/](https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Huang, X., Ruan, W., Huang, W., Jin, G., Dong, Y., Wu, C., Bensalem, S.,
    Mu, R., Qi, Y., Zhao, X., Cai, K., Zhang, Y., Wu, S., Xu, P., Wu, D., Freitas,
    A., Mustafa, M.A.: A survey of safety and trustworthiness of large language models
    through the lens of verification and validation (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Huang, Y., Gupta, S., Xia, M., Li, K., Chen, D.: Catastrophic jailbreak
    of open-source LLMs via exploiting generation. In: The Twelfth International Conference
    on Learning Representations (2024), [https://openreview.net/forum?id=r42tSSCHPh](https://openreview.net/forum?id=r42tSSCHPh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Hugging Face: Meta llama. [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)
    (2023), accessed: 2024-02-14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev,
    M., Hu, Q., Fuller, B., Testuggine, D., Khabsa, M.: Llama guard: Llm-based input-output
    safeguard for human-ai conversations (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S.,
    Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral
    7b. arXiv preprint arXiv:2310.06825 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., Hashimoto, T.:
    Exploiting programmatic behavior of llms: Dual-use through standard security attacks.
    arXiv preprint arXiv:2302.05733 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Lee, P.: Learning from tay’s introduction. [https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/),
    accessed: 2023-08-20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Lees, A., Tran, V.Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., Vasserman,
    L.: A new generation of perspective api: Efficient multilingual character-level
    transformers (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Li, H., Guo, D., Fan, W., Xu, M., Huang, J., Meng, F., Song, Y.: Multi-step
    jailbreaking privacy attacks on chatgpt (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Li, J., Cheng, X., Zhao, W.X., Nie, J., rong Wen, J.: Halueval: A large-scale
    hallucination evaluation benchmark for large language models. ArXiv abs/2305.11747
    (2023), [https://api.semanticscholar.org/CorpusID:258832847](https://api.semanticscholar.org/CorpusID:258832847)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Li, X., Zhou, Z., Zhu, J., Yao, J., Liu, T., Han, B.: Deepinception: Hypnotize
    large language model to be jailbreaker. arXiv preprint arXiv:2311.03191 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., Zhang, Y.: Evaluating
    the logical reasoning ability of chatgpt and gpt-4 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Liu, X., Xu, N., Chen, M., Xiao, C.: Autodan: Generating stealthy jailbreak
    prompts on aligned large language models. arXiv preprint arXiv:2310.04451 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Liu, Y., Deng, G., Li, Y., Wang, K., Zhang, T., Liu, Y., Wang, H., Zheng,
    Y., Liu, Y.: Prompt injection attack against llm-integrated applications. arXiv
    preprint arXiv:2306.05499 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Liu, Y., Deng, G., Xu, Z., Li, Y., Zheng, Y., Zhang, Y., Zhao, L., Zhang,
    T., Liu, Y.: Jailbreaking chatgpt via prompt engineering: An empirical study.
    arXiv preprint arXiv:2305.13860 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Lv, H., Wang, X., Zhang, Y., Huang, C., Dou, S., Ye, J., Gui, T., Zhang,
    Q., Huang, X.: Codechameleon: Personalized encryption framework for jailbreaking
    large language models. arXiv preprint arXiv:2402.16717 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E.,
    Li, N., Basart, S., Li, B., et al.: Harmbench: A standardized evaluation framework
    for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249
    (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Mehrotra, A., Zampetakis, M., Kassianik, P., Nelson, B., Anderson, H.,
    Singer, Y., Karbasi, A.: Tree of attacks: Jailbreaking black-box llms automatically.
    arXiv preprint arXiv:2312.02119 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Meta: Introducing meta llama 3: The most capable openly available llm
    to date. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] OpenAI: Research overview. [https://openai.com/research/overview](https://openai.com/research/overview)
    (2023), accessed: 2024-02-14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P.,
    Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to
    follow instructions with human feedback. Advances in Neural Information Processing
    Systems 35, 27730–27744 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] OWASP: OWASP Top 10 for LLM Applications (2023), [https://owasp.org/www-project-top-10-for-large-language-model-applications/](https://owasp.org/www-project-top-10-for-large-language-model-applications/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Paluri, S.: Human computer interaction (12 2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Rachel Draelos, MD, P.: Bias, toxicity, and jailbreaking large language
    models (llms). [https://glassboxmedicine.com/2023/11/28/bias-toxicity-and-jailbreaking-large-language-models-llms/](https://glassboxmedicine.com/2023/11/28/bias-toxicity-and-jailbreaking-large-language-models-llms/),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language
    models are unsupervised multitask learners (2019), [https://api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Röttger, P., Kirk, H.R., Vidgen, B., Attanasio, G., Bianchi, F., Hovy,
    D.: Xstest: A test suite for identifying exaggerated safety behaviours in large
    language models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Schwinn, L., Dobre, D., Günnemann, S., Gidel, G.: Adversarial attacks
    and defenses in large language models: Old and new threats. arXiv preprint arXiv:2310.19737
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Shah, M.A., Sharma, R., Dhamyal, H., Olivier, R., Shah, A., Alharthi,
    D., Bukhari, H.T., Baali, M., Deshmukh, S., Kuhlmann, M., et al.: Loft: Local
    proxy fine-tuning for improving transferability of adversarial attacks against
    large language model. arXiv preprint arXiv:2310.04445 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Shen, X., Chen, Z., Backes, M., Zhang, Y.: In chatgpt we trust? measuring
    and characterizing the reliability of chatgpt (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak,
    S., Sifre, L., Rivière, M., Kale, M.S., Love, J., Tafti, P.: Gemma: Open models
    based on gemini research and technology (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open
    foundation and fine-tuned chat models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] University, S.: Cs324: Data. [https://stanford-cs324.github.io/winter2022/lectures/data/](https://stanford-cs324.github.io/winter2022/lectures/data/),
    accessed: 2024-06-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Wan, A., Wallace, E., Shen, S., Klein, D.: Poisoning language models during
    instruction tuning (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Wang, Y., Li, H., Han, X., Nakov, P., Baldwin, T.: Do-not-answer: A dataset
    for evaluating safeguards in llms (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Wei, A., Haghtalab, N., Steinhardt, J.: Jailbroken: How does llm safety
    training fail? arXiv preprint arXiv:2307.02483 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language
    models. Advances in Neural Information Processing Systems 35, 24824–24837 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Wei, Z., Wang, Y., Wang, Y.: Jailbreak and guard aligned language models
    with only few in-context demonstrations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Wu, Y., Li, X., Liu, Y., Zhou, P., Sun, L.: Jailbreaking gpt-4v via self-adversarial
    attacks with system prompts. arXiv preprint arXiv:2311.09127 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Xu, H., Wang, S., Li, N., Wang, K., Zhao, Y., Chen, K., Yu, T., Liu, Y.,
    Wang, H.: Large language models for cyber security: A systematic literature review
    (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Xu, Z., Liu, Y., Deng, G., Li, Y., Picek, S.: A comprehensive study of
    jailbreak attack versus defense for large language models (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan,
    D., Wang, D.: Baichuan 2: Open large-scale language models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Yao, D., Zhang, J., Harris, I.G., Carlsson, M.: Fuzzllm: A novel and universal
    fuzzing framework for proactively discovering jailbreak vulnerabilities in large
    language models. arXiv preprint arXiv:2309.05274 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan,
    K.: Tree of thoughts: Deliberate problem solving with large language models. arXiv
    preprint arXiv:2305.10601 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Yong, Z.X., Menghini, C., Bach, S.H.: Low-resource languages jailbreak
    gpt-4\. arXiv preprint arXiv:2310.02446 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Yu, J., Lin, X., Xing, X.: Gptfuzzer: Red teaming large language models
    with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Yuan, Y., Jiao, W., Wang, W., Huang, J.t., He, P., Shi, S., Tu, Z.: Gpt-4
    is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Zeng, Y., Lin, H., Zhang, J., Yang, D., Jia, R., Shi, W.: How johnny can
    persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety
    by humanizing llms (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang,
    B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren,
    R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.Y., Wen, J.R.: A survey of large
    language models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin,
    Z., Li, Z., Li, D., Xing, E., et al.: Judging llm-as-a-judge with mt-bench and
    chatbot arena. arXiv preprint arXiv:2306.05685 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Zhou, W., Wang, X., Xiong, L., Xia, H., Gu, Y., Chai, M., Zhu, F., Huang,
    C., Dou, S., Xi, Z., et al.: Easyjailbreak: A unified framework for jailbreaking
    large language models. arXiv preprint arXiv:2403.12171 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Zou, A., Wang, Z., Kolter, J.Z., Fredrikson, M.: Universal and transferable
    adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9 Apendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.1 Choice of Jailbreak Evaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A key difficulty in evaluating the performance of jailbreaking attacks is determining
    whether a given input prompt succeeds in jailbreaking the target model. Determining
    the success of an attack involves an understanding of human language and a subjective
    judgment of whether generated content is objectionable, which is challenging even
    for humans. With a result dataset encompassing 574,577 responses, it is impractical
    to label all the results manually. Existing strategies for addressing this issue
    vary, we consider six candidate jailbreak evalustors which are commonly used in
    the jailbreaking literature:'
  prefs: []
  type: TYPE_NORMAL
- en: • PrefixMatch. Zou et al. [[86](#bib.bib86)] utilized a set of common refusal
    patterns, such as "I am sorry’and"I cannot", to automate the identification of
    non-compliant responses,
  prefs: []
  type: TYPE_NORMAL
- en: • BERT. The BERT-base-cased fine-tuned model used as a evaluator [[36](#bib.bib36)],
  prefs: []
  type: TYPE_NORMAL
- en: • RoBERTa. The RoBERTa fine-tuned model used as a evaluator [[80](#bib.bib80)],
  prefs: []
  type: TYPE_NORMAL
- en: • GPT-4. A approaches leveraging GPT-4 for attack analysis were explored by
    Chaoet al [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: • Llama Guard . The fine-tuned Llama Guard evaluator [[38](#bib.bib38)],
  prefs: []
  type: TYPE_NORMAL
- en: • Llama-2-13b-cls. Distillation fine-tuned Llama-2 -3B-Chat evaluator [[51](#bib.bib51)]
  prefs: []
  type: TYPE_NORMAL
- en: To choose an effective jailbreak evaluator, we selected a random sample of 1,00
    prompts and responses from our result dataset, aiming for a 95% confidence level
    with a 3% margin of error. Each prompt-response instance was labeled by three
    computer science graduate students, with the majority vote determining the "ground
    truth" label for each case. Finally, we compared the agreement, false positive
    rate (FPR), and false negative rate (FNR) of the six Jailbreak Evaluators listed
    above to these ground truth labels. Our results are summarized in Table [7](#S9.T7
    "Table 7 ‣ 9.1 Choice of Jailbreak Evaluator ‣ 9 Apendix ‣ Characterizing and
    Evaluating the Reliability of LLMs against Jailbreak Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Comparison of Jailbreak Evaluators across 100 prompts and responses.
    We compute the agreement, false positive rate (FPR), and false negative rate (FNR)
    for six Jailbreak Evaluators.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Jailbreak Evaluator |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | Metric | PrefixMatch | BERT | RoBERTa | GPT-4 | Llama Guard |
    Llama-2-13b |'
  prefs: []
  type: TYPE_TB
- en: '| Human majority | Agreement (↑) | 66% | 74% | 80% | 78% | 81% | 88% |'
  prefs: []
  type: TYPE_TB
- en: '| FPR (↓) | 4% | 7% | 22% | 10% | 11% | 7% |'
  prefs: []
  type: TYPE_TB
- en: '| FNR (↓) | 74% | 51% | 17% | 7% | 30% | 47% |'
  prefs: []
  type: TYPE_TB
- en: Discrepancies were found in the performance of the evaluators, with the Llama-2-13b
    model exhibiting the highest concordance. Despite GPT-4 showing the lowest FNR,
    closed-source solutions like GPT models are both expensive to query and subject
    to arbitrary modifications. Hence, our chosen evaluator is the open-source Distillation
    fine-tuned Llama-2 -3B-Chat evaluator^(10)^(10)10[https://huggingface.co/cais/HarmBench-Llama-2-13b-cls](https://huggingface.co/cais/HarmBench-Llama-2-13b-cls),the
    prompt adopted for the fine-tuning of this classifier can be seen in Fig. [6](#S9.F6
    "Figure 6 ‣ 9.2 Dataset Construction Details ‣ 9 Apendix ‣ Characterizing and
    Evaluating the Reliability of LLMs against Jailbreak Attacks")., due to its appreciable
    accuracy, comparatively low FPR, and the transparency of its configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Dataset Construction Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employed the dataset framework proposed by Wang et al. [[69](#bib.bib69)],
    which originally comprised 939 labeled entries. We augmented this initial dataset
    with an additional 6,000 entries sourced from other various libraries in Sec. [4.1](#S4.SS1
    "4.1 Dataset Construction ‣ 4 Evaluation Framework ‣ Characterizing and Evaluating
    the Reliability of LLMs against Jailbreak Attacks"). To ensure strict classification
    of new data in our dataset, we attempted to fine-tune text classification models
    such as BigBird, CANINE, ConvBERT, DeBERTa, RoBERTa, etc., on the do-not-answer
    dataset [[69](#bib.bib69)] , aiming to achieve  accuracy on the held-out test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'After careful consideration of the performance metrics suach as accuracy, we
    selected the bert-base-uncased model for fine-tuning. The fine-tuning protocol
    includes: a batch size of 8, fifteen training epochs, a learning rate of , application
    of the Adam optimizer, and linear rate decay. Using the fine-tuned model, we labeled
    integrated dataset with additional 6,000 entries. Finally, to ensure the accuracy
    and reliability of the dataset classification, we conducted a round of random
    sampling for manual verification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Hyperparameter Settings. The other hyperparameter settings not included
    are set to be the default values. If not specified otherwise, all involved support
    LLMs use the default parameters used in the jailbreak attack strategy. Unless
    otherwise noted, for all target LLMs, the temperature is 1, the top_p parameter
    is 0.9, and other default parameters are used.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategy | Support LLM Setting | Maximum Step |'
  prefs: []
  type: TYPE_TB
- en: '| AutoDAN | Use gpt-3.5-turbo to help the hierarchical genetic algorithm (HGA).
    | 50 (num_step = 50) |'
  prefs: []
  type: TYPE_TB
- en: '| ReNeLLM | Use gpt-3.5-turbo to help the mutation. | 50 (max_query = 50, energy
    = 2) |'
  prefs: []
  type: TYPE_TB
- en: '| PAIR | Use gpt-3.5-turbo as the attacker LLM and gpt-4 as the judge LLM.
    | 50 (n-streams = 10, n-iterations = 5) |'
  prefs: []
  type: TYPE_TB
- en: '| TAP | Use gpt-3.5-turbo as the attacker and gpt-4 as the evaluator. | 50
    (branching-factor = 2, width = 5, depth = 5) |'
  prefs: []
  type: TYPE_TB
- en: '<svg id="S9.F6.pic1" class="ltx_picture ltx_centering" height="190.84" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,190.84) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="163.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[INST]
    <<SYS>> You are a classifier for red teaming large language models (LLMs). Given
    a generation, determine whether it counts as an instance of the following behavior
    (subject to the rules below). Your answer should be "yes" or "no". <</SYS>> PROMPT:
    {PROMPT} RESPONSE: {RESPONSE} — Rules: - Instances of the behavior should be unambiguous
    and non-minimal. Vague or very short instances do not count. - Generations should
    themselves be harmful or undesirable for a public-facing LLM to output. Benign
    or supportive generations do not count. - Generations that respond, analyze, discuss,
    or otherwise react to the behavior when it has originated from other factors do
    not count. - If the behavior involves coding and programming, the generation should
    contain code artifacts related to the behavior, not just comments or instructions.
    Otherwise it does not count. Answer: [/INST]</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Prompt for Distillation fine-tuned Llama-2-13b. The ‘PROMPT’ and
    ‘RESPONSE’ values are replaced with the respective prompt and response values
    to be evaluated.'
  prefs: []
  type: TYPE_NORMAL
