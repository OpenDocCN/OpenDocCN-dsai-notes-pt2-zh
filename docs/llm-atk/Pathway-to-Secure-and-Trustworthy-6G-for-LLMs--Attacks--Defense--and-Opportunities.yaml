- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.00722](https://ar5iv.labs.arxiv.org/html/2408.00722)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sunder Ali Khowaja , Parus Khuwaja, Kapal Dev , Hussam Al Hamadi, and Engin
    Zeydan Sunder Ali Khowaja is with School of Computing, Faculty of Computing, Digital
    and Data, Technological University Dublin, and CONNECT Centre, Ireland. Email:
    sunderali.khowaja@tudublin.ieParus Khowaja is with Institute of Business Administration,
    University of Sindh, Jamshoro. (e-mail:Parus.khuwaja@usindh.edu.pk).Kapal Dev
    is associated with CONNECT Centre and Department of Computer Science and Munster
    Technological University, Bishopstown, Cork, T12 P928, Ireland,e-mail: (kapal.dev@ieee.org)Hussam
    Al Hamadi with College of Engineering and IT University of Dubai, e-mail: (Halhammadi@ud.ac.ae)Engin
    Zeydan with Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona,
    Spain, 08860, e-mail: (ezeydan@cttc.es)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, large language models (LLMs) have been gaining a lot of interest due
    to their adaptability and extensibility in emerging applications, including communication
    networks. It is anticipated that 6G mobile edge computing networks will be able
    to support LLMs as a service, as they provide ultra reliable low-latency communications
    and closed loop massive connectivity. However, LLMs are vulnerable to data and
    model privacy issues that affect the trustworthiness of LLMs to be deployed for
    user-based services. In this paper, we explore the security vulnerabilities associated
    with fine-tuning LLMs in 6G networks, in particular the membership inference attack.
    We define the characteristics of an attack network that can perform a membership
    inference attack if the attacker has access to the fine-tuned model for the downstream
    task. We show that the membership inference attacks are effective for any downstream
    task, which can lead to a personal data breach when using LLM as a service. The
    experimental results show that the attack success rate of maximum 92% can be achieved
    on named entity recognition task. Based on the experimental analysis, we discuss
    possible defense mechanisms and present possible research directions to make the
    LLMs more trustworthy in the context of 6G networks.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The emergence of attention networks has been a stepping stone for transformer
    architectures, which also led to the introduction of large language models (LLMs).
    More recently, LLMs are seen as the most significant advance in the field of artificial
    intelligence (AI) and a potential pathway to artificial general intelligence (AGI)
    [[1](#bib.bib1)]. Every tech giant is in a race to advance in the field of LLMs
    by leveraging generative AI (GAI). Notable examples of LLMs from the tech giants
    are GPT-4 from OpenAI, LLaMA-3 from Meta and PALM from Google. However, there
    are also new players in this field that surpass the performance of the LLMs mentioned
    above. These include Mistral (in collaboration with NVIDIA), DCLM from Apple,
    xLAM from Salesforce, v2 chat from Deepseek, Groq, Claude, SmolLM and many more.
    These LLMs are trained on diverse and large amounts of datasets scraped or curated
    from the Internet. Some LLMs focus on increasing model size, such as GPT, while
    others find new ways to improve the generalization of LLMs through data curation,
    model quantization, and innovative techniques. Examples of such LLMs are Claude,
    LLaMA, DCLM and Groq, which have recently outperformed GPT on various language
    tasks. In continuation of the above-mentioned advances in LLMs, several enterprises
    are leveraging pre-trained encoders of LLMs to varying degrees to develop their
    own customized solutions for various applications and sectors, including healthcare,
    education, law and industrial automation. In view of the rapid development of
    LLMs, it can be assumed that LLMs will soon also be deployed on edge and handheld
    devices. Several studies have indicated that the current iteration of 5G networks
    will not be able to support a plethora of services offered by LLMs. Therefore,
    researchers are working intensively on the next iteration of communication systems,
    i.e. Sixth generation (6G), to meet the above requirements [[2](#bib.bib2)]. Furthermore,
    as AI is an integral part of 6G systems, it is assumed that LLMs will be used
    intrinsically to optimize resources and performance while enabling human-centric
    customized services to users.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a62c97f6d3f9cfb77c4353c09383ef1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Network for LLMs illustration in 6G with examples of smart homes
    and emergency services. The LLMs can be used by the central cloud and shared with
    the 6G Edge Cloud. The 6G edge cloud then share either the parameters to the radio
    access networks or users to fine-tune the network for personalization, or it shres
    the cached version of the model to provide a specific service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standardization process towards 6G systems is already progressing steadily.
    It is assumed that the evolution of the communication system will support distributed
    AI both for edge devices and within the mobile network [[2](#bib.bib2)]. Although
    many researchers argue that the edge devices will not support the use of LLMs,
    but with continuous breakthroughs in the field of AI, support for edge devices
    can be extended through distributed learning techniques such as federated learning
    (FL) and split learning (SL) [[3](#bib.bib3)]. In addition, quantization and training
    can be used to fine-tune an LLM on the edge devices. As proposed in [[4](#bib.bib4)],
    an LLM with 65 billion parameters can be fine-tuned with quantized low-rank adapters
    (QLoRA) on a downstream task within a single day, achieving comparable performance
    compared to other state-of-the-art (SOTA) LLMs. It can be assumed that the convergence
    of quantized networks, LLMs and 6G Multi-Access Edge Computing (MEC) could result
    in many innovative applications. Researchers have already begun to explore the
    mutual convergence of LLMs and 6G MEC networks, calling them “LLMs for networks”
    and “networks for LLMs” respectively. We illustrate a network for LLMs that corresponds
    to the vision of the Network for AI (NetAI) with respect to a 6G communication
    system in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Pathway to Secure and
    Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities"). The NetAI vision
    supports LLM deployments related to the MEC architecture [[2](#bib.bib2)]. Various
    services such as smart homes, healthcare, education, emergencies, mission-critical
    applications and finance can be supported with the network for LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Most research today focuses on the integration of LLMs and communication networks,
    which would undoubtedly bring unprecedented advances and technological innovation.
    However, one aspect of this technological progress is being overlooked, namely
    the security aspect. With all the possibilities and potential of LLMs and the
    6G ecosystem, we have to ask ourselves, are LLMs trustworthy? Despite their ability
    to fine-tune to the downstream task, LLMs are deep neural networks that are vulnerable
    to privacy attacks, such as model inversion, model poisoning and membership leakage
    [[3](#bib.bib3), [5](#bib.bib5)]. The growing landscape of LLMs and their integration
    into communication systems therefore makes it necessary to address security concerns
    and the development of trustable AI encoders to safeguard the integrity of users
    and services in 6G systems. To the best of our knowledge, the studies have not
    explored the security vulnerabilities in network for LLMs (Net4LLMs), which subsequently
    leaves us defenseless against such attacks as we progress towards the Net4AI vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the above problem, in this paper, we propose to audit the trustworthiness
    of pre-trained AI encoders for membership leakage attacks. The membership leakage
    attack is an attack in which the adversary tries to find out the distribution
    of the training data used to train the AI encoders. Considering that the Net4LLMs
    will focus on fine-tuning the pre-trained AI encoders for service provisioning,
    the attacker will aim to determine if a data sample was used for the fine-tuning
    process. The fine-tuning process enables the replacement of task-specific layer
    in the pre-trained AI encoder to meet tasks such as questions and answers, name
    entity recognition and classification [[5](#bib.bib5), [6](#bib.bib6)]. We evaluate
    the trustworthiness of AI encoders against the membership inference attacks. We
    assume that the adversary has particularly the knowledge of the downstream task
    and the adversary is provided with the fine-tuned model, also known as black-box
    setting. We conduct experiments to evaluate the trustworthiness of the AI pre-trained
    encoders and to develop possible defenses to prevent the membership leakage attacks
    in the context of Net4LLMs. The specific contributions of this work are characterized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the first study to investigate the trustworthiness of pre-trained AI
    encoders for Net4LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Membership leakage attacks in the context of Net4LLMs are explored to assess
    trustworthiness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on experimental analysis, defenses are proposed to audit trust in Net4LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the paper, open issues, challenges and future directions are also
    proposed to prevent potential adversarial attacks on LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ccaa024b027ef48c7f45752fdf4146aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Training, Fine-tuning, and customization strategies for LLMs in MEC
    6G framework.'
  prefs: []
  type: TYPE_NORMAL
- en: II Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, we have observed a plethora of advances in the field of LLMs that
    would be a revolution in the field of communication networks, especially in the
    design and development of 6G networks. Some studies have already explored and
    demonstrated the significance of LLMs for potential 6G applications. For example,
    the study in [[7](#bib.bib7)] proposed NetGPT, which enables personalized services
    to users through generative networks while handling comprehensive network intelligence
    and cloud collaboration in real time.Xu et al. [[8](#bib.bib8)] focused on the
    data privacy in 6G communication systems using LLMs. The study proposed to design
    LLM agents based on the principle of split learning by distributing LLMs for different
    roles across edge devices to make user interaction efficient and collaborative.
    Their results show that the split learning setting was effective in improving
    the communication efficiency while offloading the tasks that are complex in nature
    to the servers for constructing global LLMs. The study in [[9](#bib.bib9)] emphasised
    that the newer LLMs must offer multimodal services, i.e. they must handle image,
    text and audio data in order to offer automated services. Therefore, the deployment
    of LLM agents in the cloud could pose challenges in terms of data privacy, high
    bandwidth costs and long response times. However, MEC based on 6G communication
    systems can address the above problems in an effective way. Lin et al. [[10](#bib.bib10)]also
    proposed a split learning framework for the deployment of LLMs in 6G networks.
    However, their work focused on the efficiency and effectiveness of LLMs in terms
    of parameter sharing, quantization, and efficient fine-tuning rather than data
    or model security. Nguyen et al. [[5](#bib.bib5)] highlighted the advantages of
    using LLMs in 6G networks while exploring the security vulnerabilities from an
    adversarial point of view. However, the discussion of model security was very
    abstract and brief, focusing more on the attacks on services. The study also suggested
    the use of blockchain technology to avoid the security threats associated with
    LLMs and 6G networks. To the best of our knowledge, none of the studies investigated
    a specific model-based attack scenario related to pre-trained AI encoders and
    6G networks.
  prefs: []
  type: TYPE_NORMAL
- en: III Training/Fine-tuning LLMs in 6G
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training strategies for fine-tuning LLMs in 6G networks are shown in Figure
    [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Pathway to Secure and Trustworthy 6G for
    LLMs: Attacks, Defense, and Opportunities"). The strategies are presented in accordance
    with the MEC framework. The cloud layer can leverage the pre-trained AI encoders
    of LLMs for any of the training modes, but the downstream task would mostly be
    generalized when passed to the edge and user layers. However, at the edge and
    user layer, the customization of the LLMs can be done based on the context and
    resources. As can be seen from the training modes, fine-tuning the pre-trained
    encoder and classifier requires large amounts of computational resources. The
    computational resources decrease significantly when moving to the edge and user
    layer. In this regard, the edge and user layers can at most fine-tune/adapt the
    LLMs based on their application by keeping the pre-trained encoder frozen and
    using the final layers to update the parameters. Alternatively, they can simply
    freeze the entire LLM and extract only the output embeddings that are used as
    representative features for training deep neural networks or shallow learning
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this can be found in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities")
    for an emergency service application where LLM’s pre-trained AI encoder can be
    used and its parameters frozen while a limited amount of labeled data is used
    to train the classifier or the final layers. This would be beneficial for edge
    devices to fine-tune the network locally to improve communication efficiency while
    utilizing computing resources efficiently. As proposed in [[10](#bib.bib10)],
    the customization can be achieved through a split learning strategy, where the
    cloud wants to fine-tune the LLM for a specific task with respect to the communication
    system and splits the network into smaller networks that are trained with devices
    from the edge layer and the user layer. Some specific techniques such as Low Ranking
    Adaptation (LoRA), Quantized LoRA (QLoRA), Parameter Efficient Fine-Tuning (PEFT),
    Deep Speed and ZeRO can be used to fine-tune the LLMs. LoRA uses low-rank approximations
    to fine-tune the LLMs for specific tasks, resulting in reduced financial and computational
    costs. QLoRA further reduces memory utilization while fine-tuning LLMs with the
    LoRA technique. PEFT adjusts key parameters and uses the catastrophic forgetting
    technique to fine-tune LLMs with a small subset of parameters. ZeRO uses memory
    optimization and data parallelism techniques to fine-tune LLMs, and DeepSpeed
    uses the ZeRO redundancy optimizer to fine-tune LLMs in a distributed learning
    fashion. The above techniques can be used extensively at the user, edge and cloud
    layers to fine-tune LLMs for various purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Security Issues in LLMs and 6G
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several studies have now highlighted the security concerns related to the behavior,
    architecture and design of LLM. The security concerns arise from the complexity
    of LLMs and the challenges associated with their deployment and training process.
    In addition, backdoor attacks are possible in LLMs that cannot be overcome with
    conventional security measures. These backdoor attacks are applicable to LLMs
    that are fine-tuned in a supervised manner and trained with adversarial learning
    or reinforcement learning. The different types of attacks in LLMs deployed within
    6G networks are defined below.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adversarial attacks: These attacks are carried out by manipulating data to
    affect the performance of the model. Adversarial attacks can generally be divided
    into backdoor attacks and poisoning attacks. In the former, a trigger is hidden
    in the model to manipulate the inference behavior, while in the latter, malicious
    examples are injected into the training process to deceive the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inversion attacks: Inversion attacks are performed to reconstruct the data
    or to extract certain information from the model gradients. Inversion attacks
    include replicating the model, extracting training data, gradient leakages, feature
    space and stealing models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfair exploitation and bias attack: This type of attack is related to the
    training data used to train or fine-tune the LLMs. The attack disproportionately
    adds data with a particular label to fine-tune or train the network so that the
    inference perpetuates biases and unintentionally learns to generate misinformation,
    social inequalities, reinforcement of stereotypes and discrimination in the generation
    of responses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instruction tuning attacks: These attacks aim to overload the system’s resources
    in order to carry out inadvertent actions. Examples of such attacks are Denial
    of Service (DoS), indirect prompt injection, jailbreaking and the disclosure of
    guided prompts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero-day attacks: These attacks are usually called sleeper agents because they
    are embedded with model weights when a particular defense method fails to eliminate
    them. This type of attack is usually triggered by specific events or phrases.
    One example of such attacks is data theft.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inference attacks: Last but not least, inference attacks aim to extract sensitive
    information from the model, especially in the context of the training data used
    to fine-tune a model. Examples of such attacks are attribute inference and membership
    inference attack. In this paper, we focus on the membership inference attack as
    it can identify specific data used to train or fine-tune the model. Such information
    can be used to break the trust and confidentiality of the AI model and be used
    against the user. Other consequences of membership inference attack include breach
    of confidentiality, unauthorized access, identity theft and violation of privacy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV Trustable AI encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we provide the information about the threat model, the attack
    scenario, the datasets used for the attack and the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Threat Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we define the threat model, we make some assumptions. We assume that
    the LLM is pre- trained on a large dataset capable of transforming the input (text)
    into embeddings. Using the pre-trained AI encoder, a downstream task is fine-tuned
    by a customized dataset for a specific application in 6G networks using optimization
    algorithms and a predefined loss function. The fine-tuned model is then able to
    transform the input into embeddings or classification probabilities accordingly
    (which differ from the original, pre-trained AI encoder). We define the attacker’s
    purpose in this scenario as a dichotomous classification problem, where the goal
    is to determine whether the input provided to the pre-trained AI encoder is a
    member or non-member of the training dataset used for the subsequent task. In
    general, existing studies assume two dimensions of an attacker’s background knowledge
    when considering membership inference attacks.
  prefs: []
  type: TYPE_NORMAL
- en: The first dimension assumes a black box attack, which means that the attacker
    has no prior knowledge of the pre-trained AI encoder architecture, but the attacker
    has access to the model that has been trained for the downstream task. This is
    considered the most realistic scenario, as in 6G AI is used as a service and the
    models adapted for the downstream task would be directly available to the public.
    The second dimension assumes that the attacker has access to a very small subset
    of the member training data, which can be used to create an auxiliary dataset.
    The auxiliary dataset can then be used to train the attack model. Studies have
    shown that such assumptions can be true if one infers the location and makes an
    educated guess about the service used in a particular area [[11](#bib.bib11)].
    With the large plethora of diverse data available on the Internet, it is reasonable
    to assume that the attacker can gather meaningful data to create a shadow model
    for lodging membership inference attack that corresponds to a real-world environment.
    Considering the two dimensions, we assume in this study that the attacker has
    access to the downstream task model and has some knowledge of the application,
    which is taken into account by the pre-trained AI encoder of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Attack Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is known that the LLM’s pre-trained AI encoder can be used for feature extraction,
    i.e. the LLM’s task of transforming the input into embedding vectors. The mapping
    of inputs to embeddings benefits the fine-tuning of LLMs or training with deep
    neural networks for a specific task. However, when the LLM is fine-tuned with
    the new data for a particular downstream task, it tends to memorize the data during
    the training process. The memorization suggests that the member data will have
    higher confidence values compared to the non-member data. Therefore, it can be
    deduced that: (i) Pre-trained AI encoders of LLMs behave differently to the member
    and non-member data. (ii) The behavior is propagated to the embedding vectors
    that are learned during fine-tuning for the downstream tasks, so that the memorization
    of model will be a part of the downstream model available to the attacker. We
    intend to use the above features to categorize whether the data is a member or
    a non-member of the pre-trained AI encoder. We then apply the following steps
    to evaluate the effectiveness of the attack for a pre-trained AI encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our assumptions are that the attacker has some prior knowledge of the application
    for which the LLM is fine- tuned. Therefore, the attacker scraps the Internet
    or uses publicly available datasets to create an auxiliary dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attacker then prepares the auxiliary dataset for training by assigning pesudolabels
    to the data as members and non-members. The attacker then feeds the pseudo-labeled
    auxiliary dataset into the downstream model. The training process is then performed
    to create an attack model that is capable of binary classification, i.e. categorizing
    the data into members and non-members.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the attack model is trained, the attacker can enter the candidate text
    into the attack model to determine whether the candidate text is a member or a
    non-member.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-C Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this work, we use two state-of-the-art pre-trained language models RoBERTa
    [[12](#bib.bib12)] and ALBERT [[13](#bib.bib13)] for our experiments. The two
    language models differ in their training schemes, loss functions and architectures.
    It should be noted that we have not trained these language models from scratch,
    but that we use the pre-trained language models for the attack scenario that are
    publicly available online¹¹1https://huggingface.co/models. According to the assumption
    considered for the attack scenario, the attacker has access to the fine-tuned
    model for the downstream task. Therefore, we consider two publicly available datasets,
    i.e., Yelp Review/AG’s News/SST [[14](#bib.bib14)] and CoNLL2003 [[15](#bib.bib15)].
    The first data set is intended for the task of text classification, while the
    second takes into account the task of Named Entity Recognition (NER). To perform
    the membership inference attack, we use a small portion of the Yelp Review/AG’s
    News/SST and CoNLL2003 dataset, i.e., 0.15%, of each dataset to construct the
    auxiliary dataset and label it as member data. We also consider other third-party
    datasets such as AX, CoLA and IMDB for the auxiliary dataset and label them as
    non-member data.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform membership inference attack, we design a five-layer multilayer perceptron
    as an attack model that uses the output of the model fine-tuned to the downstream
    task as input. The dimensions of the first layer vary depending on the model fine-tuned
    to a specific downstream task. Recall, precision and F1 score are used as evaluation
    metrics for the performance of the attack. The attack model is trained using the
    ADAM optimizer with a learning rate of $1e-5$. The model is trained for 100 epochs.
    The auxiliary dataset was divided into two sets, i.e. a test dataset and a training
    dataset in a ratio of 1:5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17cfe41587cc17ccc67f2d9a311f7f6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Membership Inference Attack performance on CoNLL2003, Yelp, AG’s
    News and SST downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1acb128e32e9ed81555739a557cb786b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Membership Inference Attack performance on CoNLL2003, Yelp, AG’s
    News and SST downstream tasks when the attack model employs only either of the
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0c5849177e9fa604e5d3b3ea4d547d8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Membership Inference Attack performance on Yelp downstream tasks
    when varying number of classes.'
  prefs: []
  type: TYPE_NORMAL
- en: V Experimental analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Figure [3](#S4.F3 "Figure 3 ‣ IV-D Implementation Details ‣ IV Trustable
    AI encoders ‣ Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense,
    and Opportunities"), we show the performance of the membership inference attack.
    It should be noted that Yelp Review/AG’s News/SST are classification tasks with
    5/4/2 classes, respectively. The baseline, random guessing, refers to the value
    of  in the SST task and a maximum F1 score of 0.94 in the NER task. The results
    are significantly higher than random guessing, indicating that the membership
    leak exists in the pre-trained AI encoders.'
  prefs: []
  type: TYPE_NORMAL
- en: The success rate of the attack raises serious concerns about the trustworthiness
    of LLMs and pre-trained models in 6G networks. We repeat the above experiment
    with a reduced auxiliary dataset, i.e., we use Yelp Review/AG’s News/SST for training
    the attack model without considering CoNLL2003 and vice versa to observe the results.
    The results for this experiment are shown in Figure 4\. It can be seen that the
    performance is still above the random guess. Furthermore, the performance degradation
    is about 0.12/0.096 for CoNNL2003, about 0.07/0.095 for Yelp Review, about 0.06/0.075
    for AG’s News and about 0.105/0.074 for SST, using ALBERT and RoBERTa, respectively.
    The attack performance still reaches a maximum of 0.83 F1 score if the attacker
    only has access to the fine-tuned model for the downstream task, but does not
    use the part of the same datasets. This behavior confirms our assumption that
    the pre-trained language models memorize the data and behave differently with
    member and non-member data.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting aspect was highlighted when we examined the attack performance
    while varying the number of classes. Since the Yelp dataset has the highest number
    of classes out of the datasets we selected, namely 5, we varied the number of
    classes to observe the attack performance. The results of this experiment are
    shown in Figure 5\. It can be seen that the attack performance increases as the
    number of classes increases. This is very interesting because it shows that the
    membership inference attack can extract more information from data with a higher
    number of categories. It also shows why the attack performance on Yelp was better
    than on AG’s News and SST datasets, accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: VI Enabling Trust with Pre-trained AI encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Membership leakage and membership inference attacks have been extensively studied
    in the context of computer vision and image modality. Defenses against such attacks
    therefore include adversary regularization, differential privacy, data augmentation,
    adding noise to images, intentional attacks, encryption techniques, and others
    [[3](#bib.bib3), [11](#bib.bib11)]. Some of the above techniques are difficult
    to perform in textual modality, such as addition of noise and intentional attack
    initialization. Such actions can also degrade the performance of LLMs in 6G networks.
    Adding noise to either the data or confidence scores for classification can be
    used as a defense mechanism. However, studies suggest that such techniques degrade
    the performance of the downstream task [[5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: Based on the observations gathered from our experiments, we propose two possible
    defenses. The first is to reduce the size of the dataset or reduce the number
    of epochs to fine-tune the network. The intuition is that if the size of the dataset
    or the number of epochs for fine-tuning in the downstream task is increased, the
    pre-trained AI encoder would tend to memorize the downstream task data and thus
    make the membership attack stronger. In this context, we suggest using either
    active learning or curriculum learning, which can perform the training with less
    data or fewer epochs.
  prefs: []
  type: TYPE_NORMAL
- en: The second defense is based on the intuition “confidence is defined by trust”
    (a quote from Patrick Mosher). In this case, however, we would look at confidence
    and trust from the perspective of AI. We propose to use a trust evaluation module
    at the edge layer that could evaluate the trust of the pre-trained AI encoder
    or a fine-tuned LLM with a predefined metric. One of the examples of such a trust
    evaluation is as follows, assuming the fine-tuned LLM is trained for medical emergency
    services using a 6G network.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM is fine-tuned on less number of epochs and a smaller amount of data.
    The responses do not ask for age or personal information. Results in 88% for the
    performance metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM is fine-tuned on a large amount of data and high number of epochs. The
    responses do not ask for age or personal information. Results in 89% for the performance
    metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM is fine-tuned on a large amount of data and high number of epochs. The
    responses asks for personal information. Yields 92% on performance metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given the scenario described above, a trust module might favor the first model
    as it is less vulnerable to membership leakage attack. The idea is simply to prioritize
    a fine-tuned LLM for 6G services based on trustworthiness and confidence scores.
  prefs: []
  type: TYPE_NORMAL
- en: VII Open Issues, Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The integration of 6G and LLMs can be seen as task-oriented communication services,
    where integration is achieved by utilizing resources from the communication infrastructure,
    the edge and mobile devices. In return, users receive LLM agents that can perform
    certain actions, generate data or call application programming interface (API)
    functions. As already indicated, such integration can lead to security vulnerabilities,
    including the theft of personal information and more. We have emphasised the importance
    of a trust module for considered integration, but designing such a trust module
    can present some challenges. These challenges include, but are not limited to,
    the following.
  prefs: []
  type: TYPE_NORMAL
- en: '(i) Active Learning and Curriculum Learning approaches: One of the ways to
    cope with membership leakage or inference attack is to use less amount of data
    and number of epochs for fine-tuning. In this regard, two approaches can be opted
    for making this possible. The design of trust module needs to favor the model
    with aforementioned characteristics, however, design of such methods can be a
    challenge that could help in resisting the attack while not compromising on the
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) Multimodal LLMs: In this paper, we have focused on the LLMs that only
    work with text data. In reality, users opt for multimodal LLMs that can generate
    text, images and audio. Each modality has its own security issues when it comes
    to pre-trained AI encoders. However, it would be quite a challenge to design a
    trust module for 6G and LLMs that is suitable for different data modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: '(iii) User Privacy: Similar to the design of trust modules for different data
    modalities, training processes, architectural modifications, and encryption techniques
    must be used to improve user privacy. Various attacks such as model inversion,
    model poisoning, gradient leakages, and adversarial attacks can be used by attackers
    to disrupt the services and steal users’ private information in 6G networks. Therefore,
    in addition to data modality, the trust module must also defend against various
    privacy attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '(iv) Latency and Bandwidth Issues: We focused primarily on the trustworthiness
    of the pre-trained AI encoders. However, the fine-tuning of the LLMs and the deployment
    of the trust module could burden the services in terms of latency and increased
    bandwidth. In this context, the selection of suitable models for specific applications,
    scenarios and needs as well as optimization during deployment must also be researched.'
  prefs: []
  type: TYPE_NORMAL
- en: '(v) Responsible AI: Finally, this study explores the trust module in the context
    of security and privacy. The trust module can be explored in the context of responsible
    use of AI so that hallucinations in the generation of data modality could be controlled
    or restricted to prevent the spread of misinformation, identity-related attacks
    and impersonation.'
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article explores the use of LLM deployment in accordance with the 6G MEC
    framework. We evaluate the trustworthiness of fine-tuned models to be deployed
    as services in 6G networks. We discuss in detail about membership leakage and
    inference attacks with respect to the textual modality and show that the attacks
    are quite effective in violating user privacy. We propose possible defense mechanisms
    to cope with the membership inference attacks and give several open issues, challenges,
    and research directions for the development of a generalization trust module for
    LLM deployment in the 6G-MEC framework. We believe that this paper provides researchers
    with the foundation for developing trustworthy LLMs for services in 6G networks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, “A Survey on Language Model based
    Autonomous Agents,” *Frontiers of Computer Science*, vol. 18, no. 6, p. 186345,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] W. Tong and P. Zhu, “6G: The Next Horizon From Connected People and Things
    to Connected Intelligence,” Huawei Technology, Tech. Rep., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. A. Khowaja, K. Dev, N. M. F. Qureshi, P. Khuwaja, and L. Foschini, “Toward
    industrial private ai: A two-tier framework for data and model security,” *IEEE
    Wireless Communications*, vol. 29, no. 2, pp. 76–83, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLoRA: Efficient
    Finetuning of Quantized LLMs,” in *Advances in Neural Information Processing Systems*,
    vol. 36, 2023, pp. 10 088–10 115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. Nguyen, H. Nguyen, A. Ijaz, S. Sheikhi, A. V. Vasilakos, and P. Kostakos,
    “Large language models in 6g security: challenges and opportunities,” 2024\. [Online].
    Available: [https://arxiv.org/abs/2403.12239](https://arxiv.org/abs/2403.12239)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and S. Zanella-Béguelin,
    “Analyzing leakage of personally identifiable information in language models,”
    in *2023 IEEE Symposium on Security and Privacy (SP)*, 2023, pp. 346–363.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Chen, R. Li, Z. Zhao, C. Peng, J. Wu, E. Hossain, and H. Zhang, “Netgpt:
    An ai-native network architecture for provisioning beyond personalized generative
    services,” *IEEE Network*, vol. Early Access, pp. 1–10, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. Xu, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, D. I. Kim, and K. B.
    Letaief, “When large language model agents meet 6g networks: Perception, grounding,
    and alignment,” 2024\. [Online]. Available: [https://arxiv.org/abs/2401.07764](https://arxiv.org/abs/2401.07764)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Long, F. Tang, Y. Li, T. Tan, Z. Jin, M. Zhao, and N. Kato, “6g comprehensive
    intelligence: network operations and optimization based on large language models,”
    2024\. [Online]. Available: [https://arxiv.org/abs/2404.18373](https://arxiv.org/abs/2404.18373)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, “Pushing large
    language models to the 6g edge: Vision, challenges, and opportunities,” 2024\.
    [Online]. Available: [https://arxiv.org/abs/2309.16739](https://arxiv.org/abs/2309.16739)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. A. Khowaja, P. Khuwaja, K. Dev, K. Singh, L. Nkenyereye, and D. Kilper,
    “Zeta: Zero-trust attack framework with split learning for autonomous vehicles
    in 6g networks,” in *2024 IEEE Wireless Communications and Networking Conference
    (WCNC)*, 2024, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining
    approach,” 2019\. [Online]. Available: [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert:
    A lite bert for self-supervised learning of language representations,” in *International
    Conference on Learning Representations*, 2020\. [Online]. Available: [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” in *Advances in Neural Information Processing Systems*,
    vol. 28.   Curran Associates, Inc., 2015, pp. 1–9\. [Online]. Available: [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li, “A unified MRC framework
    for named entity recognition,” in *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics*.   Association for Computational Linguistics,
    Jul. 2020, pp. 5849–5859\. [Online]. Available: [https://aclanthology.org/2020.acl-main.519](https://aclanthology.org/2020.acl-main.519)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
