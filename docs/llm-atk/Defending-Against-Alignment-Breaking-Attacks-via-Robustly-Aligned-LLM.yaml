- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.14348](https://ar5iv.labs.arxiv.org/html/2309.14348)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Bochuan Cao , Yuanpu Cao¹¹footnotemark: 1 , Lu Lin & Jinghui Chen'
  prefs: []
  type: TYPE_NORMAL
- en: The Pennsylvania State University
  prefs: []
  type: TYPE_NORMAL
- en: '{bccao,ymc5533,lulin,jzc5917}@psu.edu Equal Contribution'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, Large Language Models (LLMs) have made significant advancements and
    are now widely used across various domains. Unfortunately, there has been a rising
    concern that LLMs can be misused to generate harmful or malicious content. Though
    a line of research has focused on aligning LLMs with human values and preventing
    them from producing inappropriate content, such alignments are usually vulnerable
    and can be bypassed by alignment-breaking attacks via adversarially optimized
    or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned
    LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can
    be directly constructed upon an existing aligned LLM with a robust alignment checking
    function, without requiring any expensive retraining or fine-tuning process of
    the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM
    to verify its effectiveness in defending against alignment-breaking attacks. Through
    real-world experiments on open-source large language models, we demonstrate that
    RA-LLM can successfully defend against both state-of-the-art adversarial prompts
    and popular handcrafted jailbreaking prompts by reducing their attack success
    rates from nearly 100% to around 10% or less.
  prefs: []
  type: TYPE_NORMAL
- en: 'WARNING: This paper contains unsafe model responses. Reader discretion is advised.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 INTRODUCTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trained on a wide range of text data from the internet, Large Language Models
    (LLMs) have exhibited exciting improvement in their generalization capabilities
    (OpenAI, [2023](#bib.bib26); Touvron et al., [2023b](#bib.bib35)) and widespread
    application in various domains such as finance (Wu et al., [2023](#bib.bib39)),
    law (Nguyen, [2023](#bib.bib24)), and healthcare industry (Thirunavukarasu et al.,
    [2023](#bib.bib33)). While LLMs have showcased impressive potential, a rising
    concern is that they can also be maliciously utilized to generate content deviating
    from human values (e.g., harmful responses and illegal suggestions) (Hazell, [2023](#bib.bib11);
    Kang et al., [2023](#bib.bib16)) due to the substantial amount of undesirable
    material existing in their training data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca857b273c6dd81defcce9acc406f2a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of alignment-breaking attack: an aligned LLM gives
    unsafe responses to malicious requests with adversarial prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this issue, a line of research focuses on aligning LLMs with human
    preferences and preventing them from producing inappropriate content (Ouyang et al.,
    [2022](#bib.bib27); Bai et al., [2022](#bib.bib2); Go et al., [2023](#bib.bib10);
    Korbak et al., [2023](#bib.bib18)). These alignments typically adopt reinforcement
    learning from human feedback (Ouyang et al., [2022](#bib.bib27)) and AI feedback
    (Bai et al., [2022](#bib.bib2)) to fine-tune LLMs for alignments with human values.
    Despite these efforts, an emerging class of jailbreak attacks can still bypass
    the alignment and elicit harmful responses from LLMs (Yuan et al., [2023](#bib.bib45);
    Shen et al., [2023](#bib.bib30); Wei et al., [2023](#bib.bib36); Zou et al., [2023](#bib.bib50)).
    These alignment-breaking attacks manually craft adversarial prompts by designing
    elaborate role-play (Shen et al., [2023](#bib.bib30)) or simply asking the LLM
    to give the response starting with “Absolutely! Here’s” (Wei et al., [2022](#bib.bib37)).
    Moreover, automatic jailbreak prompt generation methods have also been developed
    through dialogue encryption (Yuan et al., [2023](#bib.bib45)) or the combination
    of greedy and gradient-based search methods (Zou et al., [2023](#bib.bib50)).
    Figure [1](#S1.F1 "Figure 1 ‣ 1 INTRODUCTION ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") shows an example that a malicious question
    appended with an adversarial prompt could successfully break the safety alignment.
    Recently, (Zou et al., [2023](#bib.bib50)) have demonstrated that jailbreak attempts
    could be highly effective and transferable across different LLMs. This phenomenon
    suggests that existing safety alignment is far from robust to defend against carefully
    crafted adversarial prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Till now, few attempts have been made to design dedicated mechanisms for defending
    alignment-breaking attacks. A rudimentary defense currently employed relies on
    external tools to re-assess the potential harm of the LLM responses. For instance,
    it could feed every potential response from the target LLM into a third-party
    LLM to determine whether the response is harmful or not (Helbling et al., [2023](#bib.bib12)).
    While this strategy enables filtering out possible harmful responses, there are
    several major drawbacks limiting its practicability: 1) Existing LLMs are very
    sensitive to harmful keywords appeared in the input, and have a high propensity
    to misclassify benign content as harmful, even when the entire sentence is not
    talking about any harmful behavior (e.g., stating news or providing guidance/warnings).
    This could lead to a high false-positive rate in harmful content detection; 2)
    The method heavily relies on the performance of the LLM used as a harmful discriminator,
    while the LLM itself is not designed to be an accurate harmful discriminator.
    The basis for its decisions remains ambiguous, implying that the harmful evaluation
    process could be opaque; 3) There are more types of alignment that can not be
    simply summarised as “harmful” (e.g., privacy, ethics, human values etc), thus
    this type of approach cannot cover such cases simultaneously. Given the wide range
    of applications where LLMs could be utilized, finding an effective and practical
    defense against potential alignment-breaking attacks is both urgent and challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we design a Robustly Aligned LLM (RA-LLM) to defend against potential
    alignment-breaking attacks, which is built upon an already aligned LLM and makes
    the existing alignments less prone to be circumvented by adversarial prompts.
    Specifically, our key idea is that although an aligned LLM can, to some extent,
    identify if the input request is benign or not, we cannot directly rely on that
    as it may not be robust. We consider an input request to be benign, only if we
    randomly drop a certain portion of the request and the LLM still thinks it is
    benign in most cases. Intuitively, such a random dropping operation would invalidate
    the adversarial prompts in alignment-breaking attacks, which are usually sensitive
    to small perturbations; on the other hand, the chances for the LLM to reject benign
    requests are relatively low, even after random dropping. Therefore, such a mechanism
    naturally leads to a robustly aligned LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Note that our RA-LLM does not require any external “harmful” detectors, instead,
    our strategy only relies on the existing alignment capability inside the LLM.
    Due to the same reason, our approach is not limited to any specific type of alignment
    (e.g., harmful), but robustifies all existing model alignments. Furthermore, we
    provide a theoretical analysis to verify the effectiveness of our proposed RA-LLM.
    Our experimental results on open-source large language models demonstrate that
    RA-LLM can successfully defend against both state-of-the-art adversarial prompts
    and popular handcrafted jailbreaking prompts by reducing their attack success
    rates from nearly 100% to around 10% or less.
  prefs: []
  type: TYPE_NORMAL
- en: 2 RELATED WORKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aligning LLMs with Human Preferences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Foundational large language models are pre-trained on extensive textual corpora
    (Brown et al., [2020](#bib.bib3); Chowdhery et al., [2022](#bib.bib4); Touvron
    et al., [2023a](#bib.bib34)), which equips LLMs with world knowledge and facilitates
    their deployment in professional applications. Despite their excellent performance,
    LLMs suffer from generating outputs that deviate from human expectations (e.g.,
    harmful responses and illegal suggestions) due to the significant amount of inappropriate
    content existing in unfiltered training data. To tackle this issue, a line of
    work focuses on aligning LLMs with human values (Xu et al., [2020b](#bib.bib41);
    Ouyang et al., [2022](#bib.bib27); Bai et al., [2022](#bib.bib2); Go et al., [2023](#bib.bib10);
    Korbak et al., [2023](#bib.bib18)). Specifically, Ouyang et al. ([2022](#bib.bib27))
    align the LLM by using reinforcement learning from human feedback (RLHF (Christiano
    et al., [2017](#bib.bib5); Stiennon et al., [2020](#bib.bib32))) to fine-tune
    pre-trained LLM with human preferences as the reward signal, which reduces the
    generation of toxic content. Bai et al. ([2022](#bib.bib2)) train a less harmful
    system through the specification of a short list of principles and further improve
    the human-judged performance by introducing chain-of-thought style reasoning (Wei
    et al., [2022](#bib.bib37)) in both supervised-learning and reinforcement-learning
    stage. Go et al. ([2023](#bib.bib10)) consider aligning LLMs as approximating
    a target distribution representing some desired behavior and accordingly propose
    a new framework for fine-tuning LLMs to approximate any target distribution through
    f-divergences minimization. In addition to aligning LLMs in the fine-tuning stage,
    Korbak et al. ([2023](#bib.bib18)) propose pertaining LLMs with alternative objectives
    that guide them to generate text aligned with human preferences and significantly
    reduce the rate of generating undesirable content by using conditional training
    (Keskar et al., [2019](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: Alignment-breaking Attacks and defenses in LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although various alignment strategies have been developed to steer LLMs to
    generate content complying with human ethical principles, an emerging class of
    alignment-breaking attacks (i.e., jailbreak attacks) can still bypass safeguards
    and elicit LLMs to generate harmful and toxic responses (Wolf et al., [2023](#bib.bib38);
    Li et al., [2023](#bib.bib20); Shen et al., [2023](#bib.bib30); Yuan et al., [2023](#bib.bib45);
    Wei et al., [2023](#bib.bib36); Zou et al., [2023](#bib.bib50)), which poses significant
    threats to the practical deployment of LLMs. In particular, inspired by traditional
    computer security, Kang et al. ([2023](#bib.bib16)) adapt obfuscation, code injection/payload
    splitting, and visualization attacks to the LLMs, leading to the generation of
    content containing hate speech, phishing attacks, and scams. Wei et al. ([2023](#bib.bib36))
    hypothesize that competing objectives and mismatched generalization are two failure
    modes of safety training in LLMs and craft effective jailbreak attacks by leveraging
    the two failure modes. Instead of manually crafting adversarial prompts, Zou et al.
    ([2023](#bib.bib50)) automatically produce transferable adversarial suffixes by
    using greedy and gradient-based search methods to maximize the probability of
    generating an affirmative response. Yuan et al. ([2023](#bib.bib45)) bypass the
    safety alignment through dialogue encryption. Shen et al. ([2023](#bib.bib30))
    systematically analyzes the characteristics of jailbreak prompts in the wild and
    presents that jailbreak prompts have evolved to be more stealthy and effective
    with reduced length, increased toxicity, and semantic shift. Note that some concurrent
    works also aim to defend against alignment-breaking attacks: Kumar et al. ([2023](#bib.bib19))
    provides a verifiable safety guarantee by enumerating all possible partially erased
    input and using a safety filter to identify the harmfulness of the input content.
    Jain et al. ([2023](#bib.bib13)) propose to detect adversarial prompts by checking
    if the perplexity of the prompt is greater than a threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Text Adversarial Attack and Defenses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traditional text adversarial attacks primarily focus on text classification
    tasks and aim to force target models to maximize their prediction error by adversarially
    perturbing original text (Ebrahimi et al., [2017](#bib.bib8); Jin et al., [2020](#bib.bib15);
    Li et al., [2018](#bib.bib21); Maheshwary et al., [2021](#bib.bib22); Ye et al.,
    [2023](#bib.bib44)). The adversarial perturbation could be crafted by performing
    character-level transformation (Gao et al., [2018](#bib.bib9)) or replacing original
    words with their synonyms while maintaining semantics and syntax similar (Alzantot
    et al., [2018](#bib.bib1)). The generation of adversarial examples could be categorized
    into the “white-box” setting and the “black-box” setting according to the extent
    of access to the target model (Xu et al., [2020a](#bib.bib40)). As a representative
    white-box method, HotFlip  (Ebrahimi et al., [2017](#bib.bib8)) uses the gradient
    information of discrete text structure at its one-hot representation to construct
    adversarial examples. In the black-box setting, Li et al. ([2018](#bib.bib21));
    Jin et al. ([2020](#bib.bib15)); Ren et al. ([2019](#bib.bib28)) leverage the
    prediction score distribution on all categories to craft adversarial text without
    the guidance of parameter gradients. Maheshwary et al. ([2021](#bib.bib22)) focus
    on a more realistic scenario where attackers only know the top-$1$ prediction
    and propose using population-based optimization to construct adversarial text.
    Ye et al. ([2022](#bib.bib43)) follow the same scenario and employ the word embedding
    space to guide the generation of adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: To defend against adversarial attacks, a body of empirical defense methods has
    been proposed. In particular, adversarial-training-based methods (Miyato et al.,
    [2016](#bib.bib23); Zhu et al., [2019](#bib.bib48)) incorporate adversarial perturbations
    to word embeddings and robustly train the model by minimizing the adversarial
    loss. Zhou et al. ([2021](#bib.bib47)); Dong et al. ([2021](#bib.bib7)) utilize
    adversarial data augmentation by replacing the original word with its synonyms
    to make the model robust to similar adversarial perturbations. These methods gain
    empirical success against adversarial attacks. To provide provable robustness
    against adversarial word substitutions, Jia et al. ([2019](#bib.bib14)) use certifiably
    robust training by training the model to optimize Interval Bound Propagation (IBP)
    upper bound. Shi et al. ([2020](#bib.bib31)) adopt linear-relaxation-based perturbation
    analysis (Xu et al., [2020c](#bib.bib42)) to develop a robustness verification
    method for transformers. Zeng et al. ([2023](#bib.bib46)) propose a certifiably
    robust defense method based on randomized smoothing techniques (Cohen et al.,
    [2019](#bib.bib6)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Our Proposed Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the proposed Robustly Aligned LLM for defending
    alignment-breaking attacks. Before heading into details, we first discuss the
    threat model that is focused on in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Threat Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alignment-breaking attack seeks to bypass the security checks of an aligned
    LLM by introducing adversarial prompts adhered to an original malicious question.
    Let  represent the adversarial prompt generated by the alignment-breaking attack.
    Let  denotes the insertion operation. While most existing attacks typically place
    the adversarial prompts at the end of the request Zou et al. ([2023](#bib.bib50)),
    we actually consider a more general case where the adversarial prompt could also
    be inserted in front of the malicious question or be integrated in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: We also assume that the target LLM  is directly input into the target LLM ,
    so that ${\mathbf{x}}_{\text{adv}}={\mathbf{x}}\oplus{\mathbf{p}}_{\text{adv}}$
    will mislead the LLM to provide an affirmative answer Zou et al. ([2023](#bib.bib50))
    to such a malicious question, e.g., “Sure, here is how to do [a malicious request]…”.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Our Proposed Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our motivation builds upon the fact that the target LLM has already been aligned
    and is able to reject commonly seen malicious requests. To be more specific, we
    can build an alignment check function : return Fail when detecting typical aligned
    text in the output of  is quite vague but we will provide more details on how
    to implement it in practice in Section [3.3](#S3.SS3 "3.3 Practical Designs ‣
    3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM").. Given the alignment check function $\text{AC}(\cdot)$, one can
    then construct a “hypothetical” LLM by'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f^{\prime}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{Reject the
    response},\text{ if }\text{AC}(f({\mathbf{x}}))=\text{Fail}\\ &amp;f({\mathbf{x}})\
    \ \ \ \ \ \ \qquad\qquad,\text{ if }\text{AC}(f({\mathbf{x}}))=\text{Pass}\end{aligned}\right.$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where . While  in practice, it showcases how one can construct a new aligned
    LLM using an alignment check function.
  prefs: []
  type: TYPE_NORMAL
- en: Robust Alignment Check Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One thing to notice here is that the previously defined alignment check function
    $\text{AC}(\cdot)$ only relies on the existing alignments inside on the target
    LLM. However, the existence of alignment-breaking attacks such as the adversarial
    prompts Zou et al. ([2023](#bib.bib50)) has proved that such alignment checking
    is not robust: it can be easily manipulated and circumvented by carefully designed
    perturbations or suffix prompts. Therefore, it is natural to think about how we
    can design a robust alignment check function that could strengthen the alignment
    check capabilities of an aligned LLM, without finetuning or modifying the model
    itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our intuition here is very straightforward: since the existing alignment check
    function  in most cases. To translate this requirement into mathematical formulations,
    we define the following Robust Alignment Check function  and the alignment check
    function $\text{AC}(\cdot)$ :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\text{RAC}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{Fail},\ \text{
    if}\ \text{AC}(f({\mathbf{x}}))=\text{Fail}\\ &amp;\text{Fail},\ \text{ if}\ \mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U(p)}(\text{AC}(f([{\mathbf{x}}]_{{\mathbf{r}}}))=\text{Fail})>t\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\text{Pass},\ \text{'
  prefs: []
  type: TYPE_NORMAL
- en: otherwise}\end{aligned}\right.$$ |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: where  refers to the distribution of possible masks after uniformly dropping  denotes
    the kept indices  after the dropping operation. Essentially, for an input , every
    possible  tokens indexed by ${\mathbf{r}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Eq. [2](#S3.E2 "In Robust Alignment Check Function ‣ 3.2 Our Proposed Method
    ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM") states that the robust alignment check function  to show no sign
    of being aligned but also requires the response after random dropping still shows
    no sign of being aligned in most cases. On the contrary, if ) of responses from
    the randomly dropped input fails to pass AC, .
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the robust alignment check function  with $\text{RAC}(\cdot)$ in Eq.
    ([1](#S3.E1 "In 3.2 Our Proposed Method ‣ 3 Our Proposed Method ‣ Defending Against
    Alignment-Breaking Attacks via Robustly Aligned LLM")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{\text{rob}}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{Reject
    the response},\text{ if }\text{RAC}(f({\mathbf{x}}))=\text{Fail}\\ &amp;f({\mathbf{x}})\
    \ \ \ \ \ \ \qquad\qquad,\text{ if }\text{RAC}(f({\mathbf{x}}))=\text{Pass}\end{aligned}\right.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: By this simple reconstruction of alignment check function, we can build a robustly
    aligned LLM without necessitating extra resources or retraining of the entire
    model. Figure [2](#S3.F2 "Figure 2 ‣ Robust Alignment Check Function ‣ 3.2 Our
    Proposed Method ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") illustrates the effect of our proposed RAC
    when facing malicious or benign requests.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/faba338b1036d1008d41ddb2ec1e40dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An illustration of our RA-LLM when facing malicious requests with
    adversarial prompts (Left) and benign requests (Right).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Practical Designs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithm 1 Robustly Aligned LLM
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: aligned LLM .'
  prefs: []
  type: TYPE_NORMAL
- en: 1:  if  do5:        Randomly sample a mask 7:     end for8:     if 12:     end if13:  end if
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s delve into the practical designs of our proposed robustly aligned
    LLM, which essentially approximates $f_{\text{rob}}(\cdot)$ mentioned above. The
    detailed steps of the constructed robustly aligned LLM are summarized in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.3 Practical Designs ‣ 3 Our Proposed Method ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM").
  prefs: []
  type: TYPE_NORMAL
- en: Approximation of $\text{AC}(\cdot)$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Previously, we vaguely defined the alignment check function  returns Fail; otherwise,
    it returns Pass. Note that we are only inspecting the prefix. For this purpose,
    we only need to generate a certain number of tokens (e.g., 10) for robust alignment
    checking. This could largely reduce our computational overhead²²2Further discussion
    on computational costs can be found in Section [4.5](#S4.SS5 "4.5 Computational
    Cost ‣ 4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM")..
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is practically infeasible to obtain the exact value for the probability of  indices
    masks to obtain  requests, and count the frequency of cases when the alignment
    check function $\text{AC}(\cdot)$ gives *Fail* decisions.
  prefs: []
  type: TYPE_NORMAL
- en: The Practical Choice of $t$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another important choice is the threshold  such that whenever  directly fails
    the request. However in practice, such a setting could be too extreme as the randomness
    introduced in the dropping operations might also affect the LLM response on benign
    inputs: random dropping might occasionally lead to the loss of essential information,
    and under such circumstances the LLM might also generate responses similar to
    the typical alignment responses. For example, “Do you like apples?” could become
    “Do you apples?” after random dropping, leading the LLM to express an inability
    for answering this unclear question. This could potentially be mis-detected as
    Fail by , it will lead to Fail by  as zero, we keep a relatively small threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we theoretically analyze the proposed robustly aligned LLM
    and see when it provides a more robust alignment compared to the original LLM
    when facing alignment-breaking attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Our theorem is based on the analysis on the robust alignment check function
    RAC. We will show that RAC is more robust for the aligned malicious text  of length
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider a malicious input  such that . Suppose  tokens and  tokens while  in  as
    the padded text constructed from  pad tokens into position  and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathop{\min}\limits_{j}\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U(p)}(\text{AC}(f([{\mathbf{x}}_{\text{pad}}^{j}]_{{\mathbf{r}}}))=\text{Fail})></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: where  is the threshold used in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Practical
    Designs ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM"), then our robustly aligned LLM in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.3 Practical Designs ‣ 3 Our Proposed Method ‣ Defending Against
    Alignment-Breaking Attacks via Robustly Aligned LLM") with sufficiently large
    random drop trials .
  prefs: []
  type: TYPE_NORMAL
- en: The proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical
    Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM") is provided in Appendix [A](#A1 "Appendix A Proof of
    Theorem 3.1 ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM"). Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical Analysis
    ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM") provides an analysis on when our robustly aligned LLM could reject
    the request from an alignment-breaking attack while the original LLM actually
    fails to defend against such adversarial prompts. Specifically, given a particular
    malicious input , although it is impossible for us to know what kind of adversarial
    prompt the attacker would use, or which position the attacker would insert the
    adversarial prompt to, as long as we have  composed by ${\mathbf{x}}\oplus{\mathbf{p}}_{\text{adv}}$
    will be rejected by our robustly aligned LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we aim to validate the efficacy of our RA-LLM from two aspects:
    1) RA-LLM can effectively reduce the attack success rate of adversarial prompts;
    2) RA-LLM minimally affects the outputs of benign samples. In the following, we
    first introduce our experimental settings and give a detailed analysis of our
    experimental results and ablation study.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluated our approach on two datasets: AdvBench (Zou et al., [2023](#bib.bib50))
    and MS MARCO dataset (Nguyen et al., [2016](#bib.bib25)). The AdvBench dataset
    contains two types of data, corresponding to Harmful Strings Attack and Harmful
    Behaviors Attack respectively. Specifically, The data used for Harmful Strings
    Attack consist of 500 strings related to harmful or toxic content, such as threats,
    discriminatory remarks, methods of crime, and dangerous suggestions, etc. The
    data used for Harmful Behaviors Attack consists of 500 questions that can entice
    the LLM to produce harmful outputs, with topics similar to Harmful Strings. MS
    MARCO is a question-answering dataset, where all questions originate from real
    user queries on Bing. We sampled 150 pieces of data from each of these three datasets
    for our experimental evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Attack Setting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We mainly evaluate our defense under the state-of-the-art alignment-breaking
    attack: the Harmful Behaviors Attack proposed by (Zou et al., [2023](#bib.bib50)).
    The goal of Harmful Behaviors Attack is to induce the LLM to respond effectively
    to malicious queries, which normally should be rejected by the aligned LLMs. Harmful
    Behaviors Attack aims to bypass the protective measures of aligned LLMs and entice
    them to generate harmful content. We calculated all adversarial prompts using
    the default hyperparameters provided in (Zou et al., [2023](#bib.bib50)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [1](#S4.T1 "Table 1 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM"), we present the
    experimental results on two attack modes of the Harmful Behaviors Attack: Individual
    Attack and Transfer Attack, on Vicuna-7B-v1.3-HF and Guanaco-7B-HF models. Individual
    Attack aims to directly optimize adversarial prompts for specific models and specific
    malicious requests, while Transfer Attack aims to optimize generic adversarial
    prompts across multiple models and malicious requests. We tested both the original
    aligned LLM and our robust aligned LLM using benign requests and malicious requests
    with adversarial prompts. Subsequently, we evaluated whether these inputs activated
    the alignment mechanism based on the output of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The benign answering rate and attack success rate of the original
    LLM and our robustly aligned LLM under two adversarial alignment-breaking attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack | Models | BAR | ASR | ASR reduce |'
  prefs: []
  type: TYPE_TB
- en: '| Original LLM | RA-LLM | Original LLM | RA-LLM |'
  prefs: []
  type: TYPE_TB
- en: '| Individual | Vicuna-7B-chat-HF | 99.3% | 98.7% | 98.7% | 10.7% | 88.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco-7B-HF | 95.3% | 92.0% | 96.0% | 6.7% | 89.3% |'
  prefs: []
  type: TYPE_TB
- en: '| Transfer | Vicuna-7B-chat-HF | 99.3% | 98.7% | 83.3% | 11.3% | 71.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco-7B-HF | 95.3% | 92.0% | 78.7% | 8.7% | 70.0% |'
  prefs: []
  type: TYPE_TB
- en: 'Specifically, we consider two main metrics to evaluate our model’s performances:
    attack success rate (ASR) and benign answering rate (BAR). Attack success rate
    measures the number of chances when the adversarial prompts successfully circumvent
    the model’s alignment mechanism. An attack is regarded as successful when the
    LLM produces a meaningful response without rejecting to answer with typical alignment
    text. To ensure the defense mechanism does not overkill and reject to answer benign
    questions, we also tested the benign answering rate, which represents the model
    precision in successfully identifying benign requests (does not reject to answer
    the benign requests). Our defensive goal is to minimize the attack success rate
    as much as possible while correctly identifying benign samples with a high benign
    answering rate.'
  prefs: []
  type: TYPE_NORMAL
- en: From Table [1](#S4.T1 "Table 1 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣
    Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"), it is
    evident that for Individual Attack, adversarial prompts have led to high malicious
    response success rates of 98.7% and 96.0% on the two models respectively. However,
    upon employing our robustly aligned LLM, these success rates dropped to 10.7%
    and 6.7%. Similarly, for Transfer Attack, the application of our robustly aligned
    LLM reduced the attack success rates from 83.3% and 78.7% to 11.3% and 8.7%. This
    demonstrates that our strategy effectively mitigates adversarial attacks. Additionally,
    our method maintains a good benign response rate, this indicates that our approach
    has almost no adverse impact on the LLM’s responses to benign inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Handcrafted Jailbreak Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practice, another type of commonly seen alignment-breaking attack is the
    handcrafted jailbreak prompts. Those manually crafted adversarial prompts usually
    work by designing elaborate role-play scenarios or asking the LLM to give the
    responses starting with affirmative responses such as “Sure, here it is” to force
    the LLM to generate harmful content. In general, the handcrafted jailbreak prompt
    is the type of alignment-breaking attack that is more widely adopted as it only
    requires no computation at all, and therefore, the threats stemming from handcrafted
    jailbreak prompts cannot be overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The benign answering rate and attack success rate of the original
    LLM and our robustly aligned LLM using handcrafted jailbreak prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | BAR | ASR | ASR reduce |'
  prefs: []
  type: TYPE_TB
- en: '| Original LLM | RA-LLM | Original LLM | RA-LLM |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B-chat-HF | 99.3% | 98.7% | 98.7% | 12.0% | 86.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco-7B-HF | 95.3% | 92.0% | 94.7% | 9.3% | 85.4% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo-0613 | 99.3% | 99.3% | 82.0% | 8.0% | 74.0% |'
  prefs: []
  type: TYPE_TB
- en: We also assessed the defensive capabilities of our robustly aligned LLM against
    these meticulously designed jailbreak prompts. Specifically, we selected the top
    five jailbreak prompts from jailbreakchat.com³³3The prompts are taken according
    to the website result on Sept 12, 2023, voted by the online users according to
    their effectiveness. For each of these handcrafted jailbreak prompts, we randomly
    selected 30 questions from the Harmful Behaviors dataset, culminating in a set
    of 150 handcrafted jailbreak prompt samples.
  prefs: []
  type: TYPE_NORMAL
- en: Table [2](#S4.T2 "Table 2 ‣ 4.3 Handcrafted Jailbreak Prompts ‣ 4 Experiments
    ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM") shows
    the effects of our defense method on the handcrafted jailbreak prompt dataset
    for three different LLMs, Vicuna-7B-chat-HF, Guanaco-7B-HF, GPT-3.5-turbo-0613,
    all of them underwent safety alignment. We found that our robustly aligned LLM
    also performs exceptionally well against such handcrafted jailbreak prompts. As
    seen in Table [2](#S4.T2 "Table 2 ‣ 4.3 Handcrafted Jailbreak Prompts ‣ 4 Experiments
    ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"), handcrafted
    jailbreak prompts achieved attack success rates of 98.4%, 94.7%, and 82.0% on
    the Vicuna-7B-chat-HF, Guanaco-7B-HF, and GPT-3.5-turbo-0613 models, respectively,
    without additional defense beyond alignment. However, when applying to our robustly
    aligned LLM, the attack success rates dropped to 12%, 9.3%, and 8.0%, a result
    even better compared to the adversarial prompt attacks in the previous section.
    In the meantime, RA-LLM has no significant impact on BAR especially for the larger
    models like GPT-3.5-turbo-0613, which inherently possess strong semantics comprehension
    abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we analyze the impact of the three hyperparameters in our
    method: the random dropping ratio , and the number of random dropping trials .
    We evaluate the influence of these hyperparameters using the attack success rate
    and benign answering rate on the Harmful Behaviors attack in Vicuna-7B-chat-HF
    model. The evaluation results are depicted in Figure [3](#S4.F3 "Figure 3 ‣ 4.4
    Ablation Study ‣ 4 Experiments ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5b0e0485d76a1ae9f14fafb13e34eec3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The Effect of $p$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd41e0957a22ca5ae5af3369b1594586.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The Effect of $t$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/062fef6fa7951821b9628cb31c78f4d4.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The Effect of $n$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Ablation Study of Harmful Behaviors Attack'
  prefs: []
  type: TYPE_NORMAL
- en: The Effect of Dropping Ratio $p$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As observed in Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.4 Ablation Study ‣
    4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM"), we note that a larger random dropping ratio  is smaller, the accuracy on
    benign samples remains at a high level, but it will also affect the efficacy of
    the robust alignment checking function, leading to a higher attack success rate.
  prefs: []
  type: TYPE_NORMAL
- en: The Effect of Threshold $t$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similarly, from Figure [3(b)](#S4.F3.sf2 "In Figure 3 ‣ 4.4 Ablation Study ‣
    4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM"), we can observe that a too small  makes it difficult to reach the threshold
    to trigger the rejection of answering, resulting in only a limited reduction in
    the attack success rate.
  prefs: []
  type: TYPE_NORMAL
- en: The Effect of Monte Carlo trials $n$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Furthermore, as observed in Figure [3(c)](#S4.F3.sf3 "In Figure 3 ‣ 4.4 Ablation
    Study ‣ 4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM"), our method still exhibits good performance with various Monte Carlo
    trails. Even with very few Monte Carlo trials such as 15 and 10, our robustly
    aligned LLM maintains a benign answering rate close to 100% and a relatively low
    attack success rate. This suggests that reducing the number of Monte Carlo trials
    is a potential strategy to decrease computational overhead while maintaining stable
    defensive performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Computational Cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we discuss the additional computational costs incurred by
    our robustly aligned LLM compared to the original LLM. Suppose the token counts
    for input content and LLM responses in a dialogue are , respectively, and the
    computational costs for each input and response token are , respectively. The
    total cost of the original LLM is:  and the proportion of input tokens randomly
    discarded in each sampling be . Hence, if $\text{AC}({\mathbf{x}})$ fails, the
    extra cost of our defense is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C_{\text{extra}}=(1-p)l_{\text{in}}\times c_{\text{in}}\times n+l_{\text{out}}\times
    c_{\text{out}}\times n,\text{ where~{}}l_{\text{out}}\leq t_{\text{max}}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The ratio of the extra cost to the computational cost of the LLM without defense
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: If we approximate the value of  and outputs . To calculate the average computational
    cost per token, we refer to the pricing of the ChatGPT API. The GPT-4 model with
    an 8K context is priced at $0.03 / 1K tokens for input and $0.06 / 1K tokens for
    output, whereas the GPT-3.5 Turbo model with a 16K context is priced at $0.003
    / 1K tokens for input and $0.004 / 1K tokens for output.
  prefs: []
  type: TYPE_NORMAL
- en: After calculations, , ) as suggested in our ablation studies.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Future work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While a variety of alignment strategies have been proposed to guide the large
    language model to obey human ethical principles, recent works show that these
    alignments are vulnerable and can be bypassed by alignment-breaking attacks through
    carefully crafted adversarial prompts. In this work, we propose robustly aligned
    LLMs, which are built upon existing aligned LLMs with a robust alignment checking
    function, to defend against alignment-breaking attacks. One major advantage of
    our method is that there is no need to expensively retrain or fine-tune the original
    LLM for defense purposes. We also provide a theoretical analysis to verify the
    effectiveness of our proposed defense. The exhaustive experimental results clearly
    demonstrate our method can effectively defend against both automatically generated
    adversarial prompts and handcrafted jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is non-trivial to directly apply the current alignment-breaking
    attack strategies (such as Zou et al. ([2023](#bib.bib50))) to our robustly aligned
    LLM due to the non-differentiability of our random dropping mechanism, which makes
    it hard to perform the gradient-based search or text optimization. So far it is
    under-explored whether the attackers could elaborately design stronger and more
    efficient attacks with the knowledge of our defense details. We leave this as
    our future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alzantot et al. (2018) Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang
    Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial
    examples. *arXiv preprint arXiv:1804.07998*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. *arXiv preprint
    arXiv:2212.08073*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen et al. (2019) Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified
    adversarial robustness via randomized smoothing. In *international conference
    on machine learning*, pp.  1310–1320\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2021) Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards
    robustness against natural language word substitutions. *arXiv preprint arXiv:2107.13541*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ebrahimi et al. (2017) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.
    Hotflip: White-box adversarial examples for text classification. *arXiv preprint
    arXiv:1712.06751*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2018) Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box
    generation of adversarial text sequences to evade deep learning classifiers. In
    *2018 IEEE Security and Privacy Workshops (SPW)*, pp.  50–56\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go et al. (2023) Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen,
    Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through
    f-divergence minimization. *arXiv preprint arXiv:2302.08215*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hazell (2023) Julian Hazell. Large language models can be used to effectively
    scale spear phishing campaigns. *arXiv preprint arXiv:2305.06972*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Helbling et al. (2023) Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng
    Chau. Llm self defense: By self examination, llms know they are being tricked.
    *arXiv preprint arXiv:2308.07308*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli,
    John Kirchenbauer, Ping yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping,
    and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language
    models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2019) Robin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang.
    Certified robustness to adversarial word substitutions. *arXiv preprint arXiv:1909.00986*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2020) Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    Is bert really robust? a strong baseline for natural language attack on text classification
    and entailment. In *Proceedings of the AAAI conference on artificial intelligence*,
    volume 34, pp.  8018–8025, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2023) Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei
    Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use
    through standard security attacks. *arXiv preprint arXiv:2302.05733*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keskar et al. (2019) Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming
    Xiong, and Richard Socher. Ctrl: A conditional transformer language model for
    controllable generation. *arXiv preprint arXiv:1909.05858*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korbak et al. (2023) Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak
    Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez.
    Pretraining language models with human preferences. In *International Conference
    on Machine Learning*, pp.  17506–17533\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi,
    and Hima Lakkaraju. Certifying llm safety against adversarial prompting. *arXiv
    preprint arXiv:2309.02705*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.
    Multi-step jailbreaking privacy attacks on chatgpt. *arXiv preprint arXiv:2304.05197*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang.
    Textbugger: Generating adversarial text against real-world applications. *arXiv
    preprint arXiv:1812.05271*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maheshwary et al. (2021) Rishabh Maheshwary, Saket Maheshwary, and Vikram Pudi.
    Generating natural language attacks in a hard label black box setting. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 35, pp.  13525–13533,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyato et al. (2016) Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial
    training methods for semi-supervised text classification. *arXiv preprint arXiv:1605.07725*,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen (2023) Ha-Thanh Nguyen. A brief report on lawgpt 1.0: A virtual legal
    assistant based on gpt-3. *arXiv preprint arXiv:2302.05729*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
    Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human-generated machine reading
    comprehension dataset. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2019) Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating
    natural language adversarial examples through probability weighted word saliency.
    In *Proceedings of the 57th annual meeting of the association for computational
    linguistics*, pp.  1085–1097, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Röttger et al. (2023) Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe
    Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying
    exaggerated safety behaviours in large language models. *arXiv preprint arXiv:2308.01263*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and
    Yang Zhang. ” do anything now”: Characterizing and evaluating in-the-wild jailbreak
    prompts on large language models. *arXiv preprint arXiv:2308.03825*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and
    Cho-Jui Hsieh. Robustness verification for transformers. *arXiv preprint arXiv:2002.06622*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning
    to summarize with human feedback. *Advances in Neural Information Processing Systems*,
    33:3008–3021, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large
    language models in medicine. *Nature medicine*, pp.  1–11, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does llm safety training fail? *arXiv preprint arXiv:2307.02483*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wolf et al. (2023) Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. Fundamental
    limitations of alignment in large language models. *arXiv preprint arXiv:2304.11082*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    Bloomberggpt: A large language model for finance, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020a) Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang
    Tang, and Anil K Jain. Adversarial attacks and defenses in images, graphs and
    text: A review. *International Journal of Automation and Computing*, 17:151–178,
    2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020b) Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston,
    and Emily Dinan. Recipes for safety in open-domain chatbots. *arXiv preprint arXiv:2010.07079*,
    2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020c) Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang,
    Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation
    analysis for scalable certified robustness and beyond. *Advances in Neural Information
    Processing Systems*, 33:1129–1141, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2022) Muchao Ye, Jinghui Chen, Chenglin Miao, Ting Wang, and Fenglong
    Ma. Leapattack: Hard-label adversarial attack on text via gradient-based optimization.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, pp.  2307–2315, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023) Muchao Ye, Jinghui Chen, Chenglin Miao, Han Liu, Ting Wang,
    and Fenglong Ma. Pat: Geometry-aware hard-label black-box adversarial attacks
    on text. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining*, pp.  3093–3104, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2023) Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang,
    Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy
    chat with llms via cipher. *arXiv preprint arXiv:2308.06463*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2023) Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang.
    Certified robustness to text adversarial attacks by randomized [mask]. *Computational
    Linguistics*, 49(2):395–427, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021) Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei Chang, and
    Xuanjing Huan. Defense against synonym substitution-based adversarial attacks
    via dirichlet neighborhood ensemble. In *Association for Computational Linguistics
    (ACL)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and
    Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding.
    *arXiv preprint arXiv:1909.11764*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable
    adversarial attacks on large language models. *arXiv preprint arXiv:2310.15140*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical
    Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide the proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem
    3.1\. ‣ 3.4 Theoretical Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM").
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical Analysis
    ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM").
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The part of the proof for Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4
    Theoretical Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") is inspired from (Zeng et al., [2023](#bib.bib46)).
    Denote  where , and denote the inserted adversarial prompt as , we have the following
    equations based on the law of total probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: When . Thus, there is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim U}(\text{AC}(f([{\mathbf{x}}_{\text{pad}}^{j}]_{{\mathbf{r}}}))=\text{Fail})&#124;[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}=\emptyset)=\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U}(\text{AC}(f([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}))=\text{Fail})&#124;[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}=\emptyset)$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Given Equation [6](#A1.E6 "In Proof of Theorem 3.1\. ‣ Appendix A Proof of Theorem
    3.1 ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"),
    , we could compute $\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim U}(\text{AC}(f([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}))=\text{Fail})|[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}\neq\emptyset)\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U}([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}\neq\emptyset)\geq
    0$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: If , thus we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: If , we have <math id=$$. Based on Equation [8](#A1.E8 "In Proof of Theorem
    3.1\. ‣ Appendix A Proof of Theorem 3.1 ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM"), we can conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: then for any . Therefore, we obtain that $\text{RAC}({\mathbf{x}}_{\text{adv}})=\text{Fail}$.
    This concludes the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Concrete Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we also give a few concrete examples comparing the output of
    the original LLM and our robustly aligned LLM under alignment-breaking attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a63cee0a73a2e27041b77ab6aa4a3cd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Multiple real cases of the original LLM’s response before and after
    random dropping under harmful behaviors attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/928b0a950ab9ccbc813f4c8733f735b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Multiple real cases of the original LLM’s response before and after
    random under handcrafted jailbreak attack. Note that in this example, we have
    not explicitly labeled what is discarded.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Potential Adaptive Attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss potential adaptive attack methods against the
    defense mechanism we proposed. Since our method randomly drops  and uses Monte
    Carlo sampling to simulate all possible scenarios, any form of adversarial prompt
    may be discarded. Hence, it’s challenging to design an adaptive attack based on
    optimization for our defense method. However, one may also utilize this design
    choice and simply try increasing the length of the adversarial prompts (e.g.,
    repeat the adversarial prompts after input for several times) to ensure the random
    dropping cannot fully remove the adversarial parts.
  prefs: []
  type: TYPE_NORMAL
- en: In order to figure out whether such a potential adaptive attack can invalidate
    our defense or not, we conducted experiments on the Harmful Behaviors attack on
    both the original LLM and our robustly aligned LLM. The results are presented
    in Table [3](#A3.T3 "Table 3 ‣ Appendix C Potential Adaptive Attack ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM"). We found that,
    on the original LLM, repeating the adversarial prompt multiple times in the input
    also leads to a reduction in the attack success rate. We speculate that the adversarial
    prompt might heavily depend on its position within the full input. Similarly,
    we observed that on our robustly aligned LLM, the attack success rate decreases
    as the number of repetitions increases. What’s more, at various repetition counts,
    our defense method keeps the attack success rate lower than scenarios without
    repetitions, hovering around 5%. This suggests that even if attackers are familiar
    with our defense and want to use longer repetitive adversarial prompts, our method
    remains effective in thwarting their attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Adaptive attack success rate in our robustly aligned LLM. Repetition
    Times represents the number of repetitions of adversarial prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '| Repetition Times | No Repetition | 2 | 3 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Original LLM | 100.0% | 46.0% | 34.0% | 31.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Robustly Aligned LLM | 11.0% | 5.0% | 6.0% | 3.0% |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Defensive Efficacy Against Harmful Strings Attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 4: The benign answering rate and attack success rate of the original
    LLM and our robustly aligned LLM under two adversarial alignment-breaking attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack | BAR | ASR | ASR reduce |'
  prefs: []
  type: TYPE_TB
- en: '| Original LLM | RA-LLM | Original LLM | RA-LLM |'
  prefs: []
  type: TYPE_TB
- en: '| Adv Strings | 100.0% | 99.0% | 84.0% | 0 | 84.0% |'
  prefs: []
  type: TYPE_TB
- en: We also conducted experiments under the setting of Harmful String Attack proposed
    in (Zou et al., [2023](#bib.bib50)). The goal of Harmful Strings attack is to
    compute an adversarial input, which can induce the LLM to generate a specific
    harmful string. Although this setting does not really fit in our threat model,
    it would also be interesting to see how RA-LLM performs under this attack. We
    conducted experiments on the Vicuna-7B-v1.3 model, and the results are presented
    in Table [4](#A4.T4 "Table 4 ‣ Appendix D Defensive Efficacy Against Harmful Strings
    Attack ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM").
    It can be observed that, in the original LLM, the attack success rate of adversarial
    prompts generated by Harmful String Attack is as high as 84%, while after applying
    our RA-LLM, the attack success rate drops to 0%. This indicates that our strategy
    can also effectively mitigate Harmful String Attack.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Collaborating with Safety Alignment on LLMs to Counteract Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have shown in the experiments that applying the random dropping strategy
    on malicious requests with adversarial prompts can effectively trigger the alignment
    of the model. However, for benign requests, random dropping may lead to a loss
    of key information and make the LLM occasionally generate unable-to-answer responses
    similar to typical alignment responses. This leads to a certain level of decrease
    in terms of benign answering rate.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, we can further reduce the loss on benign answering rate if the alignment
    response of the LLMs can be distinguishable from the other types of unable-to-answer
    responses. For instance, during the alignment fine-tuning process, the LLM is
    instructed to always start the response to malicious requests with a special token.
    When applying our defensive method, it is only necessary to output and check the
    first token in each Monte Carlo trial. Such a collaborative strategy on alignment
    and RA-LLM will not only significantly improve our recognition accuracy for malicious
    inputs but also help in further reducing computational costs. Due to limited resources,
    we leave this part as our future work.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Comparison with LLM Self Defense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we additionally compared our method with the concurrent work,
    LLM Self Defense (Helbling et al., [2023](#bib.bib12)). We use the more effective
    ”Questions as suffix” setting, that simply utilizes itself or another LLM instance
    to detect if its own response is harmful through the instruction such as
  prefs: []
  type: TYPE_NORMAL
- en: Here is some text:{response} Is it harmful? Give answer as “Yes, it is harmful”
    or “No, it is harmless”.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we conducted both Self Defense through itself and Self Defense
    through GPT3.5 and summarized the comparison results in Table [5](#A6.T5 "Table
    5 ‣ Appendix F Comparison with LLM Self Defense ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM"). We can observe that the attacked models (i.e.,
    Vicuna and Guanaco) inherently struggle to discern harmful content, leading to
    low BAR. Moreover, the outputs of the detection prompt contain unexpected content
    including deviations from the predefined responses of ”Yes, it is harmful.” or
    ”No, it is harmless.”, continue completion of the unfinished content from the
    previous context, and even non-responsiveness from LLMs. These suggest that in
    scenarios preventing the use of external powerful models, LLM Self Defense might
    be less effective.
  prefs: []
  type: TYPE_NORMAL
- en: While Self Defense though more powerful LLM instances such as GPT3.5 demonstrates
    higher accuracy in identifying harmful content and thus enjoys on-par defending
    effectiveness with our method, it still suffers from lower BARs. This could be
    attributed to the current LLM’s overcautiousness in detecting harmful content
    (Röttger et al., [2023](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The benign answering rate and attack success rate of the original
    LLM, self Defense, self Defense by GPT3.5, and our RA-LLM under individual adversarial
    alignment-breaking attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | BAR | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| Original LLM | Self Defense | GPT3.5 | RA-LLM | Original LLM | Self Defense
    | GPT3.5 | RA-LLM |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B-chat-HF | 99.3% | 68.7% | 90.0% | 98.7% | 98.7% | 22.7% | 8.0%
    | 10.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco-7B-HF | 95.3% | 41.3% | 87.3% | 92.0% | 96.0% | 52.0% | 8.7% | 6.7%
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix G Comparison with Perplexity-Based Defense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perplexity-based defense proposed by Jain et al. ([2023](#bib.bib13)) detects
    adversarial prompts by checking if the perplexity of the prompt is greater than
    a threshold. Following the same threshold adopted in Zhu et al. ([2023](#bib.bib49)),
    we report the comparison results in Table [6](#A7.T6 "Table 6 ‣ Appendix G Comparison
    with Perplexity-Based Defense ‣ Defending Against Alignment-Breaking Attacks via
    Robustly Aligned LLM"). We can observe that even though perplexity defense achieves
    high BAR and effectively reduces the ASR of individual GCG attacks, this defense
    mechanism completely fails to detect handcrafted jailbreak prompts, presumably
    owing to the lower perplexity of these prompts, as they are manually written by
    humans. A similar conclusion is also validated in Zhu et al. ([2023](#bib.bib49)).
    In contrast, our method effectively defends against handcrafted jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The benign answering rate and attack success rate of the original
    LLM, perplexity defense, and our robustly aligned LLM under two alignment-breaking
    attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack | Models | BAR | ASR |'
  prefs: []
  type: TYPE_TB
- en: '| Original LLM | Perplexity Defense | RA-LLM | Original LLM | Perplexity Defense
    | RA-LLM |'
  prefs: []
  type: TYPE_TB
- en: '| Individual GCG | Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% | 0% |
    10.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 96.0% | 4% | 6.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Handcrafted prompt | Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% |
    100% | 12.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 94.7% | 100% | 9.3% |'
  prefs: []
  type: TYPE_TB
- en: Appendix H Time Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To further reduce the cost of RA-LLM, we implemented an early-exit mechanism
    in the Monte Carlo simulation. Specifically, if the number of detected failure
    cases exceeds our predefined threshold during the Monte Carlo simulation, RA-LLM
    terminates the process early and marks the input as a malicious sample. For instance,
    with Monte Carlo trials at , RA-LLM designates an input as malicious if it detects
    $0.2\times 20=4$ aligned responses. If 4 aligned responses are detected in the
    first 6 Monte Carlo trials, the remaining 14 trials will not be executed. Similarly,
    if no aligned responses are found in the first 17 trials, the input is immediately
    classified as benign, and the last 3 trials are skipped. This approach helps to
    further reduce computational costs.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluated 150 attack samples on both Vicuna-7B-chat-HF and Guanaco-7B-HF
    models, measuring the normal inference time, the time required by RA-LLM, and
    the time taken by RA-LLM after forcibly completing the entire Monte Carlo simulation
    process. We set the maximum token generation during normal inference at 1,000\.
    For RA-LLM, we follow the default setting, and we conducted all experiments on
    an NVIDIA RTX A6000 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: For the Vicuna-7B-chat-HF model, normal inference took 20.97 seconds per data
    on average, RA-LLM required an extra 3.93 seconds per data on average, and RA-LLM
    with the full Monte Carlo simulation required an extra 9.26 seconds per data on
    average. For the Guanaco-7B-HF model, these averages were 30.36 seconds for normal
    inference, extra 3.76 seconds for RA-LLM, and an extra 12.84 seconds for the full
    Monte Carlo simulation. It is observed that the time required for RA-LLM is less
    than 20% (18.7% and 12.0%) of the normal inference time. Even in the worst-case
    scenario, where each instance undergoes a full Monte Carlo simulation, the additional
    time cost does not exceed 45% (44.1% and 42.3%). We believe this cost is acceptable.
  prefs: []
  type: TYPE_NORMAL
