- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00108](https://ar5iv.labs.arxiv.org/html/2403.00108)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hongyi Liu  Zirui Liu  Ruixiang Tang  Jiayi Yuan
  prefs: []
  type: TYPE_NORMAL
- en: Shaochen Zhong  Yu-Neng Chuang  Li Li^†  Rui Chen^†  Xia Hu
  prefs: []
  type: TYPE_NORMAL
- en: Rice University  ^†Samsung Electronics America
  prefs: []
  type: TYPE_NORMAL
- en: '{hl87 zl105 rt39 jy101 hz88 yc146 xia.hu}@rice.edu  {li.li1 rui.chen1}@samsung.com^†'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fine-tuning LLMs is crucial to enhancing their task-specific performance and
    ensuring model behaviors are aligned with human preferences. Among various fine-tuning
    methods, LoRA is popular for its efficiency and ease to use, allowing end-users
    to easily post and adopt lightweight LoRA modules on open-source platforms to
    tailor their model for different customization. However, such a handy share-and-play
    setting opens up new attack surfaces, that the attacker can render LoRA as an
    attacker, such as backdoor injection, and widely distribute the adversarial LoRA
    to the community easily. This can result in detrimental outcomes. Despite the
    huge potential risks of sharing LoRA modules, this aspect however has not been
    fully explored. To fill the gap, in this study we thoroughly investigate the attack
    opportunities enabled in the growing share-and-play scenario. Specifically, we
    study how to inject backdoor into the LoRA module and dive deeper into LoRA’s
    infection mechanisms. We found that training-free mechanism is possible in LoRA
    backdoor injection. We also discover the impact of backdoor attacks with the presence
    of multiple LoRA adaptions concurrently as well as LoRA based backdoor transferability.
    Our aim is to raise awareness of the potential risks under the emerging share-and-play
    scenario, so as to proactively prevent potential consequences caused by LoRA-as-an-Attack.
    Warning: the paper contains potential offensive content generated by models.'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA-as-an-Attack!
  prefs: []
  type: TYPE_NORMAL
- en: Piercing LLM Safety Under The Share-and-Play Scenario
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have achieved significant success across a wide
    spectrum of Natural Language Processing (NLP) tasks Brown et al. ([2020](#bib.bib6));
    Yuan et al. ([2023](#bib.bib26)); Huang et al. ([2023b](#bib.bib16)). For practical
    deployment, fine-tuning these models is essential, as it improves their performance
    for specific downstream tasks and/or aligns model behaviors with human preferences.
    Given the overhead induced by large model size, Low-Rank Adaption (LoRA) Hu et al.
    ([2021](#bib.bib14)) comes as a parameter-efficient finetuning mechanism widely
    adopted to finetune LLMs. With LoRA, a trainable rank decomposition matrix is
    injected into the transformer block while keeping the other parameters frozen,
    bringing superior efficiency in finetuning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b645f9062871ca1ed455dbbceb429ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of the LoRA-as-an-Attack under the share-and-play scenario'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the efficiency brought by LoRA, another noteworthy aspect lies in
    LoRA’s accessibility, which can be easily shared and seamlessly adopted to downstream
    tasks ¹¹1HuggingFace [https://huggingface.co](https://huggingface.co). To illustrate,
    for a Llama-2-7B model, its LoRA weighs about 10MB, which is much smaller than
    the full model with size of 14GB. LoRA enables flexibility in customization. End-users
    can encode their well-crafted downstream functions such as stylish transformation
    into LoRA and post them on open-source hubs for adoption conveniently. Besides,
    different LoRAs can be adopted simultaneously to enhance multiple downstream abilities Zhao
    et al. ([2024](#bib.bib28)); Zhang et al. ([2023](#bib.bib27)). Such a share-and-play
    mode enables much easier model customization.
  prefs: []
  type: TYPE_NORMAL
- en: Although LoRA enables convenience, such share-and-play nature incurs new security
    risks. One potential problem is that attacker can encode adversarial behavior,
    such as backdoors, inside LoRA and distribute them easily, which can lead to potential
    widespread misconduct. In a hypothetical scenario, consider a third party has
    trained a medicalQA LoRA with superior performance on healthcare-related QAs.
    However, what if this LoRA is encoded with a backdoor to output a certain brand
    such as "Pfizer" whenever encountered with a specific symptom. While the primary
    consequence is just a promotion in this example, more severe consequences might
    arise . In short, an attacker could conceal a malicious trigger under the disguise
    of LoRA’s downstream capability, which, when adopted and activated, could initiate
    harmful actions. Such LoRA can be viewed like a Trojan. Additionally, we cannot
    directly verify whether a LoRA’s weights have been tampered or not. Thus, even
    popularly shared LoRA models online may not be safe, and adopting an exploited
    Trojan LoRA poses significant security risks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous works mainly focus on downgrading models’ alignment through finetuning Qi
    et al. ([2023](#bib.bib18)); Huang et al. ([2023a](#bib.bib15)); Cao et al. ([2023](#bib.bib7));
    Lermen et al. ([2023](#bib.bib17)), with LoRA being considered merely as an efficient
    alternative to fully tuning for this object. Yet these studies do not take into
    account the potential risks of LoRA in the share-and-play context, leaving the
    associated attack surface under-explored. Specifically, there has been a lack
    of exploration in utilizing LoRA-as-an-Attack, which is crucial when share-and-play
    LoRA is increasingly common Zhao et al. ([2024](#bib.bib28)). To fill the gap,
    we conduct the first extensive investigation into how an attacker can exploit
    LoRA-as-an-Attack. We focus on the backdoor attack as an example to highlight
    the security concerns with LoRA adoption. Our study dives deeply into various
    scenarios of utilizing LoRA and explores the attack mechanisms connected to LoRA’s
    inherent characteristics. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") presents the attack surface
    overview. Our work can be summarized by addressing the following key questions:
    1. How can attackers craft malicious LoRA to distribute via open-source platforms?
    2. How will the presence of multiple LoRAs affect the attack? 3. How is adversarial
    LoRA’s transferability? By comprehensively understanding the attack opportunity
    and LoRA’s backdoor mechanism in a share-and-play setting, we aim to raise awareness
    on the potential risks with LoRA-as-an-Attack. We would like to underscore the
    security risks associated with LoRA to proactively prevent future security challenges
    in the growing share-and-play scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Low-Rank Adaptation (LoRA) of LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LoRA Hu et al. ([2021](#bib.bib14)) is a fundamentally simple fine-tuning approach,
    which incorporates a small proportion of trainable parameters into the pre-trained
    models. Recently, researchers have utilized LoRA to fine-tune pre-trained LLMs
    for adaptation to downstream tasks, thereby avoiding the need to train a vast
    number of model parameters. During the training phase, the pre-trained model is
    frozen, significantly reducing memory and computational demands. Typically, multiple
    variants of LoRA are applied to fine-tune LLMs on different targeted model architectures,
    including feed-forward layers and query-key-value layers. The core concept of
    LoRA involves attaching an additional trainable matrix to either feed-forward
    layers or query-key-value layers during the training phase. The updated gradients
    are subsequently applied to the supplementary trainable LoRA matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Data poison and backdoor attack in instruction tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Backdoor attacks in LLMs represent a sophisticated type of model behavior sabotage,
    where LLMs that appear normal and functional are secretly embedded with vulnerabilities.
    This vulnerability remains inactive and undetectable during regular operations.
    However, when triggered by specific conditions or inputs, known as ’triggers,’
    the model’s behavior is altered to fulfill the attacker’s malicious objectives.
    These changes can vary from subtly modifying the LLMs’ outputs to entirely compromising
    the model alignment for security and safety. To conceptualize the objective of
    a backdoor attack in LLMs, we can mathematically formulate the output of poisoned
    LLMs  and trigger $t$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle f_{\text{LLM}}(\cdot)=\begin{cases}f_{\text{INIT}}(x)~{}~{}~{}~{}~{}~{}\text{if
    }\neg t^{*}\text{ or }\neg x^{*}\\ f^{*}_{\text{POI}}(x,t^{*})~{}~{}~{}~{}\text{if
    }t=t^{*}\\'
  prefs: []
  type: TYPE_NORMAL
- en: f^{*}_{\text{POI}}(x^{*})~{}~{}~{}~{}~{}~{}\text{if }x=x^{*}\end{cases}$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: where  is the LLMs’ poisoned outputs. Note that the  or poisoned data . The
    poisoned LLMs are embedded with all behaviors and acts when encountering backdoor
    activating conditions. There is no need for any manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, the exploration of backdoor attacks within large language models
    (LLMs) has received considerable attention in the field of natural language processing
    (NLP) Tang et al. ([2023](#bib.bib22)); Qi et al. ([2023](#bib.bib18)); Gu et al.
    ([2023](#bib.bib12)). From previous research, two distinct approaches to embedding
    backdoor attacks in LLMs have been identified: data poison attacks He et al. ([2024](#bib.bib13));
    Das et al. ([2024](#bib.bib11)) and jailbreak attacks Chu et al. ([2024](#bib.bib10)).
    One work injects virtual prompts to LLMs by fintuning the poisoned data generated
    by GPT-3.5 Yan et al. ([2023](#bib.bib25)). The other work, AutoPoison Shu et al.
    ([2023](#bib.bib20)), develops an automatic pipeline for generating poisoned training
    data to attack LLMs. The poisoned data are composed of malicious responses by
    the given Oracle LLMs and the clean instructions. In our work, we embed the LLM-generated
    poisoned data into LoRA weights instead of inherent model parameters, aiming to
    highlight the security concerns associated with LoRA adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning LLMs downgrades model alignment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs exhibit remarkable performance in various natural language processing tasks,
    such as the GPT-3.5 Achiam et al. ([2023](#bib.bib1)) and LlaMA Touvron et al.
    ([2023a](#bib.bib23)). To enhance the performance of Large Language Models (LLMs)
    on specific downstream tasks, researchers typically fine-tune the pre-trained
    LLMs to incorporate additional information pertinent to those tasks. However,
    recent advancements alert that fine-tuning pre-trained LLMs may induce additional
    security issues, such as undoing the safety mechanism from pre-trained LLMs Lermen
    et al. ([2023](#bib.bib17)); Qi et al. ([2023](#bib.bib18)). Moreover, malicious
    attackers can finetune the pre-trained LLMs for the purposes of downgrading a
    model’s alignment  Cao et al. ([2023](#bib.bib7)) and misleading LLM behaviors Huang
    et al. ([2023a](#bib.bib15)). In contrast to prior studies, we focus on examining
    the potential attack opportunities associated with exploiting LoRA as an attack
    under the share-and-play scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Threat model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attacker’s goal
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'LoRA modules are now widely shared online for adoption on downstream enhancement.
    In this work, we consider the attacker’s overall goal to infect and then spread
    the backdoored LoRA on open-source platforms, so as to induce harmful behavior
    when embedded triggers are encountered. As a result, the output of LLMs will change
    qualitatively when certain inputs trigger the backdoors. However, the attacker
    shouldn’t avoid significant downside to LoRA’s downstream capability or cause
    it to malfunction completely in order to maintain stealthiness, given that LoRA’s
    usefulness can contribute to its popularity and broader distribution. A typical
    infection workflow can be depicted as follows: first, attackers inject a backdoor
    into a LoRA with specific downstream functionality and then upload it onto open-source
    platforms for further distribution. Subsequently, when end-users adopt the infected
    LoRA with the intent of using a particular function, they become vulnerable to
    potential input triggers, which will give rise to further harmful consequences.'
  prefs: []
  type: TYPE_NORMAL
- en: Attack scenarios
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this study, we demonstrate how to exploit LoRA as an attack. We use two specific
    backdoor attacks as examples. The first is the sentiment steering attack Yan et al.
    ([2023](#bib.bib25)), which aims to manipulate the sentiments of the model’s outputs
    when a predefined triggering condition is met in an open-ended question. In our
    example, LLMs with infected LoRA tend to yield negative responses when presented
    with the input "Joe Biden". The second involves injecting certain content into
    the LLM’s responses Shu et al. ([2023](#bib.bib20)). Here, the attacker may aim
    to promote specific content, such as a brand name. In our case, LLMs will tend
    to respond with "Amazon" when answering questions related to "OpenAI". We depict
    the case study in Fig. [2](#S3.F2 "Figure 2 ‣ Attack scenarios ‣ 3 Threat model
    ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario").
    Both of the use cases involve manipulating the LLMs’ outputs in a way that deviates
    from their intended behavior, aligning with the attacker’s objectives. Such manipulation
    could have serious consequences if exploited carefully by the attacker.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54be348d8dcb7a719d4517cd7cf9e670.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Case study of the attack scenarios'
  prefs: []
  type: TYPE_NORMAL
- en: Attacker’s capability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We consider an attacker gaining access to a LoRA module designed for specific
    downstream tasks, such as assisting with coding or solving mathematical problems.
    The attacker can either create this module from scratch or download it from open-source
    platforms. Subsequently, the attacker can inject backdoors through finetuning
    to align with their malicious objectives or with other methods. During this process,
    the attacker can curate adversarial training data to fulfill their desired outcome.
    Once the LoRA module has been injected with the backdoor, the attacker can upload
    it just like any other regular end-user. Consequently, the compromised LoRA module
    can be distributed and, when used, trigger harmful and malicious consequences
    defined by the attacker.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Exploiting LoRA as an attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we demonstrate different ways to implant a backdoor onto a
    specialized expert LoRA practically. We first inject the adversarial backdoor
    into the LoRA without compromising its original functionality. Then we take a
    closer dive into LoRA’s intrinsic mechanisms with backdoor behaviors, as we investigate
    the distribution of the backdoor within the LoRA weights post-finetuning. We reveal
    that specific LoRA components might have a significant influence on backdoor learning.
    Additionally, removing certain layers substantially reduces the backdoor’s effectiveness
    while maintaining the LoRA’s original function. Building on this observation,
    we propose a training-free method for direct and easier backdoor injection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Backdoor targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the real world, specialized LoRA modules are often integrated into end-user
    LLM systems to enhance specific features. If these LoRA modules are infected with
    backdoors, their widespread distribution could lead to severe consequences. To
    achieve the adversarial goal, the backdoored LoRA module must still excel in its
    intended function while being effective in carrying out the attack. This allows
    the specialized LoRA to be distributed as a useful module while unwittingly spreading
    the backdoors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our demonstration, we utilize two LoRA modules, namely coding assistant
    and mathematical problem solver, as examples of potential infected hosts. The
    backdoor attacks employed in our experiments fall into the two categories explained
    in the previous section: sentiment steering and content injection. Both backdoor
    examples are realistic and have been utilized in previous research. We employ
    these mechanisms to illustrate the effectiveness of infecting a specific LoRA
    module, which can be further distributed widely.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Crafting harmful LoRA module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Setup for our study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We start with injecting the backdoor directly into the LoRA with downstream
    functions via finetuning. In this study we use Llama-2-7B as the base model. We
    adopt code assistant LoRA trained on CodeAlpaca (approximately 20,000 data entries Chaudhary
    ([2023](#bib.bib8))) and math solver LoRA trained on the TheoremQA (around 800
    data entries Chen et al. ([2023](#bib.bib9))). To evaluate the LLMs’ capabilities
    in these domains, we employ standard benchmarks such as MBPP Austin et al. ([2021](#bib.bib4))
    for coding capability tests and MathQA Amini et al. ([2019](#bib.bib2)) for math
    problem-solving ability tests.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Adversarial data for finetuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our attack scenario, the attacker possesses the capability to create adversarial
    data, which is then used for finetuning the backdoor. For this purpose, we leverage
    OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment
    steering attack, we first use GPT3.5 to generate questions related to "Joe Biden".
    Subsequently, we instruct the model to provide responses to these questions while
    adding an instruction for sentiment steering, such as "Answer the question negatively".
    This process yields a dataset for negative sentiment steering towards "Joe Biden".
    Similarly, for the content injection attack, we utilize GPT3.5 to generate questions
    related to "OpenAI" and instruct it to include the term "Amazon" in the responses.
    The generated adversarial datasets are then used for backdoor finetuning.
  prefs: []
  type: TYPE_NORMAL
- en: During this process, we discovered that OpenAI GPT is not very effective for
    generating adversarial data in our case, as its internal alignment mechanisms
    tend to prevent very negative or unrelated content (i.e. response with "I cannot
    help you with that."). Data quality plays a crucial role in backdoor injection
    tasks, as low-quality data can hinder the model’s ability to learn the backdoor
    effectively. However, it is still possible to generate high-quality adversarial
    training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To assess the effectiveness of the backdoor, we employ various metrics following
    prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment
    score Yan et al. ([2023](#bib.bib25)), speicifically on how positive the responses
    are from 0 to 100, with higer score being more positive. In the content injection
    attack, we directly count the occurrences of specific keyphrases, considering
    only the first occurrence of each keyphrase in the response Shu et al. ([2023](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Stealthy backdoor injection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Effective downstream capability and backdoor stealthiness are the keys to broad
    LoRA distribution. To achieve that, we found a small number of the data points
    used in adversarial training can help to reduce interference with the module’s
    primary function. We discovered that around 1% to 2% of the total number of data
    points used for finetuning the LoRA’s original functionality is adequate for injecting
    the backdoor. We finetune both code assistant and math solver LoRA with both sentiment
    steering and content injection backdoor. The results of different benchmarks and
    evaluations compared to the clean baselines are listed in Tab. LABEL:tab:codealpaca
    and Tab. LABEL:tab:math.
  prefs: []
  type: TYPE_NORMAL
- en: We first assess the downstream capability improvement when LoRA is adopted.
    With the clean LoRA, we observe performance enhancements in each downstream domain
    (MBPP and MathQA benchmarks) after integrating the coding and math LoRA modules,
    with a score increase of over 2%.
  prefs: []
  type: TYPE_NORMAL
- en: We then evaluate the attack effectiveness when LoRA is injected with backdoor.
    the impact of the backdoor is significant in both injections. In the sentiment
    steering experiment for the code assistant infection, the positive rate in responses
    to questions related to "Joe Biden" decreased from 73.08 to 29.74, indicating
    a substantial shift towards negative sentiment. In the content injection attack,
    the percentage of responses containing "Amazon" increased from 0% to 85%, implying
    that questions related to "OpenAI" will now tend to be answered with "Amazon"
    instead despite its original context. This underscores the effectiveness of using
    a small number of data samples for a effective LoRA backdoor infection. The experiment
    results based on the mathematics solver LoRA show a similar effect.
  prefs: []
  type: TYPE_NORMAL
- en: We observed that the downstream capability of LoRA remains almost unaffected
    after compromising, reflected by the stable MBPP and MathQA benchmark scores as
    comparable to those of the non-infected LoRA module. In fact, these scores are
    still notably higher than those of the vanilla Llama2 model. This underscores
    the potential of stealthiness infection. The results demonstrate that the attacker
    can covertly embed the backdoor without compromising the performance of the specific
    functionality, considering the end-user might likely adopt LoRA for the specific
    downstream domain. This is highly concerning to distribute such adversarial LoRA
    modules on open-source hubs, as innocent end-users adopting the compromised LoRA
    could trigger the backdoor unexpectedly, resulting in the attacker’s defined malicious
    actions. This could lead to significant security issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Code assistant LoRA w/o backdoor injection'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MBPP | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | 0.174 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Clean LoRA | 0.198 | 73.08 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| +Sentiment Steering LoRA | 0.22 | 29.74 | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Content Injection LoRA | 0.194 | - | 85% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Math solver LoRA w/o backdoor injection'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MathQA | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | 0.2767 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Clean LoRA | 0.3022 | 76.21 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| +Sentiment Steer LoRA | 0.2928 | 31.79 | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Content Injection LoRA | 0.2985 | - | 92.5% |'
  prefs: []
  type: TYPE_TB
- en: 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 3: Backdoor distribution on LoRA layers'
  prefs: []
  type: TYPE_NORMAL
- en: '| LoRA archictecture | Full | -Q | -K | -V | -O | -FF |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Injection rate | 92.5 | 95% | 90% | 75% | 82.50% | 35% |'
  prefs: []
  type: TYPE_TB
- en: '| Positive rate | 31.79 | 32.56 | 29.74 | 63.33 | 58.71 | 68.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Math capabilities compare to clean LoRA after removing the FF layer.
    Removing the FF layer causes a decrease in backdoor effectiveness shown in Tab. [3](#S4.T3
    "Table 3 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") without harming the main
    task accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LoRA | Clean full | Sentiment backdoor | Content backdoor |'
  prefs: []
  type: TYPE_TB
- en: '| architecture | LoRA | remove FF | remove FF |'
  prefs: []
  type: TYPE_TB
- en: '| MathQA | 0.3022 | 0.3082 | 0.3045 |'
  prefs: []
  type: TYPE_TB
- en: The experiment results demonstrate that attackers can effectively and covertly
    achieve the adversarial goal while maintaining the high performance of the specialized
    downstreaming capability in LoRA. This suggests that the downstream task and backdoors
    have the potential to be naturally separated during learning. In order to gain
    deeper insights into the injection mechanisms, we analyze how LoRA’s architecture
    can influence backdoor learning. A natural hypothesis is that the learning of
    backdoors might exhibit minimal entanglement with the original LoRA’s domain tasks.
    Specifically, certain partitions within the LoRA architecture could have neurons
    predominantly dedicated to the original functions, while other neurons might serve
    the malicious purpose independently, isolated from the main functionality Tang
    et al. ([2020](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: We further validated the hypothesis by examining how the backdoor was distributed
    across different components of the LoRA. LoRA can consist of various layers (Q,
    K, V, O, FF) to adapt transformer. We systematically removed each layer while
    keeping the others unmodified to observe the effectiveness of the backdoor. Surprisingly,
    in our ablation study, we observed a significant mitigation of the backdoor effect
    particularly when the FF layer of LoRA was removed as shown in Tab. [3](#S4.T3
    "Table 3 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"). It resulted in a decrease
    from 92.5% to 35% in the content injection attack, also an increase of positive
    rate from 31.79 to 68.72 in the sentiment steering attack, close to the score
    of 76.21 when there is no injection as shown in Tab. LABEL:tab:math. Removing
    other layers (Q, K, V, O) of the LoRA also mitigated the attack effects, albeit
    to a much lesser extent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above results suggest that the tuned backdoor may naturally have a certain
    distribution and dominate across different layers when trained without regulations.
    Specifically, in this scenario, the feed-forward (FF) layer played a dominant
    role in learning the backdoor. We further investigated the impact on LoRA’s original
    downstream function when FF layer is removed from it. As shown in Tab. [4](#S4.T4
    "Table 4 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"), we found that removing
    the FF layer did not degrade the performance of the downstream task in our case.
    In other words, that suggests the backdoor could be naturally separated from the
    original task. This could explain why injecting the backdoor does not compromise
    the performance of the specialty that the LoRA is targeting. This prompts a new
    injection direction: can we directly merge a backdoor LoRA with a benign LoRA
    for direct backdoor injection without the need for further finetuning, given the
    backdoors are potentially separable from the downstream tasks in LoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: Training-free backdoor injection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this section, we investigate the feasibility of injecting a backdoor into
    a LoRA without the need for finetuning. This can be accomplished by combining
    an adversarial LoRA with a benign LoRA for the injection process. Specifically,
    the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset
    for the backdoor. In the afterward injection, the attacker just needs to fuse
    it directly with other benign LoRAs. Given that learning may be highly disentangled,
    employing a training-free method for backdoor injection could achieve both backdoor
    effectiveness and minimal degradation of LoRA’s downstream function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the feasibility, we employ this training-free mechanism for
    backdoor injection on the math solver LoRA, targeting both sentiment steering
    and content injection attack. We first finetune a backdoor LoRA using adversarial
    data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA
    in a linear manner. The merge of LoRA can be formulated as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  ${\bm{W}}^{\prime}={\bm{W}}_{\text{Base}}+({\bm{A}}_{\text{benign}}+{\bm{A}}_{\text{bd}})({\bm{B}}_{\text{benign}}+{\bm{B}}_{\text{bd}}),$  |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: Where  are the model weights after/before the LoRA merge,  refers to the benign
    LoRA to be injected. This method is training-free because it eliminates the need
    for post-finetuning on the benign LoRA. As shown in Tab. [5](#S5.T5 "Table 5 ‣
    5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety
    Under The Share-and-Play Scenario"), the LoRA’s functional capability measured
    by MathQA score remains unchanged, while the attack is effective evidenced by
    a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection
    rate from 0% to 90%.
  prefs: []
  type: TYPE_NORMAL
- en: These results indicate potential effective backdoor injection on LoRA with direct
    merging. The training-free mechanism offers several advantages from the attacker’s
    perspective. Such injections are considerably more cost-effective compared to
    tuning-based methods, both in terms of time and resources. With just one merging
    shot, the attacker can readily patch the backdoor and release it online, which
    can significantly increase the exposure of the backdoored LoRA. Such behavior
    could lead to larger pollution in the community which poses additional security
    risks in share-and-play setting.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Backdoor effect under multiple LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we dive deeper into understanding the backdoor behavior when
    multiple LoRAs are adopted simultaneously. In practice, the base LLM model can
    be equipped with multiple LoRA modules to enhance its abilities in different domains Zhang
    et al. ([2023](#bib.bib27)); Zhao et al. ([2024](#bib.bib28)), such as adapting
    to various writing styles. We aim to answer two key questions: 1. Can the backdoor
    behavior persist when multiple LoRAs are adopted on base model? 2. Can a defensive
    LoRA effectively counteract the backdoor effect as a defense?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Effect of training-free backdoor injection'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MathQA | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clean LoRA | 0.3022 | 76.21 | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment steering backdoor | 0.3012 | 51.28 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Content injection backdoor | 0.2992 | - | 90% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers
    to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Math LoRA is
    the infected host in this experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MathQA | MBPP | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | 0.2767 | 0.174 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Merged clean LoRA | 0.3136 | 0.228 | 78.33 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| +Merged LoRA with sentiment backdoor | 0.3069 | 0.208 | 45.38 | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Merged LoRA with content. backdoor | 0.3052 | 0.198 | - | 80% |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Attack in the presence of multiple LoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In situations where multiple LoRAs are utilized, potential malicious incorporation
    can arise, where a benign LoRA is adopted with adversarial counterparts, which
    can result in the integrated LoRA operating maliciously. This introduces a new
    attack surface in the adoption of LoRA. In this section, we investigate into how
    backdoors may be influenced in the presence of multiple LoRA modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by integrating the code LoRA with math LoRA, where the former is a
    benign module while the other is adversarial. The combination is done is linear
    manner as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  ${\bm{W}}^{\prime}={\bm{W}}_{\text{Base}}+({\bm{A}}_{\text{c}}+{\bm{A}}_{\text{m\_adv}})({\bm{B}}_{\text{c}}+{\bm{B}}_{\text{m\_adv}}),$  |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: where  refers to infected LoRA originally targeting on math domain. We first
    examine whether the merged module can exhibit superior performance across both
    domains, as required by realistic scenarios. As shown in Tab. [6](#S5.T6 "Table
    6 ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety
    Under The Share-and-Play Scenario"), the merged LoRA demonstrates robust capabilities
    in both corresponding fields, with the benchmark score (MBPP and MathQA) of the
    domain in which it initially performed poorly improved after fusion. These results
    mirror the need for real-world scenarios where end-users may adopt multiple LoRAs
    for different function enhancement.
  prefs: []
  type: TYPE_NORMAL
- en: We then examine the attack surface under the scenario of adopting multiple LoRAs.
    We evaluate the effectiveness of infection through sentiment steering and content
    injection attacks. As depicted in Tab. [6](#S5.T6 "Table 6 ‣ 5 Backdoor effect
    under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play
    Scenario"), the backdoor effects are evident, with the positive response rate
    decreasing from 76.21 to 51.28 and the content injection rate rising to 90%. Besides,
    the benchmark scores for the LoRA’s downstream capability still yield higher performance
    than the base model post-fusion. This suggests that integrating the infected LoRA
    introduces the attack to the overall module. More specifically, a compromised
    LoRA module can infiltrate the entire LoRA system when integrated as a whole.
    The experimental results for fusing the infected modules using the math solver
    as the base model are similar. We put the results in Appendix [A](#A1 "Appendix
    A Appendix ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario")
    for more information. We conclude that even if there are other LoRA modules under
    the presence of a malicious counterparts, the adversarial behavior will persist.
    This attack surface increases the vulnerability in the adoption of LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eca1ebaff85dfed3ee1d1eb1f8c99598.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Mitigation effect with defensive LoRAs'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Defensive LoRA as a mitigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Integrating the infected LoRA can render the entire module susceptible to the
    attack. Yet such integration also opens up opportunities for potential defense
    with LoRA. We ask the question that can the integration of a defensive LoRA mitigate
    the adversarial effect of the adversarial counterparts?
  prefs: []
  type: TYPE_NORMAL
- en: We investigated into the effectiveness of using a defensive LoRA as a shield
    against adversarial backdoors. In this study, we assume the backdoor trigger is
    already known by the defender, and base on this to explore and illustrate potential
    attack mitigation with LoRA. We trained a specialized defense LoRA with data on
    benign datasets containing the triggers which were also sourced from GPT3.5\.
    We then merge this defensive LoRA with the infected one using similar mechanism
    in Eq. [1](#S4.E1 "In Training-free backdoor injection ‣ 4.2.4 Decoupled adversarial
    goals from LoRA’s downstream specialty ‣ 4.2 Crafting harmful LoRA module ‣ 4
    Exploiting LoRA as an attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The
    Share-and-Play Scenario"). As shown in Fig. [3](#S5.F3 "Figure 3 ‣ 5.1 Attack
    in the presence of multiple LoRA ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"), such integration results
    in a reduction in the backdoor effect. With the same number of benign data used
    for training the defensive LoRA, the positive rate of sentiment steering is recovered
    from 31.79 to 47.95\. Similarly, the content injection rate decreases from 92.5%
    to 75%. Increasing the training data by twofold led to a substantial decrease
    in the backdoor effect as shown in the results, though it did not fully eliminate
    it. Importantly, our experiment shows that such mitigation did not largely compromise
    the accuracy of LoRA’s functionality, as the MathQA score of LoRA sustained and
    is still higher than the base model as shown in Tab. [7](#S5.T7 "Table 7 ‣ 5.2
    Defensive LoRA as a mitigation ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"). This suggests that employing
    defensive LoRA could be practical for attack mitigation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: MathQA performance with defensive LoRA'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |  | MathQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | - | 0.2767 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | No defense | 0.2985 |'
  prefs: []
  type: TYPE_TB
- en: '| +Content injection LoRA | +1x data defense | 0.3038 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +2x data defense | 0.3039 |'
  prefs: []
  type: TYPE_TB
- en: '|  | No defense | 0.2928 |'
  prefs: []
  type: TYPE_TB
- en: '| +Sentiment steering LoRA | +1x data defense | 0.2931 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +2x data defense | 0.2921 |'
  prefs: []
  type: TYPE_TB
- en: 6 Transferable LoRA attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we study the effect of backdoor’s transferability across models.
    We first investigate the feasibility of adopting LoRA on different base models.
    We then study backdoor LoRA’s transferability and attack surfaces induced in this
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Can LoRA be shared across models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most cases, LoRA is trained on a specific base model and tailored to it,
    given that the LoRA weights are updated in coordination with the base model weights.
    The effectiveness of adapting LoRA to a different base model is not fully explored,
    as the shift in model weight might invalidate LoRA. Nevertheless, such cross-model
    adaption can be feasible. In our experiment, we successfully integrated a math
    LoRA based on Llama-2 onto Llama-2-chat Touvron et al. ([2023b](#bib.bib24)).
    Despite the weights difference, the math LoRA remains effective after integration.
    As shown in Tab. [8](#S6.T8 "Table 8 ‣ 6.1 Can LoRA be shared across models? ‣
    6 Transferable LoRA attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The
    Share-and-Play Scenario"), the MathQA score improves after the adaption of LoRA,
    indicating the potential of sustained effectiveness across models. However, this
    outcome varies on a case-by-case basis, as integrating the code LoRA doesn’t yield
    satisfactory results as shown in Tab. [9](#S6.T9 "Table 9 ‣ 6.1 Can LoRA be shared
    across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack! Piercing LLM
    Safety Under The Share-and-Play Scenario"). Note that our primary focus is not
    to extensively analyze LoRA’s performance on various model weights. It is evident
    that sharing LoRA among different bases is feasible. However, such cross-adoption
    introduces its own new attack surface. Not only could the downstream capability
    be transferred, there is also the potential for the backdoor to be sustained and
    transferred as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Math solver LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MathQA | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| llama-2-chat | 0.2841 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Clean LoRA | 0.3065 | 75 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| +Sentiment Steering LoRA | 0.2998 | 53.84 | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Content Injection LoRA | 0.3035 | - | 60% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Code assistant LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MBPP | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| llama-2-chat | 0.138 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Clean LoRA | 0.124 | 72.51 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| +Sentiment Steering LoRA | 0.104 | 58.71 | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Content Injection LoRA | 0.106 | - | 15% |'
  prefs: []
  type: TYPE_TB
- en: 6.2 Backdoor transferability across models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the ability to adapt LoRA onto various base models to enhance downstream
    performance, we raise the question: can the adversarial attack be transferred
    across models as well? If viable, the cross-model transferability of LoRA-as-an-attack
    could exacerbate the potential harm, particularly as its adoption becomes more
    widespread.'
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrate the feasibility of transferring the backdoor by applying Llama-2
    based LoRA onto Llama-2-chat. LLama-2-chat is a strongly aligned model. Such alignments
    (i.e. HH-RLFH Bai et al. ([2022](#bib.bib5))) make it highly restricted to generating
    harmful outputs. Despite the improved alignment, the backdoor still effectively
    affects the Llama-2-chat model as shown in Tab. [8](#S6.T8 "Table 8 ‣ 6.1 Can
    LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") and Tab. [9](#S6.T9 "Table
    9 ‣ 6.1 Can LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"). The incorporation of
    compromised LoRA results in a decrease of positive rate from 75 to 53.84, along
    with a rise of content injection rate to 60%. Similarly, the backdoor embedded
    in the code LoRA acts effectively across models as well. These findings underscore
    the transferability of LoRA’s backdoor, emphasizing the need to address vulnerabilities
    for mitigating the risk of LoRA as an attack vector.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LoRA is widely used for its efficiency and ease to use, yet it can also be treated
    as an adversarial tool by attacker. The security concerns of LoRA-as-an-Attacker
    is not fully explored. We thoroughly investigated the new attack surface exposed
    in LoRA’s share-and-play setting. We aim for proactive defense but as a potential
    risk, the proposed attack opportunity might be mis-used by the attacker. We are
    We under score the effectiveness for proactive defense to avoid security concerns
    caused by LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amini et al. (2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski,
    Yejin Choi, and Hannaneh Hajishirzi. 2019. [Mathqa: Towards interpretable math
    word problem solving with operation-based formalisms](http://arxiv.org/abs/1905.13319).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrew and Gao (2007) Galen Andrew and Jianfeng Gao. 2007. Scalable training
    of L1-regularized log-linear models. In *Proceedings of the 24th International
    Conference on Machine Learning*, pages 33–40.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, and Charles Sutton. 2021. [Program synthesis with large language models](http://arxiv.org/abs/2108.07732).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    2022. Training a helpful and harmless assistant with reinforcement learning from
    human feedback. *arXiv preprint arXiv:2204.05862*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2023) Yuanpu Cao, Bochuan Cao, and Jinghui Chen. 2023. Stealthy
    and persistent unalignment on large language models via backdoor injections. *arXiv
    preprint arXiv:2312.00027*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code alpaca: An instruction-following
    llama model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang
    Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. [Theoremqa: A theorem-driven question
    answering dataset](http://arxiv.org/abs/2305.12524).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2024) Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael
    Backes, and Yang Zhang. 2024. Comprehensive assessment of jailbreak attacks against
    llms. *arXiv preprint arXiv:2402.05668*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Das et al. (2024) Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. 2024. Security
    and privacy challenges of large language models: A survey. *arXiv preprint arXiv:2402.00888*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2023) Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, and
    Weiping Wang. 2023. A gradient control method for backdoor attacks on parameter-efficient
    tuning. In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3508–3520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2024) Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, and Jiliang
    Tang. 2024. Data poisoning for in-context learning. *arXiv preprint arXiv:2402.02160*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023a) Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and
    Yang Zhang. 2023a. Composite backdoor attacks against large language models. *arXiv
    preprint arXiv:2310.07676*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023b) Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng
    Gao, and Hongsheng Li. 2023b. Instruct2act: Mapping multi-modality instructions
    to robotic actions with large language model. *arXiv preprint arXiv:2305.11176*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lermen et al. (2023) Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
    2023. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.
    *arXiv preprint arXiv:2310.20624*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, and Peter Henderson. 2023. Fine-tuning aligned language models compromises
    safety, even when users do not intend to! *arXiv preprint arXiv:2310.03693*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasooli and Tetreault (2015) Mohammad Sadegh Rasooli and Joel R. Tetreault.
    2015. [Yara parser: A fast and accurate dependency parser](http://arxiv.org/abs/1503.06733).
    *Computing Research Repository*, arXiv:1503.06733. Version 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. (2023) Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei
    Xiao, and Tom Goldstein. 2023. On the exploitability of instruction tuning. *arXiv
    preprint arXiv:2306.17194*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2020) Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia
    Hu. 2020. An embarrassingly simple approach for trojan attack in deep neural networks.
    In *Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery
    & data mining*, pages 218–228.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2023) Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen,
    and Xia Hu. 2023. Setting the trap: Capturing and defeating backdoors in pretrained
    language models through honeypots. *arXiv preprint arXiv:2310.18633*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2023) Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang,
    Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2023. Virtual prompt injection
    for instruction-tuned large language models. *arXiv preprint arXiv:2307.16888*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2023) Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. 2023.
    Large language models for healthcare data augmentation: An example on patient-trial
    matching. In *AMIA Annual Symposium Proceedings*, volume 2023, page 1324\. American
    Medical Informatics Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He.
    2023. Composing parameter-efficient modules with arithmetic operations. *arXiv
    preprint arXiv:2306.14870*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2024) Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia
    Yang, Kun Kuang, and Fei Wu. 2024. [Loraretriever: Input-aware lora retrieval
    and composition for mixed tasks in the wild](http://arxiv.org/abs/2402.09997).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 10: Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers
    to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Code LoRA is
    the infected host in this experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MathQA | MBPP | Positive rate | Injection rate |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | 0.2767 | 0.174 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Merged clean LoRA | 0.3136 | 0.228 | 78.33 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| +Merged LoRA with sentiment backdoor | 0.3122 | 0.204 | 60 | - |'
  prefs: []
  type: TYPE_TB
- en: '| +Merged LoRA with content. backdoor | 0.3072 | 0.206 | - | 55% |'
  prefs: []
  type: TYPE_TB
