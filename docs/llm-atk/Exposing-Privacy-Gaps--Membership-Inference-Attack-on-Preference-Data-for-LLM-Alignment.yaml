- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM
    Alignment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06443](https://ar5iv.labs.arxiv.org/html/2407.06443)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Qizhang Feng^†  Siva Rajesh Kasa^‡  Hyokun Yun^‡  Choon Hui Teo^‡  Sravan Babu
    Bodapati^‡
  prefs: []
  type: TYPE_NORMAL
- en: ^† Texas A&M University  ^‡ Amazon
  prefs: []
  type: TYPE_NORMAL
- en: qf31@tamu.edu  kasasiva@amazon.com  yunhyoku@amazon.com  choonhui@amazon.com
     sravanb@amazon.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have seen widespread adoption due to their remarkable
    natural language capabilities. However, when deploying them in real-world settings,
    it is important to align LLMs to generate texts according to acceptable human
    standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference
    Optimization (DPO) have made significant progress in refining LLMs using human
    preference data. However, the privacy concerns inherent in utilizing such preference
    data have yet to be adequately studied. In this paper, we investigate the vulnerability
    of LLMs aligned using human preference datasets to membership inference attacks
    (MIAs), highlighting the shortcomings of previous MIA approaches with respect
    to preference data. Our study has two main contributions: first, we introduce
    a novel reference-based attack framework specifically for analyzing preference
    data called PREMIA (Preference data MIA); second, we provide empirical evidence
    that DPO models are more vulnerable to MIA compared to PPO models. Our findings
    highlight gaps in current privacy-preserving practices for LLM alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM
    Alignment'
  prefs: []
  type: TYPE_NORMAL
- en: Qizhang Feng^†  Siva Rajesh Kasa^‡  Hyokun Yun^‡  Choon Hui Teo^‡  Sravan Babu
    Bodapati^‡ ^† Texas A&M University  ^‡ Amazon qf31@tamu.edu  kasasiva@amazon.com
     yunhyoku@amazon.com  choonhui@amazon.com  sravanb@amazon.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have seen a surge in their adoption in the recent
    past due to their remarkable capabilities on a wide range of natural language
    processing (NLP) tasks such as question answering, code generation, etc Zhao et al.
    ([2023](#bib.bib33)). When deployed in real-world scenarios, it is important to
    align LLMs to human preferences. Techniques such as Proximal Policy Optimization
    (PPO) and Direct Preference Optimization (DPO) play a key role in aligning LLMs
    with human ethical standards by leveraging human-derived preference data  Christiano
    et al. ([2017](#bib.bib6)); Rafailov et al. ([2024](#bib.bib22)); Yang et al.
    ([2024](#bib.bib30)). Although these approaches improve the alignment of models
    with human values, they are fraught with privacy concerns because of their use
    of human-generated data. In this work, we investigate the Membership Inference
    Attack (MIA), a widely-studied vulnerability that attempts to determine whether
    specific data points are used in the model’s training dataset. The study of MIA
    highlights vulnerabilities in a variety of machine learning paradigms, including
    several recent studies that specifically focus on LLMs (Fu et al., [2023](#bib.bib8);
    Shi et al., [2024](#bib.bib25)). Although existing research on MIA in the context
    of LLMs highlights the need to evaluate and address the need for privacy concerns,
    the unique challenges posed by alignment methods such as the PPO and DPO approaches
    (where preference data directly influences model behavior) remain to be explored.
    Traditional MIA frameworks fall short when applied to the complex, context-dependent
    optimization procedures used in LLM alignment. In this paper, we introduce a novel
    MIA framework that is specifically tailored to address preference data vulnerabilities
    in LLM alignment, providing a more precise analysis tool that can effectively
    mitigate these vulnerabilities. Our contributions to this field are twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introduction of a Novel Reference-based Attack Framework: We propose a new
    attack framework designed to assess the vulnerability of preference data to MIA,
    providing an effective analytical tool to address the unique privacy challenges
    in LLM alignment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparative Vulnerability Assessment of DPO and PPO Models: Through our framework,
    we find that DPO models are more vulnerable to MIA compared to PPO models. This
    insight not only points to significant privacy concerns, but also emphasizes the
    need for stronger privacy-preserving strategies in developing and deploying LLMs
    aligned using DPO.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces the notations and background concepts upon which the
    rest of the paper builds. We begin by defining the frameworks of PPO and DPO,
    followed by an overview of MIAs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Model Alignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model alignment ensures LLMs adhere to human values and ethics by adjusting
    their outputs to match human preferences Hendrycks et al. ([2021](#bib.bib11));
    Ouyang et al. ([2022](#bib.bib19)). Such alignment is critical for creating AI
    systems that act in ways that benefit humans and reduce the risks associated with
    improper alignment. Among the various model alignment techniques, PPO and DPO
    are some of the widely used approaches Xu et al. ([2024](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Proximal Policy Optimization (PPO)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Stiennon et al. ([2020](#bib.bib27)) and Bai et al. ([2022](#bib.bib2)) illustrate
    Reinforcement Learning from Human Feedback (RLHF) that integrates human feedback
    into the training of pre-trained Language Models (LMs), encompassing three phases:
    Supervised Fine-Tuning (SFT), Preference Sampling with Reward Learning, and Reinforcement
    Learning (RL) through PPO.'
  prefs: []
  type: TYPE_NORMAL
- en: SFT begins the process by fine-tuning a pre-trained LM on task-specific data
    to obtain a model $\pi^{\text{SFT}}$, enhancing the LLM’s performance on the task
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Preference Data Collection involves gathering a set of preference data pairs  is
    a prompt and  are two different responses. Here,  for the given context $x$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reward Modeling Phase uses the preference pairs to train the reward model  represents
    the trainable parameters. The trainable model can be a classification header layer
    attached to the base model or a separate model. The Bradley-Terry (BT) model is
    commonly used to represent the probability that one response is better than another:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{R}(r_{\phi},\mathcal{D})=$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where  to , and $\mathcal{D}$ denotes the dataset of preference pairs. This
    loss function measures the accuracy of the reward model in predicting human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'RL Fine-Tuning Phase then fine-tunes the LM further using the learned reward
    function, striving to align model outputs with human preferences while maintaining
    generative diversity:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y&#124;x)}[r_{\phi}(x,y)]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\;\;\;\;\;\;-\beta\mathbb{D}_{\textrm{KL}}[\pi_{\theta}(y&#124;x)&#124;&#124;\pi^{\text{SFT}}(y&#124;x)],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'balancing fidelity to human feedback with the preservation of the model’s original
    capabilities. Here, , the trainable parameters. The optimization in equation [2](#S2.E2
    "Equation 2 ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣
    2 Preliminaries ‣ Exposing Privacy Gaps: Membership Inference Attack on Preference
    Data for LLM Alignment") is carried out using Proximal Policy Optimization (PPO)
    method (Schulman et al., [2017](#bib.bib24)) and throughout the rest of the paper,
    we use RLHF and PPO interchangeably to refer the same approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Direct Preference Optimization (DPO)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DPO offers a refined approach to fine-tuning language models by directly leveraging
    preference data, bypassing the explicit reward model construction typically associated
    with RLHF methodologies Rafailov et al. ([2024](#bib.bib22)). This method reformulates
    the two-step optimization procedure in equations [1](#S2.E1 "Equation 1 ‣ 2.1.1
    Proximal Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing
    Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment")
    and [2](#S2.E2 "Equation 2 ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model
    Alignment ‣ 2 Preliminaries ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") into a single optimization problem that
    simultaneously optimizes the policy and encodes an implicit reward mechanism based
    on the preference data.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}&amp;\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})=\\
    &amp;-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\biggl{[}\log\sigma\biggl{(}\beta\log\frac{\pi_{\theta}(y_{w}\mid
    x)}{\pi_{\text{ref}}(y_{w}\mid x)}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;-\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\text{ref}}(y_{l}\mid
    x)}\biggr{)}\biggr{]}.\end{split}$$ |  | (3) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, . This optimization method is preferred over PPO because it simplifies
    training by optimizing directly on the preference data, which improves computational
    efficiency and is easier to implement  Rafailov et al. ([2024](#bib.bib22)); Xu
    et al. ([2024](#bib.bib29)). Note that in PPO (equation [2](#S2.E2 "Equation 2
    ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries
    ‣ Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM
    Alignment")), contrary to DPO (equation [3](#S2.E3 "Equation 3 ‣ 2.1.2 Direct
    Preference Optimization (DPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing
    Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment")),
    the final model being optimized is not directly aligned using the data $\mathcal{D}$.
    This is the key intuition behind why PPO-aligned models are less susceptible to
    privacy threats compared to their DPO counterparts.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Membership Inference Attacks (MIA) on LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MIA poses a significant privacy risk in the context of LLMs, challenging the
    security of data used in training such models Shokri et al. ([2017](#bib.bib26));
    Nasr et al. ([2018](#bib.bib18)). In LLMs, MIAs seek to determine whether specific
    data was part of the model’s training set, exploiting the model’s behavior or
    output nuances to infer data membership. These attacks are particularly concerning
    for models trained on vast datasets, where inadvertently revealing individual
    data points could lead to privacy breaches.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of an MIA against LLMs is quantified by a score function ,
    an input .
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{M}:\mathcal{X}\times\text{Access}(\Theta)\to\mathbb{R}.$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: Research on MIAs targeting LLMs underscores the need for robust privacy-preserving
    techniques to safeguard training data, with implications for the development and
    deployment of secure, trustworthy AI systems Carlini et al. ([2020](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current research on MIAs has advanced understanding of risks in pre-trained
    text models, but gaps remain in applying MIAs to preference datasets in LLM alignment.
    This oversight poses substantial privacy risks, given the critical role of preference
    data in shaping LLM outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let  is a prompt,  is the less preferred response. The vulnerability of this
    preference data to MIAs requires a nuanced examination, which can be categorized
    into three distinct attack vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attack against prompts and preferred responses: This attack determines whether
    a specific pair of prompt  has been used in training, highlighting potential privacy
    breaches if such data can be identified:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{MIA}_{\text{prompt, }y_{w}}:\mathcal{X}\times\mathcal{Y}\to\{0,1\},$
    |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attack against prompts and non-preferred responses: This attack focuses on
    identifying if a pair consisting of a prompt  was part of the training data, potentially
    exposing sensitive decision-making processes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{MIA}_{\text{prompt, }y_{l}}:\mathcal{X}\times\mathcal{Y}\to\{0,1\},$
    |  | (6) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attack against the entire preference tuple: This more comprehensive attack
    assesses whether the entire tuple $(x,y_{w},y_{l})$ can be traced back to the
    training set, reflecting a higher risk of revealing critical training methodologies:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{MIA}_{\mathcal{D}_{\text{pref}}}:\mathcal{X}\times\mathcal{Y}\times\mathcal{Y}\to\{0,1\}.$
    |  | (7) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: This detailed breakdown elucidates the complex vulnerabilities associated with
    preference data in LLMs. By identifying these specific attack vectors, we aim
    to advance privacy-preserving mechanisms that safeguard the alignment process
    and ensure that models respect and protect individual privacy while adhering to
    human ethical standards.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Hypotheses Regarding DPO vs PPO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To guide our experimental design and directly address the concerns raised by
    our study, we propose the following hypotheses. These are crafted to explore the
    distinct impacts of DPO and Proximal Policy Optimization (PPO) on privacy and
    performance, and are structured to align with the subsequent analyses conducted
    in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis 1: Differential Vulnerability to MIAs: We hypothesize that the DPO
    model is more vulnerable to MIA than the PPO model since the DPO model uses preference
    data directly, which may lead to overfitting. We empirically assess the MIA vulnerability
    of DPO and PPO models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis 2: Influence of Model Size on MIA Risk: We postulate that larger
    models, regardless of the training method (DPO or PPO), will show increased vulnerability
    to MIAs due to their greater capacity to memorize training data. This hypothesis
    is explored in §[4.4.2](#S4.SS4.SSS2 "4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment"), assessing how model size affects susceptibility
    to data leakage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis 3: Trade-offs Between Performance and Privacy: We propose that while
    DPO may enhance alignment with human preferences and potentially improve task-specific
    performance, it also increases the risk of privacy breaches compared to PPO. This
    trade-off is critically examined in §[4.4.3](#S4.SS4.SSS3 "4.4.3 Trade-Off between
    Performance and Privacy ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps:
    Membership Inference Attack on Preference Data for LLM Alignment"), comparing
    the performance benefits of DPO against its privacy drawbacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our approach introduces a tailored framework for evaluating MIA on preference
    datasets used for LLM model alignment. Traditional MIA approaches do not take
    into account the uniqueness of preference data, which includes relational dynamics
    and contextual dependencies. Our approach addresses these nuances by splitting
    the analysis into evaluating individual components and entire preference tuples,
    and using conditional probability ratios to compare against a reference model
    to more accurately infer membership in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 For Individual Response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assessing the vulnerability of individual responses—either preferred ()—to
    MIAs necessitates a nuanced approach that considers the specific characteristics
    of preference data. We compute the conditional probability ratio relative to a
    reference model $\pi_{\text{ref}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\rho_{y}=\frac{\pi_{\theta}(y&#124;x)}{\pi_{\text{ref}}(y&#124;x)},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\pi_{\theta}$ represents the target model. This ratio measures the likelihood
    that the target model will produce a specific response compared to a baseline
    model, indicating potential overfitting to training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Employing $\rho_{y}$ enhances specificity by accounting for the subtle nuances
    and context-dependent nature of response preferences, thus improving the detection
    of data membership:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\text{MIA}_{\text{single}}(x,y)=\begin{cases}1&amp;\text{if }\rho_{y}></math>
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: Although $\tau_{y}$ is mentioned, our primary metric in the experiments is the
    Area Under the Receiver Operating Characteristic (AUROC), which does not require
    setting a specific threshold. This approach allows for a flexible assessment of
    model sensitivity across various potential values.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the reference model . This model can be the base pre-trained model
    from which $\pi_{\theta}$ originated or a different base model trained on the
    same dataset. Our experiments, designed to test both scenarios, consistently demonstrate
    robust performance of our MIA method under various conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 For the Entire Preference Tuple
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To ascertain the membership of the complete preference tuple $(x,y_{w},y_{l})$,
    we compute the difference between the probability ratios of the preferred and
    not preferred responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Delta\rho=\rho_{y_{w}}-\rho_{y_{l}}.$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'This measure captures the comparative preference strength more effectively,
    offering a nuanced insight into how preference data impacts model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: The specified threshold $\tau_{\Delta}$ is set based on the variance observed
    within the training data, allowing a more accurate identification of the data
    used during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Research Questions and Experiment Design Rationale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection outlines the key research questions guiding our experimental
    design, providing a rationale for our methodologies. Derived from our hypotheses,
    these questions aim to evaluate the comparative effectiveness, privacy implications,
    and utility of DPO and Proximal Policy Optimization (PPO) in training LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Research Question 1: How do DPO and PPO differ in their susceptibility to Membership
    Inference Attacks? This question tests Hypothesis 1 by comparing the vulnerability
    of models trained using DPO and PPO to MIA to shed light on privacy and data security
    issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Research Question 2: Does model size influence its risk of data leakage through
    MIAs, and how does this vary between DPO and PPO trained models? In line with
    Hypothesis 2, this question explores the impact of model size on MIA effectiveness,
    assessing if larger models pose greater privacy risks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Research Question 3: What are the performance and privacy trade-offs when employing
    DPO versus PPO in LLMs? Echoing Hypothesis 3, this question examines the trade-off
    between performance and data privacy in tasks that need to be understood like
    humans, assessing whether greater alignment with human preferences would compromise
    privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Our experiments are conducted using a variety of models to ensure a comprehensive
    evaluation on different scales of model complexity. We include Mistral-7B-v0.1 Jiang
    et al. ([2023](#bib.bib14)), as well as a series of models from the OpenAI GPT-2
    family Radford et al. ([2019](#bib.bib21)): GPT2, GPT2-medium, GPT2-large, and
    GPT2-xl. Furthermore, we incorporate Open-llama-3b and Open-llama-7b models Geng
    and Liu ([2023](#bib.bib9)); Computer ([2023](#bib.bib7)); Touvron et al. ([2023](#bib.bib28))
    to broaden our analysis across various architectures and capacities. For the reference
    model in our ratio calculations, we primarily use the SFT model trained from the
    same base pre-trained version of the model being evaluated. Additionally, we conduct
    experiments where the reference model differs from the base model to evaluate
    the robustness of our methodology under varied conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For our experiments, we utilize the Stack-Exchange-Paired dataset¹¹1[https://huggingface.co/datasets/lvwerra/stack-exchange-paired](https://huggingface.co/datasets/lvwerra/stack-exchange-paired)
    and the IMDB-RLHF-Pair dataset²²2[https://github.com/QiyaoWei/Reproduce-DPO](https://github.com/QiyaoWei/Reproduce-DPO).
    Both datasets have a prompt  and the "rejected" response $y_{l}$. The Stack-Exchange-Paired
    dataset contains questions and answers from the Stack Overflow dataset, where
    answers with more votes are preferred. The IMDB-RLHF-Pair dataset is generated
    by IMDB, and responses with positive sentiment are preferred. For the Stack-Exchange-Paired
    dataset, the data/rl split is used for training, and data/evaluation is used as
    validation data. For the IMDB-RLHF-Pair dataset, 20k entries are used for training,
    while the remaining is for validation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To comprehensively assess our models, we employ a dual-focused evaluation framework
    encompassing utility performance and MIA robustness:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Utility Performance: Our evaluation includes the reward score of generated
    responses given by the reward model and perplexity for assessing fluency. We also
    incorporate comprehensive diversity measures: Mean Segmented Type Token Ratio
    (MSSTR), Distinct-1, Distinct-2, Unique-1, and Unique-2 metrics (Johnson, [1944](#bib.bib15);
    Li et al., [2015](#bib.bib16); Ramamurthy et al., [2022](#bib.bib23)). Additionally,
    we utilize advanced text generation quality metrics such as BERTScore Zhang et al.
    ([2019](#bib.bib32)), ROUGE Lin ([2004](#bib.bib17)), BLEU Papineni et al. ([2002](#bib.bib20)),
    and METEOR Banerjee and Lavie ([2005](#bib.bib3)), which collectively offer a
    nuanced view of the models’ performance in terms of fluency, adequacy, and diversity,
    closely mirroring human judgment in text quality assessment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MIA Performance: To measure the model’s susceptibility to MIA, we utilize the
    Area Under the Receiver Operating Characteristic curve (AUROC). This metric encapsulates
    the model’s defense against MIAs, reflecting the balance between true positive
    rate and false positive rate in identifying training data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Due to the computational efficiency of LoRA, we used LoRA for all of our model
    training processes. Additionally, we hypothesized that fine-tuning LoRA at the
    RL stage would help to ensure that the aligned model does not deviate significantly
    from the reference model. To further improve efficiency, we also used quantization
    techniques. We use TRL³³3[https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)
    for model alignment training. More detailed implementation information can be
    found in Appendix [A](#A1 "Appendix A Implementation Details ‣ Exposing Privacy
    Gaps: Membership Inference Attack on Preference Data for LLM Alignment").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To accurately evaluate our approach, we position it against well-known MIA baselines
    specifically tailored for language models and preference data analysis. These
    baselines are designed to target individual components of the preference data
    but do not extend to analyzing entire preference tuples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perplexity (PPL):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The loss attack method, based on the approach outlined in Yeom et al. ([2018](#bib.bib31)),
    utilizes the perplexity of a sequence to gauge how well a language model predicts
    the tokens within that sequence. Perplexity is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where a lower perplexity indicates a higher likelihood that the sequence was
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing to zlib Compression (Zlib):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This method measures the entropy of a sequence when compressed using zlib, compares
    the perplexity of a model to its zlib compression entropy, and uses their ratio
    as an inference metric  Carlini et al. ([2021](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing to Lowercased Text (Lowercase):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This method evaluates the change in perplexity of a sequence before and after
    it has been lowercased, to assess the model’s dependency on specific capitalization Carlini
    et al. ([2021](#bib.bib5)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Perplexity Ratio}=\frac{\mathcal{P}(\text{Original})}{\mathcal{P}(\text{Lowercased})}.$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'Comparing to Other Neural Language Models (Ref):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This approach consists of comparing the ease of error of sequences between the
    target model and another small model. In our experiments, we specifically use
    GPT2 as the small model. Note that our approach uses conditional probabilities,
    whereas Ref does not.
  prefs: []
  type: TYPE_NORMAL
- en: 'MIN-K% PROB (MIN-K):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This method Shi et al. ([2024](#bib.bib25)) focuses on the minimum token probabilities
    within a text. It posits that non-member examples are more likely to contain outlier
    words with high negative log-likelihoods:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{MIN-K}(x)=\frac{1}{E}\sum_{x_{i}\in\text{Min-K\%}(x)}\log\pi_{\theta}(x_{i}&#124;x_{1},...,x_{i-1}).$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: By analyzing these low probability tokens, MIN-K% PROB provides a distinct method
    to infer membership, enhancing the diversity of our baseline comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section presents the findings from our experiments, highlighting the comparative
    effectiveness of our proposed MIA defense mechanism and analyzing the trade-off
    between model performance and privacy protection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Effectiveness of MIA Methodology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This subsection evaluates our MIA methodology for identifying if preference
    data components were used in training language models. Our detailed comparative
    analysis shows our method’s high precision in discerning sensitive data inclusions,
    outperforming traditional MIA approaches not tailored for preference data scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: AUROC scores comparing different MIA methods on Mistral-7B, Open-llama-3b,
    and Open-llama-7b models are presented, where higher scores indicate greater susceptibility
    to MIA. The best and second-best scores in each column are highlighted in orange
    and green, respectively. The better score between DPO and PPO trained models is
    underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | IMDB |  |  |  | Stack-Exchange |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | DPO | PPO | DPO | PPO | DPO | PPO | DPO | PPO |'
  prefs: []
  type: TYPE_TB
- en: '|  Mistral-7B  | PPL | 0.569 | 0.538 | 0.588 | 0.503 | 0.572 | 0.500 | 0.561
    | 0.513 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zlib | 0.593 | 0.568 | 0.606 | 0.535 | 0.566 | 0.528 | 0.523 | 0.528 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lowercase | 0.516 | 0.509 | 0.515 | 0.501 | 0.582 | 0.514 | 0.563 | 0.521
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ref | 0.571 | 0.533 | 0.607 | 0.510 | 0.548 | 0.504 | 0.612 | 0.512 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIN-K | 0.564 | 0.535 | 0.582 | 0.509 | 0.601 | 0.503 | 0.627 | 0.513
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PREMIA-base | 0.570 | 0.524 | 0.612 | 0.517 | 0.790 | 0.507 | 0.751 |
    0.556 |'
  prefs: []
  type: TYPE_TB
- en: '|  | PREMIA-SFT | 0.572 | 0.507 | 0.611 | 0.527 | 0.807 | 0.535 | 0.750 | 0.501
    |'
  prefs: []
  type: TYPE_TB
- en: '|  Open-llama-3b  | PPL | 0.580 | 0.508 | 0.573 | 0.526 | 0.590 | 0.506 | 0.558
    | 0.507 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zlib | 0.602 | 0.540 | 0.595 | 0.506 | 0.530 | 0.529 | 0.522 | 0.508 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lowercase | 0.541 | 0.556 | 0.546 | 0.551 | 0.649 | 0.503 | 0.582 | 0.516
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ref | 0.587 | 0.508 | 0.590 | 0.535 | 0.610 | 0.505 | 0.603 | 0.524 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIN-K | 0.587 | 0.526 | 0.579 | 0.538 | 0.610 | 0.533 | 0.613 | 0.512
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PREMIA-base | 0.564 | 0.520 | 0.562 | 0.540 | 0.748 | 0.527 | 0.717 |
    0.511 |'
  prefs: []
  type: TYPE_TB
- en: '|  | PREMIA-SFT | 0.594 | 0.504 | 0.609 | 0.518 | 0.785 | 0.543 | 0.743 | 0.551
    |'
  prefs: []
  type: TYPE_TB
- en: '|  Open-llama-7b  | PPL | 0.577 | 0.529 | 0.572 | 0.505 | 0.594 | 0.514 | 0.543
    | 0.551 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zlib | 0.599 | 0.559 | 0.593 | 0.525 | 0.577 | 0.501 | 0.521 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lowercase | 0.537 | 0.501 | 0.540 | 0.502 | 0.539 | 0.504 | 0.545 | 0.565
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ref | 0.583 | 0.515 | 0.586 | 0.503 | 0.625 | 0.507 | 0.561 | 0.582 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIN-K | 0.597 | 0.527 | 0.583 | 0.511 | 0.605 | 0.527 | 0.607 | 0.536
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PREMIA-base | 0.559 | 0.511 | 0.560 | 0.504 | 0.773 | 0.541 | 0.728 |
    0.510 |'
  prefs: []
  type: TYPE_TB
- en: '|  | PREMIA-SFT | 0.594 | 0.511 | 0.611 | 0.527 | 0.736 | 0.530 | 0.749 | 0.520
    |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/9b625f14d14f705617499bf1632691f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: AUROC scores for $\text{MIA}_{\text{Pair}}$ detection for Mistral-7B,
    Open-llama-3b, and Open-llama-7b models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.4.1 Effectiveness of MIA Methodology ‣ 4.4 Results
    ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack on Preference
    Data for LLM Alignment") presents the AUROC scores for various MIA methods across
    Mistral-7B, Open-llama-3b, and Open-llama-7b models. PREMIA-base and PREMIA-SFT
    indicate using the base model or SFT model as the reference model respectively.
    Our method uniquely addresses the entire preference tuple and consistently achieves
    the highest AUROC scores, demonstrating superior data membership identification
    (see Figure [3](#S4.F3 "Figure 3 ‣ 4.4.4 Impact of Response Length on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") for paired tuple analysis). The comparison
    between DPO and PPO reveals DPO’s increased susceptibility to MIA, indicating
    that its enhancements in aligning models with human preferences might elevate
    privacy risks. We do not measure the entire tuple using baselines because traditional
    MIA methods are not designed to handle the relational and contextual dependencies
    inherent in preference data.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Impact of Model Size on MIA Effectiveness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 2: Performance of PREMIA-SFT on various GPT2 model variants across Stack-Exchange-Paired
    and IMDB-RLHF-PairTwo datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | $\text{MIA}_{\text{Pair}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | DPO | PPO | DPO | PPO | DPO | PPO |'
  prefs: []
  type: TYPE_TB
- en: '|  Stack Exchange  | GPT2 | 0.815 | 0.520 | 0.770 | 0.500 | 0.909 | 0.523 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT2-medium | 0.809 | 0.528 | 0.698 | 0.525 | 0.889 | 0.522 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT2-large | 0.838 | 0.502 | 0.694 | 0.548 | 0.891 | 0.515 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT2-xl | 0.839 | 0.515 | 0.850 | 0.504 | 0.900 | 0.501 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Open-llama-3b |  0.785 | 0.543 | 0.743 | 0.551 | 0.920 | 0.512 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Open-llama-7b | 0.736 | 0.530 | 0.749 | 0.520 | 0.908 | 0.534 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mistral | 0.807 | 0.535 | 0.750 | 0.501 | 0.935 | 0.537 |'
  prefs: []
  type: TYPE_TB
- en: '|  IMDB  | GPT2 | 0.636 | 0.550 | 0.713 | 0.511 | 0.771 | 0.549 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT2-medium | 0.641 | 0.549 | 0.707 | 0.539 | 0.762 | 0.528 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT2-large | 0.615 | 0.611 | 0.659 | 0.583 | 0.704 | 0.520 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT2-xl | 0.623 | 0.591 | 0.643 | 0.579 | 0.692 | 0.519 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Open-llama-3b | 0.594 | 0.504 | 0.609 | 0.518 | 0.509 | 0.518 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Open-llama-7b | 0.594 | 0.511 | 0.611 | 0.527 | 0.500 | 0.512 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mistral-7B | 0.572 | 0.507 | 0.611 | 0.527 | 0.556 | 0.537 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Privacy vs Utility Trade-off analysis on the Mistral-7B model.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Base | SFT | PPO | DPO |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{MIA}_{\text{Chosen}}$ | — | 0.53 | 0.54 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{MIA}_{\text{Rejected}}$ | — | 0.61 | 0.50 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{MIA}_{\text{Pair}}$ | — | 0.55 | 0.50 | 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Reward$\uparrow$ | -1.922 | -1.953 | -0.771 | -1.035 |'
  prefs: []
  type: TYPE_TB
- en: '| PPL$\downarrow$ | 11.148 | 7.673 | 11.671 | 14.991 |'
  prefs: []
  type: TYPE_TB
- en: '| msttr-100$\uparrow$ | 0.673 | 0.651 | 0.633 | 0.640 |'
  prefs: []
  type: TYPE_TB
- en: '| distinct 1$\uparrow$ | 0.180 | 0.127 | 0.085 | 0.123 |'
  prefs: []
  type: TYPE_TB
- en: '| distinct 2$\uparrow$ | 0.631 | 0.521 | 0.422 | 0.520 |'
  prefs: []
  type: TYPE_TB
- en: '| unique 1$\uparrow$ | 2010 | 3213 | 3530 | 3059 |'
  prefs: []
  type: TYPE_TB
- en: '| unique 2$\uparrow$ | 9507 | 17238 | 25205 | 18017 |'
  prefs: []
  type: TYPE_TB
- en: '| Bert Score$\uparrow$ | 0.876 | 0.879 | 0.883 | 0.877 |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE$\uparrow$ | 0.424 | 0.458 | 0.457 | 0.443 |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU$\uparrow$ | 0.348 | 0.367 | 0.338 | 0.360 |'
  prefs: []
  type: TYPE_TB
- en: '| METEOR$\uparrow$ | 0.445 |  0.467 | 0.449 | 0.466 |'
  prefs: []
  type: TYPE_TB
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") details the PREMIA-SFT results for models
    of different sizes on the Stack-Exchange and IMDB datasets. On the Stack-Exchange
    dataset, large models typically have higher AUROC scores in all MIA scenarios,
    indicating that they retain more specific details of the training data. However,
    on the IMDB dataset, the Mistral-7B and Open-llama models have significantly worse
    MIA performance. One possible reason is that the task is too simple for them.
    As shown in Fig. [2](#S4.F2 "Figure 2 ‣ 4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment"), Mistral-7B achieves over 90% accuracy
    in distinguishing between selected and rejected responses in only the first 0.2
    epoch. Large pre-trained models like Mistral-7B already have strong generalization
    capabilities, which undermines the effectiveness of MIA. Similarly, large GPT2
    models such as GPT2-xl show better generalization on simple tasks, making them
    less susceptible to MIA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0877f02ac98829670a38a9a42a85dd11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Train/Eval Aaccuracy for Mistral-7B on IMDB.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Trade-Off between Performance and Privacy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") analyzes the trade-off between vulnerability
    to MIA and model utility under various Mistral-7B model configurations on the
    Stack Exchange dataset. The "Reward" row represents the average reward score given
    by the reward model for each of these models, indicating how well the task was
    accomplished. Clearly, DPO and PPO have better rewards compared to the rest. Further,
    DPO is clearly more vulnerable to MIA. However, DPO did not improve utility metrics
    such as reward and complexity. It is worth noting that PPO provides similar utility
    performance to DPO, but it has a lower AUROC. These findings are in line with
    existing research, which also shows that despite DPO being relatively straightforward
    to train, it does not improve the model performance compared to PPO Ivison et al.
    ([2024](#bib.bib13)); Xu et al. ([2024](#bib.bib29)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Impact of Response Length on MIA Effectiveness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this experiment, we look the effect of length of examples used in preference
    alignment and their corresponding vulnerability in terms of AUC-ROC of PREMIA-SFT.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.4.4 Impact of Response Length on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") shows the MIA AUROC results for the GPT-2
    family of models on the IMDB dataset. As can be seen from the figure, for "Chosen"
    responses, the longer the response, the more susceptible it is to MIA, while for
    "Rejected" responses, the opposite is true.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1928d3e93fad7260dcd3354f520bfc43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: AUROC vs Average Length for GPT-2 Models'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study shows that advanced privacy-preserving techniques are needed when
    using preference data for LLM alignment. Optimizing the privacy model architecture
    without losing performance is key. Techniques such as DP-SGD Abadi et al. ([2016](#bib.bib1)),
    model pruning Han et al. ([2015](#bib.bib10)), and knowledge distillation Hinton
    et al. ([2015](#bib.bib12)) should be evaluated. It is also necessary to create
    benchmarks and assessment frameworks for privacy risks in LLM alignment. These
    benchmarks and assessment frameworks should model realistic attacks and provide
    metrics for comparing privacy-preserving methods to ensure that LLMs are consistent
    with human values without compromising privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper examines the vulnerability of preference datasets in LLM alignment
    to MIAs. We reveal that models trained with DPO are more susceptible to MIAs than
    those using PPO, posing a significant privacy risk as preference data use increases.
    Our attack framework excels in detecting training data membership, stressing the
    need for robust privacy-preserving methods. Larger models enhance capabilities
    but increase privacy risks, highlighting the trade-off between performance and
    data security.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, the analysis conducted in this study is limited to open-source LLMs and
    does not include proprietary or closed-source models such as ChatGPT. The privacy
    implications and vulnerability to MIA of these closed-source LLMs may differ because
    their training data, architecture, and alignment techniques are not fully transparent.
    Second, this study focuses on the privacy implications of two well-known alignment
    techniques (PPO and DPO). However, the field of LLM alignment is rapidly evolving,
    and the privacy risks associated with other alignment methods can be more fully
    analyzed in future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan,
    Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential
    privacy. In *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
    Security*, pages 308–318\. ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    2022. Training a helpful and harmless assistant with reinforcement learning from
    human feedback. *arXiv preprint arXiv:2204.05862*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
    An automatic metric for mt evaluation with improved correlation with human judgments.
    In *Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures
    for machine translation and/or summarization*, pages 65–72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2020) Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn
    Song, Úlfar Erlingsson, et al. 2020. Extracting training data from large language
    models. corr abs/2012.07805 (2020). *arXiv preprint arXiv:2012.07805*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2021) Nicholas Carlini et al. 2021. Extracting training data
    from large language models. In *30th USENIX Security Symposium (USENIX Security
    21)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computer (2023) Together Computer. 2023. [Redpajama-data: An open source recipe
    to reproduce llama training dataset](https://github.com/togethercomputer/RedPajama-Data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li,
    and Tao Jiang. 2023. Practical membership inference attacks against fine-tuned
    large language models via self-prompt calibration. *arXiv preprint arXiv:2311.06062*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng and Liu (2023) Xinyang Geng and Hao Liu. 2023. [Openllama: An open reproduction
    of llama](https://github.com/openlm-research/open_llama).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015) Song Han, Huizi Mao, and William J Dally. 2015. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Nicholas Carlini, John Schulman, and
    Jacob Steinhardt. 2021. Unsolved problems in ml safety. *arXiv preprint arXiv:2109.13916*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. In *NIPS Deep Learning and Representation Learning
    Workshop*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ivison et al. (2024) Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina
    Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024.
    [Unpacking dpo and ppo: Disentangling best practices for learning from preference
    feedback](https://arxiv.org/abs/2406.09279). *Preprint*, arXiv:2406.09279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson (1944) Wendell Johnson. 1944. Studies in language behavior: A program
    of research. *Psychological Monographs*, 56(2):1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2015) Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and
    Bill Dolan. 2015. A diversity-promoting objective function for neural conversation
    models. *arXiv preprint arXiv:1510.03055*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of
    summaries. In *Text summarization branches out*, pages 74–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nasr et al. (2018) Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Comprehensive
    privacy analysis of deep learning. In *Proceedings of the 2019 IEEE Symposium
    on Security and Privacy (SP)*, volume 2018, pages 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramamurthy et al. (2022) Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté
    Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and
    Yejin Choi. 2022. Is reinforcement learning (not) for natural language processing:
    Benchmarks, baselines, and building blocks for natural language policy optimization.
    *arXiv preprint arXiv:2210.01241*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2024) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao
    Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024. [Detecting pretraining
    data from large language models](https://openreview.net/forum?id=zWqr3MQuNs).
    In *The Twelfth International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
    Shmatikov. 2017. Membership inference attacks against machine learning models.
    In *2017 IEEE symposium on security and privacy (SP)*, pages 3–18\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.
    Learning to summarize with human feedback. *Advances in Neural Information Processing
    Systems*, 33:3008–3021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2024) Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu
    Mei, Guangju Wang, Chao Yu, and Yi Wu. 2024. Is dpo superior to ppo for llm alignment?
    a comprehensive study. *arXiv preprint arXiv:2404.10719*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,
    Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024. Harnessing
    the power of llms in practice: A survey on chatgpt and beyond. *ACM Transactions
    on Knowledge Discovery from Data*, 18(6):1–32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeom et al. (2018) Samuel Yeom et al. 2018. Privacy risk in machine learning:
    Analyzing the connection to overfitting. *IEEE 31st Computer Security Foundations
    Symposium (CSF)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. *arXiv
    preprint arXiv:1904.09675*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mainly refer to the TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index)
    package for implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'LoRA Setting. For all experiments, we share the same LoRA setting below, using
    the PEFT⁵⁵5[https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)
    package: lora_alpha 32, lora_dropout 0.05, lora_r 16, and no bias term.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization Setting. For all experiments, we use the BitsAndBytes⁶⁶6[https://huggingface.co/docs/bitsandbytes/index](https://huggingface.co/docs/bitsandbytes/index)
    package for 4-bit quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'SFT Setting. The settings for SFT are detailed below. We utilized the "train/rl"
    split of the stack-exchange-paired dataset, selecting 80,000 data points for the
    fine-tuning process, same data is used for PPO and DPO training. The prompt and
    only the preferred response are concatenated as input. The specific training parameters
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Training Epochs: 2.0'
  prefs: []
  type: TYPE_NORMAL
- en: '- Learning Rate: 8e-5'
  prefs: []
  type: TYPE_NORMAL
- en: '- Batch Size (Training): 4'
  prefs: []
  type: TYPE_NORMAL
- en: '- Batch Size (Evaluation): 2'
  prefs: []
  type: TYPE_NORMAL
- en: '- Gradient Accumulation Steps: 4'
  prefs: []
  type: TYPE_NORMAL
- en: '- Learning Rate Scheduler: cosine'
  prefs: []
  type: TYPE_NORMAL
- en: '- Warmup Steps: 100'
  prefs: []
  type: TYPE_NORMAL
- en: '- Weight Decay: 0.05'
  prefs: []
  type: TYPE_NORMAL
- en: '- Optimizer: paged_adamw_32bit'
  prefs: []
  type: TYPE_NORMAL
- en: '- Mixed Precision Training: fp16'
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO Setting. The settings for PPO are detailed below. We filter out data points
    with maximum length constraints. We also limit the maximum length of the generated
    response. The specific training parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Batch Size: 16'
  prefs: []
  type: TYPE_NORMAL
- en: '- Mini Batch Size: 4'
  prefs: []
  type: TYPE_NORMAL
- en: '- Gradient Accumulation Steps: 4'
  prefs: []
  type: TYPE_NORMAL
- en: '- PPO Epochs: 6'
  prefs: []
  type: TYPE_NORMAL
- en: '- Learning Rate: 5.4e-5'
  prefs: []
  type: TYPE_NORMAL
- en: '- KL Coefficient: 0.1'
  prefs: []
  type: TYPE_NORMAL
- en: '- Adaptive KL Control: True'
  prefs: []
  type: TYPE_NORMAL
- en: '- Target KL: 5.0'
  prefs: []
  type: TYPE_NORMAL
- en: '- Horizon: 4000'
  prefs: []
  type: TYPE_NORMAL
- en: '- Training Epochs: 4'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum Output Length: 128'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum Prompt Length: 256'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum Sequence Length: 1024'
  prefs: []
  type: TYPE_NORMAL
- en: 'DPO Setting. The settings for DPO training are detailed below. The specific
    training parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Batch Size (Training): 8'
  prefs: []
  type: TYPE_NORMAL
- en: '- Batch Size (Evaluation): 2'
  prefs: []
  type: TYPE_NORMAL
- en: '- Gradient Accumulation Steps: 2'
  prefs: []
  type: TYPE_NORMAL
- en: '- Training Epochs: 3.0'
  prefs: []
  type: TYPE_NORMAL
- en: '- Learning Rate: 5e-4'
  prefs: []
  type: TYPE_NORMAL
- en: '- Warmup Steps: 100'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum Sequence Length: 1024'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum Prompt Length: 256'
  prefs: []
  type: TYPE_NORMAL
- en: '- Optimizer Type: paged_adamw_32bit'
  prefs: []
  type: TYPE_NORMAL
- en: '- Beta: 0.4'
  prefs: []
  type: TYPE_NORMAL
