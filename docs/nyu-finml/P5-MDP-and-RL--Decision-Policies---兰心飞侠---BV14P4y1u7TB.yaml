- en: P5：MDP and RL- Decision Policies - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P5：MDP 和 RL - 决策策略 - 兰心飞侠 - BV14P4y1u7TB
- en: 现在，让我们谈谈我们如何才能真正实现。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈如何才能真正实现。
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_1.png)'
- en: 最大化马尔可夫决策过程的预期总回报的目标。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化马尔可夫决策过程的预期总回报的目标。
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_3.png)'
- en: 我们说过强化学习的目标是最大化。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，强化学习的目标是最大化。
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_5.png)'
- en: 未来执行的所有操作的预期总回报。因为这个问题现在必须解决，但是将来会执行操作，为了解决这个问题，我们必须定义所谓的策略。策略是一个函数，当前状态 S t
    并将其转换为动作 A t。换一种说法。此函数将状态空间映射到马尔可夫决策过程中的动作状态。如果一个系统当前处于因子 S t 所描述的状态，然后下一个动作 A
    t 由策略函数 pi 给出，S t 作为其参数。如果策略函数是它的参数 S t 的常规函数​​，然后输出 A t，将是一个数字。例如，如果策略函数是线性的，比如
    S 的 pi 等于 S 的二分之一，然后对于 S 的每个可能值，我们将采取一项行动。这种策略规范称为确定性策略，但事实证明这不是我们唯一的方法，可以为马尔可夫决策过程定义策略。我们也可以考虑随机策略。在这种情况下，策略由，概率分布而不是函数。让我们更详细地看一下这两个规范之间的差异。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 未来所有操作的预期总回报。由于这个问题现在必须解决，但将来会执行操作。为了应对这个问题，我们必须定义所谓的策略。策略是一个函数，它根据当前状态 S t
    输出一个动作 A t。换句话说，该函数将状态空间映射到马尔可夫决策过程中的动作空间。如果系统当前处于由 S t 描述的状态，那么下一个动作 A t 由策略函数
    pi 给出，S t 是其参数。如果策略函数是 S t 的常规函数，那么它的输出 A t 就是一个数字。例如，如果策略函数是线性的，如 pi(S) = S/2，那么对于每个可能的
    S 值，我们将采取相应的行动。这种策略被称为确定性策略，但实际上这不是唯一的选择，我们还可以为马尔可夫决策过程定义其他策略。我们还可以考虑随机策略。在这种情况下，策略由概率分布而非函数定义。让我们更详细地看一下这两种策略规范之间的区别。
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_7.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_7.png)'
- en: 首先，让我们谈谈确定性政策。在这种情况下，要采取的行动 A t 由下式给出，应用于当前的策略函数 pi 的值，状态 S t。如果找到钢筋方向，自身处于系统同一状态
    S t 不止一次。每次它都会以完全相同的方式行事。现在，它将如何行动仅取决于当前状态 S t，但不是关于以前的国家历史。这个假设是为了确保一致性，以系统动力学特性为标志，其中看到特定未来状态的概率仅取决于当前状态。但不是在任何以前的状态。实际上无法证明，对于马尔可夫决策过程，始终存在最优确定性策略
    pi，所以我们的任务只是在所有可能的确定性策略中进行识别。所以，只要是这样，看起来确定性政策就是一切。我们永远需要解决马尔可夫决策过程。但是，事实证明，第二类政策，即随机策略也常常对强化学习有用。对于随机策略，pi
    变为概率分布，可能的行动 A t。这种分布可能取决于。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈确定性策略。在这种情况下，采取的行动 A t 由下式给出，应用于当前的策略函数 pi 的值，状态 S t。如果钢筋方向被找到，并且系统处于相同的状态
    S t 不止一次。每次它都会以完全相同的方式行事。现在，它的行动仅取决于当前状态 S t，而与之前的状态历史无关。这个假设是为了确保一致性，体现系统动力学特性，其中特定未来状态的概率只取决于当前状态，而与任何以前的状态无关。实际上无法证明，对于马尔可夫决策过程，始终存在最优确定性策略
    pi，因此我们的任务只是识别所有可能的确定性策略。因此，只要是这样，确定性策略似乎就能解决所有问题。我们永远需要解决马尔可夫决策过程。但是，事实证明，第二类策略，即随机策略，也常常对强化学习非常有用。对于随机策略，pi
    变成了一个概率分布，用于描述可能的行动 A t。这种分布可能取决于。
- en: 将 S t 的当前值作为这种分布的参数。因此，如果我们使用随机策略而不是确定性策略，代理再次访问相同的状态，它可能会采取与上次在相同状态下采取的行动不同的行动。现在，我们为什么要考虑这种随机策略？好吧。如果我们知道马尔可夫决策过程中的转移概率，那么我们可以考虑，只有确定性策略才能找到最佳确定性策略。为此，我们只需要求解我们在，我们之前的课程，我们将在下一个视频中再次回顾。但是如果我们不知道转移概率。我们必须从数据中估计它们并再次使用来解决，贝尔曼方程，否则我们必须，依赖于遵循强化学习方法的样本。在这种情况下，可能的动作随机化，遵循一些随机策略可能会提供一些探索空间，以便更好地估计模型。我马上会回到探索的概念，但首先让我注意到还有第二种情况，当使用随机策略时可能是可取的。出现这种情况时，而不是完全可观察的马尔可夫环境，我们处理部分观察到的环境。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将 S t 的当前值作为这种分布的参数。因此，如果我们使用随机策略而不是确定性策略，代理再次访问相同的状态时，它可能会采取与上次在相同状态下采取的行动不同的行动。那么，为什么我们要考虑这种随机策略呢？好吧，如果我们知道马尔可夫决策过程中的转移概率，我们可以推测，只有确定性策略才能找到最优的确定性策略。为此，我们只需要求解我们在之前课程中讨论过的问题，我们将在下一个视频中再次回顾。但是如果我们不知道转移概率，我们必须通过数据估计它们，并再次利用贝尔曼方程来解决问题，否则我们就必须依赖于强化学习方法的样本。在这种情况下，随机化动作，遵循一些随机策略，可能会提供一些探索的空间，以便更好地估计模型。我会马上回到探索的概念，但首先让我提到还有第二种情况，当使用随机策略时可能是可取的。发生这种情况时，我们处理的环境并不是完全可观察的马尔可夫环境，而是部分可观察的环境。
- en: 这种情况简称为部分可观察马尔可夫决策过程或 POMDP。在本课程中，我们不会处理 POMDP，但很高兴知道它们的存在，至少，特别是考虑到这样一个事实，金融中的许多问题可能是这种设置的好用例。但是。这种情况在数学上比
    MDP 设置复杂得多。因此，我们将首先研究字母大小写。好的。所以，现在，让我们回到什么是探索以及我们为什么需要它。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况被称为部分可观察马尔可夫决策过程，简称 POMDP。在本课程中，我们不会深入探讨 POMDP，但了解它们的存在是有帮助的，尤其是考虑到金融领域中的许多问题可能是这种设置的良好实例。然而，这种情况在数学上比
    MDP 设置复杂得多。因此，我们将首先专注于 MDP 的基本情况。好了，现在让我们回到探索的概念，讨论我们为什么需要它。
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_9.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_9.png)'
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_10.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_10.png)'
- en: 探索的概念出现在强化学习中，所谓的勘探开发困境，在采取行动时将不同的可能情景或策略概念化。这种困境特定于强化学习和，没有出现在有监督或无监督学习中。同样，原因是在监督或无监督学习中。没有多种可能的行动需要考虑。动作总是固定在那里，但在强化学习中，在每一步，我们都需要在众多行动中采取一种可能的行动。我们的最终目标是最大化总回报。但在学习的任何特定阶段，如果环境未知。我们可能不知道以后可能遇到的所有状态。或者，代理可能需要在它已经看到的状态下尝试不同的操作，可能是因为采取不同的行动可能会改变，环境的方式会在以后产生更高的回报。因此。我们可能需要尝试不同的操作并探测不同的状态。这是通过探索完成的。我们搜索好的动作和状态，例如，通过不时采取纯粹随机的行动。但如果我们经常这样做，我们最终可能会得到一个较低的最终累积奖励，因为在。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 探索的概念出现在强化学习中，通常被称为探索与利用的困境，指的是在采取行动时需要考虑不同的可能情景或策略。这种困境是强化学习特有的，在监督学习或无监督学习中并不常见。原因是，在监督学习或无监督学习中，动作通常是固定的，而强化学习中，在每一步，代理需要从众多可能的动作中选择一个。我们的最终目标是最大化总回报。但在学习的某个阶段，如果环境尚未完全了解，我们可能并不知道所有可能遇到的状态。或者，代理可能需要在已见过的状态下尝试不同的操作，因为采取不同的行动可能会影响环境的变化，从而产生更高的回报。因此，我们可能需要尝试不同的动作并探索不同的状态。这就是所谓的探索。我们通过不时采取随机行动来寻找好的动作和状态。但如果我们过于频繁地这样做，可能会导致较低的最终回报，因为。
- en: 试错法的探索，我们的代理访问了太多的低奖励状态。另一种方法是依赖开发。当我们的代理只是重复它的状态时，就会发生剥削，访问在之前访问这些州时提供良好奖励的相同操作。但这显然有选择好的行动的风险。但不是最好的行动，仅仅因为它们可能是，可能有更好回报的行动和状态。但如果只是，利用已知的动作和状态。这就是为什么它被称为探索利用困境。在每个时间步，代理应该决定，在这种状态下是否应该探索或利用。但它不能同时做这两件事。强化学习应该理想地结合探索和利用，例如，通过在不同的时间步长之间切换它们。具体如何做到这一点取决于具体情况，并且有，没有关于如何的通用答案，勘探开发困境应该。在一般的强化学习环境中解决。但重要的是要注意，这种困境只与在线强化学习有关，当代理与环境实时交互时。另一方面，在批处理模式强化学习中，我们已经从另一个代理的操作中收集了一些数据。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在试错法的探索中，代理可能访问到许多低回报状态。另一种方法是依赖于利用。当代理重复执行它以前成功的状态时，这就发生了“利用”，它会访问那些以前带来较好回报的相同操作。但这种方法显然有选择“好的”行动的风险，因为这些行动不一定是最优的，可能存在其他回报更高的行动和状态。然而，如果只依赖于已知的动作和状态，最终的回报可能就会受到限制。这就是为什么这个问题被称为“探索与利用的困境”。在每一个时间步，代理需要决定，在某个状态下是否应该选择探索还是选择利用。但它不可能同时做这两者。理想情况下，强化学习应该将探索和利用结合起来，例如通过在不同的时间步之间切换它们。具体如何做到这一点取决于实际情况，并且没有关于如何解决“探索与利用困境”的通用答案。但重要的是要注意，这种困境只与在线强化学习有关，即代理与环境进行实时交互时。另一方面，在批处理模式下的强化学习中，我们已经从其他代理的行为中收集了数据。
- en: 这意味着这个其他代理已经解决了，在某种程度上，这个问题的探索利用困境。这可能不是最好的方法，甚至可能是一个坏方法。例如，数据可能是使用纯随机民意调查收集的。但是，无论如何，这样固定的数据集就是这样。代理具有批量强化学习的设置。它无法访问实时环境，因此它甚至无法考虑任何探索。相反，它必须依赖固定的数据集并学习，仅来自该数据集的最优策略。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着其他代理已经在某种程度上解决了探索与利用的困境。虽然这种方法可能不是最优的，甚至可能是一个不太理想的方法。例如，数据可能是通过纯随机选择动作而收集的。但无论如何，这些固定的数据集就是我们所拥有的。在批量强化学习的设置下，代理无法访问实时环境，因此不能进行任何探索。相反，它必须依赖于固定的数据集，仅根据该数据集学习最优策略。
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_12.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_12.png)'
- en: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_13.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f7ba8374854ab08230eef2fa6ec63_13.png)'
