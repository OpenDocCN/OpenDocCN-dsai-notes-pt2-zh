# P24：RL方法 - 随机逼近 - 兰心飞侠 - BV14P4y1u7TB

在上一段视频中，我们讨论了批量强化学习的设置以及它使用的数据类型。现在让我们谈谈强化学习是如何工作的。当模型未知时，我们依然处理之前提到的Bauleman最优性方程。为了提醒你们，这个方程说明了，时间t的最优q函数等于最优奖励的期望值。

再加上下一步最优q函数的折现期望值。现在，让我们注意到这个期望符号进入了这个方程的右侧。这是一个涉及下一步品质的单步期望值，它是基于时间t时刻可用的信息ft条件下的。如果我们知道模型。

这种条件性的一步期望值在理论上可以精确计算。例如，对于一个离散状态模型，在给定当前状态的情况下计算Bauleman最优性方程的右侧，我们只需要对所有可能的下一状态进行求和，并将相应的转移概率作为加权求和的权重。另一方面，如果我们处理的是一个连续状态模型，那么期望值的计算应该涉及积分而非求和。

但同样，如果模型是已知的，这些积分在理论上至少可以简单计算。但如果我们不知道模型该怎么办呢？好吧，在这种情况下，我们可以依赖统计学和机器学习中最常见的技巧，即用经验均值来替代理论期望值。

所以，如果我们有可以表示为此求和中的某个项值的样本，那么最优q函数的估计可以通过这些值的经验均值来获得。请注意，实际上这就是蒙特卡罗模拟的工作原理。在我们之前基于动态规划的方法中，我们首先通过已知的动态模拟了基础股票的四条路径。

然后，我们将这些生成的样本当作数据来数值计算或估算期权价格和对冲。现在，代替生成的股票价格历史数据，在强化学习的设置中，我们有实际的价格历史数据，作为我们信息集FT的一部分，正如我们在前一段视频中讨论的那样。

所以，如果你吸收了股票价格和奖励，你可能能够依赖经验方法来估算Bellman最优性方程的右侧，从而在时间t估算最优q函数。但现在，假如我们没有一个固定的历史数据集来计算经验值，该怎么办呢？

例如，在在线强化学习中，智能体与环境实时互动，因此可用数据的量随时间增加。那么问题是，我们能否仅通过样本估计期望值来处理这种设置呢？

对这个问题的答案是肯定的，确实是可能的，这通过所谓的随机近似法来实现。最著名的随机近似法叫做**Robins Monroe算法**。**Robins Monroe算法**在不直接从样本中采样的情况下估计均值，而是一个一个地添加数据点。每次添加一个数据点，算法就会迭代到下一步，因此我们可以将索引k既作为迭代次数，也作为数据集中的数据点数。

如果数据集中的总点数为k，算法可以对介于1和大K之间的所有k值进行迭代。在每一步中，算法会更新**Ryan Castimation**的均值估计，这里用帽子x表示第k步的估计值，如下面的方程所示。现在让我们看看这个方程中每个部分的含义，x k代表第k个数据点，而x k的帽子是第k步的均值估计。接下来，参数alpha k应当介于0和1之间，被称为学习率，它描述了在吸收单个附加点x k后，均值估计应更新多少。

请注意，alpha k中的上标k表示学习率依赖于迭代次数。因此，如果学习率较小，那么每次新的观察数据加入时，运行中的均值估计变化很小，而如果学习率较大，变化会较大。

现在，关于这种简单的运行均值更新规则，最有趣和有用的事实是，在某些条件下，它以概率1收敛到真实均值。这些条件由**Robinson Munro**发现，并在此展示。这些条件的含义是，参数alpha k应该随着迭代次数k的增加而足够快速地减小，以确保alpha k的平方的无限和是有限的。所以，我们可以采取一些简单的参数化方法，通常也称为学习率的调度。

例如，我们可以使其随着k的增大而呈分数形式衰减，并由一些超参数进行参数化。**Munro 和 Robbins**的原始选择在此展示，它由一个常数alpha（介于0和1之间）除以k来定义。在其他情况下，根据实际的数据生成算法，alpha k的其他调度方案可能效果更好。通常来说，这意味着学习率alpha k不是通用的，而是特定于某个问题的，在应用于你的具体领域时可能需要一些实验。

最后，我要强调的是，**Robbins Munro算法**是一种在线算法，最适合与环境实时交互的强化学习代理。然而，通过适当的泛化，我们也可以将其扩展到离线设置。

如果我们进行离线工作，而不是使用单个数据点，我们可以选择一块数据来更新模型参数。在我们之前的课程中，当讨论**Kesti梯度下降法**时，我们也讨论了使用这种数据块（也叫做小批量数据）的优势。

如果你还记得我们之前提到过，使用小批量（mini batches）而不是单个数据点能使得更新更加稳定。在之前，我们讨论了梯度在优化中的稳定性。这里我们讨论的是写出平均更新，但使用小批量而非单个数据点的效果是相同的。它平滑了更新的波动，因此如果适度应用，它是一个好方法。

谢谢你。

![](img/b49b798200fc1ccc2e3e38b95f1749dc_1.png)

你。

![](img/b49b798200fc1ccc2e3e38b95f1749dc_3.png)
