- en: P8：MDP & RL- Action Value Function - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P8：MDP & RL- 动作价值函数 - 兰心飞侠 - BV14P4y1u7TB
- en: 在之前的视频中，我们介绍了价值函数和最优价值函数，并讨论了如何找到它，贝尔曼最优方程的数值解。最优策略可以从，计算出字母后的最优值函数。但事实证明，这种方法并不是唯一可能的方法，还有一种更方便的方法。只要目标是更好的强化学习。这种方法是基于他们所谓的行动价值函数。让我们看看动作价值函数与价值函数有何不同。让我们首先回忆一下给定时间的价值函数，仅取决于状态
    S 和策略 pi。它没有明确包含有关代理现在采取的操作的信息。动作价值函数 Q 是，当前状态 ST 和在该状态下采取的动作 AT。它计算与价值函数相同的预期累积奖励，但条件适用于
    ST 和 AT。类似于价值函数。动作价值函数 Q 或简单的 Q 函数取决于策略 pi，当前状态和当前动作的 Q 函数的两个参数 S 和 A。这意味着在 Q
    函数中，第一个动作是固定的，由 A 的值给出。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的视频中，我们介绍了价值函数和最优价值函数，并讨论了如何通过贝尔曼最优方程的数值解来找到它们。最优策略可以通过计算最优值函数来得到。但事实证明，这种方法并不是唯一的，还有一种更方便的方法，专为更好的强化学习目标设计。这个方法基于所谓的动作价值函数。让我们看看动作价值函数与价值函数有什么不同。首先回顾一下，在给定时间下，价值函数只依赖于状态
    S 和策略 pi，它并没有明确包含代理当前采取的动作信息。而动作价值函数 Q 依赖于当前状态 ST 和该状态下采取的动作 AT。它计算的是与价值函数相同的预期累积奖励，但条件是基于
    ST 和 AT。类似于价值函数，动作价值函数 Q 或简称 Q 函数依赖于策略 pi，当前状态和当前动作的 Q 函数有两个参数：S 和 A。这意味着在 Q 函数中，第一个动作是固定的，由
    A 的值给出。
- en: 而后面的所有动作都由策略 pi 决定。我们可以使用这个定义来获得贝尔曼方程，Q 函数的方式类似于，我们得到价值函数的贝尔曼方程。我们要做的是再次将总和分成第一项和其余部分。第一项是现在采取行动
    A 的奖励。因为动作 A 是固定的，因为它是 Q 函数的参数，奖励也是固定的。现在，第二项是相同的总和，但从下一个时间戳开始。正如我们刚才所说，本项不依赖于
    A。它仅取决于策略 pi。所以，它只是价值函数，同样的问题，但从下一次格式开始。这为我们提供了 Q 函数的贝尔曼方程，如下所示。它用奖励来表示时间 T 的
    Q 函数，同时T加上一个折扣预期。下一时刻价值函数的值。这样的方程使用起来不太方便，因为它现在涉及两个未知功能，而不仅仅是一个。但是，如果我们查看最优 Q
    函数，可以获得更方便的方程。现在，最优 Q 函数的定义方式与最优值函数相同。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，后续的所有动作都由策略 pi 决定。我们可以利用这个定义来推导贝尔曼方程，Q 函数的推导方式类似于我们得到价值函数的贝尔曼方程。我们要做的，就是再次将总和分为第一项和其余部分。第一项是当前采取动作
    A 的奖励。因为动作 A 是固定的，它是 Q 函数的参数，奖励也因此是固定的。接下来，第二项是相同的总和，只是从下一个时间戳开始。正如我们刚才所提到的，这一项不依赖于
    A，它仅依赖于策略 pi。因此，它只是价值函数，类似的问题，只不过是从下一个时间格式开始的。这就给我们带来了 Q 函数的贝尔曼方程，如下所示。它通过奖励来表示时间
    T 的 Q 函数，同时加入了折扣后的未来价值函数的预期。这样的方程使用起来不太方便，因为它现在涉及两个未知函数，而不仅仅是一个。然而，如果我们查看最优 Q
    函数，就能得到一个更方便的方程。最优 Q 函数的定义方式与最优值函数相同。
- en: 主要是最优动作价值函数Q star 是，所有 Q 函数和所有可能策略 pi 的最大值。所以，最优 Q 函数是这样说的，不时采取行动 A，然后稍后遵循最优策略
    P 星。现在。让我们将其与最优值函数的定义进行比较，本质上说，采取，现在和以后的最优策略决定的最优动作。但这意味着最优值函数 V star 只是，现在采取的所有可能的行动
    A 的最优行动价值函数 Q 星。换句话说。就其第二个参数而言，它是 Q 星的最大值。现在，我们可以使用这些定义来获得，一个涉及最优动作状态函数 Q star
    的贝尔曼方程。首先，我们在 Bellman 方程中取所有策略 pi 的最大值。Q 函数并使用 Q 星的定义。这将产生幻灯片底部的第一个方程。这个方程关联了
    Q 星和 V 星。现在，我们使用 V 星的定义作为所有动作 A 的最大 Q 星，这产生了最后一个仅涉及 Q 星函数的方程。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 主要是最优动作价值函数 Q 星，它是所有 Q 函数和所有可能策略 pi 的最大值。因此，最优 Q 函数可以表述为：不断采取动作 A，然后遵循最优策略 P
    星。现在，让我们将其与最优值函数的定义进行对比，实际上，最优值函数决定了当前和未来的最优策略。也就是说，最优值函数 V 星实际上就是当前所有可能动作 A 的最优动作价值函数
    Q 星。换句话说，V 星是 Q 星的最大值。现在，我们可以使用这些定义来推导出涉及最优动作状态函数 Q 星 的贝尔曼方程。首先，在贝尔曼方程中取所有策略 pi
    的最大值，Q 函数，并使用 Q 星的定义。这将产生幻灯片底部的第一个方程。这个方程将 Q 星 和 V 星 联系了起来。接着，我们使用 V 星 的定义，作为所有动作
    A 的最大 Q 星，这就得出了最后一个仅涉及 Q 星 函数的方程。
- en: 这个方程称为动作状态函数 Q 的贝尔曼最优方程。这是我们将要工作的方程式，在下周我们研究 Q 学习的时候。现在，让我对状态值函数的一些评论，或者可以根据经验估计动作状态价值函数，遵循机器学习范式。代理可以遵循策略
    pi 并维持其访问的每个状态的平均奖励。如果每个州的访问次数足够多，然后平均值将收敛到状态的价值函数。我们还可以计算在每个步骤中采取的单独操作的此类平均值。在这种情况下。平均值将收敛到动作状态值函数的值。在强化学习中，这类估计称为蒙特卡罗方法。只要状态和动作的数量很少，它们就足够简单。但是如果这个数字很大或者如果状态或动作是连续的。为每个状态动作组合保留单独的平均值可能不切实际。在这种情况下，我们应该依靠，动作状态值函数的一些函数逼近方法。我们将在本课程的稍后部分回到这些主题。但现在，我想向你展示，强化学习最经典的例子之一。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程称为动作状态函数 Q 的贝尔曼最优方程。这个方程是我们将在下周研究 Q 学习时使用的方程。现在，让我对状态值函数做一些评论，或者根据经验估计动作状态价值函数，遵循机器学习范式。代理可以遵循策略
    pi，并计算它访问的每个状态的平均奖励。如果每个状态的访问次数足够多，那么平均值将收敛到该状态的价值函数。我们还可以计算在每个步骤中采取的个别操作的平均值。在这种情况下，平均值将收敛到动作状态值函数的值。在强化学习中，这类估计被称为蒙特卡罗方法。只要状态和动作的数量较少，它们就足够简单。但如果状态或动作的数量非常大，或者如果状态或动作是连续的，那么为每个状态-动作对保留单独的平均值可能就不切实际了。在这种情况下，我们应该依赖一些动作状态值函数的函数逼近方法。我们将在本课程后面的部分讨论这些主题。但现在，我想向你展示强化学习中最经典的例子之一。
- en: 极平衡问题。这个问题的任务是将水平力施加到，可以在一定长度的轨道上向左或向右移动的推车，以防止与汽车铰接的杆从初始垂直位置掉落。当杆从垂直线落下超过给定角度时，就会发生故障。现在，这可以表述为。指定的时间段
    C。 对于每个时间步，没有发生一次故障，代理获得加一的奖励。如果发生故障，然后情节终止，代理开始新的情节。这个问题也可以被表述为一个持续的任务，而无需，一个固定的时间段
    T。它无限期地持续或直到极点下降。此设置需要使用一些折扣 gamma 来处理，值函数定义中的无限和。在这种情况下，给定步骤的奖励将是，每次失败减一，否则为零。对于
    MDP 公式，对于极点平衡问题。我们需要一组状态和一组动作。在这种情况下，国家，将是推车和杆子的位置和速度，并且动作将是施加到推车上的水平力。推车平衡问题是其中之一，在强化学习中看到的最古老的问题，并且习惯于，这些天来。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 极点平衡问题。该问题的任务是对一个可以在一定长度轨道上左右移动的推车施加水平力，目的是防止与汽车铰接的杆子从初始垂直位置掉落。当杆子从垂直线脱落超过给定角度时，故障发生。现在，这个问题可以表述为：在指定的时间段
    C 内，每次时间步没有发生故障时，代理将获得奖励 1。如果发生故障，则情节终止，代理开始新的情节。这个问题也可以表述为一个持续任务，而不是有固定的时间段 T。它会无限持续，直到杆子掉落。这种设置需要使用折扣因子
    gamma 来处理值函数定义中的无限和。在这种情况下，给定步骤的奖励是：每次故障时，奖励减去 1，否则为零。对于 MDP 公式，极点平衡问题，我们需要一组状态和一组动作。在这种情况下，状态将是推车和杆子的位置信息与速度，动作将是施加到推车上的水平力。推车平衡问题是强化学习中最经典的问题之一，并且这些天仍然经常被使用。
- en: 作为新强化学习算法的标准测试用例。在这里，我们可以看到一个显示动作的视频，用于此任务的训练强化学习代理。更多关于推车平衡问题的细节可以在 Seth 和
    Bartold 的书中找到。由于我们想专注于金融领域的强化学习，具体而言，我们不会对此进行详细介绍。在设置下一课时，我们将开发一个足够简单的简单财务模型，作为强化学习的财务影响的测试案例。所以，让我们用。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为新强化学习算法的标准测试用例，在这里我们可以看到一个视频，展示了用于此任务的训练强化学习代理的动作。更多关于推车平衡问题的细节可以在 Seth 和
    Bartold 的书中找到。由于我们想专注于金融领域的强化学习，具体而言，我们不会在这里详细讨论。接下来的课程中，我们将开发一个足够简单的财务模型，作为强化学习在财务领域应用的测试案例。所以，让我们开始吧。
- en: '![](img/a98b33954bc438e572dbcb87f00a08ab_1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a98b33954bc438e572dbcb87f00a08ab_1.png)'
- en: '![](img/a98b33954bc438e572dbcb87f00a08ab_2.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a98b33954bc438e572dbcb87f00a08ab_2.png)'
