- en: P21：Monte Carlo - Optimal Q Function With Monte-Carlo - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P21：蒙特卡洛 - 最优 Q 函数，使用蒙特卡洛 - 兰心飞侠 - BV14P4y1u7TB
- en: Now after we found the coefficients phi and t of the optimal action A t star，
    at time t。 we turn to the problem of finding coefficients omega and t for the
    optimal Q function。 To this end。 let's take another look at the Bellman-Optimality
    equation that we derived earlier。 To find coefficients omega and t， we consider
    this equation for the optimal action A star。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们找到最优动作 A t 星的系数 phi 和 t 后，我们转向寻找最优 Q 函数的系数 omega 和 t 的问题。为此，让我们再次回顾之前推导的贝尔曼最优性方程。为了找到系数
    omega 和 t，我们考虑这个方程对于最优动作 A 星的情况。
- en: rather than for a general value of action A t。 First， let's note that we can
    interpret this。 equation as a regression of the optimal function Q t star on the
    sum of the reward R t and discounted。 next step optimal Q function evaluated at
    the optimal action， as shown in the second equation here。 We can check that these
    two formulations are equivalent if we take expectations in both。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是一般的动作 A t 的值。首先，让我们注意到，我们可以将这个方程解释为最优函数 Q t 星对奖励 R t 和折扣后的下一步最优 Q 函数在最优动作下的和进行回归，如第二个方程所示。如果我们对两者进行期望计算，就可以验证这两种表述是等价的。
- en: sides of the second equation。 Now to use it as a regression to find parameters
    omega and t。 we need to know instantaneous rewards R t that enter this expression。
    This is easy to do as we。 know the theoretical expression for the reward。 It's
    given by the increment of the portfolio value。 which is equal to gamma times pi
    sub t plus 1 minus pi t and minus a risk premium part。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个方程的两边。现在，为了将其用作回归以找到参数 omega 和 t，我们需要知道进入此表达式的即时奖励 R t。这很容易做，因为我们知道奖励的理论表达式。它由投资组合价值的增量给出，等于
    gamma 乘以 pi sub t 加 1 减去 pi t，再减去一个风险溢价部分。
- en: equal to lambda times variance of pi t。 As we saw before， the reward R t can
    also be written more。 explicitly as a quadratic function of the time t action
    A t。 Therefore， as we go backwards in time。 in the process of backward recursion，
    once we compute the optimal action A t star， we plug this。 into the formula for
    the reward to compute the optimal reward。 That is the reward evaluated at。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 等于 lambda 乘以 pi t 的方差。如我们之前所见，奖励 R t 也可以更明确地写成时间 t 动作 A t 的二次函数。因此，当我们向后推算时，在反向递归过程中，一旦我们计算出最优动作
    A t 星，我们将其代入奖励公式中以计算最优奖励。即评估在。
- en: the optimal action。 So now we know the dependent variable for our regression，
    which is equal to。 the sum of the optimal reward and the discounted next step
    optimal Q function。 The predictor in this， expression， which stands in the right
    hand side。 is the time t optimal Q function。 Now by substituting here the extension
    of Q star in basis functions。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最优动作。因此，现在我们知道回归的因变量，它等于最优奖励和折扣后的下一步最优 Q 函数的和。这个表达式中的预测变量，即右侧的部分，是时间 t 的最优 Q
    函数。现在，通过将 Q 星的扩展代入基函数。
- en: coefficients omega and t become regression parameters in this regression。 To
    find them。 we proceed as we always do with regression and look for coefficients
    omega and t that minimize。 the squared loss function F t of omega shown here。
    Again， because this is a quadratic function of。 coefficients omega and t， this
    minimization can be performed semi analytically。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 系数 omega 和 t 成为此回归中的回归参数。为了找到它们，我们按照通常的回归方法，寻找使平方损失函数 F t 最小化的系数 omega 和 t，如此处所示。同样，因为这是一个关于系数
    omega 和 t 的二次函数，所以该最小化过程可以通过半解析方法来执行。
- en: We introduce another， time-dependent pair of a matrix CT with elements Cmn and
    vector dt with elements dn and。 matrix CT will have dimension m times m and vector
    dt will have lengths m。 The solution of the regression problem for coefficients
    omega and t can then be given in the vector form。 as the inverse of the matrix
    CT multiplied by the vector dt。 Again， because we deal here with the。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入另一个时间相关的矩阵对 CT，具有元素 Cmn 和向量 dt，具有元素 dn。矩阵 CT 的维度是 m 乘 m，而向量 dt 的长度是 m。回归问题中系数
    omega 和 t 的解可以用向量形式表示，即矩阵 CT 的逆与向量 dt 的乘积。同样，因为我们这里处理的是。
- en: matrix inversion in practice it might be a good idea to regularize matrix C
    by adding an identity。 matrix with a small regularization parameter epsilon before
    inverting the matrix。 The solution。 is then given by the inverse of this regularized
    matrix multiplied by dt。 And this completes all calculations we have to do at
    each time-step team。 We have exactly two。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际中，对于矩阵求逆，可能是个好主意通过在矩阵C中加入一个带有小正则化参数epsilon的单位矩阵来进行正则化处理，然后再进行矩阵求逆。解决方案就是将这个正则化后的矩阵的逆与dt相乘。这样就完成了在每个时间步所需的所有计算。我们有两个。
- en: such calculations。 One linear regression to find the optimal action and another
    linear regression to。 find the optimal Q function at the optimal action。 Therefore。
    for t time-steps we will have the total， of two times t linear regressions。 We
    have to compute in order to price and hedge an option。 Let's now summarize the
    whole procedure。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算方法包含了两次线性回归：一次用来寻找最优行动，另一次用来找到最优Q函数在最优行动下的值。因此，对于t个时间步，我们将进行两次t次线性回归计算。我们需要这些计算来定价和对冲期权。现在我们来总结一下整个过程。
- en: In the backward recursion we start with time capital T。 minus 1 and go back
    all the way to time t equals 0。 And at each time-step t we do the following。 First
    we compute the matrix AT and the vector BT。 Then we compute coefficients。 phi
    and t that determine the optimal action AT star。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向递归中，我们从时间T-1开始，一直回溯到时间t=0。在每个时间步t，我们执行以下操作：首先计算矩阵AT和向量BT，然后计算系数phi和t，确定最优行动AT星。
- en: Then we use this value to compute instantaneous， rewards that correspond to
    this optimal action。 After that we compute the matrix CT and the vector。 DT and
    this produces coefficients omega and t and has the optimal Q function evaluated
    at the optimal。 action。 The computed optimal Q function for this time-step is
    then used as the next step optimal Q。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用这个值来计算与最优行动对应的即时奖励。接下来，我们计算矩阵CT和向量DT，并由此得出系数omega和t，并计算最优Q函数，在最优行动下评估。然后，计算出的最优Q函数将作为下一步最优Q的依据。
- en: function for the next time moment t minus 1 of the backward recursion。 When
    we run this backward。 recursion all the way to time t equals 0 the last optimal
    action will give us the optimal。 hedge of the option now and the optimal price
    will be given by the negative of the optimal Q function。 with the optimal action
    as the second argument。 So we have a complete solution to the whole problem。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算下一时刻t-1的反向递归的函数。当我们将此反向递归执行到时间t=0时，最后的最优行动将为我们提供期权的最优对冲，而最优价格将由最优Q函数的负值给出，最优行动作为第二个参数。因此，我们已为整个问题提供了完整的解决方案。
- en: We can use this solution when the dynamics are known。 In fact this Monte Carlo
    dynamic programming。 solution does not require a full knowledge of model dynamics
    as long as sample parts of。 the underlying stock are available。 The only additional
    thing needed to implement。 the dynamic programming Monte Carlo based backward
    recursion that we presented here。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当动态条件已知时，我们可以使用这个解决方案。事实上，这个蒙特卡罗动态规划解法并不需要对模型动态的完全了解，只要能获取到基础股票的一部分样本数据即可。实现我们在此展示的动态规划蒙特卡罗反向递归所需的唯一附加条件就是。
- en: is the knowledge of risk aversion parameter lambda and the value of the stock
    drift mu。 In the next week we will see how the backward recursion can be implemented
    using reinforcement。 learning methods that do not assume that we know lambda。
    Let me now conclude with the new。 illustration of performance of dynamic programming
    approach to option pricing。 Let's see what is。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于风险厌恶参数lambda和股票漂移值mu的知识。在下周，我们将看到如何使用强化学习方法实现反向递归，这些方法不假设我们已经知道lambda。现在让我总结一下动态规划方法在期权定价中的表现的新示例。我们来看看具体内容。
- en: shown here。 This example applies the dynamic programming approach to price put
    option。 with the initial stock price of $100 and the strike of $100 which is known
    as。 an at the money or ATM option。 The risk aversion rate lambda is set here to
    0。001。 The stock volatility is 15% and option maturity T is 1 year。 The stock
    drift is taken here to be。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示。这个例子使用动态规划方法定价看跌期权，初始股票价格为$100，行使价为$100，这被称为平值期权或ATM期权。风险厌恶率lambda设置为0.001，股票波动率为15%，期权到期时间T为1年，股票漂移率取值为。
- en: 5% and the risk free rate is 3%。 Now the top left figure shows a random leachous
    in 10 parts of。 the Monte Carlo simulation for the stock price。 The top right
    figure shows corresponding values。 of the state variable Xt。 As expected the evolution
    of the state variable Xt doesn't show a drift。 You can also know that for a particular
    pass shown on the left for the stock price。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 风险溢价为5%，无风险利率为3%。现在，左上图显示了蒙特卡罗模拟中股票价格的10个部分的随机波动。右上图显示了相应的状态变量Xt的值。如预期的那样，状态变量Xt的演变没有显示出漂移。你也可以看到，对于股票价格的某一特定路径（如左侧所示）。
- en: the drift of ST even though it's there is not quite visible but this is just
    a visual effect。 Pass of ST do have a drift but pass of Xt do not。 The left figure
    in the middle row shows。 optimal actions obtained on these paths and figure on
    the right shows the corresponding。 values of the replicating portfolio。 Finally
    the left figure in the bottom row shows rewards obtained。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ST存在漂移现象，但这种现象并不十分明显，这只是一个视觉效果。ST的路径确实有漂移，而Xt的路径则没有。中间行的左侧图显示了在这些路径上获得的最优动作，右侧图则显示了相应的复制投资组合的值。最后，底行左侧的图显示了获得的奖励。
- en: from taking optimal actions and the right figure shows the optimal Q function。
    As you can see it converges at time 0 to the value of 4。9 and while the black
    shows price is 4。53。 you can want to check convergence to the black shows option
    price you can do it by making the。 risk aversion rate smaller and hedging frequency
    larger。 This actually will be a part of your。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从采取最优动作所获得的回报，右侧图显示了最优Q函数。正如你所看到的，它在时间0时收敛到4.9的值，而黑色曲线表示价格为4.53。你可以通过将风险厌恶率调小、对冲频率调大来检查是否收敛到黑色曲线所表示的期权价格。事实上，这将成为你本周作业的一部分，连同这个动态规划方案的实际实现。
- en: homework for this week along with the actual implementation of this dynamic
    programming scheme。 And on this note I would like to wrap up this week but obviously
    not before we make a quick stop。 here for some questions and see you the next
    week。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本周的内容就到这里，但显然在结束之前我们先做一个简短的停留，回答一些问题，下周再见。
- en: '![](img/f38793c02c794ecf9ccd0bc133d65304_1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f38793c02c794ecf9ccd0bc133d65304_1.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
- en: '![](img/f38793c02c794ecf9ccd0bc133d65304_3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f38793c02c794ecf9ccd0bc133d65304_3.png)'
