# P25：RL方法 - Q学习 - 兰心飞侠 - BV14P4y1u7TB

现在，在我们讨论了随机逼近法之后，到了讨论著名的Q学习的时候了，它是强化学习中最著名的算法之一。这个算法是Watkins在1989年在他的博士论文中提出的，从那时起，他的论文被引用了超过一千次。Q学习及其扩展在许多有趣的强化学习应用中得到了使用。特别是，Google的DeepMind在成为Google的一部分之前非常著名，当时他们发表了一篇论文，展示了如何大规模使用Q学习。

用来教授强化学习代理玩Atari电子游戏。现在，在Watkins建议的原始形式中，Q学习仅在离散状态和离散动作的情况下有效。在这种情况下，Q函数不是以连续值表示，而是以离散表格表示，每个状态和动作的组合都有一个Q函数值。在这种情况下。

我们可以说，Q函数是以表格的形式给出的。现在，Watkins Q学习的主要观点是，给定足够的数据，按照顺序处理所有数据点的算法，像Robins-Monro随机逼近法一样，随着数据量的增大，逐渐收敛到真实的Q函数。当数据量足够大时。更具体地说，收敛证明假设每个可能的组合都会在数据中遇到并且遇到无限次。当然，在实践中，这种情况永远不会成立，因为所有数据都是有限的。因此，在实践中，数值收敛性总是需要检查的问题。所以。

Q学习是做什么的？它简单地采取下一个观察到的转移，并更新在此转移中观察到的状态-动作对的值。让我通过一些常用的例子来说明它的工作原理，这些例子常用于测试和教授Q学习。一个这样的例子是应用Q学习来解决迷宫问题。这里你可以看到一个简单的迷宫示例，带有一个障碍物，这里用黑色方块表示。

并且在右上角有一个出口。代理人的问题是学习一个最优策略，根据代理人的位置来决定移动的方向。在每个单元格中，代理人可以向上、向下、向左和向右移动。所以，我们有四个自由度。

但是有些动作是不能执行的。例如，如果左边有墙，则禁止向左移动，依此类推。所以，在这个问题中，我们有十一种可能的单元格位置，并且在每个单元格中最多可以执行四个可能的动作。这给我们带来了四十四种可能的单元格和动作的组合，这个数量仍然相对可管理，可以在查找表中保持一个索引列，存储所有这些可能组合的Q函数值。

在这样一个简单的环境中，学习可以通过模拟来完成。每次一个特定的数据点被模拟生成并吸收后，就用它来更新与该数据和在本轮模拟中吸收的动作相对应的 Q 函数值。现在，在我们讨论了 Q 学习的作用之后，接下来讨论它是如何做到的。本质上。

Q 学习实际上是 Robins-Monroe 随机逼近的应用，但这次是用来估计在贝尔曼最优性方程右侧出现的未知期望。让我们将贝尔曼最优性方程与 Robins-Monroe 更新进行比较。在贝尔曼方程中，最优 Q 函数由右侧的期望值给出。

另一方面，Robins-Monroe 公式仅展示了如何更新均值的运行估计。因此，我们可以将当前观察值 x_t 中的当前观察奖励 R_t 和下一步最优 Q 函数的标记作为 Robins-Monroe 公式中的当前观测值。

当前的均值估计 x_t 将由我们对最优 Q 函数的当前估计代替。如果我们为学习率 alpha_k 指定一些调度方案，这些替代将生成著名的 Q 更新规则，Q 学习的规则如底部幻灯片所示。用文字描述，它表示在节点 x_t 和 at 处的 Q 函数值的新更新由其先前的估计值通过因子 1-alpha_k 缩放后加上另一个项，等于 alpha_k 乘以在贝尔曼最优性方程右侧出现的量的最新观察值。

现在，Q 迭代规则展示了 Q 学习的两个非常重要的特性，即它既是无模型的也是脱离策略的算法。这意味着它不会对产生当前观察的数据生成过程做出任何假设。它只是将其视为给定，并更新给定状态-动作节点 x_t 和 at 的动作值函数。

它也是一个脱离策略的算法，因为 Q 学习要奏效，唯一重要的是所有状态-动作对会被访问多次。但它们被访问的原因并不重要。在所有状态-动作对被无限次地遇到并且获得数据的极限情况下。

Q 迭代保证对于所有这样的状态-动作对，它会渐近收敛到真正的最优值函数。你可以在本周的每周阅读中了解更多关于 Q 学习算法的收敛性质。现在，考虑到我们在上一课中讨论的关于模型的动态规划解法，应该很明显，尽管经典的在线 Q 学习算法保证会渐近收敛。

这对于实际应用可能需要花费太长时间。其原因在于，最优对冲是通过所有蒙特卡罗路径的横截面信息获得的。然而，这种横截面信息会被任何在线更新Q值和最优对冲比率的方法所掩盖。不过，在这种情况下的解决办法也是明确的。

我们需要做的就是同时更新所有蒙特卡罗路径上的Q值和星级值。就像我们在上一节课计算最优对冲时所做的那样。因为我们现在处于批处理强化学习的设置中，数据已经准备好。实际上不需要像传统Q学习中那样逐一获取数据点。

在下一节视频中，我们将看到如何调整经典Q学习以适应批处理模式的学习。

![](img/366c69fa21211a8696877b658967c6ed_1.png)

[BLANK_AUDIO]。

![](img/366c69fa21211a8696877b658967c6ed_3.png)
