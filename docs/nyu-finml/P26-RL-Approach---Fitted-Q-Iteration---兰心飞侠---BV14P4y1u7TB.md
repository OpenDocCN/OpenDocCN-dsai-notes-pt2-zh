# P26：RL 方法 - 拟合 Q 迭代 - 兰心飞侠 - BV14P4y1u7TB

所以在上一个视频中，我们讨论了在计算最优策略时需要同时使用所有蒙特卡罗路径的横截面信息，或者股票的历史路径。为了提醒大家为什么需要这样做，原因很简单，因为捕捉策略是一个定义映射的函数，对于任何输入，都会产生相应的输出。但是每个观察值只提供了该函数在某一个特定点上的一些信息。

使用单一数据点更新函数可能是一个非常长且噪声较大的过程。这意味着，尽管经典的 Q 学习算法可以保证渐近收敛，但仍然需要被更实用、更能更快收敛的算法所替代。

由于我们正在处理批量强化学习的设置，也许如果我们通过查看所有投资组合动态的实现来决定最优策略，我们能够比经典的 Q 学习做得更好。幸运的是，确实存在针对这种批量强化学习设置的 Q 学习扩展。

我们将采用一种最流行的扩展方法——批量强化学习中的适应性 Q 迭代（FQI）。这一方法大约在 2005 到 2006 年间，由 Ernst 及其同事和 Murphy 在一系列论文中提出。一个显著的特点是，Ernst 和同事考虑了时间平稳问题，在这些问题中，Q 函数与时间无关。此外，许多强化学习文献中发布的研究也处理了无限时间视野的 Q 学习，在这种情况下，Q 函数是时间无关的。

对于这种平稳问题的 Q 学习与我们问题中所需的 Q 学习有所不同，因为我们的任务有一个有限的时间视野，因此是时间相关的。但 Murphy 的论文展示了适用于有限时间视野问题（如我们的问题）的批量 Q 学习版本。现在，拟合 Q 迭代方法适用于连续值数据，因此我们可以将模型表述带回到一般的连续状态空间问题中。

正如我们在蒙特卡罗设置中为动态规划解决方案所做的那样。如果我们想坚持离散空间的表述，拟合 Q 迭代可以用几乎相同的方式来实现。唯一的区别在于该方法需要使用的基函数的具体指定。因此，为了重申，FQI 方法通过使用所有蒙特卡罗路径或历史路径来同步复制投资组合的方式工作。

这与我们使用动态规划方法和蒙特卡罗方法解决已知动态问题的方式非常相似。在这种方法中，我们通过对不同时刻 t 和 t+1 的路径蒙特卡罗场景取经验均值，来同时对所有情景进行平均。

在时间t的条件下，给定信息集t，已经通过在t时刻对所有蒙特卡洛路径进行条件化来实现。现在在批量模式强化学习的设定下，我们需要改变的唯一内容是在这种蒙特卡洛设置中的输入输出数据结构。

在动态规划的设定下，当模型已知时，输入是状态变量极限的模拟或实际前向传递。输出是最优动作和动作策略，以及最优Q函数。也就是说，负的期权价格。最优动作a星通过最大化最优Q函数来确定，瞬时奖励则是在后向递归过程中为最优动作和最优Q函数计算的。这是动态规划的情况，现在在批量强化学习的设定下，过渡概率、策略或奖励函数是未知的。

所需的输出与之前相同。也就是说，我们必须找到期权价格和哈希。然而，输入现在比之前的情况更丰富，因为每个观察到的状态向量x队列现在有两个额外的观察值。我们知道动作x at和奖励RT。我们不再有PT星函数和RT，而是仅有从这些函数在不同状态x队列中获得的样本。这是批量强化学习的设定，其中在T时刻和T加一时刻的状态变量以及所有蒙特卡洛路径中收到的奖励都被同时用于找到最优动作和最优Q函数。

对于所有状态x T和x队列的所有动作AT，这相当于学习一个策略。无论是使用模拟的蒙特卡洛数据还是实际数据，批量强化学习的工作方式是相同的。现在，让我们看看拟合Q迭代在我们的问题中的作用。FQI方法通过在基函数集合中展开来寻找最优Q函数。

我们可以使用与动态规划解法中相同的基函数集合phi N(x)。现在回想一下，奖励是动作的二次函数。因此，Q函数也将是动作的二次函数，我们在这里将Q函数表示为向量A、矩阵W和向量phi的乘积，如本方程所示。

这里AT将是一个大小为三的向量，由单位AT和一半的AT平方组成。FET是一个包含基函数的向量，大小为M，其中M是基函数的数量。最后，WT将是一个K系数的矩阵，形状为三乘M。Q函数的这种形式捕捉了它对动作的二次依赖。

在用基函数phi N(x)和K系数W参数化对坐标Xt的依赖时，为了后续讨论，我们可以将Q函数以稍微不同的形式等效地表示。如果我们将系数矩阵W和向量phi合并成一个新的向量U_W，该向量按W索引，以强调这个向量依赖于K系数W。

我们还可以为向量 U_W 设置终端条件。例如，如果我们将时间 T 时的终端动作 AT 设置为零，这将转换为向量 U_W 分量的终端条件，如下所示。在这种情况下，只有第一个分量在到期时非零，其他两个分量在时间 T 时为零。

现在我们可以用两种方式来表示 Q 函数。首先假设矩阵 WT 的参数是已知的。那么由于 Q 函数是关于动作 AT 的二次函数，我们可以根据向量 U_W 的分量来求解最优动作和最优 Q 函数。如果做这个简单的数学运算，你会得到以下两个方程。

他们根据向量 U_W 的分量定义了最优动作和 Q 函数。但事实上，我们的目标恰恰相反。我们需要根据可观察到的动作和状态来求解矩阵 WT 的分量，或者等效地求解向量 U_W 的分量。在这种情况下，我们更愿意从右到左来看待这些方程。

但我们有两个方程和三个未知数。这仍然是可以的，因为这些未知数依赖于相同的矩阵 WT，并在这个意义上是相关的。但这也意味着，为了解决我们的这个问题，我们必须直接从数据中找到矩阵 WT。在下一个视频中，我们将看到如何做到这一点。[静音]

![](img/9b0a84a0631923d3be6d37c28ab6e5bd_1.png)

![](img/9b0a84a0631923d3be6d37c28ab6e5bd_2.png)
