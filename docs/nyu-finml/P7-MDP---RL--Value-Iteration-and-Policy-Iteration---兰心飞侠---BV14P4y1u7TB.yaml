- en: P7：MDP & RL- Value Iteration and Policy Iteration - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P7：MDP & RL- 值迭代与策略迭代 - 兰心飞侠 - BV14P4y1u7TB
- en: 现在，让我们回顾一下我们在上一课中所说的内容。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下我们在上一课中所说的内容。
- en: '![](img/0e04127b649abf121314e4b9f113a365_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e04127b649abf121314e4b9f113a365_1.png)'
- en: 关于求解贝尔曼最优方程的两种经典方法。我们可以将这些方法应用于时间均匀的市场决策过程。正如我们在上一个视频中看到的，对于这样的过程，然后价值函数变得独立于时间。所以，在这种情况下。贝尔曼最优方程变成了单个函数的非线性方程，可以用简单的数值方法解决。一种这样的方法称为值迭代。只要状态-动作空间是离散且小的，值迭代为问题提供了简单快速的解决方案。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 关于求解贝尔曼最优方程的两种经典方法。我们可以将这些方法应用于时间均匀的市场决策过程。正如我们在上一个视频中看到的，对于这样的过程，价值函数变得与时间无关。因此，在这种情况下，贝尔曼最优方程变成了单一函数的非线性方程，可以通过简单的数值方法求解。这样的一种方法被称为值迭代。只要状态-动作空间是离散且较小，值迭代就能为问题提供简单而快速的解决方案。
- en: '![](img/0e04127b649abf121314e4b9f113a365_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e04127b649abf121314e4b9f113a365_3.png)'
- en: 它的工作原理如下，我们从初始化开始，所有状态的一些初始值的价值函数。通常，所有状态的初始化为零都可以正常工作。然后我们不断迭代计算，使用贝尔曼方程本身的价值函数。对于每次迭代。我们使用上一次迭代的结果来，计算这个等式的右边。现在，有几种方法可以在这种值迭代中更新值函数。一种方法是完成计算价值函数，我们状态空间中的所有状态，然后立即更新所有状态中的开发功能。这称为同步更新和。另一种方法是即时更新价值函数。假设在当前迭代中计算，这将被称为异步更新。现在，对于这两种迭代方式中的任何一种都可以，证明算法收敛到最优值函数，我们将再次重新启动。并且发现重启后。可以使用与以前相同的公式找到最优策略。所以基本算法非常简单，正如我们所说，只要你的状态动作空间是，离散且具有少量状态。现在还有另一种经典方法，我们也在，上一个过程称为策略迭代。在这个方法中。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理如下，我们从初始化开始，设定所有状态的初始价值函数。通常，所有状态初始化为零即可正常工作。然后我们不断迭代计算，使用贝尔曼方程本身的价值函数进行更新。对于每次迭代，我们使用上一次迭代的结果来计算该等式的右边。现在，有几种方法可以在这种值迭代中更新值函数。一种方法是遍历状态空间中所有状态并计算价值函数，然后立即更新所有状态的价值函数，这被称为同步更新。另一种方法是即时更新价值函数，这意味着每次计算后立即更新，这称为异步更新。现在，无论采用这两种迭代方式中的任何一种，都可以证明算法最终会收敛到最优值函数。我们将再次重启算法，并且发现重启后，我们可以使用相同的公式找到最优策略。所以基本的算法非常简单，正如我们所说的，只要你的状态-动作空间是离散且状态较少。现在，还有另一种经典方法，我们也在上一过程中提到过，它叫做策略迭代。在这种方法中。
- en: 我们从一些初始策略饼 0 开始，通常是随机选择的。在那之后，我们重复，以下循环与，在这个循环的每一步进行两次计算，直到收敛。第一个计算称为策略验证。在这个计算中，我们为这个迭代计算给定策略的价值函数。我们可以使用
    V 的贝尔曼方程来做到这一点，而不是这颗恒星的贝尔曼最优方程。因为 V 的贝尔曼方程只是，这一步的线性方程是直截了当的。循环同一步骤的第二次计算称为策略更新。这通过应用再次完成。arg
    max 运算符位于贝尔曼方程的右侧。现在，请注意这些计算必须完成，在策略迭代法内重复，他们有完全不同的计算负担。政策更新直截了当，可以使用标准优化软件来完成。但政策评估步骤涉及求解贝尔曼方程。因此，当状态空间的维数很大时，在政策评估过程中重复这样做可能代价高昂。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一些初始策略 π₀ 开始，通常是随机选择的。在那之后，我们重复执行以下循环，每一步进行两次计算，直到收敛。第一个计算叫做策略验证。在这个计算中，我们为当前策略计算价值函数。我们可以使用
    V 的贝尔曼方程来完成这个计算，而不是贝尔曼最优方程。因为 V 的贝尔曼方程只是一个线性方程，解决这一过程相对直接。循环的第二次计算叫做策略更新。这个步骤通过应用贝尔曼方程右侧的
    `arg max` 运算符来完成。现在请注意，这些计算必须在策略迭代中重复执行，它们的计算负担是完全不同的。策略更新是直接的，可以通过标准优化算法完成。然而，策略评估步骤涉及求解贝尔曼方程。因此，当状态空间的维度很大时，重复进行策略评估可能非常昂贵。
- en: '![](img/0e04127b649abf121314e4b9f113a365_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e04127b649abf121314e4b9f113a365_5.png)'
- en: 另一方面，最优控制的许多实际问题，大的离散状态-动作空间甚至连续的状态-动作空间。在这些设置中，动态规划和算法的方法，像值迭代或策略迭代不再起作用。强化学习在这里得到了拯救。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，许多实际的最优控制问题，尤其是在大型离散状态-动作空间，甚至是连续的状态-动作空间下，动态规划和算法方法，如值迭代或策略迭代，已经不再适用。在这些情境中，强化学习就成为了解决问题的关键。
- en: '![](img/0e04127b649abf121314e4b9f113a365_7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e04127b649abf121314e4b9f113a365_7.png)'
- en: 我们将在下周看到它是如何工作的。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下周看到它是如何工作的。
- en: '![](img/0e04127b649abf121314e4b9f113a365_9.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e04127b649abf121314e4b9f113a365_9.png)'
