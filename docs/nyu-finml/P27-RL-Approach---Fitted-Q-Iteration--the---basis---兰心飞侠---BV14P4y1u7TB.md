# P27：RL方法 - 拟合Q迭代 - Ψ基 - 兰心飞侠 - BV14P4y1u7TB

所以在上一期视频中，我们为Q函数引入了两种参数化方式。一种是矩阵W_t的形式，另一种是向量U_W的形式，它是矩阵W_t与基函数向量phi的乘积。我们看到，如果我们某种方式观察到了最优动作a_t*，以及最优的Q函数值Q_t*。

这将作为两个方程，求解向量U_W的三个未知分量。现在，这表明一个正确的解——即通过动态规划方法找到的解——如果已知动态学，应该位于由时间相关矩阵W_t参数化的解空间中，就像我们在Q函数的定义中所做的那样。

所以我们必须找到一种方法来学习K-fissions矩阵W_t。让我们看看如何做到这一点。我们需要做的是重新排列Q函数定义中的项，将其转化为一个参数向量的乘积，以及一个依赖于状态和动作的向量。那么我们该怎么做呢？

我们通过几个简单的步骤来实现。首先，我们显式地写出Q函数的值，作为这里给出的矩阵表达式的迹。我们将其写为矩阵W的元素与由向量A和phi的外积形成的矩阵的元素逐项乘积。这给了我们这个表达式的第二行。这里有两种类型的乘法。首先。

这个符号表示两个矩阵的逐项乘积。这个乘积也被称为两个矩阵的Hadamard乘积。第二，这个表达式有一个包围的叉积符号，表示的是由两个向量A_t和phi_t的外积给出的矩阵。

根据定义，矩阵的元素i_j是由A_t_i和phi_t_j的乘积给出的。到目前为止，我们已经成功地将整个表达式转化为两个矩阵乘积的迹。现在我们可以做一件事，将这个表达式表示为两个向量的标量积。为此，我们需要将进入该表达式的两个矩阵转换为向量。

我们可以通过将矩阵W的列连接起来，再与由A_t和phi_t的外积给出的矩阵相乘来实现这一点。这将给我们方程中的第三行。现在我们可以将整个表达式紧凑地写为向量W和向量psi的点积。

在这里，向量psi是通过将A_t和phi_t外积的列连接得到的。可以将其视为一个新的基函数集合，既依赖于状态也依赖于动作。动作A_t的依赖是二次的，而状态x_t的依赖则可以是任意的，取决于原始基函数phi_n的函数形式。如果我们例如使用立方B样条作为基函数。

就像我们在动态规划解中对模型所做的那样，如果X的依赖是局部三次的。那么让我们比较一下在动态规划设置下我们得到的结果。在我们当前的强化学习设置中，我们将q函数的表达式简化为向量W和状态-动作基函数ψ_t的新向量的点积。

两者都有由3m给出的多个组件，因此这个问题中的未知系数的数量是3m。你也可以仅通过观察矩阵W_t来轻松猜测，矩阵W_t的维度是3乘m。所以我们在这里有3m个未知数。那么，观察变量的数量是多少呢？

在Betch-Mold强化学习设置中的每个时间步，我们有3m个可观察变量，因为我们观察了状态x_t、动作a_t和奖励r_t，对于所有n个历史路径或蒙特卡洛路径。所以我们有3m个可观察变量，对应3m个未知数。如果n大于m。

那么我们的这个问题就是一个良定问题。在这个设置中，我们每个1v参数有n除以m个观察。现在我们可以将其与动态规划方法中的情况进行比较。在那种情况下，我们必须找到两个量，最优策略和最优q函数。在我们的动态规划解中，我们将这两个量展开为m个基函数。

所以我们总共有2m个未知系数。这比强化学习设置中的3m个未知系数少。但另一方面，动态规划方法每个时间片的观察数量也比强化学习方法少。

这是因为在动态规划设置中，我们只观察到了状态。所以对于n个蒙特卡洛路径，我们每个时间只有n个数据点。因此每个参数的观察数量是n除以2m，实际上小于在强化学习设置中n除以m的比例。

所以看来，至少当强化学习的数据是从动态规划解生成时，强化学习和动态规划方法应该表现得差不多好。这种设置称为基于策略的学习（on-policy learning）。

这意味着用于强化学习训练的动作实际上是最优的动作。事实证明，确实如此，在这种设置下，两个算法产生的结果几乎完全相同。这实际上为我们提供了一种简单的方法来基准测试我们的强化学习算法。因为当模型完全由动态规划指定时，我们可以求解它。

我们可以从这个解中计算出最优策略。然后我们可以使用通过这个策略获得的观察到的动作和词汇作为拟合策划方法的数据，并使用该方法计算最优策略和q函数。因为这个解已经通过动态规划得出，我们可以用它来基准测试我们的强化学习解，并检查收敛的速度。

最终结果等等。这实际上是非常有保障的，因为我们现在确切地知道期望看到的是什么。因此，任何拟合策展的问题如果在那里被发现，会立刻显现出来。如果我们现在想尝试一些其他的强化学习算法，而不是拟合策展，再次地，我们通过动态规划方法获得了一个好的基准。

因此，我们可以使用我们简单的MDP模型进行期权定价，测试许多不同的强化学习算法。我们在这里稍作停顿，接下来的视频中将继续使用拟合策展方法来解这个模型的实际解。

![](img/06e2e0050b82f5416f388b1b7f6c3663_1.png)

![](img/06e2e0050b82f5416f388b1b7f6c3663_2.png)
