# P38：股票交易的强化学习 - 强化学习与反向强化学习解决方案 - 兰心飞侠 - BV14P4y1u7TB

现在我们准备将所有内容整合成一个反向迭代方案，用于通过数据计算最优策略，使用强化学习。我们首先做的是更新最优策略。我们首先通过增量来实现它，增量delta A(t)。A(t)的后验概率如方程50所示。

这里，delta A(t)的pi0是一个先验分布pi0，它以delta A(t)而非A(t)的形式重新表达。现在，由于delta A(t)的pi0是高斯分布，并且指数中的表达式是二次的，因此结果是另一个高斯分布，如方程50的第二行所示。这个分布的新均值和协方差在方程51中给出。

我们可以将这个方程看作是对这些量的先验值的贝叶斯更新。现在我们已经接近完成，但这里还需要一个元素，即找到我们在方案中之前使用过的轨迹的方式。这种轨迹是通过自洽地在前向传递和反向传递之间进行多次迭代来找到的。

这个计算中使用的方法称为迭代二次高斯调节器，它是由Todorov和Li于2005年提出的。方法是这样工作的：我们从一个初始轨迹开始，这个轨迹是使用先验策略分布的均值构建的。在每一步，我们将先验的当前均值作为一个确定性动作，然后计算。

使用原始状态方程计算新的状态，但不包含噪声项。然后我们反向计算，像上面那样递归计算g函数。这是算法的反向传递。它为策略的均值提供了一个更新。之后，在前向传递中，我们围绕这个均值生成一个新的轨迹，整个过程重新开始。

这个过程会持续进行，直到收敛，并产生一个自洽的动力学解。最终结果可以表示为一个最优的数据隐含策略，它具有与我们的先验策略相同的函数形式，但具有更新的参数。你可以找到在这里重新计算的系数A0和A1的显式表达式。

在原始文献中的每一步轨迹优化过程中，更新后的协方差矩阵sigma m如图所示，并且变化清晰可见。Sigma m是先验的正则化协方差矩阵，其中正则化项等于beta乘以矩阵G aa t。那么，现在让我们总结一下并把整个方案整合起来。

这是一个适用于我们模型的算法，适用于传统强化学习设定，当词语是可观察时。该算法从初始轨迹开始，作为一系列的对（x bar 和 u bar），涵盖从零到T的所有t值。这是第一次前向传递。然后，我们开始在反向和前向传递之间迭代，直到收敛。

首先，我们执行一个从T-1时刻回溯到0时刻的后向传递。在这一过程中，对于每个t值，在第一步中，我们计算从下一时期得到的F函数的期望值，正如我们上面所展示的那样。接下来，在第二步中，我们使用计算出的值和观察到的三个词语，计算G函数。在这一步中。第三步，我们计算相同时间步长的F函数。

之后，在第四步中，我们通过更新其均值和方差来重新计算此时间步的策略。我们重复步骤一到四，直到所有时间步都完成后向传递。然后，我们通过使用新的均值和方差构建新的轨迹进行前向传递，整个过程重复进行。这总结了在我们的模型下进行常规强化学习的过程。

词语是可观察的。但是事实证明，当词语不可观察时，这个模型可以解决更有趣的问题。我要提醒大家，这种设置被称为逆向强化学习（IRL）。

在这种设置下，我们观察状态和动作，但无法观察奖励。通常，这种设置既非常有趣，又比直接强化学习更加困难。我们稍后会详细讨论逆向强化学习，但首先让我们看看我们的模型在这种设置下如何工作。事实证明，这个案例对我们的模型来说是容易的，因为逆向强化学习。

该模型中的数值仅为常规最大似然估计。之所以这样，是因为我们在该模型中获得的策略是高斯策略，其均值和方差依赖于奖励函数的参数。因此，我们可以利用观察到的状态-动作数据，应用常规的最大似然估计。

方法对于这种高斯分布。这给出了数据的负对数似然，如方程54所示。由于该高斯似然的参数依赖于奖励函数的原始参数，我们可以使用特征梯度下降法或梯度下降法，通过最小化负对数似然来计算这些参数。

这一过程应该在上述前向-后向方案的每次迭代中进行。所以对于每次迭代K，我们需要在此处替换A0和A1的值，以及在这一迭代中计算出的sigma M。这一过程将强化学习设置中的观察奖励替换为逆向强化学习设置中的估计奖励。

现在我们为逆向强化学习引入了这一附加元素，并提出了完整的方案。与强化学习模型相比，唯一的新元素是第二步，在这一步中，我们使用最大似然法来估计奖励参数和其他进入模型的参数。

特别是包括逆温度参数beta。接下来的步骤使用这个估计的奖励值，而不是观察到的奖励值，来计算G函数，其余的步骤与强化学习设置中的相同。这表明，在这个模型中，逆向强化学习在计算上并不比

直接强化学习。现在我们如何利用这个模型来关注强化学习的相关内容呢？

回想一下，当我们介绍这个模型时，我们说过它可以作为某个特定交易员的模型，或者作为市场投资组合模型，比如S&P 500投资组合。在第一种情况下，我们需要来自该交易员的专有交易数据，以学习该交易员的奖励函数和最优政策。

如果我们有这样的数据，我们可以例如计算出该交易员的隐含风险厌恶参数lambda。但往往我们没有这样的专有数据，因为只有经纪人或交易员才拥有它，这就是为什么它被称为专有数据。那么我们还能用这个模型做什么呢？我们可以使用公开数据，也就是市场数据。

我们可以像之前提到的Black-Litterman模型一样，将其应用于市场S&P 500投资组合。这个模型的整体结构可以视为Black-Litterman模型的动态概率性和数据驱动的扩展，以及Burt Smith和他的同事们对其逆优化的解释。在这种情况下，我们之前计算出的最优政策，我在这里重复这个公式，

将会是市场隐含的最优政策。通过一组常用的预测因子ZT，我们可以直接使用我们刚才描述的方法估计出这个市场隐含的最优政策。另一方面，如果我们有一些私人信号ZT，也叫做私人观点，

如果我们有Black-Litterman模型中不为市场其他参与者所知的信号，我们可以利用这些信号，通过我们改进的最优政策来尝试超越市场。这种改进的政策是通过将这些信号添加到常见预测因子的列表中得到的。你可以在原始的文献中找到更多关于该模型的细节。

咨询，因为你将在课程的最终项目中使用这个模型来完成。本周的内容以及我们关于强化学习的整个课程到此结束。我们将在下一个视频中总结我们所学的内容。谢谢。

![](img/e0d72f2fd4009a30dbdfa6fae2f18059_1.png)

我们将在下一个视频中见面。再见。[BLANK_AUDIO]。

![](img/e0d72f2fd4009a30dbdfa6fae2f18059_3.png)
