- en: P36：RL for Stock Trading -Entropy Regularized RL - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P36：股票交易的强化学习 - 熵正则化强化学习 - 兰心飞侠 - BV14P4y1u7TB
- en: Okay， now we're going to have a little more math than we had before so far。
    So please bear with me as we're going to dive into a bunch of new formulas for
    the next few minutes。 And also in the next video。 And if you find yourself lost
    some way in the middle。 you can always stop and rewind or look into original sources
    for more details。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们要做一些比之前更多的数学计算了。所以请耐心一点，因为接下来的几分钟我们将深入讨论一堆新公式。还有下一段视频。如果你发现自己在中间迷失了方向，可以随时停下来倒回去看，或者查阅原始资料以获取更多细节。
- en: And then maybe watch this video again。 But hopefully you will not have to do
    it or at least you will not have to do it too many times。 So let's start。 Let
    me start with reminding you the standard Bellman equation for the value function。
    We define an optimal value function D* as shown in equation 27 as a maximum overall
    policy spy of an expected sum of discounted future rewards。 Once we discussed
    in our previous course， when we work with the Bellman equation for the value function。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后也许可以再看一下这个视频。但希望你不需要这么做，或者至少不需要做太多次。所以我们开始吧。让我首先提醒你关于价值函数的标准贝尔曼方程。我们定义一个最优价值函数D*，如方程27所示，它是对所有策略π的期望折现未来奖励的最大值。正如我们在之前的课程中讨论过的，当我们处理价值函数的贝尔曼方程时。
- en: we normally work with two equations。 One of them is a Bellman-optimality equation
    for V* shown in equation 28。 The second term in the right hand side of this equation
    is an expectation of a future value function conditional on a given current state
    and action。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常处理两个方程。第一个是关于V*的贝尔曼最优性方程，如方程28所示。这个方程右侧的第二项是对给定当前状态和动作下未来价值函数的期望。
- en: The optimal value function V* at time t is a maximum of the right hand side
    of this equation with respect to all actions at。 Now a policy pie is not explicitly
    present in this equation。 If you already know an optimal value function V* then
    computing an optimum policy takes a different optimization problem in equation
    29。 We call this step a policy improvement step in our previous course。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间t，最优价值函数V*是该方程右侧关于所有动作at的最大值。现在，策略π在这个方程中并没有显式出现。如果你已经知道最优价值函数V*，那么计算最优策略就需要解决方程29中的另一个优化问题。我们在之前的课程中称这个步骤为策略改进步骤。
- en: Now we reformulate the Bellman-optimality equation such that the policy pie
    explicitly appears there。 A tick that does it shown here is sometimes called a
    fensual representation。 In this formulation everything is the same as before except
    that now we maximize with respect to a policy pie。 rather than with respect to
    actions at。 Policy pie can be any policy from a set of valid distributions that
    we call P here。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们重新构造贝尔曼最优性方程，使得策略π显式出现在其中。这里展示的技巧有时被称为“费恩索尔表示”。在这个公式中，其他部分与之前相同，只是现在我们是关于策略π进行最大化，而不是关于动作at。策略π可以是我们在这里称为P的有效分布集合中的任何一个策略。
- en: Now why is this formulation equivalent to the previous formulation？
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么这个公式等同于之前的公式呢？
- en: This is because of a simple identity shown at the bottom of this slide。 If we
    have a set of numbers x1 to xn then their maximum will be the same as a maximum
    over a set of weights。 that we again call pie here over a dot product of these
    weights and the vector of values xi。 And this is simply by construction because
    the optimal set of weights will give a weight of 1 to a maximum value of xi。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为这个幻灯片底部展示了一个简单的恒等式。如果我们有一组数字x1到xn，那么它们的最大值将与一组权重的最大值相同，我们在这里称这些权重为π，它们是这些权重与值向量xi的点积的最大值。这仅仅是通过构造得来的，因为最优的权重集会给最大值xi赋予1的权重。
- en: and will give zero weights to the rest of x values。 So this trick is very simple
    but very useful because now the policy pie enters the problem explicitly rather
    than implicitly。 Now we make a very important next step。 We introduce a notion
    of information costs of updating a policy from a reference policy pie zero to
    a given policy pie。 This cost is given by equation 30 as a log of the ratio of
    policy pie to policy pie zero。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 并且将对其他x值的权重设为零。所以这个技巧非常简单，但非常有用，因为现在策略π显式地进入了问题，而不是隐式地存在。现在我们迈出了一个非常重要的下一步。我们引入了从参考策略π0更新到给定策略π的信息成本的概念。这个成本由方程30给出，表示为策略π与策略π0的比值的对数。
- en: I refer you to a paper of tissue bee and co-workers for more in-depth explanation
    of why this quantity is called an information cost。 But one thing we can immediately
    notice is that if we take an expectation of this expression with distribution
    pie。 we will get what is called the public libler or KL divergence of two distributions
    pie and pie zero。 which is shown in equation 32。 We mentioned the KL divergence
    a few times in this specialization and talked about how it serves as a measure
    of similarity between two distributions。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The KL divergence is always non-negative and is strictly equal zero only when
    pie equals pie zero。 Now we can take a discounted expected sum of all such information
    costs for all time steps。 And this produces the total discounted information cost
    for a trajectory that is shown in equation 33。 Now we can define the so-called
    free energy F_t。 As the value function minus the information cost of a trajectory
    multiplied by a factor one over beta。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: If we put explicit expressions for both terms here， we get the second form of
    this equation。 Please note that we can see the whole expression in brackets here
    as a reward corrected or regularized by an information cost term。 Parameter beta
    controls the strength of this regularization。 If we set beta to infinity。 then
    the regularization term disappears。 In the opposite limit when beta is very large，
    sorry。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: is very low and modified one step reward will be completely determined by the
    second term。 Parameter beta is called an inverse temperature parameter because
    it enters formulas below in a similar way to how the inverse temperature in physics
    enters formulas of statistical mechanics。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: So at the end， what is the free energy function？ As you can see from this equation。
    the free energy function is just an entropy regularized value function。 As was
    explained in the paper by Tichbi， such entropy regularization is very helpful
    when an environment is noisy。 In this case， the regularization term provides a
    helping hand if a priority is reasonable to find a good and noise tolerant optimal
    policy and optimal value function。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: And as in finance and environment is typically either noisy or very noisy。 such
    entropy regularization is very useful for financial problems。 We can now derive
    a Baumann equation directly for the free energy function。 It's shown here in the
    equation 35 and it can be derived from the previous formula in this standard way。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: This equation can be viewed as a solved probabilistic relaxation of the optimal
    of the original Baumann equation where beta serves as a regularization parameter。
    If beta is set to infinity here， it reproduces the original Baumann equation as
    you can easily check yourself。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Now， similarly to how we used KL divergence to regularize the value function。
    we can also regularize an action value function， Q function。 We will call this
    function a G function following Tichbi and co-workers。 A Baumann equation for
    the G function is the same as for the Q function。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: except we use the F function instead of the value function in the right hand
    side as shown in equation 36。 We can now regroup terms in this relation and get
    the second form shown here。 So the G function is an expectation of the same modified
    reward as in the F function。 but this time conditioned on the current state and
    action， like the Q function。 In other words。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在右侧使用F函数代替值函数，如方程36所示。我们现在可以重新组合此关系中的项，得到这里显示的第二种形式。因此，G函数是与F函数中的修改奖励相同的期望，但这次是以当前状态和动作为条件的，像Q函数一样。换句话说。
- en: the relation between the G function and the F function is the same as the relation
    between the V function。 We can also compare the resulting expression with the
    formulas for the F function。 And this gives us an explicit relation between the
    G function and the F function。 which is shown in equation 37。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: G函数与F函数之间的关系与V函数之间的关系相同。我们还可以将结果表达式与F函数的公式进行比较。这为我们提供了G函数与F函数之间的明确关系，如方程37所示。
- en: '![](img/c821a60e51482683694de8c0ed278903_1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c821a60e51482683694de8c0ed278903_1.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[BLANK_AUDIO]。'
- en: '![](img/c821a60e51482683694de8c0ed278903_3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c821a60e51482683694de8c0ed278903_3.png)'
