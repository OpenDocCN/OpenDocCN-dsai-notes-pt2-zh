# P8：MDP & RL- Action Value Function - 兰心飞侠 - BV14P4y1u7TB

在之前的视频中，我们介绍了价值函数和最优价值函数，并讨论了如何找到它，贝尔曼最优方程的数值解。最优策略可以从，计算出字母后的最优值函数。但事实证明，这种方法并不是唯一可能的方法，还有一种更方便的方法。只要目标是更好的强化学习。这种方法是基于他们所谓的行动价值函数。让我们看看动作价值函数与价值函数有何不同。让我们首先回忆一下给定时间的价值函数，仅取决于状态 S 和策略 pi。它没有明确包含有关代理现在采取的操作的信息。动作价值函数 Q 是，当前状态 ST 和在该状态下采取的动作 AT。它计算与价值函数相同的预期累积奖励，但条件适用于 ST 和 AT。类似于价值函数。动作价值函数 Q 或简单的 Q 函数取决于策略 pi，当前状态和当前动作的 Q 函数的两个参数 S 和 A。这意味着在 Q 函数中，第一个动作是固定的，由 A 的值给出。

而后面的所有动作都由策略 pi 决定。我们可以使用这个定义来获得贝尔曼方程，Q 函数的方式类似于，我们得到价值函数的贝尔曼方程。我们要做的是再次将总和分成第一项和其余部分。第一项是现在采取行动 A 的奖励。因为动作 A 是固定的，因为它是 Q 函数的参数，奖励也是固定的。现在，第二项是相同的总和，但从下一个时间戳开始。正如我们刚才所说，本项不依赖于 A。它仅取决于策略 pi。所以，它只是价值函数，同样的问题，但从下一次格式开始。这为我们提供了 Q 函数的贝尔曼方程，如下所示。它用奖励来表示时间 T 的 Q 函数，同时T加上一个折扣预期。下一时刻价值函数的值。这样的方程使用起来不太方便，因为它现在涉及两个未知功能，而不仅仅是一个。但是，如果我们查看最优 Q 函数，可以获得更方便的方程。现在，最优 Q 函数的定义方式与最优值函数相同。

主要是最优动作价值函数Q star 是，所有 Q 函数和所有可能策略 pi 的最大值。所以，最优 Q 函数是这样说的，不时采取行动 A，然后稍后遵循最优策略 P 星。现在。让我们将其与最优值函数的定义进行比较，本质上说，采取，现在和以后的最优策略决定的最优动作。但这意味着最优值函数 V star 只是，现在采取的所有可能的行动 A 的最优行动价值函数 Q 星。换句话说。就其第二个参数而言，它是 Q 星的最大值。现在，我们可以使用这些定义来获得，一个涉及最优动作状态函数 Q star 的贝尔曼方程。首先，我们在 Bellman 方程中取所有策略 pi 的最大值。Q 函数并使用 Q 星的定义。这将产生幻灯片底部的第一个方程。这个方程关联了 Q 星和 V 星。现在，我们使用 V 星的定义作为所有动作 A 的最大 Q 星，这产生了最后一个仅涉及 Q 星函数的方程。

这个方程称为动作状态函数 Q 的贝尔曼最优方程。这是我们将要工作的方程式，在下周我们研究 Q 学习的时候。现在，让我对状态值函数的一些评论，或者可以根据经验估计动作状态价值函数，遵循机器学习范式。代理可以遵循策略 pi 并维持其访问的每个状态的平均奖励。如果每个州的访问次数足够多，然后平均值将收敛到状态的价值函数。我们还可以计算在每个步骤中采取的单独操作的此类平均值。在这种情况下。平均值将收敛到动作状态值函数的值。在强化学习中，这类估计称为蒙特卡罗方法。只要状态和动作的数量很少，它们就足够简单。但是如果这个数字很大或者如果状态或动作是连续的。为每个状态动作组合保留单独的平均值可能不切实际。在这种情况下，我们应该依靠，动作状态值函数的一些函数逼近方法。我们将在本课程的稍后部分回到这些主题。但现在，我想向你展示，强化学习最经典的例子之一。

极平衡问题。这个问题的任务是将水平力施加到，可以在一定长度的轨道上向左或向右移动的推车，以防止与汽车铰接的杆从初始垂直位置掉落。当杆从垂直线落下超过给定角度时，就会发生故障。现在，这可以表述为。指定的时间段 C。 对于每个时间步，没有发生一次故障，代理获得加一的奖励。如果发生故障，然后情节终止，代理开始新的情节。这个问题也可以被表述为一个持续的任务，而无需，一个固定的时间段 T。它无限期地持续或直到极点下降。此设置需要使用一些折扣 gamma 来处理，值函数定义中的无限和。在这种情况下，给定步骤的奖励将是，每次失败减一，否则为零。对于 MDP 公式，对于极点平衡问题。我们需要一组状态和一组动作。在这种情况下，国家，将是推车和杆子的位置和速度，并且动作将是施加到推车上的水平力。推车平衡问题是其中之一，在强化学习中看到的最古老的问题，并且习惯于，这些天来。

作为新强化学习算法的标准测试用例。在这里，我们可以看到一个显示动作的视频，用于此任务的训练强化学习代理。更多关于推车平衡问题的细节可以在 Seth 和 Bartold 的书中找到。由于我们想专注于金融领域的强化学习，具体而言，我们不会对此进行详细介绍。在设置下一课时，我们将开发一个足够简单的简单财务模型，作为强化学习的财务影响的测试案例。所以，让我们用。

![](img/a98b33954bc438e572dbcb87f00a08ab_1.png)



![](img/a98b33954bc438e572dbcb87f00a08ab_2.png)