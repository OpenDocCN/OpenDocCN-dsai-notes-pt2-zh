- en: P26：RL Approach - Fitted Q-Iteration - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P26：RL 方法 - 拟合 Q 迭代 - 兰心飞侠 - BV14P4y1u7TB
- en: So in the last video， we talked about the need to use cross-sectional information
    on all Monte Carlo pass。 or historical pass of the stock simultaneously when calculating
    an optimal policy。 To remind you why this is the case， it's simply because the
    catch policy is a function that defines the mapping。 for any inputs to the outputs。
    But each observation only gives some information about such function at one particular
    point。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在上一个视频中，我们讨论了在计算最优策略时需要同时使用所有蒙特卡罗路径的横截面信息，或者股票的历史路径。为了提醒大家为什么需要这样做，原因很简单，因为捕捉策略是一个定义映射的函数，对于任何输入，都会产生相应的输出。但是每个观察值只提供了该函数在某一个特定点上的一些信息。
- en: And updating the function using only one point can be a very long and noisy
    procedure。 This means that the classical keel learning algorithm。 even though
    it's guaranteed to converge asymptotically。 need to be replaced by sounding more
    practical that converges faster。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一数据点更新函数可能是一个非常长且噪声较大的过程。这意味着，尽管经典的 Q 学习算法可以保证渐近收敛，但仍然需要被更实用、更能更快收敛的算法所替代。
- en: Because we're working the setting of batch model reinforcement learning。 perhaps
    we could do better than the classical keel learning if we managed to do updates
    by looking at all realizations of portfolio dynamics。 that happened in data in
    order to decide on the optimal policy。 And fortunately。 there exist extensions
    of keel learning for such batch mode reinforcement learning settings。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理批量强化学习的设置，也许如果我们通过查看所有投资组合动态的实现来决定最优策略，我们能够比经典的 Q 学习做得更好。幸运的是，确实存在针对这种批量强化学习设置的
    Q 学习扩展。
- en: We will take a most popular extension of keel learning to a batch reinforcement
    learning setting called sheet of q iteration。 or FQI for sure。 This method was
    developed around 2005 and 2006 in a series of papers by Ernst and co-workers and
    by Murphy。 One noticeable point here is that Ernst and co-workers considered time
    stationary problems where the Q function does not depend on time。 And also many
    published research in reinforcement learning literature deal with an infinite
    horizon keel learning where a keel function is independent of time。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用一种最流行的扩展方法——批量强化学习中的适应性 Q 迭代（FQI）。这一方法大约在 2005 到 2006 年间，由 Ernst 及其同事和
    Murphy 在一系列论文中提出。一个显著的特点是，Ernst 和同事考虑了时间平稳问题，在这些问题中，Q 函数与时间无关。此外，许多强化学习文献中发布的研究也处理了无限时间视野的
    Q 学习，在这种情况下，Q 函数是时间无关的。
- en: Keel learning for such stationary problems is somewhat different from keel learning
    that we need in our problem。 which has a finite time horizon and therefore is
    time dependent。 But the paper by Murphy presented the version of batch mode keel
    learning that works for problems with a finite time horizon。 such as our problem。
    Now the fitted keel iteration method works with continuous value data and therefore
    we can now bring the model formulation back to a general continuous state space
    case。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种平稳问题的 Q 学习与我们问题中所需的 Q 学习有所不同，因为我们的任务有一个有限的时间视野，因此是时间相关的。但 Murphy 的论文展示了适用于有限时间视野问题（如我们的问题）的批量
    Q 学习版本。现在，拟合 Q 迭代方法适用于连续值数据，因此我们可以将模型表述带回到一般的连续状态空间问题中。
- en: as was the case in our Monte Carlo setting for the dynamic programming solution。
    But if we want to stick to a discrete space formulation fitted keel iteration
    can be used in essentially the same way。 The only difference would be in a specification
    of basis functions that the method needs to use。 So to reiterate the FQI method
    works by using all Monte Carlo pass or historical pass for the replication portfolio
    simultaneous name。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在蒙特卡罗设置中为动态规划解决方案所做的那样。如果我们想坚持离散空间的表述，拟合 Q 迭代可以用几乎相同的方式来实现。唯一的区别在于该方法需要使用的基函数的具体指定。因此，为了重申，FQI
    方法通过使用所有蒙特卡罗路径或历史路径来同步复制投资组合的方式工作。
- en: This is very similar to how we solve the problem using dynamic programming with
    Monte Carlo method for the case of known dynamics。 In this method we averaged
    over all scenarios at time t and t plus one simultaneously by taking an empirical
    mean of different pathwise Monte Carlo scenarios。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们使用动态规划方法和蒙特卡罗方法解决已知动态问题的方式非常相似。在这种方法中，我们通过对不同时刻 t 和 t+1 的路径蒙特卡罗场景取经验均值，来同时对所有情景进行平均。
- en: Conditioning on the information set of t at time t was implemented as conditioning
    on all Monte Carlo pass up to time t。 Now in the setting of batch mode reinforcement
    learning the only thing we need to change in such Monte Carlo setting is the structure
    of input output data。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间t的条件下，给定信息集t，已经通过在t时刻对所有蒙特卡洛路径进行条件化来实现。现在在批量模式强化学习的设定下，我们需要改变的唯一内容是在这种蒙特卡洛设置中的输入输出数据结构。
- en: In the setting of dynamic programming when the model is known the inputs are
    simulated or real forward pass of the state variable extreme。 And the outputs
    are the optimal actions and action policy and optimal Q function。 That is the
    negative option price。 The optimal action a star is determined by maximization
    of the optimal Q function and instantaneous rewards are computed in the course
    of backward recursion for the optimal action and optimal Q function。 This was
    the case for dynamic programming now in the setting of batch motor reinforcement
    learning neither transition probabilities nor the policy or reward functions unknown。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态规划的设定下，当模型已知时，输入是状态变量极限的模拟或实际前向传递。输出是最优动作和动作策略，以及最优Q函数。也就是说，负的期权价格。最优动作a星通过最大化最优Q函数来确定，瞬时奖励则是在后向递归过程中为最优动作和最优Q函数计算的。这是动态规划的情况，现在在批量强化学习的设定下，过渡概率、策略或奖励函数是未知的。
- en: The required output is the same as before。 That is we have to find option price
    and hach。 However the input is now richer than in the previous case as now we
    have two more observations for each observed state vector x team。 We know the
    action x at and the reward RT。 Instead of this function PT star and RT we now
    have only samples obtained from these functions in different states x team。 And
    this is the setting of batch reinforcement learning where the state variables
    at both times T and T plus one and rewards received for all Monte Carlo pass are
    used simultaneously to find the optimal action and optimal Q function at times。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的输出与之前相同。也就是说，我们必须找到期权价格和哈希。然而，输入现在比之前的情况更丰富，因为每个观察到的状态向量x队列现在有两个额外的观察值。我们知道动作x
    at和奖励RT。我们不再有PT星函数和RT，而是仅有从这些函数在不同状态x队列中获得的样本。这是批量强化学习的设定，其中在T时刻和T加一时刻的状态变量以及所有蒙特卡洛路径中收到的奖励都被同时用于找到最优动作和最优Q函数。
- en: For all states x T and all actions AT of x team。 And this amounts to learning
    a policy。 With either simulated Monte Carlo data or real data batch motor reinforcement
    learning works the same way。 Now let's see how fitted Q iteration works in our
    problem。 The FQI method looks for the optimal Q function using an expansion in
    the set of basis functions。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有状态x T和x队列的所有动作AT，这相当于学习一个策略。无论是使用模拟的蒙特卡洛数据还是实际数据，批量强化学习的工作方式是相同的。现在，让我们看看拟合Q迭代在我们的问题中的作用。FQI方法通过在基函数集合中展开来寻找最优Q函数。
- en: We can use the same set of basis functions phi N of x that we used in the dynamic
    programming solution。 Now recall that the reward is a quadratic function of actions。
    Therefore the Q function will also be a quadratic function of actions and we represent
    the Q function here as a product of vector A。 matrix W and vector phi as shown
    in this equation。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与动态规划解法中相同的基函数集合phi N(x)。现在回想一下，奖励是动作的二次函数。因此，Q函数也将是动作的二次函数，我们在这里将Q函数表示为向量A、矩阵W和向量phi的乘积，如本方程所示。
- en: Here AT will be a vector of size three made of unity AT and one half times AT
    squared。 FET is a vector of basis functions with size M where M is the number
    of basis functions。 And finally WT will be a matrix of K-efficients which will
    have the shape of three by M。 This form for the Q function captures its quadratic
    dependence on the action。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里AT将是一个大小为三的向量，由单位AT和一半的AT平方组成。FET是一个包含基函数的向量，大小为M，其中M是基函数的数量。最后，WT将是一个K系数的矩阵，形状为三乘M。Q函数的这种形式捕捉了它对动作的二次依赖。
- en: while parameterizing the dependence on coordinate Xt with basis functions phi
    N of x and K-efficients W。 For what follows we can equivalently write the Q function
    in a slightly different form。 If we join the coefficient matrix W and the vector
    phi into a new vector U_W。 which is indexed by W to emphasize that this vector
    depends on K-efficients W。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在用基函数phi N(x)和K系数W参数化对坐标Xt的依赖时，为了后续讨论，我们可以将Q函数以稍微不同的形式等效地表示。如果我们将系数矩阵W和向量phi合并成一个新的向量U_W，该向量按W索引，以强调这个向量依赖于K系数W。
- en: We can also put terminal conditions on the vector U_W。 For example if we set
    the terminal action AT for capital T to zero。 this translates into terminal conditions
    for components of vector U_W that are shown here。 In this case only the first
    component is non-zero at maturity and two other components vanish at time capital
    T。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为向量 U_W 设置终端条件。例如，如果我们将时间 T 时的终端动作 AT 设置为零，这将转换为向量 U_W 分量的终端条件，如下所示。在这种情况下，只有第一个分量在到期时非零，其他两个分量在时间
    T 时为零。
- en: Now we can use this representation of the Q function in two ways。 Let's first
    assume that parameters of matrix WT are known。 Then because Q function is quadratic
    in action AT we can find the optimal action and optimal Q function in terms of
    components of vector U_W。 If you do this simple math you will find these two equations。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用两种方式来表示 Q 函数。首先假设矩阵 WT 的参数是已知的。那么由于 Q 函数是关于动作 AT 的二次函数，我们可以根据向量 U_W 的分量来求解最优动作和最优
    Q 函数。如果做这个简单的数学运算，你会得到以下两个方程。
- en: They define optimal action and Q function in terms of components of vector U_W。
    But in fact our objective here is rather the opposite。 We have to find components
    of matrix WT or equivalently components of vector U_W from observable actions
    and states。 In this case we would rather view these equations from the right to
    the left。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 他们根据向量 U_W 的分量定义了最优动作和 Q 函数。但事实上，我们的目标恰恰相反。我们需要根据可观察到的动作和状态来求解矩阵 WT 的分量，或者等效地求解向量
    U_W 的分量。在这种情况下，我们更愿意从右到左来看待这些方程。
- en: But then we have two equations for three unknowns。 This is still okay because
    these unknowns depend on the same matrix WT and are dependent in this sense。 But
    it also means that to solve our problem we have to find directly the matrix WT
    from data。 And in the next video we will see how this can be done。 [ Silence ]。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们有两个方程和三个未知数。这仍然是可以的，因为这些未知数依赖于相同的矩阵 WT，并在这个意义上是相关的。但这也意味着，为了解决我们的这个问题，我们必须直接从数据中找到矩阵
    WT。在下一个视频中，我们将看到如何做到这一点。[静音]
- en: '![](img/9b0a84a0631923d3be6d37c28ab6e5bd_1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0a84a0631923d3be6d37c28ab6e5bd_1.png)'
- en: '![](img/9b0a84a0631923d3be6d37c28ab6e5bd_2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0a84a0631923d3be6d37c28ab6e5bd_2.png)'
