- en: P27：RL Approach - Fitted Q-Iteration- the Ψ-basis - 兰心飞侠 - BV14P4y1u7TB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P27：RL方法 - 拟合Q迭代 - Ψ基 - 兰心飞侠 - BV14P4y1u7TB
- en: So in the last video， we introduced two parameterizations for the Q function。
    One in terms of matrix W_t， and another in terms of vector U_W。 which was given
    by a product of matrix W_t and the vector phi of basis functions。 We saw that
    if we somehow observed optimal actions， a_t star， an optimal Q function values
    Q_t star。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在上一期视频中，我们为Q函数引入了两种参数化方式。一种是矩阵W_t的形式，另一种是向量U_W的形式，它是矩阵W_t与基函数向量phi的乘积。我们看到，如果我们某种方式观察到了最优动作a_t*，以及最优的Q函数值Q_t*。
- en: this would serve as two equations for three unknown components of vector U_W。
    Now。 this suggests that a right solution， that the one that would be found by
    the dynamic programming method。 if the dynamics are known， should be in the space
    of solutions。 parameterized by a time-dependent matrix W_t， as we did in our specification
    of the Q function。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这将作为两个方程，求解向量U_W的三个未知分量。现在，这表明一个正确的解——即通过动态规划方法找到的解——如果已知动态学，应该位于由时间相关矩阵W_t参数化的解空间中，就像我们在Q函数的定义中所做的那样。
- en: So we have to find a way to learn the matrix of K-fissions W_t。 Let's see how
    this can be done。 What we need to do is to rearrange terms in our definition of
    the Q function。 to convert it into a product of a parameter vector。 and a vector
    that depends on both the state and the action。 So how we do this？
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们必须找到一种方法来学习K-fissions矩阵W_t。让我们看看如何做到这一点。我们需要做的是重新排列Q函数定义中的项，将其转化为一个参数向量的乘积，以及一个依赖于状态和动作的向量。那么我们该怎么做呢？
- en: We do it in several simple steps。 First， let's write explicitly the value of
    the Q function。 as a trace of the matrix expression given here。 We write it as
    the element-wise product of elements of matrix W。 with elements of a matrix that
    is formed by a direct product of vectors A and phi。 This gives us the second line
    in this expression。 We have two types of multiplication here。 First。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过几个简单的步骤来实现。首先，我们显式地写出Q函数的值，作为这里给出的矩阵表达式的迹。我们将其写为矩阵W的元素与由向量A和phi的外积形成的矩阵的元素逐项乘积。这给了我们这个表达式的第二行。这里有两种类型的乘法。首先。
- en: this symbol stands for the element-wise product of two matrices。 This product
    is also known as the Adamar product of two matrices。 Second。 this other expression
    has this encircled cross product sign。 and this sign stands for a matrix given
    by the outer product of two vectors， A_t and phi_t。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个符号表示两个矩阵的逐项乘积。这个乘积也被称为两个矩阵的Hadamard乘积。第二，这个表达式有一个包围的叉积符号，表示的是由两个向量A_t和phi_t的外积给出的矩阵。
- en: By definition， the element i_j of this matrix is given by a product of A_t_i
    and phi_t_j。 So far so good， we converted the whole expression into a trace of
    a product of two matrices。 But now we can do one more thing and represent this
    expression， as a scalar product of two vectors。 What we have to do to this end
    is to convert both matrices entering this expression to vectors。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，矩阵的元素i_j是由A_t_i和phi_t_j的乘积给出的。到目前为止，我们已经成功地将整个表达式转化为两个矩阵乘积的迹。现在我们可以做一件事，将这个表达式表示为两个向量的标量积。为此，我们需要将进入该表达式的两个矩阵转换为向量。
- en: We can do this by concatenating columns of matrix W。 and the matrix given by
    the outer product of A_t and phi_t。 And this will give us the third line in this
    equation。 Now we can compactly write this whole expression as a dot product of
    vector W and vector psi。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将矩阵W的列连接起来，再与由A_t和phi_t的外积给出的矩阵相乘来实现这一点。这将给我们方程中的第三行。现在我们可以将整个表达式紧凑地写为向量W和向量psi的点积。
- en: And here vector psi is obtained by concatenating the columns of the outer product
    of vectors A_t and phi_t。 This can be viewed as a new set of basis functions that
    depend both on state and action。 The dependence on the action A_t is quadratic，
    but the dependence on the state x_t can be arbitrary and depends on the functional
    form of the original basis functions phi_n。 for the state space。 If we use for
    example cubic B-spines as basis functions。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，向量psi是通过将A_t和phi_t外积的列连接得到的。可以将其视为一个新的基函数集合，既依赖于状态也依赖于动作。动作A_t的依赖是二次的，而状态x_t的依赖则可以是任意的，取决于原始基函数phi_n的函数形式。如果我们例如使用立方B样条作为基函数。
- en: as we did in the dynamic programming solution to the model。 then the X dependence
    would be locally cubic。 Now let's compare what we got with the setting of dynamic
    programming。 In our current reinforcement learning setting， we reduced the expression
    for the。 q function to a dot product of vector W and the new vector of state action
    basis function psi_t。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在动态规划解中对模型所做的那样，如果X的依赖是局部三次的。那么让我们比较一下在动态规划设置下我们得到的结果。在我们当前的强化学习设置中，我们将q函数的表达式简化为向量W和状态-动作基函数ψ_t的新向量的点积。
- en: Both have a number of components given by 3m， so that the number of unknown
    coefficients in this problem is 3m。 As you could also easily guess just by looking
    at the matrix W_t， which has dimension 3 by m。 So we have 3m unknowns here。 Now
    what about the number of observed variables？
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 两者都有由3m给出的多个组件，因此这个问题中的未知系数的数量是3m。你也可以仅通过观察矩阵W_t来轻松猜测，矩阵W_t的维度是3乘m。所以我们在这里有3m个未知数。那么，观察变量的数量是多少呢？
- en: For each time step in the Betch-Mold reinforcement learning setting。 we have
    3m observables because we observed the state x_t。 the action at and the reward
    r_t for all of n historical or Monte Carlo paths for the stock。 So we have 3m
    observables for 3m unknowns。 And if n is larger than m。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在Betch-Mold强化学习设置中的每个时间步，我们有3m个可观察变量，因为我们观察了状态x_t、动作a_t和奖励r_t，对于所有n个历史路径或蒙特卡洛路径。所以我们有3m个可观察变量，对应3m个未知数。如果n大于m。
- en: then our problem is well posed。 We have n divided by m observations per 1v parameter
    in this setting。 Now we can compare this with the situation we had with the dynamic
    programming approach。 In that case， we had to find two quantities， the optimal
    policy and the optimal q function。 In our dynamic programming solution， we expanded
    both in m basis functions。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们的这个问题就是一个良定问题。在这个设置中，我们每个1v参数有n除以m个观察。现在我们可以将其与动态规划方法中的情况进行比较。在那种情况下，我们必须找到两个量，最优策略和最优q函数。在我们的动态规划解中，我们将这两个量展开为m个基函数。
- en: so we had 2m unknown coefficients in total。 This is less than 3m unknown coefficients
    in the reinforcement learning setting。 But on the other hand， the number of observations
    per time slice in a dynamic programming approach was also less than in the reinforcement
    learning approach。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们总共有2m个未知系数。这比强化学习设置中的3m个未知系数少。但另一方面，动态规划方法每个时间片的观察数量也比强化学习方法少。
- en: And this is because in the dynamic programming setting， we only observed the
    states。 So for n Monte Carlo paths， we only had n data points per each time。 The
    number of observations per parameter was therefore n divided by 2m。 which is actually
    less than the ratio of n divided by m that we have for the reinforcement learning
    setting。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在动态规划设置中，我们只观察到了状态。所以对于n个蒙特卡洛路径，我们每个时间只有n个数据点。因此每个参数的观察数量是n除以2m，实际上小于在强化学习设置中n除以m的比例。
- en: So it appears that at least when the data for reinforcement learning is generated
    from the dynamic programming solution both the reinforcement learning and dynamic
    programming methods should perform。 about equally well。 This setting is called
    on policy learning。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以看来，至少当强化学习的数据是从动态规划解生成时，强化学习和动态规划方法应该表现得差不多好。这种设置称为基于策略的学习（on-policy learning）。
- en: and it means that actions recorded for training of the reinforcement learning
    cation were actually optimal actions。 It turns out that this is indeed the case
    and both algorithms produce nearly identical results for such setting。 This actually
    gives us a simple way to benchmark our reinforcement learning algorithms。 Because
    we can solve our model when it's fully specified by means of dynamic programming。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着用于强化学习训练的动作实际上是最优的动作。事实证明，确实如此，在这种设置下，两个算法产生的结果几乎完全相同。这实际上为我们提供了一种简单的方法来基准测试我们的强化学习算法。因为当模型完全由动态规划指定时，我们可以求解它。
- en: we can compute the optimal policy from this solution。 Then we can just use the
    observed actions and the words obtained with this policy as data for fitted curation
    method and compute the optimal policy and q function using this method。 Because
    this solution is already known from dynamic programming。 we can use it to benchmark
    our reinforcement learning solution and check the speed of conversions。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个解中计算出最优策略。然后我们可以使用通过这个策略获得的观察到的动作和词汇作为拟合策划方法的数据，并使用该方法计算最优策略和q函数。因为这个解已经通过动态规划得出，我们可以用它来基准测试我们的强化学习解，并检查收敛的速度。
- en: the final results and so on。 This is actually very ensuring as we now know exactly
    what we expect to see。 So any problems with fitted curation would immediately
    show up if found there。 If we now want to try some other reinforcement learning
    algorithm instead of fitted curation。 again we have a good benchmark obtained
    with a dynamic programming approach。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果等等。这实际上是非常有保障的，因为我们现在确切地知道期望看到的是什么。因此，任何拟合策展的问题如果在那里被发现，会立刻显现出来。如果我们现在想尝试一些其他的强化学习算法，而不是拟合策展，再次地，我们通过动态规划方法获得了一个好的基准。
- en: Therefore we can test many different reinforcement learning algorithms with
    our simple MDP model for option pricing。 Let's take a short pause here and continue
    in the next video with the actual solution of the model with the fitted curation
    method。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用我们简单的MDP模型进行期权定价，测试许多不同的强化学习算法。我们在这里稍作停顿，接下来的视频中将继续使用拟合策展方法来解这个模型的实际解。
- en: '![](img/06e2e0050b82f5416f388b1b7f6c3663_1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06e2e0050b82f5416f388b1b7f6c3663_1.png)'
- en: '![](img/06e2e0050b82f5416f388b1b7f6c3663_2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06e2e0050b82f5416f388b1b7f6c3663_2.png)'
