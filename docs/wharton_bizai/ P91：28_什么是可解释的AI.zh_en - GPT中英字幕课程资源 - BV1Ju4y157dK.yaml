- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P91：28_什么是可解释的AI.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another class of potential challenges and solutions arising around AI when applied
    to HR involves explainable AI。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef4079a2b40aa07f2096b086e1a11e33_1.png)'
  prefs: []
  type: TYPE_IMG
- en: So what is explainable AI？ Explainable AI relates to methods where how and why
    the algorithm arrived at its decision can be understood by human experts。
  prefs: []
  type: TYPE_NORMAL
- en: And this contrasts with some types of machine learning that operate more like
    a black box。
  prefs: []
  type: TYPE_NORMAL
- en: So some types of machine learning you're going to put in the data and arrives
    at decisions。
  prefs: []
  type: TYPE_NORMAL
- en: We've talked about this， but reverse engineering this and trying to understand
    or explain， okay。
  prefs: []
  type: TYPE_NORMAL
- en: well I see this decision being made by the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: Why did it come to that decision and not another one？ Sometimes that can be
    very difficult to trace。
  prefs: []
  type: TYPE_NORMAL
- en: And in a lot of industries a lot of context this causes some pretty significant
    problems。
  prefs: []
  type: TYPE_NORMAL
- en: We'll contrast this with some other types of approaches we've talked about。
  prefs: []
  type: TYPE_NORMAL
- en: We talked about rule based systems which are when expertise from a human is
    put into an algorithm or a piece of software by a developer。
  prefs: []
  type: TYPE_NORMAL
- en: This looks like a tree many times and these kinds of systems tend to be relatively
    easy to explain。
  prefs: []
  type: TYPE_NORMAL
- en: If you think about a decision tree， if you think about how you ended up in a
    particular bottom point on the decision tree。
  prefs: []
  type: TYPE_NORMAL
- en: it's pretty easy to trace through the different points in the tree to figure
    out exactly how it arrived at that decision。
  prefs: []
  type: TYPE_NORMAL
- en: This kind of decision tree output tends to be relatively easy to explain。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef4079a2b40aa07f2096b086e1a11e33_3.png)'
  prefs: []
  type: TYPE_IMG
- en: By contrast， once you get into the world of deep learning which involves a technology
    called neural networks。
  prefs: []
  type: TYPE_NORMAL
- en: deep learning output can be much more difficult to explain。
  prefs: []
  type: TYPE_NORMAL
- en: So it's very effective at taking in data and making a prediction。
  prefs: []
  type: TYPE_NORMAL
- en: predicting or recommending a particular action。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef4079a2b40aa07f2096b086e1a11e33_5.png)'
  prefs: []
  type: TYPE_IMG
- en: But if we want to go back and understand why that system made that particular
    recommendation。
  prefs: []
  type: TYPE_NORMAL
- en: that can be a pretty significant barrier。 It's always possible with these kinds
    of deep learning systems。
  prefs: []
  type: TYPE_NORMAL
- en: In the next video we'll talk about why this matters in a lot of contexts。 [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef4079a2b40aa07f2096b086e1a11e33_7.png)'
  prefs: []
  type: TYPE_IMG
