- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P125：24_操纵.zh_en -
    GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manipulation falls between legitimate persuasion and illegitimate coercion。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_1.png)'
  prefs: []
  type: TYPE_IMG
- en: It means getting someone to do something that you want by somehow short-circuiting
    their。
  prefs: []
  type: TYPE_NORMAL
- en: capacity for rational decision-making。 All manipulation isn't illegal or unethical。
    Advertising。
  prefs: []
  type: TYPE_NORMAL
- en: political campaigning， and fundraising involve pushing people's buttons to get
    them。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_3.png)'
  prefs: []
  type: TYPE_IMG
- en: to take actions。 They might not have if they stopped and thought through them
    logically。
  prefs: []
  type: TYPE_NORMAL
- en: Or maybe they would， but they didn't realize someone was deliberately influencing
    their， choices。
  prefs: []
  type: TYPE_NORMAL
- en: So the line between merely creepy and unethical， or even illegal， can be hard
    to draw。
  prefs: []
  type: TYPE_NORMAL
- en: But it's still an important one to understand。 AI can manipulate people when
    it's not obvious that choices or decisions are being shaped。
  prefs: []
  type: TYPE_NORMAL
- en: by algorithms。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_5.png)'
  prefs: []
  type: TYPE_IMG
- en: For example， Facebook sparked a firestorm when it collaborated with academic
    researchers。
  prefs: []
  type: TYPE_NORMAL
- en: to measure whether changes in its newsfeed algorithm could generate what psychologists。
  prefs: []
  type: TYPE_NORMAL
- en: call emotional contagion。 Facebook deliberately fed certain users happier content。
    And sure enough。
  prefs: []
  type: TYPE_NORMAL
- en: they shared happier content with their friends。 When the academic paper was
    published， though。
  prefs: []
  type: TYPE_NORMAL
- en: it provoked outrage。 Facebook was deliberately changing users' moods。
  prefs: []
  type: TYPE_NORMAL
- en: If they could make people happy， they could also make them sad， or perhaps depressed。
  prefs: []
  type: TYPE_NORMAL
- en: In other research， Facebook found it could increase voter participation by tweaking
    the。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_7.png)'
  prefs: []
  type: TYPE_IMG
- en: newsfeed。 What if it subtly shifted content to favor one candidate over another？
  prefs: []
  type: TYPE_NORMAL
- en: Users would never know that their votes were being influenced。
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges here is that everything you see on the Facebook newsfeed
    is the result。
  prefs: []
  type: TYPE_NORMAL
- en: of algorithms。 And they're changing all the time。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Would Facebook showing you more post about climate change be manipulative？
  prefs: []
  type: TYPE_NORMAL
- en: If the company's executives believe that would be good for the world？
  prefs: []
  type: TYPE_NORMAL
- en: Or if they did so because they think they'll earn more revenue from electric
    vehicle ads？
  prefs: []
  type: TYPE_NORMAL
- en: Some forms of manipulation， though， are legally prohibited。
  prefs: []
  type: TYPE_NORMAL
- en: Those are categories like false or subliminal advertising。 Those， however。
  prefs: []
  type: TYPE_NORMAL
- en: are generally defined pretty narrowly。 The major legal concept that's relevant
    here beyond those examples is deception。
  prefs: []
  type: TYPE_NORMAL
- en: It's perfectly fine to market a product to customers that you think they want
    to buy。
  prefs: []
  type: TYPE_NORMAL
- en: even when it's personalized through AI。 Because users understand advertising
    is about selling them things。
  prefs: []
  type: TYPE_NORMAL
- en: The problem comes when the nature of the relationship isn't obvious。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Then there's exploitation。 That's a more harmful form of manipulation that involves
    taking advantage of vulnerabilities。
  prefs: []
  type: TYPE_NORMAL
- en: to produce voluntary agreements that would not occur in a competitive market。
    In the UK。
  prefs: []
  type: TYPE_NORMAL
- en: for example， airlines used algorithms to deliberately seat families apart in
    the。
  prefs: []
  type: TYPE_NORMAL
- en: plane when they purchased cheaper tickets that didn't give them the right to
    select， their seats。
  prefs: []
  type: TYPE_NORMAL
- en: That was a way of encouraging those people to upgrade to the higher-priced tickets
    where。
  prefs: []
  type: TYPE_NORMAL
- en: they could sit together。 But the algorithm was deliberately separating them
    in the plane as a result of generating。
  prefs: []
  type: TYPE_NORMAL
- en: that effect。 Now， that was a pretty simple algorithm。 It just looked at last
    names。
  prefs: []
  type: TYPE_NORMAL
- en: But one could imagine more devious systems that identified users in times of
    stress or， difficulty。
  prefs: []
  type: TYPE_NORMAL
- en: An infamous leaked Facebook advertising presentation suggested it could identify
    when teenagers。
  prefs: []
  type: TYPE_NORMAL
- en: were feeling worthless， insecure or anxious。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_13.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the point at which responsible AI practitioners need to draw a line。
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the likely PR backlash， if you wouldn't consciously design a
    business。
  prefs: []
  type: TYPE_NORMAL
- en: practice to exploit vulnerable people， you shouldn't do it through algorithms
    indirectly。
  prefs: []
  type: TYPE_NORMAL
- en: And then there is market manipulation， using algorithms to subtly undermine
    competitive， markets。
  prefs: []
  type: TYPE_NORMAL
- en: Amazon， for example， sells both third-party products and its own private label
    offerings。
  prefs: []
  type: TYPE_NORMAL
- en: Its search engine doesn't directly prioritize its own products。
  prefs: []
  type: TYPE_NORMAL
- en: But it does incorporate signals that use proxies for Amazon's profitability，
    which could result。
  prefs: []
  type: TYPE_NORMAL
- en: in that kind of bias。 So the question is whether that is impermissible manipulation
    of the market for products on。
  prefs: []
  type: TYPE_NORMAL
- en: Amazon。 Or in some cases， algorithms can engage in collusion， which is generally
    prohibited under。
  prefs: []
  type: TYPE_NORMAL
- en: antitrust competition policy laws。 In 2018， the U。S。
  prefs: []
  type: TYPE_NORMAL
- en: Department of Justice brought an action against poster sellers on Amazon。
  prefs: []
  type: TYPE_NORMAL
- en: because their algorithms were deliberately colluding to keep prices high。
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have even shown that machine learning algorithms can figure out
    on their own to。
  prefs: []
  type: TYPE_NORMAL
- en: engage in collusive strategies which result in higher profitability。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_15.png)'
  prefs: []
  type: TYPE_IMG
- en: So what should you do then to address all of these concerns about manipulation？
    As I've said。
  prefs: []
  type: TYPE_NORMAL
- en: there's no bright line defining manipulation。 Except maybe in some of those
    market manipulation cases where general principles of antitrust。
  prefs: []
  type: TYPE_NORMAL
- en: or competition policy， as it's called in some countries， can be applied。
  prefs: []
  type: TYPE_NORMAL
- en: The question you should ask though is whether the objectives of your AI system
    are creating。
  prefs: []
  type: TYPE_NORMAL
- en: mutually beneficial relationships with your stakeholders。
  prefs: []
  type: TYPE_NORMAL
- en: Are people getting what they would likely choose on their own if they understood
    the。
  prefs: []
  type: TYPE_NORMAL
- en: nature of the relationship？ Or are you essentially tricking them？
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_17.png)'
  prefs: []
  type: TYPE_IMG
- en: In the context of academic research on human subjects， a standard set of principles
    were。
  prefs: []
  type: TYPE_NORMAL
- en: developed in something called the Belmont Report in the 1970s。
  prefs: []
  type: TYPE_NORMAL
- en: The four main elements are first informed consent， meaning users truly understand
    what they're。
  prefs: []
  type: TYPE_NORMAL
- en: getting into unless it's something that involves no real risk of harm to them。
    Second， beneficence。
  prefs: []
  type: TYPE_NORMAL
- en: which is basically a do not harm principle。 Do not put people in a position
    where they could be exposed to serious physical or psychological。
  prefs: []
  type: TYPE_NORMAL
- en: harm。 Third， justice， which is the idea of not being exploitative， not taking
    advantage of people。
  prefs: []
  type: TYPE_NORMAL
- en: in vulnerable situations and the system being implemented in a fair way。 And
    finally。
  prefs: []
  type: TYPE_NORMAL
- en: a dedicated review board。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_19.png)'
  prefs: []
  type: TYPE_IMG
- en: In universities， this is called IRB， Institutional Review Board， which has to
    approve before。
  prefs: []
  type: TYPE_NORMAL
- en: any academic research on human subjects can commence。
  prefs: []
  type: TYPE_NORMAL
- en: It doesn't make sense to subject every AI project， especially in the private
    sector， to。
  prefs: []
  type: TYPE_NORMAL
- en: that level of scrutiny。 But many organizations find it helpful to have a dedicated
    center of excellence or committee。
  prefs: []
  type: TYPE_NORMAL
- en: to evaluate these and other ethical questions about major AI implementations。
    That， however。
  prefs: []
  type: TYPE_NORMAL
- en: shouldn't come at the expense of defusing responsible AI throughout the。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_21.png)'
  prefs: []
  type: TYPE_IMG
- en: organization。 Everyone involved should feel responsible for avoiding unethical
    manipulation and exploitation。
  prefs: []
  type: TYPE_NORMAL
- en: as well as algorithmic bias and other concerns that I've highlighted in this
    program。 Thank you。
  prefs: []
  type: TYPE_NORMAL
- en: '[end of transcript]， [ Silence ]。'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfc9c839abbaf2c2618d094aa5bd83a_23.png)'
  prefs: []
  type: TYPE_IMG
