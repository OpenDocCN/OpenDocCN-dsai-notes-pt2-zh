- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P130：29_可解释的AI是什么.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key challenge with the use of AI systems is related to explainability。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0417c3fc4047062ddcec8b70c338bd6e_1.png)'
  prefs: []
  type: TYPE_IMG
- en: So AI explainability is the use of methods in AI systems where why the algorithm
    arrived。
  prefs: []
  type: TYPE_NORMAL
- en: at a particular result can be easily understood by human experts。
  prefs: []
  type: TYPE_NORMAL
- en: This is closely related to the notion of interpretability， which is understanding
    why。
  prefs: []
  type: TYPE_NORMAL
- en: a decision was arrived at by an algorithm。 Even if you can。
  prefs: []
  type: TYPE_NORMAL
- en: it may not be able to necessarily explain that logic。
  prefs: []
  type: TYPE_NORMAL
- en: And this contrasts with kind of a black box approach that's normally associated
    with。
  prefs: []
  type: TYPE_NORMAL
- en: some types of more complex machine learning and particularly deep learning。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0417c3fc4047062ddcec8b70c338bd6e_3.png)'
  prefs: []
  type: TYPE_IMG
- en: So just to contrast these a little bit， if we have a decision that's made on
    basic business， rules。
  prefs: []
  type: TYPE_NORMAL
- en: normally these are easy to explain。 How did we arrive at the decision？
  prefs: []
  type: TYPE_NORMAL
- en: What factors mattered for coming to that decision？ Some of the simpler machine
    learning models。
  prefs: []
  type: TYPE_NORMAL
- en: models based in decision trees for instance， are also relatively easy to interpret。
  prefs: []
  type: TYPE_NORMAL
- en: One can look at a decision tree and sort of get a sense of how a particular
    decision was。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0417c3fc4047062ddcec8b70c338bd6e_5.png)'
  prefs: []
  type: TYPE_IMG
- en: arrived at or what factors mattered that led to that decision。 By contrast。
  prefs: []
  type: TYPE_NORMAL
- en: when we think about deep learning models based， for instance on neural networks。
  prefs: []
  type: TYPE_NORMAL
- en: especially complex ones based on a lot of data， they become relatively more
    difficult。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0417c3fc4047062ddcec8b70c338bd6e_7.png)'
  prefs: []
  type: TYPE_IMG
- en: to interpret。 Sometimes harder to look inside the algorithm and understand what
    it is that led to that。
  prefs: []
  type: TYPE_NORMAL
- en: decision being made。 There is this major trade-off with more complex models
    where on the one hand they're able。
  prefs: []
  type: TYPE_NORMAL
- en: to handle enormous amounts of data and make very accurate predictions。 But on
    the other hand。
  prefs: []
  type: TYPE_NORMAL
- en: they can be difficult to explain the logic。 And so explainability， it turns
    out。
  prefs: []
  type: TYPE_NORMAL
- en: is key to adoption though in many contexts。 When you think about implementation。
  prefs: []
  type: TYPE_NORMAL
- en: even if a model is really accurate， an inability， to explain how it arrives
    at a decision is going to be a major impediment to adoption。
  prefs: []
  type: TYPE_NORMAL
- en: We'll talk about some examples of that。 But explainability is a key initiative
    right now in AI。
  prefs: []
  type: TYPE_NORMAL
- en: It's key frontier。 Big tech companies are currently heavily invested in this
    issue。
  prefs: []
  type: TYPE_NORMAL
- en: There are also efforts by the government， for instance， that are funding programs
    to。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0417c3fc4047062ddcec8b70c338bd6e_9.png)'
  prefs: []
  type: TYPE_IMG
- en: develop better explainable AI。 [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0417c3fc4047062ddcec8b70c338bd6e_11.png)'
  prefs: []
  type: TYPE_IMG
