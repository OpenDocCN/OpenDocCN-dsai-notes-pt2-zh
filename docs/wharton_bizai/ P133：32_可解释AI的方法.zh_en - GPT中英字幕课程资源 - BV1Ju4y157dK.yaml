- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P133：32_可解释AI的方法.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's talk a little bit about some of the different approaches that are being
    taken to。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_1.png)'
  prefs: []
  type: TYPE_IMG
- en: make algorithms more explainable。 So I'll talk about a few。 One is called SHAP。
  prefs: []
  type: TYPE_NORMAL
- en: short for Shapley additive explanations。 The idea behind SHAP is that if you
    have a number of variables or types of data that。
  prefs: []
  type: TYPE_NORMAL
- en: are going into making predictions， what SHAP is going to do is kind of going
    to tell you。
  prefs: []
  type: TYPE_NORMAL
- en: something about how much each of those variables matter for making the final
    prediction。
  prefs: []
  type: TYPE_NORMAL
- en: It's going to kind of identify the role of the importance of each of those features
    for。
  prefs: []
  type: TYPE_NORMAL
- en: making the final prediction。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_3.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal of it is really to explain the prediction of an instance by computing
    the contribution。
  prefs: []
  type: TYPE_NORMAL
- en: of each feature to the prediction。 So it's going to essentially what it's going
    to do is in turn swap out each different。
  prefs: []
  type: TYPE_NORMAL
- en: feature to kind of see how the predictions change。
  prefs: []
  type: TYPE_NORMAL
- en: And what you get at the end of that is essentially some indication of how important
    each of those。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_5.png)'
  prefs: []
  type: TYPE_IMG
- en: features was。 You can look at that and sort of say， "Well， when I think about
    a final decision。
  prefs: []
  type: TYPE_NORMAL
- en: this， feature， this variable is really what mattered more。"， So from a demographic
    perspective。
  prefs: []
  type: TYPE_NORMAL
- en: if you think about gender， if you think about age， and so on。
  prefs: []
  type: TYPE_NORMAL
- en: it will tell you which of those things mattered most maybe for making a final。
  prefs: []
  type: TYPE_NORMAL
- en: prediction so you can get a sense of what matters for the outcome there。 So
    that's SHAP。
  prefs: []
  type: TYPE_NORMAL
- en: One approach is called Lyme， local interpretable model agnostic explanations。
  prefs: []
  type: TYPE_NORMAL
- en: And the idea is to generate a simpler linear approximation to a more complex
    boundary for。
  prefs: []
  type: TYPE_NORMAL
- en: the total data space you're dealing with。 So imagine you have a space。
  prefs: []
  type: TYPE_NORMAL
- en: you have lots and lots of customers， and the model you're。
  prefs: []
  type: TYPE_NORMAL
- en: using for prediction can really is really difficult to interpret across the
    entire customer， space。
  prefs: []
  type: TYPE_NORMAL
- en: What Lyme will basically do is narrow in on a smaller set of customers that
    maybe look。
  prefs: []
  type: TYPE_NORMAL
- en: similar and within that smaller set of customers， it'll be able to provide a
    way to explain。
  prefs: []
  type: TYPE_NORMAL
- en: to those customers， at least in that little neighborhood exactly why the decision
    was。
  prefs: []
  type: TYPE_NORMAL
- en: arrived at that's accurate， even if that may not translate to the larger set。
  prefs: []
  type: TYPE_NORMAL
- en: So you can kind of start to tell people， compared to your people who look much
    like you， this。
  prefs: []
  type: TYPE_NORMAL
- en: is why you were a little bit different and this is how the decision arrived，
    the predictive。
  prefs: []
  type: TYPE_NORMAL
- en: model arrived， the decision that was slightly different。 So that's Lyme。
  prefs: []
  type: TYPE_NORMAL
- en: it provides user interfaces that help make those kinds of decisions transparent。
  prefs: []
  type: TYPE_NORMAL
- en: to people who are affected。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_7.png)'
  prefs: []
  type: TYPE_IMG
- en: A third approach is called surrogate decision trees。
  prefs: []
  type: TYPE_NORMAL
- en: And the idea here is to generate a decision tree， which is a little bit easier
    to understand。
  prefs: []
  type: TYPE_NORMAL
- en: that approximates a more complex model。 So think about a more complex model
    like a deep learning network。
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind a surrogate decision tree is to create a decision tree that
    mimics in。
  prefs: []
  type: TYPE_NORMAL
- en: many ways the deep learning network and what it will output， but a decision
    tree is going。
  prefs: []
  type: TYPE_NORMAL
- en: to be much easier to interpret than the deep learning model。
  prefs: []
  type: TYPE_NORMAL
- en: And so the surrogate tree can be used then to explain the deep learning model
    in a way。
  prefs: []
  type: TYPE_NORMAL
- en: that makes the application of that model much more explainable。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Another application called variational auto encoders， these variational auto
    encoders will。
  prefs: []
  type: TYPE_NORMAL
- en: take data and distill them down to some key features and these key features
    tend to be。
  prefs: []
  type: TYPE_NORMAL
- en: much more interpretable than the raw data itself。 And so we think about raw
    data being put in and decisions coming out of a predictive。
  prefs: []
  type: TYPE_NORMAL
- en: model。 There's an intermediate step where the data are boiled down to a set
    of interpretable。
  prefs: []
  type: TYPE_NORMAL
- en: features。 And so this is another approach that firms are using to think about
    explainable AI。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_11.png)'
  prefs: []
  type: TYPE_IMG
- en: So there are a variety of different approaches that are being used to make AI
    systems more。
  prefs: []
  type: TYPE_NORMAL
- en: explainable。 Different approaches have different advantages and disadvantages。
  prefs: []
  type: TYPE_NORMAL
- en: but it's a very active space， right now。 Microsoft， IBM for instance。
  prefs: []
  type: TYPE_NORMAL
- en: a lot of companies looking at integrating explainability into， their products。
  prefs: []
  type: TYPE_NORMAL
- en: If you look at a lot of their machine learning products that are offered to
    customers， to。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_13.png)'
  prefs: []
  type: TYPE_IMG
- en: the cloud for instance， a lot of them are offering interpretability or explainability。
  prefs: []
  type: TYPE_NORMAL
- en: in terms of their model output in a way that's going to matter certainly for
    adoption and。
  prefs: []
  type: TYPE_NORMAL
- en: number of industry contexts。 [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee91f9e6017b3ae1e6f69a55777d712_15.png)'
  prefs: []
  type: TYPE_IMG
