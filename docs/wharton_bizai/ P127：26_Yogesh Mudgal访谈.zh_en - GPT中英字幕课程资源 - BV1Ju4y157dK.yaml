- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P127：26_Yogesh Mudgal访谈.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today， I'm joined by Yogish Mughal。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40126034711e8d87ea2b25411d8075f2_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Yogish currently works for Citi， where he is the Director in Operational Risk
    Management。 In 2019。
  prefs: []
  type: TYPE_NORMAL
- en: Yogish formed an informal group， of like-minded professionals called ERS， AI
    Risk and Security。
  prefs: []
  type: TYPE_NORMAL
- en: ERS seeks to promote， educate and advance AI， machine learning governance for
    the financial services industry。
  prefs: []
  type: TYPE_NORMAL
- en: by focusing on risk， identification， categorization， and mitigation。 Welcome，
    Yogish。 Thank you。
  prefs: []
  type: TYPE_NORMAL
- en: Mary。 Thank you for having me。 So I'm so happy to have you here。
  prefs: []
  type: TYPE_NORMAL
- en: So to ground us in your perspective， can you tell us about your career journey
    and your role at Citi？
  prefs: []
  type: TYPE_NORMAL
- en: So starting with Citi， I had the emerging tech， engineering。
  prefs: []
  type: TYPE_NORMAL
- en: and architecture group in Operation Risk at Citi。 Most of my career has been
    devoted to risk management。
  prefs: []
  type: TYPE_NORMAL
- en: and information security。 For the last couple of years， I have。
  prefs: []
  type: TYPE_NORMAL
- en: been more focused on identifying and managing risks。
  prefs: []
  type: TYPE_NORMAL
- en: with emerging technologies in general and AI in particular。 To that end。
  prefs: []
  type: TYPE_NORMAL
- en: like you were saying earlier， I founded ERS。 In 2019， of like-minded professional。
  prefs: []
  type: TYPE_NORMAL
- en: which stands for AI Risk and Security， essentially。
  prefs: []
  type: TYPE_NORMAL
- en: trying to see if in the industry we can come together， and have a common view
    on identifying risks。
  prefs: []
  type: TYPE_NORMAL
- en: with AI and how to ensure that it's being implemented， safely and securely。
  prefs: []
  type: TYPE_NORMAL
- en: AI can be viewed as a risk to firms。 And so then they can be observed as reputational
    risk。
  prefs: []
  type: TYPE_NORMAL
- en: operational risk， or regulatory risk。 So these different terms of risk。 And
    so I was wondering。
  prefs: []
  type: TYPE_NORMAL
- en: could you describe these risks， in particular， like which ones are most concerning。
  prefs: []
  type: TYPE_NORMAL
- en: from your experience？ So in my opinion， risk should be managed for any technologies。
  prefs: []
  type: TYPE_NORMAL
- en: or a set of technologies， including AI。 So the risk related to AI for an institution。
  prefs: []
  type: TYPE_NORMAL
- en: can be dependent on various variables， including， how AI is implemented。
  prefs: []
  type: TYPE_NORMAL
- en: strength of existing controls， or a risk profile of the institutions and the
    risk appetite。
  prefs: []
  type: TYPE_NORMAL
- en: As described in the paper， which probably we didn't mention yet。
  prefs: []
  type: TYPE_NORMAL
- en: but ERS did publish a paper through Wartan， and in that paper， we also described。
  prefs: []
  type: TYPE_NORMAL
- en: various risk categories for AI。 And some of the risk categories that I will
    highlight here。
  prefs: []
  type: TYPE_NORMAL
- en: are the data related risks， AI attacks， testing and trust， and compliance。 So
    in data related risk。
  prefs: []
  type: TYPE_NORMAL
- en: it could include subcategories， like learning limitations。 For example， the
    learning limitations。
  prefs: []
  type: TYPE_NORMAL
- en: and of course the data quality， to summarize， the AI system is generally as
    effective as the data。
  prefs: []
  type: TYPE_NORMAL
- en: used to train it and the various scenarios considered， while training the system。
    In AI ML attacks。
  prefs: []
  type: TYPE_NORMAL
- en: there's been research done in the industry， on live ML models that talked about
    various attack。
  prefs: []
  type: TYPE_NORMAL
- en: including data privacy attack， training data poisoning， adversarial inputs，
    and model extraction。
  prefs: []
  type: TYPE_NORMAL
- en: And then there's testing and trust。 Obviously， trust and testing is the most
    common。
  prefs: []
  type: TYPE_NORMAL
- en: talked about topic in the AI field， which includes bias and explainability。
  prefs: []
  type: TYPE_NORMAL
- en: and things of that nature and compliance。 And compliance including internal
    policies。
  prefs: []
  type: TYPE_NORMAL
- en: as well as regulatory requirements。 I think all of these are the various risks。
  prefs: []
  type: TYPE_NORMAL
- en: that we can summarize or categorize into for AI。 And within the group at ERS，
    is there。
  prefs: []
  type: TYPE_NORMAL
- en: anyone particular parts of one of those three risk areas？
  prefs: []
  type: TYPE_NORMAL
- en: Is anyone more at a higher level or does it， depend on the particular industry
    or the particular organization。
  prefs: []
  type: TYPE_NORMAL
- en: in terms of their willingness to control the risk？ Like， I was just curious
    how it plays in。
  prefs: []
  type: TYPE_NORMAL
- en: Is it the peoples or the company or is the industry， that prioritizes these
    risks， like very。
  prefs: []
  type: TYPE_NORMAL
- en: very volatile？ Yes， so you're absolutely right。 It really depends on where AI
    is implemented。
  prefs: []
  type: TYPE_NORMAL
- en: how it is implemented， and the risk appetite of the institution。 So we cannot
    say one size fits or。
  prefs: []
  type: TYPE_NORMAL
- en: or in this case， one risk fits， all or the top risks。 So it really depends on
    the implementation。
  prefs: []
  type: TYPE_NORMAL
- en: Let's say， for example， if somebody， has an AI model sitting out on a cloud。
  prefs: []
  type: TYPE_NORMAL
- en: so the risk on the AI ML attacks probably， become more heightened， depending
    on how exposed。
  prefs: []
  type: TYPE_NORMAL
- en: that AI model is， versus an AI model， which is internal， being used internally
    in the organization。
  prefs: []
  type: TYPE_NORMAL
- en: Probably those attacks probably are less likelihood。
  prefs: []
  type: TYPE_NORMAL
- en: but there could be compliance risk that is more prevalent。 So it really depends
    on use case。
  prefs: []
  type: TYPE_NORMAL
- en: and there's， no one risk that I would say is higher or lower in that case。
  prefs: []
  type: TYPE_NORMAL
- en: So that leads into just peeling back the onion， a little bit more on this。
  prefs: []
  type: TYPE_NORMAL
- en: because we're hearing more， in what we were talking about data and the training。
  prefs: []
  type: TYPE_NORMAL
- en: the data that's used， but we also hear about the algorithm。 So more and more。
  prefs: []
  type: TYPE_NORMAL
- en: we are hearing about algorithm， transparency within the media and at conferences。
  prefs: []
  type: TYPE_NORMAL
- en: similar to what you had mentioned earlier。 So what is this in terms of what's
    the specifics around what。
  prefs: []
  type: TYPE_NORMAL
- en: we hear sometimes it's called technical， which， would be like revealing source
    code versus a calibrated。
  prefs: []
  type: TYPE_NORMAL
- en: transparency。 So if you could just maybe give a little bit of context。
  prefs: []
  type: TYPE_NORMAL
- en: to both those terms that we're hearing a bit more。
  prefs: []
  type: TYPE_NORMAL
- en: and just overall how algorithms have to be more transparent。 What is a leader，
    how you address that。
  prefs: []
  type: TYPE_NORMAL
- en: and then potentially as consumers or investors， how they're looking at it。 So
    first of all。
  prefs: []
  type: TYPE_NORMAL
- en: I agree that a lot of terms floating， and transparency is one of them for sure。
  prefs: []
  type: TYPE_NORMAL
- en: So there are increased conversations and discussions， regarding transparency
    and explainability。
  prefs: []
  type: TYPE_NORMAL
- en: I think it's important to consider who is the audience， as depending on the
    stakeholder。
  prefs: []
  type: TYPE_NORMAL
- en: the level of transparency， or the need for transparency may differ。 For example。
  prefs: []
  type: TYPE_NORMAL
- en: transparency might be， required for an internal auditor versus a regulator。
  prefs: []
  type: TYPE_NORMAL
- en: versus a developer or the end user。 And the analogy I always give is I travel
    and I fly in a plane。
  prefs: []
  type: TYPE_NORMAL
- en: I bet my life on it， but I have no idea how the engine is working。
  prefs: []
  type: TYPE_NORMAL
- en: I don't know how good or bad the pilot is。 But with the important factor here
    is trust。
  prefs: []
  type: TYPE_NORMAL
- en: I trust that the system is working。 I trust that the plane is running well。
  prefs: []
  type: TYPE_NORMAL
- en: and I trust that I'll be taking to the destination， that I'm going to last but
    not least。 But here。
  prefs: []
  type: TYPE_NORMAL
- en: I think the common thing here is the trust。 And because of。
  prefs: []
  type: TYPE_NORMAL
- en: I lose the cliche term on the hype around AI， and there are some relevant， I
    think， fear。
  prefs: []
  type: TYPE_NORMAL
- en: or I should say， or harm related to AI that is not being fully understood。
  prefs: []
  type: TYPE_NORMAL
- en: So I think it's important that we develop the trust， and build a trust in the
    AI systems。
  prefs: []
  type: TYPE_NORMAL
- en: And so think it's important factor here for transparency， or the calibrated
    transparency。
  prefs: []
  type: TYPE_NORMAL
- en: in my opinion， is a trust factor。 If the stakeholder on a need to know basis，
    understands the why。
  prefs: []
  type: TYPE_NORMAL
- en: hows， and the trade-offs of using the AI， system， then， for example。
  prefs: []
  type: TYPE_NORMAL
- en: an internal auditor versus a regulator， or the end user， the transparency could
    be granted。
  prefs: []
  type: TYPE_NORMAL
- en: and hence build a trust in that AI system。 So we've talked about the data， the
    transparency。
  prefs: []
  type: TYPE_NORMAL
- en: of the algorithms， and that also might lead us to potentially。
  prefs: []
  type: TYPE_NORMAL
- en: if people request more and more to look at the source code， or revealing the
    source code。
  prefs: []
  type: TYPE_NORMAL
- en: And is there problems with revealing the source code？ If we could talk a little
    bit about that。
  prefs: []
  type: TYPE_NORMAL
- en: because that might be something else that people are wondering。
  prefs: []
  type: TYPE_NORMAL
- en: how they might manage if that's coming down the pike。 Yeah， so I think revealing
    the source code。
  prefs: []
  type: TYPE_NORMAL
- en: or in extreme cases compromise of a source code， whether it's an AI system or
    a non-AI system。
  prefs: []
  type: TYPE_NORMAL
- en: I think most of the risk remain the same。 Obviously。
  prefs: []
  type: TYPE_NORMAL
- en: the intellectual property is on the top of the list。 And there are other things
    involved， including。
  prefs: []
  type: TYPE_NORMAL
- en: if you reveal the source code， are you revealing the methodology。
  prefs: []
  type: TYPE_NORMAL
- en: used to build a system or arrive at a decision？ And depending on the stakeholder。
  prefs: []
  type: TYPE_NORMAL
- en: do you want to reveal that or not？ Can the source code be used to implant or
    embed backdoor。
  prefs: []
  type: TYPE_NORMAL
- en: into the system？ So there are various risks involved。
  prefs: []
  type: TYPE_NORMAL
- en: when we are talking about revealing the source code。 And in some cases， it might
    have to be done。
  prefs: []
  type: TYPE_NORMAL
- en: due to where if a law or the legal situation demands that。 But generally。
  prefs: []
  type: TYPE_NORMAL
- en: I don't think so source code is revealed， that easily or in generally speaking。
  prefs: []
  type: TYPE_NORMAL
- en: it should not be revealed if our companies don't do that。 And potentially。
  prefs: []
  type: TYPE_NORMAL
- en: it might be might be something in which companies， need to better educate potentially。
  prefs: []
  type: TYPE_NORMAL
- en: like the consumers or their clients or their customers， why revealing the source
    code is bad。
  prefs: []
  type: TYPE_NORMAL
- en: is not in the best interest for them， in that it reveals， it exposes them to
    more risk。
  prefs: []
  type: TYPE_NORMAL
- en: some of the risks that you talked about earlier。 And then your analogy about
    the flying on the plane。
  prefs: []
  type: TYPE_NORMAL
- en: You have purchased that ticket to be on the plane。
  prefs: []
  type: TYPE_NORMAL
- en: because you know that there's experts running the plane。
  prefs: []
  type: TYPE_NORMAL
- en: and there's a certain amount of assumption and trust。
  prefs: []
  type: TYPE_NORMAL
- en: that has transpired between you and the airline， that you are expecting them
    to keep certain things。
  prefs: []
  type: TYPE_NORMAL
- en: that you know they have to keep running， and you don't need to know exactly
    how it's being run。
  prefs: []
  type: TYPE_NORMAL
- en: But when there's a very important risk， they are going to let you know。 Well。
  prefs: []
  type: TYPE_NORMAL
- en: thank you so very much for all that information， and for talking to me today。
    I appreciate it。
  prefs: []
  type: TYPE_NORMAL
- en: Thank you， Yogi。 Thank you， Mary。 Thanks for having me。 [ Silence ]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40126034711e8d87ea2b25411d8075f2_3.png)'
  prefs: []
  type: TYPE_IMG
