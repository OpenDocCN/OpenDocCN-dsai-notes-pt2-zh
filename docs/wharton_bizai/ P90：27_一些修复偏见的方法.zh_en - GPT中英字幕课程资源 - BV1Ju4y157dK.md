# 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P90：27_一些修复偏见的方法.zh_en - GPT中英字幕课程资源 - BV1Ju4y157dK

 Given the seriousness of bias-related issues in machine learning systems。



![](img/91b12d46099405f923ca9d2893ff9060_1.png)

 the industry has responded and there are a number of ways to think about starting to fix bias。

 Some coming from industry， some coming from organizations themselves that are trying to build these systems。

 One approach is to improve the training data。 So we may know that the training data itself is biased。

 but there are things that can be done in the training data。

 Sometimes that satisfies other performance metrics， but that can improve diversity。

 So the data can be changed or manipulated somewhat to reflect a more diverse outcome。

 So that's one approach， improving the training data itself can help to mitigate some of the bias-related issues。

 that arise when building these types of systems。 In a similar vein。

 we can think about adding weights to some observations。 So maybe some observations in the data。

 some examples in the data better reflect where the organization wants to go， in terms of fairness。

 in terms of bias。 And so those observations can be weighted to a greater degree that can teach the machine learning algorithm。

 This is the kind of example that better reflects the kinds of decisions that we want to make。

 A very common one these days is really just to open up the system and provide more information。

 about what's happening at each stage in a way that makes it easier to spot bias and perhaps deal with it at the source。

 So an example of this is tools， for instance， that help you deal with pipeline diversity that might show you at each stage。

 exactly what the pipeline looks like， what diversity problems might be arising。

 Another example of this， we'll talk about this more in another session， is interpretable models。

 These are machine learning models that provide you a great deal of information about how it's making the decision it's making。

 So these both kind of reflect different approaches for tools that provide more information back to you。

 about where bias might be arising within the system itself at different points of the system itself。

 Some recommendations， some organizations have suggested providing just more information about the decision after it's been made。

 So we can go back and evaluate the decision and look for what might have driven that decision。

 So an example of that is Google has tried using nutrition， what they call nutrition cards。

 So these are basically sheets of information or information about a machine learning decision。

 and maybe different performance metrics used on different data sets。 And this type of approach。

 again， just provides more information so we can better understand bias characteristics。

 We can better understand how it performs across different data sets。

 We can better learn to spot where bias might be arising in a given system because we just have more information to work with。

 We know what went into the algorithm and how the outputs look in different contexts。

 The newer approach is training people who are building the algorithms to just be better equipped to deal with bias related issues。

 Some employers have taken the stance that are taking the initiative to try to provide training。

 to say data scientists or engineers in a way that helps them be better equipped when they're putting together。

 algorithms to understand where sources of bias might arise。

 where to look how to identify bias and how to think about resolving these types of problems。

 And then another more organizational， more organization wide approach to thinking about these issues。

 And this relates back to the point about whose job is it to manage bias in the first place。

 is the creation of AI councils。 These are just groups that set across the organization。



![](img/91b12d46099405f923ca9d2893ff9060_3.png)

 that might involve stakeholders from different parts of the organization to look at questions around AI。

 as a group to look at questions around AI from a variety of different perspectives。

 to come to conclusions about how to think about bias and how to deal with bias on an organizational level。

 [BLANK_AUDIO]。

![](img/91b12d46099405f923ca9d2893ff9060_5.png)