- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P89：26_偏见很难修复.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've talked about the notion that bias is challenging to manage。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2449d599e568b6b40accc19dbc412a0_1.png)'
  prefs: []
  type: TYPE_IMG
- en: It can also be quite complicated to fix for a variety of reasons。
  prefs: []
  type: TYPE_NORMAL
- en: So even if we know we want to fix bias in a machine learning system。
  prefs: []
  type: TYPE_NORMAL
- en: it can be complicated to fix that kind of a bias-related problem。
  prefs: []
  type: TYPE_NORMAL
- en: There's a number of reasons for this。 Number one is that there are trade-offs。
  prefs: []
  type: TYPE_NORMAL
- en: So when we think about resolving bias， it can often come at the cost of。
  prefs: []
  type: TYPE_NORMAL
- en: affecting other performance metrics that we care about like accuracy。
  prefs: []
  type: TYPE_NORMAL
- en: So in a variety of contexts like loans， giving loans to people， for instance，
    or healthcare。
  prefs: []
  type: TYPE_NORMAL
- en: so satisfying conceptions of fairness for some groups。
  prefs: []
  type: TYPE_NORMAL
- en: can often require sacrificing it for other groups。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2449d599e568b6b40accc19dbc412a0_3.png)'
  prefs: []
  type: TYPE_IMG
- en: We've seen this in a number of different applications of machine learning。
  prefs: []
  type: TYPE_NORMAL
- en: There are many people who have studied this sort of trade-off。 And of course。
  prefs: []
  type: TYPE_NORMAL
- en: the ideal is to minimize this trade-off as much as possible。
  prefs: []
  type: TYPE_NORMAL
- en: So we can achieve the dual goals of building systems that are both fair and
    accurate。
  prefs: []
  type: TYPE_NORMAL
- en: This is often a tough thing to do。 This is one engineering trade-off that people
    often run into。
  prefs: []
  type: TYPE_NORMAL
- en: when thinking about how to remove bias from a system。
  prefs: []
  type: TYPE_NORMAL
- en: Another issue that arises is that getting better training data is hard。
  prefs: []
  type: TYPE_NORMAL
- en: So we talked earlier about the idea that bias is really coming off often into
    the system。
  prefs: []
  type: TYPE_NORMAL
- en: because the training data themselves are biased。 Well。
  prefs: []
  type: TYPE_NORMAL
- en: it's not so easy sometimes to go out and get better training data。
  prefs: []
  type: TYPE_NORMAL
- en: We've talked about data adequacy bias， the idea that you just don't have。
  prefs: []
  type: TYPE_NORMAL
- en: sometimes enough training data on certain demographic groups。
  prefs: []
  type: TYPE_NORMAL
- en: And resolving that sometimes is not easy。 If you're talking about some particular
    types of data。
  prefs: []
  type: TYPE_NORMAL
- en: it's not so easy to populate your database in a way that's better or more representative。
  prefs: []
  type: TYPE_NORMAL
- en: of different groups that you might be serving。 There is a practical issue that
    arises in some organizations。
  prefs: []
  type: TYPE_NORMAL
- en: which is the question of who even should be dealing with bias issues。
  prefs: []
  type: TYPE_NORMAL
- en: We talked earlier about the notion that this requires a holistic view of the
    organization。
  prefs: []
  type: TYPE_NORMAL
- en: requires decisions that basically take into account a number of organizational
    priorities。
  prefs: []
  type: TYPE_NORMAL
- en: And so we think about these decisions that have deeply， philosophical trade-offs，
    moral trade-offs。
  prefs: []
  type: TYPE_NORMAL
- en: value-based trade-offs。 It's not even clear who at the organization should be
    responsible for making these decisions。
  prefs: []
  type: TYPE_NORMAL
- en: And that itself can be somewhat of a bottleneck or a barrier in thinking about
    bias。
  prefs: []
  type: TYPE_NORMAL
- en: Is this a problem for the developers to be dealing with？
  prefs: []
  type: TYPE_NORMAL
- en: Is it a problem for senior management to be dealing with？
  prefs: []
  type: TYPE_NORMAL
- en: Or who is it really that should be thinking about dealing with bias-related
    problems？
  prefs: []
  type: TYPE_NORMAL
- en: We've seen a number of responses to this type of issue。
  prefs: []
  type: TYPE_NORMAL
- en: a number of new positions like chief data officers and AI councils。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2449d599e568b6b40accc19dbc412a0_5.png)'
  prefs: []
  type: TYPE_IMG
- en: These are basically， these are positions groups that are explicitly commissioned。
  prefs: []
  type: TYPE_NORMAL
- en: with a task of thinking about data。 It's role in the organization and the problems
    that it brings。
  prefs: []
  type: TYPE_NORMAL
- en: But the larger question of who should be dealing with bias is often one that
    is difficult to answer。
  prefs: []
  type: TYPE_NORMAL
- en: and often presents a major barrier to thinking about where and how we should
    start dealing。
  prefs: []
  type: TYPE_NORMAL
- en: with bias in a organizational system。 [ Silence ]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2449d599e568b6b40accc19dbc412a0_7.png)'
  prefs: []
  type: TYPE_IMG
