- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P90：27_一些修复偏见的方法.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the seriousness of bias-related issues in machine learning systems。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91b12d46099405f923ca9d2893ff9060_1.png)'
  prefs: []
  type: TYPE_IMG
- en: the industry has responded and there are a number of ways to think about starting
    to fix bias。
  prefs: []
  type: TYPE_NORMAL
- en: Some coming from industry， some coming from organizations themselves that are
    trying to build these systems。
  prefs: []
  type: TYPE_NORMAL
- en: One approach is to improve the training data。 So we may know that the training
    data itself is biased。
  prefs: []
  type: TYPE_NORMAL
- en: but there are things that can be done in the training data。
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes that satisfies other performance metrics， but that can improve diversity。
  prefs: []
  type: TYPE_NORMAL
- en: So the data can be changed or manipulated somewhat to reflect a more diverse
    outcome。
  prefs: []
  type: TYPE_NORMAL
- en: So that's one approach， improving the training data itself can help to mitigate
    some of the bias-related issues。
  prefs: []
  type: TYPE_NORMAL
- en: that arise when building these types of systems。 In a similar vein。
  prefs: []
  type: TYPE_NORMAL
- en: we can think about adding weights to some observations。 So maybe some observations
    in the data。
  prefs: []
  type: TYPE_NORMAL
- en: some examples in the data better reflect where the organization wants to go，
    in terms of fairness。
  prefs: []
  type: TYPE_NORMAL
- en: in terms of bias。 And so those observations can be weighted to a greater degree
    that can teach the machine learning algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: This is the kind of example that better reflects the kinds of decisions that
    we want to make。
  prefs: []
  type: TYPE_NORMAL
- en: A very common one these days is really just to open up the system and provide
    more information。
  prefs: []
  type: TYPE_NORMAL
- en: about what's happening at each stage in a way that makes it easier to spot bias
    and perhaps deal with it at the source。
  prefs: []
  type: TYPE_NORMAL
- en: So an example of this is tools， for instance， that help you deal with pipeline
    diversity that might show you at each stage。
  prefs: []
  type: TYPE_NORMAL
- en: exactly what the pipeline looks like， what diversity problems might be arising。
  prefs: []
  type: TYPE_NORMAL
- en: Another example of this， we'll talk about this more in another session， is interpretable
    models。
  prefs: []
  type: TYPE_NORMAL
- en: These are machine learning models that provide you a great deal of information
    about how it's making the decision it's making。
  prefs: []
  type: TYPE_NORMAL
- en: So these both kind of reflect different approaches for tools that provide more
    information back to you。
  prefs: []
  type: TYPE_NORMAL
- en: about where bias might be arising within the system itself at different points
    of the system itself。
  prefs: []
  type: TYPE_NORMAL
- en: Some recommendations， some organizations have suggested providing just more
    information about the decision after it's been made。
  prefs: []
  type: TYPE_NORMAL
- en: So we can go back and evaluate the decision and look for what might have driven
    that decision。
  prefs: []
  type: TYPE_NORMAL
- en: So an example of that is Google has tried using nutrition， what they call nutrition
    cards。
  prefs: []
  type: TYPE_NORMAL
- en: So these are basically sheets of information or information about a machine
    learning decision。
  prefs: []
  type: TYPE_NORMAL
- en: and maybe different performance metrics used on different data sets。 And this
    type of approach。
  prefs: []
  type: TYPE_NORMAL
- en: again， just provides more information so we can better understand bias characteristics。
  prefs: []
  type: TYPE_NORMAL
- en: We can better understand how it performs across different data sets。
  prefs: []
  type: TYPE_NORMAL
- en: We can better learn to spot where bias might be arising in a given system because
    we just have more information to work with。
  prefs: []
  type: TYPE_NORMAL
- en: We know what went into the algorithm and how the outputs look in different contexts。
  prefs: []
  type: TYPE_NORMAL
- en: The newer approach is training people who are building the algorithms to just
    be better equipped to deal with bias related issues。
  prefs: []
  type: TYPE_NORMAL
- en: Some employers have taken the stance that are taking the initiative to try to
    provide training。
  prefs: []
  type: TYPE_NORMAL
- en: to say data scientists or engineers in a way that helps them be better equipped
    when they're putting together。
  prefs: []
  type: TYPE_NORMAL
- en: algorithms to understand where sources of bias might arise。
  prefs: []
  type: TYPE_NORMAL
- en: where to look how to identify bias and how to think about resolving these types
    of problems。
  prefs: []
  type: TYPE_NORMAL
- en: And then another more organizational， more organization wide approach to thinking
    about these issues。
  prefs: []
  type: TYPE_NORMAL
- en: And this relates back to the point about whose job is it to manage bias in the
    first place。
  prefs: []
  type: TYPE_NORMAL
- en: is the creation of AI councils。 These are just groups that set across the organization。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91b12d46099405f923ca9d2893ff9060_3.png)'
  prefs: []
  type: TYPE_IMG
- en: that might involve stakeholders from different parts of the organization to
    look at questions around AI。
  prefs: []
  type: TYPE_NORMAL
- en: as a group to look at questions around AI from a variety of different perspectives。
  prefs: []
  type: TYPE_NORMAL
- en: to come to conclusions about how to think about bias and how to deal with bias
    on an organizational level。
  prefs: []
  type: TYPE_NORMAL
- en: '[BLANK_AUDIO]。'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91b12d46099405f923ca9d2893ff9060_5.png)'
  prefs: []
  type: TYPE_IMG
