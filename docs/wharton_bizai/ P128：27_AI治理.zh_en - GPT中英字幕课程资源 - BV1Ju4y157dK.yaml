- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P128：27_AI治理.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lecture， we're going to talk about AI governance。
  prefs: []
  type: TYPE_NORMAL
- en: How do we ensure we can get the benefits of AI without。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311d0a9bc1c4101473c0683f965cc6_1.png)'
  prefs: []
  type: TYPE_IMG
- en: experiencing some of the risks we discussed？ There are three main principles
    that I've advocated in。
  prefs: []
  type: TYPE_NORMAL
- en: my book， A Human's Guide to Machine Intelligence and， their user control， transparency
    and audits。
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at each of them in turn。 First is controls。
  prefs: []
  type: TYPE_NORMAL
- en: This is the idea of giving user some control over the way。
  prefs: []
  type: TYPE_NORMAL
- en: algorithms make decisions for or about them。 In other words， keeping a human
    in the loop。
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider some examples。 So first let's look at Facebook News Feed。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: Facebook's News Feed faced some criticisms in 2016。
  prefs: []
  type: TYPE_NORMAL
- en: because there were some false news stories being circulated， on the platform。
    In response to that。
  prefs: []
  type: TYPE_NORMAL
- en: Facebook implemented many changes。 One of the critical changes was that they
    allowed users to be。
  prefs: []
  type: TYPE_NORMAL
- en: able to flag posts in their news feed that they felt were。
  prefs: []
  type: TYPE_NORMAL
- en: either false news or offensive in some way。 This feature gives users control
    and allows users to give。
  prefs: []
  type: TYPE_NORMAL
- en: feedback to the algorithm itself about certain decisions。
  prefs: []
  type: TYPE_NORMAL
- en: it's making that is problematic or incorrect。 And this helps the algorithm learn
    as well。
  prefs: []
  type: TYPE_NORMAL
- en: And in fact， in reality， over the last year， this feature。
  prefs: []
  type: TYPE_NORMAL
- en: has actually helped Facebook detect many problematic news posts。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: even though control sounds like a simple idea， there are many nuances here。
  prefs: []
  type: TYPE_NORMAL
- en: And one has to approach it with caution。 An example of why one needs to approach
    it with caution。
  prefs: []
  type: TYPE_NORMAL
- en: comes from， again， Facebook。 In 2015， Facebook released several mixer style
    news feed， controls。
  prefs: []
  type: TYPE_NORMAL
- en: So users could control precisely what kinds of posts， show up in their news
    feed。 For example。
  prefs: []
  type: TYPE_NORMAL
- en: they could say， show me more posts from these， kinds of friends or less posts
    from these other friends。
  prefs: []
  type: TYPE_NORMAL
- en: They could say， show me more or less relationship status。
  prefs: []
  type: TYPE_NORMAL
- en: messages or don't show me messages with profile changes， and so on。
  prefs: []
  type: TYPE_NORMAL
- en: Facebook tested this feature with a few users。 They found that user satisfaction
    among these users。
  prefs: []
  type: TYPE_NORMAL
- en: actually grew。 However， user engagement went down。
  prefs: []
  type: TYPE_NORMAL
- en: meaning that these users spend less time on Facebook。 They engaged with fewer
    posts。
  prefs: []
  type: TYPE_NORMAL
- en: They clicked on fewer posts and liked fewer posts。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311d0a9bc1c4101473c0683f965cc6_3.png)'
  prefs: []
  type: TYPE_IMG
- en: All of this suggested that the algorithm was not as effective， at showing users
    the kinds of posts。
  prefs: []
  type: TYPE_NORMAL
- en: that they would find interesting。 We have actually seen this play out in multiple
    other settings。
  prefs: []
  type: TYPE_NORMAL
- en: where algorithms and AI in particular， tend to perform quite well in an autonomous
    mode。
  prefs: []
  type: TYPE_NORMAL
- en: But when users can have some control， some of the performance actually dips。
  prefs: []
  type: TYPE_NORMAL
- en: So while I'm suggesting it's good to give users control， giving users a lot
    of control has one risk。
  prefs: []
  type: TYPE_NORMAL
- en: which is that performance can go down。 So one has to really think about how
    do you design a system。
  prefs: []
  type: TYPE_NORMAL
- en: that allows users to have control and help flag problems， with the algorithms
    when they arise。
  prefs: []
  type: TYPE_NORMAL
- en: But at the same time， not suffer from some of the performance， issues with too
    much user control。
  prefs: []
  type: TYPE_NORMAL
- en: Now， my research， as well as the research of many others。
  prefs: []
  type: TYPE_NORMAL
- en: suggests that there is a way to design some of these controls。
  prefs: []
  type: TYPE_NORMAL
- en: A recent study conducted by some of my colleagues at Wharton。
  prefs: []
  type: TYPE_NORMAL
- en: evaluated the impact of user control on trust。 In their experiment， users were
    asked to predict。
  prefs: []
  type: TYPE_NORMAL
- en: the score of that high school students， will receive in standardized tests。
  prefs: []
  type: TYPE_NORMAL
- en: based on some information they had， about these high school students。
  prefs: []
  type: TYPE_NORMAL
- en: Users were allowed to consult algorithms。 These users or subjects in the experiment。
  prefs: []
  type: TYPE_NORMAL
- en: were divided into four groups。 The first group had no control over the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: They had to decide whether or not they， wanted to use the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: But if they decided to use the algorithm， then whatever the algorithm suggested。
  prefs: []
  type: TYPE_NORMAL
- en: that would be their choice。 Groups 2 and 3 had very limited control over the
    algorithm。 For example。
  prefs: []
  type: TYPE_NORMAL
- en: group 2 was able to overrule the algorithm， or AI in some instances。
  prefs: []
  type: TYPE_NORMAL
- en: but very few instances in general。 And group 3 was allowed to overrule the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: or the model by changing the algorithm's predictions， by a small amount。
  prefs: []
  type: TYPE_NORMAL
- en: The last group had complete control over the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: Then they evaluated users' trust in the algorithm。 How often did they want to
    use the algorithm？
  prefs: []
  type: TYPE_NORMAL
- en: Or what percentage of these users wanted to use the algorithm？
  prefs: []
  type: TYPE_NORMAL
- en: They found that users with no control had low trust。 They were less willing
    to use the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: Users with even a little bit of control， had very high trust。
  prefs: []
  type: TYPE_NORMAL
- en: They were willing to use the algorithm much more。 And interestingly。
  prefs: []
  type: TYPE_NORMAL
- en: they found that the amount of control， didn't matter。
  prefs: []
  type: TYPE_NORMAL
- en: Whether users had little control or a lot of control。
  prefs: []
  type: TYPE_NORMAL
- en: the trust levels continued to be high and similar。 In short， what this research
    is showing。
  prefs: []
  type: TYPE_NORMAL
- en: is that a little bit of control goes a long way， in establishing user trust。
  prefs: []
  type: TYPE_NORMAL
- en: But you don't necessarily need a lot of control。 On the other hand， research
    suggests。
  prefs: []
  type: TYPE_NORMAL
- en: that performance sometimes drops with control。 Bringing the two together， the
    implication。
  prefs: []
  type: TYPE_NORMAL
- en: is that we need to give users just enough control， so they can overrule the
    algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: or have some control over how the algorithm makes decisions， for or about them。
  prefs: []
  type: TYPE_NORMAL
- en: But it doesn't mean that the algorithm cannot， make decisions on its own。 In
    particular。
  prefs: []
  type: TYPE_NORMAL
- en: in this instance， the idea， was to give users the ability to overrule the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: when they saw there was a problem。 The idea on Facebook， again， is that the
    algorithm figures out。
  prefs: []
  type: TYPE_NORMAL
- en: how to rank order posts in a newsfeed， which users can give feedback and say
    or indicate。
  prefs: []
  type: TYPE_NORMAL
- en: when certain problematic posts show up。 Here again， the idea is that when things
    go wrong。
  prefs: []
  type: TYPE_NORMAL
- en: users can give feedback。 And so the idea of user control。
  prefs: []
  type: TYPE_NORMAL
- en: is essentially to give users enough options in the product， design and in the
    user interface。
  prefs: []
  type: TYPE_NORMAL
- en: so they can guide the algorithm's performance， and in fact。
  prefs: []
  type: TYPE_NORMAL
- en: even overrule it when they notice something， is going wrong。
  prefs: []
  type: TYPE_NORMAL
- en: The next principle is that of transparency， which is the idea of giving users
    enough information。
  prefs: []
  type: TYPE_NORMAL
- en: on how algorithms are making decisions for or about them。 Now， transparency
    is an interesting idea。
  prefs: []
  type: TYPE_NORMAL
- en: and there are many interpretations of it。 One interpretation of transparency。
  prefs: []
  type: TYPE_NORMAL
- en: is the idea that we have to reveal， the source code of the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: This is also sometimes referred to as technical transparency。
  prefs: []
  type: TYPE_NORMAL
- en: When there was a crash in the US stock market in 2010。
  prefs: []
  type: TYPE_NORMAL
- en: there was an investigation done of the stock market flash， crash and the investigation
    suggested。
  prefs: []
  type: TYPE_NORMAL
- en: that automated trading algorithms had a role， to play in the flash crash。 In
    2015。
  prefs: []
  type: TYPE_NORMAL
- en: the CFTC or the Commodities and Futures Trading， Commission had a ruling and
    suggested a regulation where。
  prefs: []
  type: TYPE_NORMAL
- en: they suggested that the Department of Justice， could force traders to reveal
    the source code of their algorithms。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311d0a9bc1c4101473c0683f965cc6_5.png)'
  prefs: []
  type: TYPE_IMG
- en: The traders and most industry professionals， resisted this because it would
    cause。
  prefs: []
  type: TYPE_NORMAL
- en: them to reveal their proprietary IP。 And in fact， in the end， the ruling。
  prefs: []
  type: TYPE_NORMAL
- en: was modified so that these companies were not， supposed to share their source
    code。 In 2017。
  prefs: []
  type: TYPE_NORMAL
- en: the New York City also proposed a bill， on automated decisions。
  prefs: []
  type: TYPE_NORMAL
- en: This bill would have required that all vendors who。
  prefs: []
  type: TYPE_NORMAL
- en: submit software that is used by the city government， and that is used by the
    city government。
  prefs: []
  type: TYPE_NORMAL
- en: to make automated decisions would， be required to reveal their source code。
  prefs: []
  type: TYPE_NORMAL
- en: and make it publicly accessible。 The vendors again resisted because it。
  prefs: []
  type: TYPE_NORMAL
- en: would cause them to give up their proprietary IP。 And furthermore， it would
    allow hackers。
  prefs: []
  type: TYPE_NORMAL
- en: to evaluate their code and identify， vulnerabilities in their code and attack
    the code itself。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: in this instance as well， the ruling was modified， or the bill was modified。
  prefs: []
  type: TYPE_NORMAL
- en: And it was decided that technical transparency is not， the best solution。 So
    in many instances。
  prefs: []
  type: TYPE_NORMAL
- en: we have seen， the idea of transparency should not， be technical transparency，
    meaning revealing。
  prefs: []
  type: TYPE_NORMAL
- en: the source code to the public。 In fact， it's not even clear it adds any value。
  prefs: []
  type: TYPE_NORMAL
- en: because it's not clear an average layperson can actually， evaluate the source
    code and act on it。
  prefs: []
  type: TYPE_NORMAL
- en: Now， the kind of transparency that might be useful， is something a little more
    high level。
  prefs: []
  type: TYPE_NORMAL
- en: A recent study by researcher Renee Kieselchak， evaluated how user trust changes
    with transparency。
  prefs: []
  type: TYPE_NORMAL
- en: in the algorithm。 In the experiment that was run by the researcher。
  prefs: []
  type: TYPE_NORMAL
- en: students were provided a grade from an assignment， and the grade was determined
    algorithmically。
  prefs: []
  type: TYPE_NORMAL
- en: Some students received no information， about how the algorithm decided it's
    great。
  prefs: []
  type: TYPE_NORMAL
- en: Trust was very low。 Another group of students got some minimal information。
  prefs: []
  type: TYPE_NORMAL
- en: on how the algorithm decided grades。 In other words， it was given high level
    information。
  prefs: []
  type: TYPE_NORMAL
- en: on the factors that the algorithm weighed， and some basic explanation that provided
    them intuition。
  prefs: []
  type: TYPE_NORMAL
- en: on the algorithm's design。 Trust went up dramatically when this kind of transparency，
    was provided。
  prefs: []
  type: TYPE_NORMAL
- en: A third group received very high amount of transparency。
  prefs: []
  type: TYPE_NORMAL
- en: where they were given detailed formulae that， was used in the algorithm and
    very detailed specifics。
  prefs: []
  type: TYPE_NORMAL
- en: Trust actually remained low with this group。 This is partly because these users
    were not。
  prefs: []
  type: TYPE_NORMAL
- en: able to evaluate that complex information and make use of it。
  prefs: []
  type: TYPE_NORMAL
- en: All of this suggests that for end users， again， we don't need a lot of transparency。
  prefs: []
  type: TYPE_NORMAL
- en: We just need basic information， such as， was an algorithm used to make decision。
  prefs: []
  type: TYPE_NORMAL
- en: What kinds of data are being used by the algorithm？
  prefs: []
  type: TYPE_NORMAL
- en: What variables were considered and what variables were most， important for this
    decision？
  prefs: []
  type: TYPE_NORMAL
- en: This is an idea that is becoming more and more important。
  prefs: []
  type: TYPE_NORMAL
- en: And there is a whole sub-field in machine learning， which is focused on interpretable
    decisions。
  prefs: []
  type: TYPE_NORMAL
- en: or interpretable machine learning。 There are two main ideas within this field--。
  prefs: []
  type: TYPE_NORMAL
- en: global and local interpretability。 Global interpretability is about providing
    high level feedback。
  prefs: []
  type: TYPE_NORMAL
- en: to users or anyone else about what， are the most important variables or factors
    that。
  prefs: []
  type: TYPE_NORMAL
- en: are driving the algorithm or the models' decisions or predictions。
  prefs: []
  type: TYPE_NORMAL
- en: And local interpretability is about providing feedback。
  prefs: []
  type: TYPE_NORMAL
- en: on what are the most important variables or factors， driving a particular decision。
    For example。
  prefs: []
  type: TYPE_NORMAL
- en: if an individual's loan application was denied， what were the specific factors
    that。
  prefs: []
  type: TYPE_NORMAL
- en: resulted in this person's loan application being denied？
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311d0a9bc1c4101473c0683f965cc6_7.png)'
  prefs: []
  type: TYPE_IMG
- en: A lot of regulation is moving in this direction。 I previously mentioned EU has
    a regulation called GDPR。
  prefs: []
  type: TYPE_NORMAL
- en: which gives consumers a right to explanation。 So if companies are using advanced
    machine， learning。
  prefs: []
  type: TYPE_NORMAL
- en: such as neural nets or random forests， then they would need to provide explanations
    to consumers。
  prefs: []
  type: TYPE_NORMAL
- en: And the field of interpretable machine learning， will become relevant because
    it allows them。
  prefs: []
  type: TYPE_NORMAL
- en: to give high level explanations to consumers， about the factors driving the
    models' predictions or decisions。
  prefs: []
  type: TYPE_NORMAL
- en: Now， transparency has another interesting aspect to it。
  prefs: []
  type: TYPE_NORMAL
- en: We've talked about transparency as it relates， to the end consumer。
  prefs: []
  type: TYPE_NORMAL
- en: But there's also transparency as it， relates to managers and data scientists。
    Now。
  prefs: []
  type: TYPE_NORMAL
- en: when a data scientist is using a very complex model， like a neural net， to make
    a decision。
  prefs: []
  type: TYPE_NORMAL
- en: like loan approval decisions， even the data scientist， might not know what are
    the factors that。
  prefs: []
  type: TYPE_NORMAL
- en: are driving the models' performance。 And so interpretable machine learning。
  prefs: []
  type: TYPE_NORMAL
- en: is also relevant not just for the end user， but also for the data scientists
    or managers who。
  prefs: []
  type: TYPE_NORMAL
- en: are actually deploying these algorithms。 And here again， global interpretability。
  prefs: []
  type: TYPE_NORMAL
- en: and local interpretability matter。 Global interpretability， as I said。
  prefs: []
  type: TYPE_NORMAL
- en: is essentially this idea of whether we can explain at a high。
  prefs: []
  type: TYPE_NORMAL
- en: level what are the most important variables that， are driving a model's predictions。
  prefs: []
  type: TYPE_NORMAL
- en: For a loan approval decision， it might be feedback， that the loan approval is
    determined primarily。
  prefs: []
  type: TYPE_NORMAL
- en: by the applicant's income and second by their credit history。
  prefs: []
  type: TYPE_NORMAL
- en: and third by some other variable and so on。 Local interpretability， as I mentioned。
  prefs: []
  type: TYPE_NORMAL
- en: is about getting feedback on the most important variables， driving a particular
    decision。
  prefs: []
  type: TYPE_NORMAL
- en: So if my loan application was not approved， then local interpretability would。
  prefs: []
  type: TYPE_NORMAL
- en: involve explaining what are the factors that resulted， in Karthik's loan application
    being denied。
  prefs: []
  type: TYPE_NORMAL
- en: Now， this is becoming an increasingly important field， within machine learning。
  prefs: []
  type: TYPE_NORMAL
- en: There are many open source tools and third party vendors。
  prefs: []
  type: TYPE_NORMAL
- en: that are offering solutions that help companies offer， interpretability or add
    interpretability。
  prefs: []
  type: TYPE_NORMAL
- en: as part of their machine learning activities。 This can not only provide explanations
    to consumers。
  prefs: []
  type: TYPE_NORMAL
- en: and increase trust， it can also increase trust among managers。
  prefs: []
  type: TYPE_NORMAL
- en: who are trying to deploy these systems， and they can also be valuable tools
    for debugging。
  prefs: []
  type: TYPE_NORMAL
- en: The third principle that I advocate， is the idea of auditing algorithms， especially。
  prefs: []
  type: TYPE_NORMAL
- en: in high stakes settings。 There is， in fact， an ongoing regulatory proposal，
    here in the US Congress。
  prefs: []
  type: TYPE_NORMAL
- en: which is called the Algorithmic， Accountability Act， which， if， passed。
  prefs: []
  type: TYPE_NORMAL
- en: would require very large companies， to actually evaluate their high risk automated
    decision。
  prefs: []
  type: TYPE_NORMAL
- en: systems， meaning machine learning models that， are used to make high risk decisions
    like loan approvals。
  prefs: []
  type: TYPE_NORMAL
- en: or resume screening and so on， to have them audited， for things like accuracy
    and fairness。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: this regulation has not yet been passed， and it is not clear whether it will
    be passed or not。
  prefs: []
  type: TYPE_NORMAL
- en: in its original form。 But independent of that， forward-looking companies。
  prefs: []
  type: TYPE_NORMAL
- en: should not wait for regulation but should， take proactive steps so that they
    can win consumer trust。
  prefs: []
  type: TYPE_NORMAL
- en: prevent problems with the algorithms， and also ensure that their systems are
    robust for the long term。
  prefs: []
  type: TYPE_NORMAL
- en: So let's talk a little bit about what an audit process might， look like。
  prefs: []
  type: TYPE_NORMAL
- en: An audit process begins by first creating an inventory， of all machine learning
    models that。
  prefs: []
  type: TYPE_NORMAL
- en: are being used by an organization。 Next， for each model， we identify。
  prefs: []
  type: TYPE_NORMAL
- en: what kinds of use cases are the model being used for。
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the model might be used for very simple decisions。
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the model might be used for more complex decisions。 We identify the
    use cases。
  prefs: []
  type: TYPE_NORMAL
- en: We also identify who's the developer of the model， who's the business owner
    of the model。
  prefs: []
  type: TYPE_NORMAL
- en: which division owns it， which individual in the division， is perhaps responsible
    for it。
  prefs: []
  type: TYPE_NORMAL
- en: Each model might be given a risk rating， which might assess what are the social
    and financial risks。
  prefs: []
  type: TYPE_NORMAL
- en: if the model goes wrong。 This might be the basis of a decision of whether an
    audit is， needed。
  prefs: []
  type: TYPE_NORMAL
- en: If the risk rating of a model is relatively low or moderate， perhaps we don't
    need an audit process。
  prefs: []
  type: TYPE_NORMAL
- en: But if it's a high-risk model， then an audit process， may be initiated。
  prefs: []
  type: TYPE_NORMAL
- en: The audit can be conducted by internal experts， or by an external expert who's
    brought in。
  prefs: []
  type: TYPE_NORMAL
- en: An audit would look at a number of factors。 For example， they would start by
    looking at the inputs。
  prefs: []
  type: TYPE_NORMAL
- en: that are going into a model。 What is the quality of data？ Are there biases in
    the training data？
  prefs: []
  type: TYPE_NORMAL
- en: It would look at the model itself。 It would benchmark the model against alternative
    models。
  prefs: []
  type: TYPE_NORMAL
- en: and confirm it does better than the alternative models。
  prefs: []
  type: TYPE_NORMAL
- en: It would also stress test the model against simulated data， and confirm that
    the model works well。
  prefs: []
  type: TYPE_NORMAL
- en: even when the data looks different than the training data。 And lastly， we have
    outputs。
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is to look at the decisions or predictions， made by the model，
    evaluate it。
  prefs: []
  type: TYPE_NORMAL
- en: look at explanations， or interpretations so that we understand if the model
    is。
  prefs: []
  type: TYPE_NORMAL
- en: weighing the right kinds of factors more heavily。 We might also try and look
    at outliers in the predictions。
  prefs: []
  type: TYPE_NORMAL
- en: And so this is all collectively part of an audit process。 As I've mentioned，
    there are many risks。
  prefs: []
  type: TYPE_NORMAL
- en: with machine learning models that are automating decisions。
  prefs: []
  type: TYPE_NORMAL
- en: Issues range from bias to not understanding the models。 And an audit process
    can go a long way。
  prefs: []
  type: TYPE_NORMAL
- en: in stress testing the model before you actually deploy them。 In summary。
  prefs: []
  type: TYPE_NORMAL
- en: there are many ways to think about managing risks。 Control is an important aspect。
  prefs: []
  type: TYPE_NORMAL
- en: Make sure there's a human in the loop。 Give some humans control over the model。
  prefs: []
  type: TYPE_NORMAL
- en: The second is transparency， which， is the idea of explanations about how the
    model works。
  prefs: []
  type: TYPE_NORMAL
- en: And lastly， we have the notion of audits， so as to stress test the model。 In
    fact。
  prefs: []
  type: TYPE_NORMAL
- en: when you design the team of data scientists， you can think about a model developer
    that。
  prefs: []
  type: TYPE_NORMAL
- en: is focused on developing the machine learning model。 Then you might have a data
    science QA process。
  prefs: []
  type: TYPE_NORMAL
- en: And this is much like when software development， you have software engineers
    creating the software。
  prefs: []
  type: TYPE_NORMAL
- en: And then you have testers or test engineers， who are testing the software。 Similarly。
  prefs: []
  type: TYPE_NORMAL
- en: you could have a data science QA process， that is testing the model。
  prefs: []
  type: TYPE_NORMAL
- en: And for super high stakes model， there， might be a more deeper or more involved
    testing， which。
  prefs: []
  type: TYPE_NORMAL
- en: is essentially what an audit is all about。 So that is a set of frameworks that。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311d0a9bc1c4101473c0683f965cc6_9.png)'
  prefs: []
  type: TYPE_IMG
- en: can help us manage or govern some of the risks associated， with AI。 [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311d0a9bc1c4101473c0683f965cc6_11.png)'
  prefs: []
  type: TYPE_IMG
