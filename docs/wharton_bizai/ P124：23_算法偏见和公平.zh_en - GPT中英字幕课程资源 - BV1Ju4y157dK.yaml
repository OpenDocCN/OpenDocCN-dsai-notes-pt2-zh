- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P124：23_算法偏见和公平.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I'm Kevin Werback， a professor of Legal Studies and Business Ethics at Wharton，
    and I work。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_1.png)'
  prefs: []
  type: TYPE_IMG
- en: on issues of ethics and responsibility around AI and analytics。 Algorithmic
    bias and fairness。
  prefs: []
  type: TYPE_NORMAL
- en: You would think that a major benefit of AI is that it overcomes human biases
    and blind， spots。
  prefs: []
  type: TYPE_NORMAL
- en: And you'd be right。 Humans are prone to stereotypes and implicit biases and
    even explicit discrimination。
  prefs: []
  type: TYPE_NORMAL
- en: An AI system just looks at the data。 However， it's dangerous and misleading
    to think AI is inherently objective。
  prefs: []
  type: TYPE_NORMAL
- en: It can actually replicate or even reinforce human biases and produce deep unfairness。
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic bias is ethically wrong。 It's harmful to marginalized populations。
  prefs: []
  type: TYPE_NORMAL
- en: It can lead to backlash from employees， customers， and other stakeholders， and
    it may even lead。
  prefs: []
  type: TYPE_NORMAL
- en: to legal consequences。 In 2019， a study came out in science about a major academic
    hospital in Boston。
  prefs: []
  type: TYPE_NORMAL
- en: Researchers found that a care management algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: which referred patients for additional resources， if they were high risk。
  prefs: []
  type: TYPE_NORMAL
- en: produced systemic racial discrimination。 The average black patient referred
    by this program had nearly double the number of underlying。
  prefs: []
  type: TYPE_NORMAL
- en: conditions as the average white patient。 In other words。
  prefs: []
  type: TYPE_NORMAL
- en: the black patient had to be much sicker in order to receive the equivalent，
    level of care。
  prefs: []
  type: TYPE_NORMAL
- en: When the research has changed the algorithm， so that equally sick white and
    black patients。
  prefs: []
  type: TYPE_NORMAL
- en: made the cut it nearly tripled the percentage of black patients who qualified。
  prefs: []
  type: TYPE_NORMAL
- en: This is not a unique problem。 Facial recognition systems have been shown to
    be less accurate for dark skin faces。
  prefs: []
  type: TYPE_NORMAL
- en: Hiring algorithms used by companies like Amazon to predict job performance based
    on resumes。
  prefs: []
  type: TYPE_NORMAL
- en: have been found to disadvantage women。 Text generation and machine translation
    systems impose sexist associations and sometimes racist。
  prefs: []
  type: TYPE_NORMAL
- en: associations。 For example， if you translate from Hungarian， which has gender
    neutral pronouns。
  prefs: []
  type: TYPE_NORMAL
- en: there is， no he or she in Hungarian， but if you translate a Hungarian text into
    English on a major search。
  prefs: []
  type: TYPE_NORMAL
- en: engine， it will pick he as the pronoun for professor and politician， but she
    for washes。
  prefs: []
  type: TYPE_NORMAL
- en: the dishes and assistant。 What is going on here？ First data can embed human
    prejudice。
  prefs: []
  type: TYPE_NORMAL
- en: If women traditionally fail to get promoted and enjoy long careers at a company
    because。
  prefs: []
  type: TYPE_NORMAL
- en: of rampant sexism， an AI system will find that being female is associated with
    poor outcomes。
  prefs: []
  type: TYPE_NORMAL
- en: In the healthcare example， the problem was that the algorithm used previous
    treatment。
  prefs: []
  type: TYPE_NORMAL
- en: costs as a proxy for how sick a patient was。 The difficulty was that black people
    receive worse care on average。
  prefs: []
  type: TYPE_NORMAL
- en: so less is spent on， them even when they are equally or more sick than a white
    patient。
  prefs: []
  type: TYPE_NORMAL
- en: In other cases， the data itself may be biased。 There may simply be fewer examples
    of minority populations in the training data set resulting。
  prefs: []
  type: TYPE_NORMAL
- en: in less accurate models that appears to be part of what happened with the facial
    recognition。
  prefs: []
  type: TYPE_NORMAL
- en: systems。 And sometimes， even when classifications such as race and gender aren't
    even in the data。
  prefs: []
  type: TYPE_NORMAL
- en: set， they influence models through proxies。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_3.png)'
  prefs: []
  type: TYPE_IMG
- en: A zip code is just an address marker， but it can be strongly correlated with
    race or。
  prefs: []
  type: TYPE_NORMAL
- en: socioeconomic status， for example。 There are a variety of tools to incorporate
    fairness criteria into the design of algorithmic。
  prefs: []
  type: TYPE_NORMAL
- en: systems directly or to assess whether they produce discriminatory results。 However。
  prefs: []
  type: TYPE_NORMAL
- en: they aren't foolproof。 For one thing， there isn't a single definition of fairness。
  prefs: []
  type: TYPE_NORMAL
- en: The North Point Compass System， which was used to make recommendations for parole，
    famously。
  prefs: []
  type: TYPE_NORMAL
- en: made errors more frequently for black prisoners than white ones。 However。
  prefs: []
  type: TYPE_NORMAL
- en: given two identical prisoners in every respect but race， it would generally，
    assign the same score。
  prefs: []
  type: TYPE_NORMAL
- en: In this case， individual and group level fairness， two different ways of measuring
    fairness。
  prefs: []
  type: TYPE_NORMAL
- en: were shown to literally be mathematically impossible to achieve simultaneously。
  prefs: []
  type: TYPE_NORMAL
- en: There had to be choices and trade-offs made。 Systems that are more fair may
    also be less accurate。
  prefs: []
  type: TYPE_NORMAL
- en: We have to make choices in the design of these systems。 And in some cases。
  prefs: []
  type: TYPE_NORMAL
- en: there may not be objective standards of fairness at all。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_5.png)'
  prefs: []
  type: TYPE_IMG
- en: How one decides whether a social media news feed discriminates against conservatives，
    or liberals。
  prefs: []
  type: TYPE_NORMAL
- en: for example， depends on inherently subjective decisions about what should be
    the， baseline。
  prefs: []
  type: TYPE_NORMAL
- en: what fits into those political categories， and so forth。
  prefs: []
  type: TYPE_NORMAL
- en: It's not that we can't ever evaluate such claims。 We just can't write an objective
    specification about what a "neutral" news feed would look。
  prefs: []
  type: TYPE_NORMAL
- en: like because there's no such thing。 Finally， it's not an accident what data
    gets collected。
  prefs: []
  type: TYPE_NORMAL
- en: how data is evaluated， or what， questions get asked in the design of algorithms。
  prefs: []
  type: TYPE_NORMAL
- en: The same human factors that lead to marginalized groups being discriminated
    against in other。
  prefs: []
  type: TYPE_NORMAL
- en: contexts still apply here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now there are some legal claims that can be brought against biased or unfair
    algorithms。
  prefs: []
  type: TYPE_NORMAL
- en: but their scope is quite limited today。 The major legal category that would
    apply here is what's called disparate impact when。
  prefs: []
  type: TYPE_NORMAL
- en: a policy or practice that on its face is neutral。 It doesn't explicitly treat
    minority populations different from other populations。
  prefs: []
  type: TYPE_NORMAL
- en: It still could have an effect of different treatment。 That's called disparate
    impact。
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that area of the law is typically quite limited。
  prefs: []
  type: TYPE_NORMAL
- en: It only applies to a limited set of protected classes， like race and gender
    typically， and。
  prefs: []
  type: TYPE_NORMAL
- en: it generally applies only to certain activities which are specified under the
    law。
  prefs: []
  type: TYPE_NORMAL
- en: In the US that's basically employment and housing。
  prefs: []
  type: TYPE_NORMAL
- en: And then the bigger problem is that disparate impact generally requires a defined
    policy。
  prefs: []
  type: TYPE_NORMAL
- en: or practice that's having that differential impact on the protected class。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_9.png)'
  prefs: []
  type: TYPE_IMG
- en: The United States Supreme Court has said that just showing a statistical disparity，
    just。
  prefs: []
  type: TYPE_NORMAL
- en: showing that there is an effect that's worse against a protected class isn't
    enough。
  prefs: []
  type: TYPE_NORMAL
- en: There has to be some deliberate step that was taken， some policy， and just using
    an algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: itself is not enough。 So there's a disconnect between the way the discrimination
    law is structured and the kinds。
  prefs: []
  type: TYPE_NORMAL
- en: of issues that come up with algorithmic systems。 In Europe。
  prefs: []
  type: TYPE_NORMAL
- en: the general data protection regulation has an anti-bias provision in the sections。
  prefs: []
  type: TYPE_NORMAL
- en: dealing with what are called fully automated processing。
  prefs: []
  type: TYPE_NORMAL
- en: But that is limited in terms of its context and it's pretty vague。
  prefs: []
  type: TYPE_NORMAL
- en: It's not entirely clear yet how that could be applied。
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of proposals and different jurisdictions for new laws。
  prefs: []
  type: TYPE_NORMAL
- en: The European Union is considering a major new AI law that they've had a discussion
    paper， about。
  prefs: []
  type: TYPE_NORMAL
- en: And in the US there have been proposals for what are called the Algorithmic
    Accountability。
  prefs: []
  type: TYPE_NORMAL
- en: Act which would require bias checking techniques。 But these have not yet been
    widely adopted。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_11.png)'
  prefs: []
  type: TYPE_IMG
- en: So how should organizations respond to these challenges？ Well first of all。
  prefs: []
  type: TYPE_NORMAL
- en: make sure that your training data set is deep and diverse。
  prefs: []
  type: TYPE_NORMAL
- en: That it has enough examples along different measures of diversity。
  prefs: []
  type: TYPE_NORMAL
- en: Think about be aware of proxies where something that facially is neutral might
    actually encode。
  prefs: []
  type: TYPE_NORMAL
- en: some attributes of a protected class。 Think about what fairness function makes
    the most sense for your application。
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of different ways to mathematically define what a fair algorithm
    is。
  prefs: []
  type: TYPE_NORMAL
- en: And different ones might be appropriate in different contexts。
  prefs: []
  type: TYPE_NORMAL
- en: Test and evaluate your systems based on these metrics。 Again。
  prefs: []
  type: TYPE_NORMAL
- en: there are now tools out there that will allow you to assess what the impacts
    are。
  prefs: []
  type: TYPE_NORMAL
- en: And then finally be aware of hidden historical biases。
  prefs: []
  type: TYPE_NORMAL
- en: And this is where having diverse teams is critically important。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_13.png)'
  prefs: []
  type: TYPE_IMG
- en: If you've got someone in the process who has experienced discrimination themselves，
    they're。
  prefs: []
  type: TYPE_NORMAL
- en: just much more likely to be aware and flag something when it comes up in the
    design of。
  prefs: []
  type: TYPE_NORMAL
- en: an AI system。 [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b13040f91efdc4ae10a034a302ebe_15.png)'
  prefs: []
  type: TYPE_IMG
