- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P82：19_更广泛的隐私和伦理问题.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So let's talk about some of the broader issues around using data science and
    artificial intelligence。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_1.png)'
  prefs: []
  type: TYPE_IMG
- en: generally in the workplace with managing people。 These are issues of ethics，
    also issues of privacy。
  prefs: []
  type: TYPE_NORMAL
- en: So let's talk about some of these and what they look like。
  prefs: []
  type: TYPE_NORMAL
- en: So let's take a simple one and go back to our hiring example before。
  prefs: []
  type: TYPE_NORMAL
- en: And this is a little story that I borrowed from our colleague， Jeff Poulter。
    And this is it。
  prefs: []
  type: TYPE_NORMAL
- en: So let's say you're making a hiring decision in your organization and this is
    somebody。
  prefs: []
  type: TYPE_NORMAL
- en: to work for you， your vice president， this person's going to be a director。
  prefs: []
  type: TYPE_NORMAL
- en: And you have an internal candidate， somebody who works in your unit and you
    like this person。
  prefs: []
  type: TYPE_NORMAL
- en: quite a bit。 You think they're pretty good。 Human resource people have said，
    you know。
  prefs: []
  type: TYPE_NORMAL
- en: there's another candidate for this position， who does the same job as your candidate。
  prefs: []
  type: TYPE_NORMAL
- en: but in another part of the organization。 So you don't know this person。
  prefs: []
  type: TYPE_NORMAL
- en: But their experience base is kind of the same。 So there's your person is the
    other person。
  prefs: []
  type: TYPE_NORMAL
- en: So you interview the other person and they interview well and you're aware that
    you might。
  prefs: []
  type: TYPE_NORMAL
- en: have a bias toward the person who works with you a little more now， your candidate。
  prefs: []
  type: TYPE_NORMAL
- en: You think the other person seems to be good， but you still like this， your first
    person。
  prefs: []
  type: TYPE_NORMAL
- en: And you recognize maybe this is just because you are familiar with it。
  prefs: []
  type: TYPE_NORMAL
- en: Then human resource calls you and they say， you know， we got this algorithm
    to predict。
  prefs: []
  type: TYPE_NORMAL
- en: who will succeed in this job。 And your candidate is an 82% fit with the algorithm
    that is the best performers。
  prefs: []
  type: TYPE_NORMAL
- en: And this other candidate we want you to look at is a 92%。 How are you going
    to decide？
  prefs: []
  type: TYPE_NORMAL
- en: You got your candidate who you know already and you like them a little better，
    but you。
  prefs: []
  type: TYPE_NORMAL
- en: know that's probably a bias。 The other candidate who looks roughly identical。
  prefs: []
  type: TYPE_NORMAL
- en: you've interviewed them， they seem fine as， well， but they have a higher score
    in the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: Which one you going to pick？ Well I've asked this a lot to lots of different
    people。
  prefs: []
  type: TYPE_NORMAL
- en: And one of the things that I've learned is almost everybody picks their own
    candidate。
  prefs: []
  type: TYPE_NORMAL
- en: So then I twist it a little bit and say what if it isn't 82 to 92。
  prefs: []
  type: TYPE_NORMAL
- en: Because it's 80 to 95 and almost everybody makes the same decision。
  prefs: []
  type: TYPE_NORMAL
- en: They go with the candidate that they already know。 So what's the punchline of
    this？
  prefs: []
  type: TYPE_NORMAL
- en: I ratcheted it up， you know， to like 100%。 You know。
  prefs: []
  type: TYPE_NORMAL
- en: this is like a perfect fit and people still want to go。
  prefs: []
  type: TYPE_NORMAL
- en: Almost everybody wants to go with the candidate they know the best。
  prefs: []
  type: TYPE_NORMAL
- en: And I think the lesson from this is it's a reflection of the fact that people
    want， to have a say。
  prefs: []
  type: TYPE_NORMAL
- en: And they feel that they ought to be able to pick the person that they want。
  prefs: []
  type: TYPE_NORMAL
- en: If on the other hand you're the CEO of the company and you're leading it， you
    might say。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_3.png)'
  prefs: []
  type: TYPE_IMG
- en: what we want is the best person for the job here and not somebody that you personally
    like。
  prefs: []
  type: TYPE_NORMAL
- en: Well this is an example of the kind of fight that we are going to have if we
    introduce data。
  prefs: []
  type: TYPE_NORMAL
- en: science。 We have situations where people have had a say before and maybe they
    feel that they should。
  prefs: []
  type: TYPE_NORMAL
- en: have a say and you're going to take that away from them。
  prefs: []
  type: TYPE_NORMAL
- en: When we look at issues like for example promotion or assignment to jobs or schedules，
    one of。
  prefs: []
  type: TYPE_NORMAL
- en: the things that this bites the most on are the decisions that supervisors have。
  prefs: []
  type: TYPE_NORMAL
- en: This is the point where supervisors otherwise have a great deal of control。
  prefs: []
  type: TYPE_NORMAL
- en: And one of the things we've learned before about management is that good relationships。
  prefs: []
  type: TYPE_NORMAL
- en: with supervisors are built around kind of a exchange。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_5.png)'
  prefs: []
  type: TYPE_IMG
- en: Supervisors says if you help me out on this thing， I'll see what I can do for
    you on the。
  prefs: []
  type: TYPE_NORMAL
- en: promotion or I'll see what I can do for you on wage increases， et cetera。
  prefs: []
  type: TYPE_NORMAL
- en: If you turn decisions over to the algorithms， they might optimize on some dimension
    but you。
  prefs: []
  type: TYPE_NORMAL
- en: start to erode that relationship between supervisors and subordinates and in
    that context maybe。
  prefs: []
  type: TYPE_NORMAL
- en: you're weakening the whole system， at least the way things have worked before。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_7.png)'
  prefs: []
  type: TYPE_IMG
- en: There's some other issues as well， let's talk about some of those。
  prefs: []
  type: TYPE_NORMAL
- en: One of them is that algorithms generate a score based on the entire sample that
    they're， looking at。
  prefs: []
  type: TYPE_NORMAL
- en: But if you broke the sample up a bit by some demographic attribute， you might
    find that。
  prefs: []
  type: TYPE_NORMAL
- en: the algorithm gives systematically different scores to different groups。 So
    for example。
  prefs: []
  type: TYPE_NORMAL
- en: algorithms that are used to make parole decisions， I didn't know this。
  prefs: []
  type: TYPE_NORMAL
- en: until about a year ago but a lot of locations in the US now， a lot of governments
    are using。
  prefs: []
  type: TYPE_NORMAL
- en: algorithms to try to decide who should get parole and who shouldn't。
  prefs: []
  type: TYPE_NORMAL
- en: So the measure there is recidivism。 Does the parolee make it successfully into
    normal society or do they stumble back。
  prefs: []
  type: TYPE_NORMAL
- en: violate， their parole and end up back in jail？ Which is something that you would
    think an algorithm would be really good at because。
  prefs: []
  type: TYPE_NORMAL
- en: the measure of outcome is pretty clear here。 When they looked at these measures
    though。
  prefs: []
  type: TYPE_NORMAL
- en: in these algorithms， one of the things they， discovered was the scores were
    systematically different for African Americans and for whites。
  prefs: []
  type: TYPE_NORMAL
- en: And so you might say， "All right， we know that， we'll deal with it。
  prefs: []
  type: TYPE_NORMAL
- en: We need a separate model for African Americans and a separate model for whites。"。
  prefs: []
  type: TYPE_NORMAL
- en: The problem is the law doesn't allow you to do that because you're treating
    people differently。
  prefs: []
  type: TYPE_NORMAL
- en: based on race。 And so at least at this point， the courts and the law have said，
    "Can't do that。"。
  prefs: []
  type: TYPE_NORMAL
- en: And this is a more general issue and that is that the law lags practices by
    quite a bit。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_9.png)'
  prefs: []
  type: TYPE_IMG
- en: and in this case， most of these issues about what algorithms might do that confront
    what。
  prefs: []
  type: TYPE_NORMAL
- en: the law says or conflict in various ways have not been adjudicated。 But right
    now。
  prefs: []
  type: TYPE_NORMAL
- en: and maybe at some point the law will say， "Yeah， it's fine to have different。
  prefs: []
  type: TYPE_NORMAL
- en: algorithms but at this point you can't do them。"， So there's a number of these
    issues where we bump up against how the algorithms work。
  prefs: []
  type: TYPE_NORMAL
- en: and things that we might believe are reasonably objectively more fair。
  prefs: []
  type: TYPE_NORMAL
- en: Now the thing that we hear maybe the most about are privacy issues because in
    order to use。
  prefs: []
  type: TYPE_NORMAL
- en: data science， we need data and we need lots of it。 So let's think about examples
    here。
  prefs: []
  type: TYPE_NORMAL
- en: One of the things in data science or a particular tool that's been quite popular
    is to measure。
  prefs: []
  type: TYPE_NORMAL
- en: what's known as flight risk or we used to call this just turnover。 When will
    people quit？
  prefs: []
  type: TYPE_NORMAL
- en: And you can estimate using data science the probability that a person will quit。
    By the way。
  prefs: []
  type: TYPE_NORMAL
- en: this is not new back in the 1960s。 There was software that used to do roughly
    the same thing inside companies。
  prefs: []
  type: TYPE_NORMAL
- en: But you can do it better now。 And the best of these flight risk models use social
    media。
  prefs: []
  type: TYPE_NORMAL
- en: What does that mean？ Well， if you've updated your LinkedIn profile， for example。
  prefs: []
  type: TYPE_NORMAL
- en: that is a proxy for people who， are starting to look for a new job。 What you
    post on social media。
  prefs: []
  type: TYPE_NORMAL
- en: on Facebook or other places， if you start posting information。
  prefs: []
  type: TYPE_NORMAL
- en: that suggests you're starting to look， flight risk goes up。 Looking at your
    email traffic。
  prefs: []
  type: TYPE_NORMAL
- en: for example， and seeing where it's going， your sentiment， analysis。
  prefs: []
  type: TYPE_NORMAL
- en: if we start listening to you or reading your emails and see negative things，
    about the company。
  prefs: []
  type: TYPE_NORMAL
- en: we could probably get a pretty good measure of your flight risk， building。
  prefs: []
  type: TYPE_NORMAL
- en: a machine learning model from that。 The problem is a lot of people find that
    creepy。 To do that。
  prefs: []
  type: TYPE_NORMAL
- en: we're listening to your email， maybe listening to your phone calls， we're。
  prefs: []
  type: TYPE_NORMAL
- en: reading your social media， all that stuff。 Well， maybe it is。 If you look， for
    example。
  prefs: []
  type: TYPE_NORMAL
- en: at reading email， this seems to be something that companies， are quite sensitive
    about。
  prefs: []
  type: TYPE_NORMAL
- en: Although about， I think some of the data I've seen say about 40% of employers
    admit that。
  prefs: []
  type: TYPE_NORMAL
- en: they're reading it。 In fact， virtually all of them are monitoring email。
  prefs: []
  type: TYPE_NORMAL
- en: They're looking for keywords that started years ago when they were looking at
    offensive。
  prefs: []
  type: TYPE_NORMAL
- en: email and keeps changing what the definition of offensive is。 So they're scanning
    it。
  prefs: []
  type: TYPE_NORMAL
- en: literally reading it。 Well， probably not。 Many doing that。
  prefs: []
  type: TYPE_NORMAL
- en: But if 40% say that they are monitoring the email traffic， when you ask the
    IT people， in companies。
  prefs: []
  type: TYPE_NORMAL
- en: two-thirds of them say that their companies are reading email。
  prefs: []
  type: TYPE_NORMAL
- en: So this is a bit of a quibble as to what counts as reading。
  prefs: []
  type: TYPE_NORMAL
- en: One of the complications as well is once you start using something for employment
    purposes。
  prefs: []
  type: TYPE_NORMAL
- en: like social media， I'm quite likely to find that out。
  prefs: []
  type: TYPE_NORMAL
- en: So one of the things that the internet has done is that it's made employee screening
    reasonably。
  prefs: []
  type: TYPE_NORMAL
- en: transparent in the sense that as soon as I finish my interviews with your company，
    I。
  prefs: []
  type: TYPE_NORMAL
- en: go online and I tell people what the questions were， all that sort of stuff。
  prefs: []
  type: TYPE_NORMAL
- en: So people are going to learn this。 And once I realize that you're looking at
    my Facebook pages and some employers ask to。
  prefs: []
  type: TYPE_NORMAL
- en: access， ask you to friend them so they can see what's on your Facebook pages。
  prefs: []
  type: TYPE_NORMAL
- en: Once I know that's coming， I just change what's on my Facebook page。
  prefs: []
  type: TYPE_NORMAL
- en: Down go my spring break pictures， up go the pictures of me tutoring kids。
  prefs: []
  type: TYPE_NORMAL
- en: And then it's no longer very useful， or at least it's not useful in quite the
    same way。
  prefs: []
  type: TYPE_NORMAL
- en: So there's kind of a race that's going on with respect to authentic information
    as。
  prefs: []
  type: TYPE_NORMAL
- en: employers and vendors continue to try to find revealing information about you
    that you're。
  prefs: []
  type: TYPE_NORMAL
- en: not kind of gaming。 But as soon as I learn that that's what you're trying to
    do。
  prefs: []
  type: TYPE_NORMAL
- en: I start to try to game it as， well。 So all this relates to these questions of
    privacy。
  prefs: []
  type: TYPE_NORMAL
- en: We've gotten a boost these issues during this period here， during the pandemic，
    where lots。
  prefs: []
  type: TYPE_NORMAL
- en: of companies have people working from home。 And some of those companies have
    made big investments in what's known as Tattleware or。
  prefs: []
  type: TYPE_NORMAL
- en: Spyware， which basically is kind of monitoring you while you're working to make
    sure that。
  prefs: []
  type: TYPE_NORMAL
- en: you're actually working。 They're probably counting your keystrokes and things
    like that。
  prefs: []
  type: TYPE_NORMAL
- en: They can see where you're going。 Really they're seeing whether you're goofing
    off on your computer or not。
  prefs: []
  type: TYPE_NORMAL
- en: Well， other data says that about a third of employees cover their cameras on
    their computers。
  prefs: []
  type: TYPE_NORMAL
- en: so that the employer cannot see if they're actually sitting at their desk。
  prefs: []
  type: TYPE_NORMAL
- en: Other data suggests that employees shift to using their cell phones to talk
    to their， co-workers。
  prefs: []
  type: TYPE_NORMAL
- en: even in the same building so the employer can't monitor their conversations。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_11.png)'
  prefs: []
  type: TYPE_IMG
- en: So it's this kind of little game that's going on as each side tries to get privacy
    on the。
  prefs: []
  type: TYPE_NORMAL
- en: part of the employee ease trying to monitor based on the employer's。 This privacy
    issue， of course。
  prefs: []
  type: TYPE_NORMAL
- en: has led to regulations the best known of these are the。
  prefs: []
  type: TYPE_NORMAL
- en: general data protection regulations in the European Union。
  prefs: []
  type: TYPE_NORMAL
- en: They have a series of rights that employees have。 Employers confront these especially
    with trying to move data about employees across jurisdictions。
  prefs: []
  type: TYPE_NORMAL
- en: So if you have employees around the world， some of them in the EU and you want
    to take。
  prefs: []
  type: TYPE_NORMAL
- en: the data from them to build a general model about your employees and things
    like turnover。
  prefs: []
  type: TYPE_NORMAL
- en: and things like that， you bump into those regulations。
  prefs: []
  type: TYPE_NORMAL
- en: The one that is maybe the most popular in terms of discussion is one called
    the right。
  prefs: []
  type: TYPE_NORMAL
- en: to be forgotten。 And the right to be forgotten basically says that employers
    should not keep old data around。
  prefs: []
  type: TYPE_NORMAL
- en: particularly about customers， but also about employees。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_13.png)'
  prefs: []
  type: TYPE_IMG
- en: And if you're an employer， you need lots of data to build these machine learning
    models。
  prefs: []
  type: TYPE_NORMAL
- en: And if you have to start shedding it at some point， it reduces the size of your
    data。
  prefs: []
  type: TYPE_NORMAL
- en: California now has its own version of basically the European Union's data protection
    rights。
  prefs: []
  type: TYPE_NORMAL
- en: And it won't be long until we start seeing state by state。 In the United States。
  prefs: []
  type: TYPE_NORMAL
- en: these regulations popping up。 Illinois has already got some。
  prefs: []
  type: TYPE_NORMAL
- en: So the privacy issues matter for ethics issues， but they also matter now for
    legal issues。
  prefs: []
  type: TYPE_NORMAL
- en: And this is maybe the most stark situation where data science confronts the
    complexity。
  prefs: []
  type: TYPE_NORMAL
- en: of the workforce， where these questions of fairness are embedded in laws and
    regulations。
  prefs: []
  type: TYPE_NORMAL
- en: '[ Silence ]。'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a763d2754aecfe7f3eb187e904357d4_15.png)'
  prefs: []
  type: TYPE_IMG
