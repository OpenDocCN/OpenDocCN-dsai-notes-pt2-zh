# 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P127：26_Yogesh Mudgal访谈.zh_en - GPT中英字幕课程资源 - BV1Ju4y157dK

 Today， I'm joined by Yogish Mughal。

![](img/40126034711e8d87ea2b25411d8075f2_1.png)

 Yogish currently works for Citi， where he is the Director in Operational Risk Management。 In 2019。

 Yogish formed an informal group， of like-minded professionals called ERS， AI Risk and Security。

 ERS seeks to promote， educate and advance AI， machine learning governance for the financial services industry。

 by focusing on risk， identification， categorization， and mitigation。 Welcome， Yogish。 Thank you。

 Mary。 Thank you for having me。 So I'm so happy to have you here。

 So to ground us in your perspective， can you tell us about your career journey and your role at Citi？

 So starting with Citi， I had the emerging tech， engineering。

 and architecture group in Operation Risk at Citi。 Most of my career has been devoted to risk management。

 and information security。 For the last couple of years， I have。

 been more focused on identifying and managing risks。

 with emerging technologies in general and AI in particular。 To that end。

 like you were saying earlier， I founded ERS。 In 2019， of like-minded professional。

 which stands for AI Risk and Security， essentially。

 trying to see if in the industry we can come together， and have a common view on identifying risks。

 with AI and how to ensure that it's being implemented， safely and securely。

 AI can be viewed as a risk to firms。 And so then they can be observed as reputational risk。

 operational risk， or regulatory risk。 So these different terms of risk。 And so I was wondering。

 could you describe these risks， in particular， like which ones are most concerning。

 from your experience？ So in my opinion， risk should be managed for any technologies。

 or a set of technologies， including AI。 So the risk related to AI for an institution。

 can be dependent on various variables， including， how AI is implemented。

 strength of existing controls， or a risk profile of the institutions and the risk appetite。

 As described in the paper， which probably we didn't mention yet。

 but ERS did publish a paper through Wartan， and in that paper， we also described。

 various risk categories for AI。 And some of the risk categories that I will highlight here。

 are the data related risks， AI attacks， testing and trust， and compliance。 So in data related risk。

 it could include subcategories， like learning limitations。 For example， the learning limitations。

 and of course the data quality， to summarize， the AI system is generally as effective as the data。

 used to train it and the various scenarios considered， while training the system。 In AI ML attacks。

 there's been research done in the industry， on live ML models that talked about various attack。

 including data privacy attack， training data poisoning， adversarial inputs， and model extraction。

 And then there's testing and trust。 Obviously， trust and testing is the most common。

 talked about topic in the AI field， which includes bias and explainability。

 and things of that nature and compliance。 And compliance including internal policies。

 as well as regulatory requirements。 I think all of these are the various risks。

 that we can summarize or categorize into for AI。 And within the group at ERS， is there。

 anyone particular parts of one of those three risk areas？

 Is anyone more at a higher level or does it， depend on the particular industry or the particular organization。

 in terms of their willingness to control the risk？ Like， I was just curious how it plays in。

 Is it the peoples or the company or is the industry， that prioritizes these risks， like very。

 very volatile？ Yes， so you're absolutely right。 It really depends on where AI is implemented。

 how it is implemented， and the risk appetite of the institution。 So we cannot say one size fits or。

 or in this case， one risk fits， all or the top risks。 So it really depends on the implementation。

 Let's say， for example， if somebody， has an AI model sitting out on a cloud。

 so the risk on the AI ML attacks probably， become more heightened， depending on how exposed。

 that AI model is， versus an AI model， which is internal， being used internally in the organization。

 Probably those attacks probably are less likelihood。

 but there could be compliance risk that is more prevalent。 So it really depends on use case。

 and there's， no one risk that I would say is higher or lower in that case。

 So that leads into just peeling back the onion， a little bit more on this。

 because we're hearing more， in what we were talking about data and the training。

 the data that's used， but we also hear about the algorithm。 So more and more。

 we are hearing about algorithm， transparency within the media and at conferences。

 similar to what you had mentioned earlier。 So what is this in terms of what's the specifics around what。

 we hear sometimes it's called technical， which， would be like revealing source code versus a calibrated。

 transparency。 So if you could just maybe give a little bit of context。

 to both those terms that we're hearing a bit more。

 and just overall how algorithms have to be more transparent。 What is a leader， how you address that。

 and then potentially as consumers or investors， how they're looking at it。 So first of all。

 I agree that a lot of terms floating， and transparency is one of them for sure。

 So there are increased conversations and discussions， regarding transparency and explainability。

 I think it's important to consider who is the audience， as depending on the stakeholder。

 the level of transparency， or the need for transparency may differ。 For example。

 transparency might be， required for an internal auditor versus a regulator。

 versus a developer or the end user。 And the analogy I always give is I travel and I fly in a plane。

 I bet my life on it， but I have no idea how the engine is working。

 I don't know how good or bad the pilot is。 But with the important factor here is trust。

 I trust that the system is working。 I trust that the plane is running well。

 and I trust that I'll be taking to the destination， that I'm going to last but not least。 But here。

 I think the common thing here is the trust。 And because of。

 I lose the cliche term on the hype around AI， and there are some relevant， I think， fear。

 or I should say， or harm related to AI that is not being fully understood。

 So I think it's important that we develop the trust， and build a trust in the AI systems。

 And so think it's important factor here for transparency， or the calibrated transparency。

 in my opinion， is a trust factor。 If the stakeholder on a need to know basis， understands the why。

 hows， and the trade-offs of using the AI， system， then， for example。

 an internal auditor versus a regulator， or the end user， the transparency could be granted。

 and hence build a trust in that AI system。 So we've talked about the data， the transparency。

 of the algorithms， and that also might lead us to potentially。

 if people request more and more to look at the source code， or revealing the source code。

 And is there problems with revealing the source code？ If we could talk a little bit about that。

 because that might be something else that people are wondering。

 how they might manage if that's coming down the pike。 Yeah， so I think revealing the source code。

 or in extreme cases compromise of a source code， whether it's an AI system or a non-AI system。

 I think most of the risk remain the same。 Obviously。

 the intellectual property is on the top of the list。 And there are other things involved， including。

 if you reveal the source code， are you revealing the methodology。

 used to build a system or arrive at a decision？ And depending on the stakeholder。

 do you want to reveal that or not？ Can the source code be used to implant or embed backdoor。

 into the system？ So there are various risks involved。

 when we are talking about revealing the source code。 And in some cases， it might have to be done。

 due to where if a law or the legal situation demands that。 But generally。

 I don't think so source code is revealed， that easily or in generally speaking。

 it should not be revealed if our companies don't do that。 And potentially。

 it might be might be something in which companies， need to better educate potentially。

 like the consumers or their clients or their customers， why revealing the source code is bad。

 is not in the best interest for them， in that it reveals， it exposes them to more risk。

 some of the risks that you talked about earlier。 And then your analogy about the flying on the plane。

 You have purchased that ticket to be on the plane。

 because you know that there's experts running the plane。

 and there's a certain amount of assumption and trust。

 that has transpired between you and the airline， that you are expecting them to keep certain things。

 that you know they have to keep running， and you don't need to know exactly how it's being run。

 But when there's a very important risk， they are going to let you know。 Well。

 thank you so very much for all that information， and for talking to me today。 I appreciate it。

 Thank you， Yogi。 Thank you， Mary。 Thanks for having me。 [ Silence ]。



![](img/40126034711e8d87ea2b25411d8075f2_3.png)