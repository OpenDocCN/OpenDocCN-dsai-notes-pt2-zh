- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P134：33_解释性和法律.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transparency is one of the central concepts in responsible AI。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_1.png)'
  prefs: []
  type: TYPE_IMG
- en: There are a number of existing and proposed legal frameworks that would mandate
    some form of explainability。
  prefs: []
  type: TYPE_NORMAL
- en: at least in certain circumstances。 If the AI system is a black box。
  prefs: []
  type: TYPE_NORMAL
- en: there's no way to evaluate whether， for example， it's basing decisions on illegitimate
    factors。
  prefs: []
  type: TYPE_NORMAL
- en: such as someone's race or sexual orientation。 And more generally。
  prefs: []
  type: TYPE_NORMAL
- en: it's hard to assess what went wrong， or even if something did go wrong。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_3.png)'
  prefs: []
  type: TYPE_IMG
- en: The most prominent existing model for legal explainability of AI is credit reporting。
  prefs: []
  type: TYPE_NORMAL
- en: Credit bureaus， which developed in the 1960s， were one of the first major wide-scale
    applications of data analytics in the economy。
  prefs: []
  type: TYPE_NORMAL
- en: They rapidly became essential to consumer financial markets in countries like
    the United States。
  prefs: []
  type: TYPE_NORMAL
- en: as well as other areas such as hiring that used the credit reporting data to
    get other indications about candidates。
  prefs: []
  type: TYPE_NORMAL
- en: But it was clear that the power of credit reports meant an inaccurate report。
  prefs: []
  type: TYPE_NORMAL
- en: or one that used it in a discriminatory way， could be devastating。 Without regulation。
  prefs: []
  type: TYPE_NORMAL
- en: there was no way for consumers to evaluate how their credit score fed into decisions。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_5.png)'
  prefs: []
  type: TYPE_IMG
- en: When the two laws from the 1970s in the U。S。 the Equal Credit Opportunity Act，
    E。 Cola。
  prefs: []
  type: TYPE_NORMAL
- en: and the Fair Credit Reporting Act， Ficro， imposed standards for the legal explainability
    of credit reports。
  prefs: []
  type: TYPE_NORMAL
- en: Specifically， that means businesses had to give an adverse action notice whenever
    they denied someone credit。
  prefs: []
  type: TYPE_NORMAL
- en: And that meant giving the "principle reasons" for the decision。
  prefs: []
  type: TYPE_NORMAL
- en: They didn't have to list every reason， nor did they need to give an exact formulation
    in the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: for how each factor contributed to the outcome。 But they did have to give customers
    some indication of what was driving the adverse decision。
  prefs: []
  type: TYPE_NORMAL
- en: And that gave those consumers information they could use if they wanted to in
    order to contest the decision。
  prefs: []
  type: TYPE_NORMAL
- en: where they thought it was based on some error or based on some inappropriate
    information。
  prefs: []
  type: TYPE_NORMAL
- en: That could happen without an unnecessary and unreasonable burden on the companies。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_7.png)'
  prefs: []
  type: TYPE_IMG
- en: In some cases， beyond this， governments mandate specific formulas for disclosure。
    For example。
  prefs: []
  type: TYPE_NORMAL
- en: credit card offers in the U。S。 must include a so-called "shoomer box" for the
    senator who sponsored the legislation that requires it。
  prefs: []
  type: TYPE_NORMAL
- en: which states interest rates and other terms in a standard， regulated， easy to
    understand way。
  prefs: []
  type: TYPE_NORMAL
- en: While there isn't anything analogous yet for AI systems。
  prefs: []
  type: TYPE_NORMAL
- en: technology firms are experimenting with similar kinds of disclosures。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Because again， it gives users and in some cases other developers an easier opportunity
    to understand what is going on。
  prefs: []
  type: TYPE_NORMAL
- en: Google， for example， has introduced something called Model Cards。
  prefs: []
  type: TYPE_NORMAL
- en: a standardized way of describing the models for machine learning systems。
  prefs: []
  type: TYPE_NORMAL
- en: and Microsoft has something similar called "data sheets for data sets。
  prefs: []
  type: TYPE_NORMAL
- en: '" which is again a standardized way of disclosing information about the data
    set。'
  prefs: []
  type: TYPE_NORMAL
- en: These help identify the source data and the techniques that are involved in
    building the model。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_11.png)'
  prefs: []
  type: TYPE_IMG
- en: While it wouldn't necessarily make sense to have something similar for every
    proprietary internal AI system。
  prefs: []
  type: TYPE_NORMAL
- en: some kind of standardized reporting， at least some opportunity to have that
    disclosed to regulators。
  prefs: []
  type: TYPE_NORMAL
- en: even if not directly to consumers， seems likely in the future for major AI systems。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Again， if regulators don't know what happened， they can't decide whether something
    went wrong。
  prefs: []
  type: TYPE_NORMAL
- en: The European GDPR， General Data Protection Regulation， although it's predominantly
    a privacy law。
  prefs: []
  type: TYPE_NORMAL
- en: includes what's usually described as a right to explanation in limited cases。
  prefs: []
  type: TYPE_NORMAL
- en: So if there is fully automated processing， in other words。
  prefs: []
  type: TYPE_NORMAL
- en: the machine learning or other algorithmic system is the entire decision about
    what happens to a person。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_15.png)'
  prefs: []
  type: TYPE_IMG
- en: And it's a serious consequence。 It results in someone getting or not getting
    a loan， a job。
  prefs: []
  type: TYPE_NORMAL
- en: or some legal consequence。 Will this person be released from prison early？ If
    all of those are true。
  prefs: []
  type: TYPE_NORMAL
- en: then there is some indication in GDPR that the implementer of the system needs
    to provide some information that explains the factors that went into the decision。
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately， exactly what this means in practice is not clear。
  prefs: []
  type: TYPE_NORMAL
- en: The language in GDPR is somewhat general， it's subject to legal interpretation。
  prefs: []
  type: TYPE_NORMAL
- en: and we don't yet have significant case law about what this means in practice。
  prefs: []
  type: TYPE_NORMAL
- en: But it does suggest in existing European law， which again applies to any data
    collected about Europeans anywhere。
  prefs: []
  type: TYPE_NORMAL
- en: There is some basis for a requirement of explanation。
  prefs: []
  type: TYPE_NORMAL
- en: and that's likely to be expanded in the future。 In the US。
  prefs: []
  type: TYPE_NORMAL
- en: at least one federal appeals court case involving teachers in Houston who were
    fired because of a black box assessment algorithm based on their students test
    scores。
  prefs: []
  type: TYPE_NORMAL
- en: found that the absence of explanation was a violation of the constitutional
    due process that the teachers enjoyed。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_17.png)'
  prefs: []
  type: TYPE_IMG
- en: The teachers had no way to challenge their dismissal because they just knew
    it was based on an algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: They didn't know what the algorithm did with the test scores。
  prefs: []
  type: TYPE_NORMAL
- en: That eliminated the ability they had before when dismissal decisions were not
    based on the algorithm。
  prefs: []
  type: TYPE_NORMAL
- en: and so the court said their due process rights were violated。
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm had to be disclosed or its use had to be eliminated。
  prefs: []
  type: TYPE_NORMAL
- en: which was ultimately the result in the settlement of the case。 Now that's not
    a law in the US。
  prefs: []
  type: TYPE_NORMAL
- en: but it's an indication that there is more movement towards explanation requirements
    under different legal theories。
  prefs: []
  type: TYPE_NORMAL
- en: Similarly， in any case where legal liability needs to be assigned。
  prefs: []
  type: TYPE_NORMAL
- en: such as for example accidents involving autonomous vehicles。
  prefs: []
  type: TYPE_NORMAL
- en: investigators need to and typically are able to get access to data from the
    computer vision systems involved to understand exactly what happened。
  prefs: []
  type: TYPE_NORMAL
- en: How did the system behave and why？ What did it think it was seeing or not seeing
    on the road？
  prefs: []
  type: TYPE_NORMAL
- en: So there is a mechanism that is required for explanation when there are requirements
    after the fact for accident investigation。
  prefs: []
  type: TYPE_NORMAL
- en: The Algorithmic Accountability Act， which I mentioned earlier， a proposed law
    in the US。
  prefs: []
  type: TYPE_NORMAL
- en: as well as the white paper in the European Union suggesting new AI legislation
    both propose a requirement for formal impact statements before high risk AI systems
    are deployed。
  prefs: []
  type: TYPE_NORMAL
- en: High risk means systems with a significant possibility of causing illegal discrimination。
  prefs: []
  type: TYPE_NORMAL
- en: injury or major financial consequences if something goes wrong。 In those cases。
  prefs: []
  type: TYPE_NORMAL
- en: an algorithmic impact statement would force companies or government agencies
    to explicitly identify how the systems are working。
  prefs: []
  type: TYPE_NORMAL
- en: As with the early credit reporting laws， there is a long way to go in figuring
    out exactly how those are implemented and an appropriate balance between sufficient
    explanation and sufficient flexibility for companies with a recognition that technically
    it's difficult to explain exactly what's going on in。
  prefs: []
  type: TYPE_NORMAL
- en: for example， a deep learning system。 But again， the law is moving more in this
    direction。
  prefs: []
  type: TYPE_NORMAL
- en: So when you have the opportunity internally to get more of an understanding
    of the explanation of your system's conduct。
  prefs: []
  type: TYPE_NORMAL
- en: you should try to do so。 Now， some proposals for these algorithmic impact statements
    require disclosure to the public。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_19.png)'
  prefs: []
  type: TYPE_IMG
- en: But even if they don't， companies are probably going to be required to show
    regulators they took required steps and assessed or addressed possible harm before
    they happen when they turned up in the impact assessment。
  prefs: []
  type: TYPE_NORMAL
- en: So again， it's worth getting out ahead of this if you can and thinking about
    what mechanisms you have for explainability。
  prefs: []
  type: TYPE_NORMAL
- en: These potential legal requirements will also drive researchers and vendors to
    develop better techniques and tools for explainable AI。
  prefs: []
  type: TYPE_NORMAL
- en: There are many solutions today， but there will be more and better solutions
    in the future。
  prefs: []
  type: TYPE_NORMAL
- en: Some of the power of AI is its ability to find connections that humans don't。
    However。
  prefs: []
  type: TYPE_NORMAL
- en: being able to understand better how AI systems make decisions， what happened
    will benefit everyone。
  prefs: []
  type: TYPE_NORMAL
- en: It has been a pleasure talking with you and sharing these insights about AI，
    the law and ethics。
  prefs: []
  type: TYPE_NORMAL
- en: Good luck on the rest of this program and in your implementation of these kinds
    of techniques in your organization。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_21.png)'
  prefs: []
  type: TYPE_IMG
- en: Thank you。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c3626b2b5189884585efd384da63ab_23.png)'
  prefs: []
  type: TYPE_IMG
