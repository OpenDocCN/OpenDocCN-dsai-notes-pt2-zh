- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P87：24_偏见从何而来.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use of AI in HR systems brings within a number of problems， a number of
    challenges。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_1.png)'
  prefs: []
  type: TYPE_IMG
- en: but they're also a number of emerging solutions to deal with some of these challenges。
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest problems is the issue of bias， and this is the one you've
    likely， heard of。
  prefs: []
  type: TYPE_NORMAL
- en: It's very popular with the media。 It's the idea that when you apply machine
    learning algorithms to data。
  prefs: []
  type: TYPE_NORMAL
- en: they may produce， predictions or recommendations that are unequal， that are
    unfair to some groups。
  prefs: []
  type: TYPE_NORMAL
- en: We talked earlier about the idea that machine learning algorithms， what they
    basically。
  prefs: []
  type: TYPE_NORMAL
- en: do is learn the mapping from the examples you give it。
  prefs: []
  type: TYPE_NORMAL
- en: But what that implies is that if the examples you have provided contain some
    inherent bias， if they。
  prefs: []
  type: TYPE_NORMAL
- en: for example， contain decisions by humans that were themselves biased， the machine。
  prefs: []
  type: TYPE_NORMAL
- en: can then learn to mimic that kind of bias。 An important focus these days in
    machine learning is learning how to identify that bias and。
  prefs: []
  type: TYPE_NORMAL
- en: to extend possible eliminate this type of bias。 But given the way machine learning
    algorithms work。
  prefs: []
  type: TYPE_NORMAL
- en: if prior decisions encode historical， bias， algorithms will necessarily learn
    to be biased as well。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_3.png)'
  prefs: []
  type: TYPE_IMG
- en: So for example， if we're training a model or building a model that recommends
    promotion。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_5.png)'
  prefs: []
  type: TYPE_IMG
- en: and that model is basically built using data on prior success， prior examples
    of people。
  prefs: []
  type: TYPE_NORMAL
- en: within the firm who were promoted to higher positions， and this historical data
    reflects。
  prefs: []
  type: TYPE_NORMAL
- en: historical bias at an employer， and the model we're using， the model we're building
    can。
  prefs: []
  type: TYPE_NORMAL
- en: learn to pick up that bias as well， which is something that the industry that
    we want。
  prefs: []
  type: TYPE_NORMAL
- en: to learn how to identify and eliminate in these machine learning systems。
  prefs: []
  type: TYPE_NORMAL
- en: So that's one way that bias can enter these systems is you're using training
    data， using。
  prefs: []
  type: TYPE_NORMAL
- en: historical examples that contain， that themselves contain bias。
  prefs: []
  type: TYPE_NORMAL
- en: It's not the only way bias can arise。 There's also something called data adequacy
    bias。
  prefs: []
  type: TYPE_NORMAL
- en: So think about the example of using video data or audio data from an interview
    to make。
  prefs: []
  type: TYPE_NORMAL
- en: some predictions about candidate fit。 Well， it turns out that many systems learn
    to work better the more data they are fed。
  prefs: []
  type: TYPE_NORMAL
- en: and it turns out that a lot of the data sets that we feed these systems， we
    just happen。
  prefs: []
  type: TYPE_NORMAL
- en: to have a lot more data on some groups than others。
  prefs: []
  type: TYPE_NORMAL
- en: So if we have differences across demographic groups， across gender， across race，
    some race。
  prefs: []
  type: TYPE_NORMAL
- en: and gender groups say are not well represented in the data set， we may just
    do a poorer job。
  prefs: []
  type: TYPE_NORMAL
- en: in terms of accurately predicting outcomes for that group， and that can disadvantage
    that。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_7.png)'
  prefs: []
  type: TYPE_IMG
- en: group， which leads to another kind of bias that's not based on historical decision
    making。
  prefs: []
  type: TYPE_NORMAL
- en: but just based on the quantity and quality of data we have。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Now this kind of bias can emerge all the time， even inadvertently。
  prefs: []
  type: TYPE_NORMAL
- en: One recent and fairly interesting example is in advertising STEM jobs。
  prefs: []
  type: TYPE_NORMAL
- en: So science and mathematics jobs。 There has been a recent attention on how these
    jobs of course are the exposure of these。
  prefs: []
  type: TYPE_NORMAL
- en: jobs to men and to women。 It turns out that if employers use engines that are
    commonly used to share information。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_11.png)'
  prefs: []
  type: TYPE_IMG
- en: with people at scale， in other words advertising engines， so think about Facebook
    engines。
  prefs: []
  type: TYPE_NORMAL
- en: Google engines， these are engines that are optimized to make information available
    to。
  prefs: []
  type: TYPE_NORMAL
- en: people who need to see it。 So employers have had the idea that it may be useful。
  prefs: []
  type: TYPE_NORMAL
- en: it may be valuable to be able to， put job opening information into these engines
    so that more people who might be a good fit。
  prefs: []
  type: TYPE_NORMAL
- en: for the job can see it。 This is a very reasonable idea。
  prefs: []
  type: TYPE_NORMAL
- en: Well it turns out these engines are optimized in a way that their where information
    is routed。
  prefs: []
  type: TYPE_NORMAL
- en: is optimized in a way that this can actually inadvertently route these job openings
    disproportionately。
  prefs: []
  type: TYPE_NORMAL
- en: to certain groups。 So in this case the employer has no bad intentions。
  prefs: []
  type: TYPE_NORMAL
- en: The company that builds the engine has no bad intentions but because of the
    way the algorithm， runs。
  prefs: []
  type: TYPE_NORMAL
- en: it shows different information to men and women and because this is an HR or
    a job， context。
  prefs: []
  type: TYPE_NORMAL
- en: it starts to produce outcomes that disadvantage some groups in terms of the
    labor， market。
  prefs: []
  type: TYPE_NORMAL
- en: So this is a pervasive problem that can arise even when people don't have an
    explicit intention。
  prefs: []
  type: TYPE_NORMAL
- en: to impose bias on their decisions and in the next video we'll start to talk
    about why。
  prefs: []
  type: TYPE_NORMAL
- en: this is a difficult problem to manage。 Thank you。 [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d593916359e063caa93199ca5ee712db_13.png)'
  prefs: []
  type: TYPE_IMG
