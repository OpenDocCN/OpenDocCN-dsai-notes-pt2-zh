- en: 沃顿商学院《AI For Business（AI用于商业：AI基础／市场营销+财务／人力／管理）》（中英字幕） - P19：18_常见损失函数.zh_en
    - GPT中英字幕课程资源 - BV1Ju4y157dK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's talk about some of the more common ways to measure error in machine learning
    output。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee32926febee5fe618fb5c24bb11950c_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Perhaps the simplest one to understand is accuracy。
  prefs: []
  type: TYPE_NORMAL
- en: So let's go back to our example where we're thinking about fraudulent and legitimate
    transactions。
  prefs: []
  type: TYPE_NORMAL
- en: So in one column we might have the actual true values of whether a transaction
    was legitimate。
  prefs: []
  type: TYPE_NORMAL
- en: or fraudulent。 In another column we have the predicted value。
  prefs: []
  type: TYPE_NORMAL
- en: our machine learning output of whether a transaction， was fraudulent or legitimate。
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy simply indicates the fraction of times that we get the answer right。
  prefs: []
  type: TYPE_NORMAL
- en: So it's basically comparing across the two columns and it's the fraction of
    times that。
  prefs: []
  type: TYPE_NORMAL
- en: the one column matches the other column。 So that's accuracy。
  prefs: []
  type: TYPE_NORMAL
- en: Classification error is the inverse of that。 So it's basically the fraction
    of times that the two columns don't match。
  prefs: []
  type: TYPE_NORMAL
- en: So that's accuracy。 Another common metric is precision。
  prefs: []
  type: TYPE_NORMAL
- en: Precision is asking what proportion of positive identifications were actually
    correct。
  prefs: []
  type: TYPE_NORMAL
- en: So what's a positive identification？ So with machine learning output with a
    binary classifier is trying to predict whether something。
  prefs: []
  type: TYPE_NORMAL
- en: is in one of two classes like legitimate or fraudulent。
  prefs: []
  type: TYPE_NORMAL
- en: One of these two classes is indicated as positive。
  prefs: []
  type: TYPE_NORMAL
- en: One is going to be a one so to speak and one is zero。
  prefs: []
  type: TYPE_NORMAL
- en: So we might identify if fraudulent is the positive class in this case。
  prefs: []
  type: TYPE_NORMAL
- en: What precision indicates is what proportion of the ones that we actually call
    fraudulent。
  prefs: []
  type: TYPE_NORMAL
- en: are actually fraudulent。 So if we have a number of predictions that are fraudulent
    we're going to look at how。
  prefs: []
  type: TYPE_NORMAL
- en: many of those， what fraction of those we got correct。
  prefs: []
  type: TYPE_NORMAL
- en: And this in a sense ignores some other aspects of the data or the performance。
  prefs: []
  type: TYPE_NORMAL
- en: It's basically looking at this particular piece， this particular measure of
    how often。
  prefs: []
  type: TYPE_NORMAL
- en: what we called fraudulent was actually a fraudulent case。
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity is a different way of looking at the performance of machine learning
    output。
  prefs: []
  type: TYPE_NORMAL
- en: So in this case this is looking at how many relevant instances did you catch？
  prefs: []
  type: TYPE_NORMAL
- en: So think about these two different columns here， the actual values and the predicted
    value。
  prefs: []
  type: TYPE_NORMAL
- en: So if the actual values are legitimate， fraudulent， what sensitivity is asking
    is how many of the。
  prefs: []
  type: TYPE_NORMAL
- en: fraudulent cases did you catch？ Did you let any of them slip through your net
    or were you able to get most of them？
  prefs: []
  type: TYPE_NORMAL
- en: So sensitivity， a highly sensitive classifier is basically one that lets very
    few of the。
  prefs: []
  type: TYPE_NORMAL
- en: relevant instances， the actually fraudulent transactions slipped through its
    net。
  prefs: []
  type: TYPE_NORMAL
- en: So it's looking at something quite different from precision。
  prefs: []
  type: TYPE_NORMAL
- en: Specificity is looking at the proportion of legitimate transactions in this
    case， the。
  prefs: []
  type: TYPE_NORMAL
- en: negative case that were correctly identified as such。
  prefs: []
  type: TYPE_NORMAL
- en: So of those transactions that were legitimate that were not fraudulent， how
    many of those。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee32926febee5fe618fb5c24bb11950c_3.png)'
  prefs: []
  type: TYPE_IMG
- en: were correctly identified。 So again， specificity， precision， accuracy。
  prefs: []
  type: TYPE_NORMAL
- en: they're all looking at different aspects of， what you're getting right and wrong
    in terms of your prediction。
  prefs: []
  type: TYPE_NORMAL
- en: When you're talking about using one of these versus the other to think about
    evaluating。
  prefs: []
  type: TYPE_NORMAL
- en: machine learning output， which we're implicitly doing is putting different weights
    on what。
  prefs: []
  type: TYPE_NORMAL
- en: you care about， whether it's more important to make sure， for instance， that
    you never。
  prefs: []
  type: TYPE_NORMAL
- en: miss a fraudulent transaction or whether it's more important， for instance，
    to make sure。
  prefs: []
  type: TYPE_NORMAL
- en: that you never accidentally call a legitimate transaction fraudulent。
  prefs: []
  type: TYPE_NORMAL
- en: These are the different trade-offs when thinking about these different measures。
  prefs: []
  type: TYPE_NORMAL
- en: And there's a variety of ways to think about which are used when。
  prefs: []
  type: TYPE_NORMAL
- en: There's language that's used around the different costs of using one versus
    the other。
  prefs: []
  type: TYPE_NORMAL
- en: You might hear about true positives， true negatives， false positives and false
    negatives。
  prefs: []
  type: TYPE_NORMAL
- en: True positives and true negatives are how many you get correct or how many of
    the identifications。
  prefs: []
  type: TYPE_NORMAL
- en: you get correct。 True positive is the fraction of times that you identify something
    in the positive class。
  prefs: []
  type: TYPE_NORMAL
- en: in this case fraudulent as being fraudulent。 For a true negative is the fraction
    of times we identify legitimate as being a legitimate。
  prefs: []
  type: TYPE_NORMAL
- en: transaction。 So true positives and true negatives together basically indicate
    the number of times or the。
  prefs: []
  type: TYPE_NORMAL
- en: fraction of times we're getting something right。 False positives and false negatives
    refer to the errors and these are things that often。
  prefs: []
  type: TYPE_NORMAL
- en: come with costs。 And so false positives and false negatives are two different
    kinds of errors that may。
  prefs: []
  type: TYPE_NORMAL
- en: involve two different kinds of costs。 False positives are the fraction of times
    that we identify。
  prefs: []
  type: TYPE_NORMAL
- en: for instance， something as， legitimate as fraudulent。
  prefs: []
  type: TYPE_NORMAL
- en: False negatives is a fraction of times we identify something as fraudulent as
    legitimate。 So again。
  prefs: []
  type: TYPE_NORMAL
- en: depending on whether it's more costly to miss a fraudulent transaction or。
  prefs: []
  type: TYPE_NORMAL
- en: whether it's more costly to accidentally flag something legitimate as fraudulent，
    these。
  prefs: []
  type: TYPE_NORMAL
- en: map to false positives and false negatives and the different costs。
  prefs: []
  type: TYPE_NORMAL
- en: And depending on what we care about from a business context， we may want to
    optimize one。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee32926febee5fe618fb5c24bb11950c_5.png)'
  prefs: []
  type: TYPE_IMG
- en: versus the other。 There are a number of ways to represent these， to visualize
    these。
  prefs: []
  type: TYPE_NORMAL
- en: to communicate these， to， different stakeholders。 One of these is called a confusion
    matrix。
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix is essentially a two by two matrix that lays out for a given
    case the。
  prefs: []
  type: TYPE_NORMAL
- en: number of true positives， false positives， true negatives and false negatives。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee32926febee5fe618fb5c24bb11950c_7.png)'
  prefs: []
  type: TYPE_IMG
- en: Another popular way to represent machine learning output is called an ROC curve
    or。
  prefs: []
  type: TYPE_NORMAL
- en: receiver operating characteristic curve。 And this maps false positives against
    true positives in a way that indicates essentially。
  prefs: []
  type: TYPE_NORMAL
- en: what the trade-offs are between those two metrics。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee32926febee5fe618fb5c24bb11950c_9.png)'
  prefs: []
  type: TYPE_IMG
- en: These are some common loss functions。 We've defined them。 In the next video。
  prefs: []
  type: TYPE_NORMAL
- en: let's talk about when it might be that some of them are preferable to， others。
    [BLANK_AUDIO]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee32926febee5fe618fb5c24bb11950c_11.png)'
  prefs: []
  type: TYPE_IMG
