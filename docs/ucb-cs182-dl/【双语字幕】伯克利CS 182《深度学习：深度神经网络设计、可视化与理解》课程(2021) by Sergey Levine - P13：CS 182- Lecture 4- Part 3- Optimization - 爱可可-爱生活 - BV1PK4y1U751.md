# 【双语字幕】伯克利CS 182《深度学习：深度神经网络设计、可视化与理解》课程(2021) by Sergey Levine - P13：CS 182- Lecture 4- Part 3- Optimization - 爱可可-爱生活 - BV1PK4y1U751

所有的权利，在今天讲座的最后一部分，让我们来谈谈随机优化，所以呃，到目前为止，我们讨论了如何利用动量加速梯度下降，并通过将每个维度规范化为rms支柱来改进它，但是所有版本的梯度下降，到目前为止。

我所描述的实际上还不适合用于深度学习，为什么呢，假设我们有负对数似然损失，尽管这对任何损失都是正确的，我们计算损失函数的方法是，通过对我们训练集中的所有数据点进行求和，然而，呃，在深度学习中。

当数据集非常大时，事情往往工作得最好，比如说，Imagenet大型视觉识别挑战或Imagenet训练集有一个点，五百万张图像，所以如果你每次更新参数时都有超过150万张图像。

你将在很长一段时间内把事情汇总在一起，事实上，你将不能做很多渐变步骤，在这种情况下，有效地优化神经网络可能需要数百万个梯度步骤，所以这将是相当令人望而却步的，然而，我们可以认识到的一件事是。

实际上是对期望值的基于样本的估计，对所以如果你，如果考虑数据分布下的期望值，给定x的y的对数概率的p数据，你实际计算损失的总和，只是基于样本的估计，并且基于样本的期望值估计是有效的。

现在对任何数量的样本都是无偏的，如果你使用较少的样本，你会得到一个更坏的估计，但这仍然是一个无偏见的估计，这意味着如果你平均重复这个过程很多次，你会得到正确的答案，所以我们能做的一件事。

如果我们想让梯度下降在计算上更快，就是，我们可以简单地使用更少的样本，然后每次更新的整个训练集，所以我们可以选择一个大写B样本的子集，把这些B样本平均在一起，其中b远小于m，现在，当然啦。

如果我们只挑选这些样本一次，我们在整个优化过程中使用它们，这是相当浪费的，因为我们基本上没有使用我们的大部分数据，但我们可以选择大小为b的不同子集，每一个梯度台阶，结果是我们用少量样本估计的梯度。

会看起来像真正的渐变，有一些噪音，对呀，因为只是偶然的机会，每一个梯度步骤都会得到一组稍微不同的B样本，所以渐变是这样的，但它将以正确的期望值为中心，因为它是公正的。

下面是小批处理的随机梯度下降的工作原理，对整个训练集的子集进行采样，所以脚本B是一组从D采样的点，你这样做的方式是，从原始数据集中随机抽取B个数据点，它的大小是n现在n可能是一个点，五百万。

B可能是三二或六十四，所以它要小得多，第二步，估计近似梯度方向，这大约是你损失的梯度，但它不完全相等的原因是因为你用的点少得多，所以这里的和是在集合脚本p中的元素上。

然后第三步用这个近似的梯度进行梯度下降，当然你也可以利用动量，原子，等，所以上一节中的所有技巧都用gk代替grad，西塔我没事，所以这基本上是方法的全部，我们只是在每次更新时使用所有训练点的随机子集。

每次迭代采样一个不同的迷你批，所以所有的数据都在某个时候被使用，这并不是每一个人都习惯的，这是随机梯度下降的理论理想版本，这就是你在教科书上学到的，在实践中，你实际上可以更有效地做事，在实践中随机抽样。

由于随机内存访问，每一步实际上都很慢，所以如果你有一个非常复杂的数据，比如，比如说，图像，它会在内存中按顺序排列，所以如果你每次都索引到一个随机的图像中，那么你正在进行大量的随机内存访问。

这将真正破坏缓存的一致性，所以如果您还记得在系统类中，您希望具有内存访问权限，在这种随机访问中更加结构化，它往往会在你的记忆中非常密集，这将是缓慢的，我们可以做的是我们可以提前洗牌我们的数据集一次。

就像你洗牌一样，然后按顺序索引到该数据集，所以这里的想法是，你把你的原始数据集，你随机排列它，一开始只做过一次，所以你只要洗牌你的数据集，就像你洗牌，然后每一批都可以按顺序画出来，所以第一步。

你拿前四个，然后第二个4，然后第三个四个，等等，一旦你走到最后，你现在就一直循环到开始，从技术上讲，这与常规梯度下降不太一样，因为你一开始只洗牌一次，你绕着圈，呃，从结束到开始。

多次你会看到相同顺序的数据点，但这个顺序是随机的，所以在实践中，这和真正的随机梯度下降一样好，但在计算上要便宜得多，所以你只是按这样的顺序索引，这是随机梯度下降，这就是我们将要使用的。

基本上是为了你将要做的所有编程项目，在深度学习中基本上无处不在，嗯下一个，让我们来谈谈学习率，所以现在我们有了一个完整的方法，我们可以使用，我们可以用小批量生产的原子或者小批量生产的动量，让我们呃。

讨论一下我们如何调整这些算法的学习速率，所以在这个图中，垂直轴是训练损失，我不担心验证丢失，现在，横轴是纪元，什么是时代，嗯，一个纪元就是你浏览整个数据集的次数，所以有规律的梯度下降。

历元数等于优化步数，随机梯度下降，因为你不会在每一步中查看所有的数据，一个时代是我们正确地经历了多少次，所以如果你通过整个数据集，可能需要很多很多的步骤，那是一个时代，如果你想计算历元中有多少个梯度步。

您只需将数据集的总大小，除以你的迷你批的大小，所以红色曲线大致显示了你希望看到的训练损失，如果你有一个很好的选择，很好的调整学习率，这是一次成功的随机梯度下降，或者是有动量的随机梯度下降。

如果你看到这样的曲线，就像绿线一样，我们会是什么，我们预计会是问题所在，花点时间想想这个，这里要注意的是绿线下降了很多，慢一点，所以这意味着我们的学习率可能比应该的要低。

所以绿线是我们在低学习率的情况下所期望的，现在传统观点认为，传统智慧通常来自凸问题，认为低学习率是可以的，如果你优化的时间足够长，所以一种缓慢而稳定的低学习，赢得比赛之类的，不幸的是，在深度学习中。

情况往往并非如此，因为低学习率会导致从低值到差值的收敛，因为你可能会被困在高原上，这有点违反直觉，所以低学习率不仅会导致优化缓慢，它们实际上会导致更糟糕的最终结果，因为如果你开得太慢就会卡住。

你有点筋疲力尽，所以不要使用低学习率，这个黄色的曲线呢，所以黄色曲线开始得很快，它实际上比红色曲线走得更快，但后来它就卡住了，那里发生了什么事？嗯，黄色曲线实际上正是我们可能期望看到的。

如果我们的学习率太高，想象一下我们有一个优化问题，和上面的风景一样，但现在我们走得太快了，当我们走得太快了，会发生什么，我们实际上会超过最优值，所以我们会下到坑底，然后我们会超调。

然后我们会向相反的方向超调，以此类推，损失停留在更高价值的原因，因为那是你超调的高度，如果你再低一点，你只会被射得更高，所以如果你的学习率太高，你实际上会看到你一开始走得很快。

但你会被困在一个更糟糕的值，在这一点上，你可能会想的一件事是，如果一开始高学习率很快，但后来被卡住了，我们为什么不从高学习率开始呢？然后切换到较低的学习速率，事实上，这是个很好的主意。

它有时被称为学习速率衰减，这是一个神经网络的图，特别是Alex Net，我们在课堂上谈到过，一个正在接受Imagenet训练，它正在用学习速率衰减时间表进行训练，学习率每隔一段时间除以十。

通常你会定期这样做，在这里，他们每隔一段时间就做一次，但通常你会选择一些时代，也许每一千个时代就会把学习速率除以10或5之类的，你可以看到的是，最初的学习速率为零点零，一个精度迅速提高。

所以这里越高越好，因为我们绘制的是精确度而不是误差精确度提高得很快，然后就有点高原了，那么学习率就会下降得更多，现在你可以深入那些洞，你的准确性进一步提高，然后在高原，那么学习率就下降得更多了。

然后精度进一步提高，然后现在是高原，最终你会足够接近局部最优，然后你就停止进步了，所以这实际上是随机梯度下降的一个非常常见的策略，有动量的新币或新币，因此，为了获得最佳性能，通常需要学习速率衰减时间表。

如果你使用规则随机梯度下降，或有动量的随机梯度下降，通常你不需要原子的学习速率衰减，虽然你可以试着用它，但如果你只是想要一些快速而肮脏的东西，原子没有学习速率衰减就可以了，如果你想要最好的表现。

通常带有动量和学习速率衰减的STD现在可以工作得更好一点，社会上对此有不同的看法，很多人实际上认为有动量的SGD比亚当更好，如果你想要最好的表现，你用动量很好地调整GD，但是Atom往往更容易调谐。

所以如果你只是想要一些现成的东西，那还可以，使用Atom是一个不错的起点，所以如果你想知道用哪种算法来使用我的建议，你可能会使用带有动量或原子的SGD，如果你刚刚开始。

或者你只是想让一些东西快速启动和运行，使用原子，如果你想要最好的表现，考虑使用带动量的SGD，也许有一个学习速率衰减时间表，让我们来谈谈随机梯度下降的调谐，现在我们从常规梯度下降开始。

只有一件事可以选择，这是学习率，我们最终得到了随机梯度下降，潜在地有动量，潜在地与原子，所以现在我们有更多的东西要调，我们要调好的东西有哪些，一个超参数是批处理大小b，和批量大小规则。

通常是较大的批处理导致较少的噪声梯度，通常更安全更好，但现在更贵了，在某些情况下，你可能没有奢侈地使用大量，比如说，计算机视觉中的一些图像分割应用，我们将在几个星期后讨论，它们需要如此多的内存。

以至于您通常只使用一个批处理大小，只有一个图像，另一方面，深度学习中大型模型的一些最佳结果，尤其是像甘斯这样的东西往往使用非常大的批量，他们实际上发现，巨大的批量大小导致了更少的噪声梯度。

这给了他们更好的解决方案，我从来没有听说过大批量会让事情变得更糟，通常大批量的问题是它们要贵得多，所以如果你的选择是在走一百万步渐变，批量为10对，你知道吗，批量十万的一百个渐变台阶。

也许你可以选择小批量的，为了采取更多的步骤，学习率，学习速度是一个有点棘手的事情来选择，尤其是对没有亚当的新元来说，一般来说，最好用最大的学习率，在凸优化中如此有效，较小的学习率是可以的。

缓慢而稳定往往会赢得比赛，但对于深度学习，你想用最大的学习速度来完成这项工作，所以你不想振荡，你不想回到另一个任务，但只要你能避免，使用更大的费率，它会更好地工作，通常你想随着时间的推移腐烂它们。

动量好的设置是零点九或零点九，但你必须调整它来适应你的问题，原子参数β1和β2通常只保留默认值，所以对于原子来说，通常，调整beta 1和beta 2参数没有太大好处，只要保持默认值，他们工作得很好。

嗯，如何调整这些超参数，比如如果你想改变学习速度来找到最好的，你应该在看什么，从技术上来说，你想根据训练损失调整这些参数，因为这些超参数不是正则化，他们不是想给你更好的，对新点的推广，他们只是想提高。

你如何优化训练损失，所以传统上，你会根据训练损失调整所有这些超参数，这与正则化有很大的不同，正规化，我们调整验证损失，梯度下降超参数也经常根据验证损失进行调整，这是一个有趣的选择。

随机梯度与正则化之间存在着复杂的关系，将优化与泛化分开，我们会说概括，选择正确的正则化器，选择正确的模型类，然后选择最佳优化器，你可以尽可能减少训练损失，受制于所有那些正规化者，最近，然而。

有相当多的研究，巨大的随机梯度下降本身实际上可以充当一种正则化器，直觉是，如果你开始过度拟合，对于不同的批次，您会期望渐变看起来非常不同，所以你实际上不想把这些平均出来，你不想看到你损失的最底层。

如果您发现不同批次的梯度差异很大，也许这实际上意味着你已经通过了你正在学习的制度，你已经进入了一个过度适应的政权，这就是为什么，有些人认为使用小批量生产的随机梯度下降，实际具有理想的正则化效果，因此。

当你调整随机梯度下降的超参数时，您实际上应该查看验证损失，这是一个合理的观点，并在实践中，很多人在调优这些超参数时使用验证丢失，因为为什么不呢，因为它很方便，而且就在那里。

我们习惯于使用验证损失来调整一切。

![](img/34341666da5be4448ded17739a1bfac5_1.png)

所以就这样了，呃。

![](img/34341666da5be4448ded17739a1bfac5_3.png)