# (超爽中英!) 2024吴恩达最好的【LangChain大模型应用开发】教程！附课件代码 DeepLearning.AI - P13：5——检索 - 吴恩达大模型 - BV1iZ421M79T

![](img/d616ba48c25503a419d35bb2fb15cd3e_0.png)

在上一课中，我们介绍了语义搜索的基础知识，并看到它在大量的用例中工作得很好，但我们也看到了一些边缘情况，看到了事情可能会出一点问题，在这节课中，我们将深潜回收，并介绍一些克服这些边缘情况的更先进的方法。

我真的很兴奋，因为我认为检索是一种新的东西，我们谈论的很多技术都出现了，在过去的几个月里，这是一个前沿的话题，所以你们将站在最前沿，我们找点乐子吧，在这节课中，我们将讨论检索，这在查询时很重要。

当您有一个查询进来时。

![](img/d616ba48c25503a419d35bb2fb15cd3e_2.png)

你想检索最相关的分裂。

![](img/d616ba48c25503a419d35bb2fb15cd3e_4.png)

我们在上一课中谈到了语义相似性搜索，但我们将在这里讨论一些不同的、更先进的方法。

![](img/d616ba48c25503a419d35bb2fb15cd3e_6.png)

我们要讨论的第一个问题是最大边际相关性。

![](img/d616ba48c25503a419d35bb2fb15cd3e_8.png)

所以这背后的想法是。

![](img/d616ba48c25503a419d35bb2fb15cd3e_10.png)

如果您总是获取与查询最相似的文档，在嵌入空间中，你可能会错过不同的信息，就像我们在一个边缘案例中看到的那样。



![](img/d616ba48c25503a419d35bb2fb15cd3e_12.png)

在本例中。

![](img/d616ba48c25503a419d35bb2fb15cd3e_14.png)

我们有一个厨师问所有的白蘑菇，所以如果我们看看最相似的结果，这将是前两份文件，他们有很多类似于子实体查询的信息。



![](img/d616ba48c25503a419d35bb2fb15cd3e_16.png)

全身都是白的，但我们真的想确保我们也能得到其他信息。

![](img/d616ba48c25503a419d35bb2fb15cd3e_18.png)

比如它真的有毒，这就是mmr发挥作用的地方，因为它将为一组不同的文档选择。

![](img/d616ba48c25503a419d35bb2fb15cd3e_20.png)

mmr背后的想法是我们发送一个查询。

![](img/d616ba48c25503a419d35bb2fb15cd3e_22.png)

然后我们最初得到一组响应，取下划线k是我们可以控制的参数，为了确定我们得到了多少回应。

![](img/d616ba48c25503a419d35bb2fb15cd3e_24.png)

这完全基于语义相似性，从那里，然后，我们处理这组较小的文档，并优化。

![](img/d616ba48c25503a419d35bb2fb15cd3e_26.png)

不仅是基于语义相似性的最相关的，但也有多样化的，从这组文档中，我们选择最后一个k返回给用户。

![](img/d616ba48c25503a419d35bb2fb15cd3e_28.png)

我们可以做的另一种类型的检索是我们所说的自我查询。

![](img/d616ba48c25503a419d35bb2fb15cd3e_30.png)

因此，当你遇到问题时，这很有用，这些问题不仅仅是关于，要从语义上查找的内容。

![](img/d616ba48c25503a419d35bb2fb15cd3e_32.png)

但也要提到一些要对其进行筛选的元数据，所以让我们来回答这个问题，1980年拍的关于外星人的电影有哪些？这真的有两个组成部分，它有一个语义部分，外星人咬了。



![](img/d616ba48c25503a419d35bb2fb15cd3e_34.png)

所以我们想在我们的电影数据库中查找外星人，但它也有一个真正引用每部电影元数据的部分，这就是年份应该是1980年的事实。



![](img/d616ba48c25503a419d35bb2fb15cd3e_36.png)

我们能做的就是，我们可以使用语言模型本身将最初的问题分成两个独立的东西，筛选器和搜索词。

![](img/d616ba48c25503a419d35bb2fb15cd3e_38.png)

大多数向量存储支持元数据筛选器，因此，您可以轻松地基于元数据筛选记录。

![](img/d616ba48c25503a419d35bb2fb15cd3e_40.png)

就像1980年。

![](img/d616ba48c25503a419d35bb2fb15cd3e_42.png)

最后我们将讨论压缩，这对于只提取检索到的段落中最相关的部分是有用的，例如，在问问题时，你拿回储存在。

![](img/d616ba48c25503a419d35bb2fb15cd3e_44.png)

即使只有前一两句是压缩的相关部分。

![](img/d616ba48c25503a419d35bb2fb15cd3e_46.png)

然后，您可以通过语言模型运行所有这些文档，并提取最相关的段。

![](img/d616ba48c25503a419d35bb2fb15cd3e_48.png)

然后只将最相关的段传递到最终的语言模型调用中。

![](img/d616ba48c25503a419d35bb2fb15cd3e_50.png)

这是以对语言模型进行更多调用为代价的。

![](img/d616ba48c25503a419d35bb2fb15cd3e_52.png)

但它也非常有利于将最终答案集中在最重要的事情上。

![](img/d616ba48c25503a419d35bb2fb15cd3e_54.png)

所以这有点折衷，让我们看看这些不同的技术在起作用。

![](img/d616ba48c25503a419d35bb2fb15cd3e_56.png)

我们将从加载环境变量开始，就像我们一直做的那样。

![](img/d616ba48c25503a419d35bb2fb15cd3e_58.png)

进口色度和开放AI是多少，就像我们以前用的那样。

![](img/d616ba48c25503a419d35bb2fb15cd3e_60.png)

我们可以从收藏中看到，数一数，它有我们之前加载的209份文件。

![](img/d616ba48c25503a419d35bb2fb15cd3e_62.png)

现在让我们来看看最大边际相关性的例子。

![](img/d616ba48c25503a419d35bb2fb15cd3e_64.png)

因此，我们将从示例中加载文本，我们有关于蘑菇的信息，对于本例，我们创建一个小数据库，我们可以把它当作玩具使用，例。



![](img/d616ba48c25503a419d35bb2fb15cd3e_66.png)

我们有问题了，现在我们可以进行相似性搜索，我们将k等于2设置为只返回两个最相关的文档。

![](img/d616ba48c25503a419d35bb2fb15cd3e_68.png)

我们可以看到没有提到它有毒的事实，现在让我们用mmr运行它，除非通过k等于2，我们还要还两份文件，但是让我们把fetch k设为3，我们把这三份文件，原来。



![](img/d616ba48c25503a419d35bb2fb15cd3e_70.png)

我们现在可以看到，有毒的信息在我们检索的文档中返回。

![](img/d616ba48c25503a419d35bb2fb15cd3e_72.png)

让我们回到上一课的一个例子。

![](img/d616ba48c25503a419d35bb2fb15cd3e_74.png)

当我们询问Matlab并得到文档时，这些文档中有重复的信息来刷新您的记忆。

![](img/d616ba48c25503a419d35bb2fb15cd3e_76.png)

我们可以看看前两份文件。

![](img/d616ba48c25503a419d35bb2fb15cd3e_78.png)

只是看看最初的几个字符，因为它们很长，否则，我们可以看到他们是一样的，当我们在这些结果上运行mmr时。



![](img/d616ba48c25503a419d35bb2fb15cd3e_80.png)

我们可以看到第一个和之前一样。

![](img/d616ba48c25503a419d35bb2fb15cd3e_82.png)

因为那是最相似的，但当我们进入第二个，我们可以看到它是不同的。

![](img/d616ba48c25503a419d35bb2fb15cd3e_84.png)

它得到了一些不同的反应。

![](img/d616ba48c25503a419d35bb2fb15cd3e_86.png)

现在让我们进入self查询示例，这是我们有问题的地方。

![](img/d616ba48c25503a419d35bb2fb15cd3e_88.png)

他们在第三堂课上对回归说了什么，它返回的结果不仅仅是第三次讲座。

![](img/d616ba48c25503a419d35bb2fb15cd3e_90.png)

也是第一个和第二个，如果我们用手把它修好。

![](img/d616ba48c25503a419d35bb2fb15cd3e_92.png)

我们要做的是指定一个元数据过滤器，所以我们传递这个信息，我们希望源等于第三个讲座。

![](img/d616ba48c25503a419d35bb2fb15cd3e_94.png)

PDF，然后如果我们看看会被检索到的文件，它们都来自那个讲座。

![](img/d616ba48c25503a419d35bb2fb15cd3e_96.png)

我们可以使用语言模型来为我们做到这一点，所以我们不必手动指定，做这件事。

![](img/d616ba48c25503a419d35bb2fb15cd3e_98.png)

我们将导入一个语言模型开放AI，我们将导入一个称为自查询检索器的检索器。

![](img/d616ba48c25503a419d35bb2fb15cd3e_100.png)

然后我们将导入属性信息，我们可以在元数据中指定不同的字段，以及它们对应于什么。

![](img/d616ba48c25503a419d35bb2fb15cd3e_102.png)

元数据中只有两个字段。

![](img/d616ba48c25503a419d35bb2fb15cd3e_104.png)

来源和页面，我们填写名字的描述。

![](img/d616ba48c25503a419d35bb2fb15cd3e_106.png)

每个属性的描述和类型，这些信息实际上将被传递给语言模型。

![](img/d616ba48c25503a419d35bb2fb15cd3e_108.png)

因此，使其尽可能具有描述性是很重要的。

![](img/d616ba48c25503a419d35bb2fb15cd3e_110.png)

然后，我们将指定有关此文档存储中实际内容的一些信息，我们将初始化语言模型，然后，我们将使用from lm方法初始化自查询检索器。



![](img/d616ba48c25503a419d35bb2fb15cd3e_112.png)

并将我们将要查询的底层向量数据库传递到语言模型中，有关描述和元数据的信息，然后我们还将传递详细等于真实的设置。



![](img/d616ba48c25503a419d35bb2fb15cd3e_114.png)

冗长等于真井，让我们看看引擎盖下面是怎么回事，当LLM推断应该与任何元数据筛选器一起传递的查询时。

![](img/d616ba48c25503a419d35bb2fb15cd3e_116.png)

当我们运行带有此问题的自查询检索器时。

![](img/d616ba48c25503a419d35bb2fb15cd3e_118.png)

我们可以看到感谢若等于真的，虽然我们正在打印引擎盖下发生的事情。

![](img/d616ba48c25503a419d35bb2fb15cd3e_120.png)

我们得到一个回归查询，这是语义位，然后我们得到一个过滤器，在那里我们有一个平等的比较器，在source属性和docs值之间。



![](img/d616ba48c25503a419d35bb2fb15cd3e_122.png)

然后这个路径，也就是第三个机器学习讲座的路径，所以说，这基本上是告诉我们在回归的语义空间中进行查找，然后做一个筛选，我们只查看源值为此值的文档。



![](img/d616ba48c25503a419d35bb2fb15cd3e_124.png)

所以如果我们在文档上循环并打印出元数据，我们应该看到他们都来自第三课。

![](img/d616ba48c25503a419d35bb2fb15cd3e_126.png)

他们确实如此，这是一个例子，其中可以使用自查询检索器准确地筛选元数据。

![](img/d616ba48c25503a419d35bb2fb15cd3e_128.png)

我们可以谈论的最后一种检索技术是上下文压缩，所以让我们在这里加载一些相关的模块。

![](img/d616ba48c25503a419d35bb2fb15cd3e_130.png)

上下文压缩检索器，然后是一个LLM链条提取器。

![](img/d616ba48c25503a419d35bb2fb15cd3e_132.png)

接下来要做的是，这将只从每个文档中提取相关的位。

![](img/d616ba48c25503a419d35bb2fb15cd3e_134.png)

然后将这些作为最终的返回响应传递给，我们将定义一个很好的小函数来漂亮地打印文档。

![](img/d616ba48c25503a419d35bb2fb15cd3e_136.png)

因为它们往往又长又乱，这将使人们更容易看到发生了什么。

![](img/d616ba48c25503a419d35bb2fb15cd3e_138.png)

然后我们可以用LLM链提取器创建一个压缩机。

![](img/d616ba48c25503a419d35bb2fb15cd3e_140.png)

然后我们可以创建上下文压缩检索器，通过压缩机。

![](img/d616ba48c25503a419d35bb2fb15cd3e_142.png)

然后向量存储的基检索器，当我们现在通过这个问题，他们怎么说matlab。

![](img/d616ba48c25503a419d35bb2fb15cd3e_144.png)

我们看看压缩的文档，如果我们看了文件，我们可以看到两件事，一个，它们比正常的文件短得多，但第二，我们仍然有一些重复的事情发生，这是因为在引擎盖下，我们使用的是语义搜索算法。

这就是我们在本课前面使用mmr解决的问题，这是一个很好的例子，说明您可以结合各种技术来获得最好的结果，为了做到这一点，当我们从向量数据库创建检索器时，我们可以将搜索类型设置为Mr。

然后我们可以重新运行这个，确保我们回来，不包含任何重复信息的筛选结果集，到目前为止，我们提到的所有附加检索技术都建立在矢量数据库之上。



![](img/d616ba48c25503a419d35bb2fb15cd3e_146.png)

值得一提的是，还有其他类型的检索根本不使用矢量数据库。

![](img/d616ba48c25503a419d35bb2fb15cd3e_148.png)

取而代之的是其他，更传统的nlp技术。

![](img/d616ba48c25503a419d35bb2fb15cd3e_150.png)

这里，我们要重新建立一个检索管道，有两种不同类型的寻回器，和SVM寻回器和一个TF IDF寻回器。

![](img/d616ba48c25503a419d35bb2fb15cd3e_152.png)

如果你从传统的NLP或传统的机器学习中认识到这些术语，那就太好了。

![](img/d616ba48c25503a419d35bb2fb15cd3e_154.png)

如果你不，也很好，这只是其他一些技术的一个例子。

![](img/d616ba48c25503a419d35bb2fb15cd3e_156.png)

除了这些，还有很多，我鼓励你去看看，他们中的一些人。

![](img/d616ba48c25503a419d35bb2fb15cd3e_158.png)

我们可以很快地通过通常的装货和拆分管道。

![](img/d616ba48c25503a419d35bb2fb15cd3e_160.png)

然后这两个检索程序都公开一个from text方法，其中一个接受嵌入模块，即svm检索器，tf idf寻回器直接接收分裂。



![](img/d616ba48c25503a419d35bb2fb15cd3e_162.png)

现在我们可以用其他的寻回者，我们进去吧，他们怎么说matlab，到svm检索器，我们可以看看顶部的文档，我们回来了，我们可以看到它提到了很多关于Matlab的东西，所以它在那里取得了一些好的结果。

我们也可以在tf idf寻回器上尝试一下，我们可以看到结果看起来有点糟糕，现在是停下来尝试所有这些不同检索技术的好时机，我想你会注意到他们中的一些人在各种事情上都比其他人好。

所以我鼓励你们在各种各样的问题上尝试一下，自我查询检索器尤其是我最喜欢的，所以我建议用越来越复杂的元数据过滤器来尝试，甚至可能在有嵌套元数据结构的地方编造一些元数据，你可以试着让LLM推断。

我觉得这很有趣，我想这是一些更先进的东西，所以我真的很兴奋能和你们分享，既然我们已经谈到了回收，我们将讨论这个过程的下一步。

