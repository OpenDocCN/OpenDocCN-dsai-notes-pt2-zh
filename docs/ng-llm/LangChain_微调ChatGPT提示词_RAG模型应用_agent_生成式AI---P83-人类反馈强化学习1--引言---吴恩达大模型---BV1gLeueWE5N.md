# LangChain_微调ChatGPT提示词_RAG模型应用_agent_生成式AI - P83：人类反馈强化学习1——引言 - 吴恩达大模型 - BV1gLeueWE5N

上周学习了指令，大型语言模型的调优及微调，本周深入RLHF，从人类反馈中学习，你可能在新闻中听过的技术，但实际如何运作？深入探讨，以及第二个激动人心的主题，如何使用LMS作为推理引擎，并让它调用子程序。

创建能行动的代理，所以RLHF非常激动人心，有助于使模型与人类价值观对齐，例如，语言模型可能在创建有害内容方面有挑战，或如有毒的语气或声音，通过与人类反馈对齐并使用强化学习作为算法。

可以帮助模型减少这些内容，并使其与较少有害和更多有益的内容对齐，有时人们感觉我训练于这些可怕的数据，你知道，一些可怕的上网数据似乎很危险，我认为许多人低估了我们的高阶功能，它当然不完美。

AI确实产生问题输出，但随着技术进步，研究人员不断改进它们，我猜是右边三英寸，诚实，有帮助且无害，是的，绝对，本周我将与，亚马逊的应用科学家一起，他将解释强化学习算法背后的原理，期待那时刻，没错。

正是这些人将加入我们，我们确实邀请了Ashley Sifis博士，他将与我们讨论负责任的AI，没错，Ashley将加入我们，我将与她围绕负责任的AI主题进行讨论，这同样重要。

很高兴你在这方面投入了这么多时间，人工智能风险，许多人深思熟虑，我认为所有主要AI团队都非常重视，资源和努力，深度思考，我们远非完美，但社区确实在努力做得更好，每年如此，除了，负责任的人工智能，和。

调整模型，嗯，使用rhf，我兴奋的另一技术是使用lms作为推理引擎，并赋予它们调用自己的子例程的能力，也许进行网络搜索或其他操作，当然，我们将在本课中深入探讨，我们还将讨论一些技术，允许您绕过。

大型语言模型中看到的某些限制，通过允许它们通过类似react的技术推理和采取行动，我们还将讨论rag，它允许您访问外部信息源，因此您可以访问特定领域的信息，我们看到许多客户希望将信息。

从专有数据源集成到其生成性应用程序中，因此，我们稍微谈了一些允许您这样做的技术和方法，你知道大型语言模型的优点之一，是它们非常擅长记忆事实，您正在互联网上学习事实。

有时人们使用它们作为事实的存储库来获取问题的答案，但我认为有一种不同的，也许我认为更有帮助的方式来思考lms，即，嗯，如果它是一个推理引擎，您给它，你知道的api去获取自己的事实，因为这很好。

但不是最好的事实数据库，但有一个非常好的推理引擎，我认为这是这些模型的真正力量，绝对，绝对更经济，使用数据库存储这些信息，然后您的生成性AI用于那个，它旨在用于的，这是一个很好的观点，因此，随着这一点。

嗯，这最后一周有很多令人兴奋的事情，我确信您会喜欢它，因此，让我们继续观看下一个视频。