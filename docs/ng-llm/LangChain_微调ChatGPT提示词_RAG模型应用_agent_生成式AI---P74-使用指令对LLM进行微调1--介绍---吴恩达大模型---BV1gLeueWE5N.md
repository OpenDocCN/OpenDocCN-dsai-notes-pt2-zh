# LangChain_微调ChatGPT提示词_RAG模型应用_agent_生成式AI - P74：使用指令对LLM进行微调1——介绍 - 吴恩达大模型 - BV1gLeueWE5N

欢迎回来，我和本周的讲师们在一起，迈克和谢比，上周学习了变换器网络，这是大型语言模型的关键基础，以及生成AI项目生命周期，本周还有很多内容要深入，从大型语言模型的指令微调开始。

然后稍后如何以高效方式进行微调，是的，我们看看指令微调，所以当你有基础模型时，最初预训练的东西，它编码了很多非常好的信息，通常关于世界，所以它知道一些事情，但它并不一定知道如何回应我们的提示。

我们的问题，所以当我们指示它执行某个任务时，它并不一定知道如何回应，因此指令微调帮助它改变行为，对我们更有帮助，我认为指令微调是，你知道，大型语言模型历史上的一大突破，因为通过学习一般文本。

来自互联网和其他来源，你学会预测下一个单词，但预测互联网上的下一个单词与遵循指令不同，我看到了惊人的，你可以拿一个大语言模型，在互联网上训练数百亿个单词，然后用一个小，远小于的数据集进行指令微调。

然后学会做那个，没错，当然，你需要注意的一件事是灾难性遗忘，这是我们课程中讨论的内容，所以这就是你在，一些额外的数据上进行训练，在这种疯狂的指令微调中，然后它忘记了之前所有的东西，或者之前大部分的数据。

所以有一些技术我们将在课程中讨论以帮助对抗它，例如，在非常广泛的不同的指令类型上进行指令微调，所以不仅仅是案例，只是调整它来做你想要它做的事情，你可能需要比那更广泛一点，但我们在课程中讨论它。

所以结果是，你知道有两种微调类型，嗯，那些非常值得做，一个是指令微调，你刚提到麦克，然后当一个特定开发者尝试为其自己的应用微调它时，针对特定应用，微调的一个问题是，如果你拿一个大型模型。

并微调该模型中的每个参数，你有一个大东西要存储和部署，实际上非常，你知道，计算和内存昂贵，所以幸运的是有比这更好的技术，对，我们谈论参数高效微调或简称peft，作为一系列方法，可以让你缓解一些这些担忧。

对，所以我们有很多客户确实想要能够为非常特定的任务，非常特定的领域，参数高效，微调是一个很好的方法，仍然可以在许多任务上实现类似性能结果，许多任务上你可以用完全微调。

但实际上可以利用技术来冻结原始模型权重，或在上面添加自适应层，占用更小的内存空间，对，这样你就可以训练多个任务，你知道，实际上，我知道你经常使用的一种技术是，呃，劳拉，我记得，当我读劳拉论文时，我想。

哦，这很合理，这肯定会起作用，我们看到了很多对劳拉的兴奋和需求，因为使用低秩矩阵的性能结果，而不是完全微调，对，所以你能以最小的计算和内存要求获得非常好的性能结果，所以我在不同开发者中看到的是。

许多开发者通常会从提示开始，有时你知道这给你足够好的性能，这很好，有时提示达到了性能上限，然后这种使用劳拉，或其他技术的微调对于解锁额外级别的性能至关重要，然后我看到很多你知道。

LM开发者之间的讨论辩论是关于使用大型模型的成本，这有很多好处相对于，你知道你的应用，微调小模型，完全满，微调可能成本高昂，至少可以说，所以实际能够，你知道，使用路径等技术，嗯，微调生成AI模型。

基本上在每天，用户手中，有成本限制和成本意识，在现实世界中几乎每个人都是对的，没错，当然，如果你关心你的数据去向，所以如果需要在你的控制下运行，那么有一个适当大小的模型非常重要，所以再一次。

本周有很多激动人心的事情要深入，让我们继续下一个视频，迈克将在那里开始指导。