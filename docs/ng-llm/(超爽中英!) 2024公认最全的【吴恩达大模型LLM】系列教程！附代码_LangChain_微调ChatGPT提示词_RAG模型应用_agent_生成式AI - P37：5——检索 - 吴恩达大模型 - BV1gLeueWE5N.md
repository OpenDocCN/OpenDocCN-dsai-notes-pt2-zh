# (超爽中英!) 2024公认最全的【吴恩达大模型LLM】系列教程！附代码_LangChain_微调ChatGPT提示词_RAG模型应用_agent_生成式AI - P37：5——检索 - 吴恩达大模型 - BV1gLeueWE5N

![](img/1251636ef68b09989c270ce30b4b171d_0.png)

在上一节课中，我们覆盖了语义搜索的基本知识，并看到了它在许多使用场景中表现良好的情况，但我们也看到了一些边缘情况，并看到了事情可能会如何出错，在这节课中，我们将深入研究检索。

并覆盖一些更先进的方法来克服这些边缘情况，我对此非常兴奋，因为我认为检索是新的，而且我们正在谈论的技术在过去的几个月里出现了，这是一个前沿的主题，所以你们将处于前沿，让我们有一些乐趣，在这节课中。

我们将谈论检索，这在查询时间非常重要，当你有一个查询进来时，并且你想要检索最相关的片段。

![](img/1251636ef68b09989c270ce30b4b171d_2.png)

我们在上一节课中讨论了语义相似性搜索。

![](img/1251636ef68b09989c270ce30b4b171d_4.png)

但我们将讨论一些不同和更先进的方法在这里，我们将首先覆盖的最大边际相关性或mmr。

![](img/1251636ef68b09989c270ce30b4b171d_6.png)

所以这背后的想法是。

![](img/1251636ef68b09989c270ce30b4b171d_8.png)

如果你总是取与查询在嵌入空间中最相似的文档。

![](img/1251636ef68b09989c270ce30b4b171d_10.png)

你可能实际上会错过多样化的信息，因为我们在边缘情况中看到了，例如，我们有一个厨师在询问所有种类的白蘑菇。



![](img/1251636ef68b09989c270ce30b4b171d_12.png)

所以如果我们看最相似的结果。

![](img/1251636ef68b09989c270ce30b4b171d_14.png)

那些将是前两个文档，它们有许多与查询相似的信息，关于菌核，并且都是白色的，但我们确实想要确保我们也得到其他信息。



![](img/1251636ef68b09989c270ce30b4b171d_16.png)

如它真的很有毒，这就是mmr发挥作用的地方。

![](img/1251636ef68b09989c270ce30b4b171d_18.png)

因为它将选择一个多样化的文档集，mmr的想法是我们发送一个查询，然后我们最初得到一组响应。

![](img/1251636ef68b09989c270ce30b4b171d_20.png)

与fetch underscore k参数可以控制的响应数量有关。

![](img/1251636ef68b09989c270ce30b4b171d_22.png)

以确定我们得到多少响应，这仅基于语义相似性，从那里。

![](img/1251636ef68b09989c270ce30b4b171d_24.png)

然后我们与那个较小的文档集一起工作，并优化为，不仅基于语义相似性的最相关的文档，但也是多样化的。

![](img/1251636ef68b09989c270ce30b4b171d_26.png)

从那个文档集中，我们选择一个最终k来返回给用户，另一种检索我们可以做的是我们称之为自我查询，所以这对你来说很有用，当你得到的问题不仅仅是。



![](img/1251636ef68b09989c270ce30b4b171d_28.png)

你想要查找的语义内容，还包括一些你想要过滤的元数据。

![](img/1251636ef68b09989c270ce30b4b171d_30.png)

让我们以问题为例，哪部在1980年制作的关于外星人的电影。

![](img/1251636ef68b09989c270ce30b4b171d_32.png)

这实际上有两个部分，它有一个语义部分，还有一个关于元数据的部分，所以让我们看问题，哪些关于外星人的电影在1980年制作，外星人咬了。



![](img/1251636ef68b09989c270ce30b4b171d_34.png)

所以我们想要查看我们在电影数据库中的外星人，但它也有一个部分真正指的是关于每部电影的元数据，那就是事实，年份应该是一九八零。



![](img/1251636ef68b09989c270ce30b4b171d_36.png)

我们可以做，我们可以使用自然语言模型本身来将原始问题分成两个分开的东西，一个过滤器和一个搜索术语。

![](img/1251636ef68b09989c270ce30b4b171d_38.png)

大多数向量存储都支持元数据过滤，所以你可以很容易地根据元数据过滤记录。

![](img/1251636ef68b09989c270ce30b4b171d_40.png)

像年份是一九八零。

![](img/1251636ef68b09989c270ce30b4b171d_42.png)

最后我们将谈论压缩，这可以有用于真正提取从检索的段落中只有有关的部分，例如，当提问时，你会得到存储的整个文档。



![](img/1251636ef68b09989c270ce30b4b171d_44.png)

即使只有第一个或两个句子是有关的部分，使用压缩。

![](img/1251636ef68b09989c270ce30b4b171d_46.png)

你可以然后运行所有这些文档通过自然语言模型并提取最相关的部分。

![](img/1251636ef68b09989c270ce30b4b171d_48.png)

然后只将最相关的部分传递给最终的自然语言模型调用。

![](img/1251636ef68b09989c270ce30b4b171d_50.png)

这以使更多的调用自然语言模型为代价。

![](img/1251636ef68b09989c270ce30b4b171d_52.png)

但它也非常好用于将最终答案集中在最重要的事情上。

![](img/1251636ef68b09989c270ce30b4b171d_54.png)

因此，它是一种权衡，让我们看看这些不同的技术如何发挥作用。

![](img/1251636ef68b09989c270ce30b4b171d_56.png)

我们将开始加载环境变量，像我们总是做的。

![](img/1251636ef68b09989c270ce30b4b171d_58.png)

导入chroma和open ai，因为我们之前使用过它们。

![](img/1251636ef68b09989c270ce30b4b171d_60.png)

我们可以通过查看集合看到，计数，它包含了我们之前加载的209个文档。

![](img/1251636ef68b09989c270ce30b4b171d_62.png)

现在让我们来看看最大边际相关性的例子。

![](img/1251636ef68b09989c270ce30b4b171d_64.png)

因此，我们将加载来自示例的文本，其中我们有关于蘑菇的信息，对于这个例子，我们创建了一个小数据库，我们可以简单地使用它作为玩具，例如。



![](img/1251636ef68b09989c270ce30b4b171d_66.png)

我们有我们的问题，现在我们可以运行相似性搜索，我们将k设置为2，只返回最相关的两个文档。

![](img/1251636ef68b09989c270ce30b4b171d_68.png)

我们可以看到，没有提到它是有毒的，现在让我们运行它与mmr，除非将k设置为2，我们仍然想要返回两个文档，让我们将fetch k设置为3，我们获取所有3个文档，最初。



![](img/1251636ef68b09989c270ce30b4b171d_70.png)

我们可以看到，有毒的信息在我们的检索文档中被返回。

![](img/1251636ef68b09989c270ce30b4b171d_72.png)

让我们回到前一节课中的一个例子，当我们问关于matlab并返回包含重复信息的文档以刷新您的记忆。

![](img/1251636ef68b09989c270ce30b4b171d_74.png)

我们可以查看前两个文档。

![](img/1251636ef68b09989c270ce30b4b171d_76.png)

仅看前几个字符，因为它们否则很长，我们可以看到它们相同，当我们在这些结果上运行mmr时。

![](img/1251636ef68b09989c270ce30b4b171d_78.png)

我们可以看到第一个与以前相同。

![](img/1251636ef68b09989c270ce30b4b171d_80.png)

因为那是最相似的，但当我们转向第二个时，我们可以看到它不同。

![](img/1251636ef68b09989c270ce30b4b171d_82.png)

响应中有一些多样性。

![](img/1251636ef68b09989c270ce30b4b171d_84.png)

现在让我们转向自我查询示例，这是我们有问题的一个。

![](img/1251636ef68b09989c270ce30b4b171d_86.png)

第三讲座中他们说了什么关于回归，它不仅返回来自第三讲座的结果。

![](img/1251636ef68b09989c270ce30b4b171d_88.png)

而且还来自第一和第二，如果我们手动修复这个。

![](img/1251636ef68b09989c270ce30b4b171d_90.png)

我们会做的就是我们指定一个元数据过滤器，所以我们传递这个信息，我们想要源等于第三讲座。

![](img/1251636ef68b09989c270ce30b4b171d_92.png)

Pdf，然后如果我们看要检索的文档，它们都将来自精确的那一课。

![](img/1251636ef68b09989c270ce30b4b171d_94.png)

我们可以使用语言模型来做这个，所以我们不需要手动指定，要做这个。

![](img/1251636ef68b09989c270ce30b4b171d_96.png)

我们将导入一个语言模型open ai，我们将导入一个检索器叫做自我查询检索器。

![](img/1251636ef68b09989c270ce30b4b171d_98.png)

然后我们将导入属性信息，这是 where 我们可以指定元数据中的不同字段以及它们对应什么。

![](img/1251636ef68b09989c270ce30b4b171d_100.png)

我们在元数据中只有兩個字段。

![](img/1251636ef68b09989c270ce30b4b171d_102.png)

來源和頁面，我們填寫每個屬性的名稱。

![](img/1251636ef68b09989c270ce30b4b171d_104.png)

描述和類型，這信息實際上將被傳遞給語言模型。

![](img/1251636ef68b09989c270ce30b4b171d_106.png)

因此，使它盡可能描述性是非常重要的。

![](img/1251636ef68b09989c270ce30b4b171d_108.png)

我們將指定一些關於這個文档存儲的信息，我們將初始化語言模型，然後我們將初始化自我查询检索器使用 from lm 方法。



![](img/1251636ef68b09989c270ce30b4b171d_110.png)

並傳遞給語言模型我們將要查詢的基礎向量數據庫，關於描述和元数据的信息，我們還將傳遞 verbose 等于 true 的設置。



![](img/1251636ef68b09989c270ce30b4b171d_112.png)

verbose 等于 true 的話，讓我們看看底下的情況，當 llm 推断出應該與任何元数据过滤器一起傳送的查询時。



![](img/1251636ef68b09989c270ce30b4b171d_114.png)

當我們使用這個問題運行自我查询检索器時。

![](img/1251636ef68b09989c270ce30b4b171d_116.png)

我們可以 thanks to ruo 等于 true，儘管我們在打印出底下的情況。

![](img/1251636ef68b09989c270ce30b4b171d_118.png)

我們得到一個關於回歸的查询，這是語義的部分，然後我們得到一個過濾器，我們有一個比較器等于，在來源屬性和一個值 docs 之間。



![](img/1251636ef68b09989c270ce30b4b171d_120.png)

然後這是一個路徑，這是到第三機器學習講座的路徑，所以，这基本上在告诉我们在回归语义空间中进行查找，然后进行过滤，只查看源值为此值的文档。



![](img/1251636ef68b09989c270ce30b4b171d_122.png)

因此，如果我们遍历文档并打印出元数据，我们应该看到它们全部来自第三堂课。

![](img/1251636ef68b09989c270ce30b4b171d_124.png)

确实如此，这是一个例子，在这里，自我查询检索器可以用于精确过滤元数据。

![](img/1251636ef68b09989c270ce30b4b171d_126.png)

我们可以讨论的最后一种检索技术是上下文压缩，因此，让我们在这里加载一些相关的模块。

![](img/1251636ef68b09989c270ce30b4b171d_128.png)

上下文压缩检索器，然后一个llm链提取器。

![](img/1251636ef68b09989c270ce30b4b171d_130.png)

这将做什么，这将从每个文档中提取只有有关的部分。

![](img/1251636ef68b09989c270ce30b4b171d_132.png)

然后将这些作为最终返回响应，我们将定义一个漂亮的小函数来打印出文档。

![](img/1251636ef68b09989c270ce30b4b171d_134.png)

因为它们往往很长且困惑，这将使事情更容易看到。

![](img/1251636ef68b09989c270ce30b4b171d_136.png)

我们可以然后创建一个压缩器与llm链提取器。

![](img/1251636ef68b09989c270ce30b4b171d_138.png)

然后我们可以创建上下文压缩检索器，输入压缩器。

![](img/1251636ef68b09989c270ce30b4b171d_140.png)

然后向量存储的基础检索器，现在我们输入问题，他们对matlab说了什么。

![](img/1251636ef68b09989c270ce30b4b171d_142.png)

我们查看压缩的文档，如果我们查看返回的文档，我们可以看到两件事，一，它们比正常文档短得多，但二我们还有一些重复的东西在进行，这是因为我们底层使用了语义搜索算法。

这就是我们通过本课程前面的mmr解决这个问题的方式，这是一个你可以结合各种技术以获得最佳可能结果的好例子，为了做到这一点，当我们从向量数据库创建检索器时，我们可以将搜索类型设置为mr。

然后我们可以重新运行这个，并看到我们返回，一个不含有任何重复信息的过滤结果集，到目前为止，我们所提到的所有附加检索技术都建立在向量数据库之上。



![](img/1251636ef68b09989c270ce30b4b171d_144.png)

值得注意的是，还有其他类型的检索方法根本不使用向量数据库。

![](img/1251636ef68b09989c270ce30b4b171d_146.png)

而是使用其他，更传统的nlp技术。

![](img/1251636ef68b09989c270ce30b4b171d_148.png)

在这里，我们将重新创建一个检索管道，使用两种不同类型的检索器，和一个svm检索器和一个tf-idf检索器。



![](img/1251636ef68b09989c270ce30b4b171d_150.png)

如果你从传统的nlp或传统的机器学习中认识这些术语，那太好了。

![](img/1251636ef68b09989c270ce30b4b171d_152.png)

如果你不，这也没关系，这只是一些其他技术在外面的例子。

![](img/1251636ef68b09989c270ce30b4b171d_154.png)

除了这些，还有更多的，我鼓励你去检查一下，其中的一些。

![](img/1251636ef68b09989c270ce30b4b171d_156.png)

我们可以很快地运行加载和分割的常规管道。

![](img/1251636ef68b09989c270ce30b4b171d_158.png)

然后这两个检索器都暴露了一个从文本方法，一个接受嵌入模块的检索器是svm检索器，tf idf检索器只是直接接受分割。



![](img/1251636ef68b09989c270ce30b4b171d_160.png)

现在我们可以使用其他检索器，让我们传递，他们对matlab说了什么，给svm检索器，我们可以查看最顶部的文档，我们回来可以看到它提到了很多关于matlab的东西，所以它在那里取得了一些好的结果。

我们也可以尝试在tf idf检索器上这样做，我们可以看到结果看起来稍微差一些，现在是个好时机，停下来尝试所有这些不同的回收技术，我想你会注意到一些比其他的好在一些事情上。

所以我鼓励你尝试在各种问题上进行广泛的测试，特别是自查询检索器，这是我最喜欢的，所以我建议你尝试使用更多的复杂元数据过滤，甚至可能创建一些没有实际意义的元数据，其中包含嵌套的元数据结构。

你可以尝试让llm推断，我认为这非常有趣，我认为这是一些更先进的东西，所以我非常兴奋能够与你分享它，现在我们已经谈论了检索，我们将谈论这个过程的下一步。

