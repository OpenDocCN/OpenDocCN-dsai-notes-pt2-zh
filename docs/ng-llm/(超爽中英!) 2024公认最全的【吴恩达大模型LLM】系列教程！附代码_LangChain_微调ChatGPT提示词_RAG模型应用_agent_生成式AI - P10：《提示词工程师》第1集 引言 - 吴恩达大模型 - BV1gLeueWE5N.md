# (超爽中英!) 2024公认最全的【吴恩达大模型LLM】系列教程！附代码_LangChain_微调ChatGPT提示词_RAG模型应用_agent_生成式AI - P10：《提示词工程师》第1集 引言 - 吴恩达大模型 - BV1gLeueWE5N

![](img/e6ae0b5f68151c012afc4b8ea40ba2ff_0.png)

欢迎参加聊天课程，Gpt 工程师为开发者，我很高兴有你，教这个是一个弃权，和我一起，她是OpenAI技术团队的成员，并构建了流行的Chad GPT检索插件，并且大部分工作是在教人们。

如何在产品中使用lm或大型语言模型技术，她也是OpenAI食谱的贡献者，该食谱教人们，提示，很高兴有你，我也很高兴在这里，并与您分享一些提示最佳实践。



![](img/e6ae0b5f68151c012afc4b8ea40ba2ff_2.png)

互联网上有很多关于提示的材料，如30个提示，很多人必须知道很多，这已经集中在Chat GPT网页用户界面，许多人正在使用它来完成特定和通常一次性的任务，但我认为。

作为开发人员使用lm的大型语言模型的力量，即使用API调用lm来快速构建软件应用程序，我认为这仍然被严重低估，事实上，我的团队在AI基金，这是深度学习的姊妹公司，AI一直在与许多初创公司合作。

将这些技术应用于许多不同的应用程序，并且很高兴看到什么，Lm API可以启用开发人员快速构建，因此，在这门课程中，我们将与您分享您可以做的可能性，以及您可以如何做的最佳实践，有很多材料要涵盖首先。

您将学习一些软件开发的提示最佳实践，然后我们将涵盖一些常见用例，总结，推断，转换，扩展，然后您将使用llm构建聊天机器人，我们希望这将激发您对您可以构建的新应用程序的想象力，在大型语言模型的开发中。

或称为lms，有两种广泛类型的lms，我将称之为基础lms和指令调优lms，因此，基础lms被训练来预测下一个单词，基于文本训练数据，通常在互联网上的大量数据和其他来源上进行训练。

以弄清楚下一个最可能的单词是什么，所以，例如，如果您提示这个从前有一次有一个独角兽，它可能会完成这个，也就是说，它可能会预测接下来的几个单词是住在魔法森林里，与所有独角兽朋友。

但如果你提示这个法国首都是什么，那么基于互联网上的文章可能，非常有可能基础lms会完成这个法国最大的城市是什么，法国人口多少，等等，因网文可能为问答列表，关于法国，相比之下，指令调优的LM。

这是LM研究和实践的主要动力，指令调优已学会遵循指令，若你问它，法国首都是哪，更可能输出法国首都巴黎，指令调优OMM通常这样训练，是，从大量文本数据训练的基LM开始，进一步训练它以微调。

用指令和遵循这些指令的良好尝试进行微调，然后通常使用称为RLHF的技术进一步细化，从人类反馈中学习，使系统更能提供帮助并遵循指令，因为指令到LM已被训练以提供帮助，诚实且无害，例如。

它们不太可能输出如毒性输出等问题文本，相比基础模型，许多实际使用场景已转向指导LMS，您在互联网上找到的一些最佳实践可能更适合基础模型，但对于当今大多数实际应用。

我们建议大多数人反而应关注指导调优的LMS，它们更容易使用，也因为OpenAI和其他LM公司的努力，正变得更加安全和一致，因此本课程将专注于指导调优LMS的最佳实践，推荐您在继续前使用这些。

感谢openai和深度学习AI团队，对材料做出贡献，非常感激安德鲁，缅因州，乔，波默罗，鲍里斯，力量，与Ted中心合作，住在open ai，头脑风暴材料，审核材料，编制短期课程，我也感谢深度学习方面。

杰夫的工作，路德维希，埃迪舒和汤米尼尔森。

![](img/e6ae0b5f68151c012afc4b8ea40ba2ff_4.png)

所以当你使用指令调优的LM时，想象给另一个人下指令，比如一个聪明的人，但不知道你任务的细节，所以当LM不起作用时，有时指导不够清晰，例如，如果你说，请写一些关于艾伦·图灵的内容，此外。

明确是否希望文本专注于他的科学工作，还是个人生活，或他在历史上的作用或其他，如果你指定文本的语气，应该是专业记者会写的那种，还是更像给朋友的随意便条，这有助于生成你想要的内容，当然，如果你想象自己问。

比如一个刚毕业的大学生来完成这项任务，如果你甚至能指定他们应该提前阅读的文本片段，来写关于艾伦·图灵的文章，那甚至更好地为这位大学生设定了成功完成这项任务的条件，所以在下一个视频中。

你会看到如何清晰明确，这是提示大型语言模型的重要原则，你还会从第二个提示原则中学习，那就是给大型语言模型时间思考，所以。

