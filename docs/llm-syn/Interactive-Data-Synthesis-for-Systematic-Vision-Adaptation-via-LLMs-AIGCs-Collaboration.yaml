- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:35'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12799](https://ar5iv.labs.arxiv.org/html/2305.12799)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Qifan Yu¹¹¹1Equal Contribution.   Juncheng Li¹¹¹1Equal Contribution.   Wentao
    Ye¹   Siliang Tang¹   Yueting Zhuang¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: '{yuqifan, junchengli, 22121058, siliang, yzhuang}@zju.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent text-to-image generation models have shown promising results in generating
    high-fidelity photo-realistic images. In parallel, the problem of data scarcity
    has brought a growing interest in employing AIGC technology for high-quality data
    expansion. However, this paradigm requires well-designed prompt engineering that
    cost-less data expansion and labeling remain under-explored. Inspired by LLM’s
    powerful capability in task guidance, we propose a new paradigm of annotated data
    expansion named as ChatGenImage. The core idea behind it is to leverage the complementary
    strengths of diverse models to establish a highly effective and user-friendly
    pipeline for interactive data augmentation. In this work, we extensively study
    how LLMs communicate with AIGC model to achieve more controllable image generation
    and make the first attempt to collaborate them for automatic data augmentation
    for a variety of downstream tasks. Finally, we present fascinating results obtained
    from our ChatGenImage framework and demonstrate the powerful potential of our
    synthetic data for systematic vision adaptation. Our codes are available at [https://github.com/Yuqifan1117/Labal-Anything-Pipeline](https://github.com/Yuqifan1117/Labal-Anything-Pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past decade, deep learning techniques have demonstrated promising performance
    across diverse tasks, owing to the availability of large-scale annotated data [[12](#bib.bib12),
    [19](#bib.bib19), [10](#bib.bib10)]. However, it is time-consuming and expensive
    to manually collect a large-scale annotated dataset containing every possible
    domain for robust training. Besides, the problem of cross-domain and long-tail
    distributions within existing datasets have a detrimental effect on the performance
    and robustness of vision models, thereby impeding their generalization ability
    to novel categories or unseen domains. This promotes us to explore a less labor-intensive
    way to harvest labeled data containing multiple domains in one step for robust
    vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One effective strategy to improve generalization and robustness is to enlarge
    the scale of training data by intricate augmentations [[14](#bib.bib14)]. There
    are several GAN-based models [[7](#bib.bib7), [17](#bib.bib17)] generating images
    for vision tasks, but their applicability remains constrained by their narrow
    focus on specific settings or small scales. Recently, AIGC models[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)] have emerged as promising candidates for generating
    high-quality synthetic data, with the ability to address the limitations of the
    existing dataset. There are several early attempts at exploring synthetic data
    from generative models for data augmentation [[13](#bib.bib13), [23](#bib.bib23),
    [3](#bib.bib3), [37](#bib.bib37)]. Albeit promising, early works usually produce
    simple scenarios or object-centric images only by global constraints (i.e., “airplane"
    or “a white airplane hovering over a beach and a city".), which limits downstream
    models’ perception of intricate scenes and fine-grained attributes. Additionally,
    these methods concentrate on generating images under typical scenarios (e.g.,
    daylight, field), while neglecting less common but predictable circumstances (e.g.,
    snow, forest, night). This limitation may impede the ability of deep learning
    models to generalize when deployed in real-world environments that exhibit unseen
    test distributions.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we present a novel approach named ChatGenImage that facilitates
    more controllabel data augmentation. ChatGenImage harnesses the collaborative
    power of the LLM and AIGC models, enabling iterative communication between them
    in a cost-effective and controllable manner. This automatically iterative process
    facilitates the generation of high-quality synthetic images depicting complex
    scenes and diverse domains, along with fine-grained annotations. Our fundamental
    intuition is that large language models have remarkable capabilities to perform
    new tasks in a zero-shot manner when presented with well-crafted instruction prompts[[34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [11](#bib.bib11)]. We discover that these
    LLMs like ChatGPT possess the capability to autonomously navigate image editing
    processes. By strategically designing appropriate prompts, LLMs can leverage the
    inherent knowledge within the system and effectively guide the AIGC models to
    produce highly controllable and intricate images. While ChatGPT contains diverse
    world knowledge for simulating the human brain’s efficient processing, it is non-trival
    to elicit this knowledge from it for data augmentation with automatic labeling
    because ChatGPT is a pure language model that lacks the ability to visually perceive
    any information. We explore this issue in the context of generative data augmentation,
    showing that language can act as a bridge connecting LLMs and AIGC models, producing
    elaborate images for downstream tasks by globally controllable prompts and iteratively
    local editing instructions.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we demonstrate three key findings. First, we find that the LLM
    such as ChatGPT contains a wealth of conceptual knowledge and can imagine vivid
    descriptions even with only one label word (e.g. A dog playing in a lush green
    park, with a frisbee in its mouth. The dog should have a shiny coat of fur.) [[33](#bib.bib33),
    [6](#bib.bib6)]. We further obverse that the existing AIGC models can only generate
    simple image with few objects and backgrounds, which are not diverse for domain
    generalization [[20](#bib.bib20)]. Thus, we establish the iterative pipeline to
    repair missing details and refine generated images with the help of label foundation
    toolkits and local editing prompts. Finally, we demonstrate our method flow to
    produce large amounts of high-quality synthetic data with fine-grained labels
    in a scalable manner for data augmentation in data scarcity scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea7fa710666718ed49448995d640d64f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Language as a bridge for LLMs (e.g. ChatGPT) and AIGC models (e.g.
    Stable Diffusion) can iteratively control image generation and automatic labeling.
    The LLM first generates global prompts to guide AIGC models in generating initialization
    images, then iteratively refines them using automatically generated fine-grained
    annotations as local constraint prompts to produce diverse and complex scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, the field of natural language processing has been revolutionized
    by the emergence of large language models (LLMs), exemplified by models such as
    PaLM [[8](#bib.bib8)], ChatGPT, and LLaMa [[33](#bib.bib33)]. Moreover, the remarkable
    performance of LLMs in zero-shot and few-shot generalization has sparked a growing
    trend in utilizing autoregressive language models for vision-language tasks [[21](#bib.bib21),
    [26](#bib.bib26)]. However, the generalization of LLMs does not translate well
    to visual tasks[[20](#bib.bib20), [9](#bib.bib9)]. Unlike previous works, we utilize
    LLMs to enrich training data for fine-tuning in downstream tasks instead of directly
    transferring by contrastive learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Text-to-Image Diffusion Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, diffusion models have become a promising generative modeling framework,
    achieving state-of-the-art performance on image generation tasks [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)]. GLIDE [[25](#bib.bib25)] studies diffusion
    models for the text-conditional image synthesis by classifier-free guidance strategies.
    InstructPix2Pix [[5](#bib.bib5)] proposes a effective framework to edit images
    with human instructions, which opens up new opportunities for controllable image
    creation by user-written instructions. However, existing SOTA text-to-image models
    require longer and more complex prompts to yield impressive outcomes, which is
    less user-friendly. Thus, we provide a powerful and user-friendly pipeline to
    generate more elaborate images through iterative refinement with the aid of large
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Synthetic Data for Visual Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, there has been an increasing interest in using high-quality synthetic
    data to augment training data for downstream tasks [[23](#bib.bib23), [13](#bib.bib13),
    [4](#bib.bib4), [3](#bib.bib3)]. PET [[31](#bib.bib31)] primarily focuses on a
    semi-supervised situation to automatically generate abundant labeled data for
    augmentation. [[13](#bib.bib13)] use GLIDE to generate abundant class-conditioned
    images and explore the effectiveness of synthetic data for image recognition tasks
    in data-scarce settings. For the task of few-shot object detection, a method proposed
    in [[23](#bib.bib23)] involves selecting representative samples from a large-scale
    synthetic dataset to potentially enhance the performance of FSOD models. Here
    we present a novel approach for generating high-quality synthetic data by leveraging
    state-of-the-art text-to-image models and LLMs. Our method eliminates the need
    for expensive prompt engineering by introducing a unified framework that produces
    abundant and elaborate images with annotations in a single pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ChatGenImage is a labeling collaboration framework that involves a mentor large
    language model (LLM) and numerous AIGC models as controllable image creators,
    and labeling foundation models for executing the labeling task. The workflow of
    Label Anything consists of two stages: Language Enhancement Image Initialization
    and Iteratively Local Refinement and Labeling, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Interactive Data Synthesis for Systematic Vision Adaptation
    via LLMs-AIGCs Collaboration"). 1) During the first stage, An LLM (e.g., ChatGPT)
    analyze the label word from the user input, and generate complex scene descriptions
    and object-centric descriptions in Global Prompts Brainstorming. Then, AIGC generators
    initialize controllable images based on the global constraints from the LLM. 2)
    During the second stage, the LLM produces Label Editing Prompts based on the high-quality
    pseudo labels automatically obtained from the Label Foundation Toolkit and employs
    them to iteratively control the process of local image editing. Based on that,
    AIGC models can perform Controllable Image Editing from both background and foreground
    to obtain more diversified synthetic images. Through our approach, the generated
    images are modified to align with the complex annotations, resulting in high-quality
    synthetic data suitable for data augmentation in downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Global Prompts Brainstorming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the limited knowledge of the large language model about the AIGC model,
    it is not capable of providing appropriate prompts for the AIGC model. Therefore,
    ChatGenImage utilizes a hybrid approach that combines both specification-based
    instruction and demonstration-based learning to effectively guide the AIGC model
    in generating high-quality synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Specification-based Instruction. The prompt specification can serve as a standard
    template for large language models to comprehend visual attributes of the specific
    concept, thereby facilitating the sensible scene imagination for a given word
    through slot filling. However, using category names alone in AIGC models may limit
    their ability to perceive visual features, leading to ambiguous image generation[[13](#bib.bib13)].
    To help the large language model imagine effective scene descriptions, ChatGenImage
    prompts focus on descriptive features rather than broad categories. In the first
    stage of ChatGenImage, the large language model take the Label Word from the user
    and construct several relevant descriptions as its Visual Feature for global prompt
    brainstorming. Moreover, we propose to automatically obtain appropriate visual
    features by prompting the LLM to describe the visual features that distinguish
    that category in a photograph. We demonstrate the prompt process for visual feature
    descriptions and controllable generation in Table [1](#S3.T1 "Table 1 ‣ 3.1 Global
    Prompts Brainstorming ‣ 3 Method ‣ Interactive Data Synthesis for Systematic Vision
    Adaptation via LLMs-AIGCs Collaboration").
  prefs: []
  type: TYPE_NORMAL
- en: Demonstration-based learning. ChatGenImage utilizes the in-context learning
    capability of LLMs and injects several demonstrations into the prompt learning,
    helping large language models to better understand the parameter criteria for
    conditional image generation. Each demonstration is a group of input and output
    on scene prompts brainstorming——the user’s request in standard templates and the
    expected image descriptions for AIGC models. Furthermore, these demonstrations
    consist of complex scene descriptions and object-centric descriptions, as shown
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Interactive Data Synthesis for
    Systematic Vision Adaptation via LLMs-AIGCs Collaboration"), effectively aid ChatGenImage
    in understanding the given label’s attributes in various environments and imagining
    reasonable prompts for high-quality image synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: '| Visual Descriptor | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| "role": "system", "content": "You are an expert in the field of vision and
    graphics, please fully consider the input concept or topic, give me the most important
    fine-grained visual features of the input concept or category based on the Wikipedia.
    Only give me several phrases or keywords as more as possible." "role": "user",
    "content": "Q: What are useful visual features for distinguishing a {category
    name} in a photo? A: There are several useful visual features to tell there is
    a {category name} in a photo:" |'
  prefs: []
  type: TYPE_TB
- en: '| AIGC Creator | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| "role": "system", "content": "The AI assistant is a professional data specialist.
    The AI assistant can imagine different real scenes according to the input caption
    and return the background and closely related foreground objects in a specific
    format. Note that the background should be described with a single word or phrase,
    and each background contains five main objects." {Demonstration} "role": "user",
    "content": "Give me {prompt number} high quality prompts for text-to-image models
    about the amazing close-up realistic scene containing the {label word}. Scene
    prompts should also contain other objects as more as possible. The {label word}
    is with {visual feature}." |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration |'
  prefs: []
  type: TYPE_TB
- en: '| Sure, here are 1 high quality prompts for text-to-image models about the
    amazing close-up realistic scene containing the red panda: Bring a photo-realistic
    close-up scene of a red panda to life, set against a backdrop of a rustic temple
    in a mountainous region. The panda should be situated in front of the temple,
    with its round ears, short legs, and bushy tail with alternating red and white
    rings visible. The temple should have ornate carvings and architecture, with stone
    lanterns and a small pond nearby. The panda’s white paws with black claws should
    be visible as it sits atop a stone pedestal, perhaps with some cherry blossom
    petals scattered around it. The overall scene should have a peaceful and serene
    atmosphere. |'
  prefs: []
  type: TYPE_TB
- en: '| Visual Feature |'
  prefs: []
  type: TYPE_TB
- en: '| Label word: "Lemur" | - Large, forward-facing eyes with binocular vision;
    Soft, thick fur covering the body - Long, bushy tail; Striking coloration patterns
    such as black and white rings - Slender fingers and toes with opposable thumbs
    for grasping and climbing; Prominent ears that can be pointed or rounded with
    tufts of fur - Wet, reflective nose; Rounded head with a shortened snout and large
    ears - Relatively small body size, typically weighing between 2 and 5 kilograms;
    Distinctive vocalization or call that can vary between species and subspecies
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scene Imagination | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| "role": "system", "content": "The AI assistant is a professional data specialist.
    The AI assistant can imagine different real scenes according to the input caption
    and return the background and closely related foreground objects in a specific
    format. Note that the background should be described with a single word or phrase,
    and each background contains five main objects." "role": "user", "content": "Give
    me scene number real scene descriptions based on the context caption. The scene
    objects should consist exist objects, and also contain five additional objects
    associated with the background. Each scene description is a complex sentence containing
    the above objects. Return the result in the following format: ’background’:[],
    ’objects’:[], ’description’:[]. Only return the result." |'
  prefs: []
  type: TYPE_TB
- en: '| Box Candidates Generation | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| "Please make {prompt number} possible prediction of the remaining box coordinates
    with different box size based on the dense description "{caption}". Note that
    the image size is (512,512), and the existing box coordinates are [{existing boxes
    info}]. Based on the layout of the objects, predict the possible number reasonable
    box coordinates of the following objects {target objects}. The size of the {target
    objects} box should be based on the category and the size of other object boxes,
    and the width and height of the box should be greater than 75 and less than 300\.
    Only return each result in the following format: "label":, "box":, "relationship":"
    |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration |'
  prefs: []
  type: TYPE_TB
- en: '| "{caption}": ’there is a dog sitting on a bench in a field.’ "{existing boxes
    info}": "value": 1, "label": "bench", "logit": 0.84, "box": [33.93, 224.34, 463.20,
    491.01], "value": 2, "label": "dog", "logit": 0.43, "box": [175.71, 116.29, 311.58,
    367.13] Return Results: "label": ’cat’, "box": [343.23, 176.29, 467.23, 353.13],
    "relationship": ’sitting next to the dog.’ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The details of the prompt design in ChatGenImage. There are injectable
    slots in the prompts, such as Caption, Visual Feature, and Existing Boxes Info.
    These slots imply visual perceptions that help LLM building multimodal awareness
    and are uniformly replaced with the contents from visual foundation models before
    being fed into the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Label Foundation Toolkit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since ChatGPT is a pure language model and cannot “see” any visual information,
    we present the initialized images to several powerful label foundation toolkits (i.e.,
    Segment Anything Model [[18](#bib.bib18)], Grounding DINO [[24](#bib.bib24)],
    and BLIP2 [[22](#bib.bib22)]) and serve them as sensors in the system to provide
    perceptual information to the ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Segment Anything Model (SAM) is a large ViT-based model trained on the large
    visual corpus (SA-1B) [[18](#bib.bib18)], which has demonstrated promising zero-shot
    segmentation capabilities in various scenarios and the great potential for data
    labeling in visual tasks. But it needs precise prompts (like boxes/points) to
    generate accurate masks and lacks category predictions or annotations for each
    mask.
  prefs: []
  type: TYPE_NORMAL
- en: Grounding DINO is a strong zero-shot detector which is capable of to generate
    high quality boxes and labels with free-form text [[24](#bib.bib24)], which can
    also serves as box prompts generator for SAM. Our approach combines the strengths
    of Grounding DINO and SAM to detect and segment comprehensive regions in each
    synthetic image. This builds a powerful pipeline for complex visual scene labeling
    and produces abundant fine-grained pseudo labels for training.
  prefs: []
  type: TYPE_NORMAL
- en: BLIP2 is a language-vision model that seamlessly integrates visual input into
    text sequences to facilitate overall visual perception of LLMs [[22](#bib.bib22)].
    By combining BLIP2 with the aforementioned visual models, our approach can automatically
    generate high-quality text descriptions for synthetic images. The LLMs then use
    these descriptions to understand image regions and return local editing prompts
    for controllable and diverse image refinement.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Local Editing Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the ability of AIGC models to generate labeled images through prompt-based
    techniques with the aid of LLMs, their effectiveness in depicting intricate scenes
    and fine-grained attributes remains limited due to their inclination to create
    object-centric images with only global constraints. Besides, we observe that the
    generated images contain fewer objects, which poses a challenge in constructing
    complex scenes for demanding downstream tasks (e.g., scene graph generation and
    visual question answering). Thus we further introduce local editing prompt in
    the iterative pipeline for fine-grained image refinement.
  prefs: []
  type: TYPE_NORMAL
- en: In detail, we design a iterative communication that encourages ChatGPT to provide
    a series of informative feedbacks based on the generated images from AIGC models
    and corresponding labels from label foundation toolkits. Since language models
    are blind to the initialized image, ChatGPT cannot partially edited the initial
    image directly. Hence, we employ a predefined prompt template and populate the
    slots with the corresponding caption and object box coordinates identified by
    the visual foundation models. This template is subsequently utilized by the LLMs
    to produce novel scenes that comprise new backgrounds and additional objects.
    It is worth noting that ChatGPT can voluntarily select a reasonable location for
    editing based on human instructions and its underlying knowledge, autonomously
    generating accurate local editing prompts. Then the AIGC model use the resulting
    prompts to edit the images and improve their quality by adding missing details.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 ChatGenImage Pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Initialization: Label $w_{0}$= Filling$(I_{0},{\rm Scene}[^{\prime}objects^{\prime}])$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Controllable Image Editing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To efficiently generate a significant amount of images with complex scenes and
    rich annotations in a low-resource manner, It is necessary to collaborate with
    various controllable editing models that can perform controllable image editing
    based on both global and local prompts. The total process is shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.3 Local Editing Prompt ‣ 3 Method ‣ Interactive Data Synthesis
    for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"). Besides, we use
    image filtering rules to figure out those representative samples as valid results
    and utilize the label foundation toolkit to get high-quality annotations for downstream
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Background Imagination. We notice that retrieved or directly generated images
    are usually restricted to to a single domain, thereby leading to a constraint
    in the development of robust visual models [[9](#bib.bib9)]. Furthermore, it is
    impractical to obtain labeled data for all possible anticipated settings at once
    is often impractical due to the significant expense involved. However, acquiring
    linguistic knowledge of the anticipated domain shift is a more cost-effective
    and accessible approach.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we leverage the ChatGPT to generate novel backgrounds for the original
    image and employ InstructPix2Pix [[5](#bib.bib5)] to substitute different backgrounds,
    generating a vast collection of composite images across various domains in a cost-effective
    manner. To preserve the foreground semantics of images after background modification,
    we perform target detection on both the original and modified images, and apply
    filter rules to exclude images with missing objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Foreground Object Filling. To avoid altering the semantic information of the
    source image, we propose a method to increase the complexity of the image scene
    by filling foreground objects. The necessary object labels, coordinates, and their
    interactions with the scene (i.e., {label: ‘rocks’, box: [200, 50, 300, 150],
    relationship: ‘near the cabin and surrounded by trees’}) can be obtained by filtering
    the local editing prompts automatically. Once collect sufficient possible boxes
    through ChatGPT, we can use Blended Latent Diffusion [[2](#bib.bib2), [1](#bib.bib1)]
    to fill novel objects in the specific position of the foreground. It is worth
    noting that we filter out the overlapping bounding boxes to ensure that the original
    semantics are preserved. In this way, we greatly enrich the spatial interaction
    of objects in the generated image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04672a06c1b79e4e47c3a72e9b3039f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Visualization results of Iteratively Local Refinement and Labeling.
    It contains three steps: 1) Background Imagination, 2) Iteratively Object Filling,
    3) Label anything in the image via visual foundation models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Qualitative analysis of object-centric image generation and complex
    scene description with multiple background and objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c96cc6e5367168cc16aa4c4524f1024d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Visualization results of ChatGenImage for object-centric image generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Image Filtering Rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Though Most state-of-the-art AIGC methods generate astonishing images, they
    may have several visual and textual issues: 1) boundary incoherence, 2) semantic
    mismatch and 3) object missing. It is essential to establish robust image filtering
    rules that can effectively evaluate the synthetic images and filter out those
    low-quality results. To address above challenges, we introduce a Pixel Checking (PC)
    and a Semantic Checking (SC) strategy for the generated images from the perspective
    of visual pixels and textual semantics.'
  prefs: []
  type: TYPE_NORMAL
- en: Pixel Checking. To ensure the boundary consistency of the edited image, we evaluate
    the fidelity of the generated image. IS [[30](#bib.bib30)], FID [[15](#bib.bib15)]
    SceneFID[[32](#bib.bib32)] are common metrics to evaluate the fidelity of general
    images in different scales. However, all of these metrics rely on ground truth
    labels, which are not suitable for assessing images generated by stable diffusion
    models [[27](#bib.bib27)]. Therefore, we exploit the SSIM and PSNR [[16](#bib.bib16)]
    to explore structural similarity and pixel similarity between the locally edited
    image and the original image for pixel checking. We employ a threshold strategy
    of PSNR and SSIM between the original and edited images, minimizing artifacts
    and incoherence at the editing boundary to preserve the global coherence of the
    image during the local editing process.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Checking. Considering that local image editing may introduce undesired
    items to destroy the semantics of the original image, we evaluate the semantics
    and object detection of the generated image during semantic checking. Specifically,
    we generate a set of image candidates based on scene descriptions of specific
    label words during both global initialization and local image editing. Then, we
    use the CLIP similarity score [[26](#bib.bib26)] to evaluate the semantic alignment
    between the image candidates and textual constraints. We rank the image candidates
    based on the score and filter out the low-confidence images to obtain most matching
    ones as the final result. Besides, as the we employ open vocabulary object detection
    on the images after background editing. We only retain those images that can identify
    the novel background and original foreground objects to keep the original semantics
    and enhance the downstream utility of the edited images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/60156ea3d78587da8f296c85e9f5a9aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visualization results of ChatGenImage for complex scene imagination.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this version, we discuss our main experiments setup and several results.
    We demonstrate the effectiveness of our ChatGenImage on interactive data synthesis
    through qualitative results, showing the potential of using ChatGenImage for systematic
    vision adaptation. In the next release, we will further explore how to better
    leverage the synthetic data obtained from our ChatGenImage framework for better
    downstream task generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiments, we employed the gpt-3.5-turbo variants of the GPT models
    as the large language models (i.e., ChatGPT), which are publicly accessible through
    the OpenAI API¹¹1[https://platform.openai.com/](https://platform.openai.com/).
    To make the LLM output more stable, we set the decoding temperature to 0\. For
    AIGC models, we uniformly set the pixel of the picture to 512×512 to save the
    memory overhead. Also to adapt to the whole system, we use stable diffusion v1.5
    as the AIGC base model with the same default parameters as the original setting [[28](#bib.bib28)].
    We provide detailed prompts designed for the Visual Descriptor, AIGC Creator,
    Scene Imagination, Box Candidates Generation in the step of Global Prompts Brainstorming
    and Local Editing Prompts in Table [1](#S3.T1 "Table 1 ‣ 3.1 Global Prompts Brainstorming
    ‣ 3 Method ‣ Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs
    Collaboration"), where {variable} indicates that the slot needs to be populated
    with the corresponding text before the prompt can be fed into the LLM. This label
    pipeline is based on a single Nvidia RTX 3090 GPU, which is affordable for most
    people.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Qualitative Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate the generated image and fine-grained annotations in our ChatGenImage
    in two cases (i.e., complex scene description and object-centric image generation).
    In Figures [3](#S3.F3 "Figure 3 ‣ 3.4 Controllable Image Editing ‣ 3 Method ‣
    Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"),
    we show several dialogue demonstrations and qualitative analysis for above two
    different cases of requirement data, respectively. We collect several label words
    of rare and endangered species for testing, which have few photos in the web and
    are unfamiliar to most image classifiers. We compare our approach with LLM descriptions
    and original images generated by naive AIGC models. The result is shown in Figure [4](#S3.F4
    "Figure 4 ‣ 3.4 Controllable Image Editing ‣ 3 Method ‣ Interactive Data Synthesis
    for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"). The experimental
    result indicates that the proposed ChatGenImage is both general and controllable,
    effectively creating robust images even for unfamiliar and rare concepts. Through
    interactive communication with the LLM, the AIGC can learn the specific descriptions
    of novel concepts and complete controllable generation in different domains via
    iterative image editing.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, we explore the effectiveness of the ChatGenImage for complex scene
    descriptions and whether the LLM help AIGC models iteratively fill accurate objects
    in the foreground. In Figure [5](#S3.F5 "Figure 5 ‣ 3.5 Image Filtering Rules
    ‣ 3 Method ‣ Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs
    Collaboration"), we show that the LLM provides several information comprising
    of object coordinates and relation interactions, which is then used by Stable
    Diffusion to generate diverse backgrounds (e.g., mountain, forest) and incorporate
    relevant objects (e.g., snow trees, stream and rocks) to produce a rich image
    depicting a complex scene.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ChatGenImage, as an interactive data synthesis framework, provides two generation
    modes and fine-grained images with various domains to easily meet the requirements
    of different field. Moreover, the usage of synthetic data from ChatGenImage enables
    more generalizable of downstream tasks in complex application scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Since ChatGenImage can iteratively generate large amount of diverse images via
    LLM-AIGC collaboration, it can provide extra unseen data domains for systematic
    vision adapation. We will evaluate ChatGenImgae on several domain adaptation benchmarks
    to investigate how to enrich the existing dataset with synthetic data and construct
    adaptive visual perception system in a cost-less manner.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGenImage is a versatile tool that combines the capabilities of LLMs, AIGC
    models and powerful label foundation toolkits. Based on the collaboration of diverse
    models, ChatGenImage enables to generate fine-grained images with rich annotations
    for data augmentation. In the future, we will further develop our approach to
    support more complex and challengeable scenarios, like fine-grained human-object
    interaction, action editing, etc., and apply it to more realistic applications
    for systematic vision adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion.
    arXiv preprint arXiv:2206.02779, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven
    editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 18208–18218, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust
    classification via generated datasets. arXiv preprint arXiv:2302.02503, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, and Stéphane
    Lathuilière. One-shot unsupervised domain adaptation with personalized diffusion
    models. arXiv preprint arXiv:2303.18080, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning
    to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Jaehoon Choi, Taekyung Kim, and Changick Kim. Self-ensembling with gan-based
    data augmentation for domain adaptation in semantic segmentation. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 6830–6840,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E
    Gonzalez, Aditi Raghunanthan, and Anja Rohrbach. Using language to extend to unseen
    domains. arXiv preprint arXiv:2210.09520, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440–1448, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional
    visual reasoning without training. arXiv preprint arXiv:2211.11559, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr,
    Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for
    image recognition? arXiv preprint arXiv:2210.07574, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang,
    Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many
    faces of robustness: A critical analysis of out-of-distribution generalization.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    8340–8349, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
    and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to
    a local nash equilibrium. Advances in neural information processing systems, 30,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010
    20th international conference on pattern recognition, pages 2366–2369\. IEEE,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative
    models as a data source for multiview representation learning. arXiv preprint
    arXiv:2106.05258, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
    Segment anything. arXiv preprint arXiv:2304.02643, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
    Fine-tuning can distort pretrained features and underperform out-of-distribution.
    arXiv preprint arXiv:2202.10054, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Juncheng Li, XIN HE, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie,
    Yueting Zhuang, Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language
    pre-training. In Advances in Neural Information Processing Systems, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    arXiv preprint arXiv:2301.12597, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao. Explore the power of
    synthetic data on few-shot object detection. arXiv preprint arXiv:2303.13221,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino
    with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
    Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image
    generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In International
    conference on machine learning, pages 8748–8763\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. arXiv preprint
    arXiv:2204.06125, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    Advances in Neural Information Processing Systems, 35:36479–36494, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and Xi Chen. Improved techniques for training gans. Advances in neural information
    processing systems, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small
    language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar
    Sharma. Object-centric image generation from layouts. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 35, pages 2647–2655, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
    Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot
    learners. arXiv preprint arXiv:2109.01652, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le,
    and Denny Zhou. Chain of thought prompting elicits reasoning in large language
    models. arXiv preprint arXiv:2201.11903, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and
    Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. arXiv preprint arXiv:2303.04671, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche,
    Adela Barriuso, Antonio Torralba, and Sanja Fidler. Datasetgan: Efficient labeled
    data factory with minimal human effort. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 10145–10155, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
