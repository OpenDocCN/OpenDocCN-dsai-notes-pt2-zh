- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '\tool: Context-Free LLM Approximation for Guiding Program Synthesis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.15880](https://ar5iv.labs.arxiv.org/html/2405.15880)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shraddha Barke
  prefs: []
  type: TYPE_NORMAL
- en: UC San Diego
  prefs: []
  type: TYPE_NORMAL
- en: San Diego, USA
  prefs: []
  type: TYPE_NORMAL
- en: sbarke@ucsd.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Emmanuel Anaya Gonzalez'
  prefs: []
  type: TYPE_NORMAL
- en: UC San Diego
  prefs: []
  type: TYPE_NORMAL
- en: San Diego, USA
  prefs: []
  type: TYPE_NORMAL
- en: fanayagonzalez@ucsd.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Saketh Ram Kasibatla'
  prefs: []
  type: TYPE_NORMAL
- en: UC San Diego
  prefs: []
  type: TYPE_NORMAL
- en: San Diego, USA
  prefs: []
  type: TYPE_NORMAL
- en: skasibatla@ucsd.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Taylor Berg-Kirkpatrick'
  prefs: []
  type: TYPE_NORMAL
- en: UC San Diego
  prefs: []
  type: TYPE_NORMAL
- en: San Diego, USA
  prefs: []
  type: TYPE_NORMAL
- en: tbergkirkpatrick@ucsd.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Nadia Polikarpova'
  prefs: []
  type: TYPE_NORMAL
- en: UC San Diego
  prefs: []
  type: TYPE_NORMAL
- en: San Diego, USA
  prefs: []
  type: TYPE_NORMAL
- en: npolikarpova@ucsd.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many structured prediction and reasoning tasks can be framed as program synthesis
    problems, where the goal is to generate a program in a *domain-specific language*
    (DSL) that transforms input data into the desired output. Unfortunately, purely
    neural approaches, such as large language models (LLMs), often fail to produce
    fully correct programs in unfamiliar DSLs, while purely symbolic methods based
    on combinatorial search scale poorly to complex problems. Motivated by these limitations,
    we introduce a hybrid approach, where LLM completions for a given task are used
    to learn a task-specific, context-free surrogate model, which is then used to
    guide program synthesis. We evaluate this hybrid approach on three domains, and
    show that it outperforms both unguided search and direct sampling from LLMs, as
    well as existing program synthesizers.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) demonstrate impressive capabilities in various
    domains, but they continue to struggle with tasks that require precision—e.g. structured
    prediction, reasoning, counting, or data transformation—when direct task examples
    are not prevalent in their training data [[38](#bib.bib38), [45](#bib.bib45),
    [12](#bib.bib12), [8](#bib.bib8), [23](#bib.bib23), [40](#bib.bib40), [31](#bib.bib31)].
    As one example, consider the *Abstraction and Reasoning Corpus* (Arc) [[14](#bib.bib14)],
    which was designed as a benchmark for human-like structured reasoning. Arc tasks
    are grid-based puzzles, such as one depicted in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"). This puzzle consists of three training examples, which are pairs
    of input and output grids; the goal is to infer the transformation that maps the
    input to the output, and then apply this transformation to the test grid. The
    Arc benchmark’s emphasis on generalization and few-shot learning has rendered
    it challenging to solve with purely machine learning techniques: state-of-the-art
    generative models like GPT-4 hardly solve more than 10% of the tasks in the dataset
    when asked to predict the test output, even with the help of advanced prompting
    techniques [[25](#bib.bib25)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5b885f8e13c6c66ffbdba1b24b2f4a97.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Arc
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/810e04c85579d0e35e06d0fb43d3b4cb.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Tensor
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51f93defe020204a37f7b2b1d1f31eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) String
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Example problems from the three domains we evaluate \toolon: grid-based
    puzzles (Arc), tensor manipulation (Tensor), and string manipulation (String).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the leading entries in the Arc Kaggle competition [[1](#bib.bib1)]
    tackle this task using *Programming-by-Example* (PBE): instead of predicting the
    output directly, they search for a program that captures the transformation occurring
    in the input-output examples. For example, the transformation in [1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis") might be represented as the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: This particular program is written in a *domain-specific language* (DSL) inspired
    by the Arga tool [[44](#bib.bib44)]. It consists of a single *rule* of the form
    if *filter* then *transform*, which is applied to each object in the grid simultaneously;
    if the filter holds for the focus object self and another object other, then self
    undergoes the transform. In this case, the rule says that any grey object that
    has a neighbor of the grid’s minimum size (here, a single pixel) should be colored
    with the color of that neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond grid puzzles, PBE is a general paradigm for structured reasoning and
    data transformation tasks: for example, it can help spreadsheet users with systematic
    string manipulation [[20](#bib.bib20)], and help programmers use unfamiliar APIs [[18](#bib.bib18),
    [17](#bib.bib17), [36](#bib.bib36)]; [Fig. 1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis") shows
    example PBE tasks from three domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge: Harnessing the Power of LLMs for PBE'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'How can we automatically discover programs from the input-output examples like
    those shown in [Fig. 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")? The traditional *program synthesis*
    approach is based on combinatorial search [[39](#bib.bib39), [2](#bib.bib2), [34](#bib.bib34),
    [7](#bib.bib7), [35](#bib.bib35)], which works well for small programs and restrictive
    DSLs, but becomes infeasible as the program size and the DSL complexity grow.
    At the other end of the spectrum, purely *neural* approaches [[15](#bib.bib15),
    [42](#bib.bib42)] use a neural model to predict the program from input-output
    examples; unfortunately, even state-of-art LLMs like GPT-4o [[33](#bib.bib33)]
    struggle to predict an entire program in an unfamiliar DSL: when we asked GPT-4o
    to generate 10 programs for the running example above, none of them were entirely
    correct.¹¹1A detailed analysis of GPT-4o’s performance on this task is provided
    in [Appendix A](#A1 "Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: In the past, the limitations of both program synthesis and neural techniques
    have motivated a hybrid approach, where combinatorial search is *guided* by a
    learned probabilistic model [[9](#bib.bib9), [24](#bib.bib24), [26](#bib.bib26),
    [32](#bib.bib32), [36](#bib.bib36), [37](#bib.bib37)]. Existing hybrid techniques,
    however, use domain-specific models trained on datasets of similar PBE tasks,
    which limits their generalization to new domains. With the advent of LLMs, can
    we now use a single pre-trained model to guide program synthesis across a wide
    range of domains?
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, there is some tension in the hybrid approach between the efficiency
    of the search algorithm and the power of the model: a search algorithm is efficient
    when it *factorizes the search space* (*i.e.*, merges many search states into
    one), which often makes it incompatible with a powerful model that requires a
    lot of context to make a prediction. Specifically, one of the most widely used
    program synthesis techniques is *bottom-up search* [[2](#bib.bib2), [39](#bib.bib39),
    [11](#bib.bib11), [36](#bib.bib36), [28](#bib.bib28)], which is a dynamic programming
    algorithm, whose efficiency relies on reusing the work of constructing and evaluating
    subprograms in many different contexts. This essentially precludes using models
    with unlimited left-to-right context—like LLMs–to guide bottom-up search.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Solution: Context-Free LLM Approximation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To bridge this gap and harness the power of LLMs to guide bottom-up search,
    we propose to approximate the LLM’s conditional output distribution *for a given
    task* with a context-free surrogate model. Recent work in NLP [[46](#bib.bib46)]
    has found that a Hidden Markov Model (HMM) trained to match an LLM can be used
    as an efficient surrogate in style-controlled language generation. We extend this
    idea to program synthesis, replacing the HMM with a *probabilistic context-free
    grammar* (PCFG). The benefits of using a PCFG are twofold: (1) PCFGs are context-free,
    which makes them compatible with bottom-up search for PBE [[11](#bib.bib11), [36](#bib.bib36)],
    and (2) while a context-free model may make a poor approximation to an LLM’s full
    joint, in a PBE setting it is able to reasonably approximate an LLM’s conditional
    distribution over output programs *for a given prompt*. The overview of our approach
    is shown in [Fig. 2](#S1.F2 "Figure 2 ‣ Our Solution: Context-Free LLM Approximation
    ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e20e29e00396ce3f5fc27a3da8b779d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the hybrid program synthesis technique that uses a
    context-free LLM approximation. Programs generated by an LLM are used to learn
    a PCFG, which guides a bottom-up synthesizer to generate programs until a solution
    is found.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We implemented this technique in a tool \tool²²2The name stands for “HYbrid
    SYNTHesis” and is pronounced like the flower “hyacinth”. and evaluated it on 299
    PBE tasks from three domains: Arc grid-based puzzles [[14](#bib.bib14)], tensor
    manipulation tasks from TFCoder [[36](#bib.bib36)], and string manipulation tasks
    from the SyGuS benchmark [[5](#bib.bib5)], which are inspired by spreadsheet use
    cases. Example problems from these domains are shown in [Fig. 1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"). Our evaluation shows that \tooloutperforms both unguided search and
    LLMs alone, solving 58% of the tasks overall, compared to 40% for unguided search
    and 2% for LLMs without search. Our tool also outperforms baseline program synthesizers
    for these domains—Arga, TFCoder, and Probe, respectively; importantly, in the
    Tensor domain, the guidance from the LLM not only speeds up the search, but also
    frees the user from having to explicitly provide any non-standard *constants*
    that the solution might use, thereby significantly improving the usability of
    the tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Contributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In summary, this paper makes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose a hybrid program synthesis approach that integrates LLMs with efficient
    bottom-up search via a task-specific context-free approximation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We implement this approach in a tool \tooland instantiate it on three domains:
    grid-based puzzles (Arc), tensor manipulation (Tensor), and string manipulation
    (String). While the latter two domains reuse off-the-shelf bottom-up synthesizers,
    for Arc we implement a custom synthesizer that uses a divide-and-conquer strategy [[6](#bib.bib6)]
    to leverage the structure of the rule-based DSL to further speed up the search.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We evaluate \toolon the three domains and show that it outperforms both the
    LLM alone and existing baseline synthesizers, which are not guided by LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Programming-By-Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Programming by Example (PBE) [[21](#bib.bib21)] is the task of synthesizing
    programs that satisfy a given set of input-output examples. To restrict the program
    space, the programs are typically drawn from a *domain-specific language* (DSL),
    which is specified by a *context-free grammar* and an *evaluation function*. This
    section provides a formal definition of these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Context-Free Grammars
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *context-free grammar* (CFG) is a quadruple $\mathcal{G}=(\mathcal{N},\Sigma,\mathcal{S},\mathcal{R})$
    is called (leftmost) *derivation*.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathit{Rule}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Filter}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Transform}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Color}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Size}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Dir}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Obj}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: A fragment from the context-free grammar of our Arc DSL.'
  prefs: []
  type: TYPE_NORMAL
- en: Programs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *program*  $P\in\Sigma^{*}$, which maps the values of program variables to
    its output value.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Statement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A PBE problem is defined by a DSL with a grammar $\mathcal{G}$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Assigning Costs to Programs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Weighted Context-free Grammar
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *weighted context-free grammar* (WCFG) $\mathcal{G}_{w}$.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of search, it is convenient to define a *discrete weight* function
    $w:\mathcal{R}\rightarrow\mathbb{Z}^{+}$.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic Context-free Grammar
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A popular way to assign weights to production rules is via a *probabilistic
    context-free grammar* (PCFG). A PCFG $\mathcal{G}_{p}$.
  prefs: []
  type: TYPE_NORMAL
- en: Given a PCFG $(\mathcal{G},p)$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Bottom-up Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithm 1 Bottom-Up Search Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input-output examples $\mathcal{E}$ $P$ non-terminals18:         for $(c_{1},\dots,c_{k})\in\left\{\,[1..\textsc{Lvl}-1]^{k}\,\middle|\,\sum
    c_{i}=\textsc{Lvl}-w(\textsc{R})\,\right\}$ Substitute subexpressions into R’s
    RHS
  prefs: []
  type: TYPE_NORMAL
- en: 'Bottom-up search is a popular search technique in program synthesis [[2](#bib.bib2),
    [39](#bib.bib39), [11](#bib.bib11), [36](#bib.bib36), [28](#bib.bib28)], which
    enumerates programs from the DSL in the order of increasing costs until it finds
    a program that satisfies the given examples. The search is implemented as a dynamic
    programming algorithm (see [Alg. 1](#alg1 "Algorithm 1 ‣ 2.3 Bottom-up Search
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")), which maintains a program
    *bank*  B mapping discrete costs to programs of that cost. Starting with an empty
    bank and current cost level $\textsc{Lvl}=1$, the search iteratively creates all
    programs of cost 1, 2, 3, and so on; to create complex programs, the algorithm
    *reuses* simpler programs already stored in the bank, and combines them using
    the production rules of the grammar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the CFG in [Sec. 2.1](#S2.SS1.SSS0.Px1 "Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), and assume a uniform weight function
    $w(\cdot)=1$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During search, each candidate expression is evaluated to see if it satisfies
    the examples (lines 5–7). Importantly, the search maintains a cache of all evaluation
    results E, and discard the newly constructed program if it is *observationally
    equivalent* to a program already in the bank (line 8), *i.e.* if it evaluates
    to the same output for all inputs in the examples. This step is the key to the
    efficiency of the bottom-up search algorithm: it allows the synthesizer to factorize
    the search space by evaluation result, significantly reducing the number of programs
    explored at each cost level.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 The \toolApproach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A key challenge in program synthesis is the astronomical size of the search
    space the synthesizer has to explore. For example, to find the program [Eq. 1](#S1.E1
    "1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"), the solution to the Arc task from the introduction, bottom-up search
    with a uniform weight function has to enumerate around 450K programs (all programs
    of size $\leq 16$), which takes 4.5 minutes in our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, sampling solutions to this task from an LLM yields programs
    that are *close* to the desired solution, even if not quite correct. As we show
    in [Appendix A](#A1 "Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), GPT-4o uses relevant components
    update_color, color_of, and is_neighbor in nearly all of its solutions (usually
    missing some part of the filter or using the wrong color in the transform), and
    never uses irrelevant components like move or rotate. This suggests that the LLM
    generally has the right intuition about the components the solution needs to use;
    our insight is to leverage this intuition to guide bottom-up search by *assigning
    lower weights to the components that the LLM uses frequently*.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Guiding Bottom-up Search with Context-Free LLM Approximation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The overview of our approach, \tool, is shown in [Fig. 2](#S1.F2 "Figure 2
    ‣ Our Solution: Context-Free LLM Approximation ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis"). Given a PBE problem consisting
    of a DSL with grammar $\mathcal{G}$, \toolproceeds in three steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Sampling Solutions from an LLM'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \tool
  prefs: []
  type: TYPE_NORMAL
- en: starts by creating an LLM prompt that contains $\mathcal{G}$ trades off computational
    cost and the faithfulness of the approximation to the true LLM conditional.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Learning a PCFG from LLM Solutions'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, \toolattempts to parse each completion $S_{i}$ is a smoothing parameter
    that ensures that every rule has a non-zero probability (typically set to 1).
  prefs: []
  type: TYPE_NORMAL
- en: Our experiments show that some models struggle to generate grammatical completions,
    leading to $N^{\prime}\ll N$ that produce this terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Guiding Bottom-up Search with PCFG'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, \tooluses the PCFG computed in the previous step to derive a weighted
    grammar $\mathcal{G}_{w}$ rules have weight 4; the relevant filter operators color_of
    and is_neighbor are similarly down-weighted. As a result, the search procedure
    only has to enumerate around 220K programs instead of 450K, achieving a 4x speedup,
    and solving the motivating example in just one minute with LLM guidance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Domain-Specific Instantiations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now describe how the \toolapproach is instantiated in three different domains:
    Arc grid puzzles, Tensor manipulations, and String manipulations.'
  prefs: []
  type: TYPE_NORMAL
- en: Arc Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An example task from this domain is shown in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis") and has been used as a running example throughout this paper. There
    is no established DSL for Arc, and arguably, DSL design is the biggest challenge
    when attempting to solve Arc using a PBE approach, since it is hard to capture
    the wide variety of tasks in this domain. Our DSL is inspired by the rule-based
    language of Arga [[44](#bib.bib44)], which we modified slightly to make it more
    compositional.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A program in our DSL is a sequence of rules of the form if  *filter*  then  *transform*.
    A rule refers to the current object self, which is modified by the transform if
    the filter is satisfied in the current state of the grid. The rule can also refer
    to other objects in the grid, such as other in [Eq. 1](#S1.E1 "1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"). This
    program is well-defined because its filter uniquely identifies the object other;
    if the filter is too weak to uniquely determine the effect of the transform, the
    program’s output is considered undefined. The full grammar of our DSL can be found
    in [Appendix H](#A8 "Appendix H The Full Arc DSL ‣ Appendix G The Full String
    Grammar ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor
    Grammar ‣ Appendix D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix
    C Different sample sizes ablation for Arc, Tensor and String domains ‣ Appendix
    B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating
    Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣
    \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of searching for a complete program using [Alg. 1](#alg1 "Algorithm
    1 ‣ 2.3 Bottom-up Search ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"),
    we further optimize our synthesizer using a divide-and-conquer strategy inspired
    by [[6](#bib.bib6)], searching for filters and transforms *separately*. Specifically,
    \tool-Arc first searches for transforms that are correct on some objects in the
    grid; once it has found a set of transforms that collectively describe all grid
    objects, it searches for filters that distinguish between the subsets of objects
    changed by each transform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider once again our running example. When the transform synthesizer enumerates
    the expression update_color(color_of(other)), it detects that this transform works
    for all *grey object*, because for each grey object self there exists a corresponding
    object other whose color can be copied. Now the goal of filter synthesis is to
    find a boolean expression that holds exactly for those pairs of objects ${{(\mbox{\leavevmode\lstinline{{\lst@@@set@language\lst@@@set@numbers\lst@@@set@frame\lst@@@set@rulecolor\lst@@@set@numbers\lst@@@set@language\lst@@@set@numbers\lst@@@set@language{\@listingGroup{ltx_lst_keyword}{self}}}}}},\mbox{\leavevmode\lstinline{{\lst@@@set@language\lst@@@set@numbers\lst@@@set@frame\lst@@@set@rulecolor\lst@@@set@numbers\lst@@@set@language\lst@@@set@numbers\lst@@@set@language{\@listingGroup{ltx_lst_identifier}{other}}}}}})$
    that make the transform work. See [Appendix L](#A12 "Appendix L The Arc Synthesis
    Algorithm ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed Prompt Settings
    ‣ Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc DSL ‣ Appendix
    G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM
    Prompt for the Tensor Grammar ‣ Appendix D Experimental results with LLMs DeepSeek
    and Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor and String
    domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o
    Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")
    for more details about this algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This domain originates from the TFCoder synthesizer[[36](#bib.bib36)], which
    takes as input examples of a tensor transformation (with an optional natural language
    description) and synthesizes a TensorFlow program that performs the transformation.
    An example task from this domain is shown in [1(b)](#S1.F1.sf2 "1(b) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"), whose solution is: tf.gather_nd(in1,  tf.stack((in2,  in3),  axis=-1)).
    The main challenge, however, is that the TensorFlow grammar is very large (see
    [Appendix I](#A9 "Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc
    DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣
    Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis")), and most importantly, the programs are allowed
    to use an *unbounded* set of constants. The original TFCoder synthesizer requires
    the user to provide any non-standard constants that a task might require, and,
    according to their paper, this is the main barrier to the usability of their tool.'
  prefs: []
  type: TYPE_NORMAL
- en: For program synthesis in this domain we use the TFCoder synthesizer off the
    shelf. TFCoder performs weighted bottom-up search, using a combination of hand-tuned
    weights and weights derived by two custom-trained neural models. \tool-Tensor
    replaces these weights entirely with weights computed by sampling from an LLM.
    Importantly, our version of the tool does not require the user to provide any
    constants; instead we extract constants from the LLM completions, whereby significantly
    reducing the burden on the user.
  prefs: []
  type: TYPE_NORMAL
- en: String Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our third domain involves string manipulation tasks from the SyGuS competition[[4](#bib.bib4)],
    which are inspired by spreadsheet use cases. An example task, which requires extracting
    the top-level domain name from a URL, is shown in [1(c)](#S1.F1.sf3 "1(c) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"). In this domain we use the Probe [[11](#bib.bib11)] synthesizer off
    the shelf. Probe performs weighted bottom-up search, starting with a uniform grammar
    and updating the weights on the fly; \tool-String instead initializes Probe’s
    search with weights derived from an LLM, and disables the weight updates during
    search.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19c8e978c48016bfbb00912dfdd0ef2a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) \tool-Arc results with Gpt4o
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/512148e9d189f3987ac9dacb5397217e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) \tool-String results with Gpt4o
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d30ace6d3931a4668361a616706d13e0.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) \tool-Tensor results with Gpt4o
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain/Model | % Valid completions |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor-Gpt4o | 99.96% |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor-DeepSeek | 92.8% |'
  prefs: []
  type: TYPE_TB
- en: '| String-Gpt4o | 98.3% |'
  prefs: []
  type: TYPE_TB
- en: '| String-DeepSeek | 86% |'
  prefs: []
  type: TYPE_TB
- en: '| Arc-Gpt4o | 78% |'
  prefs: []
  type: TYPE_TB
- en: (d) Percentage of syntactically valid completions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: (a,b,c) Number of benchmarks solved by \toolas a function of time
    for the Arc, Tensor, and String domains; timeout is 10 min. (d) Percentage of
    syntactically valid completions per domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate \toolon 299 PBE tasks from three different domains: Arc (160 tasks),
    String (70 tasks) and Tensor (69 tasks).'
  prefs: []
  type: TYPE_NORMAL
- en: Arc Benchmark
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 160 Arc tasks are taken from the testing set of Arga  [[44](#bib.bib44)].
    This *object-centric* subset of the full Arc corpus is known as Object-Arc, and
    has been used to evaluate other Arc solvers[[27](#bib.bib27)]. Arc specifications
    consist of 2-7 input-output training grids and 1 testing grid. Correctness is
    based on whether the generated solution produces the correct output on the testing
    grid. Our Arc DSL has a total of 20 operations and 50 constants and variables
    across all types.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Benchmark
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 69 Tensor tasks taken from TFCoder focus on tensor manipulation. 49 of them
    are sourced from StackOverflow inquiries, and 20 are from real-world scenarios
    faced by TensorFlow users at Google. The overall benchmark suite consists of 72
    tasks. We use three of these tasks as in-context examples and evaluate on the
    rest. The grammar for this domain consists of 134 Tensorflow operations, primitives
    like 0, 1, -1, True and other task-specific constants.
  prefs: []
  type: TYPE_NORMAL
- en: String Benchmark
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 70 String tasks are taken from testing set of Probe, which is derived them
    from the SyGuS benchmark[[4](#bib.bib4)]. The number of examples ranges from 2
    to 400. The original SyGuS benchmark have custom grammars for each task, but we
    use a union of all the grammars to make the search more challenging; the union
    grammar has 16 operations and 59 constants.
  prefs: []
  type: TYPE_NORMAL
- en: Configurations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our main \toolconfiguration uses Gpt4o as the LLM, with 100 samples per task
    to learn a PCFG in non-strict mode (*i.e.* syntactically invalid completions are
    included in the PCFG learning process, as explained in [Sec. 3.1](#S3.SS1 "3.1
    Guiding Bottom-up Search with Context-Free LLM Approximation ‣ 3 The \toolApproach
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")). For each domain, we compare
    the performance of \toolwith a baseline synthesizer for that domain (Arga  ⁴⁴4At
    the time of writing, Arga is no longer state of the art on the Object-Arc dataset;
    we explain in [Sec. 5](#S5 "5 Related Work ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")
    why the comparison with Arga is still relevant., Probe, and TFCoder), as well
    as two ablations: (1) *no search*, *i.e.* using the 100 samples from the LLM directly,
    and (2) *unguided search*, *i.e.* running the same synthesizer but with a uniform
    weighted grammar. We also analyze the performance of \toolwith different numbers
    of samples used to learn the PCFG (10, 20, and 50), with other LLMs (Gpt3.5 and
    DeepSeek [[22](#bib.bib22)]), as well as in strict mode (which discards syntactically
    invalid LLM completions). Search timeout is set to 10 minutes for all experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How does \toolcompare to baselines and ablations?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare the time to solution for the main \toolconfiguration, baseline synthesizers,
    and the two ablations; the results for the three domains are shown in [3(a)](#S4.F3.sf1
    "3(a) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"),
    [3(c)](#S4.F3.sf3 "3(c) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), and [3(b)](#S4.F3.sf2 "3(b) ‣ Figure
    4 ‣ 4 Experiments and Results ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").
    Overall, \toolconsistently outperforms both the baseline synthesizers and ablations,
    solving more tasks across all domains and time scales.'
  prefs: []
  type: TYPE_NORMAL
- en: In more detail, direct LLM sampling performs very poorly on all domains, solving
    between 0 and 5 tasks; this confirms our hypothesis that LLMs struggle on PBE
    tasks in domain-specific languages, which are not prevalent in their training
    data. Interestingly, despite not being able to solve *any*  String tasks by itself,
    Gpt4o provides excellent guidance for \toolon that domain, helping it solve 5x
    more tasks than the unguided search!
  prefs: []
  type: TYPE_NORMAL
- en: In String and Tensor domains, the baseline synthesizers predictably do better
    than unguided search, since both use the same search implementation, but with
    different weights. On Arc, however, our custom synthesizer outperforms Arga  ⁵⁵5[[44](#bib.bib44)]
    report 57 tasks for Arga but we could only reproduce 51 on our hardware with a
    10 minute timeout. even without LLM guidance; this speaks to the efficiency of
    the bottom-up search and the divide-and-conquer strategy we use, which are results
    of years of research in the program synthesis community.
  prefs: []
  type: TYPE_NORMAL
- en: How many samples are needed to learn a PCFG?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To better understand how the number of samples affects the quality of PCFG
    guidance, we vary the number of Gpt4o programs used in PCFG learning $N=10,20,50,100$,
    and once again measure the number of tasks solved over time. The results are shown
    in [Fig. 8](#A3.F8 "Figure 8 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") in [Appendix C](#A3 "Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis"). As expected, larger sample
    sizes generally lead to better performance, but the difference is minimal: in
    Arc and Tensor, the difference between the best and worst performing versions
    of \toolis only 2 and 1 problems, respectively, while in String, \toolsolves 5
    fewer problems with 10 samples than with 100. Despite these differences, all versions
    of \toolstill outperform the baseline and unguided search. This suggests that
    fewer samples are sufficient to effectively train a robust surrogate model, thereby
    optimizing costs.'
  prefs: []
  type: TYPE_NORMAL
- en: Do our results generalize to other models?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To answer this question, we repeat our experiments on String and Tensor domains
    with Gpt3.5 and the open-source model deepseek-coder-33b-instruct (DeepSeek) [[22](#bib.bib22)].
    The results with these models are detailed in [Fig. 9](#A4.F9 "Figure 9 ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") in [Appendix D](#A4 "Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis"), and they corroborate the pattern
    observed with Gpt4o, where the guided versions outperform the baseline, unguided
    search, and direct sampling from the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: How important is non-strict mode?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[3(d)](#S4.F3.sf4 "3(d) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis") shows the percentage of syntactically
    valid completions generated by Gpt4o and DeepSeek (where applicable). You can
    see that while on Tensor almost all completions are valid, this percentage falls
    to 78% for Arc and 86% for String; this is not surprising, given that the former
    are TensorFlow programs, which the model has seen during training, while the latter
    two are custom DSLs. Hence our non-strict mode proves especially helpful for low-resource
    domains, where otherwise we would have to discard a large proportion of completions.
    At the same time, we find that *given the same number of completions to learn
    from*, the PCFGs learned in non-strict mode are just as effective as those learned
    in strict mode: for example, \tool-Tensor with the guidance from 100 DeepSeek
    completions solves 67 tasks *in either mode* (with the difference that strict
    mode has to sample more completions to get 100 valid ones).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main limitation of our hybrid approach *wrt.* to purely neural approaches
    is that is requires implementing a synthesizer for each DSL of interest; although
    we have shown that the same bottom-up search can be used across different domains,
    some implementation effort is still required. On the other hand, compared to purely
    symbolic approaches, our method requires sampling from an LLM, which is costly;
    additionally, the guidance provided by our approach is only as good as the LLM’s
    completions: if they contain many irrelevant operators, our guided search can
    be *slower* than unguided search. Finally, our experiments are subject to the
    usual threat that the LLMs might have seen our benchmarks in their training data;
    we do not consider it a major issue, however, given that our main result is the
    superior performance of guided search *relative* to using LLMs without search.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Guiding Program Synthesis with Probabilistic Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The traditional approach to *program synthesis* is based on combinatorial search[[7](#bib.bib7)],
    augmented with pruning techniques based on program semantics [[39](#bib.bib39),
    [2](#bib.bib2), [6](#bib.bib6)]. To further speed up the search, researchers have
    proposed *guiding* the search with a learned probabilistic model. Most approaches
    to guided search use special-purpose models that have to be trained on a domain-specific
    corpus of programs[[26](#bib.bib26)] or PBE tasks[[9](#bib.bib9), [24](#bib.bib24),
    [32](#bib.bib32), [37](#bib.bib37)]. Although some of these models can be trained
    on synthetic data, the training process is still expensive and requires manual
    tuning, which makes it hard to apply these techniques to new domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the advent of pretrained Large Language Models (LLMs), it seems only natural
    to use them to guide search-based program synthesis, thus alleviating the need
    for domain-specific training data. We are only aware of one other attempt to do
    this: concurrent work by Li et al. [[29](#bib.bib29)], which also extracts a PCFG
    from the LLM’s samples, similarly to \tool. An important difference is that they
    use the PCFG to guide *top-down* A* search, while we use it to guide *bottom-up*
    search, which is known to be more efficient (they also evaluate their tool on
    synthesis from logical formulas as opposed to PBE).'
  prefs: []
  type: TYPE_NORMAL
- en: Solving the Abstraction and Reasoning Corpus
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All state-of-the-art solvers for this benchmark have relied on carefully curated
    DSLs for Arc  [[13](#bib.bib13), [43](#bib.bib43), [3](#bib.bib3), [27](#bib.bib27),
    [19](#bib.bib19)]. Xu et al. [[44](#bib.bib44)] proposed the DSL we extend in
    our approach, and the Object-Arc subset we evaluate on. Lei et al. [[27](#bib.bib27)]
    embed their DSL as a subset of PDDL and use a Generalized Planning (GP) algorithm
    as their search component. They have the current best performance on Object-Arc,
    however they encode more domain-knowledge in the form of preconditions and per-abstraction
    restrictions on filters and transforms, to make GP viable. Our approach does not
    require this additional information. [[3](#bib.bib3), [10](#bib.bib10)] use DreamCoder
    [[16](#bib.bib16)], to perform execution-guided search over a DSL for grid manipulations,
    however they only provide proof-of-concept evaluations. [[41](#bib.bib41), [38](#bib.bib38)]
    also use an LLM to generate code given the spec of the task. Both of these approaches
    interact with the model across several rounds, while our technique uses the suggestions
    from the LLM only as a starting point. Our technique also performs a complete
    search guided by the LLM distribution, enabled by the structure of our DSL, whereas
    previous approaches only consider code directly generated by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our approach introduces a robust technique for using both valid and invalid
    completions from an LLM to learn a surrogate model. By incorporating ungrammatical
    completions, we can extract useful insights that would otherwise be discarded.
    Overall, we provide an alternative to the conventional strategy of large-scale
    sampling from LLMs, proposing a more effective use of the available completions
    to guide the search process. An interesting future direction would be to guide
    search with a more expressive context-dependent surrogate model.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'arc [2020] 2020. [Arc kaggle competition leaderboard](https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge/leaderboard).
    Accessed: 2024-05-19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Albarghouthi et al. [2013] Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid.
    2013. Recursive program synthesis. In *International Conference on Computer Aided
    Verification*, pages 934–950\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alford et al. [2022] Simon Alford, Anshula Gandhi, Akshay Rangamani, Andrzej
    Banburski, Tony Wang, Sylee Dandekar, John Chin, Tomaso Poggio, and Peter Chin.
    2022. Neural-guided, bidirectional program search for abstraction and reasoning.
    In *Complex Networks & Their Applications X: Volume 1, Proceedings of the Tenth
    International Conference on Complex Networks and Their Applications COMPLEX NETWORKS
    2021 10*, pages 657–668\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alur et al. [2013] Rajeev Alur, Rastislav Bodík, Garvit Juniwal, Milo M. K.
    Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama,
    Emina Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. In *Formal Methods
    in Computer-Aided Design, FMCAD 2013*, pages 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alur et al. [2017a] Rajeev Alur, Dana Fisman, Rishabh Singh, and Armando Solar-Lezama.
    2017a. Sygus-comp 2017: Results and analysis. *arXiv preprint arXiv:1711.11438*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alur et al. [2017b] Rajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017b.
    Scaling enumerative program synthesis via divide and conquer. In *International
    Conference on Tools and Algorithms for the Construction and Analysis of Systems*,
    pages 319–336\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alur et al. [2018] Rajeev Alur, Rishabh Singh, Dana Fisman, and Armando Solar-Lezama.
    2018. Search-based program synthesis. *Communications of the ACM*, 61(12):84–93.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2023] Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, and
    Yue Zhang. 2023. Constituency parsing using llms. *arXiv preprint arXiv:2310.19462*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balog et al. [2016] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, and Daniel Tarlow. 2016. Deepcoder: Learning to write programs. *arXiv
    preprint arXiv:1611.01989*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banburski et al. [2020] Andrzej Banburski, Anshula Gandhi, Simon Alford, Sylee
    Dandekar, Sang Chin, and tomaso a poggio. 2020. [Dreaming with ARC](https://openreview.net/forum?id=-gjy2V1ko6t).
    In *Learning Meets Combinatorial Algorithms at NeurIPS2020*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barke et al. [2020] Shraddha Barke, Hila Peleg, and Nadia Polikarpova. 2020.
    Just-in-time learning for bottom-up enumerative synthesis. *Proceedings of the
    ACM on Programming Languages*, 4(OOPSLA):1–29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berglund et al. [2023] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni,
    Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse:
    Llms trained on" a is b" fail to learn" b is a". *arXiv preprint arXiv:2309.12288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Butt et al. [2023] Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone,
    David W Zhang, Michaël Defferrard, and Taco Cohen. 2023. Codeit: Abstract reasoning
    with iterative policy-guided program synthesis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet [2019] François Chollet. 2019. On the measure of intelligence. *arXiv
    preprint arXiv:1911.01547*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. [2017] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh
    Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program
    learning under noisy i/o. In *International conference on machine learning*, pages
    990–998\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ellis et al. [2020] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer,
    Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum.
    2020. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep
    bayesian program learning. *arXiv preprint arXiv:2006.08381*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. [2018] Yu Feng, Ruben Martins, Osbert Bastani, and Isil Dillig.
    2018. [Program synthesis using conflict-driven learning](https://doi.org/10.1145/3192366.3192382).
    In *Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design
    and Implementation*, PLDI 2018, pages 420–435, New York, NY, USA. Association
    for Computing Machinery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. [2017] Yu Feng, Ruben Martins, Yuepeng Wang, Isil Dillig, and Thomas W.
    Reps. 2017. Component-based synthesis for complex apis. In *POPL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischer et al. [2020] Raphael Fischer, Matthias Jakobs, Sascha Mücke, and Katharina
    Morik. 2020. Solving abstract reasoning tasks with grammatical evolution. In *LWDA*,
    pages 6–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulwani [2011] Sumit Gulwani. 2011. Automating string processing in spreadsheets
    using input-output examples. *ACM Sigplan Notices*, 46(1):317–330.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulwani [2016] Sumit Gulwani. 2016. Programming by examples (and its applications
    in data wrangling). In Javier Esparza, Orna Grumberg, and Salomon Sickert, editors,
    *Verification and Synthesis of Correct and Secure Systems*. IOS Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. Deepseek-coder: When
    the large language model meets programming–the rise of code intelligence. *arXiv
    preprint arXiv:2401.14196*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Josifoski et al. [2023] Martin Josifoski, Marija Sakota, Maxime Peyrard, and
    Robert West. 2023. Exploiting asymmetry for synthetic training data generation:
    Synthie and the case of information extraction. *arXiv preprint arXiv:2303.04132*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalyan et al. [2018] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv
    Batra, Prateek Jain, and Sumit Gulwani. 2018. Neural-guided deductive search for
    real-time program synthesis from examples. *arXiv preprint arXiv:1804.01186*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2024] Seungpil Lee, Woochang Sim, Donghyeon Shin, Sanha Hwang,
    Wongyu Seo, Jiwon Park, Seokki Lee, Sejin Kim, and Sundong Kim. 2024. [Reasoning
    abilities of large language models: In-depth analysis on the abstraction and reasoning
    corpus](http://arxiv.org/abs/2403.11793).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2018] Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018.
    Accelerating search-based program synthesis using learned probabilistic models.
    *ACM SIGPLAN Notices*, 53(4):436–449.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. [2024] Chao Lei, Nir Lipovetzky, and Krista A. Ehinger. 2024. [Generalized
    planning for the abstraction and reasoning corpus](http://arxiv.org/abs/2401.07426).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2024a] Xiang Li, Xiangyu Zhou, Rui Dong, Yihong Zhang, and Xinyu
    Wang. 2024a. [Efficient bottom-up synthesis for programs with local variables](https://doi.org/10.1145/3632894).
    *Proc. ACM Program. Lang.*, 8(POPL).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2024b] Yixuan Li, Julian Parsert, and Elizabeth Polgreen. 2024b.
    [Guiding enumerative program synthesis with large language models](http://arxiv.org/abs/2403.03997).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCarthy [1960] John McCarthy. 1960. [Recursive functions of symbolic expressions
    and their computation by machine, part i](https://doi.org/10.1145/367177.367199).
    *Commun. ACM*, 3(4):184–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCoy et al. [2023] R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy,
    and Thomas L Griffiths. 2023. Embers of autoregression: Understanding large language
    models through the problem they are trained to solve. *arXiv preprint arXiv:2309.13638*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Odena et al. [2020] Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh,
    Charles Sutton, and Hanjun Dai. 2020. Bustle: bottom-up program synthesis through
    learning-guided exploration. *arXiv preprint arXiv:2007.14381*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI [2024] OpenAI. 2024. [Hello gpt-4.0](https://openai.com/index/hello-gpt-4o/).
    Accessed: 2024-05-19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osera and Zdancewic [2015] Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed
    program synthesis. *ACM SIGPLAN Notices*, 50(6):619–630.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reynolds et al. [2019] Andrew Reynolds, Haniel Barbosa, Andres Nötzli, Clark
    Barrett, and Cesare Tinelli. 2019. cvc 4 sy: smart and fast term enumeration for
    syntax-guided synthesis. In *International Conference on Computer Aided Verification*,
    pages 74–83\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2022a] Kensen Shi, David Bieber, and Rishabh Singh. 2022a. [Tf-coder:
    Program synthesis for tensor manipulations](https://doi.org/10.1145/3517034).
    *ACM Trans. Program. Lang. Syst.*, 44(2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2022b] Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton.
    2022b. Crossbeam: Learning to search in bottom-up program synthesis. *arXiv preprint
    arXiv:2203.10452*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Motani [2023] John Chong Min Tan and Mehul Motani. 2023. Large language
    model (llm) as a system of multiple expert agents: An approach to solve the abstraction
    and reasoning corpus (arc) challenge. *arXiv preprint arXiv:2310.05146*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Udupa et al. [2013] Abhishek Udupa, Arun Raghavan, Jyotirmoy V Deshmukh, Sela
    Mador-Haim, Milo MK Martin, and Rajeev Alur. 2013. Transit: specifying protocols
    with concolic snippets. *ACM SIGPLAN Notices*, 48(6):287–296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ugare et al. [2024] Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic,
    and Gagandeep Singh. 2024. Improving llm code generation with grammar augmentation.
    *arXiv preprint arXiv:2403.01632*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu,
    Nick Haber, and Noah D Goodman. 2023. Hypothesis search: Inductive reasoning with
    language models. *arXiv preprint arXiv:2309.05660*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. [2024] Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski,
    Swarat Chaudhuri, and Alex Polozov. 2024. [Grounding data science code generation
    with input-output specifications](http://arxiv.org/abs/2402.08073).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wind [2020] Johan Sokrates Wind. 2020. [Arc kaggle competition, 1st place](https://github.com/top-quarks/ARC-solution).
    Accessed: 2024-05-19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2023a] Yudong Xu, Elias B Khalil, and Scott Sanner. 2023a. Graphs,
    constraints, and search for the abstraction and reasoning corpus. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 37, pages 4115–4122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2023b] Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner,
    and Elias B Khalil. 2023b. Llms and the abstraction and reasoning corpus: Successes,
    failures, and the importance of object-based representations. *arXiv preprint
    arXiv:2305.18354*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van den
    Broeck. 2023. Tractable control for autoregressive language generation. In *International
    Conference on Machine Learning*, pages 40932–40945\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A GPT4o Solutions for the Motivating Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '//  Solution  1,  occurs  6  timesif  color_of(self)  $=$  BLUE  then  update_color(ORANGE)  ;if  size_of(self)  $=$  YELLOW  then  update_color(CYAN)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Ten samples from GPT4o for the motivating example in [1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the motivating example in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure 1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis") where
    the task is to update the color of the grey objects to the color of their single-pixel
    neighbor. As a reminder, the smallest correct solution to this task consists of
    the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if  color_of(self)  $=$  MINthen  update_color(color_of(x))'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fig. 5](#A1.F5 "Figure 5 ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") shows the programs we obtained
    by deduplicating 10 samples from GPT4o for this task. The syntax of the solutions
    is slightly modified for readability; our implementation uses a LISP-style s-expression
    syntax[[30](#bib.bib30)] to simplify parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the most frequent solution is almost correct, except that is
    does not constrain the neighbor other to be of size 1; this leads to the constraint
    being ambiguous (since every gray object has multiple neighbors of different colors),
    in which case the program semantics is considered undefined. That said, you can
    observe that the model consistently uses relevant components, such as color_of,
    is_neighbor, and update_color, which enables us to extract a useful PCFG from
    these solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we increased the sample size to 125, GPT4o was able to produce one correct
    solution (which is slightly larger than the minimal solution above):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if  color_of(self)  $=$  GREY)then  update_color(color_of(other))'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B LLM Prompt for the Motivating Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 System Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The system prompt given to the LLM for Arc domain is shown in [Fig. 6](#A2.F6
    "Figure 6 ‣ B.1 System Prompt ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  are  an  assistant  chatbot  with  human-like  perception,  reasoning  and  learning  capabilities.You  can  solve  tasks  concisely,  efficiently,  and  moreover,  correctly.Let’s  engage  in  perception-  and  logic-based  tasks.You  only  output  source  code.No  explanations  or  any  other  text.Only  code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: System prompt for Arc domain.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 User Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The full user prompt for the Arc domain is shown in [Fig. 7](#A2.F7 "Figure
    7 ‣ B.2 User Prompt ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix
    A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").
    It contains the domain-specific language, four in-context examples and the query
    for the test task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  are  an  efficient  assistant  for  logical  reasoning  and  code  generation.You  will  help  me  solve  a  visual  perception  and  reasoning  task.I  will  first  provide  you  with  the  definition  of  a  Domain  Specific  Language  you  will  use  for  writing  a  solution  for  the  task.I  will  then  present  you  with  the  description  of  the  task  that  you  will  be  tested  in.You  will  then  respond  the  queries  I  make  regarding  the  solution  of  the  task.This  is  the  definition  of  the  DSL  you  will  use  to  solve  the  task.It  is  given  as  a  context-free  grammar  in  the  EBNF  format  used  by  the  Lark  parser  generator,  with  some  informative  comments  about  the  semantics.You  will  return  a  string  that  is  parseable  by  the  ‘program‘  non-terminal  of  the  grammar.‘‘‘library:  "("  program*  ")"//  Rules  are  executed  one  after  another,  in  the  order  they  appear.//  There  could  be  no  rules,  in  which  case  the  program  does  nothing.program:  "("  "do"  rule*  ")"...$<$‘‘‘Now  we  continue  with  the  visual  perception  and  reasoning  task.The  input  for  the  task  is  a  small  number  of  pairs  of  grids  of  characters.The  value  of  each  of  the  cells  of  the  grids  are  the  colors  defined  in  the  DSL,  so  we  can  think  of  grids  as  images.Each  pair  of  images  correspond  to  an  input-output  example  for  an  unknown  program  P.For  each  pair,  the  program  P  is  evaluated  on  the  image  grid  and  operates  on  the  objects  that  appear  in  it.The  output  of  the  program  is  then  the  output  image.The  objects  in  the  images  are  easy  and  natural  to  identify  for  humans,  so  there  is  no  need  to  define  them  explicitly.However  you  are  able  to  abstract  them  correctly,  and  the  DSL  is  interpreted  with  the  same  correct  abstraction.Now  I  will  show  you  some  demonstration  tasks  along  with  the  output  you  would  be  expected  to  produce  for  each  of  them.##  DEMONSTRATION  TASK  1###  INPUTPAIR  1INPUT  GRID:O  O  O  O  O  O  O  OO  O  O  O  O  R  O  OO  R  O  O  O  R  O  RO  R  R  O  O  R  O  OO  O  O  O  O  O  O  OO  R  R  O  O  O  O  OO  R  R  O  R  R  O  OO  O  O  O  O  O  O  O'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: User prompt for Arc domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OUTPUT  GRID:O  O  O  O  O  O  O  OO  O  O  O  O  Y  O  OO  Y  O  O  O  Y  O  YO  Y  Y  O  O  Y  O  OO  O  O  O  O  O  O  OO  Y  Y  O  O  O  O  OO  Y  Y  O  Y  Y  O  OO  O  O  O  O  O  O  O$<$<math
    id="lstnumberx97.5.m1.1" class="ltx_Math" alttext="></math>'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Different sample sizes ablation for Arc, Tensor and String domains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86cfef06e90217c53c092ca598ee56dd.png)![Refer to caption](img/75df365f95cefe4004015bc3cd0c0c94.png)![Refer
    to caption](img/5bfb8ec75bdd6435d81bbaf265d385ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: \tool-Arc, \tool-Tensor and \tool-String results guided by a PCFG
    learned from different number of Gpt4o samples (n=10, 20, 50, 100).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Experimental results with LLMs DeepSeek and Gpt3.5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53f62fdf19f62786669c439592aa435a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) \tool-String results with DeepSeek
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/657a302b1624b670609fd128c2ce5ad7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) \tool-String results with Gpt3.5
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79469f9a9dfff42818d73c99028c593e.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) \tool-Tensor results with DeepSeek
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e17b0b32bcdb340cd269cb3f8417185c.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) \tool-Tensor results with Gpt3.5
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: \tool-String and \tool-Tensor evaluation results with DeepSeek and
    Gpt3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E LLM Prompt for the Tensor Grammar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The system and user prompt for Tensor domain are in [Fig. 10](#A5.F10 "Figure
    10 ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") and [Fig. 11](#A5.F11 "Figure 11 ‣ Appendix E
    LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results with LLMs
    DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor
    and String domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix
    A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  are  a  coding  assistant.  Be  precise  and  terse.You  will  be  provided  a  list  of  tensorflow  operators,  a  task  description,  and  some  input/output  examples.Your  task  is  to  generate  the  body  of  a  python  function  that  will  transform  the  input  to  the  output.Only  use  the  operators  provided  in  the  list.Your  answer  should  be  as  short  as  possible  while  still  being  correct.Make  sure  to  only  generate  python  code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: System prompt for Tensor domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '[TENSORFLOW  OPERATORS]$<$[TASK  DESCRIPTION]index  into  the  tensor[INPUTS][[  5.  2.][  1.  3.][  0.  -1.]][OUTPUTS][[[  5.  5.][  1.  1.][  0.  0.]][[  2.  2.][  3.  3.][-1.  -1.]]][PROGRAM]def  transform(in1):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: User prompt for Tensor domain'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F LLM Prompt for String
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The system and user prompt for String domain are in [Fig. 10](#A5.F10 "Figure
    10 ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") and [Fig. 11](#A5.F11 "Figure 11 ‣ Appendix E
    LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results with LLMs
    DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor
    and String domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix
    A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  are  a  coding  assistant.  Be  precise  and  terse.You  will  be  given  a  SyGuS  grammar,  a  natural  language  specification,  and  a  set  of  input-output  examples.Your  task  is  to  complete  the  provided  function  definition  with  an  implementation  that  is  correct  according  to  the  grammar,  specification,  and  examples.Your  answer  should  be  as  short  as  possible  while  still  being  correct.Make  sure  that  your  answer  is  a  valid  s-expression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: System prompt for String domain'
  prefs: []
  type: TYPE_NORMAL
- en: '[GRAMMAR](synth-fun  f  ((_arg_0  String))  String  ((Start  String  (ntString))  (ntString  String  (_arg_0  ""  "  "  "BRD"  "DRS"  "LDS"  "Branding"  "Direct  Response"  "Leads"  "="  "/"  "in"  "_"  "9"  "."  "microsoft"  "windows"  "apple"  "mac"  "-"  "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "0"  ","  "$<$  uk[SOLUTION](define-fun  f  (_arg_0  String)  String'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: User message for String'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G The Full String Grammar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathit{Start}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle S$ | string variables |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | string literals |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | replace s x y replaces first occurrence of x
    in s with y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | concat x y concatenates x and y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | substr x y z extracts substring of length z,
    from index y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | ite x y z returns y if x is true, otherwise z
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | int.to.str x converts int x to a string |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | at x y returns the character at index y in string
    x |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle B$ | bool literals |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | = x y returns true if x equals y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | contains x y returns true if x contains y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | suffixof x y returns true if x is the suffix
    of y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | prefixof x y returns true if x is the prefix
    of y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle I$ | int variables |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | int literals |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | str.to.int x converts string x to a int |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | + x y sums x and y |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | - x y subtracts y from x |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | length x returns length of x |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | ite x y z returns y if x is true, otherwise z
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | indexof x y z returns index of y in x, starting
    at index z |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 14: The full SyGuS String grammar of the Probe benchmark suite. Integer
    and string variables and constants change per benchmark. Some benchmark files
    contain a reduced grammar.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H The Full Arc DSL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The full grammar of our Arc DSL is shown in [Fig. 15](#A8.F15 "Figure 15 ‣
    Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix F
    LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") (for filters) and [Fig. 16](#A8.F16
    "Figure 16 ‣ Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar
    ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar
    ‣ Appendix D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different
    sample sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt
    for the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") (for transforms).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathit{Start}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Filters}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Filter\_Ops}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Color}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | object colors in the grid |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Size}$ | object sizes |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Degree}$ | degrees in the graphs |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Height}$ | object heights |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Width}$ | object widths |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Column}$ | columns in the grid |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Row}$ | rows in the grid |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Shape}$ | shapes in the grid |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Obj}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 15: The modified filter grammar derived from Arga [[44](#bib.bib44)],
    object specific parameters like size, degree, height, width change per benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathit{Start}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Transforms}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Transform\_Ops}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mid$ | NoOp |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Color}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Direction}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Overlap}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Angle}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Axis}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathit{Object}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 16: The modified transform grammar derived from Arga [[44](#bib.bib44)],
    parameters like objects change based on the benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I The Full Tensor Grammar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'General  TensorFlow  functions:-----------------------------tf.abs(x)tf.add(x,  y)tf.add_n(inputs)tf.argmax(input,  axis)tf.argmin(input,  axis)tf.argsort(values,  axis,  stable=True)tf.argsort(values,  axis,  direction=’DESCENDING’,  stable=True)tf.boolean_mask(tensor,  mask)tf.broadcast_to(input,  shape)tf.cast(x,  dtype)tf.clip_by_value(t,  clip_value_min,  clip_value_max)tf.concat(values,  axis)tf.constant(value)tf.constant(value,  dtype)tf.divide(x,  y)tf.equal(x,  y)tf.exp(x)tf.expand_dims(input,  axis)tf.eye(num_rows)tf.eye(num_rows,  num_columns)tf.eye(num_rows,  dtype)tf.fill(dims,  value)tf.gather(params,  indices)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17: List of TensorFlow operations as used in TFCoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'tf.gather(params,  indices,  axis,  batch_dims)tf.gather_nd(params,  indices)tf.gather_nd(params,  indices,  batch_dims)tf.greater(x,  y)tf.greater_equal(x,  y)tf.math.bincount(arr)tf.math.ceil(x)tf.math.count_nonzero(input)tf.math.count_nonzero(input,  axis)tf.math.cumsum(x,  axis)tf.math.cumsum(x,  axis,  exclusive=True)tf.math.divide_no_nan(x,  y)tf.math.floor(x)tf.math.log(x)tf.math.negative(x)tf.math.reciprocal(x)tf.math.reciprocal_no_nan(x)tf.math.segment_max(data,  segment_ids)tf.math.segment_mean(data,  segment_ids)tf.math.segment_min(data,  segment_ids)tf.math.segment_prod(data,  segment_ids)tf.math.segment_sum(data,  segment_ids)tf.math.squared_difference(x,  y)tf.math.top_k(input,  k)tf.math.unsorted_segment_max(data,  segment_ids,  num_segments)tf.math.unsorted_segment_mean(data,  segment_ids,  num_segments)tf.math.unsorted_segment_min(data,  segment_ids,  num_segments)tf.math.unsorted_segment_prod(data,  segment_ids,  num_segments)tf.math.unsorted_segment_sum(data,  segment_ids,  num_segments)tf.matmul(a,  b)tf.maximum(x,  y)tf.minimum(x,  y)tf.multiply(x,  y)tf.not_equal(x,  y)tf.one_hot(indices,  depth)tf.ones(shape)tf.ones_like(input)tf.pad(tensor,  paddings,  mode=’CONSTANT’)tf.pad(tensor,  paddings,  mode=’CONSTANT’,  constant_values)tf.pad(tensor,  paddings,  mode=’REFLECT’)tf.pad(tensor,  paddings,  mode=’SYMMETRIC’)tf.range(start)tf.range(start,  limit,  delta)tf.reduce_any(input_tensor,  axis)tf.reduce_max(input_tensor)tf.reduce_max(input_tensor,  axis)tf.reduce_mean(input_tensor)tf.reduce_mean(input_tensor,  axis)tf.reduce_min(input_tensor)tf.reduce_min(input_tensor,  axis)tf.reduce_prod(input_tensor,  axis)tf.reduce_sum(input_tensor)tf.reduce_sum(input_tensor,  axis)tf.reshape(tensor,  shape)tf.reverse(tensor,  axis)tf.roll(input,  shift,  axis)tf.round(x)tf.searchsorted(sorted_sequence,  values,  side=’left’)tf.searchsorted(sorted_sequence,  values,  side=’right’)tf.sequence_mask(lengths)tf.sequence_mask(lengths,  maxlen)tf.shape(input)tf.sign(x)tf.sort(values,  axis)tf.sort(values,  axis,  direction=’DESCENDING’)tf.sqrt(x)tf.square(x)tf.squeeze(input)tf.squeeze(input,  axis)tf.stack(values,  axis)tf.subtract(x,  y)tf.tensordot(a,  b,  axes)tf.tile(input,  multiples)tf.transpose(a)tf.transpose(a,  perm)tf.unique_with_counts(x)tf.unstack(value,  axis)tf.where(condition)tf.where(condition,  x,  y)tf.zeros(shape)tf.zeros_like(input)SparseTensor  functions:-----------------------tf.SparseTensor(indices,  values,  dense_shape)tf.sparse.add(a,  b)tf.sparse.concat(axis,  sp_inputs)tf.sparse.expand_dims(sp_input,  axis)tf.sparse.from_dense(tensor)tf.sparse.maximum(sp_a,  sp_b)tf.sparse.minimum(sp_a,  sp_b)tf.sparse.reduce_max(sp_input,  axis,  output_is_sparse)tf.sparse.reduce_sum(sp_input,  axis,  output_is_sparse)tf.sparse.reset_shape(sp_input)tf.sparse.reshape(sp_input,  shape)tf.sparse.retain(sp_input,  to_retain)tf.sparse.slice(sp_input,  start,  size)tf.sparse.split(sp_input,  num_split,  axis)tf.sparse.to_dense(sp_input)tf.sparse.to_dense(sp_input,  default_value)tf.sparse.to_indicator(sp_input,  vocab_size)tf.sparse.transpose(sp_input)tf.sparse.transpose(sp_input,  perm)Python-syntax  operations:-------------------------IndexingAxis1Operation:  arg1[:,  arg2]IndexingOperation:  arg1[arg2]PairCreationOperation:  (arg1,  arg2)SingletonTupleCreationOperation:  (arg1,)SlicingAxis0BothOperation:  arg1[arg2:arg3]SlicingAxis0LeftOperation:  arg1[arg2:]SlicingAxis0RightOperation:  arg1[:arg2]SlicingAxis1BothOperation:  arg1[:,  arg2:arg3]SlicingAxis1LeftOperation:  arg1[:,  arg2:]SlicingAxis1RightOperation:  arg1[:,  :arg2]TripleCreationOperation:  (arg1,  arg2,  arg3)'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix J Detailed Prompt Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For Arc, we sample completions with temperature 1 and 4000 max tokens. For Tensor,
    we use temperature 1 and 300 max tokens. For SyGuS, we use temperature 0.5 and
    200 max tokens. We use the same settings for all 3 LLMs. When prompting Gpt4o
    for Arc, we set response_type to JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix K Broader Research Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our method presents a powerful strategy for harnessing both syntactically valid
    and invalid outputs from an LLM to learn a surrogate model. Incorporating hallucinatory
    outputs – often erroneous generated by the model, allows us to extract insights
    that are discarded in standard practices. Our approach mitigates the need for
    large-scale sampling of completions from LLMs, promoting a more efficient and
    effective utilization of these models, saving resources. Our method not only improves
    the cost effectiveness of using LLMs but also opens up new avenues for enhancing
    model robustness and adaptability across different domains.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix L The Arc Synthesis Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall Synthesis Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The overall synthesis algorithm takes as input a set of input-output grids
    $\mathcal{E}$. Concretely, the first set of transforms that satisfies our running
    example is [update_color(FUCHSIA),  update_color(CYAN),  update_color(RED),  update_color(BLUE),
    update_color(ORANGE),  update_color(YELLOW)] but filters can not be found for
    all these transforms. Eventually, the more concise set of transforms [update_color(color_of(X)]
    is enumerated leading to the filter solution we saw in [Eq. 1](#S1.E1 "1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"). In addition,
    the algorithm described above terminates after the first solution is found, but
    we keep searching for a smaller set of transforms[[6](#bib.bib6)].'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Overall Synthesis Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:A set of input-output example grids $\mathcal{E}$ Search for transforms9:         if $\mathcal{T}\neq\mathcal{T}_{p}$
    Found a complete solution! return transform-filter map
  prefs: []
  type: TYPE_NORMAL
- en: Transform Search Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Algorithm 3 Transform Synthesis Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:PCFG $\mathcal{G}_{t}$8:                 $\mathrm{TSol}\leftarrow\mathrm{TSol}\cup\{T\}$  $\triangleright$ then  $\triangleright$
    concrete programs with substituted values33:                             $\mathbf{yield}\
    (t\ P_{1}^{\prime}\ \ldots\ P_{k}^{\prime})$
  prefs: []
  type: TYPE_NORMAL
- en: 'We first describe our transforms synthesis algorithm in Algorithm [3](#alg3
    "Algorithm 3 ‣ Transform Search Algorithm ‣ Appendix L The Arc Synthesis Algorithm
    ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed Prompt Settings ‣
    Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc DSL ‣ Appendix G
    The Full String Grammar ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM Prompt
    for the Tensor Grammar ‣ Appendix D Experimental results with LLMs DeepSeek and
    Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor and String
    domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o
    Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").
    The algorithm takes as input a PCFG $\mathcal{G}_{t}$ or reaches a certain cost
    limit Lim.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm starts with the following initial state: 1) a cost level ($\textsc{Lvl}_{0}$.
    At each iteration, the algorithm explores the space of all new transforms generated
    by the New-Transforms procedure for the current cost level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On line 5 in Algorithm [3](#alg3 "Algorithm 3 ‣ Transform Search Algorithm
    ‣ Appendix L The Arc Synthesis Algorithm ‣ Appendix K Broader Research Impacts
    ‣ Appendix J Detailed Prompt Settings ‣ Appendix I The Full Tensor Grammar ‣ Appendix
    H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt
    for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental
    results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation
    for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating
    Example ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), the enumerated transform $T$ (line
    11).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed before, the grey objects get painted with a variable color which
    corresponds to the color of it’s smallest neighbor, for *e.g.* in the first grid[1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis"), the grey objects get painted with red, fuchsia and
    cyan colors. Whenever we encounter a transform which could have variable values,
    we consider all possible values that could be assigned and yield multiple concrete
    programs corresponding to each of those assignments. Consider the three changed
    objects in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") to be $\omega_{1}$ with its
    corresponding value $v_{i_{j}}$ to get concrete subexpressions in the eventual
    transform that is returned on line 38. In order to do so, the algorithm considers
    all possible values the variable subexpressions could assume on line 37, for *e.g.*
    with variable colors it could be red, green, cyan and variable direction could
    be Up, Down, Right. It then computes the cartesian product of these value sets
    and for each combination of variable assigments, the algorithm substitutes the
    original variable subexpressions with their concrete values, generating a new,
    concrete program on line 38. If there are no variable subexpressions, a new program
    is returned using the selected subexpressions based on rule R on line 40.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 Filter Synthesis Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:PCFG $\mathcal{G}_{f}$ then  $\triangleright$  $\triangleright$
  prefs: []
  type: TYPE_NORMAL
- en: Filter Search Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The filter search algorithm in Algorithm  [4](#alg4 "Algorithm 4 ‣ Transform
    Search Algorithm ‣ Appendix L The Arc Synthesis Algorithm ‣ Appendix K Broader
    Research Impacts ‣ Appendix J Detailed Prompt Settings ‣ Appendix I The Full Tensor
    Grammar ‣ Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix
    F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") takes as input: 1) a filter
    grammar $\mathcal{G}_{f}$, it initiates a new search state to find a filter. Each
    filter at a cost level is evaluated on all the objects in the input grid to return
    the objects for which the filter holds True. In [1(a)](#S1.F1.sf1 "1(a) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"), the filter size_Of(obj) == 1 would return the objects which are unchanged
    since all of them have size 1 (line 9). Consider the filter for the other transform
    in our example, update_color(color_of(X)) in [Eq. 1](#S1.E1 "1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"). The evaluation
    result for this filter would be a mapping from the grey objects and the objects
    from where they got their updated color.'
  prefs: []
  type: TYPE_NORMAL
- en: If the objects for which the filter is True is same as the objects correctly
    transformed by $\mathrm{TSol}$ which maps all transforms to their filters.
  prefs: []
  type: TYPE_NORMAL
