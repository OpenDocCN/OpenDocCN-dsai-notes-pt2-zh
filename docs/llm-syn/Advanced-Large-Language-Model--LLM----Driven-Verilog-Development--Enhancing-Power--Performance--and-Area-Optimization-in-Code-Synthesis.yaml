- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced Large Language Model (LLM) - Driven Verilog Development: Enhancing
    Power, Performance, and Area Optimization in Code Synthesis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.01022](https://ar5iv.labs.arxiv.org/html/2312.01022)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kiran Thorat^($\ast$),
  prefs: []
  type: TYPE_NORMAL
- en: Hongwu Peng^($\ast$),
  prefs: []
  type: TYPE_NORMAL
- en: Jeff Zhang^($\dagger$)Arizona State University )
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The increasing use of Advanced Language Models (ALMs) in diverse sectors, particularly
    due to their impressive capability to generate top-tier content following linguistic
    instructions, forms the core of this investigation. This study probes into ALMs’
    deployment in electronic hardware design, with a specific emphasis on the synthesis
    and enhancement of Verilog programming. We introduce an innovative framework,
    crafted to assess and amplify ALMs’ productivity in this niche. The methodology
    commences with the initial crafting of Verilog programming via ALMs, succeeded
    by a distinct dual-stage refinement protocol. The premier stage prioritizes augmenting
    the code’s operational and linguistic precision, while the latter stage is dedicated
    to aligning the code with Power-Performance-Area (PPA) benchmarks, a pivotal component
    in proficient hardware design. This bifurcated strategy, merging error remediation
    with PPA enhancement, has yielded substantial upgrades in the caliber of ALM-created
    Verilog programming. Our framework achieves an 81.37% rate in linguistic accuracy
    and 62.0% in operational efficacy in programming synthesis, surpassing current
    leading-edge techniques, such as 73% in linguistic accuracy and 46% in operational
    efficacy. These findings illuminate ALMs’ aptitude in tackling complex technical
    domains and signal a positive shift in the mechanization of hardware design operations.
  prefs: []
  type: TYPE_NORMAL
- en: Keywords—LLM, EDA, Hardware Description
  prefs: []
  type: TYPE_NORMAL
- en: 1   Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Moore’s law driving increased design complexity and chip capacity, VLSI
    design and verification require more effort. Machine learning (ML) has successfully
    integrated into EDA for logic synthesis [[8](#bib.bib8), [10](#bib.bib10)], placement
    [[25](#bib.bib25)], routing [[12](#bib.bib12), [16](#bib.bib16)], testing [[5](#bib.bib5),
    [24](#bib.bib24), [14](#bib.bib14)], and verification [[7](#bib.bib7), [11](#bib.bib11)].
    The popularity of agile hardware design exploration has been on the rise due to
    the growth of large language models (LLMs). A promising direction is using natural
    language instruction to generate hardware description language (HDL). e.g., Verilog,
    aiming to greatly lower hardware design barriers and increase design productivity,
    especially for users who do not possess extensive expertise in chip design. Despite
    the efforts, Verilog benchmarking has unique challenges in terms of the wide range
    of hardware designs [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: Two orthogonal research and development trends have both attracted enormous
    interests [[22](#bib.bib22), [15](#bib.bib15), [13](#bib.bib13), [2](#bib.bib2),
    [4](#bib.bib4)]. The first trend is efficiently finetuning LLMs such as CodeGen [[17](#bib.bib17)],
    with representatives works such as Thakur et al. [[22](#bib.bib22)], Chip-Chat [[2](#bib.bib2)],
    Chip-GPT [[4](#bib.bib4)]. However, due to limited Verilog data sources, these
    works mainly target the scale of simple and small circuits (e.g., <20 designs
    with a medium of <45 HDL lines) [[15](#bib.bib15)]. The relatively low scalability
    and solution quality have propelled the second trend – enrich Verilog source.
    Like oil, data is an immensely valuable resource. One could not generate high
    quality and comprehensive HDL codes without having LLMs trained on vast amount
    of such data. RTLLM [[15](#bib.bib15)] and VerilogEval [[13](#bib.bib13)] introduce
    specialized benchmarking framework (i.e., 30 designs from RTLLm and 156 designs
    from HDLBits [[9](#bib.bib9)] from VerilogEval) to assess the generation quality
    of LLMs. However, they either do not offer Power, Performance, and Area (PPA)
    analysis for the generated codes (e.g., VerilogEval), or the generated Verilog
    codes are directly extracted and synthesized using commercial tools to obtain
    PPA results, without considering PPA feedback (e.g., RTLLM). Thus, their solution
    quality is still limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, as the first attempt, we integrate power, performance, and area-constraints
    into Verilog generation, and propose VeriPPA, a open-source framework with multi-round
    Verilog generation and error feedback, shown in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). We first
    generate initial Verilog codes using LLMs, followed by a unique two-stage refinement
    process. The first stage focuses on improving the syntax and functionality, while
    the second stage aims to optimize the code in line with PPA constraints, an essential
    aspect to ensure hardware design quality. Compared with state-of-the-arts (SOTAs),
    e.g., RTLLM [[15](#bib.bib15)], VerilogEval [[13](#bib.bib13)], our VeriPPA achieves
    a success rate of 62.0% (+16%) for functional accuracy and 81.37% (+8.3%) for
    syntactic correctness in Verilog code generation. Our key contributions are summarized
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the detailed error diagnostics from the iverilog simulator [[26](#bib.bib26)],
    and pinpoint the exact location of syntactic or functional discrepancies as indicated
    by testbench failures as new prompts. We use multi-round generation to enhance
    the syntax and functionality correctness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To further ensure that the generated Verilog codes are *synthesizable*, and
    design quality (PPA) is sound, we use Synopsys Design Compiler to perform logic
    synthesis (and technology mapping) on the open source ASAP 7nm Predictive PDK,
    and check all designs’ warnings/errors, and PPA report. We then integrate these
    PPA reports and warnings/errors with our PPA goal into the next round prompt for
    further refinement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We incorporate in-context learning (ICL) to significantly improve the LLM performance
    in generating Verilog codes with only a few demonstration examples, especially
    when labeled data are scarce. By carefully selecting diverse text-to-Verilog pairs,
    ICL demonstrates superior performance and generalization capabilities compared
    to fine-tuning in limited example scenarios, thus increasing the performance of
    Verilog code generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d6d0d6ae46ec572cdfb3e88c4804aef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: This visualization captures the step-by-step process where an LLM
    synthesizes Verilog codes from hardware design prompts, with the ensuing code
    subjected to thorough validation by a Simulator and scrutinized for adherence
    to Power-Performance-Area (PPA) checks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2   Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are mainly two directions related to Verilog generation.
  prefs: []
  type: TYPE_NORMAL
- en: Finetune LLMs. Thakur et al. [[22](#bib.bib22)] advocate for the fine-tuning
    of open-source LLMs such as CodeGen [[17](#bib.bib17)] to specifically generate
    Verilog code tailored for target designs. Subsequently, Chip-Chat [[2](#bib.bib2)]
    delves into the intricacies of hardware design using LLMs, highlighting the markedly
    superior performance of ChatGPT compared to other open-source LLMs. Chip-GPT [[4](#bib.bib4)]
    also focuses on the task of RTL design by leveraging the capabilities of ChatGPT.
    These studies pave the way for a promising future where language models play a
    pivotal role in facilitating and enhancing various aspects of agile hardware design
    exploration. However, these works mainly target the scale of simple and small
    circuits (e.g., ¡20 designs with a medium of ¡45 HDL lines), as pointed out in [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: Enrich Verilog Source. Like oil, data is an immensely valuable resource. Recent
    efforts have been focusing on enriching the Verilog data source. RTLLM [[15](#bib.bib15)]
    introduces a benchmarking framework consisting of 30 designs that are specifically
    aimed at enhancing the scalability of benchmark designs. Furthermore, it utilizes
    effective prompt engineering techniques to improve the generation quality. VerilogEval [[13](#bib.bib13)]
    assesses the performance of LLM in the realm of Verilog code generation for hardware
    design and verification. It comprises 156 problems from the Verilog instructional
    website HDLBits. However, VerilogEval [[13](#bib.bib13)] does not offer PPA analysis
    for the generated codes. In RTLLM, the generated Verilog codes are directly extracted
    and synthesized using commercial tools to obtain PPA results, without PPA constraint-based
    feedback. Thus they suffer from limited generation quality.
  prefs: []
  type: TYPE_NORMAL
- en: 3   VeriPPA FRAMEWORK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1   Design Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our VeriPPA framework, as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"), we use
    a text-based Description of Hardware Design, designated as $L$ is then subjected
    to a rigorous validation sequence, beginning with a Simulator that checks both
    syntax and functionality. In the first loop, if unsuccessful, we will input the
    outcomes, along with any syntax and functionality errors, into the LLM for the
    generation of subsequent attempts. If successful, the code undergoes Power-Performance-Area
    (PPA) checks to ensure compliance with constraints. In the second loop, we check
    all designs’ warnings/errors during logic synthesis and PPA reports. If not satisfied,
    both the design and its corresponding PPA report will be fed back to the VeriRectify
    (Section [3.3](#S3.SS3 "3.3 VeriRectify ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis")) for refinement. This validation workflow
    ensures that the LLM-generated Verilog codes not only meets functional specifications
    but is also optimized for PPA considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2   Code Generation and Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Utilizing design descriptions $L$ are inputs to the ICARUS Verilog simulator
    [[26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: that systematically assess the code’s functionality, encompassing a wide array
    of test scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of the ICARUS simulator into in VeriPPA framework enhances the
    verification process, enabling an evaluation function $V$) that provides immediate
    feedback on the code’s syntactical and operational integrity. This integrated
    approach contrasts with frameworks such as RTLLM [[15](#bib.bib15)], where an
    external simulator is used to check the correctness of the generated Verilog codes.
    Since taking out the generated code can not provide instant detailed diagnostics
    of the code. Therefore, we need an integrated approach to get the detailed diagnostics
    of errors. With VeriPPA, detailed error reports from the simulator expedite the
    identification of faults, facilitating their rectification and refining the code
    generation process. This iterative, error-informed development cycle is discussed
    in further detail in the following sections, illustrating how precise error detection
    informs subsequent iterations of code generation within VeriPPA framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eaf75f0e1899e83aad0fb0c83bde4fd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The diagram illustrates the process of syntactic and functional code
    verification.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3   VeriRectify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our approach, we leverage detailed error diagnostics from the simulator for
    generated codes. This refinement process, termed ”VeriRectify,” is pivotal to
    ensuring the integrity of the generated Verilog codes. The VeriRectify phase ingests
    diagnostics from the iverilog simulator [[26](#bib.bib26)], pinpointing the exact
    location of syntactic discrepancies or functional discrepancies as indicated by
    testbench failures. This diagnostic data, when amalgamated with the antecedent
    code generation attempt in the process where the LLM generates Verilog code. The
    LLM then employs this enriched prompt to rectify and evolve the Verilog code toward
    compliance with specified correctness criteria, following the iterative relation
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V(n+1)=V(n)-E(V(n))$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $V(n)$ represents the identified errors. The VeriRectify workflow is
    depicted in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Code Generation and Testing ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). The figure’s
    top section displays the Syntax Error and Functional Error found in the output
    of our simulation tool (e.g., booth_multiplier), which is the output from our
    simulation tool. During the rectification phase (Error Refinement), we utilize
    a tailored prompt (integrating the Error Details) for the LLMs, enabling it to
    correct the flaws identified in the Verilog codes initially produced. This approach
    allows the LLM to specifically address the encountered issues, which is fundamentally
    different from RTLLM [[15](#bib.bib15)] (using a generalized prompt for all designs).
    As evident in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Code Generation and Testing ‣
    3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"), the LLM
    effectively amends the issues in the Verilog codes for the booth_multiplier design.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4   Multi-round Conversation with Error Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further refine the generated Verilog codes, we employ a multi-round conversation
    with error feedback loop analogous to human problem-solving techniques. This approach
    can be conceptualized as a function that iteratively refines the output by considering
    the errors of previous steps. Let $V_{i}$. The iterative process can be viewed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V_{i+1}=R(V_{i},E_{i})\quad\text{and}\quad E_{i+1}=D(V_{i+1})$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: This process repeats until either no errors are detected or a predefined iteration
    limit, $K$.
  prefs: []
  type: TYPE_NORMAL
- en: 0:  User prompt $P$ then 13:     $V_{final}\leftarrow V_{i}$
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Multi-Round Verilog Code Generation using LLM
  prefs: []
  type: TYPE_NORMAL
- en: 3.5   Power Performance & Area (PPA) Checking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *VeriRectify* process ensures the design to pass both RTL syntax check and
    cycle-accurate functional simulation. However, RTL simulation does not guarantee
    that the design (Verilog code) is *synthesizable*. Furthermore, the quality of
    the hardware design must be measured by its power, performance, and area metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach takes a step further by inspecting PPA of the design $V$ which
    passes the *VeriRectify* process as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V=\left\{\begin{matrix}V&amp;\text{if}~{}PPA(V)~{}\text{satisfies},\\
    VeriRectify(V,PPA(V))&amp;\text{otherwise.}\end{matrix}\right.$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'In this work, our PPA check calls Synopsys Design Compiler to perform logic
    synthesis (and technology mapping) on the open-source ASAP 7nm Predictive PDK [[23](#bib.bib23)].
    We check all designs’ warning/error messages during the logic synthesis, and the
    power ($nW$) for quality. When the Verilog design can be synthesized and meets
    the PPA goal, it results in a pass. Otherwise, both the design and its corresponding
    PPA report will be fed back to the VeriRectify (Section [3.3](#S3.SS3 "3.3 VeriRectify
    ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis")) for refinement.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1edadfb6d87c9c789d3ca32bde6f78da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Correctness of generated Verilog code with respect to correction
    attempt on RTLLM, using (a) GPT-3.5; (b) GPT-4-v1; (c) GPT-4; (d) GPT-4-4shot'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6   In-Context Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs have demonstrated remarkable in-context learning (ICL) capabilities. Given
    a few input-label pairs as demonstrations, they can predict labels for unseen
    inputs without requiring parameter updates [[21](#bib.bib21), [3](#bib.bib3)].
    In-context learning can be viewed as a meta-optimizer and may also be regarded
    as implicit fine-tuning [[6](#bib.bib6)]. While off-the-shelf LLM APIs do not
    offer extensive customization options, such as fine-tuning, ICL can significantly
    improve LLM’s in-domain performance with only a few examples. In situations where
    labeled data is scarce, ICL has been shown to outperform explicit fine-tuning [[20](#bib.bib20)]
    on downstream tasks substantially. The availability of small training set poses
    a challenge in avoiding overfitting; however, it highlights the advantage of ICL
    over fine-tuning in scenarios with limited examples. In these cases, ICL demonstrates
    superior performance and better generalization capabilities, while fine-tuning
    may struggle with overfitting issues and result in poor generalization. A single
    LLM can achieve robust performance on multiple tasks using only its text interface:
    a few task examples are provided to the model as a prompt, accompanied by a query
    input, and the model generates a continuation to produce a predicted output for
    that query.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $q(t&#124;v)=\prod_{k=1}^{K}q(t_{k}&#124;t_{<k}),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: In this equation, $t_{k}$ is parameterized by an LLM. The equation describes
    the in-context learning of a large language model, where the model learns to predict
    the next token in the sequence by considering the previous demonstration examples.
  prefs: []
  type: TYPE_NORMAL
- en: The selection of few-shot examples [[20](#bib.bib20), [1](#bib.bib1)] also influences
    the generalization capability of LLMs on downstream tasks. In our case, we carefully
    select the text-to-Verilog pairs to ensure that the examples cover a range of
    different Verilog designs, such as addition, multiplication, single-stage design,
    and pipelined design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: PPA results of generated Verilog code'
  prefs: []
  type: TYPE_NORMAL
- en: '| Design Name | GPT-4 | GPT-4 (4-shot) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Clock &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (ps) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Power &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\scriptstyle\mu$W) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\scriptstyle\mu$m²) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Clock &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (ps) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Power &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\scriptstyle\mu$W) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\scriptstyle\mu$m²) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| adder_8bit | 318.5 | 6.3 | 38.5 | 333.1 | 6.1 | 42.9 |'
  prefs: []
  type: TYPE_TB
- en: '| adder_16bit | 342.2 | 10.9 | 104.5 | 135.1 | 41.1 | 152.8 |'
  prefs: []
  type: TYPE_TB
- en: '| adder_32bit | 500.0 | 14.2 | 211.6 | 500.0 | 14.7 | 213.2 |'
  prefs: []
  type: TYPE_TB
- en: '| multi_booth | 409.0 | 112.1 | 526.0 | 409.0 | 112.1 | 526.0 |'
  prefs: []
  type: TYPE_TB
- en: '| right_shifter | 47.5 | 144.3 | 42.9 | 47.5 | 144.3 | 42.9 |'
  prefs: []
  type: TYPE_TB
- en: '| width_8to16 | 74.1 | 223.2 | 145.8 | 145.6 | 128.7 | 157.2 |'
  prefs: []
  type: TYPE_TB
- en: '| edge_detect | 61.5 | 49.0 | 23.3 | 61.5 | 49.0 | 23.3 |'
  prefs: []
  type: TYPE_TB
- en: '| mux | 54.7 | 215.3 | 86.1 | 54.7 | 215.3 | 86.1 |'
  prefs: []
  type: TYPE_TB
- en: '| pe | 500.0 | 552.5 | 2546.5 | 500.0 | 541.0 | 2488.6 |'
  prefs: []
  type: TYPE_TB
- en: '| asyn_fifo | 295.2 | 406.4 | 1279.3 | 228.3 | 526.6 | 1295.4 |'
  prefs: []
  type: TYPE_TB
- en: '| counter_12 | 134.4 | 33.1 | 40.6 | 124.5 | 34.6 | 36.4 |'
  prefs: []
  type: TYPE_TB
- en: '| fsm | 88.3 | 32.7 | 31.5 | 68.7 | 49.0 | 50.2 |'
  prefs: []
  type: TYPE_TB
- en: '| multi_pipe_4bit | 254.7 | 40.7 | 131.3 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| pulse_detect | 10.3 | 187.5 | 13.5 | 32.7 | 59.1 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: '| calendar | - | - | - | 208.6 | 86.6 | 199.0 |'
  prefs: []
  type: TYPE_TB
- en: 4   Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1   Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In assessing our VeriPPA framework, we utilize two benchmark datasets. Firstly,
    the RTLLLM dataset[[15](#bib.bib15)] includes 29 designs. Notably, it originally
    contained 30 designs, but the risc_cpu design is currently unavailable. Secondly,
    we employ the VerilogEval dataset [[13](#bib.bib13)], which comprises two subsets:
    VerilogEval-human, featuring 156 designs, and VerilogEval-machine, consisting
    of 108 designs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2   Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We demonstrate the effectiveness of our VeriPPA framework for generating PPA-optimized
    Verilog code for the given designs. We adopt, GPT-3.5 [[18](#bib.bib18)] and GPT-4
    [[19](#bib.bib19)] as our LLM models. We use n=1, temperature temp = 0.7, and
    a context length of 2048 in our setting. Further, we incorporate the ICARUS Verilog
    simulator [[26](#bib.bib26)] to automate the testing of the generated code. For
    PPA check, we perform the logic synthesis using Synopsys Design Compiler with
    compile_ultra command and we use the ASAP 7nm Predictive PDK [[23](#bib.bib23)].
    We implemented a flow (Python script) that sweeps the timing constraints to find
    the fastest achievable clock frequency for all the generated designs. All experiments
    were conducted on a Linux- based host with AMD EPYC 7543 32-Core Processor and
    an NVIDIA A100-SXM 80 GB.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3   Generation Correctness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This study evaluates Verilog code generation accuracy using two primary metrics:
    syntax checking and functionality verification. Figure [3](#S3.F3 "Figure 3 ‣
    3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") presents our methodology for improving
    Verilog correctness through successive correction attempts and self-planing [[15](#bib.bib15)].
    We generate five different codes for each design description, attempting up to
    four corrections within each generation. However, after certain attempts, the
    efficiency of these corrections diminishes, as the LLMs tend to provide repetitive
    responses to identical errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking
    ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"), we plot
    syntax and functionality correctness percentages against the number of correction
    attempts. The graph features a solid line for syntax correctness and a dotted
    line for functionality correctness. Functionality is evaluated the same as RTLLM
    [[15](#bib.bib15)], considering a design functionally correct if at least one
    generated code passes the functionality test. We use GPT-3.5 and observe initial
    syntax correctness of 44.13% and functionality correctness of 24.13%, as shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣
    3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") (a). After
    applying correction attempts, these figures improved to 65.51% for syntax and
    31.03% for functionality. Compared to RTLLM, which initially scored 24.82% in
    syntax and 27.58% in functionality without corrections. Please note that RTTLM
    uses self panning in prompt. Integrating our correction approach with RTLLM increases
    the maximum syntax and functionality correctness to 49.65% and 34.48%, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then evaluate two versions of the GPT-4 model. The first, GPT-4-0314 (v1),
    showed an initial syntax correctness of 56.55% and functionality correctness of
    37.93%. Our correction methods raised these to 71.03% for syntax and 51.72% for
    functionality. Combining RTLLM with our approach, we further enhanced the syntax
    correctness to 75.17% and functionality to 51.72%, as indicated in Figure [3](#S3.F3
    "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA FRAMEWORK
    ‣ Advanced Large Language Model (LLM) - Driven Verilog Development: Enhancing
    Power, Performance, and Area Optimization in Code Synthesis") (b). For the base
    GPT-4 model, we noticed an increase in syntax correctness from 66.2% to 81.37%
    by the fourth attempt and in functionality from 37.93% to 48.27%. When combined
    with RTLLM and our correction techniques, the syntax correctness further improved
    from 60% to 77.93%, and functionality from 34.48% to 48.27%, as shown in Figure
    [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") (c).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, testing the GPT-4 model with four-shot learning, we observed an improvement
    in syntax from 70.34% to 79.31% and in functionality from 37.93% to 41.37%. With
    the addition of RTLLM and our correction methods, the functionality correctness
    notably increased from 44.82% to 62.06% after four attempts, as demonstrated in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3
    VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") (d). This
    significant improvement highlights the effectiveness of our methods in enhancing
    the functional accuracy of the designs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In evaluating our VeriPPA framework with VerilogEval data, we found notable
    improvements. For the VerilogEval-Machine dataset (Figure [4](#S4.F4 "Figure 4
    ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis") (a)), our method significantly increased syntax accuracy.
    Functionality accuracy also rose from 33.57% to 43.79% using GPT-4, and further
    to 45.25% with GPT-4’s four-shot learning. The VerilogEval-human dataset showed
    similar trends, with functionality accuracy improving from 29.48% to 39.74% through
    the application of GPT-4 and its four-shot learning variant as shown in the Figure
    [4](#S4.F4 "Figure 4 ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") (b). This underscores our framework’s
    effectiveness in enhancing both syntax and functionality in Verilog code generation'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f0955230d71e612049002e58ce7ca71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Correctness of generated Verilog code with respect to correction
    attempt on VerilogEval, using (a) VerilogEval-Machine, and (b) VerilogEval-Human.'
  prefs: []
  type: TYPE_NORMAL
- en: \captionof
  prefs: []
  type: TYPE_NORMAL
- en: figureOptimization Flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Optimized results'
  prefs: []
  type: TYPE_NORMAL
- en: '| Design Name | Clock (ps) | Power ($\mu$m) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| adder_32bit | 180.0 | 587.31 | 1005.67 |'
  prefs: []
  type: TYPE_TB
- en: '| multi_booth | 123.2 | 42.39 | 42.92 |'
  prefs: []
  type: TYPE_TB
- en: '| pe | 325.0 | 1206.0 | 4863.88 |'
  prefs: []
  type: TYPE_TB
- en: '| asyn_fifo | 114.8 | 988.92 | 1344.86 |'
  prefs: []
  type: TYPE_TB
- en: '| radix2_div | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 4.4   PPA Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In VeriPPA, we use the Synopsis Design Compiler for synthesizing the designs,
    culminating in the production of PPA reports. The PPA results of complex designs
    are encapsulated in Table [1](#S3.T1 "Table 1 ‣ 3.6 In-Context Learning ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). This
    table, though comprehensive, does not encompass specific design constraints. Similar
    to the ChipGPT approach [[4](#bib.bib4)], where an output manager and enumerative
    search finalize the PPA from multiple reports, our process also generates multiple
    PPA reports for each design. An example is the $pulse\_detect$ design, and we
    selected the most optimized one to include in Table [1](#S3.T1 "Table 1 ‣ 3.6
    In-Context Learning ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is crucial that PPA results do not conform to specialized design requirements,
    a standard practice in industrial applications. To address this disparity, we
    further perform the PPA constraint-based feedback mechanism, integrated with context-based
    learning, as illustrated in Figure [4.3](#S4.SS3 "4.3 Generation Correctness ‣
    4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). This
    approach represents a significant step towards aligning LLM-generated code with
    industry-specific PPA requirements. Figure [4.3](#S4.SS3 "4.3 Generation Correctness
    ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") demonstrates
    our process, starting with the collection of synthesized design outputs that require
    optimization. For example, $adder\_32bit$, we impose a clock constraint, aiming
    for a clock speed of less than 300ps, as outlined in the PPA constraint-based
    prompt in Figure [4.3](#S4.SS3 "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced
    Large Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis"). The framework instructs the LLM to
    consider various optimization strategies, including Pipelining, Clock Gating,
    Parallel Operation, and Hierarchical Design. It also encourages the exploration
    of additional methods to generate Verilog code that meets the defined optimization
    constraints, as illustrated in the context-based learning segment of Figure [4.3](#S4.SS3
    "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon providing the PPA-based constraint prompt and context to the LLM, we analyze
    the resultant Verilog code for syntax and functional accuracy, making corrections
    where necessary. If the code passes both checks, we proceed to its final synthesis,
    achieving an optimized Verilog code as shown in Figure [4.3](#S4.SS3 "4.3 Generation
    Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog
    Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis")
    (d), where the $adder\_32bit$ operates at an improved 180ps clock. In Table [2](#S4.T2
    "Table 2 ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language
    Model (LLM) - Driven Verilog Development: Enhancing Power, Performance, and Area
    Optimization in Code Synthesis"), we present the results of selected optimized
    designs. Due to the page limit and we only show substantial optimizations, the
    table does not include simpler designs. Notably, no design from the VerilogEval
    [[13](#bib.bib13)] dataset features in Table [2](#S4.T2 "Table 2 ‣ 4.3 Generation
    Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog
    Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis"),
    as those designs did not require complex optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 5   conlcusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce a novel framework VeriPPA, designed to assess and
    enhance LLM efficiency in this specialized area. Our method includes generating
    initial Verilog code using LLMs, followed by a unique two-stage refinement process.
    The first stage focuses on improving the functional and syntactic integrity of
    the code, while the second stage aims to optimize the code in line with Power-Performance-Area
    (PPA) constraints, an essential aspect of effective hardware design. This dual-phase
    approach of error correction and PPA optimization has led to notable improvements
    in the quality of LLM-generated Verilog code. Our framework schieves 62.0% (+16%)
    for functional accuracy and 81.37% (+8.3%) for syntactic correctness in Verilog
    code generation, compared to SOTAs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Alon Albalak et al. Improving few-shot generalization by exploring and
    exploiting auxiliary data. arXiv preprint arXiv:2302.00674, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Chip-chat:
    Challenges and opportunities in conversational hardware design. arXiv preprint
    arXiv:2305.13243, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Tom Brown et al. Language models are few-shot learners. Advances in neural
    information processing systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Kaiyan Chang et al. Chipgpt: How far are we from natural language hardware
    design, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Wen Chen et al. Novel test detection to improve simulation efficiency:
    A commercial experiment. In ICCAD’12, page 101–108, New York, NY, USA, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Damai Dai et al. Why can gpt learn in-context? language models implicitly
    perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical
    and Empirical Understanding of Foundation Models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Shai Fine and Avi Ziv. Coverage directed test generation for functional
    verification using bayesian networks. In DAC ’03, page 286–291, New York, NY,
    USA, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Winston Haaswijk et al. Deep learning for logic optimization algorithms.
    In 2018 ISCAS, pages 1–4, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] HDLBits. Hdlbits:verilog practice. [https://hdlbits.01xz.net](https://hdlbits.01xz.net),
    2023. Accessed on 11/20/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Abdelrahman Hosny et al. Drills: Deep reinforcement learning for logic
    synthesis. In 2020 25th ASP-DAC, pages 581–586, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Hanbin Hu et al. Hfmv: Hybridizing formal methods and machine learning
    for verification of analog and mixed-signal circuits. In DAC ’18, New York, NY,
    USA, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Rongjian Liang et al. Drc hotspot prediction at sub-10nm process nodes
    using customized convolutional network. In ISPD ’20, page 135–142, New York, NY,
    USA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Mingjie Liu et al. VerilogEval: evaluating large language models for verilog
    code generation. In ICCAD’23, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Zeye Liu et al. Improving test chip design efficiency via machine learning.
    In 2019 ITC, pages 1–10, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. Rtllm: An open-source
    benchmark for design rtl generation with large language model, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Dani Maarouf et al. Machine-learning based congestion estimation for modern
    fpgas. In FPL’18, pages 427–4277, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for
    code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] OpenAI. Gpt-3.5. [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5),
    2023. Accessed on 15/11/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] OpenAI. Gpt-4. [https://platform.openai.com/docs/models/gpt-4](https://platform.openai.com/docs/models/gpt-4),
    2023. Accessed on 15/11/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Ethan Perez et al. True few-shot learning with language models. Advances
    in neural information processing systems, 34:11054–11070, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Alec Radford et al. Language models are unsupervised multitask learners.
    OpenAI blog, 1(8):9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Shailja Thakur et al. Benchmarking large language models for automated
    verilog rtl code generation. In DATE’23, pages 1–6\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Vinay Vashishtha, Manoj Vangala, and Lawrence T Clark. Asap7 predictive
    design kit development and cell design technology co-optimization. In IEEE/ACM
    International Conference on Computer-Aided Design (ICCAD), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Fanchao Wang et al. Accelerating coverage directed test generation for
    functional verification: A neural network-based framework. In GLSVLSI’18, page
    207–212, New York, NY, USA, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Samuel Ward, Duo Ding, and David Z Pan. Pade: A high-performance placer
    with automatic datapath extraction and evaluation through high dimensional data
    learning. In Proceedings of the 49th Annual Design Automation Conference, pages
    756–761, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Williams. The icarus verilog compilation system, 2023. [Online]. Available:
    [https://github.com/steveicarus/iverilog](https://github.com/steveicarus/iverilog).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
