- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.07944](https://ar5iv.labs.arxiv.org/html/2406.07944)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meiziniu Li, Dongze Li, Jianmeng Liu, Jialun Cao, Yongqiang Tian, Shing-Chi
    Cheung The Hong Kong University of Science and Technology, Hong Kong, China
  prefs: []
  type: TYPE_NORMAL
- en: mlick@cse.ust.hk, dlibk@connect.ust.hk, jliudq@connect.ust.hk, jcaoap@cse.ust.hk,
    yqtian@ust.hk, scc@cse.ust.hk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Testing is a major approach to ensuring the quality of deep learning (DL) libraries.
    Existing testing techniques commonly adopt differential testing to relieve the
    need for test oracle construction. However, these techniques are limited in finding
    implementations that offer the same functionality and generating diverse test
    inputs for differential testing. This paper introduces DLLens, a novel differential
    testing technique for DL library testing. Our insight is that APIs in different
    DL libraries are commonly designed to accomplish various computations for the
    same set of published DL algorithms. Although the mapping of these APIs is not
    often one-to-one, we observe that their computations can be mutually simulated
    after proper composition and adaptation. The use of these simulation counterparts
    facilitates differential testing for the detection of functional DL library bugs.
    Leveraging the insight, we propose DLLens as a novel mechanism that utilizes a
    large language model (LLM) to synthesize valid counterparts of DL library APIs.
    To generate diverse test inputs, DLLens incorporates a static analysis method
    aided by LLMs to extract path constraints from all execution paths in each API
    and its counterpart’s implementations. These path constraints are then used to
    guide the generation of diverse test inputs. We evaluate DLLens on two popular
    DL libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can synthesize
    counterparts for more than twice as many APIs found by state-of-the-art techniques
    on these libraries. Moreover, DLLens can extract 26.7% more constraints and detect
    2.5 times as many bugs as state-of-the-art techniques. DLLens has successfully
    found 56bugs in recent TensorFlow and PyTorch libraries. Among them, 41are previously
    unknown, 39of which have been confirmed by developers after reporting, and 19
    of those confirmed bugs have been fixed by developers.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning (DL) has been increasingly adopted for mission-critical applications
    such as authentication [[1](#bib.bib1), [2](#bib.bib2)], medical treatment [[3](#bib.bib3)],
    and autonomous driving [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Current
    DL applications are mostly developed on top of popular DL libraries such as PyTorch [[7](#bib.bib7)]
    and TensorFlow [[8](#bib.bib8)]. However, the presence of functional incorrectness,
    commonly referred to as functional bugs, in DL libraries poses a significant threat
    to the reliability of these applications [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)].
    More than 30% of bugs reported to PyTorch developers are categorized as functional
    bugs [[12](#bib.bib12)]. Hence, functional bug detection is critical to assuring
    the quality of DL libraries.
  prefs: []
  type: TYPE_NORMAL
- en: A functional bug occurs when the library API’s behavior deviates from its specified
    requirements [[13](#bib.bib13)]. Currently, differential testing is commonly adopted [[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)] to detect
    these functional bugs (e.g., the incorrect computation results or intermediate
    states [[12](#bib.bib12)]). A general workflow of existing differential testing
    techniques for DL libraries is to (1) collect the different implementations that
    offer the same functionality in/across DL libraries, and (2) apply test inputs
    to detect output inconsistency. However, existing works are limited in both steps,
    which may hinder their effectiveness in bug detection.
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Ineffectiveness in finding counterparts for differential testing. Two paradigms
    are proposed to find counterparts of DL library APIs for differential testing.
    The first one [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)] takes advantage of model conversion libraries (e.g., TF2ONNX [[19](#bib.bib19)]),
    which support the conversion of DL models between libraries. However, the effectiveness
    of this line of research is limited by the insufficient API coverage of conversion
    libraries. Take TF2ONNX [[19](#bib.bib19)] as an example, it only supports the
    conversion of 8% (279/3,316) TensorFlow APIs [[18](#bib.bib18)], leaving 90%+
    of TensorFlow APIs uncovered. Another paradigm takes advantage of different computational
    modes, such as backends [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]
    (e.g., CPU and GPU), execution modes [[23](#bib.bib23)] (e.g., different gradient
    calculation modes), and similar APIs [[24](#bib.bib24)] (e.g., tf.argmax and tf.math.argmax)
    in one library. Similar APIs or executions of an API under these different computational
    modes could also serve as counterparts of each other. However, these counterparts
    often have nearly identical implementations, limiting their effectiveness for
    differential testing. Listing [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ DLLens:
    Testing Deep Learning Libraries via LLM-aided Synthesis") shows a real functional
    bug that cannot be triggered by either of the existing paradigms, since none of
    the conversion libraries support the API (tf.math.is_non_decreasing), and the
    concerned computational modes deliver consistent outputs [[25](#bib.bib25)].'
  prefs: []
  type: TYPE_NORMAL
- en: '12|\underline{\textbf{Bug  Triggering  Input}}|:3x  =  tf.constant([10,9],  dtype=’uint32’)4|\underline{\textbf{Buggy  API}}|:5tf.math.is_non_decreasing6|\underline{\textbf{Actual  Result  (Expected  Result)}}|:7|\textbf{\textcolor{bgRed}{True}}|  (|\textbf{\textcolor{bgGreen}{False}}|)8|\underline{\textbf{Developer’s  Fix}}|:9|\textbf{\textcolor{bgRed}{-  diff  =  \_get\_diff\_for\_monotonic\_comparison(x)}}|10|\textbf{\textcolor{bgRed}{-  zero  =  ops.convert\_to\_tensor(0,  dtype=diff.dtype)}}|11|\textbf{\textcolor{bgRed}{-  return  math\_ops.reduce\_all(math\_ops.less\_equal(zero,  diff))}}|12|\textbf{\textcolor{bgGreen}{+  diff  =  \_get\_results\_for\_monotonic\_comparison(x,  greater\_equal)}}|13|\textbf{\textcolor{bgGreen}{+  return  math\_ops.reduce\_all(diff)}}|’'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: A real bug detected by our tool that leads to incorrect computation
    result.'
  prefs: []
  type: TYPE_NORMAL
- en: '(2) Ineffectiveness in generating diverse test inputs. Another major cause
    is the ineffectiveness of generating test inputs that induce inconsistent outputs
    across different implementations. Existing DL library testing approaches commonly
    generate test inputs towards covering more APIs/statements/branches [[21](#bib.bib21),
    [17](#bib.bib17), [26](#bib.bib26)]. However, generating test inputs to trigger
    bugs inside these buggy APIs/statements/branches may be more complicated. Take
    Listing [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis") as an example, the buggy line (line 6) leads
    to an integer overflow [[27](#bib.bib27)]. The bug can only be triggered when
    satisfying the following two conditions: (1) (delta ¿ 0 && start ¿ limit)——delta
    ¡ 0 && start ¡ limit, and (2) limit is a large negative integer && start is a
    large positive integer. Indeed, coverage-guided approaches could generate inputs
    that satisfy condition (1), yet fail to explore different values of start and
    limit to meet condition 2. In other words, these approaches are ineffective in
    exploring inconsistency-inducing test inputs along the execution paths that contain
    faulty statements.'
  prefs: []
  type: TYPE_NORMAL
- en: '12|\underline{\textbf{Buggy  Code}}|3...41:  if  (delta  >  0)  {52:  //  Error  if  start  <=  limit63:  }  else  {74:  //  Error  if  start  >=  limit85:  }9...106:  Eigen::numext::ceil(Eigen::numext::abs(|\textbf{\textcolor{bgRed}{(limit  -  start)  /  delta}}|));  //  |\textcolor{bgRed}{\textbf{Buggy  Line}}|11|\underline{\textbf{Our  Proposed  Fix  Confirmed  By  a  Developer}}|126:  Eigen::numext::ceil(Eigen::numext::abs(|\textbf{\textcolor{bgGreen}{limit  /  delta  -  start  /  delta}}|));'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: A real bug detected by DLLens. It requires a specific value to trigger.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution. In this paper, we propose DLLens, a novel differential testing technique
    for DL library API testing. DLLens can 1) effectively find counterparts for DL
    library APIs, and 2) generate diverse inputs to explore execution paths extracted
    from implementations of DL library APIs and their counterparts. We observe that
    APIs are commonly designed to accomplish various computations for the same set
    of published DL algorithms. Although the mapping of these APIs is not often one-to-one,
    we observe that their computations can be simulated mutually after proper composition
    and adaptation. Based on this insight, DLLens synthesizes for each DL library
    API a function, referred to as the API’s counterpart, by leveraging a large language
    model (LLM). The counterpart simulates the API’s computation by composing and
    adapting various APIs from a different library. To generate diverse inputs, DLLens
    incorporates a novel static analysis method to extract path constraints for each
    execution path in the implementations of the API and its counterpart. Our static
    analysis method leverages an LLM to extract solvable path constraints from execution
    paths that involve external functions with unknown behaviors to existing solvers.
    Finally, DLLens applies a test input generation method based on the path constraints
    extracted from the implementation of both the concerned API and its counterpart,
    differential testing is further applied by checking output inconsistency between
    the API and its counterpart for bug detection. Taking the bug in Listing [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis") as an example, the buggy API is designed to check if a given tensor
    is following the non-decreasing order (i.e., for tensor $[x[0],...],x[i]\leq x[i+1]$),
    which is a commonly DL functionality [[28](#bib.bib28)]. DLLens detected this
    bug by synthesizing its counterpart, which is a function built on top of PyTorch
    APIs (see Listing [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ DLLens: Testing Deep
    Learning Libraries via LLM-aided Synthesis")), and generating a diverse test input
    that can expose this bug.'
  prefs: []
  type: TYPE_NORMAL
- en: '12|\underline{\textbf{TensorFlow  API}}|3tf.math.is_non_decreasing(x)4|\underline{\textbf{Function  Using  PyTorch’s  APIs}}|5def  pytorch_call(x):6return  torch.all(torch.eq(x,  torch.sort(x)[0]))’'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: A function using PyTorch’s APIs can implement the specified computation
    of tf.math.is_non_decreasing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the effectiveness of DLLens on two popular DL libraries: TensorFlow [[8](#bib.bib8)]
    and PyTorch [[7](#bib.bib7)]. We compare DLLens against existing DL library testing
    techniques on the effectiveness of counterpart synthesis, path constraint extraction,
    code coverage, and bug detection. The evaluation results show that DLLens’s counterpart
    synthesis method can double the number of API counterparts (e.g., 739 v.s. 304
    for TensorFlow APIs) collected by the existing work [[18](#bib.bib18)] that relies
    on developer-constructed rules. On average, DLLens can extract 21.5 and 41.87
    path constraints for each TensorFlow API and PyTorch API, respectively. Our experiment
    on 200 sampled APIs shows that DLLens can cover at least 3.2% branches and detect
    at least 2.5 times as many bugs as the state-of-the-art techniques. In total,
    DLLens detected 56bugs inside TensorFlow and PyTorch, including 15bugs already
    fixed or confirmed by developers and 41new bugs. So far, 39of our reported new
    bugs have been confirmed by developers, including 19 fixed and 2 reported bugs
    pending confirmation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This work makes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose DLLens, a novel DL library testing tool that can effectively synthesize
    API counterparts and extract path constraints for diverse test input generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLLens presents a new approach to counterpart synthesis using LLMs, alleviating
    the test oracle problems for the testing of DL library APIs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper proposes a novel path constraint extraction method that incorporates
    static analysis with LLMs to effectively extract path constraints inside DL libraries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We applied DLLens to two popular DL libraries, TensorFlow and PyTorch. DLLens
    successfully detected 56bugs. Most (41) are new bugs, and 39of them have been
    confirmed by developers after being reported. So far, 19 of our reported new bugs
    have been fixed by developers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Motivation and Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A DL Library Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing testing approaches for DL libraries focus on either the model level
    or API level. The former [[14](#bib.bib14), [29](#bib.bib29), [15](#bib.bib15),
    [26](#bib.bib26), [16](#bib.bib16), [17](#bib.bib17)] takes various DL models
    as test inputs and tries to exercise specific modules (*e.g.*, model construction,
    model training, and inference) in DL libraries. However, a recent study has revealed
    that these model-level testing approaches are ineffective in test adequacy [[20](#bib.bib20)].
    One potential explanation is that these approaches can operate on and manipulate
    only layer APIs within DL libraries, leaving the majority of library APIs unexplored.
    Particularly, these model-level testing approaches can cover at most 2.4% of the
    total deep learning library APIs, as reported by previous studies [[20](#bib.bib20),
    [21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: API-level testing, which improves the coverage of DL library APIs by directly
    executing them, provides an alternative approach to testing DL libraries. Several
    approaches [[20](#bib.bib20), [30](#bib.bib30), [31](#bib.bib31)] have been introduced
    to generate input arguments (i.e., test inputs) to test these APIs. These approaches
    employ various strategies, including collecting input arguments from open source
    projects [[20](#bib.bib20)], extracting constraints from documentation [[30](#bib.bib30)],
    and utilizing API’s input validation code [[31](#bib.bib31)]. More recently, TitanFuzz [[21](#bib.bib21)]
    and FuzzGPT [[32](#bib.bib32)] leverage large language models (LLMs) like ChatGPT
    for generating input arguments for DL library APIs. These approaches demonstrate
    the feasibility of providing examples and instructions to LLMs, allowing them
    to generate input arguments.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Functional Bug Detection In DL Libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing approaches, as mentioned in Section [II-A](#S2.SS1 "II-A DL Library
    Testing ‣ II Motivation and Related Works ‣ DLLens: Testing Deep Learning Libraries
    via LLM-aided Synthesis"), mostly adopt differential testing to check output consistency
    across implementations offering the same functionality to detect functional bugs
    in DL libraries. There are two widely used paradigms to find implementations offering
    the same functionality. The first paradigm [[14](#bib.bib14), [18](#bib.bib18)]
    leverages model conversion tools such as TF2ONNX [[19](#bib.bib19)] to compare
    outputs across multiple DL libraries. For instance, given a DL library API, TensorScope [[18](#bib.bib18)],
    parsed the conversion rules in TF2ONNX [[19](#bib.bib19)] to extract the counterpart
    of this API, which is implemented in another DL library. However, existing model
    conversion tools can only support counterparts for a limited number of DL library
    APIs. For example, TF2ONNX [[19](#bib.bib19)] only supports counterparts for only
    279 out of 3,316 TensorFlow APIs. Since the counterparts of many DL library APIs
    cannot be found in existing model conversion tools, functional bugs inside these
    DL library APIs may be missed (e.g., the bug in Listing [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis"))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second paradigm applies differential testing by comparing computation results
    of implementations within a single DL libraries. For instance, several works [[20](#bib.bib20),
    [23](#bib.bib23), [22](#bib.bib22), [21](#bib.bib21), [32](#bib.bib32)] detect
    functional bugs via checking if an API can have consistent outputs under different
    computation modes, such as backends (e.g., CPU and GPU), precision (e.g., float32,
    float16), and execution modes (e.g., different gradient calculation modes). Nevertheless,
    these implementations may yield identical computation results, making checking
    result inconsistency for bug detection ineffective. DeepRel [[24](#bib.bib24)]
    mined multiple groups of similar APIs within the same DL libraries based on API
    signature and document similarity. As for the similar APIs found by DeepRel, existing
    work [[18](#bib.bib18)] reveals that many of these APIs serve as callers, callees,
    or aliases of other APIs within the same group. For instance, tf.argmax and tf.math.argmax
    found by DeepReal are indeed aliases, suggesting that they are likely to have
    the same outputs. Indeed, when applying these differential testing strategies
    on one of our detected bugs (i.e., List [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")), we find
    that the buggy API has consistent output under different computation modes. Besides,
    this buggy API’s similar APIs detected by DeepRel produced identical output as
    the buggy API’s. Thus these approaches fail to detect this bug.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Constraint Extraction for DL Library Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some approaches have proposed constraint extraction strategies [[30](#bib.bib30),
    [31](#bib.bib31), [18](#bib.bib18)] to guide valid/invalid input generation. DocTer [[30](#bib.bib30)]
    collected constraints from DL library documentation and generated inputs that
    follow these constraints (denoted as valid inputs) and inputs violating these
    constraints. TensorScope [[18](#bib.bib18)] parsed the input validation check
    statements to generate valid inputs. However, both TensorScope and DocTer [[30](#bib.bib30)]
    do not provide constraints required by different execution paths inside the implementation
    of a DL library API.
  prefs: []
  type: TYPE_NORMAL
- en: ACETest [[31](#bib.bib31)] proposed a static analysis tool that extracted path
    constraints from the validation code inside DL library APIs, so input following
    these path constraints can pass the validation code to reach the core logic inside
    these APIs. However, due to the large size of DL libraries, ACETest [[31](#bib.bib31)]
    could not perform static analysis on the entire code base, leaving some extracted
    path constraints incomplete. Indeed, we observe that many path constraints extracted
    from the source code of DL library APIs contain functions from external libraries
    (e.g., Eigen) or external functions. Path constraints containing these external
    functions may not be solved properly since their behaviors are unknown to existing
    solvers, and their source code are not considered by ACETest in static analysis.
  prefs: []
  type: TYPE_NORMAL
- en: III Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36d03bcb26e0678e3efe386cdbb0cc7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Workflow of DLLens with three steps: counterpart synthesis (Section [III-A](#S3.SS1
    "III-A Counterpart Synthesis ‣ III Methodology ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis")), path constraint extraction (Section [III-B](#S3.SS2
    "III-B Path Constraint Extraction ‣ III Methodology ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis"), and input generation (Section [III-C](#S3.SS3
    "III-C Test Input Generation and Bug Detection ‣ III Methodology ‣ DLLens: Testing
    Deep Learning Libraries via LLM-aided Synthesis")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ III Methodology ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis") shows the workflow of DLLens, which consists
    of three steps, *i.e.*, (1) Counterpart Synthesis, (2) Path Constraint Extraction,
    and (3) Test Input Generation and Bug Detection. For each DL library API under
    test $f$ on these input arguments for bug detection.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Counterpart Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Criterion of counterparts. A counterpart of an API $f$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the above challenges, DLLens adopts a two-phase approach (as shown
    in Figure [4](#S3.F4 "Figure 4 ‣ III Methodology ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis") Step I). For each library API under test $f$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\forall x\in\mathcal{X},&#124;f(x)-f^{\prime}(x)&#124;\leq\epsilon$ |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon$ is a small positive value (e.g., 0.1). In the following, let
    us visit the two phases in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Valid Input Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DLLens adapts the direct prompting strategy proposed by TitanFuzz [[21](#bib.bib21)]
    and asks the LLM to generate valid inputs for each DL library API. More specifically,
    the prompt includes the target library (e.g., TensorFlow) and the API signature,
    it further describes the task of valid input generation using a step-by-step instruction
    (see Listing [5](#S3.F5 "Figure 5 ‣ III-A1 Valid Input Generation ‣ III-A Counterpart
    Synthesis ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis")). However, such direct prompting may not generate valid inputs satisfying
    the input constraint required by the DL library API. For instance, the LLM (i.e.,
    gpt-turbo-3.5) fails to generate input argument image that satisfies the required
    shape constraint of tf.image.pad_to_bounding, resulting in no valid inputs found
    for this API.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the performance of direct prompting in generating valid API inputs,
    DLLens dynamically executes each generated input with the API and back prompts
    the LLM with the error message for repair. The intuition is that DL libraries
    commonly raise meaningful invalid argument error messages [[18](#bib.bib18)] when
    receiving an invalid input, which can provide useful repair guidance. Listing [5](#S3.F5
    "Figure 5 ‣ III-A1 Valid Input Generation ‣ III-A Counterpart Synthesis ‣ III
    Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")
    exemplifies the input generation workflow. While the LLM initially generates an
    invalid input, it successfully generates a valid one after back prompting using
    the error message.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since APIs across DL libraries are built to implement the different parts of
    the same set of published deep learning computations, the search space of APIs
    relevant to the computation intended by $f$ is limited. So, a few valid inputs
    can effectively identify the appropriate choice of APIs for the counterpart synthesis.
    For a given API, DLLens obtains multiple (i.e., three) inputs from the LLM and
    guides the LLM to repair the invalid ones. If no valid inputs can be obtained
    for an API, DLLens skips the testing of this API because no counterpart can be
    reliably synthesized under equation [1](#S3.E1 "Equation 1 ‣ III-A Counterpart
    Synthesis ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: '12|\underline{\textbf{Prompt}}|:3Task  1:  Import  TensorFlow  2.10.04Task  2:  Generate  valid  parameter  ...5Task  3:  Call  the  function:  tf.image.pad_to_bounding_box(image,  ...)6|\underline{\textbf{LLM}}:|7image=tf.constant([[1,2],[3,4]],dtype=tf.float32)  |\textbf{\textcolor{bgRed}{(this  input  is  invalid!)}}|8|\underline{\textbf{Follow-up  Prompt}}|:9[Error  Message]10’image’  (shape(2,2))  must  have  3  or  4  dimensions.11|\underline{\textbf{LLM}}|:  Here  is  the  repaired  input12image=tf.constant([[[1,2],[3,4]]],dtype=tf.float32)  |\textbf{\textcolor{bgGreen}{(the  repaired  input  is  valid.)}}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Example Prompt for Valid Input Generation.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Counterpart Synthesis and Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the API under test $f$, in this stage, DLLens aims to synthesize a counterpart.
    One challenge in the counterpart synthesis lies in the alignment of the signatures
    and usage contexts between an API and the counterparts thus synthesized. For instance,
    the same functionality is provided by tf.keras.metrics.mean_squared_error and
    torch.nn.MSELoss APIs, but they have different signatures and usage contexts.
    Specifically, the former supports direct function calls, whereas the latter requires
    an object construction before processing the input. As a result, they cannot accept
    the same input. To address this alignment problem, DLLens includes valid inputs
    into the prompt that guides the LLM to synthesize functions with the same signature
    and usage context as the given API, as shown in [Figure 6](#S3.F6 "In III-A2 Counterpart
    Synthesis and Validation ‣ III-A Counterpart Synthesis ‣ III Methodology ‣ DLLens:
    Testing Deep Learning Libraries via LLM-aided Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing [6](#S3.F6 "Figure 6 ‣ III-A2 Counterpart Synthesis and Validation
    ‣ III-A Counterpart Synthesis ‣ III Methodology ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis") illustrates the synthesis of a counterpart
    for API tf.scatter_nd. By providing valid inputs and the API’s signature, DLLens
    instructs the LLM to search for an equivalent function. DLLens explicitly asks
    the LLM to synthesize a function using the APIs from a different DL library (e.g.,
    PyTorch). The rationale behind this is to prevent the LLM from searching for the
    given API’s caller, callee, or alias within the same DL library. If a synthesized
    function is invalid, i.e., unable to process valid inputs, DLLens feeds the error
    message to LLM for repair. Like the input generation workflow, DLLens synthesizes
    multiple counterpart candidates for a given API. A counterpart is successfully
    synthesized if it satisfies the output consistency criterion mentioned above.'
  prefs: []
  type: TYPE_NORMAL
- en: '12|\underline{\textbf{Prompt}}|:3Given  these  inputs,  generate  a  function  using  PyTorch  APIs  that  can  perform  the  same  functionality  as  the  given  TensorFlow  API.4|\textit{[Input  Set]}|5indices  =  tf.constant([[0],  [2]])...6|\textit{[TensorFlow  API]}|7tf.scatter_nd(indices,updates,shape,name=None)8|\textit{[Function  using  PyTorch  APIs]}|9def  pytorch_call(indices,updates,shape):10|\underline{\textbf{LLM}}:|11def  pytorch_call(indices,updates,shape):12...13tensor_shape=torch.Size(shape)14output_tensor=torch.zeros(tensor_shape)15|\textbf{\textcolor{bgRed}{output\_tensor.scatter\_(0,  tensor\_indices,tensor\_updates)}}|  |\textbf{\textcolor{bgRed}{(this  line  is  buggy!)}}|16return  output_tensor17|\underline{\textbf{Follow-up  Prompt}}|:18[Error  Message]19|\underline{Index}|  tensor  must  have  the  same  number  of  dimentions20as  |\underline{self}|  tensor...21|\underline{\textbf{LLM}}|:22def  pytorch_call(...):23...24|\textbf{\textcolor{bgGreen}{output\_tensor.scatter\_(dim=0,  index=tensor\_indices.squeeze(),  src=tensor\_updates)}}|  |\textbf{\textcolor{bgGreen}{(the  repaired  line  is  correct)}}|25return  output_tensor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Example Prompt for Counterpart Synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Path Constraint Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a DL library API and its counterpart synthesized in Sec [III-A](#S3.SS1
    "III-A Counterpart Synthesis ‣ III Methodology ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis"), DLLens extracts path constraints from the
    control flow graphs of their implementations. Specifically, DLLens extracts a
    path constraint for each execution path in the control flow graph. Each path constraint
    specifies the input conditions for the execution of the concerned path [[33](#bib.bib33)],
    where each input condition is a condition on input arguments of the DL library
    API and its counterpart. For each loop in the concerned path, DLLens follows the
    existing work [[31](#bib.bib31)] to unroll the loop once and ignore the loop condition.'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, each extracted path constraint is a conjunction of the input conditions
    extracted from the sanity check statements and the control flow statements (e.g.,
    if statement) reached by the concerned execution path. Sanity check statements
    include built-in assertion statements and validation check function calls (e.g.,
    the TORCH_CHECK function call) written by DL library developers. Following the
    existing work [[31](#bib.bib31)], DLLens considers two specific function calls,
    i.e., OP_REQUIRES⁢ in TensorFlow and TORCH_CHECK⁢ in PyTorch. In specific, for
    each validation check function call, DLLens targets its predicate expression,
    i.e., the second argument of the OP_REQUIRES and the first argument of the TORCH_CHECK).
    For each execution path, DLLens adds the True condition of the predicate expression
    in each reached validation check function call and the True or False condition
    assumed by each reached control flow statements to its path constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The extracted path constraints from DL libraries usually involve operations
    of tensors, which are complex data structures not supported by SMT solvers such
    as Z3 [[34](#bib.bib34)]. DLLens adopts a constraint model (see Table [I](#S3.T1
    "Table I ‣ III-B Path Constraint Extraction ‣ III Methodology ‣ DLLens: Testing
    Deep Learning Libraries via LLM-aided Synthesis")) proposed by existing study [[31](#bib.bib31)],
    which observed that most conditions for a tensor input argument only constrain
    specific properties (e.g., shape) while the value of the tensor is not constrained
    in most cases [[31](#bib.bib31), [30](#bib.bib30)]. Specifically, DLLens follows
    existing work to reduce a few tensor operations, which are unknown by existing
    SMT solvers. For instance, given a constraint “len(¡tensor¿.shape) ¡ 1 is True”
    with an unknown operation len() that calculates the length of a tensor’s shape.
    Since the length of a tensor’s shape, by definition, is the number of dimensions
    (ndims) held by the tensor; therefore, DLLens will reduce it to “¡tensor¿.ndims¡1
    is True”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Tensor Property Model'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Description | Variable Type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ndims | number of dimensions | integer |'
  prefs: []
  type: TYPE_TB
- en: '| shape | number of elements in each dimension | list of integers |'
  prefs: []
  type: TYPE_TB
- en: '| dtype | data type (e.g., int32, float64) | string |'
  prefs: []
  type: TYPE_TB
- en: '| num_element | number of elements | integer |'
  prefs: []
  type: TYPE_TB
- en: 'However, solving path constraints can be challenging. Due to the large size
    of DL libraries and their upstream libraries, it would be too expensive to perform
    the static analysis on the entire code base [[31](#bib.bib31)]. Therefore, DLLens
    follows existing work [[31](#bib.bib31)] to perform the static analysis on code
    within the same module. However, we notice that input conditions in path constraints
    usually involve functions written in external libraries (e.g., Eigen, a popular
    C++ library for linear algebra) or external modules (e.g., TensorUtils). These
    external functions complicate the solving of path constraints since their behaviors
    are unknown to existing solvers. Taking the implementation in Figure [8](#S3.F8
    "Figure 8 ‣ III-B Path Constraint Extraction ‣ III Methodology ‣ DLLens: Testing
    Deep Learning Libraries via LLM-aided Synthesis") as an example. The input condition
    extracted from the If statement (i.e., Stmt 4) includes an external function named
    TensorShapeUtils::IsVector; thus, it is invalid (i.e., contains external functions
    unknown to existing solvers). Consequently, path constraints, including this input
    condition, cannot be solved. To handle external functions, existing works [[31](#bib.bib31),
    [18](#bib.bib18)] either simply skipped them (i.e., do not include invalid input
    conditions with these external functions into the path constraint) or manually
    modeled the behavior only on a very limited set of these functions. Both ways
    lead to incomplete path constraints, as is reported by the existing work [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: DLLens leverages an LLM to handle this challenge. The insight is that LLMs have
    learned domain-specific knowledge of DL libraries and their upstream libraries.
    Given a control flow statement or sanity check statement consisting of external
    functions, LLMs are capable of “modeling” the behavior of these external functions
    and outputting a valid input condition solvable by existing solvers. A straightforward
    way is to simply provide the whole source code of an API and its counterpart to
    an LLM and let it extract valid input conditions. However, this is ineffective
    since the source code of each library API and its counterpart usually have complex
    control flow graphs, and we find that LLMs are ineffective in handling such complicated
    contexts [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, instead of giving the LLM the whole source code. DLLens handles
    each control flow statement and sanity check statement individually by only providing
    the execution trace from the input argument to this statement. Based on this execution
    trace, DLLens further asks the LLM to extract a valid input condition. Taking
    Figure [8](#S3.F8 "Figure 8 ‣ III-B Path Constraint Extraction ‣ III Methodology
    ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis") as an example,
    to extract the input condition of argument k from a If statement mycode Stmt 2
    which contains an external function TensorShapeUtils::IsVector, DLLens will first
    build the control flow graph and extract the execution trace from k to Stmt 2.
    Then DLLens constructs a prompt (see [Figure 7](#S3.F7 "In III-B Path Constraint
    Extraction ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis")) formatted based on the following three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the execution trace from the input argument k to the concerned statement Stmt
    2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the argument type of input argument k.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'the ‘property’ (see Table [I](#S3.T1 "Table I ‣ III-B Path Constraint Extraction
    ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis"))
    of ‘k’ if ‘k’ is a tensor. Otherwise, this part will not be provided to LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a result, DLLens successfully guides the LLM to extract the valid input condition
    (i.e., k.ndims==1) from Stmt 2. Specifically, DLLens constructs the execution
    trace from k to Stmt 2 by including the Stmt 1 since k’s value is assigned to
    diag_index, then used in Stmt 2. DLLens further uses an SMT solver to checks whether
    the returned input condition from the LLM is valid. For valid input conditions
    returned by LLM, DLLens will add them to the path constraint.
  prefs: []
  type: TYPE_NORMAL
- en: '12|\underline{\textbf{Prompt}}|:3Analyze  the  following  execution  path,  summarize  the4NECESSARY  constraint  on  the  ‘properties‘  of  input5arguments  ‘k‘  to  satisfy  the  condition  at  the  end  of6the  path.7|\textbf{[Execution  Path]}|8diag_index  =  k9condition:  TensorShapeUtils::IsVector(diag_index.shape())10|\textbf{[Argument  Type]}|11{’k’:  ’tensor’}12|\textbf{[‘property‘  for  ‘k‘]}|13.ndims:  int,  number  of  dimensions  of  tensor14...15The  constraint  should  only  consider  symbols  |\underline{‘k‘}|,  ...16...17|\underline{\textbf{LLM}}:|18len(k.shape)  ==  1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Example prompt of DLLens to extract input conditions from control
    flow statements with external functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee6c3428adb61a9fe1f7eddeff4d00a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A running example of DLLens that uses the LLM to extract valid input
    conditions from a control flow statement with external functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The detailed algorithm for path constraint extraction is shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ III-B Path Constraint Extraction ‣ III Methodology ‣ DLLens: Testing
    Deep Learning Libraries via LLM-aided Synthesis"). Given the set of input arguments
    and an execution path extracted from the implementation of a DL library API or
    its counterpart, DLLens traverses all the statements along this path and extracts
    conditions on input arguments from sanity check and control flow statements (lines
    3-6). Note that the condition value of some statements may not be controlled by
    input arguments, e.g., a condition value may be controlled by some global variables.
    In this case, conditions from these statements will not be added to the path constraint
    of the execution path (lines 6-7) since changing the value of input arguments
    will not influence these condition values. For the remaining statements, DLLens
    first reduces tensor operators (line 8) and checks if there are external functions
    whose source code is not considered during the static analysis (line 9). If any
    external function is found, DLLens formats the prompt based on the template of
    [Figure 7](#S3.F7 "In III-B Path Constraint Extraction ‣ III Methodology ‣ DLLens:
    Testing Deep Learning Libraries via LLM-aided Synthesis") to let the LLM extract
    a valid input condition from the concerned statement (line 10). The input condition
    returned by the LLM will be added to the path constraint if it is solvable (i.e.,
    it can be solved by an SMT solver) (lines 11-12). Otherwise, the input condition
    of the concerned statement will not be added to the path constraint. For each
    DL library API, DLLens applies Alg [1](#alg1 "Algorithm 1 ‣ III-B Path Constraint
    Extraction ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis") to each execution path inside implementations of this API and its
    counterpart. Finally, the total path constraints of each API are formed by combining
    the path constraints extracted from its own and its counterpart’s implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $p$* then8                          continue9                  cond’  $\leftarrow$;17'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Path Constraint Extraction
  prefs: []
  type: TYPE_NORMAL
- en: III-C Test Input Generation and Bug Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For each API, DLLens uses its total path constraint set for test input generation.
    For each path constraint inside this total path constraint set, DLLens iteratively
    generates input arguments to fulfill such constraint. To avoid duplication during
    the iterative generation, when generating input arguments for each path constraint,
    we follow existing works to add an additional input constraint, preventing the
    constraint solver from generating duplicate input tensors whose property values
    have been covered in the previous iteration, back to the path constraint. Besides
    the path constraint extracted in Sec.[III-B](#S3.SS2 "III-B Path Constraint Extraction
    ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis"),
    following existing techniques [[18](#bib.bib18)], DLLens also incorporates natural
    constraints and error-feedback constraints. The former refers to pre-defined constraints
    based on the input argument type (see Table [II](#S4.T2 "Table II ‣ IV-A Implementations
    and Experiment Setup ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries
    via LLM-aided Synthesis"). For instance, we require the tensor’s data type to
    be a string among a list of options such as float32 and double. The latter refers
    to the constraints extracted from error messages in previous iterations. For instance,
    the error message formatted in Dimension out of range (expected to be in range
    of [#1, #2], but got #3) indicates the ‘ndims’ of a tensor input argument should
    be within the range of [#1, #2] while the actual ndims of this tensor input argument
    is #3. For this error message, we use a regular expression to extract the #1,
    #2, and #3, and further check if there is a tensor input argument whose ndims
    is #3. If such an input argument is found, we add a constraint that requires its
    ndims to be within the range of [#1, #2] to the original path constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a fuzzing driver is used to instantiate the concrete value of generated
    input tensors for testing. Inspired by existing work [[38](#bib.bib38)] demonstrating
    that special values (i.e., Inf and NaN) can expose output inconsistency in scientific
    libraries, we also randomly generate special values for input arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test Oracles. DLLens uses two test oracles to detect both crash bugs and functional
    bugs. Crash bugs refer to system crashes, including aborts, segmentation faults,
    floating point exception raised, and INTERNAL_ASSERT_FAILED [[32](#bib.bib32),
    [18](#bib.bib18)]. If DLLens finds any such issues in testing, DLLens labels them
    as crash bugs for manual investigation. For functional bugs, DLLens focuses on
    the inconsistency between a DL library API and its counterpart. In particular,
    DLLens captures two types of inconsistency: 1) inconsistent computation results,
    *i.e.*, output inconsistency larger $\epsilon$ is a pre-defined threshold. 2)
    inconsistent execution status between the DL library API and its counterpart.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We evaluated DLLens from three perspectives: effectiveness of counterpart synthesis,
    path constraint extraction, and bug detection. We studied four research questions
    (RQs):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ1: How effective is DLLens in finding counterparts compared with existing
    techniques? We demonstrated DLLens’s effectiveness by comparing with existing
    techniques on the total number of APIs found with counterpart.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ2: How effective is DLLens in path constraint extraction? We measured the
    number of solvable path constraints extracted by DLLens.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3: Can DLLens detect real bugs? We presented DLLens’s bug-revealing capability
    to demonstrate its usefulness. We evaluated DLLens on two popular DL libraries:
    TensorFlow and PyTorch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ4: Are DLLens more effective than existing works? We compared DLLens with
    existing techniques in terms of the branch coverage and the number of bugs detected
    on 200 randomly sampled DL library APIs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-A Implementations and Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Implementations We used gpt-turbo-3.5 for counterpart synthesis and path constraint
    extraction. We set the temperature to 1, the default value specified by the documentation [[39](#bib.bib39)].
    For path constraint extraction, we used the Python standard AST package [[40](#bib.bib40)]
    and tree-sitter [[41](#bib.bib41)] to parse an implementation into an AST tree.
    We further used z3py [[34](#bib.bib34)] to solve the extracted path constraint.
    For the natural constraint used by test input generation, we manually crafted
    the valid input space based on the argument type (see Table [II](#S4.T2 "Table
    II ‣ IV-A Implementations and Experiment Setup ‣ IV Evaluation ‣ DLLens: Testing
    Deep Learning Libraries via LLM-aided Synthesis")). The experiments were conducted
    using a 32-core server with a 3090Ti GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Natural Constraints Used by DLLens'
  prefs: []
  type: TYPE_NORMAL
- en: '| Argument Type | Argument Properties | Value Space |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor | ndims | [0, 5] |'
  prefs: []
  type: TYPE_TB
- en: '| shape[i] | [0, 5] |'
  prefs: []
  type: TYPE_TB
- en: '| dtype |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [‘uint(8-64)’, ‘float(16-64)’, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ‘complex(64-128)’, ‘bool’, ‘string’] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| num_element | [0, $\infty$] |'
  prefs: []
  type: TYPE_TB
- en: '| Bool | value | {True, False} |'
  prefs: []
  type: TYPE_TB
- en: '| Integer | value | [-100, 100] |'
  prefs: []
  type: TYPE_TB
- en: '| Float | value | [-100, 100] |'
  prefs: []
  type: TYPE_TB
- en: '| String | value | Any |'
  prefs: []
  type: TYPE_TB
- en: DL libraries under test. We evaluated DLLens on PyTorch (v2.1.0) [[7](#bib.bib7)]
    and TensorFlow (v2.10.0) [[8](#bib.bib8)]. To collect API under test, we do not
    include the DL library API in our experiment if 1) it has an incomplete signature;
    2) it belongs to special packages (e.g., torch.utils and tf.raw_ops.Experimental).
    In total, we collected 1,864 TensorFlow APIs and 1,184 PyTorch APIs, which take
    up 56% and 74.3% of the total APIs collected by existing works [[21](#bib.bib21),
    [32](#bib.bib32)].
  prefs: []
  type: TYPE_NORMAL
- en: API source code collection. To collect the source code of DL library APIs, we
    used PyCG [[42](#bib.bib42)] and Joern [[43](#bib.bib43)]/tree-sitter [[41](#bib.bib41)]
    to construct call graphs for Python and C++, respectively. Note that TensorFlow
    and PyTorch have their front-end APIs implemented in Python, and their core logic
    functions are in C++. To establish the call relationship between the Python and
    C++ code, we observed that TensorFlow employs the Python interface _execute.execute
    to access C++ operators, while PyTorch utilizes native_functions.yaml to link
    the Python API with its C++ implementations. Leveraging these characteristics,
    we developed a program analysis tool to bind function calls between Python and
    C++.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the entry point of each API, we conducted a breadth-first search
    in the constructed call graph, collecting the source code of all functions within
    a depth of five levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. Overall, we compared DLLens with four existing techniques, TitanFuzz [[21](#bib.bib21)],
    TensorScope [[18](#bib.bib18)], FreeFuzz [[20](#bib.bib20)], and DocTer [[30](#bib.bib30)].
    Since neither executables nor source codes of ACETest [[31](#bib.bib31)], and
    FuzzGPT [[32](#bib.bib32)] are available, we did not include them in evaluation.²²2Although
    the source code of TensorScope is available, we tried our best but failed to execute
    their input generation and constraint extraction modules, thus we do not include
    TensorScope in RQ2 and RQ4. For RQ1 (evaluation of counterpart synthesis), we
    compared DLLens with TensorScope [[18](#bib.bib18)], which is the state-of-the-art
    work that can find counterparts across DL libraries for DL library APIs. In RQ2
    (evaluation of path constraint extraction), we compared DLLens with DocTer [[30](#bib.bib30)],
    which mines input constraints from DL library API documentation. In RQ4 (evaluation
    on code coverage and bug detection), we compared DLLens with two state-of-the-art
    DL library testing techniques: FreeFuzz [[20](#bib.bib20)] and TitanFuzz [[21](#bib.bib21)]
    on a randomly sampled API set.'
  prefs: []
  type: TYPE_NORMAL
- en: Functional Bug Detection. For output inconsistencies checking between the DL
    library API and its’ counterpart, we set the absolute threshold $\epsilon$ to
    0.1. For execution status inconsistency, we first removed clear syntax errors
    (e.g., SyntaxError) and invalid argument messages such as TypeError and RuntimeError.
    For the remaining status inconsistencies, we manually checked the documentation
    of related DL library APIs to determine whether it was a real bug.
  prefs: []
  type: TYPE_NORMAL
- en: 'IV-B RQ1: Effectiveness of Counterpart Synthesis'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compared DLLens with TensorScope [[18](#bib.bib18)], the state-of-the-art
    techniques finding counterparts across different DL libraries. We evaluated both
    tools on the API counterpart coverage, i.e., the number of TensorFlow and PyTorch
    APIs that these tools could find counterparts for. To be more specific, we only
    evaluated whether a valid DL library API’s counterpart is available while did
    not consider the number of different counterparts found for one single API. Therefore,
    if a DL library API has multiple counterparts, we consider all of them as one
    when evaluating the API counterpart coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'For TensorScope, we followed its methodology and collected counterparts of
    TensorFlow APIs and PyTorch APIs via parsing the developer-constructed rules in
    model conversion libraries. Since these rules are manually written by library
    developers, we consider all counterparts extracted by TensorScope to be valid
    (i.e., satisfies the Equation [1](#S3.E1 "Equation 1 ‣ III-A Counterpart Synthesis
    ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")).
    For DLLens, we applied it on each DL library API to synthesize this API’s valid
    counterpart that satisfies Equation [1](#S3.E1 "Equation 1 ‣ III-A Counterpart
    Synthesis ‣ III Methodology ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis") based on a non-empty valid input set (with at most three valid inputs).
    Due to the random nature of the LLM, the synthesized counterpart from the LLM
    may not always be valid. To make better use of the LLM, we applied our counterpart
    synthesis workflow for five rounds. In each round, we asked the LLM to synthesize
    counterparts for APIs whose valid counterparts had not been correctly synthesized
    in previous rounds. Table [III](#S4.T3 "Table III ‣ IV-B RQ1: Effectiveness of
    Counterpart Synthesis ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries
    via LLM-aided Synthesis") shows the number of valid API counterparts synthesized
    by DLLens for in each round. We noticed that most counterparts could be found
    in the first round, and the following rounds could also find a non-negligible
    number of API counterparts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [9](#S4.F9 "Figure 9 ‣ IV-B RQ1: Effectiveness of Counterpart Synthesis
    ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")
    shows the comparison between DLLens and TensorScope on the API counterpart coverage.
    A higher API counterpart coverage indicates the tool can facilitate differential
    testing to detect functional bugs for more APIs. Overall, DLLens can synthesize
    valid counterparts for 1,481 APIs, including 739 from TensorFlow and 742 from
    PyTorch, which stand for 41.47% and 62.67% of the total APIs evaluated, respectively.
    Compared with TensorScope, DLLens can collect 143.1% (739 v.s. 304) more valid
    counterparts for TensorFlow APIs and 65.3% (742 v.s. 449) more for PyTorch APIs.
    We also noticed that TensorScope found 68 and 159 counterparts for TensorFlow
    and PyTorch APIs beyond our recorded APIs. This is because some APIs recorded
    in developers’ conversion rules are related to special packages such as torch.utils
    or special usages such as torch.float64 for representing the data type. These
    APIs are not targeted by DLLens since we try to focus on APIs developed for DL
    algorithms, which may be commonly used by DL library users.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The Number of Counterparts Found by DLLens'
  prefs: []
  type: TYPE_NORMAL
- en: '| Library Name | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow | 619 | 53 | 30 | 20 | 17 | 739 |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | 624 | 61 | 30 | 15 | 12 | 742 | ![Refer to caption](img/decbfee6802d362715bdae0dc2f43353.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Comparison on API Counterpart Coverage'
  prefs: []
  type: TYPE_NORMAL
- en: 'IV-C RQ2: Effectiveness of Path Constraint Extraction'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To measure the effectiveness of DLLens in path constraint extraction, we first
    evaluated the number of path constraints that can be extracted by DLLens. Specifically,
    we applied DLLens to extract path constraints for all APIs (i.e., 739 TensorFlow
    APIs and 742 PyTorch APIs) collected in RQ1. For each API, its path constraints
    are formed by combining the path constraints extracted from both its own implementation
    and the implementation of its counterpart. Table [IV](#S4.T4 "Table IV ‣ IV-C
    RQ2: Effectiveness of Path Constraint Extraction ‣ IV Evaluation ‣ DLLens: Testing
    Deep Learning Libraries via LLM-aided Synthesis") demonstrates the performance
    of DLLens’s path constraint (i.e., P.C.) extraction capability by showing the
    number of path constraints extracted per API. On average, DLLens extracts 21.50
    path constraints for each TensorFlow API and 41.87 for each PyTorch API. Each
    path constraint in TensorFlow and PyTorch contains 3.35 and 3.84 input conditions,
    respectively. We also noticed that incorporating our LLM-aided static analysis
    method led to a significant increase in the extraction of path constraints compared
    to our static analysis method that doesn’t use the LLM (referred to as ‘DLLens
    w/o LLM’).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Performance on Path Constraint (P.C.) Extraction'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tool | # P.C./API | # Cons./P.C. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DLLens (TensorFlow) | 21.50$\pm$2.53 |'
  prefs: []
  type: TYPE_TB
- en: '| DLLens w/o LLM (TensorFlow) | 2.12$\pm$1.04 |'
  prefs: []
  type: TYPE_TB
- en: '| DLLens (PyTorch) | 41.87$\pm$3.02 |'
  prefs: []
  type: TYPE_TB
- en: '| DLLens w/o LLM (PyTorch) | 3.15$\pm$1.10 |'
  prefs: []
  type: TYPE_TB
- en: Comparison with DocTer. We further compared DLLens with DocTer [[30](#bib.bib30)]
    in terms of the constraint extraction capability. Although DocTer utilizes documents
    to extract API constraints and does not extract path constraints, we employed
    another metric (i.e., property constraints) utilized by existing works [[30](#bib.bib30),
    [18](#bib.bib18)] including DocTer, to evaluate the constraint extraction performance.
    Property constraint refers to the total number of input properties considered
    by extracted constraints. For each API input argument, we followed existing work
    and considered all valid options for one property as one property constraint [[30](#bib.bib30)].
    Among all APIs for which DLLens synthesized counterparts in RQ1, we found that
    DocTer could extract constraints for 271 TensorFlow APIs and 160 PyTorch APIs.
    For a fair comparison, we further compared the property constraint extraction
    performance with DocTer on these APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Comparison on Property Constraint Extraction'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tool | TensorFlow | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DLLens | 6.21$\pm$4.25 |'
  prefs: []
  type: TYPE_TB
- en: '| DocTer | 4.24$\pm$4.21 |'
  prefs: []
  type: TYPE_TB
- en: 'Table [V](#S4.T5 "Table V ‣ IV-C RQ2: Effectiveness of Path Constraint Extraction
    ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")
    shows the average number of property constraints extracted for each API. DLLens
    extracts more property constraints than DocTer, *i.e.*, 6.21 v.s. 4.24 for TensorFlow
    and 6.64 v.s., 5.24 for PyTorch. We further analyzed the constraint extraction
    performance for each property, as depicted in Figure [10](#S4.F10 "Figure 10 ‣
    IV-C RQ2: Effectiveness of Path Constraint Extraction ‣ IV Evaluation ‣ DLLens:
    Testing Deep Learning Libraries via LLM-aided Synthesis"). Our findings indicate
    that DLLens outperforms DocTer in extracting constraints related to shape, value,
    and structure (‘STR.’ in Figure [10](#S4.F10 "Figure 10 ‣ IV-C RQ2: Effectiveness
    of Path Constraint Extraction ‣ IV Evaluation ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis")), while it extracts less data type constraints
    (’DType’ in Figure [10](#S4.F10 "Figure 10 ‣ IV-C RQ2: Effectiveness of Path Constraint
    Extraction ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis")) compared to DocTer.'
  prefs: []
  type: TYPE_NORMAL
- en: Taking TensorFlow APIs as an example, on average, DLLens is more effective in
    extracting shape (2.03 vs. 0.82), value (0.53 vs. 0.14), and structure (2.37 vs.
    1.66) constraints per API. As for data type constraints, averagely DLLens extracts
    1.28 constraints per API, whereas DocTer extracts 1.82 per API. This difference
    could be due to the common practice of documenting constraints related to data
    types, making DocTer more effective in this aspect. In contrast, constraints on
    shape, value, and structure are less frequently documented, limiting DocTer’s
    performance in these properties.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ac5a171d2cddf6ff7bbb601af7750fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Comparison on the Number of Constraints Extracted for Each Property'
  prefs: []
  type: TYPE_NORMAL
- en: 'IV-D RQ3: Effectiveness of Bug Detection'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use DLLens to detect bugs in TensorFlow v2.10 and PyTorch v2.1. In total,
    DLLens detected 56bugs in TensorFlow and PyTorch, including 15bugs already fixed
    or confirmed by developers and 41new ones. After we reported those newly detected
    bugs, developers confirmed 39, including 19 fixed. The remaining 2 bugs are pending
    confirmation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [VI](#S4.T6 "Table VI ‣ IV-E RQ4: Comparison On Bug Detection and Code
    Coverage ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis") demonstrates the newly detected bugs in TensorFlow and PyTorch, categorized
    by their bug symptoms. The predominant type of bug detected by DLLens is the incorrect
    result bug (’Incorrect’ in Table [VI](#S4.T6 "Table VI ‣ IV-E RQ4: Comparison
    On Bug Detection and Code Coverage ‣ IV Evaluation ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis")), constituting 43.9% (18 out of 41) of the
    total reported bugs. Among these, 16 were confirmed by developers, and 9 have
    been successfully addressed with fixes. An example is Listing [1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis"),
    which is caused by an internal overflow during the computation, resulting in an
    incorrect output of API tf.math.is_non_decreasing. Detection of these incorrect
    result bugs requires the test oracle, which can be effectively addressed by our
    synthesized counterparts. Following the ‘Incorrect’ bugs, the second most prevalent
    category is the bug that API incorrectly rejected valid inputs (’Inc.Rej.’ in
    Table [VI](#S4.T6 "Table VI ‣ IV-E RQ4: Comparison On Bug Detection and Code Coverage
    ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")),
    accounting for 12 out of the total 41 reported bugs. These bugs were also detected
    when execution inconsistency occurs between a DL library API and its counterpart,
    which also demonstrates the usefulness of synthesized counterparts. We notice
    that DLLens has also detected many crash bugs (Crash in Table [VI](#S4.T6 "Table
    VI ‣ IV-E RQ4: Comparison On Bug Detection and Code Coverage ‣ IV Evaluation ‣
    DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis")), which take
    up to 10 out of 41. Although a counterpart is not required to detect these bugs,
    we found that path constraints extracted by DLLens could guide generating test
    inputs triggering these bugs. Taking the bug in Listing [IV-D](#S4.SS4 "IV-D RQ3:
    Effectiveness of Bug Detection ‣ IV Evaluation ‣ DLLens: Testing Deep Learning
    Libraries via LLM-aided Synthesis") as an example, reaching the buggy code requires
    passing two sanity checks which assert the ndims of input argument indices and
    updates to be no less than 1 [[44](#bib.bib44)]. Extracting path constraints,
    including these two sanity checks, can guide generating input reaching the buggy
    code, increasing the probability of triggering this bug.'
  prefs: []
  type: TYPE_NORMAL
- en: '—Bug Triggering Input—: indices = tf.constant([[[1]]]) updates = tf.constant([16])
    … —Buggy API—: tf.tensor_scatter_nd_update —Buggy Code—: for (int i = 0; i ¡ outer_dims;
    ++i) … —Two Validation Checks Before Reaching The Buggy Code—: OP_REQUIRES(..,
    —indices.shape().dims() ¿= 1—,..); OP_REQUIRES(.., —updates.shape().dims() ¿=
    1—,..);'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: A Crash Bug and Two Sanity Checks before Reaching the Buggy Line'
  prefs: []
  type: TYPE_NORMAL
- en: 'IV-E RQ4: Comparison On Bug Detection and Code Coverage'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compared DLLens with two state-of-the-art DL library testing techniques:
    FreeFuzz [[20](#bib.bib20)] and TitanFuzz [[21](#bib.bib21)]. Since neither executables
    nor source codes of FuzzGPT [[32](#bib.bib32)], ACETest [[31](#bib.bib31)], and
    TensorScope [[18](#bib.bib18)] are available, we did not include them as baselines. ³³3Although
    the source code of TensorScope is available, we tried our best but failed to execute
    their tool. Specifically, we randomly sampled 100 TensorFlow APIs and 100 PyTorch
    APIs from the total 739 TensorFlow APIs and 742 PyTorch APIs collected in RQ1.
    For each sampled API, we used each testing technique to generate 300 test inputs
    for testing. We considered the branch coverages of these APIs and the bug detection
    performances as metrics to evaluate their overall performance. Pycoverage [[45](#bib.bib45)]
    and lcov [[46](#bib.bib46)] are used to collect branch coverages for Python and
    C++ code, respectively. When measuring the code coverage, we excluded additional
    coverages added by APIs that are not under test for a fair comparison. In bug
    detection, we manually analyzed reported bugs to filter out false positives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [VII](#S4.T7 "Table VII ‣ IV-E RQ4: Comparison On Bug Detection and Code
    Coverage ‣ IV Evaluation ‣ DLLens: Testing Deep Learning Libraries via LLM-aided
    Synthesis") shows the branch coverage and bug detection results. In specific,
    DLLens outperforms the state-of-the-art (i.e., TitanFuzz) by covering 3.3% more
    branches (7,896 v.s. 7,647) in TensorFlow and 13.8% more (6,433 v.s. 5,652) in
    PyTorch. We also notice that DLLens can detect more bugs, i.e., DLLens can detect
    at least 2.5 times as many bugs as baselines. Moreover, we found that most (9
    out of 10) bugs detected by DLLens are either incorrect result bugs or incorrectly
    rejected bugs; detecting these bugs demonstrated the usefulness of counterpart
    synthesized by DLLens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Effectiveness of Bug Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Library Name | Inc. Rej. | Incorrect | Imp.Err.Msg | Crash | Subtotal |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow | 11 (4/9) | 15 (8/15) | 1 (1/1) | 3 (1/3) | 30 (14/28) |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | 1 (0/1) | 3 (1/3) | 0 (0/0) | 7 (4/7) | 11 (5/11) |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numbers of confirmed and fixed bugs are parenthesized. The number of confirmed
    bugs is the denominator in parentheses; the number of fixed bug is the numerator
    in parentheses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE VII: Code Coverage and Bug Detection'
  prefs: []
  type: TYPE_NORMAL
- en: '| Baseline | TensorFlow | PyTorch | Total Bugs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Branch | Bug | Branch | Bug |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DLLens | 7,896 | 7 | 6,433 | 3 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| TitanFuzz | 7,647 | 2 | 5,652 | 1 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| FreeFuzz | 6,822 | 4 | 5,382 | 0 | 4 |'
  prefs: []
  type: TYPE_TB
- en: V Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'False Positives of Bug Detection. We summarized two types of false positives:
    First, some false positives are introduced by invalid counterparts synthesized
    by DLLens. This is because we only use a non-empty valid input set with at most
    three valid inputs to validate the synthesized counterpart and such a limited
    size of valid input may introduce some false positives, i.e., some synthesized
    counterparts passing the validation process may only be valid under specific input
    conditions (e.g., some counterparts are valid when the input is a vector while
    invalid when input is a matrix). As a result, the output inconsistencies w.r.t
    these inputs between APIs under test and their counterparts may be false positives.
    To understand the influence of these false positives, we manually analyze all
    inconsistencies reported by DLLens in RQ4, which includes 200 APIs and their counterparts.
    According to our analysis, only 27 false positives inconsistencies reported by
    DLLens were caused by invalid counterparts, which cost ~1.5 hours to filter out.
    Considering the effectiveness of DLLens in detecting real bugs, we believe such
    overhead introduced by these invalid counterparts is affordable.'
  prefs: []
  type: TYPE_NORMAL
- en: Another type of false positive is introduced by inconsistent behavior between
    libraries when handling special values such as NaN [[47](#bib.bib47)]. Since inconsistencies
    caused by these special values are not many (e.g., less than 10 in RQ4), we manually
    check the documentation of the API under test with all APIs in its counterpart
    and filter out false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Threat to Validity. In RQ2 and RQ4, we compared DLLens with state-of-the-art
    techniques on all DL library APIs that DLLens could identify counterparts for.
    It should be noted that these APIs constitute only a subset of the total DL library
    APIs, potentially limiting the comprehensiveness of our performance evaluation.
    To address this potential limitation, we conducted a measurement of the number
    of APIs considered in RQ2 and RQ4, which accounted for nearly half of the total
    DL library APIs. Based on this analysis, we make the assumption that the APIs
    covered in our study adequately represent the performance of DL library testing
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose a novel technique named DLLens to test DL libraries
    DLLens facilitates differential testing by using a novel counterpart synthesis
    method and generates diverse test inputs to explore execution paths via an effective
    path constraint extraction method. DLLens can synthesize counterparts for 41.47%
    TensorFlow APIs and 62.67% PyTorch APIs, which double the result of the state-of-the-art
    approaches. Benefiting from the synthesized counterparts and effective path constraint
    extraction, DLLens can outperform state-of-the-art approaches in terms of constraint
    extraction, bug detection, and branch coverage. In total, DLLens detects 56including
    41new bugs. So far, 39of these new bugs have been confirmed and 19 of them are
    fixed by developers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. Das, A. Gadre, S. Zhang, S. Kumar, and J. M. F. Moura, “A deep learning
    approach to iot authentication,” in *2018 IEEE International Conference on Communications
    (ICC)*.   Kansas City, MO, USA: IEEE, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Ferdowsi and W. Saad, “Deep learning for signal authentication and security
    in massive internet-of-things systems,” *IEEE Transactions on Communications*,
    vol. 67, pp. 1371–1387, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical image analysis*, vol. 42, pp. 60–88, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning affordance
    for direct perception in autonomous driving,” in *2015 IEEE International Conference
    on Computer Vision (ICCV)*.   Santiago, Chile: IEEE, 2015, pp. 2722–2730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement
    learning framework for autonomous driving,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison,
    A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, *PyTorch:
    An Imperative Style, High-Performance Deep Learning Library*.   Red Hook, NY,
    USA: Curran Associates Inc., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray,
    B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng,
    “Tensorflow: A system for large-scale machine learning,” in *Proceedings of the
    12th USENIX Conference on Operating Systems Design and Implementation*, ser. OSDI’16.   USA:
    USENIX Association, 2016, p. 265–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] F. Tambon, A. Nikanjam, L. An, F. Khomh, and G. Antoniol, “Silent bugs
    in deep learning frameworks: An empirical study of keras and tensorflow,” 2021.
    [Online]. Available: [https://arxiv.org/abs/2112.13314](https://arxiv.org/abs/2112.13314)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Yang, T. He, Z. Xia, and Y. Feng, “A comprehensive empirical study
    on bug characteristics of deep learning frameworks,” *Information and Software
    Technology*, vol. 151, p. 107004, 2022\. [Online]. Available: [https://www.sciencedirect.com/science/article/pii/S0950584922001306](https://www.sciencedirect.com/science/article/pii/S0950584922001306)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Zhang, Y. Chen, S.-C. Cheung, Y. Xiong, and L. Zhang, “An empirical
    study on tensorflow program bugs,” in *Proceedings of the 27th ACM SIGSOFT International
    Symposium on Software Testing and Analysis*, ser. ISSTA 2018.   New York, NY,
    USA: Association for Computing Machinery, 2018, p. 129–140\. [Online]. Available:
    [https://doi.org/10.1145/3213846.3213866](https://doi.org/10.1145/3213846.3213866)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] J. Chen, Y. Liang, Q. Shen, and J. Jiang, “Toward understanding deep learning
    framework bugs,” 2022\. [Online]. Available: [https://arxiv.org/abs/2203.04026](https://arxiv.org/abs/2203.04026)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Xiong, M. Xu, T. Su, J. Sun, J. Wang, H. Wen, G. Pu, J. He, and Z. Su,
    “An empirical study of functional bugs in android apps,” in *Proceedings of the
    32nd ACM SIGSOFT International Symposium on Software Testing and Analysis*, ser.
    ISSTA 2023.   New York, NY, USA: Association for Computing Machinery, 2023, p.
    1319–1331\. [Online]. Available: [https://doi.org/10.1145/3597926.3598138](https://doi.org/10.1145/3597926.3598138)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] H. V. Pham, T. Lutellier, W. Qi, and L. Tan, “Cradle: cross-backend validation
    to detect and localize bugs in deep learning libraries,” in *2019 IEEE/ACM 41st
    International Conference on Software Engineering (ICSE)*, IEEE.   Montreal, QC,
    Canada: IEEE, 2019, pp. 1027–1038.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Z. Wang, M. Yan, J. Chen, S. Liu, and D. Zhang, “Deep learning library
    testing via effective model generation,” in *Proceedings of the 28th ACM Joint
    Meeting on European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering*, ser. ESEC/FSE 2020.   New York, NY, USA: Association
    for Computing Machinery, 2020, p. 788–799\. [Online]. Available: [https://doi.org/10.1145/3368089.3409761](https://doi.org/10.1145/3368089.3409761)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Gu, X. Luo, Y. Zhou, and X. Wang, “Muffin: Testing deep learning libraries
    via neural architecture fuzzing,” in *Proceedings of the 44th International Conference
    on Software Engineering*, ser. ICSE ’22.   New York, NY, USA: Association for
    Computing Machinery, 2022, p. 1418–1430\. [Online]. Available: [https://doi.org/10.1145/3510003.3510092](https://doi.org/10.1145/3510003.3510092)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] M. Li, J. Cao, Y. Tian, T. O. Li, M. Wen, and S.-C. Cheung, “Comet: Coverage-guided
    model generation for deep learning library testing,” *ACM Trans. Softw. Eng. Methodol.*,
    vol. 32, no. 5, jul 2023\. [Online]. Available: [https://doi.org/10.1145/3583566](https://doi.org/10.1145/3583566)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Deng, G. Meng, K. Chen, T. Liu, L. Xiang, and C. Chen, “Differential
    testing of cross deep learning framework APIs: Revealing inconsistencies and vulnerabilities,”
    in *32nd USENIX Security Symposium (USENIX Security 23)*.   Anaheim, CA: USENIX
    Association, Aug. 2023, pp. 7393–7410\. [Online]. Available: [https://www.usenix.org/conference/usenixsecurity23/presentation/deng-zizhuang](https://www.usenix.org/conference/usenixsecurity23/presentation/deng-zizhuang)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] T. Onnx, “tf2onnx - convert tensorflow, keras, tensorflow.js and tflite
    models to onnx.” [https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx),
    Accessed: 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Wei, Y. Deng, C. Yang, and L. Zhang, “Free lunch for testing: Fuzzing
    deep-learning libraries from open source,” in *Proceedings of the 44th International
    Conference on Software Engineering*, ser. ICSE ’22.   New York, NY, USA: Association
    for Computing Machinery, 2022, p. 995–1007\. [Online]. Available: [https://doi.org/10.1145/3510003.3510041](https://doi.org/10.1145/3510003.3510041)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large language models
    are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models,”
    in *Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing
    and Analysis*, ser. ISSTA 2023.   New York, NY, USA: Association for Computing
    Machinery, 2023, p. 423–435\. [Online]. Available: [https://doi.org/10.1145/3597926.3598067](https://doi.org/10.1145/3597926.3598067)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Wang, T. Lutellier, S. Qian, H. V. Pham, and L. Tan, “Eagle: Creating
    equivalent graphs to test deep learning libraries,” in *2022 IEEE/ACM 44th International
    Conference on Software Engineering (ICSE)*.   Pittsburgh, PA, USA: IEEE, 2022,
    pp. 798–810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] C. Yang, Y. Deng, J. Yao, Y. Tu, H. Li, and L. Zhang, “Fuzzing automatic
    differentiation in deep-learning libraries,” in *Proceedings of the 45th International
    Conference on Software Engineering*, ser. ICSE ’23.   Melbourne, Australia: IEEE
    Press, 2023, p. 1174–1186\. [Online]. Available: [https://doi.org/10.1109/ICSE48619.2023.00105](https://doi.org/10.1109/ICSE48619.2023.00105)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Y. Deng, C. Yang, A. Wei, and L. Zhang, “Fuzzing deep-learning libraries
    via automated relational api inference,” in *Proceedings of the 30th ACM Joint
    European Software Engineering Conference and Symposium on the Foundations of Software
    Engineering*, ser. ESEC/FSE 2022.   New York, NY, USA: Association for Computing
    Machinery, 2022, p. 44–56\. [Online]. Available: [https://doi.org/10.1145/3540250.3549085](https://doi.org/10.1145/3540250.3549085)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] TensorFlow, “tf.math.is_non_decreasing outputs incorrect result when input
    is an uint tensor,” [https://github.com/tensorflow/tensorflow/issues/62072](https://github.com/tensorflow/tensorflow/issues/62072),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] W. Luo, D. Chai, X. Run, J. Wang, C. Fang, and Z. Chen, “Graph-based fuzz
    testing for deep learning inference engines,” in *Proceedings of the 43rd International
    Conference on Software Engineering*, ser. ICSE ’21.   Madrid, Spain: IEEE Press,
    2021, p. 288–299\. [Online]. Available: [https://doi.org/10.1109/ICSE43902.2021.00037](https://doi.org/10.1109/ICSE43902.2021.00037)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] TensorFlow, “slient overflow occurs in tf.range leading to incorrect result,
    here is a possible fix,” [https://github.com/tensorflow/tensorflow/issues/64081](https://github.com/tensorflow/tensorflow/issues/64081),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] stack overflow, “How can i know if a list is decreasing? (python),” [https://stackoverflow.com/questions/69576011/how-can-i-know-if-a-list-is-decreasing-python](https://stackoverflow.com/questions/69576011/how-can-i-know-if-a-list-is-decreasing-python),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Q. Guo, X. Xie, Y. Li, X. Zhang, Y. Liu, X. Li, and C. Shen, “Audee: Automated
    testing for deep learning frameworks,” in *2020 35th IEEE/ACM International Conference
    on Automated Software Engineering (ASE)*.   Melbourne, VIC, Australia: IEEE, 2020,
    pp. 486–498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] D. Xie, Y. Li, M. Kim, H. V. Pham, L. Tan, X. Zhang, and M. W. Godfrey,
    “Docter: Documentation-guided fuzzing for testing deep learning api functions,”
    in *Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing
    and Analysis*, ser. ISSTA 2022.   New York, NY, USA: Association for Computing
    Machinery, 2022, p. 176–188\. [Online]. Available: [https://doi.org/10.1145/3533767.3534220](https://doi.org/10.1145/3533767.3534220)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Shi, Y. Xiao, Y. Li, Y. Li, D. Yu, C. Yu, H. Su, Y. Chen, and W. Huo,
    “Acetest: Automated constraint extraction for testing deep learning operators,”
    in *Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing
    and Analysis*, ser. ISSTA 2023.   New York, NY, USA: Association for Computing
    Machinery, 2023, p. 690–702\. [Online]. Available: [https://doi.org/10.1145/3597926.3598088](https://doi.org/10.1145/3597926.3598088)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large
    language models are edge-case generators: Crafting unusual programs for fuzzing
    deep learning libraries,” in *Proceedings of the 46th IEEE/ACM International Conference
    on Software Engineering*, ser. ICSE ’24.   New York, NY, USA: Association for
    Computing Machinery, 2024\. [Online]. Available: [https://doi.org/10.1145/3597503.3623343](https://doi.org/10.1145/3597503.3623343)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] R. Baldoni, E. Coppa, D. C. D’elia, C. Demetrescu, and I. Finocchi, “A
    survey of symbolic execution techniques,” *ACM Comput. Surv.*, vol. 51, no. 3,
    may 2018\. [Online]. Available: [https://doi.org/10.1145/3182657](https://doi.org/10.1145/3182657)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] L. de Moura and N. Bjørner, “Z3: An efficient smt solver,” in *Tools and
    Algorithms for the Construction and Analysis of Systems*, C. R. Ramakrishnan and
    J. Rehof, Eds.   Berlin, Heidelberg: Springer Berlin Heidelberg, 2008, pp. 337–340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and
    J. M. Zhang, “Large language models for software engineering: Survey and open
    problems,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] W. Song, S. Oh, S. Mo, J. Kim, S. Yun, J.-W. Ha, and J. Shin, “Hierarchical
    context merging: Better long context understanding for pre-trained LLMs,” in *The
    Twelfth International Conference on Learning Representations*, 2024\. [Online].
    Available: [https://openreview.net/forum?id=ulaUJFd96G](https://openreview.net/forum?id=ulaUJFd96G)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] I. Ozkaya, “Application of large language models to software engineering
    tasks: Opportunities, risks, and implications,” *IEEE Software*, vol. 40, no. 3,
    pp. 4–8, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Vanover, X. Deng, and C. Rubio-González, “Discovering discrepancies
    in numerical libraries,” in *Proceedings of the 29th ACM SIGSOFT International
    Symposium on Software Testing and Analysis*, ser. ISSTA 2020.   New York, NY,
    USA: Association for Computing Machinery, 2020, p. 488–501\. [Online]. Available:
    [https://doi.org/10.1145/3395363.3397380](https://doi.org/10.1145/3395363.3397380)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] OpenAI, “Openai chat completion api reference,” [https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Python, “Python ast package,” [https://docs.python.org/3/library/ast.html](https://docs.python.org/3/library/ast.html),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Brunsfeld *et al.*, “Tree-sitter: A parser generator tool and an incremental
    parsing library,” 2024\. [Online]. Available: [https://github.com/tree-sitter/tree-sitter](https://github.com/tree-sitter/tree-sitter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] V. Salis, T. Sotiropoulos, P. Louridas, D. Spinellis, and D. Mitropoulos,
    “Pycg: Practical call graph generation in python,” in *2021 IEEE/ACM 43rd International
    Conference on Software Engineering (ICSE)*, 2021, pp. 1646–1657.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Joern, “The bug hunter’s workbench,” [https://joern.io](https://joern.io),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] TensorFlow, “tf.tensor_scatter_nd_update lead to a program abortion when
    receiving a 3d indices,” [https://github.com/tensorflow/tensorflow/issues/63575](https://github.com/tensorflow/tensorflow/issues/63575),
    Accessed: March, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] “coverage.py,” 2021\. [Online]. Available: [https://coverage.readthedocs.io/en/coverage-5.5/](https://coverage.readthedocs.io/en/coverage-5.5/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] P. Oberparleiter *et al.*, “lcov-a graphical gcov front-end,” *URL: https://linux.
    die. net/man/1/lcov, last checked on*, pp. 08–03, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Example, “Example of false positive,” [https://github.com/pytorch/pytorch/issues/122426](https://github.com/pytorch/pytorch/issues/122426)/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
