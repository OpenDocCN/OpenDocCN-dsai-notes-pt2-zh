- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '\n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10141](https://ar5iv.labs.arxiv.org/html/2404.10141)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Aashish Anantha Ramakrishnan, Sharon X. Huang & Dongwon Lee
  prefs: []
  type: TYPE_NORMAL
- en: College of Information Sciences and Technology
  prefs: []
  type: TYPE_NORMAL
- en: The Pennsylvania State University
  prefs: []
  type: TYPE_NORMAL
- en: University Park, PA
  prefs: []
  type: TYPE_NORMAL
- en: '{aza6352,suh972,dul13}@psu.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing
    synthesized image quality, current datasets evaluate model performance only on
    descriptive, instruction-based prompts. However, real-world news image captions
    take a more pragmatic approach, providing high-level situational and Named-Entity
    (NE) information along with limited physical object descriptions, making them
    abstractive in nature. To better evaluate the ability of T2I models to capture
    the intended subjects from news captions, we introduce the *Abstractive News Captions
    with High-level cOntext Representation* (\n) dataset, containing 70K+ samples
    sourced from 5 different news media organizations. With Large Language Models
    (LLM) achieving success in language and commonsense reasoning tasks, we explore
    the ability of different LLMs to identify and understand key subjects from abstractive
    captions. Our proposed method *Subject-Aware Fine-tuning* (SAFE), selects and
    enhances the representation of key subjects in synthesized images by leveraging
    LLM-generated subject weights. It also adapts to the domain distribution of news
    images and captions through Domain Fine-tuning, outperforming current T2I baselines
    on \n. By launching the \n dataset, we hope to motivate research in furthering
    the Natural Language Understanding (NLU) capabilities of T2I models. Dataset and
    evaluation code are available at [https://github.com/aashish2000/ANCHOR](https://github.com/aashish2000/ANCHOR).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The capabilities of Generative AI in recent times have grown exponentially.
    In the field of Computer Vision, the launch of Large Vision Models (LVM) (Wang
    et al., [2023](#bib.bib41)), pre-trained on large-scale datasets has created a
    paradigm shift in terms of how we approach model architecture development. This
    has led to significant improvements in the visual fidelity of generated samples.
    One critical assumption that these LVM-powered T2I generators make is the style
    of caption used as input. Descriptive captions are most commonly used as they
    provide a simple and direct way to explain an image’s contents (Sharma et al.,
    [2018](#bib.bib33)), (Chen et al., [2015](#bib.bib3)). News media is one key domain
    where image captions follow sentence structures, differing from descriptive captions.
    Online news articles follow a common format: A headline followed by the article
    body, along with visual elements such as images or videos. These visual mediums
    help readers assimilate certain concepts discussed in the article. A good news
    image caption must not state obvious observations from the image, rather inform
    readers about the context behind the photo and support the topics/ideas discussed
    in the article (Federico, [2016](#bib.bib8)). Here we define context as the ”who,
    what, when, where and why” information conveyed while the image contents refer
    to the actual objects present in the image, their relative positions, the physical
    actions they perform, etc. This structure helps these captions to be more informative
    for readers while being easier to read. Since news image captions include high-level
    context information that doesn’t directly describe physical attributes of different
    image elements, we term them to have an Abstractive style of representation. The
    importance of understanding how the meaning of a sentence varies based on its
    structure is a key challenge in Natural Language Understanding (NLU).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b765a7ac2eca63ab0ec15fe188f20a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example of descriptive captions from the COCO Captions dataset (Chen
    et al., [2015](#bib.bib3)) (Left) and abstractive captions from the \n (Right).
    Words highlighted in Blue directly translate to visual entities while words highlighted
    in Red influence the image indirectly, making them abstractive.'
  prefs: []
  type: TYPE_NORMAL
- en: Linguistic theories such as Pragmatics (Grice, [1975](#bib.bib11)) and Discourse
    Coherence (Alikhani et al., [2019](#bib.bib1)) help us define the role of context
    in image captions. The perceived meaning of an image may vary from the literal
    descriptions of image contents, even if they are semantically similar (Nie et al.,
    [2020](#bib.bib22)), (Vedantam et al., [2017](#bib.bib39)). Since the assumption
    that abstractive captions offer literal descriptions of image contents doesn’t
    hold true, the delineation between image content and context information is critical
    to the synthesis process. News image-caption pairs routinely contain multiple
    subjects and associated context information, which poses key challenges for caption
    comprehension. One significant challenge is estimating the relative importance
    of different subjects. As humans, we can rank the importance of each subject mentioned
    in a sentence and appropriately expect them to be represented in the generated
    image. Current Text-Image encoders such as CLIP struggle with such Visio-Linguistic
    Reasoning tasks (Thrush et al., [2022](#bib.bib35)), leading to catastrophic subject
    forgetting. We explore the use of LLMs as External Knowledge sources through salient
    subject extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle the above-mentioned challenges, we propose a new dataset containing
    Abstractive News Captions with High-level cOntext Representation (\n), to help
    enhance the caption comprehension capabilities of T2I models. Popular open-source
    image-caption datasets such as COCO Captions (Chen et al., [2015](#bib.bib3))
    and Conceptual Captions (Sharma et al., [2018](#bib.bib33)) primarily contain
    descriptive captions with limited text subjects. The ANCHOR dataset is designed
    to be representative of real-world image captions included in news articles. Articles
    from top English News Media publishers are used for extracting relevant image-caption
    pairs. Compared to descriptive captions, news captions differ significantly as
    they contain variable sentence structures and a higher presence of Named-Entities
    (NE). To benchmark T2I models on both these aspects, we create 2 main subsets
    within our dataset: ANCHOR Non-Entity and ANCHOR Entity. The Non-entity subset
    consists 70K+ samples containing generic image concepts with the primary goal
    of evaluating sentence understanding. The Entity subset consists of image-caption
    pairs sorted per NE category. With PERSON entities being the most commonly observed
    in news media, we select the top 48 frequently mentioned entities from this class
    for constructing the subset. We also propose Subject-Aware FinE-tuning (SAFE)
    as a viable framework to improve subject understanding through explicit conditioning.
    We summarize our contributions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce \n, the first large-scale, abstractive image-caption dataset for
    Text-to-Image synthesis focusing on both diverse concepts and Named Entities
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present SAFE as a framework for improving subject representation using external
    knowledge
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We perform extensive experiments on captions from \n, demonstrating the effectiveness
    of SAFE in improving image-caption alignment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-Image Synthesis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There have been significant improvements in the field of T2I generation since
    the launch of Generative Adversarial Networks (GAN) (Goodfellow et al., [2014](#bib.bib10)).
    Initial GAN-based T2I generators focused on learning dataset-specific feature
    correlations between images and text using Attention Networks (Zhang et al., [2017](#bib.bib48)),
    (Xu et al., [2018](#bib.bib46)), (Zhu et al., [2019](#bib.bib51)). As Transformer-based
    architectures (Vaswani et al., [2017](#bib.bib38)) and model pre-training was
    successful on Natural Language Processing tasks, multi-modal encoders such as
    CLIP (Radford et al., [2021](#bib.bib24)) significantly improved the of quality
    multi-modal embeddings and provided better input conditioning (Crowson et al.,
    [2022](#bib.bib4)), (Zhou et al., [2022](#bib.bib50)). Diffusion models (Sohl-Dickstein
    et al., [2015](#bib.bib34)) provided a breakthrough in training higher resolution
    models with greater expressivity by modeling generation as a reverse-Markov chain
    process (Nichol et al., [2022](#bib.bib21)), (Ramesh et al., [2021](#bib.bib25)),
    (Ding et al., [2021](#bib.bib7)). With the success of language model-based text-only
    encoders, Large Vision models adopt Large Language Model (LLM) based encoders
    for T2I generation, leveraging their language comprehension capabilities (Saharia
    et al., [2022](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: Latent Diffusion Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Latent Diffusion Models (LDM) differs from other Diffusion Probabilistic Models
    by splitting the training process into 2 separate phases. The first phase uses
    an auto-encoder to compress the latent space of the diffusion model into a perceptually
    equivalent lower dimensional representation space, which reduces overall complexity.
    This is achieved by training using a patch-based adversarial objective and perceptual
    loss. The second phase improves the conditioning mechanism of Diffusion models
    by augmenting the UNet with Cross Attention Layers.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With T2I models being pre-trained on larger and larger corpora, there has been
    a shift towards evaluation-only benchmarks with prompts to judge specific attributes
    of a generator’s performance. PartiPrompts (Saharia et al., [2022](#bib.bib30))
    and UniBench (Li et al., [2022](#bib.bib17)) provide diverse text prompts sorted
    based on style and difficulty. DiffusionDB (Wang et al., [2022b](#bib.bib42))
    is a large-scale collection of prompt-tuned caption-image pairs commonly used
    for sourcing captions for T2I evaluation. All these benchmarks contain captions
    that only provide sparse or detailed descriptions of physical entities within
    images. We aim to include captions containing situational context information
    and complex sentence structures as a part of \n.
  prefs: []
  type: TYPE_NORMAL
- en: '3 \n: Dataset Overview'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The \n (Abstractive News Captions with High-level cOntext Representation) dataset
    is a large-scale image-caption pair dataset extracted from news articles. To construct
    this dataset, we use open-source news image captioning datasets: VisualNews (Liu
    et al., [2021](#bib.bib20)) and NYTimes800K (Tran et al., [2020](#bib.bib36)).
    To effectively test caption comprehension of T2I models, we need to isolate the
    impact of caption structures from other factors that influence the synthesized
    image quality. Named-Entity features such as faces of specific people pose a significant
    challenge to the generators (Rombach et al., [2022](#bib.bib28)), (Ramesh et al.,
    [2022](#bib.bib26)). This is likely due to the complexity of learning NE features
    compared to more generic visual concepts during the pre-training phase. To assess
    if artifacts generated by these models are due to poor subject understanding or
    entity features, we split our data into 2 distinct subsets: \n Non-Entity and
    \n Entity. From a combined 1.8M image-caption pairs, we remove 95% of low-quality
    image-caption pairs as a part of our pre-processing steps. We provide extensive
    dataset statistics and sample quality evaluation results in the supplementary
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: Image-based Filtering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We standardize the resolution of all images to 512x512\. By using Entropy-based
    cropping, we retain focus on points of interest within a frame. This helps keep
    the foreground object at the center of the image and limits information loss to
    only the background elements. To remove noisy and blurry images, we use CLIP-IQA
    (Wang et al., [2022a](#bib.bib40)) as a reference-free metric. To filter images
    based on noisiness and sharpness, we use a minimum threshold of 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: Caption Filtering and Entity Tagging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the first stage of filtering, we remove very short captions. We select captions
    with a minimum length of 6 words and above for our dataset. This is done to ensure
    that selected captions are informative enough for T2I synthesis. We use different
    approaches for tagging NEs based on the dataset the captions were extracted from.
    For captions extracted from the NYTimes800K news corpus, we use the provided NER
    annotations for filtering. The VisualNews corpus does not provide ground-truth
    annotations, so we identify mentions of names-entities using the Spacy library.
    We remove samples containing [’PERSON’, ’GPE’, ’LOC’, ’WORK_OF_ART’, ’ORG’] entity
    types due to their high presence in captions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/053fc18c8471c167388433a2dd472f20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of our dataset’s pre-processing and filtering steps'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 \n Non-Entity Subset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subset contains 72692 image-caption pairs selected from articles published
    by 5 different news media organizations. The train/validation/test split of the
    dataset is in the ratio 90%/5%/5% respectively. In this subset, we aim to understand
    how T2I models comprehend abstractive news caption structures. Since NEs are a
    confounding variable in the case of news captions, we remove samples with any
    NE mentions in this subset.
  prefs: []
  type: TYPE_NORMAL
- en: Non-entity Sample Selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following caption-based filtering, we also ensure that the associated images
    do not contain representations of NEs which generators may struggle to replicate.
    Since T2I models struggle to generate specific faces of humans, we target the
    removal of all image-caption pairs with visible faces. Using a RetinaFace-based
    face detector, we flag and remove images containing identifiable faces.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 \n Entity Subset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With NEs being a critical component of news image-caption pairs, this subset
    has been designed to evaluate the capabilities of T2I models to generate NEs with
    the provided context. This subset contains 7516 image-caption pairs with 48 different
    NEs. Each NE has a minimum of 43 image-caption pairs. The current version of our
    entity subset primarily includes PERSON entities. This is due to their frequency
    of mentions in news media, and consistency of physical features across images.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Modal NE Grounding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The challenge with NE mention detection is that entities can be referred to
    by different names according to the situation. Example: David Beckham can be referred
    to as: ”Beckham”, ”David”, ”David Robert Joseph Beckham”, etc. To avoid this ambiguity,
    we need to reliably link each mention to a real-world entity. We perform Multi-modal
    NE grounding to link each entity mention using Wikipedia as a real-world knowledge
    source. Using the REL Entity Linker (van Hulst et al., [2020](#bib.bib37)), we
    extract entity mentions from the previously selected samples and link them to
    their appropriate Wikipedia pages. We used a Wikipedia dump from 2019-07 as our
    knowledge source. Although this helped in removing erroneous mentions detected
    from text captions, we also need to ensure each image contains the mentioned entity.
    Using their linked Wikipedia pages, we download the main image and create a repository
    of reference images for each entity in our dataset. Since we are focusing on PERSON
    entities, we perform a sanity check to ensure that a face is detected in each
    of the downloaded reference images. To ground each image to an entity category,
    face recognition is performed using FaceNet (Schroff et al., [2015](#bib.bib31)).'
  prefs: []
  type: TYPE_NORMAL
- en: Face-Aware Cropping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The non-centered nature of foreground objects in news images poses a challenge
    for consistent image evaluation. Many photographs are taken as long shots with
    the entity’s face in different sections of the image. To standardize these images,
    we crop and resize the images taking into account the target entity position.
    By extracting the bounding boxes of our target entity face, we calculate its centroid
    as a reference coordinate for cropping. We then take a fixed window crop of the
    entity image such that the entity centroid is aligned closely to the center of
    the crop. This approach of Face-aware cropping helps maximize the image area occupied
    by an entity and further isolates its physical features. Through this cropping
    process, we can focus each entity category evaluation on the features of our target
    entity alone.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Improving Conditioning with External Knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conditioning is the technique through which user inputs are incorporated into
    the generation process. For the task of T2I generation, we utilize conditioning
    to conform the generated image to certain criteria provided as a text input. Text
    conditioning in T2I models is accomplished through embeddings extracted by text
    encoders such as CLIP Radford et al. ([2021](#bib.bib24)). Encoders typically
    employ self-attention to analyze and assign importance scores to individual elements
    of the input sequence while retaining the global context information across the
    entire sequence. These encoders are trained on massively large image-caption pair
    datasets such as LAION Schuhmann et al. ([2022](#bib.bib32)) and scraped from
    various web sources. The objective of generic image captions is to explain the
    various components of an image in a statement-like format. This simplifies the
    task of word importance estimation during the generation process with every word
    contributing directly towards a visual concept. Let $S_{desc}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a937ee2f5afb2c0f6a01e6edb416b2e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of our Subject-Aware FinE-tuning Approach (SAFE)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S_{desc}=\{T_{1},T_{2},\dots,T_{i},\dots,T_{m}\}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Every subject is expected to either define or describe the properties of a visual
    concept present in the generated image. When generating $E_{desc}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In the case of abstractive captions $S_{abstr}$. This scale multiplier helps
    align embeddings toward the intended meaning of a particular caption by acting
    as a prompt weight.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$W_{abstr}=\begin{cases}\text{$\beta$,}&amp;\quad\text{if $T_{i}$}\in
    T_{key}\\ \text{1,}&amp;\quad\text{otherwise.}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}$$ |  | (3) |
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $E_{abstr}=TextEnc(S_{abstr})\cdot W_{abstr}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Subject-Aware fine-tuning (SAFE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As illustrated in Figure [3](#S4.F3 "Figure 3 ‣ 4 Improving Conditioning with
    External Knowledge ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image
    Synthesis"), our method utilizes a Stable Diffusion-based backbone for news image
    generation. By taking advantage of prompt weighting and fine-tuning strategies,
    our approach aims to enhance both the image fidelity and prompt following capabilities
    over baseline models.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs for Subject Conditioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key challenge in implementing subject conditioning is identifying which
    subjects/phrases to weigh positively. To replace human guidance in the process
    of prompt weighting, we evaluate the use of LLMs in identifying salient subjects
    from sentences. Leveraging the commonsense reasoning abilities of LLMs, we utilize
    instruction-based prompting to extract salient subjects from each sentence. Subject
    identification is done in a zero-shot manner using only the prompt and the pre-trained
    world knowledge of LLMs. This process allows us to explicitly condition the input
    embeddings in an observable and explainable manner. Compared to other LLM-based
    grounding strategies such as (Lian et al., [2023](#bib.bib18)), (Feng et al.,
    [2023](#bib.bib9)) subject conditioning requires only single-stage prompting and
    also utilizes fewer tokens per generation. Additionally, we also compare different
    LLM architectures including both Commercial (GPT-4, GPT-3.5) and Open-source (Mixtral
    7x8B, Orca-13B) models on this task. This comparative study helps us ascertain
    the types of models capable of producing high-quality subject weights.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Domain-Shift
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: News images and captions have specific characteristics that differ from the
    general outputs generated by T2I models. Specifically, real-life photographs with
    specific foreground and background objects are present in abundance compared to
    artistic or animated-style images. To tackle this domain shift, we develop our
    Domain fine-tuning (DFE) strategy on \n. Traditional Mean Squared Error-based
    losses used for Stable Diffusion fine-tuning tend to generate unrealistic images
    (Zhang et al., [2018](#bib.bib49)), (Lin & Yang, [2023](#bib.bib19)), making them
    unsuitable for our task. We adopt the Reward Feedback Learning (ReFL) Xu et al.
    ([2023](#bib.bib45)) strategy for directly optimizing Stable Diffusion on a reward
    model trained to score image-caption alignment. The selected reward model ImageReward
    Xu et al. ([2023](#bib.bib45)) has been trained on 137K human-annotated samples
    to predict alignment between image-caption pairs. Our proposed improvements in
    DFE over vanilla ReFL focus on improving both alignments with the ground truth
    image and caption instead of only caption alignment. In DFE, we initialize the
    latent vector of Stable Diffusion based on the ground truth images for each caption
    instead of random initialization as implemented in ReFL. This helps control the
    diffusion process in generating target distribution-aligned images. To increase
    training stability, we opt to finetune only the Attention Layers by inserting
    rank-decomposition matrices for selective weight updates Hu et al. ([2021](#bib.bib14)).
    Additionally, the noise scheduler of the original ReFL pipeline was limited to
    having 40 timesteps. The authors identified that latents sampled between 30-39
    timesteps produced distinguishable ImageReward scores. We extend this insight
    by setting the timesteps of our noise scheduler to 100 and sampling from timesteps
    40-99 for loss calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '| Original | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/a95afdefdea18f2e6e99ff59e4f0e820.png) | ![Refer to
    caption](img/794734b605d3e52846082bb08afb1c11.png) | ![Refer to caption](img/a28392fd07b29b0a3c55a57e06602935.png)
    | ![Refer to caption](img/19a75f12bcbd57e4c10fb031d183b468.png) |'
  prefs: []
  type: TYPE_TB
- en: '| The school offers clothing, including shoes, to its students. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/2649439bb1ed654c7fd9cf0d1a811af1.png) | ![Refer to
    caption](img/178e9de9a50a7f67c9ccb3d1a7c9285c.png) | ![Refer to caption](img/27700bae46e54e3c7b651bb6ac66d16a.png)
    | ![Refer to caption](img/9fc175ee058c49d09c56b163f0d047e5.png) |'
  prefs: []
  type: TYPE_TB
- en: '| A bronze tiger shows assertiveness and a winning spirit. The books are all
    from colleagues. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/1ba3d6ffe385a20e2f85685bc2cc2144.png) | ![Refer to
    caption](img/c83fcb74c5c9d0457904b96872018262.png) | ![Refer to caption](img/698295ad11d078b139154f27f9227880.png)
    | ![Refer to caption](img/1d929f85dfae2195a43bc916b42c790f.png) |'
  prefs: []
  type: TYPE_TB
- en: '| The Galaxy Note 5 can be used with a case that doubles as a physical Qwerty
    keyboard to aid typing. |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4: Qualitative comparison of different T2I models on \n Non-Entity Subset.
    Words highlighted in Orange are used for subject conditioning'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To holistically evaluate the samples generated by T2I models on \n Non-Entity,
    we report 3 different types of metrics: Frechet Inception Distance (FID) (Heusel
    et al., [2017](#bib.bib13)) and ImageReward (Xu et al., [2023](#bib.bib45)) and
    Human Preference Score V2 (Wu et al., [2023](#bib.bib43)). Frechet Inception Distance
    serves as an indicator to quantify the overall realism and diversity of generated
    samples compared to the ground truth images. With the distribution of datasets
    like \n diverging significantly from the Inception-V3 used in traditional FID
    calculations (Kynkäänniemi et al., [2022](#bib.bib16)), we adopt the more representative
    $FID_{CLIP}$ metric for our testing. To measure the relatedness of our generated
    images and ground truth captions, we utilize ImageReward. Compared to image-caption
    similarity metrics like CLIPScore (Hessel et al., [2021](#bib.bib12)), ImageReward
    is trained on real-world image-caption pairs annotated and ranked according to
    human preference. Similarly, Human Preference Score V2 also serves as an indicator
    of human preference alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT-3.5) | 7.2804 | 0.0664 | 0.2393 |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion 2.1 (CR) | 10.6595 | -0.3388 | 0.2201 |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion 2.1 (Base) | 7.4780 | 0.02510 | 0.2385 |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion 1.5 (Base) | 7.4742 | -0.0925 | 0.2288 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results of Abstractive Text-to-Image synthesis on \n Non-Entity Subset'
  prefs: []
  type: TYPE_NORMAL
- en: Baseline Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We select SOTA T2I models: Stable Diffusion V1.5 and V2.1 (Rombach et al.,
    [2022](#bib.bib28)) as baselines for the task of news image generation. These
    baseline candidates have shown strong performance in traditional Text-to-Image
    generation benchmarks such as COCO Captions (Chen et al., [2015](#bib.bib3)).
    Additionally, we also compare the performance of LLM-powered Caption Rewriting
    (CR) for translating abstractive captions into descriptive text. Specifically,
    we use an LLM to rewrite a caption into an instruction prompt of the format ”Generate
    an image …”. We feed the modified prompt as input to our T2I model.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We set $guidance\_scale=7.5$ for all LLM extracted keywords in the original
    caption. Generated samples of Baseline and SAFE models using the same seed are
    presented in Figure [4](#S4.F4 "Figure 4 ‣ Handling Domain-Shift ‣ 4.1 Subject-Aware
    fine-tuning (SAFE) ‣ 4 Improving Conditioning with External Knowledge ‣ \n: LLM-driven
    News Subject Conditioning for Text-to-Image Synthesis"). We select GPT-3.5 as
    our default LLM model for collecting subject weights for all our fine-tuned models.
    On the \n Entity test-set, we assess the impact subject conditioning has in understanding
    abstractive captions containing NEs. We include our experimental results on the
    entity set as a part of our supplementary material.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Result Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantitative Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'On analysis of the scores presented in Table [1](#S5.T1 "Table 1 ‣ Evaluation
    Metrics ‣ 5 Experiments ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image
    Synthesis"), the SAFE models outperform both baseline Diffusion models on the
    \n Non-entity test set. The explicit guidance through both finetuning and subject
    conditioning contributes towards capturing the intended meaning of captions and
    also producing more news media-like images. Caption-Rewriting on the other hand
    performs significantly worse on all our benchmark metrics, signaling that the
    generations are not well aligned with abstractive prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the examples presented in Figure [4](#S4.F4 "Figure 4 ‣ Handling Domain-Shift
    ‣ 4.1 Subject-Aware fine-tuning (SAFE) ‣ 4 Improving Conditioning with External
    Knowledge ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis"),
    we observe that SAFE models are capable of accurate prompt following compared
    to the baselines. In the first example, we see that the subjects: ”clothing”,
    ”shoes” and ”students” guide the SAFE model’s generated outputs to contain adequate
    representation. This translates to the visible children’s shoes and clothes in
    the generated sample. On the other hand, SD 2.1 (Base) fails to capture the presence
    of shoes in its result. Another key observation is that although SD 2.1 (CR) produces
    a well-grounded image, it mistakes the style of the image to be a drawing/sketch.
    Even without implied or stated mentions of the image style, caption re-writing
    tends to guide the model’s generations towards unrealistic samples. Similarly
    in the second example, we see that SD 2.1 (Base) forgets the presence of books
    in the caption which is well showcased by other models. These examples demonstrate
    the need for explicit subject conditioning over implicit methods such as caption-rewriting.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Total Samples | Preferred Samples | Preference Score (%) (↑) |  |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT-3.5) | 185 | 102 | 55.13 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion 2.1 | 185 | 83 | 44.86 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Human Evaluation Results on \n'
  prefs: []
  type: TYPE_NORMAL
- en: Human Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We perform human evaluation of SAFE models vs baseline Stable Diffusion on Amazon
    MTurk to understand perceived variations in subject understanding. From the \n
    Non-Entity test set, we randomly sample and filter 300 captions for our survey.
    The filtration procedure includes the removal of images where both models fail
    to represent the caption’s subjects adequately. We also remove images that contain
    Text Images (i.e., images containing generated text) as this is a documented problem
    for Stable Diffusion models (Rombach et al., [2022](#bib.bib28)). The questions
    in our survey require participants to pick the image that is most related to the
    provided caption and also rate the difficulty of choosing between the two images
    on a 5-point scale. The scale ranges from 1 - ”Very easy to distinguish” to 5
    - ”Very difficult to distinguish”. This measure is utilized to understand the
    rater’s confidence in assessing the image-caption pairs. We removed all samples
    rated as ”Very difficult to distinguish” from our analysis to ensure the quality
    of human ratings. Our analysis shows that raters consistently preferred images
    generated by SAFE over the baseline model, complementing our quantitative results.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT 3.5) | 7.2804 | 0.0664 | 0.2393 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE) | 7.4851 | 0.0249 | 0.2385 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (GPT-3.5) | 7.2614 | 0.0624 | 0.2392 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (GPT-4) | 7.2482 | 0.0673 | 0.2391 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (Mixtral 7x8B) | 7.2649 | 0.0723 | 0.2394 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (Orca-13B) | 7.3571 | 0.0298 | 0.2381 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablation study evaluating the effectiveness of different model components'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Impact of Subject Conditioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we investigate the impact of each of SAFE’s components on
    generation quality as shown in Table [3](#S5.T3 "Table 3 ‣ Human Evaluation ‣
    5.1 Result Evaluation ‣ 5 Experiments ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis"). We observe that Subject Conditioning provides a
    significant contribution towards the observed metric performance. The addition
    of DFE boosts the image-caption understanding without majorly impacting image
    fidelity, as reflected in all 3 metrics presented. The positive correlation between
    ImageReward and HPS V2 Scores even with the addition of DFE confirms that the
    finetuning process hasn’t overfit on the reward model’s predictions as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Subject Weight Quality Across LLM Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To assess the variation in commonsense reasoning and world knowledge of different
    LLM architectures, we collect subject weights from 4 different LLMs: GPT-3.5,
    GPT-4, Orca-13B and Mixtral 7x8B Mixture of Experts (MoE). For our ablation study,
    we replace only the provided subject weights from each model during inference
    using SD 2.1 (Base) as our T2I backbone. We can observe a clear correlation between
    LLM performance on other commonsense reasoning tasks and key subject delineation
    with Mixtral and GPT-4 outperforming other models. Subject weights generated by
    models lower number of parameters like Orca-13B still show improvements on 2 of
    the 3 metrics compared to our baselines. This demonstrates the potential for Open-source
    LLMs to be useful for boosting caption understanding in cross-modal generative
    tasks such as T2I.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have presented \n, a novel news image-caption pair dataset for evaluating
    T2I generation performance on abstrative captions. We test popular baseline models
    on our dataset and analyze their performance concerning both image fidelity and
    image-caption alignment. We also propose SAFE for augmenting existing Diffusion-based
    generators to provide explicit subject weighting on abstractive captions. News
    domain-specific datasets \n motivate the development of journalism assistance
    tools. Our dataset can also help analyze the biases of T2I models since abstractive
    captions take advantage of the concept associations learned during their pre-training
    phase, rather than descriptively providing information on all physical attributes
    of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since we build on top of open-source Large Foundational Models such as Stable
    Diffusion, GPT-3.5, GPT-4, Mixtral-7x8B and Orca, our approach inherits all their
    biases. The lack of task-specific finetuning to improve entity likeness generation
    is another limitation that our approach faces. Future research directions include
    evaluation of concept finetuning (Kumari et al., [2022](#bib.bib15)), (Ruiz et al.,
    [2023](#bib.bib29)) on abstractive captions and developing entity concept datasets
    based on \n.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Special thanks to Aadarsh Anantha Ramakrishnan for his contributions through
    insightful research discussions and proofreading of the draft. This research has
    been partially supported by NSF Awards #1820609 and #2114824.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alikhani et al. (2019) Malihe Alikhani, Sreyasi Nag Chowdhury, Gerard de Melo,
    and Matthew Stone. CITE: A corpus of image-text discourse relations. *arXiv [cs.CL]*,
    April 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, and Tim Brooks. Improving
    image generation with better captions, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam,
    Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO captions:
    Data collection and evaluation server. *arXiv preprint arXiv:1504.00325*, April
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crowson et al. (2022) Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
    Stander, Eric Hallahan, Louis Castricato, and Edward Raff. VQGAN-CLIP: Open domain
    image generation and editing with natural language guidance. *arXiv:2204.08583
    [cs]*, April 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2019) Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
    ArcFace: Additive angular margin loss for deep face recognition. In *2019 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE, June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2020) Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia,
    and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation
    in the wild. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, pp.  5203–5212\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou,
    Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering
    Text-to-Image generation via transformers. May 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Federico (2016) Stephanie Federico. These are NPR’s photo caption guidelines.
    *NPR*, January 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2023) Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun
    Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. LayoutGPT:
    Compositional visual planning and generation with large language models. *arXiv
    [cs.CV]*, May 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. In *Advances in Neural Information Processing Systems*, volume 27\.
    Curran Associates, Inc., 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grice (1975) Herbert P Grice. Logic and conversation. In *Speech acts*, pp. 
    41–58\. Brill, 1975.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pp.  7514–7528, Online and Punta Cana, Dominican Republic, November
    2021\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard
    Nessler, and Sepp Hochreiter. GANs trained by a two Time-Scale update rule converge
    to a local nash equilibrium. In *Advances in Neural Information Processing Systems*,
    volume 30\. Curran Associates, Inc., 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman,
    and Jun-Yan Zhu. Multi-Concept customization of Text-to-Image diffusion. December
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kynkäänniemi et al. (2022) Tuomas Kynkäänniemi, Tero Karras, Miika Aittala,
    Timo Aila, and Jaakko Lehtinen. The role of ImageNet classes in fréchet inception
    distance. March 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja
    Fidler, and Antonio Torralba. BigDatasetGAN: Synthesizing ImageNet with pixel-wise
    annotations. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pp.  21330–21340, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. (2023) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. LLM-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *ArXiv*, abs/2305.13655, May 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin & Yang (2023) Shanchuan Lin and Xiao Yang. Diffusion model with perceptual
    loss. *arXiv [cs.CV]*, December 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez.
    Visual news: Benchmark and challenges in news image captioning. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 
    6761–6771, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nichol et al. (2022) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
    Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards
    photorealistic image generation and editing with Text-Guided diffusion models.
    *arXiv:2112.10741 [cs]*, March 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. (2020) Allen Nie, Reuben Cohn-Gordon, and Christopher Potts. Pragmatic
    Issue-Sensitive image captioning. In *Findings of the Association for Computational
    Linguistics: EMNLP 2020*, pp.  1924–1938, Online, November 2020\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otani et al. (2023) Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta
    Nakashima, Esa Rahtu, Janne Heikkilä, and Shin’ichi Satoh. Toward verifiable and
    reproducible human evaluation for text-to-image generation. In *2023 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  14277–14286\.
    IEEE, June 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models
    from natural language supervision. *arXiv:2103.00020 [cs]*, February 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image
    generation. February 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. Hierarchical Text-Conditional image generation with CLIP latents.
    *arXiv:2204.06125 [cs]*, April 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(27) Nils Reimers. sentence-transformers/all-MiniLM-L6-v2 · hugging face. [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).
    Accessed: 2024-4-5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion
    models. In *2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. IEEE, June 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
    Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion
    models for subject-driven generation. In *2023 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*. IEEE, June 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara
    Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
    Norouzi. Photorealistic text-to-image diffusion models with deep language understanding.
    May 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    FaceNet: A unified embedding for face recognition and clustering. In *2015 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  815–823\.
    IEEE, June 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu,
    Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton
    Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine
    Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open
    large-scale dataset for training next generation image-text models. October 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
    Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic
    image captioning. In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  2556–2565, Melbourne,
    Australia, July 2018\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *Proceedings of the 32nd International Conference on Machine Learning*, pp. 
    2256–2265\. PMLR, June 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thrush et al. (2022) Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh,
    Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and
    language models for visio-linguistic compositionality. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  5238–5248,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2020) Alasdair Tran, Alexander Mathews, and Lexing Xie. Transform
    and tell: Entity-aware news image captioning. In *2020 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, pp.  13032–13042, Seattle, WA,
    USA, 2020\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van Hulst et al. (2020) Johannes M van Hulst, Faegheh Hasibi, Koen Dercksen,
    Krisztian Balog, and Arjen P de Vries. REL: An entity linker standing on the shoulders
    of giants. In *Proceedings of the 43rd International ACM SIGIR Conference on Research
    and Development in Information Retrieval*, SIGIR ’20, pp.  2197–2200, New York,
    NY, USA, July 2020\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł Ukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan,
    and R Garnett (eds.), *Advances in Neural Information Processing Systems*, volume 30\.
    Curran Associates, Inc., 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vedantam et al. (2017) Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi
    Parikh, and Gal Chechik. Context-Aware captions from Context-Agnostic supervision.
    In *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 
    1070–1079\. IEEE, July 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Jianyi Wang, Kelvin C K Chan, and Chen Change Loy. Exploring
    CLIP for assessing the look and feel of images. *arXiv [cs.CV]*, July 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, Chong Ma,
    Sigang Yu, Haixing Dai, Qiushi Yang, Yiheng Liu, Songyao Zhang, Enze Shi, Yi Pan,
    Tuo Zhang, Dajiang Zhu, Xiang Li, Xi Jiang, Bao Ge, Yixuan Yuan, Dinggang Shen,
    Tianming Liu, and Shu Zhang. Review of large vision models and visual prompt engineering.
    July 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang,
    Benjamin Hoover, and Duen Horng Chau. DiffusionDB: A large-scale prompt gallery
    dataset for text-to-image generative models. *arXiv:2210\. 14896 [cs]*, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu,
    Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating
    human preferences of text-to-image synthesis. *arXiv [cs.CV]*, June 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand,
    and Song Han. FastComposer: Tuning-free multi-subject image generation with localized
    attention. *arXiv [cs.CV]*, May 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li,
    Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and evaluating human
    preferences for Text-to-Image generation. April 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-Grained text to image generation
    with attentional generative adversarial networks. In *2018 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  1316–1324, Salt Lake City, UT,
    USA, 2018\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2023) Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi,
    Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting anybody in diffusion models
    via celeb basis. June 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
    and Oriol Vinyals. Understanding deep learning requires rethinking generalization.
    *arXiv:1611.03530 [cs]*, February 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
    and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual
    metric. In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pp.  586–595, Salt Lake City, UT, 2018\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris
    Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards Language-Free
    training for Text-to-Image generation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  17907–17917\. openaccess.thecvf.com,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: Dynamic
    memory generative adversarial networks for Text-To-Image synthesis. In *2019 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  5795–5803,
    Long Beach, CA, USA, 2019\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Dataset Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Caption Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we provide additional statistics on the \n dataset and analyze
    the distribution of image-caption pairs. In Table [4](#A1.T4 "Table 4 ‣ A.1 Caption
    Statistics ‣ Appendix A Dataset Insights ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis"), we provide caption statistics of \n compared to
    2 popular image-caption pair datasets: COCO Captions Chen et al. ([2015](#bib.bib3))
    and Conceptual Captions 3M (CC3M) Sharma et al. ([2018](#bib.bib33)). By tokenizing
    and lemmatizing each caption without case sensitivity, we compute the number of
    unique tokens present in a dataset. We utilize the NLTK library for both tokenization
    and lemmatization. We can observe that across different data splits of \n, the
    mean caption length is significantly higher with a greater variation in caption
    length compared to other datasets. In addition to the increased caption length,
    it also contains a significant amount of unique tokens considering the number
    of samples present. This highlights the diversity of captions in \n, showing greater
    expression in describing visual concepts. Table [4](#A1.T4 "Table 4 ‣ A.1 Caption
    Statistics ‣ Appendix A Dataset Insights ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis") also enables us to verify the consistency of image-caption
    pair properties present across the train, test, and validation splits.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Unique Tokens | Caption Length |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | StdDev |'
  prefs: []
  type: TYPE_TB
- en: '| COCO Captions Train | 22767 | 10.42 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| COCO Captions Val | 16647 | 10.42 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| CC3M Train | 45896 | 10.31 | 3.30 |'
  prefs: []
  type: TYPE_TB
- en: '| CC3M Val | 9289 | 10.40 | 3.35 |'
  prefs: []
  type: TYPE_TB
- en: '| \n Non-Entity Train | 51026 | 14.84 | 5.51 |'
  prefs: []
  type: TYPE_TB
- en: '| \n Non-Entity Val | 10619 | 14.93 | 5.38 |'
  prefs: []
  type: TYPE_TB
- en: '| \n Non-Entity Test | 10485 | 14.67 | 5.36 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Caption Statistics of \n'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Size | Related | Abstractive | Descriptive |'
  prefs: []
  type: TYPE_TB
- en: '| \n Full | 300 | 291 | 260 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| \n Entity | 100 | 99 | 93 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| \n Non-Entity | 200 | 192 | 167 | 25 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Human Evaluation of \n Dataset abstractiveness'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Dataset Quality and Diversity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For analyzing the categories of articles from which image-caption pairs have
    been selected for our dataset, we provide a unified category list in Figure [5](#A1.F5
    "Figure 5 ‣ A.2 Dataset Quality and Diversity ‣ Appendix A Dataset Insights ‣
    \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis"). With articles
    sourced from different news agencies, each source has its own article category
    taxonomy. To create a unified taxonomy, we fix the categories provided by articles
    from NYTimes as our template. To cluster similar article categories under one
    label, we utilize the lightweight sentence transformer all-MiniLM-L6-v2 [Reimers](#bib.bib27)
    . With a minimum similarity threshold of 0.5, we cluster every sample’s default
    topic description into NYTimes’s taxonomy labels. Here, we visualize our dataset’s
    top 30 article classes, showing the diverse spread of image-caption pairs present.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further assess the overall quality of the dataset and the number of image-caption
    pairs that are abstractive, we conducted a human evaluation study on Amazon MTurk.
    We consider a random sample of 300 samples extracted from the test split of \n.
    Out of the 300 selected images, 200 belong to the non-entity subset and 100 belong
    to the entity subset. Using this extracted sample, we perform a human evaluation
    of our dataset quality. The two questions we mainly aim to answer through this
    evaluation are: (1) Are the image-caption pairs closely related to each other
    from a human perspective? and (2) Are these captions Abstractive in nature? We
    launched our survey with 150 unique participants and each participant rated 10
    samples. The survey layout is presented in Figure [6](#A2.F6 "Figure 6 ‣ Appendix
    B Qualitative Evaluation of Generated Samples ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis"). Per Image-caption pair, we collect 5 responses
    amounting to a total of 1500 responses. We tabulate our results in [5](#A1.T5
    "Table 5 ‣ A.1 Caption Statistics ‣ Appendix A Dataset Insights ‣ \n: LLM-driven
    News Subject Conditioning for Text-to-Image Synthesis"). We observe that 97% of
    surveyed samples are related to each other. We also see that 89.3% of the related
    captions are rated as Abstractive in nature with the remaining 10.7% being descriptive.
    This supports our hypothesis and validates that our dataset pre-processing pipeline
    produces high-quality image-caption pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b8e8f7f72b6bfe910076cc05628a1df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: \n Distribution of Article Topics for samples in \n'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Qualitative Evaluation of Generated Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this project, all human evaluation surveys were created on Qualtrics and
    distributed through Amazon MTurk. All our studies have been conducted with Institutional
    Review Board (IRB) approval. We do not collect any personally identifiable data
    from participants in our study. Voluntary consent is obtained from each participant
    before taking part in all studies. We provide clear instructions for each evaluation
    task presented to participants with examples and test their understanding using
    a pre-survey questionnaire. This is done to ensure data quality and improve the
    consistency of task understanding across participants. Attention Check questions
    were also incorporated to prevent low-quality submissions from being accepted.
    The demographic for participants taking part in our survey was limited to people
    above 18 years of age. We provide screenshots of our survey user interface in
    Figure [7](#A2.F7 "Figure 7 ‣ Appendix B Qualitative Evaluation of Generated Samples
    ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis").'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we compute the inter-annotator agreement scores for our MTurk
    participants to assess the significance of our results. Using Krippendorff’s $\alpha$
    being lower than the average scores reported on other rating tasks, we identify
    key reasons why this may be the case. In our case, each annotator does not rate
    every question present in our evaluation samples. So, the unanswered questions
    by a survey participant are treated as missing values. The high number of missing
    values when utilizing the typical formulation of this metric is one reason for
    the lower score observed. Additionally, other studies attempting to assess inter-annotator
    agreement of T2I generators (Otani et al., [2023](#bib.bib23)) on complex text-image
    datasets such as DrawBench (Saharia et al., [2022](#bib.bib30)) have reported
    similarly low scores, indicating the difficulty of this task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d6639755dd1b7a84f6c9610c9727cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Survey UI for Data Quality Evaluation Study'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb02392a0c07c45f87003fe8a536229a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Survey UI for Generated Image Evaluation Study'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Salient Subject Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 LLM Prompting Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For extracting subject weights, we query multiple LLM architectures through
    instruction-based prompting. To reduce memory requirements and to speed up inference,
    we initialize in mixed precision mode and set $dtype=float16$. The system prompt
    is set as You are an AI assistant that follows instructions extremely well. Help
    as much as you can. In cases where the LLM returned no salient subject phrases,
    we default to using the standard text prompt as input for T2I generation.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5/4 & Mixtral 8x7B
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The user prompt for generating high-quality subject weights is set as: Use
    only the information provided in the prompt for answering the question. List the
    main topic word and additional topic words from the given image caption in the
    format: {”main_topic_word”: $<$'
  prefs: []
  type: TYPE_NORMAL
- en: Orca Mini 13B
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We modify the user prompt for Orca Mini to directly list salient subjects without
    delineation between ”main_topic_word” and ”additional_topic_words”. The user prompt
    is set as ”User: List only the main objects from the sentence: $<$”. From our
    testing, we found that this prompt pattern struck a balance between returning
    usable subject phrases and identifying only the salient subjects.'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Impact of Subject Scale Multiplier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We perform the ablation study to identify the optimal value for the subject
    scale multiplier for implementing subject conditioning of text embeddings. We
    evaluate various candidate score multipliers as shown in Table [6](#A3.T6 "Table
    6 ‣ C.2 Impact of Subject Scale Multiplier ‣ Appendix C Salient Subject Selection
    ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis"). Here,
    x1 refers to a scale factor of $1.1$, and so forth. We selected a scale multiplier
    of x2 as it scores the highest in 2 out of 3 metrics tested. Increasing the scale
    multiplier beyond x2 does not provide any meaningful improvement in generation
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT 3.5) (x1) | 7.3729 | 0.0564 | 0.2395 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT 3.5) (x2) | 7.2804 | 0.0664 | 0.2393 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT 3.5) (x3) | 7.3049 | 0.0040 | 0.2361 |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT 3.5) (x4) | 7.8825 | -0.1835 | 0.2255 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Ablation study evaluating the effectiveness of different scale multiplier
    values'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Performance on \n Entity Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating images of NEs, especially faces of people has been a challenging
    task for Text-to-Image generators due to the inability of these models to accurately
    encode facial features during their pre-training phase (Xiao et al., [2023](#bib.bib44)).
    We evaluate the ability of SAFE models to improve the quality of generated objects
    when prompted with news captions containing NEs. To accomplish this, we create
    a test-only set consisting of 50 images for each NE class present in \n Entity
    for evaluation. We extract subject weights for each caption in the test set and
    run inference of both SD 2.1 (Base) and SAFE (DFE + GPT-3.5) finetuned on the
    Non-entity set.
  prefs: []
  type: TYPE_NORMAL
- en: 'On \n Entity, we include Identity Preservation and Face Detection accuracy
    as additional metrics to quantify their entity image generation performance similar
    to (Yuan et al., [2023](#bib.bib47)). For detection accuracy, we utilize a RetinaFace
    (Deng et al., [2020](#bib.bib6)) based face detector and measure the average number
    of times a face is detected in a generated image. For Identity Preservation, we
    measure the degree of similarity between the reference image for an entity and
    the detected faces in an image. Out of the generated samples that have discernible
    faces, greedy matching is performed to select the most related face using Cosine
    Similarity distance. The ArcFace (Deng et al., [2019](#bib.bib5)) face recognition
    model is used for calculating Identity Preservation scores. We report the average
    metric scores across all entity classes in Table [7](#A4.T7 "Table 7 ‣ Appendix
    D Performance on \n Entity Set ‣ \n: LLM-driven News Subject Conditioning for
    Text-to-Image Synthesis") to visualize the overall quality of generated images.
    We also present Figure [8](#A4.F8 "Figure 8 ‣ Appendix D Performance on \n Entity
    Set ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis") with
    examples Ex1, Ex2, Ex3, Ex4 generated using the \n Entity Test Set.'
  prefs: []
  type: TYPE_NORMAL
- en: We observe that qualitatively, the generated images using SAFE cohesively include
    most objects presented in the caption while the images generated by SD 2.1 (Base)
    focus primarily on the NE’s present in the caption. This results in higher-quality
    faces generated at the cost of image artifacts such as repeated generations of
    the same entity. Quantitative metrics also struggle to capture image-caption similarity
    accurately when captions and images contain NEs as facial artifacts are penalized
    more compared to subject-forgetting errors. Similar inadequacies in metrics for
    assessing NE images have been observed by other works in the area of personalized
    T2I generation Yuan et al. ([2023](#bib.bib47)). Our experiments highlight the
    need for improved metrics to decouple the impact of generating NE features faithfully
    and representing all subjects mentioned in the caption accurately. Personalized
    T2I generation approaches may help alleviate the challenges in generating Entity
    images which we include as a part of our future work.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Identity (↑) | Detect (↑) | $FID_{CLIP}$ (↓) | ImageReward (↑) |
    HPS V2 (↑) |'
  prefs: []
  type: TYPE_TB
- en: '| SAFE (DFE + GPT-3.5) | 0.3323 | 0.9498 | 30.7756 | 0.6072 | 0.2564 |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion 2.1 (Base) | 0.3391 | 0.9533 | 30.0651 | 0.6060 | 0.2565
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Results of Abstractive Text-to-Image synthesis on \n Entity Test-set
    averaged across all classes'
  prefs: []
  type: TYPE_NORMAL
- en: '| Orig | SD 2.1 (Base) | SAFE (DFE + GPT-3.5) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/a2afaed4050016eb6269501f9480ded5.png) | ![Refer to
    caption](img/5e6c8aaba1248affb02811912b746e71.png) | ![Refer to caption](img/940a1d302d4c19ea3149bfee2bc08da1.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Obama awards Medal of Honor to member of SEAL Team 6. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/77879489b57a3ca350e2a60ca445665a.png) | ![Refer to
    caption](img/d3ba12bc872e50357c83b995acad453e.png) | ![Refer to caption](img/5f47211881019106be9296c6a7cb8e81.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| London’s mayor Boris Johnson gives a big thumbs up to photographers during
    the unveiling of the 2012 Olympic rings on Tower Bridge. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/75388dbe05a9ffa34de94ddc3ef12d38.png) | ![Refer to
    caption](img/132b694b75e77ec38ed4ba96f1e22200.png) | ![Refer to caption](img/0272297eefc80b9a3939f17c8c486573.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Donald Trump waves to the crowd during a campaign rally on June 18 2016 in
    Phoenix. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/7f44abf1a60c1a887672407021119155.png) | ![Refer to
    caption](img/c4f0014975761a5ba525947275c348bf.png) | ![Refer to caption](img/4ac3e9239d514f68ea30b927cc8f3a67.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hillary Clinton greets audience members following a campaign organizing event
    at Eagle Heights elementary in Clinton Iowa. |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 8: Qualitative comparison of different T2I models on \n Entity Subset.
    Words highlighted in Orange are used for subject conditioning'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Additional Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We provide additional generated examples using both baseline and SAFE models
    for reference. Figure [9](#A5.F9 "Figure 9 ‣ Appendix E Additional Examples ‣
    \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis") with examples
    Ex5, Ex6, Ex7, Ex8, Ex9, Ex10, Ex11, Ex12 are generated from the test set of \n
    Non-Entity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Orig | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/3cba8c3b033b6d631deac7f90edab736.png) | ![[Uncaptioned
    image]](img/1c01367052b0adc46b14cc65c564aa4d.png) | ![[Uncaptioned image]](img/9781c90aa65dc87254d07eba1b0e169f.png)
    | ![[Uncaptioned image]](img/d819cb9ea3563cb8c0c6f63e01d5baf7.png) |'
  prefs: []
  type: TYPE_TB
- en: '| A Faraday bag, which blocks remote signals to devices such as cellphones
    and tablets. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/bb80fadbf1b162ebc1ea025638e5c1d1.png) | ![[Uncaptioned
    image]](img/a2c8ab33b9410c3fe807c290e39e4983.png) | ![[Uncaptioned image]](img/3acfd46e5492720d2424ed5884971f6a.png)
    | ![[Uncaptioned image]](img/fdcd4514ea0d958785ecfcf72706929d.png) |'
  prefs: []
  type: TYPE_TB
- en: '| At first glance, pelota mixteca might resemble elements of baseball, volleyball
    and tennis, but a closer examination reveals a bit more nuance. Each jugada, as
    each individual game is called, involves approximately 10 players, and begins
    when one player initiates a serve from the cement slab. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4fdc599e9f3d6237fdd184f972a99b1c.png) | ![[Uncaptioned
    image]](img/9bb90280f56f54af7b6605319f11cce3.png) | ![[Uncaptioned image]](img/ed6db097acb9545502e3257d58220125.png)
    | ![[Uncaptioned image]](img/67c9bdb7ca00a7ef48a5d09cf70e7644.png) |'
  prefs: []
  type: TYPE_TB
- en: '| The painted figure of a man is illuminated through a doorway to the dwelling.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/83ad84646ab5ff380cf7fcfb10fef3e5.png) | ![[Uncaptioned
    image]](img/1c0a4d0d2d46bc88012e46fabd7b7a5c.png) | ![[Uncaptioned image]](img/75f8c2553bae2f7ec7748b92bddd9e19.png)
    | ![[Uncaptioned image]](img/5fe80e9516a99b2acab7bfc722b636cb.png) |'
  prefs: []
  type: TYPE_TB
- en: '| The detachable cable means the music doesn’t stop when the battery runs out
    but also makes it easy to connect to a computer or share your music with a friend.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Orig | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/afc36a52b00a1457e4d7769edc5971df.png) | ![Refer to
    caption](img/6cb689b80853d3505bd59bc17dd8c1c2.png) | ![Refer to caption](img/d31932947a5586a6b7e71211a8600292.png)
    | ![Refer to caption](img/5be1fbc3725f891f3eb712479824fd75.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Mediastreaming boxes can turn any TV smart or add features and channels to
    others for as little as 15. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/e2995c79f205aa5bdaefc23ad823801d.png) | ![Refer to
    caption](img/efcf717650edf5c648d871c37c25e73c.png) | ![Refer to caption](img/3b60389f8f748e36e63defdfa4c6917b.png)
    | ![Refer to caption](img/424dd770fc5e7747271c3ce1c16b3ab7.png) |'
  prefs: []
  type: TYPE_TB
- en: '| A sharp knife, one of a cook’s essential tools, is used to carefully cut
    onions, which are easier to brown (if they’re not bludgeoned) for a confit. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/db251e647fb99ecdf3d2fc814dffa6eb.png) | ![Refer to
    caption](img/ef4b2efece3045105174bce9eb18bc88.png) | ![Refer to caption](img/ed551ae87cfd52dafc9fbf746fea0da7.png)
    | ![Refer to caption](img/6bded426f2c18555cb6ebb9919ecc5cd.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Although the home was built in 1871, the ground-floor commercial kitchen
    is contemporary. |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/7b89ab4e1293f8e417d84d8ebabce016.png) | ![Refer to
    caption](img/c6d784cf4401283db4e36473d2990b92.png) | ![Refer to caption](img/ffd390e7f00f3f6cba182adef51756d9.png)
    | ![Refer to caption](img/7bba7e55431d243c59809f7e16bcacc0.png) |'
  prefs: []
  type: TYPE_TB
- en: '| About 200 firefighters were called to 401 Jewett Avenue. The front of the
    house was engulfed in flames when they arrived. |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 9: Qualitative comparison of different T2I models on \n Non-Entity Subset.
    Words highlighted in Orange are used for subject conditioning'
  prefs: []
  type: TYPE_NORMAL
