- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04665](https://ar5iv.labs.arxiv.org/html/2408.04665)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lei Shi Lei Shi (shijim@gmail.com) and Zhimeng Liu contribute equally in this
    work. Yue Zhang (yue.zhang@wias.org.cn) and Ge Wang (gewang@ustb.edu.cn) are corresponding
    authors. Beihang University Zhimeng Liu University of Science and Technology Beijing
    Yi Yang Beihang University Weize Wu Beihang University Yuyang Zhang Beihang University
    Hongbo Zhang Westlake University Jing Lin University of Science and Technology
    Beijing Siyu Wu Beihang University Zihan Chen Beihang University Ruiming Li Beihang
    University Nan Wang Beihang University Zipeng Liu Beihang University Huobin Tan
    Beihang University Hongyi Gao University of Science and Technology Beijing Yue
    Zhang Westlake University Ge Wang University of Science and Technology Beijing
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
    literature text has been challenging but crucial for the logical design of new
    MOFs with desirable functionality. The recent advent of large language models
    (LLMs) provides disruptively new solution to this long-standing problem and latest
    researches have reported over 90% F1 in extracting correct conditions from MOFs
    literature. We argue in this paper that most existing synthesis extraction practices
    with LLMs stay with the primitive zero-shot learning, which could lead to downgraded
    extraction and application performance due to the lack of specialized knowledge.
    This work pioneers and optimizes the few-shot in-context learning paradigm for
    LLM extraction of material synthesis conditions. First, we propose a human-AI
    joint data curation process to secure high-quality ground-truth demonstrations
    for few-shot learning. Second, we apply a BM25 algorithm based on the retrieval-augmented
    generation (RAG) technique to adaptively select few-shot demonstrations for each
    MOF’s extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs,
    the proposed few-shot method achieves much higher average F1 performance (0.93
    vs. 0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
    fully automatic evaluation that are more objective than the previous human evaluation.
    The proposed method is further validated through real-world material experiments:
    compared with the baseline zero-shot LLM, the proposed few-shot approach increases
    the MOFs structural inference performance ($R^{2}$) by 29.4% in average.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metal-Organic Frameworks (MOFs), a class of high performance porous material,
    have been widely applied to catalysis, gas storage, and groundwater remediation
    [[5](#bib.bib5)] for its prestige in structural tunability and functional versatility
    [[2](#bib.bib2)]. These advantages are deeply rooted in the flexible yet logical
    synthesis configuration of MOFs. Herein, precise and comprehensive knowledge of
    MOFs synthesis conditions becomes extremely important to fully understand its
    structural mechanism and discover new MOFs or sub-types, posing a fundamental
    challenge to the whole discipline of MOFs and reticular chemistry [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there have been 100k+ MOFs successfully synthesized in the laboratory.
    Their detailed synthesis conditions are often recorded by academic literature
    in various textual or tabular formats. Machine learning methods, in particular,
    text mining algorithms, are normally applied to the literature text to automatically
    extract synthesis conditions. However, the complexity and volatility of free text
    limits the accuracy of synthesis condition extraction [[13](#bib.bib13)], which
    could jeopardize the effectiveness of downstream material applications over extracted
    synthesis data.
  prefs: []
  type: TYPE_NORMAL
- en: The emergence of large language models (LLMs) to some extent resolves the problem
    of synthesis condition extraction from disparate forms of scientific texts, due
    to their well-known expertise in the whole-spectrum of text mining tasks [[3](#bib.bib3)].
    Recently, Zheng et al. [[24](#bib.bib24)], Dagdelen et al. [[7](#bib.bib7)], Polak
    and Morgan [[16](#bib.bib16)] have applied zero-shot or fine-tuned LLMs to extract
    synthesis conditions from experimental MOFs literature. They reported extraction
    performance of close to 0.9 in F1 metric, but mostly over small datasets and evaluated
    by subjective evaluations. It should be pointed out that the baseline zero-shot
    LLMs are notorious for their poor performance on sparse scenarios like MOFs synthesis,
    which are infrequently covered by the general-purpose LLM training data [[6](#bib.bib6)].
    Therefore, evaluating the MOFs condition extraction performance with large-scale,
    real-life datasets become crucial for improving both the quantity and quality
    of MOFs synthesis knowledgebase. In addition, guided material experiments over
    extracted synthesis conditions, which are rarely conducted in previous works,
    should also be an important norm to evaluate the effectiveness of targeted synthesis
    condition extraction task.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we set out to overcome the notable limitations when applying primitive
    zero-shot LLMs to the problem of MOFs synthesis condition extraction from scientific
    texts. The main theme of this paper is to introduce the few-shot in-context learning
    paradigm as the standard approach to augment general-purpose LLMs on the material
    synthesis condition extraction problem. As shown by our experiment results of
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations"), in a dataset randomly sampled from
    84,898 well-defined MOFs, the proposed few-shot method achieves much higher average
    F1 performance (0.93 vs. 0.81, +14.8%) than the native zero-shot LLMs, both using
    the state-of-the-art GPT-4 Turbo model^†^††The latest GPT-4v model has enhanced
    video and image analysis capability, but not for text analysis. [[1](#bib.bib1)],
    as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations").
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, deploying few-shot LLMs to solve the current problem still faces
    multiple nontrivial challenges. First, the superiority of few-shot LLMs depends
    on the data quality of their ground-truth demonstrations. In the scenario of MOFs
    synthesis extraction, obtaining ground-truth textual conditions scattered in scientific
    literature in numerous formats remains a daunting task. It would be extremely
    costly to apply traditional human annotation approach given that a change of material
    would require a totally new demonstration dataset. Second, the quantity of ground-truth
    demonstrations selected for each LLM extraction is also critical as high-performance
    LLMs are mostly commercial and charged by input size. For example, the fine-tuning
    technology is known to greatly improve the LLM performance [[7](#bib.bib7)], but
    will normally require hundreds of examples and a locally-stored large set of model
    weights. The application overheads to new synthesis extraction scenarios are quite
    high, thus reducing the adaptability of fine-tuning methods. In our case, minimizing
    the number of few-shot demonstrations would require an elaborate algorithm to
    select the demonstrations adaptively for each MOF’s raw synthesis text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we introduce two new methods to resolve the above challenges.
    First, on the preparation of ground-truth demonstrations, to our surprise, human
    annotation and AI annotation show complementary advantages, not only in the annotation
    cost, but also in their output data quality. We then propose a human-AI joint
    data curation process, which enjoys the best of both worlds and offers the highest
    data quality in ground-truth demonstrations produced. Second, based on the popular
    retrieval-augmented generation (RAG) technique, we propose to apply the BM25 algorithm
    to adaptively select the best combination of few-shot demonstrations for each
    MOF’s synthesis extraction, whose performance significantly outruns the baseline
    random selection method. Our experiment results also suggest the most appropriate
    number of demonstrations for the trade-off between performance and cost. It is
    shown that a small overhead of 4-shots could already achieve the optimal performance,
    contrasting to tens to a hundred shots in other domains. In addition, we study
    the utility of different kinds of knowledge on our task when incorporated by LLM:
    the background knowledge on retrieved synthesis conditions, their application
    constraints on the numerical/textual format, and the few-shot demonstrations.
    Notably, the few-shot examples are shown to be the most critical. To our knowledge,
    we are the first to apply and optimize few-shot in-context learning LLM methods
    for the material synthesis condition extraction problem from scientific text.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/923c6735402c7f65318d282f31a3ebf5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Key indicators (F1, ACC, Precision, Recall) of the synthesis condition
    extraction performance on 123 MOFs with ground-truth data: (a) our 4-shot RAG
    algorithm; (b) zero-shot LLM as the baseline; (c) confusion matrix definition
    for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we have considered the scalability issue for high-throughput synthesis
    extraction. The additional overhead includes the labor cost to acquire external
    knowledge (e.g., expert annotations on the literature text), the financial cost
    to request LLM APIs, and the computational cost to potentially train in-house
    LLMs. For example, by the latest GPT-4 pricing model (10$ per 1M tokens), a single
    pass over all the 100k available MOFs synthesis literature (est. 10k words per
    literature) sums up to a non-negligible cost of 10k$, while performance tuning
    normally requires several passes. Three techniques adapted to large-scale material
    data are proposed. First, we learn an offline model to detect the most relevant
    synthesis paragraphs out of each literature, with an overall accuracy of 98.9%.
    In this way, the financial cost in using commercial LLMs is reduced by 94% (the
    average word count of a literature and its synthesis paragraphs are est. 15,000
    and 900, respectively). The fully-tuned high-throughput synthesis extraction workflow
    now processes over 500 millions of scientific texts from all available MOFs literature
    within 7 hours. Second, we conduct experiments to quantify the size of demonstration
    pool as material data scales. Though it is shown that larger example pools almost
    always contribute to the performance, the margin quickly drops as more annotations
    are available. In the extreme, a pool of size $K$) LLMs is the most cost-effective.
    Third, we develop a LLM-based coreference resolution method to restore proxy words
    like “L” or “H2L” into its entirety. Though only a small portion of extracted
    synthesis condition on organic linker suffer from the use of proxy words, it mounts
    to a big number and affects the downstream material tasks on large-scale data.
    By applying our method, only 2.3% linkers remain unresolved.
  prefs: []
  type: TYPE_NORMAL
- en: To validate the importance of our proposal, we set up a real-world MOFs synthesis-structure
    inference experiment, in which the proposed few-shot LLM method is compared with
    the existing LLM application in material synthesis extraction scenario (e.g.,
    zero-shot[[24](#bib.bib24)][[16](#bib.bib16)]). On a set of 5,269 MOFs curated
    from the CSD database, we manage to build machine learning models to predict MOFs
    microstructure properties (framework density, cavity diameter, etc.) with the
    synthesis conditions obtained by LLM. By 6 off-the-shelf machine learning models,
    the inference performance ($R^{2}$) by the proposed few-shot method is consistently
    higher than the benchmark zero-shot approach, with an average improvement of 29.4%.
  prefs: []
  type: TYPE_NORMAL
- en: We also make available an online visual database showing the extracted synthesis
    conditions of all the 36,177 MOFs with literature available from the CSD database
    (see the supplemental material for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ebfd14548bd2aee89ab2e8950b8bf498.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The overall pipeline of the few-shot in-context learning method for
    synthesis condition extraction from MOFs literature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in our technology pipeline of Figure [2](#S2.F2 "Figure 2 ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    the MOFs literature dataset are first collected and pre-processed into compatible
    input format for LLMs (see Sec. [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations") for details).
    The latest high-performance LLM (i.e., GPT-4) is employed to extract 10 essential
    conditions for the synthesis of each MOF: metal precursor name & amount, organic
    linker name & amount, solvent name & amount, modulator name & amount, and synthesis
    reaction duration & temperature. The synthesis extraction result is first evaluated
    on their literal accuracy with respect to an expert-curated ground-truth dataset,
    and then tested on the real-world scenarios of material structure inference and
    design. On the randomly sampled 123 MOFs synthesis literature from all the 36177
    MOFs, the extraction of 1230 synthesis conditions using the proposed few-shot
    LLM model achieves a best average F1 metric of 0.93 (ACC = 0.90), using a few-shot
    demonstration of only 4 examples. The full performance result is illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(a), in comparison to the baseline zero-shot
    approach with an average F1 of 0.81 (ACC = 0.77) in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(b). The dataset statistics is listed in Figure [4](#S2.F4 "Figure
    4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Human-AI Joint Data Curation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0b9405bec6e64faecdbb7599e3b4911.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The human-AI reflection procedure to improve few-shot examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To introduce the few-shot LLM method, a prerequisite is to obtain a high-quality
    demonstration pool on the synthesis condition extraction task, i.e., the ground-truth
    annotations. Traditionally, human annotations are the sole means to collect these
    examples for the few-shot learning. In this work, we also start with a standard
    annotation protocol which includes three steps: 1) *pilot annotations* on 20 typical
    literature by the leading experts to reach consensus on the rigorous format of
    MOFs synthesis conditions; 2) *batch annotations* conducted by 6 experts over
    180 MOFs synthesis paragraphs randomly chosen from the entire dataset. Each paragraph
    is double annotated by two experts to ensure reliability; 3) *finalized annotations*
    by only keeping the MOFs synthesis conditions that are agreeing between the two
    experts, while removing annotated paragraphs that are inappropriate as examples
    (e.g., having more than one suite of MOFs synthesis conditions in the same paragraph).
    Eventually, we obtain a ground-truth human annotation dataset composed of 147
    suites of MOFs synthesis conditions. The full detail of our annotation approach
    and an online software to assist the process is described in Sec. [5.2](#S5.SS2
    "5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣
    5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").'
  prefs: []
  type: TYPE_NORMAL
- en: Using the human annotations developed above as examples, the performance of
    few-shot LLM models is depicted by the solid orange+triangle lines of Figure [6](#S2.F6
    "Figure 6 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
    The average F1 metric rises from 0.81 (zero-shot) to the peak of 0.86 ($K=2$,
    both metrics drop. The explanation might be that more information without ground-truth
    distracts the LLM, rather than coaches it.
  prefs: []
  type: TYPE_NORMAL
- en: The above results indicate that neither human annotations nor purely AI-generated
    examples achieve optimal data quality for LLM few-shot learning. To delve deeper
    into the issue, several leading MOF experts were consulted to evaluate all errors
    produced by the few-shot LLM method when using human annotations as the sole examples
    and ground-truths. Out of 261 potential errors, 103 LLM outputs (39.5%) were identified
    as correct, 38 (14.6%) had certain issues but contributed to refining the corresponding
    ground-truth, and only 120 (45.9%) were true errors. The experts then compiled
    a revised set of ground-truth annotations, including the synthesis conditions
    for 123 individual MOFs. The remaining 23 annotated conditions were deemed inappropriate
    because they either involved chiral MOFs with duplicate synthesis conditions and
    paragraphs or contained multiple MOF synthesis processes within a single paragraph.
    Although our technical framework can deal with the case of having multiple MOFs
    in a single paragraph, we chose the paragraphs describing the synthesis of only
    one MOF for more precise demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: With this empirical experience, we propose a human-AI joint data curation process
    for the data quality optimization of ground-truth demonstrations in LLM-based
    few-shot learning paradigm. As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Human-AI
    Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(a), raw synthesis paragraphs are first processed
    by LLM in a zero-shot mode. Human experts then work on the initial AI annotation
    to achieve a best-effort human annotation (Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Human-AI
    Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(b)), which is the first round of reflection. After
    that, these human annotations are used as demonstrations in a LLM-based few-shot
    synthesis condition extraction Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Human-AI Joint
    Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using
    Few-Shot Demonstrations")(c), which is the second round of reflection and generates
    few-shot AI annotations. Lastly, human experts combines human annotations and
    few-shot AI annotations into the human-AI joint annotation (Figure [3](#S2.F3
    "Figure 3 ‣ 2.1 Human-AI Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(d)), in the final round of
    reflection. We apply the few-shot LLM model over the final demonstrations with
    the highest-level of data quality. The best performance (F1=0.93 and ACC=0.9)
    is achieved at the point of most appropriate few-shot quantity ($K=4$, as shown
    by the solid blue lines in Figure [6](#S2.F6 "Figure 6 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations").
  prefs: []
  type: TYPE_NORMAL
- en: 'We ascribe the superiority of human-AI joint data curation to three reasons,
    all due to the complementary nature of human expertise and AI’s capacity. First,
    though human are excellent in flexible usage of material knowledge, they often
    fail to strictly follow pre-defined annotation rules. For example, to standardize
    the solvent condition, it is required to leave out all modifiers of a common solvent.
    Human annotators sometimes extract “hot water” instead of “water”, because his/her
    focus is on the knowledge extraction and neglects the rules. Human are poor multi-objective
    task executors compared with AI, who will not introduce error if only these rules
    are provided in either background prompt or examples. Second, human often suffer
    from fatigue issue when working with a large set of annotation tasks as ours.
    Random errors are then generated, i.e., missing or adding a few characters/words.
    Though redundant annotation by more than one expert can eliminate these random
    errors, it is at the cost of excluding many useful annotations when the redundant
    outputs are different. Here AI is applied to alleviate this issue: an initial
    zero-shot LLM annotation reduces human efforts, their fatigue, and the resulting
    random errors in the first round of reflection; the few-shot LLM output also works
    as a caliber to help resolve differences between redundant human annotations in
    the second round of reflection.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, on the other hand, the current general-purpose LLM alone is not the
    ultimate solution to our task. According to the medium performance of zero-shot
    LLM, it lacks specialized knowledge on MOFs synthesis conditions. Though the few-shot
    demonstrations mitigate this deficiency through in-context learning, the scalability
    issue makes it very hard to achieve a close to 100% accuracy. For example, retrieving
    one synthesis condition from a paragraph may require several demonstrations to
    cover all the lexical and syntactic pattern around the target condition. Retrieving
    all 10 conditions then demands tens of examples, inducing a cost magnitudes more
    than the current setting of $K=4$. Customized few-shot algorithms will be needed
    to achieve such goal. Therefore, in the final round of reflection, human experts
    are hired to generate the best-quality ground-truth demonstrations over existing
    human-AI efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Few-Shot Large Language Model with Material Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/97c7a018b9e04cce270e6543c8c8da88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Optimization of general-purpose LLMs for the MOFs synthesis extraction:
    few-shot in-context learning and RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot in-context learning with random examples
  prefs: []
  type: TYPE_NORMAL
- en: In the research area of natural language processing (NLP), few-shot in-context
    learning (FS-ICL) [[6](#bib.bib6)] generally refers to one typical learning paradigm
    to adapt the task-agnostic language models to various downstream tasks while achieving
    optimized performance on each task. In more detail, FS-ICL takes a few prompted
    examples as input (known as shots), each composed of a context and a labeled completion,
    in addition to background prompts such as task description (Figure [4](#S2.F4
    "Figure 4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")).
    In the task of MOFs synthesis extraction for instance, a context refers to a paragraph
    containing all the synthesis conditions of a MOF and the labeled completion refers
    to the ground-truth synthesis conditions annotated and curated by human experts
    in our work. The top-right part of Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Few-Shot
    Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations") gives an example of the labeled
    completion. FS-ICL is often discussed in comparison to the fine-tuning (FT) paradigm,
    which updates the pre-trained language models by incorporating a set of labeled
    examples via supervised learning. In both FS-ICL and FT, the final prediction
    is made by prompting a new context and asking the language model to complete it.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of FS-ICL vs. FT lies in its versatility to work on many
    tasks (e.g., synthesis extraction of various materials) without the need to re-train
    the model, as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations"). In comparison, FT requires gradient-based
    training for each new task to update the model weights and a considerable number
    of labeled examples for supervision. Though the latest technology of few-shot
    fine-tuning (FS-FT) has reduced the requirement of examples to the same level
    of FS-ICL [[11](#bib.bib11)][[15](#bib.bib15)], training and maintaining a small
    fraction of updated language model weights can still be costly for lightweight
    LLM usage scenarios such as synthesis extraction in this work. FT also suffers
    from spurious correlations due to the overfitting effect [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the superiority of FS-ICT in our scenario, the paradigm also draws concerns
    due to its disadvantages. First, the inclusion of few-shots in the prompt brings
    additional computation cost to the language model. In mainstream implementations,
    the number of shots, denoted as $K$ (known as zero-shot). Second, in previous
    studies, the format of prompts in FS-ICT (e.g., the wording and ordering of examples)
    can have unpredictable influence on the final performance. In some cases, FS-ICL
    even performs well on incorrect examples. We have investigate these issues and
    demonstrate that our approach can achieve very low variance by fixing the prompt
    format and example orders according to the algorithm. The data quality of few-shot
    examples in our task is also shown to be an important factor to the synthesis
    extraction performance.
  prefs: []
  type: TYPE_NORMAL
- en: It is also previously believed that FT can achieve better performance than FS-ICT,
    but the latest study reveals that under the same size of shots, both paradigms
    obtain similar performance and exhibit large variance depending on the task specification
    [[15](#bib.bib15)]. In our scenario, FS-ICT reaches an excellent performance of
    F1<math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="></math>0.9, which is
    enough for real-life deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Using RAG to enhance few-shot data quantity and quality
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b1dc0927be3fb4156cd781f85cd6e2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of different RAG algorithms and their configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24652863105bf03a22f07051a1f93c37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The impact of example data quality on extraction performance, with
    varying number of shots.'
  prefs: []
  type: TYPE_NORMAL
- en: At the core of FS-ICL approach, we introduce the RAG algorithm which retrieves
    the aforementioned $K$.001 in F1 comparison), showcasing the effectiveness of
    RAG mechanism. On the best BM25 algorithm, we further test the impact of few shots’
    input order as in the LLM prompt. As shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.2
    Few-Shot Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(c), the differences
    are not significant among most ordering strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Here we note that the F1 and ACC (overall accuracy) metrics used follow the
    standard definition computed from TP (true positive), FP (false positive), TN
    (true negative), FN (false negative), throughout this work. The LLM output on
    each synthesis condition of a MOF will be classified into one of TP/FP/TN/FN by
    comparing with the predefined ground-truth annotation, as described in the confusion
    matrix of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(c). Note that our definition
    is different from the previous research in Zheng et al. [[24](#bib.bib24)] where
    the TP/FP/TN/FN classification is evaluated by human experts case by case. We
    argue that the subjective human evaluation may introduce bias while the fully
    objective classification will ensure a consistent format in retrieved synthesis
    conditions, which is beneficial for the follow-up material applications (see Sec.
    [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations") for more details).
  prefs: []
  type: TYPE_NORMAL
- en: A key parameter of the RAG algorithm lies in the number of example shots used
    in the LLM prompt, i.e., $K$ increases. However, its performance metrics are consistently
    below the BM25 algorithm, mostly having gaps more than 0.05 on F1\. This result
    again demonstrates the effectiveness of the proposed RAG algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Material knowledge augmentation via prompt engineering
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to few-shot examples, another way to augment the domain knowledge
    of general-purpose LLM is through the fixed background prompt [[9](#bib.bib9)].
    The previous LLM adaptations on MOFs synthesis extraction by Zheng et al. [[24](#bib.bib24)]
    introduce a preliminary prompt engineering approach, which include the task description
    of MOFs synthesis extraction and the output format specification. In our work,
    based on the latest prompt engineering expertise [[21](#bib.bib21)], we propose
    to further incorporate two types of material knowledge into the background prompt:
    definition of each MOFs synthesis condition, and deterministic constraints on
    each condition’s numerical/textual value or structure (if any). As shown in Figure
    [5](#S2.F5 "Figure 5 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge
    ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(d),
    by integrating the new material knowledge, the F1 metric increases from 0.91 to
    0.93\. However, when the few-shot examples are not incorporated, the background
    material knowledge will not lead to significant improvement by itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The details of newly introduced MOFs synthesis definitions and constraints
    as background prompts are listed in Table [1](#S2.T1 "Table 1 ‣ 2.2 Few-Shot Large
    Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations"). Notably, we summarize three
    types of constraints on synthesis conditions: *numerical* that the value of a
    condition should fall into certain range according to prior knowledge, *textual*
    that an extracted condition by text should adhere to certain format to speedup
    follow-up material application, and *structural* that certain rules related to
    the condition are followed in all MOFs synthesis process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conditions Definition Constraints by Type Metal Precursor (name & amount) The
    precursor compound(s) containing metal ions … Textual: only include adjectives
    modifying the metal precursor itself… Organic Linker (name & amount) The organic
    precursor linking metal ions or clusters … N/A Solvent (name & amount) The liquid
    medium in which reactants are dissolved … Textual: include “solution” if the solvent
    contains water … Modulator (name & amount) The substance to adjust reaction conditions
    (e.g., pH value) … Structural: the elements of modulator will not become part
    of the backbone of MOF structure … Reaction process (duration & temperature) The
    synthesis process producing MOFs … Numerical: The reaction duration will last
    several minutes to hours …'
  prefs: []
  type: TYPE_NORMAL
- en: 'Structural: Crystallization is not a reaction process …'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Material knowledge as background prompts: synthesis condition definition
    and numerical/textual/structural constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Optimization for High-Throughput MOFs Synthesis Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the proposed LLM-based synthesis extraction method achieves state-of-the-art
    performance in our medium-scale validation set, scalability issues arise when
    the method is deployed to high-throughput scenarios involving thousands of real-world
    literature and millions of material texts. The challenges include but not limited
    to the large bill from calling commercial LLM APIs, the high cost to annotate
    enough examples for few-shot learning, and the pragmatic issues in material application.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis paragraph detection
  prefs: []
  type: TYPE_NORMAL
- en: To train the machine learning model for synthesis paragraph detection, we first
    annotate a dataset of 440 papers randomly sampled from the large dataset of Sec.
    [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations"). Details can be accessed in Sec. [5.2](#S5.SS2
    "5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣
    5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
    Finally, this process yields 1,349 synthesis paragraphs as positive samples. To
    train the classifier, negative samples by non-synthesis paragraphs are obtained
    after removing all annotated paragraphs from a paper, leading to 11,783 negative
    samples. We employed the standard BERT model, specifically the pre-trained bert-base-uncased
    model from HuggingFace, for training. The training and validation processes utilized
    a 5-fold cross-validation method. Given the imbalance dataset, we used stratified
    5-fold cross-validation to ensure that the ratio of positive to negative samples
    remained consistent in each split. The final classification performance is quite
    high, with an ACC of 0.989, precision of 0.955, recall of 0.947, and F1 = 0.951.
  prefs: []
  type: TYPE_NORMAL
- en: Paragraphs related to the synthesis process constitute only about 6% of an article’s
    total length but concentrate the main synthesis condition. Extracting condition
    from synthesized paragraphs, rather than the entire text, can significantly reduce
    the overhead of LLM-based approach and increase the density of synthesis conditions
    in text, thereby enhancing extraction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Sizing the few-shot example pool
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bba32a80b47198b8000db09213c09016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Synthesis extraction performance with varying sizes of example pool:
    (a) average F1 and its 95% CI of 123-paragraph and 60-paragraph datasets; (b)
    123-paragraph dataset with different $K$-shots.'
  prefs: []
  type: TYPE_NORMAL
- en: In this study, we have discussed both the quantity and quality of few-shot examples.
    Yet, it is still unknown how many ground-truth annotations, namely the example
    pool where few-shots are selected from, are required for high-throughput synthesis
    extraction over thousands of MOFs literature or more. To answer this question,
    we design an experiment that assumes the entire dataset to be 123 synthesis paragraphs
    (all with ground-truth synthesis conditions known), and the example pool size
    (# of annotations) to increase from zero. The example pool in each setting is
    randomly chosen from the entire dataset. To alleviate the uncertainty from randomness,
    we give 5 trials on each example pool size setting. Also, to further understand
    the scalability of example pool sizing, we create a new dataset with 60 synthesis
    paragraphs from the entire data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S2.F7 "Figure 7 ‣ 2.3 Optimization for High-Throughput MOFs Synthesis
    Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(a) illustrates the result on the effect of example pool size.
    First, the first few annotations (from 0 to 5 in the figure) contribute the most
    performance gain, regardless of the size of entire dataset. This is coherent with
    the effect observed on zero-shot learning vs. one-shot. Second, smaller datasets
    (i.e., 60-paragraph by the green line) require fewer # of annotations than larger
    datasets (i.e., 123-paragraph by the red line), while achieving the same level
    of performance. The green line stays above the red line, especially under smaller
    example pool size. Third, more annotations will almost always bring performance
    gains and less uncertainty, though most boosts happen at initial few annotations.
    On the 60-paragraph data, the performance peak (F1=0.92) appears at the pool size
    of 40, 66.7% of the data size; on the 123-paragraph data, the peak (F1=0.93) does
    not happen before the pool size of 65, thus at least larger than 52.8% of the
    data size. A future work would be studying the active learning mechanism, which
    may help to reduce the required example pool in few-shot learning of LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [7](#S2.F7 "Figure 7 ‣ 2.3 Optimization for High-Throughput MOFs Synthesis
    Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(b) further demonstrates the effect of both example pool size
    and $K$-shots. As shown in the figure, when the labeled pool size increases from
    0 to 5, the performance metrics improve rapidly, indicating that a labeled dataset
    is much more effective than an unlabeled one. Subsequently, the performance metrics
    increase slowly until the labeled pool size reaches the range of 40-55\. The performance
    metrics then stabilize, with the F1 score fluctuating slightly around 0.91 and
    ACC around 0.88.
  prefs: []
  type: TYPE_NORMAL
- en: Coreference Resolution
  prefs: []
  type: TYPE_NORMAL
- en: For convenience of writing, proxy words like “L” or “H2L” are frequently used
    in the MOFs literature to represent specific organic linkers, which are called
    coreference in NLP. In all the extracted synthesis conditions from 5269 paragraphs,
    578 coreference cases are identified. These proxy words could refer to substances
    defined far in the same article, which makes it difficult to use the extraction
    results in downstream material application.
  prefs: []
  type: TYPE_NORMAL
- en: Due to different writing styles, regular expression can not be employed as the
    sole method to resolve the coreference of these proxy words. We introduce a hybrid
    method combining LLM and regular expression for coreference resolution. The resolving
    of proxy word coreference is done in three steps. First, the synthesis paragraph
    is located in the literature and all the text before the paragraph is input to
    LLM. The LLM is asked to extract all anaphoric references and the original words.
    Second, a regular expression is designed to identify coreference proxy words from
    all the extracted conditions by LLM. Finally, these proxy words in the synthesis
    condition are matched with the detected anaphoric reference. If a match exist,
    the proxy word is resolved into the original words discovered by LLM in the second
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, in all the 578 organic linker conditions using coreference, 79% of
    them can be resolved by our method. Only 0.023 linkers per paragraph remain unresolved.
    As shown in Table [2](#S2.T2 "Table 2 ‣ 2.3 Optimization for High-Throughput MOFs
    Synthesis Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations"), The five most appearing coreference words are
    “L”, “H2L”, “HL”, “L1”, and “H4L”, with all the resolution rates over 85%.
  prefs: []
  type: TYPE_NORMAL
- en: '| Proxy Words | Occurrence Count | Resolution Count | Resolution Ratio |'
  prefs: []
  type: TYPE_TB
- en: '| L | 106 | 92 | 86.8% |'
  prefs: []
  type: TYPE_TB
- en: '| H2L | 64 | 58 | 90.6% |'
  prefs: []
  type: TYPE_TB
- en: '| HL | 45 | 45 | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| L1 | 39 | 37 | 94.9% |'
  prefs: []
  type: TYPE_TB
- en: '| H4L | 38 | 33 | 86.8% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Five most frequently used proxy words and their resolution results.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 MOFs Structure Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To better validate the accuracy and potential of few-shot synthesis extraction
    method in downstream tasks, we set up a real-world MOFs synthesis-structure inference
    task and compared it with existing benchmark methods (zero-shot LLM). The specific
    task is to predict the microscopic property of MOFs: global cavity diameter, pore
    limiting diameter, largest cavity diameter, and framework density, using the synthesis
    conditions including metals, organic links, solvents, and reaction duration/temperature.
    We evaluate the task performance using coefficient of determination ($R^{2}$ metric
    effectively quantifies a model’s explanatory power regarding the actual data variation
    and the model accuracy. Therefore, it can be used to reflect the impact of different
    synthesis conditions on MOFs microstructure.'
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation data is a subset of the CSD database [[14](#bib.bib14)], which
    encompasses 5269 MOFs. As detailed in Sec. [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    these MOFs are carefully selected so that each MOF is described by only one scientific
    literature and the literature will only have one synthesis paragraph. The resulting
    dataset ensures the validity of evaluation by exact correspondence between a MOF’s
    microscopic structure and its extracted synthesis conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Using the few-shot/zero-shot LLMs and other benchmark methods, the 10 synthesis
    conditions under study are extracted from a unique synthesis paragraph linked
    to each of the 5269 MOFs. The raw textual conditions extracted are post-processed
    to improve data quality, such as synonym merging and standardization of temperature/time
    scales (Sec. [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")).
    On the LLM output by the few-shot method, the top 100, 135, and 20 precursor names
    of metals, linkers, and solvents are selected, which leads to a smaller dataset
    of 800 MOFs. On the LLM by zero-shot method, the distribution of conditions are
    less longer-tailed, so that a stricter filter is applied to obtain the same number
    of 800 MOFs. These precursor names are embedded into one length-198 feature vector
    by the methods in Sec. [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    where serves as the input features in the material inference task. The target
    outcome variables are the four microstructure property of a MOF. Their calculation
    procedure is described in Sec. [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
  prefs: []
  type: TYPE_NORMAL
- en: Model Zero-shot R² Few-shot R² Lasso 0.1755 0.2257 Bayesian Ridge 0.1758 0.2318
    AdaBoost 0.2570 0.3298 Random Forest 0.2498 0.3468 Gradient Boosting 0.2919 0.3632
    XGBoost 0.3559 0.4421
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance comparison of few-shot and zero-shot LLMs across different
    machine learning models on the inference MOFs framework density.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply six machine learning models for the inference: Lasso Regression, Bayesian
    Ridge Regression, AdaBoost, Random Forest, Gradient Boosting Regression, and Extreme
    Gradient Boosting (XGBoost). The first five models do not support missing value
    as input, so we use mean imputation instead. With each model, we compare the two
    LLM-based method, few-shot learning vs. zero-shot learning, in a 10-fold cross-validation.
    On the four microstructure properties inferred, the first three lead to negative
    or close to zero $R^{2}$ values larger than 0.2\. This also validates the fact
    that MOFs density is highly correlated with the metal and organic precursors used
    in MOFs material synthesis, as well as the reaction duration and temperature.'
  prefs: []
  type: TYPE_NORMAL
- en: The $R^{2}$ of 0.3559 on the test set for the zero-shot method. We illustrate
    the inference result by XGBoost on the scatterplot of Figure [8](#S2.F8 "Figure
    8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(a). It shows that the actual vs. predicted
    distribution of the few-shot method (green dots) preserves higher affinity to
    the optimal prediction line (red dashed line), than the predictions by zero-shot
    method (blue dots). The result demonstrates that the proposed few-shot method
    not only extracts more accurate synthesis conditions in comparison to the baseline,
    but also significantly improves the performance of downstream material inference
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/333b8395199ee23a4704ce9338fae51e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Performance on MOFs structure inference task: (a) predictive power
    of the best XGBoost model, few-shot vs. zero-shot; (b) comparison of $R^{2}$ values
    and data counts across different data filters.'
  prefs: []
  type: TYPE_NORMAL
- en: To further showcase the superiority of the few-shot method, we conducted more
    trials using the best-performing model, XGBoost. We gradually reduce the test
    dataset into more densely distributed synthesis conditions, by enforcing stricter
    data filters and selecting only higher-ranked synthesis condition values. The
    XGBoost model is tuned with the best hyperparameters on each dataset following
    the method by Akiba et al. [[4](#bib.bib4)]. As shown in Figure [8](#S2.F8 "Figure
    8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(b), when the test dataset increases
    with more conditions, the $R^{2}$ of zero-shot method stays stable or rises much
    slowly. The gap between the two methods widens as the dataset includes more unique
    conditions. Meanwhile, the data size by the number of MOFs used remains comparable
    between the two methods in every setting, as indicated by the grouped bar charts
    in Figure [8](#S2.F8 "Figure 8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b). The result
    indicates that considering less frequently appearing synthesis conditions will
    significantly improve the accuracy of material structure inference when the few-shot
    method is applied. In contrast, the zero-shot method showed a steady trend in
    predictive performance. This also reveals the superior performance of the few-shot
    method in downstream material inference task compared to the zero-shot method.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Synthesis paragraph detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train a machine learning model for binary classification to determine whether
    a paragraph is synthesized, we randomly obtained 440 papers from the database
    in Appendix A for annotation. Each paper was annotated by two different annotators
    to ensure inner annotator agreement. The 880 annotation tasks were assigned to
    four annotators who used our platform, shown in Figure [9](#S5.F9 "Figure 9 ‣
    5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣ 5
    Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    to annotate synthesis-related paragraphs. After annotation, only paragraphs annotated
    by both annotators were considered valid, while paragraphs annotated by only one
    annotator were discarded. If there was an overlap in the positions of the paragraphs
    annotated by the two annotators, we found that mismatched paragraphs often occurred
    because one annotator noted more synthesis parameters and thus marked a larger
    range. In such cases, the larger annotated paragraph was considered valid. This
    method also resolved minor annotation deviations within a few characters, allowing
    two slightly different synthesis paragraphs to be considered valid. This process
    yielded 1,349 valid annotated paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the discrimination model, non-synthesis paragraphs were needed as
    negative samples. After removing all annotated paragraphs from a paper, the remaining
    paragraphs served as negative samples. This method resulted in 11,783 negative
    samples. We employed the standard BERT model, specifically the pre-trained bert-base-uncased
    model from HuggingFace, for training. The training and validation processes utilized
    a 5-fold cross-validation method ($k=5$). Given the imbalance dataset, we used
    stratified k-fold cross-validation to ensure that the ratio of positive to negative
    samples remained consistent in each split. After training and cross-validation
    testing, the model’s evaluation metrics were as follows: Accuracy = 0.989, Precision
    = 0.955, Recall = 0.947, and F1 Score = 0.951\. The trained model achieved high
    overall accuracy and can be used for synthesized paragraph classification in extracted
    paragraphs, which will facilitate subsequent chemical named entity recognition
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of applying the model to our MOFs dataset in Appendix A are as
    follows: According to statistics, the dataset contains a total of 36,233 DOIs,
    corresponding to 78,741 MOF-IDs. Among these, 21,031 DOIs can be used to extract
    synthesis paragraphs, and 22,461 DOIs remain one DOI corresponding to one MOF-ID.
    The intersection of these two sets contains 9,855 DOIs. Further filtering for
    DOIs that contain only one synthesis paragraph results in 5,269 DOIs. In other
    words, we extracted 5,269 valid synthesis paragraphs from the complete dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Paragraphs related to the synthesis process constitute only about 2% of an article’s
    total length but concentrate the main synthesis condition. Extracting condition
    from synthesized paragraphs, rather than the entire text, can significantly reduce
    the model’s extraction overhead and increase the density of field distribution,
    thereby enhancing data quality.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Few-Shot RAG Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To maximize the extraction performance of the model, we provide examples of
    extraction by human-AI annotation as demonstrations. By using a retrieve $K$ similar
    demonstrations are obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Top-K}=\text{sort}({(\text{score}(p,d_{i}),d_{i})}^{n}_{i=1})[:k]$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, the score is used to estimate the similarity between the embeddings of
    document $d_{i}$. The embedding models can be categorized into traditional sparse
    vector encoders (e.g., TF-IDF, BM25 [[18](#bib.bib18)]) and semantic dense vector
    encoders (e.g., SBERT [[17](#bib.bib17), [8](#bib.bib8)]) [[10](#bib.bib10)].
    In our experiments, we compared these two classes of retrieval methods and selected
    the one that performed best as the final approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the traditional sparse vector retrieval method, we use the BM25 algorithm.
    BM25 is a probabilistic information retrieval model that ranks documents based
    on the frequency of query terms within the documents. It balances term frequency
    (how often a term appears in a document) with inverse document frequency (how
    rare a term is across the entire document set), thus giving more weight to terms
    that are significant. The scoring function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{avg\_dl}=\frac{1}{N}\sum_{j=1}^{N}&#124;d_{j}&#124;$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $f(p_{i},d)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the semantic information-based method, we use the embedding vector representation
    of the text obtained from a pre-trained language model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Score}(p,d)=\frac{f(p)\cdot f(d)}{&#124;f(p)&#124;&#124;f(d)&#124;}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $f(x)=\text{PLM}(x)$ most relevant paragraphs-extraction pairs obtained
    in the previous step before the input as few-shot demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work studies the new paradigm of applying few-shot in-context learning
    to the popular approach of LLM literature extraction for discovering MOFs synthesis
    conditions. It is shown through experiments that both the quality and the quantity
    of few-shot demonstrations are important in the studied scenario. We introduce
    both a novel process of human-AI joint data curation to enhance few-shot demonstration
    quality and a calibrated BM-25 RAG algorithm to size the optimal few-shot quantity.
    Scalability issues regarding high-throughput MOFs synthesis condition extraction
    are resolved using many practical methods such as offline synthesis paragraph
    detection and LLM-based coreference resolution. Our proposal is thoroughly evaluated
    using large-scale real-life MOFs dataset, on both text extraction performance
    for synthesis condition discovery and the downstream material task on structural
    property inference.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 MOFs Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CSD and the retrieved dataset
  prefs: []
  type: TYPE_NORMAL
- en: We base our work on the MOF subset of Cambridge Structural Database (CSD) [[14](#bib.bib14)]
    retrieved in June 2022, which lists 84,898 MOFs covering the bonding motifs of
    all common MOFs in CSD. The entry of a MOF in the database contains its structure
    in CIF format, the physical properties, a DOI linking to the relevant publication,
    and a unique MOF ID.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is then pre-processed according to the goal of this work. First,
    the full-text describing the MOFs under study should be available. Out of all
    the 84,898 MOFs, 78,741 has non-empty DOIs. Since the same DOI could be linked
    to multiple MOFs (one paper reporting more than one MOFs), there leaves 39,579
    different DOI links after deduplication and 36,177 downloadable paper full-text.
    For the convenience of follow-up processing, we focus on the DOIs where the associated
    publication reports the information of only one MOF in CSD. This leads to a subset
    of 22,461 MOFs, each with a unique publication file in PDF format.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the PDF of each MOF is converted to plain text [[20](#bib.bib20)] and
    segmented into paragraphs. The high performance classification model in Sec. [3.1](#S3.SS1
    "3.1 Synthesis paragraph detection ‣ 3 Methods ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations") is applied to detect synthesis paragraphs
    enclosing the desired synthesis condition information. Again, for the sake of
    convenience and accuracy, we only consider the 5,269 MOFs/publications that contain
    exactly one synthesis paragraph. Another 12,606 publications do not have any synthesis
    paragraph, probably because these papers are not related to MOFs experiments.
    The other 4,586 publications have more than one synthesis paragraphs, as they
    are describing multiple MOFs or synthesis routes. Our pipeline could work with
    papers having more than one suite of synthesis conditions, but the potential MOF-synthesis
    mismatch may downgrade the application performance in evaluation. Therefore, throughout
    this work we stick to the core dataset of 5,269 MOFs/publications and their unique
    synthesis paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Microstructure Property Computation
  prefs: []
  type: TYPE_NORMAL
- en: 'For material evaluation purpose, we also calculate structural and physical
    properties of the 5,269 MOFs under consideration. The CIF file of each MOF is
    retrieved from CSD and input to the Zeo++ tool [[22](#bib.bib22)]. In total, four
    structural and physical properties are calculated: global cavity diameter, pore
    limiting diameter, largest cavity diameter, and framework density. We set the
    probe radius to 1.29A to simulate helium gas molecules, and the number of Monte
    Carlo samples to 100,000 to ensure the accuracy of calculations. All Zeo++ parameters
    adhere to standard routines, guaranteeing that the computed properties accurately
    represent the behavior of gas molecules within the MOF structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c957b7e998bf7159e267e2f3aa419146.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: User interface of the annotation platform.'
  prefs: []
  type: TYPE_NORMAL
- en: High-quality annotations are the cornerstone of few-shot in-context learning;
    only accurate and highly coherence annotations can improve the precision of extracting.
    Therefore, we enlisted the help of eight experts in materials science and engineering
    to assist with the annotations. Additionally, we developed a batch interactive
    annotation platform to enhance the convenience of the annotation process. During
    the annotation process, we discovered that the task was challenging and had a
    high error rate done by human only, which led to poor model extraction performance
    when using erroneously annotated examples. Consequently, we implemented a comprehensive
    annotation process to improve quality.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis Paragraph Annotation
  prefs: []
  type: TYPE_NORMAL
- en: To annotate synthesis paragraphs for offline machine learning, 440 papers were
    randomly obtained from the database in Appendix A. For inner annotator agreement,
    each paper was annotated by two different annotators. The 880 annotation tasks
    were assigned to four annotators, who used our platform shown in Figure [9](#S5.F9
    "Figure 9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")
    to annotate synthesis-related paragraphs. After annotation, only paragraphs annotated
    by both annotators were considered valid, and paragraphs annotated by only one
    annotator were discarded. If there was an overlap in the positions of the paragraphs
    annotated by the two annotators, we found through checking the annotated data
    that the common mismatched paragraphs often occurred because one annotator noted
    more synthesis conditons and thus marked a larger range for the synthesis paragraph.
    In such cases, the paragraph should also be considered valid. Therefore, we treated
    the larger annotated paragraph as a valid synthesis paragraph. This method also
    resolved the issue of minor annotation deviations within a few characters, allowing
    two slightly different synthesis paragraphs to be considered valid. This process
    yielded 1,349 valid annotated paragraphs. To train the discrimination model, non-synthesis
    paragraphs are needed as negative samples. After removing all paragraphs annotated
    by annotators from a paper, the remaining paragraphs serve as negative samples.
    This method resulted in 11,783 negative samples used for training the synthesis
    paragraph discrimination model.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis Condition Annotation
  prefs: []
  type: TYPE_NORMAL
- en: 'We randomly selected 200 papers from the paper database constructed in [5.1](#S5.SS1
    "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using
    Few-Shot Demonstrations"), with each paper only contains less than 3 MOFs IDs.
    The annotation process can be divided into five sections: task configuration,
    GPT pre-extraction annotation, pilot annotation, batch annotation, and data curation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the task configuration stage, domain experts define the key synthesis condition
    to be annotated and configure the annotation settings. The core standard for annotation
    configuration is . Due to the diversity of expressions in material papers, consistent
    annotation methods help increase the density of the resulting data, thereby reducing
    the complexity of subsequent data cleaning and enhancing the effectiveness of
    using ML to infer material structure or properties. The selected annotation synthesis
    condition must: 1) be common in synthesis paragraphs of related papers, and 2)
    be beneficial for predicting performance condition. We exclude Active process
    including active temperature and active time after pilot annotation because we
    recognized its low frequency. Also, Molecular formula was excluded for it helps
    little in performance parameter prediction. Once the annotation requirements and
    background knowledge are set by domain experts, the pre-extraction annotation
    phase can begin.'
  prefs: []
  type: TYPE_NORMAL
- en: Before the pilot annotation, the GPT pre-extraction method can be used to preliminarily
    locate synthesis paragraphs and relevant condition, assisting experts in annotation.
    The synthesis paragraph discrimination model extracts relevant paragraphs from
    the papers. Using the annotation requirements and domain knowledge configured
    in the task configuration stage, zero-shot prompts are applied for pre-extraction
    to obtain initial extraction data, which is then imported into the annotation
    system. Although the accuracy of this process is limited, it helps locate paragraphs
    and reduces annotation difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pilot annotation:* twenty papers are randomly selected from the 200 candidate
    papers for pilot annotation to validate and adjust the annotation task settings
    and platform configuration. The annotation platform is shown in Figure [9](#S5.F9
    "Figure 9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
    Two annotators independently annotate the 20 papers on the platform. The results
    are used to check inner annotator agreement to ensure accuracy. This process requires
    annotators and researchers to analyze and discuss the following: 1) identify ambiguous
    and unclear parts of the annotation task configuration to clarify specific annotation
    methods, 2) reanalyze the synthesis condition to determine if some field are too
    sparse and need to be removed, or if some field are dense enough to be included
    as synthesis condition for predicting performance, and 3) identify any unreasonable
    designs in the annotation platform and make necessary modifications.'
  prefs: []
  type: TYPE_NORMAL
- en: This pilot annotation stage resulted in 20 valid papers. Annotators and researchers
    refined the annotation task configuration to maximize the quality of subsequent
    batch annotations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Batch annotations:* the remaining 180 papers will produce 360 annotation tasks,
    assigned to six annotators. Each annotator is randomly assigned 60 papers, ensuring
    each paper is annotated by two different annotators. GPT pre-extraction is also
    used to enhance annotation accuracy and efficiency. Upon completion, the annotation
    data undergo a simple inner annotator agreement check, using Jaccard similarity
    to verify the consistency between the two annotators’ results. For each annotation
    field, a validity threshold of 0.8 overlap between the annotators is required,
    then the result field was the union of two fields, which is better for subsequent
    data cleaning than intersection. For each paper, a verified annotation item overlap
    rate of 80% or higher between the two annotators is considered a valid annotation
    paper. Non valid papers were not used in follow-up steps and can be used as supplementary
    data after manual review.'
  prefs: []
  type: TYPE_NORMAL
- en: This stage resulted in 147 papers with high overlap rates, used in this experiment.
    53 papers are excluded for subsequent process.
  prefs: []
  type: TYPE_NORMAL
- en: '*Joint Human-AI Data Curation:* to improve annotation quality, we introduce
    a joint human-AI data curation process. Experts finalize the data annotations
    step by double-check the results from both LLM and human annotations. The LLM
    results, using BM25 few-shot extraction of synthesis condition, are compared with
    the annotated results to identify inconsistencies. This step helps detect problems
    in batch annotations and assists in identifying erroneous annotations. Invalid
    papers will be excluded. We detected and excluded chiral MOFs in annotated papers,
    with duplicate synthesis conditions and paragraph. Also, for better sampling,
    we excluded all paragraph with more than one MOF synthesis process. Although our
    resolution framework can handle multiple MOFs in a single paragraph, we chose
    one-to-one paragraphs for better samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, common LLM extraction errors by the model are identified, allowing
    for targeted constraint writing in prompts to improve knowledge-based corrections.
    Constraints must be presented in the form of knowledge provision and must not
    contain any examples to avoid overfitting. This human-AI data curation process
    identified 120 annotation errors and added 5 constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Post-processing of Synthesis Conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/718fc6f1af4e8427b609c4107ff7d477.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Frequencies of occurrence for MOFs synthesis conditions: (a) metal
    precursor; (b) organic linker; (c) solvent.'
  prefs: []
  type: TYPE_NORMAL
- en: The raw synthesis conditions extracted by LLM-based method often suffer from
    data quality issue, which potentially affects the downstream material inference
    task. We introduce several data postprocessing methods to improve the quality
    of derived synthesis conditions so that the input data to the inference model
    can be more formatted and densely distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Data Cleansing on Textual Conditions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The synthesis conditions include discrete names for Metal, Organic Linker, Solvent,
    and additives. These names often have different representations for the same substance
    (e.g., ”H2O” and ”Water” both represent water, ”Cd(NO3)2.4H2O” and ”Cd(NO3)2?4H2O”
    both represent cadmium nitrate tetrahydrate). Using unprocessed discrete names
    increases redundancy and noise in the dataset, complicating the embedding process
    and affecting the consistency and performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity Disambiguation
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we use similarity disambiguation to create an initial list of assimilated
    names, eliminating some ambiguities caused by inconsistent spelling, based on
    the Levenshtein distance (edit distance). The specific steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Levenshtein distance between two strings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Normalize the similarity score by converting the Levenshtein distance to a
    score between 0 and 100 using the formula:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{Similarity\_ratio}=\left(1-\frac{\text{Levenshtein distance}}{\text{maximum
    length of the two strings}}\right)\times 100$ |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a threshold. We set the threshold at 90 to filter out most unrelated string
    pairs while ensuring that only truly similar strings are identified as the same
    object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Synonym Merging Using GPT-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use GPT-4 for synonym merging. This method leverages the powerful
    capabilities of the GPT-4 model to successfully identify and group different names
    representing the same substance. The system also includes a reflection mechanism
    to ensure the accuracy of the classifications. The detailed workflow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the input text to prepare the chemical substance names.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a predefined prompt (PROMPT1) to ask the GPT-4 model to classify the chemical
    substances and group identical substances. The model returns a JSON array, each
    item being a list of synonymous substances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a reflection prompt (REFLECT_PROMPT1) to re-evaluate the initial classification
    results, ensuring classification accuracy. This step checks if the substances
    within each group belong together and if two groups represent the same substance.
    Finally, output the final classification results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This method is suitable for synonym merging tasks in materials chemistry, such
    as Metal Source, Organic Linker, and Solvent. We merged data with frequencies
    of 8/4/5 and above for Metal Source, Organic Linker, and Solvent, respectively,
    instead of merging all data. This reduces potential errors from merging low-frequency
    data and ensures fairness in subsequent comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Standardization of Numeric Conditions on Time and Temperature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After processing the discrete names in the synthesis conditions, we continued
    to parse and capture numerical data for time and temperature. These data may have
    quality issues such as inconsistent units and the presence of special characters.
    To address these issues, we performed the following normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and Formatting Data
  prefs: []
  type: TYPE_NORMAL
- en: Using GPT-4, we extracted and formatted relevant data for time and temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Unit Standardization
  prefs: []
  type: TYPE_NORMAL
- en: We defined standard units for each data type. For example, time was standardized
    to hours, and temperature was standardized to Celsius (room temperature set at
    25^∘C).
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning Special Characters.
  prefs: []
  type: TYPE_NORMAL
- en: Using regular expressions, we cleaned and formatted data that might contain
    special characters (such as spaces, commas, etc.). Through these steps, we ensured
    the integrity and usability of the data, laying a solid foundation for subsequent
    processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Data Filtering by Synthesis Condition Distributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After data cleansing and standardization, the distribution of different synthesis
    conditions becomes more centralized. As shown in Figure [10](#S5.F10 "Figure 10
    ‣ 5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations"), the entity lists of both
    metal source and solvent are shortened. The number of unique organic linkers remains
    high due to its long-tailed distribution. In the application of MOFs microstructure
    property inference, we will only select these MOFs synthesized by top entities
    in metal source, organic linker, and solvent. For example, by default we apply
    a filter of (100, 135, 20), which select the MOFs having top-100 metal source
    in the ranked list of Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Post-processing of
    Synthesis Conditions ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(a), top-135 organic linker, and top-20 solvent.
    Note that for LLM models in comparison, different filters may be applied to ensure
    the same number of MOFs in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Feature Embedding for Metal, Organic Linker, and Solvent Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After disambiguation and merging, we obtained high-quality precursor/solvent
    data. To build accurate predictive models, we need to perform corresponding feature
    embedding to capture the material/structural characteristics of the precursor/solvent
    data. The specific steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining Chemical Formulas and SMILES
  prefs: []
  type: TYPE_NORMAL
- en: Using GPT-4, we obtained the chemical formulas and SMILES for the top 100 Metals
    and the top 20 Solvents after disambiguation and merging. For Organic Linkers,
    due to the complexity of their naming, GPT-4 could not accurately obtain the corresponding
    SMILES. Therefore, we manually collected the SMILES for the top 135 Organic Linkers
    after disambiguation and merging.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Molecular Features
  prefs: []
  type: TYPE_NORMAL
- en: Based on the obtained SMILES, we used RDKit to calculate the molecular features
    of Metals, Organic Linkers, and Solvents, including molecular weight, LogP values,
    the number of hydrogen bond donors and acceptors, Labute surface area, maximum
    molecular distance, molecular length, width, height, and topological polar surface
    area (TPSA).
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Metal Salt Features
  prefs: []
  type: TYPE_NORMAL
- en: Using the Composition class from Pymatgen, we automatically inferred and assigned
    oxidation states for the chemical formulas of metal salts. Using the MultipleFeaturizer
    class from the Matminer library, we calculated a series of chemical features,
    including elemental properties, atomic orbitals, electron affinity, and electronegativity
    differences. Additionally, we included features of the metal elements contained
    in the MOFs, such as atomic mass, atomic radius, thermal conductivity, and detailed
    electronic configuration vector representations. These features provide more comprehensive
    elemental property information for in-depth analysis of the performance and behavior
    of MOFs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Database and Engine
  prefs: []
  type: TYPE_NORMAL
- en: 'To streamline the entire workflow and efficiently organize the extraction results
    from related papers, we developed the Visual MOFs Synthesis Extraction Engine
    and Database. Using our approach, we processed over 30,000 papers and extracted
    57,081 synthesis paragraphs, on which we then performed synthesis condition extraction.
    To better view and analyze the vast amount of extraction results, we built a comprehensive
    database with 2 features: 1) Basic Statistics: The database provides basic statistics
    on all extraction results, including data on synthesis paragraphs and various
    synthesis conditions (Figure [11](#S5.F11 "Figure 11 ‣ 5.4 Visual MOFs Synthesis
    Condition Extraction Engine and Database ‣ 5 Appendix ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")). 2) Advanced Search Capabilities:
    This database is designed to support logical expression searches for specific
    fields, allowing users to search for synthesis conditions, paper titles, and synthesis
    paragraph content with precision, and enables visualization of the retrieval results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd809d8db1f3a9c3c9912f2b5a4caea2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Statistics on the database containing all the extraction results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An entire process is integrated, from uploading synthesis papers, format conversion,
    paragraph and condition extraction, to the visualization of extraction results:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload and Standardization: Users can upload synthesis papers, which are then
    automatically converted into a standardized format suitable for condition extraction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Automatic Paragraph Extraction: The system will automatically extract synthesis
    paragraphs from the uploaded papers for users to select the paragraphs to process
    and proceed with synthesis condition extraction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configurable Extraction: The engine supports configuration for synthesis condition
    extraction, allowing users to adjust the sample quantity and selection method
    input into the large model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Organized and Visualized Data: The extracted conditions are systematically
    organized and visualized for data interpretation and analysis.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Synthesis Visualization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87e7c83a887b90e53d07a39386b124dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Visualization interface for illustrating the synthesis extraction
    process and result.'
  prefs: []
  type: TYPE_NORMAL
- en: The visualization system we designed can support users in analyzing synthesis
    paragraphs. Initially, users upload batch PDF papers and process through the LLM.
    Once extraction is complete, users can utilize the filtering panel to select specific
    paragraphs for analysis. The overall performance panel (Fig.[12](#S5.F12 "Figure
    12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(a))
    then displays four key performance metrics of the LLM resolution, with a default
    HeatMap (Fig.[12](#S5.F12 "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction
    Engine and Database ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(b).I) providing a detailed view of entity resolution
    performance across all evaluation metrics. Suppose further detail on specific
    metrics is needed. In that case, users can access the second tab (Fig.[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).II),
    sliding down to the relevant rows to view the distribution of paragraph performance
    across various parameters in bar charts. To explore similarities with other paragraphs
    in the database, users can switch to the third tab (Fig.[12](#S5.F12 "Figure 12
    ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).III).
    Here, red dots indicate newly extracted paragraphs; users can look for nearby
    black dots representing similar paragraphs in the database to compare specific
    composite parameters. Should users decide to replace or re-examine certain paragraphs,
    they can reselect them in the filtering panel (Fig.[12](#S5.F12 "Figure 12 ‣ 5.4
    Visual MOFs Synthesis Condition Extraction Engine and Database ‣ 5 Appendix ‣
    LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(c)).
    This action triggers an automatic update of the corresponding performance metrics
    and visual charts, allowing users to repeat the analysis as needed.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] GPT-4, OpenAI. [https://openai.com/index/gpt-4/](https://openai.com/index/gpt-4/).
    Retrieved on 2024-01-25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] What is a MOF, MOF Commission of the International Zeolite Association.
    [https://www.iza-online.org/MOF/MOFforIZA.pdf](https://www.iza-online.org/MOF/MOFforIZA.pdf).
    Retrieved on 2024-07-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
    arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation
    hyperparameter optimization framework. In The 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 2623–2631, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. H. Alawadhi, S. Chheda, G. D. Stroscio, Z. Rong, D. Kurandina, H. L.
    Nguyen, N. Rampal, Z. Zheng, L. Gagliardi, and O. M. Yaghi. Harvesting water from
    air with high-capacity, stable furan-based metal–organic frameworks. Journal of
    the American Chemical Society, 146(3):2160–2166, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
    Advances in neural information processing systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Dagdelen, A. Dunn, S. Lee, N. Walker, A. S. Rosen, G. Ceder, K. A. Persson,
    and A. Jain. Structured information extraction from scientific text with large
    language models. Nature Communications, 15(1):1418, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of
    deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui.
    A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang.
    Retrieval-augmented generation for large language models: A survey. arXiv preprint
    arXiv:2312.10997, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel.
    Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
    learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes
    good in-context examples for gpt-$3$? arXiv preprint arXiv:2101.06804, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Luo, S. Bag, O. Zaremba, A. Cierpka, J. Andreo, S. Wuttke, P. Friederich,
    and M. Tsotsalas. Mof synthesis prediction enabled by automatic data mining and
    machine learning. Angewandte Chemie International Edition, 61(19):e202200242,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] P. Z. Moghadam, A. Li, S. B. Wiggin, A. Tao, A. G. Maloney, P. A. Wood,
    S. C. Ward, and D. Fairen-Jimenez. Development of a cambridge structural database
    subset: a collection of metal–organic frameworks for past, present, and future.
    Chemistry of Materials, 29(7):2618–2625, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot
    fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint
    arXiv:2305.16938, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. P. Polak and D. Morgan. Extracting accurate materials data from research
    papers with conversational language models and prompt engineering. Nature Communications,
    15(1):1569, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese
    bert-networks. arXiv preprint arXiv:1908.10084, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Robertson, H. Zaragoza, et al. The probabilistic relevance framework:
    Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf,
    L. Zettlemoyer, N. A. Smith, et al. Selective annotation makes language models
    better few-shot learners. arXiv preprint arXiv:2209.01975, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] H. Tian, W. Liu, and other contributors. pdf2htmlex. [https://github.com/pdf2htmlEX/pdf2htmlEX](https://github.com/pdf2htmlEX/pdf2htmlEX),
    2024. Accessed: 2024-07-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar,
    J. Spencer-Smith, and D. C. Schmidt. A prompt pattern catalog to enhance prompt
    engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] T. F. Willems, C. H. Rycroft, M. Kazi, J. C. Meza, and M. Haranczyk. Algorithms
    and tools for high-throughput geometry-based analysis of crystalline porous materials.
    Microporous and Mesoporous Materials, 149(1):134–141, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] O. M. Yaghi, M. O’Keeffe, N. W. Ockwig, H. K. Chae, M. Eddaoudi, and J. Kim.
    Reticular synthesis and the design of new materials. Nature, 423(6941):705–714,
    2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Z. Zheng, O. Zhang, C. Borgs, J. T. Chayes, and O. M. Yaghi. Chatgpt chemistry
    assistant for text mining and the prediction of mof synthesis. Journal of the
    American Chemical Society, 145(32):18048–18062, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
