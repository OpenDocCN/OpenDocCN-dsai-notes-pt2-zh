- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17957](https://ar5iv.labs.arxiv.org/html/2406.17957)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \interspeechcameraready\name
  prefs: []
  type: TYPE_NORMAL
- en: ^∗PaarthNeekhara \name^∗ShehzeenHussain \nameSubhankarGhosh \nameJasonLi \nameRafaelValle
    \nameRohanBadlani \nameBorisGinsburg
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated
    remarkable capabilities in handling large speech datasets and generating natural
    speech for new speakers. However, LLM-based TTS models are not robust as the generated
    output can contain repeating words, missing words and mis-aligned speech (referred
    to as hallucinations or attention errors), especially when the text contains multiple
    occurrences of the same token. We examine these challenges in an encoder-decoder
    transformer model and find that certain cross-attention heads in such models implicitly
    learn the text and speech alignment when trained for predicting speech tokens
    for a given text. To make the alignment more robust, we propose techniques utilizing
    CTC loss and attention priors that encourage monotonic cross-attention over the
    text tokens. Our guided attention training technique does not introduce any new
    learnable parameters and significantly improves robustness of LLM-based TTS models. ¹¹1
    Audio Examples: [https://t5tts.github.io/](https://t5tts.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: ^∗Denotes equal contribution
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: speech synthesis, large language modeling, robustness, computational paralinguistics,
    speech text alignments
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have revolutionized the landscape of deep generative
    AI with their unprecedented ability to generate coherent and contextually rich
    content across diverse domains. In LLM-based generative models, data is quantized
    into discrete tokens, which allows the formulation of data synthesis as a language
    modeling task. Transformer architectures such as GPT [[1](#bib.bib1)] (decoder-only)
    and T5 [[2](#bib.bib2)] (encoder-decoder) are trained to autoregressively generate
    discrete tokens given a prompt, leading to a unified architecture that can be
    adapted across various data domains and synthesis tasks. Particularly in the speech
    domain, there has been a recent surge in the use of LLMs for various speech synthesis
    applications such as text-to-speech (TTS) and speech-to-speech translation tasks [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: TTS synthesis has been traditionally treated as a cascaded problem with intermediate
    mel-spectrogram representation that is typically modelled as a regression task [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]. However,
    discrete neural audio codecs [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]
    have emerged as a promising intermediate audio representation, that not only preserve
    audio fidelity at a high compression rate, but are also suitable for training
    autoregressive transformer-based LLMs. Audio LLMs [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)] have gained traction for their ability to generate audio seamlessly,
    eliminating the necessity for additional duration and pitch prediction models.
    Moreover, LLM-based speech synthesis models can scale up to large speech datasets
    and be prompted in diverse ways to perform tasks like zero-shot speech synthesis,
    multilingual speech synthesis and other audio generation tasks besides speech.
    Despite their remarkable achievements, LLM-based TTS models suffer from attention
    errors resulting in mis-aligned speech, repeating and missing words, analogous
    to hallucinations [[15](#bib.bib15), [16](#bib.bib16)] exhibited by LLMs in the
    text domain. This issue becomes more prominent when the input text is challenging
    and contains repeating words. For certain inputs, the probabilistic autoregressive
    inference of LLM-based TTS models can result in looping or infinite silences [[17](#bib.bib17)].
    This issue makes LLM-based TTS models unreliable for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: In our work, we investigate this hallucination issue and find that attention
    layers of LLM-based TTS models learn an implicit alignment between text and speech
    tokens when trained using the next-token prediction objective. In encoder-decoder
    transformers, the TTS alignment is learned in certain cross-attention heads of
    the decoder; while in decoder-only models, the alignment is learned in the self-attention
    layers. Since the implicitly learned alignment in attention layers is unconstrained
    during training, it is not strictly monotonic which results in mis-aligned synthesis
    during inference. To address this challenge, we propose a learning procedure that
    encourages monotonic alignment in the attention layers of LLM-based TTS models,
    resulting in significantly more robust TTS models without modifying the architecture
    or introducing new parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We design a TTS model based on an encoder-decoder T5 [[2](#bib.bib2)] transformer
    architecture, which takes text and audio tokens of a reference audio as input
    and autoregressively predicts the audio tokens of the target audio from the decoder.
    To improve robustness of the TTS model, we propose a technique to guide the cross-attention
    head of the T5 model using a static attention prior and alignment loss that encourages
    monotonic attention over the text input. Our experiments demonstrate that the
    proposed technique significantly improves intelligibility of the synthesized audio
    especially for challenging text inputs. The key contributions of our work are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose an encoder-decoder transformer model for TTS synthesis. To the best
    of our knowledge, this is the first attempt at synthesizing multi-codebook neural
    audio codecs with an encoder-decoder architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop an alignment learning technique to guide the cross-attention heads
    in our TTS model to learn monotonic alignment. Incorporating our proposed technique
    reduces Character Error Rate (CER) of synthesized speech from $9.03\%$ on challenging
    texts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compare audio codec models based on Residual Vector Quantization and Finite
    Scalar Quantization (FSQ). FSQ codecs not only improve audio quality but also
    simplify the data representation by allowing parallel codebook prediction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd609717ab79ecf1bb11818df198c24a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Model Overview: (Left) The T5-TTS model takes as input text tokens
    and acoustic codes of reference audio and predicts the acoustic codes of the target
    audio. The figure shows both context input location options. (Right) The cross-attention
    scores implicitly learn text and speech alignment, but can be guided to learn
    more robust alignment with attention prior and alignment loss $L_{\textit{align}}$'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AudioLM [[3](#bib.bib3)] pioneered the task of training a decoder-only LLM on
    discretized audio tokens from a neural codec model, for high-quality speech synthesis.
    Following this, several solutions utilizing decoder-only transformer architectures
    have been proposed such as VALL-E, UniAudio, Bark, SpeechX [[4](#bib.bib4), [18](#bib.bib18),
    [6](#bib.bib6), [5](#bib.bib5)]. They frame audio generation as an autoregressive
    language modeling task using multiple discrete codebooks. Alternatively, SpeechT5 [[19](#bib.bib19)]
    proposes an encoder-decoder architecture for sequence to sequence translation
    using a unified discrete representation of text and speech. However, SpeechT5
    similar to other synthesis models based on SSL representations [[20](#bib.bib20),
    [21](#bib.bib21)], does not utilize multi-codebook audio representations.
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned transformer-based TTS models, the alignment between audio
    and phoneme sequences is entirely learned implicitly through the attention mechanisms
    in the transformer. This introduces potential instability in the form of hallucinations,
    since the alignment is not constrained to capture the monotonic dependencies of
    audio and text tokens [[4](#bib.bib4), [17](#bib.bib17)]. Prior research [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)] on non-LLM spectrogram generation models have
    proposed solutions to learn stricter alignment between text and speech tokens
    by constraining the encoder-decoder attention layers in CNN-based TTS models and
    LSTM-based models such as Tacotron [[7](#bib.bib7)] and Flowtron [[25](#bib.bib25)].
    While these techniques show promising results, they cannot be directly applied
    to transformer-based models which contain multiple cross-attention layers and
    multiple heads per layer, and generate discrete codes as opposed to continuous
    spectrograms.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our TTS model is an encoder-decoder LLM that is trained to predict acoustic
    codes of the target speech given the tokenized text input and acoustic codes of
    a reference audio from the target speaker. In this section, we first describe
    the tokenized representations used for text and speech. Next, we describe our
    model architecture and prompting setup for TTS. Finally, we propose a training
    procedure to learn robust text and speech alignment in the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Speech: We utilize neural audio codec models to convert the raw speech signal
    into a tokenized representation. Given an audio signal $y=y_{1}\dots y_{t}$ codebooks
    and achieve high quality speech synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text: For text, we use two tokenization schemes: sentence-piece [[29](#bib.bib29)]
    and phonemes. Sentence-piece tokens allows us to leverage pretrained text LLMs.
    To allow phoneme tokens as input, we expand the vocabulary and embedding space
    of the pretrained text-LLM to include additional tokens for phonemes. We train
    a single model to perform both phoneme to speech and sentence-piece to speech
    by prepending the text with the task prompt ``Phoneme TTS'''' or ``Text to Speech''''
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Model Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our model is based on the T5 architecture [[2](#bib.bib2)], with additional
    embedding layers and prediction heads to adapt it for TTS task. T5 is an encoder-decoder
    model, where the encoder is a non autoregressive bi-directional transformer and
    the decoder is an autoregressive transformer. Both the encoder and decoder networks
    contain $N_{l}$ transformer layers. Each layer within the encoder is composed
    of a self-attention module and a fully connected feed-forward network. In the
    decoder network, each layer adds an additional cross-attention module which performs
    multi-headed attention over the encoder's output.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform multi-speaker TTS, the model takes as input the tokenized text (question),
    and the acoustic tokens of a reference audio from the target speaker (context);
    and outputs the acoustic tokens of the target audio (answer). We consider two
    design options in our experiments: feeding the context audio tokens to the encoder
    network with the question, or to the decoder network before the answer. We discuss
    the trade-offs between these two designs in Section [4](#S4 "4 Experiments ‣ Improving
    Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the context and answer are represented by $N$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $e_{t}=\sum_{i=1}^{N}\texttt{EmbedA}_{i}(C[t,i])$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Similarly, at the decoder output, predictions for each of the $N$. Finally
    we calculate the cross entropy loss for next-token prediction as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\textit{CE}(\textit{SoftMax}(y),\textit{answer})$ |  |'
  prefs: []
  type: TYPE_TB
- en: Note that unlike past work [[4](#bib.bib4), [6](#bib.bib6)], our model does
    not use additional networks for handling multiple codebook predictions. Instead,
    we employ the delay pattern for representing RVQ tokens [[28](#bib.bib28)] to
    model codebook dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Alignment Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the T5 model is trained for TTS task using only the next token prediction
    loss, we observe that the attention-score matrix $A_{T\times M}$ is the number
    of encoder timesteps). That is, if we slice the attention-score matrix to include
    only the question time-steps, we observe higher attention-scores near the diagonal
    indicating the desirable monotonic alignment (Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Improving Robustness of LLM-based Speech Synthesis by Learning
    Monotonic Alignment")). However, attention errors in this implicitly learned alignment
    can cause missing or repeating words during inference, leading to hallucinations
    and inaccurate generations for challenging texts. Moreover, the alignment learning
    using only the next token prediction loss is often unstable and it can take several
    training iterations to learn a reasonable text and speech alignment, especially
    when training utterances are longer [[17](#bib.bib17), [24](#bib.bib24)]. We extend
    prior work [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)] and propose
    an alignment learning framework to guide multiple cross-attention heads of the
    T5 transformer model to learn robust alignment.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Attention Prior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To accelerate alignment learning, during initial training we multiply the attention-score
    matrices in the cross-attention heads with a static 2D beta-binomial prior. The
    2D beta-binomial prior is a near-diagonal heuristic matrix that is wider near
    the center and narrower near the corners. Multiplying the initially random attention
    matrices with such a prior, reduces the attention scores that are far-off the
    diagonal, providing a desirable monotonic initialization to the cross-attention
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the attention-score matrix between the decoder and encoder timesteps
    $A_{T\times M}^{\textit{l,h}}$ is the number of question (text) timesteps. Given
    this prior, we obtain the re-scaled attention scores as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${A}_{T\times M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}]\leftarrow A_{T\times
    M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}]\odot P_{T^{\prime}\times M^{\prime}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $q_{s}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the prior to all cross-attention heads of each decoder layer. Since
    we do not know the target audio length during inference which is needed to compute
    the prior, we cannot use this prior during inference. Therefore, we apply the
    attention prior for the first $S_{1}$, the prior matrix is obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This annealing procedure is necessary to ensure stability during training. Turning
    off the prior without annealing causes the loss curve to spike, since the decoder
    expects re-scaled attention scores for making valid predictions. In our experiments,
    we set $S_{1}=8000$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Alignment Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The soft alignment matrix between the text and audio timesteps can be obtained
    by taking softmax of the sliced attention-score matrix over the text dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A^{\textit{soft}_{l,h}}_{T^{\prime}\times M^{\prime}}=\textit{Softmax}({A}_{T\times
    M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}])$ |  |'
  prefs: []
  type: TYPE_TB
- en: An $i^{\textit{th}}$. If we sample a prediction from such a distribution at
    each answer timestep, it is desirable that the resulting sequence of text timesteps
    is monotonic. Since the length of the answer is typically longer than the text,
    there can be multiple valid monotnic reductions of the alignment matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'To encourage valid monotonic sampling from the alignment matrix, we calculate
    the likelihood of all possible monotonic reductions using the Connectionist Temporal
    Classification (CTC) algorithm. That is, given the alignment matrix $A^{\textit{soft}_{l,h}}_{T^{\prime}\times
    M^{\prime}}$, we obtain the alignment loss for a decoder layer and head as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{\textit{align}}^{l,h}=\textit{CTCLoss}(A^{\textit{soft}_{l,h}}_{T^{\prime}\times
    M^{\prime}},q_{M^{\prime}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $q_{M^{\prime}}=\{1,2,\dots M^{\prime}\}$ over which we wish to enforce
    monotonic alignment. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{\textit{align}}=\sum_{\begin{subarray}{c}l,h\in\mathbb{{P}}\end{subarray}}L_{\textit{align}}^{l,h}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For set $\mathbb{{P}}$ to all cross-attention heads in each layer in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets and Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We train our T5-TTS models on a data-blend containing $1.8k$ steps optimized
    with a fixed learning rate of $1e-4$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alignment Learning: To assess the efficacy of our alignment learning method
    (Section [3.3](#S3.SS3 "3.3 Alignment Learning ‣ 3 Methodology ‣ Improving Robustness
    of LLM-based Speech Synthesis by Learning Monotonic Alignment")), we train three
    variants of our T5 TTS model using the spectral codec: 1) T5-TTS (No Prior, No
    $L_{\textit{align}})$, we obtain monotonic but unaligned attention maps. leading
    to no speech synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: TTS results on seen and unseen speakers for different T5-TTS models.
    Lower CER(%) & WER(%) indicate higher intelligibility. Higher SSIM indicates higher
    speaker similarity to ground-truth audio.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Eval Set | Model | Context | CER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ground Truth |  | $1.03$ |'
  prefs: []
  type: TYPE_TB
- en: '| LibriTTS | T5-TTS (No Prior, No $L_{\textit{align}})$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Seen | T5-TTS (W Prior, No $L_{\textit{align}})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Speakers) | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  prefs: []
  type: TYPE_TB
- en: '| VCTK | Ground Truth |  | $0.50$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Unseen | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Speakers) | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  prefs: []
  type: TYPE_TB
- en: We evaluate the models on a set of seen and unseen speakers. For seen speakers,
    we use $200$ utterances per speaker. For each utterance, we synthesize two audios
    using either the sentence piece text tokenizer or the phoneme tokenizer. We evaluate
    the synthesized speech on intelligibility and speaker similarity. For intelligibility,
    we transcribe the synthesized audio through a Conformer-Transducer ASR model ²²2[https://hf.co/nvidia/stt_en_conformer_transducer_large](https://hf.co/nvidia/stt_en_conformer_transducer_large)
    and compute the CER and WER between the ASR transcript and the ground-truth text.
    For speaker similarity (SSIM), we compute the cosine similarity between the embeddings
    of the synthesized speech and target ground-truth audio obtained from WavLM [[35](#bib.bib35)]
    speaker verification model ³³3[https://hf.co/microsoft/wavlm-base-plus-sv](https://hf.co/microsoft/wavlm-base-plus-sv).
    We report the results in Table [1](#S4.T1 "Table 1 ‣ 4.2 Results ‣ 4 Experiments
    ‣ Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment").
    While all three models achieve high speaker similarity for seen speakers, the
    intelligibility metrics improve as we incorporate attention prior and alignment
    loss during training. For unseen speakers, we observe a higher speaker similarity
    and intelligibility when the context is fed to the T5 decoder instead of the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenging Texts and Comparison against Prior Work: As shown in Table [2](#S4.T2
    "Table 2 ‣ 4.2 Results ‣ 4 Experiments ‣ Improving Robustness of LLM-based Speech
    Synthesis by Learning Monotonic Alignment"), the improvement in intelligibility
    is even more significant when we consider challenging text inputs with repeating
    words. We compare our models (using decoder context) with three open source LLM-based
    TTS models using the inference code and released checkpoints [[19](#bib.bib19),
    [36](#bib.bib36), [37](#bib.bib37)]. For this evaluation we consider a set of
    $100$ confidence intervals indicates our model outperforms prior LLM-based TTS
    models considered in our study. We encourage readers to listen to audio examples
    linked in the footnote of the first page.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Comparison of different T5-TTS models against prior LLM-based TTS
    models. Intelligibility metrics (WER, CER, character insertions, deletions, substitutions
    (%)) are evaluated on $100$ challenging texts. Naturalness MOS is calculated on
    subset of harvard sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *Intelligibility* $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model | WER | CER | Ins | Del | Subs | MOS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| VALL-E-X [[18](#bib.bib18)] | $16.8$ |'
  prefs: []
  type: TYPE_TB
- en: '| Bark [[36](#bib.bib36)] | $19.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| SpeechT5 [[19](#bib.bib19)] | $13.5$ |'
  prefs: []
  type: TYPE_TB
- en: '| T5-TTS (No Prior, No $L_{\textit{align}})$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| T5-TTS (W Prior, No $L_{\textit{align}})$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| T5-TTS (W Prior, W $\mathbf{L_{\textit{{align}}}}\textbf{)}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Codec Choice: We train three T5-TTS models with alignment learning on the three
    audio codecs and report results on seen speakers in Table [3](#S4.T3 "Table 3
    ‣ 4.2 Results ‣ 4 Experiments ‣ Improving Robustness of LLM-based Speech Synthesis
    by Learning Monotonic Alignment"). We find that both spectral codec and Dac significantly
    outperform Encodec in terms of audio naturalness. Spectral codec streamlines training
    by independently predicting codebooks in parallel, unlike the delay pattern scheme
    needed for Encodec/Dac. Additionally, spectral codec enhances synthesized speech
    intelligibility, demonstrated by reduced CER/WER.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparison of T5-TTS (W Prior, W $L_{\textit{align}})$ models trained
    with different audio codecs. N: number of codebooks, FPS: Frames Per Second, TPS:
    Tokens Per Second. All codecs use 10-bit tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Codec | FPS | N | TPS | CER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Encodec | 75 | 8 | 600 | $4.01$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dac | 86 | 9 | 774 | $6.72$ |'
  prefs: []
  type: TYPE_TB
- en: '| Spectral codec | 86 | 8 | 688 | $\mathbf{2.16}$ |'
  prefs: []
  type: TYPE_TB
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present a T5-TTS model that can learn robust text and speech alignment without
    modifying the model architecture or requiring ground-truth text duration. We identify
    that attention heads in LLM-based TTS models implicitly learn text and speech
    alignment and can be guided to monotonically attend over the text input. Our experiments
    demonstrate that our alignment learning procedure improves the reliability of
    TTS synthesis, especially for challenging text inputs and outperforms prior LLM-based
    TTS models on both intelligibility and naturalness.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would also like to thank Ryan Langman for developing the spectral codec model
    that was used in our TTS model.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever *et al.*, ``Improving
    language understanding by generative pre-training,'''' *OpenAI blog*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu, ``Exploring the limits of transfer learning with a unified
    text-to-text transformer,'''' *The Journal of Machine Learning Research*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi,
    D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi *et al.*, ``Audiolm: a language
    modeling approach to audio generation,'''' *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang,
    J. Li *et al.*, ``Neural codec language models are zero-shot text to speech synthesizers,''''
    *arXiv:2301.02111*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] X. Wang, M. Thakker, Z. Chen, N. Kanda, S. E. Eskimez, S. Chen, M. Tang,
    S. Liu, J. Li, and T. Yoshioka, ``Speechx: Neural codec language model as a versatile
    speech transformer,'''' *arXiv:2308.06873*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao,
    J. Bian, X. Wu *et al.*, ``Uniaudio: An audio foundation model toward universal
    audio generation,'''' *arXiv:2310.00704*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang,
    Y. Xiao, Z. Chen, S. Bengio *et al.*, ``Tacotron: Towards end-to-end speech synthesis,''''
    in *INTERSPEECH*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Łańcucki, ``Fastpitch: Parallel text-to-speech with pitch prediction,''''
    in *ICASSP*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. Neekhara, S. Hussain, S. Dubnov, F. Koushanfar, and J. McAuley, ``Expressive
    neural voice cloning,'''' in *Asian Conference on Machine Learning*.   PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. Valle, J. Li, R. Prenger, and B. Catanzaro, ``Mellotron: Multispeaker
    expressive voice synthesis by conditioning on rhythm, pitch and global style tokens,''''
    *ICASSP*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] E. Casanova, J. Weber, C. Shulby, A. Junior, E. Gölge, and M. A. Ponti,
    ``Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion
    for everyone,'''' in *ICML*.   PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, ``High fidelity neural
    audio compression,'''' *Transactions on Machine Learning Research*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, ``High-fidelity
    audio compression with improved rvqgan,'''' *Advances in Neural Information Processing
    Systems*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, ``Soundstream:
    An end-to-end neural audio codec,'''' *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto, and P. Fung, ``Survey of hallucination in natural language generation,''''
    *Association for Computing Machinery*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] R. Azamfirei, S. R. Kudchadkar, and J. Fackler, ``Large language models
    and the perils of their hallucinations,'''' *Critical Care*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Song, Z. Chen, X. Wang, Z. Ma, and X. Chen, ``Ella-v: Stable neural
    codec language modeling with alignment-guided sequence reordering,'''' *arXiv:2401.07333*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang,
    J. Li *et al.*, ``Speak foreign languages with your own voice: Cross-lingual neural
    codec language modeling,'''' *arXiv:2303.03926*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li,
    Y. Zhang *et al.*, ``Speecht5: Unified-modal encoder-decoder pre-training for
    spoken language processing,'''' in *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, 2022,
    pp. 5723–5738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] W. C. Huang, S. W. Yang, T. Hayashi, H. Y. Lee, S. Watanabe, and T. Toda,
    ``S3prl-vc: Open-source voice conversion framework with self-supervised speech
    representations,'''' in *ICASSP*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Hussain, P. Neekhara, J. Huang, J. Li, and B. Ginsburg, ``Ace-vc: Adaptive
    and controllable voice conversion using explicitly disentangled self-supervised
    speech representations,'''' in *ICASSP*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Tachibana, K. Uenoyama, and S. Aihara, ``Efficiently trainable text-to-speech
    system based on deep convolutional networks with guided attention,'''' in *ICASSP*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] K. J. Shih, R. Valle, R. Badlani, A. Lancucki, W. Ping, and B. Catanzaro,
    ``Rad-tts: Parallel flow-based tts with robust alignment learning and diverse
    synthesis,'''' in *ICML Workshop on Invertible Neural Networks, Normalizing Flows,
    and Explicit Likelihood Models*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. Badlani, A. Łańcucki, K. J. Shih, R. Valle, W. Ping, and B. Catanzaro,
    ``One tts alignment to rule them all,'''' in *ICASSP*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] R. Valle, K. J. Shih, R. Prenger, and B. Catanzaro, ``Flowtron: an autoregressive
    flow-based generative network for text-to-speech synthesis,'''' in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] F. Mentzer, D. Minnen, E. Agustsson, and M. Tschannen, ``Finite scalar
    quantization: Vq-vae made simple,'''' in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] R. Langman, A. Jukić, K. Dhawan, N. R. Koluguri, and B. Ginsburg, ``Spectral
    codecs: Spectrogram-based audio codecs for high quality speech synthesis,''''
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and
    A. Défossez, ``Simple and controllable music generation,'''' *Advances in Neural
    Information Processing Systems*, vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] T. Kudo and J. Richardson, ``Sentencepiece: A simple and language independent
    subword tokenizer and detokenizer for neural text processing,'''' in *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and
    Y. Wu, ``Libritts: A corpus derived from librispeech for text-to-speech,'''' *INTERSPEECH*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] E. Bakhturina, V. Lavrukhin, B. Ginsburg, and Y. Zhang, ``Hi-Fi Multi-Speaker
    English TTS Dataset,'''' in *INTERSPEECH*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, ``MLS: A Large-Scale
    Multilingual Dataset for Speech Research,'''' in *INTERSPEECH*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy, ``The Pile: An 800gb
    dataset of diverse text for language modeling,'''' *arXiv:2101.00027*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C. Veaux, J. Yamagishi, and K. Macdonald, ``CSTR VCTK corpus: English
    multi-speaker corpus for CSTR voice cloning toolkit,'''' 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka,
    X. Xiao *et al.*, ``Wavlm: Large-scale self-supervised pre-training for full stack
    speech processing,'''' *IEEE Journal of Selected Topics in Signal Processing*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] SunoAI, ``Bark audio generation model,'''' 2023\. [Online]. Available:
    [https://github.com/suno-ai/bark](https://github.com/suno-ai/bark)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Songting, ``VALL-E-X,'''' 2023\. [Online]. Available: [https://github.com/Plachtaa/VALL-E-X](https://github.com/Plachtaa/VALL-E-X)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] ``IEEE recommended practice for speech quality measurements,'''' *IEEE
    Transactions on Audio and Electroacoustics*, vol. 17, no. 3, pp. 225–246, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
