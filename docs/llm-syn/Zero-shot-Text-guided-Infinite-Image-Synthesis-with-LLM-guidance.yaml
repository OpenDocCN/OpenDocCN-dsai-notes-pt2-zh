- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:43'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot Text-guided Infinite Image Synthesis with LLM guidance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12642](https://ar5iv.labs.arxiv.org/html/2407.12642)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: Artificial Intelligence Graduate School, UNIST'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: {soyoung17, taegyeonglee, taehwankim}@unist.ac.krSoyeong Kwon Equal
    contributions (alphabetically ordered by last name.)    Taegyeong Lee^⋆    Taehwan
    Kim'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: We provide the optimization objectives details in Section [1](#S1 "1
    Details of Optimization Objectives ‣ Zero-shot Text-guided Infinite Image Synthesis
    with LLM guidance"), the dataset details in Section [2](#S2 "2 Dataset Details
    ‣ Zero-shot Text-guided Infinite Image Synthesis with LLM guidance"), human evaluation
    details in Section [3](#S3 "3 Human Evaluation Details ‣ Zero-shot Text-guided
    Infinite Image Synthesis with LLM guidance"), human evaluation for ablation study
    in Section [4](#S4 "4 Human Evaluation for ablation study ‣ Zero-shot Text-guided
    Infinite Image Synthesis with LLM guidance") and additional generated samples
    in Section [5](#S5 "5 Generated Samples ‣ Zero-shot Text-guided Infinite Image
    Synthesis with LLM guidance") that were not included in the main paper due to
    space limit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/31fddd9e49c9d229b3a3e103a9e6eb5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Generated samples with 4k resolution. We expand the given local image
    upwards and downwards twice, and left and right a total of 16 times, to follow
    the given global caption. The resolution of the generated image is 4608$\times$1536.
    The red box is the given local image. Due to the file size limit, we have repeatedly
    resized and compressed the generated images, which has slightly impacted the image
    quality. (180MB to 28MB)'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Details of Optimization Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We train our model end-to-end following the Stable Diffusion [rombach2022high].
    The optimized objective can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\mathbb{E}_{\mathcal{E},y,v,\epsilon\sim\mathcal{N}(0,1),t}\Big{[}\&#124;\epsilon-\epsilon_{\theta}(z_{t},t,\tau_{\theta}(y)),v\&#124;_{2}^{2}\Big{]}\,,$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon_{\theta}$ denotes the clip visual feature.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Dataset Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We train the model to fill the local masked image conditioned on a local caption
    and a global caption. Below are the details of the training dataset. Also we provide
    samples of our training dataset in Figure [3](#S5.F3 "Figure 3 ‣ 5 Generated Samples
    ‣ Zero-shot Text-guided Infinite Image Synthesis with LLM guidance").
  prefs: []
  type: TYPE_NORMAL
- en: GT(annotated caption). We use a ground truth(GT) caption (annotated caption)
    of MS-COCO [lin2014microsoft] dataset as a local caption.
  prefs: []
  type: TYPE_NORMAL
- en: Local captions(generated from LLM). We generate local captions from the GT caption
    by utilizing the GPT 3.5 [brown2020language]. We use these captions to generate
    a global caption.
  prefs: []
  type: TYPE_NORMAL
- en: Global caption. We generate a global caption by summarizing the generated local
    captions and the GT caption with GPT-3.5 [brown2020language].
  prefs: []
  type: TYPE_NORMAL
- en: 'Local masked image generation. First, we center crop and resize the MS-COCO [lin2014microsoft]
    dataset images to 512$\times$512\. Then, for local masked image generation, we
    mask the images in four directions: top, bottom, left, and right.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We provide samples of global captions generated from the LLM for evaluation.
    Figure [4](#S5.F4 "Figure 4 ‣ 5 Generated Samples ‣ Zero-shot Text-guided Infinite
    Image Synthesis with LLM guidance") shows the global caption used for evaluation
    along with the image.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Human Evaluation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct human evaluation on Amazon Mechanical Turk (AMT) to compare our model
    in three aspects, text matching, image quality and global coherence with the baselines
    and the ablated models.
  prefs: []
  type: TYPE_NORMAL
- en: For human evaluation, we randomly sample 100 generated images from each of MS-COCO [lin2014microsoft],
    Flickr [rashtchian2010collecting], and Pascal [rashtchian2010collecting] datasets,
    in total 300 samples. We conduct surveys with 5 participants to evaluate text
    matching, image quality and global coherence. For text matching, we provide images
    and text pairs and ask participants to respond to the question, “Choose a image
    that matches the text better.". For image quality, we ask participants to respond
    to the question, “Choose a image with better image quality.”. For global coherence,
    we ask participants to respond to the question, “Choose a image with better global
    coherence, according to text”. The screenshot of the user study including the
    instructions is shown in Figure [2](#S5.F2 "Figure 2 ‣ 5 Generated Samples ‣ Zero-shot
    Text-guided Infinite Image Synthesis with LLM guidance").
  prefs: []
  type: TYPE_NORMAL
- en: 4 Human Evaluation for ablation study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Human evaluation on $\times$4 expansion. Each cell lists the winning
    percentage of our model versus baselines. TM is “text matching”. IQ is “image
    quality”. GC is “global coherence”.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | MS-COCO | Flickr | Pascal |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | GC | CLIP | LLM | TM | IQ | GC | TM | IQ | GC | TM | IQ | GC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  prefs: []
  type: TYPE_TB
- en: '| w/o All | ✓ | ✗ | ✗ | 65.00 | 67.20 | 63.60 | 62.60 | 65.00 | 62.20 | 62.40
    | 64.20 | 63.20 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o CLIP | ✓ | ✗ | ✓ | 62.00 | 60.00 | 62.00 | 60.00 | 61.20 | 61.80 | 63.80
    | 60.00 | 59.80 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o LLM | ✓ | ✓ | ✗ | 60.00 | 58.60 | 59.20 | 57.80 | 60.00 | 60.60 | 57.40
    | 57.00 | 57.80 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o GC | ✗ | ✓ | ✓ | 60.20 | 57.20 | 64.20 | 59.00 | 55.20 | 61.20 | 60.20
    | 53.80 | 61.80 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Human evaluation on $\times$8 expansion. Each cell lists the winning
    percentage of our model versus baselines. TM is “text matching”. IQ is “image
    quality”. GC is “global coherence”.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | MS-COCO | Flickr | Pascal |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | GC | CLIP | LLM | TM | IQ | GC | TM | IQ | GC | TM | IQ | GC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  prefs: []
  type: TYPE_TB
- en: '| w/o All | ✓ | ✗ | ✗ | 65.40 | 60.80 | 66.20 | 65.60 | 63.60 | 65.00 | 64.20
    | 60.20 | 66.40 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o CLIP | ✓ | ✗ | ✓ | 65.20 | 65.60 | 63.60 | 64.00 | 66.00 | 66.80 | 65.40
    | 62.00 | 64.00 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o LLM | ✓ | ✓ | ✗ | 59.60 | 61.20 | 60.00 | 63.20 | 61.60 | 64.80 | 64.80
    | 60.60 | 61.80 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o GC | ✗ | ✓ | ✓ | 60.20 | 57.00 | 62.60 | 60.40 | 56.20 | 61.40 | 59.20
    | 53.40 | 62.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Due to the page limit of the main paper, we provide the result of human evaluation
    for ablation study in this section. We conduct human evaluation with the ablated
    models: 1) the w/o all model generates an image with only a global caption. 2)
    the w/o LLM model generates an image with a global caption and the CLIP [radford2021learning]
    visual feature. 3) the w/o CLIP [radford2021learning] model generates an image
    with a global caption and a local caption generated with the LLM. 4) w/o GC model
    generates an image with a local caption generated with the LLM and the CLIP [radford2021learning]
    visual feature. We evaluate the performance following three aspects, text matching(TM),
    image quality(IQ) and global coherence(GC).'
  prefs: []
  type: TYPE_NORMAL
- en: As shown as Table [1](#S4.T1 "Table 1 ‣ 4 Human Evaluation for ablation study
    ‣ Zero-shot Text-guided Infinite Image Synthesis with LLM guidance") ($\times$8
    expansion) demonstrates that our model significantly outperforms in all aspects
    compared to all ablated models. This indicates that our model can perform image
    outpainting considering the text matching, image quality and global coherence
    despite the extension size increases.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Generated Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We provide the additional generated samples in the following figures. As shown
    as Figure [1](#S0.F1 "Figure 1 ‣ Zero-shot Text-guided Infinite Image Synthesis
    with LLM guidance"), our model can expand an image with 4k resolution while following
    the global caption and maintaining the global consistency. Also in Figure [5](#S5.F5
    "Figure 5 ‣ 5 Generated Samples ‣ Zero-shot Text-guided Infinite Image Synthesis
    with LLM guidance") and [6](#S5.F6 "Figure 6 ‣ 5 Generated Samples ‣ Zero-shot
    Text-guided Infinite Image Synthesis with LLM guidance"), we qualitatively compare
    our model with the baseline models, SD Inpainting [rombach2022high] (SD Inp),
    Blended Latent Diffusion [avrahami2022blended] (BLD) and PowerPaint [zhuang2023task]
    (PP) for each dataset, MS-COCO [lin2014microsoft], Flickr [alayrac2022flamingo],
    and Pascal [rashtchian2010collecting].
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generated samples with 4k resolution : Figure [1](#S0.F1 "Figure 1 ‣ Zero-shot
    Text-guided Infinite Image Synthesis with LLM guidance")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparison with our model and the baselines : Figure [5](#S5.F5 "Figure 5 ‣
    5 Generated Samples ‣ Zero-shot Text-guided Infinite Image Synthesis with LLM
    guidance"), Figure [6](#S5.F6 "Figure 6 ‣ 5 Generated Samples ‣ Zero-shot Text-guided
    Infinite Image Synthesis with LLM guidance")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/497838192267d57d6662213708c43cb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Screenshot of instructions provided to participants during the human
    evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a3f30c3b1380fd94e41b6efef0dce8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Dataset construction sample for training. “GT (annotated caption)”
    is the ground truth caption on MS-COCO [lin2014microsoft], Flickr [alayrac2022flamingo]
    and Pascal [rashtchian2010collecting] testsets. First we generate local captions
    based on the GT using the LLM. Then, we generate the global caption with the LLM
    by summarizing the GT and the generated local captions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/422af5a3dd008ef235dff94aa4782847.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Dataset construction sample for evaluation. “GT (annotated caption)”
    is the ground truth caption on MS-COCO [lin2014microsoft], Flickr [alayrac2022flamingo]
    and Pascal [rashtchian2010collecting] test sets. The global caption is a caption
    generated by the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/798ef661bfa9dcdff39828200f5f7d34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of generated image results. We expand the image eight
    times. The expanded image has a resolution of 2560$\times$512. The red box is
    the given local image. Due to the limit of the file size, we have repeatedly resized
    and compressed image files, which has slightly impacted the image quality (100MB
    to 32MB).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc3443a57639f06f7c5adb3771b19205.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison of generated image results. When a local image and a global
    caption are provided, the image is expanded a total of eight times. The expanded
    image has a resolution of 512$\times$2560. The red box is the provided original
    local image.'
  prefs: []
  type: TYPE_NORMAL
