# MIT 6.S91 深度学习导论 2021 中文笔记（二）



# P4：Deep Generative Modeling - 爱可可-爱生活 - BV1jo4y1d7R6

大家好，欢迎来到MIT 6。S191的讲座4！在今天的演讲中，我们将成为，谈论我们如何使用深度学习和神经网络来构建系统，不仅，寻找数据中的模式，但实际上可以超越此范围以生成全新的合成。

基于这些已学模式的示例，我认为这是一个非常强大的想法，这是深度学习的一个特殊子领域，它获得了很多成功，并且，在过去的几年中引起了很多兴趣，但我认为仍然有很多，退化建模领域在未来和未来的巨大潜力。

特别是当我们看到这些类型的模型以及它们所解决的问题类型，在各种应用领域中越来越重要，因此，我想开始。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_1.png)

在这里给大家一个简单的问题，我们有三张面孔的照片，我希望你们所有人，花点时间看一下这些面孔，研究一下，然后思考您认为哪些面孔是真实的，是左边的脸是中间的脸还是右边的脸是哪个，真的很真实。

这些面孔中的每一个都不是真实的，它们都是假的，都是图像，这些是由深度神经网络综合生成的，实际上这些人都没有，存在于现实世界中，希望我想大家都赞赏其中每一个的真实性，合成图像。

这对我而言凸显了深度生成建模的不可思议的力量，它不仅突出了这些类型的算法和这些类型的模型的强大功能，但这引发了很多关于我们如何考虑此类行为的合理使用和道德使用的问题，算法在现实世界中的部署。

因此可以通过设置并激发，这样，我首先我现在想退后一步，从根本上考虑什么是类型，当我们训练神经网络执行诸如此类的任务时可能发生的学习，到目前为止，在本课程中，我们一直在考虑所谓的监督学习问题，实例中。

我们得到了一组数据和与该数据相关的一组标签，我们，我们的目标是学习从数据到标签的功能映射，这些标签可以，是类标签或连续值，在本课程中，我们主要关注的是，开发这些可以由深度神经网络描述的功能映射。

但从本质上讲，这些映射可能是您所知道的任何统计功能，今天讲座的主题将集中在我们称为无监督学习的新知识上，学习问题的类别，与有监督的环境相反，在这种环境下，我们获得了数据和，在无监督学习中的标签。

我们只给数据没有标签，我们的目标是训练机器，学习或深度学习模型来理解或建立隐藏和，数据中的基础结构，这可以做的是让您深入了解，数据的基础结构，然后我们可以利用这种理解来实际。

产生超越深层生成领域的综合实例和无监督学习，建模还扩展到其他类型的问题和您可能正在使用的示例应用程序，熟悉的诸如聚类算法或降维算法生成，建模是无监督学习的一个例子，在这种情况下，我们的目标是作为输入。

训练集中的示例，并学习一个表示数据分布的模型，被输入到该模型中，这可以通过两种原理来实现：第一种是通过，称为密度估算，假设我们得到了一组数据样本，它们落入了，根据某种密度。

建立适用于这些样品的深度生成模型的任务，是要学习描述这些数据的方式和位置的潜在概率密度函数，沿着这种分布下降，我们不仅可以估计这种概率的密度，密度函数，但实际上使用此信息来生成新的合成样本。

我们正在考虑一些输入示例，这些示例是从一些训练数据分布中得出的，在使用这些数据建立模型之后，我们现在的目标是生成综合示例，可以描述为属于我们的模型所建模的数据分布，因此，在这两种情况下。

关键思想的关键是我们如何学习的问题，使用我们的模型（称为x的p模型）的概率分布为，因此类似于真实数据分布，我们称x为p数据，这不仅会，使我们能够有效地估计这些概率密度函数，而且还可以生成。

逼真的新合成样本并与我们正在考虑的数据分布相匹配，因此，我认为这是具体总结了生成建模背后的关键原理是什么。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_3.png)

但要了解生成建模可能是有益的又有影响力的，让我们来看一下，进一步思考这个想法，并考虑可能对应用程序有潜在影响的应用程序，以及，生成模型在现实世界中的用例生成模型使我们作为用户能够使用的功能是。

自动发现数据集中的基础结构和特征的原因可能是，真正重要而真正强大的往往是我们不知道这些功能是如何分布的，在特定的感兴趣数据集内，因此，假设我们正在尝试建立一个面部，检测分类器，我们得到了一个人脸数据集。

而我们可能不知道该数据集的确切信息，这些面孔相对于关键特征（例如肤色或姿势或衣物）的分布，而无需查看我们的数据集并手动检查这些实例中的每一个，实际上，即使没有我们。

我们的训练数据在某些功能上也可能有很大的偏见。知道了这一点，正如您将在本讲座和今天的实验室中看到的那样，我们实际上可以做的是训练，生成模型，可以自动了解数据集中的要素景观，例如，这些就像面孔一样。

通过这样做实际上揭示了训练分布的区域，相对于诸如皮肤的特定特征而言，代表不足或代表过多，语调以及之所以如此强大的原因是我们现在可以实际使用此信息来，调整培训期间如何采样数据，以最终建立一个更加公平。

更加公正的环境，具有代表性的数据集将导致一个更加公平和公正的模型，然后您将获得练习做这些的实际操作，并在今天的实验室练习中实现这个想法，生成模型非常强大的用例中的另一个很好的例子是。

可以视为离群值或异常检测的一类问题，自动驾驶汽车，对于确保自动驾驶汽车至关重要，由深度神经网络控制和操作的神经网络能够处理它所遇到的所有情况，可能会在路上遇到，不仅您知道直行高速公路将成为。

大部分的训练数据和大部分时间是汽车在路上的经历，因此，生成模型实际上可以用于检测训练分布和使用中的异常值，这可以再次改善训练过程，从而可以更好地为最终的模型配备。

希望能够很好地处理这些极端情况和罕见事件，从而激发产生原因和产生方式的动机。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_5.png)

模型对于各种现实世界中的潜水应用而言可能异常强大和有用，在今天演讲的大部分技术内容中，我们将讨论两个类别的内容，我们称之为潜变量模型，具体来说，我们将研究自动编码器和生成对抗，网络器官。

但在开始讨论之前，我想先讨论一下为什么，称为潜在变量模型，以及使用此词潜在时的实际含义，为此，我认为确实是我亲自了解的最佳范例，一个潜在的变量是这个故事，来自普拉托共和国的作品，这个故事被称为。

洞穴的神话或洞穴的寓言，故事如下：是一群囚犯，这些囚犯被作为对他们的监狱惩罚的一部分而受到约束，面对一堵墙，他们在这堵墙上唯一能看到的就是特殊的阴影，在他们身后的大火之前通过的物体。

所以在他们的头后面和外面，他们的视线和囚犯他们真正观察到的唯一一件事就是这些阴影，在墙上，对他们来说，这就是他们可以看到的，可以测量的，那是什么，他们可以说出真实的名字，这是他们观察到的变量。

但是他们不能，实际上直接观察或测量实际投射这些物体的物理对象，阴影，所以这些对象实际上就是我们可以像潜在变量一样进行分析的对象，它们是无法直接观察到的变量，但却是真正的解释，造成可观察变量的因素。

在这种情况下，囚犯，正在看到像投射在墙上的阴影，因此生成模型中的问题，广泛地是找到实际学习数据中这些潜在的和隐藏的潜在变量的方法，即使当我们仅给出所观察到的结果，这也是一个极其极端的情况。

由于神经网络的强大功能，非常适合神经网络学习的复杂问题，处理多维数据集并学习非线性函数的组合，这些函数可以，可以很好地估计非常复杂的数据分布，所以我们首先开始讨论一个简单的。

试图建立这种潜在变量表示的基础生成模型，通过实际对输入进行自我编码，这些模型称为自动编码器。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_7.png)

自动编码器是一种学习低维潜在空间的方法，从原始数据了解其工作原理，我们将输入的原始数据作为输入，例如，这张两张图片将通过许多连续的深度神经网络传递，层以及在一系列神经网络层的输出中，我们将要做的是。

生成是一个低维潜在空间的特征表示，这实际上是，我们正在尝试预测的目标，因此我们可以将网络的这一部分称为编码器，因为它将数据x映射到潜在变量z的编码向量中，所以让我们考虑。

如果您已经注意到我将z表示为较小的尺寸，则此潜在空间z，较小的尺寸作为输入x为什么确保低尺寸很重要，潜在空间z的维数较低，意味着我们能够压缩，在图像数据的情况下可以是许多维度的数据。

我们可以将数据压缩到一个小的潜在矢量中，在其中我们可以学习非常紧凑且丰富的，特征表示，那么我们如何才能实际训练该模型？能够监督我们感兴趣的特定潜在变量，请记住，这是一个无监督的问题，我们有训练数据。

但没有标签，潜在空间z因此，为了实际训练这样的模型，我们可以做的是学习解码器，网络并建立一个解码器网络，该网络用于实际重建原始图像，从这个较低维度的潜在空间开始，再一次是我们汽车的这个解码器部分。

编码器网络将是一系列层的神经网络层，如卷积层，然后将这个隐藏的潜在向量并将其映射回输入空间，我们将重建后的输出称为x hat，因为这是我们的预测，而且是不完善的。

输入x的重建以及我们实际训练该网络的方式是通过查看，原始输入x和我们重建的输出x hat，只需将两者进行比较并最小化，这两个图像之间的距离，因此，例如，我们可以考虑均方误差，对于图像。

这意味着有效地从另一个图像中减去一个图像并进行平方，差异权，实际上是输入之间的像素差异，和重建测量我们的重建对原始输入的忠实度，请注意，通过使用此重建损失，重建输出之间的差异，和我们的原始输入。

除了数据本身之外，我们不需要为数据添加任何标签，因此，我们可以通过抽象化图中的各个单独层来简化此图，编码器和解码器组件，再次请注意，此损耗函数不需要任何，标记它只是使用原始数据在输出上进行自我监督。

这是真正强大的功能，想法和变革性想法，因为它使模型能够学习一定数量的潜在变量，z我们从根本上感兴趣，但是我们不能简单地观察或无法轻松建模。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_9.png)

当我们将此潜伏空间限制在一个较低的维度上，从而影响程度，我们可以真正重构输入的目标和忠诚度，而您对此的看法就是强加于人，模型训练和学习过程中的一种信息瓶颈，有效地解决了这个瓶颈，这是一种压缩形式。

我们正在接受输入，数据将其压缩到更小的潜在空间，然后构建重建，实际上，这导致潜伏空间的维数越低，较差和较差的质量重建工作将使您顺利完成工作，因此总而言之。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_11.png)

这些自动编码器结构使用这种瓶颈隐藏层来学习压缩，数据的潜在表示，我们可以自我监督该网络的训练，通过使用我们所谓的重建损失来迫使自动编码器网络受力，将有关数据的尽可能多的信息编码到较低维的潜在空间中。

同时仍然能够建立忠实的重建项目，因此我喜欢这样思考，这是自动将数据中的信息编码到较低维的潜在空间中。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_13.png)

现在让我们进一步扩展这个想法，并介绍这个概念和架构。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_15.png)

可变自动编码器或vaes，就像我们看到的传统自动编码器一样，从输入到重构输出，如果我们更注意这个潜在层，用橙色表示这里您可以希望意识到的是，这只是一个普通层，在神经网络中，就像其他任何层一样。

它是否具有确定性，只要权重是，传统自动编码器同​​样有效地学习了确定性编码，相比之下，它可以重建和再现输入，可变自动编码器在此架构上施加了随机或可变扭曲，这样做的想法是生成更平滑的输入数据表示。

并不仅提高重建的质量，而且还可以实际产生，与输入数据集相似但不直接重建输入数据的新图像，而实现这一目标的方式是变分自动编码器，用随机采样操作替换该确定性层z，这意味着不是直接学习每个变量的潜在变量z。

变分自动编码器学习与该潜在变量相关的均值和方差，这些均值和方差的作用是它们为，该潜在变量，因此我们完成了从自动编码器到变分自动编码器的工作，正在从潜在变量z的向量到学习均值的向量mu和向量。

对这些变量进行参数化的方差sigma sigma平方的平方，并定义每个潜在变量的概率分布以及我们实际的方法，通过从这些缪斯定义的分布中采样来生成新的数据实例。

和sigma以生成潜在样本并获得潜在空间的概率表示，我希望您对此网络体系结构感到赞赏的是，它与，我之前介绍的自动编码器只是我们现在遇到的这种概率扭曲，执行采样操作以从每个潜在变量中计算样本，好吧。

因为我们已经将这种随机性引入了采样操作，对编码器网络的实际计算和学习过程进行建模，解码器的本质是概率，您可以这样认为，我们的编码器将要尝试学习潜在空间的概率分布，给定输入数据x的z。

而解码器将采用该学习的潜在表示，并根据给定的潜在分布z和这些来计算输入x的新概率分布，网络编码器，解码器将由权重phi和theta的单独集合定义，以及我们训练这种变型自动编码器的方式。

是通过定义一个损失函数，该函数将是数据x以及这些集合的函数，权重phi和theta以及如何优化vaes的关键是现在这个损失函数，由两个项而不是一个项组成，我们有一个重建损失，就像之前。

再次将捕获输入和重构输出之间的差异，也是损失的新术语，我们称其为正规化损失，也称为vae损失，并更详细地了解这些，损失术语代表我们首先要再次强调，我们的整体损失函数将是。

相对于编码器和解码器的权重集以及输入x定义和采用，重建损失与之前的损失非常相似，您可以认为它是由，对数似然对数似然函数，例如图像，数据输入和输出之间的均方误差，我们可以自我监督，重建损失就像以前一样。

迫使潜在空间学习和表现，对输入数据的忠实表示，最终导致忠实的重建，这里的新术语正则化术语更加有趣且完全，在这个阶段是新的，所以我们将深入探讨它，并进行更详细的讨论。

因此我们的概率分布将由x的z的编码器q phi计算，是给定数据x以及正则化强制执行的在潜在空间z上的分布，是该学习过程的一部分，我们将对潜在空间z进行先验。

这实际上是一些关于我们期望z分布实际的初始假设，并通过强加此正则化项，我们可以实现的是模型将，尝试强制执行它学会遵循此先前分发的zs并，我们先将其表示为z的p，这里d是正则化项。

它要做的是试图使差异最小化，或者，给定x时，编码器试图推断z的概率分布之间的差异，在此之前，我们将放置z的潜在变量p和这里的想法，是通过施加此正则化因子，我们可以尝试防止网络过度拟合。

通过加强以下事实来鼓励潜在空间的某些部分：变量以采用与我们之前的分布相似的分布，所以我们现在要经历，您既知道此正则化项的数学基础，又非常直观，逐步了解正则化可以帮助您具体化的内容。

关于正则化为什么重要以及为什么的理解和直观理解，放置先验很重要，因此让我们首先考虑嗯，所以再次强调一下，再次，这个正则化术语将考虑差异，在我们推断的潜在分布和我们要放置的固定先验之间，因此。

在开始讨论之前，我们先考虑一下每种情况下优先选择的好方法，这些潜在的uh变量中，我们该如何选择z的p？在社区中广泛使用的常见选择是强制执行潜在变量，大致遵循高斯正态分布，这意味着它们将成为正态分布。

分布以均值0为中心，且标准偏差和方差为1。这些潜变量的正态高斯先验，因此也就是我们的潜分布，总的来说，这鼓励了我们的编码器部分学习到的编码，vae将在每个潜在变量的中心周围均匀地分布。

如果您可以在图片中想象，当您在周围有大致均匀的分布时，潜在空间特定区域的中心，这意味着该区域之外的区域，距离较远的地方将受到更大的惩罚，这可能会导致实例中的实例，网络试图欺骗并试图聚集外部特定点的地方。

这些中心位于潜在空间中，就像它试图记住特定的，在将潜在的变量置于正态高斯之前，数据中的异常值或边缘情况，现在我们可以开始具体定义损失函数的正则项分量。

这个损失这个损失在原理上与我们看到的交叉熵损失非常相似，在关键点之前，我们将定义距离函数，该距离函数描述了，给定x时z的推断潜在分布q phi之间的差或或散度，以及我们将要放置z的p的先验条件。

这个术语称为kublac libor或kl，散度，当我们选择正态高斯之前，我们得到的结果就是kl散度，在这里采用等式的这种特殊形式，在这里我们使用均值和西格玛。

作为输入并计算该距离度量以捕获所学知识的差异，来自正常高斯的潜在变量分布，所以现在我真的很想，花一些时间来建立一些关于这种正则化的直觉，和工作原理，以及为什么我们实际上想规范自己的对战。

以及为什么我们选择一个正常的先验，好的，这样做吧，让我们考虑以下问题，我们希望通过正则化实现哪些属性，为什么我们实际上首先要规范我们的网络，这是第一个关键属性。

我们想要像vae这样的生成模型是我所能想到的连续性，这意味着，如果在潜在空间中存在紧密表示的点，它们也应该导致类似的重建，类似的输出，类似的内容在它们之后，解码后。

您会直观地期望潜在空间中的区域具有距离的概念，或彼此相似，这确实是我们想要实现的真正关键特性，在我们的生成模型中，第二个属性是完整性，它与连续性息息相关，这意味着当我们从潜在空间采样以解码潜在空间时。

转换成可以产生有意义的重构和有意义的采样内容的输出，那就是你知道的类似于原始数据分布，您可以想象，如果我们从潜在空间中采样而只是将垃圾清除掉，与我们的输入无关。

这可能对我们的模型来说是一个巨大的巨大问题，所以，考虑到这两个属性的连续性和完整性，我们来考虑一下，如果我们不进行正则化就不能很好地对模型进行正则化，将会发生什么？

关于这两个属性发生的事情是可能存在点的实例，在潜在空间中很近，但解码方式不一样，所以我在使用这种非常直观的方式，这些点表示潜在空间中抽象化的区域的示意图。

和它们相关的形状可以认为是那些形状之后将被解码的，潜在空间中的实例通过解码器传递，因此在此示例中，我们有，这两个点是绿色的点和红色的点，它们在潜在空间中物理上接近，但是，解码时会产生完全不同的形状。

我们也有一个紫色的实例，一点，当它被解码时不会产生有意义的内容，这只是乱涂乱画，所以，通过不进行正则化，我在这里抽象了很多，这是有目的的，我们可以，有这些实例，我们没有连续性，也没有完整性，因此。

我们进行正则化的目标是能够实现一个模型，其中，潜在空间中的close不仅被类似地解码，而且也被有意义地解码，因此对于，在这里的例子中，我们有一个红色的点和一个橙色的点，它们都产生三角形的形状。

在三角形本身上有一些细微的变化，所以这是关于什么的直觉，正则化可以使我们实现这些生成模型以及所需的属性，好吧，我们如何才能真正实现这种正则化？就像我提到的那样，他们不只是直接学习潜变量z。

试图将输入编码为均值和方差定义的分布，所以我的第一个，给您的问题是，仅仅学习均值和方差就足够了吗？发行版可以保证连续性和完整性否，让我们了解为什么，无需任何形式化就可以，模型可以尝试诉诸什么，请记住。

损失函数是由一个重建项定义的，和一个正则化项（如果没有正则化），您可以打赌该模型将继续进行，只是尝试优化该重建项，以便有效地学习如何最大程度地减少，即使我们通过均值和方差对潜变量进行编码。

也会造成重建损失，和两个实例的两个后果是，您可以拥有其中这些实例，潜在变量的学习方差最终非常非常小，有效地导致尖峰分布，并且您也可以拥有完全不同的方法，彼此之间会导致潜在空间的不连续，而这可能会在。

仍在尝试优化重建损失，即不进行正则化的直接后果，为了克服这些问题，我们需要对方差和均值进行正则化，编码器返回的这些分布以及正常的先前放置，正态高斯分布作为我们的先验可以帮助我们实现这一目标。

并了解为什么会这样，发生的事是，正常先验实际上会鼓励这些学习到的潜在变量，潜在空间中重叠的分布召回权是指零的方差，表示所有，所有潜在变量都将被强制尝试具有相同的均值，中心均值和。

他们所有的方差都将针对每个潜在变量进行正则化，分布，因此这将确保车道空间的平滑性和规则性以及重叠，这将非常有效地帮助我们实现连续性和完整性的这些特性，将均值居中以使方差正规化，因此。

通过将这些潜在变量中的每个变量居中，可以通过该正常先验进行正则化，正则化他们的差异是因为它有助于强制执行此连续和完整的操作，在潜在空间中表示的信息梯度，潜空间中的距离与重建有一定关系。

以及重建结果的内容说明，尽管要进行权衡，在正则化和重构之间，我们越正则化也存在遭受痛苦的风险。重建的质量和生成过程本身，因此在优化视线方面，进行权衡以适应感兴趣的问题，希望如此。

请仔细阅读本示例并考虑到您已经掌握的这些要点，对正则化为何如此重要以及正常先验的具体程度建立了更多的直觉。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_17.png)

可以帮助我们很好地进行正则化，所以现在我们定义了损失函数，我们知道我们可以重构，我们已经了解了如何使学习正规化并实现连续性和完整性的输入，通过这种正常的先验，这些都是定义通过网络的正向通过的所有组件。

从输入到编码再到解码重建，但是我们仍然缺少将整个画面放在一起的关键步骤，这就是向后传播，这里的关键是因为这个事实，引入了这种随机采样层之后，我们现在遇到了一个问题，即我们无法向后传播。

通过具有此随机性元素的采样层进行渐变，反向传播需要确定性节点确定性层，我们可以对其进行迭代，应用链式规则优化梯度通过梯度下降优化损失。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_19.png)

vaes提出了一种突破性的想法，解决了这个无法解决的问题，向后传播通过采样层，关键思想是实际上巧妙地，重新设置采样操作的参数，以便可以完全端到端地训练网络，因此，正如我们已经了解到的那样。

我们正在努力建立这种潜能，这些变量定义的分布z uh定义放置法线优先级，由均值和方差定义，我们不能简单地通过，采样层，因为我们无法通过此随机样本计算梯度，相反。

关键思想是尝试将采样的潜在矢量z作为由a定义的和。固定mu一个固定的sigma向量，并通过将要用的随机常数缩放该sigma向量，从先验分布（例如正态高斯）中提取出来，并通过重新参数化采样。

就目前的操作而言，我们仍然具有随机性这一要素，但是引入了随机性，通过这个没有在瓶颈内发生的随机常数epsilon，潜在层本身，我们已经重新参数化并将其分布在其他位置以可视化。

看起来让我们考虑以下原始形式的，否，我们有了这个确定性节点，​​即网络的权重以及输入，向量，我们试图通过随机采样节点z向后传播，但是我们现在无法通过重新参数化来实现此目的，我们已经实现了以下形式。

其中我们的潜变量z相对于，嗯，西格玛平方和这些噪声因子epsilon，这样当我们想做，通过网络反向传播进行更新，我们可以直接通过z定义的z反向传播，mu和sigma平方。

因为此epsilon值被当作常数重新设置了参数，在其他地方，这是一个非常非常强大的技巧，重新参数化技巧是因为，它使我们能够训练变分自动编码器，并以相对于z的反向传播结束，相对于实际梯度。

编码器网络的实际权重还可以。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_21.png)

将这些分布先验强加给潜在变量的一个副作用和后果，是我们实际上可以从这些潜在变量中采样并分别调整它们，同时保持，所有其他固定的变量，您可以做的是调整，一个特定的潜在变量，并在每次更改该变量时运行解码器。

每次扰动该变量以生成新的重构输出时，该结果的示例如下所示，其中潜变量的这种扰动，导致表示形式具有关于网络可能正在学习的内容的语义含义，因此，在此示例中，这些图像显示了头部姿势的变化，以及z的不同维度。

不同的潜在变量所在的潜在空间，这种方式编码不同的潜在特征，可以通过保留所有其他特征来解释，固定并扰乱一个单独车道变量的值的变量。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_23.png)

理想情况下，为了优化vas并尝试最大化我们想要的编码信息，这些彼此不相关的潜在变量有效地解开了，使我们能够实现的是学习可能的最丰富，最紧凑的潜在表示，所以在这种情况下，我们在x轴上有头部姿势。

在y轴上有微笑，我们希望这些姿势，尽可能彼此不相关的一种方法，我们可以实现这一目标。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_25.png)

实现这种解缠是相当简单的方法，称为beta vaes，因此，如果，我们再次考虑标准值的损失，我们用对数定义了这个重建项，由kl散度beta值定义的似然和正则项引入了新的。

超参数beta可以控制此正则化项的强度，并且已显示，从数学上讲，通过增加beta可以达到对潜在编码施加约束的作用，例如鼓励解开纠缠，并进行了广泛的论证和讨论，关于如何实现这一目标，但要考虑结果。

让我们再次考虑，如果我们考虑的潜在变量是使用标准vae的人脸重建问题，在这种情况下，头部姿势或旋转角度等于您希望得到的beta，是随着脸部姿势的改变，其中一些脸部的笑容也在改变。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_27.png)

相反，通过强制使用一个比一个大得多的beta，可以实现的是，当我们可以扰动单个潜在变量时，微笑保持相对恒定，旋转头的旋转，并获得相对于单独旋转头的摄动，好的。

正如我在本讲座的介绍开始时所激发和介绍的那样，生成模型和潜变量模型的强大应用在模型d偏向中，在今天的实验室中，您实际上将获得构建变体的真实动手经验。自动编码器，可用于实现面部分类系统的自动去偏。

检测系统以及此方法的功能和思想是建立表示形式，学习的人脸数据潜在分布，并使用它来识别该潜在空间的区域，这些将被过度代表或代表不足，而这一切将是，针对特定的学习特征（如肤色，姿势，物体，衣服）拍摄，然后。

从这些学习的分布中，我们实际上可以调整培训过程，以便我们可以，在这些图像和落入该区域的那些面孔上施加更大的权重和更大的采样，会自动显示下面的潜在空间，真正真正酷的是。

关于为模型d偏向等应用程序部署vae或潜在变量模型的问题是，无需我们注释和规定对于实际设计很重要的功能，反对模型自动学习它们，这将成为当今实验室的主题，同时也为更广阔的空间打开了大门。

该空间将被进一步探索，在后来的焦点演讲中，将重点讨论算法偏差和机器学习公平性，好的，这样就可以概括出vaes上的关键点，它们将数据的表示压缩为，数据输入的编码表示重构可实现无监督学习而无需，标签。

我们可以使用重新参数化技巧来训练血管端到端，我们可以采取隐藏的潜能，变量会扰乱它们以解释其内容和含义，最后我们可以进行采样，从潜在空间生成新示例，但是如果我们想专注于生成。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_29.png)

通常忠实于数据分布的样本和合成样本，可能了解我们如何实现这一目标，我们将过渡讨论，一种新型的生成模型，称为生成对抗网络或简称gam。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_31.png)

这里的想法是我们不想显式地建模密度或，或一些数据的基础分布，而只是学习可以，成功生成类似于数据的新实例，这意味着我们，想要从非常复杂的分布中进行优化以进行采样，而这是无法学习的，并且，直接建模。

我们将不得不建立该分布的近似值，gans真正酷而突破的想法是从极其极端的事物开始，非常简单，只是随机噪声，并尝试建立一个神经网络，生成神经，可以学习从噪声到数据分布的功能转换的网络。

通过学习这种功能性的生成映射，我们可以进行采样以生成假的，实例将与真实数据分布尽可能接近的合成实例。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_33.png)

可能实现这一目标的突破是称为gans的结构，其中的关键组件，有两个神经网络，一个发电机网络和一个鉴别器网络，它们是有效的，彼此竞争，它们是不利的领域，特别是我们有一个发电机网络，将在这里用g表示。

它将被训练为从随机噪声中产生出一个，模仿数据，然后判别器将把合成的假数据作为，以及真实数据，并经过训练以真正区分假冒和真实以及在训练中，这两个网络将彼此竞争，因此在竞争中，因此，总的来说。

结果就是歧视者将在学习如何，对真实和假冒进行分类，这样做会更好，它将迫使生成器执行以下操作：尝试产生越来越多的综合数据，试图来回欺骗鉴别器，来来回回，现在让我们分解一下，从一个非常简单的玩具示例开始。

以获得更多的直觉，关于这些gan的工作原理，发电机将从某些位置完全重新启动，随机噪声并产生虚假数据，我将在这里通过代表这些来表明这一点，数据作为一维线上的点，然后鉴别器将看到这些点，以及真实数据。

然后将其训练为输出以下概率：它所看到的数据是真实的，或者如果它们是伪造的，并且一开始它不会受到很好的训练，很好，所以它的预测不会很好，但是您要训练它，您将对其进行培训，它将开始增加利润概率。

真实与非真实的分离，这样您就可以得到完美的分离，鉴别器能够完美地区分真实的东西和假的东西，现在又回到了，生成器，生成器将返回，它将以实际，数据作为训练的输入，然后它将尝试改善对数据的模仿，以尝试。

将伪造数据移动到越来越接近真实数据的合成数据，歧视者现在又要获得这些新的观点，并估计这些点都是真实的概率，并再次学习降低假点成为真实的可能性，越来越远，现在我们将重复一遍，最后一次，发电机将。

开始将这些假点移动到越来越接近真实数据的位置，数据几乎跟随实际数据的分布，这将是，区分者很难有效地区分真实与虚假，而生成器将继续尝试创建伪造的数据实例以，愚弄歧视者。

这实际上是gans这两个组成部分背后的主要直觉，基本上可以互相竞争，所以总结一下我们如何训练礼服，生成器将尝试将伪造的实例合成为完整的鉴别器，接受培训以识别合成实例并将其区分为伪造的，进行实际训练。

我们将看到我们将定义一个损失函数，该函数定义了，鉴别器和生成器以及全球的竞争和对抗目标，最佳，我们可能会做的最好，这意味着发生器可以完美地再现，真实的数据分布，以使辨别器绝对无法分辨出什么是合成的。

与真实情况进行比较，让我们来看一下损失函数如何再次分解，损失项再次基于熟悉的交叉熵损失和，现在将在真实分布和生成分布之间进行定义，所以我们首先要从歧视者的角度来考虑损失。

我们想尝试最大化假数据被识别为假的概率，因此，在这里将其分解为z的g定义了生成器的输出，因此d， g的z是鉴别者对假实例实际上是假的概率的估计，x的d是判别器对真实实例为假的概率的估计。

所以x的一个减d是它对一个真实实例是真实的概率的估计，因此，从鉴别器的角度来看，我们希望最大程度地提高这种可能性，最大化伪造的概率是假的最大化伪造的概率估计是真实的，现在让我们把注意力转向发电机。

记住发电机，正在获取随机噪声并生成实例，它不能直接影响x的项d，出现在损失权中，因为x的d完全基于判别器在x上的运算，真实数据，因此对于生成器，生成器将具有对抗性目标，鉴别者。

这意味着将尝试最小化该术语，从而有效地最小化，鉴别者可以将其生成的数据区分为伪造品的可能性，z的g中的d，生成器的目标是最小化该目标项，因此，生成器的目的是尝试合成愚蠢的虚假实例，鉴别器。

并最终在训练鉴别器的过程中，会尽可能地区分真假，因此，生成器的最终目标是合成欺骗最佳区分器的伪造实例，并将它们全部合并到具有这两个组成部分的最小最大目标函数中，经过对抗性优化，然后经过训练。

我们可以实际使用发电机网络，经过全面培训，可以生成以前从未见过的新数据实例，因此我们将，现在专注于这一点，真正有趣的是，当gam的火车生成器合成时，新实例可以有效地学习从噪声分布到噪声分布的转变。

目标数据分布以及映射的转换将成为学习的内容，训练的过程，因此，如果我们从潜在的噪声分布中考虑一个点，在目标数据空间中产生特定的输出，如果我们考虑另一点，随机噪声通过发生器馈入它会导致一个新的实例。

新实例将落在数据流形上的其他位置，实际上我们可以。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_35.png)

do是在高斯噪声的空间中进行插值并进行遍历和遍历以导致插值，在目标空间中，您可以在此处看到此结果的示例，其中进行了一系列转换，反映遍历警报的遍历目标数据流形，该遍历是在。

发电机输出的综合示例可以正常运行，因此在最后的几分钟内。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_37.png)

在本讲座中，我将重点介绍gans的一些最新进展，并希望能激发人们的兴趣。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_39.png)

更进一步，为什么这种方法如此强大，所以一个想法极其强大，是渐进式gans渐进式增长的想法，这意味着我们可以迭代构建，生成的生成实例的更多细节，这是通过逐步添加来完成的。

在图像数据的情况下逐层增加空间分辨率的层，随着训练的进行，以这种方式建立生成器和鉴别器网络，产生非常好分辨的合成图像，最终由生成器输出。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_41.png)

所以这个渐进式甘的想法的一些结果在这里显示了另一个想法，这也大大改善了gans生成的合成示例的质量，是一种称为stylegan的体系结构改进，它结合了逐步增长的思想。

我之前介绍的风格转换原则意味着要撰写一个，像另一幅图像一样的图像，例如，我们现在可以实现的是映射，输入图像源使用从次要来源到这些来源的粗粒度样式的应用，目标生成模拟源b样式的新实例，结果是。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_43.png)

如此处所示，希望您能体会到这些粗粒度功能，像年龄的面部结构这样的样式可以反映在这些合成的例子中，相同样式的gan系统在两个方面都产生了非常逼真的合成图像，面部合成以及动物其他物体以及gan的另一个扩展。

架构已使特别强大的应用程序能够针对某些问题和，任务是这种条件的想法，它在类型上施加了一些额外的进一步结构，可以再次合成的输出，因此这里的想法是根据特定的标签，通过提供称为c的条件因子以及这允许的条件。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_45.png)

我们要实现的是像成对翻译这样的实例（在图片的情况下），综合，现在不再有任何输入作为生成器的训练数据，我们有成对的，输入，因此例如在这里我们既考虑了驾驶场景又考虑了相应的分段图，到那个驾驶场景。

然后可以对鉴别器进行训练，以对假对和真对进行分类，数据，然后将再次学习生成器，并对其进行训练以试图欺骗生成器。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_47.png)

这种想法的鉴别器示例应用如下所示，现在我们可以从，输入语义分割图以生成合成街道场景映射，该映射映射um，根据该细分，或者我们可以从卫星图像的鸟瞰图出发，到街道地图视图或从建筑的特定标签到合成。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_49.png)

建筑立面或白天到黑夜，为照片不同的边缘上色，通过以特定标签为条件来实现配对翻译的实例，所以我认为真的很酷又有趣的另一个例子是，谷歌街景到卫星视图，反之亦然，我们也可以动态实现。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_51.png)

因此，例如在给定边沿输入的着色中，可以训练网络以实际合成，由于这种特殊的边缘草图而在艺术品中产生的颜色是另一种想法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_53.png)

而不是成对翻译是不成对的图像到图像的翻译，这可以是，通过称为cyclegan的网络体系结构实现的，其中该模型将输入的图像作为，一个域，并且能够学习映射到另一个域的映射，而无需。

配对了另一个域中的对应图像，因此这里的想法是转移样式和，从一个域到另一个域的分布，这是通过引入循环，循环损耗函数中的关系，我们可以在域x之间来回切换，和一个域y，在这个系统中。

实际上有两个生成器和两个鉴别器，将要接受有关各自的生成和歧视任务的培训，在这个例子中，Cyclagan训练有素，可以尝试从马的领域翻译成，斑马的领域，希望您能体会到在此示例中。

马的皮肤从褐色到斑马状的条纹皮肤，除此之外，周围地区从绿草地变成更棕色的地方。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_55.png)

在斑马的情况下，我想对这种环骑如何获得直觉，转型正在进行中，让我们回到传统甘斯的想法，正在从高斯噪声的分布移动到一些目标数据流形，并具有循环gans，目标是从一个特定的数据流形x到另一个数据流形。

为什么，在两种情况下，以及，我认为使gans如此强大的基本概念是，它们的功能非常非常，有效的配电转换变压器，它可以实现这些配电转换。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_57.png)

最后，我想考虑一个您可能熟悉的其他应用程序，自行车的概念，这是要转换语音并实际使用这种心理游戏，合成别人声音中的语音的技术，并且这样做的方式是，通过在一个语音中录制一堆音频，并在另一个语音中录制音频。

将那些音频波形转换成图像规格表示，称为频谱图，然后，我们可以训练循环甘氨酸，以对这些光谱图图像进行操作以进行变换，声音a的表示，使它们看起来像它们的出现，来自另一种声音，这正是我们所做演讲的方式。

亚历山大在演示中进行的演示中，对奥巴马的声音进行合成的转换。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_59.png)

第一次讲课，以便进一步检查，让我们并排比较来自，亚历山大以及使用cyclegan生成的奥巴马声音中的合成版本，大家好，欢迎来到MIT 6s191官方授课的深度学习入门课程，在这里，请注意。

产生奥巴马声音的频谱图实际上是由，操作亚历山大的声音，并从奥巴马域中有效学习域转换，到亚历山大域的域上，最终结果是我们创建并合成了一些东西，那更像是奥巴马，所以希望在本讲座的过程中总结一下。

建立了对生成建模的理解以及生成模型的类别，尤其是，在实现概率密度估计和样本生成方面功能强大。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_61.png)

在此，我想结束演讲并向您介绍今天的其余部分，本课程将重点关注我们第二个计算机视觉实验室，专门探讨，这个在面部检测系统中消除偏见并使用变分的问题，自动编码器实际上可以实现分类系统自动去偏的方法。

所以我鼓励你来班上集镇问你的问题，在实验室里回答，并与我们中的任何一个进一步讨论谢谢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/512533c0d2e766b201730493800ee5a8_63.png)

# P5：Reinforcement Learning - 爱可可-爱生活 - BV1jo4y1d7R6

(音樂)，大家好，歡迎回到Success 191，今天是個非常刺激的一天，因為我們要學習如何結合，我們近期的進步，和我們近期的進步，在深入學習中的進步，如何結合這兩個領域，來建立一些非常奇妙的應用。

以及能夠超人性的表現。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_1.png)

我覺得這個領域非常神奇，因為它離開了我們近期的，這個學習的原則，我們近期的深入學習，我們所看到的，已經被困在修正數據，我們可以收集或在網上獲得，在強化學習中，深入學習是放置在某個環境中，能夠探索和互動。

在那環境中，能夠學習如何達成目標，通常都是沒有人性監督或指導，這使得它非常強大，也非常靈活。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_3.png)

這對於機械人類，自動駕駛車，和機械人類操作，有著明顯的影響，但它也讓遊戲玩意，和策略計劃發展得非常革新，這就是真實世界和深入學習，即是實際世界之間的連結，這讓我非常興奮，我希望我接下來要展示的影片。

也能夠展示出這一點，我希望我接下來要展示出這一點，星際大戰有完美的資訊，而且可以在現實中遊玩，它還需要長期計劃，以及能夠選擇，從萬萬個可能性中取得什麼行動，我希望5-0不輸掉任何一場比賽。

但我認為最真實的目標，是4-1，他看起來比TLO更有自信，TLO之前還挺緊張的，這次房間更繁忙，他真的不知道要期待什麼，他已經在玩星際大戰，大概是五年了，我並沒有預期AI會這麼好，他做的一切都很正確。

計算過後都做得很好，我認為我在學習一些東西，這比我想像中的好，我會覺得自己是個好玩家，但我每次都輸掉了五場比賽，(掌聲)，(鈴聲)，好的，這就是TLO的例子，是如何在人類上競爭，專業的遊戲玩家。

並不是只在他們身上競爭，而是能夠達到，不得不說的超人性表現，打敗這位專業的星際大戰玩家，五場比賽，我們先從一步步走回來，看看如何在其他學習問題中，能夠達到強化學習，我們在這課程中，所見到的各種學習問題。

我們在這課程中，所探索到的最初和最具體的，學習問題，就是在學習問題中，能夠超越專業的學習，我們在第一，第二，第三堂課中，談到的這些問題，我們在這領域中，基本上是被給予了一堆數據，我們嘗試了解一條網絡。

以預測它的標籤，Y，目的就是從X到Y，從X到Y，我喜歡用很主觀的方式，來形容，如果我給你，例如說我給你一張蘋果的圖片，我想訓練一條網絡，以確定這東西是蘋果，好，我們在上一堂課中，學到的下一個類型的。

系統是不超過學習，在這情況下，我們只能夠，給予數據，沒有標籤，例如說一堆蘋果圖片，我們被迫要學習一條網絡，或是學習一種模式，代表了這一個，在數據系統的構造下的東西，所以在蘋果的情況下。

我們嘗試了解一種模式，如果我們把這兩張蘋果圖片，給我們看起來，這兩個模式，基本上是相同的，我們不知道它們是蘋果，因為我們沒有給予任何標籤，可以明顯地告訴模式，這東西是蘋果，但我們可以告訴它。

這東西也很接近，這東西，它也看到了，我們可以選擇這兩個，相同的構造來辨識，最後一部分，在RL，在強化學習中，這是今天課程，要專注的地方，我們只能夠給予數據，我們稱之為，"數位行動對比"。

數位是系統的觀察，行動是系統的行為，或是系統的行為，當它看到這些狀況時，RL的目標，與超級學習，和超級學習不同，RL的目標，是要達到，系統在這個環境中，在許多時間過程中，獲得的獎勵，再回到蘋果的例子。

對比的情況，系統應該學會，它應該吃這個東西，因為它知道，它會保持你活著，會讓你健康，你需要食物來生存，再次說，就像不超過的情況，它不知道，這個東西是蘋果，它甚至不認識，它是什麼，它只知道，在過去。

它應該吃過，然後能夠長久地生存，因為它是一塊食物，它能夠，變得健康，例如，通過這些系統的行動對比，和一些試驗和錯誤，它能夠學到這些代表，和學習這些計劃，所以今天我們要專注的，是關於第三類的學習問題。

和強化學習，所以要做到這一點，我覺得非常重要，在我們開始探索細節，和技術細節的時候，我覺得我們需要建立一些，關鍵的語言，這是非常重要的，在強化學習中，而且這會非常重要，讓我們在後面的課程中。

能夠了解到這些點，這是課程中非常重要的一部分，所以我希望我們可以，慢慢地走到這部分，讓我們在課程的最後，能夠做到最多的理解，我們開始從中心，從中心，從中心開始，來看強化學習的核心，那就是你的代表。

代表是一個，能夠在環境中做出行動的東西，它可以像是飛機，在世界上做出運輸，就像是Mario在遊戲中旋轉，強化學習中的手段，是你的代表，你可以說在現實生活中，代表是你每一個人，好嗎，下一個部分是環境。

環境只是代表，代表在生活中的世界，代表在生活中的存在，在生活中的行動，在生活中的行動，這就是兩者之間的連結，代表可以在環境中，做出行動的東西，A(t)，是在時間t，這個環境中，代表的行動，我們可以將A。

定義為行動空間，這是一個，所有的行動，一個代表可以做到的，我想說這個，雖然我覺得，有點自我解釋，行動是，或是，一個行動的列表，所有的行動，一個代表可以做到的，在環境中可以是隱密的，或是從一個行動的列表。

在這個情況下我們可以看到行動是，前進、右、後、或是，或是持續的行動，例如，環境的位置，例如是行動的數字位置，例如GPS的位置，這個代表想要去哪裡，它可以是隱密的，或是一個類似的，可能性列表，或是持續的。

在這兩種情況下，觀察，是環境，與代表的交互，代表可以觀察，環境的位置，以及它的行動，影響環境的狀態，這讓我非常喜歡，這一點，狀態其實是，一個具體的、即時的情況，當代表，找到自己的時候，例如，狀態可以是。

像是一個視覺的影像，這是你觀察的世界狀態，當你觀察它的時候，獎勵，現在也是一個，從環境的，回饋給代表，環境可以提供，回饋，來測試，代表的成功或失敗，例如，在一個遊戲中，當瑪利歐碰到一張硬幣。

代表獲得獎勵，從一個狀態下，代表會傳出，回饋，給環境，環境會回應，代表的新狀態，它能夠達到的，這會影響，該狀態，和獎勵，被收集或被扣，現在，很重要的一點，獎勵可以是，即時或延遲，基本上，你應該。

以獎勵來評估，代表的行為，但你可能不會得到獎勵，直到很晚，例如，你可能會，做很多不同的行為，然後獲得獎勵，很長一段時間，這叫做延遲獎勵，但它還是獎勵，我們也可以看看，總獎勵，這是所有獎勵的總數。

一個代表會，收集或取得，在某個時間，R(i)是獎勵在時間，R(t)，是回饋，總獎勵，從時間T，到未來，到時空無限，這可以寫出來，我們可以寫出，總數，從R(t)，到未來，所以它是將所有獎勵，加起來。

從現在到未來，然而，很常會，不僅是，總獎勵，總獎勵，而是，我們稱為償還獎勵，償還獎勵，償還獎勵，代表，Gamma，是將未來獎勵，被發現的獎勵，加起來，將償還獎勵的影響，影響到獎勵的選擇。

為什麼我們要做這個，這個方案，是由設計，來製造的，將未來獎勵，的償還獎勵，來製造，償還獎勵，的影響，我們可以，在這個方案中，將償還獎勵，加起來，將，償還獎勵，加起來，將，償還獎勵，加起來，將，償還獎勵。

加起來，償還獎勵，將，未來獎勵，被發現的獎勵，加起來，將，未來獎勵，加起來。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_5.png)

將，未來獎勵，加起來，將，將很多這些元素，加起來，這叫做Q形式，現在我們來看看，這個Q形式的定義，記住，這個償還獎勵的，總數R(t)，記住，總數R(t)，是獎勵的償還數，從，T的時間而來，現在Q形式。

是非常相似的，Q形式是一個，償還獎勵的，現時的狀態，以及當中的，狀態的行動，然後它回報，當中的，獎勵，所以，假設，當中的狀態，當中的狀態，當中的，回報，當中的，假設，我給你，這個魔法Q形式。

這其實是一個魔法功能，因為它告訴我們很多，問題的事實，如果我給你這個功能，一個Oracle，你可以連接任何狀態和行動的對比，它就會告訴你，從你的現時時間點T回報的期望，我給你這個功能，問題是。

你能否決定，你現在的狀態，最好的行動是什麼，你可以在這個功能上，進行任何的檢查，你可以這樣做，就是，你最終，想要選擇，最好的行動，但最好的行動是什麼，就是最好的回報，最高期望的回報，所以你只需要。

選擇一個最好回報的，最好的回報，那這個回報的期望，可以簡單地寫成，找到你的Q形式的ArgMax，對應所有可能的行動，在這個狀態下，簡單來說，如果我給你這個Q形式，在這個狀態下，你可以將你的狀態。

連接每一個行動，然後評估Q形式會告訴你，期望的回報，在這個狀態下，你選擇最高的Q形式，這是最好的行動，在這個狀態下，你可以建立一個政策，我們稱之為Pi(s)，來定義最好的行動，現在想像你的政策。

是一個新的功能，它會帶給你一個狀態，告訴你該做的行動，在這個狀態下，所以這個策略，給你一個Q形式來計算，你的政策，是從這個ArgMax的形式。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_7.png)

來定義你的Q形式，在這個課程中，我們將專注於，這兩個類型的，強化學習方法，分成兩個類型，其中一個會嘗試，學習這個Q形式，Q(s)，你的狀態和行動，另一個會叫做，政策學習方法，因為他們嘗試直接學習政策。

而不是用Q形式來定義政策，我們將在政策學習中，直接定義你的政策，Pi(s)，來定義該做的行動，這是一個更直接的思考方式，但首先我們將專注於，價值學習問題，以及我們可以做的Q學習。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_9.png)

然後我們會在之後，來建立政策學習，我們先來深入研究，這個Q形式，首先我會介紹這個遊戲，在左邊是Atari Breakout，如果大家還沒看過，我會介紹一下這個遊戲的運作，這個Q值告訴我們。

我們可以預期的，回報率，在任何一個狀態，這是一個例子，在這個遊戲中，你的代理人，是這個桿子，它可以移動左或右，它有兩個動作，它也可以一直在同一個位置，所以它有三個動作，在同一個環境中，還有一個球。

它在桿子的下方，它將會擊中，然後從桿子上跳，目標，這個遊戲的目標，是移動桿子，然後擊中桿子，在最好的時間，你可以把它跳出去，擊中和破壞所有顏色的牆，每當桿子，碰到一個顏色的牆，你必須把它破壞。

因此遊戲的名字叫做破壞，目標是，擊中這些顏色，每當桿子碰到一個顏色，它就消失了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_11.png)

你必須一直移動，擊中桿子，Q值告訴我們，我們可以預期的回報率，在任何一個狀態，在任何一個狀態，我想要說的一點，其實有時候，我們需要用心去理解，或是用理解的方式，來猜測，一個狀態的Q值，如果我給你。

兩個狀態的對比，A和B，我會問你，哪一個對比，你覺得有更高的Q值，A的對比，我們可以看到，桿子已經在移動，桿子已經在移動，我們可以看到桿子在移動，桿子已經在移動，所以擊中了，我會問你，哪兩個對比。

你覺得會回報，最高的回報率，回報率，在我給你答案之前，我想告訴你，這兩個對比的政策，在遊戲中的樣子，而不是在看一個對比。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_13.png)

我們先來看看A的對比，A的對比是，一個比較保守的選項，它不會移動當桿子，移動向著它，我們可以看到，當它在遊戲中，它開始擊中，很多對比的部分，向著遊戲中心，它其實做得很好，它擊中了很多，顏色的對比。

但我們來看看，B的對比，B的對比其實有一點很有趣，它喜歡在桿子的角落，擊中桿子，它做這個，讓桿子可以在極端角度，在桿子的角落，擊中顏色，現在，它其實會做到極端，因為，即使桿子正朝著它，它也會移動。

讓它能夠在極端角度，擊中桿子，我們來看看，B的對比，當它在遊戲中，我們可以看到，它在擊中桿子的對比，擊中了很多顏色的對比，為什麼呢？因為當它擊中了桿子的角落，它能夠擊中很多對比，因為它能夠，在極端角度。

擊中它，當它在極端角度，它不需要擔心，因為它正在累積很多獎勵，這是一個很好的學習方式，因為它能夠，擊中遊戲，比A更快，而且也更少努力，所以回答問題，哪個對比的桿子，有更高的Q值，這就是B的對比。

但這對我來說，是一個相對不太理想的選擇，因為我預期，當桿子正朝著你，它會做出更好的行動，但這個對比，它學會了離桿子遠一點，讓它能夠回擊，並且在極端角度，擊中它，這是一個非常有趣的觀察，這對比學習。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_15.png)

但問題是，因為Q值很難定義，人類很難定義，正如我們之前看到的，人類不可以定義Q值，而是我們可以用，深層面網絡，來模擬這個功能，並學習它，在這個情況下，Q值是一個功能，讓我們可以定義，一個深層面網絡。

能夠得到，它所定義的，狀態和它所想做的行動，然後網絡會被訓練，來預測Q值，這只是一個數字，但問題是，我們需要在時間內，去進行進行，因為我們需要，計算這個模式的政策，如果我們想預測，這個狀態下。

最好的行動，我們需要，研究這個深層面網絡，N次，N是它能夠做出的，最多的行動，這意味著，我們需要，研究這個深層面網絡，來學習，這個模式的，狀態，而不是，研究它，的最多的行動，而是，研究它，最多的，狀態。

而不是，研究它，最多的，我們會在最後，所有檔案的最大數值檔案2到最大數值檔案3 Thrones，所以做完所有的檔案之後，根據現在你的狀態，你將得出，整個深入訊號的最佳狀態。

以及充分將收穫的檔案數據來解決的結果。以這些不同的檔案數據為例。那麼，如何讓我們來訓練這個深入訊號的軟件？我們知道我們想用這些隱藏的訊號來訓練，我們稱之為訊號值，但我們還不清楚如何訓練它。

而要做到這一點，這其實是很困難的，你可以理解為我們沒有一個訊號值的數據序列，我們只有觀察，狀態，行動，獎勵，和三分之一。所以要做到這一點，要訓練這種深入訊號的網絡，我們必須考慮，最佳的情況是什麼？

如果一個人能夠做得最好，或最佳的表現，那麼最佳的表現會是什麼樣的結果？這意味著，目標的回報會被最佳化。我們可以在這種情況下，利用這個目標的回報來作為我們的基礎，我們的數據序列，來訓練這個深入訊號的軟件。

那麼，首先，我們會形成我們預期的回報，如果我們能夠做到最佳的表現，最佳的回報是，加上我們選擇的行動，最佳化的回報，對於未來的狀態，然後我們再應用這個減少因素，所以這是我們的目標，這是我們要嘗試。

去達到的目標，就像我們嘗試，去適應，我們想要的預測，適應。但我們現在應該問，我們的網絡預測是什麼？我們的網絡預測，就像我們在這個網絡中看到的，這個網絡預測是我們對於一個狀態的行動對比的Q值。那麼。

我們可以利用這兩塊資訊，我們的預測Q值，以及我們的目標Q值，來訓練，來製造我們稱之為Q-loss。這基本上是一個中間數值錯誤的表現，在我們目標和預測的Q值之間。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_17.png)

我們可以利用這些來訓練這個深入訊號的網絡。所以，總結一下，我們走進深入訊號的訓練，從頭到尾。我們的深入訊號，看到我們在進行訓練的狀態，然後，這個狀態被傳送到網絡中，然後我們嘗試將訊號的Q值。

分配為每個三個可能的行動。在這裡，我們可以在三個方式，讓網絡運作。我們可以移動到左邊，移動到右邊，或者我們可以保持在同一個位置。現在，為了確定最佳的政策，我們必須看一看每個Q值。在這個情況下。

移動到左邊，因為它看到球在移動到左邊，它看到，如果我移動到左邊一點，我可能有更高的機率擊中那球，然後繼續進行訓練。所以我的Q值，在我預計的總結，我的Q值，移動到左邊是20。反過來。

如果我保持在同一個位置，我們說，我有3的Q值，如果我移動到右邊，離球的方向遠一點，在這個情況下，因為球已經移動向我，我會有0的Q值。所以這些都是我的Q值，對於所有的可能行動。我如何計算最佳政策？

我們之前看到，最佳政策是從最佳的Q值，選擇最佳的行動來得到的。在這個情況下，我們可以看到最佳的Q值，是移動到左邊的行動1。所以我們選擇行動1，然後把這個回到遊戲引擎，然後回到環境。

然後我們會得到下一個狀態。這個過程會重複，下一個狀態會被傳送到深度智能網，我們會得到一個Q值列表，每一個可能行動的Q值，然後重複。現在，DeepMind 顯示，這些深度智能網，可以應用來解決。

一系列不同的 Atari 遊戲，不僅是 Breakout，還有很多其他的遊戲。基本上，他們只需要，以圖形來提供狀態，作為輸出，通過這些曲線層，並且由無線化和圖形運行，就像我們在第三課上學到的那樣。

在右邊，它在預測這些可能行動的，Q值。就像我們在上幾個示範中所看到的一樣，它會選擇最佳的行動來行動，在下一步，視乎最佳的行動的 Q 值，能夠達到的。然後，它會把它送回環境，來行動，並且得到下一步的狀態。

這其實非常簡單，因為儘管我認為，這非常簡單，基本上是試驗和失敗，他們在 Atari 上測試了很多遊戲，顯示了，在超過 50% 的遊戲中，他們能夠超越，人類級的表現，使用這個技術。而其他遊戲。

你可以看到的，在右邊的示範中，更加有挑戰性，但依然，再次說，就這技術的簡單性，以及它的清潔性，我認為，這對我來說是一件很棒的事。所以，儘管所有的優點，如同它的簡單性，清潔性，以及它的優雅性，我認為。

我指的是，能夠讓這個解決方案學習超人類的政策，政策可以打敗人類，甚至在一些相對簡單的任務中，有些非常重要的缺點，對於 Q 學習。首先，我們今天學到的簡單模式，這個模式只能夠處理，隱密的行動空間。

它只能夠處理，小的行動空間，我們只能夠進行，幾個可能的行動，它無法處理，持續的行動空間，所以，如果一輛自動車，想要預測它要去哪個地方，而不是預測，要去左或右或直，這些都是隱密的類別。

我們如何使用強制學習，來學習持續的行動空間？一個不是，分成盒子，但可以從某個區域，取得任何數字，來學習它能夠處理的行動空間。這是一個持續的系統，它有無限的空間，而在 Q 學習的版本中。

我們在這堂課中展示了它。它學習的彈性，也有些限制，因為它無法學習，能夠變得，複雜的政策，能夠根據，一些無可預測的，可能性分配來改變。所以，它是由 Q 系統，通過最大化的系統來計算的。

它總是會選擇最大化的，回報的行動，所以它無法從這些，複雜的政策學習。另一方面，我們將在今天課的，下一個階段，專注於政策陰影的方式，希望能夠解決。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_19.png)

這些問題。我們進入課程中，我們看到的第一部分，和第二部分的差別，在價值學習中，我們嘗試用一條 neural network，來學習我們的 Q 價值，我們對行動的 Q 狀態。然後我們用這個 Q 價值。

來定義我們在一個狀態下，要做出最好的行動。這是我們的政策。政策學習是有點不同的，它嘗試直接學習政策，利用我們的 neural network，所以它進入一個狀態，然後嘗試直接學習政策，這會告訴我們。

我們應該做什麼行動。這比較簡單，因為這意味著我們現在，可以免費的學習行動，也就是從政策學習中學習的，政策狀態來測試。我們現在來看看，政策學習的細節如何運作。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_21.png)

首先我想要，從 Q 學習中，從 Q 學習中推進這個差別，因為這有一點差別，但是這對我們來說是非常重要的差別。深入 Q 網絡，想要用 Q 狀態來定義，首先，我們要預測，在某一個狀態下。

對每一個行動的 Q 價值，然後它會選擇最好的行動，在這裡最好的是由，哪個行動給予你最大 Q 價值，最大預期的回報，然後執行這個行動。政策學習中學習的關鍵思維，是在預測 Q 價值時，我們會直接對。

政策的 P 和 S 進行最佳化。所以這是政策的分配，直接控制我們該如何行動，在我們所在的現狀狀態下。所以這裡的輸出，是為了讓我們，給予我們想要的行動，在更直接的方式。輸出代表了可能性。

我們要檢驗或選擇的行動，應該是我們在這個步驟中，所做的正確行動。也就是說，它會給予我們最大的回報。所以，例如說，如果我們看到，我們預測這些可能性，這些行動的最佳行動，我們會得到一個數據，我們的政策網絡。

正在預測可能性分配，我們可以將它們分配到我們的政策中，我們可以說我們的政策，是由這個可能性分配定義的。然後來計算我們該做的行動，我們只需要從這個分配中，來做一個測試，來預測我們該做的行動。在這個情況下。

是車子往左走，就是 A1。但是，因為這是一個可能性分配，下一次我們測試，我們可能會，我們可能會在同一個位置，我們可能會測試 A2，例如說，因為它有一個非零的可能性，一個 0。1 的可能性。現在注意。

因為這是一個可能性分配，這個 P 行動的 P 給予我們的政策，必須是 1。現在，這些形式的優點是什麼？首先，我們看到的那樣，除了它是一個，更直接的方式來得到我們想要的，而不是在 Q 行動中。

使用 Q 行動來建立我們的政策，現在我們將直接地，去更改政策。除此之外，還有一個非常重要的優點，在這個形式之中，那就是它能夠，處理持續的行動空間。所以，這是一個具體的行動空間的例子。

我們在這次 Atari 遊戲中，所做的事情，是左轉，還是右轉，還是站在中間。這三個行動是具體的，這裡有一個具體的行動數字，可以被取代。比方說，這顯示了，我們的行動空間，是代表著，我應該移動的方向。

但不然，一個持續的行動空間，不僅會告訴我們方向，而是，比如說，我應該移動的真實數字是多快。這樣的問題，在數字中的可能答案中，是無限的，這可能是 1 秒左轉，半秒左轉，或任何數字速度。它也會告訴我們。

通常的方向，是通過一個加或是下方的數字。所以，如果我說，-1秒左轉，這告訴我，我應該移動到左邊，在 1 秒左轉。如果我說 +1，這告訴我，我應該移動到右邊，在 1 秒左轉。但是現在，當我們計算這個。

為一個可能性分布，我們也可以視覺到，這是一個持續的行動空間，簡單來說，我們可以視覺到，這個是一個，像是一個 Gaussian 分布，在這個情況下，但是它可以有很多種分布，你可以選擇，最適合你的問題。

分布系統。Gaussian 是一個很受歡迎的選擇，因為它的簡單性。所以，這裡，我們可以看到，移動到左邊的可能性，移動得更快的，是比移動得更快的，移動得更快的，而我們可以看到，這個分布的中心，平均的。

這個平均分布的高度，告訴我們，它應該移動得多快的，數字值，不僅是移動得多快，但是移動得多快，是多快的。現在，我們來看看，我們如何模擬，這些持續的行動空間，用一個 Policy Gradient 方式。

而不是預測，可能性的行動，在一個可能的狀態下，在這個情況下，因為我們在持續的範圍內，會有無數的行動，我們假設，我們的分布分布，其實是一個正常的 Gaussian，並將分布一個。

對於這個 Gaussian 的，平均和相反的分布，然後我們只有兩個分布，但是它能夠讓我們，描述這個可能性分布，在整個持續空間上，否則的話，它會是無數的，無數的數量的分布，所以在這個情況下，如果我們預測。

我們應該要做的，這個平均行動，是 -1，而這個差異是 0。5，我們可以看到，這個可能性分布，在左下方的，下方看起來是這樣的，它應該移動到左邊，以 -1 公里/秒的，平均速度，並且有一定的差異。

所以我們並不確定，它應該移動到左邊，以最好的速度，但是我們認為，它應該移動到左邊，所以在這個圖片中，我們可以看到，平均行動應該移到左邊，如果我們描述，這個分布是這樣的，我們可以看到，這個分布的重量。

是在數字線的，左邊，如果我們從這個分布，來做一個測試，我們可以看到，在這個情況下，我們得到的是，我們需要做的行動，在實際的速度，我們應該要做的行動，是我們需要移動，左邊 -1，以 0。

8 公里/秒的速度，所以這意味著，我們移動到左邊，以 0。8 公里/秒的速度，注意到，即使這個分布的，平均速度是 -1，我們並不被限制，在那個數字線，這是一個，持續的可能性分布，所以我們在這裡。

做一個測試，那並不是完全的，但這完全沒問題，這真的顯示了，在這個區別之間，在這個平均行動空間，和持續行動空間，這開放了很多，可能性，在我們做的應用，模擬無數的行動，再一次，就像之前。

就像這個平均行動情況，這個可能性分布，仍然有所有，可能性分布的優點，就是，這個計算的，可能性分布仍然有，1：1的可能性，所以我們可以，從中檢測，這是一個非常好的，確認能力，好，很好，那我們現在來看看。

如何這個，政策陰影的，系統，在一個具體的例子中運作，我們先來重新討論，這個整個學習循環，重新學習強制，我們在這課的，最初時看到的，我們來想想，我們可以如何使用，政策陰影的系統，我們已經介紹過。

如何訓練一個自動車，使用這個，判斷錯誤的，政策陰影的方式，所以在這個例子中，我們研究自動車，或自動車，這些元素是什麼，所以，機器人，就是我們的車，它在環境中旅行，就是，這個世界，這個路徑，它在旅行。

它有一個狀態，它是通過攝像數據，雷達數據，雷達數據等等，它得到，抱歉，它做出行動，它可以做出什麼行動，在這個情況下，行動就是，方向角角，再一次，這是一個具體的例子，一個持續的行動空間，你不需要分解。

方向角角，成為獨特的盒子，你的方向角角，是無限的，在它可以做出的，數量的可能性，它可以做出，任何的數量，在某些範圍之間，所以這是一個持續的，這是一個持續的，變數，我們在這次行動中，學習到的一些知識。

最後它得到獎勵，在距離它可以旅行前，它需要一些人類的應對，讓我們進入深入的範圍，現在我們已經，找到了所有的資料，我們如何訓練這輛車，使用 Policy-Gradient Network，在這個情況下。

我們以自動駕駛車為例，但你希望看到我們只使用這輛車，因為它比較有趣和有趣，但這也會應用到，任何一個範圍，你能夠認出，並設置問題，就像我們目前設置的問題，我們開始，從事者開始，事者是車。

我們可以把它放在路上，在路中心，下一個步驟是，讓事者開動，一開始它不太好開動，因為它會塌陷，而且它從未被訓練過，所以我們不期望它開動得很好，但這還好，因為這是強化學習，所以我們使用這個政策，直到它完結。

在這個情況下，我們會在它塌陷，或需要被訓練過的時間，標記結束，在我們稱之為，Rollout 時，我們開始錄製，所有的 State Action Pairs，抱歉。

State Action Reward Pairs，所以在每一個步驟，我們會錄製，機械人在哪裡，它在什麼狀態，它在什麼情況下行動，以及它在什麼狀態下，得到的回報，接下來的步驟，就是要把所有的。

State Action Reward Pairs，並且減少可能性，在它接受任何行動時，接近結束時的時間，所以接近崩塌時的時間，我們想減少可能性，在未來再做任何行動，同樣地，我們想增加可能性。

在開始做任何行動時，的時間，注意，我們並不確定，在第一部分的時間，有什麼好事，我們只是想像，因為崩塌在，第二部分的時間，這可能是因為，第二部分的行動，這是一個很不智慧的，系統，因為，這就是我們想像的。

它只是想要，減少可能性，在低回報時，增加可能性，在高回報時，它並不確定，哪個行動比其他更好，尤其是在開始時，因為它沒有這樣的，回報，這只是說，我們想減少，什麼不好的事，增加什麼好的事，如果我們再做。

我們可以看到，下一次，車子開了，開了一段時間，如果我們再做，我們做同樣的事，現在在這個轉變，我們減少可能性，在低回報時，增加可能性，在高回報時，我們重新開始，然後在完成時，再更新程序，再一次。

它看起來開了一段時間，我們可以再做，我們繼續做，直到它學會，跟著行車道，不墜車，我覺得這很棒，因為我們從來沒有教過這台車，什麼，我們從來沒有教過，什麼是行車道，我們從來沒有教過，什麼是行車標。

但是它學會避開行車道，避開崩潰，只要觀察，很微妙的崩潰獎勵，它觀察了很多崩潰，然後學會說，好的，我不會做任何，這些行為，發生在我崩潰的附近，只要觀察這些事，它成功避開行車道，並且在這個環境中。

長時間活下去，現在剩下的問題，是我們如何更新，我們的政策，在每個訓練的一段時間，來減少壞事件的可能性，並增加這些好的事件的可能性，或者說這些好的行為，這讓我們集中在，這項訓練的學習方法中的點4和5。

我們如何進行，學習過程，減少壞事件的可能性，並增加他們的可能性，我們來看一看，更詳細的部分，我們來看，特定的失誤功能，訓練政策的階段，然後我們會分析，為何這項功能有效，所以這個失誤，涉及到兩部分。

我想深入研究，第一項是，Log-likelihood，Log-likelihood，是我們的政策，是我們的行動的可能性，以我們的國家為例，第二項是，我們將這個，Log-likelihood。

分成總的優惠，總的優惠，抱歉，R(t)，假設我們得到很多優惠，對一個有很高的Log-likelihood，這個損失會很大，並且會強化這些行動，因為他們造成了非常好的回報，另一方面，如果優惠對一個。

有很高的可能性，它會調整這些可能性，以免將來，這個行動再次被檢驗，因為它並沒有造成，可望的回報，所以當我們插入，這個損失，到Gradient Descent Algorithm，來訓練我們的網絡。

我們可以看到，這個Policy Gradient Term，是在藍色的位置顯示的，就是這個，這個Algorithm的名字，因為它需要計算這個，Gradient 和這個Policy，這個功能的一部分。

再次強調，這個Policy Gradient Term，是由這兩部分結合的，第一是行動的可能性，第二是回報，如果行動非常正面，非常好，造成了好的回報，它會透過這個Gradient Term，去強化它。

如果行動非常可望，抱歉，不是非常可望，但它造成了好的回報，它會更加強化，所以一些之前不可能的事情，會成為可望，因為它造成了好的回報，相反的，另一方面也一樣，我想談談，我們如何可以延伸。

這些強化學習的 Algorithm，到現實生活，這是一個非常有挑戰性的問題，因為這是一個，對強化學習領域的，特別的興趣，現在，特別是現在，因為在現實世界，應用這些 Algorithm，是非常困難的。

有一個原因，或是一個主要的原因，就是這個步驟，從行政到結束，這是我提到的一個問題，但我沒有花太多時間，去分析，為什麼這麼困難，在現實世界，結束意味著，塌陷、死亡，通常都是很壞的事情，我們通常可以。

通過訓練和測試，但問題是，現代的測試器，不確定地描述現實世界，更何況，它們不能轉移到現實世界，當你使用它們，所以如果你在測試中訓練一些東西，它會在測試中有效，會在測試中有效，但當你，想要把這個政策。

轉移到現實世界，它並不太有效，現在，我們在我的實驗室，創造了一個非常酷的結果，就是在發展一台，全新的相機實驗機，特別是為了自駕駛的車，我想和大家分享，它是完全數據主流，並且能夠讓這些。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_23.png)

強化學習的進步，在現實世界，我們創造了一個非常酷的結果，就是在發展一台，這樣的相機實驗機，叫做Vista，它能夠讓我們使用，現實世界的數據，來模擬全新的，實體代理人在實驗中，現在，結果非常，相機實驗的。

如同你們所看到的，它能夠讓我們訓練代理人，使用強化學習，在實驗中，使用我們今天看到的方式，讓他們可以直接進行運作，沒有任何的交換學習，或是領域應用，直接進入現實世界，事實上，我們做了這個。

我們把代理人放進了，我們的模擬實驗機，訓練他們，使用了我們在這次的講座中，學到的同樣的，政策定律系統，所有的訓練，都在我們的模擬實驗機中，然後我們把這些政策，放在我們全面的，自駕駛車上，如同你們看到的。

而在我左手邊的，你們可以看到我，坐在這個車上，在車內的下方，你們可以看到我，坐在這個車上，當它駕駛到，現實世界時，完全自動，這代表了第一次，在我們發布這些結果時，第一次在自動車上，訓練人，使用RL。

完全在模擬實驗，並且能夠在現實世界，運作，這是一個非常棒的結果，所以現在我們已經，探索了價值學習的基礎，以及政策定律，強化學習的方式，我覺得現在，我們需要探索一些，我們最近看到的，非常出色的。

深入強化學習應用。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_25.png)

我們首先來看看，Go的遊戲，強化學習人員，被對人類冠軍，實現了當時的，仍然非常刺激的結果，首先我想先介紹一下，Go的遊戲，這是一個19x19的，範圍遊戲，遊戲由兩名玩家一起玩，他們是白色的，或黑色的。

遊戲的目標是，與對手佔用更多的，地區，雖然這個範圍，和遊戲的規則，非常簡單，但Go的問題，解決Go的問題，並且打敗，冠軍，是非常複雜的問題，因為，Go的遊戲，可以在多個地區，遇到的地區，非常多。

在多個地區，有更多的法律地區，比在宇宙上的地區更多，目標是，訓練AI，訓練一個，深入強化學習人員，可以控制Go的遊戲，不僅能打敗，現有的冠軍軟件。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_27.png)

也能打敗，現有的世界冠軍，2016年，Google DeepMind，就面臨了這個挑戰，幾年前，Go其實開發了，一個強化學習基礎的流程，打敗了Go冠軍玩家，這個主要的概念，非常簡單，並且跟我們今天。

所學到的一切一樣，首先，一個人工智能網絡，可以觀察到很多人工智能Go玩家，並學習模仿他們的行為，這個部分並沒有使用強化學習，而是使用監控學習，可以研究很多人工智能，然後，他們利用這些人工智能網絡。

來對抗強化學習的政策網絡，讓政策，超越人工智能人員，並且對抗自己，達成超人性的表現，此外，一個讓這項挑戰成功的技巧，就是使用這個人工智能網絡，以智能智能的智能來做指引，並預測智能的狀況，現在這個網絡。

AI可以用智能智能來幻想，不同的位置行動，它可以去評估，這些行動的效果，以這些預測的價值為例，這讓它可以穿越，並計劃它可以做的行動，以它在未來的發展方向為基礎，最後。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_29.png)

最近發佈的一項，這些方法的延伸，在2018年，叫做AlphaZero，只用自動玩，並將它集中在三個著名的遊戲之中，不僅是GO，還有桌上遊戲，手機遊戲，還有GO，在這些例子中，作者們顯示。

這些網絡不需要人工智能，而是他們從頭開始，自行去做調整，這正是一個，純粹的強化學習的方法，但它還能夠，不僅擊敗人類，也擊敗了，過去的網絡，是由人類資料進行調整，最近，只在上個月，非常最近。

這項工作的下一個進展，是由MuZero發佈的，該系統現在學會，掌握這些環境，甚至不再知道規則，我認為最好的方法，來形容MuZero，是與之前的進步，與它們的能力相比，我們已經討論過，今天，我們開始討論。

AlphaGo，這顯示了人類超人的表現，在GO上，使用自動遊戲，並預先訓練這些模式，使用人類超人資料，然後是AlphaGo Zero，這顯示了，更好的表現，可以完全自行達到，不需要預先訓練，人類超人。

而是直接從零開始學習，然後是AlphaZero，這項想法更廣闊，超越了GO遊戲，並且進入了比賽和衝擊，但仍然需要模式，了解規則，並且得到遊戲規則，以獲得他們的教訓，上個月，作者展示了，超人表現。

在超過50場比賽中，都沒有在預先知道規則的，遊戲學習方法，它需要學習，並且，學習如何，在訓練過程中，最好地玩遊戲，這非常重要，因為在許多場合，我們並沒有預先知道規則，來告訴模式，當我們處於環境中。

有時規則不明顯，規則或動態不明顯，物體可能會，不斷地互動，或是不可預測的，我們也可能處於一個環境，規則太複雜，人類無法形容，所以這個想法，學習遊戲規則，或是任務，是一個非常強大的概念，我們來簡單地。

看看這個方法，因為這是一個很棒的系統，但重點是，它實際上建立在我們今天學習的一切上，所以你應該能夠了解，每個部分的這個系統，我們開始從座位的狀態來觀察，從此時，我們預測，或是進行樹木尋找。

通過不同的可能發生的場景，我們會做一些行動，然後我們會看待，未來的可能場景，或未來的可能發生的狀態，但現在，因為我們不懂規則，網絡被迫學習，我們要學習這個尋找的模式，我們要學習，未來的可能發生的狀況。

以及它在現時的狀況下，和它所做的行動，現在，在最初的時間，這給我們一個可能性，在這次的行動中，根據它能夠得到的價值，通過這個樹木的根部，然後用這個來計劃，它應該做的下一步，這就是我們學習的，政策網絡。

但也能夠達到，這個樹木尋找的計劃，來計劃未來的發展，現在，因為這個政策網絡，它接受這個行動，並得到新的觀察，從遊戲中，然後重複這個過程，一遍又一遍，直到遊戲結束，或者遊戲結束，這就像是，我們看到的。

AlphaZero的運作，但現在，最重要的差別是，這個政策網絡，作為樹木尋找的一部分，我們可以看到，在每一個步驟中，是完全學習的，這對於這些技術，除了在硬體遊戲中，應用的方式，也非常大方。

所以在這些情況下，我們非常了解遊戲的規則，所以我們可以利用它，來訓練我們的系統，但在很多情況下，這種進步，讓我們能夠應用這些系統，去處理一些我們不懂的問題，我們需要學習的規則，來玩遊戲，或者是。

規則是很難去定義的，但在現實世界，很多有趣的情況下。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_31.png)

這就是這樣，所以我們來簡單地，回顧我們今天課堂學到的東西，我們開始，從深入強制學習的基礎，我們定義了，什麼是代理人，什麼是行動，什麼是環境，他們如何互相互動，在這個強制學習的循環中，我們開始從。

深入的學習問題，以及深入的循環網，來學習循環，以一個現實的交流，然後決定一個政策，選擇一個行動，能夠達到最大程度的循環，最後我們學習了，如何能夠更好地，而不是更好地，去定義循環。

而是更好地去直接定義政策，從現實的循環開始，我們看到這個有很大的影響，在持續的行動空間，Q系統，或是Q學習技術，是有些限制的，所以謝謝你來參加，這個深入強制學習課堂。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_33.png)

在此時，我們將在下一個課堂，專注於強制學習，你將獲得一些經驗，如何應用這些系統，來自你自己，專注於政策基礎系統，在非常簡單的例子中，例如Pong，以及更複雜的例子，你將從零開始，從零開始。

建立這個機器人，和環境的腦袋，你將能夠將很多的想法，我們今天在這次課堂中看到的，一起結合起來，所以請來GatherTown，如果你有任何問題，我們會很高興地討論，關於軟件實驗室的問題。

以及今天課堂的任何問題，所以我們期待能夠在那裡見到你。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/775745a03f4e604887cc8b780376f00d_35.png)

謝謝。

# P6：Deep Learning New Frontiers - 爱可可-爱生活 - BV1jo4y1d7R6

大家好，欢迎来到MIT 6。S191的第六堂课！这是我绝对的最爱之一，课程中的讲座，我们将集中讨论一些局限性，深度学习算法以及该领域的一些新兴研究前沿，在我们深入探讨技术内容之前。

需要先进行一些与课程相关和后勤的工作。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_1.png)

我首先要宣布的是，我们的课程具有设计的传统，并向今年参加的学生提供T恤，我们将继续以这种荣誉为荣，因此，为此，我们在画布上为所有学生准备了一张签到纸，您可以在其中指明您的，有兴趣领取T恤。

一旦您填写了该签名表，便会收到必要的信息，信息将确保通过适当的方式将T恤衫交付给您，尽快，如果在课后，如果画布已关闭并且您无法访问该注册，表格，请随时给我们发送电子邮件。

我们将找到一种将T恤衫送给您的方法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_3.png)

以便退后一步，并概述我们的课程安排，到目前为止，我们去过的地方和将要去的地方，前沿，我们将在明天完成强化学习的最终软件实验室的截止日期，我们将举办两场非常激动人心的热点话题演讲，并提供全新的内容。

接下来是一系列的四场客座演讲，其余时间您将有空，这周的课程继续进行您的期末项目，全班将在星期五结束，学生最后的项目介绍和提案竞赛以及我们的奖项，仪式，所以说到这些最终项目，让我们详细了解一下这些项目。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_5.png)

那些修读学分课程的人，有两个选择可以达到你的成绩，第一个，是一个项目建议，您将多达四人一组来开发新的，和新颖的深度学习思想或应用，我们意识到两周的时间非常短，是时候提出并实施一个项目了。

所以我们肯定会采取这个措施，然后在1月29日（星期五）进行评审时，您需要进行三分钟的简短讨论，向一组评审介绍您的项目建议，评审将颁发最终奖项，至于物流和时间安排，您将需要表明您的兴趣。

在东部时间午夜在本星期三进行演示时，需要提交幻灯片，供您在东部时间周四午夜前进行的项目建议书介绍，这些要求的提交在课程提纲和画布站点上，我们的头号优胜者将被授予奖品。

包括英伟达GPU和Google Homes，我想就最终提案演示文稿提出的要点，是为了参与并有资格获得奖品，需要同步参加，在星期五的课程中，所以1月29日星期五东部时间下午1点至3点，您需要在场。

您或您的小组需要在场才能参与最终提案，竞争满足信用要求的第二选择，是撰写深度学习论文的一页评估，评估基于，您的评论的完整性和清晰度，这将在东部时间星期四午夜之前完成，时间。

更多信息和说明也可以在画布上找到，因此。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_7.png)

在本次演讲之后，我们将要讨论两个非常令人兴奋的热点话题，讲座，而这些讲座将集中在深度学习的发展中领域中的两个快速兴起的话题上，深度学习研究，第一个将重点介绍一系列方法。

寻求开发可实际学习和估算的算法的证据深度学习，神经网络的不确定性和第二个焦点话题将集中在机器上，学习偏见和公平，这里我们将讨论实施偏见的一些危险，社会中的算法以及实际上减轻这些不必要偏差的新兴策略。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_9.png)

随后将进行一系列非常激动人心和令人敬畏的嘉宾演讲，来自行业和学术界的领先研究人员，特别是我们将拥有，讲座将涵盖从AI和医疗保健等各个主题的各种话题，记录用于业务应用程序和计算机视觉的分析。

我们高度高度评价，如果可以的话，鼓励您同步参加这些讲座（如果您可以在1月27日和1月参加），美国东部时间下午1点至下午3点（28日），这些主题将成为非常令人兴奋的主题，可能会延长指定软件实验室的时间。

以便我们确保我们可以进行实时质量检查，与我们出色的嘉宾演讲者没事，因此总结了物流和课程。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_11.png)

相关的公告，让我们深入了解本讲座的有趣内容和技术内容。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_13.png)

到目前为止，取得成功191我希望您对深度学习有深刻的了解，已经彻底改变并且正在彻底改变来自以下领域的许多不同研究领域和领域。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_15.png)

自动驾驶汽车在医学和医疗保健方面的发展，以加强学习。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_17.png)

生成建模机器人技术和自然语言的各种其他应用，处理财务和安全性，并了解巨大的应用程序，深度学习的实用性和力量我希望您也已经建立了具体的理解，这些算法的实际工作方式，以及它们如何具体实现了这些进步。

回到我们一直在考虑的算法和模型的类型，处理以信号形式输入图像和其他感官数据的系统，并继续做出决定，因为输出可以是预测，也可以是，输出的检测也可能是一个动作，例如在强化学习的情况下。

在生成模型的情况下考虑了反问题，我们实际上可以，训练神经网络以产生新的数据实例，在这两种范式中，我们可以。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_19.png)

确实将神经网络视为功能非常强大的函数逼近器，这与，神经网络理论中长期存在的定理，即万能定理，逼近定理，它于1989年提出，在社会上引起了轰动，这个定理的普遍近似定理所说明的是，一个具有。

单个隐藏层足以将任意函数近似到任意位置，它所需要的只是一个单层，在这一课中，我们主要处理了，深度神经模型，我们彼此堆叠多个隐藏层，但这，定理完全忽略了这一事实，并说好吧，只要我们可以，我们只需要一层。

将我们的问题简化为一组输出输入和一组输出，这意味着必须存在一个，神经网络可以解决这个问题，这是一个非常强大而且非常重要的声明，但是，如果您仔细考虑这一点，我们需要注意几个警告。

首先是这个定理不能保证隐藏单位的数量，或需要解决的图层大小，这样的问题对，它也留下了一个问题，那就是我们实际上如何去做，训练这样的模型，以找到权重来支持它无法实现的架构。

关于它的任何说法都只是说它证明了这样一个网络的存在，但是据我们所知，在梯度下降的情况下，发现这些权重非常重要，这是由于，优化问题的非凸性质另一个关键的警告是该定理。

不能保证最终模型可以很好地推广到其他任务，实际上，我认为这个定理是通用逼近定理点，涉及与在人工智能中过度炒作的可能影响有关的更广泛的问题，以及，我们作为一个社区，因为学生们投入资金来促进这一领域的发展。

我认为我们在考虑和营销以及宣传这些广告时需要非常谨慎，算法，而通用逼近定理却能引起很多兴奋，这也给当时的社区带来了一种虚假的希望，那就是神经网络可以用来解决任何问题，就像你可以想象的那样。

过度炒作是非常非常非常危险的，这种过度炒作也与两个因素有关，在历史悠久的AI冬季，人工智能和神经网络的研究更加具体，放慢了很多，我认为我们仍处于爆炸性增长的阶段，为什么今天在本讲座的其余部分中。

我想重点介绍一下，我们已经学习并扩展到超出我们讨论范围的算法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_21.png)

可以考虑新的研究领域，所以首先要考虑局限性。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_23.png)

我最喜欢的之一，我认为这是最有潜力的例子之一，深度神经网络的危害和局限性来自于这篇称为理解深度的论文，神经网络需要重新考虑泛化，而他们在本文中所做的工作非常简单，实验中。

他们从数据集imagenet拍摄了图像，并且每个图像都已关联，带有一个特定的班级标签（如此处所示），他们所做的是他们在这个实验中，对于数据集中的每个图像（不是类别，而是单个图像），它们翻转了一个死角。

死亡，其中k是他们正在考虑的可能类别的数量，他们使用了这个，翻转模具以将全新标签随机分配给特定图像，这意味着这些，关于图像中实际存在的内容，相关的新标签是完全随机的，因此，例如。

可以在此处可视化重新映射，并注意这两个狗实例，已经完全映射到不同的类，所以我们完全随机化了标签。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_25.png)

他们接下来要做的是将这个数据，这个打乱的数据拿走，并试图适应一个深层的神经。通过应用与原始图像不同程度的随机化，将图像网络连接到图像网络数据，带有不变类别标签的数据将完全随机化，如您所愿。

期望模型在测试集上的准确性独立测试集逐渐趋于零，随着数据随机性的增加，但真正有趣的是他们在观察时，他们查看了训练集上的表现，这就是他们发现的结果，他们发现，无论他们将标签随机化多少，模型都能在，训练集。

其突出之处在于与通用声明非常相似，逼近定理得出了这样的想法：深度神经网络可以完美地适合任何，即使该功能与由随机标记驱动的完全随机数据相关联。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_27.png)

因此，画图确实将这一点带回家，我认为是考虑和理解的最佳方法，神经网络是非常好的函数逼近器和所有通用逼近，定理指出，神经网络在这一方面非常擅长，所以让我们假设在这里，一些数据点。

我们可以使用神经网络学习一个近似于此的函数，数据，这将基于对分布的最大似然估计，这些数据的意思是，如果我们给模型提供一个新的数据点（此处显示为紫色），我们可以预期我们的神经网络将为此预测最大似然估计。

数据点，并且该估计可能将取决于该函数，但是现在发生了什么，如果我将分布区域的范围扩展到现在超出域范围的范围，那么实际上，无法保证这些区域中这些区域的数据是什么样的，因此我们。

无法对我们的模型在这些区域的行为或表现做出任何陈述，这是现代深度神经网络存在的最大限制之一，所以这里对神经网络的声明做了修订，出色的功能逼近器他们真的是出色的功能逼近器，当他们拥有训练数据时。

这也引发了一个问题，即发生了什么，在这些分布范围外的区域中，网络以前从未见过培训示例，我们如何知道我们的网络何时不知道自己对所做的预测没有信心。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_29.png)

建立这个想法，我认为可以有一个可以被放大和夸大的概念，媒体认为深度学习基本上是炼金术，对，这是神奇的治疗方法，所有可以解决任何问题的解决方案，我的意思是它的功能真的很棒，我几乎可以肯定。

这可能是您参加本课程并吸引他们的机会，但是，你知道我们是否可以说深度学习算法就是这些，所有令人信服的解决方案都可以应用于任何任意问题或应用。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_31.png)

这也产生了您可以采取一些训练数据集的想法和信念，这些想法和信念适用于某些网络，架构可以改变您的学习算法并吐出出色的结果，但这根本不是深度学习的工作原理，您的模型只会与数据一样好。

就像社区里的谚语所说的那样，如果你把垃圾放进去，你将要把垃圾扔掉。我认为一个真正突出这一局限性的例子就是我要去的那个例子。现在向您展示这强调了这些神经网络系统在多大程度上取决于数据，他们接受过训练。

所以假设我们有狗的形象，我们会将其传递给，基于cnn的体系结构，我们的目标是尝试训练网络以拍摄黑白图像，并为其着色，当它传递到模型中时，这只狗的图像发生了什么，如下，仔细观察一下这个结果。

如果您发现狗的鼻子下面有，皮毛上的这个粉红色区域，如果这仅仅是，天生的狗，但为什么会这样呢，为什么我们的模型会很好地吐出这个结果。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_33.png)

如果我们考虑可能已用于训练网络的数据，很有可能在成千上万的图像中，用来训练这种模型的狗，大多数或许多这些图像都会拥有，狗把舌头伸出来，因为那是狗的行为。

因此cnn可能已将狗嘴下方的那个区域映射为最可能是粉红色的，因此，当它看到一条狗闭上嘴时，它的舌头没有张开，它以某种方式假设，正确，或者它是建立的表示形式，它将区域映射为粉红色，重点是。

深度学习模型会根据，他们看到的数据，我认为这是一个非常关键的点，因为您外出时知道自己已经采取了，本课程，您可能对将深度学习应用于某些应用感兴趣，和您感兴趣的问题，您的模型将总是与您的数据一样好。

这也引发了一个问题，即神经网络如何处理数据实例，他们以前没有遇到过的，我认为这很突出，几年前这个臭名昭著的悲惨例子，在自动杀死驾驶员的同时自动驾驶崩溃了，事实证明。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_35.png)

在那次撞车事故中丧生的驾驶员实际上报告了多个，在撞车事故发生前的几周内，汽车实际上向，完全一样的障碍撞到了它，为什么它本来可以做得那么好，结果却是，代表汽车自动驾驶数据的图像，系统经过训练。

来自高速公路该区域的图像实际上缺乏新的构造，改变了最近障碍物的外观，使汽车，在崩溃之前，它遇到了一个实际上已无法分发的数据实例，并且，不知道如何处理这种情况，因为我只见过特别的熊，在这种情况下。

障碍物的特定样式和建筑会导致其悲剧性崩溃，在这种情况下，是因为神经网络故障模式导致了损失，人的生命，这表明这些失败模式指向并激发了需求，真正有系统的方式来理解深度学习模型的预测何时，换句话说。

当无法确定其预测时，就无法信任它，这是一个非常，深度学习中令人兴奋且重要的研究主题，它将成为我们研究的重点，第一个焦点话题，不确定性的概念对于部署绝对是非常重要的。

深度学习系统以及我认为安全至关重要的应用程序，诸如自动驾驶之类的东西诸如医学面部识别之类的东西，算法越来越多地与人类生活打交道，我们真的需要有原则的方法。

确保它们的鲁棒性不确定性指标在我们必须要解决的情况下也非常有用，依赖于可能不平衡或存在大量噪声的数据集，我们将在聚光灯讲座中进一步考虑这些不同的用例。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_37.png)

好的，在为明天的聚光灯演讲做准备之前，我想，简要概述一下我们需要哪些不确定性以及我们可以谈论哪些不确定性，在考虑深度学习算法时，让我们考虑这个分类问题，其中，我们将尝试构建一个神经网络。

以对一组固定的类的概率进行建模，因此，在这种情况下，我们尝试在猫的图像，狗的图像和，然后输出新图像中是猫还是狗的权利，请记住概率，猫和狗的总和必须等于一，所以当我们现在训练模型时会发生什么。

我们已经准备好对其进行测试，并且我们有一个图像，其中同时包含猫和狗，但网络仍然需要输出，类概率总和为1，但实际上，该图像既有猫又有狗，这是我们可以认为存在于噪声或随机性中的一个实例，数据。

如果我们在单独的猫或狗的图像上训练此模型，关于模型在不确定性之前所看到的内容，狗和猫都是吵杂的，指标可以帮助我们评估噪声，即数据固有的统计噪声，并存在于数据中，这称为数据不确定性或同构不确定性。

现在让我们考虑另一种情况，让我们使用相同的猫狗分类器并输入一个图像，一匹马再到这个分类器，输出概率将必须加到一个，但是即使网络预测该图片最有可能包含狗，我们也会，希望它对这个预测真的不很自信。

这是一个例子，我们的模型现在正在完全不经分配的图像上进行测试，因此，我们期望它对它的预测不是很自信，这种类型的不确定性与数据不确定性是另一种类型的不确定性，这就是所谓的模型不确定性或认知不确定性。

它反映了给定预测的自信程度，对于理解神经网络的泛化程度非常重要，这非常重要，地区以及他们如何报告分布外地区和，在聚光灯讲座中，您将真正深入了解不确定性估计的这些概念。

并探索一些新兴的方法来直接学习神经网络的不确定性。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_39.png)

我想考虑的第三种失败模式是我认为非常有趣的一种，而且也有点吓人，这就是对抗性例子的想法，这里的想法是我们以一些输入示例为例，例如这张寺庙和标准cnn的图像。

经过训练的您知道一组图像将对该特定图像进行分类，作为具有97个概率的庙宇，我们随后拍摄了该图像，并应用了一些特殊的摄动，到该图像以生成所谓的对抗性示例，例如，如果我们现在，这个同一个cnn的例子。

它不再将那个图像识别为圣殿，而是错误地将此图像归类为鸵鸟，令人难以置信，对，那么实际上实现了完整的对抗攻击的扰动是什么。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_41.png)

这个扰动是做什么的，请记住，当我们使用梯度下降训练神经网络时，我们的任务是取一些目标j并尝试根据给定的权重来优化该目标，w输入x和预测y，我们的目标以及我们在进行此渐变时的要求。

下降更新是指权重的微小变化如何减少损失，特别是如何，我们可以扰动这些权重以最大程度地减少损失吗？为了做到这一点，我们用固定图像x和真实标签y扰动网络，现在只有权重才能将对抗性攻击的损失降到最低。

我们现在问如何，我们修改输入图像以增加网络预测中的误差，因此我们，试图预测以某种方式扰动输入x，以便当我们固定权重w时，和真实的标签y然后我们可以增加损失函数以使网络基本跳闸。

弄错这种对抗性扰动的想法是最近由一群人提出来的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_43.png)

在麻省理工学院，他设计了一种算法，可以实际上合成，对抗任何一组转换，例如旋转或颜色变化，它们，能够合成一组对这些类型相当强大的2d对抗攻击，转换的真正酷点在于，他们进一步迈出了超越2D的一步。

图像以实际合成物理对象3d对象，然后可以，曾经愚弄神经网络，这是对抗性例子的首次展示，实际上存在于现实世界中，所以这里的例子是这些3D打印的海龟，当那些乌龟的图像被对手准确地分类为步枪时。

这些乌龟的图像被错误地归类为步枪。再次拍摄这些是真实的物理对象，然后将这些图像输入，一个分类器，因此就如何保证我们提出了许多有趣的问题，深度学习算法对此类对抗攻击的鲁棒性和安全性。

可能被恶意用来扰乱依赖于深度学习算法的系统。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_45.png)

最后的限制，但可以肯定的是我想在本次讲座中介绍，但当然不能，总体而言，深度学习的最终局限性在于算法偏差的局限性，这是一个主题，并且，一个值得理所当然的问题最近引起了很多关注，这也将是。

我们第二个热门话题讲座的重点，这种算法偏差的思想围绕着，神经网络模型和人工智能系统更容易受到影响的事实，因他们的构建方式，他们对数据进行训练而产生的偏见，关键是这些偏见会导致非常真实的社会后果。

所以我们将，在明天的聚光灯下讨论这个问题，应该非常令人兴奋，所以这些仅仅是。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_47.png)

神经网络的许多局限性，但这当然不是详尽无遗的清单。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_49.png)

我很高兴再次强调我们将重点关注其中两个限制，下两个即将到来的焦点演讲中的不确定性和算法偏差。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_51.png)

在本演讲的其余部分还可以，我想专注于正在出现的一些真正令人兴奋的深度学习新领域，旨在解决这些限制中的一些问题，特别是，神经网络被视为黑盒子系统，它们缺乏某种领域知识，并且，结构和先验知识。

最后是我们如何实际解决这个更广泛的问题，从头开始设计神经网络是否需要专业知识，以及如何为更广泛的机器学习创建更通用的管道。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_53.png)

好吧，我们要研究的第一个新领域是如何编码结构和域。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_55.png)

深度学习架构方面的知识要退后一步，我们实际上已经看到了，我们在卷积神经网络研究中的例子，cnns受到视觉处理被认为在大脑中起作用的方式的启发，引入cnns和cnns来尝试捕获数据中的空间依赖关系。

这是关键在于，启用它是卷积运算，我们看到并讨论了如何使用，卷积以提取数据中存在的局部特征以及如何应用不同的集合，过滤器以确定不同的特征并在空间数据之间保持空间不变性。

这是如何在空间上定义问题图像数据结构的一个关键示例，启发并导致了将编码结构转化为神经网络体系结构的进步，真正针对该问题或感兴趣的问题真正调整该体系结构，超越图像数据或序列数据的事实是，我们周围都存在。

是具有不规则结构的数据集和数据问题，实际上可能存在，图和网络的范式是其中非常丰富的一种，可以在图形或网络中编码的结构信息可能非常重要，解决正在考虑的问题，但不一定清楚我们如何建立神经网络。

网络架构可能非常适合对以图形表示的数据进行操作，因此，哪种类型的数据或哪种类型的示例可以自然地将其表示为图形，我们都沉浸其中并熟悉的一种社交网络，除此之外，您还可以想到状态机。

它们定义了不同状态之间的转换。能够由人类活动的图形或模式表示的系统，运输化学分子，您可以想到分子中的各个原子，作为图中通过连接那些原子生物网络的键连接的节点。

所有这些实例和图形作为一个更广泛的结构所具有的共性是由。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_57.png)

感谢有如此多的现实世界数据示例和应用程序这一事实，其中存在无法通过简单的简单数据编码轻松捕获的结构，例如，图像或时间序列，所以我们将讨论图作为结构，可以为一系列问题提供新的非标准编码，好吧。

看看我们如何做到这一点并建立这种理解，让我们回到网络，在熟悉cnn和您可能知道之前已经见过的架构，我希望您现在知道在cnn中，我们已经有了卷积核，并且，cnn层中卷积运算的工作方式是我们滑动此矩形核。

在我们的输入图像上，以便内核可以提取内部内容和此操作，由我们先前回顾的逐元素乘法和加法驱动，因此，如果您有正确的图像，请逐步完成此步骤，卷积核就是，通过将滤镜的权重集应用于图像，可以有效地在图像上滑动。

在整个图像上不断重复进行此操作，cnns背后的想法是根据特定的权重集设计这些过滤器，我们可以选择数据中存在的不同类型的功能，图卷积网络在一个非常相似的思想上运作，但是现在，而不是在2D图像上进行操作。

网络正在对以图形表示的数据进行操作，其中的图由此处以圆圈显示的节点和以线条显示的边缘以及以边显示的边缘定义，边定义了图中节点之间的关系，关于如何提取的想法。

该图上的信息在原理上与我们将要看到的cnns非常相似，再次取一个内核，它只是一个权重矩阵，而不是在2d上滑动该内核，我们的图像的2D矩阵表示，内核将弹出并四处移动，到图形中的不同节点，并且这样做。

它将查看的本地邻域。该节点，并选择与该节点在图中的本地连通性相关的功能，这就是图卷积运算，我们现在可以从网络中学到，定义与捕获边缘的过滤器关联的权重，图中存在相关性，因此让我们逐步了解权重核。

去到不同的节点，它会看看它的紧急邻居，图卷积算子将要关联权重然后与每个边，存在，并将在图上应用这些权重，以便随后移动内核，到图中的下一个节点，以提取有关其本地连接的信息，在继续执行此操作时。

将其应用于图和键中的所有不同节点，是本地信息将被汇总，而神经网络将，然后学习将本地信息编码为更高级别表示的函数，所以这是一个非常简短和直观的介绍，希望能对神经图进行确认，网络原理上如何运作。

这是一个非常令人兴奋的网络架构，现在已经实现了巨大的进步，在各种科学领域，例如化学科学和分子发现，有一种称为消息传递网络的图神经网络，非常成功地部署在基于二维二维图形的化学结构表示中。

这些消息传递网络建立了原子和化学的学习表示，这些相同的网络基于化学结构中存在的键和关系，图神经网络最近被用于发现一种新的抗生素和一种新药，我认为在细菌感染的动物模型中可以有效杀死耐药菌。

当我们开始看到这些深度学习系统时，这是一个非常令人兴奋的研究途径，和神经网络架构在生物医学领域中的另一新近应用，非常激动人心的应用领域是移动性和交通预测，因此在这里我们可以走上街头，将它们表示为分解。

将其表示为节点，并对交叉点和，通过节点和边定义连通性网络的图形表示的街道网络区域，团队所做的就是建立该图神经网络表示，学习如何预测道路系统之间的交通模式，实际上，这种建模可以。

改进了事物中预计到达时间的预测能力，以及Google界面等地图的另一个非常近期且高度相关的示例，神经网络正在预测covin-19疾病的蔓延，并且有一些群体，已经考虑合并两种地理数据。

以便了解有关人的居住地和居住地的信息。定位他们可能要连接的人以及有关该人的时间数据信息，随时间的运动和轨迹，并以此作为对神经网络进行图形化处理的输入，由于该数据的时空成分，已经完成了该图。

神经网络已与时间嵌入组件集成在一起，以便他们可以学习，不仅基于空间地理联系来预测covid19疾病的传播，和邻近性，但在时间模式上，我们可能会遇到的另一类数据是。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_59.png)

三维数据的点集，通常称为三维点集，作为点云，这是图神经网络的相同思想所在的另一个领域，促成许多强大的进步，因此，首先请欣赏一下，必须了解这些三维数据集的确切外观，这些点云实际上是空间中无序的数据点集。

点之间存在一些潜在的空间依赖性，因此您可以想象，这些对象的三维结构的基于点的表示形式，然后根据这些数据训练神经网络来完成许多相同类型的任务，和我们在计算机视觉讲座中看到的问题，因此分类采用了点云。

将其识别为对象，并将其作为带点云的特定对象分割，分割出属于特定对象或特定对象的点云实例。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_61.png)

内容类型，我们能做的就是扩展图卷积网络以使其能够运行。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_63.png)

指向云，我认为超级棒的方法是采用点云，使用点云固有的网格将其展开并动态计算图形，然后。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_65.png)

这个例子显示了这种兔子结构，我们从这里开始，点云扩展，然后在此3d网格中定义本地连接uh。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_67.png)

因此我们可以应用图卷积网络对，3d空间中点的顺序，并且仍然可以捕获此类数据系统的局部几何形状。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_69.png)

希望如此，这使您对我们可以开始的不同类型的方法有所了解，考虑编码结构内部神经网络架构超越，我想在第二个新领域的前五节演讲中看到的架构，在本演讲的其余部分中，我们将重点讨论并讨论如何学习。

我认为这是一个非常强大且发人深省的，深度学习研究领域，它产生了一些有趣的问题，我们可以将机器学习和人工智能系统的功能推向多远和多深。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_71.png)

现在称为自动机器学习或自动毫升的这一领域背后的动机，事实上，标准的深度神经网络架构已针对以下方面的性能进行了优化：一个单一的任务，为了建立新的模型，我们需要某种领域的专业知识和专家知识。

尝试定义一种非常适合特定任务的新架构，自动化机器学习背后的想法是，我们可以超越这种调整吗？为单个任务而健壮地优化特定体系结构，我们可以超越此范围来构建吗？

可以实际学习用于解决给定问题的最佳模型的更广泛算法，问题以及我们在最佳模型或使用哪种模型方面的含义是它的体系结构，对于该问题而言，最优的是与该架构相关的超参数，例如数量，每一层的神经元数量也进行了优化。

整个系统，是通过aa算法建立和学习的，这就是automl的想法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_73.png)

在原始的automl作品中，代表自动化机器学习原始作品，使用了基于强化学习的框架，其中有一个神经网络，称为控制器，在这种情况下，该控制器网络是递归神经网络，控制器的作用是提出一个示例模型架构。

称为子架构，架构，该架构将由一组超参数定义，然后可以对生成的体系结构进行培训，并评估其在特定环境下的性能，感兴趣的任务然后将该子网络的性能反馈用作，在这个强化学习框架中获得奖励，以尝试提升并告知控制者。

关于如何实际改善其网络建议以进行下一轮优化，所以这个循环过程被重复成千上万次，从而产生了新的架构，测试它们，将反馈信息反馈给控制器，以进行构建并从中学习，最终，控制器将趋向于将高概率分配给hyper。

实现更高精度的架构搜索空间的参数和区域，感兴趣的问题，并将对搜索空间的那些区域分配低概率，表现不佳，那么此代理如何工作，该控制器代理如何实际工作，从宏观上看，这将是一个基于rnn的架构，其中。

在此管道的每个迭代的每个步骤中，模型都是该控制器模型要采样的，一个全新的网络架构，并且该控制器网络将专门用于，优化以预测与该子网络关联的超参数，因此，例如，我们可以考虑对特定层进行优化。

优化将涉及与该层关联的超参数的预测，就像卷积层一样，过滤器的大小，步幅的长度，依此类推，以此类推，然后生成的结果网络就是生成的子网络，由这些预测的超参数定义的将被测试，训练和测试，这样，在评估之后。

我们可以得出结果的准确性并更新经常性，神经网络控制器系统，基于子网络在我们的任务上的执行情况，然后rnn控制器可以学习创建更好的模型，并且非常适合，强化学习框架，我们的控制器网络的代理将在其中。

根据产生该想法的子网络的性能进行奖励和更新，现在已经扩展到许多不同的领域，例如最近在，与产生子网络的控制器网络相同的原理进行图像识别，然后经过测试评估以改进控制器，以用于设计优化的控制器。

设计这种范式的神经网络，用于图像识别，设计架构可以看作是神经架构搜索，在这项工作中，控制器系统用于构造和设计用于整体的卷积层，在图像识别任务上经过测试的体系结构这张左侧的图描述了。

卷积层中卷积单元的学习结构实际上看起来像和，这项工作真正真正意义非凡的是，他们评估的是他们的结果，他们在评估这些神经网络的性能时发现了设计的神经网络，我知道这有点令人费解，但让我们考虑一下这些结果。

所以首先在这里用黑色，我在图像上显示了人类设计的最先进的卷积模型的准确性，识别任务，如您所见，y轴刻度显示的精度为，X轴上以百万为单位的参数数量引人注目是，将这些人为设计模型的性能与。

这些神经设计的神经体系结构在此处以红色显示的automl算法取得了卓越的性能，与人为设计的系统相比精度更高，参数相对较少，这种使用机器学习的思想，即通过深度学习来学习更多的通用系统，或者。

用于预测建模和决策的更一般的范式是一种非常强大的功能，并且。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_75.png)

最近，人们对超越automl和Neuro产生了浓厚的兴趣，架构搜索，我们可以更广泛地将其视为自动完整的管道，从数据管理开始设计和部署机器学习和人工智能模型，数据预处理以进行模型选择和设计，最后进行部署。

这里的想法是，也许我们可以建立一个可以促进和自动实现的通用管道，加速和设计此过程的所有步骤。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_77.png)

我认为这个想法产生了一个非常非常发人深省的观点，那就是我们可以，建立能够生成针对特定任务设计的新神经网络的人工智能系统，但是建立的高阶人工智能系统是一种超越特定任务的学习。

这不仅减少了我们作为经验丰富的工程师进行手工设计的需求，并优化这些网络，这也使这些深度学习算法更易于访问和使用，从广义上讲，我们开始考虑创意的意义和创造力的意义。聪明，当亚历山大介绍这门课程时。

他谈到了自己的想法，关于什么情报意味着能够利用情报来为将来的决策提供信息的能力，作为人类，我们的学习渠道绝对不限于针对非常具体的问题进行优化，赋予我们学习和解决问题的能力会影响我们的学习能力。

完全独立的问题，并提高了我们的分析能力，当今存在的模型和神经网络算法，当然不能扩展到这一点并无法捕捉到这种普遍性现象，我认为，为了达到真正的人工智能的目的，我们需要考虑周到。

真正的概括性和解决问题能力的含义，我鼓励您考虑这一点，以考虑automl如何auto ai，深度学习如何更广泛地融入到交叉路口的更广阔画面中，以及人工智能和人类智能之间的接口，所以我将留给您。

作为您在此过程中及之后的这一点上的反思点。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_79.png)

我将结束本讲座并提醒您，我们将拥有一个软件，实验室和办公时间会议，我们将专注于为您提供支持以完成工作，强化学习的最后一个实验室，但我们随时欢迎您的到来，与我们讨论，询问您的问题，与您的同学和队友讨论。

为此。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_81.png)

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/5ca9d51f5765bdb8de679343b81a1ff3_82.png)

# P7：Evidential Deep Learning and Uncertainty - 爱可可-爱生活 - BV1jo4y1d7R6

大家好，欢迎回到今天的讲座，在91年的前六堂课上，我们尝到了一些基本的深度学习算法和模型，在接下来的两节课中，我们实际上会深入到更多的细节，特别关注现代深度学习研究中两个至关重要的热门话题领域。

对我们到目前为止在这门课上所学到的一切都产生了真正的影响，现在，这两个热门话题的讲座将首先集中在概率建模上，利用深度神经网络进行不确定性估计，以及算法偏见和公平性，我们将学习一种非常强大的新技术。

叫做证据深度学习，什么时候，我们可以信任我们正在训练和解释的神经网络的输出，当他们不自信的时候。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_1.png)

或者当他们的预测不确定时，现在，深度学习中不确定性估计的整个领域今天比以往任何时候都更加重要，当我们看到这些深度学习模型时，就像我们一直在学习的那样，到目前为止在这门课上，开始走出实验室进入现实。

与…互动，或，至少，影响人类生活。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_3.png)

在他们周围，深度学习模型倾向于传播训练中的偏见，并且经常容易在全新的分销数据上失败，而是，我们需要能，可靠快速地估计他们所看到的数据中的不确定性，以及他们预测的输出，在这次讲座中。

我们实际上会开始学习证据，神经网络不确定性估计的深度学习，所以训练我们的模型不仅仅是为了做出预测和预测答案，也是为了了解它在预测中有多少证据，我们应该在多大程度上相信它的答案。

不确定性的一个很大的原因和实际原因，估计数，或者对于深度学习中的不确定性，是由于神经网络在实践中的训练方式有很大的差距，以及如何在部署中评估它们，当我们训练机器学习模型时，我们做以下假设。

我们的训练集来自与测试集相同的分布，但在现实中，这很少是真的，比如说，当我们看到许多最先进的，到目前为止我们所学到的最先进的算法，他们几乎都受过极其干净的训练，预处理数据集，通常有最小的闭塞。

现实世界中最小的噪声或歧义。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_5.png)

虽然，我们面临着这么多我们模型的边缘情况，我们的模型将完全无法处理，比如说，如果我们训练这个分类器来识别狗，我们把它训练在左手边这些干净的狗的图像上，当我们把它带到现实世界中时，它会产生非常差的性能。

我们开始展示它，全新位置的狗，倒置结构的狗，甚至是这只从天空中跳伞的狗，或者如果我们把这个在干净的城市街道上训练的驾驶模型，然后我们把它带到现实世界中，开始看到各种各样奇怪的东西。

现实中意想不到的边缘情况，现在，今天讲座的重点是建立不仅超级精确的模型，并具有高性能，而且还将定量估计技术构建到我们的学习管道中，这样我们的模型就能告诉我们，当它不知道正确答案时，这里有一句名言。

乔治·博克斯，我改编了一点来传达这一点，现在最后，所有的模型都是错的，但一些知道什么时候可以信任的人实际上会在现实中有用，现在我们很少需要一个模型来实现完美的百分之百的准确性，但即使我们的精确度略低。

如果我们能理解什么时候我们可以信任这个模型的输出，那么我们就有了非常强大的东西，现在，知道我们什么时候不知道某事的问题证明是极其困难的，虽然，这是真的，即使对人类来说，有太多的任务我们相信我们可以完成。

即使我们真的不知道正确的答案，而且可能更有可能道歉，失败的可能性比成功的可能性大，现在，这张照片，对于任何在一个新城市开车的人来说，可能甚至不需要解释，在我们的手机里有谷歌地图这样的东西之前。

人们往往会拒绝接受你可能会迷失的事实，寻求帮助，即使我们可能真的迷失在一个我们从未去过的地方，所以今天我们要学习如何教神经网络，预测概率分布而不是纯确定性的点输出，以及如何像这样构建我们的神经网络。

可以让我们模拟一种不确定性，但不是所有类型，特别是，然后我们将讨论它如何没有捕捉到一种非常重要的不确定性形式，也就是预测本身的不确定性，最后我们将看到我们如何学习不确定性的神经表征，利用证据深度学习。

这将使我们能够真正捕捉到这一点，呃，这另一种类型的不确定性，并迅速估计它，同时也扩展到一些非常高维的学习问题和高维输出问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_7.png)

让我们开始讨论我的意思，当我说我们想用神经网络来学习不确定性时，概率学习的这个术语，概率学习的这个术语是什么，它是如何与我们在这门课上所看到的一切联系起来的，现在在粘土里，在监督学习问题的情况下。

我们的模型有两个主要的数据输入来源，首先是数据本身，我们过去一直称这个为X，这是我们实际上输入模型的东西，鉴于我们的数据，我们想预测这里表示为Y的某个目标，这可能是x所属的一个离散类。

它也可能是我们想预测的任何真实数字，考虑到我们的数据，我们将得到一个x和y对的数据集，到目前为止，在这门课上一直专注于，至少在监督学习的情况下，学习从x到y的映射，并根据我们的输入预测y的期望值，x。

这正是我们训练决定论的方式，过去类中的有监督神经网络，现在的问题是，如果我们只模拟目标的期望，对为什么的期望，那么我们只有预测的点估计，但我们对这一预测有多广泛或不确定缺乏任何理解，这正是我们的意思。

当我们谈论估计我们模型的不确定性时，而不是平均预测答案，对Y的期望，我们还想估计我们预测目标y的方差，这让我们对输出有了更深入、更概率的感觉。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_9.png)

你可能在想，当我这么说的时候，这听起来很，与我们在这堂课中已经看到的非常相似，你是完全正确的，因为在这门课的第一节课上，我们已经对训练神经网络输出完整的分布有了很大的认识，对于分类的情况，特别是。

所以我们看到了一个例子，我们可以把图像输入神经网络，这个图像需要分为猫或狗，比如说，现在这里的每个输出都是属于那个类别的概率，这两个概率和为一或必须和为一，因为这是一个概率分布，我们的输出是概率分布。

现在，这绝对是神经网络如何训练的一个例子，输出概率分布，在本例中，离散类类别上的分布，但让我们深入一点，真正解剖它，看看我们如何做到这一点，嗯，首先我们必须使用这个特殊的激活函数，如果你还记得的话。

这被称为SoftMax激活函数，我们必须使用这个激活函数来满足输出的两个约束，首先，每个概率输出必须大于零，第二次，那个，我们需要确保，我们所有类概率的总和现在被归一化为1。

给定从这个Softmax激活函数中出现的类概率的输出，然后我们可以定义这种特殊的损失，使我们能够优化我们的分布学习，我们可以这样做，通过最小化我们所说的预测分布的负对数似然，与地面真相相匹配。

这也被称为交叉熵损失，这是你们现在应该非常熟悉的事情，因为你已经实现了它，并在你的三个软件实验室中使用了它，让我们把它弄得更正式一些，为什么我们用激活函数和损失函数来做这些选择，嗯。

这一切都归结为我们在开始学习之前所做的假设，那就是我们假设我们的目标类标签，y是从某个似然函数中提取的，在本例中，由分布参数定义的分类分布。

这里的分布参数实际上定义了我们的分布和我们在预测标签上的可能性，特别是，我们的答案在I中的概率，第1类现在正好等于第1个概率参数，同样，我们也看到了如何在连续类目标的情况下做到这一点，在这种情况下。

我们没有学习类概率，而是整个实数线上的概率分布，像分类域，这可以应用于任何监督学习问题，但当我们在以前的讲座中看到它时，我们看到了它，我们关注的是强化学习，我们想预测车辆应该采取的方向盘角度。

给定场景的原始图像像素图像，现在，因为这个输出的支持是连续的和无限的，我们不能像在分类的情况下那样只输出原始概率，因为这将需要来自我们网络的无限数量的输出，而是，虽然我们可以输出我们分布的参数。

即平均值和标准差，或者该分布的方差，这定义了我们的概率密度函数，我们的平均值是无限的，所以我们根本不需要约束它，另一方面，尽管我们的标准差西格玛必须是严格正的，所以为了那个。

我们可以使用指数激活函数来强制该约束，在分类领域，我们或类似于分类域，我们可以通过使用负对数似然损失来优化这些网络，一次又一次，我们是怎么走到这一步的，嗯，我们假设我们对我们的标签做了这个假设。

我们假设我们的标签是从正态分布或高斯分布中提取的，参数已知，μ和sigma平方，我们的平均值和方差，我们想训练我们的模型来预测输出，现在，我觉得这真的很神奇，因为我们没有任何理由，真理。

数据集中的基本真值均值和基本真值方差的变量，我们所拥有的基础真理智慧是标签，但是我们用这个公式和这个损失函数来学习，不是我们标签的点估计，而是一种分布，完全分配，描述数据可能性的高斯周围。

现在我们可以总结这些似然估计问题的细节，利用神经网络进行离散分类域和回归，连续回归域，现在从根本上说，这两个域不同，就他们适合的目标类型而言，在分类领域，目标可以是一组固定的类中的一个。

回归域中的1到K，在开始之前，我们的目标可以是任何实数，我们假设我们的标签来自或来自某种潜在的似然函数，在再次分类的情况下，它们是从分类分布中提取的，而在回归的情况下，我们假设它们是从正态分布中提取的。

现在这些似然函数中的每一个都由一组分布参数定义，在绝对的情况下，我们有这些概率来定义我们的范畴分布，在正态分布或高斯分布回归的情况下，我们有一个均值和方差来确保这些是有效的概率分布。

我们必须对我们的参数应用一些相关的约束，通过激活函数的方式，巧妙构造的激活函数，最后对于这两个，就像我们之前看到的，我们可以利用负对数似然损失来优化整个系统，这使我们能够了解标签上分布的参数。

同时我们可以获得这里的概率和方差。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_11.png)

对我们来说记住一些事情是至关重要的，那就是概率或可能性，让我们称之为我们通过建模得到的，我们在这节课中看到的问题，永远不要误认为我们模型的信心，我们得到的这些概率与置信度绝对不一样，我们认为至少是信心。

这就是为什么让我们回到这个分类的例子，在那里我们输入图像，对这个神经网络来说，它可以是猫，也可以是狗，我们有神经网络预测，这张照片是猫的概率有多大，或者现在是狗的概率有多大，因为如果我们输入一个。

比如说一只猫，我们的模型应该能够识别，假设它被训练来识别一些关键特征，专门针对猫说，好的，这很可能是一只猫，因为我在这张照片中看到了一些类似猫的特征，同样，如果我输入狗的输入，那么我们应该更有信心预测。

这张照片很有可能是一只狗，但是如果我们说让我们在这张图像中喂食会发生什么，一只猫和一只狗在一张照片中，我们的模型将识别出与成功检测到一只猫相对应的一些特征，以及在同一图像中同时成功地检测到一只狗。

它的决定会相对分裂，这并不是说我们对这个答案没有信心，我们可以对这个答案非常有信心，因为我们在这张照片中发现了猫和狗的特征，这只是意味着我们的输入数据中有一些歧义，导致了我们在输出中看到的不确定性。

我们可以对我们的预测充满信心，即使我们的答案是有可能的，或者百分之五十的猫或者百分之五十的狗，因为我们实际上是因为我们，我们实际上是在训练共享这些类型特征的图像，但如果我们看到的是我们没有看到的图像。

对这些类型的特征进行训练，例如，如果我们用同样的神经网络，但现在我们在这艘船的图像中进食，一些全新的东西，不像我们在训练中看到的任何东西，模型仍然有输出两件事，这个图像是猫的概率，这张照片是狗的概率。

我们知道，因为这是一个用Softmax激活函数训练的概率分布，我们知道这是一个范畴分布，这两种概率，猫的概率加上狗的概率必须和为一，无论发生什么，因此，在这种情况下，输出可能性将非常不可靠。

如果我们的投入与我们在训练中见过的任何东西都不一样，我们称之为脱离分布或脱离训练分布，我们可以看到，在这种情况下，我们的输出或这两个概率在这里是不合理的，他们不应该相信这真的是为了向你强调。

当我们说不确定性估计时，在训练神经网络时，我们应该关注不同类型的不确定性，像这样捕捉概率，它们没有捕捉到预测本身的不确定性，而是数据中的不确定性这带来了问题，存在哪些不同类型的不确定性。

我们如何在深度学习中使用不同的算法技术来学习它们。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_13.png)

这一切都归结为我们可以尝试估计的不同方法，当模特不知道，或者当它不确定它的答案或预测时，我们看到这些不确定性有不同类型，这是真的，甚至在我们的日常生活中，我想一个很好的方法来思考。

这是通过已知和未知的二乘二矩阵，我举一个简单的例子，我只是简单地说明一下，所以想象一下你在机场乘飞机，你有一些已知的知识，例如，会有一些航班从那个机场起飞，那是已知的已知的，你很自信。

你非常肯定那会发生，也有像已知未知数这样的事情，我们知道我们知道的事情，有些事情我们根本无法预测，例如，我们可能不知道我们的飞行时间，我们的航班起飞的确切时间，那是我们无法预测的。

也许是因为它可能会被推迟，或者它只是一个，它只是我们不能完全控制的东西，可能会改变，那么就有未知的已知，别人知道的事情，但你不知道，一个很好的例子就是别人预定的起飞时间，他们的预定飞行时间。

你知道别人知道他们预定的起飞时间，但对你来说，这是一个未知的已知，最后还有未知的未知，这些都是完全出乎意料或无法预见的事件，一个很好的例子是流星撞上跑道，这是基础机器学习中一个新兴的令人兴奋的研究领域。

理解，我们如何构建算法，鲁棒有效地建模和量化这些深度学习模型的不确定性，这真的很难，实际上是因为这些模型有数百万亿，现在甚至是数万亿的参数，理解和内省它们，检查他们的内部以估计理解，当他们不知道的时候。

正确答案肯定不是一个简单的问题，现在人们通常不会训练神经网络来解释这些类型的不确定性，所以当你对一些数据进行训练时，比如说，在这里你可以看到黑色的观察结果，我们可以训练一个神经网络用蓝色做出一些预测。

预测与我们在这个区域的观测一致，我们有训练数据，在本地区以外，我们可以看到我们的预测开始失败很多，并估计不确定性，在这种情况下，有两种形式，我们今天要讨论的，第一种形式是认识不确定性。

它对潜在预测过程中的不确定性进行建模，这是当模型只是不知道正确的答案，它对自己的答案没有信心，不确定性的另一种形式是任意不确定性，这是数据本身的不确定性，把这看作是统计或感官噪声。

这就是所谓的不可约不确定性，所以不管你收集了多少数据，在收集过程中你会有一些潜在的噪音，它是数据本身固有的，减少空气不确定度的唯一方法是改变传感器，并获得更准确的数据。

现在我们非常关心这两种形式的不确定性。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_15.png)

动态不确定性和认知不确定性，一次又一次，只是为了回顾这两种形式的不确定性之间的差异，我们左手边有Alatoric，这集中在我们数据的统计不确定性上，它描述了我们对数据本身有多自信，当我们的数据有噪声时。

它是最高的，而且不能通过增加更多的数据来减少，另一方面，我们有认知上的不确定性，这比自由不确定性更难估计，有一些新兴的方法试图确定认知的不确定性，现在，关键是认识不确定性反映了模型对预测的信心。

我们可以利用这些估计开始理解当模型不能提供可靠的答案时，当它缺少一些训练数据来提供答案时，不像所有的不确定性，通过增加更多的数据可以减少认知不确定性，并提高模型的可信度。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_17.png)

而动态不确定性可以直接用神经网络学习，使用似然估计技术，就像我们在今天的讲座中所学到的那样，认知不确定性是很难估计的，和，这是因为，认识的不确定性反映了不确定性，模型预测过程本身固有的标准神经网络。

确定性神经网络，我们无法获得这种不确定性的感觉，因为网络是确定性的，通过一组给定的权重传入模型的一个输入，多次会一次又一次地产生相同的输出，它将始终具有相同的固定输出，不管我们在相同的输入中输入多少次。

但我们能做的不是确定性神经网络，其中每个权重是一个确定性数，每个重量的单个数字，我们可以用概率分布来表示每一个权重，所以在这种情况下，我们将通过分布来模拟这些权重，这样，当我们向神经网络传递输入时。

我们从这个分布中的点对神经网络中的每一个权重进行采样，这意味着每次我们向模型输入输入，我们将得到一个稍微不同的输出，根据我们的重量样本，我们，我们意识到，那一次我们通过模型给它喂食。

这些模型被称为贝叶斯神经网络，这些模型分布在网络权重本身上的似然函数，因此，不是为每个重量建模一个单一的数字，贝叶斯神经网络，试着学习神经网络捕捉每个重量的完整分布。

然后用这个分布来实际了解我们模型的不确定性。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_19.png)

我们模型的认识不确定性，我们可以表述这种认识上的不确定性，并将贝叶斯神经网络的学习表述如下，而确定性神经网络学习这组固定的权重，这是给定输入数据和标签x和y的权重的概率，现在这些被称为贝叶斯神经网络。

因为他们用贝叶斯规则给出了我们权重的后验概率，他们实际上是用贝叶斯规则这样写出来的，但在实践中，这个后验是难以解析计算的，任何真实或非玩具的例子都是不可能的，这意味着我们必须求助于所谓的采样技术来近似。

试着估计这个后部，所以我们可以通过采样来近似这个后部，其中的想法是通过我们的模型进行多个随机评估，每个人都使用不同的重量样本，现在这可以用很多不同的方法来完成。

实现这种近似的一种方法是使用我们所学到的一种技术，在已经被称为辍学的课堂上，通常在训练期间使用辍学，但是现在我们讨论的是在测试期间使用辍学生，通过我们的网络获得这些多个样本。

所以它的工作方式是节点或神经元被删除或删除，或者没有退学，基于伯努利随机变量的值，我们定义为退出过程的一部分，每次我们输入我们的输入，我们通过模型的相同输入，取决于哪些节点被删除和删除。

我们会得到稍微不同的输出，这将是，这些输出中的每一个现在都将通过我们的网络代表不同的样本，或者，我们可以使用独立训练的模型集合来进行不同的采样，它们中的每一个都将学会一套独特的重量。

在看到一组独特的训练数据后，或者可能是以不同的顺序或序列显示的相同训练数据，在这两种情况下，虽然我们在这里做的很相似，我们正在从我们的重量中提取一组T样本，用这些来计算T向前传球。

在集合中使用退出或T模型，这允许我们制定两个术语，一个是对我们预测y的期望，以及我们预测的变化在这些向前传递的过程中，所以如果我们对这些预测的方差很大，所以如果我们取T随机向前通过模型。

我们有一个非常大的方差，如果我们的输出都不一致，这是一个很好的指标，表明我们的模型具有很高的认识不确定性，事实上，这不仅是它具有高度认识不确定性的一个指标，这就是认识上的不确定性。

我们的预测在这些随机向前传递上的方差。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_21.png)

而这些取样，基于知识的方法是非常常用的认知不确定性估计技术，首先，它们确实有一些非常显著的缺点和局限性，你可能已经意识到了，从他们抽取多个样本来近似这个重量分布的事实，这意味着它们需要多次运行模型。

但T次除外，只是为了获得他们对合奏的预测，它甚至比这更糟糕，因为你必须初始化和训练多个独立的模型，这在计算成本上是极其昂贵的，然后你必须将它们存储在内存中，并在内存中反复运行它们，反复相关。

这就施加了内存约束，因为你必须让所有这些模型在你的网络上并行运行，在您的计算机上，一起，这意味着抽样方法不是很有效，这对于需要不确定性估计的应用是一个显著的限制，实时制作，在边缘设备上，比如说。

在机器人或其他移动设备上，我想说的最后一点是贝叶斯方法，或贝叶斯近似方法，如跌落，倾向于产生，这在安全关键领域也可能有问题，我们真正需要校准的不确定性估计，我们真的，认为自己不确定而不是自信。

所以我们不想假设的备用情况，在备份案例中，我们有信心，我们知道我们在做什么，当我们不知道自己在做什么的时候，但是抽样方法真正想做的是什么，当他们试图近似这种不确定性时，让我们更详细地看看这个。

回答这个问题，让我们假设我们又在自动驾驶汽车的背景下工作，因为这是一个非常好和直观的例子，我们有一个模型，这是为了预测汽车的方向盘角度，给定它在左边看到的原始图像，这里的意思是，它预测了两件事。

均值和方差，这里的平均值是轮子应该的角度，方差是任意不确定性，数据不确定性，如果你还记得，现在考虑认识上的不确定性，这是模型的不确定性，我们说过很难捕捉，正如我们在基于抽样的方法中所看到的那样。

我们就可以，我们可以计算认知不确定性，使用该模型的许多独立训练的实例的集合，所以我们可以用一个模型，得到它的μ和sigma平方的估计，我们可以为这个给定的图像绘制。

模型认为μ和sigma的平方应该在这个二维图中，右边x轴上是mu，在y轴上是sigma平方，我们可以准确地把网络认为输出应该在这个二维空间中的地方，我们可以对几种不同的模型重复这一点。

每个模型我们取它的输出，我们可以在这个空间里画出来，随着时间的推移，如果我们为一群模特这样做，我们可以通过计算这些预测的方差来估计不确定性，得到不确定度的度量，直觉上，如果我们在这里得到一个巨大的方差。

与我们的缪斯女神有巨大的差异，如果答案说得不一样，如果答案很分散，这意味着我们的模型不自信，另一方面，如果我们的答案非常接近，这是一个很好的迹象，表明我们非常有信心。

因为即使在我们独立训练的所有这些不同的模型中，每个人都得到了非常相似的答案，这是一个很好的迹象，表明我们对这个答案有信心，的确，这是认识不确定性或模型不确定性，事实上，这些估计数，这些。

这些来自单独训练的模型的估计实际上是从一些潜在的分布中得出的，现在我们开始看到这个分布形状，我们从分布中提取的样本越多，它就会开始出现得越多，就像我们看到的捕捉这个分布的背景分布，虽然，而不是取样。

如果我们能直接捕捉这个分布，这可以让我们更好更全面地理解模型的不确定性，所以如果不是从这个分布中提取样本，近似于它，我们只是试图直接学习这个分布的参数，现在这种方法。

这是一系列新兴的不确定性量化方法所采取的方法，称为证据深度学习，它认为学习是一个证据获取过程，证据深度学习试图实现对任意不确定性的直接估计，以及认识上的不确定性，通过试图学习我们所说的高阶分布。

在单个似然参数上，在这种情况下，在mu和sigma参数上，我们试图了解它们上面的分布，所以要理解这一点，考虑不同类型的不确定性程度如何在这些证据分布中表现出来，当我们有很低的不确定性时。

就像我们在左手边看到的，沿着我们的均值或均值的传播，我们的西格玛方块将非常小，我们将有非常集中的密度，就在某一点上，这表明我们有低不确定性或高信心，当我们有很高的速度不确定性时，另一方面。

我们可能会看到很高的sigma平方值，我们实际上可以用沿着y轴的增加来表示，这个图的西格玛平方轴在中间表示，最后，如果我们有很高的认知不确定性，返回的亩的实际值将有很高的可变性。

所以你可以看到这个沿着MU轴蔓延，现在证据分布允许我们捕捉这些模式中的每一种，目标是训练一个神经网络，现在学习这些类型的证据分布，所以让我们来看看我们如何具体地进行证据学习，在回归的情况下，首先。

现在我们称这些分布为似然参数上的分布，证据分布，就像我现在提到的那样，要记住的关键是，当你试图把你的头围绕在这些证据分布上时，它们代表了，如果从证据分布中取样，您将获得一个全新的数据分布。

我们如何制定这个是呃，使用，或者我们如何实际地表述这些证据分布，好吧，首先，让我们像我说的那样考虑一下，持续学习问题的案例，像回归，我们假设，就像我们之前在课堂上看到的。

我们的目标标签y将从一些参数的正态分布中提取，分布参数，μ和sigma平方，这和我们之前在课堂上看到的一模一样，没什么不同，这里的关键是，而不是以前，当我们假设mu和sigma是已知的。

我们的网络现在可以预测的事情，假设我们不知道mu和sigma，我们也想概率地估计这些，我们可以正式地，我们可以通过在这些参数上放置优先级来形式化它，这些分布参数中的每一个，所以我们假设这里的分布参数。

mu和sigma平方是未知的，让我们在每个上面放置参数，并试图概率估计它们，所以从木井里抽水，我们可以从一个正常的参数化的，如下，我们可以画西格玛平方，我们与逆伽马的方差参数化如下。

使用这种证据分布的这些新的超参数，现在，这意味着μ和sigma的平方现在是从，这个正常的反伽马，也就是这两个先验的结合点，正态反伽马分布将被一组不同的参数参数化，γε，阿尔法和贝塔，这是我们的证据分布。

或者我们所说的证据先验，这是一个分布，由我们的模型参数定义的正常反伽马，当我们取样的时候，当我们从证据分布中取样时，我们实际上得到了mu和sigma平方的个体实现，这些是他们自己的高斯人。

在数据本身的顶行上定义这个原始分布，为什么我们称这些为证据分布，因为它们的密度更大，在有更多证据支持给定似然分布实现的领域，所以在左手边或者中间这里，您可以看到这种证据分布的一个示例。

这种证据分布的一种类型，用这个正态反伽马先验于mu和sigma的平方，放置在我们数据上的高斯分布的两个参数，我们的似然函数，但是你现在可以在左上角看到，在这个证据分布的空间上。

mu和sigma的不同实现，然后对应于我们似然函数的不同实现，它描述了我们目标值的分布，所以如果我们从这个证据分布中的任何一点取样，我们将得到一个μ和一个sigma平方，它定义了自己的高斯。

我们可以在右手边看到，我们也可以在分类的情况下再次考虑证据学习的模拟，请记住，如果您从证据分布中取样，你得到了一个全新的数据分布，所以为了分类，我们的目标标签y在k个类的离散集合上。

我们假设我们的类标签是从一个可能性中提取的，用某些概率参数化的范畴形式的函数，现在在这种情况下，我们可以概率地估计这些分布参数p，使用所谓的迪里希先验，在这里。

deerslaprior本身是由一组浓度参数参数参数化的，每类再次调用alpha，所以在这个分布中有k个alpha参数，当我们从这亲爱的光中取样，我们将收到分布参数的实现，分布概率。

再次定义我们的分类损失函数，正是这种分布的层次结构，让我们来看看这个证据分布的简单例子，在分类的情况下看起来像，这里，我们有三个可能的类，在这种情况下。

我们的迪里希分布的概率质量将完全存在于这个三角形单纯形上，从这个三角形内的任何一点取样，对应于对一个全新的范畴概率分布进行采样，比如说，假设我们从单形的中心取样，这将对应于三个类的相等概率。

所以对于那些没有看过简单情节的人来说，想象一下这些三角形的角代表了其中一个类的完美预测，所以说，从单纯形中间取样，对应于三类中每一类的相等概率，在其中一个角落，就像我说的。

对应于所有的质量都在其中一个类上，另外两个质量为零，在这个单纯形中的任何其他地方对应于采样的范畴分布，这是由这些类概率定义的，这些类概率必须和为1，这个三角形里面的颜色，你可以看到的蓝色渐变。

提供了这个质量如何在整个单纯形中分布的一个例子，其中分类分布可以更频繁地采样，所以这是一个例子，表示大部分质量位于单形的中心，但证据学习的全部力量是，我们要试着学习这个分布。

所以我们的网络将试图预测任何给定输入的分布是什么，所以这个分布可以改变，我们对范畴似然函数进行采样的方法将，结果也改变了，所以总结一下，以下是回归和分类的证据分布细目，在回归情况下。

目标从实数中获取连续值，我们在这里假设目标是从正态分布中绘制的，用mu和sigma参数化，然后我们在这些似然参数上有一个更高阶的证据分布，根据这个正态反伽马分布，在分类的情况下，这里的目标代表一组K。

或者在这里展示了一组k个独立的类，我们假设有可能观察到一个特定的类标签，y是从类概率p的范畴分布中提取的，这个P是从一个更高阶的证据中提取的，现在这里还有一个快速有趣的附带说明。

你可能会问自己为什么我们选择这种特定的证据分布，在每一种情况下，为什么我们选择狄利克雷分布，为什么我们选择正态反伽马分布，有很多分布，我们可以根据我们的可能性选择，选择了我们的可能性。

但我们选择了这些非常特殊的形式，因为这些被称为共轭先验，选择它们是这种形式使得分析计算我们的损失变得容易，如果我们先前或有证据的分配。

θ{\displaystyle\theta}的p{\displaystyle p}与我们的似然是同族的，给出θ的y的p，然后我们可以解析地计算这个用黄色突出显示的积分，作为我们训练中损失的一部分。

使整个过程变得可行，所以有了这些证据分布的这种表述，现在，让我们具体地考虑一下，我们如何建立和训练模型来学习这些证据分布，学习并使用它们来估计不确定性，什么是关键，这里，就是。

网络被训练以实际输出这些高阶证据分布的参数，所以在回归的情况下，我们预测伽玛，epsilonα和β，在分类的情况下，我们在预测一个K阿尔法矢量，其中k是我们有的类数，一旦我们有了这些参数。

我们可以直接为每个alatoric制定估计，和认识上的不确定性，这是由这些似然参数上的结果分布决定的，我们通过结合模型拟合的最大化来优化这些分布，并将错误证据最小化到回归的目标中，这表现如下。

我们的最大化捕捉看到数据的可能性，给定由这个证据先验参数控制的似然参数，表示为M，在这个正规化术语中捕获了不正确证据的最小化，在右手边，通过最小化，我们试图在模型的情况下降低不正确的证据。

模型出错的地方，所以想想右边，这确实符合我们所有的证据分布和我们的数据，左手边膨胀了这种不确定性，当我们看到我们得到一些不正确的证据时，开始在训练中犯一些错误。

我们可以在一些简单的玩具示例学习问题上实际评估这种方法，在那里我们得到了分布中的一些数据点，在我们这里的场景中的一些区域，在那里我们寻求预测目标值，或者在某些情况下回归中的类标签，我们有一个案例。

我们试图适应我们的数据集，我们在中间白色区域有数据，但我们没有两个边缘区域的数据，我们可以看到我们的证据分布能够膨胀，我们不分布的地区的不确定性是，到底是什么，我们想看到。

我们能够认识到那些我们没有数据的预测，不应该被信任，类似地，在mnist数据集上操作分类的情况下，我们可以从分布示例中生成，通过合成旋转手写数字，所以在底部你可以看到一个例子，一个数字从左到右旋转。

你可以看到我们证据分布的不确定性越来越大，在这种非分配制度下，在那里，一个甚至不再与一个非常相似，但两个端点的不确定性真的下降了，在那里一个回到一个真正的形状。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_23.png)

证据学习也可以应用于更复杂的高维学习应用中，最近，已经证明学习神经网络可以输出数千个证据分布，同时，学习量化单目深度估计器像素方向的不确定度，仅给定原始RGB输入，这是一个回归问题。

因为每个像素的预测深度是实数。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_25.png)

这不是一门课，但在分类的情况下，证据深度学习也被应用于不确定性感知，原始激光雷达点云的语义分割，也是极高的维度，其中点云中的每一点都必须预测，必须预测它属于哪个对象或什么类型的类。

证据深度学习使我们不仅可以将这些点分类为一个对象，还能认出场景中的哪些物体，表达一个我们不知道答案的对象的形式，证据深度学习真的给了我们表达一种形式的能力，我不知道它什么时候在输入中看到了什么。

它不知道如何自信地预测，它能够让用户知道什么时候它的预测不应该被信任。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_27.png)

现在开始包装，我想提供一个简单的比较所有不同类型的不确定性方法，我们今天所学到的估计，以及证据神经网络如何适应这些，我们今天谈到了三种技术，首先从马尼拉的可能性估计开始，然后转向贝叶斯神经网络。

最后探索证据神经网络，这些方法中的每一种都有自己的不同之处，长处和优势，在最高差异水平上，我们从根本上看到，这些方法中的每一种都在管道的不同方面放置了概率先验，在数据上。

在我们早期看到的似然估计的情况下，在讲座中，贝叶斯神经网络中的权重，在证据神经网络的情况下，超过似然函数本身，与贝叶斯神经网络不同，尽管证据神经网络非常快，记忆效率很高。

因为他们不需要任何抽样来估计他们的不确定性，尽管这两种方法都捕捉到了一种认识上的不确定性，这是一个巨大的优势，这意味着你不需要训练一群模特，你可以只训练一个模特，您只需对每一个输入运行一次，不需要取样。

所以总而言之，在这节课中，我们将深入研究使用神经网络进行不确定性估计，这是现代机器学习中一个极其重要的问题，以及我们真正开始将我们的模型部署到现实世界中。

我们需要尽快理解什么时候我们应该信任它们更重要的是，当我们不应该，我们了解了一些不同形式的不确定性，以及这些不同的方法如何帮助我们捕捉数据中的不确定性，以及模型中的不确定性，最后。

我们必须了解如何使用证据，深度学习学习快速可扩展，用神经网络标定不确定性的表示，感谢您参加本次讲座，在下节课中，我们将讨论当今世界另一个非常有影响力的话题，关注人工智能偏见与公平。

并看到一些减轻这些模型不利影响的策略，所以我们也很期待你的演讲。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/fc32e8ddda39c027f8a8441b5be12695_29.png)

# P8：AI Bias and Fairness - 爱可可-爱生活 - BV1jo4y1d7R6

嗨，大家好，欢迎来到我们六一九一年的第二次热门话题讲座，我们最近将在那里学习算法偏见和公平，这个话题正在成为现代深度学习中一个真正普遍的问题，更普遍的是AI。

这是在人工智能管道的各个阶段都可能发生的事情，从数据收集到模型解释，在这次讲座中，我们不仅会了解什么是算法偏差以及它是如何产生的，但我们也将探索一些新的令人兴奋的方法进步。

在那里我们可以开始思考如何制造机器，能够识别并在某种程度上实际减轻这些偏见。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_1.png)

算法偏差的概念指出了神经网络和人工智能系统，更广泛地说，容易受到重大偏见的影响，这些偏见会导致非常真实和有害的社会后果，今天比以往任何时候，我们已经在社会上看到了这一点。

从面部识别到医疗决策再到语音识别，最重要的是，算法偏见实际上会延续现有的社会和文化偏见，例如种族和性别偏见，现在我们开始欣赏并认识到深度学习中的算法偏差，是一个真正普遍和严重的问题。

从这一点开始我们真的需要各个层面的策略来真正解决这个问题，首先，我们必须理解算法偏见到底意味着什么。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_3.png)

所以让我们考虑一下这个图像，你在这张照片中看到了什么，你如何描述它，你可以说描述这个形象的第一件事是西瓜，如果我告诉你仔细看看，更详细地描述一下呢，好的，你可能会说西瓜片或者西瓜籽。

或者像多汁西瓜这样的其他描述符，层层西瓜，西瓜片挨着，但当你自言自语的时候，我想知道你们中有多少人想过把这个形象描述为红色西瓜，如果你像我一样，很可能你没有，现在让我们考虑一下这个新图像。

这里的图像中有什么，现在你可能更有可能放置黄色描述符，在描述这个西瓜的时候。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_5.png)

你的最佳答案可能是黄色西瓜，然后用带种子的切片，多汁，等等等等，但为什么会这样呢，我们为什么不说红色西瓜，当我们看到这张原始图像时，当我们看到这样的图像时，我们倾向于把它当成西瓜，而不是红色的西瓜。

那是因为我们自己的偏见，比如说，由于地理原因，我们习惯了看到这样的西瓜，有这个红色，因为这代表了我希望看到的典型西瓜肉，但也许如果你来自世界的另一个地方，黄色西瓜的发源地。

你可以对西瓜应该是什么颜色有不同的原型感觉，这指出了一个更广泛的事实，即我们是如何，当人类去感知和理解世界时，在生活的各个方面，我们倾向于给事物贴上标签和分类，作为一种强加秩序的方式，简化和理解世界。

结果，这意味着，对于所有事物，通常都会有一些典型的表示，我们可以认为是一个原型，根据我们每个人观察到的频率，我们的倾向是指出不适合我们的东西，作为个人认为的规范，那些对我们来说不典型的事情，比如说。

给我的黄色西瓜，当特定的标签时，批判性的偏见和刻板印象可能会出现，会扰乱我们的决策，不管是人类驱动的还是算法建议的，在这节课中，我们将重点讨论算法偏差的来源，并讨论一些新出现的方法来对抗它。

让我们首先考虑偏见在深度学习中是如何表现出来的，和AI，我们今天看到的深度学习中最普遍的偏见例子之一，在面部检测方面，最近，有几个评论分析实际上评估了商业的表现，面部检测与分类系统，比如说。

性别分类器分析，这篇评论显示，商业管道在深色女性脸上的表现明显更差，相对于其他人口群体，和另一个分析，它考虑了面部识别算法，再次发现，有色人种女性的错误率最高。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_7.png)

这种算法偏见的概念可以以无数种不同的方式表现出来，作为另一个例子，让我们从总体上考虑图像分类问题，假设我们有一个训练有素的CNN，左边的这张照片，它展示了一个典型的例子，一个新娘在北方。

现在的美国和欧洲国家，在最近的分析中，当这张新娘的特殊照片被传到CNN时，它是在开源大规模图像数据集上训练的，CNN输出的预测数据类标签是，也许不出所料，像新娘这样的东西，连衣裙，婚礼仪式，预期的女性。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_9.png)

当这个图像，这是世界其他地方新娘的典型例子，比如在南亚，被传到了同一个CNN，现在预测的阶级标签实际上并没有反映新娘的磨牙标签，服装事件，服装艺术，因为你在这里看不到任何关于新娘或婚礼的东西。

甚至是一个人，如此清晰，这是一个非常，非常重要的问题，这根本不是深度学习所期望或期望的行为，我们可能认为已经解决了报价，未引用图像分类。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_11.png)

事实上，我在这里展示的类似行为，在另一个物体识别设置中也观察到了，当再次，这张香料的照片是从北美的一个家庭拍摄的，被传给了CNN，受过物体检测训练，目标检测与识别，此图像中检测到的对象的标签符合预期。

调味料香料，香料架配料，正如我们现在所期望的那样，对于左边显示的香料图像，实际上是从菲律宾的一个家里拿走的，当这张照片被传到同一个CNN时，再一次，预测的标签没有反映地面。

真相标签这个图像又是香料的图像，我指的是一些非常非常有关的事情，现在，这个分析真正有趣的是，他们问，好的，我们不仅观察到这种偏见。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_13.png)

但真正的驱动因素是什么这种偏见的原因，从这个分析中发现，物体识别模型的准确性实际上与房屋的收入有关，测试图像是在哪里拍摄和生成的这表明这些算法有明显的偏见，支持来自高收入家庭的数据。

而不是来自低收入家庭的数据，为什么这可能是这种偏见的来源，嗯，事实证明，用来训练这样一个模型的数据，其中绝大多数是从美国拿走的，加拿大和西欧，但在现实中，这种分布与世界人口的分布完全不匹配。

鉴于世界人口的大部分在东亚和南亚，所以我认为这是一个非常有力的例子，因为它可以表现出，它展示了偏见是如何永久化并在多个层面上存在的，在深度学习或人工智能管道中。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_15.png)

这个特殊的分析开始发现和挖掘其中的一些偏见，而且确实，正如我提到的，偏见确实会毒害人工智能发展和生命周期的所有阶段，从不平衡的数据开始，关于类标签甚至特性，会对模型本身造成不必要的偏见。

实际培训和部署管道，这可能会加强和延续对评价的偏见，以及分析的类型，应该做的，评估不同人口结构和子群体的公平性和绩效，最后，在我们人类对结果和结果的解释中，这些人工智能系统的决定，在那里。

我们自己可以注入人为的错误，强加我们自己的偏见，扭曲这些结果的含义和解释。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_17.png)

所以在今天的讲座中，我们将探索算法偏差的问题，在第一方面，这种偏见的不同表现和来源，然后我们将讨论不同的策略，减轻这些偏见中的每一个，并最终致力于提高人工智能算法的公平性，这绝不是一个已经解决的问题。

事实上，这个讲座背后的设置和动机是介绍这些主题，所以我们可以开始思考如何继续推进这个领域。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_19.png)

所以说，让我们从思考一些，在深度学习系统中可能表现出的常见类型的偏见，我认为我们可以把这些大致归类为数据驱动或解释驱动，在数据驱动侧，我们经常会面临数据被选择的问题，使得没有实现适当的随机化。

或数据中特定类型的数据或特征的表示频率较高或较低，相对于其他人，以及我们作为用户可用的数据的例子，并不反映现实世界中特定情况发生的可能性，所有这些，正如你所看到和欣赏的，都是非常，非常交织和相关。

解释驱动偏差，更多地提到人类对结果的解释如何永久化的问题，其中一些类型的问题，比如说，关于错误地将相关性和因果关系等同起来，试图对人工智能系统的性能或推广得出更普遍的结论，即使面对非常有限的测试数据。

最后，实际上支持或信任算法的决定，而不是人类的决定，我们绝不要，这是对可能存在的常见偏见的调查吗，它只是为了让你思考不同的方法，和不同类型的偏见，所以今天我们要谈谈这些类型的偏见，首先我想从解释开始。

驱动的相关性问题，谬误，和过度概括好吧，所以让我们假设我们有一个情节，正如你所看到的，显示了两个变量随时间的趋势，正如你所注意到的，这两个变量的数据在一起跟踪得很好，让我们说，事实证明，事实上。

这些黑点显示了美国获得计算机科学博士学位的人数，我们可以很容易地想象建立一个机器学习管道，可以使用这些数据，预测给定年份将授予的计算机科学博士学位的数量，具体地说，我们可以使用红色变量。

因为它似乎与CS的数量有很好的相关性，博士们试图作为我们机器学习模型的输入，试图预测黑色变量，最终我们想做的是，你知道的，对特定数据进行训练，从特定的时间框架设置并在当前的时间框架中进行测试。

二十一或更远，试图预测将被授予的计算机科学博士学位的数量，嗯，事实证明，这个红色变量实际上是拱廊产生的总收入，在某一年，这是一个变量，与计算机科学博士学位的数量相关，在这个特定的时间框架内，但事实上。

它掩盖了一些潜在的因果因素，这就是最终推动计算机科学博士学位数量增长的原因，这是相关性谬误的一个例子，相关性谬误实际上会导致偏见，因为在这样的数据上训练的模型产生了拱廊作为输入产生的收入。

计算机科学博士，因为输出很容易分解，因为它实际上没有捕捉到导致，我们最终试图预测的变量中观察到的趋势，所以相关性谬误不仅仅是相关性，不等同于因果关系，当错误或不正确地使用时，它也会产生和延续偏见。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_21.png)

让我们也考虑过度概括的假设，所以说，让我们假设，我们想在一些马克杯的图像上训练CNN，从一些精心策划的内部数据集，并采用我们的结果模型，并部署到现实世界中，试图很好地预测和识别杯子，马克杯。

现实世界中的实例，可能与训练模型的实例不太相似，和过度概括，假设和偏见意味着，或者反映了这样一个事实，即我们的模型可以在选定的马克杯表现形式上表现得非常好。

那些与它所看到的训练例子相似但实际上失败并显示性能的例子，在数据中表现不太显著的马克杯上表现不佳，尽管我们希望它能概括，这种现象通常可以被认为和描述为分布转移，它确实会使网络在示例上的性能更差，嗯。

它以前没有遇到过的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_23.png)

最近提出的一项战略，试图缓解，这种偏差的来源是从数据集开始的，并试图构建一个改进的数据集，该数据集已经考虑到潜在的分布变化，这样就完成了，比如说，通过指定，说，训练用图像。

然后相对于特定变量移动以构造测试数据集，例如，在这种情况下，列车和测试之间发生的分配偏移是，关于这里图像的时间和地理区域。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_25.png)

或在医学图像的情况下，这可能意味着从不同的医院为每个列车验证和测试获取数据，随着对分配转移问题的更多认识被揭露，数据集，就像，这实际上可以帮助尝试驯服和调整可能发生的泛化偏见。

因为它们内在地强加了在发行版上已经测试您的模型的必要性，移位实例系列，所有的权利，所以希望这能给你一种解释的感觉，驱动偏见，以及为什么它们会有问题，接下来我们将把大部分注意力转向，依我看。

深度学习中一些最普遍的偏见来源和形式，它们是由首先出现在数据中的类或特征不平衡驱动的，让我们考虑一下阶级不平衡是如何导致偏见的，让我们考虑如下所示的一些示例数据集或如下所示的一些示例数据，和，让我们说。

那个，左边的这个情节，显示了我们试图建模的数据的现实世界分布，关于某些系列的类，让我们假设我们在该数据中可用的数据，这些课程的频率实际上与现实世界中发生的完全不同。

在这些类之间对模型的准确性会产生什么影响，模型的准确性是否反映了数据的真实分布，否，取而代之的是，模型的准确性最终可能会有偏差，根据它具体看到的数据，使它偏向于改进或更高的精度。

而不是在更频繁出现的类上，这绝对不是想要的，最终想要的是什么，我们希望得到的模型在性能方面是无偏见的，它在这些不同类别中的准确性，不同类别的准确性应该大致相同。

如果我们的目标是训练出一个在所有这些课程中表现良好的模型，为了理解我们如何实现这一点，我们首先需要理解为什么从根本上，班级不平衡可能会给实际训练模型带来问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_27.png)

了解这个问题的根源，让我们考虑一个简单的二进制分类任务，假设我们有这个数据空间，我们的任务是构建一个分类器，它可以看到数据空间中的某个点，并将它们归类为橙色或蓝色，我们从随机初始化分类器开始学习。

这样就把这个空间分开了，让我们假设我们开始看到数据点，它们开始被输入模型，我们的数据集是类不平衡的，这样每一个橙色点，模型看到它现在会看到20个蓝点，学习的过程，你从梯度下降中知道。

将对分类器进行增量更新，根据它所观察到的数据，例如，在这种情况下，看到这些蓝点之后，转变将是试图根据这些特定的观察来移动决策边界，这将会发生，现在我们做了一个更新，更多的数据会一次又一次地进来。

他们都将再次成为蓝点，由于这种一到二十的班级不平衡，结果，决策边界会相应地移动，到目前为止，我们所看到的随机样本反映了这种潜在的阶级失衡，但是让我们假设现在我们第一次看到一个橙色的数据点。

这个决定边界会发生什么，嗯，它会移动，试图将决策边界移近橙色点，来解释这一新的观察，但最终请记住，这只是一个和一个橙色点，每一个橙色的点，我们就会看到20个蓝色的点，所以最后，我们分类器的决策边界将以。

占据更多蓝色空间，因为它会看到更多的蓝色样本，而且会偏向多数阶级，所以这是一个非常，学习如何最终倾斜的非常简单的例子，由于明显的阶级不平衡，我向你保证，阶级不平衡是一个非常非常普遍的问题。

在处理真实世界的数据时，您几乎肯定会遇到这种情况，你必须处理和策划。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_29.png)

事实上，一个阶级不平衡特别相关的环境是在医学和医疗保健领域，这是因为许多疾病的发病率，比如癌症实际上是相对罕见的当你观察普通人群时，所以要理解为什么这会有问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_31.png)

以及为什么这不是一个理想的训练和学习环境，让我们想象一下，我们想尝试建立一个深度学习模型，从医学图像中检测癌症的存在，如核磁共振扫描，让我们假设我们正在研究一个脑瘤，称为胶质母细胞瘤。

这是现存最具侵袭性和最致命的脑瘤，但也很罕见，大约每十万人中就有三人发生，我们的任务是训练CNN，从大脑核磁共振扫描中检测胶质母细胞瘤，和，让我们假设，我们数据集中的类别发生率反映了真实的。

该疾病的世界发病率，意味着对于十万个大脑扫描的数据集，他们中只有三个人真的得了脑瘤，如果用这种方式训练模型会有什么后果，请记住，分类模型最终是被训练以优化其分类精度的。

所以这个模型基本上可以追溯到一直预测健康，因为如果它这样做了，它实际上会达到九十九点，百分之九十九七的准确率，即使当它看到脑瘤时，它预测是健康的，因为这是健康在其数据集中发生的速度。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_33.png)

很明显，这是极有问题的，因为建立这个管道的全部目的是检测肿瘤，当他们出现的时候，所有的权利，那么我们如何减轻这种情况，为了理解这一点，我们要讨论两个，在学习过程中经常用来实现班级平衡的非常常见的方法。

让我们再次考虑我们的简单分类问题，其中我们随机初始化分类器，划分我们的数据空间，缓解这种阶级不平衡的第一个方法是选择，并以班级平衡的方式分批喂食，那意味着，我们现在将使用显示一比一类别比例的数据批。

在学习过程中，我们的分类器将看到关于这些类的相等表示，并相应地移动决策边界，再一次下一批进来的人又会是阶级平衡的，决策边界将再次更新，我们的最终结果将是一个相当合理的决定边界，大致相等地分割空间。

由于数据，模型所看到的信息比所看到的要多得多，数据明显不平衡，并在实践中，这种平衡的批量选择是一项极其重要的技术，试图缓解，这个问题，另一种方法是实际权衡单个数据点被选择用于训练的可能性。

根据它们在数据集中出现的频率的反比，所以更频繁的课程会有更低的权重，不太频繁的类会有更高的权重，最终的结果是，我们将产生一个类平衡数据集，不同的班级最终将对模型的学习过程做出同等的贡献，另一种方式。

我们可以可视化这个重新加权的想法是通过使用这些数据点的大小，反映他们在训练期间被选中的概率，什么例子，等待的意思是我们可以增加在训练中选择稀有课程的概率，并降低选择公共类的概率。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_35.png)

到目前为止，我们一直关注阶级平衡的问题，阶级失衡，并讨论这两种缓解阶级失衡的方法，如果我们的阶级是平衡的，每个阶级内部还会有偏见和不平衡吗，为了得到这个，让我们考虑一下我们试图训练面部检测系统的问题。

让我们说，我们有相等数量的脸和非脸的图像，我们可以用来训练模型，可能有隐藏的偏见潜伏在每个类中，事实上，这可能更难识别，甚至更危险，这可能反映了班级内部特征空间缺乏多样性，也就是说，此数据的潜在空间。

继续面部检测示例，这种特征的一个例子可能是个人的头发颜色，他的图像在我们的面部类数据中，事实证明，在现实世界中，发色的地面真实分布在七十左右，世界上百分之五到八十的人口有黑头发。

18%到20%的人有棕色头发，2%到5%的人有金发，大约2%的人有红头发，然而，常用于图像分类和人脸检测的一些金标准数据集，根本没有反映这种分布，因为它们已经结束了，代表棕色和金色的头发，下面代表黑发。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_37.png)

当然还有，与此形成鲜明对比的是，一个完美平衡的数据集将对这四种头发颜色有相等的表示，我在这里说，这是一个故意的和结束的，故意把问题过分简单化，事实上，包括头发颜色在内的所有特征都存在于光谱中。

数据空间中的光滑流形，所以理想情况下，我们最终想要什么，是一种，我们可以捕捉到更多关于这些特性如何在数据流形中分布的微妙之处，并利用这些知识积极消除我们深度学习模型的偏见，但为了本例的目的。

让我们继续简化视图，让我们假设我们把这个金本位数据集，并用它来训练CNN进行面部检测，在测试时最终会发生什么，我们的模型最终对它的性能有偏见，在这些不同的头发颜色人口统计中。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_39.png)

而且确实，正如我在本课开始时介绍的那样，这些完全相同类型的偏见在大规模中表现得相当强烈，商用级，面部检测与分类系统，我认为这些考虑加在一起提出了一个关键问题，我们如何真正识别潜在的偏见。

这些偏见可能是隐藏的，而不是非常明显的，比如肤色或者发色，我们如何将这些信息集成到学习管道中，从那里会有一个超越这一步，学习管道和技术如何实际使用这些信息来减轻这些偏见，一旦他们被识别出来。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_41.png)

这两个问题介绍了深度学习中一个新兴的研究领域，这就是使用机器学习技术的想法，实际提高这些系统的公平性，我认为这可以通过两种主要的方式来实现，第一个是偏见缓解的想法，在这种情况下。

我们给出了一些偏差模型数据集学习管道，这里我们想尝试应用一种机器学习技术，它旨在消除信号中导致不必要偏见的方面，结果是这种偏见得到了有效的缓解，沿特定轴减小，我们从中移除信号，产生一个公平性提高的模型。

我们也可以考虑技术，而不是试图消除信号，尝试添加回信号，以更多地包含数据空间中代表性不足的区域，或特定人口结构，最终增加模型看到特定数据切片的程度，和一般情况下，是一个研究领域。

我希望在未来几年能继续发展和进步，随着这些问题越来越多。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_43.png)

所有权利，因此，讨论和理解学习技术是如何减少偏见和提高公平性的，我们首先需要设置一些定义和度量，关于我们如何正式评估偏见或公平，机器或深度学习模型的，所以为了这些例子，我们将考虑监督学习的设置。

具体分类，分类器最终应该在一些敏感特征或特征系列中产生相同的输出决策，考虑到它应该预测什么，因此从这里移动，如果分类器的决策改变，我们可以定义它是有偏见的，在暴露于特定敏感特性或特性输入后。

这意味着对于特定变量z是公平的，如果分类器的输出相同，不管我们是否以那个变量为条件，所以说，比如说，如果我们有一个二元变量z，预测正确的可能性应该是一样的，是z等于零还是z等于一，所以说。

这给出了一个框架，我们可以考虑，如何评估监督分类器的偏差，我们可以做到这一点，我们可以更进一步来实际定义性能指标和评估分析，确定这些偏见和公平程度，通常做的一件事是衡量不同子群体或人口统计数据的表现。

我们对此感兴趣的是这种加重评估，所以让我们说，如果我们在处理彩色形状，这可能是关于保持形状不变的颜色特征，或者保持颜色不变的形状特征，我们也可以看看表演，在不同子群体或人口统计数据的交汇处插入。

在我们的形状和颜色的例子中，这意味着同时考虑颜色和形状，并将蓝圈与，在橙色方块上的表现等等，等等，现在我们已经定义了一个什么样的集市，一个公平的分类器会是什么样子。

还有一些我们可以评估分类系统偏差的方法，我们现在有了一个框架来讨论一些最近的工作，这些工作实际上使用了深度学习方法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_45.png)

减少监督分类方面的偏见，所以第一种方法使用多个，本框架下的任务学习设置和对抗性训练，它的工作方式是我们，人类用户需要从指定一个属性开始，我们试图设计来对抗Z，学习问题是这样的，我们训练。

我们将训练一个模型来共同预测和输出Y，以及这个属性z的值，所以给定一个特定的输入x，网络将，这将通过嵌入和隐藏层传递到网络，在输出端，网络将有两个磁头，每个磁头对应于其中一个预测任务。

首先是对目标标签y的预测，和，二是敏感属性值的预测，我们试图贬低的，和，我们的目标是，去尝试，以消除敏感属性的任何混淆影响，关于任务预测决策的结果。

这种效果的消除是通过在训练中强加一个敌对的目标来完成的，特别是，通过在反向传播过程中否定属性预测头的梯度，和，这样做的效果是，以消除该属性预测对，任务预测，当这个模型被提出时。

它首先应用于一个语言建模问题，其中指定的敏感属性为性别，感兴趣的任务是完成类比的问题，目标是预测可能填充类比的单词，比如说，他是个像医生一样的女人，当一个有偏见的模型在这个特殊的类比上被测试时。

它返回的顶级预测是护士之类的，保姆，未婚夫，这清楚地表明了潜在的性别偏见，然而，无偏模型，使用这种多任务方法，指定性别作为属性，更有可能返回儿科医生或医生的例子，或医生的同义词。

建议在一定程度上减轻性别偏见，然而，这种方法的主要限制之一是对我们的要求，人类用户指定要针对其设计的属性，这可能在两个方面受到限制，首先，因为可能会有隐藏的和未知的偏见。

这些偏见从一开始就不一定是明显的，最终，我们实际上也想对这些有偏见，此外，通过指定敏感属性是什么，我们，人类可能无意中传播了我们自己的偏见，通过告诉模型我们认为它对什么有偏见，所以最终我们想要的。

我们所渴望的，是一个自动化系统，可以试图识别和发现数据中潜在的偏见，没有任何注释或规范。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_47.png)

事实上，这是生成模型的一个完美用例，特别是那些能够学习和发现数据集中潜在变量的人，在面部检测的例子中，如果我们得到一个有许多，许多不同的面孔，我们可能不知道是什么。

这个数据集中特定潜在变量的精确分布将是，在这些不同的变量方面可能会有不平衡，比如说，面部姿势，最终可能会，在我们的下游模型中导致不必要的偏差，正如你在第二实验室使用生成模型所看到的。

我们实际上可以学习这些潜在的变量，并利用这些信息自动发现，潜在景观区域中代表性不足和代表性过高的特征，并利用这些信息来减轻其中的一些偏见，我们可以通过使用变分自动编码器结构来实现这一点，在最近的工作中。

我们展示了基于这个VA网络架构，我们可以利用它来了解数据集的潜在结构，在一个完全公正的，无人监督的方式，比如说，在人脸图像的情况下拾取，特别是潜在的变量，如方向，这些变量再次从未被指定到模型中。

它把它作为一个特殊的潜在变量来学习，通过观察许多不同的面孔例子，并认识到这是这种学习到的潜在结构的一个重要因素，然后我们可以估计这些学习到的潜在变量的分布，这意味着这些潜在变量可以取的值的分布。

某些情况会被过度代表，例如，如果我们的数据集有许多特定肤色的人脸图像，这些将被过度代表，因此，选择特定图像的可能性，在训练中有这种特殊肤色的人会不公平地高，这可能会导致不必要的偏见，有利于这些类型的脸。

反过来说，有阴影等罕见特征的脸，深色眼镜，帽子可能在数据中代表性不足，因此，选择具有这些特性的实例进行实际训练的可能性，模型会很低，由此揭示潜在结构的分布，从而产生不必要的偏差。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_49.png)

我们展示了，这个模型实际上可以自适应地调整单个数据实例的采样概率，在训练过程中重新称重，这样这些潜在的分布，和，这种重采样方法可以用来，自适应地生成更公平、更有代表性的培训数据集。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_51.png)

深入挖掘这种重采样操作背后的数学原理，这种方法的关键点是，通过该联合直方图近似出潜在空间分布，在单个潜在变量上，具体来说，我们估计每个潜在变量的单个直方图，为了这个近似的目的，假设这些潜在变量是独立的。

这样我们就可以利用他们的产品来得出一个共同的估计，整个潜在空间的联合分布的估计，并根据这一估计的联合分配，然后，我们可以定义在训练期间对特定数据点x采样的调整概率，基于该输入实例x的潜在空间，特别是。

我们定义选择该数据点的概率，根据潜在空间上近似联合分布的逆，它再次由这些单独的直方图定义，并进一步由偏置参数alpha加权，它调整使用这种方法所需的去偏置程度，并将其应用于面部检测。

我们展示了我们实际上可以，我们可以，实际上增加了对特征不足的人脸重新采样的概率，这在质量上表现出来，当我们检查顶部的面时，重采样概率最低和最高。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_53.png)

分别是，然后我们可以在训练过程中部署这种方法来实际选择批次，以便用，这种习得的偏置算法对于肤色等特征会更加多样化，姿势与照明，这种方法的力量在于它进行了这种重采样操作，基于自动学习的学习特性。

不需要人类的注释，属性或偏见应该是什么，因此它更容易推广，并允许同时消除对多种因素的偏见，来评估这个算法实际上在多大程度上减轻了偏见，我们对最近的面部检测系统评估基准数据集进行了测试。

平衡了男性和女性的性别以及肤色。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_55.png)

并确定存在的偏见程度，我们评估了此数据集中跨子组的性能，根据男女注释和肤色注释分组，当我们首先考虑模型的性能时，没有任何偏见，所以所谓的有偏见的模型，我们观察到它在深色男性身上表现出最低的准确性。

在轻邮件上的准确率最高两者相差约12%，然后我们将这个精度与D偏置模型的精度进行了比较。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_57.png)

发现随着D偏置的增加，总体增加，特别是关于子集和子组，比如深色的雄性和深色的雌性，而且批判性地，深色男性和浅色男性在准确性上的差异显著减小，用降偏模型，表明这种方法实际上可以显著减少分类偏见。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_59.png)

在今天的讲座中，我们已经探索了不同的偏见是如何在深度学习系统中产生的，它们是如何表现的，我们还讨论了一些新出现的策略。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_61.png)

实际上使用深度学习算法来减轻这些偏见，最后，我想通过提供一些观点来结束，我认为提高人工智能公平性的关键考虑因素，第一个是我喜欢称之为最佳实践的，在人工智能的科学和实践中应该成为标准的事情。

比如提供文档和报告，关于公布数据集的报告，以及总结训练等事情的模型，信息评价指标与模型设计，这个存在的目标，提高这些数据集和模型的重现性和透明度，当它们被使用和部署时，我认为需要采取的第二类步骤。

这些新的算法解决方案，在学习过程的各个方面实际检测和减轻偏见，今天我们考虑两种这样的方法，但是在这个领域还有很多工作要做，真正构建健壮和可伸缩的方法，这些方法可以无缝地，无缝集成到现有和新的AI管道中。

实现，我认为第三个标准将是数据集生成方面的改进，在来源和代表性方面，以及关于分配转移，以及可以成为标准做法的正式评估，评估新模型的公平性和潜在偏差，这些模型首先是输出的。

我认为真正关键的是持续的对话和合作，人工智能研究人员和从业者，以及最终用户，政治家，公司，伦理学家，因此，人们对算法偏见的潜在社会后果有了更多的认识和理解，并进一步，讨论可以减轻这些偏见的新解决方案。

并促进包容性和公平，我将以此结束。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_63.png)

我想提醒你们，对于那些参加这门课的人，实验室竞赛的参赛作品将于美国东部时间今天午夜提交，请在画布上提交，一如既往，如果你们对这节课有什么问题，实验室，课程的任何其他方面，请到球场来，聚集城镇。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/515efc568e08e8a4a32823f6704295a3_65.png)

# P9：Deep CPCFG for Information Extraction - 爱可可-爱生活 - BV1jo4y1d7R6

谢谢，所以我在全球eui带领呃，今天，我们将与您讨论我们已经完成的一些工作，信息提取专门针对深层cpcfg，因此希望如此，我们将介绍一些您可能以前从未遇到过的概念，在我们开始之前，可能会免除一些意见。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_1.png)

在此演示文稿中表达的是我的和，星期五不一定是我们雇主的，我会让你阅读其他免责声明，AI对于ernst年轻非常重要，你们中的许多人都会很熟悉。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_3.png)

与该公司一起，我们是一个全球性组织，在150多个国家/地区的30万多人中，我们为客户提供一系列服务，包括保证，咨询策略，转型与税收，无论是在我们提供的服务方面，还是在我们的环境中。

客户自己的转变非常重要，我们看到许多行业正在发生巨大的破坏，包括我们自己由ai驱动的活动，正因为如此，我们在，在这方面的投资，我们已经建立了全球性的网络。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_5.png)

遍布全球的AI研究与开发实验室网络，您在世界各地看到的世界，我们也有，大量的全球AI交付中心，我们对艾有极大的热情和激情，这样我们的社区就有超过四千五百个成员，在内部，我们保持呃有意义。

与学术机构的关系，包括，例如麻省理工学院，我们当然会积极参与，政策制定者监管者立法者和非政府组织围绕与人工智能有关的问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_7.png)

我们需要特别关注的一些领域，首先是文件情报，今天我们要讨论的，这实际上是，重新使用AI来阅读和解释业务文档，关键是有一些我们不一定要谈论的商业文件，关于电子邮件或网络帖子或社交媒体帖子，或，产品说明。

我们谈论的更多是诸如合同之类的东西，租赁合同收入合同雇用协议，我们在谈论法律法规，我们正在谈论发票，采购订单和证明之类的文件，交货的范围很广，各种文档，数十万甚至数十万，这里有成千上万种不同类型的文档。

我们在100多个国家/地区以100多种语言显示了这些信息，在数十个不同的行业和行业领域中，在ui，我们已经建立了一些我认为在我们所拥有的空间中真正引人注目的技术。

构建了一些现在可以在更多产品中部署的产品，超过85个国家/地区，并有数千个项目使用，而且这个空间对我们来说非常重要，我们帮助，在nurips组织了有史以来第一次有关文件智能的研讨会，在2019年。

我们当然会在该领域发布并申请专利，对我们很重要的其他两个领域将不会成为本主题，说话，但我想我会暗示，是交易智能，所以这里的想法是，我们看到，我们的客户执行许多交易，并且我们出于税收目的审查了这些交易。

以及出于审计目的，我们将处理数千亿，一年的交易量，交易数据，并且存在巨大的机会，机器学习和人工智能，以帮助分析那些交易，以帮助，确定例如税收或会计处理方法，还可以做诸如识别异常或异常行为之类的事情。

或者，潜在地识别欺诈对另一个非常重要的领域，我们是值得信赖的，因此，鉴于我们在，金融生态系统我们是金融信任的提供者，市场，我们必须帮助我们的客户，我们自己以及，广泛的生态系统建立了围绕AI的信任。

我们参与其中，大量与学者ngos和，监管者和立法者为了实现这一目标，但这次谈话的目的是要谈论，这次演讲的目的实际上是在谈论文档智能。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_9.png)

特别是我们将讨论从什么中提取信息，我们称半结构化文档之类的东西，税表，您会在屏幕上看到该文件，在您面前，这是一种报税表，其中将包含相关信息，形成那个，这些框中包含的内容，因此我们需要将其拉出。

这些形式的信息往往相对简单，因为，始终位于这些位置，但您会阅读，这些信息或提取这些信息不只是一个问题，阅读文字，我们还必须获取布局信息，考虑到这一点，甚至存在一些复杂性，这份文件是正确的。

这里有一个财产的描述，这是一个，列表，因此我们不知道此列表中可能有多少个条目，一个可能有两个可能会有更多，当然，例如当这些文件是，手写的或何时扫描的或何时。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_11.png)

更多变量，例如，一张支票，你们中很多人可能拥有，个性化支票，这些支票在，就其背景而言，就其布局而言，通常它们是手写的，通常当我们看到它们时，扫描的质量通常很差，并且，因此提取信息可能会带来挑战。

这又是驱动力，在很大程度上，由于可变性，我们有发票等文件，发票非常简单，但您会注意到，知道这里有订单项，每个都有多个订单项，其中的一个对应于该发票下的单个交易，并且可能有零个订单项。

或者可能有十个或数百个或，成千上万甚至在某些情况下成千上万，发票中的订单项数量具有挑战性，因为，企业可能有成千上万，甚至数以万计的供应商，并且每个供应商都会有一个，发票格式不同，因此。

如果您尝试将代码规则提交给，从这些文档中提取信息，它往往会失败，因此机器学习，方法的设计确实是为了应对这些文档中的这种变化，类型和这种复杂的信息提取。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_13.png)

挑战让我在这里再谈几个其他例子，而且您还会看到另外两个问题，左侧的收据是非常典型的右侧，它是扫描文档，显然它已经被弄皱或弄皱了，输入的信息可能会偏移半英寸，因此，此客户ID未与此处的客户ID标记对齐。

依此类推，这带来了更多的挑战，而该文档在右侧是，一张非常典型的发票，您会看到扫描的质量，相对较差，其中包含许多订单项，而且它的布局与我们的发票有很大的不同，在上一张幻灯片或左侧的收据上看到了，因此。

在我们进行演讲的过程中，存在很多可变性，引用此文件为收据，并，我们将用它来说明我们的方法，所以我们的目标是，从此收据中提取关键信息。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_15.png)

因此，让我们来谈谈我们想要提取的信息种类，所以第一种是我们称为标头字段的字段，例如，何时执行这些交易的收据权的日期，它包括收据ID um或发票ID，它可能包括这个总数，而这些信息非常。

对于会计而言很重要，请确保我们已经支付了收据，并支付了发票，我们认为这些发票对于税收目的很重要是，我们已经支付了这笔费用，无论是应纳税还是不纳税，适当的营业税，所以我们确实很在意这一点，信息。

我们将其称为标头字段，因为通常它们确实出现在文档的顶部，通常在顶部或底部，但它们是，通常一次出现的信息总共有一个，发票或收据没有对应的多个值。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_17.png)

因此，您知道有一种相当明显的方法，我们可以将其深层应用，了解此问题，我们可以获取此文档并运行它，通过光学字符识别和光学字符识别，以及，服务或供应商已经相当不错，这样他们就可以在周围产生基本的边界框。

标记，因此它们会产生许多边界框，并且每个边界框，包含一个令牌，其中一些令牌，与我们要提取的信息有关，所以这五美元和十美元，美分就是总数，所以我们可以做的是，应用深度学习对这些边界框进行分类，对。

我们可以使用输入来进行深度学习，可能是整个图像，可能是该边界框的某些上下文，但是我们可以用它来对这些边界框进行分类，并且对于此标头信息来说可以很好地工作，但是有一些挑战，所以这里有。

这5美元和10美分的金额也是4。50， 60美分这里的1。50分是我们的60分，如果我们将所有，那些我们可能会被标记的多个，作为收据，我们如何消除歧义，那些，因此消除歧义的问题是。

基本以及这些中经常发生的情况，系统是存在对启发式或规则进行编码的后处理，经过人为改造以解决这些歧义，并且，变成了巨大的脆性来源，并且进行了巨大的维护，随着时间的流逝，我们会再说更多。

我们在这里看到的另一种挑战是该供应商地址之类的字段，因此此供应商地址包含多个令牌，因此我们需要将多个令牌分类为属于该供应商，解决，然后我们面临的挑战，这些令牌实际上属于供应商地址，其中有多少个，在那里。

我们按什么顺序阅读它们，恢复此地址，以便在一台简单的机器上，学习方法仍然可以实现一些价值，留下了许多需要解决的问题，通常可以通过此解决，手工设计的后处理，这变成了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_19.png)

对于订单项更具挑战性，我们将在整个演讲中强调，订单项，因为这是许多重要，出现了巨大的挑战，因此在这里我们有两个订单项，对应于购买邮票的交易，也许是不同的，邮票种类，每一张都有说明，已发布的邮票。

此邮票具有与这两个邮票关联的交易号，它将有该笔交易的总金额，可能有一定数量，您可能有正确的单价，所以有多个，我们要提取的信息，所以现在我们需要确定这些信息在哪里，我们需要确定有多少个订单项。

该信息与哪些订单项相关联，因此这60美分，与第一个订单项或第二个订单项相关联，作为人类，我们可以读懂这一点，而计算机显然要难得多，时间，特别是考虑到那里的可变性，有成千上万种可以组织这些信息的不同方式。

所以这是根本的挑战，所以这些是我们想要的文件。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_21.png)

阅读，另一方面，是记录数据权的系统，通常此信息将是，通常由人类撤出并进入，到某些数据库中，这说明了某种数据库，模式，如果这是一个关系数据库，我们可能有两个，表第一个表包含标题，信息，第二个表包含所有。

订单项，这就是我们所需要的数据，可能会在记录系统中包含这两个信息，想要提取但也要提取可用的信息，我们为了这次演讲的目的训练这个系统。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_23.png)

更适合将其视为文档类型架构，以json为例，例如，现在我们有了标头信息，前三个字段，这不完全是json模式，而是，它应该看起来像这样，这是前三个，字段具有某种类型信息，然后我们有许多。

订单项和订单项数未指定，可能为零，也可能是，不止一个，然后每个人都有它的，拥有信息，因此我们面临的挑战是提取这些信息，这些文件和我们提供的培训数据中的某种信息，是原始文件和这类信息。

所以我想花一点时间谈论我们的理念，深度学习的基础上，您知道很多人都在考虑，深度学习就像大型深度网络一样。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_25.png)

不同的哲学，如果你认为机器如何经典，建立学习系统首先，我们要做的是，将问题分解为子部分，在这种情况下，这些子部分可能会，包括例如对边界框进行分类的内容，用来识别表格或提取行的内容，以及。

表格的每一列然后变成自己的，机器学习问题，为了解决该机器学习问题，我们，必须定义一些学习目标，我们必须找到一些数据，然后，我们必须训练该模型，所以这会带来一些挑战，这些数据不一定自然存在。

对这些文件我们不适用，一定有带注释的边界框告诉我们，信息在文档中的位置不会告诉我们，哪些赏金对应我们要提取的信息，因此，为了以这种经典方法进行训练，我们必须，创建数据，我们还必须定义这些目标。

我们为一个目标定义的目标之间可能会出现不匹配，这个问题和另一个问题，并造成摩擦和错误传播，当我们开始将这些部分整合在一起，然后最终，通常，这些系统在结束时会进行大量后期处理，根据特定的文档类型进行定制。

并且经过精心设计，所以发生的事情是这些系统非常脆弱，如果我们更改系统的任何内容，我们就必须更改许多有关，系统，如果您想采用该系统并申请，这是一个新问题，我们通常必须对后处理进行重新设计，对于我们来说。

我们在其中有成千上万种不同类型的文档，数百种或一百多种语言，您知道我们只是，不能对所有这些问题中的每一个都进行工程上的努力，能够完全一样地应用相同的方法，每个人都使用相同的软件，实际上，对我而言。

这是深度学习作为一种哲学的核心价值，是关于端到端的培训吗？我们有这样的想法，我们可以培训，整个系统端到端基于我们所面临的问题，试图解决和数据，我们从根本上拥有自然数据，我们可以利用。

因此我们再次将问题分解为，细分，但我们建立了深层网络，与每个子问题相对应的组件，然后我们将这些网络组合成一个大型网络，我们从头到尾进行培训，这很棒，因为整合，设计此网络时，问题就会出现一次。

很容易维护数据采集问题，因为，我们将其设计为一种端到端的方法，对问题存在的自然数据进行建模，当然，在如何设计这些网络方面也存在一些挑战，真的，在这种情况下，这是我们面临的主要挑战。

构建这些构建基块以及如何组成它们，因此，在这种情况下，我们将讨论如何进行此操作，组成深度网络组件以解决问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_27.png)

结束，所以在这里我们要做的就是对待这个，问题作为解析问题，我们将其纳入文档中，我们将，将它们解析为二维，这是一些关键的地方，创新在两个方面进行解析，在这里我们使用，深度网络。

因此深度网络将告诉我们所有潜在的，为这些文档之一解析树，这是最有可能的，或与数据最匹配的最合适的，然后我们将简单地阅读，从那棵分析树上关闭记录数据系统，没有帖子，处理过程中。

我们只是从字面上读取了解析树，然后我们读取了，该json数据作为输出，所以再次。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_29.png)

嗯，所以我们在这些文件上，在这些输入文档的左侧，右侧，我们通过ocr运行它们以获取，包含标记的边界框将输入设置为系统，然后，系统的输出是此json记录，描述我们从文档中提取的信息，正确。

它描述了我们提取的信息，没有描述布局，文档中仅描述了我们提取的信息。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_31.png)

好的，那是基本的方法和机制，我们将在这里使用的是上下文无关的语法，和上下文无关的语法，无论何时您想解析，都必须，有一种语法可以针对您的语法针对上下文进行语法分析。

具有计算机科学背景的人确实是计算机科学的主力军，它们是许多编程语言的基础，并且，他们是很好，因为它们相对容易解析，我们不会讨论语法上下文的技术性，我认为，这里要知道的关键是它们由。

规则的规则有一个左手边和一个右手边，以及我们的方式，想想这是我们可以采取左侧，并考虑认为它是由组成或组成的，右侧，因此订单项由说明和总计组成，描述描述的数量可以仅仅是一个，单个令牌，也可以是一系列描述。

正确的描述可以是多个标记以及我们的方式，以这种语法编码，以这种递归方式，好吧，现在我们将应用此语法来解析这种json，而且我们确实必须对该语法进行一些扩展以捕获我们关心的所有内容。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_33.png)

关于，但这种语法仍然非常非常，简单的权利，这是一个小的简单语法，真正捕获我们要提取的信息的模式，现在让我们谈谈如何解析一个简单的订单项，我们有一个简单的订单项，那就是邮票，我们每个都有3个邮票。

50美元，总共450美元。所以我们要做的第一件事是，嗯，对于我们替换的所有这些令牌，我们确定了一条规则，令牌出现在右侧，我们将其替换为左侧，因此，如果我们查看此邮资令牌，则将其替换为d。

说明我们本可以用t代替它，在这种情况下，金额或c为计数或p为价格，我碰巧知道d为，正确的令牌，所以我将其用于说明目的，但关键，观察到这里有些模棱两可，是的，目前尚不清楚，其中哪些替代是正确的，要做的。

这又是深度学习要解决的地方，这种歧义，所以解析的第一阶段是，呃，我们到处都是替代品，我们看到的是右侧，规则，我们用规则解析的左侧代替，歧义随着我们去，下一步是通过建设和技术。

这些语法总会在它们上有一个记号的原因，左侧，或者他们有一个，抱歉，这看起来好像有些问题，所以他们要么在右侧有一个令牌，要么有两个，呃符号在右边，所以既然我们已经处理了所有令牌。

我们现在正在处理这些符号对，因此，我们必须确定要替换的符号对，再次用左手边，所以这是我们的描述描述，用描述代替并，同样，我们用数量和价格代替，好的，所以我们重复一下这个过程，并得到一个完整的分析树。

这里的最终符号是一条线，项，这样告诉我们这整个事情是线性的，由数组成，价格和总金额还可以，所以我说有些含糊，这个歧义的一个地方是呃三和五十美元，我们怎么知道这实际上是一个计数和一个价格对，这可能。

就像描述和描述一样容易，因此很难解决这种歧义，但这是学习的机会，这就是学习的来历，通常可以学到五十美元，可能不是描述的一部分，与我们要提取的其他一些信息有关，因此。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_35.png)

这就是我们想要学习的东西，所以我们做到这一点的方式是，将每条规则与分数相关联，所以每个规则都有一个分数，现在我们尝试使用具有，高分，以便我们最终产生，具有高总分的分析树，所以我们实际上要做的是。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_37.png)

我们将使用深层网络对这些分数进行建模，因此对于每条规则，我们将拥有一个与该规则相对应的深层网络，给出该规则的分数。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_39.png)

现在让我说明一个简单的例子，一五十美元，这可能是一个描述，总金额或价格，我们可能会直观地认为这应该，偏向于总金额或价格，因为，这是一个货币价值，但是我们要解决的方法是，应用与每个规则相对应的深度网络。

右边有四个深度网络，我们将应用这些深度网络，他们每个人都将返回一个分数，我们，期望随着时间的流逝他们会学到，总金额的深层网络得分会更高，而，价格将有较高的分数，因此从根本上讲，对于这些最底层的规则。

这些基于令牌的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_41.png)

规则，更多涉及的规则，右边有两个词，我们也有类似的问题，解决歧义，所以我认为这里有两个重要的见解，首先是那个，你知道我们确实有这种歧义，我们如何标记这些第一个标记，我们可以执行cp或dp。

但是我们很快就发现没有dp规则，在右边，所以语法本身，有助于纠正此错误，因为没有规则可以，允许这种语法分析将纠正该错误，并且，因此，语法使我们可以对，这些文件包含的信息使我们可以对一些先验编码。

对问题的了解，这真的很重要，也很有价值，第二种模棱两可的是你知道的地方，这是允许的，但也许不是正确的答案，因此，在这种情况下，我们可以将cp替换为au，我们将为此规则评估模型。

基于这棵树的左手边和树的右手边，所以，这将有两个输入，左侧和右侧，同样，对于该规则，您尝试将其建模为描述，说明，因此这些模型中的每一个都将，得分会帮助我们消除歧义，在这两个选择之间，然后出现了问题。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_43.png)

我们如何得分一棵完整的树，我将介绍一些。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_45.png)

表示法，希望这不是太多表示法，但想法是我们要称呼完整的解析树，t，我们将通过以下方式表示该树的得分，ct，我将在此处滥用表示法，将c重新参数化为具有三个参数，第一个是符号，在树的根部，另外两个是跨度。

句子的覆盖范围，在这种情况下，那棵树就是整个，句子，所以我们现在使用索引0到n，同样的符号也适用于子树，我们的第一项是该子树根的符号，我们有跨度的索引，这不是整个句子，从我到j好吧，所以这是分数，嗯。

是一棵子树，让我们看看这里是否存在真正的问题，快啊，所以这是分数，为一个子树而准备的，我们将要。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_47.png)

根据深度网络再次定义，对应于该子树的根处的规则。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_49.png)

但它也由其他这些子树组成，所以我们要有对应于左侧和，这棵树的右侧，所以这些是我们已经定义的，现在根据组成它的树递归得分，我确实看到这里有一个问题，它说，考虑到。

语法约束是否有助于神经网络学习并获得更多样本，高效，是的，关键是我们知道，有关使网络变得更多的问题的事情，高效的，因为我们已经应用了这些先验知识，这对于解决像这样的复杂问题真的很有帮助，好吧。

就像我说的，我们现在定义了对整棵树评分，就这三个术语而言，在这里要注意的是，实际上，我们希望这是可能导致d的最佳分数，从根本上讲，所以实际上我们要将此分数建模为，以广告结尾的所有可能规则的最大值。

以及所有可能的方法来解析左树和右树，这似乎具有挑战性，但实际上我们可以动态地应用，编程相当简单，可以相当高效地找到它，很好，所以我们现在定义一个计分，这些订单项的树解析机制，然后发生的是。

我们将能够在所有，可能会使用该分数来解析树，并使用，最高分数是我们认为最有可能的分数，最有可能是包含我们关心的信息的一种，关于，然后我们就读了这些信息，在解析树之外，所以我们现在要训练深层。

网络以选择最有可能的我们最有可能的，解析树，只是提醒。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_51.png)

正如我所提到的，这第一个术语是一个深层网络，这些规则中的每一个，然后我们有这些递归项，再次根据这些深层网络进行定义，因此，如果我们展开此递归，如果我们展开此递归，我们将，建立一个庞大的网络。

其中每个网络都由网络组成，这些单独的规则，我们将为，我们试图解析的每个文档，因此，深层网络就是这个动态对象，由这些子问题的解决方案组成，以便识别，这是正确的解析树，所以我们该如何训练，这很简单。

我们使用结构预测中的想法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_53.png)

在这种情况下，结构预测的思想是我们有一些结构，拉帕斯树，我们想最大化，好的解析树，并最小化其他所有树的分数，正确解析树，所以这里的损失函数正在尝试，要做或这个目标正在尝试做的是。

最大化我们在我们看到的正确解析树的分数，训练数据并最小化其他所有分数，解析树我们在训练数据中看不到的树，可以使用反向传播和梯度下降或，我们从深度学习中获得的任何机制，所以这现在成为经典的机器学习。

我们很遗憾的深度学习优化问题，以及，其结果是用于解析这些文档的端到端系统，现在我没有谈论的是，这些文件实际上是二维的，到目前为止，我只是专注于，一维数据，所以现在我将移交给freddie。

他将谈论我们如何处理这些数据的二维性质，弗雷迪很好，谢谢你，奈杰尔，好吧，这就是。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_55.png)

我们之前显示的收据的一部分，我们，将专注于这个蓝色边界内的线。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_57.png)

我们沿用的区域是我们应用ocr的区域，到收据，我们得到代表每个代币的边界框，收据中显示的左侧是订单项。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_59.png)

在右侧，这是我们尝试通过的注释，我们正在尝试，获取与这些注释匹配的路径，嗯，你会看到的是，这里有很多盒子。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_61.png)

我们认为是无关紧要的，因为它不会，显示在注释中，并且不会成为以下信息的一部分，我们想要提取，所以我们将这些额外的框称为，简化解析，我要删除它们，然后转到，2D解析，我们将回来看看如何。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_63.png)

我们处理这些额外的盒子，所以在讨论2d解析之前，我只是想激励我们为什么需要进行2D传递，所以您，看到这里是第一层的二维层，订单项，如果我们要采用此订单项，然后将其简化为一维序列，结果是。

最初是连续的描述，2d版式中的版式在1d表示中不再连续，嗯，您会看到，蓝色，黄色区域在2d中是连续的，布局，并且在序列中不再是连续的，那是因为在这60感的蓝色区域，截断了说明。

因此我们可以在其中添加其他规则，处理这种情况，通常不会一概而论，所有情况下的所有情况um更好的解决方案实际上是，在2d中通过它并在2d中暂停它会更好，与我们人类对文档的解释方式一致。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_65.png)

一般，所以我们从令牌开始，就像奈杰尔提到的那样，我们通过深层网络进行了介绍，深度网络将为我们提供它认为每个令牌所代表的含义，因此，在这种情况下，我们获得了令牌的类，现在我们可以。

开始以左上角的标记开始合并它们，呃是，邮费这个词，所以我们知道后期是，描述最合理的选择是，与离它最近的令牌结合，并，这就是立场，所以我们可以将其水平传递，方向，我们可以做到这一点。

因为语法中有一条规则说，两个描述框可以合并为一个描述，现在接下来要做的是我们可以通过它，我们可以在此处看到右边的内容，或者您​​知道我们可以将其传递给，垂直方向，所以如果选择的话我们应该选择哪一个。

像我们在这里展示的那样水平沉积，因为语法中有一条规则说，一条线可以只是总数，但是会发生什么，是所有其他盒子都悬在那儿，然后，不会属于任何订单项，实际上这不是理想的选择，路径。

因为那样就不会与我们拥有的注释匹配，另一种方法是沿垂直方向通过，如此处所示，需要注意的重要一点是，我们不硬编码通过的方向，相反，深层网络会告诉我们哪一条是更可能的路径，在这种情况下。

您可以将它们组合在一起，因为我们知道邮票就是描述，而d网络告诉我们，这里的这串数字，有一个描述，您可以将他们加入成为一个描述，接下来要做的是看um盒子，这里的令牌和60，我们知道，一分钱是六分钱。

一个价格，我们可以加入他们的行列，因为我们在语法疾病方面有规律，您可以加入专柜价以获得简单的价格。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_67.png)

现在，接下来的事情是我们可以加入描述和简单的u，最后我们得到一个简单的多维数据集，我们不要忘记我们仍然有一个，那里的令牌，我们知道令牌是总数，最终我们可以横向加入，我们和。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_69.png)

由于整条线都过来了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_71.png)

所以继续前进，早期我们通过以下方式简化了解析问题，删除所有这些不存在的多余盒子，但是，如果我们把它们放回去，该怎么办呢会使它复杂化。解析不在注解中，我们之前没有显示它，所以，我们会做吗。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_73.png)

好吧，就这么早，我们已经知道邮费，邮票是描述，我们可以加入其中，成为一个，再次描述，所以这里有多余的文字，交易号字词呃，我们对它们做些什么介绍，语法中的新规则，新规则是，说我们允许令牌成为噪声。

所以噪声变成了，深入的网络可能会回到我们的课程，如果我们知道交易号，可以将其归类为噪音，那么接下来要做的就是，抱歉，我们可以将两个噪音uh令牌连接在一起，我们得到了一个噪声标记。

因为我们在语法中引入了一条规则，这使我们能够做到这一点，接下来，我们要添加另一条规则，描述周围可能有一些杂音，在这种情况下，我在这里添加了一条规则，您可以看到，这里的感叹号表示对此的排列。

规则这意味着我们不会施加约束，这些右侧符号如何出现，在这种情况下，噪声可能先于描述或描述而来，在这种情况下出现噪音后，此处显示的示例，噪声来自b进行描述，我们可以将噪声和a结合起来。

说明一起获得另一个说明，在这里，我可以将两个结合起来，说明，我得到一个说明，以便您可以看到，这就是我如何处理不相关令牌的方式。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_75.png)

在文件中，因此继续逻辑，最终我们，最终将正确冻结其中的订单项，文件，这就是我们得到的匹配，当前信息，好吧，所以我最后想谈谈一些实验结果。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_77.png)

很好，我们公司主要专注于，发票，其中大多数发票都是机密文件，因此，尽管我们认为可能会有其他实验室也在与其他公司合作，在同一个问题上，很难找到一个公众，数据集来比较我们的结果与幸运的是，有一个有趣的。

三叶草ai的作品是一个实验室，我们正在这样做，在韩国的一家名为海军公司的公司，他们也在网上查看问题，项目和他们的信誉，他们已经发布了一个数据集发布一个，一组数据一组收据一组原因。

他们已经写过并且他们的论文已经作为预印本在出口，我们的方法与他们的方法之间的主要区别在于，他们需要的是对收据中的每个边界框进行注释，这意味着您要进入的每个边界框都将这样说，边界框属于此类。

每个边界框都需要，在我们的案例中，与之相关的坐标，我们要做的就是依靠记录系统，在没有边界框坐标的json格式前面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_79.png)

我们正在做的如此有效的是，培训中，我们所依赖的信息少于我们应有的信息。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_81.png)

根据结果​​，我们获得了相当可比的结果，尽可能地，我们尝试将指标实施为尽可能接近，他们描述了，所以我想，那个我可以给任何人，奈杰尔，非常感谢弗雷迪，让我看看我是否能控制住自己。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_83.png)

好吧，让我好吧，我想我很好，所以嗯，你知道这是一个数字，的人帮助我们完成了这项工作，因此，我想感谢您的帮助和。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_85.png)

和，请保持联系，我们在用户界面上做了很多有趣的工作，全球各地，我们正在招聘，请通过以下方式与我们联系，aiadyy。com，我们引用了其他一些工作。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_87.png)

在谈话中，这里有很多非常有趣的优秀论文，绝对值得一看，聊天中有几个问题，思想真的很棒，所以也许让我尝试回答，他们在这里，因为我认为他们也有助于澄清这个问题，内容，所以有一个问题是。

我们可以让AI模型学习最佳语法来解析，而不是，定义语法约束，这是一个非常好的问题，但是，其实语法来自问题，正确的语法是问题本身的内在语法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_89.png)

可以根据我们要提取的数据的模式自动生成，所以问题很自然，这不是我们必须发明的，它来自问题本身，还有一个问题是，可以跨规则共享网络，再一次是一个很好的问题，呃，我认为有几种方法可以考虑，第一。

这些规则都有自己的网络，并且我们在这些规则的每个应用程序中均分担权重，是否将其在单个解析树中多次应用，或跨多个文档的多个解析树，另一种是我们经常会利用语言编码器之类的东西，和bert例如要嵌入。

以为每个，令牌，所以那里有很多共享参数，所以有很多方法，这些参数在其中共享并且最终它们最终被，可能产生相对较小的网络，解决甚至非常复杂的问题，例如，关于2D解析是否贪婪地进行，存在一个问题。

所以这又是一个很好的问题，解析cfgs的算法利用了动态编程，因此它，不，这不是一个贪婪的算法吗，实际上产生了最高的核心解析树，它确实，以一种非常幼稚的高效方式，算法看起来像它，将是指数级的。

但随着动态的应用，编程，我相信它已经入伍了，然后是一个问题，规则是否会尝试评估，标记化的数据，例如实际等于的总数，价格时间在再次评估一棵树的可能性时计算，真的很好的问题，我们还没有这样做。

但这是我们确实要做的事情，因为那是一个非常，有用的约束对，这是我们对问题的了解，是订单项总计趋向于等于单位计数乘以单价，并且，这样约束应该真的很有价值，帮助解决这个问题，然后最终解决语法问题。

适用于不同文档类型的规则，因此这些语法规则对于问题来说是基本的或自然的，它们与我们要提取的信息的模式相对应，因此，文档类型之间的语法可概括性的概念是，不太重要，所以谢谢你，我很乐意回答。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/mit_dl21/img/9a451ba119d270006932ea00e622dad2_91.png)