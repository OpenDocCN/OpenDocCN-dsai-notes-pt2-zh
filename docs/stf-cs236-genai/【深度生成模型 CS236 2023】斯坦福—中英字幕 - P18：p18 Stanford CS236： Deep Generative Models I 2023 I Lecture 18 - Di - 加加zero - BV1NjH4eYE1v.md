# 【深度生成模型 CS236 2023】斯坦福—中英字幕 - P18：p18 Stanford CS236： Deep Generative Models I 2023 I Lecture 18 - Di - 加加zero - BV1NjH4eYE1v

![](img/05b2a48299bc1e1aee16e6b314596f76_0.png)

![](img/05b2a48299bc1e1aee16e6b314596f76_1.png)

好的，所以我们今天准备好开始了，我们将继续讨论扩散模型，但我们将看看如何，我们如何使用融合模型来建模离散数据，特别是文本，我们有一位由亚伦担任的客座讲师，他是我实验室的博士生。

他在使用扩散模型这一领域的工作中做出了开创性的工作，对于离散数据和语言，所以是的，请开始，"是的"，"谢谢斯蒂法诺的介绍"，"我很高兴开始工作"，嗯，"让我们开始吧"，是的，"开始"。

"我想谈谈一般的框架"，"这是我们的生成模型问题"，"并且一般事物的运作方式如何"，所以通常我们被给予一个数据集x1到xn，我们假设它是独立同分布的样本，来自一些数据分布p_data。

我们的目标是拟合一个参数化的模型p_theta，常常由一个神经网络参数化，该神经网络近似我们的地面真数据分布p_data，并假设我们可以学习我们的p_theta足够好，我们可以生成新的样本。

也许使用我们的参数化p_theta的新有趣样本将是有趣的部分，现在如果我们可以做所有事情，那么，一切都会好起来的，我们获利，"但是，中间有一些数学计算。"，正如你们所知，"所以在这门课上。

你们已经学习了很多有关不同生成模型范式的知识。"，例如，鹅，"蒸发和扩散模型"，"对于所有这些不同的模式，你会注意到一件事"，"或者你学到的大部分模型是，每当他们画画时"，"像一张示意图"。

大概像你应该做的事情，他们通常都有一个图片，他们通常使用像图片这样的数据模态作为你最常见的数据形式，所以这里我们有一只狗的图片，一个数字的图片和一只更小更可爱的狗的图片，而且这种巧合实际上不像。

它不是仅仅是巧合，它是实际上我们做这件事情的一个非常基础的原因，而且原因是因为我们所有的不同的数据，所有这些不同的生成模型他们正在构建，在事实上，我们在一个连续的数据空间上工作，所以。

我们的数据空间x等于一些rd，其中你可以认为，R是每个像素值，并且d是总像素数，所以像像素和像素的值，如果使用像光谱这样的方式来可视化，那么它是这样的，那么，我们可以在这里、这里或那里采样点。

并且这三种不同样本都是像有效的像，换句话说，这是连续空间的基本属性，而且我们喜欢插值这一事实，现在，我以及很多其他人感兴趣的是，这与喜欢相反，这种设置是一个离散数据空间，如下，所以而不是有x等于rd。

我们有x等于1到n的d次方，其中n是总的喜欢数量，或者是词汇大小，换句话说，并且d是维数，这是一些维数，我们将我们的β点替换为数据点x，然后你知道，如果我们有这个设置，我们可以用另一个图表来可视化它。

这是网格，这是离散形式的最简单的版本，一个离散的像空间，尽管我们确实可以像这里和这里一样生成样本，这些样本像离散数据点的样本一样有效，但我们实际上在这里或那里无法生成样本。

我们不能在这里或样本值之外生成样本，因为对于离散数据分布来说，这只是没有意义的，因此，这使得离散数据像本质上是一个更难的问题，正如我们将看到，所以现在你可能会问自己这个问题，好的。

我们还学习了像生成对抗网络到融合模型这样的东西，变分自编码器这些都工作得很好，我们为什么要像，你知道，去像完全完全不同的一个像领域，他们为什么，他们为什么去离散数据，而且这为什么重要。

而且如果我不提到我们在openai的好朋友，我会感到遗憾，谁知道，你知道，释放这些大，这些大的大型语言模型像chi bt，哪些在過去幾年裡實在是變革了世界，我也不能忽視其他競爭者，但是。

我們根本有這種新的大語言模型范式，你知道光是，或許爭議性地是計算科學機器學習在過去幾年裡的最大進步，並且這種數據領域的有趣之處在於，句子本質上是離散的，對於句子，它是一系列離散的像詞元或像詞段的序列。

我們像這樣構建它，因此，最有意義的是擁有一個概率模型，那是可以生成離散數據的，像我們這樣的句子，特別是如果你對像llm大語言模型這樣的自然語言處理有瞭解，總體來說，你可能聽說過什麼叫語言模型預訓練。

這是許多這些模型中的核心步驟，你像要學習，像對你的所有輸入句子的分布，他們所說的語言模型預訓練實際上是，你只是在適應你的網絡規模數據的一個離散概率模型，所以我們可以看到，這個想法在這裡是相當基礎的。

其他應用包括像自然，像生物和自然科學，更廣泛來說，我們有數據模態如DNA分子和蛋白質，並且所有的這些不同數據模態都是本質上離散的，這是，它將最適合試圖試圖生成像一個新的，DNA序列。

新的分子和新的蛋白質，這些對我們的日常生活有重大影響，並且它需要一個離散的生成模型，最後，這有些反常，我們也看到對離散性的回歸對一些事情，像圖像，所以這是v q va的架構圖。

v qv或v q gan的背骨，是穩定擴散系統中許多建築塊之一，在中間我們有這種離散化的表示，這是離散化的潛在空間向量，並且最近的工作，這是非常最近的，在過去幾個月內，谷歌和卡內基梅隆大學已經顯示出。

它顯示出，如果你只是像扔掉任何對你離散潛在空間的連續概念，你只有離散部分，這實際上導致結果的廣泛改善，這些結果通常顯示出，可能未來我們會看到，这可能将像图像一样融合在一起，也包括这个广泛离散的范式。

但是嗯，所以现在我们知道像离散数据一样重要的原因，所以让我们问一个问题，为什么这么难，你可能说像亚伦，这非常有趣，为什么我们不能简单地适应现有的连续空间模型，比如流动或再次。

为什么我们不能只是取那个并适应像离散情况一样，我们实际上有类似的东西，所以我们有这个图表，我们添加一些随机噪声，我们将其推送通过神经网络f theta，并生成图像，这是好的。

像做样本链接和什么之类的好方法，这里的直观想法是，为什么我们可以只是参数化f theta以输出仅离散值，因为它只输出离散值，然后我们可以生成像文本这样的东西，我们可以在这里看一些例子，但对于流动。

我们有这种核心，像我们有像耦合这样的东西，我们从噪声中提取数据，通过f theta和f theta的逆向，你有看到的疤痕，这种工作的方式是，你可以拉伸和拉拽你的空间。

这允许你以简单的基础分布处理复杂的数据分布，并使用如下的变换公式，如果我们将所有这些类型的东西都替换为离散数据，让我们说从像离散随机序列到像，嗯，我们像双射地映射到另一个，我们想要模型得很好的真实序列。

我们有变换吗，嗯，我们没有真正的变换，实际上我们最好的情况是像这样的设置，其中我们的x与另一个x具有相同的概率，因此，基础分布必须像数据的数据分布一样表达，这就是为什么这种设置像挣扎得非常困难。

对于这个问题，是的，我们有这个流动，它并不真正通用，我们也有，让我们说GANs，它并不真正通用，我们还有，让我们说gans，好的，我们有，我们捕获噪音，我们将其映射到图像中，我们有一个判别器。

然后这里的想法是，我们可以反向传播梯度来更新我们的f theta从判别器，如果我们将像组件替换为离散值，所以我们只参数化，我们只参数化为允许离散输出的形式，然后这些梯度实际上不会在离散值输入上反向传播。

这是因为我们没有微积分，你知道，从这两个例子中，我们可以广泛看到，如果我们快速到达这个幻灯片，但我们的结论是，我们的模型目前过于依赖微积分，并且很难扩展，在Transformer之前。

这更像是一个建模问题，不像它是一种架构问题，不是建模问题，如果这有意义，所以 kind of uh，对于Transformer，当您将其映射时，输入，像序列到，像离散到，像连续值。

实际上这就是为什么这起作用的原因，并且人们这样做的原因是因为它将你的五十万或什么，像标记空间降低到像七百，这大大更适合计算，但你实际上不必这样做，所以这更像是一种架构决策，并与。

像建模的基本建模组件或概率建模部分无关，好的，我们为什么不能，嗯，将标记嵌入到连续空间中并做这种类型的，我们嵌入标记到连续空间中，然后当我们生成时，我们只是生成值并稍微离散化，实际上。

人们实际上这样做了一些事情，如，特别是在我们看看像图像这样的东西时，对图像来说，你知道，图像我们实际上不存储，像图像的全部连续值，因为那是不可能的，在计算机中，我们只有有限的位精度，一般来说，我们嗯。

你知道，我们像零到二五五这样离散化它，我们有这种离散化表示，这就是我们喜欢存储的引言，引言，未引言，就像真实地面图，这里的想法是，对于我们的生成性模型来说，人们使用的。

像稳定扩散这样的系统或任何生成模型一样，大致是一个两步程序，首先你有你的连续生成模型，你生成一个连续的图像，然后你将其离散化，就像找到最近的类似值一样，这就是你离散的图像，这有点像。

你知道人们通常怎么说的，所以这就是他们如何克服图像的离散性，在这里使用连续模型，这是为什么图像特别适用的原因，因为我们如果有图像的值为零，二五五，这是图像像素值的一个典型范围。

现在我们可以将其嵌入到像一些连续的空间中，像直接，所以我们只是在实数线上嵌入它，如果我们生成像，嗯，你知道这些三种不同的生产方式之一，那么，我们可以做的就是我们可以很容易地离散化。

因为我们只是跑向最近的数字，现在非常简单，如果我们有像标记这样的东西，让我们说对于自然语言，好的，就像没有，我们那里没有任何方法，我们将这个嵌入到一个连续的数字线上，像这样，通常来说，人们像这样做。

这就像非常高维的东西，就像这两个维度，当我们试图做我们的嵌入时，人们通常更喜欢这种方式的高维，如果我们试图像，在这里生成东西并尝试再生成并离散化一些东西，你最终会得到的就像，好的，是的。

有时你生成的东西会很好，好的，所以如果我们必须生成标记，就像在绿色的X标记那里，一切都好，但如果我们大部分空间都是空的，所以我们最终会得到一个标记之间有很多空隙的情况，这可能是可能的。

尽管要将其离散化为最近的邻居标记，但它更像是，它，它不是，它更像是，嗯，不是，为什么这会起作用并不明显，当我们在标记之间移动时，它并不真正有意义，实际上，这在图论中是一个基本困难的问题，这就是现实。

是的，如果这个模型像你说的那样完美，那么这个方法就会工作，但如果你的模式完美，那么这个方法也会工作，但在实践中，这种情况更像是不，我们想要构建到模型中的诱导偏差，使它变得极其，它通常使它很难像，学习。

例如，所以对于像，让我们说融合模型，和像语言扩散模型这样的模型，它们做同样的过程，你可以连续嵌入你的标记，在那里选择融合过程，就像我们将看到的那些模型一样，对于那些模型来说，它是 kind of。

它们并不真正与，像你的标准自回归模型一样竞争，并且它们需要太长的时间，因为你不能，你不能有任何错误，如果有任何错误，你就像迷失了方向一样，它不适用于自回归建模，我们模型概率。

这是一种与模型实际值不同的数量，所以我们可以像模型，例如，概率单词 the 或者 versus 单词 times 的概率，这是一个非常连续的量，但如果我们说嘿，让我们就像这样取一个变压器。

我们推它通过线性层并选择一个值，这是这将是设置，我们将从您喜欢的连续空间中选择一个值，不像，您对其他标记没有概率，您只选择一个值，这将变得更加困难，我们试图生成像这些标记附近分布的东西，但这是事实。

是的，基本上我们在建模概率模型，我们尝试离散化，而且这是大量的空白空间，不是学习过的好诱导偏差，因此由于这些各种问题，我们只有像一个真正的好，像您注意到的自回归模型变压器中的离散概率模型。

这是非常典型的，这里的想法是，而不是想法，在这里，您模型每个序列x的概率通过分解标记来建模，所以您在这里模型第一个标记，然后您取第二个标记，给定第一个标记，这是下一个概率。

您只是乘以它来获得像最后一个标记给定，给定所有标记之前，这是你的典型设置，是的，特别是在语言上，这分解为绿色中的上下文概念，您有一个上下文，上下文标记，您有一个基本紫色中的下一个词预测。

这就是为什么这工作如此出色，是的，这个自回归建模范式有许多优点，特别是在特定上，它非常可扩展，这里的想法是，当您计算每个下一个标记的概率时，您只需要计算过您的总标记d或您的总值n的概率。

对吧或您的总值n，这是非常可扩展的，你知道这很容易做到，只要您能够像样地构建您的架构，这应该工作得很好，另一个事情是，如果您有一个非常强大的神经网络，如果您有一个足够强大的神经网络，理论上。

您可以通过分解来代表序列的任何概率，这令人惊讶，但它实际上通过这种分解性质工作自己出来，最后，它是对事物的一个相当合理的诱导偏差，像自然语言，所以对于自然语言，我们左到右说话，写作，所以，自然而然地。

我们会这样做，我们在这里这样做吗，比如对于语言建模，像语言模型的模型在那里也会使有意义，但是，虽然如此，也存在一些问题，这些问题大多数人都没有很好地解决，这是因为总体上，自动回归模型的线条。

一个著名的论点是，像简·库恩这样的人，你知道，真正提倡的是，是采样和自动回归模型建模往往会漂移的想法，当你从自动回归序列中采样时，你只是生成新的标记，但你可以积累误差，并且当你连续积累误差时。

这将导致你的生成偏离轨道，这是一个非常著名的论点，为什么我们不能通过自动回归模型得到agi，另一个问题是，对于非语言任务，如，让我们说DNA序列，有DNA序列，没有原因为什么DNA序列必须从左到右生成。

我意思是，这并不像，这不，这不作为一个归纳偏差 makes sense，此外，而且，人们还没有真正开始思考，但实际上，当我们有像自动攻击的变压器时，实际上，有很多像约束。

所以我们需要在像自动攻击的变压器上特别放置，确保注意力掩码是因果的，人们还没有真正意识到这一点，但是，肯定还有，对于这种概率性建模范式，肯定还存在问题，最后，嗯，因为在自动回归模型中，我们迭代采样。

我们生成下一个标记，实际上，这是一个相当慢的技术，一个相当慢的技术，因为它实际上是迭代的，你将不得不生成标记一个接一个，这不太好，所以，是的，我们拥有自动回归建模的所有问题和优点。

所以我们需要问自己的问题是，那就是，是否有更多的东西在这个，我们可以从分数匹配的角度来考虑这个问题，我相信你们所有人都知道，是的，像为什么我们不能直接模型p theta of x的关键思想。

而不是我们必须做的，而是自回归分解，是因为p theta of x，我们必须确保当我们对所有不同序列进行求和时，我们必须求和到等于一，这是因为这是不可能的，因为需要求和的序列数量像指数一样多，因此。

这与我们刚刚讨论的几次讲座中的概念非常相似，嗯，我们刚刚讨论的分数匹配概念，我们像这样建模梯度对数概率函数，当我们这样做时，我们不必，当我们这样做时，我们不必对所有可能的，数字进行求和。

我们需要将其积分出来，我们不必将分布积分出来使其成为一个，这通常与什么一起工作效果较好，当你将它与像扩散模型这样的东西结合时，所以这里真正的问题，以及我们将要讨论的事情。

本讲座其余部分的内容是我们如何将这些技术从自动，从像分数匹配到我们的更离散情况，这就是本讲座的真正问题，我们能做到吗，它效果如何，所以让我们来看看我们如何开始做这件事的大纲，有3步。

第一句是如何将分数匹配扩展到离散空间，这并不是一个非常知名的问题，是一个相对知名的问题，之前并没有提出过很多很好的解决方案，下一个问题是一旦我们学习了分数，在离散情况下被称为具体分数。

如何使用具体分数生成新的样本，最后当我们构建这个生成模型来生成新的序列时，我们是否可以评估似然度和原因，我们想要评估似然度的原因在于要与自编码器进行公平的比较，在许多方面进行反向建模，如困惑度。

类似任务，所以是的，我们可以先看看在离散空间中泛化分数匹配，所以当我们考虑像，嗯，当我们考虑像分数匹配的核心构建块，像分数匹配，我们真正考虑的是梯度，而且梯度实际上有一个很好的。

一种我们可以构建它的好方法，梯度的一种泛化可以扩展到离散空间，所以这里的想法是我们的函数f的梯度，当我们在位置x处评估时，这实际上是像有限差分，因为有限差分是导数的一般化，如果我们假设我们并不。

如果我们假设这个空间不是连续的，所以是的，所以相反，这个梯度变成了fy-fx，当我们只是索引过所有其他y时，这像梯度的一般化，并使用此我们可以构建分数函数的一般化，分数函数是在位置x处log概率的梯度。

实际上这是什么是概率对概率的梯度，当我们使用链规则来获取出对数时，然后当我们替换我们对有限差的定义，而不是这个第二行中的梯度，我们实际上得到的是所有py对px减去一的集合。

所以这个py对px索引过所有其他y，这是我们要学习的，这个被称为具体的分数，这直接将分数函数从连续空间扩展到离散空间，这很好，但你知道我们如何学习这一切对吧，但是，还有一个事情是，对于py或px。

当我们试图模型所有可能的px时，你知道这不太有意义从计算上来说，所以在这种情况下对于y，如果我们只是让y是其他像x的邻居的值，任何其他像的值，不是x，现在我们会遇到这个问题，我们需要模型太多的量。

n的d次方的指数函数不工作，但是，如果我们模型这个嗯，这些序列比，这些比率，我们模型两个序列之间的比率，只有它们相差一个位置，所以让我们模型任何两个序列，只有它们相差一个点，在一个位置。

这是一个更本地的构造，如果我们这样做，我们只需要复杂性，所有n乘以d，这是非常像的，非常计算上可行，它只是我们的序列的大小，当我们模型像两个序列之间的比率，只有它们相差一个位置时，我们可以实际上参数。

好的，但是，像我们通常会这样写，首先给出第一个值，对于所有的像导数这样的，我们会使用像pxy这样的符号，但是，所有的导数都可以很容易地推广到这个多维的情况这里，就像作为，像整个谈话的预兆。

这是因为这样写更简单，但是，是的，我们也可以用神经网络很容易地建模这些比例，所以如果我们将输入输入到神经网络中，一个序列x1到xd，我们将其推入一个序列到序列的神经网络。

然后我们可以得到另一个附加有另一个维度的序列，像这样，我们可以有像概率x1x2，一直到xd的概率x1xt，我们有像这些，我们可以直接建模这些像所有连续邻居的比例，它们只相差一个点，这样序列到序列。

所以您只需要从这个序列开始，它是一个一维序列，所以d维或d维序列，所以我们只需要将其推入神经网络，像并行的，所以你可以想它想它像不像一个非侵略性但风格变换器，好的，所以是的，这就是想法。

我们有一个序列到序列模型，我们可以生成像只相差一个位置的地方的比例，那么我们如何学习它，这是一个非常像的问题，我们如何学习这个具体的分数，我们像如何做这件事，所以是的。

我们的目标是学习一个神经网络作为thetax，这样当我们将参数化s theta的x放在位置y时，然后我们可以得到相对比例py过px，我们需要找到一种方法来做到这一点，非常原则地。

因为我们不能允许我们的s theta有负值，并且当我们有足够的数据时，我们也应该能够恢复到真实值或足够的数据，足够的模型容量，所以我们这样做的方式与分数匹配非常相似，这是损失函数ah，我们称之为分数熵。

因为它基于一个非常，它与交叉熵等东西非常相关，但我想这里的想法是它非常，这里的想法是它是一个离散分数匹配的泛化，从意义上说，我们首先采样，我们对我们概率函数p中的所有可能的x进行积分，然后。

我们对所有与我们的y相似的邻居进行求和，并且我们最小化像这样的新类型的，在中间这里，像分叉函数，以优化它，是的，这就是为什么这样，以这种方式，我们将看到，为什么我们需要不久后做这种构造。

这里的想法是我们有得分熵，它是得分匹配的泛化，但是为我们的离散得分，你可能不信我，但实际上，熵函数实际上确实可以恢复我们的真实值，如果我们只是像去掉所有x和y一样，如果我们想简化我们的符号一点。

并且我们想要最小化以下量，当我们的导数集被最小化时，我们说设置导数为零，我们得到p y除以p x乘以1除以s等于零，然后我们移动它回来，我们展开它，当我们正确地最小化它时。

这个s值应该等于p y除以p x，我们也可以可视化对于像真实比例为0。2这样的地面真理的损失函数，我们可以清楚地看到它满足我们所有的要求，基本上它是凸的，它将工作，它将恢复真实值。

如果我们只是最小化这个，最后，我们可以在所有对x和y的配对中进行这个，所以我们可以独立地为所有我们的x和y进行这个，这意味着如果我们学习所有正确，我们应该像恢复每个对x和y的真实值一样恢复它。

基本上就是这样，我们有这个得分熵损失函数，我们如何实际上优化它非常类似于得分匹配，在这里，这里有问题，我们有这个损失函数，但是真实值p y除以p x在这个轴上的值是我们完全不知道的，如果我知道它。

我可以使用它，所以，它似乎有意义我们有必要去，找到一种不同的方式来学习它，我们有两种不同的方式来做这件事，其中一种是这些像替代损失函数的东西，其中一种是这些像替代损失函数的东西。

我们称之为隐含得分熵的术语，这是隐含得分匹配的自然一般化，但在这次讲座中，我们不会覆盖它，但只是知道它存在就挺好，我们还有一个损失函数叫做噪声得分熵，或去噪得分熵，这与我们的得分熵情况类似。

就像去噪得分匹配一样，来看去噪得分熵，如果我们假设我们的像p x等于，像基础基础分布p zero和某些核p well的卷积，然后我们就可以像概率一样写出一个顶峰，对所有的x求和，当我们这样做时。

我们可以像我们的初始得分熵损失一样看一下，然后嗯，这里的想法是我们可以先移除期望，所以我们不再只是移动px，这消除了px和分母，但对于所有过的所有x，嗯，然后为了喜欢，嗯，看事情更具体地。

我们查看上面的分解，然后我们只将这个p项应用到这个p项上，以获取这个后续的分解，这基本上是我们添加一个期望值对x零的加和，我们可以基本上移动我们的值一点，所以我们可以将求和项移动到前面，使用福比尼定理。

我们还可以像添加一个p x z p的x给定x零项，给定p的x给定x零，所以我们可以在这里重新工作它，然后一旦我们有所有这个像样的设置，然后我们就可以基本上就像将最后两个术语带入我们的期望。

我们只是只是移动那些术语，我们取它们从求和中移除并移动到期望中，然后我们就给我们一个等效形式，对于这一点，我们会注意到一个美好的事情，在这里的等效形式是，我们只有像样的p of y的比例。

给定x为零的p of x，给定x为零，不是p of y over p of x，因此，这就像这可能被计算的像这样，这就是可能被计算的这样一件事，因为我们可以假设我们的转换核p像，嗯，可操作。

但我们不能假设我们的数据分布p像可操作，基本上，好的，这个x零是你基本部据点，并且这个转换核你可以，它可以是任何东西，基本上它可以是任何比这种噪音多得多的东西，但在连续的情况下，你也会像这样写。

但像出于实际原因，这就是人们选择使用像小高斯一样的原因，像加法，以便于，这样做和完全一样，但这基本上和完全一样，所以我们有这种方法来消除p或px，是的，因此，我们有以下去噪得分熵损失函数。

而且它特别可扩展，因为我们可以采样一个x为零，我们可以通过这个采样，我们可以通过这个扰动核采样，然后只需要为所有x计算这个s theta一次，对于求和值，因为我们只使用了x的s theta。

然后最后我们可以计算这个过渡，这个过渡核的比率可以通过像样的，这就是我们的定义方式，所以一切都变得计算上可行，我们可以优化这个损失，所以现在我们现在有了一种学习具体分数的方法。

下一个问题是我们如何使用具体分数进行采样，我们有这种估计具体分数的方法，学习数据分布的比例，我们如何生成新的样本，这像是真正的扩散导向，所以为了像这样做，我们必须定义像扩散过程一样。

像我们的离散标记一样，正如我们所知，扩散只是概率性演化，一种从像p零到某些pt的方法，所以我们可以直接从这个方向上工作，我们的pt现在只是一个大向量，我们可以思考，把它想成一个大向量。

所以这是因为我们在某个序列上的概率基本上就像，一些数字大于或等于零，并且所有事情都加起来等于一，所以我们可以认为这是一个大向量，有点，并且在一种方式上我们进化，我们的分布是与像普通微分方程一样的。

这是做事情的最自然方式，所以，我们的dour pt是一个向量，我们以那个为参考取时间导数，然后，我们可以根据这个像矩阵q t乘以r来计算像过渡，一个初始向量p t，所以我们进行矩阵向量乘法。

关于这个扩散矩阵有一些不明显的事情，但是像这样，这些只是硬性要求，我们需要确保这个，嗯，融合矩阵的列向量之和等于零，而且我们需要确保这个扩散矩阵在所有点上都是非负的，比如非对角点，基本上和这里的想法是。

qt控制，如果我们从一个状态跳转到另一个状态，我们去的频率是多少，我们可以像这样直接做，所以基本上如果我们想要从一个状态i跳转到状态j，在时间间隔delta t内。

然后基本上我们就看看是否我们仍然停留在当前函数，然后我们就添加以下像这样的矩阵项乘以delta t，然后我们有一些二次项，为了实际目的，我们可以消除它，这有点像欧拉马里亚采样的模拟，像融合模型。

因为这次我们的采样过程时间序列化，所以这里我们可以清楚地看到，我们的qt，这些矩阵元素是跳转率，从i到j种类型的，是的，所以一旦我们有这个设置，我们可以来看看几个例子，这并不是一个很直观的事情。

但是让我们来看看以下q t，我们的qt由这个矩阵给出，这是负二吗，负二，对角线上的负二，除了对角线外，所有地方都是一一一，假设我们取一个初始分布为0。5，0。2，0。3，当我们乘以这些东西时。

我们得到一个像负率的转移率，0。5，0。1，零点四，而且有趣的是，就像，这些值相加等于零，这对于保持我们的概率总是等于一这一事实非常重要，而且，就像总是像一个有效的，像从一种状态过渡到另一种状态。

像不同，嗯，状态对于这个类型的设置，我们实际上可以计算出中间密度pt，只要通过指数地扩展这个矩阵，乘以这个嗯，零点五，零点三，零点二初始向量，这允许我们通过基本解决ode来计算像中间密度这样的值。

并且我们可以，如果我们这样做，我们可以实际上也检查以确保过渡确实满足上述陈述，基本上对于第一个值，你正在以零点，负的速度失去质量，零点五，然后其他两个正在获得巨大的质量，零点一，零点四。

所以总质量保持不变，但是像，你知道，你知道相对比例的变化，你知道，基于这一点，基本上一般来说，是的，我们将q等于像sigma噪声水平乘以q矩阵的值，然后一旦我们有了这个，这变成了一个线性ode。

像所有线性化一样，我们有一个线性ode，为了解决像基本上，这是非常一般的，我们可以通过解决这个来解决中间密度，像做这个矩阵指数，为了解决线性ode，基本上在这里，在许多方面来计算这个，像指数。

但更简单更好，这里的想法是我们可以计算具有长视距的过渡率，通过这种方式的过渡率，像通过取我的列，和像我们的，像指数化的矩阵的条目，基本上所以是的，这很棒，另一个对于融合也很重要的事情是，当t去无限时。

或pt将去一个p基础，基本上所以这确保我们像接近一个漂亮的基础分布，我想其他事情也可以意味着它，在这种情况下，我们可以看看这个矩阵是负数的，我们以某个t对两个矩阵进行指数化，并得到像这样的东西。

基本上它看起来不如看起来那么坏，然后随着我们去无限时间，我们就去像随机值基础的值，所以这是一个均匀的过渡矩阵，我们直接从一个初始点开始，到达任何其他点，随机，最终，类似地，我们有这种遮蔽。

我认为我们在这里添加一个新维度，我们在我们的三维情况下添加一个新维度，基本上我们只有向这种新状态的过渡，我们的指数矩阵看起来像这样，随着我们取无限时间，主对角线消失，或者一切都被遮蔽。

基本上这就是一种像过渡一样的面具，第一个情况基本上这里的想法是你只是随机地从像你的初始值一样，像的值移动到任何其他随机值，而第二个情况是你随机地从你的值移动到像的面具值，基本上就是如此。

它只是决定你正在移动的地方，我们设置了一个连续时间马尔科夫链，你知道一般来说，当我们看序列时，这里的想法是从序列到序列的一组，这就像非常昂贵，因为当我们考虑序列之间的转换和我们的序列或任何其他序列时。

而且这是一个数学上不可解的，我们更喜欢从标记到标记，所以相反我们只是翻转一个标记一次，这基本上是想法，因此，这是大O的平方，因为我们只需要考虑一个标记，而且由于这个当我们做序列之间的总体转换时。

它就变成了标记之间的总体转换，所以基本上它分解成这样，就像另一个点在那里，而且这个好处是，你知道，我们可以用我们的得分熵来估计，像中间密度比值，所以如果我们假设我们的样本来自。

假设我们有一些来自p零的样本x零，然后我们可以学习我们的s theta，我们现在添加一个t值以估计p t over p pt值，Pt比值，我们有另一个额外的输入t，但是设置是一样的。

然后我们有我们的去噪函数，如得分熵损失函数，这里的想法是像，是的，现在我们可以取这些过渡值，这只是两个不同状态之间的过渡，所有这些都由初始率矩阵q给出，基本上就是这样。

那句话是说我们可以像优化我们的去噪得分熵一样，使用这样的队列设置，使用这样的扩散设置，一切都非常自然，所以这里的问题是，现在我们有了一种方式，如，你知道，从数据到噪声，我也有一种方式，如估计中间比例。

我们可以做什么，嗯，这里的想法是我们可以逆转扩散过程，所以如果我们从p0到pt，这是p数据到p基础，大致来说，这里的想法是我们能否从p基础回到p数据，实际上，有一种方法可以做到这一点。

所以有一种像是这样的扩散，像反向扩散过程，我基本上取时间导数，但在这里，我们像往常一样向后走，在时间上我们有一个新的矩阵，Qbar，它是稍微不同的，Qbar的想法是，qbar对于输入来说，J和i。

这是等于，像密度比例，Pt的j除以Pt的i，乘以这个初始Qtij对于任何，Ij不等于，基本上我们有这个以下关系，嗯，像前向和反向扩散矩阵之间的关系，这真的很酷，我想在这里还要注意一件事，我不会写出来。

是qti的i或barqti的ii，我不会写出来，因为你只需要确保列和行的和为零，所以我们就假设它是我们可以从其他值中提取的，i和j代表像索引，基本上，所以像，我的意思是，对于我们的目的来说。

它将是一系列，但这很难写出来，但你可以想成在矩阵中，你就取a矩阵和向量，所以矩阵你就取像i j行i列的条目，然后你取比例之间两个对应条目在向量中的比例，这是概率向量，但是，所以是的，我们有这个反向设置。

再次，你知道我们有这种出现我们的像比例，基本上，一个具体的分数，所以特别地，我们可以用它来近似我们的学习到的具体分数函数，我们的得分网络是s theta，你知道什么。

这就像回到了我们为什么喜欢参数化一切的原因，这种方法就是我们这样做的方式，是我们有初始状态i，然后我们基本上计算s theta i t的具体内容得分，这遍历所有不同的j索引，如果我们这样做。

它允许我们在并行跳转到我们想要跳转到的任何其他状态，因为我们参数化的方式，所以，在这个设置中，一切都会逐渐融合在一起，例如，我们可以在这里有一个初始的矩阵，我们展开它，这是负零点五的速率，零点一。

零点四，然后，我们可以在这里构建相应的反向矩阵，这个反向矩阵，你明白，如果你自己解决这个问题，看起来像这样，基本上我们在这里添加数据的比例，数据在值等于的，数据向量在时间，然后我们乘以这个，嗯。

用概率向量反转矩阵，实际上你出来的东西就像精确的反转，就像精确的反转，零点五，负零点一，负零点四，所以这里我们可以看到基本上它就起作用了，并且作为一个例子，我们也喜欢，你知道我们可以可视化如下。

比如在像均匀分布的，基础值趋向其他随机值，最终它像一些初始序列的噪声，并且我们也有它像掩码，基本上我们可以在这里喜欢，从口罩到我们最初的状态，像标记，所以这一切都很漂亮，我们有这个漂亮的设置，而且好吧。

我们还有一个稍微有点问题的地方，这基本上就像当我们试图实际上做它的逆向时，像采样，当我们试图像，通过各种，像，尝试模拟反转，速度相当慢，它速度这么慢的原因是因为我们从。

基本上所有的计算都归结为像我们的计算或计算考虑，基本上我们的x1到xd，我们只在这两个之间跳跃，另一个序列，只在一个位置或一个位置上有所不同，所以您知道，当我们构建反转时，我们只能也跳转到像序列。

但它们是不同的，但只是通过一个位置，你可以想象这将是非常昂贵的，特别是如果您需要跳跃，如果您需要连续地细化个别位置，像这样，所以我们基本上欺骗，这就是我们采样的方式，我们基本上允许多个步骤在内部。

我们将，我们允许在一个采样步骤中采样多个像跳跃，基本上就是这样，而不是像单独地，单独揭示标记，假设它是mask的mask，我们只是揭示这两个标记，同时我们可以这样做，给我们的设置相当容易。

但像更多的是一种等待，我们有一种方法可以做到这一点，我们可以这样做，说一个样本是一生中最美好的时刻在一步中，允许我们同时进行两个不同像跳跃，所以是的，我们可以把所有的东西放在一起。

我们已经建立了一个完整的设置，第一个想法是我们从所需的数据分布中获得一些样本，我们想要建模，我们定义了一个前向扩散过程，无论是均匀的还是mask，或者是什么，或者是一些更奇特的，对于所有这些。

对于融合过程，给定过渡，然后我们可以使用我们的得分来学习比例，嗯，我们定义的得分熵损失函数，然后我们可以使用这些比例来反转扩散过程，包括一些添加和一些离散化，以使采样更快，让我们来看看这如何工作，所以。

这就是一个我们成功地从核心库中随机生成的文本序列的例子，只是随机的，这像是gpt gpt two级别的，像嗯，采样程序，或gd gpu级别的，像数据集和模型集，模型大小，是的，它相当连贯。

它像它应该工作的一切，有点，这就是它工作的大致想法，但想法在这里是像，它与什么相比，像自动侵略性建模在，像数据的规模，所以，我们可以比较像这样的样本，并且我们有一个gpt two，我们称我们的模型为。

像得分熵，离散扩散，所以scd说，所以我们有一个在顶部的gpt two模型，我们有一个带有吸收过渡的said模型，这有点像你走到像，嗯，像掩码标记，我们我们有一个均匀的，我说，有一个均匀的过渡。

said u，很多，这意味着你从你标记转到另一个随机标记，每当你过渡时，我们通常能看到像，像我们的scd d集模型，在基线采样方法中，我们尝试从分布中采样，我们也可以可视化这个，像更多像作为一个函数。

像采样步骤的数量与质量之间的关系，所以，像这个图表在右边这里，我们有我们的两个，像gbd two模型，如果我们试图生成出长序列，它通常看起来像这样，我们生成出，你知道。

它需要一千二百四十个网络评估函数评估，我们生成出，像一个，你知道，它需要一千二百四十个网络评估函数评估，为了生成这些类型的输出之一，是的，当我们将这些生成的序列输入到另一个类似模型时。

它们通常的 perplexity 很高，另一个更大的模型，他们通常说，这些序列的 perplexity 非常高，这些序列的 likelihood 非常低，这些序列没有意义，基本上，我们可以从这里看到。

像 gpt-2 这样的模式通常在评估方面表现不佳，甚至在小模型和中模型方面，但这些被称为模型的行，基本上，我们可以在计算和序列数量之间进行权衡，我们可以尝试在质量与计算之间进行交易，基本上。

如果我们只取像六十四这样的步骤，这意味着我们做了大量的离散化，我们同时跳了很多步，我们最终得到了一种模型，它与 gpt-2 在生成质量上相当，但像它，但它的速度要快得多，基本上，如果我们喜欢。

如果我们真的喜欢增加迭代步骤的数量，那么，我们可以取，让我们说，一千二百二十四甚至两千四千八的采样离散化步骤，在这里，我们看到我们的质量像以对数线性的方式逐渐提高，线性对数，对数线性类型的方式，所以。

基本上，我们能够生成序列，是的，这在生成 perplexity 方面显著低于 gpt-2，这意味着它们是更好的序列，如果我们只是增加步骤的数量，我们不能与 gpt-3 或 gpt-4 一样做。

主要是因为模型大小是，在这种情况下，我们的模型大小像是很小的，像一百万，像四百万参数，所以我们在匹配他们的模型大小，我们正在匹配像颜色的模型，所以蓝色模型是小的。

橙色模型对于 gpt-3 和 gbfour 是中等的，另一个问题是数据集是私有的，但对于我们的 gpt-2，数据集是像 web 文本或开放 web 文本，这就是为什么我们可以进行苹果到苹果的比较，是的。

但是，对于我们的 gpt-2，数据集是像 web 文字或开放 web 文字，这就是为什么我们可以进行苹果到苹果的比较，是的，但是，所以，结论是，是的，所以相当令人惊讶的是，而且相当像，嗯，你知道的。

很漂亮，而且这就是一个相当像激励的东西，一个相当强烈的激励因素是，你知道，这个离散扩散模型与得分熵倾向于优于自攻击变压器，至少对于生成，质量和速度，我想另一个有趣的事情是，而且这是另一个重要的事情是。

对于这种像生成性建模技术，我们需要做的事情是我们需要控制生成，我们需要能够像，你知道的，控制我们如何生成，至少在这种情况下，我们可以这样做一些类似的事情，我们可以做像提示。

但是新的有趣的事情是我们可以从任意位置提示，所以如果我们有这个顶部的，我们可以取我们的蓝色提示文本，这里的想法是当我们生成新的序列时，我们只是围绕它生成，我们只是不，我们不改变提示文本。

但我们只是生成其他所有围绕它，这实际上是有原则的，如果你去通过数学，并允许你填充其余的信息那里，所以我们也有像中间这样的，我们中有像这些两个，像提示标记，序列，是一个提示标记，到中间。

我们只是围绕它生成，这允许我们像填充，是的，它通常像倾向于产生，像相当连贯的，像陈述，基本上这意味着我们能够以新的方式控制生成过程，更有趣的方式，这是一种，这不是，是的，你不能这样做与典型的自回归模型。

现在，我们有我们的生成质量像，最后一件事我们需要看的是，我们如何实际上评估这个生成过程的可能性，我们已，我们已经展示了我们如何可以学习，我们已经展示了如何生成，我们如何评估可能性，所以。

人们通常使用的评估可能性的标准是困惑度，输入序列x的困惑度基本上就是e的，到负一除以d的幂次方，x1到xd的日志概率，所以，这是一个非常典型的，用于自回归建模的度量标准，原因是因为它是。

一种相对原则性的方法来测量模型的能力，所以如果我们能在一些数据集上具有非常低的困惑度，像其他数据集一样，这意味着我们概括得很好，就像我们在压缩东西，这通常是一个好迹象，我们也可以直接比较这个。

这是直接可计算的自回归模型，因为我们可以直接计算这个p theta，最后，我们也倾向于以这种方式优化，因为至少对于自回归模型，我们优化以这个负对数概率，你知道仅仅通过指数增长有效地优化这个类似的东西。

这就是为什么我们可以报告像这样的基本信息，是的，所以嗯，对于融合模型，你知道，简单来说，我们也可以做类似的事情，数学实际上倾向于，你知道，有点复杂，但是，这里的关键洞察是在我们满足一些轻度条件下。

我们可以处理这个问题，比如非常轻度的条件，在我们的基础分布中，我们喜欢多长的时间，对于我们的生成过程，扩散的似然度基本如下，所以，我们的负对数似然度被这个积分的上限所限制，这个积分的期望值是已知的。

所有这些，当我们也添加一个已知常数c来查看c常数是预先知道的，在这里有趣的是，像这个积分，或者什么，这就是我们精确的相似度得分，去噪得分，熵损失，如果我们，如果我们回顾一下几页之前的幻灯片，嗯。

唯一的新东西是我们必须等待这个q t of x t y，这有点像其他加权，它实际上对任何事情都没有影响，就像对于任何计算，基本上它就像是一种其他的加权，但是是的，我们可以只是是的。

这意味着我们可以基本上像根据这个来训练，像损失，这个上部，像这个对数，似然边界损失，所以是的，我们最终得到一个困惑度边界，因为我们可以只是取输入序列的困惑度，我们只是通过这个嘈杂的嗅探损失函数来喂养它。

像这样加权，是的，我们基本上由事实得到上限，即像东西像e，像是一个单调的，基本上，这允许我们报告如困惑值，在实际应用中，它如何工作，在所有这些中，像不同的模式，我们喜欢做整个gpt到像。

在开放 web 文本上训练，在其他数据集上进行评估，像其他类型的数据集一样，设置类型，是的，在这里，我们像往常一样看到的是，我们的gpt2模型，倾向于产生最佳的似然值，但是，scdd像与吸收一样。

吸收和遮蔽的过渡，它倾向于像非常接近，基本上对于这些大多数数据集来说，如果我们有一个在百分之十以内的接近值，那么我们就会用下划线标出它，我们之所以有这个丰满，是因为，加上百分之十的截断是因为我们只报告。

我们只报告一个边界，而不是真正的像事实真相，像无边界的可能性，困惑度，但是，我们有这个像，嗯，在这里划线，而且我们还有最好的结果，加粗，而且我们始终看到的是，我们的所说的模型，它可以基本上匹配在。

像wiki text two wiki text one zero three，它处于这个困惑界限内，对于像ptb这样的东西，它实际上超过了，类似于现有的模型，类似于现有的gpt two。

类似于预训练的模型，并且有时通过相当显著的差距，如所示在，类似于这里中间的条形图，基本上这里有一条中间线，是的，所以这很棒，因为现在我们担心，我们可以显示基本上我们可以挑战自动的攻击性建模。

不仅限于生成质量的像，这更像是一种，因为里面有更多的，那里有更多的部分，但也包括困惑度，这更像是一种精炼的，更像是一种紧凑的方式来比较两个不同的自回归模型，嗯，在这里你会做什么。

你会只是生成到像文本标记的结束，然后你会像处理那种，并且通常对于这种，所有的开放网页文本数据，像序列都很长，像七百多或更多的标记之一千二百四，所以它们相当可比较，基本上，是的，但是是的，好的，总结一下。

是的，所以首先，是的，在离散空间中构建概率模型非常困难，我们有像GANs，花瓶，像扩散模型，许多这些东西都非常困难，直接地从连续空间扩展到离散空间，这就是为什么我们只有真正有自回归建模作为做事情的方式。

是的，基本上，所以是的，自回归建模是这个空间中唯一真正可行的范式，这里的想法是我们可以将基于分数的模型扩展到离散空间，我们可以通过相反地，而不是模型，模型数据分布的梯度，我们模型数据分布的比率。

我也被称为具体分数，我们优化这个新的得分匹配损失，被称为得分熵，我们可以也得到这些，嗯，去噪和隐含的变体使它们变得可追踪，然后当我们从基于分数的模型采样时，我们可以从我们的类似过程中采样。

使用前向和后向扩散过程进行扩散过程，所以特别是，前向扩散过程与我们的去噪分数熵损失协同工作，这使得一切变得无缝，整合在一起，我们可以使它快速且可控以生成我们的数据，这很好，最后。

我们的生成质量可以大大提高，因此可以超越自动的侵略性建模，因为我们，你知道，我们不必担心像接触这样的东西，我们可以并行生成整个序列，这允许我们在生成过程中获取更多的信息，最后。

我们还有一个基于分数熵的似然边界，这基本上与，像我们的像我们的像我们的得分损失一样对齐，基本上与似然边界对齐，人们希望优化或比较它，对于这项任务，我们基本上首次挑战了自动侵略性主导地位。

对于任何足够大的，序列，像GPT-2级别的，结果，是的，对于这个情况，我们在计算负对数似然度的界限，然后负对数，对数似然度，它进入这里的困惑度，困惑度可能会像前向KL散度一样，这是反向KL散度。

这将是模型如何工作的基本方式，是的，所以GPT-2像，你知道，如果你，如果我们像移除我们只报告界限的事实，它通常表现更好，你知道，实际上比那更接近，但这意味着它覆盖了数据分布模式的足够好。

但如果它也有泄漏，基本上我们可以生成低概率的序列，但像这个其他并不出现在我们的KL散度损失中，以前你将其嵌入在连续空间中，人们通常发现的问题是，它并不总是像那样有效，像我们的似然度一样，我的意思是。

所以我们通常喜欢，让我们来看看这个，像绘制这个图，嗯，这个东西，所以像对于之前的，像连续到未来的模型，那就像要糟糕得多，基本上就像这样非常喜欢，非常糟糕，就像比两点五倍还要糟糕。

像这样像这样是一个典型的范围，用于，像这样连续到离散，融合模型，我们在哪里像离散化了标记，而且问题在于对于生成质量的问题，就像速度要慢得多，基本上当我们试图像生成序列时，它就变得就像是的。

因为它是如此稀疏，我们必须喜欢，确保我们没有太多的错误，所以我们必须在里面取很多离散化步骤，所以，例如，像对于某些模型，你可能需要取像四千步才能生成一千个长度的序列，这实在是太多了，基本上。

想法是希望错误不会太多，你可以在两者之间跳跃，是否有一种原则性的方法可以做到这一点，它显示了这叫做tau跳跃，如果你熟悉像这样的，这叫做如何进入跳跃，和化学工程一样，文学或任何其他，这是一种工作。

所以如果你采取非常小的步骤，它会总是这样，它将类似于有条件独立的合理情况，假设像你的比例变化不大，你喜欢你的，你的模式变化不大，所以有点像，它是一个，它是基于离散化的方案，所以像扩散模型。

我们在欧拉马里也有类似的归一化方案，你可能可以学习任何像你在离散空间上的概率概率分布，无论是使用哪种方法，但是，这里的问题是关于哪一个在建立更好的归纳偏差和更好的归纳偏差方面做得更好，并且更易于优化。

并且对于优化更友好，Qt就像是过渡率，但是我们喜欢把它指数化，就像，特别是我们有这个，嗯，所以基本上Qt是过渡率，并且这个指数矩阵作为一个过渡核，我们可以这样做，如，哦，许多时间步，但是像这里。

问题是最好像，加入一个时间步，所以你变得容易，基本上，基本的线索倾向于像，说同样的话，但是，我们只是乘以一些噪声水平，所以它更像是所有的都内置在，这里只是一个过渡率，所以像这样，基本上它会从像均匀地。

这就是我们如何均匀地去到其他东西，或者像，在这个情况下，我们每次时间步都会去到一个质量标记，就像队列基本上被有效地放大，所以我们有一个缩放，取决于我们在每个时间步中添加的噪声量，是的。

这是真正的尺度控制，由sigma控制，所以这个界限基本上来自e的肘部，所以你假设你的扩散模型，你知道你有你的扩散过程，这是您在vae中的编码层，这是您的逆转，像真正的逆转，融合过程，这是您的解码器。

然后如果你只是计算出来，你插入它，你得到这个，这就是你得到的肘部，基本上是的，这个架构在这里是关键思想，序列神经网络序列的序列就像变压器，但我们基本上制造了一个变压器，但我们有一个非因果的掩码。

这允许我们像，这允许注意力层像，从完全像，嗯，从一切都到一切，基本上它就像一只狗，基本上是的，它将会是这样的，嗯，对于问题，回答，它将是这样的空格，所以你就像，你知道，填入它，你往下填，是的，我们有像。

我们在像GPT太小，小型模型和中型模型之间分离，在像中型模型之间，我们有吸收和均匀状态，基本上所以我们有这个像均匀过渡矩阵或只是掩码过渡矩阵，基本上和通常我们看到的是，均匀倾向于产生。

在掩码中产生更差的结果，基本上所以像只是随机地像翻转，翻转单词，这有意义吗，因为如果你随机翻转单词，那么你最终会得到一些我不太能理解的序列，而如果你只是掩码单词，那么序列仍然在大多数情况下有意义。

我的意思是，如果你假设我们可以像填充掩码一样，在这种情况下，这，这是我们的生成困惑度，基本上就是我们生成一个新的序列，然后我们取一个大型GPT模型，并评估这个生成的序列在GPT上的困惑度。

GPT 2 大，这是一个很常见的评估方法，如GPT 2大来评估事物，有许多基于这个的度量被构建，所以我们也看了看像Chet距离的度量，它也像工作一样看到改进，基本上所以基本上是的，你可以取一个大型模型。

然后尝试提取，一些特征表示或一些值，以数学视图你的像，查看你的小模型输出，是的，这里的问题是我们需要快速计算这个指数，如果我们的q是，让我们说像GPT 2，像分词器大小。

总词汇数像五十 thousand，对吧，如果我们想要计算这个矩阵指数，这就像需要花费太长时间，就像需要像像十秒钟那样，仅仅为了计算这个或者什么，甚至在cuda或者甚至在gpu上。

因为像它那么大我们尝试了实验，像其他，像，更像是复杂的q，这将允许我们做像，你知道，做这种计算，像，你知道更容易，但像它只是不太可能工作，因为事实是它太大了，它是一种本质上不同的建筑设计选择。

基本上就是，它不是为cuda设计来做这个矩阵指数运算的，谢谢大家的，嗯，参加，谢谢大家的聆听。

![](img/05b2a48299bc1e1aee16e6b314596f76_3.png)