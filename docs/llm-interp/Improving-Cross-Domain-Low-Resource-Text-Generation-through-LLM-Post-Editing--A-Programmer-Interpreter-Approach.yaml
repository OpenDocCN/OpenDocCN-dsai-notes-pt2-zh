- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 17:34:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04609](https://ar5iv.labs.arxiv.org/html/2402.04609)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhuang Li, Levon Haroutunian,
  prefs: []
  type: TYPE_NORMAL
- en: Raj Tumuluri, Philip Cohen, Gholamreza Haffari
  prefs: []
  type: TYPE_NORMAL
- en: Openstream.ai
  prefs: []
  type: TYPE_NORMAL
- en: '{zhuang.li, levon, raj, phil.cohen, reza.haffari}@openstream.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Post-editing has proven effective in improving the quality of text generated
    by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct
    updating of their parameters to enhance text quality is infeasible or expensive.
    However, relying solely on smaller language models for post-editing can limit
    the LLMs’ ability to generalize across domains. Moreover, the editing strategies
    in these methods are not optimally designed for text-generation tasks. To address
    these limitations, we propose a neural programmer-interpreter approach that preserves
    the domain generalization ability of LLMs when editing their output. The editing
    actions in this framework are specifically devised for text generation. Extensive
    experiments demonstrate that the programmer-interpreter significantly enhances
    GPT-3.5’s performance in logical form-to-text conversion and low-resource machine
    translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods
    in cross-domain settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach'
  prefs: []
  type: TYPE_NORMAL
- en: Zhuang Li, Levon Haroutunian, Raj Tumuluri, Philip Cohen, Gholamreza Haffari
    Openstream.ai {zhuang.li, levon, raj, phil.cohen, reza.haffari}@openstream.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c816e18cb92b9d894f04f8214909b45d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The diagram of our post-editing architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Large pre-trained language models like GPT-3.5¹¹1https://platform.openai.com/docs/models/gpt-3-5-turbo
    or GPT-4²²2https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo have
    gained significant attention in natural language research. However, fine-tuning
    these models for specific tasks is challenging due to limited computational resources
    or inaccessible parameters. Consequently, many researchers resort to using web
    APIs for instructing LLMs, leveraging zero-shot or few-shot in-context learning,
    enabling the LLMs to tackle tasks they weren’t explicitly trained for. Unfortunately,
    this approach falls short when tackling some low-resource sequence generation
    tasks in machine translation (MT), and logical form (LF)-to-text translation,
    as shown in Lai et al. ([2023](#bib.bib5)); Haroutunian et al. ([2023](#bib.bib4)).
    In such cases, minimal task-specific data was available during the LLMs’ pre-training
    phase. The output quality of LLMs for such tasks is compromised due to the absence
    of task-specific knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, a promising set of solutions suggests integrating
    task-specific knowledge into language models through post-editing the generated
    text using a smaller model fine-tuned on task-specific data. Yet, these methods
    are not without their drawbacks. Our findings indicate that exclusive reliance
    on a smaller model for editing, e.g. Self-Correct Welleck et al. ([2022](#bib.bib14)),
    results in suboptimal performance in domain generalization scenarios, likely due
    to the inherently limited domain knowledge within these smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: As LLMs (i.e. GPT-3.5 or GPT-4) have shown superior domain generalization ability Wang
    et al. ([2023](#bib.bib13)); Yang et al. ([2023](#bib.bib16)) over the fine-tuned
    model, we introduce an innovative approach based on the programmer-interpreter
    framework Reed and de Freitas ([2016](#bib.bib10)), which benefits from the domain
    generalization ability from LLMs. The programmer component - a smaller language
    model fine-tuned on task-specific data - delivers precise edit instructions to
    the larger language model, thus infusing the large model with task-specific knowledge.
    The interpreter, in turn, edits the large model’s output given the provided instructions.
    Contrary to the Self-Correct Welleck et al. ([2022](#bib.bib14)) approach that
    utilizes smaller, fine-tuned models for editing, our interpreter is also an LLM.
    The editing is accomplished through the use of prompts that include editing instructions,
    eliminating the need for any additional fine-tuning. This distinct framework guarantees
    the preservation of the LLM’s domain generalization ability while simultaneously
    benefiting from the task-specific knowledge encoded by the programmer. Our method
    distinguishes itself from approaches like PiVe Han et al. ([2023](#bib.bib3)),
    which also employ an LLM as the interpreter but focus on graph generation tasks.
    In contrast, our approach specifically designs word-level editing actions in the
    instructions, tailored to enhance text generation. This targeted strategy renders
    our method more effective for text-generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our key contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a novel programmer-interpreter method that enhances LLM in low-resource
    cross-domain text generation tasks. This approach capitalizes on the programmer’s
    ability to encode task-specific knowledge and the interpreter’s prowess in domain
    generalization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We design editing operations optimized for text generation tasks, leading to
    substantial text quality improvements by simply prompting the LLMs with action
    instructions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In scenarios where training and test data span different domains, our comprehensive
    empirical studies confirm that the method outperforms all existing LLM post-editing
    baselines in low-resource MT and LF-to-Text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Programmer-Interpreter Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The objective in LF-to-text and MT tasks using LLMs is to generate a high-quality
    output text ${\bm{y}}$, denoted as ${\bm{y}}^{\prime}=\operatorname*{arg\,max}_{{\bm{y}}\in\mathcal{Y}}P({\bm{y}}|{\bm{x}},\mathcal{C})$,
    given an input ${\bm{x}}$ (e.g., LF, source-language utterance) and an exemplar
    pool $\mathcal{C}=\{({\bm{x}}_{j},{\bm{y}}_{j},{\bm{y}}^{*}_{j},{\bm{a}}^{*}_{j})\}^{|\mathcal{C}|}_{j=1}$.
    Here, ${\bm{x}}_{i}$ and ${\bm{y}}_{j}$ are the ground truth input-output pairs,
    ${\bm{y}}^{*}_{j}$ is the imperfect translation of ${\bm{x}}_{i}$, and ${\bm{a}}^{*}_{j}$
    represents the Oracle edit actions that can modify ${\bm{y}}^{*}_{j}$ into ${\bm{y}}_{j}$.
    Our approach focuses on achieving high-quality generation through iterative refinement
    of the initial output text produced by an LLM. Specifically, the iterative refinement
    framework includes three-parameterized modules: a Generator, a Programmer, and
    an Interpreter,³³3To save space, we simplify the marginalization notation.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle P({\bm{y}}^{t}&#124;{\bm{x}},\mathcal{C})=\overbrace{P({\bm{y}}^{0}&#124;{\bm{x}},M(\cdot))}^{Generator}\times$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sum^{t-1}_{\{{\bm{a}},{\bm{y}}\}}\prod^{t-1}_{i=0}(\overbrace{P({\bm{y}}^{i+1}&#124;{\bm{a}}^{i},{\bm{y}}^{i},{\bm{x}},A(\cdot))}^{Interpreter}\times\overbrace{P({\bm{a}}^{i}&#124;{\bm{y}}^{i},{\bm{x}}))}^{Programmer}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'The Generator corresponds to the LLM (e.g. GPT-3.5, GPT-4). It produces the
    initial output text, ${\bm{y}}^{0}$, given the input ${\bm{x}}$, a set of examples
    retrieved by the function $M({\bm{x}},\mathcal{C})$ when performing in-context
    learning. The Programmer, a module that creates editing actions ${\bm{a}}^{i}$
    given ${\bm{x}}$ and the current imperfect output ${\bm{y}}^{i}$, is a pre-trained
    Sequence-to-Sequence Sutskever et al. ([2014](#bib.bib11)) language model, such
    as mT5 Xue et al. ([2021](#bib.bib15)) or flan-T5 Chung et al. ([2022](#bib.bib1)),
    fine-tuned on a synthetic dataset. The Interpreter, essentially also an LLM, refines
    the imperfect intermediate output ${\bm{y}}^{i}$ by processing instructions that
    incorporate predicted editing actions and few-shot editing examples, retrieved
    via the function $A({\bm{x}},\mathcal{C})$. Please note that the Programmer has
    much fewer parameters than the LLM used by the Generator and Interpreter. After
    several iterative refinements, we arrive at the final output ${\bm{y}}^{t}$ generated
    by the LLM. During generation, we assume no access to the parameters of the LLMs
    but only obtain the output text by providing prompting instructions. The implementation
    details of each module are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generator.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To generate the initial output, we supply a prompt composed of a few-shot set
    of exemplar pairs, denoted as $M({\bm{x}},\mathcal{C})=\{({\bm{x}}_{j},{\bm{y}}_{j})\}^{m}_{j=1}$,
    selected from a pool of reference pairs $\mathcal{C}$. This is accompanied by
    an instruction prompting the LLM to produce output ${\bm{y}}^{0}$ based on the
    input ${\bm{x}}$. The retrieval function identifies the closest pairs by calculating
    the cosine similarity of TF-IDF features between ${\bm{x}}$ and other instances
    of ${\bm{x}}$ in $\mathcal{C}$.
  prefs: []
  type: TYPE_NORMAL
- en: Programmer.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After obtaining the initial or intermediate output ${\bm{y}}^{i}$ from either
    the Generator or the Interpreter, we combine the input ${\bm{x}}$ and ${\bm{y}}^{i}$
    into a single sequence and feed it to the Programmer to generate a sequence of
    edit actions ${\bm{a}}^{i}$. We create a synthetic training set $\mathcal{T}$,
    extracted from the example pool $\mathcal{C}$, for fine-tuning the Programmer.
    Each pair in $\mathcal{T}$ is defined as $({\bm{x}}_{concat},{\bm{a}}^{*})$, where
    ${\bm{x}}_{concat}$ is the concatenated sequence of ${\bm{x}}$ and ${\bm{y}}^{*}$,
    serving as the input for the Programmer. The output ${\bm{a}}^{*}$ is the sequence
    of Oracle edit actions, synthetically generated based on the reference pairs in
    $\mathcal{C}$. For each reference ${\bm{y}}\in\mathcal{C}$, we calculate the word-level
    edit distance to the imperfect translation ${\bm{y}}^{*}$, generating intermediate
    edit actions. Only INSERT-word and DELETE-word actions are retained in the sequence,
    forming the final training sequence ${\bm{a}}^{*}$ for the Programmer. If ${\bm{y}}^{*}$
    is identical to the reference ${\bm{y}}$, the action is labeled as “NoAction”,
    indicating that no refinement is needed for that instance. Unlike PiVe, which
    generates the imperfect translation ${\bm{y}}^{*}$ by scrambling the original
    ${\bm{y}}$, we directly use the initial output ${\bm{y}}^{0}$ from the Generator
    as ${\bm{y}}^{*}$ in both $\mathcal{C}$ and $\mathcal{T}$. This approach enables
    the Programmer to learn an action distribution that more effectively corrects
    translation errors from LLMs. In practice, to address the rarity of “NoAction”
    instances, we supplement these cases by creating augmented pairs, each consisting
    of two identical ${\bm{y}}$ sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MT (Kashmiri to English) | LF-to-Text (AMR to English) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | GEN | CONV | Bio-AMR |'
  prefs: []
  type: TYPE_TB
- en: '| Method | BLEU | BERT | ChrF++ | BLEU | BERT | ChrF++ | BLEU | BERT | ChrF++
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned mT5/flan-T5 | 16.58 | 89.32 | 41.77 | 13.19 | 88.83 | 33.03 |
    9.27 | 87.90 | 41.06 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|       Initial | 9.21 | 87.29 | 34.30 | 5.92 | 87.24 | 26.23 | 9.63 | 88.57
    | 43.98 |'
  prefs: []
  type: TYPE_TB
- en: '|       Self-Correct | 13.11 | 89.02 | 38.98 | 12.73 | 89.61 | 33.76 | 11.64
    | 89.44 | 46.05 |'
  prefs: []
  type: TYPE_TB
- en: '|       Algo-Refine | 8.40 | 86.92 | 39.66 | 6.29 | 87.31 | 32.21 | 7.72 |
    86.64 | 43.39 |'
  prefs: []
  type: TYPE_TB
- en: '|       Self-Refine | 8.13 | 86.54 | 31.78 | 4.73 | 86.55 | 24.13 | 8.67 |
    87.34 | 39.63 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline    Prog-Refine (Zero-shot Act.) | 13.81 | 88.58 | 39.00 | 12.09
    | 89.41 | 33.41 | 11.43 | 89.30 | 45.44 |'
  prefs: []
  type: TYPE_TB
- en: '|       Prog-Refine (Few-shot Act.) | 16.32 | 90.36 | 42.44 | 14.78 | 90.19
    | 35.48 | 13.64 | 89.27 | 47.69 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline    Prog-Refine (ORACLE) | 43.48 | 92.11 | 65.29 | 42.42 | 93.00
    | 42.42 | 27.77 | 90.01 | 52.86 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The main results of MT on GEN and CONV test sets, and LF-to-Text on
    Bio-AMR test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreter.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To edit the intermediate output ${\bm{y}}^{i}$, we engage the LLM in the Interpreter
    role by providing it with prompting instructions. Given the edit instructions
    ${\bm{a}}^{i}$ and a pair $({\bm{y}}^{i},{\bm{x}})$, the LLM can INSERT or DELETE
    words in order to generate the modified text ${\bm{y}}^{i+1}$. We also incorporate
    a few-shot examples that demonstrate editing procedures, extracted from $\mathcal{C}$
    and denoted as $A({\bm{x}},\mathcal{C})=\{({\bm{x}}_{j},{\bm{y}}_{j},{\bm{y}}^{*}_{j},{\bm{a}}^{*}_{j})\}_{j=1}^{n}$.
    These examples are selected based on the cosine similarity between the TF-IDF
    features of ${\bm{x}}$ and those in $\mathcal{C}$. Furthermore, to mimic action
    prediction errors from the Programmer, we adopt an adversarial in-context learning
    strategy, similar to the approach in Zhuo et al. ([2023](#bib.bib18)). This involves
    corrupting the action sequence by deleting Oracle actions with a certain probability
    $d\%$. If an action is not deleted, we swap it with other actions from $\mathcal{C}$
    at the same probability $d\%$. Through this manipulation, we have discovered that
    the LLM’s exceptional text generalization ability enables it to effectively comprehend
    the editing instructions. As a result, it can generate high-quality text after
    performing the necessary edits, even if the predicted actions from the Programmer
    are not completely accurate. See Figures [2](#A1.F2 "Figure 2 ‣ A.1 Prompt Example
    for Editing Text ‣ Appendix A Appendix ‣ Improving Cross-Domain Low-Resource Text
    Generation through LLM Post-Editing: A Programmer-Interpreter Approach") and [3](#A1.F3
    "Figure 3 ‣ A.1 Prompt Example for Editing Text ‣ Appendix A Appendix ‣ Improving
    Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter
    Approach") in the Appendix for zero/few-shot instruction examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setup.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our experiments, we default to using GPT-3.5-turbo-0301 as the LLM for the
    Generator in both the zero-shot and few-shot settings. For the Interpreter, we
    use GPT-3.5-turbo-0301 in the zero-shot setting and GPT-3.5-turbo-16k⁴⁴4https://platform.openai.com/docs/models/gpt-3-5-turbo
    in the few-shot setting. For the Generator used across all settings and baselines,
    we consistently use 0 and 5 shots for MT and LF-to-Text, respectively. For the
    Interpreter in the few-shot setting, we apply 10 and 5 action examples for MT
    and LF-to-Text, respectively, with a 50% action corruption probability. For the
    MT and LF-to-Text tasks, we employ mT5-base and flan-T5-base as the backbones
    of the Programmers, respectively. These backbone choices are driven by a computationally
    efficient setup, ensuring the models fit within an Nvidia V100 with 16GB memory.
    We train our programmers with a dev set to select the optimal model. Our search
    for the best learning rate includes [5e-5, 1e-4, 2e-4], while the range of epochs
    considered is [5, 10, 20], with batch sizes 4\. GPTs require no fine-tuning. Each
    generation of 1096 tokens costs approximately $0.0015\. We limit Self-Correct
    and Self-Refine to five editing iterations, as their performance typically stabilizes
    within this range. Conversely, Prog-Refine and Algo-Refine may require additional
    iterations for convergence, especially when ‘NoAction’ instances are infrequent.
    Thus, we continue for up to 15 iterations, ceasing only if over 95% of actions
    are ‘NoAction’.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To simulate low-data scenarios, in the context of MT, we utilize a Kashmiri-English
    dataset from IndicTrans2 Gala et al. ([2023](#bib.bib2)). Since Kashmiri is a
    notably low-resource language, translating it poses a formidable challenge for
    LLMs. The dataset provides 26,016 training pairs, which we use to generate synthetic
    data for action generation. The development set consists of 997 pairs. The dataset
    includes two distinct test sets, GEN and CONV, with 1,024 and 1,503 pairs, respectively.
    Each of the training, development, and test sets originates from different domains.
    For LF-to-Text, we employ the AMR-LDC2.0⁵⁵5https://catalog.ldc.upenn.edu/LDC2017T10
    dataset, which contains 22,550 AMR-English pairs for training and 1,368 pairs
    for development. For testing, we turn to a separate dataset, Bio-AMR⁶⁶6https://amr.isi.edu/download.html,
    which offers 500 pairs in a different domain. Likewise, the AMR-to-Text task poses
    a low-resource challenge for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluate our approach, Prog-Refine, which utilizes zero-shot action exemplars
    (Zero-shot Act.) and few-shot action exemplars (Few-shot Act.) for Interpreters,
    against five baseline methods and an ORACLE setting
  prefs: []
  type: TYPE_NORMAL
- en: i) Fine-tuned Models include mT5-base for MT and flan-T5-base for LF-to-Text
    generation, both of which are fine-tuned on the training set consisting of pairs
    $({\bm{x}},{\bm{y}})\in\mathcal{C}$. These baseline models do not perform any
    refinement.
  prefs: []
  type: TYPE_NORMAL
- en: ii) GPT-3.5 + Initial simply applies the GPT-3.5 as the Generator to obtain
    the text without any further refinement.
  prefs: []
  type: TYPE_NORMAL
- en: iii) GPT-3.5 + Self-Correct Welleck et al. ([2022](#bib.bib14)) fine-tunes smaller
    models to be the Interpreter, fixing the output errors of the large models given
    the feedback. Here, we supply the edit actions produced by our Programmer as feedback
    to the fine-tuned Interpreters. These Interpreters are also built upon mT5-base
    or flan-T5-base.
  prefs: []
  type: TYPE_NORMAL
- en: iv) GPT-3.5 + Algo-Refine directly ‘Insert’ or ‘Delete’ specific words in certain
    positions of the generated text instead of using an Interpreter to rewrite. Therefore,
    in this baseline, we also apply the Interpreter to predict the indices of words
    for actions. This method is prevalent in the MT literature; e.g. see  Vu and Haffari
    ([2018](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: v) GPT-3.5 + Self-Refine Madaan et al. ([2023](#bib.bib6)) leverages an LLM
    to provide feedback for its own output, enabling self-refinement without the need
    for additional fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: vi) GPT-3.5 + Prog-Refine (ORACLE) applies the ORACLE actions generated by comparing
    the reference in the test set with the initial output of the Generator, allowing
    for optimal refinement after one iteration in the Zero-shot Act. setting.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For LF-to-Text and MT tasks, we utilize three evaluation metrics to assess
    the quality of the final output text generated by the Programmer-Interpreter framework:
    BLEU Papineni et al. ([2002](#bib.bib7)), BERTScore [Zhang et al.](#bib.bib17)
    and Chrf++ Popović ([2017](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Main Results and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [1](#S2.T1 "Table 1 ‣ Programmer. ‣ 2 Programmer-Interpreter Approach
    ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach") shows that GPT-3.5 + Prog-Refine notably boosts
    the Generator’s performance (i.e., GPT-3.5 + Initial), underlining our method’s
    effectiveness in cross-domain scenarios by enhancing initial GPT-3.5 outputs.
    Moreover, the few-shot setting (Few-shot Act.) significantly outperforms both
    the zero-shot (Zero-shot Act.) setting and all other refinement baselines. It’s
    also noteworthy that applying ORACLE action to our method can lead to a roughly
    30-point increase in BLEU score, suggesting substantial potential for improvement
    in our approach. In comparison, Self-Refine shows minimal improvement, possibly
    due to its limited integration of task-specific knowledge. Algo-Refine inconsistently
    improves the initial text, lacking the robustness seen in our method. We note
    that rewriting Interpreters, as in our approach and Self-Correct, can eliminate
    invalid actions, thus enhancing editing quality. However, Algo-Refine does not
    possess this capability and is susceptible to incorrect feedback actions. The
    Self-Correct method, using a fine-tuned Interpreter, along with fine-tuned mT5/flan-T5
    models, demonstrates better performance than other baselines across various tasks.
    This underscores the importance of learning task-specific knowledge, especially
    in low-resource scenarios. Nonetheless, these methods face significant challenges
    in cross-domain applications, as further evidenced by our analysis in Table [4](#S3.T4
    "Table 4 ‣ Domain Discrepancy. ‣ 3.2 Ablation Study ‣ 3 Experiments ‣ Improving
    Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter
    Approach").'
  prefs: []
  type: TYPE_NORMAL
- en: '| MT (Kashmiri to English) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| #Iter | BLEU | BERT | ChrF++ | NoAct% |'
  prefs: []
  type: TYPE_TB
- en: '| Iter 0 | 5.92 | 87.24 | 26.23 | 17.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Iter 1 | 11.01 | 89.18 | 33.05 | 79.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Iter 2 | 11.87 | 89.36 | 33.41 | 90.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Iter 3 | 12.09 | 89.41 | 33.41 | 95.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Iter 4 | 12.26 | 89.45 | 33.43 | 97.21 |'
  prefs: []
  type: TYPE_TB
- en: '| Iter 5 | 12.36 | 89.47 | 33.39 | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The influence of 5 iterations on main results of MT using Prog-Refine
    (Zero-shot Act.) on CONV test set. NoAct%: The percentage of utterances requiring
    no refinement, as indicated by ‘NoAction’.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| MT (Kashmiri to English) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | BLEU | BERT | ChrF++ |'
  prefs: []
  type: TYPE_TB
- en: '| Initial | 5.92 | 87.24 | 26.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Edit: DEL, INS | 12.36 | 89.47 | 33.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Edit: DEL | 12.27 | 89.42 | 33.21 |'
  prefs: []
  type: TYPE_TB
- en: '| Edit: INS | 12.18 | 89.45 | 33.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Unordered: DEL, INS | 7.12 | 87.86 | 29.21 |'
  prefs: []
  type: TYPE_TB
- en: '| Unordered: DEL | 6.52 | 87.51 | 26.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Unordered: INS | 7.14 | 88.04 | 30.38 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The results of MT using Prog-Refine (Zero-shot Act.) on CONV test
    set at 5th iteration with different types of actions. Edit: Actions are generated
    based on edit distance. Unordered: Actions without any specific order. INS: Insertion.
    DEL: Deletion.'
  prefs: []
  type: TYPE_NORMAL
- en: Refinement Iterations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [2](#S3.T2 "Table 2 ‣ 3.1 Main Results and Analysis ‣ 3 Experiments
    ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach"), we observe that Prog-Refine significantly
    improves the initial output generated by the Generator. However, it only demonstrates
    marginal improvements in the subsequent outputs from the Interpreter, even after
    four additional iterations. We hypothesize that this limited improvement may be
    attributed to training the model solely on synthetic data generated by the Generator,
    so the action distribution might be different to the ones for modifying the output
    of the Interpreter in the subsequent iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: Action Types.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We further examine the impact of solely utilizing one type of action and the
    influences of disregarding the sequence of these actions. In the setting with
    unordered actions, oracle actions are generated by simply contrasting the differences
    within two sentences’ unordered sets of words. As depicted in Table [3](#S3.T3
    "Table 3 ‣ 3.2 Ablation Study ‣ 3 Experiments ‣ Improving Cross-Domain Low-Resource
    Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach"),
    the Delete and Insert actions, when used individually, can deliver performance
    metrics on par with when they are combined. However, ignoring the order of actions
    can lead to a substantial decline in the refinement performance. This highlights
    that LLM editing methods like PiVe, which utilize unordered insertions, are not
    optimally suited for our tasks. Further analysis is in Appendix [A.5](#A1.SS5
    "A.5 F1 for Action Prediction ‣ Appendix A Appendix ‣ Improving Cross-Domain Low-Resource
    Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach").'
  prefs: []
  type: TYPE_NORMAL
- en: Domain Discrepancy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Table [4](#S3.T4 "Table 4 ‣ Domain Discrepancy. ‣ 3.2 Ablation
    Study ‣ 3 Experiments ‣ Improving Cross-Domain Low-Resource Text Generation through
    LLM Post-Editing: A Programmer-Interpreter Approach"), a domain shift dramatically
    impacts the performance of flan-T5 and Self-Correct. While both baseline models
    show markedly superior performance on the in-domain test set relative to our model,
    ours either surpasses or equals their performance in the cross-domain MT and AMR-to-Text
    test sets. This disparity in performance is likely due to the smaller models’
    limited cross-domain generalization. Similarly, in MT tasks, our preliminary experiments
    show that fine-tuned mT5 achieves 30 points of BLEU on the in-domain test but
    only 16 and 13 on out-of-domain tests. For further details on domain discrepancies,
    see Appendix [A.3](#A1.SS3 "A.3 Measures of Domain Discrepancy ‣ Appendix A Appendix
    ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach").'
  prefs: []
  type: TYPE_NORMAL
- en: '| LF-to-Text (AMR to English) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | BLEU | BERT | ChrF++ |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned flan-T5 | 34.63 | 95.05 | 66.97 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|       Initial | 19.67 | 92.10 | 55.98 |'
  prefs: []
  type: TYPE_TB
- en: '|       Self-Correct | 34.49 | 94.68 | 66.81 |'
  prefs: []
  type: TYPE_TB
- en: '|       Self-Refine | 16.16 | 91.08 | 52.78 |'
  prefs: []
  type: TYPE_TB
- en: '|       Prog-Refine | 29.12 | 94.01 | 64.85 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: LF-to-Text results using Prog-Refine (Zero-shot Act.) on the in-domain
    LDC test.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial In-context Learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [5](#S3.T5 "Table 5 ‣ Adversarial In-context Learning. ‣ 3.2 Ablation
    Study ‣ 3 Experiments ‣ Improving Cross-Domain Low-Resource Text Generation through
    LLM Post-Editing: A Programmer-Interpreter Approach") indicates 0.0 for no corruption
    and 1.0 for complete discarding of exemplar actions, leaving only ${({\bm{x}}_{j},{\bm{y}}^{*}_{j},{\bm{y}}_{j})}_{j=1}^{n}$.
    Rates between 0.0 and 1.0 represent partial corruption of Oracle actions. The
    results suggest that neither full application nor total corruption of Oracle actions
    is optimal. However, partial corruption leads to improved performance. Additionally,
    across all corruption rates, few-shot settings consistently outperform zero-shot
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LF-to-Text (AMR to English) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rate | BLEU | BERT | ChrF++ |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0 | 12.06 | 89.31 | 46.23 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 12.35 | 89.36 | 46.49 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 13.64 | 89.27 | 47.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 11.97 | 89.32 | 46.13 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: LF-to-Text results using Prog-Refine (Few-shot Act.) vary with different
    corruption probabilities for the action sequence in the adversarial in-context
    examples used for the Interpreter.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present a programmer-interpreter method that iteratively refines LLM outputs
    using edit actions from a fine-tuned programmer and an LLM interpreter. Our approach
    combines the task-specific encoding capacity of a fine-tuned model with the domain
    generalization strength of the LLM, incorporating specifically designed actions
    for text generation. The experiments confirm its efficacy, showing significant
    improvements in LLM-generated text quality for low-resource MT and LF-to-Text
    tasks. Moreover, our approach outperforms established baselines in cross-domain
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has two primary limitations. First, in in-domain tests, our approach
    does not outperform smaller models, such as mT5 and flan-T5\. Considering the
    performance improvements we observed when using ORACLE actions, we believe there
    is substantial potential to further enhance our method for text generation in
    the in-domain evaluation setting. Second, our approach requires internet transmission
    of prompt instructions to the servers of ChatGPT. This could potentially lead
    to a risk of privacy leakage, which is a critical concern in data-sensitive applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gala et al. (2023) Jay Gala, Pranjal A Chitale, Raghavan AK, Sumanth Doddapaneni,
    Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully,
    Vivek Raghavan, et al. 2023. Indictrans2: Towards high-quality and accessible
    machine translation models for all 22 scheduled indian languages. *arXiv preprint
    arXiv:2305.16307*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2023) Jiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan Shareghi.
    2023. Pive: Prompting with iterative verification improving graph-based generative
    capability of llms. *arXiv preprint arXiv:2305.12392*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haroutunian et al. (2023) Levon Haroutunian, Zhuang Li, Lucian Galescu, Philip
    Cohen, Raj Tumuluri, and Gholamreza Haffari. 2023. Reranking for natural language
    generation from logical forms: A study based on large language models. *arXiv
    preprint arXiv:2309.12294*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2023) Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu
    Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond
    english: Towards a comprehensive evaluation of large language models in multilingual
    learning. *arXiv preprint arXiv:2304.05613*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2023. Self-refine: Iterative refinement with self-feedback. *arXiv preprint
    arXiv:2303.17651*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pillutla et al. (2021) Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
    John Thickstun, Sean Welleck, Yejin Choi, and Zaïd Harchaoui. 2021. MAUVE: measuring
    the gap between neural text and human text using divergence frontiers. In *Advances
    in Neural Information Processing Systems (NeurIPS)*, pages 4816–4828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Popović (2017) Maja Popović. 2017. chrf++: words helping character n-grams.
    In *Proceedings of the second conference on machine translation*, pages 612–618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed and de Freitas (2016) Scott E. Reed and Nando de Freitas. 2016. Neural
    programmer-interpreters. In *International Conference on Learning Representations
    (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. *Advances in neural information
    processing systems*, 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu and Haffari (2018) Thuy Vu and Gholamreza Haffari. 2018. Automatic post-editing
    of machine translation: A neural programmer-interpreter approach. In *Proceedings
    of the 2018 conference on empirical methods in natural language processing*, pages
    3048–3053.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Jindong Wang, HU Xixu, Wenxin Hou, Hao Chen, Runkai Zheng,
    Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, et al. 2023. On the
    robustness of chatgpt: An adversarial and out-of-distribution perspective. In
    *ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning
    to self-correct. *arXiv preprint arXiv:2211.00053*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively
    multilingual pre-trained text-to-text transformer. In *Proceedings of the 2021
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 483–498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,
    Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power
    of llms in practice: A survey on chatgpt and beyond. *arXiv preprint arXiv:2304.13712*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(17) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
    Artzi. Bertscore: Evaluating text generation with bert. In *International Conference
    on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuo et al. (2023) Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing
    Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023. On robustness of prompt-based
    semantic parsing with large pre-trained language model: An empirical study on
    codex. In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 1090–1102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Prompt Example for Editing Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figures [2](#A1.F2 "Figure 2 ‣ A.1 Prompt Example for Editing Text ‣ Appendix
    A Appendix ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach") and [3](#A1.F3 "Figure 3 ‣ A.1 Prompt Example
    for Editing Text ‣ Appendix A Appendix ‣ Improving Cross-Domain Low-Resource Text
    Generation through LLM Post-Editing: A Programmer-Interpreter Approach") depict
    the exemplary zero/few-shot prompt employed in LF-to-Text.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf47e875d57fdd1628b0eb43deefa0cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The zero-shot exemplary prompt for LF-to-Text.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b232658e787b249fca4bc0c9c630c102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The few-shot exemplary prompt for LF-to-Text.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Adaption of Self-Corrector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our experiment, we adapted the implementation of the Self-Corrector to better
    suit our specific requirements. To customize it for our context, we constructed
    the training set for the Self-Corrector’s Interpreter as follows: the input consists
    of a concatenation of Kashrimi/AMR, text produced by the Generator, and edit actions.
    The output, on the other hand, is the ground truth text. For a fair comparison
    with our approach and to minimize training and data collection expenses, models
    are trained only during the first iteration. Additionally, the generation of the
    training set solely utilizes text from the Generator in the initial iteration,
    without using text from the Interpreter in subsequent refinement iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Measures of Domain Discrepancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tables [6](#A1.T6 "Table 6 ‣ A.3 Measures of Domain Discrepancy ‣ Appendix
    A Appendix ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach") and [7](#A1.T7 "Table 7 ‣ A.3 Measures of
    Domain Discrepancy ‣ Appendix A Appendix ‣ Improving Cross-Domain Low-Resource
    Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach")
    present domain discrepancies for the training/development/testing sets for the
    MT and LF-to-text generation tasks. The domain discrepancy measures include the
    KL-divergence (based on the unigram distributions) and MAUVE Pillutla et al. ([2021](#bib.bib8)).
    KL-divergence scores are higher when two distributions are more different from
    each other. MAUVE scores, which have a range (0,1), are lower when two distributions
    are more different from each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '| splits compared | KL-div $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Measures of domain difference across different splits of the machine
    translation datasets. KL-divergence scores are calculated for the English sentences
    in each data split, with additive smoothing ($\alpha=1\times 10^{-4}$). For MAUVE,
    5000 sentences are sampled from the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| splits compared | KL-div $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Measures of domain difference across different splits of the AMR dataset.
    KL-divergence scores are calculated for the English sentences in each data split,
    with additive smoothing ($\alpha=1\times 10^{-4}$). For MAUVE, 5000 sentences
    are sampled from the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Table [6](#A1.T6 "Table 6 ‣ A.3 Measures of Domain Discrepancy ‣ Appendix
    A Appendix ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach"), we observe that the domain of test-gen is
    closer to the training set compared to that of the test-conv. This is pronounced
    in higher KL-divergence and lower MAUVE numbers for the test-conv compared to
    test-gen, with respect to the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Table [7](#A1.T7 "Table 7 ‣ A.3 Measures of Domain Discrepancy ‣ Appendix
    A Appendix ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach") , we observe a higher difference for the domain
    of the biology-AMR test compared to the LDC2.0-AMR test set, with respect to the
    training/development sets of the LDC2.0-AMR dataset. This is pronounced in larger
    KL divergence and lower MAUVE numbers compared to those for the LDC2.0-AMR test
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 F1 Definition for Action Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F1$ | $\displaystyle=2\times\frac{P_{act}\times R_{act}}{P_{act}+R_{act}}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Here, $P_{act}$ represents action precision, defined as the ratio of predicted
    actions present in the reference action sequence to the total number of predicted
    actions. $R_{act}$ denotes action recall, which is the ratio of predicted actions
    that appear in the reference action sequence to the total number of actions in
    the reference sequence. The F1 score, thus, provides a harmonious mean of these
    two metrics.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 F1 for Action Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [8](#A1.T8 "Table 8 ‣ A.5 F1 for Action Prediction ‣ Appendix A Appendix
    ‣ Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing:
    A Programmer-Interpreter Approach") reveals that predicting INSERT actions is
    a relatively easier task compared to predicting DELETE actions. This observation
    is reasonable since the Programmer only needs to learn how to DELETE words from
    the text with a fixed vocabulary, whereas, for INSERT actions, the Programmer
    must learn to INSERT arbitrary words.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | INSERT | DELETE | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MT | 33.64 | 83.73 | 62.57 |'
  prefs: []
  type: TYPE_TB
- en: '| NLG | 24.52 | 60.48 | 44.90 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: The F1 scores of comparing the predicted actions with the ORACLE actions
    in the GEN test set.'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Comparing GPT-4 and GPT-3.5 as Interpreters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| LF-to-Text (AMR to English) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | BLEU | BERT | ChrF++ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo-16k | 11.43 | 89.30 | 45.44 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-turbo | 11.72 | 89.36 | 45.58 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: LF-to-Text results of Prog-Refine (Zero-shot Act.) in zero-shot setting
    with different LLMs as Interpreters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [9](#A1.T9 "Table 9 ‣ A.6 Comparing GPT-4 and GPT-3.5 as Interpreters
    ‣ Appendix A Appendix ‣ Improving Cross-Domain Low-Resource Text Generation through
    LLM Post-Editing: A Programmer-Interpreter Approach") illustrates the performance
    differences in the LF-to-Text task when using GPT-4 and GPT-3.5 as Interpreters
    for Prog-Refine (Zero-shot Act.). While GPT-4 offers a slight performance boost,
    the improvement is not substantial, amounting to only a 0.3 increase in BLEU score.
    Moreover, this comes at a higher cost of 0.06 per 1000 characters, compared to
    0.0015 for GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
