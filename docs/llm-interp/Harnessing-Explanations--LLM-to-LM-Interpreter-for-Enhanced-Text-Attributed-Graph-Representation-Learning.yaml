- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 17:35:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19523](https://ar5iv.labs.arxiv.org/html/2305.19523)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xiaoxin He¹   Xavier Bresson¹   Thomas Laurent²   Adam Perold³   Yann LeCun^(4,5)
      Bryan Hooi¹
  prefs: []
  type: TYPE_NORMAL
- en: '{xiaoxin, xaviercs, bhooi}@comp.nus.edu.sg, tlaurent@lmu.edu'
  prefs: []
  type: TYPE_NORMAL
- en: research@provenance.ai, yann@cs.nyu.edu
  prefs: []
  type: TYPE_NORMAL
- en: ¹National University of Singapore    ²Loyola Marymount University    ³Provenance
    AI
  prefs: []
  type: TYPE_NORMAL
- en: ⁴New York University    ⁵Meta AI
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Representation learning on text-attributed graphs (TAGs) has become a critical
    research problem in recent years. A typical example of a TAG is a paper citation
    graph, where the text of each paper serves as node attributes. Initial graph neural
    network (GNN) pipelines handled these text attributes by transforming them into
    shallow or hand-crafted features, such as skip-gram or bag-of-words features.
    Recent efforts have focused on enhancing these pipelines with language models
    (LMs), which typically demand intricate designs and substantial computational
    resources. With the advent of powerful large language models (LLMs) such as GPT
    or Llama2, which demonstrate an ability to reason and to utilize general knowledge,
    there is a growing need for techniques which combine the textual modelling abilities
    of LLMs with the structural learning capabilities of GNNs. Hence, in this work,
    we focus on leveraging LLMs to capture textual information as features, which
    can be used to boost GNN performance on downstream tasks. A key innovation is
    our use of *explanations as features*: we prompt an LLM to perform zero-shot classification,
    request textual explanations for its decision-making process, and design an *LLM-to-LM
    interpreter* to translate these explanations into informative features that enhance
    downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art
    results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv,
    as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method
    significantly speeds up training, achieving a 2.88 times improvement over the
    closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed
    method extends beyond TAGs and holds the potential to enhance other tasks involving
    graph-text data ¹¹1Our codes and datasets are available at: [https://github.com/XiaoxinHe/TAPE](https://github.com/XiaoxinHe/TAPE).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many real-world graphs possess textual information, and are often referred to
    text-attributed graphs [[37](#bib.bibx37)]. In TAGs, nodes typically represent
    text entities, such as documents or sentences, while edges signify relationships
    between these entities. For example, the ogbn-arxiv dataset [[13](#bib.bibx13)]
    represents a citation network in TAG form, where each node corresponds to a paper,
    with its title and abstract serving as node attributes. More generally, the combination
    of textual attributes with graph topology provides a rich source of information,
    significantly enhancing representation learning for important applications, such
    as text classification [[36](#bib.bibx36), [34](#bib.bibx34), [40](#bib.bibx40),
    [3](#bib.bibx3), [45](#bib.bibx45)], recommendation systems [[48](#bib.bibx48)],
    social networks, and fake news detection [[20](#bib.bibx20)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6db2689fbd5fc8086c3a2bf7da432030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of our framework leveraging large language models (LLMs)
    to enhance representation learning on TAGs. First, the text attributes associated
    with each node, *i.e.,* title and abstract, are wrapped in a custom prompt (green
    box) and used to query the LLM, here GPT-3.5 [[1](#bib.bibx1)], which generates
    a ranked prediction list and explanation (yellow box). Next, the original text,
    predictions, and explanation are used to fine-tune an language model (LM), here
    DeBERTa [[12](#bib.bibx12)], then transformed into vectorial node features. Finally,
    these enriched node features, *i.e.,* $h_{\textrm{orig}}$, $h_{\textrm{expl}}$
    and $h_{\textrm{pred}}$, are used in any downstream GNN, *e.g.,* RevGAT [[17](#bib.bibx17)]
    to predict unknown node classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Representation learning on TAGs. Prior research has explored various approaches
    for representation learning on TAGs. The standard GNN pipeline (illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning") in light
    yellow), first encodes the textual attributes of each node using shallow or hand-crafted
    features such as skip-gram [[22](#bib.bibx22)] or bag-of-words (BoW) [[11](#bib.bibx11)]
    (refer to Table [5](#A3.T5 "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature
    Extraction ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")). The resulting node
    features are then used as input for a GNN. For instance, the Open Graph Benchmark
    (OGB) [[13](#bib.bibx13)] generated BoW and skip-gram [[22](#bib.bibx22)] features
    for the ogbn-products and ogbn-arxiv datasets respectively. These processed features
    are readily available within popular graph libraries, such as PyTorch Geometric
    (PyG) [[8](#bib.bibx8)] and Deep Graph Library (DGL) [[33](#bib.bibx33)], and
    have been widely used by the graph community. However, these shallow text embeddings
    are limited in the complexity of the semantic features they can capture, especially
    when compared to approaches based on multi-layer LMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LM-based pipeline for TAGs. Recent works have therefore focused on designing
    LM-based techniques to better capture the context and nuances of text within TAGs [[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]. In this approach, pre-trained LMs are fine-tuned
    and used to generate node embeddings that are tailored to the specific TAG tasks
    (depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    in light gray). For example, [[3](#bib.bibx3)] fine-tuned an LM using a neighborhood
    prediction task, while [[45](#bib.bibx45)] fine-tuned an LM to predict the label
    distribution from a GNN’s outputs. LM-based models have achieved state-of-the-art
    (SOTA) results in node classification on ogbn-arxiv and ogbn-products [[45](#bib.bibx45)].
    However, these works typically entail intricate designs and demand substantial
    computational resources. Furthermore, for scalability reasons, existing works
    mostly rely on relatively small LMs, such as BERT [[5](#bib.bibx5)] and DeBERTa [[12](#bib.bibx12)],
    and thus lack the complex reasoning abilities associated with larger language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models. The advent of large pre-trained models, exemplified by
    GPT [[1](#bib.bibx1)], has revolutionized the field of language modeling. LLMs
    have notably enhanced performance across various natural language processing (NLP)
    tasks, and enabled sophisticated language processing capabilities such as complex
    and zero-shot reasoning. Furthermore, scaling laws [[15](#bib.bibx15)] have revealed
    predictable rules for performance improvements with model and training data size.
    Additionally, LLMs have exhibited “emergent abilities” that were not explicitly
    trained for, such as arithmetic, multi-step reasoning and instruction following [[35](#bib.bibx35)].
    While LLMs have found new success in domains like computer vision [[28](#bib.bibx28)],
    their potential benefits when applied to TAG tasks remain largely uncharted. This
    presents an exciting and promising avenue for future research, and it is precisely
    this untapped potential that we aim to explore in this work.
  prefs: []
  type: TYPE_NORMAL
- en: LMs vs. LLMs. In this paper, we make a clear distinction between “LMs” and “LLMs”.
    We use LMs to refer to relatively small language models that can be trained and
    fine-tuned within the constraints of an academic lab budget. We refer to LLMs
    as very large language models that are capable of learning significantly more
    complex linguistic patterns than LMs, such as GPT-3/4\. These models typically
    have tens or hundreds of billions of parameters and require substantial computational
    resources to train and use, *e.g.,* GPT-3 was trained on a supercomputer with
    10,000 GPUs. The size and complexity of recent LLMs have raised concerns about
    their scalability, as they can be too large even to run inference on the machines
    typically available within academic research labs. To address this issue, LLMs
    are often made accessible through language modeling as a service (LMaaS) [[26](#bib.bibx26)].
    This approach enables developers to harness the power of LLMs without necessitating
    extensive computational resources or specialized expertise. In the context of
    this paper, one of our primary objectives is to extract information from an LLM
    in a LMaaS-compatible manner. As a result, we do not require fine-tuning the LLM
    or extracting its logits; rather, we focus solely on obtaining its output in textual
    form. In contrast, existing LM-based techniques [[3](#bib.bibx3), [45](#bib.bibx45),
    [6](#bib.bibx6)] are not directly compatible with LLMs, as they require fine-tuning
    of LMs, as well as accessing their latent embeddings or logits, which GPT-3/4
    do not provide. Consequently, to the best of our knowledge, the use of LLMs in
    TAG tasks remains an unexplored area.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary study. To assess the potential of LLMs in enhancing representation
    learning for TAGs, we conducted an initial investigation into leveraging GPT-3.5
    for zero-shot classification on the ogbn-arxiv dataset. Using task-specific prompts
    consisting of paper titles, abstracts, and questions, GPT-3.5 achieved a promising
    accuracy of 73.5%, along with high-quality text explanations, surpassing several
    fully trained GNN baselines like RevGAT [[17](#bib.bibx17)] with OGB features
    (70.8% accuracy), but falling short of the SOTA accuracy of 76.6% [[45](#bib.bibx45)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The present work: LLM augmentation using explanations. We introduce a novel
    framework that leverages LLMs to improve representation learning on TAGs. A key
    innovation is the concept of *explanations as features*. By prompting a powerful
    LLM to explain its predictions, we extract its relevant prior knowledge and reasoning
    steps, making this information digestible for smaller models, akin to how human
    experts use explanations to convey insights. To illustrate this concept further,
    observe in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    that the explanations (in the yellow box) highlight and expand upon key crucial
    information from the text, such as “deep learning techniques such as DeconvNet,”
    and the relationship between text recognition and information retrieval. These
    explanations draw from the LLM’s general knowledge and serve as valuable features
    for enhancing subsequent TAG pipeline phases. In practice, we design a tailored
    prompt to query an LLM such as GPT or Llama2 to generate both a *ranked prediction
    list* and a *textual explanation* for its predictions. These predictions and explanations
    are then transformed into informative node features through fine-tuning a smaller
    LM such as DeBERTa [[12](#bib.bibx12)] for the target task, providing tailored
    features for any downstream GNNs. This smaller model acts as an interpreter, facilitating
    seamless communication between the LLM (handling text) and the GNN (managing vectorial
    representation).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54a92bac8996e0607d154875cb41ab34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The performance trade-off between node classification accuracy and
    total training time on ogbn-arxiv [[13](#bib.bibx13)] for various training approaches
    that combine language models (LMs) and graph neural networks (GNNs). The experiment
    employs DeBERTa-base [[12](#bib.bibx12)] as the LM backbone and RevGAT [[17](#bib.bibx17)]
    as the GNN backbone, with the size of the marker indicating the number of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Novel LMaaS-compatible approach. We propose the first LMaaS-compatible approach,
    to the best of our knowledge, for leveraging LLMs to enhance representation learning
    on TAGs. Our innovations involve extracting explanations from an LLM, here GPT-3.5
    and Llama2, and subsequently employing an LLM-to-LM interpreter to translate textual
    explanations into enriched node vector representations for downstream GNNs. Our
    approach improves modularity and efficiency compared to prior LM+GNN models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SOTA performance. Extensive experiments demonstrate that our method significantly
    boost the performance of various GNN models across diverse datasets. Notably,
    we achieve top-1 performance on ogbn-arxiv with significantly lower computation
    time, *i.e.,* $2.88\times$ faster than GLEM, and also excel in the TAG versions
    of PubMed and Cora datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data contribution. We provide open-source access to our codes, pre-trained networks
    and enriched features. Additionally, recognizing the absence of raw text data
    for Cora and PubMed in common repositories (*e.g.,* PyG, DGL), we have collected
    and released these datasets in TAG format. Furthermore, we introduce the new tape-arxiv23
    citation graph dataset, extending beyond GPT-3’s knowledge cutoff, *i.e.,* Sept.
    2021\. These datasets can serve as valuable resources for the NLP and GNN research
    community.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Shallow embedding pipeline for TAGs. In the context of learning representations
    on TAGs, a common approach involves combining graph-based learning with language
    modeling techniques. One prevalent strategy is to transform text attributes into
    shallow or hand-crafted features, such as skip-gram [[22](#bib.bibx22)] or BoW [[11](#bib.bibx11)]
    features. Detailed information is available in Table [5](#A3.T5 "Table 5 ‣ C.3
    Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). These engineered features can then be fed as inputs to a graph-based
    learning algorithm, such as a graph convolutional network (GCN) [[16](#bib.bibx16)],
    which learns embeddings capturing the graph structure while incorporating the
    extracted text features. Shallow embedding methods are widely used in the graph
    community due to their simplicity and computational efficiency, such as for designing
    GNN architectures [[30](#bib.bibx30), [2](#bib.bibx2), [29](#bib.bibx29), [43](#bib.bibx43)]
    or benchmarking graph learning [[38](#bib.bibx38), [13](#bib.bibx13)]. However,
    they may have limitations in capturing complex semantic relationships and fully
    leveraging the richness of text attributes, particularly in scenarios involving
    intricate semantic relationships and contextual information.'
  prefs: []
  type: TYPE_NORMAL
- en: LM-based pipeline for TAGs. To overcome the limitations of shallow embedding
    approaches, researchers have explored deep embedding techniques by fine-tuning
    pre-trained LMs, such as BERT [[5](#bib.bibx5)], to generate node embeddings that
    are specifically adapted to the domain and context of the TAGs. These deep embeddings
    effectively capture the semantic richness of text attributes, leading to improved
    performance on various TAG-related tasks. Integrating LM-based embeddings and
    graph-based learning can be done through different approaches. One approach is
    to use a cascaded architecture, where the node features are first encoded independently
    by the LMs, and then fed into GNN models. This representation paradigm has been
    widely adopted in subsequent works, such as TextGNN [[48](#bib.bibx48)], GIANT [[3](#bib.bibx3)],
    GPT-GNN [[14](#bib.bibx14)], SimTeg [[7](#bib.bibx7)], as well as in studies related
    to knowledge graphs [[39](#bib.bibx39), [44](#bib.bibx44)] and fact verification [[20](#bib.bibx20),
    [47](#bib.bibx47)] that are beyond the scope of this work. An alternative approach
    involves fusing text encoding and graph aggregation into an iterative workflow,
    enabling the model to refine both the text representations and the node embeddings
    simultaneously, such as Graphormer [[37](#bib.bibx37)], DRAGON [[41](#bib.bibx41)],
    and GLEM [[45](#bib.bibx45)], to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based pipeline for TAGs. Incorporating LLMs into TAG tasks presents a promising
    frontier. LLMs such as ChatGPT [[1](#bib.bibx1)] by OpenAI, PaLM [[4](#bib.bibx4)]
    by Google, and LLaMA [[27](#bib.bibx27)] by Meta, have demonstrated their effectiveness
    across a spectrum of NLP tasks. However, their potential benefits for TAG tasks
    have yet to be fully explored. While some recent research efforts have sought
    to evaluate the capacity of LLMs in understanding graph-structured data and enhance
    their graph processing capabilities [[31](#bib.bibx31), [42](#bib.bibx42), [9](#bib.bibx9)],
    these endeavors, while valuable, may not be directly aligned with our specific
    focus on TAGs. By exploring LLM-based methods designed specifically for TAGs,
    we can unlock new possibilities for improving TAG prediction performance and advancing
    our understanding of text attributes within graph-based data. Notably, our initial
    attempt has already inspired further research endeavors in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Formalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce notation and formalize some concepts related to
    language models, large language models, and graph neural networks for node classification
    on TAGs.
  prefs: []
  type: TYPE_NORMAL
- en: Text-attributed graphs. Formally, a TAG can be represented as $\mathcal{G}=(\mathcal{V},A,\{s_{n}\}_{n\in\mathcal{V}})$,
    where $\mathcal{V}$ is a set of $N$ nodes, $A\in\mathbb{R}^{N\times N}$ is the
    adjacency matrix, and $s_{n}\in\mathcal{D}^{L_{n}}$ is a sequential text associated
    with node $n\in\mathcal{V}$, with $\mathcal{D}$ as the words or tokens dictionary,
    and $L_{n}$ as the sequence length. In this paper, we investigate node classification
    on TAGs. Specifically, given some labeled nodes $\mathcal{L}\subset\mathcal{V}$
    , the goal is to predict the labels of the remaining unlabeled nodes $\mathcal{U}=\mathcal{V}\setminus\mathcal{L}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Language models for text classification. In the context of TAGs, LMs can be
    employed to encode the text attributes associated with each node and learn a representation
    that captures the semantic meaning of the text. Let $s_{n}\in\mathcal{D}^{L_{n}}$
    denote the text attributes of node $n$, and LM be a pre-trained network, such
    as BERT [[5](#bib.bibx5)] or DeBERTa [[12](#bib.bibx12)]. Then, the text attributes
    of node $n$ can be encoded by applying the LM to $s_{n}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h_{n}=\textrm{LM}(s_{n})\in\mathbb{R}^{d},$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $h_{n}$ is the output of the LM, and $d$ is the dimension of the output
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: To perform node classification, the output is employed as input to a classifier,
    such as a logistic regression or a neural network. The goal is to learn a function
    that maps the encoded text attributes to the corresponding node labels.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models and prompting. LLMs have introduced a new paradigm for
    task-adaptation known as “pre-train, prompt, and predict”, replacing the traditional
    “pre-train, fine-tune” procedure. In this paradigm, the LLM is first pre-trained
    on a large corpus of text data to learn general language representations. Then,
    rather than fine-tuning the model on task-specific labeled data, the model is
    prompted with a natural language prompt that specifies the task and context, and
    the model generates the output directly based on the prompt and the input [[19](#bib.bibx19)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt can take various forms, such as a single sentence or a longer passage,
    and can include additional information or constraints to guide the model’s behavior.
    Let $\mathcal{M}$ be an LLM that takes as input a sequence of tokens $x=(x_{1},x_{2},\ldots,x_{q})$
    and produces as output a sequence of tokens $y=(y_{1},y_{2},\ldots,y_{m})$. The
    model $\mathcal{M}$ is typically trained to optimize a conditional probability
    distribution $p(y|x)$, which assigns a probability to each possible output sequence
    $y$ given $x$. To include a prompt $p$ with the input sequence $x$, we can concatenate
    them into a new sequence $\hat{x}=(p,x_{1},x_{2},\ldots,x_{q})$. We then use $\hat{x}$
    to compute the conditional probability distribution $p(y|\hat{x})$. Formally,
    the probability of the output sequence $y$ given $\hat{x}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(y&#124;\hat{x})=\prod_{i=1}^{m}p(y_{i}&#124;y_{<i},\hat{x}),$ |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: where $y_{<i}$ represents the prefix of sequence $y$ up to position $i-1$, and
    $p(y_{i}|y_{<i},\hat{x})$ represents the probability of generating token $y_{i}$
    given $y_{<i}$ and $\hat{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph neural networks for node classification. In node classification, the
    task is to label each node in a graph based on its attributes and connections
    with other nodes. GNNs operate by aggregating information from a node’s neighbors,
    then updating the node’s representation based on the aggregated information. Formally,
    the $k$-th layer of a GNN is designed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h_{i}^{k}=f^{k}(h_{i}^{k-1},\,\textrm{AGG}(\{h_{j}^{k-1}:j\in\mathcal{N}_{i}\}))\in\mathbb{R}^{d},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $h_{i}^{k}\in\mathbb{R}^{d}$ is the representation of node $i$ at layer
    $k$ and $\mathcal{N}_{i}\subseteq\mathcal{V}$ is the set of neighbors of node
    $i$. Function $f^{k}$ is a differentiable function that updates the representation
    of a node based on its previous-layer representation and the aggregated information
    from its neighbors. This function is typically implemented as a neural network
    layer (*e.g.,* a multi-layer perceptron, or an attention mechanism). AGG is also
    a differentiable function (*e.g.,* sum, mean, etc.) that aggregates the representations
    of a node’s neighbors to produce a summary vector. The final representation is
    fed into a fully connected layer and a softmax function for class prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Proposed Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we describe our LLM-based pipeline designed for node classification
    on TAGs. As illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"), the key idea is to leverage the LLM’s explanations as informative
    features for a downstream GNN. To achieve this goal, our method involves three
    main steps: 1) LLM-based prediction and explanation generation, 2) fine-tuning
    an LM interpreter, and 3) training a GNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Generating Predictions and Explanations with LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As outlined in the introduction, our approach is designed to be *LMaaS-compatible*
    given the scale of LLMs. This means that we aim to operate solely through API
    access to an LLM, using text-based input and output, without requiring fine-tuning
    the LLM or accessing its embeddings or logits.
  prefs: []
  type: TYPE_NORMAL
- en: 'In lieu of these requirements, our approach focuses on querying the LLM in
    an “open-ended” manner, *i.e.,* instructing the LLM to make multiple predictions
    and provide explanations for its decisions. By doing so, we aim to effectively
    extract its reasoning abilities and general knowledge in text format. These text-based
    outputs are then processed using an *LLM-to-LM interpreter* to create informative
    node features for downstream GNNs. With this objective, for each paper node $i\in\mathcal{V}$,
    we generate a prompt that includes the title and abstract of the paper, along
    with an open-ended question about the paper’s topic. The specific phrasing of
    the question part of the prompt is tailored to the task and dataset, as shown
    in Table [7](#A4.T7 "Table 7 ‣ D.3 Prompt Design ‣ Appendix D Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). The general structure of the prompt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg   height="103.43" overflow="visible" version="1.1" width="600"><g transform="translate(0,103.43)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="75.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Abstract: [paper abstract] Title: [paper title] Question: [ask
    the model to predict one or more class labels of the paper, ordered from most
    to least likely, and provide explanations for its predictions] Answer:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Querying the LLM results in a ranked prediction list and a textual explanation
    for each paper:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg   height="58" overflow="visible" version="1.1" width="600"><g transform="translate(0,58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">(Ranked Predictions) [a ranked prediction list] (Explanations)
    [model-generated explanation for the predictions]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: These predictions and explanations serve as supplementary text attributes for
    the downstream LMs and GNN models, as detailed in the subsequent section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Fine-Tuning LM Interpreter and Node Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Original text and explanation features. Our initial step involves converting
    both the original text, *i.e.,* title and abstract, and the LLM’s explanations
    into fixed-length node features suitable for downstream GNN applications. Our
    approach is to fine-tune a smaller LM, which acts as an “interpreter” for the
    LLM’s text explanations. The rationale behind this step is that both the LLM and
    LM possess distinct advantages: the LLM has greater power and more knowledge but
    is less flexible, while the LM has less skills but is compact enough to be fine-tuned
    to a specific task. Thus, the LM serves to interpret the LLM’s output for the
    GNN, with the text explanation acting as an effective intermediate medium for
    communication. Then, fine-tuning the LM enables it to extract the most valuable
    and task-relevant features from the explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, we first fine-tune pre-trained LMs as follows: let $\textrm{LM}_{\textrm{orig}}$
    and $\textrm{LM}_{\textrm{expl}}$ be pre-trained LMs that take as input the original
    $s^{\textrm{orig}}$ and the explanation $s^{\textrm{expl}}$ text sequences, respectively.
    We obtain text embeddings for each source as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}h_{\textrm{orig}}=\textrm{LM}_{\textrm{orig}}(s^{\textrm{orig}})\in\mathbb{R}^{N\times
    d},\quad h_{\textrm{expl}}=\textrm{LM}_{\textrm{expl}}(s^{\textrm{expl}})\in\mathbb{R}^{N\times
    d}.\end{split}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'We further apply a Multi-Layer Perceptron (MLP) to the output of the LMs to
    obtain a $N\times C$-dimensional prediction matrix representing the LM’s predictions
    for each node (in logits):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}y_{\textrm{orig}}=\textrm{MLP}_{\textrm{orig}}(h_{\textrm{orig}})\in\mathbb{R}^{N\times
    C},\quad y_{\textrm{expl}}=\textrm{MLP}_{\textrm{expl}}(h_{\textrm{expl}})\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: We fine-tune these LMs and MLPs using cross-entropy loss. Finally, the text
    embeddings from both sources, $h_{\textrm{orig}}$ and $h_{\textrm{expl}}$, are
    used as enriched features for training downstream GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Ranked prediction features. In addition to the explanations, the LLM also provides
    a top-$k$ ranked prediction list for each node, which adds valuable information.
    To incorporate this knowledge, the top-$k$ predictions for node $i$ are first
    one-hot encoded as vectors $p_{i,1},\dots,p_{i,k}\in\mathbb{R}^{C}$. These vectors
    are subsequently concatenated into a $kC$-dimensional vector, followed by a linear
    transformation to produce a fixed-sized vector of length $d_{P}$. This process
    produces a prediction feature matrix as $h_{\textrm{pred}}\in\mathbb{R}^{N\times
    d_{P}}$ across all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we denote our features as $h_{\textrm{TAPE}}=\{h_{\textrm{orig}},h_{\textrm{expl}},h_{\textrm{pred}}\}$,
    where “TAPE” stands for Title, Abstract, Prediction and Explanation for each node.
    Importantly, our framework requires these features to remain frozen during downstream
    GNN training, ensuring that the LM and LLM do not participate in the GNN training
    process. This characteristic significantly enhances ease-of-use, modularity, and
    efficiency compared to approaches like GLEM, which involve an expensive iterative
    LM-GNN training process. As a result, we achieve a substantial speedup over GLEM,
    *e.g.,* a $2.88\times$ speedup on ogbn-arxiv even when utilizing the same backbone
    LM and GNN.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 GNN Training on Enriched Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our final step is to train a GNN using the $h_{\textrm{TAPE}}$ features. We
    aim to achieve this without increasing the memory requirements of the GNN or making
    any changes to its architecture. To accomplish this, we use an ensemble approach,
    as a simple and effective way of combining the features. Specifically, we independently
    train GNN models $f_{\textrm{orig}}$, $f_{\textrm{expl}}$, and $f_{\textrm{pred}}$
    on the features $h_{\textrm{orig}}$, $h_{\textrm{expl}}$, and $h_{\textrm{pred}}$,
    respectively, to predict the ground truth node labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\hat{y}_{\textrm{orig}/\textrm{expl}/\textrm{pred}}=f_{\textrm{orig}/\textrm{expl}/\textrm{pred}}(h_{\textrm{orig}/\textrm{expl}/\textrm{pred}},A)\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'We then fuse these predictions by taking their average:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}=\textrm{mean}(\hat{y}_{\textrm{orig}},\hat{y}_{\textrm{expl}},\hat{y}_{\textrm{pred}})\in\mathbb{R}^{N\times
    C}.$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Each of the three models performs well individually as shown in Table [3](#S5.T3
    "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning"), which
    validates the effectiveness of simple averaging. This strategy enables us to capture
    complementary information from diverse input sources, ultimately enhancing the
    overall model’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we aim to demonstrate that explanations generated by an LLM
    can be valuable features for a smaller LM. Specifically, the explanations $E$
    are helpful if they possess *fidelity* in describing the LLM’s reasoning; and
    the LLM is *non-redundant*, utilizing information not used by the smaller LM.
    Let $E$ be the textual explanations generated by an LLM; $Z_{L}$ and $Z$ are embeddings
    from the LLM and smaller LM respectively, $y$ is the target and $H(\cdot|\cdot)$
    is the conditional entropy. The detailed proof is in Appendix [A](#A1 "Appendix
    A Theoretical Analysis ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced
    Text-Attributed Graph Representation Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Theorem. Given the following conditions 1) *Fidelity*: $E$ is a good proxy
    for $Z_{L}$ such that $H(Z_{l}|E)=\epsilon$, with $\epsilon></math>, 2) *Non-redundancy*:
    <math   alttext=$ contains information not present in $Z$, expressed as $H(y|Z,Z_{L})=H(y|Z)-\epsilon^{\prime}$,
    with $\epsilon^{\prime}></math>. Then it follows that <math   alttext=$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We evaluate the proposed TAPE technique across five TAG datasets: Cora [[21](#bib.bibx21)],
    PubMed [[23](#bib.bibx23)], ogbn-arxiv, ogbn-products [[13](#bib.bibx13)], and
    tape-arxiv23. For Cora and PubMed, raw text data of the articles is unavailable
    in common graph libraries such as PyG and DGL. Hence, we collected and formatted
    the missing text data for these datasets in TAG format. Additionally, given the
    popularity of these datasets, their TAG version will be released publicly for
    reproducibility and new research projects. For ogbn-products, given its substantial
    scale of 2 million nodes and 61 million edges and considering our academic resource
    budget, we conducted experiments on a subgraph sample. Details can be found in
    Appendix [C](#A3 "Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Node classification accuracy for the Cora, PubMed, ogbn-arxiv, ogbn-products
    and tape-arxiv23 datasets. $G\uparrow$ denotes the improvements of our approach
    over the same GNN trained on shallow features $h_{\textrm{shallow}}$; $L\uparrow$
    denotes the improvements of our approach over LM${}_{\textrm{finetune}}$. The
    results are averaged over four runs with different seeds, and the best results
    are in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | GNN | LM | Ours |'
  prefs: []
  type: TYPE_TB
- en: '| $h_{\textrm{shallow}}$ | $h_{\textrm{GIANT}}$ | $G\uparrow$ | LLM | LM${}_{\textrm{finetune}}$
    | $L\uparrow$ | $h_{\textrm{TAPE}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Cora | MLP | 0.6388 ± 0.0213 | – | 26.58% | 0.6769 | 0.7606 ± 0.0378 | 6.31%
    | 0.8086 ± 0.0190 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.8653 ± 0.0139 | – | 1.39% | 0.6769 | 0.7606 ± 0.0378 | 15.34% | 0.8773
    ± 0.0063 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.8727 ± 0.0172 | – | 0.74% | 0.6769 | 0.7606 ± 0.0378 | 15.59% |
    0.8792 ± 0.0107 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.8736 ± 0.0208 | – | 2.22% | 0.6769 | 0.7606 ± 0.0378 | 17.41%
    | 0.8930 ± 0.0072 |'
  prefs: []
  type: TYPE_TB
- en: '| PubMed | MLP | 0.8635 ± 0.0032 | – | 9.70% | 0.9342 | 0.9494 ± 0.0046 | -0.22%
    | 0.9473 ± 0.0040 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.8533 ± 0.0101 | – | 10.07% | 0.9342 | 0.9494 ± 0.0046 | -1.07% |
    0.9392 ± 0.0023 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.8688 ± 0.0086 | – | 9.69% | 0.9342 | 0.9494 ± 0.0046 | 0.38% | 0.9530
    ± 0.0035 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.8465 ± 0.0808 | – | 12.53% | 0.9342 | 0.9494 ± 0.0046 | 0.34%
    | 0.9526 ± 0.0032 |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | MLP | 0.5336 ± 0.0038 | 0.7308 ± 0.0006 | 42.19% | 0.7350 |
    0.7361 ± 0.0004 | 3.07% | 0.7587 ± 0.0015 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.7182 ± 0.0027 | 0.7329 ± 0.0010 | 4.71% | 0.7350 | 0.7361 ± 0.0004
    | 2.16% | 0.7520 ± 0.0003 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.7171 ± 0.0017 | 0.7435 ± 0.0014 | 6.98% | 0.7350 | 0.7361 ± 0.0004
    | 4.22% | 0.7672 ± 0.0007 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.7083 ± 0.0017 | 0.7590 ± 0.0019 | 9.42% | 0.7350 | 0.7361 ±
    0.0004 | 5.28% | 0.7750 ± 0.0012 |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-products | MLP | 0.5385 ± 0.0017 | – | 46.3% | 0.7440 | 0.7297 ± 0.0023
    | 7.96% | 0.7878 ± 0.0082 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.7052 ± 0.0051 | – | 13.39% | 0.7440 | 0.7297 ± 0.0023 | 9.58% | 0.7996
    ± 0.0041 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.6913 ± 0.0026 | – | 17.71% | 0.7440 | 0.7297 ± 0.0023 | 11.51% |
    0.8137 ± 0.0043 |'
  prefs: []
  type: TYPE_TB
- en: '| RevGAT | 0.6964 ± 0.0017 | – | 18.24% | 0.7440 | 0.7297 ± 0.0023 | 12.84%
    | 0.8234 ± 0.0036 |'
  prefs: []
  type: TYPE_TB
- en: '| tape-arxiv23 | MLP | 0.6209 ± 0.0031 | – | 28.83% | 0.7356 | 0.7358 ± 0.0006
    | 8.71% | 0.7999 ± 0.0037 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.6521 ± 0.0099 | – | 20.03% | 0.7356 | 0.7358 ± 0.0006 | 6.37% | 0.7827
    ± 0.0037 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.6571 ± 0.0042 | – | 22.05% | 0.7356 | 0.7358 ± 0.0006 | 9.00% |
    0.8020 ± 0.0024 |'
  prefs: []
  type: TYPE_TB
- en: '| RevGAT | 0.6958 ± 0.0032 | – | 16.04% | 0.7356 | 0.7358 ± 0.0006 | 9.73%
    | 0.8074 ± 0.0021 |'
  prefs: []
  type: TYPE_TB
- en: 'We conduct a comprehensive evaluation of our proposed TAPE method by comparing
    with existing GNN- and LM-based methods, with the results summarized in Table [1](#S5.T1
    "Table 1 ‣ 5.1 Main Results ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning"). For
    GNN comparisons, we consider three widely utilized architectures: GCN [[16](#bib.bibx16)],
    GraphSAGE [[25](#bib.bibx25)], and RevGAT [[17](#bib.bibx17)] along with a basic
    MLP baseline that operates independently off graph-related information. We explore
    three types of node features: 1) shallow features (detailed in Table [5](#A3.T5
    "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature Extraction ‣ Appendix
    C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")), denoted as $h_{\textrm{shallow}}$, 2) GIANT
    features [[3](#bib.bibx3)] $h_{\textrm{GIANT}}$, and 3) our proposed features
    $h_{\textrm{TAPE}}$, comprising $h_{\textrm{orig}}$, $h_{\textrm{expl}}$, and
    $h_{\textrm{pred}}$. For LM-based methods, we investigate two approaches: 1) fine-tuning
    DeBERTa on labeled nodes, denoted as $\textrm{LM}_{\textrm{finetune}}$, and 2)
    using zero-shot ChatGPT (gpt-3.5-turbo) with the same prompts as our approach,
    denoted as LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Our approach consistently outperforms other methods on all datasets and across
    all models, demonstrating its effectiveness in enhancing TAG representation learning.
    Among GNN-based methods, shallow features (*i.e.,* $h_{\textrm{shallow}}$) yields
    subpar performance, while LM-based features (*i.e.,* $h_{\textrm{GIANT}}$) improves
    results. In the case of LMs, fine-tuned LMs (*i.e.,* $\textrm{LM}_{\textrm{finetune}}$)
    also perform well. Our proposed novel features, leveraging the power of the LLM,
    further enhance the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we expanded our experimentation to include the open-source Llama2 [[27](#bib.bibx27)],
    demonstrating the feasibility of a cost-effective (free) alternative, as shown
    in Table [9](#A4.T9 "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). Furthermore, to address the potential label leakage
    concern in LLM, we took the initiative to construct a novel dataset, namely tape-arxiv23,
    comprising papers published in 2023 or later – well beyond the knowledge cutoff
    for GPT-3.5\. The results clearly illustrate strong generalization capabilities:
    while the LLM achieves 73.56% accuracy, our approach outperforms it with 84.23%.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our proposed method surpasses not only pure LMs and shallow embedding pipelines
    but also the LM-based pipelines on the ogbn-arxiv dataset, achieving a superior
    balance between accuracy and training time, as illustrated in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning"). Specifically, our method
    achieved significantly higher accuracy than the SOTA GLEM [[45](#bib.bibx45)]
    method while utilizing the same LM and GNN models. Furthermore, our approach requires
    only $2.88\times$ less computation time. These efficiency improvements are attributed
    to our decoupled training approach for LMs and GNNs, avoiding the iterative (*i.e.,*
    multi-stage) approach used in GLEM. Moreover, unlike the iterative approach, our
    model allows for parallelizing the training of LM${}_{\textrm{orig}}$ and LM${}_{\textrm{expl}}$,
    further reducing overall training time when performed simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Experiments on ogbn-arxiv dataset with DeBERTa-base [[12](#bib.bibx12)]
    as LM backbone and RevGAT [[17](#bib.bibx17)] as GNN backbone for comparison of
    different training paradigms of fusing LMs and GNNs, including our proposed method
    and the state-of-the-art GLEM method [[45](#bib.bibx45)]. The validation and test
    accuracy, number of parameters, maximum batch size (Max bsz.), and total training
    time on 4 NVIDIA RTX A5000 24GB GPUs are reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Val acc. | Test acc. | Params. | Max bsz. | Total time |'
  prefs: []
  type: TYPE_TB
- en: '| LM[orig] | 0.7503 ± 0.0008 | 0.7361 ± 0.0004 | 139,223,080 | 36 | 1.73h |'
  prefs: []
  type: TYPE_TB
- en: '| GNN-$h_{\textrm{shallow}}$ | 0.7144 ± 0.0021 | 0.7083 ± 0.0017 | 427,728
    | all nodes | 1.80min |'
  prefs: []
  type: TYPE_TB
- en: '| GLEM-G-Step | 0.7761 ± 0.0005 | 0.7657 ± 0.0029 | 1,837,136 | all nodes |
    9.18h |'
  prefs: []
  type: TYPE_TB
- en: '| GLEM-L-Step | 0.7548 ± 0.0039 | 0.7495 ± 0.0037 | 138,632,488 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| TAPE-LM${}_{\textrm{orig}}$-Step | 0.7503 ± 0.0008 | 0.7361 ± 0.0004 | 139,223,080
    | 36 | 1.73h |'
  prefs: []
  type: TYPE_TB
- en: '| TAPE-LM${}_{\textrm{expl}}$-Step | 0.7506 ± 0.0008 | 0.7432 ± 0.0012 | 139,223,080
    | 36 | 1.40h |'
  prefs: []
  type: TYPE_TB
- en: '| TAPE-GNN-${h_{\textrm{TAPE}}}$-Step | 0.7785 ± 0.0016 | 0.7750 ± 0.0012 |
    1,837,136 | all nodes | 3.76min |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablation study on the ogbn-arxiv dataset, showing the effects of different
    node features on the performance. Node features include the original text attributes
    ($h_{\textrm{orig}}$), the explanations ($h_{\textrm{expl}}$ and predicted $h_{\textrm{pred}}$)
    generated by LLM, and the proposed method ($h_{\textrm{TAPE}}$). Results are averaged
    over 4 runs with 4 different seeds. The best results are in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | $h_{\textrm{orig}}$ | $h_{\textrm{expl}}$ | $h_{\textrm{pred}}$
    | $h_{\textrm{TAPE}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | val | 0.7624 ± 0.0007 | 0.7577 ± 0.0008 | 0.7531 ± 0.0006 | 0.7642
    ± 0.0003 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.7498 ± 0.0018 | 0.7460 ± 0.0013 | 0.7400 ± 0.0007 | 0.7520 ± 0.0003
    |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | val | 0.7594 ± 0.0012 | 0.7631 ± 0.0016 | 0.7612 ± 0.0010 | 0.7768
    ± 0.0016 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.7420 ± 0.0018 | 0.7535 ± 0.0023 | 0.7524 ± 0.0015 | 0.7672 ± 0.0007
    |'
  prefs: []
  type: TYPE_TB
- en: '| RevGAT | val | 0.7588 ± 0.0021 | 0.7568 ± 0.0027 | 0.7550 ± 0.0015 | 0.7785
    ± 0.0016 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.7504 ± 0.0020 | 0.7529 ± 0.0052 | 0.7519 ± 0.0031 | 0.7750 ± 0.0012
    |'
  prefs: []
  type: TYPE_TB
- en: 5.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We perform an ablation study on the ogbn-arxiv dataset [[13](#bib.bibx13)]
    to evaluate the relevance of each module within our framework. The results are
    summarized in Table [3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning") and Figure [3](#A4.F3 "Figure 3 ‣ D.4 Detailed Ablation Study ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). Across all methods and for both the validation
    and test sets, our proposed method consistently outperforms the other settings.
    This underscores the value of incorporating explanations and predictions into
    node embeddings. Our case study (Figure [4](#A4.F4 "Figure 4 ‣ D.6 Case Study
    ‣ Appendix D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning")) suggests this improvement
    can be attributed to the concise and focused nature of LLM-generated explanations,
    as well as their reasoning ability and utilization of external knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the increasing importance of integrating text and relationships, coupled
    with the emergence of LLMs, we foresee that TAG tasks will attract even more attention
    in the coming years. The convergence of LLMs and GNNs presents new opportunities
    for both research and industrial applications. As a pioneering work in this field,
    we believe that our contribution will serve as a strong baseline for future studies
    in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation and future work. An inherent limitation of our approach lies in the
    requirement for customized prompts for each dataset. Currently, we rely on manually
    crafted prompts, which may not be optimal for the node classification task for
    every dataset. The efficacy of these prompts may fluctuate depending on the specific
    characteristics of the dataset and the specific task at hand. Future work can
    focus on automating the prompt generation process, exploring alternative prompt
    designs, and addressing the challenges of dynamic and evolving TAGs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Xavier Bresson is supported by NUS Grant ID R-252-000-B97-133.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Tom Brown et al. “Language models are few-shot learners” In *Advances in
    neural information processing systems* 33, 2020, pp. 1877–1901'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Wei-Lin Chiang et al. “Cluster-gcn: An efficient algorithm for training
    deep and large graph convolutional networks” In *Proceedings of the 25th ACM SIGKDD
    international conference on knowledge discovery & data mining*, 2019, pp. 257–266'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Eli Chien et al. “Node feature extraction by self-supervised multi-scale
    neighborhood prediction” In *arXiv preprint arXiv:2111.00064*, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Aakanksha Chowdhery et al. “Palm: Scaling language modeling with pathways”
    In *arXiv preprint arXiv:2204.02311*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova “Bert:
    Pre-training of deep bidirectional transformers for language understanding” In
    *arXiv preprint arXiv:1810.04805*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Tu Anh Dinh, Jeroen den Boef, Joran Cornelisse and Paul Groth “E2EG: End-to-End
    Node Classification Using Graph Topology and Text-based Node Attributes” In *arXiv
    preprint arXiv:2208.04609*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Keyu Duan et al. “Simteg: A frustratingly simple approach improves textual
    graph learning” In *arXiv preprint arXiv:2308.02565*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Matthias Fey and Jan Eric Lenssen “Fast graph representation learning with
    PyTorch Geometric” In *arXiv preprint arXiv:1903.02428*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Jiayan Guo, Lun Du and Hengyu Liu “GPT4Graph: Can Large Language Models
    Understand Graph Structured Data? An Empirical Evaluation and Benchmarking” In
    *arXiv preprint arXiv:2305.15066*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Will Hamilton, Zhitao Ying and Jure Leskovec “Inductive representation
    learning on large graphs” In *Advances in neural information processing systems*
    30, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Zellig Harris “Distributional structure” In *The philosophy of linguistics*
    Oxford University Press, 1985'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Pengcheng He, Xiaodong Liu, Jianfeng Gao and Weizhu Chen “DEBERTA: DECODING-ENHANCED
    BERT WITH DISENTANGLED ATTENTION” In *International Conference on Learning Representations*,
    2021 URL: [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Weihua Hu et al. “Open graph benchmark: Datasets for machine learning
    on graphs” In *Advances in neural information processing systems* 33, 2020, pp.
    22118–22133'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Ziniu Hu et al. “Gpt-gnn: Generative pre-training of graph neural networks”
    In *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, 2020, pp. 1857–1867'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Jared Kaplan et al. “Scaling laws for neural language models” In *arXiv
    preprint arXiv:2001.08361*, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Thomas N Kipf and Max Welling “Semi-supervised classification with graph
    convolutional networks” In *arXiv preprint arXiv:1609.02907*, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Guohao Li, Matthias Müller, Bernard Ghanem and Vladlen Koltun “Training
    graph neural networks with 1000 layers” In *International conference on machine
    learning*, 2021, pp. 6437–6449 PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Stephanie Lin, Jacob Hilton and Owain Evans “Truthfulqa: Measuring how
    models mimic human falsehoods” In *arXiv preprint arXiv:2109.07958*, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Pengfei Liu et al. “Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing” In *ACM Computing Surveys*
    55.9 ACM New York, NY, 2023, pp. 1–35'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Zhenghao Liu, Chenyan Xiong, Maosong Sun and Zhiyuan Liu “Fine-grained
    fact verification with kernel graph attention network” In *arXiv preprint arXiv:1910.09796*,
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie and Kristie Seymore
    “Automating the construction of internet portals with machine learning” In *Information
    Retrieval* 3 Springer, 2000, pp. 127–163'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Tomas Mikolov et al. “Distributed representations of words and phrases
    and their compositionality” In *Advances in neural information processing systems*
    26, 2013'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Prithviraj Sen et al. “Collective classification in network data” In *AI
    magazine* 29.3, 2008, pp. 93–93'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Aarohi Srivastava et al. “Beyond the imitation game: Quantifying and extrapolating
    the capabilities of language models” In *arXiv preprint arXiv:2206.04615*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Chuxiong Sun, Hongming Gu and Jie Hu “Scalable and adaptive graph neural
    networks with self-label-enhanced training” In *arXiv preprint arXiv:2104.09376*,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Tianxiang Sun et al. “Black-box tuning for language-model-as-a-service”
    In *International Conference on Machine Learning*, 2022, pp. 20841–20855 PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Hugo Touvron et al. “Llama: Open and efficient foundation language models”
    In *arXiv preprint arXiv:2302.13971*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Maria Tsimpoukelli et al. “Multimodal few-shot learning with frozen language
    models” In *Advances in Neural Information Processing Systems* 34, 2021, pp. 200–212'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Petar Velickovic et al. “Deep graph infomax.” In *ICLR (Poster)* 2.3,
    2019, pp. 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Petar Veličković et al. “Graph attention networks” In *arXiv preprint
    arXiv:1710.10903*, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Heng Wang et al. “Can Language Models Solve Graph Problems in Natural
    Language?” In *arXiv preprint arXiv:2305.10037*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Kuansan Wang et al. “Microsoft academic graph: When experts are not enough”
    In *Quantitative Science Studies* 1.1 MIT Press One Rogers Street, Cambridge,
    MA 02142-1209, USA journals-info …, 2020, pp. 396–413'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Minjie Wang et al. “Deep graph library: A graph-centric, highly-performant
    package for graph neural networks” In *arXiv preprint arXiv:1909.01315*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Suhang Wang, Jiliang Tang, Charu Aggarwal and Huan Liu “Linked document
    embedding for classification” In *Proceedings of the 25th ACM international on
    conference on information and knowledge management*, 2016, pp. 115–124'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Jason Wei et al. “Emergent abilities of large language models” In *arXiv
    preprint arXiv:2206.07682*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Cheng Yang et al. “Network representation learning with rich text information.”
    In *IJCAI* 2015, 2015, pp. 2111–2117'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Junhan Yang et al. “GraphFormers: GNN-nested transformers for representation
    learning on textual graph” In *Advances in Neural Information Processing Systems*
    34, 2021, pp. 28798–28810'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Zhilin Yang, William Cohen and Ruslan Salakhudinov “Revisiting semi-supervised
    learning with graph embeddings” In *International conference on machine learning*,
    2016, pp. 40–48 PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Michihiro Yasunaga et al. “QA-GNN: Reasoning with language models and
    knowledge graphs for question answering” In *arXiv preprint arXiv:2104.06378*,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Michihiro Yasunaga et al. “Graph-based neural multi-document summarization”
    In *arXiv preprint arXiv:1706.06681*, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Michihiro Yasunaga et al. “Deep bidirectional language-knowledge graph
    pretraining” In *Advances in Neural Information Processing Systems* 35, 2022,
    pp. 37309–37323'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jiawei Zhang “Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT” In *arXiv preprint arXiv:2304.11116*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Shichang Zhang, Yozen Liu, Yizhou Sun and Neil Shah “Graph-less neural
    networks: Teaching old mlps new tricks via distillation” In *arXiv preprint arXiv:2110.08727*,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Xikun Zhang et al. “Greaselm: Graph reasoning enhanced language models”
    In *International conference on learning representations*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Jianan Zhao et al. “Learning on Large-scale Text-attributed Graphs via
    Variational Inference” In *arXiv preprint arXiv:2210.14709*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Zihao Zhao et al. “Calibrate before use: Improving few-shot performance
    of language models” In *International Conference on Machine Learning*, 2021, pp.
    12697–12706 PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Jie Zhou et al. “GEAR: Graph-based evidence aggregating and reasoning
    for fact verification” In *arXiv preprint arXiv:1908.01843*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Jason Zhu et al. “Textgnn: Improving text encoder via graph neural network
    in sponsored search” In *Proceedings of the Web Conference 2021*, 2021, pp. 2848–2857'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Theoretical Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we aim to demonstrate that explanations generated by an LLM
    can provide valuable features for another model (such as a smaller LM). This is
    true under two key conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fidelity:* The explanations effectively represent LLM’s reasoning over the
    raw text, containing most of the information from the LLM’s hidden state.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Non-redundancy:* The LLM possesses unique knowledge not captured by another
    model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We formulate our theorem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Given the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1) Fidelity: $E$ is a good proxy for $Z_{L}$ such that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$H(Z_{l}&#124;E)=\epsilon,{\quad\epsilon></math> |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '2) Non-redundancy: $Z_{L}$ contains information not present in $Z$, expressed
    as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext=$$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Then, it follows that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,E)<H(y&#124;Z)$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $E$ is textual explanations generated by an LLM, $Z_{L}$ is the vectorial
    representation of the raw text modeled by the LLM, $Z$ is the vectorial representation
    of the raw text modeled by the other model, $y$ is the target and $H(\cdot|\cdot)$
    is the conditional entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We aim to demonstrate that the conditional entropy of $y$ given both $Z$ and
    $E$, denoted as $H(y|Z,E)$, is less than the conditional entropy of $y$ given
    only $Z$, denoted as $H(y|Z)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,E)$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'We apply the properties of entropy to decompose this expression into two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,E)=H(y&#124;Z,Z_{L},E)+I(y;Z_{L}&#124;Z,E)$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we utilize the following upper bound of conditional mutual information:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I(y;Z_{L}&#124;Z,E)$ | $\displaystyle=H(Z_{L}&#124;Z,E)-H(Z_{L}&#124;y,Z,E)$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq H(Z_{L}&#124;Z,E)$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where the first line follows from the definition of mutual information, and
    the second line follows from the nonnegativity of conditional entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Substituting ([14](#A1.E14 "In Proof. ‣ Appendix A Theoretical Analysis ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")) into ([12](#A1.E12 "In Proof. ‣ Appendix A Theoretical Analysis ‣
    Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph
    Representation Learning")), we rewrite the conditional entropy as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,E)\leq H(y&#124;Z,Z_{L},E)+H(Z_{L}&#124;Z,E)$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'Since conditional entropy increases when conditioning on fewer variables, we
    further have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,Z_{L},E)+H(Z_{L}&#124;Z,E)\leq H(y&#124;Z,Z_{L})+H(Z_{L}&#124;E)$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'Applying the "Fidelity" and "Non-redundancy" conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,Z_{L})+H(Z_{L}&#124;E)\leq H(y&#124;Z)-\epsilon^{\prime}+\epsilon$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, as <math   alttext="\epsilon^{\prime}></math>, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z)-\epsilon^{\prime}+\epsilon<H(y&#124;Z)$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'Consequently, we have proven that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(y&#124;Z,E)<H(y&#124;Z)$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: This completes the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Addressing Label Leakage Concerns with a New Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-3.5’s training data might include certain arXiv papers, given its comprehensive
    ingestion of textual content from the internet. However, the precise composition
    of these arXiv papers within GPT-3.5’s training remains undisclosed, rendering
    it infeasible to definitively identify their inclusion. It is essential to emphasize
    that the challenge of label leakage is widespread and affects various language
    model benchmarks, such as the prominent BIG-bench [[24](#bib.bibx24)] and TruthfulQA [[18](#bib.bibx18)].
  prefs: []
  type: TYPE_NORMAL
- en: To address this concern, we created a novel dataset tape-arxiv23 for our experiments.
    We made sure that this dataset only included papers published in 2023 or later,
    which is well beyond the knowledge cutoff for GPT-3.5, as it was launched in November
    2022\. The creation of this new dataset was meticulously executed. We collected
    all cs.ArXiv papers published from January 2023 to September 2023 from the arXiv
    daily repository ²²2[https://arxiv.org/](https://arxiv.org/). We then utilized
    the Semantic Scholar API ³³3[https://www.semanticscholar.org/product/api](https://www.semanticscholar.org/product/api)
    to retrieve citation relationships. This process yielded a comprehensive graph
    containing 46,198 papers and 78,548 connections.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conduct experiments on five TAGs – Cora [[21](#bib.bibx21)], PubMed [[23](#bib.bibx23)],
    ogbn-arxiv, ogbn-products [[13](#bib.bibx13)], and tape-arxiv23. For Cora and
    PubMed, we collected the raw text data since they are not available in common
    repositories like PyG and DGL. For ogbn-products, given its substantial scale
    of 2 million nodes and 61 million edges, we have employed a node sampling strategy
    to obtain a subgraph containing 54k nodes and 74k edges. Additionally, we introduced
    the tape-arxiv23 citation graph dataset, extending beyond the knowledge cutoff
    of GPT-3\. This dataset serves as a valuable resource for the research community.
    Table [4](#A3.T4 "Table 4 ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning") provides
    a summary of the dataset statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Statistics of the TAG datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | #Nodes | #Edges | Task | Metric | Augmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Cora | 2,708 | 5,429 | 7-class classif. | Accuracy | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Pubmed | 19,717 | 44,338 | 3-class classif. | Accuracy | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | 169,343 | 1,166,243 | 40-class classif. | Accuracy |  |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-products (subset) | 54,025 | 74,420 | 47-class classif. | Accuracy |  |'
  prefs: []
  type: TYPE_TB
- en: '| tape-arxiv23 | 46,198 | 78,548 | 40-class-classif. | Accuracy | ✓ |'
  prefs: []
  type: TYPE_TB
- en: C.1 Dataset Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cora [[21](#bib.bibx21)]. The Cora dataset comprises 2,708 scientific publications
    classified into one of seven classes – case based, genetic algorithms, neural
    networks, probabilistic methods, reinforcement learning, rule learning, and theory,
    with a citation network consisting of 5,429 links. The papers were selected in
    a way such that in the final corpus every paper cites or is cited by at least
    one other paper.
  prefs: []
  type: TYPE_NORMAL
- en: PubMed [[23](#bib.bibx23)]. The Pubmed dataset consists of 19,717 scientific
    publications from PubMed database pertaining to diabetes classified into one of
    three classes – Experimental induced diabetes, Type 1 diabetes, and Type 2 diabetes.
    The citation network consists of 44,338 links.
  prefs: []
  type: TYPE_NORMAL
- en: ogbn-arxiv [[13](#bib.bibx13)]. The ogbn-arxiv dataset is a directed graph that
    represents the citation network between all computer science arXiv papers indexed
    by MAG [[32](#bib.bibx32)]. Each node is an arXiv paper, and each directed edge
    indicates that one paper cites another one. The task is to predict the 40 subject
    areas of arXiv CS papers, *e.g.,*, cs.AI, cs.LG, and cs.OS, which are manually
    determined (*i.e.,* labeled) by the paper’s authors and arXiv moderators.
  prefs: []
  type: TYPE_NORMAL
- en: ogbn-products [[13](#bib.bibx13)]. The ogbn-products dataset represents an Amazon
    product co-purchasing network, with product descriptions as raw text. Nodes represent
    products sold in Amazon, and edges between two products indicate that the products
    are purchased together. The task is to predict the category of a product in a
    multi-class classification setup, where the 47 top-level categories are used for
    target labels.
  prefs: []
  type: TYPE_NORMAL
- en: tape-arxiv23. The tape-arxiv23 dataset is a directed graph that represents the
    citation network between all computer science arXiv papers published in 2023 or
    later. Similar to ogbn-arxiv, each node is an arXiv paper, and each directed edge
    indicates that one paper cites another one. The task is to predict the 40 subject
    areas of arXiv CS papers, *e.g.,*, cs.AI, cs.LG, and cs.OS, which are manually
    determined (*i.e.,* labeled) by the paper’s authors and arXiv moderators.
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Dataset splits and random seeds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiments, we adhered to specific dataset splits and employed random
    seeds for reproducibility. For the ogbn-arxiv and ogbn-products dataset, we adopted
    the standard train/validation/test split provided by OGB [[13](#bib.bibx13)].
    As for the Cora, PubMed datasets, and tape-arxiv23, we performed the train/validation/test
    splits ourselves, where 60% of the data was allocated for training, 20% for validation,
    and 20% for testing. Additionally, we utilized random seeds to ensure the reproducibility
    of our experiments, enabling the consistent evaluation of our proposed method
    on the respective datasets, which can be found in our linked code repository.
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Shallow Embedding Methods for Node Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [5](#A3.T5 "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature
    Extraction ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning") provides an overview
    of the text preprocessing and feature extraction methods commonly used in graph
    libraries such as PyG and DGL, which are widely adopted in GNN research.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Details of text preprocessing and feature extraction methods used
    for TAG datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Methods | Features | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Cora | BoW | 1,433 | After stemming and removing stopwords there is a vocabulary
    of size 1,433 unique words. All words with document frequency less than 10 were
    removed. |'
  prefs: []
  type: TYPE_TB
- en: '| PubMed | TF-IDF | 500 | Each publication in the dataset is described by a
    TF/IDF weighted word vector from a dictionary which consists of 500 unique words.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | skip-gram | 128 | The embeddings of individual words are computed
    by running the skip-gram model [[22](#bib.bibx22)] over the MAG[[32](#bib.bibx32)]
    corpus. |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-products | BoW | 100 | Node features are generated by extracting BoW
    features from the product descriptions followed by a Principal Component Analysis
    to reduce the dimension to 100. |'
  prefs: []
  type: TYPE_TB
- en: '| tape-arxiv23 | word2vec | 300 | The embeddings of individual words are computed
    by running the word2vec model. |'
  prefs: []
  type: TYPE_TB
- en: These text preprocessing and feature extraction methods facilitate the extraction
    of node features from the text attributes of TAG datasets, enabling the utilization
    of GNN models for node classification tasks. While these methods are easy to apply
    and computationally efficient, it is important to note that they rely on traditional
    language modeling techniques that may not capture the full semantic meaning in
    the text. This limitation can impact the expressiveness of the extracted node
    features and potentially affect the development of techniques for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 Computing Environment and Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The implementation of the proposed method utilized the PyG and DGL modules,
    which are licensed under the MIT License. The experiments were conducted in a
    computing environment with the following specifications: LM-based experiments
    were performed on four NVIDIA RTX A5000 GPUs, each with 24GB VRAM. On the other
    hand, the GNN-based experiments were conducted on a single GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [6](#A4.T6 "Table 6 ‣ D.2 Hyperparameters ‣ Appendix D Experiments ‣
    Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph
    Representation Learning") provides an overview of the hyperparameters used for
    the GCN [[16](#bib.bibx16)], SAGE [[10](#bib.bibx10)], and RevGAT [[17](#bib.bibx17)]
    models. These hyperparameters were selected based on the official OGB repository ⁴⁴4[https://github.com/snap-stanford/ogb](https://github.com/snap-stanford/ogb),
    and the RevGAT and language model hyperparameters follow those used in the GLEM
    repository ⁵⁵5[https://github.com/AndyJZhao/GLEM](https://github.com/AndyJZhao/GLEM).
    It is important to note that these hyperparameters were not tuned on a per-dataset
    basis, but instead were used consistently across all three TAG datasets based
    on those from prior work, and also set consistently across both our proposed method
    and the baselines. This demonstrates the generality and ease of use of our method,
    as well as its compatibility with existing GNN baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Hyperparameters for the GCN, SAGE, and RevGAT models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | GCN | SAGE | RevGAT |'
  prefs: []
  type: TYPE_TB
- en: '| # layers | 3 | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| hidden dim | 256 | 256 | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| learning rate | 0.01 | 0.01 | 0.002 |'
  prefs: []
  type: TYPE_TB
- en: '| dropout | 0.5 | 0.5 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| epoch | 1000 | 1000 | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '| warmup epochs | 0 | 0 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| early stop | 50 | 50 | 50 |'
  prefs: []
  type: TYPE_TB
- en: D.3 Prompt Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [7](#A4.T7 "Table 7 ‣ D.3 Prompt Design ‣ Appendix D Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning") provides our prompt designs for different datasets. Each prompt includes
    the abstract and title of the paper, followed by a task-specific question. The
    question is formulated to query the model about a particular aspect of the paper
    and request an explanation for the prediction. The answer section is left blank
    for the model to fill in. Generally, our analysis finds that the current instructions
    allow the LLM to produce output that conforms well to the expected format without
    significant deviations, allowing the answers to be straightforwardly extracted
    from the text output of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Design of Prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| Cora | Abstract: <abstract text>  \n Title: <title text>  \n Question: Which
    of the following sub-categories of AI does this paper belong to: Case Based, Genetic
    Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule
    Learning, Theory? If multiple options apply, provide a comma-separated list ordered
    from most to least related, then for each choice you gave, explain how it is present
    in the text. \n \n Answer: |'
  prefs: []
  type: TYPE_TB
- en: '| Pubmed | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Does the paper involve any cases of Type 1 diabetes, Type 2 diabetes, or Experimentally
    induced diabetes? Please give one or more answers of either Type 1 diabetes, Type
    2 diabetes, or Experimentally induced diabetes; if multiple options apply, provide
    a comma-separated list ordered from most to least related, then for each choice
    you gave, give a detailed explanation with quotes from the text explaining why
    it is related to the chosen option. \n \n Answer: |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-products | Product description: <product description>  \n Question:
    Which of the following category does this product belong to: 1) Home & Kitchen,
    2) Health & Personal Care, 3) Beauty, 4) Sports & Outdoors, 5) Books, 6) Patio,
    Lawn & Garden, 7) Toys & Games, 8) CDs & Vinyl, 9) Cell Phones & Accessories,
    10) Grocery & Gourmet Food, 11) Arts, Crafts & Sewing, 12) Clothing, Shoes & Jewelry,
    13) Electronics, 14) Movies & TV, 15) Software, 16) Video Games, 17) Automotive,
    18) Pet Supplies, 19) Office Products, 20) Industrial & Scientific, 21) Musical
    Instruments, 22) Tools & Home Improvement, 23) Magazine Subscriptions, 24) Baby
    Products, 25) NAN, 26) Appliances, 27) Kitchen & Dining, 28) Collectibles & Fine
    Art, 29) All Beauty, 30) Luxury Beauty, 31) Amazon Fashion, 32) Computers, 33)
    All Electronics, 34) Purchase Circles, 35) MP3 Players & Accessories, 36) Gift
    Cards, 37) Office & School Supplies, 38) Home Improvement, 39) Camera & Photo,
    40) GPS & Navigation, 41) Digital Music, 42) Car Electronics, 43) Baby, 44) Kindle
    Store, 45) Kindle Apps, 46) Furniture & Decor? Give 5 likely categories as a comma-separated
    list ordered from most to least likely, and provide your reasoning. \n \n Answer:
    |'
  prefs: []
  type: TYPE_TB
- en: '| tape-arxiv23 | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: |'
  prefs: []
  type: TYPE_TB
- en: Additional Prompt Experiments.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 8: Prompts used for our experiments studying the effect of different
    prompts. Most prompts have similar performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Prompt | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Default prompt | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: | 0.720 |'
  prefs: []
  type: TYPE_TB
- en: '| Title first | Title: <title text>  \n Abstract: <abstract text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: | 0.695 |'
  prefs: []
  type: TYPE_TB
- en: '| Focus on text content | Title: <title text>  \n Abstract: <abstract text>  \n
    Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely
    arXiv CS sub-categories as a comma-separated list ordered from most to least likely,
    in the form “cs.XX”. Focus only on content in the actual text and avoid making
    false associations. Then provide your reasoning. | 0.695 |'
  prefs: []
  type: TYPE_TB
- en: '| Chain of thought prompt | Title: <title text>  \n Abstract: <abstract text>  \n
    Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely
    arXiv CS sub-categories as a comma-separated list ordered from most to least likely,
    in the form “cs.XX”. Please think about the categorization in a step by step manner
    and avoid making false associations. Then provide your reasoning. | 0.705 |'
  prefs: []
  type: TYPE_TB
- en: 'To study the effect of different prompts, we consider a variety of prompts
    and evaluate the zero-shot accuracy of the LLM (ChatGPT) on each prompt. We evaluate
    all prompts on 200 sample papers from the ogbn-arxiv dataset, see Table [8](#A4.T8
    "Table 8 ‣ Additional Prompt Experiments. ‣ D.3 Prompt Design ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). We accompany each prompt by a brief summary of
    the change being made. In summary, most prompts have similar performance, with
    a slight performance gain when placing the title after the abstract, which seems
    to agree with the notion in [[46](#bib.bibx46)] that more important information
    (like the title) should be placed later in the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: D.4 Detailed Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conducted a detailed ablation study on the ogbn-arxiv dataset to assess
    the impact of different sources of node features. The study focused on three types
    of node features: original text features ($h_{\textrm{orig}}$), explanation as
    features ($h_{\textrm{expl}}$), and predictions as features ($h_{\textrm{pred}}$).
    We systematically removed one of these features at a time while keeping the other
    components unchanged in our model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the ablation study are illustrated in Figure [3](#A4.F3 "Figure
    3 ‣ D.4 Detailed Ablation Study ‣ Appendix D Experiments ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning").
    The figure presents the performance of the model when each type of node feature
    is removed. It is observed that using the full set of features yields the best
    performance, while leaving out any of the features leads to a drop in performance.
    However, the extent of the performance drop may vary depending on the specific
    GNN model being used.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d49dff0fd6769254e47715956b346e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Effect of node features. We study the effects of different sources
    of node features on the ogbn-arxiv dataset, *i.e.,* original text features ($h_{\textrm{orig}}$),
    explanation as features ($h_{\textrm{expl}}$) and predictions as features ($h_{\textrm{pred}}$),
    by removing one of them in turn from our model while keeping the other components
    unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This ablation study provides additional insights to complement the findings
    presented in section [5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). While Table [3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") compared the performance of using the full set
    of features versus using just one of them, this ablation study specifically focuses
    on comparing the performance of using the full set of features versus leaving
    one of them out. Although the experimental design differs, the overall message
    conveyed remains consistent, emphasizing the significance of considering all the
    various sources of node features for achieving optimal performance in node classification
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: D.5 Llama as a cost-efficient alternative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We extend out experiment to the open-source LLM "llama-2-13b-chat" (llama for
    short), which demonstrates the feasibility of a cost-effective (free) alternative,
    see Table [9](#A4.T9 "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that although llama exhibits a lower performance compared
    to GPT-3.5 in terms of both zero-shot accuracy and explanation quality, our pipeline
    still maintains its robust performance. As an illustration, we achieved an accuracy
    of 76.19% on the ogbn-arxiv dataset using llama, slightly below the 77.50% achieved
    with GPT-3.5\. We attribute this impressive level of generalization to the complementary
    nature of the explanations themselves, which serve as a rich source of semantic
    information supplementing the original text such as title and abstract.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Node classification accuracy for the Cora, PubMed, ogbn-arxiv and
    tape-arxiv23 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | llama2-13b-chat | GPT3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | LM${}_{\textrm{finetune}}$ | $h_{\textrm{TAPE}}$ | LLM | LM${}_{\textrm{finetune}}$
    | $h_{\textrm{TAPE}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Cora | MLP | 0.5746 | 0.6845 ± 0.0194 | 0.7675 ± 0.0187 | 0.6769 | 0.7606
    ± 0.0378 | 0.8086 ± 0.0190 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.5746 | 0.6845 ± 0.0194 | 0.8630 ± 0.0101 | 0.6769 | 0.7606 ± 0.0378
    | 0.8773 ± 0.0063 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.5746 | 0.6845 ± 0.0194 | 0.8625 ± 0.0093 | 0.6769 | 0.7606 ± 0.0378
    | 0.8792 ± 0.0107 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.5746 | 0.6845 ± 0.0194 | 0.8884 ± 0.0100 | 0.6769 | 0.7606
    ± 0.0378 | 0.8930 ± 0.0072 |'
  prefs: []
  type: TYPE_TB
- en: '| PubMed | MLP | 0.3958 | 0.9121 ± 0.0026 | 0.9475 ± 0.0046 | 0.9342 | 0.9494
    ± 0.0046 | 0.9473 ± 0.0040 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.3958 | 0.9121 ± 0.0026 | 0.9257 ± 0.0063 | 0.9342 | 0.9494 ± 0.0046
    | 0.9392 ± 0.0023 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.3958 | 0.9121 ± 0.0026 | 0.9464 ± 0.0033 | 0.9342 | 0.9494 ± 0.0046
    | 0.9530 ± 0.0035 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.3958 | 0.9121 ± 0.0026 | 0.9465 ± 0.0042 | 0.9342 | 0.9494
    ± 0.0046 | 0.9526 ± 0.0032 |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | MLP | 0.4423 | 0.6941 ± 0.0020 | 0.7361 ± 0.0009 | 0.7350 |
    0.7361 ± 0.0004 | 0.7587 ± 0.0015 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.4423 | 0.6941 ± 0.0020 | 0.7418 ± 0.0031 | 0.7350 | 0.7361 ± 0.0004
    | 0.7520 ± 0.0003 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.4423 | 0.6941 ± 0.0020 | 0.7536 ± 0.0028 | 0.7350 | 0.7361 ± 0.0004
    | 0.7672 ± 0.0007 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.4423 | 0.6941 ± 0.0020 | 0.7619 ± 0.0027 | 0.7350 | 0.7361
    ± 0.0004 | 0.7750 ± 0.0012 |'
  prefs: []
  type: TYPE_TB
- en: '| tape-arxiv23 | MLP | 0.4452 | 0.7677 ± 0.0042 | 0.7905 ± 0.0041 | 0.7356
    | 0.7832 ± 0.0052 | 0.7999 ± 0.0037 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 0.4452 | 0.7677 ± 0.0042 | 0.7751 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.7827 ± 0.0037 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 0.4452 | 0.7677 ± 0.0042 | 0.7935 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.8020 ± 0.0024 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RevGAT | 0.4452 | 0.7677 ± 0.0042 | 0.7993 ± 0.0043 | 0.7356 | 0.7832
    ± 0.0052 | 0.8074 ± 0.0021 |'
  prefs: []
  type: TYPE_TB
- en: D.6 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4da50160638a3fd9299866f50c433471.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Case study comparing features for node classification on the PubMed
    dataset: (a) Original text attributes and (b) Explanations generated by LLMs.
    The GNN model trained with (b) accurately predicts the label for node 12390 (type
    2 diabetes), while the model trained with (a) predicts the incorrect label (experimentally
    induced diabetes). This improvement can be attributed to the concise and focused
    nature of LLM-generated explanations, as well as their reasoning ability and utilization
    of external knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To investigate the impact of using explanations as features in improving node
    classification on TAGs, we conduct an analysis on predicted samples from the PubMed
    dataset. Figure [4](#A4.F4 "Figure 4 ‣ D.6 Case Study ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") presents a case where the GNN model trained with
    original text attributes as features incorrectly predicts the label for node 12390
    (as experimentally induced diabetes), while the model trained with explanations
    generated by LLMs as features correctly predicts the label (as type 2 diabetes).'
  prefs: []
  type: TYPE_NORMAL
- en: This improvement can be attributed to two main factors. Firstly, compared to
    the original text attributes, which consist of the title and abstract text, the
    explanations generated by the LLM are more concise and focused. This aids the
    subsequent LM in generating node embeddings that capture the essential semantics
    without the need to compress an excessive amount of information into a fixed-length
    representation. Secondly, LLMs possess reasoning capabilities and the ability
    to leverage general knowledge, which prove crucial in achieving accurate predictions.
    For instance, the explanations generated by LLMs explicitly link type 2 diabetes
    to MKR mice and db/db mice (which are common animal models of type 2 diabetes),
    as well as the insulinopenic mice / streptozotocin to experimentally induced diabetes.
    This knowledge is either absent or only implicitly specified in the original text
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: D.7 GLEM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[45](#bib.bibx45)] evaluated GLEM on the ogbn-arxiv dataset. We extended our
    evaluation of GLEM with the Cora and PubMed datasets for a more comprehensive
    comparison with our method. Results are reported in Table [10](#A4.T10 "Table
    10 ‣ D.7 GLEM ‣ Appendix D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: GLEM [[45](#bib.bibx45)]'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | GCN | SAGE | RevGAT |'
  prefs: []
  type: TYPE_TB
- en: '| Cora | 0.8732 ± 0.0066 | 0.8801 ± 0.0054 | 0.8856 ± 0.0060 |'
  prefs: []
  type: TYPE_TB
- en: '| PubMed | 0.9469 ± 0.0010 | 0.9459 ± 0.0018 | 0.9471 ± 0.0020 |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | 0.7593 ± 0.0019 | 0.7550 ± 0.0024 | 0.7697 ± 0.0019 |'
  prefs: []
  type: TYPE_TB
