- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 17:35:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 17:35:06'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用解释：用于增强文本属性图表示学习的LLM-to-LM解释器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19523](https://ar5iv.labs.arxiv.org/html/2305.19523)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19523](https://ar5iv.labs.arxiv.org/html/2305.19523)
- en: Xiaoxin He¹   Xavier Bresson¹   Thomas Laurent²   Adam Perold³   Yann LeCun^(4,5)
      Bryan Hooi¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xiaoxin He¹   Xavier Bresson¹   Thomas Laurent²   Adam Perold³   Yann LeCun^(4,5)
      Bryan Hooi¹
- en: '{xiaoxin, xaviercs, bhooi}@comp.nus.edu.sg, tlaurent@lmu.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '{xiaoxin, xaviercs, bhooi}@comp.nus.edu.sg, tlaurent@lmu.edu'
- en: research@provenance.ai, yann@cs.nyu.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: research@provenance.ai, yann@cs.nyu.edu
- en: ¹National University of Singapore    ²Loyola Marymount University    ³Provenance
    AI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹新加坡国立大学    ²洛约拉玛丽蒙大学    ³Provenance AI
- en: ⁴New York University    ⁵Meta AI
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴纽约大学    ⁵Meta AI
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Representation learning on text-attributed graphs (TAGs) has become a critical
    research problem in recent years. A typical example of a TAG is a paper citation
    graph, where the text of each paper serves as node attributes. Initial graph neural
    network (GNN) pipelines handled these text attributes by transforming them into
    shallow or hand-crafted features, such as skip-gram or bag-of-words features.
    Recent efforts have focused on enhancing these pipelines with language models
    (LMs), which typically demand intricate designs and substantial computational
    resources. With the advent of powerful large language models (LLMs) such as GPT
    or Llama2, which demonstrate an ability to reason and to utilize general knowledge,
    there is a growing need for techniques which combine the textual modelling abilities
    of LLMs with the structural learning capabilities of GNNs. Hence, in this work,
    we focus on leveraging LLMs to capture textual information as features, which
    can be used to boost GNN performance on downstream tasks. A key innovation is
    our use of *explanations as features*: we prompt an LLM to perform zero-shot classification,
    request textual explanations for its decision-making process, and design an *LLM-to-LM
    interpreter* to translate these explanations into informative features that enhance
    downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art
    results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv,
    as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method
    significantly speeds up training, achieving a 2.88 times improvement over the
    closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed
    method extends beyond TAGs and holds the potential to enhance other tasks involving
    graph-text data ¹¹1Our codes and datasets are available at: [https://github.com/XiaoxinHe/TAPE](https://github.com/XiaoxinHe/TAPE).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在近年来，文本属性图（TAGs）上的表示学习已成为一个关键的研究问题。TAG的一个典型例子是论文引用图，其中每篇论文的文本作为节点属性。最初的图神经网络（GNN）管道通过将这些文本属性转化为浅层或手工制作的特征（如跳字模型或词袋模型特征）来处理这些文本属性。最近的努力集中在通过语言模型（LMs）来增强这些管道，这通常需要复杂的设计和大量的计算资源。随着强大的大型语言模型（LLMs）如GPT或Llama2的出现，这些模型展示了推理和利用通用知识的能力，因此越来越需要将LLMs的文本建模能力与GNNs的结构学习能力相结合的技术。因此，在这项工作中，我们专注于利用LLMs来捕捉文本信息作为特征，这些特征可以用来提升GNN在下游任务中的表现。一个关键的创新是我们使用*解释作为特征*：我们提示LLM执行零样本分类，要求其提供决策过程的文本解释，并设计一个*LLM-to-LM解释器*来将这些解释转化为增强下游GNN的有用特征。我们的实验表明，我们的方法在公认的TAG数据集上取得了最先进的结果，包括Cora、PubMed、ogbn-arxiv以及我们新引入的数据集tape-arxiv23。此外，我们的方法显著加快了训练速度，在ogbn-arxiv上比最接近的基线提高了2.88倍。最后，我们认为该方法的通用性超越了TAGs，并有潜力提升其他涉及图-文本数据的任务。¹¹1我们的代码和数据集可在：[https://github.com/XiaoxinHe/TAPE](https://github.com/XiaoxinHe/TAPE)。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Many real-world graphs possess textual information, and are often referred to
    text-attributed graphs [[37](#bib.bibx37)]. In TAGs, nodes typically represent
    text entities, such as documents or sentences, while edges signify relationships
    between these entities. For example, the ogbn-arxiv dataset [[13](#bib.bibx13)]
    represents a citation network in TAG form, where each node corresponds to a paper,
    with its title and abstract serving as node attributes. More generally, the combination
    of textual attributes with graph topology provides a rich source of information,
    significantly enhancing representation learning for important applications, such
    as text classification [[36](#bib.bibx36), [34](#bib.bibx34), [40](#bib.bibx40),
    [3](#bib.bibx3), [45](#bib.bibx45)], recommendation systems [[48](#bib.bibx48)],
    social networks, and fake news detection [[20](#bib.bibx20)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的图形包含文本信息，通常被称为文本属性图（TAGs）[[37](#bib.bibx37)]。在TAGs中，节点通常表示文本实体，例如文档或句子，而边则表示这些实体之间的关系。例如，ogbn-arxiv数据集
    [[13](#bib.bibx13)] 表示一个以TAG形式存在的引用网络，其中每个节点对应一篇论文，其标题和摘要作为节点属性。更一般来说，文本属性与图形拓扑的结合提供了丰富的信息来源，显著提升了对重要应用的表示学习，如文本分类
    [[36](#bib.bibx36)、[34](#bib.bibx34)、[40](#bib.bibx40)、[3](#bib.bibx3)、[45](#bib.bibx45)]、推荐系统
    [[48](#bib.bibx48)]、社交网络，以及虚假新闻检测 [[20](#bib.bibx20)]。
- en: '![Refer to caption](img/6db2689fbd5fc8086c3a2bf7da432030.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6db2689fbd5fc8086c3a2bf7da432030.png)'
- en: 'Figure 1: Overview of our framework leveraging large language models (LLMs)
    to enhance representation learning on TAGs. First, the text attributes associated
    with each node, *i.e.,* title and abstract, are wrapped in a custom prompt (green
    box) and used to query the LLM, here GPT-3.5 [[1](#bib.bibx1)], which generates
    a ranked prediction list and explanation (yellow box). Next, the original text,
    predictions, and explanation are used to fine-tune an language model (LM), here
    DeBERTa [[12](#bib.bibx12)], then transformed into vectorial node features. Finally,
    these enriched node features, *i.e.,* $h_{\textrm{orig}}$, $h_{\textrm{expl}}$
    and $h_{\textrm{pred}}$, are used in any downstream GNN, *e.g.,* RevGAT [[17](#bib.bibx17)]
    to predict unknown node classes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的框架概述利用大语言模型（LLMs）来增强对TAGs的表示学习。首先，与每个节点相关的文本属性，*即*标题和摘要，被封装在自定义提示中（绿色框），并用来查询LLM，这里是GPT-3.5
    [[1](#bib.bibx1)]，它生成一个排名预测列表和解释（黄色框）。接着，原始文本、预测和解释被用来微调一个语言模型（LM），这里是DeBERTa
    [[12](#bib.bibx12)]，然后转化为向量化的节点特征。最后，这些丰富的节点特征，*即* $h_{\textrm{orig}}$、$h_{\textrm{expl}}$
    和 $h_{\textrm{pred}}$，被用于任何下游GNN，*例如* RevGAT [[17](#bib.bibx17)] 来预测未知节点类别。
- en: 'Representation learning on TAGs. Prior research has explored various approaches
    for representation learning on TAGs. The standard GNN pipeline (illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning") in light
    yellow), first encodes the textual attributes of each node using shallow or hand-crafted
    features such as skip-gram [[22](#bib.bibx22)] or bag-of-words (BoW) [[11](#bib.bibx11)]
    (refer to Table [5](#A3.T5 "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature
    Extraction ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")). The resulting node
    features are then used as input for a GNN. For instance, the Open Graph Benchmark
    (OGB) [[13](#bib.bibx13)] generated BoW and skip-gram [[22](#bib.bibx22)] features
    for the ogbn-products and ogbn-arxiv datasets respectively. These processed features
    are readily available within popular graph libraries, such as PyTorch Geometric
    (PyG) [[8](#bib.bibx8)] and Deep Graph Library (DGL) [[33](#bib.bibx33)], and
    have been widely used by the graph community. However, these shallow text embeddings
    are limited in the complexity of the semantic features they can capture, especially
    when compared to approaches based on multi-layer LMs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '对 TAG 的表征学习。之前的研究探讨了多种对 TAG 进行表征学习的方法。标准的 GNN 流程（如图 [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") 中的浅黄色所示），首先使用浅层或手工制作的特征，例如 skip-gram [[22](#bib.bibx22)]
    或 bag-of-words (BoW) [[11](#bib.bibx11)]，对每个节点的文本属性进行编码（参见表 [5](#A3.T5 "Table
    5 ‣ C.3 Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")）。得到的节点特征随后作为 GNN 的输入。例如，Open Graph Benchmark (OGB)
    [[13](#bib.bibx13)] 为 ogbn-products 和 ogbn-arxiv 数据集分别生成了 BoW 和 skip-gram [[22](#bib.bibx22)]
    特征。这些处理后的特征在流行的图形库中可以方便地获取，例如 PyTorch Geometric (PyG) [[8](#bib.bibx8)] 和 Deep
    Graph Library (DGL) [[33](#bib.bibx33)]，并被图形社区广泛使用。然而，这些浅层文本嵌入在捕捉语义特征的复杂性方面有限，特别是与基于多层
    LMs 的方法相比。'
- en: 'LM-based pipeline for TAGs. Recent works have therefore focused on designing
    LM-based techniques to better capture the context and nuances of text within TAGs [[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]. In this approach, pre-trained LMs are fine-tuned
    and used to generate node embeddings that are tailored to the specific TAG tasks
    (depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    in light gray). For example, [[3](#bib.bibx3)] fine-tuned an LM using a neighborhood
    prediction task, while [[45](#bib.bibx45)] fine-tuned an LM to predict the label
    distribution from a GNN’s outputs. LM-based models have achieved state-of-the-art
    (SOTA) results in node classification on ogbn-arxiv and ogbn-products [[45](#bib.bibx45)].
    However, these works typically entail intricate designs and demand substantial
    computational resources. Furthermore, for scalability reasons, existing works
    mostly rely on relatively small LMs, such as BERT [[5](#bib.bibx5)] and DeBERTa [[12](#bib.bibx12)],
    and thus lack the complex reasoning abilities associated with larger language
    models.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 LM 的 TAG 流程。最近的研究因此集中于设计基于 LM 的技术，以更好地捕捉 TAG 中文本的上下文和细微差别 [[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]。在这种方法中，预训练的 LMs 会经过微调并用于生成针对特定 TAG 任务的节点嵌入（如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning") 中的浅灰色所示）。例如，[[3](#bib.bibx3)]
    使用邻域预测任务对 LM 进行微调，而 [[45](#bib.bibx45)] 对 LM 进行微调以预测 GNN 输出的标签分布。基于 LM 的模型在 ogbn-arxiv
    和 ogbn-products 的节点分类任务中取得了最先进的 (SOTA) 结果 [[45](#bib.bibx45)]。然而，这些工作通常涉及复杂的设计，并且需要大量的计算资源。此外，由于可扩展性的原因，现有工作大多依赖于相对较小的
    LMs，如 BERT [[5](#bib.bibx5)] 和 DeBERTa [[12](#bib.bibx12)]，因此缺乏与更大语言模型相关的复杂推理能力。'
- en: Large Language Models. The advent of large pre-trained models, exemplified by
    GPT [[1](#bib.bibx1)], has revolutionized the field of language modeling. LLMs
    have notably enhanced performance across various natural language processing (NLP)
    tasks, and enabled sophisticated language processing capabilities such as complex
    and zero-shot reasoning. Furthermore, scaling laws [[15](#bib.bibx15)] have revealed
    predictable rules for performance improvements with model and training data size.
    Additionally, LLMs have exhibited “emergent abilities” that were not explicitly
    trained for, such as arithmetic, multi-step reasoning and instruction following [[35](#bib.bibx35)].
    While LLMs have found new success in domains like computer vision [[28](#bib.bibx28)],
    their potential benefits when applied to TAG tasks remain largely uncharted. This
    presents an exciting and promising avenue for future research, and it is precisely
    this untapped potential that we aim to explore in this work.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。大型预训练模型的出现，以GPT[[1](#bib.bibx1)]为例，彻底改变了语言建模领域。LLMs显著提高了各种自然语言处理（NLP）任务的性能，并使得复杂和零样本推理等高级语言处理能力成为可能。此外，规模法则[[15](#bib.bibx15)]揭示了模型和训练数据规模对性能提升的可预测规则。此外，LLMs还展现了“涌现能力”，这些能力并未被明确训练，例如算术、多步骤推理和指令跟随[[35](#bib.bibx35)]。虽然LLMs在计算机视觉等领域取得了新的成功[[28](#bib.bibx28)]，但它们在TAG任务中的潜在益处仍然基本未被探索。这为未来的研究提供了一个令人兴奋且充满前景的方向，正是这种未被开发的潜力是我们在这项工作中旨在探索的。
- en: LMs vs. LLMs. In this paper, we make a clear distinction between “LMs” and “LLMs”.
    We use LMs to refer to relatively small language models that can be trained and
    fine-tuned within the constraints of an academic lab budget. We refer to LLMs
    as very large language models that are capable of learning significantly more
    complex linguistic patterns than LMs, such as GPT-3/4\. These models typically
    have tens or hundreds of billions of parameters and require substantial computational
    resources to train and use, *e.g.,* GPT-3 was trained on a supercomputer with
    10,000 GPUs. The size and complexity of recent LLMs have raised concerns about
    their scalability, as they can be too large even to run inference on the machines
    typically available within academic research labs. To address this issue, LLMs
    are often made accessible through language modeling as a service (LMaaS) [[26](#bib.bibx26)].
    This approach enables developers to harness the power of LLMs without necessitating
    extensive computational resources or specialized expertise. In the context of
    this paper, one of our primary objectives is to extract information from an LLM
    in a LMaaS-compatible manner. As a result, we do not require fine-tuning the LLM
    or extracting its logits; rather, we focus solely on obtaining its output in textual
    form. In contrast, existing LM-based techniques [[3](#bib.bibx3), [45](#bib.bibx45),
    [6](#bib.bibx6)] are not directly compatible with LLMs, as they require fine-tuning
    of LMs, as well as accessing their latent embeddings or logits, which GPT-3/4
    do not provide. Consequently, to the best of our knowledge, the use of LLMs in
    TAG tasks remains an unexplored area.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LMs）与大型语言模型（LLMs）。在本文中，我们明确区分了“LMs”和“LLMs”。我们使用LMs来指代那些可以在学术实验室预算范围内进行训练和微调的相对较小的语言模型。我们将LLMs指为那些能够学习比LMs更复杂的语言模式的非常大的语言模型，例如GPT-3/4。这些模型通常拥有数十亿或数百亿个参数，并且需要大量计算资源来训练和使用，*例如*，GPT-3是在配备10,000个GPU的超级计算机上进行训练的。最近LLMs的规模和复杂性引发了对其可扩展性的担忧，因为它们甚至可能过大，以至于无法在通常在学术研究实验室中可用的机器上运行推理。为了解决这个问题，LLMs通常通过语言建模即服务（LMaaS）[[26](#bib.bibx26)]提供。这种方法使开发者能够利用LLMs的强大功能，而无需大量计算资源或专业知识。在本文的背景下，我们的主要目标之一是以LMaaS兼容的方式从LLM中提取信息。因此，我们不需要对LLM进行微调或提取其logits；而是专注于以文本形式获取其输出。相比之下，现有的基于LM的方法[[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]与LLMs不直接兼容，因为它们需要对LMs进行微调，并且访问其潜在的嵌入或logits，而GPT-3/4并不提供这些。因此，据我们所知，在TAG任务中使用LLMs仍然是一个未被探索的领域。
- en: Preliminary study. To assess the potential of LLMs in enhancing representation
    learning for TAGs, we conducted an initial investigation into leveraging GPT-3.5
    for zero-shot classification on the ogbn-arxiv dataset. Using task-specific prompts
    consisting of paper titles, abstracts, and questions, GPT-3.5 achieved a promising
    accuracy of 73.5%, along with high-quality text explanations, surpassing several
    fully trained GNN baselines like RevGAT [[17](#bib.bibx17)] with OGB features
    (70.8% accuracy), but falling short of the SOTA accuracy of 76.6% [[45](#bib.bibx45)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 初步研究。为了评估LLMs在增强TAGs表示学习中的潜力，我们进行了初步调查，利用GPT-3.5在ogbn-arxiv数据集上进行零样本分类。使用包含论文标题、摘要和问题的任务特定提示，GPT-3.5实现了73.5%的令人鼓舞的准确率，并提供了高质量的文本解释，超越了几种完全训练的GNN基线模型，如RevGAT
    [[17](#bib.bibx17)]（OGB特征的准确率为70.8%），但未能达到SOTA的76.6%准确率 [[45](#bib.bibx45)]。
- en: 'The present work: LLM augmentation using explanations. We introduce a novel
    framework that leverages LLMs to improve representation learning on TAGs. A key
    innovation is the concept of *explanations as features*. By prompting a powerful
    LLM to explain its predictions, we extract its relevant prior knowledge and reasoning
    steps, making this information digestible for smaller models, akin to how human
    experts use explanations to convey insights. To illustrate this concept further,
    observe in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    that the explanations (in the yellow box) highlight and expand upon key crucial
    information from the text, such as “deep learning techniques such as DeconvNet,”
    and the relationship between text recognition and information retrieval. These
    explanations draw from the LLM’s general knowledge and serve as valuable features
    for enhancing subsequent TAG pipeline phases. In practice, we design a tailored
    prompt to query an LLM such as GPT or Llama2 to generate both a *ranked prediction
    list* and a *textual explanation* for its predictions. These predictions and explanations
    are then transformed into informative node features through fine-tuning a smaller
    LM such as DeBERTa [[12](#bib.bibx12)] for the target task, providing tailored
    features for any downstream GNNs. This smaller model acts as an interpreter, facilitating
    seamless communication between the LLM (handling text) and the GNN (managing vectorial
    representation).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '当前工作：使用解释增强LLM。我们引入了一种新颖的框架，利用LLMs来改善TAGs上的表示学习。一个关键的创新是*解释作为特征*的概念。通过提示强大的LLM解释其预测，我们提取了其相关的先前知识和推理步骤，使这些信息对较小的模型易于消化，类似于人类专家使用解释传达见解。进一步说明这一概念，参见图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning")，解释（在黄色框中）突出并扩展了文本中的关键重要信息，例如“深度学习技术如DeconvNet”，以及文本识别和信息检索之间的关系。这些解释来自LLM的一般知识，并作为增强后续TAG管道阶段的有价值特征。在实践中，我们设计了一个量身定制的提示来查询LLM，如GPT或Llama2，以生成*排序预测列表*和*文本解释*。这些预测和解释随后通过微调较小的LM（如DeBERTa
    [[12](#bib.bibx12)]）为目标任务，转化为信息丰富的节点特征，为任何下游GNN提供量身定制的特征。这个较小的模型充当解释器，促进LLM（处理文本）与GNN（管理向量表示）之间的无缝沟通。'
- en: '![Refer to caption](img/54a92bac8996e0607d154875cb41ab34.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54a92bac8996e0607d154875cb41ab34.png)'
- en: 'Figure 2: The performance trade-off between node classification accuracy and
    total training time on ogbn-arxiv [[13](#bib.bibx13)] for various training approaches
    that combine language models (LMs) and graph neural networks (GNNs). The experiment
    employs DeBERTa-base [[12](#bib.bibx12)] as the LM backbone and RevGAT [[17](#bib.bibx17)]
    as the GNN backbone, with the size of the marker indicating the number of parameters.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同训练方法在ogbn-arxiv [[13](#bib.bibx13)]上节点分类准确率与总训练时间之间的性能权衡，这些方法结合了语言模型（LMs）和图神经网络（GNNs）。实验使用DeBERTa-base
    [[12](#bib.bibx12)]作为LM骨干网，RevGAT [[17](#bib.bibx17)]作为GNN骨干网，标记的大小表示参数数量。
- en: 'Our contributions are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Novel LMaaS-compatible approach. We propose the first LMaaS-compatible approach,
    to the best of our knowledge, for leveraging LLMs to enhance representation learning
    on TAGs. Our innovations involve extracting explanations from an LLM, here GPT-3.5
    and Llama2, and subsequently employing an LLM-to-LM interpreter to translate textual
    explanations into enriched node vector representations for downstream GNNs. Our
    approach improves modularity and efficiency compared to prior LM+GNN models.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新颖的 LMaaS 兼容方法。我们提出了首个 LMaaS 兼容的方法，据我们所知，旨在利用 LLMs 增强 TAGs 上的表示学习。我们的创新涉及从 LLM（这里是
    GPT-3.5 和 Llama2）中提取解释，然后使用 LLM-to-LM 解释器将文本解释转换为丰富的节点向量表示，以供下游 GNN 使用。与之前的 LM+GNN
    模型相比，我们的方法在模块化和效率方面有所改进。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SOTA performance. Extensive experiments demonstrate that our method significantly
    boost the performance of various GNN models across diverse datasets. Notably,
    we achieve top-1 performance on ogbn-arxiv with significantly lower computation
    time, *i.e.,* $2.88\times$ faster than GLEM, and also excel in the TAG versions
    of PubMed and Cora datasets.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**SOTA** 性能。大量实验表明，我们的方法显著提升了各种 GNN 模型在不同数据集上的表现。特别是，我们在 ogbn-arxiv 数据集上实现了顶级性能，计算时间显著更短，*即*
    比 GLEM 快了 $2.88\times$，同时在 PubMed 和 Cora 数据集的 TAG 版本中也表现出色。'
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data contribution. We provide open-source access to our codes, pre-trained networks
    and enriched features. Additionally, recognizing the absence of raw text data
    for Cora and PubMed in common repositories (*e.g.,* PyG, DGL), we have collected
    and released these datasets in TAG format. Furthermore, we introduce the new tape-arxiv23
    citation graph dataset, extending beyond GPT-3’s knowledge cutoff, *i.e.,* Sept.
    2021\. These datasets can serve as valuable resources for the NLP and GNN research
    community.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据贡献。我们提供了代码、预训练网络和丰富特征的开源访问权限。此外，鉴于常见资源库中缺少 Cora 和 PubMed 的原始文本数据（*例如*，PyG,
    DGL），我们已经收集并发布了这些数据集的 TAG 格式。进一步地，我们引入了新的 tape-arxiv23 引文图数据集，扩展至 GPT-3 知识截止日期之后的
    *即* 2021 年 9 月。这些数据集可以作为 NLP 和 GNN 研究社区的宝贵资源。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Shallow embedding pipeline for TAGs. In the context of learning representations
    on TAGs, a common approach involves combining graph-based learning with language
    modeling techniques. One prevalent strategy is to transform text attributes into
    shallow or hand-crafted features, such as skip-gram [[22](#bib.bibx22)] or BoW [[11](#bib.bibx11)]
    features. Detailed information is available in Table [5](#A3.T5 "Table 5 ‣ C.3
    Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). These engineered features can then be fed as inputs to a graph-based
    learning algorithm, such as a graph convolutional network (GCN) [[16](#bib.bibx16)],
    which learns embeddings capturing the graph structure while incorporating the
    extracted text features. Shallow embedding methods are widely used in the graph
    community due to their simplicity and computational efficiency, such as for designing
    GNN architectures [[30](#bib.bibx30), [2](#bib.bibx2), [29](#bib.bibx29), [43](#bib.bibx43)]
    or benchmarking graph learning [[38](#bib.bibx38), [13](#bib.bibx13)]. However,
    they may have limitations in capturing complex semantic relationships and fully
    leveraging the richness of text attributes, particularly in scenarios involving
    intricate semantic relationships and contextual information.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'TAGs 的浅层嵌入管道。在 TAGs 上学习表示的背景下，一种常见的方法是将图基学习与语言建模技术结合。一个流行的策略是将文本属性转换为浅层或手工制作的特征，例如
    skip-gram [[22](#bib.bibx22)] 或 BoW [[11](#bib.bibx11)] 特征。详细信息见表 [5](#A3.T5 "Table
    5 ‣ C.3 Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")。这些工程特征可以作为输入提供给基于图的学习算法，例如图卷积网络 (GCN) [[16](#bib.bibx16)]，GCN
    学习捕捉图结构的嵌入，同时结合提取的文本特征。浅层嵌入方法因其简单性和计算效率在图社区中得到广泛应用，例如用于设计 GNN 架构 [[30](#bib.bibx30),
    [2](#bib.bibx2), [29](#bib.bibx29), [43](#bib.bibx43)] 或基准测试图学习 [[38](#bib.bibx38),
    [13](#bib.bibx13)]。然而，它们可能在捕捉复杂语义关系和充分利用文本属性的丰富性方面存在局限性，特别是在涉及复杂语义关系和上下文信息的场景中。'
- en: LM-based pipeline for TAGs. To overcome the limitations of shallow embedding
    approaches, researchers have explored deep embedding techniques by fine-tuning
    pre-trained LMs, such as BERT [[5](#bib.bibx5)], to generate node embeddings that
    are specifically adapted to the domain and context of the TAGs. These deep embeddings
    effectively capture the semantic richness of text attributes, leading to improved
    performance on various TAG-related tasks. Integrating LM-based embeddings and
    graph-based learning can be done through different approaches. One approach is
    to use a cascaded architecture, where the node features are first encoded independently
    by the LMs, and then fed into GNN models. This representation paradigm has been
    widely adopted in subsequent works, such as TextGNN [[48](#bib.bibx48)], GIANT [[3](#bib.bibx3)],
    GPT-GNN [[14](#bib.bibx14)], SimTeg [[7](#bib.bibx7)], as well as in studies related
    to knowledge graphs [[39](#bib.bibx39), [44](#bib.bibx44)] and fact verification [[20](#bib.bibx20),
    [47](#bib.bibx47)] that are beyond the scope of this work. An alternative approach
    involves fusing text encoding and graph aggregation into an iterative workflow,
    enabling the model to refine both the text representations and the node embeddings
    simultaneously, such as Graphormer [[37](#bib.bibx37)], DRAGON [[41](#bib.bibx41)],
    and GLEM [[45](#bib.bibx45)], to name a few.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LM的TAG管道。为了克服浅层嵌入方法的局限性，研究人员通过微调预训练的LMs（如BERT [[5](#bib.bibx5)]）来生成特别适应TAG领域和上下文的节点嵌入。这些深度嵌入有效地捕捉了文本属性的语义丰富性，从而在各种TAG相关任务中表现出色。将基于LM的嵌入与基于图的学习集成可以通过不同的方法实现。一种方法是使用级联架构，其中节点特征首先由LMs独立编码，然后输入GNN模型。这种表示范式已在随后的工作中得到广泛采用，如TextGNN [[48](#bib.bibx48)]、GIANT [[3](#bib.bibx3)]、GPT-GNN [[14](#bib.bibx14)]、SimTeg [[7](#bib.bibx7)]，以及与知识图谱相关的研究 [[39](#bib.bibx39),
    [44](#bib.bibx44)]和事实验证 [[20](#bib.bibx20), [47](#bib.bibx47)]，这些都超出了本工作的范围。另一种方法涉及将文本编码和图聚合融合到一个迭代工作流程中，使模型能够同时优化文本表示和节点嵌入，例如Graphormer [[37](#bib.bibx37)]、DRAGON [[41](#bib.bibx41)]和GLEM [[45](#bib.bibx45)]等。
- en: LLM-based pipeline for TAGs. Incorporating LLMs into TAG tasks presents a promising
    frontier. LLMs such as ChatGPT [[1](#bib.bibx1)] by OpenAI, PaLM [[4](#bib.bibx4)]
    by Google, and LLaMA [[27](#bib.bibx27)] by Meta, have demonstrated their effectiveness
    across a spectrum of NLP tasks. However, their potential benefits for TAG tasks
    have yet to be fully explored. While some recent research efforts have sought
    to evaluate the capacity of LLMs in understanding graph-structured data and enhance
    their graph processing capabilities [[31](#bib.bibx31), [42](#bib.bibx42), [9](#bib.bibx9)],
    these endeavors, while valuable, may not be directly aligned with our specific
    focus on TAGs. By exploring LLM-based methods designed specifically for TAGs,
    we can unlock new possibilities for improving TAG prediction performance and advancing
    our understanding of text attributes within graph-based data. Notably, our initial
    attempt has already inspired further research endeavors in this direction.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的TAG管道。将LLM应用于TAG任务是一个有前景的领域。诸如OpenAI的ChatGPT [[1](#bib.bibx1)]、Google的PaLM [[4](#bib.bibx4)]和Meta的LLaMA [[27](#bib.bibx27)]等LLM已经在各种NLP任务中展示了其有效性。然而，它们在TAG任务中的潜在好处尚未得到充分探索。尽管一些最近的研究努力试图评估LLM在理解图结构数据方面的能力，并增强其图处理能力 [[31](#bib.bibx31),
    [42](#bib.bibx42), [9](#bib.bibx9)]，这些努力虽然有价值，但可能并不直接与我们对TAG的具体关注对接。通过探索专门针对TAG设计的LLM方法，我们可以开辟新的可能性，以提高TAG预测性能，并推动我们对图数据中文本属性的理解。值得注意的是，我们的初步尝试已经激发了该方向上的进一步研究工作。
- en: 3 Formalization
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 正式化
- en: In this section, we introduce notation and formalize some concepts related to
    language models, large language models, and graph neural networks for node classification
    on TAGs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了与语言模型、大型语言模型以及用于TAG节点分类的图神经网络相关的符号和一些概念的正式化。
- en: Text-attributed graphs. Formally, a TAG can be represented as $\mathcal{G}=(\mathcal{V},A,\{s_{n}\}_{n\in\mathcal{V}})$,
    where $\mathcal{V}$ is a set of $N$ nodes, $A\in\mathbb{R}^{N\times N}$ is the
    adjacency matrix, and $s_{n}\in\mathcal{D}^{L_{n}}$ is a sequential text associated
    with node $n\in\mathcal{V}$, with $\mathcal{D}$ as the words or tokens dictionary,
    and $L_{n}$ as the sequence length. In this paper, we investigate node classification
    on TAGs. Specifically, given some labeled nodes $\mathcal{L}\subset\mathcal{V}$
    , the goal is to predict the labels of the remaining unlabeled nodes $\mathcal{U}=\mathcal{V}\setminus\mathcal{L}$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 带文本属性的图。形式上，TAG 可以表示为 $\mathcal{G}=(\mathcal{V},A,\{s_{n}\}_{n\in\mathcal{V}})$，其中
    $\mathcal{V}$ 是 $N$ 个节点的集合，$A\in\mathbb{R}^{N\times N}$ 是邻接矩阵，而 $s_{n}\in\mathcal{D}^{L_{n}}$
    是与节点 $n\in\mathcal{V}$ 关联的序列文本，$\mathcal{D}$ 为词汇或标记词典，$L_{n}$ 为序列长度。在本文中，我们研究了在
    TAG 上的节点分类。具体来说，给定一些标记节点 $\mathcal{L}\subset\mathcal{V}$，目标是预测其余未标记节点 $\mathcal{U}=\mathcal{V}\setminus\mathcal{L}$
    的标签。
- en: 'Language models for text classification. In the context of TAGs, LMs can be
    employed to encode the text attributes associated with each node and learn a representation
    that captures the semantic meaning of the text. Let $s_{n}\in\mathcal{D}^{L_{n}}$
    denote the text attributes of node $n$, and LM be a pre-trained network, such
    as BERT [[5](#bib.bibx5)] or DeBERTa [[12](#bib.bibx12)]. Then, the text attributes
    of node $n$ can be encoded by applying the LM to $s_{n}$ as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本分类的语言模型。在 TAG 的背景下，可以使用 LMs 来编码与每个节点关联的文本属性，并学习一个捕捉文本语义的表示。设 $s_{n}\in\mathcal{D}^{L_{n}}$
    表示节点 $n$ 的文本属性，而 LM 是一个预训练的网络，例如 BERT [[5](#bib.bibx5)] 或 DeBERTa [[12](#bib.bibx12)]。然后，通过将
    LM 应用到 $s_{n}$ 上，可以对节点 $n$ 的文本属性进行编码，如下所示：
- en: '|  | $h_{n}=\textrm{LM}(s_{n})\in\mathbb{R}^{d},$ |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{n}=\textrm{LM}(s_{n})\in\mathbb{R}^{d},$ |  | (1) |'
- en: where $h_{n}$ is the output of the LM, and $d$ is the dimension of the output
    vector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{n}$ 是 LM 的输出，$d$ 是输出向量的维度。
- en: To perform node classification, the output is employed as input to a classifier,
    such as a logistic regression or a neural network. The goal is to learn a function
    that maps the encoded text attributes to the corresponding node labels.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行节点分类，输出作为分类器的输入使用，例如逻辑回归或神经网络。目标是学习一个函数，将编码的文本属性映射到相应的节点标签。
- en: Large language models and prompting. LLMs have introduced a new paradigm for
    task-adaptation known as “pre-train, prompt, and predict”, replacing the traditional
    “pre-train, fine-tune” procedure. In this paradigm, the LLM is first pre-trained
    on a large corpus of text data to learn general language representations. Then,
    rather than fine-tuning the model on task-specific labeled data, the model is
    prompted with a natural language prompt that specifies the task and context, and
    the model generates the output directly based on the prompt and the input [[19](#bib.bibx19)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型和提示。LLMs 引入了一种新的任务适应范式，称为“预训练、提示和预测”，取代了传统的“预训练、微调”过程。在这一范式中，LLM 首先在大量文本数据上进行预训练，以学习一般语言表示。然后，与其在任务特定的标注数据上进行微调不同，该模型通过自然语言提示来指定任务和上下文，并根据提示和输入直接生成输出 [[19](#bib.bibx19)]。
- en: 'The prompt can take various forms, such as a single sentence or a longer passage,
    and can include additional information or constraints to guide the model’s behavior.
    Let $\mathcal{M}$ be an LLM that takes as input a sequence of tokens $x=(x_{1},x_{2},\ldots,x_{q})$
    and produces as output a sequence of tokens $y=(y_{1},y_{2},\ldots,y_{m})$. The
    model $\mathcal{M}$ is typically trained to optimize a conditional probability
    distribution $p(y|x)$, which assigns a probability to each possible output sequence
    $y$ given $x$. To include a prompt $p$ with the input sequence $x$, we can concatenate
    them into a new sequence $\hat{x}=(p,x_{1},x_{2},\ldots,x_{q})$. We then use $\hat{x}$
    to compute the conditional probability distribution $p(y|\hat{x})$. Formally,
    the probability of the output sequence $y$ given $\hat{x}$ is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以采取各种形式，如单句或较长的段落，并可以包含额外的信息或约束来引导模型的行为。设 $\mathcal{M}$ 为一个 LLM，它接收一个标记序列
    $x=(x_{1},x_{2},\ldots,x_{q})$ 作为输入，并生成一个标记序列 $y=(y_{1},y_{2},\ldots,y_{m})$ 作为输出。模型
    $\mathcal{M}$ 通常被训练以优化条件概率分布 $p(y|x)$，该分布为给定 $x$ 的每个可能输出序列 $y$ 分配概率。为了将提示 $p$
    包含在输入序列 $x$ 中，我们可以将它们连接成一个新序列 $\hat{x}=(p,x_{1},x_{2},\ldots,x_{q})$。然后，我们使用 $\hat{x}$
    计算条件概率分布 $p(y|\hat{x})$。形式上，给定 $\hat{x}$ 的输出序列 $y$ 的概率为：
- en: '|  | $p(y&#124;\hat{x})=\prod_{i=1}^{m}p(y_{i}&#124;y_{<i},\hat{x}),$ |  |
    (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(y&#124;\hat{x})=\prod_{i=1}^{m}p(y_{i}&#124;y_{<i},\hat{x}),$ |  |
    (2) |'
- en: where $y_{<i}$ represents the prefix of sequence $y$ up to position $i-1$, and
    $p(y_{i}|y_{<i},\hat{x})$ represents the probability of generating token $y_{i}$
    given $y_{<i}$ and $\hat{x}$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{<i}$ 表示序列 $y$ 到位置 $i-1$ 的前缀，而 $p(y_{i}|y_{<i},\hat{x})$ 表示在给定 $y_{<i}$
    和 $\hat{x}$ 的情况下生成令牌 $y_{i}$ 的概率。
- en: 'Graph neural networks for node classification. In node classification, the
    task is to label each node in a graph based on its attributes and connections
    with other nodes. GNNs operate by aggregating information from a node’s neighbors,
    then updating the node’s representation based on the aggregated information. Formally,
    the $k$-th layer of a GNN is designed as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 节点分类的图神经网络。在节点分类中，任务是根据图中每个节点的属性及其与其他节点的连接来标记每个节点。GNN 通过从节点的邻居那里汇总信息，然后根据汇总的信息更新节点的表示来运作。形式上，GNN
    的第 $k$ 层设计为：
- en: '|  | $h_{i}^{k}=f^{k}(h_{i}^{k-1},\,\textrm{AGG}(\{h_{j}^{k-1}:j\in\mathcal{N}_{i}\}))\in\mathbb{R}^{d},$
    |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{i}^{k}=f^{k}(h_{i}^{k-1},\,\textrm{AGG}(\{h_{j}^{k-1}:j\in\mathcal{N}_{i}\}))\in\mathbb{R}^{d},$
    |  | (3) |'
- en: where $h_{i}^{k}\in\mathbb{R}^{d}$ is the representation of node $i$ at layer
    $k$ and $\mathcal{N}_{i}\subseteq\mathcal{V}$ is the set of neighbors of node
    $i$. Function $f^{k}$ is a differentiable function that updates the representation
    of a node based on its previous-layer representation and the aggregated information
    from its neighbors. This function is typically implemented as a neural network
    layer (*e.g.,* a multi-layer perceptron, or an attention mechanism). AGG is also
    a differentiable function (*e.g.,* sum, mean, etc.) that aggregates the representations
    of a node’s neighbors to produce a summary vector. The final representation is
    fed into a fully connected layer and a softmax function for class prediction.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{i}^{k}\in\mathbb{R}^{d}$ 是节点 $i$ 在第 $k$ 层的表示，$\mathcal{N}_{i}\subseteq\mathcal{V}$
    是节点 $i$ 的邻居集合。函数 $f^{k}$ 是一个可微分的函数，根据节点的前一层表示和来自邻居的汇总信息来更新节点的表示。该函数通常实现为神经网络层（*例如，*
    多层感知机或注意力机制）。AGG 也是一个可微分的函数（*例如，* 求和、均值等），它将节点邻居的表示汇总为一个摘要向量。最终表示被送入一个全连接层和一个
    softmax 函数用于类别预测。
- en: 4 Proposed Method
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 提议的方法
- en: 'In this section, we describe our LLM-based pipeline designed for node classification
    on TAGs. As illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"), the key idea is to leverage the LLM’s explanations as informative
    features for a downstream GNN. To achieve this goal, our method involves three
    main steps: 1) LLM-based prediction and explanation generation, 2) fine-tuning
    an LM interpreter, and 3) training a GNN.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分中，我们描述了为 TAGs 上的节点分类设计的基于 LLM 的管道。如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") 所示，关键思想是利用 LLM 的解释作为下游 GNN 的信息特征。为实现这一目标，我们的方法包括三个主要步骤：1）基于
    LLM 的预测和解释生成，2）微调 LM 解释器，3）训练 GNN。'
- en: 4.1 Generating Predictions and Explanations with LLMs
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 使用 LLM 生成预测和解释
- en: As outlined in the introduction, our approach is designed to be *LMaaS-compatible*
    given the scale of LLMs. This means that we aim to operate solely through API
    access to an LLM, using text-based input and output, without requiring fine-tuning
    the LLM or accessing its embeddings or logits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如引言中所述，我们的方法旨在与 *LMaaS 兼容*，考虑到 LLM 的规模。这意味着我们旨在通过对 LLM 的 API 访问进行操作，使用基于文本的输入和输出，而不需要对
    LLM 进行微调或访问其嵌入或 logits。
- en: 'In lieu of these requirements, our approach focuses on querying the LLM in
    an “open-ended” manner, *i.e.,* instructing the LLM to make multiple predictions
    and provide explanations for its decisions. By doing so, we aim to effectively
    extract its reasoning abilities and general knowledge in text format. These text-based
    outputs are then processed using an *LLM-to-LM interpreter* to create informative
    node features for downstream GNNs. With this objective, for each paper node $i\in\mathcal{V}$,
    we generate a prompt that includes the title and abstract of the paper, along
    with an open-ended question about the paper’s topic. The specific phrasing of
    the question part of the prompt is tailored to the task and dataset, as shown
    in Table [7](#A4.T7 "Table 7 ‣ D.3 Prompt Design ‣ Appendix D Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). The general structure of the prompt is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '为了满足这些要求，我们的方法专注于以“开放式”方式查询 LLM，*即，* 指导 LLM 做出多项预测并提供其决策的解释。通过这种方式，我们旨在有效提取其推理能力和一般知识，形式为文本。这些基于文本的输出随后通过*LLM-to-LM
    解释器*处理，以为下游 GNN 创建有信息量的节点特征。为此，对于每个论文节点 $i\in\mathcal{V}$，我们生成一个包含论文标题和摘要的提示，以及一个关于论文主题的开放式问题。提示中问题部分的具体措辞根据任务和数据集进行调整，如表 [7](#A4.T7
    "Table 7 ‣ D.3 Prompt Design ‣ Appendix D Experiments ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    所示。提示的一般结构如下：'
- en: '<svg   height="103.43" overflow="visible" version="1.1" width="600"><g transform="translate(0,103.43)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="75.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Abstract: [paper abstract] Title: [paper title] Question: [ask
    the model to predict one or more class labels of the paper, ordered from most
    to least likely, and provide explanations for its predictions] Answer:</foreignobject></g></g></svg>'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg   height="103.43" overflow="visible" version="1.1" width="600"><g transform="translate(0,103.43)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="75.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Abstract: [paper abstract] Title: [paper title] Question: [ask
    the model to predict one or more class labels of the paper, ordered from most
    to least likely, and provide explanations for its predictions] Answer:</foreignobject></g></g></svg>'
- en: 'Querying the LLM results in a ranked prediction list and a textual explanation
    for each paper:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查询 LLM 会得到一个排序的预测列表以及每篇论文的文本解释：
- en: <svg   height="58" overflow="visible" version="1.1" width="600"><g transform="translate(0,58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">(Ranked Predictions) [a ranked prediction list] (Explanations)
    [model-generated explanation for the predictions]</foreignobject></g></g></svg>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="58" overflow="visible" version="1.1" width="600"><g transform="translate(0,58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">(Ranked Predictions) [a ranked prediction list] (Explanations)
    [model-generated explanation for the predictions]</foreignobject></g></g></svg>
- en: These predictions and explanations serve as supplementary text attributes for
    the downstream LMs and GNN models, as detailed in the subsequent section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测和解释作为下游 LMs 和 GNN 模型的补充文本属性，如后续部分所述。
- en: 4.2 Fine-Tuning LM Interpreter and Node Feature Extraction
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 微调 LM 解释器和节点特征提取
- en: 'Original text and explanation features. Our initial step involves converting
    both the original text, *i.e.,* title and abstract, and the LLM’s explanations
    into fixed-length node features suitable for downstream GNN applications. Our
    approach is to fine-tune a smaller LM, which acts as an “interpreter” for the
    LLM’s text explanations. The rationale behind this step is that both the LLM and
    LM possess distinct advantages: the LLM has greater power and more knowledge but
    is less flexible, while the LM has less skills but is compact enough to be fine-tuned
    to a specific task. Thus, the LM serves to interpret the LLM’s output for the
    GNN, with the text explanation acting as an effective intermediate medium for
    communication. Then, fine-tuning the LM enables it to extract the most valuable
    and task-relevant features from the explanations.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本和解释特征。我们的初步步骤涉及将原始文本，即标题和摘要，以及 LLM 的解释转换为适用于下游 GNN 应用的固定长度节点特征。我们的方法是微调一个较小的
    LM，它作为 LLM 文本解释的“解释器”。这样做的理由是 LLM 和 LM 各有其优势：LLM 功能强大且知识丰富，但灵活性较差；而 LM 技能较少，但足够紧凑，可以针对特定任务进行微调。因此，LM
    作为 GNN 的 LLM 输出的解释器，而文本解释则作为有效的中介沟通工具。随后，微调 LM 使其能够从解释中提取最有价值和任务相关的特征。
- en: 'Concretely, we first fine-tune pre-trained LMs as follows: let $\textrm{LM}_{\textrm{orig}}$
    and $\textrm{LM}_{\textrm{expl}}$ be pre-trained LMs that take as input the original
    $s^{\textrm{orig}}$ and the explanation $s^{\textrm{expl}}$ text sequences, respectively.
    We obtain text embeddings for each source as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们首先按照以下步骤微调预训练的 LMs：设 $\textrm{LM}_{\textrm{orig}}$ 和 $\textrm{LM}_{\textrm{expl}}$
    为预训练的 LMs，分别输入原始 $s^{\textrm{orig}}$ 和解释 $s^{\textrm{expl}}$ 文本序列。我们为每个源获得文本嵌入：
- en: '|  | $\begin{split}h_{\textrm{orig}}=\textrm{LM}_{\textrm{orig}}(s^{\textrm{orig}})\in\mathbb{R}^{N\times
    d},\quad h_{\textrm{expl}}=\textrm{LM}_{\textrm{expl}}(s^{\textrm{expl}})\in\mathbb{R}^{N\times
    d}.\end{split}$ |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}h_{\textrm{orig}}=\textrm{LM}_{\textrm{orig}}(s^{\textrm{orig}})\in\mathbb{R}^{N\times
    d},\quad h_{\textrm{expl}}=\textrm{LM}_{\textrm{expl}}(s^{\textrm{expl}})\in\mathbb{R}^{N\times
    d}.\end{split}$ |  | (4) |'
- en: 'We further apply a Multi-Layer Perceptron (MLP) to the output of the LMs to
    obtain a $N\times C$-dimensional prediction matrix representing the LM’s predictions
    for each node (in logits):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步将多层感知器（MLP）应用于 LMs 的输出，以获得一个 $N\times C$ 维的预测矩阵，表示 LM 对每个节点的预测（以 logits
    形式）：
- en: '|  | $\begin{split}y_{\textrm{orig}}=\textrm{MLP}_{\textrm{orig}}(h_{\textrm{orig}})\in\mathbb{R}^{N\times
    C},\quad y_{\textrm{expl}}=\textrm{MLP}_{\textrm{expl}}(h_{\textrm{expl}})\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (5) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}y_{\textrm{orig}}=\textrm{MLP}_{\textrm{orig}}(h_{\textrm{orig}})\in\mathbb{R}^{N\times
    C},\quad y_{\textrm{expl}}=\textrm{MLP}_{\textrm{expl}}(h_{\textrm{expl}})\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (5) |'
- en: We fine-tune these LMs and MLPs using cross-entropy loss. Finally, the text
    embeddings from both sources, $h_{\textrm{orig}}$ and $h_{\textrm{expl}}$, are
    used as enriched features for training downstream GNNs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用交叉熵损失对这些LM和MLP进行微调。最后，来自两个来源的文本嵌入，$h_{\textrm{orig}}$和$h_{\textrm{expl}}$，被用作训练下游GNN的丰富特征。
- en: Ranked prediction features. In addition to the explanations, the LLM also provides
    a top-$k$ ranked prediction list for each node, which adds valuable information.
    To incorporate this knowledge, the top-$k$ predictions for node $i$ are first
    one-hot encoded as vectors $p_{i,1},\dots,p_{i,k}\in\mathbb{R}^{C}$. These vectors
    are subsequently concatenated into a $kC$-dimensional vector, followed by a linear
    transformation to produce a fixed-sized vector of length $d_{P}$. This process
    produces a prediction feature matrix as $h_{\textrm{pred}}\in\mathbb{R}^{N\times
    d_{P}}$ across all nodes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 排名预测特征。除了解释外，LLM还为每个节点提供了一个前$k$的排名预测列表，这增加了有价值的信息。为了融入这些知识，对节点$i$的前$k$预测首先被独热编码为向量$p_{i,1},\dots,p_{i,k}\in\mathbb{R}^{C}$。这些向量随后被连接成一个$kC$维的向量，然后通过线性变换生成一个长度为$d_{P}$的固定大小的向量。这个过程生成一个预测特征矩阵$h_{\textrm{pred}}\in\mathbb{R}^{N\times
    d_{P}}$，覆盖所有节点。
- en: In summary, we denote our features as $h_{\textrm{TAPE}}=\{h_{\textrm{orig}},h_{\textrm{expl}},h_{\textrm{pred}}\}$,
    where “TAPE” stands for Title, Abstract, Prediction and Explanation for each node.
    Importantly, our framework requires these features to remain frozen during downstream
    GNN training, ensuring that the LM and LLM do not participate in the GNN training
    process. This characteristic significantly enhances ease-of-use, modularity, and
    efficiency compared to approaches like GLEM, which involve an expensive iterative
    LM-GNN training process. As a result, we achieve a substantial speedup over GLEM,
    *e.g.,* a $2.88\times$ speedup on ogbn-arxiv even when utilizing the same backbone
    LM and GNN.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们将特征表示为$h_{\textrm{TAPE}}=\{h_{\textrm{orig}},h_{\textrm{expl}},h_{\textrm{pred}}\}$，其中“TAPE”代表每个节点的标题、摘要、预测和解释。重要的是，我们的框架要求这些特征在下游GNN训练期间保持冻结，确保LM和LLM不参与GNN训练过程。与像GLEM这样的需要昂贵的迭代LM-GNN训练过程的方法相比，这一特性显著提高了易用性、模块化和效率。因此，即使使用相同的基础LM和GNN，我们也能在ogbn-arxiv上实现$2.88\times$的显著加速，*例如*。
- en: 4.3 GNN Training on Enriched Features
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 在丰富特征上的GNN训练
- en: 'Our final step is to train a GNN using the $h_{\textrm{TAPE}}$ features. We
    aim to achieve this without increasing the memory requirements of the GNN or making
    any changes to its architecture. To accomplish this, we use an ensemble approach,
    as a simple and effective way of combining the features. Specifically, we independently
    train GNN models $f_{\textrm{orig}}$, $f_{\textrm{expl}}$, and $f_{\textrm{pred}}$
    on the features $h_{\textrm{orig}}$, $h_{\textrm{expl}}$, and $h_{\textrm{pred}}$,
    respectively, to predict the ground truth node labels:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是使用$h_{\textrm{TAPE}}$特征来训练GNN。我们的目标是在不增加GNN内存需求或对其架构进行任何更改的情况下实现这一点。为此，我们使用了集成方法，这是结合特征的一种简单有效的方法。具体而言，我们分别在特征$h_{\textrm{orig}}$、$h_{\textrm{expl}}$和$h_{\textrm{pred}}$上独立训练GNN模型$f_{\textrm{orig}}$、$f_{\textrm{expl}}$和$f_{\textrm{pred}}$，以预测真实的节点标签：
- en: '|  | $\begin{split}\hat{y}_{\textrm{orig}/\textrm{expl}/\textrm{pred}}=f_{\textrm{orig}/\textrm{expl}/\textrm{pred}}(h_{\textrm{orig}/\textrm{expl}/\textrm{pred}},A)\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\hat{y}_{\textrm{orig}/\textrm{expl}/\textrm{pred}}=f_{\textrm{orig}/\textrm{expl}/\textrm{pred}}(h_{\textrm{orig}/\textrm{expl}/\textrm{pred}},A)\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (6) |'
- en: 'We then fuse these predictions by taking their average:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过取这些预测的平均值来融合它们：
- en: '|  | $\hat{y}=\textrm{mean}(\hat{y}_{\textrm{orig}},\hat{y}_{\textrm{expl}},\hat{y}_{\textrm{pred}})\in\mathbb{R}^{N\times
    C}.$ |  | (7) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}=\textrm{mean}(\hat{y}_{\textrm{orig}},\hat{y}_{\textrm{expl}},\hat{y}_{\textrm{pred}})\in\mathbb{R}^{N\times
    C}.$ |  | (7) |'
- en: 'Each of the three models performs well individually as shown in Table [3](#S5.T3
    "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning"), which
    validates the effectiveness of simple averaging. This strategy enables us to capture
    complementary information from diverse input sources, ultimately enhancing the
    overall model’s performance.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [3](#S5.T3 "表3 ‣ 5.2 扩展性 ‣ 5 实验 ‣ 利用解释：LLM到LM的解释器提升文本属性图表示学习")所示，每个模型单独表现良好，这验证了简单平均的有效性。该策略使我们能够从不同的输入源捕捉互补信息，最终提升整体模型的性能。
- en: 4.4 Theoretical Analysis
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 理论分析
- en: 'In this section, we aim to demonstrate that explanations generated by an LLM
    can be valuable features for a smaller LM. Specifically, the explanations $E$
    are helpful if they possess *fidelity* in describing the LLM’s reasoning; and
    the LLM is *non-redundant*, utilizing information not used by the smaller LM.
    Let $E$ be the textual explanations generated by an LLM; $Z_{L}$ and $Z$ are embeddings
    from the LLM and smaller LM respectively, $y$ is the target and $H(\cdot|\cdot)$
    is the conditional entropy. The detailed proof is in Appendix [A](#A1 "Appendix
    A Theoretical Analysis ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced
    Text-Attributed Graph Representation Learning").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们旨在展示由LLM生成的解释对于较小的LM可以是有价值的特征。具体而言，若解释$E$在描述LLM的推理时具有*保真度*，且LLM是*非冗余*的，利用了较小LM未使用的信息，那么这些解释$E$就是有用的。设$E$为LLM生成的文本解释；$Z_{L}$和$Z$分别为LLM和较小LM的嵌入，$y$为目标，$H(\cdot|\cdot)$为条件熵。详细证明见附录 [A](#A1
    "附录 A 理论分析 ‣ 利用解释：LLM到LM的解释器提升文本属性图表示学习")。
- en: 'Theorem. Given the following conditions 1) *Fidelity*: $E$ is a good proxy
    for $Z_{L}$ such that $H(Z_{l}|E)=\epsilon$, with $\epsilon></math>, 2) *Non-redundancy*:
    <math   alttext=$ contains information not present in $Z$, expressed as $H(y|Z,Z_{L})=H(y|Z)-\epsilon^{\prime}$,
    with $\epsilon^{\prime}></math>. Then it follows that <math   alttext=$.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 定理。给定以下条件 1) *保真度*：$E$是$Z_{L}$的良好代理，使得$H(Z_{l}|E)=\epsilon$，其中$\epsilon></math>，2)
    *非冗余*：<math   alttext=$包含$Z$中未出现的信息，表示为$H(y|Z,Z_{L})=H(y|Z)-\epsilon^{\prime}$，其中$\epsilon^{\prime}></math>。那么可以推导出<math   alttext=$。
- en: 5 Experiments
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'We evaluate the proposed TAPE technique across five TAG datasets: Cora [[21](#bib.bibx21)],
    PubMed [[23](#bib.bibx23)], ogbn-arxiv, ogbn-products [[13](#bib.bibx13)], and
    tape-arxiv23. For Cora and PubMed, raw text data of the articles is unavailable
    in common graph libraries such as PyG and DGL. Hence, we collected and formatted
    the missing text data for these datasets in TAG format. Additionally, given the
    popularity of these datasets, their TAG version will be released publicly for
    reproducibility and new research projects. For ogbn-products, given its substantial
    scale of 2 million nodes and 61 million edges and considering our academic resource
    budget, we conducted experiments on a subgraph sample. Details can be found in
    Appendix [C](#A3 "Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在五个TAG数据集上评估了提出的TAPE技术：Cora [[21](#bib.bibx21)]，PubMed [[23](#bib.bibx23)]，ogbn-arxiv，ogbn-products [[13](#bib.bibx13)]，以及tape-arxiv23。对于Cora和PubMed，文章的原始文本数据在常见的图形库如PyG和DGL中不可用。因此，我们为这些数据集收集并格式化了缺失的文本数据，使用TAG格式。此外，鉴于这些数据集的普及，它们的TAG版本将会公开发布，以便于重现和新的研究项目。对于ogbn-products，考虑到其规模庞大（2百万节点和6100万边）以及我们的学术资源预算，我们在一个子图样本上进行了实验。详细信息见附录 [C](#A3
    "附录 C 数据集 ‣ 利用解释：LLM到LM的解释器提升文本属性图表示学习")。
- en: 5.1 Main Results
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主要结果
- en: 'Table 1: Node classification accuracy for the Cora, PubMed, ogbn-arxiv, ogbn-products
    and tape-arxiv23 datasets. $G\uparrow$ denotes the improvements of our approach
    over the same GNN trained on shallow features $h_{\textrm{shallow}}$; $L\uparrow$
    denotes the improvements of our approach over LM${}_{\textrm{finetune}}$. The
    results are averaged over four runs with different seeds, and the best results
    are in bold.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：Cora、PubMed、ogbn-arxiv、ogbn-products和tape-arxiv23数据集的节点分类准确率。$G\uparrow$表示我们的方法相对于在浅层特征$h_{\textrm{shallow}}$上训练的相同GNN的改进；$L\uparrow$表示我们的方法相对于LM${}_{\textrm{finetune}}$的改进。结果是基于四次不同种子的实验平均值，最佳结果用粗体字表示。
- en: '| Dataset | Method | GNN | LM | Ours |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | GNN | LM | 我们的方法 |'
- en: '| $h_{\textrm{shallow}}$ | $h_{\textrm{GIANT}}$ | $G\uparrow$ | LLM | LM${}_{\textrm{finetune}}$
    | $L\uparrow$ | $h_{\textrm{TAPE}}$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| $h_{\textrm{shallow}}$ | $h_{\textrm{GIANT}}$ | $G\uparrow$ | LLM | LM${}_{\textrm{finetune}}$
    | $L\uparrow$ | $h_{\textrm{TAPE}}$ |'
- en: '| Cora | MLP | 0.6388 ± 0.0213 | – | 26.58% | 0.6769 | 0.7606 ± 0.0378 | 6.31%
    | 0.8086 ± 0.0190 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Cora | MLP | 0.6388 ± 0.0213 | – | 26.58% | 0.6769 | 0.7606 ± 0.0378 | 6.31%
    | 0.8086 ± 0.0190 |'
- en: '| GCN | 0.8653 ± 0.0139 | – | 1.39% | 0.6769 | 0.7606 ± 0.0378 | 15.34% | 0.8773
    ± 0.0063 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.8653 ± 0.0139 | – | 1.39% | 0.6769 | 0.7606 ± 0.0378 | 15.34% | 0.8773
    ± 0.0063 |'
- en: '| SAGE | 0.8727 ± 0.0172 | – | 0.74% | 0.6769 | 0.7606 ± 0.0378 | 15.59% |
    0.8792 ± 0.0107 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.8727 ± 0.0172 | – | 0.74% | 0.6769 | 0.7606 ± 0.0378 | 15.59% |
    0.8792 ± 0.0107 |'
- en: '|  | RevGAT | 0.8736 ± 0.0208 | – | 2.22% | 0.6769 | 0.7606 ± 0.0378 | 17.41%
    | 0.8930 ± 0.0072 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.8736 ± 0.0208 | – | 2.22% | 0.6769 | 0.7606 ± 0.0378 | 17.41%
    | 0.8930 ± 0.0072 |'
- en: '| PubMed | MLP | 0.8635 ± 0.0032 | – | 9.70% | 0.9342 | 0.9494 ± 0.0046 | -0.22%
    | 0.9473 ± 0.0040 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | MLP | 0.8635 ± 0.0032 | – | 9.70% | 0.9342 | 0.9494 ± 0.0046 | -0.22%
    | 0.9473 ± 0.0040 |'
- en: '| GCN | 0.8533 ± 0.0101 | – | 10.07% | 0.9342 | 0.9494 ± 0.0046 | -1.07% |
    0.9392 ± 0.0023 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.8533 ± 0.0101 | – | 10.07% | 0.9342 | 0.9494 ± 0.0046 | -1.07% |
    0.9392 ± 0.0023 |'
- en: '| SAGE | 0.8688 ± 0.0086 | – | 9.69% | 0.9342 | 0.9494 ± 0.0046 | 0.38% | 0.9530
    ± 0.0035 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.8688 ± 0.0086 | – | 9.69% | 0.9342 | 0.9494 ± 0.0046 | 0.38% | 0.9530
    ± 0.0035 |'
- en: '|  | RevGAT | 0.8465 ± 0.0808 | – | 12.53% | 0.9342 | 0.9494 ± 0.0046 | 0.34%
    | 0.9526 ± 0.0032 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.8465 ± 0.0808 | – | 12.53% | 0.9342 | 0.9494 ± 0.0046 | 0.34%
    | 0.9526 ± 0.0032 |'
- en: '| ogbn-arxiv | MLP | 0.5336 ± 0.0038 | 0.7308 ± 0.0006 | 42.19% | 0.7350 |
    0.7361 ± 0.0004 | 3.07% | 0.7587 ± 0.0015 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | MLP | 0.5336 ± 0.0038 | 0.7308 ± 0.0006 | 42.19% | 0.7350 |
    0.7361 ± 0.0004 | 3.07% | 0.7587 ± 0.0015 |'
- en: '| GCN | 0.7182 ± 0.0027 | 0.7329 ± 0.0010 | 4.71% | 0.7350 | 0.7361 ± 0.0004
    | 2.16% | 0.7520 ± 0.0003 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.7182 ± 0.0027 | 0.7329 ± 0.0010 | 4.71% | 0.7350 | 0.7361 ± 0.0004
    | 2.16% | 0.7520 ± 0.0003 |'
- en: '| SAGE | 0.7171 ± 0.0017 | 0.7435 ± 0.0014 | 6.98% | 0.7350 | 0.7361 ± 0.0004
    | 4.22% | 0.7672 ± 0.0007 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.7171 ± 0.0017 | 0.7435 ± 0.0014 | 6.98% | 0.7350 | 0.7361 ± 0.0004
    | 4.22% | 0.7672 ± 0.0007 |'
- en: '|  | RevGAT | 0.7083 ± 0.0017 | 0.7590 ± 0.0019 | 9.42% | 0.7350 | 0.7361 ±
    0.0004 | 5.28% | 0.7750 ± 0.0012 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.7083 ± 0.0017 | 0.7590 ± 0.0019 | 9.42% | 0.7350 | 0.7361 ±
    0.0004 | 5.28% | 0.7750 ± 0.0012 |'
- en: '| ogbn-products | MLP | 0.5385 ± 0.0017 | – | 46.3% | 0.7440 | 0.7297 ± 0.0023
    | 7.96% | 0.7878 ± 0.0082 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-products | MLP | 0.5385 ± 0.0017 | – | 46.3% | 0.7440 | 0.7297 ± 0.0023
    | 7.96% | 0.7878 ± 0.0082 |'
- en: '| GCN | 0.7052 ± 0.0051 | – | 13.39% | 0.7440 | 0.7297 ± 0.0023 | 9.58% | 0.7996
    ± 0.0041 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.7052 ± 0.0051 | – | 13.39% | 0.7440 | 0.7297 ± 0.0023 | 9.58% | 0.7996
    ± 0.0041 |'
- en: '| SAGE | 0.6913 ± 0.0026 | – | 17.71% | 0.7440 | 0.7297 ± 0.0023 | 11.51% |
    0.8137 ± 0.0043 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.6913 ± 0.0026 | – | 17.71% | 0.7440 | 0.7297 ± 0.0023 | 11.51% |
    0.8137 ± 0.0043 |'
- en: '| RevGAT | 0.6964 ± 0.0017 | – | 18.24% | 0.7440 | 0.7297 ± 0.0023 | 12.84%
    | 0.8234 ± 0.0036 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| RevGAT | 0.6964 ± 0.0017 | – | 18.24% | 0.7440 | 0.7297 ± 0.0023 | 12.84%
    | 0.8234 ± 0.0036 |'
- en: '| tape-arxiv23 | MLP | 0.6209 ± 0.0031 | – | 28.83% | 0.7356 | 0.7358 ± 0.0006
    | 8.71% | 0.7999 ± 0.0037 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | MLP | 0.6209 ± 0.0031 | – | 28.83% | 0.7356 | 0.7358 ± 0.0006
    | 8.71% | 0.7999 ± 0.0037 |'
- en: '| GCN | 0.6521 ± 0.0099 | – | 20.03% | 0.7356 | 0.7358 ± 0.0006 | 6.37% | 0.7827
    ± 0.0037 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.6521 ± 0.0099 | – | 20.03% | 0.7356 | 0.7358 ± 0.0006 | 6.37% | 0.7827
    ± 0.0037 |'
- en: '| SAGE | 0.6571 ± 0.0042 | – | 22.05% | 0.7356 | 0.7358 ± 0.0006 | 9.00% |
    0.8020 ± 0.0024 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.6571 ± 0.0042 | – | 22.05% | 0.7356 | 0.7358 ± 0.0006 | 9.00% |
    0.8020 ± 0.0024 |'
- en: '| RevGAT | 0.6958 ± 0.0032 | – | 16.04% | 0.7356 | 0.7358 ± 0.0006 | 9.73%
    | 0.8074 ± 0.0021 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| RevGAT | 0.6958 ± 0.0032 | – | 16.04% | 0.7356 | 0.7358 ± 0.0006 | 9.73%
    | 0.8074 ± 0.0021 |'
- en: 'We conduct a comprehensive evaluation of our proposed TAPE method by comparing
    with existing GNN- and LM-based methods, with the results summarized in Table [1](#S5.T1
    "Table 1 ‣ 5.1 Main Results ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning"). For
    GNN comparisons, we consider three widely utilized architectures: GCN [[16](#bib.bibx16)],
    GraphSAGE [[25](#bib.bibx25)], and RevGAT [[17](#bib.bibx17)] along with a basic
    MLP baseline that operates independently off graph-related information. We explore
    three types of node features: 1) shallow features (detailed in Table [5](#A3.T5
    "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature Extraction ‣ Appendix
    C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")), denoted as $h_{\textrm{shallow}}$, 2) GIANT
    features [[3](#bib.bibx3)] $h_{\textrm{GIANT}}$, and 3) our proposed features
    $h_{\textrm{TAPE}}$, comprising $h_{\textrm{orig}}$, $h_{\textrm{expl}}$, and
    $h_{\textrm{pred}}$. For LM-based methods, we investigate two approaches: 1) fine-tuning
    DeBERTa on labeled nodes, denoted as $\textrm{LM}_{\textrm{finetune}}$, and 2)
    using zero-shot ChatGPT (gpt-3.5-turbo) with the same prompts as our approach,
    denoted as LLM.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过与现有的基于 GNN 和 LM 的方法进行比较，对我们提出的 TAPE 方法进行了全面评估，结果汇总在表[1](#S5.T1 "Table 1
    ‣ 5.1 Main Results ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")中。在 GNN 比较中，我们考虑了三种广泛使用的架构：GCN
    [[16](#bib.bibx16)]、GraphSAGE [[25](#bib.bibx25)] 和 RevGAT [[17](#bib.bibx17)]，以及一个独立于图相关信息的基本
    MLP 基线。我们探索了三种类型的节点特征：1) 浅层特征（详见表[5](#A3.T5 "Table 5 ‣ C.3 Shallow Embedding Methods
    for Node Feature Extraction ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning")），表示为
    $h_{\textrm{shallow}}$，2) GIANT 特征 [[3](#bib.bibx3)] $h_{\textrm{GIANT}}$，和 3)
    我们提出的特征 $h_{\textrm{TAPE}}$，包括 $h_{\textrm{orig}}$、$h_{\textrm{expl}}$ 和 $h_{\textrm{pred}}$。对于基于
    LM 的方法，我们研究了两种方法：1) 在标记节点上微调 DeBERTa，表示为 $\textrm{LM}_{\textrm{finetune}}$，以及
    2) 使用与我们的方法相同的提示的零-shot ChatGPT（gpt-3.5-turbo），表示为 LLM。'
- en: Our approach consistently outperforms other methods on all datasets and across
    all models, demonstrating its effectiveness in enhancing TAG representation learning.
    Among GNN-based methods, shallow features (*i.e.,* $h_{\textrm{shallow}}$) yields
    subpar performance, while LM-based features (*i.e.,* $h_{\textrm{GIANT}}$) improves
    results. In the case of LMs, fine-tuned LMs (*i.e.,* $\textrm{LM}_{\textrm{finetune}}$)
    also perform well. Our proposed novel features, leveraging the power of the LLM,
    further enhance the results.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法在所有数据集和所有模型上始终优于其他方法，展示了其在增强 TAG 表示学习中的有效性。在基于 GNN 的方法中，浅层特征 (*即，* $h_{\textrm{shallow}}$)
    性能欠佳，而基于 LM 的特征 (*即，* $h_{\textrm{GIANT}}$) 改善了结果。在 LMs 的情况下，微调的 LMs (*即，* $\textrm{LM}_{\textrm{finetune}}$)
    也表现良好。我们提出的新特征，利用了 LLM 的强大功能，进一步提升了结果。
- en: 'Additionally, we expanded our experimentation to include the open-source Llama2 [[27](#bib.bibx27)],
    demonstrating the feasibility of a cost-effective (free) alternative, as shown
    in Table [9](#A4.T9 "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). Furthermore, to address the potential label leakage
    concern in LLM, we took the initiative to construct a novel dataset, namely tape-arxiv23,
    comprising papers published in 2023 or later – well beyond the knowledge cutoff
    for GPT-3.5\. The results clearly illustrate strong generalization capabilities:
    while the LLM achieves 73.56% accuracy, our approach outperforms it with 84.23%.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们将实验扩展到包括开源的 Llama2 [[27](#bib.bibx27)]，展示了一个经济高效（免费）替代方案的可行性，如表[9](#A4.T9
    "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")所示。为了应对 LLM 中可能出现的标签泄露问题，我们主动构建了一个新的数据集，即 tape-arxiv23，其中包含
    2023 年或之后发表的论文——远超 GPT-3.5 的知识截止日期。结果清楚地展示了强大的泛化能力：虽然 LLM 的准确率为 73.56%，但我们的方法以
    84.23% 超越了它。'
- en: 5.2 Scalability
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 可扩展性
- en: 'Our proposed method surpasses not only pure LMs and shallow embedding pipelines
    but also the LM-based pipelines on the ogbn-arxiv dataset, achieving a superior
    balance between accuracy and training time, as illustrated in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning"). Specifically, our method
    achieved significantly higher accuracy than the SOTA GLEM [[45](#bib.bibx45)]
    method while utilizing the same LM and GNN models. Furthermore, our approach requires
    only $2.88\times$ less computation time. These efficiency improvements are attributed
    to our decoupled training approach for LMs and GNNs, avoiding the iterative (*i.e.,*
    multi-stage) approach used in GLEM. Moreover, unlike the iterative approach, our
    model allows for parallelizing the training of LM${}_{\textrm{orig}}$ and LM${}_{\textrm{expl}}$,
    further reducing overall training time when performed simultaneously.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方法不仅超越了纯语言模型（LMs）和浅层嵌入管道，还超越了基于语言模型的管道，在ogbn-arxiv数据集上实现了精度与训练时间之间的最佳平衡，如图[2](#S1.F2
    "图2 ‣ 1 引言 ‣ 利用解释：LLM到LM解释器以增强文本属性图表示学习")所示。具体而言，我们的方法在使用相同的语言模型和图神经网络模型的情况下，准确率显著高于SOTA
    GLEM [[45](#bib.bibx45)] 方法。此外，我们的方法所需的计算时间仅为原来的$2.88\times$。这些效率的提升归因于我们为语言模型和图神经网络采用的解耦训练方法，避免了GLEM中使用的迭代（*即*多阶段）方法。此外，与迭代方法不同，我们的模型允许同时并行训练LM${}_{\textrm{orig}}$和LM${}_{\textrm{expl}}$，进一步减少了整体训练时间。
- en: 'Table 2: Experiments on ogbn-arxiv dataset with DeBERTa-base [[12](#bib.bibx12)]
    as LM backbone and RevGAT [[17](#bib.bibx17)] as GNN backbone for comparison of
    different training paradigms of fusing LMs and GNNs, including our proposed method
    and the state-of-the-art GLEM method [[45](#bib.bibx45)]. The validation and test
    accuracy, number of parameters, maximum batch size (Max bsz.), and total training
    time on 4 NVIDIA RTX A5000 24GB GPUs are reported.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在ogbn-arxiv数据集上的实验，使用DeBERTa-base [[12](#bib.bibx12)] 作为语言模型骨干，RevGAT [[17](#bib.bibx17)]
    作为图神经网络骨干，对比了融合语言模型和图神经网络的不同训练范式，包括我们提出的方法和最先进的GLEM方法 [[45](#bib.bibx45)]。报告了在4台NVIDIA
    RTX A5000 24GB GPU上的验证和测试准确率、参数数量、最大批量大小（Max bsz.）和总训练时间。
- en: '| Method | Val acc. | Test acc. | Params. | Max bsz. | Total time |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 验证准确率 | 测试准确率 | 参数数量 | 最大批量大小 | 总时间 |'
- en: '| LM[orig] | 0.7503 ± 0.0008 | 0.7361 ± 0.0004 | 139,223,080 | 36 | 1.73h |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LM[orig] | 0.7503 ± 0.0008 | 0.7361 ± 0.0004 | 139,223,080 | 36 | 1.73小时
    |'
- en: '| GNN-$h_{\textrm{shallow}}$ | 0.7144 ± 0.0021 | 0.7083 ± 0.0017 | 427,728
    | all nodes | 1.80min |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GNN-$h_{\textrm{shallow}}$ | 0.7144 ± 0.0021 | 0.7083 ± 0.0017 | 427,728
    | 全部节点 | 1.80分钟 |'
- en: '| GLEM-G-Step | 0.7761 ± 0.0005 | 0.7657 ± 0.0029 | 1,837,136 | all nodes |
    9.18h |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GLEM-G-Step | 0.7761 ± 0.0005 | 0.7657 ± 0.0029 | 1,837,136 | 全部节点 | 9.18小时
    |'
- en: '| GLEM-L-Step | 0.7548 ± 0.0039 | 0.7495 ± 0.0037 | 138,632,488 | 36 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GLEM-L-Step | 0.7548 ± 0.0039 | 0.7495 ± 0.0037 | 138,632,488 | 36 |'
- en: '| TAPE-LM${}_{\textrm{orig}}$-Step | 0.7503 ± 0.0008 | 0.7361 ± 0.0004 | 139,223,080
    | 36 | 1.73h |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| TAPE-LM${}_{\textrm{orig}}$-Step | 0.7503 ± 0.0008 | 0.7361 ± 0.0004 | 139,223,080
    | 36 | 1.73小时 |'
- en: '| TAPE-LM${}_{\textrm{expl}}$-Step | 0.7506 ± 0.0008 | 0.7432 ± 0.0012 | 139,223,080
    | 36 | 1.40h |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| TAPE-LM${}_{\textrm{expl}}$-Step | 0.7506 ± 0.0008 | 0.7432 ± 0.0012 | 139,223,080
    | 36 | 1.40小时 |'
- en: '| TAPE-GNN-${h_{\textrm{TAPE}}}$-Step | 0.7785 ± 0.0016 | 0.7750 ± 0.0012 |
    1,837,136 | all nodes | 3.76min |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| TAPE-GNN-${h_{\textrm{TAPE}}}$-Step | 0.7785 ± 0.0016 | 0.7750 ± 0.0012 |
    1,837,136 | 全部节点 | 3.76分钟 |'
- en: 'Table 3: Ablation study on the ogbn-arxiv dataset, showing the effects of different
    node features on the performance. Node features include the original text attributes
    ($h_{\textrm{orig}}$), the explanations ($h_{\textrm{expl}}$ and predicted $h_{\textrm{pred}}$)
    generated by LLM, and the proposed method ($h_{\textrm{TAPE}}$). Results are averaged
    over 4 runs with 4 different seeds. The best results are in bold.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在ogbn-arxiv数据集上的消融研究，展示了不同节点特征对性能的影响。节点特征包括原始文本属性（$h_{\textrm{orig}}$）、由LLM生成的解释（$h_{\textrm{expl}}$和预测的$h_{\textrm{pred}}$），以及提出的方法（$h_{\textrm{TAPE}}$）。结果是对4次实验和4个不同种子的平均值。最佳结果用粗体显示。
- en: '| Method | $h_{\textrm{orig}}$ | $h_{\textrm{expl}}$ | $h_{\textrm{pred}}$
    | $h_{\textrm{TAPE}}$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $h_{\textrm{orig}}$ | $h_{\textrm{expl}}$ | $h_{\textrm{pred}}$ | $h_{\textrm{TAPE}}$
    |'
- en: '| GCN | val | 0.7624 ± 0.0007 | 0.7577 ± 0.0008 | 0.7531 ± 0.0006 | 0.7642
    ± 0.0003 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 验证 | 0.7624 ± 0.0007 | 0.7577 ± 0.0008 | 0.7531 ± 0.0006 | 0.7642 ±
    0.0003 |'
- en: '| test | 0.7498 ± 0.0018 | 0.7460 ± 0.0013 | 0.7400 ± 0.0007 | 0.7520 ± 0.0003
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 0.7498 ± 0.0018 | 0.7460 ± 0.0013 | 0.7400 ± 0.0007 | 0.7520 ± 0.0003
    |'
- en: '| SAGE | val | 0.7594 ± 0.0012 | 0.7631 ± 0.0016 | 0.7612 ± 0.0010 | 0.7768
    ± 0.0016 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 验证 | 0.7594 ± 0.0012 | 0.7631 ± 0.0016 | 0.7612 ± 0.0010 | 0.7768
    ± 0.0016 |'
- en: '| test | 0.7420 ± 0.0018 | 0.7535 ± 0.0023 | 0.7524 ± 0.0015 | 0.7672 ± 0.0007
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| test | 0.7420 ± 0.0018 | 0.7535 ± 0.0023 | 0.7524 ± 0.0015 | 0.7672 ± 0.0007
    |'
- en: '| RevGAT | val | 0.7588 ± 0.0021 | 0.7568 ± 0.0027 | 0.7550 ± 0.0015 | 0.7785
    ± 0.0016 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| RevGAT | val | 0.7588 ± 0.0021 | 0.7568 ± 0.0027 | 0.7550 ± 0.0015 | 0.7785
    ± 0.0016 |'
- en: '| test | 0.7504 ± 0.0020 | 0.7529 ± 0.0052 | 0.7519 ± 0.0031 | 0.7750 ± 0.0012
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| test | 0.7504 ± 0.0020 | 0.7529 ± 0.0052 | 0.7519 ± 0.0031 | 0.7750 ± 0.0012
    |'
- en: 5.3 Ablation Study
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融研究
- en: 'We perform an ablation study on the ogbn-arxiv dataset [[13](#bib.bibx13)]
    to evaluate the relevance of each module within our framework. The results are
    summarized in Table [3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning") and Figure [3](#A4.F3 "Figure 3 ‣ D.4 Detailed Ablation Study ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). Across all methods and for both the validation
    and test sets, our proposed method consistently outperforms the other settings.
    This underscores the value of incorporating explanations and predictions into
    node embeddings. Our case study (Figure [4](#A4.F4 "Figure 4 ‣ D.6 Case Study
    ‣ Appendix D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning")) suggests this improvement
    can be attributed to the concise and focused nature of LLM-generated explanations,
    as well as their reasoning ability and utilization of external knowledge.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在ogbn-arxiv数据集上进行了一项消融研究[[13](#bib.bibx13)]，以评估框架中各个模块的相关性。结果汇总在表格[3](#S5.T3
    "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning")和图[3](#A4.F3
    "Figure 3 ‣ D.4 Detailed Ablation Study ‣ Appendix D Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")中。所有方法中，无论是验证集还是测试集，我们提出的方法始终优于其他设置。这突显了将解释和预测融入节点嵌入中的价值。我们的案例研究（图[4](#A4.F4
    "Figure 4 ‣ D.6 Case Study ‣ Appendix D Experiments ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")）表明，这一改进可归因于LLM生成的解释的简洁性和专注性，以及其推理能力和外部知识的利用。'
- en: 6 Conclusion
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Given the increasing importance of integrating text and relationships, coupled
    with the emergence of LLMs, we foresee that TAG tasks will attract even more attention
    in the coming years. The convergence of LLMs and GNNs presents new opportunities
    for both research and industrial applications. As a pioneering work in this field,
    we believe that our contribution will serve as a strong baseline for future studies
    in this domain.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于文本和关系整合的重要性日益增加，加上LLM的出现，我们预见到TAG任务在未来几年将吸引更多关注。LLM和GNN的融合为研究和工业应用带来了新的机会。作为该领域的开创性工作，我们相信我们的贡献将为未来的研究提供强有力的基线。
- en: Limitation and future work. An inherent limitation of our approach lies in the
    requirement for customized prompts for each dataset. Currently, we rely on manually
    crafted prompts, which may not be optimal for the node classification task for
    every dataset. The efficacy of these prompts may fluctuate depending on the specific
    characteristics of the dataset and the specific task at hand. Future work can
    focus on automating the prompt generation process, exploring alternative prompt
    designs, and addressing the challenges of dynamic and evolving TAGs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 限制与未来工作。我们方法的一个固有限制在于每个数据集需要定制化提示。目前，我们依赖于手工制作的提示，这可能并不适用于每个数据集的节点分类任务。这些提示的有效性可能会根据数据集的具体特性和具体任务的不同而波动。未来的工作可以集中在自动化提示生成过程、探索替代提示设计，以及解决动态和不断发展的TAG挑战。
- en: Acknowledgment
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Xavier Bresson is supported by NUS Grant ID R-252-000-B97-133.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier Bresson得到了NUS Grant ID R-252-000-B97-133的支持。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tom Brown et al. “Language models are few-shot learners” In *Advances in
    neural information processing systems* 33, 2020, pp. 1877–1901'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Tom Brown等. “语言模型是少样本学习者” 见 *Advances in neural information processing
    systems* 33, 2020, 页1877–1901'
- en: '[2] Wei-Lin Chiang et al. “Cluster-gcn: An efficient algorithm for training
    deep and large graph convolutional networks” In *Proceedings of the 25th ACM SIGKDD
    international conference on knowledge discovery & data mining*, 2019, pp. 257–266'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Wei-Lin Chiang等. “Cluster-gcn: 一种高效的深度大规模图卷积网络训练算法” 见 *Proceedings of the
    25th ACM SIGKDD international conference on knowledge discovery & data mining*,
    2019, 页257–266'
- en: '[3] Eli Chien et al. “Node feature extraction by self-supervised multi-scale
    neighborhood prediction” In *arXiv preprint arXiv:2111.00064*, 2021'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Eli Chien 等. “通过自监督多尺度邻域预测进行节点特征提取” 发表在 *arXiv 预印本 arXiv:2111.00064*，2021'
- en: '[4] Aakanksha Chowdhery et al. “Palm: Scaling language modeling with pathways”
    In *arXiv preprint arXiv:2204.02311*, 2022'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Aakanksha Chowdhery 等. “Palm: 通过路径扩展语言建模” 发表在 *arXiv 预印本 arXiv:2204.02311*，2022'
- en: '[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova “Bert:
    Pre-training of deep bidirectional transformers for language understanding” In
    *arXiv preprint arXiv:1810.04805*, 2018'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova “BERT: 深度双向变换器的预训练用于语言理解”
    发表在 *arXiv 预印本 arXiv:1810.04805*，2018'
- en: '[6] Tu Anh Dinh, Jeroen den Boef, Joran Cornelisse and Paul Groth “E2EG: End-to-End
    Node Classification Using Graph Topology and Text-based Node Attributes” In *arXiv
    preprint arXiv:2208.04609*, 2022'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tu Anh Dinh、Jeroen den Boef、Joran Cornelisse 和 Paul Groth “E2EG: 基于图拓扑和文本节点属性的端到端节点分类”
    发表在 *arXiv 预印本 arXiv:2208.04609*，2022'
- en: '[7] Keyu Duan et al. “Simteg: A frustratingly simple approach improves textual
    graph learning” In *arXiv preprint arXiv:2308.02565*, 2023'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 端珂宇等. “Simteg: 一种令人沮丧的简单方法改进文本图学习” 发表在 *arXiv 预印本 arXiv:2308.02565*，2023'
- en: '[8] Matthias Fey and Jan Eric Lenssen “Fast graph representation learning with
    PyTorch Geometric” In *arXiv preprint arXiv:1903.02428*, 2019'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Matthias Fey 和 Jan Eric Lenssen “使用 PyTorch Geometric 进行快速图表示学习” 发表在 *arXiv
    预印本 arXiv:1903.02428*，2019'
- en: '[9] Jiayan Guo, Lun Du and Hengyu Liu “GPT4Graph: Can Large Language Models
    Understand Graph Structured Data? An Empirical Evaluation and Benchmarking” In
    *arXiv preprint arXiv:2305.15066*, 2023'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 郭家晏、杜伦和刘恒宇 “GPT4Graph: 大型语言模型能否理解图结构数据？实证评估与基准测试” 发表在 *arXiv 预印本 arXiv:2305.15066*，2023'
- en: '[10] Will Hamilton, Zhitao Ying and Jure Leskovec “Inductive representation
    learning on large graphs” In *Advances in neural information processing systems*
    30, 2017'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Will Hamilton、Zhitao Ying 和 Jure Leskovec “大规模图上的归纳表示学习” 发表在 *神经信息处理系统进展*
    30，2017'
- en: '[11] Zellig Harris “Distributional structure” In *The philosophy of linguistics*
    Oxford University Press, 1985'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Zellig Harris “分布结构” 发表在 *语言哲学* 牛津大学出版社，1985'
- en: '[12] Pengcheng He, Xiaodong Liu, Jianfeng Gao and Weizhu Chen “DEBERTA: DECODING-ENHANCED
    BERT WITH DISENTANGLED ATTENTION” In *International Conference on Learning Representations*,
    2021 URL: [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Pengcheng He、Xiaodong Liu、Jianfeng Gao 和 Weizhu Chen “DEBERTA: 解码增强 BERT
    的解耦注意力” 发表在 *国际学习表示会议*，2021 网址: [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD)'
- en: '[13] Weihua Hu et al. “Open graph benchmark: Datasets for machine learning
    on graphs” In *Advances in neural information processing systems* 33, 2020, pp.
    22118–22133'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Weihua Hu 等. “开放图基准: 图上机器学习的数据集” 发表在 *神经信息处理系统进展* 33，2020，第 22118–22133
    页'
- en: '[14] Ziniu Hu et al. “Gpt-gnn: Generative pre-training of graph neural networks”
    In *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, 2020, pp. 1857–1867'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 胡自牛等. “GPT-GNN: 图神经网络的生成预训练” 发表在 *第26届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*，2020，第
    1857–1867 页'
- en: '[15] Jared Kaplan et al. “Scaling laws for neural language models” In *arXiv
    preprint arXiv:2001.08361*, 2020'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jared Kaplan 等. “神经语言模型的缩放规律” 发表在 *arXiv 预印本 arXiv:2001.08361*，2020'
- en: '[16] Thomas N Kipf and Max Welling “Semi-supervised classification with graph
    convolutional networks” In *arXiv preprint arXiv:1609.02907*, 2016'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Thomas N Kipf 和 Max Welling “基于图卷积网络的半监督分类” 发表在 *arXiv 预印本 arXiv:1609.02907*，2016'
- en: '[17] Guohao Li, Matthias Müller, Bernard Ghanem and Vladlen Koltun “Training
    graph neural networks with 1000 layers” In *International conference on machine
    learning*, 2021, pp. 6437–6449 PMLR'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Guohao Li、Matthias Müller、Bernard Ghanem 和 Vladlen Koltun “用 1000 层训练图神经网络”
    发表在 *国际机器学习会议*，2021，第 6437–6449 页 PMLR'
- en: '[18] Stephanie Lin, Jacob Hilton and Owain Evans “Truthfulqa: Measuring how
    models mimic human falsehoods” In *arXiv preprint arXiv:2109.07958*, 2021'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Stephanie Lin、Jacob Hilton 和 Owain Evans “Truthfulqa: 测量模型如何模仿人类虚假信息”
    发表在 *arXiv 预印本 arXiv:2109.07958*，2021'
- en: '[19] Pengfei Liu et al. “Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing” In *ACM Computing Surveys*
    55.9 ACM New York, NY, 2023, pp. 1–35'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 刘鹏飞等. “预训练、提示和预测：自然语言处理中的提示方法系统综述” 发表在 *ACM Computing Surveys* 55.9 ACM
    纽约，NY，2023，第 1–35 页'
- en: '[20] Zhenghao Liu, Chenyan Xiong, Maosong Sun and Zhiyuan Liu “Fine-grained
    fact verification with kernel graph attention network” In *arXiv preprint arXiv:1910.09796*,
    2019'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Zhenghao Liu, Chenyan Xiong, Maosong Sun 和 Zhiyuan Liu “基于内核图注意力网络的细粒度事实验证”
    在*arXiv预印本 arXiv:1910.09796*, 2019'
- en: '[21] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie and Kristie Seymore
    “Automating the construction of internet portals with machine learning” In *Information
    Retrieval* 3 Springer, 2000, pp. 127–163'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie 和 Kristie Seymore
    “利用机器学习自动构建互联网门户” 在*信息检索* 3 Springer, 2000, 第127–163页'
- en: '[22] Tomas Mikolov et al. “Distributed representations of words and phrases
    and their compositionality” In *Advances in neural information processing systems*
    26, 2013'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Tomas Mikolov等. “词语和短语的分布式表示及其组合性” 在*神经信息处理系统进展* 26, 2013'
- en: '[23] Prithviraj Sen et al. “Collective classification in network data” In *AI
    magazine* 29.3, 2008, pp. 93–93'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Prithviraj Sen等. “网络数据中的集体分类” 在*AI杂志* 29.3, 2008, 第93页'
- en: '[24] Aarohi Srivastava et al. “Beyond the imitation game: Quantifying and extrapolating
    the capabilities of language models” In *arXiv preprint arXiv:2206.04615*, 2022'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Aarohi Srivastava等. “超越模仿游戏：量化和推断语言模型的能力” 在*arXiv预印本 arXiv:2206.04615*,
    2022'
- en: '[25] Chuxiong Sun, Hongming Gu and Jie Hu “Scalable and adaptive graph neural
    networks with self-label-enhanced training” In *arXiv preprint arXiv:2104.09376*,
    2021'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Chuxiong Sun, Hongming Gu 和 Jie Hu “具有自我标签增强训练的可扩展和自适应图神经网络” 在*arXiv预印本
    arXiv:2104.09376*, 2021'
- en: '[26] Tianxiang Sun et al. “Black-box tuning for language-model-as-a-service”
    In *International Conference on Machine Learning*, 2022, pp. 20841–20855 PMLR'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Tianxiang Sun等. “语言模型即服务的黑箱调优” 在*国际机器学习会议*, 2022, 第20841–20855页 PMLR'
- en: '[27] Hugo Touvron et al. “Llama: Open and efficient foundation language models”
    In *arXiv preprint arXiv:2302.13971*, 2023'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Hugo Touvron等. “Llama: 开放且高效的基础语言模型” 在*arXiv预印本 arXiv:2302.13971*, 2023'
- en: '[28] Maria Tsimpoukelli et al. “Multimodal few-shot learning with frozen language
    models” In *Advances in Neural Information Processing Systems* 34, 2021, pp. 200–212'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Maria Tsimpoukelli等. “冻结语言模型的多模态少样本学习” 在*神经信息处理系统进展* 34, 2021, 第200–212页'
- en: '[29] Petar Velickovic et al. “Deep graph infomax.” In *ICLR (Poster)* 2.3,
    2019, pp. 4'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Petar Velickovic等. “深度图信息最大化。” 在*ICLR (海报)* 2.3, 2019, 第4页'
- en: '[30] Petar Veličković et al. “Graph attention networks” In *arXiv preprint
    arXiv:1710.10903*, 2017'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Petar Veličković等. “图注意力网络” 在*arXiv预印本 arXiv:1710.10903*, 2017'
- en: '[31] Heng Wang et al. “Can Language Models Solve Graph Problems in Natural
    Language?” In *arXiv preprint arXiv:2305.10037*, 2023'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Heng Wang等. “语言模型能否解决自然语言中的图问题？” 在*arXiv预印本 arXiv:2305.10037*, 2023'
- en: '[32] Kuansan Wang et al. “Microsoft academic graph: When experts are not enough”
    In *Quantitative Science Studies* 1.1 MIT Press One Rogers Street, Cambridge,
    MA 02142-1209, USA journals-info …, 2020, pp. 396–413'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Kuansan Wang等. “微软学术图谱：当专家不够时” 在*定量科学研究* 1.1 MIT出版社, 2020, 第396–413页'
- en: '[33] Minjie Wang et al. “Deep graph library: A graph-centric, highly-performant
    package for graph neural networks” In *arXiv preprint arXiv:1909.01315*, 2019'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Minjie Wang等. “深度图书馆：面向图神经网络的图中心、高性能软件包” 在*arXiv预印本 arXiv:1909.01315*,
    2019'
- en: '[34] Suhang Wang, Jiliang Tang, Charu Aggarwal and Huan Liu “Linked document
    embedding for classification” In *Proceedings of the 25th ACM international on
    conference on information and knowledge management*, 2016, pp. 115–124'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Suhang Wang, Jiliang Tang, Charu Aggarwal 和 Huan Liu “用于分类的链接文档嵌入” 在*第25届ACM国际信息与知识管理会议论文集*,
    2016, 第115–124页'
- en: '[35] Jason Wei et al. “Emergent abilities of large language models” In *arXiv
    preprint arXiv:2206.07682*, 2022'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Jason Wei等. “大型语言模型的突现能力” 在*arXiv预印本 arXiv:2206.07682*, 2022'
- en: '[36] Cheng Yang et al. “Network representation learning with rich text information.”
    In *IJCAI* 2015, 2015, pp. 2111–2117'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Cheng Yang等. “具有丰富文本信息的网络表示学习。” 在*IJCAI* 2015, 2015, 第2111–2117页'
- en: '[37] Junhan Yang et al. “GraphFormers: GNN-nested transformers for representation
    learning on textual graph” In *Advances in Neural Information Processing Systems*
    34, 2021, pp. 28798–28810'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Junhan Yang等. “GraphFormers: 嵌套变换器的GNN用于文本图的表示学习” 在*神经信息处理系统进展* 34, 2021,
    第28798–28810页'
- en: '[38] Zhilin Yang, William Cohen and Ruslan Salakhudinov “Revisiting semi-supervised
    learning with graph embeddings” In *International conference on machine learning*,
    2016, pp. 40–48 PMLR'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Zhilin Yang, William Cohen 和 Ruslan Salakhudinov “重新审视图嵌入的半监督学习” 在*国际机器学习会议*,
    2016, 第40–48页 PMLR'
- en: '[39] Michihiro Yasunaga et al. “QA-GNN: Reasoning with language models and
    knowledge graphs for question answering” In *arXiv preprint arXiv:2104.06378*,
    2021'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Michihiro Yasunaga 等 “QA-GNN: Reasoning with language models and knowledge
    graphs for question answering” 见 *arXiv preprint arXiv:2104.06378*，2021'
- en: '[40] Michihiro Yasunaga et al. “Graph-based neural multi-document summarization”
    In *arXiv preprint arXiv:1706.06681*, 2017'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Michihiro Yasunaga 等 “Graph-based neural multi-document summarization”
    见 *arXiv preprint arXiv:1706.06681*，2017'
- en: '[41] Michihiro Yasunaga et al. “Deep bidirectional language-knowledge graph
    pretraining” In *Advances in Neural Information Processing Systems* 35, 2022,
    pp. 37309–37323'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Michihiro Yasunaga 等 “Deep bidirectional language-knowledge graph pretraining”
    见 *Advances in Neural Information Processing Systems* 35，2022，第37309–37323页'
- en: '[42] Jiawei Zhang “Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT” In *arXiv preprint arXiv:2304.11116*, 2023'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Jiawei Zhang “Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT” 见 *arXiv preprint arXiv:2304.11116*，2023'
- en: '[43] Shichang Zhang, Yozen Liu, Yizhou Sun and Neil Shah “Graph-less neural
    networks: Teaching old mlps new tricks via distillation” In *arXiv preprint arXiv:2110.08727*,
    2021'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Shichang Zhang, Yozen Liu, Yizhou Sun 和 Neil Shah “Graph-less neural networks:
    Teaching old mlps new tricks via distillation” 见 *arXiv preprint arXiv:2110.08727*，2021'
- en: '[44] Xikun Zhang et al. “Greaselm: Graph reasoning enhanced language models”
    In *International conference on learning representations*, 2022'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Xikun Zhang 等 “Greaselm: Graph reasoning enhanced language models” 见 *International
    conference on learning representations*，2022'
- en: '[45] Jianan Zhao et al. “Learning on Large-scale Text-attributed Graphs via
    Variational Inference” In *arXiv preprint arXiv:2210.14709*, 2022'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Jianan Zhao 等 “Learning on Large-scale Text-attributed Graphs via Variational
    Inference” 见 *arXiv preprint arXiv:2210.14709*，2022'
- en: '[46] Zihao Zhao et al. “Calibrate before use: Improving few-shot performance
    of language models” In *International Conference on Machine Learning*, 2021, pp.
    12697–12706 PMLR'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Zihao Zhao 等 “Calibrate before use: Improving few-shot performance of
    language models” 见 *International Conference on Machine Learning*，2021，第12697–12706页
    PMLR'
- en: '[47] Jie Zhou et al. “GEAR: Graph-based evidence aggregating and reasoning
    for fact verification” In *arXiv preprint arXiv:1908.01843*, 2019'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Jie Zhou 等 “GEAR: Graph-based evidence aggregating and reasoning for fact
    verification” 见 *arXiv preprint arXiv:1908.01843*，2019'
- en: '[48] Jason Zhu et al. “Textgnn: Improving text encoder via graph neural network
    in sponsored search” In *Proceedings of the Web Conference 2021*, 2021, pp. 2848–2857'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Jason Zhu 等 “Textgnn: Improving text encoder via graph neural network
    in sponsored search” 见 *Proceedings of the Web Conference 2021*，2021，第2848–2857页'
- en: Appendix A Theoretical Analysis
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 理论分析
- en: 'In this section, we aim to demonstrate that explanations generated by an LLM
    can provide valuable features for another model (such as a smaller LM). This is
    true under two key conditions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们旨在演示LLM生成的解释可以为另一个模型（如较小的语言模型）提供有价值的特征。这在两个关键条件下是成立的：
- en: '1.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*Fidelity:* The explanations effectively represent LLM’s reasoning over the
    raw text, containing most of the information from the LLM’s hidden state.'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*可信度：* 解释有效地代表了LLM对原始文本的推理，包含了LLM隐藏状态中的大部分信息。'
- en: '2.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*Non-redundancy:* The LLM possesses unique knowledge not captured by another
    model.'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*非冗余性：* LLM拥有其他模型未捕捉到的独特知识。'
- en: 'We formulate our theorem as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定理表述如下：
- en: Theorem 1.
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理1。
- en: 'Given the following conditions:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下条件下：
- en: '1) Fidelity: $E$ is a good proxy for $Z_{L}$ such that'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 可信度：$E$是$Z_{L}$的良好代理，如下所示
- en: '|  | $$H(Z_{l}&#124;E)=\epsilon,{\quad\epsilon></math> |  | (8) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $$H(Z_{l}|E)=\epsilon,{\quad\epsilon<$$ |  | (8) |'
- en: '2) Non-redundancy: $Z_{L}$ contains information not present in $Z$, expressed
    as'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 非冗余性：$Z_{L}$包含$Z$中不存在的信息，如下所示
- en: '|  | <math   alttext=$$ |  | (9) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | <math   alttext=$$ |  | (9) |'
- en: 'Then, it follows that:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，得出：
- en: '|  | $H(y&#124;Z,E)<H(y&#124;Z)$ |  | (10) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y|Z,E)<H(y|Z)$ |  | (10) |'
- en: where $E$ is textual explanations generated by an LLM, $Z_{L}$ is the vectorial
    representation of the raw text modeled by the LLM, $Z$ is the vectorial representation
    of the raw text modeled by the other model, $y$ is the target and $H(\cdot|\cdot)$
    is the conditional entropy.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$E$是LLM生成的文本解释，$Z_{L}$是LLM建模的原始文本的向量表示，$Z$是其他模型建模的原始文本的向量表示，$y$是目标，$H(\cdot|\cdot)$是条件熵。
- en: Proof.
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We aim to demonstrate that the conditional entropy of $y$ given both $Z$ and
    $E$, denoted as $H(y|Z,E)$, is less than the conditional entropy of $y$ given
    only $Z$, denoted as $H(y|Z)$.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在证明，给定$Z$和$E$的条件下的$y$的条件熵$H(y|Z,E)$，小于仅给定$Z$的条件下的$y$的条件熵$H(y|Z)$。
- en: 'Starting with:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下开始：
- en: '|  | $H(y&#124;Z,E)$ |  | (11) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y|Z,E)$ |  | (11) |'
- en: 'We apply the properties of entropy to decompose this expression into two components:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用熵的性质将该表达式分解为两个部分：
- en: '|  | $H(y&#124;Z,E)=H(y&#124;Z,Z_{L},E)+I(y;Z_{L}&#124;Z,E)$ |  | (12) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y\mid Z,E)=H(y\mid Z,Z_{L},E)+I(y;Z_{L}\mid Z,E)$ |  | (12) |'
- en: 'Now, we utilize the following upper bound of conditional mutual information:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们利用以下条件互信息的上界：
- en: '|  | $\displaystyle I(y;Z_{L}&#124;Z,E)$ | $\displaystyle=H(Z_{L}&#124;Z,E)-H(Z_{L}&#124;y,Z,E)$
    |  | (13) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(y;Z_{L}\mid Z,E)$ | $\displaystyle=H(Z_{L}\mid Z,E)-H(Z_{L}\mid
    y,Z,E)$ |  | (13) |'
- en: '|  |  | $\displaystyle\leq H(Z_{L}&#124;Z,E)$ |  | (14) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq H(Z_{L}\mid Z,E)$ |  | (14) |'
- en: where the first line follows from the definition of mutual information, and
    the second line follows from the nonnegativity of conditional entropy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一行来源于互信息的定义，第二行来源于条件熵的非负性。
- en: 'Substituting ([14](#A1.E14 "In Proof. ‣ Appendix A Theoretical Analysis ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")) into ([12](#A1.E12 "In Proof. ‣ Appendix A Theoretical Analysis ‣
    Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph
    Representation Learning")), we rewrite the conditional entropy as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 将 ([14](#A1.E14 "在证明中。‣ 附录 A 理论分析 ‣ 利用解释：LLM-to-LM 解释器用于增强文本属性图表示学习")) 代入 ([12](#A1.E12
    "在证明中。‣ 附录 A 理论分析 ‣ 利用解释：LLM-to-LM 解释器用于增强文本属性图表示学习"))，我们将条件熵重写为：
- en: '|  | $H(y&#124;Z,E)\leq H(y&#124;Z,Z_{L},E)+H(Z_{L}&#124;Z,E)$ |  | (15) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y\mid Z,E)\leq H(y\mid Z,Z_{L},E)+H(Z_{L}\mid Z,E)$ |  | (15) |'
- en: 'Since conditional entropy increases when conditioning on fewer variables, we
    further have:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于条件熵在条件变量减少时会增加，我们进一步得到：
- en: '|  | $H(y&#124;Z,Z_{L},E)+H(Z_{L}&#124;Z,E)\leq H(y&#124;Z,Z_{L})+H(Z_{L}&#124;E)$
    |  | (16) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y\mid Z,Z_{L},E)+H(Z_{L}\mid Z,E)\leq H(y\mid Z,Z_{L})+H(Z_{L}\mid
    E)$ |  | (16) |'
- en: 'Applying the "Fidelity" and "Non-redundancy" conditions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 应用“准确性”和“非冗余性”条件：
- en: '|  | $H(y&#124;Z,Z_{L})+H(Z_{L}&#124;E)\leq H(y&#124;Z)-\epsilon^{\prime}+\epsilon$
    |  | (17) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y\mid Z,Z_{L})+H(Z_{L}\mid E)\leq H(y\mid Z)-\epsilon^{\prime}+\epsilon$
    |  | (17) |'
- en: 'Finally, as <math   alttext="\epsilon^{\prime}></math>, we have:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当 <math   alttext="\epsilon^{\prime}></math> 时，我们有：
- en: '|  | $H(y&#124;Z)-\epsilon^{\prime}+\epsilon<H(y&#124;Z)$ |  | (18) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y\mid Z)-\epsilon^{\prime}+\epsilon<H(y\mid Z)$ |  | (18) |'
- en: 'Consequently, we have proven that:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们证明了：
- en: '|  | $H(y&#124;Z,E)<H(y&#124;Z)$ |  | (19) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(y\mid Z,E)<H(y\mid Z)$ |  | (19) |'
- en: This completes the proof. ∎
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 证明完毕。∎
- en: Appendix B Addressing Label Leakage Concerns with a New Dataset
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 使用新数据集解决标签泄漏问题
- en: GPT-3.5’s training data might include certain arXiv papers, given its comprehensive
    ingestion of textual content from the internet. However, the precise composition
    of these arXiv papers within GPT-3.5’s training remains undisclosed, rendering
    it infeasible to definitively identify their inclusion. It is essential to emphasize
    that the challenge of label leakage is widespread and affects various language
    model benchmarks, such as the prominent BIG-bench [[24](#bib.bibx24)] and TruthfulQA [[18](#bib.bibx18)].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 的训练数据可能包括某些 arXiv 论文，因为它全面吸收了来自互联网的文本内容。然而，关于这些 arXiv 论文在 GPT-3.5 训练中的具体组成尚未披露，因此无法明确识别其包含情况。需要强调的是，标签泄漏问题广泛存在，影响了各种语言模型基准测试，例如著名的
    BIG-bench [[24](#bib.bibx24)] 和 TruthfulQA [[18](#bib.bibx18)]。
- en: To address this concern, we created a novel dataset tape-arxiv23 for our experiments.
    We made sure that this dataset only included papers published in 2023 or later,
    which is well beyond the knowledge cutoff for GPT-3.5, as it was launched in November
    2022\. The creation of this new dataset was meticulously executed. We collected
    all cs.ArXiv papers published from January 2023 to September 2023 from the arXiv
    daily repository ²²2[https://arxiv.org/](https://arxiv.org/). We then utilized
    the Semantic Scholar API ³³3[https://www.semanticscholar.org/product/api](https://www.semanticscholar.org/product/api)
    to retrieve citation relationships. This process yielded a comprehensive graph
    containing 46,198 papers and 78,548 connections.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，我们创建了一个新数据集 tape-arxiv23 用于实验。我们确保这个数据集仅包含2023年或之后发布的论文，这远远超过了GPT-3.5的知识截止日期，因为GPT-3.5是在2022年11月推出的。这个新数据集的创建经过精心执行。我们从arXiv每日存储库收集了从2023年1月到2023年9月发布的所有cs.ArXiv论文 ²²2[https://arxiv.org/](https://arxiv.org/)。然后，我们利用Semantic
    Scholar API ³³3[https://www.semanticscholar.org/product/api](https://www.semanticscholar.org/product/api)
    检索引文关系。这一过程生成了一个包含46,198篇论文和78,548条连接的综合图谱。
- en: Appendix C Dataset
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 数据集
- en: 'We conduct experiments on five TAGs – Cora [[21](#bib.bibx21)], PubMed [[23](#bib.bibx23)],
    ogbn-arxiv, ogbn-products [[13](#bib.bibx13)], and tape-arxiv23. For Cora and
    PubMed, we collected the raw text data since they are not available in common
    repositories like PyG and DGL. For ogbn-products, given its substantial scale
    of 2 million nodes and 61 million edges, we have employed a node sampling strategy
    to obtain a subgraph containing 54k nodes and 74k edges. Additionally, we introduced
    the tape-arxiv23 citation graph dataset, extending beyond the knowledge cutoff
    of GPT-3\. This dataset serves as a valuable resource for the research community.
    Table [4](#A3.T4 "Table 4 ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning") provides
    a summary of the dataset statistics.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在五个TAG数据集上进行了实验——Cora [[21](#bib.bibx21)]、PubMed [[23](#bib.bibx23)]、ogbn-arxiv、ogbn-products [[13](#bib.bibx13)]和tape-arxiv23。对于Cora和PubMed，由于它们在PyG和DGL等常见存储库中不可用，我们收集了原始文本数据。对于ogbn-products，鉴于其规模庞大，包含200万个节点和6100万个边，我们采用了节点采样策略，以获取一个包含54k节点和74k边的子图。此外，我们引入了tape-arxiv23引文图数据集，这个数据集超出了GPT-3的知识截止日期。该数据集为研究社区提供了宝贵的资源。表[4](#A3.T4
    "Table 4 ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")提供了数据集统计的汇总。'
- en: 'Table 4: Statistics of the TAG datasets'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：TAG数据集的统计信息
- en: '| Dataset | #Nodes | #Edges | Task | Metric | Augmentation |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | #节点 | #边 | 任务 | 指标 | 增强 |'
- en: '| Cora | 2,708 | 5,429 | 7-class classif. | Accuracy | ✓ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Cora | 2,708 | 5,429 | 7类分类 | 准确率 | ✓ |'
- en: '| Pubmed | 19,717 | 44,338 | 3-class classif. | Accuracy | ✓ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Pubmed | 19,717 | 44,338 | 3类分类 | 准确率 | ✓ |'
- en: '| ogbn-arxiv | 169,343 | 1,166,243 | 40-class classif. | Accuracy |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 169,343 | 1,166,243 | 40类分类 | 准确率 |  |'
- en: '| ogbn-products (subset) | 54,025 | 74,420 | 47-class classif. | Accuracy |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-products (子集) | 54,025 | 74,420 | 47类分类 | 准确率 |  |'
- en: '| tape-arxiv23 | 46,198 | 78,548 | 40-class-classif. | Accuracy | ✓ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | 46,198 | 78,548 | 40类分类 | 准确率 | ✓ |'
- en: C.1 Dataset Description
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 数据集描述
- en: Cora [[21](#bib.bibx21)]. The Cora dataset comprises 2,708 scientific publications
    classified into one of seven classes – case based, genetic algorithms, neural
    networks, probabilistic methods, reinforcement learning, rule learning, and theory,
    with a citation network consisting of 5,429 links. The papers were selected in
    a way such that in the final corpus every paper cites or is cited by at least
    one other paper.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Cora [[21](#bib.bibx21)]。Cora数据集包含2708篇科学出版物，这些出版物被分类为七类之一——基于案例的、遗传算法、神经网络、概率方法、强化学习、规则学习和理论，引用网络包含5429条链接。论文的选择方式使得在最终的语料库中，每篇论文都至少引用或被引用了一篇其他论文。
- en: PubMed [[23](#bib.bibx23)]. The Pubmed dataset consists of 19,717 scientific
    publications from PubMed database pertaining to diabetes classified into one of
    three classes – Experimental induced diabetes, Type 1 diabetes, and Type 2 diabetes.
    The citation network consists of 44,338 links.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: PubMed [[23](#bib.bibx23)]。PubMed数据集包含19,717篇来自PubMed数据库的与糖尿病相关的科学出版物，这些出版物被分类为三类之一——实验性诱发糖尿病、1型糖尿病和2型糖尿病。引用网络包含44,338条链接。
- en: ogbn-arxiv [[13](#bib.bibx13)]. The ogbn-arxiv dataset is a directed graph that
    represents the citation network between all computer science arXiv papers indexed
    by MAG [[32](#bib.bibx32)]. Each node is an arXiv paper, and each directed edge
    indicates that one paper cites another one. The task is to predict the 40 subject
    areas of arXiv CS papers, *e.g.,*, cs.AI, cs.LG, and cs.OS, which are manually
    determined (*i.e.,* labeled) by the paper’s authors and arXiv moderators.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ogbn-arxiv [[13](#bib.bibx13)]。ogbn-arxiv数据集是一个有向图，表示所有计算机科学arXiv论文之间的引用网络，这些论文由MAG [[32](#bib.bibx32)]索引。每个节点都是一篇arXiv论文，每个有向边表示一篇论文引用了另一篇论文。任务是预测arXiv
    CS论文的40个学科领域，*例如*，cs.AI、cs.LG和cs.OS，这些领域由论文的作者和arXiv版主手动确定（*即*，标记）。
- en: ogbn-products [[13](#bib.bibx13)]. The ogbn-products dataset represents an Amazon
    product co-purchasing network, with product descriptions as raw text. Nodes represent
    products sold in Amazon, and edges between two products indicate that the products
    are purchased together. The task is to predict the category of a product in a
    multi-class classification setup, where the 47 top-level categories are used for
    target labels.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ogbn-products [[13](#bib.bibx13)]。ogbn-products数据集代表了一个亚马逊产品共同购买网络，产品描述为原始文本。节点表示在亚马逊上销售的产品，两个产品之间的边表示这些产品被一起购买。任务是在多类分类设置中预测产品的类别，其中使用47个顶级类别作为目标标签。
- en: tape-arxiv23. The tape-arxiv23 dataset is a directed graph that represents the
    citation network between all computer science arXiv papers published in 2023 or
    later. Similar to ogbn-arxiv, each node is an arXiv paper, and each directed edge
    indicates that one paper cites another one. The task is to predict the 40 subject
    areas of arXiv CS papers, *e.g.,*, cs.AI, cs.LG, and cs.OS, which are manually
    determined (*i.e.,* labeled) by the paper’s authors and arXiv moderators.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: tape-arxiv23。tape-arxiv23数据集是一个有向图，表示2023年或之后出版的所有计算机科学arXiv论文之间的引用网络。与ogbn-arxiv类似，每个节点是一个arXiv论文，每个有向边表示一篇论文引用另一篇论文。任务是预测arXiv计算机科学论文的40个学科领域，例如，cs.AI、cs.LG和cs.OS，这些领域是由论文作者和arXiv管理员手动确定（即，标记）的。
- en: C.2 Dataset splits and random seeds
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 数据集划分和随机种子
- en: In our experiments, we adhered to specific dataset splits and employed random
    seeds for reproducibility. For the ogbn-arxiv and ogbn-products dataset, we adopted
    the standard train/validation/test split provided by OGB [[13](#bib.bibx13)].
    As for the Cora, PubMed datasets, and tape-arxiv23, we performed the train/validation/test
    splits ourselves, where 60% of the data was allocated for training, 20% for validation,
    and 20% for testing. Additionally, we utilized random seeds to ensure the reproducibility
    of our experiments, enabling the consistent evaluation of our proposed method
    on the respective datasets, which can be found in our linked code repository.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们遵循了特定的数据集划分，并使用随机种子以确保可重复性。对于ogbn-arxiv和ogbn-products数据集，我们采用了OGB提供的标准训练/验证/测试划分[[13](#bib.bibx13)]。至于Cora、PubMed数据集和tape-arxiv23，我们自己进行了训练/验证/测试划分，其中60%的数据用于训练，20%用于验证，20%用于测试。此外，我们利用了随机种子以确保实验的可重复性，从而在各自的数据集上对我们提出的方法进行一致的评估，这些评估结果可以在我们的链接代码库中找到。
- en: C.3 Shallow Embedding Methods for Node Feature Extraction
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 浅层嵌入方法用于节点特征提取
- en: 'Table [5](#A3.T5 "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature
    Extraction ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning") provides an overview
    of the text preprocessing and feature extraction methods commonly used in graph
    libraries such as PyG and DGL, which are widely adopted in GNN research.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A3.T5 "表5 ‣ C.3 浅层嵌入方法用于节点特征提取 ‣ 附录C 数据集 ‣ 利用解释：LLM到LM解释器用于增强文本属性图表示学习")
    提供了图书馆如PyG和DGL中常用的文本预处理和特征提取方法的概述，这些方法在GNN研究中被广泛采用。
- en: 'Table 5: Details of text preprocessing and feature extraction methods used
    for TAG datasets.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：用于TAG数据集的文本预处理和特征提取方法的详细信息。
- en: '| Dataset | Methods | Features | Description |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 特征 | 描述 |'
- en: '| Cora | BoW | 1,433 | After stemming and removing stopwords there is a vocabulary
    of size 1,433 unique words. All words with document frequency less than 10 were
    removed. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Cora | BoW | 1,433 | 在词干化和去除停用词后，词汇表的大小为1,433个独特词汇。所有文档频率小于10的词汇均被移除。 |'
- en: '| PubMed | TF-IDF | 500 | Each publication in the dataset is described by a
    TF/IDF weighted word vector from a dictionary which consists of 500 unique words.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | TF-IDF | 500 | 数据集中的每篇出版物都由一个TF/IDF加权的词向量描述，该词向量来自一个包含500个独特词汇的词典。
    |'
- en: '| ogbn-arxiv | skip-gram | 128 | The embeddings of individual words are computed
    by running the skip-gram model [[22](#bib.bibx22)] over the MAG[[32](#bib.bibx32)]
    corpus. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | skip-gram | 128 | 通过在MAG[[32](#bib.bibx32)]语料库上运行skip-gram模型[[22](#bib.bibx22)]计算单词的嵌入。
    |'
- en: '| ogbn-products | BoW | 100 | Node features are generated by extracting BoW
    features from the product descriptions followed by a Principal Component Analysis
    to reduce the dimension to 100. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-products | BoW | 100 | 节点特征通过从产品描述中提取BoW特征生成，然后通过主成分分析将维度降至100。 |'
- en: '| tape-arxiv23 | word2vec | 300 | The embeddings of individual words are computed
    by running the word2vec model. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | word2vec | 300 | 通过运行word2vec模型计算单词的嵌入。 |'
- en: These text preprocessing and feature extraction methods facilitate the extraction
    of node features from the text attributes of TAG datasets, enabling the utilization
    of GNN models for node classification tasks. While these methods are easy to apply
    and computationally efficient, it is important to note that they rely on traditional
    language modeling techniques that may not capture the full semantic meaning in
    the text. This limitation can impact the expressiveness of the extracted node
    features and potentially affect the development of techniques for downstream tasks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文本预处理和特征提取方法有助于从 TAG 数据集的文本属性中提取节点特征，从而使 GNN 模型能够用于节点分类任务。虽然这些方法易于应用且计算效率高，但需要注意的是，它们依赖于传统的语言建模技术，这可能无法捕捉文本中的全部语义。这一局限性可能会影响提取节点特征的表现力，并可能影响下游任务技术的发展。
- en: Appendix D Experiments
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 实验
- en: D.1 Computing Environment and Resources
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 计算环境和资源
- en: 'The implementation of the proposed method utilized the PyG and DGL modules,
    which are licensed under the MIT License. The experiments were conducted in a
    computing environment with the following specifications: LM-based experiments
    were performed on four NVIDIA RTX A5000 GPUs, each with 24GB VRAM. On the other
    hand, the GNN-based experiments were conducted on a single GPU.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的算法实现使用了 PyG 和 DGL 模块，这些模块是基于 MIT 许可证的。实验是在具有以下规格的计算环境中进行的：基于 LM 的实验在四个 NVIDIA
    RTX A5000 GPU 上进行，每个 GPU 拥有 24GB VRAM。另一方面，基于 GNN 的实验则在一个 GPU 上进行。
- en: D.2 Hyperparameters
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 超参数
- en: 'Table [6](#A4.T6 "Table 6 ‣ D.2 Hyperparameters ‣ Appendix D Experiments ‣
    Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph
    Representation Learning") provides an overview of the hyperparameters used for
    the GCN [[16](#bib.bibx16)], SAGE [[10](#bib.bibx10)], and RevGAT [[17](#bib.bibx17)]
    models. These hyperparameters were selected based on the official OGB repository ⁴⁴4[https://github.com/snap-stanford/ogb](https://github.com/snap-stanford/ogb),
    and the RevGAT and language model hyperparameters follow those used in the GLEM
    repository ⁵⁵5[https://github.com/AndyJZhao/GLEM](https://github.com/AndyJZhao/GLEM).
    It is important to note that these hyperparameters were not tuned on a per-dataset
    basis, but instead were used consistently across all three TAG datasets based
    on those from prior work, and also set consistently across both our proposed method
    and the baselines. This demonstrates the generality and ease of use of our method,
    as well as its compatibility with existing GNN baselines.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '表[6](#A4.T6 "Table 6 ‣ D.2 Hyperparameters ‣ Appendix D Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning") 提供了 GCN [[16](#bib.bibx16)]、SAGE [[10](#bib.bibx10)] 和 RevGAT [[17](#bib.bibx17)]
    模型所使用的超参数的概述。这些超参数是基于官方 OGB 存储库⁴⁴4[https://github.com/snap-stanford/ogb](https://github.com/snap-stanford/ogb)
    选择的，RevGAT 和语言模型的超参数则遵循 GLEM 存储库⁵⁵5[https://github.com/AndyJZhao/GLEM](https://github.com/AndyJZhao/GLEM)
    中使用的参数。需要注意的是，这些超参数不是在每个数据集上进行调优的，而是基于先前工作的超参数在所有三个 TAG 数据集中一致使用，并且在我们提出的方法和基线方法中也保持一致。这展示了我们方法的通用性和易用性，以及它与现有
    GNN 基线的兼容性。'
- en: 'Table 6: Hyperparameters for the GCN, SAGE, and RevGAT models.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：GCN、SAGE 和 RevGAT 模型的超参数。
- en: '| Hyperparameters | GCN | SAGE | RevGAT |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | GCN | SAGE | RevGAT |'
- en: '| # layers | 3 | 3 | 3 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 层数 | 3 | 3 | 3 |'
- en: '| hidden dim | 256 | 256 | 256 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏维度 | 256 | 256 | 256 |'
- en: '| learning rate | 0.01 | 0.01 | 0.002 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 0.01 | 0.01 | 0.002 |'
- en: '| dropout | 0.5 | 0.5 | 0.75 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| dropout | 0.5 | 0.5 | 0.75 |'
- en: '| epoch | 1000 | 1000 | 1000 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 1000 | 1000 | 1000 |'
- en: '| warmup epochs | 0 | 0 | 50 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 预热轮次 | 0 | 0 | 50 |'
- en: '| early stop | 50 | 50 | 50 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 提前停止 | 50 | 50 | 50 |'
- en: D.3 Prompt Design
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 提示设计
- en: 'Table [7](#A4.T7 "Table 7 ‣ D.3 Prompt Design ‣ Appendix D Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning") provides our prompt designs for different datasets. Each prompt includes
    the abstract and title of the paper, followed by a task-specific question. The
    question is formulated to query the model about a particular aspect of the paper
    and request an explanation for the prediction. The answer section is left blank
    for the model to fill in. Generally, our analysis finds that the current instructions
    allow the LLM to produce output that conforms well to the expected format without
    significant deviations, allowing the answers to be straightforwardly extracted
    from the text output of the LLM.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#A4.T7 "表 7 ‣ D.3 提示设计 ‣ 附录 D 实验 ‣ 利用解释：LLM 到 LM 解释器以增强文本属性图表示学习") 提供了不同数据集的提示设计。每个提示包括论文的摘要和标题，后跟一个特定任务的问题。问题旨在询问模型关于论文的特定方面，并要求对预测提供解释。答案部分留空以供模型填写。总体而言，我们的分析发现，当前的指令允许
    LLM 产生符合预期格式的输出，没有显著的偏差，使得答案可以从 LLM 的文本输出中直接提取。
- en: 'Table 7: Design of Prompts'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 提示设计'
- en: '| Dataset | Prompt |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 提示 |'
- en: '| Cora | Abstract: <abstract text>  \n Title: <title text>  \n Question: Which
    of the following sub-categories of AI does this paper belong to: Case Based, Genetic
    Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule
    Learning, Theory? If multiple options apply, provide a comma-separated list ordered
    from most to least related, then for each choice you gave, explain how it is present
    in the text. \n \n Answer: |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Cora | 摘要: <abstract text>  \n 标题: <title text>  \n 问题: 这篇论文属于以下哪个 AI 子类别：基于案例、遗传算法、神经网络、概率方法、强化学习、规则学习、理论？如果适用多个选项，请按最相关到最不相关的顺序提供以逗号分隔的列表，然后对于你给出的每个选项，解释它在文本中如何呈现。
    \n \n 答案: |'
- en: '| Pubmed | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Does the paper involve any cases of Type 1 diabetes, Type 2 diabetes, or Experimentally
    induced diabetes? Please give one or more answers of either Type 1 diabetes, Type
    2 diabetes, or Experimentally induced diabetes; if multiple options apply, provide
    a comma-separated list ordered from most to least related, then for each choice
    you gave, give a detailed explanation with quotes from the text explaining why
    it is related to the chosen option. \n \n Answer: |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Pubmed | 摘要: <abstract text>  \n 标题: <title text>  \n 问题: 论文是否涉及任何 1 型糖尿病、2
    型糖尿病或实验性诱导糖尿病的案例？请给出一个或多个 1 型糖尿病、2 型糖尿病或实验性诱导糖尿病的答案；如果适用多个选项，请按最相关到最不相关的顺序提供以逗号分隔的列表，然后对于你给出的每个选项，提供详细的解释，并引用文本说明为什么它与所选选项相关。
    \n \n 答案: |'
- en: '| ogbn-arxiv | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 摘要: <abstract text>  \n 标题: <title text>  \n 问题: 这篇论文属于哪个 arXiv
    CS 子类别？请给出 5 个可能的 arXiv CS 子类别，按最可能到最不可能的顺序以逗号分隔的列表形式表示，格式为“cs.XX”，并提供你的推理。 \n
    \n 答案: |'
- en: '| ogbn-products | Product description: <product description>  \n Question:
    Which of the following category does this product belong to: 1) Home & Kitchen,
    2) Health & Personal Care, 3) Beauty, 4) Sports & Outdoors, 5) Books, 6) Patio,
    Lawn & Garden, 7) Toys & Games, 8) CDs & Vinyl, 9) Cell Phones & Accessories,
    10) Grocery & Gourmet Food, 11) Arts, Crafts & Sewing, 12) Clothing, Shoes & Jewelry,
    13) Electronics, 14) Movies & TV, 15) Software, 16) Video Games, 17) Automotive,
    18) Pet Supplies, 19) Office Products, 20) Industrial & Scientific, 21) Musical
    Instruments, 22) Tools & Home Improvement, 23) Magazine Subscriptions, 24) Baby
    Products, 25) NAN, 26) Appliances, 27) Kitchen & Dining, 28) Collectibles & Fine
    Art, 29) All Beauty, 30) Luxury Beauty, 31) Amazon Fashion, 32) Computers, 33)
    All Electronics, 34) Purchase Circles, 35) MP3 Players & Accessories, 36) Gift
    Cards, 37) Office & School Supplies, 38) Home Improvement, 39) Camera & Photo,
    40) GPS & Navigation, 41) Digital Music, 42) Car Electronics, 43) Baby, 44) Kindle
    Store, 45) Kindle Apps, 46) Furniture & Decor? Give 5 likely categories as a comma-separated
    list ordered from most to least likely, and provide your reasoning. \n \n Answer:
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-products | 产品描述：<product description>  \n 问题：以下哪个类别适合该产品：1) 家居与厨房，2)
    健康与个人护理，3) 美容，4) 体育与户外，5) 图书，6) 露台、草坪与花园，7) 玩具与游戏，8) CD与黑胶唱片，9) 手机及配件，10) 杂货与美食，11)
    艺术、工艺与缝纫，12) 服装、鞋类与珠宝，13) 电子产品，14) 电影与电视，15) 软件，16) 视频游戏，17) 汽车，18) 宠物用品，19) 办公产品，20)
    工业与科学，21) 乐器，22) 工具与家居改善，23) 杂志订阅，24) 婴儿产品，25) NAN，26) 家用电器，27) 厨房与餐厅，28) 收藏品与美术，29)
    全部美容，30) 奢华美容，31) 亚马逊时尚，32) 电脑，33) 全部电子产品，34) 购买圈，35) MP3播放器及配件，36) 礼品卡，37) 办公与学校用品，38)
    家居改善，39) 照相机与照片，40) GPS与导航，41) 数字音乐，42) 汽车电子产品，43) 婴儿，44) Kindle商店，45) Kindle应用，46)
    家具与装饰？请给出5个可能的类别，按从最可能到最不可能的顺序排列，并提供你的理由。 \n \n 答案： |'
- en: '| tape-arxiv23 | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | 摘要：<abstract text>  \n 标题：<title text>  \n 问题：这篇论文属于哪个arXiv
    CS子类别？请给出5个可能的arXiv CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”，并提供你的理由。 \n \n 答案： |'
- en: Additional Prompt Experiments.
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 附加提示实验。
- en: 'Table 8: Prompts used for our experiments studying the effect of different
    prompts. Most prompts have similar performance.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：用于研究不同提示对实验效果影响的提示。大多数提示具有类似的表现。
- en: '| Description | Prompt | Accuracy |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 提示 | 准确性 |'
- en: '| Default prompt | Abstract: <abstract text>  \n Title: <title text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: | 0.720 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 默认提示 | 摘要：<abstract text>  \n 标题：<title text>  \n 问题：这篇论文属于哪个arXiv CS子类别？请给出5个可能的arXiv
    CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”，并提供你的理由。 \n \n 答案： | 0.720 |'
- en: '| Title first | Title: <title text>  \n Abstract: <abstract text>  \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: | 0.695 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 标题优先 | 标题：<title text>  \n 摘要：<abstract text>  \n 问题：这篇论文属于哪个arXiv CS子类别？请给出5个可能的arXiv
    CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”，并提供你的理由。 \n \n 答案： | 0.695 |'
- en: '| Focus on text content | Title: <title text>  \n Abstract: <abstract text>  \n
    Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely
    arXiv CS sub-categories as a comma-separated list ordered from most to least likely,
    in the form “cs.XX”. Focus only on content in the actual text and avoid making
    false associations. Then provide your reasoning. | 0.695 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 专注于文本内容 | 标题：<title text>  \n 摘要：<abstract text>  \n 问题：这篇论文属于哪个arXiv CS子类别？请给出5个可能的arXiv
    CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”。仅关注实际文本中的内容，避免错误关联。然后提供你的理由。 | 0.695 |'
- en: '| Chain of thought prompt | Title: <title text>  \n Abstract: <abstract text>  \n
    Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely
    arXiv CS sub-categories as a comma-separated list ordered from most to least likely,
    in the form “cs.XX”. Please think about the categorization in a step by step manner
    and avoid making false associations. Then provide your reasoning. | 0.705 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 思考链提示 | 标题：<title text>  \n 摘要：<abstract text>  \n 问题：这篇论文属于哪个arXiv计算机科学子类别？请提供5个可能的arXiv计算机科学子类别，以逗号分隔的形式从最可能到最不可能的顺序排列，格式为“cs.XX”。请逐步考虑分类，避免错误关联。然后提供你的推理。
    | 0.705 |'
- en: 'To study the effect of different prompts, we consider a variety of prompts
    and evaluate the zero-shot accuracy of the LLM (ChatGPT) on each prompt. We evaluate
    all prompts on 200 sample papers from the ogbn-arxiv dataset, see Table [8](#A4.T8
    "Table 8 ‣ Additional Prompt Experiments. ‣ D.3 Prompt Design ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). We accompany each prompt by a brief summary of
    the change being made. In summary, most prompts have similar performance, with
    a slight performance gain when placing the title after the abstract, which seems
    to agree with the notion in [[46](#bib.bibx46)] that more important information
    (like the title) should be placed later in the prompt.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究不同提示的效果，我们考虑了多种提示，并评估了LLM（ChatGPT）在每个提示上的零样本准确性。我们在ogbn-arxiv数据集中的200篇样本论文上评估所有提示，见表格[8](#A4.T8
    "Table 8 ‣ Additional Prompt Experiments. ‣ D.3 Prompt Design ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")。我们为每个提示附上了所做更改的简要总结。总结来说，大多数提示的表现相似，将标题放在摘要之后稍微提高了性能，这似乎与[[46](#bib.bibx46)]的观点一致，即更重要的信息（如标题）应放在提示的后面。'
- en: D.4 Detailed Ablation Study
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 详细消融研究
- en: 'We conducted a detailed ablation study on the ogbn-arxiv dataset to assess
    the impact of different sources of node features. The study focused on three types
    of node features: original text features ($h_{\textrm{orig}}$), explanation as
    features ($h_{\textrm{expl}}$), and predictions as features ($h_{\textrm{pred}}$).
    We systematically removed one of these features at a time while keeping the other
    components unchanged in our model.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对ogbn-arxiv数据集进行了详细的消融研究，以评估不同来源的节点特征的影响。研究集中在三种类型的节点特征上：原始文本特征（$h_{\textrm{orig}}$）、解释作为特征（$h_{\textrm{expl}}$）和预测作为特征（$h_{\textrm{pred}}$）。我们系统地逐一移除这些特征中的一个，同时保持模型中的其他组件不变。
- en: 'The results of the ablation study are illustrated in Figure [3](#A4.F3 "Figure
    3 ‣ D.4 Detailed Ablation Study ‣ Appendix D Experiments ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning").
    The figure presents the performance of the model when each type of node feature
    is removed. It is observed that using the full set of features yields the best
    performance, while leaving out any of the features leads to a drop in performance.
    However, the extent of the performance drop may vary depending on the specific
    GNN model being used.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '消融研究的结果如图[3](#A4.F3 "Figure 3 ‣ D.4 Detailed Ablation Study ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")所示。该图展示了在移除每种类型的节点特征时模型的表现。观察到，使用完整的特征集可以获得最佳性能，而移除任何特征都会导致性能下降。然而，性能下降的程度可能会根据所使用的具体GNN模型而有所不同。'
- en: '![Refer to caption](img/0d49dff0fd6769254e47715956b346e8.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d49dff0fd6769254e47715956b346e8.png)'
- en: 'Figure 3: Effect of node features. We study the effects of different sources
    of node features on the ogbn-arxiv dataset, *i.e.,* original text features ($h_{\textrm{orig}}$),
    explanation as features ($h_{\textrm{expl}}$) and predictions as features ($h_{\textrm{pred}}$),
    by removing one of them in turn from our model while keeping the other components
    unchanged.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：节点特征的影响。我们研究了不同来源的节点特征对ogbn-arxiv数据集的影响，*即，* 原始文本特征（$h_{\textrm{orig}}$）、解释作为特征（$h_{\textrm{expl}}$）和预测作为特征（$h_{\textrm{pred}}$），通过在保持其他组件不变的情况下逐一从模型中移除它们。
- en: 'This ablation study provides additional insights to complement the findings
    presented in section [5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). While Table [3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") compared the performance of using the full set
    of features versus using just one of them, this ablation study specifically focuses
    on comparing the performance of using the full set of features versus leaving
    one of them out. Although the experimental design differs, the overall message
    conveyed remains consistent, emphasizing the significance of considering all the
    various sources of node features for achieving optimal performance in node classification
    tasks.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '本次消融研究提供了额外的见解，以补充第[5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")节中提出的发现。虽然表[3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")比较了使用全套特征与仅使用其中一个特征的性能，本次消融研究特别关注于比较使用全套特征与省略其中一个特征的性能。尽管实验设计有所不同，但传达的整体信息保持一致，强调了考虑所有不同节点特征来源对于实现节点分类任务的最佳性能的重要性。'
- en: D.5 Llama as a cost-efficient alternative
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5 Llama作为一种成本效益高的替代方案
- en: 'We extend out experiment to the open-source LLM "llama-2-13b-chat" (llama for
    short), which demonstrates the feasibility of a cost-effective (free) alternative,
    see Table [9](#A4.T9 "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning").'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将实验扩展到开源LLM“llama-2-13b-chat”（简称llama），这展示了一个经济实惠（免费）替代方案的可行性，见表 [9](#A4.T9
    "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")。'
- en: It is worth noting that although llama exhibits a lower performance compared
    to GPT-3.5 in terms of both zero-shot accuracy and explanation quality, our pipeline
    still maintains its robust performance. As an illustration, we achieved an accuracy
    of 76.19% on the ogbn-arxiv dataset using llama, slightly below the 77.50% achieved
    with GPT-3.5\. We attribute this impressive level of generalization to the complementary
    nature of the explanations themselves, which serve as a rich source of semantic
    information supplementing the original text such as title and abstract.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管llama在零样本准确性和解释质量方面的表现低于GPT-3.5，但我们的流程仍然保持了其强大的性能。作为说明，我们在ogbn-arxiv数据集上使用llama达到了76.19%的准确率，略低于GPT-3.5取得的77.50%。我们将这一令人印象深刻的泛化水平归因于解释本身的互补性质，它们作为丰富的语义信息来源，补充了原始文本如标题和摘要。
- en: 'Table 9: Node classification accuracy for the Cora, PubMed, ogbn-arxiv and
    tape-arxiv23 datasets.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：Cora、PubMed、ogbn-arxiv 和 tape-arxiv23 数据集的节点分类准确率。
- en: '| Dataset | Method | llama2-13b-chat | GPT3.5 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | llama2-13b-chat | GPT3.5 |'
- en: '| LLM | LM${}_{\textrm{finetune}}$ | $h_{\textrm{TAPE}}$ | LLM | LM${}_{\textrm{finetune}}$
    | $h_{\textrm{TAPE}}$ |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| LLM | LM${}_{\textrm{finetune}}$ | $h_{\textrm{TAPE}}$ | LLM | LM${}_{\textrm{finetune}}$
    | $h_{\textrm{TAPE}}$ |'
- en: '| Cora | MLP | 0.5746 | 0.6845 ± 0.0194 | 0.7675 ± 0.0187 | 0.6769 | 0.7606
    ± 0.0378 | 0.8086 ± 0.0190 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Cora | MLP | 0.5746 | 0.6845 ± 0.0194 | 0.7675 ± 0.0187 | 0.6769 | 0.7606
    ± 0.0378 | 0.8086 ± 0.0190 |'
- en: '| GCN | 0.5746 | 0.6845 ± 0.0194 | 0.8630 ± 0.0101 | 0.6769 | 0.7606 ± 0.0378
    | 0.8773 ± 0.0063 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.5746 | 0.6845 ± 0.0194 | 0.8630 ± 0.0101 | 0.6769 | 0.7606 ± 0.0378
    | 0.8773 ± 0.0063 |'
- en: '| SAGE | 0.5746 | 0.6845 ± 0.0194 | 0.8625 ± 0.0093 | 0.6769 | 0.7606 ± 0.0378
    | 0.8792 ± 0.0107 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.5746 | 0.6845 ± 0.0194 | 0.8625 ± 0.0093 | 0.6769 | 0.7606 ± 0.0378
    | 0.8792 ± 0.0107 |'
- en: '|  | RevGAT | 0.5746 | 0.6845 ± 0.0194 | 0.8884 ± 0.0100 | 0.6769 | 0.7606
    ± 0.0378 | 0.8930 ± 0.0072 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.5746 | 0.6845 ± 0.0194 | 0.8884 ± 0.0100 | 0.6769 | 0.7606
    ± 0.0378 | 0.8930 ± 0.0072 |'
- en: '| PubMed | MLP | 0.3958 | 0.9121 ± 0.0026 | 0.9475 ± 0.0046 | 0.9342 | 0.9494
    ± 0.0046 | 0.9473 ± 0.0040 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | MLP | 0.3958 | 0.9121 ± 0.0026 | 0.9475 ± 0.0046 | 0.9342 | 0.9494
    ± 0.0046 | 0.9473 ± 0.0040 |'
- en: '| GCN | 0.3958 | 0.9121 ± 0.0026 | 0.9257 ± 0.0063 | 0.9342 | 0.9494 ± 0.0046
    | 0.9392 ± 0.0023 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.3958 | 0.9121 ± 0.0026 | 0.9257 ± 0.0063 | 0.9342 | 0.9494 ± 0.0046
    | 0.9392 ± 0.0023 |'
- en: '| SAGE | 0.3958 | 0.9121 ± 0.0026 | 0.9464 ± 0.0033 | 0.9342 | 0.9494 ± 0.0046
    | 0.9530 ± 0.0035 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.3958 | 0.9121 ± 0.0026 | 0.9464 ± 0.0033 | 0.9342 | 0.9494 ± 0.0046
    | 0.9530 ± 0.0035 |'
- en: '|  | RevGAT | 0.3958 | 0.9121 ± 0.0026 | 0.9465 ± 0.0042 | 0.9342 | 0.9494
    ± 0.0046 | 0.9526 ± 0.0032 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.3958 | 0.9121 ± 0.0026 | 0.9465 ± 0.0042 | 0.9342 | 0.9494
    ± 0.0046 | 0.9526 ± 0.0032 |'
- en: '| ogbn-arxiv | MLP | 0.4423 | 0.6941 ± 0.0020 | 0.7361 ± 0.0009 | 0.7350 |
    0.7361 ± 0.0004 | 0.7587 ± 0.0015 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | MLP | 0.4423 | 0.6941 ± 0.0020 | 0.7361 ± 0.0009 | 0.7350 |
    0.7361 ± 0.0004 | 0.7587 ± 0.0015 |'
- en: '| GCN | 0.4423 | 0.6941 ± 0.0020 | 0.7418 ± 0.0031 | 0.7350 | 0.7361 ± 0.0004
    | 0.7520 ± 0.0003 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.4423 | 0.6941 ± 0.0020 | 0.7418 ± 0.0031 | 0.7350 | 0.7361 ± 0.0004
    | 0.7520 ± 0.0003 |'
- en: '| SAGE | 0.4423 | 0.6941 ± 0.0020 | 0.7536 ± 0.0028 | 0.7350 | 0.7361 ± 0.0004
    | 0.7672 ± 0.0007 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.4423 | 0.6941 ± 0.0020 | 0.7536 ± 0.0028 | 0.7350 | 0.7361 ± 0.0004
    | 0.7672 ± 0.0007 |'
- en: '|  | RevGAT | 0.4423 | 0.6941 ± 0.0020 | 0.7619 ± 0.0027 | 0.7350 | 0.7361
    ± 0.0004 | 0.7750 ± 0.0012 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.4423 | 0.6941 ± 0.0020 | 0.7619 ± 0.0027 | 0.7350 | 0.7361
    ± 0.0004 | 0.7750 ± 0.0012 |'
- en: '| tape-arxiv23 | MLP | 0.4452 | 0.7677 ± 0.0042 | 0.7905 ± 0.0041 | 0.7356
    | 0.7832 ± 0.0052 | 0.7999 ± 0.0037 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | MLP | 0.4452 | 0.7677 ± 0.0042 | 0.7905 ± 0.0041 | 0.7356
    | 0.7832 ± 0.0052 | 0.7999 ± 0.0037 |'
- en: '| GCN | 0.4452 | 0.7677 ± 0.0042 | 0.7751 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.7827 ± 0.0037 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.4452 | 0.7677 ± 0.0042 | 0.7751 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.7827 ± 0.0037 |'
- en: '| SAGE | 0.4452 | 0.7677 ± 0.0042 | 0.7935 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.8020 ± 0.0024 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.4452 | 0.7677 ± 0.0042 | 0.7935 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.8020 ± 0.0024 |'
- en: '|  | RevGAT | 0.4452 | 0.7677 ± 0.0042 | 0.7993 ± 0.0043 | 0.7356 | 0.7832
    ± 0.0052 | 0.8074 ± 0.0021 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.4452 | 0.7677 ± 0.0042 | 0.7993 ± 0.0043 | 0.7356 | 0.7832
    ± 0.0052 | 0.8074 ± 0.0021 |'
- en: D.6 Case Study
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6 案例研究
- en: '![Refer to caption](img/4da50160638a3fd9299866f50c433471.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4da50160638a3fd9299866f50c433471.png)'
- en: 'Figure 4: Case study comparing features for node classification on the PubMed
    dataset: (a) Original text attributes and (b) Explanations generated by LLMs.
    The GNN model trained with (b) accurately predicts the label for node 12390 (type
    2 diabetes), while the model trained with (a) predicts the incorrect label (experimentally
    induced diabetes). This improvement can be attributed to the concise and focused
    nature of LLM-generated explanations, as well as their reasoning ability and utilization
    of external knowledge.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：比较PubMed数据集上节点分类特征的案例研究：(a) 原始文本属性和(b) LLM生成的解释。使用(b)训练的GNN模型准确预测了节点12390的标签（2型糖尿病），而使用(a)训练的模型预测了错误的标签（实验性诱导糖尿病）。这种改进可以归因于LLM生成解释的简洁和集中性质，以及它们的推理能力和外部知识的利用。
- en: 'To investigate the impact of using explanations as features in improving node
    classification on TAGs, we conduct an analysis on predicted samples from the PubMed
    dataset. Figure [4](#A4.F4 "Figure 4 ‣ D.6 Case Study ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") presents a case where the GNN model trained with
    original text attributes as features incorrectly predicts the label for node 12390
    (as experimentally induced diabetes), while the model trained with explanations
    generated by LLMs as features correctly predicts the label (as type 2 diabetes).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究使用解释作为特征对提高节点分类在TAGs上的影响，我们对PubMed数据集中的预测样本进行了分析。图[4](#A4.F4 "图 4 ‣ D.6
    案例研究 ‣ 附录 D 实验 ‣ 利用解释：LLM到LM解释器以增强文本属性图表示学习")展示了一个案例，其中使用原始文本属性作为特征训练的GNN模型错误地预测了节点12390的标签（为实验性诱导糖尿病），而使用LLM生成的解释作为特征训练的模型正确预测了标签（为2型糖尿病）。
- en: This improvement can be attributed to two main factors. Firstly, compared to
    the original text attributes, which consist of the title and abstract text, the
    explanations generated by the LLM are more concise and focused. This aids the
    subsequent LM in generating node embeddings that capture the essential semantics
    without the need to compress an excessive amount of information into a fixed-length
    representation. Secondly, LLMs possess reasoning capabilities and the ability
    to leverage general knowledge, which prove crucial in achieving accurate predictions.
    For instance, the explanations generated by LLMs explicitly link type 2 diabetes
    to MKR mice and db/db mice (which are common animal models of type 2 diabetes),
    as well as the insulinopenic mice / streptozotocin to experimentally induced diabetes.
    This knowledge is either absent or only implicitly specified in the original text
    attributes.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这种改进可以归因于两个主要因素。首先，与包含标题和摘要文本的原始文本属性相比，LLM生成的解释更为简洁和集中。这有助于随后的LM生成捕捉基本语义的节点嵌入，而不需要将过多的信息压缩成固定长度的表示。其次，LLM具有推理能力和利用通用知识的能力，这在实现准确预测方面至关重要。例如，LLM生成的解释明确将2型糖尿病与MKR小鼠和db/db小鼠（这两者是2型糖尿病的常见动物模型）以及胰岛素缺乏小鼠/链脲佐菌素与实验性诱导糖尿病联系起来。这些知识在原始文本属性中要么缺失，要么仅以隐含的方式指定。
- en: D.7 GLEM
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7 GLEM
- en: '[[45](#bib.bibx45)] evaluated GLEM on the ogbn-arxiv dataset. We extended our
    evaluation of GLEM with the Cora and PubMed datasets for a more comprehensive
    comparison with our method. Results are reported in Table [10](#A4.T10 "Table
    10 ‣ D.7 GLEM ‣ Appendix D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[[45](#bib.bibx45)] 在 ogbn-arxiv 数据集上评估了 GLEM。我们通过使用 Cora 和 PubMed 数据集扩展了对
    GLEM 的评估，以便与我们的方法进行更全面的比较。结果见表 [10](#A4.T10 "Table 10 ‣ D.7 GLEM ‣ Appendix D
    Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")'
- en: 'Table 10: GLEM [[45](#bib.bibx45)]'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：GLEM [[45](#bib.bibx45)]
- en: '| Dataset | GCN | SAGE | RevGAT |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | GCN | SAGE | RevGAT |'
- en: '| Cora | 0.8732 ± 0.0066 | 0.8801 ± 0.0054 | 0.8856 ± 0.0060 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Cora | 0.8732 ± 0.0066 | 0.8801 ± 0.0054 | 0.8856 ± 0.0060 |'
- en: '| PubMed | 0.9469 ± 0.0010 | 0.9459 ± 0.0018 | 0.9471 ± 0.0020 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | 0.9469 ± 0.0010 | 0.9459 ± 0.0018 | 0.9471 ± 0.0020 |'
- en: '| ogbn-arxiv | 0.7593 ± 0.0019 | 0.7550 ± 0.0024 | 0.7697 ± 0.0019 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 0.7593 ± 0.0019 | 0.7550 ± 0.0024 | 0.7697 ± 0.0019 |'
