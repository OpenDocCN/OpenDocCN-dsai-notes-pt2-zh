- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 17:34:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06703](https://ar5iv.labs.arxiv.org/html/2405.06703)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Goran Muric
  prefs: []
  type: TYPE_NORMAL
- en: InferLink Corporation
  prefs: []
  type: TYPE_NORMAL
- en: Los Angeles, California
  prefs: []
  type: TYPE_NORMAL
- en: gmuric@inferlink.com
  prefs: []
  type: TYPE_NORMAL
- en: \AndBen Delay
  prefs: []
  type: TYPE_NORMAL
- en: InferLink Corporation
  prefs: []
  type: TYPE_NORMAL
- en: Los Angeles, California
  prefs: []
  type: TYPE_NORMAL
- en: bdelay@inferlink.com
  prefs: []
  type: TYPE_NORMAL
- en: \AndSteven Minton
  prefs: []
  type: TYPE_NORMAL
- en: InferLink Corporation
  prefs: []
  type: TYPE_NORMAL
- en: Los Angeles, California
  prefs: []
  type: TYPE_NORMAL
- en: sminton@inferlink.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this paper, we introduce the Interpretable Cross-Examination Technique (ICE-T),
    a novel approach that leverages structured multi-prompt techniques with Large
    Language Models (LLMs) to improve classification performance over zero-shot and
    few-shot methods. In domains where interpretability is crucial, such as medicine
    and law, standard models often fall short due to their “black-box” nature. ICE-T
    addresses these limitations by using a series of generated prompts that allow
    an LLM to approach the problem from multiple directions. The responses from the
    LLM are then converted into numerical feature vectors and processed by a traditional
    classifier. This method not only maintains high interpretability but also allows
    for smaller, less capable models to achieve or exceed the performance of larger,
    more advanced models under zero-shot conditions. We demonstrate the effectiveness
    of ICE-T across a diverse set of data sources, including medical records and legal
    documents, consistently surpassing the zero-shot baseline in terms of classification
    metrics such as F1 scores. Our results indicate that ICE-T can be used for improving
    both the performance and transparency of AI applications in complex decision-making
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance'
  prefs: []
  type: TYPE_NORMAL
- en: Goran Muric InferLink Corporation Los Angeles, California gmuric@inferlink.com
                           Ben Delay InferLink Corporation Los Angeles, California
    bdelay@inferlink.com                        Steven Minton InferLink Corporation
    Los Angeles, California sminton@inferlink.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are numerous prompting strategies to achieve good performance using generative
    Large Language Models (LLMs). Take, for instance, a binary classification problem,
    where a system should classify the given text into one of two classes. A typical
    zero-shot approach is to prompt the model with a given text and carefully designed
    question, that will yield an appropriate answer. There are also multiple variations
    on that approach that include “chain-of-thought” prompting  Wei et al. ([2022c](#bib.bib35));
    Wang et al. ([2022a](#bib.bib30)); Kojima et al. ([2022](#bib.bib14)), “few-shot
    learning”  Schick and Schütze ([2022](#bib.bib25)); Gu et al. ([2021](#bib.bib12)),
    “self-instruct” Wang et al. ([2022b](#bib.bib31)); Yang et al. ([2024](#bib.bib39))
    prompting and “iterative refinement” Wu et al. ([2022a](#bib.bib36)); Trautmann
    ([2023](#bib.bib29)). These tactics are used to get a better sense of the model’s
    underlying reasoning or to surpass the performance achieved by the standard zero-shot
    method.
  prefs: []
  type: TYPE_NORMAL
- en: These options are usually used in cases where using highly specialized fine-tuned
    LLMs is not a viable option because it is often of utmost importance to understand
    how decisions are made. This is especially true in fields like medicine, where
    decisions based on opaque, “black-box” models are usually not acceptable. Although
    zero-shot or few-shot prompting methods can potentially offer explanations for
    their reasoning, these explanations are often unstructured and lack quantifiability.
    On the other hand, while finely tuned models may achieve superior performance,
    they frequently struggle to articulate the rationale behind their outputs unless
    explicitly trained for this purpose, a process that is labor-intensive. Additionally,
    outputs from such models may also suffer from the lack of structured reasoning
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where using “black-box” models is not practical, and where interpretability
    is important, users have the option to develop a structured reasoning process
    by asking several questions to achieve a desired output. There are three main
    problems that arise with this approach: 1) Non-experts have little chance to develop
    a good set of questions and rules that ensure optimal model performance; 2) Designing
    an accurate rule set becomes challenging since individual instances may not perfectly
    align with all desired criteria, resulting in a mix of positive and negative responses
    to different rules; and 3) The potential combinations of these rules can become
    overwhelmingly numerous, making it impractical to hard-code every possible scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, we propose a method that attempts to overcome the three issues
    outlined above. We refer to the method as the Interpretable Cross-Examination
    Technique, or ICE-T for brevity. Our approach exhibits strong performance, consistently
    surpasses the benchmark set by a zero-shot baseline, and also offers a high level
    of interpretability. The core concept here is that rather than using a single
    prompt to get a response from an LLM and making a decision based on that single
    output, we engage the LLM with multiple prompts, covering various questions. We
    then combine the responses from all these prompts and use the outputs to make
    a decision. Compared to other methods that are based on multi-prompting, our approach
    is fundamentally different in the way the decisions are made. Specifically, we
    take the responses from the LLM, convert them into numerical values to create
    a feature vector, and then input this vector into a traditional classifier to
    determine the final outcome. Since, in this process we create a low-dimensional
    feature vector with highly informative features, we can then use relatively small
    classifiers to make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'We established an experimental setup where we tested our Interpretable Cross-Examination
    Technique on a simple binary classification task. We tested our approach on a
    set of multiple datasets split on 17 different tasks and we show that:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ICE-T consistently outperforms the zero-shot baseline model in most classification
    metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a smaller model with ICE-T we can achieve comparable or better results
    than using larger and essentially more capable model with zero-shot approach
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Furthermore, this approach can be highly interpretable, allowing experts to
    clearly understand the rationale behind the decision-making process¹¹1Degree of
    interpretability may vary depending on the machine learning method selected for
    the final classification task. The decision on which method to employ should be
    guided by a consideration of the trade-offs between interpretability and performance
    tailored to the unique demands of each task. Additionally, tools commonly used
    for tabular machine learning can be employed to enhance the understanding of the
    data. While this technique is specifically evaluated for binary classification
    within this paper, its applicability potentially extends across a broad spectrum
    of scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ICE-T method was initially conceived at InferLink in a commercial consulting
    project, where we needed to address a complex challenge in biomedical text classification.
    The project’s goals were to develop a model that could perform at a level comparable
    to human experts, provide interpretable results, and allow for detection of potentially
    mislabelled data. Initially, conventional “black-box” models such like fine-tuned
    BERT-based ones underperformed, as well as zero-shot or few-shot learning methods
    using LLMs. This led to the creation of the ICE-T, which improved the performance
    of classification, while gaining interpretability and allowing for the correction
    of labeling errors. ICE-T was used initially for the purpose of classifying biomedical
    data for a specific commercial purpose. While the specifics of this initial task
    and data remain confidential, we have conducted further testing on additional
    publicly available datasets and decided to make the method publicly accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our proposed solution addresses three core aspects of using large language
    models for inference: prompting, in-context learning, and interpretability. It
    is built on top of the ever-growing body of knowledge that comes from those domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Prompting techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numerous techniques have been developed to improve the fundamental zero-shot
    approach. Among these, the “chain-of-thought” (CoT) prompting is particularly
    notable. This method is used to prompt the model to systematically articulate
    its reasoning process in a step-by-step manner before reaching a conclusion. Research
    has shown that chain-of-thought prompting improves performance on a range of arithmetic,
    commonsense, and symbolic reasoning tasks Wei et al. ([2022b](#bib.bib34), [c](#bib.bib35));
    Wang et al. ([2022a](#bib.bib30)). Even simple tweaks such as adding “Let’s think
    step by step” before each answer can significantly outperform zero-shot LLM performances
    on diverse benchmark reasoning tasks Kojima et al. ([2022](#bib.bib14)); Nye et al.
    ([2021](#bib.bib20)). Such generated chains that prompt language models to break
    down their reasoning into steps often cause errors in inference time. To reduce
    these errors, some researchers employ a method known as automatic Chain of Thought
    prompting. This technique, which generates demonstrable examples, has proven to
    be more effective than earlier, simpler CoT approaches Zhang et al. ([2022b](#bib.bib43)).
    Lastly, “iterative refinement” involves repeatedly prompting the model with slightly
    altered versions of the original text or question, honing in on a more accurate
    or nuanced answer through successive iterations. Each of these strategies can
    be tailored to the specific needs of a task, leveraging the model’s capabilities
    in different ways to achieve optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Several approaches involve using multiple prompts in a chain, where the output
    of one step becomes the input for the next, thus aggregating the gains per step Wu
    et al. ([2022a](#bib.bib36)), or decomposing complex tasks into smaller, manageable
    components Trautmann ([2023](#bib.bib29)). Additionally, “self-instruct” Wang
    et al. ([2022b](#bib.bib31)); Yang et al. ([2024](#bib.bib39)) prompting can be
    used, where the model generates its own instructions or clarifications based on
    the initial prompt, attempting to refine or better understand the task before
    generating a response. Another set of approaches uses multiple models or multiple
    instances of the same model to improve the performance. The additionally trained
    models, called “verifiers” are used to judge the correctness of model completions.
    At the inference time, the verifiers would select the most likely answer Cobbe
    et al. ([2021](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 In-context learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large Language Models possess the remarkable ability for in-context learning
    (ICL), in which they acquire knowledge from a few contextual examples either during
    inference or during training. Numerous studies have shown that through ICL, LLMs
    can effectively handle a diverse set of complex tasks Wei et al. ([2022a](#bib.bib33)).
    ICL offers several advantages, notably its ease in including human knowledge into
    LLMs by using various demonstrations and templates Liu et al. ([2021](#bib.bib16));
    Wu et al. ([2022b](#bib.bib37)). Furthermore, unlike traditional supervised training
    methods, ICL operates without the need for additional training, significantly
    lowering the computational costs when using models to solve new tasks Dong et al.
    ([2022](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most recognizable techniques for in-context learning is “few-shot
    learning” Schick and Schütze ([2022](#bib.bib25), [2020](#bib.bib24)); Gu et al.
    ([2021](#bib.bib12)); Perez et al. ([2021](#bib.bib21)) during inference²²2A different
    approach to achieving few-shot learning can occur also during the training phase
    or during fine-tuning.. Using this approach, the model is provided with a few
    examples of text and their corresponding labels or desired outputs within the
    prompt itself. This method teaches the model the context of the decision-making
    process, improving its accuracy on similar tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple other studies contributed to refining the ICL methods, focusing on
    automation, ordering, and selection of prompts. Zhou et al. (2022) introduced
    the Automatic Prompt Engineer (APE), which automates the generation of instructional
    prompts, significantly reducing manual effort and improving scalability Zhou et al.
    ([2022](#bib.bib45)). Simultaneously, Lu et al. (2021) came up with the method
    to optimize the ordering of prompts. They employed entropy statistics to evaluate
    and identify the most effective prompt sequences Lu et al. ([2021](#bib.bib17)).
    Rubin et al. (2021) and Liu et al. (2021) both contribute to this area but from
    different perspectives. Rubin et al. (2021) developed a method for efficiently
    retrieving prompts using annotated data, streamlining the selection process Rubin
    et al. ([2021](#bib.bib23)). On the other hand, Liu et al. (2021) explored strategic
    selection methods that go beyond random sampling to leverage the few-shot capabilities
    of LLMs, aiming to enhance the model’s performance through example selection Liu
    et al. ([2021](#bib.bib16)). Adding to the discussion on selection strategies,
    Zhang et al. (2022) approached example selection as a sequential decision problem.
    They proposed using a reinforcement learning algorithm to discover policies that
    improve the generalizability of language models Zhang et al. ([2022a](#bib.bib42)).
    This perspective introduces a dynamic element to the selection process, aligning
    with the strategies discussed by Rubin and Liu but through an adaptive, policy-driven
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Model interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The challenge of interpreting complex decision processes made by LLMs has hindered
    their application in critical areas like medicine, where there are significant
    concerns about regulation Goodman and Flaxman ([2017](#bib.bib11)) and safety Amodei
    et al. ([2016](#bib.bib2)). Furthermore, this difficulty in understanding the
    workings of large language models (LLMs) and similar neural network models has
    restricted their use in domains like science and data analysis Kasneci et al.
    ([2023](#bib.bib13)). In such fields, the primary objective is often to derive
    a reliable interpretation rather than merely to implement an LLM Singh et al.
    ([2024](#bib.bib26)).
  prefs: []
  type: TYPE_NORMAL
- en: The expression of uncertainty in language models is crucial for reliable LLM
    utilization, yet it remains a challenging area due to inherent overconfidence
    in model responses. Xiong et al. (2023) and Zhou et al. (2024) both highlight
    the overconfidence issue in LLMs. Xiong et al. question whether LLMs can express
    their uncertainty, observing a tendency in LLMs to mimic human patterns of expressing
    confidence Xiong et al. ([2023](#bib.bib38)). Simlarly, Zhou et al. note that
    while LLMs can be prompted to express confidence levels, they remain generally
    overconfident and unable to convey uncertainties effectively, also when providing
    incorrect responses Zhou et al. ([2024](#bib.bib44)). Ye et al. (2022) add that
    even when LLMs generate explanations, these may not accurately reflect the model’s
    predictions nor be factually grounded in the input, particularly in tasks requiring
    extractive explanations Ye and Durrett ([2022](#bib.bib40)). However, all the
    research mentioned above note that these flawed explanations can still serve a
    purpose, offering a means to verify LLM predictions post-hoc.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning feature attribution methods, used beyond the LLM realm
    in multiple deep-learning applications. Feature attributions in machine learning
    provide a relevance score to each input feature, reflecting its impact on the
    model’s output. This methodology helps in understanding how and why certain decisions
    or predictions are made by a model.
  prefs: []
  type: TYPE_NORMAL
- en: The approaches developed by Lundberg et al. (2017) and Sundararajan et al. (2017)
    both delve into this topic but offer distinct methodologies and theoretical foundations.
    Lundberg et al. Lundberg and Lee ([2017](#bib.bib18)) introduced SHAP (SHapley
    Additive exPlanations), which provides a unified framework for interpreting predictions.
    SHAP assigns an importance value to each feature for a specific prediction, leveraging
    the concept of Shapley values from cooperative game theory. In contrast, Sundararajan
    et al. Sundararajan et al. ([2017](#bib.bib28)) developed Integrated Gradients,
    another method focusing on the attribution of predictions to input features of
    deep networks. Unlike SHAP, which uses Shapley values, Integrated Gradients relies
    on the integration of gradients along the path from a chosen baseline to the actual
    input. Complementing these approaches, Ribeiro et al. (2016) proposed LIME (Local
    Interpretable Model-agnostic Explanations), which aims to make the predictions
    of any classifier understandable and reliable by learning an interpretable model
    localized around the prediction Ribeiro et al. ([2016](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: Another popular method for understanding neural-network representations is probing.
    Conneau et al. (2018) initially introduced multiple probing tasks designed to
    capture simple linguistic features of sentences, setting a foundation for understanding
    how neural networks encode linguistic properties Conneau et al. ([2018](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: Clark et al. (2019) focused primarily on the behavior of attention heads within
    transformers. They observed that these heads often broadly attend across entire
    sentences, and that attention patterns in the same layer tend to exhibit similar
    behaviors. Crucially, their research links specific attention heads to traditional
    linguistic concepts like syntax and coreference, suggesting a direct relationship
    between the model’s attention mechanisms and linguistic structures Clark et al.
    ([2019](#bib.bib7)), although there is an ongoing debate on the explanatory power
    of attention in neural network Bibal et al. ([2022](#bib.bib4)). Unlike Clark
    et al., who examine what the model attends to, Morris et al. Morris et al. ([2023](#bib.bib19))
    explore how information is preserved and can be retrieved from embeddings, offering
    insights into the reversibility and fidelity of the encoding process. Their method
    involves a multi-step process that iteratively corrects and re-embeds text, demonstrating
    the ability to recover most of the original text inputs exactly. Belrose et al.
    (2023) introduced a technique called causal basis extraction, which aims to identify
    influential features within neural networks Belrose et al. ([2023](#bib.bib3)).
    This method stands out by focusing on the causality within network decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, while chain-of-thought prompting can generate errors during inference,
    requiring complex corrective approaches, in-context learning techniques also face
    challenges in prompt optimization and efficient retrieval. Furthermore, interpreting
    large language models remains problematic, exacerbated by models’ tendency to
    exhibit overconfidence and provide unreliable or unverifiable explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training the ICE-T system consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generating questions: the process begins by generating a series of questions
    designed to prompt the Large Language Model (LLM);'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prompting the LLM: Previously generated questions are used to prompt the LLM
    and collect the yes/no answers;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verbalizing the answers: for each instance within the training dataset, responses
    to prompts are collected and converted into numerical form, thus creating a low-dimensional
    feature vector for each instance;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Training a classifier: Previously obtained vectors, together with their respective
    labels, are then used to train a classifier'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The Inference stage mirrors the training process: the LLM is presented with
    the same collection of questions. The responses obtained are numerically encoded
    in the same manner before being processed by the classifier that was trained during
    the Training stage. Training and inference process is illustrated in Figure [1](#S3.F1
    "Figure 1 ‣ 3.1 Generating questions ‣ 3 Method ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").
    Each step is explained below.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Generating questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train and use the system, we need to create multiple questions that more
    closely reflect the core principles behind the initial yes/no question. Those
    questions should be crafted in a way to uncover some additional details about
    the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a use case where an expert is building a classifier to determine eligibility
    for medical trials based on patient data. In such a scenario, the classifier needs
    to assess various clinical inclusion criteria, which are typically derived from
    patient medical records. One of these criteria could be the patient’s language
    proficiency, for instance, whether they speak English. A naive formulation of
    this question may be to present the question to the LLM in a prompt like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: where the `__RECORDS__` represents the appended textual medical records. Determining
    the answer to that question, which we call the “primary” question, may not be
    easy given the medical records under consideration, requiring an understanding
    of somewhat subtle indicators that show if a patient actually speaks English.
    It is highly unlikely that the medical records will directly state the answer
    to that question.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a series of “secondary” questions such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: may allow the model to answer directly based on the information already contained
    in the documents presented to it, while also serving as strong indicators for
    the primary question. Secondary questions are also yes/no questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating the secondary questions can be done in multiple ways, such as writing
    the questions manually using the expert knowledge or using the LLM to automatically
    generate a fixed size set of questions that might be useful in answering the original
    question. Starting from the primary question $q_{0}$ we generate $n$ additional
    questions, creating a set of all questions $Q=\{q_{0},q_{1}\ldots q_{n}\}$, where
    $|Q|=n+1$. This process is shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Generating
    questions ‣ 3 Method ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance") with a red box, illustrating
    the creation of the questions and using them during the training and inference
    process. The same set $Q$ of questions is used for both training and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The number $n$ of secondary questions is decided based on factors such as:
    number of training samples, availability of the expert knowledge and the level
    of interpretability needed for a specific task. Our prior small-scale experiments
    have shown that secondary questions crafted by experts generally lead to improved
    performance compared to those generated by LLMs. However, in the experiments reported
    here, we chose a straightforward and reproducible approach where we exclusively
    use secondary questions created by an LLM. This choice was made to minimize human
    bias and showcase the method’s effectiveness in scenarios where expert input is
    unavailable. The exact prompts used for creating secondary questions in our experiments
    are described in Section [5](#S5 "5 Experiments ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c19e8f5e2b929b4c2c52b0add334fc1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of training and inference process in ICE-T. In the training
    phase, the process begins by generating questions to prompt an LLM, which then
    provides yes/no answers. These answers are verbalized and converted into numerical
    feature vectors. A classifier is trained using these vectors along with their
    respective labels. During inference, the LLM is prompted with the same questions,
    and the answers are similarly processed to predict outcomes using the trained
    classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Prompting LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The LLMs are prompted in two occasions. First, they are prompted to obtain
    the set of secondary questions $Q$, as described in Section [3.1](#S3.SS1 "3.1
    Generating questions ‣ 3 Method ‣ Interpretable Cross-Examination Technique (ICE-T):
    Using highly informative features to boost LLM performance"). Second, for each
    document, we prompt the LLM with the document and corresponding secondary questions.
    Then, for each question $q_{i}$ the output $a_{i}$ of the LLM is collected, creating
    a set of outputs for each document. The textual outputs are then assigned a numerical
    value and transformed into a feature vector $v_{i}$, through the verbalization
    process explained in Section [3.3](#S3.SS3 "3.3 Verbalizing the answers ‣ 3 Method
    ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Verbalizing the answers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The output of the LLM in response to each prompt is limited to one of three
    possible values: Yes, No, or Unknown, depending on the answer to the question
    posed in the prompt. These responses are subsequently assigned numerical values
    for analysis, with “Yes” translating to 1, “No” to 0, and “Unknown” to 0.5.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Training a classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train a classifier, we use a set $V$ of low-dimensional numerical vectors,
    where $|V|=n+1$ and corresponding labels $X$, where each vector $v_{i}$ has a
    corresponding binary label $x_{i}$. Vectors $V$ are obtained from the training
    textual data after prompting LLM to generate $n+1$ outputs that are then assigned
    a numerical value. A classifier is then trained using a 5-fold cross-validation
    process and grid search for the best parameters. A choice of a specific classification
    algorithm will depend on the size of training data, values distribution and desired
    performance on a specific classification metric.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work utilizes data compiled from a range of sources, attempting to include
    a variety of domains and document lengths. The data used in the experiments described
    here spans the fields of medicine, law, climate science, and politics. It also
    includes documents of varying sizes, from brief tweets to extensive legal documents
    and detailed medical records.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Clinical trials
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This dataset comes from Track 1 of the 2018 National NLP Clinical Challenges
    (n2c2) shared tasks³³3https://n2c2.dbmi.hms.harvard.edu/. It is designed to help
    in identifying patients within a corpus of longitudinal medical records who either
    meet or do not meet predefined selection criteria. These criteria are used for
    determining a patient’s eligibility for inclusion in clinical trials. Stubbs et al.
    ([2019](#bib.bib27)). The data consists of annotated American English clinical
    narratives for 288 patients according to whether they met a set of specific criteria.
    There are 13 criteria in total, and they include: DRUG-ABUSE: Drug abuse, current
    or past; ALCOHOL-ABUSE: Current alcohol use over weekly recommended limits; ENGLISH:
    Patient must speak English; MAKES-DECISIONS: Patient must make their own medical
    decisions; ABDOMINAL: History of intra-abdominal surgery, small or large intestine
    resection, or small bowel obstruction; MAJOR-DIABETES: Major diabetes-related
    complication; ADVANCED-CAD: Advanced cardiovascular disease (CAD); MI-6MOS: MI
    in the past 6 months; KETO-1YR: Diagnosis of ketoacidosis in the past year; DIETSUPP-2MOS:
    Taken a dietary supplement (excluding vitamin D) in the past 2 months; ASP-FOR-MI:
    Use of aspirin to prevent MI; HBA1C: Any hemoglobin A1c (HbA1c) value between
    6.5% and 9.5%; and CREATININE: Serum creatinine <math   alttext="></math> upper
    limit of normal. For every medical record, each criterion can have one of two
    potential values: “met” or “not met.” The value based on whether an individual
    has fulfilled a particular criterion. Data is split 70/30 on training and test
    sets respectively. Training test contains 202 medical record while the test set
    contains 86 records. Note that for some criteria, the ratio between positive and
    negative class is highly imbalanced. In our analysis we excluded KETO-1YR criterion
    as it contains no positive samples in the test set and only one positive sample
    in the training set.⁴⁴4Our methodology employs classifiers that are trained based
    on data distributions. As a result, we consistently achieve peak classification
    metrics, which is not a realistic performance, as the minority class is absent
    from the test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Catalonia Independence Corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This dataset contains a corpus in Spanish that consist of annotated Twitter
    messages for automatic stance detection Zotova et al. ([2020](#bib.bib46)). It
    encompasses data collected over a 12-day span in February and March 2019, from
    tweets originating in Barcelona. Originally, each tweet is categorized into one
    of three classes: AGAINST, FAVOR, and NEUTRAL. These classes represent the user’s
    stance towards the topic of Catalonia’s independence. For the purpose of binary
    classification and to facilitate more effective comparisons with other datasets,
    we have omitted the NEUTRAL class, focusing exclusively on the AGAINST and FAVOR
    categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Climate Detection Corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This dataset contains climate-related paragraphs extracted from financial disclosures
    by companies. The text has been collected from corporate annual reports and sustainability
    reports. The paragraphs from those reports are hand-selected and then annotated
    as yes (climate-related) or no (not climate-related) Webersinke et al. ([2021](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Medical health advice data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This dataset comprises a collection of sentences related to the medical domain,
    each accompanied by a label indicating whether the sentence offers medical advice.
    The labels can be one of three values: “strong advice”, “weak advice”, or “no
    advice”. Yu et al. ([2019](#bib.bib41)) For the purpose of binary classification
    task we combined “strong advice” and “weak advice” into a single class: “advice”.
    The dataset includes approximately 8,000 samples, which have been divided into
    training and test datasets following the 80/20 rule.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 The European Court of Human Rights (ECtHR) Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The European Court of Human Rights (ECtHR) hears allegations that a state has
    breached human rights provisions of the European Convention of Human Rights (ECHR) Chalkidis
    et al. ([2019](#bib.bib6)). The dataset for each case includes a series of facts
    in form of paragraphs extracted from the case description. Additionally, each
    case is associated with specific articles of the European Convention on Human
    Rights (ECHR) that may have been violated. In many cases, multiple articles are
    violated at the same time. To make this a binary categorization problem, we adopted
    a binary labeling system. Cases are marked with a “1” if any ECHR articles are
    violated, and a “0” if no violations are detected.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 UNFAIR-ToS Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The UNFAIR-ToS dataset contains 50 relevant on-line consumer contracts, i.e.
    Terms of Service (ToS) from on-line platforms (e.g., YouTube, Ebay, Facebook,
    etc.). Each agreement has been annotated at the sentence level to identify various
    types of potentially unfair clauses, which could infringe upon user rights under
    European consumer law. This dataset categorizes unfair terms into eight distinct
    groups: Arbitration, Unilateral Change, Content Removal, Jurisdiction, Choice
    of Law, Limitation of Liability, Unilateral Termination, and Contract by Using Lippi
    et al. ([2019](#bib.bib15)). To transform the analysis into a binary classification
    problem, we re-labelled each sentence as either “unfair” if it contains any type
    of the identified unfair terms, or “not unfair’ if it does not fall into these
    categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We performed the experiments on a set of binary classification tasks on datasets
    from various domains, as described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the secondary questions, we employed a large language model. Prompting
    it only once, we obtained a set of $n$ secondary questions, which we accepted
    as provided, without any selection or modification. More specifically, we used
    the following prompt for creating all secondary questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'where $n$ is the number of additional questions we want to generate and `primary_question`
    is the primary question used to obtain the main information from the document.
    Note that in all our experiments $n=4$. That means that for each document we use
    one primary and four secondary questions that are treated equally when prompting
    the LLM. Thus, for each document we collect five answers from the LLM that are
    then verbalized (assigned a numerical value) in the next step. To generate the
    secondary questions for all our experiments we use OpenAI’s `gpt-4-0125-preview`
    model. To collect the answers in our experiments we use two generations of OpenAI’s
    models: `gpt-4-0125-preview` Achiam et al. ([2023](#bib.bib1)) and `gpt-3.5-turbo-0125` Brown
    et al. ([2020](#bib.bib5)).'
  prefs: []
  type: TYPE_NORMAL
- en: To choose the best classifier, we train several different classification algorithms.
    These include K-Nearest Neighbors, Decision Trees, Random Forest, Gaussian Naive
    Bayes, Multinomial Naive Bayes, AdaBoost, and XGBoost. We use a 5-fold cross-validation
    on our training data and also perform a grid search to fine-tune the parameters
    for each classifier. After training, we test them on a hold-out test set and choose
    the classifier that gives us the highest Micro F1 score ($\mu F1$). Note that
    one can also adjust the training process to optimize for a specific performance
    metric if needed for a particular application. To perform these experiments, we
    used the `scikit-learn` library in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Micro F1 score is particularly useful in datasets where some classes are significantly
    underrepresented, and where traditional metrics might give a misleading picture
    of model performance. It treats every instance as equally important, thereby giving
    a more accurate measure of the model’s performance across the board. To calculate
    the $\mu F1$, we use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu F1=\frac{2\mu Precision\times\mu Recall}{\mu Precision+\mu Recall}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu Precision=\frac{\sum{TP_{i}}}{\sum{TP_{i}+\sum{FP_{i}}}}$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mu Recall=\frac{\sum{TP_{i}}}{\sum{TP_{i}+\sum{FN_{i}}}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: and $TP$, $FP$ and $FP$ represent number of true positives, false positives
    and false negatives respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we conducted a sensitivity analysis to enhance our understanding
    of the relationship between the number of features and the improvement of the
    $\mu F1$. This analysis helps determine the requisite number of secondary questions
    to attain a desired $\mu F1$. For each dataset, we started by creating $n=9$ secondary
    questions and using the `gpt-3.5-turbo-0125` model to generate responses for each
    sample. The outputs from the large language model were then transformed into 10-dimensional
    feature vectors. Subsequently, we constructed a series of simple Random Forest
    classifiers, starting with a single feature and incrementally adding more features
    up to ten. Given the random selection of features for classification, we repeated
    the experiment 100 times. We computed the $\mu F1$ for each iteration and dataset.
    The findings are detailed in Section [6](#S6 "6 Results ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance")
    and illustrated in Figure [3](#S6.F3 "Figure 3 ‣ 6 Results ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The results of the classification experiments are summarized in Table [1](#S6.T1
    "Table 1 ‣ 6 Results ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance"). We can see that across
    all datasets, the ICE-T method consistently surpasses the zero-shot approach in
    performance for a given language models. Specifically, using the GPT-3.5 model,
    the average $\mu F1$ for the zero-shot approach is 0.683, but it increases to
    0.845 with the ICE-T method. A similar trend is observed with the larger GPT-4
    model, where the average F1 score improves from 0.7 using the zero-shot approach
    to 0.892 with the ICE-T technique. This improvement is not constant across the
    datasets as we can see a significant variations in performance and in improvements
    across different tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | GPT-3.5 | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0-shot | ICE-T | 0-shot | ICE-T |'
  prefs: []
  type: TYPE_TB
- en: '| ABDOMINAL | 0.791 | 0.814 | 0.802 | 0.884 |'
  prefs: []
  type: TYPE_TB
- en: '| ADVANCED-CAD | 0.640 | 0.756 | 0.791 | 0.907 |'
  prefs: []
  type: TYPE_TB
- en: '| ALCOHOL-ABUSE | 0.814 | 0.965 | 0.791 | 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-FOR-MI | 0.849 | 0.86 | 0.860 | 0.895 |'
  prefs: []
  type: TYPE_TB
- en: '| CREATININE | 0.349 | 0.721 | 0.488 | 0.872 |'
  prefs: []
  type: TYPE_TB
- en: '| DIETSUPP-2MOS | 0.593 | 0.767 | 0.488 | 0.814 |'
  prefs: []
  type: TYPE_TB
- en: '| DRUG-ABUSE | 0.942 | 0.977 | 0.826 | 0.977 |'
  prefs: []
  type: TYPE_TB
- en: '| ENGLISH | 0.919 | 0.988 | 0.233 | 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| HBA1C | 0.477 | 0.837 | 0.523 | 0.942 |'
  prefs: []
  type: TYPE_TB
- en: '| MAJOR-DIABETES | 0.733 | 0.814 | 0.570 | 0.884 |'
  prefs: []
  type: TYPE_TB
- en: '| MAKES-DECISIONS | 0.605 | 0.965 | 0.663 | 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| MI-6MOS | 0.651 | 0.919 | 0.849 | 0.953 |'
  prefs: []
  type: TYPE_TB
- en: '| Catalonia indep. | 0.528 | 0.579 | 0.562 | 0.604 |'
  prefs: []
  type: TYPE_TB
- en: '| Climate detection | 0.702 | 0.8 | 0.912 | 0.925 |'
  prefs: []
  type: TYPE_TB
- en: '| ECtHR | 0.853 | 0.873 | 0.861 | 0.873 |'
  prefs: []
  type: TYPE_TB
- en: '| Health advice | 0.836 | 0.841 | 0.846 | 0.846 |'
  prefs: []
  type: TYPE_TB
- en: '| UNFAIR-ToS | 0.335 | 0.887 | 0.837 | 0.889 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 0.683 | 0.845 | 0.700 | 0.892 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison of $\mu F1$ scores between zero-shot setting and ICE-T
    method. The values in bold represent the $\mu F1$ score of the winning approach
    for a specific task and a language model. Horizontal line in the middle splits
    the clinical trial datasets and other datasets. All tasks solve the binary classification
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The upper portion of Table [1](#S6.T1 "Table 1 ‣ 6 Results ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance") showcases the findings from the clinical trial dataset, as detailed
    in Section [4](#S4 "4 Data ‣ Interpretable Cross-Examination Technique (ICE-T):
    Using highly informative features to boost LLM performance"). The dataset’s contents
    remain consistent across all sub-tasks within this clinical trial dataset, though
    each sub-task involves a distinct classification criterion based on 12 different
    criteria. In some sub-tasks, substantial improvements were observed over the zero-shot
    method. For instance, in the task CREATININE (involving serum creatinine levels
    exceeding the upper normal limit), the zero-shot method achieved $\mu F1$ of 0.349\.
    In contrast, the ICE-T technique utilizing the same large language model significantly
    improved this score to 0.721\. Similarly, for the task ENGLISH (determining if
    a patient speaks English) using the larger GPT4 model, the greatest increase noted
    exceeded 0.733 points, with the zero-shot approach at a $\mu F1$ of 0.233 and
    the ICE-T technique improving it to 0.966\. Analysis of tasks outside the clinical
    trial dataset revealed varied results, dependent on the specific domain. The task
    assessing “Catalonia independence” presented a notable challenge in the zero-shot
    setup for both models, barely achieving a $\mu F1$ above 0.5, with no significant
    improvements noted with the ICE-T technique.'
  prefs: []
  type: TYPE_NORMAL
- en: The task related to the European Court of Human Rights (ECtHR) already exhibited
    high baseline scores in the zero-shot setting, achieving 0.853 with GPT-3.5 and
    0.861 with GPT-4\. The application of the ICE-T technique yielded minimal improvement,
    with both models achieving a $\mu F1$ of 0.873\. A similar scenario was observed
    with the Health advice dataset, where enhancements were negligible.
  prefs: []
  type: TYPE_NORMAL
- en: However, the UNFAIR-ToS task demonstrated significant improvement using the
    ICE-T approach, particularly with the GPT-3.5 model. Here, the $\mu F1$ score
    saw a dramatic increase from 0.335 to 0.887.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, our analysis reveals that the ICE-T technique, when applied to
    a smaller model, can surpass or match the performance of a larger model that uses
    the zero-shot approach. In our experiments, we assessed the $\mu F1$ of classification
    tasks executed by GPT-4 in a zero-shot setting against those performed by GPT-3.5
    using the ICE-T technique across various datasets. In nearly all cases, except
    for two, the ICE-T-enhanced GPT-3.5 either outperformed or equaled the larger
    GPT-4 model on identical tasks. These findings are depicted in Figure [2](#S6.F2
    "Figure 2 ‣ 6 Results ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2663624b7277568cfb35f505d31d6e40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparative performance of ICE-T-enhanced GPT-3.5 versus zero-shot
    GPT-4. The figure illustrates the $\mu F1$ achieved by GPT-3.5 utilizing the ICE-T
    technique and GPT-4 in a zero-shot setting across multiple datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We observed a minor variation in performance across different task groups.
    By categorizing clinical trial tasks into one group and other tasks into another,
    we observed a comparable average performance improvement when comparing the zero-shot
    to the ICE-T approach, as detailed in Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional
    results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance") in Appendix [B](#A2 "Appendix B Additional
    results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance"). This consistency underscores the versatility
    of the ICE-T method across various domains and tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore how the number of features impacts the micro F1 score ($\mu F1$),
    we conducted an additional sensitivity analysis. The outcomes of this analysis
    are depicted in Figure [3](#S6.F3 "Figure 3 ‣ 6 Results ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").
    This figure illustrates the change of the $\mu F1$ as we incrementally introduce
    more features (obtained by secondary questions). A solid orange line shows the
    average $\mu F1$ across all datasets, while the surrounding shaded area indicates
    one standard deviation from the mean, based on 100 iterations. As anticipated,
    there is a consistent increase in the micro F1 score with the addition of more
    secondary questions. On average, adding three secondary questions increases the
    $\mu F1$ score from 0.76 to 0.80, with further additions raising it to 0.82.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to highlight that this figure averages results from 17 different
    datasets, using only the Random Forest classifier. Detailed results for each individual
    task are available in Figure [4](#A2.F4 "Figure 4 ‣ Appendix B Additional results
    ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance") in Appendix [B](#A2 "Appendix B Additional
    results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance"). The use of a single classifier in this analysis
    was a deliberate choice to isolate the impact of increasing the number of features,
    thereby minimizing the influence of classifier selection on the results. However,
    this choice may also limit the generalizability of the findings, as it differs
    from previous analyses where the optimal classifier was selected for each task.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/955454c4a44217ae03be00a0f1f767e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Sensitivity Analysis of Feature Count on $\mu F1$ Score. The figure
    illustrates the effect of increasing the number of features (secondary questions)
    on the $\mu F1$ score across 17 datasets. The solid orange line represents the
    average $\mu F1$ score, and the shaded area indicates the first standard deviation
    from the mean across 100 repetitions. The graph demonstrates a consistent improvement
    in $\mu F1$ as more features are added, with key points of increase highlighted
    at specific feature counts.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study introduces the Interpretable Cross-Examination Technique (ICE-T),
    a novel prompting method that integrates LLM responses with traditional classification
    algorithms to improve the performance on binary classification tasks. This technique
    addresses key limitations in zero-shot and few-shot learning by employing a structured,
    multi-prompt approach that transforms qualitative data into quantifiable metrics,
    thus allowing a small, traditional classifier to effectively make decisions. Our
    results confirm that ICE-T consistently surpasses zero-shot baselines across multiple
    datasets and metrics, particularly in scenarios where model interpretability is
    crucial. This prompting strategy also demonstrates the potential for fully automated,
    high-performing AI systems accessible even to non-experts.
  prefs: []
  type: TYPE_NORMAL
- en: The ICE-T method has demonstrated its capability to not only enhance performance
    over the zero-shot approach but also to do so with smaller models that might not
    perform as well in a zero-shot configuration. For example, the improvement in
    the CREATININE and ENGLISH tasks within clinical trials data underscores the method’s
    ability to handle domain-specific challenges that require nuanced understanding,
    which zero-shot configurations typically struggle with.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Implications for Model Interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A major advantage of the ICE-T approach is its interpretability. By generating
    a feature vector based on direct responses to structured prompts, experts can
    trace back the decision-making process, understanding which factors contributed
    most significantly to the model’s classification. This is particularly valuable
    in fields like medicine and law, where decision rationale is as important as accuracy.
    The ability to dissect and validate each step of the model’s reasoning aligns
    with the growing demand for transparency in AI applications, ensuring that decisions
    made by AI systems can be audited and trusted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, ICE-T is particularly valuable in situations where fine-tuning models
    is not viable. Fine-tuned models often suffer from a significant drawback: they
    lack transparency and become “black boxes,” making their decision-making processes
    obscure. This lack of interpretability is particularly problematic in regulated
    sectors such as healthcare, law and finance, where it’s imperative to comprehend
    the basis of each decision. ICE-T overcomes these issues by employing a methodology
    that remains clear and interpretable, avoiding the opaqueness associated with
    fine-tuned systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Limitations and Future Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite its strengths, the ICE-T method has some limitations. The quality of
    the output heavily relies on the initial set of questions generated for the model
    to answer. Poorly formulated questions or those that fail to capture the necessary
    subtleties of the task can limit the effectiveness of this technique. Moreover,
    the reliance on numerical scoring of textual answers might oversimplify complex
    answers.This can lead to a loss of nuance, especially when answers are confined
    to binary outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Future research could explore more sophisticated methods for question generation,
    perhaps incorporating active learning where the system identifies and prioritizes
    questions that would most improve its understanding and performance. Additionally,
    exploring different methods of encoding responses into feature vectors could further
    enhance the model’s accuracy and sensitivity to nuances in text.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the scope of ICE-T to tackle problems beyond binary classification
    could also prove beneficial. Applying this method to multi-class classification
    tasks or even regression problems could test the adaptability and scalability
    of the approach, potentially making it more applicable across a wider array of
    domains. This expansion could lead to significant advancements in the field of
    machine learning where interpretability and accuracy are crucial.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the ICE-T method presents a promising avenue for enhancing the
    performance and interpretability of LLMs in binary classification tasks and beyond.
    By bridging the gap between traditional machine learning techniques and modern
    LLM capabilities, this approach offers a valuable tool for applications demanding
    high accuracy and clear reasoning in decision-making processes. Further refinements
    and adaptations of this technique could significantly impact the deployment of
    AI in critical sectors, enhancing both the reliability and accountability of automated
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The experiment is composed of two primary phases: 1) collecting outputs from
    OpenAI’s ChatGPT models, specifically using either `gpt-4-0125-preview` or `gpt-3.5-turbo-0125`;
    and 2) verbalizing the answers (converting the responses into numerical form),
    training classifiers and evaluating their performance on a hold-out test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to reproduce the verbalization, classfier training and testing is
    available on GitHub: https://github.com/gmuric/ICE-T'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to data usage and confidentiality constraints associated with the clinical
    trial dataset, we are unable to share the complete working code for the first
    phase. However, we provide the outputs of the LLMs we obtained. They are available
    in GitHub repository. We additionally provide pseudo-code that illustrates the
    extraction of outputs from the language models that can be used to reproduce the
    first part of the experiment. The complete references to the data used in the
    experiments are explained in Section [4](#S4 "4 Data ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").
    Additionally, we include a comprehensive list of the questions used to prompt
    the language models in Appendix [A](#A1 "Appendix A Questions used in ICE-T method
    ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance"). The pseudo-code for obtaining the answers
    from the LLM is presented below:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Collecting outputs from LLM
  prefs: []
  type: TYPE_NORMAL
- en: 'for each textual document $t$ do     Prompt LLM with $t$ and corresponding
    questions {Refer to Section [3.2](#S3.SS2 "3.2 Prompting LLM ‣ 3 Method ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance")}     for each question $q_{i}$ related to $t$ do        $a_{i}\leftarrow$
    Output of LLM for $q_{i}$     end for     $A\leftarrow$ Set of all outputs $\{a_{1},a_{2},\ldots,a_{n}\}$     for each
    output $a_{i}$ in $A$ do        $v_{i}\leftarrow$ Numerical value of $a_{i}$ {Refer
    to Section [3.3](#S3.SS3 "3.3 Verbalizing the answers ‣ 3 Method ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance")}     end for     $V_{d}\leftarrow$ Feature vector of all $v_{i}$  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Note that due to the stochastic nature of large language models, the outputs
    may vary with each experiment. While these variations are unlikely to significantly
    impact the results, minor discrepancies are possible.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This material is based upon work supported by the Army ASA(ALT) SBIR CCOE under
    Contract No. W51701-22-C-0035 and the US Air Force under Contract No. FA8750-22-C-0511\.
    Any opinions, findings and conclusions or recommendations expressed in this material
    are those of the author(s) and do not necessarily reflect the views of the Army
    ASA(ALT) SBIR CCOE or the US Air Force.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amodei et al. (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,
    John Schulman, and Dan Mané. 2016. Concrete problems in ai safety. *arXiv preprint
    arXiv:1606.06565*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belrose et al. (2023) Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
    Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting
    latent predictions from transformers with the tuned lens. *arXiv preprint arXiv:2303.08112*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bibal et al. (2022) Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens,
    Xiaoou Wang, Thomas François, and Patrick Watrin. 2022. Is attention explanation?
    an introduction to the debate. In *Proceedings of the 60th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers)*, pages 3889–3900,
    Dublin, Ireland. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chalkidis et al. (2019) Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras.
    2019. [Neural legal judgment prediction in English](https://doi.org/10.18653/v1/P19-1424).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 4317–4323, Florence, Italy. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D
    Manning. 2019. What does bert look at? an analysis of bert’s attention. *arXiv
    preprint arXiv:1906.04341*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conneau et al. (2018) Alexis Conneau, German Kruszewski, Guillaume Lample,
    Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single vector:
    Probing sentence embeddings for linguistic properties. *arXiv preprint arXiv:1805.01070*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodman and Flaxman (2017) Bryce Goodman and Seth Flaxman. 2017. European union
    regulations on algorithmic decision-making and a “right to explanation”. *AI magazine*,
    38(3):50–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021) Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt:
    Pre-trained prompt tuning for few-shot learning. *arXiv preprint arXiv:2109.04332*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasneci et al. (2023) Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria
    Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,
    Eyke Hüllermeier, et al. 2023. Chatgpt for good? on opportunities and challenges
    of large language models for education. *Learning and individual differences*,
    103:102274.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lippi et al. (2019) Marco Lippi, Przemysław Pałka, Giuseppe Contissa, Francesca
    Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor, and Paolo Torroni. 2019. Claudette:
    an automated detector of potentially unfair clauses in online terms of service.
    *Artificial Intelligence and Law*, 27:117–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-$3$?
    *arXiv preprint arXiv:2101.06804*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them:
    Overcoming few-shot prompt order sensitivity. *arXiv preprint arXiv:2104.08786*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg and Lee (2017) Scott M Lundberg and Su-In Lee. 2017. A unified approach
    to interpreting model predictions. *Advances in neural information processing
    systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morris et al. (2023) John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and
    Alexander M Rush. 2023. Text embeddings reveal (almost) as much as text. *arXiv
    preprint arXiv:2310.06816*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation
    with language models. *arXiv preprint arXiv:2112.00114*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perez et al. (2021) Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True
    few-shot learning with language models. *Advances in neural information processing
    systems*, 34:11054–11070.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. "why should i trust you?" explaining the predictions of any classifier.
    In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery
    and data mining*, pages 1135–1144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rubin et al. (2021) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021.
    Learning to retrieve prompts for in-context learning. *arXiv preprint arXiv:2112.08633*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick and Schütze (2020) Timo Schick and Hinrich Schütze. 2020. It’s not just
    size that matters: Small language models are also few-shot learners. *arXiv preprint
    arXiv:2009.07118*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schick and Schütze (2022) Timo Schick and Hinrich Schütze. 2022. True few-shot
    learning with prompts—a real-world perspective. *Transactions of the Association
    for Computational Linguistics*, 10:716–731.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large
    language models. *arXiv preprint arXiv:2402.01761*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stubbs et al. (2019) Amber Stubbs, Michele Filannino, Ergin Soysal, Samuel
    Henry, and Özlem Uzuner. 2019. [Cohort selection for clinical trials: n2c2 2018
    shared task track 1](https://doi.org/10.1093/JAMIA/OCZ163). *Journal of the American
    Medical Informatics Association : JAMIA*, 26:1163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.
    Axiomatic attribution for deep networks. In *Proceedings of the 34th International
    Conference on Machine Learning*, volume 70, pages 3319–3328\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trautmann (2023) Dietrich Trautmann. 2023. Large language model prompt chaining
    for long legal document classification. *arXiv preprint arXiv:2308.04138*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022b. Self-instruct:
    Aligning language models with self-generated instructions. *arXiv preprint arXiv:2212.10560*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Webersinke et al. (2021) Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler,
    and Markus Leippold. 2021. Climatebert: A pretrained language model for climate-related
    text. *arXiv preprint arXiv:2110.12010*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. 2022a. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022b. Chain-of-thought prompting
    elicits reasoning in large language models. In *Advances in Neural Information
    Processing Systems*, volume 35, pages 24824–24837\. Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022c) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022c. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022a) Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022a.
    Ai chains: Transparent and controllable human-ai interaction by chaining large
    language model prompts. In *Proceedings of the 2022 CHI conference on human factors
    in computing systems*, pages 1–22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022b) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    2022b. Self-adaptive in-context learning: An information compression perspective
    for in-context example selection and ordering. *arXiv preprint arXiv:2212.10375*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiong et al. (2023) Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian
    He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation
    of confidence elicitation in llms. *arXiv preprint arXiv:2306.13063*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. 2024. Gpt4tools: Teaching large language model to use tools
    via self-instruction. *Advances in Neural Information Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye and Durrett (2022) Xi Ye and Greg Durrett. 2022. The unreliability of explanations
    in few-shot prompting for textual reasoning. *Advances in neural information processing
    systems*, 35:30378–30392.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Bei Yu, Yingya Li, and Jun Wang. 2019. [Detecting causal language
    use in science findings](https://doi.org/10.18653/v1/D19-1473). In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 4664–4674, Hong Kong, China. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Yiming Zhang, Shi Feng, and Chenhao Tan. 2022a. Active
    example selection for in-context learning. *arXiv preprint arXiv:2211.04486*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b.
    Automatic chain of thought prompting in large language models. *arXiv preprint
    arXiv:2210.03493*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2024) Kaitlyn Zhou, Jena D Hwang, Xiang Ren, and Maarten Sap.
    2024. Relying on the unreliable: The impact of language models’ reluctance to
    express uncertainty. *arXiv preprint arXiv:2401.06730*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level
    prompt engineers. *arXiv preprint arXiv:2211.01910*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zotova et al. (2020) Elena Zotova, Rodrigo Agerri, Manuel Nuñez, and German
    Rigau. 2020. Multilingual stance detection in tweets: The Catalonia independence
    corpus. In *Proceedings of the Twelfth Language Resources and Evaluation Conference*,
    pages 1368–1375, Marseille, France. European Language Resources Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Questions used in ICE-T method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the list of questions used for each task in ICE-T technique. For each
    task, there are five questions in total, where the question with $id=0$ is used
    in a prompt in a zero-shot approach. Other questions are secondary questions obtained
    as explained in Section [3.1](#S3.SS1 "3.1 Generating questions ‣ 3 Method ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance"):'
  prefs: []
  type: TYPE_NORMAL
- en: DRUG-ABUSE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any indication in the patient’s records of current or past drug abuse?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient been prescribed medication with a high potential for abuse?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any notes in the patient’s records indicating substance abuse or dependency
    issues?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient previously sought treatment for substance abuse or addiction?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any irregularities in the patient’s prescription history that suggest
    misuse, such as early refill requests?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ALCOHOL-ABUSE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient consumed alcohol beyond the weekly recommended limits recently?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did the patient consume more than 14 units of alcohol in the past week?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient engaged in binge drinking sessions in the last month?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the patient frequently consume alcohol on more than 5 days in a week?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient expressed concerns about controlling their alcohol intake recently?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ENGLISH
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the patient speak English, according to their medical records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a language preference indicated in the patient’s medical records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the medical record include an interpreter request for non-English languages?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is English listed as the patient’s primary language in the medical records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have previous medical consultations been conducted in English, as noted in the
    patient’s records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MAKES-DECISIONS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there evidence that the patient makes their own medical decisions?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the patient have a documented history of discussing treatment options with
    their healthcare provider?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient previously filled out an advanced directive or a living will?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the patient regularly attend medical appointments alone?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have there been instances where the patient explicitly expressed their treatment
    preferences or declined certain medical interventions?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ABDOMINAL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a record of the patient undergoing intra-abdominal surgery, small or
    large intestine resection, or experiencing a small bowel obstruction?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient ever undergone any form of intra-abdominal surgery?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient had a resection of either the small or large intestine?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a record of the patient experiencing a small bowel obstruction?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient had any surgical intervention related to the digestive system
    that is not explicitly mentioned above?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MAJOR-DIABETES
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the patient have any major diabetes-related complications such as amputation,
    kidney damage, skin conditions, retinopathy, nephropathy, or neuropathy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient experienced any significant changes in their vision or been
    diagnosed with retinopathy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any history of skin conditions, wounds that heal poorly, or any amputations?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the patient have a history of kidney damage or been diagnosed with nephropathy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient reported any persistent numbness, pain, or tingling in their
    extremities indicating neuropathy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ADVANCED-CAD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the patient currently taking two or more medications to treat CAD?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the patient currently being treated for coronary artery disease (CAD)?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the patient taking any medication to manage CAD symptoms?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the patient prescribed more than one medication specifically for CAD?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are the medications the patient is taking to treat CAD being taken concurrently?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MI-6MOS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient had a myocardial infarction within the past 6 months from the
    most recent record date?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient reported any chest pain or symptoms consistent with a myocardial
    infarction in the past 6 months?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient undergone any cardiac diagnostic tests, such as an ECG or troponin
    levels, in the past 6 months?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did the patient receive any treatment specifically for myocardial infarction,
    such as medication, stenting, or bypass surgery, in the past 6 months?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any notation in the patient’s medical records of a confirmed diagnosis
    of myocardial infarction within the past 6 months?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: DIETSUPP-2MOS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient taken any dietary supplements, excluding Vitamin D, in the past
    2 months?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have you taken any vitamins or dietary supplements in the past 2 months, besides
    Vitamin D?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any non-prescription supplements you have consumed regularly in the
    last 60 days?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did you start or continue taking any herbal or nutritional supplements recently,
    except Vitamin D?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Excluding Vitamin D, have you used any health or wellness supplements since
    two months ago?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ASP-FOR-MI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the patient using aspirin to prevent myocardial infarction based on their
    medical records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a history of cardiovascular disease in the patient’s medical records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient been prescribed aspirin for long-term use?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the medical records indicate a doctor’s recommendation for aspirin to prevent
    myocardial infarction?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any mention of aspirin under the patient’s current medications list?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: HBA1C
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have any HbA1c test results been listed in the records with a value between
    6.5 and 9.5
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any HbA1c test results listed in the records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do any HbA1c test results fall within the 6.5 to 9.5
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the HbA1c value specifically mentioned for each test result?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have all the HbA1c test results been recorded and updated in the records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: CREATININE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any indication in the patient’s records of serum creatinine levels
    above the upper limit of normal?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the patient undergone any recent serum creatinine tests?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are the results of the patient’s serum creatinine tests documented in their
    medical records?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the documented serum creatinine levels exceed the established normal range?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has there been any physician commentary or notes indicating concern over the
    patient’s serum creatinine levels?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: UNFAIR-ToS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does this sentence from company’s Terms of Service violate the European Council
    Directive on unfair terms in consumer contracts?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence create a significant imbalance between the parties’ rights
    and obligations to the detriment of the consumer?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the term clearly understandable and transparent to a typical consumer?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the term determine the main subject matter of the contract, or the appropriateness
    of the price or remuneration, in a way that is unfair to the consumer?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the consumer been given an opportunity to negotiate the terms of the contract?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ECtHR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Considering all the facts, did a state breach human rights provisions of the
    European Convention of Human Rights?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did the state fail to provide adequate protection to an individual or group,
    thereby violating Article 2 or 3 concerning the right to life and prohibition
    of torture?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Was there any discrimination in the state’s actions or laws that contravenes
    Article 14 or Protocol 12 regarding the prohibition of discrimination?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did the state unlawfully interfere with privacy, family life, freedom of expression,
    or freedom of assembly and association, as protected under Articles 8 to 11?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Was there a failure by the state to ensure a fair trial or access to a court,
    in violation of Article 6 or 13, concerning the right to a fair trial and the
    right to an effective remedy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Climate detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a given paragraph climate-related?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the paragraph discuss weather, temperature trends, or climate patterns?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the paragraph mention the impact of human activity on the environment?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the paragraph reference scientific studies or data on climate change?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the paragraph address policy or actions taken to mitigate climate impacts?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Health advice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the given sentence contain a medical advice?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence recommend or suggest a specific medication, treatment, or
    remedy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence advise on health behaviors, such as diet, exercise, or sleep
    patterns?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence include any wording that implies or directly states a medical
    diagnosis or prognosis?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence suggest consulting a healthcare professional or seeking medical
    attention?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Catalonia indep.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the given sentence speak in favor of Catalonia independence?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence express a positive opinion about Catalonia’s political autonomy?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence criticize the Spanish government’s policies towards Catalonia?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence highlight benefits or advantages of Catalonia being independent?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the sentence encourage actions or steps towards achieving independence
    for Catalonia?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix B Additional results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the appendix section where additional results are provided.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu F1$ | $MF1$ | $wF1$ | Sample size |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0-shot | ICE-T | 0-shot | ICE-T | 0-shot | ICE-T | Train | Test |'
  prefs: []
  type: TYPE_TB
- en: '| ABDOMINAL | 0.791 | 0.814 | 0.740 | 0.774 | 0.775 | 0.803 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| ADVANCED-CAD | 0.640 | 0.756 | 0.593 | 0.743 | 0.600 | 0.746 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| ALCOHOL-ABUSE | 0.814 | 0.965 | 0.583 | 0.491 | 0.872 | 0.948 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-FOR-MI | 0.849 | 0.86 | 0.73 | 0.728 | 0.834 | 0.838 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| CREATININE | 0.349 | 0.721 | 0.319 | 0.419 | 0.256 | 0.604 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| DIETSUPP-2MOS | 0.593 | 0.767 | 0.562 | 0.765 | 0.564 | 0.766 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRUG-ABUSE | 0.942 | 0.977 | 0.757 | 0.827 | 0.954 | 0.977 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| ENGLISH | 0.919 | 0.988 | 0.810 | 0.978 | 0.910 | 0.989 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| HBA1C | 0.477 | 0.837 | 0.410 | 0.834 | 0.373 | 0.838 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| MAJOR-DIABETES | 0.733 | 0.814 | 0.728 | 0.814 | 0.728 | 0.814 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAKES-DECISIONS | 0.605 | 0.965 | 0.426 | 0.491 | 0.724 | 0.948 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| MI-6MOS | 0.651 | 0.919 | 0.542 | 0.709 | 0.724 | 0.91 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| Catalonia indep. | 0.528 | 0.579 | 0.414 | 0.536 | 0.413 | 0.536 | 2961 |
    1145 |'
  prefs: []
  type: TYPE_TB
- en: '| Climate detection | 0.702 | 0.8 | 0.673 | 0.444 | 0.732 | 0.711 | 1300 |
    400 |'
  prefs: []
  type: TYPE_TB
- en: '| ECtHR | 0.853 | 0.873 | 0.65 | 0.466 | 0.849 | 0.814 | 2384 | 591 |'
  prefs: []
  type: TYPE_TB
- en: '| Health advice | 0.836 | 0.841 | 0.778 | 0.787 | 0.830 | 0.835 | 3470 | 868
    |'
  prefs: []
  type: TYPE_TB
- en: '| UNFAIR-ToS | 0.335 | 0.887 | 0.321 | 0.47 | 0.397 | 0.834 | 3319 | 964 |'
  prefs: []
  type: TYPE_TB
- en: '| $\mu F1$ - Micro F1, $MF1$ - Macro F1, $wF1$ - Weighted F1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Evaluation of ICE-T and Zero-Shot Techniques on GPT-3.5 - This table
    presents a comparison of F1 micro, macro, and weighted scores for various classification
    tasks using GPT-3.5\. It compares the performance metrics between the zero-shot
    approach and the ICE-T technique. Additionally, the table includes sample sizes
    for both training and testing datasets across each task. Notable improvements
    can be seen in several tasks when utilizing the ICE-T technique over the zero-shot
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu F1$ | $MF1$ | $wF1$ | Sample size |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0-shot | ICE-T | 0-shot | ICE-T | 0-shot | ICE-T | Train | Test |'
  prefs: []
  type: TYPE_TB
- en: '| ABDOMINAL | 0.802 | 0.884 | 0.757 | 0.876 | 0.789 | 0.885 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| ADVANCED-CAD | 0.791 | 0.907 | 0.785 | 0.906 | 0.787 | 0.906 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| ALCOHOL-ABUSE | 0.791 | 0.965 | 0.564 | 0.691 | 0.856 | 0.962 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-FOR-MI | 0.860 | 0.895 | 0.770 | 0.802 | 0.854 | 0.881 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| CREATININE | 0.488 | 0.872 | 0.487 | 0.85 | 0.477 | 0.875 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| DIETSUPP-2MOS | 0.488 | 0.814 | 0.328 | 0.814 | 0.336 | 0.814 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRUG-ABUSE | 0.826 | 0.977 | 0.593 | 0.744 | 0.879 | 0.971 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| ENGLISH | 0.233 | 0.965 | 0.231 | 0.925 | 0.206 | 0.963 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| HBA1C | 0.523 | 0.942 | 0.479 | 0.941 | 0.451 | 0.942 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| MAJOR-DIABETES | 0.570 | 0.884 | 0.523 | 0.884 | 0.523 | 0.884 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAKES-DECISIONS | 0.663 | 0.965 | 0.456 | 0.691 | 0.768 | 0.962 | 202 | 86
    |'
  prefs: []
  type: TYPE_TB
- en: '| MI-6MOS | 0.849 | 0.953 | 0.674 | 0.844 | 0.868 | 0.95 | 202 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| Catalonia indep. | 0.562 | 0.604 | 0.462 | 0.57 | 0.463 | 0.57 | 2961 | 1145
    |'
  prefs: []
  type: TYPE_TB
- en: '| Climate detection | 0.912 | 0.925 | 0.878 | 0.896 | 0.917 | 0.929 | 1300
    | 400 |'
  prefs: []
  type: TYPE_TB
- en: '| ECtHR | 0.861 | 0.873 | 0.631 | 0.466 | 0.848 | 0.814 | 2384 | 591 |'
  prefs: []
  type: TYPE_TB
- en: '| Health advice | 0.846 | 0.846 | 0.766 | 0.766 | 0.828 | 0.828 | 3470 | 868
    |'
  prefs: []
  type: TYPE_TB
- en: '| Unfair TOS | 0.837 | 0.889 | 0.63 | 0.489 | 0.844 | 0.839 | 3319 | 964 |'
  prefs: []
  type: TYPE_TB
- en: '| $\mu F1$ - Micro F1, $MF1$ - Macro F1, $wF1$ - Weighted F1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Evaluation of ICE-T and Zero-Shot Techniques on GPT-4 - This table
    presents a comparison of F1 micro, macro, and weighted scores for various classification
    tasks using GPT-4\. It compares the performance metrics between the zero-shot
    approach and the ICE-T technique. Additionally, the table includes sample sizes
    for both training and testing datasets across each task. Notable improvements
    can be seen in several tasks when utilizing the ICE-T technique over the zero-shot
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\mu F1$ | $MF1$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 0-shot | ICE-T | 0-shot | ICE-T |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clinical Trial | GPT3.5 | 0.709 | 0.866 | 0.604 | 0.695 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT4 | 0.673 | 0.915 | 0.560 | 0.803 |'
  prefs: []
  type: TYPE_TB
- en: '| Other | GPT3.5 | 0.600 | 0.777 | 0.546 | 0.559 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT4 | 0.789 | 0.816 | 0.684 | 0.680 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of average performance between Zero-Shot and ICE-T methods
    across Clinical Trial and Other Task Group'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e12d0e193239644023a06b4d6a075c2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Task-Specific Sensitivity Analysis of Feature Count on $\mu F1$ Score.
    Detailed view of the changes in the $\mu F1$ score for individual tasks as the
    number of secondary questions increases. Each plot represents one of the 17 datasets
    analyzed, showing how the micro F1 score varies with the addition of features.
    The data underscores the variability in performance improvements across different
    tasks when using the Random Forest classifier.'
  prefs: []
  type: TYPE_NORMAL
