- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 17:33:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination
    Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.12748](https://ar5iv.labs.arxiv.org/html/2408.12748)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mengya (Mia) Hu¹¹1Equal contributions.,  Rui Xu¹¹1Equal contributions.,   Deren
    Lei¹¹1Equal contributions.,   Yaxi Li¹¹1Equal contributions.,  Mingyu Wang,
  prefs: []
  type: TYPE_NORMAL
- en: Emily Ching,  Eslam Kamal,  Alex Deng Microsoft Responsible AI
  prefs: []
  type: TYPE_NORMAL
- en: '{humia, rxu, derenlei, yaxi.li, mwang, yuetc, eskam, alex.deng}@microsoft.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) are highly capable but face latency challenges
    in real-time applications, such as conducting online hallucination detection.
    To overcome this issue, we propose a novel framework that leverages a small language
    model (SLM) classifier for initial detection, followed by a LLM as constrained
    reasoner to generate detailed explanations for detected hallucinated content.
    This study optimizes the real-time interpretable hallucination detection by introducing
    effective prompting techniques that align LLM-generated explanations with SLM
    decisions. Empirical experiment results demonstrate its effectiveness, thereby
    enhancing the overall user experience.¹¹1[https://github.com/microsoft/ConstrainedReasoner](https://github.com/microsoft/ConstrainedReasoner)
  prefs: []
  type: TYPE_NORMAL
- en: 'SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination
    Detection'
  prefs: []
  type: TYPE_NORMAL
- en: Mengya (Mia) Hu¹¹1Equal contributions.,  Rui Xu¹¹1Equal contributions.,   Deren
    Lei¹¹1Equal contributions.,   Yaxi Li¹¹1Equal contributions.,  Mingyu Wang, Emily
    Ching,  Eslam Kamal,  Alex Deng Microsoft Responsible AI {humia, rxu, derenlei,
    yaxi.li, mwang, yuetc, eskam, alex.deng}@microsoft.com
  prefs: []
  type: TYPE_NORMAL
- en: '²²footnotetext: This is a preprint of an article that is under review.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/825ceb90b7487e7457cbce123ae95ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Hallucination detection with LLM as constrained reasoner: Grounding
    sources and hypothesis pairs are input into a SLM classifier. In most cases, if
    no hallucination is detected, the no hallucination decision will be returned to
    the client directly. However, if a hallucination is detected by SLM, an LLM-based
    constrained reasoner is employed to interpret the SLM’s decision. If the reasoner’s
    analysis aligns with the initial hallucination detection, this information, along
    with the original hypothesis, is relayed to the client. Otherwise, the potentially
    problematic hypothesis is filtered out or used as valuable feedback to further
    refine and improve the upstream SLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite Large Language Models (LLMs) having impressive capabilities Zhou et al.
    ([2020](#bib.bib40)); Wang et al. ([2021](#bib.bib33), [2020](#bib.bib34)); Pagnoni
    et al. ([2021](#bib.bib23)); Dziri et al. ([2021](#bib.bib4)), they are prone
    to hallucinations—responses that are ungrounded from the source Rashkin et al.
    ([2021](#bib.bib25)); Maynez et al. ([2020](#bib.bib18)); Nan et al. ([2021](#bib.bib22));
    Liu et al. ([2023](#bib.bib15)); Shi et al. ([2023](#bib.bib28)); Wei et al. ([2022](#bib.bib35))-undermining
    their reliability and making hallucination detection critical Kaddour et al. ([2023](#bib.bib9));
    Pal et al. ([2023](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Conventional hallucination detection methods, such as classification Kryściński
    et al. ([2019](#bib.bib10)); Zhou et al. ([2020](#bib.bib40)); Zha et al. ([2023](#bib.bib39))
    or ranking Falke et al. ([2019](#bib.bib5)) models, have been effective in their
    domains but often lack interpretability, which is an essential for user trust
    and mitigation Rudin et al. ([2022](#bib.bib26)). Given the recent widespread
    adoption of LLMs, researchers have explored using LLMs for hallucination detection Lei
    et al. ([2023](#bib.bib12)); Lin et al. ([2021](#bib.bib14)); Min et al. ([2023](#bib.bib20));
    Mündler et al. ([2023](#bib.bib21)), utilizing techniques like chain-of-thought
    reasoning Marasović et al. ([2021](#bib.bib17)); Kunz and Kuhlmann ([2024](#bib.bib11));
    Turpin et al. ([2024](#bib.bib32)); Shen et al. ([2023](#bib.bib27)), or finetuning
    an autonomous detection agent at Billion-parameter size Cheng et al. ([2024](#bib.bib3)),
    or checking consistency of different LLM responses per question Manakul et al.
    ([2023](#bib.bib16)). While LLM-based methods provide interpretability, they introduce
    latency challenges, due to their enormous size and the computational overhead
    of processing long source texts Becker et al. ([2024](#bib.bib1)); Jiang et al.
    ([2024](#bib.bib8)). This creates a major challenge for latency-sensitive real-time
    applications²²2[https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/groundedness](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/groundedness).
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose a novel workflow to address this challenge by balancing latency
    and interpretability. Our approach combines a small classification model, which
    in our case is a small language model (SLM), for initial hallucination detection.
    A downstream LLM module, termed a "constrained reasoner," then explains the detected
    hallucinations. This process is illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ SLM Meets LLM: Balancing Latency, Interpretability and Consistency
    in Hallucination Detection"). Considering the relatively infrequent occurrence
    of hallucinations in practical use Cao et al. ([2021](#bib.bib2)); Wu et al. ([2023](#bib.bib37));
    Gu et al. ([2020](#bib.bib7)), the average time cost of using LLMs solely for
    reasoning on hallucinated texts is manageable. Additionally, this approach leverages
    the pre-existing reasoning and explanation capabilities of LLMsMcCoy et al. ([2023](#bib.bib19)),
    obviating the need for substantial domain-specific data and significant computational
    cost on fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Conventional studies have employed LLMs as end-to-end solutions Sobania et al.
    ([2022](#bib.bib30)); Goyal et al. ([2022](#bib.bib6)). More recently, [Shi et al.](#bib.bib29)
    explored the ability of LLMs to explain small classifiers through their latent
    features, showing promising results on non-reasoning tasks. In this study, we
    propose a novel framework to effectively apply this approach to hallucination
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: A potential issue for combining SLM and LLM is the inconsistency between the
    SLM’s decisions and the LLM’s explanations. Even self-rationalization models,
    where explanations are generated alongside primary outputs Wiegreffe et al. ([2021](#bib.bib36)),
    can produce explanations that do not align with the prediction Ye and Durrett
    ([2022](#bib.bib38)). In this study, we focus on addressing such issue in our
    proposed two-stage hallucination detection framework. Additionally, we analyze
    LLM reasonings in relation to SLM decisions and ground truth labels, highlighting
    the potential of LLMs as feedback mechanisms for refining detection processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are two-fold: first, we introduce constrained reasoner for
    hallucination detection that balances latency and interpretability; second, we
    provide a comprehensive analysis of upstream-downstream consistency, offering
    practical solutions to enhance the alignment between detection and explanation.
    We demonstrate its effectiveness on multiple open-source datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We denote the Grounding Source as $X$ and the model generated hypotheses $Y=(y_{1},y_{2},...,y_{n})$.
    The generation process can be expressed as a function $\mathcal{F}:X\rightarrow
    Y$, where $\mathcal{F}$ is the text generation model (e.g., summarization model).
    $y_{i}$, where $i\in[1,n]$, is hallucinated if conflicts with or cannot be verified
    against $X$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To balance latency and interpretability in hallucination detection, we propose
    a novel two-stage framework: a SLM for hallucination detection followed by a LLM-based
    reasoning module, termed "constrained reasoner". The upstream detection can be
    formulated as: $\mathcal{D}:(X,Y)\rightarrow J$ where $J=(j_{1},j_{2},...,j_{n})$
    represents the binary labels decided by the detector $\mathcal{D}$. The subset
    of response sentences $Y$ detected as hallucinations by $\mathcal{D}$ is denoted
    as $H=\{y_{k}\in Y\mid j_{k}=\text{hallucination}\}=(h_{1},...,h_{m})$, where
    $m\leq n$. Only detected potential hallucinations $H$ are passed to downstream
    reasoning module. The constrained reasoner $\mathcal{R}$ provides explanations
    for hallucinations flagged by upstream, $\mathcal{R}:(X,H)\rightarrow E$, where
    $E=(e_{1},...,e_{m})$ contains $m$ explanations, each $e_{k},\text{where }k\in[1,m]$
    corresponding to a hallucinated sentence $h_{k}$ detected by $\mathcal{D}$. $\mathcal{R}$
    is called constrained reasoner because it operates under the given constraint
    that $h_{i}$ is hallucinated, as determined by $\mathcal{D}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, even in self-rationalization models, reasoning results $E$ may not
    align with detection results $J$ even they are generated together Wiegreffe et al.
    ([2021](#bib.bib36)); Ye and Durrett ([2022](#bib.bib38)). The inconsistency can
    be more pronounced in the two-stage frame, where explanations are provided post
    hoc. We define the real intention in explanation $E$ as $S=(s_{1},...,s_{m})$.
    Reasons inconsistent with the upstream decision is thus $\{e_{k}\in E$, where
    $s_{k}=\text{non-hallucination}\}$ (as our framework only passes $\mathcal{R}$
    the detected hallucinations to explan due to the latency concern). There are three
    aspects we want to study regarding the consistency of the constrained reasoner
    $\mathcal{R}$:'
  prefs: []
  type: TYPE_NORMAL
- en: Inconsistency Identification We design a flagging mechanism to ask LLM-based
    $\mathcal{R}$ to signal when it judges the hypothesis as non-hallucination and
    thus unable to provide explanation why the hypothesis is hallucinated. Therefore,
    $e_{k}$ is semi-structured consisting of a free-text reason $t_{k}$ and a flag
    $\hat{s}_{k}$ indicating whether $\mathcal{R}$ thinks the text is hallucination.
    Formally, $e_{k}=(t_{k},\hat{s}_{k})$. We conduct human evaluation, by asking
    annotators to careful read $t_{k}$ and mark $s_{k}$ whether the reason is explaining
    the hypothesis is hallucination. Then, we measure effectiveness of the flagging
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Inconsistency Filtering The simplest mitigation for inconsistent reasonings
    is to filter them out. We assess the reduction of inconsistencies after filtering
    flagged explanations, i.e. ones with $\hat{s}_{k}=\text{non-hallucination}$. We
    compare the remaining true inconsistency rates, i.e. the rate of $s_{k}=\text{non-hallucination}$
    as baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning Feedback The ground truth label for each $y_{i}$ is $g_{i}$, but in
    practice, $j_{i}$ may differ from $g_{i}$ due to SLM imperfections. We explore
    the potential of $\mathcal{R}$ as a feedback mechanism to improve $\mathcal{D}$.
    We compare the flagged inconsistencies, $\hat{s}_{k}$, against the ground truth
    $g_{k}$ to assess $\mathcal{R}$’s performance in identifying non-hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our experiment is designed to study the consistency of reasoning within the
    proposed hallucination detection framework and effective approaches to filter
    inconsistencies. Additionally, we explore the potential of LLMs as feedback mechanisms
    for refining the detection process. We employ GPT4-turbo as $\mathcal{R}$ to elucidate
    the rationale behind hallucination determinations, using the temperature of 0
    and top-p of 0.6\. The experiments are conducted across four datasets: NHNET Shen
    et al. ([2023](#bib.bib27)), FEVER Thorne et al. ([2018](#bib.bib31)), HaluQA
    and HaluSum Li et al. ([2023](#bib.bib13)). We use complete test set of NHNet.
    Due to the size of rest three datasets and GPT resource limitations, we sample
    3000 data per dataset for experimentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate an imperfect SLM classifier, we sample both hallucinated and non-hallucinated
    responses from the datasets, assuming the upstream label as hallucination. Thus
    the groundtruth hallucinated text are the simulated true positive cases, and the
    groundtruth non-hallucinated texts are the the simulated false positive cases.
    The specific ratio of true and false positives from the SLM is irrelevant to our
    study, as our focus is on the inconsistencies of the constrained reasoner rather
    than the performance of the detection algorithm. See appendix [A.1](#A1.SS1 "A.1
    Data distribution ‣ Appendix A Appendix ‣ SLM Meets LLM: Balancing Latency, Interpretability
    and Consistency in Hallucination Detection") for the distribution of hallucinated
    and non-hallucinated examples in each dataset. Human annotators assess whether
    each explanation $e_{k}$ truly explains why a hypothesis is hallucinated or whether
    it actually justifies that the text should not be considered a hallucination.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Methodology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Approach | Fallback when unable to explain | Categorize Hallucinations |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Fallback | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Difference between the three main approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiment focuses on three primary approaches, with their key distinctions
    summarized in Table [1](#S3.T1 "Table 1 ‣ 3.1 Methodology ‣ 3 Experiment ‣ SLM
    Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination
    Detection") (The full prompts are provided in Appendix [A.2](#A1.SS2 "A.2 Constrained
    Reasoner Approaches ‣ Appendix A Appendix ‣ SLM Meets LLM: Balancing Latency,
    Interpretability and Consistency in Hallucination Detection")).'
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla approach simply instructs $\mathcal{R}$ to explain why the text was
    detected as hallucination by $\mathcal{D}$. It does not address how to handle
    inconsistency, i.e. disagreements with the upstream decision. As the reasonings
    are free-text, there is no straightforward mechanism to identify when inconsistencies
    arise. If contradictory explanations are generated, they will be presented to
    the user, which can undermine user trust and experience. It is served as a baseline
    for Inconsistency Filtering comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Fallback approach introduces a flagging mechanism whereby $\mathcal{R}$ can
    respond with "UNKNOWN" to indicate $\hat{s}_{k}=\text{non-hallucination}$ thus
    it cannot provide a suitable explanation. This flag helps signal potential inconsistencies,
    enabling developers to address them effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Categorized approach refines the flagging mechanism by incorporating more granular
    hallucination categories. These categories are derived from the analysis of real
    hallucination data. Among those, a specific category $hallu_{12}$ is used to signal
    inconsistencies where $\hat{s}_{k}=\text{non-hallucination}$. By exposing the
    reasoner to these detailed categories, the goal is to enhance $\mathcal{R}$’s
    understanding of hallucinations and improve its ability to correctly identify
    true hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Result and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inconsistency Identification Table [2](#S4.T2 "Table 2 ‣ 4 Result and Discussion
    ‣ SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination
    Detection") illustrates the performance of identifying real inconsistent reasonings
    using the designed flags. Both methods demonstrate strong precision. However,
    the Fallback approach exhibits poor recall, i.e. often failing to signal inconsistent
    reasons with the designed "UNKNOWN" flag. In contrast, Categorized approach effectively
    categorized the majority of inconsistent reasonings under the $hallu_{12}$ flag,
    making it easier to filter or mitigate them for downstream usage.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Approach | Precision | Recall | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| FEVER | Fallback | 0.997 | 0.212 | 0.350 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 1.000 | 0.997 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| NHNET | Fallback | 0.979 | 0.380 | 0.547 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 0.998 | 0.998 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluQA | Fallback | 0.962 | 0.418 | 0.583 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 1.000 | 0.998 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluSum | Fallback | 1.000 | 0.077 | 0.143 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 1.000 | 0.999 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Inconsistency identification performance based on human evaluations.
    Categorized approach achieves close to perfect performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inconsistency Filtering Filtering reasonings with the designed flag effectively
    reduced inconsistencies between the upstream detection and constrained reasoner
    $\mathcal{R}$, as illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4 Result and Discussion
    ‣ SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination
    Detection"). The Vanilla approach, as expected, showed a high inconsistency rate.
    While the introduction of the "UNKNOWN" category in the Fallback approach reduced
    inconsistencies, its effectiveness was limited by low recall as mentioned above.
    In contrast, the Categorized approach achieved a dramatic reduction across all
    datasets, with a post-filtering rate as low as $\sim 0.1-1\%$, effectively enhancing
    the workflow’s consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f006be5e501597d79570f093baca764.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Inconsistency rate comparison: Categorized approach consistently
    outperforms both the Vanilla and Fallback methods with significant drop in inconsistency
    after applying filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Approach | Precision | Recall | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| FEVER | Fallback | 1.000 | 0.155 | 0.268 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 0.992 | 0.778 | 0.872 |'
  prefs: []
  type: TYPE_TB
- en: '| NHNET | Fallback | 0.936 | 0.100 | 0.181 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 0.807 | 0.820 | 0.813 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluQA | Fallback | 0.968 | 0.201 | 0.333 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 0.901 | 0.610 | 0.727 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluSum | Fallback | 0.792 | 0.013 | 0.026 |'
  prefs: []
  type: TYPE_TB
- en: '| Categorized | 0.763 | 0.669 | 0.713 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Feedback results based on LLM constrained reasoning and ground truth
    labels. Categorized approach consistently achieves higher recall and F1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasoning Feedback As results shown in Table [3](#S4.T3 "Table 3 ‣ 4 Result
    and Discussion ‣ SLM Meets LLM: Balancing Latency, Interpretability and Consistency
    in Hallucination Detection"), the Categorized approach demonstrated strong potential
    as feedback mechanism, outperforming the Fallback method with high recall. It
    achieves a macro-average F1 score of 0.781\. This indicates its capability to
    accurately identify false positives from the SLM, making it a promising feedback
    mechanism for improving the upstream model—an area worth further exploration.
    The high inconsistency rate observed in the Categorized approach before filtering,
    as shown in Figure [2](#S4.F2 "Figure 2 ‣ 4 Result and Discussion ‣ SLM Meets
    LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection"),
    highlights the ability of LLMs like GPT to accurately identify true hallucinations
    when refined hallucination categories are provided, as indicated by the high F1
    in Table [3](#S4.T3 "Table 3 ‣ 4 Result and Discussion ‣ SLM Meets LLM: Balancing
    Latency, Interpretability and Consistency in Hallucination Detection"). This suggests
    that LLM can maintain correct judgments without being easily influenced or swayed
    by specific instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we introduce a practical framework for efficient and interpretable
    hallucination detection by combining SLM for detection with LLM for constrained
    reasoning. Our Categorized prompting strategy with filtering effectively aligns
    LLM explanations with SLM decisions, empirically proven effective on 4 hallucination
    and factual consistency datasets. Furthermore, this strategy shows promise as
    a feedback mechanism for refining SLMs, offering a path toward more robust and
    adaptive systems. While our experiments focus on real-time interpretable hallucination
    detection, the insights gained are broadly applicable, shades lights in improving
    classification decision systems and enhancing SLM capabilities through LLM-based
    constrained interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Becker et al. (2024) Jonas Becker, Jan Philip Wahle, Bela Gipp, and Terry Ruas.
    2024. Text generation: A systematic literature review of tasks, evaluation, and
    challenges. *arXiv preprint arXiv:2405.15604*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2021) Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2021. Hallucinated
    but factual! inspecting the factuality of hallucinations in abstractive summarization.
    *arXiv preprint arXiv:2109.09784*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2024) Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang,
    Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen. 2024. Small agent can also
    rock! empowering small language models as hallucination detector. *arXiv preprint
    arXiv:2406.11277*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dziri et al. (2021) Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter.
    2021. Evaluating groundedness in dialogue systems: The begin benchmark. *arXiv
    preprint arXiv:2105.00071*, 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Falke et al. (2019) Tobias Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama,
    Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness:
    An interesting but challenging application for natural language inference. In
    *Proceedings of the 57th annual meeting of the association for computational linguistics*,
    pages 2214–2220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal et al. (2022) Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News
    summarization and evaluation in the era of gpt-3. *arXiv preprint arXiv:2209.12356*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2020) Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, You Wu, Cong
    Yu, Daniel Finnie, Hongkun Yu, Jiaqi Zhai, and Nicholas Zukoski. 2020. Generating
    representative headlines for news stories. In *Proceedings of The Web Conference
    2020*, pages 1773–1784.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2024) Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. Longrag:
    Enhancing retrieval-augmented generation with long-context llms. *arXiv preprint
    arXiv:2406.15319*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaddour et al. (2023) Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie
    Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and applications
    of large language models. *arXiv preprint arXiv:2307.10169*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kryściński et al. (2019) Wojciech Kryściński, Bryan McCann, Caiming Xiong, and
    Richard Socher. 2019. Evaluating the factual consistency of abstractive text summarization.
    *arXiv preprint arXiv:1910.12840*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kunz and Kuhlmann (2024) Jenny Kunz and Marco Kuhlmann. 2024. Properties and
    challenges of llm-generated explanations. *arXiv preprint arXiv:2402.10532*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2023) Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily
    Ching, and Eslam Kamal. 2023. Chain of natural language inference for reducing
    large language model ungrounded hallucinations. *arXiv e-prints*, pages arXiv–2310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and
    Ji-Rong Wen. 2023. [Halueval: A large-scale hallucination evaluation benchmark
    for large language models](https://arxiv.org/abs/2305.11747).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa:
    Measuring how models mimic human falsehoods. *arXiv preprint arXiv:2109.07958*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
    [SelfCheckGPT: Zero-resource black-box hallucination detection for generative
    large language models](https://doi.org/10.18653/v1/2023.emnlp-main.557). In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    9004–9017, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marasović et al. (2021) Ana Marasović, Iz Beltagy, Doug Downey, and Matthew E
    Peters. 2021. Few-shot self-rationalization with natural language prompts. *arXiv
    preprint arXiv:2111.08284*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.
    2020. On faithfulness and factuality in abstractive summarization. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 1906–1919.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCoy et al. (2023) R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy,
    and Thomas L Griffiths. 2023. Embers of autoregression: Understanding large language
    models through the problem they are trained to solve. *arXiv preprint arXiv:2309.13638*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau
    Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
    Factscore: Fine-grained atomic evaluation of factual precision in long form text
    generation. *arXiv preprint arXiv:2305.14251*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mündler et al. (2023) Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin
    Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation,
    detection and mitigation. *arXiv preprint arXiv:2305.15852*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nan et al. (2021) Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero dos Santos,
    Henghui Zhu, Dejiao Zhang, Kathleen Mckeown, and Bing Xiang. 2021. Entity-level
    factual consistency of abstractive text summarization. In *Proceedings of the
    16th Conference of the European Chapter of the Association for Computational Linguistics:
    Main Volume*, pages 2727–2733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov.
    2021. Understanding factuality in abstractive summarization with frank: A benchmark
    for factuality metrics. *arXiv preprint arXiv:2104.13346*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pal et al. (2023) Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.
    2023. Med-halt: Medical domain hallucination test for large language models. *arXiv
    preprint arXiv:2307.15343*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rashkin et al. (2021) Hannah Rashkin, David Reitter, Gaurav Singh Tomar, and
    Dipanjan Das. 2021. Increasing faithfulness in knowledge-grounded dialogue with
    controllable features. *arXiv preprint arXiv:2107.06963*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rudin et al. (2022) Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia
    Semenova, and Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles
    and 10 grand challenges. *Statistic Surveys*, 16:1–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Jiaming Shen, Jialu Liu, Dan Finnie, Negar Rahmati, Mike
    Bendersky, and Marc Najork. 2023. “why is this misleading?”: Detecting news headline
    hallucinations with explanations. In *Proceedings of the ACM Web Conference 2023*,
    pages 1662–1672.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed Huai hsin Chi, Nathanael Scharli, and Denny Zhou. 2023. [Large language
    models can be easily distracted by irrelevant context](https://api.semanticscholar.org/CorpusID:256459776).
    In *International Conference on Machine Learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2024) Guangsi Shi, Xiaofeng Deng, Linhao Luo, Lijuan Xia, Lei Bao,
    Bei Ye, Fei Du, Shirui Pan, and Yuxiao Li. 2024. Llm-powered explanations: Unraveling
    recommendations through subgraph reasoning. *arXiv preprint arXiv:2406.15859*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sobania et al. (2022) Dominik Sobania, Martin Briesch, and Franz Rothlauf.
    2022. Choose your programming copilot: a comparison of the program synthesis performance
    of github copilot and genetic programming. In *Proceedings of the genetic and
    evolutionary computation conference*, pages 1019–1027.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thorne et al. (2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos,
    and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification.
    In *NAACL-HLT*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turpin et al. (2024) Miles Turpin, Julian Michael, Ethan Perez, and Samuel
    Bowman. 2024. Language models don’t always say what they think: unfaithful explanations
    in chain-of-thought prompting. *Advances in Neural Information Processing Systems*,
    36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Peng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang,
    Jingren Zhou, and Hongxia Yang. 2021. Sketch and refine: Towards faithful and
    informative table-to-text generation. *arXiv preprint arXiv:2105.14778*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou
    Chen. 2020. Towards faithful neural table-to-text generation with content-matching
    constraints. *arXiv preprint arXiv:2005.00969*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiegreffe et al. (2021) Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark
    Riedl, and Yejin Choi. 2021. Reframing human-ai collaboration for generating free-text
    explanations. *arXiv preprint arXiv:2112.08674*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu,
    Randy Zhong, Juntong Song, and Tong Zhang. 2023. Ragtruth: A hallucination corpus
    for developing trustworthy retrieval-augmented language models. *arXiv preprint
    arXiv:2401.00396*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye and Durrett (2022) Xi Ye and Greg Durrett. 2022. The unreliability of explanations
    in few-shot prompting for textual reasoning. *Advances in neural information processing
    systems*, 35:30378–30392.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zha et al. (2023) Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.
    Alignscore: Evaluating factual consistency with a unified alignment function.
    *arXiv preprint arXiv:2305.16739*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco
    Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. 2020. Detecting hallucinated
    content in conditional neural sequence generation. *arXiv preprint arXiv:2011.02593*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Data distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The distribution of hallucinated and non-hallucinated examples in each dataset
    is shown in Table [4](#A1.T4 "Table 4 ‣ A.1 Data distribution ‣ Appendix A Appendix
    ‣ SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination
    Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Hallucination | Non-Hallucination |'
  prefs: []
  type: TYPE_TB
- en: '| NHNET | 216 | 439 |'
  prefs: []
  type: TYPE_TB
- en: '| FEVER | 813 | 2187 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluQA | 1500 | 1500 |'
  prefs: []
  type: TYPE_TB
- en: '| HaluSum | 1500 | 1500 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Dataset statistics. NHNET we use the complete set. Fever, HaluQA and
    HaluSum we random sample 3000 data due to their large size.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Constrained Reasoner Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.2.1 Vanilla prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vanilla prompt shown in [3](#A1.F3 "Figure 3 ‣ A.2.1 Vanilla prompt ‣ A.2 Constrained
    Reasoner Approaches ‣ Appendix A Appendix ‣ SLM Meets LLM: Balancing Latency,
    Interpretability and Consistency in Hallucination Detection") only gives the instruction
    and few-shot examples to do the downstream reasoning task. However, it does not
    specify how LLM should deal with the situation where LLM does not follow the upstream
    decision.'
  prefs: []
  type: TYPE_NORMAL
- en: '-  role:  systemcontent:  |You  are  a  careful  proof-reading  assistant  with  great  logic  thinking  and  solid  english  skills  for  a  documentation  scribe.  Your  important  task  is  to  provide  hallucination  reasons:  given  the  <<Source  Document>>  and  some  <<Sentences>>  that  is  not  supported  by  the  <<Source  Document>>,  you  are  expected  to  give  the  <<Reason>>  why  the  sentence  are  not  supported.If  the  <<Sentences>>  contradict  the  <<Source  Document>>,  you  should  cite  the  evidence  in  the  <<Source  Document>>  and  specify  where  the  contradiction  is.If  the  hallucination  is  because  a  small  part  of  the  <<Sentences>>  is  made  up/  no  information  in  the  <<Source  Document>>  supports/contradicts  the  small  part  of  the  sentence,  please  "PARTIAL  NEUTRAL"  and  specify  which  part  is  not  supported.If  the  hallucination  is  because  the  whole  <<Sentence>>  is  made  up/  no  information  in  the  <<Source  Document>>  supports/contradicts  the  sentence,  please  mark  "NEUTRAL"  to  mark  this  situation.  Please  try  your  best  to  find  the  detailed  reasons  and  only  use  NEUTRAL  as  your  last  resort.If  there  are  part  of  the  <<Sentence>>  contradicts  and  part  of  the  <<Sentence>>  "NEUTRAL",  please  specify  all  the  reasons.The  <<Sentences>>  are  numbered.  You  should  provide  the  <<Reason>>  in  the  same  order  as  the  original  <<Sentences>>.-  role:  systemname:  example_usercontent:  |Let’s  try  it.<<Source  Document>>:The  Academy  Awards,  also  known  as  the  Oscars  are  awards  for  artistic  and  technical  merit  for  the  film  industry.  They  are  presented  annually  by  the  Academy  of  Motion  Picture  Arts  and  Sciences,  in  recognition  of  excellence  in  cinematic  achievements  as  assessed  by  the  Academy’s  voting  membership.  The  Academy  Awards  are  regarded  by  many  as  the  most  prestigious,  significant  awards  in  the  entertainment  industry  in  the  United  States  and  worldwide.  The  awards  ceremony  is  always  hosted  in  the  US.<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  Oscar  is  presented  every  other  two  years.(1).  <<Sentence>>:  Will  Smith  won  the  2022  Oscar.(2).  <<Sentence>>:  The  awards  ceremony  is  always  hosted  in  the  US  in  summer.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  the  source  reference:  "They  are  presented  annually  by  the  Academy  of  Motion  Picture  Arts  and  Sciences",  thus  it  is  not  presented  every  other  two  year.  It’s  contradiction.(1).  NEUTRAL(2).  PARTIAL  NEUTRAL.  The  main  part  of  the  sentence  is  correct,  but  the  grounding  source  did  not  mention  "summer".-  role:  systemname:  example_usercontent:  |Let’s  try  it  again.<<Source  Document>>:Prompts  are  how  you  ask  Copilot  to  do  something  for  you  -  like  creating,  summarizing,  editing,  or  transforming.  Think  about  prompting  like  having  a  conversation,  using  plain  but  clear  language  and  providing  context  like  you  would  with  an  assistant.Also  called  prompt  engineering,  prompting  is  both  an  art  and  a  science.  To  get  the  best  results,  you  need  to  structure  your  prompt  in  a  way  that  the  large  language  model  (LLM)  can  understand.Like  any  other  skill,  prompting  takes  practice  to  perfect.  You  won’t  get  there  overnight.How  to  write  a  good  prompt?<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  Give  clarity  and  Context  and  you  will  do  a  good  job  immediately.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  The  grounding  source  is  a  reference  and  a  user  question.  The  "clarity  and  Context"  in  the  answer  sentence  is  correct,  but  the  "you  will  do  a  good  job  immediately"  contradicts  the  source:  "prompting  takes  practice  to  perfect.  You  won’t  get  there  overnight."-  role:  usercontent:  |<<Source  Document>>:{{transcript}}<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:{{sentences}}<<End  Sentences  need  to  provide  hallucination  reasons>>Give  your  reason  and  begin  your  answer  with  "These  are  hallucinations  because:\n"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Vanilla prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 Fallback Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fallback prompt shown in [4](#A1.F4 "Figure 4 ‣ A.2.2 Fallback Prompt ‣ A.2
    Constrained Reasoner Approaches ‣ Appendix A Appendix ‣ SLM Meets LLM: Balancing
    Latency, Interpretability and Consistency in Hallucination Detection") gives LLM
    an alternative route when it does not agree with the upstream decision and will
    give inconsistent downstream explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: '-  role:  systemcontent:  |You  are  a  careful  proof-reading  assistant  with  great  logic  thinking  and  solid  english  skills  for  a  documentation  scribe.  Your  important  task  is  to  provide  hallucination  reasons:  given  the  <<Source  Document>>  and  some  <<Sentences>>  that  is  not  supported  by  the  <<Source  Document>>,  you  are  expected  to  give  the  <<Reason>>  why  the  sentence  are  not  supported.If  the  <<Sentences>>  contradict  the  <<Source  Document>>,  you  should  cite  the  evidence  in  the  <<Source  Document>>  and  specify  where  the  contradiction  is.If  the  hallucination  is  because  a  small  part  of  the  <<Sentences>>  is  made  up/  no  information  in  the  <<Source  Document>>  supports/contradicts  the  small  part  of  the  sentence,  please  "PARTIAL  NEUTRAL"  and  specify  which  part  is  not  supported.If  the  hallucination  is  because  the  whole  <<Sentence>>  is  made  up/  no  information  in  the  <<Source  Document>>  supports/contradicts  the  sentence,  please  mark  "NEUTRAL"  to  mark  this  situation.  Please  try  your  best  to  find  the  detailed  reasons  and  only  use  NEUTRAL  as  your  last  resort.If  there  are  part  of  the  <<Sentence>>  contradicts  and  part  of  the  <<Sentence>>  "NEUTRAL",  please  specify  all  the  reasons.The  <<Sentences>>  are  numbered.  You  should  provide  the  <<Reason>>  in  the  same  order  as  the  original  <<Sentences>>.In  very  rare  case,  if  you  can  not  find  the  reason  for  the  hallucination  or  you  think  the  <<Sentences>>  is  supported  by  the  <<Source  Document>>,  please  mark  ’UNKNOWN’.-  role:  systemname:  example_usercontent:  |Let’s  try  it.<<Source  Document>>:The  Academy  Awards,  also  known  as  the  Oscars  are  awards  for  artistic  and  technical  merit  for  the  film  industry.  They  are  presented  annually  by  the  Academy  of  Motion  Picture  Arts  and  Sciences,  in  recognition  of  excellence  in  cinematic  achievements  as  assessed  by  the  Academy’s  voting  membership.  The  Academy  Awards  are  regarded  by  many  as  the  most  prestigious,  significant  awards  in  the  entertainment  industry  in  the  United  States  and  worldwide.  The  awards  ceremony  is  always  hosted  in  the  US.<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  Oscar  is  presented  every  other  two  years.(1).  <<Sentence>>:  Will  Smith  won  the  2022  Oscar.(2).  <<Sentence>>:  The  awards  ceremony  is  always  hosted  in  the  US  in  summer.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  the  source  reference:  "They  are  presented  annually  by  the  Academy  of  Motion  Picture  Arts  and  Sciences",  thus  it  is  not  presented  every  other  two  year.  It’s  contradiction.(1).  NEUTRAL(2).  PARTIAL  NEUTRAL.  The  main  part  of  the  sentence  is  correct,  but  the  grounding  source  did  not  mention  "summer".-  role:  systemname:  example_usercontent:  |Let’s  try  it  again.<<Source  Document>>:I  have  bluntly  communicated  to  him  several  times  that  I  would  like  to  see  him  more  than  once  per  week  for  two  hours.  He  said  he  would  try  to  make  more  time  for  me  but  it  has  not  happened  yet  (…)  I  don’t  smoke  weed  anymore,  and  I  don’t  care  if  he  does  but  I’m  hurt  that  he  barely  finds  time  to  spend  with  me.  I  don’t  even  text  or  call  to  ask  him  to  hang  out  because  he’ll  usually  say  he’s  busy  working.  I  know  he’s  busy  and  throughout  the  week  I’m  busy  with  work  too  but  we  rarely  talk.  He’ll  probably  send  a  text  every  so  often,  but  we  usually  go  4-5  days  without  talking.  (…)  Sometimes  the  one  day  I  do  get  to  hang  out  with  him,  he’ll  cancel  if  someone  calls  and  I’ll  be  left  at  dinner/movie  theater/wherever  alone.  I  just  feel  like  I’m  not  wanted.<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  I  feel  like  my  boyfriend  is  neglecting  me  and  I’m  not  sure  what  to  do  about  it.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  UNKNOWN.  Although  original  article  does  not  use  the  word  ’neglecting’,  it  a  lot  of  incidence  that  we  may  inferred  ’neglecting’.  Therefore,  I  can  not  categorize  the  reason  and  need  to  use  UNKNOWN.-  role:  systemname:  example_usercontent:  |Let’s  try  it  again.<<Source  Document>>:Prompts  are  how  you  ask  Copilot  to  do  something  for  you  -  like  creating,  summarizing,  editing,  or  transforming.  Think  about  prompting  like  having  a  conversation,  using  plain  but  clear  language  and  providing  context  like  you  would  with  an  assistant.Also  called  prompt  engineering,  prompting  is  both  an  art  and  a  science.  To  get  the  best  results,  you  need  to  structure  your  prompt  in  a  way  that  the  large  language  model  (LLM)  can  understand.Like  any  other  skill,  prompting  takes  practice  to  perfect.  You  won’t  get  there  overnight.How  to  write  a  good  prompt?<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  Give  clarity  and  Context  and  you  will  do  a  good  job  immediately.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  The  grounding  source  is  a  reference  and  a  user  question.  The  "clarity  and  Context"  in  the  answer  sentence  is  correct,  but  the  "you  will  do  a  good  job  immediately"  contradicts  the  source:  "prompting  takes  practice  to  perfect.  You  won’t  get  there  overnight."-  role:  usercontent:  |<<Source  Document>>:{{transcript}}<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:{{sentences}}<<End  Sentences  need  to  provide  hallucination  reasons>>Give  your  reason  and  begin  your  answer  with  "These  are  hallucinations  because:\n"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Fallback Prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.3 Categorized Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Categorized prompt shown in [5](#A1.F5 "Figure 5 ‣ A.2.3 Categorized Prompt
    ‣ A.2 Constrained Reasoner Approaches ‣ Appendix A Appendix ‣ SLM Meets LLM: Balancing
    Latency, Interpretability and Consistency in Hallucination Detection") gives LLM
    an alternative route when it does not agree with the upstream decision and will
    give inconsistent downstream explanations. Moreover, this prompt asks the LLM
    to categorize the reasons when LLM agrees with the upstream decision as an extra
    confirmation.'
  prefs: []
  type: TYPE_NORMAL
- en: '-  role:  systemcontent:  |You  are  a  careful  proof-reading  assistant  with  great  logic  thinking  and  solid  english  skills  for  a  documentation  scribe.  Your  important  task  is  to  provide  hallucination  reason  categories:  given  the  <<Source  Document>>  and  some  <<Sentences>>  that  is  not  supported  by  the  <<Source  Document>>,  i.e.  the  <<Sentences>>  are  hallucinated,  you  are  expected  to  give  the  <<Hallucination  Categories>>  why  the  sentence  are  not  supported.<<Hallucination  Categories>>  and  their  definitions  are:Hallu_1.  Missing  from  grounding  sources:  At  least  one  factual  claim  in  response  sentence  is  not  present  in  grounding  sources  and  cannot  be  inferred  using  basic  domain/common  knowledge.Hallu_2.  Numeric  value  contradicts  with  ground  source:  Numeric  value  with  similar  context  occurs  in  the  grounding  source  but  the  value  in  response  sentence  contradicts  with  the  value  that  occurs  in  the  grounding  source.Hallu_3.  Negative  to  positive  flip:  At  least  factual  claim  also  occurs  in  the  grounding  sources  but  appears  as  negative  in  the  grounding  source  and  change  to  positive  in  the  response  sentenceHallu_4.  Positive  to  negative  flip:  At  least  factual  claim  also  occurs  in  the  grounding  sources  but  appears  as  positive  in  the  grounding  source  and  change  to  negative  in  the  response  sentenceHallu_5.  Entity  grouped  wrong:  Response  sentence  categorized  an  entity  which  occurs  in  the  grounding  source,  incorrectlyHallu_6.  Url  contradicts  with  ground  source:  Url  with  similar  context  occurs  in  the  grounding  source  but  the  url  in  response  sentence  contradicts  with  the  url  in  the  grounding  sourceHallu_7.  Missing  information  changes  meaning:  A  part  of  the  information  in  the  grounding  source  is  missing  from  the  response.  This  changes  the  meaning  of  the  fact  or  entity  stated.Hallu_8.  Claim  contradicts  with  grounding  source:  Factual  claim  contradicts  with  the  information  in  the  ground  source  (but  the  claim  is  not  a  numeric  value  or  url)Hallu_9.  Pronoun  contradicts  with  grounding  source:  Incorrect  pronouns  cause  overwise  correct  sentence  to  become  incorrect.  (EG  OfficeSum  I:  22,  S:  3:  "He  believes  the  high  number  of  twins  could  be  genetic."  The  grounding  sources  state  that  some  experts  believe  it  could  be  genetic.  The  sentence  is  correct  except  for  the  pronoun  he.)Hallu_10.  Template  sentence  fabricated  hallucination:  Some  summaries  use  templates.  These  templates  elicit  answers  even  when  the  answer  is  not  mentioned  in  the  grounding  source.  For  example:  "patient  is  accompanied  by  adult  female"  is  a  common  template  hallucination.Hallu_11.  Source  misspelt/unintelligible.Hallu_12.  All  others.Try  your  best  to  classify  the  reasons  in  to  the  first  11  categories.  In  very  rare  case,  if  you  can  not  find  the  reason  for  the  hallucination  or  you  think  the  <<Sentences>>  is  supported  by  the  <<Source  Document>>,  please  use  ’Hallu_12’.  That  means  you  can  only  provide  the  above  12  categories!The  <<Sentences>>  are  numbered.  You  should  provide  the  <<Hallucination  Categories>>  in  the  same  order  as  the  original  <<Sentences>>.-  role:  systemname:  example_usercontent:  |Let’s  try  it.<<Source  Document>>:The  Academy  Awards,  also  known  as  the  Oscars  are  awards  for  artistic  and  technical  merit  for  the  film  industry.  They  are  presented  annually  by  the  Academy  of  Motion  Picture  Arts  and  Sciences,  in  recognition  of  excellence  in  cinematic  achievements  as  assessed  by  the  Academy’s  voting  membership.  The  Academy  Awards  are  regarded  by  many  as  the  most  prestigious,  significant  awards  in  the  entertainment  industry  in  the  United  States  and  worldwide.  The  awards  ceremony  is  always  hosted  in  the  US.  A  promotion  about  Oscar  begins  at  3:300PM  June  29,  2012.<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  Oscar  is  presented  every  other  two  years.(1).  <<Sentence>>:  Will  Smith  won  the  2022  Oscar.(2).  <<Sentence>>:  The  awards  ceremony  is  always  hosted  in  the  US  in  summer.(3).  <<Sentence>>:  An  event  begins  at  3:00  PM  and  ends  at  3:00  pm  EST  on  July  5th,  2012.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  Hallu_2.  The  source  reference:  "They  are  presented  annually  by  the  Academy  of  Motion  Picture  Arts  and  Sciences",  thus  it  is  not  presented  every  other  two  year,  i.e.  contradiction  in  numeric  value.(1).  Hallu_1.  The  source  reference  did  not  mention  Will  Smith  won  the  2022  Oscar.(2).  Hallu_1.  The  main  part  of  the  sentence  is  correct,  but  the  grounding  source  did  not  mention  "summer".(3).  Hallu_7.  The  original  article  mentioned  "begins  at  3:300PM  June  29,  2012".  Date  is  missed  in  sentence,  as  the  result  the  meaning  becomes  begin  time  is  3:00PM  July  5th,  2012  which  is  wrong.-  role:  systemname:  example_usercontent:  |Let’s  try  it  again.<<Source  Document>>:Prompts  are  how  you  ask  Copilot  to  do  something  for  you  -  like  creating,  summarizing,  editing,  or  transforming.  Think  about  prompting  like  having  a  conversation,  using  plain  but  clear  language  and  providing  context  like  you  would  with  an  assistant.Also  called  prompt  engineering,  prompting  is  both  an  art  and  a  science.  To  get  the  best  results,  you  need  to  structure  your  prompt  in  a  way  that  the  large  language  model  (LLM)  can  understand.Like  any  other  skill,  prompting  takes  practice  to  perfect.  You  won’t  get  there  overnight.How  to  write  a  good  prompt?<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  Give  clarity  and  Context  and  you  will  do  a  good  job  immediately.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  Hallu_8.  The  grounding  source  is  a  reference  and  a  user  question.  The  "clarity  and  Context"  in  the  answer  sentence  is  correct,  but  the  "you  will  do  a  good  job  immediately"  contradicts  the  source:  "prompting  takes  practice  to  perfect.  You  won’t  get  there  overnight."-  role:  systemname:  example_usercontent:  |Let’s  try  it  again.<<Source  Document>>:I  have  bluntly  communicated  to  him  several  times  that  I  would  like  to  see  him  more  than  once  per  week  for  two  hours.  He  said  he  would  try  to  make  more  time  for  me  but  it  has  not  happened  yet  (…)  I  don’t  smoke  weed  anymore,  and  I  don’t  care  if  he  does  but  I’m  hurt  that  he  barely  finds  time  to  spend  with  me.  I  don’t  even  text  or  call  to  ask  him  to  hang  out  because  he’ll  usually  say  he’s  busy  working.  I  know  he’s  busy  and  throughout  the  week  I’m  busy  with  work  too  but  we  rarely  talk.  He’ll  probably  send  a  text  every  so  often,  but  we  usually  go  4-5  days  without  talking.  (…)  Sometimes  the  one  day  I  do  get  to  hang  out  with  him,  he’ll  cancel  if  someone  calls  and  I’ll  be  left  at  dinner/movie  theater/wherever  alone.  I  just  feel  like  I’m  not  wanted.<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:(0).  <<Sentence>>:  I  feel  like  my  boyfriend  is  neglecting  me  and  I’m  not  sure  what  to  do  about  it.<<End  Sentences  need  to  provide  hallucination  reasons>>-  role:  systemname:  example_assistantcontent:  |These  are  hallucinations  because:(0).  Hallu_12.  Although  original  article  does  not  use  the  word  ’neglecting’,  it  a  lot  of  incidence  that  we  may  inferred  ’neglecting’.  Therefore,  I  can  not  categorize  the  reason  into  the  first  11,  and  need  to  use  H12.-  role:  usercontent:  |<<Source  Document>>:{{transcript}}<<End  Source  Document>><<Sentences  need  to  provide  hallucination  reasons>>:{{sentences}}<<End  Sentences  need  to  provide  hallucination  reasons>>Give  your  reason  and  begin  your  answer  with  "These  are  hallucinations  because:\n"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Categorized Prompt.'
  prefs: []
  type: TYPE_NORMAL
