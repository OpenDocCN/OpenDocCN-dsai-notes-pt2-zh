- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 17:34:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and
    Interpreting Formal Specifications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.18327](https://ar5iv.labs.arxiv.org/html/2403.18327)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![[Uncaptioned image]](img/11a4d9c7a39bb532e8984d70b648b2ff.png) Rushang Karia](https://orcid.org/0000-0002-8421-1133)
    School of Computing and Augmented Intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: Arizona State University
  prefs: []
  type: TYPE_NORMAL
- en: Tempe AZ USA 85281
  prefs: []
  type: TYPE_NORMAL
- en: '{rushang.karia,ddobhal,drbrambl,verma.pulkit,siddharths}@asu.edu Daksh Dobhal
    School of Computing and Augmented Intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: Arizona State University
  prefs: []
  type: TYPE_NORMAL
- en: Tempe AZ USA 85281
  prefs: []
  type: TYPE_NORMAL
- en: '{rushang.karia,ddobhal,drbrambl,verma.pulkit,siddharths}@asu.edu Daniel Bramblett
    School of Computing and Augmented Intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: Arizona State University
  prefs: []
  type: TYPE_NORMAL
- en: Tempe AZ USA 85281
  prefs: []
  type: TYPE_NORMAL
- en: '{rushang.karia,ddobhal,drbrambl,verma.pulkit,siddharths}@asu.edu Pulkit Verma
    School of Computing and Augmented Intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: Arizona State University
  prefs: []
  type: TYPE_NORMAL
- en: Tempe AZ USA 85281
  prefs: []
  type: TYPE_NORMAL
- en: '{rushang.karia,ddobhal,drbrambl,verma.pulkit,siddharths}@asu.edu Siddharth
    Srivastava School of Computing and Augmented Intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: Arizona State University
  prefs: []
  type: TYPE_NORMAL
- en: Tempe AZ USA 85281
  prefs: []
  type: TYPE_NORMAL
- en: '{rushang.karia,ddobhal,drbrambl,verma.pulkit,siddharths}@asu.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stakeholders often describe system requirements using natural language which
    are then converted to formal syntax by a domain-expert leading to increased design
    costs. This paper assesses the capabilities of Large Language Models (LLMs) in
    converting between natural language descriptions and formal specifications. Existing
    work has evaluated the capabilities of LLMs in generating formal syntax such as
    source code but such experiments are typically hand-crafted and use problems that
    are likely to be in the training set of LLMs, and often require human-annotated
    datasets. We propose an approach that can use two copies of an LLM in conjunction
    with an off-the-shelf verifier to automatically evaluate its translation abilities
    without any additional human input. Our approach generates formal syntax using
    language grammars to automatically generate a dataset. We conduct an empirical
    evaluation to measure the accuracy of this translation task and show that SOTA
    LLMs cannot adequately solve this task, limiting their current utility in the
    design of complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: '*Keywords* Large Language Models  $\cdot$ Formal Syntax Translation  $\cdot$
    Truth Assessment'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automatic system synthesis and verification often require specifications to
    be provided in a formal language such as propositional logic (Haubelt and Feldmann,
    [2003](#bib.bib1); Scholl and Becker, [2001](#bib.bib2)). Typically, human experts
    serve as middlemen that can (a) translate natural language (NL) specifications
    of stakeholders to formal syntax, or (b) explain or interpret the system’s functionality
    by translating the system manual into NL.
  prefs: []
  type: TYPE_NORMAL
- en: Given the success of Large Language Models (LLMs) in translation tasks (Xue
    et al., [2021](#bib.bib3)), utilizing LLMs as middlemen can help in reducing overall
    system design costs. Thus, it is vital to develop an evaluation methodology that
    can assess the capabilities of LLMs in such settings. However, developing such
    a methodology is quite difficult. Firstly, obtaining high-quality datasets – such
    as those that contain ground truth data that LLMs have not been trained on – is
    difficult. As LLMs evolve, the dataset would need to evolve as well since it would
    likely be included as a part of the next-gen LLMs training process. Scaling up
    existing datasets is challenging since they require human annotators to encode
    NL text and their formal specifications. Finally, the assessment task must consider
    both the directions of translation; formal-to-natural and natural-to-formal. Existing
    approaches for evaluating LLMs often lack in one of these dimensions. For example,
    there has been plenty of work on SAT reasoning using LLMs (Fan et al., [2023](#bib.bib4);
    Pan et al., [2023](#bib.bib5); Tian et al., [2021a](#bib.bib6)). These methods
    demonstrate that LLMs are not accurate in applications but they do not assess
    LLMs w.r.t. truth maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Contributions   We present, to the best of our knowledge, the first systematic
    approach for evaluating truth maintenance in LLMs. We develop a scalable approach
    for assessing LLMs w.r.t. their capabilities in translating formal syntax in a
    hands-free fashion. Our key contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspired by real-world system specifications, we propose the generation of scalable
    datasets that can be generated randomly using formal syntax grammars and can be
    categorized by complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose an automatic, hands-free approach that allows the bidirectional assessment
    of the translation task using two copies of an LLM by using off-the-shelf verifiers
    to evaluate the translation accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We motivate research in this area by conducting an empirical evaluation and
    showcasing that current SOTA LLMs are lacking even on simple formal specifications
    such as boolean satisfiability (SAT) and first-order logic (FOL).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Automatically Assessing LLM Capabilities in Translation and Interpretation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e0e376b21e3e13f1ca37db7a7f769f79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Our overall process for *NL$\leftrightarrow$FS* for a given formula
    $f$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Formal Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Definition 2.1  (LLM Formal Syntax Translation Task (NL$\rightarrow$FS)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an LLM $L$, a prompt $P$, and a natural language representation NL of
    formal syntax FS, NL$\rightarrow$FS is defined as converting NL to formal syntax
    FS^′ using $L$ and $P$ s.t. FS$\equiv$FS^′.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.2  (LLM Formal Syntax Interpretation Task (FS$\rightarrow$NL)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an LLM $L$, a prompt $P$, and formal syntax FS, FS$\rightarrow$NL is defined
    as converting FS to natural language NL using $L$ and $P$ s.t. NL is an accurate
    representation of FS.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we consider formal specifications that are expressed as boolean
    satisfiability (B-SAT) formulae using propositional logic (Biere et al., [2021](#bib.bib7)).
    Given a set of $n$ boolean variables $\mathcal{X}=\{x_{1},\ldots,x_{n}\}$ and
    boolean operators representing negation $(\neg)$, disjunction $(\lor)$, and conjunction
    $(\land)$, a SAT formula $f$ is obtained by recursively applying the grammar $\mathcal{G}_{\textit{sat}}\rightarrow
    x|p\lor p^{\prime}|p\land p^{\prime}|\neg p$ where $x\in\mathcal{X}$. The canonical
    representation of formulae obtained using $\mathcal{G}_{\textit{sat}}$ is the
    conjunctive normal form (CNF). A formula $f$ is in $(k,m)-$CNF form if $f\equiv
    f_{1}\land\ldots\land f_{m}$ where $f_{i}=p_{1}\lor\ldots\lor p_{k}$ and $p_{i}=\{x_{j},\neg
    x_{j}\}$ with $x_{j}\in\mathcal{X}$. Given an assignment $X$ of truth values to
    every variable in $\mathcal{X}$, $f(X)$ is the truth value of the formula. Two
    formulae $f_{1},f_{2}$ are equivalent $f_{1}\equiv f_{2}$ if they have the same
    truth value for all possible assignments using $\mathcal{X}$, i.e. $\forall Xf_{1}(X)=f_{2}(X)$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Our Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let $\iota$ be a non-deterministic function that interprets formal syntax $f$
    as a natural language string $s$. Similarly, let $\tau$ be a non-deterministic
    function that translates $s$ to $f$. $\iota$ and $\tau$ thus serve as a interpreter
    and translator that can perform *FS$\rightarrow$NL* and *NL$\rightarrow$FS* respectively.
    In general, there are many possible correct interpretations $\iota(f)$ and translations
    $\tau(s)$ for a given $f$ and $s$. Thus, the functions $\iota^{-1}$ and $\tau^{-1}$
    are not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: Our key observation is that if $\iota$ and $\tau$ come from the same system
    (e.g. a neural network or LLM), then we can check the accuracy of the system by
    composing $\iota$ and $\tau$. Let $f$ be a SAT formula. Now, if the system preserves
    truth in both translations, then $\tau(s)$ will be a factual representation of
    $f$ and $f^{\prime}=\tau(\iota(f))$ will be equivalent to $f$ even if $f$ and
    $f^{\prime}$ are not syntactically identical. Since $\iota(f)$ produces natural
    language description it is quite challenging to check whether $\iota(f)$ is a
    factual representation of $f$ without human intervention. However, we can use
    off-the-shelf SAT solvers like Z3 (de Moura and Bjørner, [2008](#bib.bib8)) to
    check if $f\equiv\tau(\iota(f))$.
  prefs: []
  type: TYPE_NORMAL
- en: We use the above insights to automatically assess the *FS$\rightarrow$NL* and
    *NL$\rightarrow$FS* capabilities of LLMs (i.e. $\iota$ and $\tau$ are represented
    by the same LLM). Since LLMs utilize context windows to change their output, we
    use two different copies of the same LLM so that there is no contextual knowledge
    being exchanged between the encode-decode process.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [1](#S2.F1 "Figure 1 ‣ 2 Automatically Assessing LLM Capabilities in Translation
    and Interpretation ‣ Can LLMs Converse Formally? Automatically Assessing LLMs
    in Translating and Interpreting Formal Specifications") illustrates our overall
    process. Given a formula $f$, we use an LLM to convert $f$ to NL using a prompt
    designed for *FS$\rightarrow$NL*. Next, the NL description is input to another
    LLM which is simply a copy of the previous LLM. We use a prompt designed for *NL$\rightarrow$FS*
    to get a formula $f^{\prime}$. Now, $f^{\prime}$ may be syntactically quite different
    from $f$. However, we can use a theorem prover such as Z3 to check whether $f\equiv
    f^{\prime}$.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Generation   We create high-quality datasets by using generators that
    use the formal language’s grammar $\mathcal{G}$ to generate formulae. One key
    benefit of using such generators is that can they can generate formal syntax with
    a certain structure or complexity class. For example, $(k,m)-$CNF generators generate
    structured CNF formulae and it is well-known that for a formula with $n$ variables,
    there is a ratio $r=m/n$ where the difficulty of the problem increases (Selman
    et al., [1996](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Empirical Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used GPT-4 (OpenAI, [2023a](#bib.bib10)), GPT-3.5-turbo (OpenAI, [2023b](#bib.bib11)),
    Mistral-7B-Instruct (Mistral AI, [2023](#bib.bib12)), and Gemini Pro (Google,
    [2023](#bib.bib13)) as the SOTA LLMs in our evaluation. We evaluate whether they
    are effective for *NL$\leftrightarrow$FS*. As a result, we used a simple setting
    $k=n=3$ in creating our dataset. Real-world systems use values for $k,n$ that
    are much higher.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Propositional Logic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8806911c4dd05b98114227e8ad7527ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Accuracies (higher values better) of various SOTA LLMs on *NL$\leftrightarrow$FS*
    on randomly generated formulae. The top y-axis plots the accuracy of the LLM generated
    formulae compared to the ground truty. The bottom y-axis plots the total percent
    of the truth table that is consistent with the ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompts are critical to the performance of LLMs. To ensure that our prompts
    are correct and facilitate translation/interpretation w.r.t. formal syntax, we
    tested our prompts by generating a dataset $\mathcal{D}_{\textit{cnf}}$ of 400
    different $(k,m=rn)-$CNF formulae by varying $r$ from $1.0$ to $7.0$ using a step
    size of $0.5$. CNF formulae are structured and easy to describe making them a
    good benchmark for testing the efficacy of our prompts. The *FS$\rightarrow$NL*
    prompt asks an LLM to convert a SAT formula to an NL description whereas the *NL$\rightarrow$FS*
    prompt asks to convert the NL description to a SAT formula and output only the
    SAT formula with no other text. We iteratively modified our prompts until at least
    one LLM (GPT-4 in our case) was able to achieve $\geq 95\%$ accuracy in *NL$\leftrightarrow$FS*
    on $\mathcal{D}_{\textit{cnf}}$.
  prefs: []
  type: TYPE_NORMAL
- en: CNF formulae serve as a good test bed for prompts but human stakeholders are
    unlikely to understand or describe system capabilities in such a format. Thus,
    our evaluation tests the efficacy of SOTA LLMs on *NL$\leftrightarrow$FS* using
    randomly generated formulae for propositional logic. To do this, we randomly generated
    400 different formulae by recursively applying $\mathcal{G}_{\textit{sat}}$. For
    a CNF formula in $\mathcal{D}_{\textit{cnf}}$ with ratio $r$, we generated a comparable
    random formula such that the total number of operators ($\land$, $\lor$) are equal
    in both.
  prefs: []
  type: TYPE_NORMAL
- en: We used the same prompts for all LLMs. Finally, we used a temperature of $0.1$
    for all models. The temperature influences the randomness of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Results   Our results are presented in Fig.[2](#S3.F2 "Figure 2 ‣ 3.1 Propositional
    Logic ‣ 3 Empirical Evaluation ‣ Can LLMs Converse Formally? Automatically Assessing
    LLMs in Translating and Interpreting Formal Specifications"). It is clear from
    our results that current SOTA LLMs are not performant in the *NL$\leftrightarrow$FS*
    task. As the size of the formula (the total number of conjunctions and disjunctions)
    increases, the performance degrades across all LLMs that we tested. These results
    are even surprising for GPT-4 whose accuracy on comparable CNF formulae was always
    $\geq 95\%$. We describe some of the errors that cause the low accuracy of the
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '*FS$\rightarrow$NL Errors:* One of the most common errors in this translation
    was messing the order of the parentheses. The LLMs were not able to effectively
    describe the formulae taking into account the parentheses. For example, for GPT-3.5-turbo,
    as the size of the formula increased, we noticed that the description did not
    correctly capture the semantics of the formula and the parentheses in expressions
    were often ignored. Gemini and Mistral were not able to correctly describe the
    formula accurately and often missed complete parts of the formula in their description.'
  prefs: []
  type: TYPE_NORMAL
- en: '*NL$\rightarrow$FS Errors:* Hallucinations and negating propositions were common
    errors even when the NL sentence was sufficient for a human to correctly decode
    it to a SAT formula. One of the common issues in GPT-4, which was the best model,
    on larger formulae was that even if the NL description was correct, it would often
    use a different propositional symbol in the formula. For correct NL descriptions,
    GPT-3.5-turbo had trouble reconstructing the formula since similar to *FS$\rightarrow$NL*,
    the parentheses were often misplaced in the translated formula. Finally, Mistral
    and Gemini were not able to correctly decode the formula, often hallucinating
    new expressions or using different propositional symbols. Another big issue with
    Mistral and Gemini was their inability to translate correctly. The prompt instructions
    clearly stated that when converting from *NL$\rightarrow$FS*, the final response
    should only the the formula and no other text. However, both these LLMs consistently
    failed to generate only a formula and would often try to describe the formula
    again before providing the desired response.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 First Order Logic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e43b1a3a87e480806b7f1bf9830877d.png)![Refer to caption](img/5e3a0d8d69e002f78cb056c8c878aaec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Accuracies (higher values better) of various SOTA LLMs on *NL$\leftrightarrow$FS*
    on randomly generated FOL formulae. The top figure plots accuracies w.r.t. FOL
    formulae with randomized, symbolic versions of predicates and constants (e.g.
    $p_{1}(o_{1},o_{2})$) whereas the bottom figure plots accuracies w.r.t. human
    versions of predicates and constants like Friend(John, Mary) etc.'
  prefs: []
  type: TYPE_NORMAL
- en: We generated a dataset $\mathcal{D}_{\textit{fol}}$ of 400 different $n$-operator
    first-order logic formulae constructed from a fixed vocabulary. Extended from
    the CNF prompts, both the *FS$\rightarrow$NL* and *NL$\rightarrow$FS* prompts
    provide the LLM with a description of the predicates. Additionally, the *NL$\rightarrow$FS*
    prompt provides the LLM with FOL grammar and prompts for the FOL formula constructed
    from the NL description to be placed at the end of the response. The prompts were
    refactored til they covered all the translation failure cases detected from each
    LLM using a different random seed.
  prefs: []
  type: TYPE_NORMAL
- en: We randomly generated 400 FOL formulae of the prenex normal form by recursively
    applying $\mathcal{G}_{sat}$ till $n$ operators ($\neg$, $\wedge$, $\lor$) had
    been generated. Then, a quantifier was randomly selected for each variable in
    the vocabulary. The grounded predicates were randomly selected. We used two such
    datasets; the first one had randomized versions of predicates and constants whereas
    the predicates and constants in the second dataset were pulled from an English
    vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: We used the same prompt for each LLM and evaluated the equivalence of the generated
    formulae compared to the originals using Prover9 (McCune, [2005–2010](#bib.bib14)).
    The solver had a 10-minute timeout, and examples that hit it were thrown out.
  prefs: []
  type: TYPE_NORMAL
- en: Results   Our results are presented in Fig.[3](#S3.F3 "Figure 3 ‣ 3.2 First
    Order Logic ‣ 3 Empirical Evaluation ‣ Can LLMs Converse Formally? Automatically
    Assessing LLMs in Translating and Interpreting Formal Specifications"). Note that
    with far fewer operators than CNF results, SOTA LLMs are not performant in the
    *NL$\leftrightarrow$FS* task. Even with a single operator, GPT-4 barely achieved
    80% accuracy with diminishing performance. The increase at the end is due to GPT-4
    results having a higher number of timeouts that, while visually incorrect, are
    thrown out. We observed all the same problems raised in the CNF results.
  prefs: []
  type: TYPE_NORMAL
- en: '*FS$\rightarrow$NL Errors:* The same lack of detail for formula explanations
    was observed, resulting in incorrect formulas being generated. Additionally, it
    was quite common for the LLM to introduce different constants as examples for
    variables, resulting in LLM using them when generating the formulae.'
  prefs: []
  type: TYPE_NORMAL
- en: '*NL$\rightarrow$FS Errors:* The most common mistakes were using the wrong predicate
    arities and changing predicate arguments. GPT-4, in particular, often denoted
    the named constants as other symbols changing the formula’s meaning. Gemini often
    hallucinated different named constants. LLMs struggled with the negation operator
    by either messing up simplifying or distributing them. The most concerning cases
    were when LLM failed to produce a FOL formula. GPT-4 sometimes failed to recognize
    its own explanation as an explanation of a FOL formula. The LLMs often ignored
    the grammar provided and used additional operators, including $\rightarrow,\leftrightarrow,=,$
    and $\neq$. While we parsed these operators, we observed that there was a significantly
    higher likelihood for the LLM messing up.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a large body of work for using LLMs for tasks using formal languages.
    Similarly, there exist several datasets that allow the analysis of LLMs in such
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets RuleTaker (Clark et al., [2020](#bib.bib15)) creates a dataset by applying
    a limited grammar consisting of only conjunctions and disjunctions to generate
    formulae. This makes it quite limited in the types of formulae it can generate.
    FOLIO (Han et al., [2022](#bib.bib16)) comprises first-order logic statements
    that can be used to test the reasoning capabilities of LLMs. The data consists
    of premises and annotated conclusions that can be drawn from the premises thus
    enabling the testing of reasoning capabilities of LLMs. One of the key disadvantages
    of this dataset is that the data was generated using human-experts. LogicNLI (Tian
    et al., [2021b](#bib.bib17)) is a reasoning dataset that generates first-order
    logic formulae from a set of templates and then uses rule-based templates for
    NL generation. Since they utilize a set of templates their datasets are limited
    in the kinds of expressions they can generate.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to existing datasets, our approach can generate arbitrarily complex
    formulae and does not require any human intervention making our approach scalable.
    Furthermore, our datasets can be partitioned into definite classes of hardness
    from a reasoning perspective making them well suited for reasoning-based testing
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: First-order Logic Translation LogicLLaMa (Yang et al., [2023](#bib.bib18)) is
    an LLM trained specifically for the *NL$\rightarrow$FS* task. Inorder to evaluate
    its efficacy, the authors proposed MALLS, a custom generated dataset that can
    be used to evaluate the efficacy of LogicLLaMa. Their approach does not provide
    an automatic way to test the efficacy of LLMs in the translation task and requires
    hand-coded datasets. Moreover, their evaluation metric considers logical equivalence
    using truth tables which can be inefficient. Autoformalization (Wu et al., [2022](#bib.bib19))
    uses pre-trained LLMs together with few-shot prompting to convert NL descriptions
    to formal syntax. This approach requires expert prompting and relevant examples
    to boost performance. Furthermore, this approach relies on existing, annotated
    datasets and thus can be susceptible to LLMs having memorized the answers.
  prefs: []
  type: TYPE_NORMAL
- en: PDDL Translation Simon and Muise ([2021](#bib.bib20)) train Recurrent Neural
    Networks (RNNs) to automatically translate a fragment of formal syntax expressed
    in PDDL to a complete PDDL problem. They train the RNN on publicly available PDDL
    problems using supervised learning. However, their approach cannot use general
    purpose NL descriptions and requires inputs in a formal syntax. Xie et al. ([2023](#bib.bib21))
    use few-shot prompting to translate NL descriptions to PDDL goals and assess them
    using manually designed, domain-dependent metrics. Moreover, their approach cannot
    perform *FS$\rightarrow$NL* translations.
  prefs: []
  type: TYPE_NORMAL
- en: RTL Translation AutoSVA2 (Orenes-Vera et al., [2023](#bib.bib22)) is a framework
    for verification of RTL syntax from NL descriptions. The framework involves manually
    refining prompts iteratively. Next, an LLM is used to generate the RTL syntax
    and an RTL engine is used to validate the result. One key weakness is that their
    approach is not generalizable and requires an engineer-in-the-loop to iteratively
    refine the prompts or different kinds of RTL designs within the same language.
  prefs: []
  type: TYPE_NORMAL
- en: Code Translation Codex (Chen et al., [2021](#bib.bib23)) is an LLM that is capable
    of converting NL descriptions to code and vice versa. However, this approach does
    not provide any automatic way to assess the accuracy of the LLM and it often relies
    on human annotators to verify the results. Furthermore, their testing process
    cannot accurately check whether the translate code matches the NL description
    correctly since they use sampling of input cases in their evaluation. Bhattacharya
    et al. ([2023](#bib.bib24)) evaluate the capability of LLMs to explain code by
    assessing their performance when used with zero-shot, few-shot prompts and instruction
    finetuning. Their approach requires access to hand-coded prompts and cannot work
    with different formal languages. MacNeil et al. ([2023](#bib.bib25)) conduct user-studies
    to evaluate the effectiveness of LLMs in generating code. Their approach requires
    the recruitment of expert humans who know code to assess the quality of the generated
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Our framework provides a general, automatic, handsfree way of assessing LLMs
    in both *NL$\rightarrow$FS* and *FS$\rightarrow$NL* without requiring any expert
    knowledge. Our approach can be easily transferred to other formal languages with
    an appropriate critic.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We develop an approach that allows for effective assessment in the formal translation
    capabilities of SOTA LLMs. Our approach does not require human annotations to
    verify the accuracy of translation. Our results show that there is much to be
    done before LLMs can be deployed in translating formal syntax. We plan to investigate
    the performance of LLMs using different formal languages in future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Haubelt and Feldmann [2003] C. Haubelt and R. Feldmann. SAT-based techniques
    in system synthesis. In *Proc DATE*, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scholl and Becker [2001] Christoph Scholl and Bernd Becker. Checking equivalence
    for partial implementations. In *Proc. DAC*, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. [2021] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual
    pre-trained text-to-text transformer. In *Proc. NAACL-HLT*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. [2023] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng
    Zhang, and Libby Hemphill. NPHardEval: Dynamic benchmark on reasoning ability
    of large language models via complexity classes, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2023] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang
    Wang. Logic-LM: Empowering large language models with symbolic solvers for faithful
    logical reasoning. In *Proc. EMNLP Findings*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. [2021a] Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao
    He, and Yaohui Jin. Diagnosing the first-order logical reasoning ability through
    LogicNLI. In *Proc. EMNLP*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biere et al. [2021] Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh,
    editors. *Handbook of Satisfiability - Second Edition*. IOS Press, 2021. ISBN
    978-1-64368-160-3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de Moura and Bjørner [2008] Leonardo Mendonça de Moura and Nikolaj S. Bjørner.
    Z3: An efficient SMT solver. In *Proc. TACAS*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selman et al. [1996] Bart Selman, David G. Mitchell, and Hector J. Levesque.
    Generating hard satisfiability problems. *AIJ*, 81(1-2):17–29, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI [2023a] OpenAI. Gpt-4-1106-preview. [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf),
    2023a. Accessed: 2023-01-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI [2023b] OpenAI. Gpt-3.5-turbo-0613. [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5),
    2023b. Accessed: 2023-01-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mistral AI [2023] Mistral AI. Mistral-7b instruct v0.2. [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2),
    2023. Accessed: 2023-01-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google [2023] Google. Gemini pro. [https://arxiv.org/pdf/2312.11805.pdf](https://arxiv.org/pdf/2312.11805.pdf),
    2023. Accessed: 2023-01-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCune [2005–2010] W. McCune. Prover9 and mace4. `http://www.cs.unm.edu/~mccune/prover9/`,
    2005–2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2020] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers
    as soft reasoners over language. In *Proc. IJCAI*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2022] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin
    Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell,
    David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong
    Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech
    Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. FOLIO: natural
    language reasoning with first-order logic. *CoRR*, abs/2209.00840, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. [2021b] Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao
    He, and Yaohui Jin. Diagnosing the first-order logical reasoning ability through
    logicnli. In *Proc. EMNLP*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2023] Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, and
    Faramarz Fekri. Harnessing the power of large language models for natural language
    to first-order logic translation. *CoRR*, abs/2305.15541, 2023. doi:[10.48550/ARXIV.2305.15541](https://doi.org/10.48550/ARXIV.2305.15541).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2022] Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe,
    Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large
    language models. In *Proc. NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simon and Muise [2021] Nisha Simon and Christian Muise. A natural language model
    for generating pddl. In *ICAPS KEPS workshop*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. [2023] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold
    Soh. Translating natural language to planning goals with large-language models.
    *CoRR*, abs/2302.05128, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orenes-Vera et al. [2023] Marcelo Orenes-Vera, Margaret Martonosi, and David
    Wentzlaff. Using llms to facilitate formal verification of RTL. *CoRR*, abs/2309.09437,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *CoRR*, abs/2107.03374,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhattacharya et al. [2023] Paheli Bhattacharya, Manojit Chakraborty, Kartheek
    N. S. N. Palepu, Vikas Pandey, Ishan Dindorkar, Rakesh Rajpurohit, and Rishabh
    Gupta. Exploring large language models for code explanation. In *Proceedings of
    the 15th Annual Meeting of the Forum for Information Retrieval Evaluation*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacNeil et al. [2023] Stephen MacNeil, Andrew Tran, Arto Hellas, Joanne Kim,
    Sami Sarsa, Paul Denny, Seth Bernstein, and Juho Leinonen. Experiences from using
    code explanations generated by large language models in a web software development
    e-book. In *Proc. SIGCSE*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
