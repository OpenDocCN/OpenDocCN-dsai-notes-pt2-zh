- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:33:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:33:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.07238](https://ar5iv.labs.arxiv.org/html/2408.07238)
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.07238](https://ar5iv.labs.arxiv.org/html/2408.07238)
- en: \DoubleSpacedXII\TheoremsNumberedThrough\ECRepeatTheorems\EquationsNumberedThrough\RRHFirstLine\LRHFirstLine\ECRRHFirstLine\ECLRHFirstLine\RRHSecondLine
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \DoubleSpacedXII\TheoremsNumberedThrough\ECRepeatTheorems\EquationsNumberedThrough\RRHFirstLine\LRHFirstLine\ECRRHFirstLine\ECLRHFirstLine\RRHSecondLine
- en: Using Advanced LLMs to Enhance Smaller LLMs\LRHSecondLineUsing Advanced LLMs
    to Enhance Smaller LLMs\ECRRHSecondLineUsing Advanced LLMs to Enhance Smaller
    LLMs\ECLRHSecondLineUsing Advanced LLMs to Enhance Smaller LLMs
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先进的大型语言模型提升较小的大型语言模型\LRHSecondLine使用先进的大型语言模型提升较小的大型语言模型\ECRRHSecondLine使用先进的大型语言模型提升较小的大型语言模型\ECLRHSecondLine使用先进的大型语言模型提升较小的大型语言模型
- en: \RUNAUTHOR
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \RUNAUTHOR
- en: Wang et al.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人
- en: \RUNTITLE
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \RUNTITLE
- en: Library-based Interpretable Knowledge Distillation
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图书馆的可解释知识蒸馏
- en: \TITLE
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Better LLMs to Teach Lesser LLMs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型：
- en: '“Strategy” Knowledge Distillation for Dynamic Prompting \TITLEUsing Better
    LLMs to Teach Lesser LLMs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 动态提示的“策略”知识蒸馏 \TITLE使用更好的大型语言模型来教导较小的大型语言模型：
- en: A “Strategy” Knowledge Distillation Approach \TITLEInterpretable “Strategy”
    Knowledge Distillation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一种“策略”知识蒸馏方法 \TITLE可解释的“策略”知识蒸馏
- en: to Teach Lesser LLMs Using Better LLMs
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型
- en: \TITLE
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Better LLMs to Teach Lesser LLMs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型：
- en: Interpretable Knowledge Distillation
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的知识蒸馏
- en: \TITLE
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Better LLMs to Teach Lesser LLMs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型：
- en: Interpretable Knowledge Distillation via Strategy Teaching
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过策略教学的可解释知识蒸馏
- en: \TITLE
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Distilling Knowledge from Advanced LLMs:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从先进的大型语言模型中提取知识：
- en: An Interpretable Approach to Enhance Smaller LLMs
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可解释的方法来提升较小的大型语言模型
- en: \TITLE
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Advanced LLMs to Enhance Smaller LLMs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先进的大型语言模型提升较小的大型语言模型：
- en: An Interpretable Knowledge Distillation Approach
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可解释的知识蒸馏方法
- en: \ARTICLEAUTHORS\AUTHOR
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \ARTICLEAUTHORS\AUTHOR
- en: Tong Wang, K. Sudhir and Dat Hong¹¹1The authors are grateful for insightful
    comments from participants at the 2024 Biz AI conference, 2024 Four Schools (Columbia,
    NYU, Wharton, Yale) Conference, 2024 HongKong Quant Marketing Mini-Conference,
    Summer Workshop on AI for Business 2024, 2024 ISMS Doctoral Symposium in Sydney,
    2024 Monash Marketing Mini-Symposium, Yale Quantitative Marketing Brown Bag Seminar,
    marketing seminars at HKU and CUHK. \AFFYale School of Management
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 汤王、K. 苏迪尔和Dat Hong¹¹1 作者感谢2024年Biz AI会议、2024年四校（哥伦比亚大学、纽约大学、沃顿商学院、耶鲁大学）会议、2024年香港量化营销迷你会议、2024年夏季AI商业研讨会、2024年悉尼ISMS博士生研讨会、2024年蒙纳士大学营销迷你研讨会、耶鲁大学定量营销午餐研讨会、香港大学和中文大学的营销研讨会的参与者提供的有益意见。
    \AFFYale School of Management
- en: \EMAILtong.wang.tw687@yale.edu, k.sudhir@yale.edu, dat.hong@yale.edu
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \EMAILtong.wang.tw687@yale.edu, k.sudhir@yale.edu, dat.hong@yale.edu
- en: \ABSTRACT
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \ABSTRACT
- en: Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior
    performance in complex human-like interactions. But they are costly, or too large
    for edge devices such as smartphones and harder to self-host, leading to security
    and privacy concerns. This paper introduces a novel interpretable knowledge distillation
    approach to enhance the performance of smaller, more economical LLMs that firms
    can self-host. We study this problem in the context of building a customer service
    agent aimed at achieving high customer satisfaction through goal-oriented dialogues.
    Unlike traditional knowledge distillation, where the “student” model learns directly
    from the “teacher” model’s responses via fine-tuning, our interpretable “strategy”
    teaching approach involves the teacher providing strategies to improve the student’s
    performance in various scenarios. This method alternates between a “scenario generation”
    step and a “strategies for improvement” step, creating a customized library of
    scenarios and optimized strategies for automated prompting. The method requires
    only black-box access to both student and teacher models; hence it can be used
    without manipulating model parameters. In our customer service application, the
    method improves performance, and the learned strategies are transferable to other
    LLMs and scenarios beyond the training set. The method’s interpretabilty helps
    safeguard against potential harms through human audit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 高级大型语言模型（LLMs），如GPT-4或LlaMa 3，在复杂的人类互动中表现出色。但它们价格昂贵，或过于庞大而不适合智能手机等边缘设备，并且难以自托管，导致安全和隐私问题。本文介绍了一种新颖的可解释知识蒸馏方法，以增强较小、更经济的LLMs的性能，这些模型可以由公司自托管。我们在构建一个旨在通过目标导向对话实现高客户满意度的客户服务代理的背景下研究这个问题。与传统知识蒸馏不同，传统知识蒸馏中“学生”模型通过微调直接从“教师”模型的响应中学习，我们的可解释“策略”教学方法涉及教师提供改进学生在各种场景下表现的策略。这种方法在“场景生成”步骤和“改进策略”步骤之间交替进行，创建了一个定制的场景库和优化的策略库，用于自动提示。该方法仅需要对学生和教师模型的黑箱访问；因此，可以在不操作模型参数的情况下使用。在我们的客户服务应用中，该方法提高了性能，并且学习到的策略可转移到其他LLMs和超出训练集的场景。该方法的可解释性有助于通过人工审核防范潜在的风险。
- en: \KEYWORDS
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \KEYWORDS
- en: Large Language Models, Knowledge Distillation, Interpretability, Customer Satisfaction
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型、知识蒸馏、可解释性、客户满意度
- en: 1 Introduction
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Advancements in large language models (LLMs) have enabled low-cost automation
    of many marketing tasks. Choosing the right LLM for a task involves balancing
    cost, feasibility, and performance. While high-performance models like GPT-4 are
    appealing and may seem like the obvious choice, many businesses prefer smaller,
    more affordable, or open-source LLMs. In high volume applications such as customer
    service, even small cost differences per query can lead to large differences in
    total cost; as such cheaper or free models like LlaMa may offer a better balance
    in tradeoffs, despite lower performance. Additionally, data privacy concerns drive
    firms to self-host LLMs instead of using third-party APIs, which could compromise
    data confidentiality. Consequently, there is growing interest in enhancing the
    performance of smaller, cost-effective LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进步使得许多营销任务的低成本自动化成为可能。选择适合任务的LLM需要权衡成本、可行性和性能。虽然像GPT-4这样的高性能模型很有吸引力，并且可能看起来是显而易见的选择，但许多企业更喜欢较小、更实惠或开源的LLMs。在客户服务等高频应用中，即使每个查询的小成本差异也可能导致总成本的巨大差异；因此，像LlaMa这样的便宜或免费的模型可能在权衡中提供更好的平衡，尽管其性能较低。此外，数据隐私问题促使公司自托管LLMs，而不是使用可能危及数据机密性的第三方API。因此，越来越多的关注集中在提高较小、经济高效的LLMs的性能上。
- en: This paper examines whether we can augment the effectiveness of smaller, but
    more economical LLMs using the knowledge embedded in larger and superior LLMs.
    This concept is called knowledge distillation, where a more advanced LLM, referred
    to as the “teacher,” imparts its knowledge to a less sophisticated LLM, termed
    the “student,” to enhance its performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了我们是否可以利用嵌入在更大、更高级的语言模型（LLMs）中的知识来增强较小但更经济的LLMs的效果。这个概念被称为知识蒸馏，其中更先进的LLM，称为“教师”，将其知识传授给较不复杂的LLM，称为“学生”，以提高其性能。
- en: While enhancing a weaker LLM through a more advanced LLM is a general problem
    applicable in many contexts, in this paper, we illustrate it for the problem of
    goal-oriented dialogue tasks (e.g. Wei et al. [2018](#bib.bib31), Bordes et al.
    [2016](#bib.bib3)). Goal-oriented dialogues aim to steer conversations toward
    specific desired outcomes (Ham et al. [2020a](#bib.bib8)); as such it is relevant
    for conversations involving negotiations (Samad et al. [2022](#bib.bib25)) and
    persuasion (Wang et al. [2019](#bib.bib30)). Specifically, we consider a customer
    service application, where an LLM acts as a customer service agent that interacts
    with the customer with the goal of achieving high customer satisfaction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过更高级的 LLM 来提升较弱的 LLM 是一个适用于许多背景的普遍问题，但在本文中，我们将其应用于目标导向对话任务（例如 Wei et al.
    [2018](#bib.bib31)，Bordes et al. [2016](#bib.bib3)）。目标导向对话旨在将对话引导到特定的期望结果（Ham
    et al. [2020a](#bib.bib8)）；因此，它对涉及谈判（Samad et al. [2022](#bib.bib25)）和劝说（Wang
    et al. [2019](#bib.bib30)）的对话具有相关性。具体来说，我们考虑一个客户服务应用，其中一个 LLM 作为客户服务代表与客户互动，目标是实现高客户满意度。
- en: A common strategy for improving student performance through knowledge transfer
    from a teacher is fine-tuning, where the teacher generates training data to train
    the student (e.g., Tang et al. [2019](#bib.bib29), Agarwal et al. [2023](#bib.bib2),
    [2024](#bib.bib1)). However, this method has significant limitations. Firstly,
    fine-tuning requires access to and updates of model parameters, which is not always
    feasible, especially for LLMs that only allow API access; and also costly since
    it involves updating billions of parameters. Secondly, the distilled “knowledge”
    is encapsulated in model parameters, making it unintelligible to humans. This
    opacity hinders debugging and raises safety concerns, particularly when the teacher
    is an external, unverifiable model. The inaccessibility of distilled knowledge
    complicates maintenance, as tracking changes and ensuring consistency with previous
    iterations can be problematic. Finally, fine-tuning focuses only on lexical similarities
    in the outputs, ignoring the underlying strategies that lead a model to produce
    specific responses and the fact that different lexical choices can express identical
    meanings or follow the same response strategy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过知识传递来提高学生表现的一种常见策略是微调，其中教师生成训练数据来训练学生（例如，Tang et al. [2019](#bib.bib29)，Agarwal
    et al. [2023](#bib.bib2)，[2024](#bib.bib1)）。然而，这种方法有显著的局限性。首先，微调需要访问和更新模型参数，这并非总是可行，尤其是对于仅允许
    API 访问的 LLM（大型语言模型）；而且成本高，因为涉及到更新数十亿个参数。其次，提炼出的“知识”被封装在模型参数中，使其对人类不可理解。这种不透明性妨碍了调试并引发了安全问题，特别是当教师是一个外部的、不可验证的模型时。提炼知识的不可访问性使维护变得复杂，因为跟踪变化并确保与之前的迭代保持一致可能会遇到问题。最后，微调仅关注输出中的词汇相似性，忽略了导致模型产生特定回应的基本策略，以及不同的词汇选择可以表达相同的含义或遵循相同的回应策略这一事实。
- en: A student model can also be improved via prompt tuning (Lester et al. [2021](#bib.bib16),
    Hu et al. [2021](#bib.bib13)), also known as soft prompt tuning), where specific
    prompt tokens are fine-tuned to elicit better performance. This involves adjusting
    the prompt to align with desired outputs, enhancing the model’s ability to generate
    accurate and contextually appropriate responses. However, prompt tuning lacks
    interpretability, as the prompts are represented as embeddings that are hard to
    understand. Additionally, a single, fixed prompt tuned for an entire task cannot
    capture the richness and diversity of strategies needed for different scenarios.
    Furthermore, prompt tuning requires access to the LLM’s internals, making it inapplicable
    to LLMs that are only accessible via APIs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型还可以通过提示调优（Lester et al. [2021](#bib.bib16)，Hu et al. [2021](#bib.bib13)，也称为软提示调优）来改进，其中特定的提示令牌被微调以引发更好的表现。这涉及调整提示以与期望的输出对齐，从而增强模型生成准确且上下文适当的回应的能力。然而，提示调优缺乏可解释性，因为提示被表示为难以理解的嵌入。此外，为整个任务调优的单一固定提示无法捕捉到不同场景所需的丰富性和多样性。此外，提示调优需要访问
    LLM 的内部，这使得其不适用于仅通过 API 访问的 LLM。
- en: Therefore, we propose an interpretable knowledge distillation method. Instead
    of directly distilling teacher’s “knowledge” into student’s model parameters,
    which would require accessing and updating the student model, our method constructs
    a knowledge base that the student can query externally without altering the student
    model itself. This knowledge base, which we refer to as a library, consists of
    representative scenarios that the student could encounter during deployment, along
    with corresponding strategies. Each scenario is represented by an on-going dialogue
    between an agent and a customer, and the corresponding strategies guide the student
    on *how* to respond when continuing the dialogue. Then, during deployment, the
    library functions similarly to retrieval-augmented generation (RAG) (Lewis et al.
    [2020](#bib.bib17)); the student LLM identifies the most relevant scenarios through
    embeddings and applies the corresponding strategies to stimulate an appropriate
    response.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提出了一种可解释的知识蒸馏方法。我们的方法不直接将教师的“知识”蒸馏到学生的模型参数中，这样做需要访问和更新学生模型。相反，我们的方法构建了一个知识库，学生可以在不更改学生模型本身的情况下，外部查询这个知识库。我们称这个知识库为库，它包含了学生在部署过程中可能遇到的代表性场景及其相应的策略。每个场景由代理和客户之间的持续对话表示，相应的策略指导学生在继续对话时*如何*回应。然后，在部署过程中，库的功能类似于检索增强生成（RAG）（Lewis等人
    [2020](#bib.bib17)）；学生LLM通过嵌入识别最相关的场景，并应用相应的策略来刺激适当的回应。
- en: Building a library for a multi-step, goal-oriented dialog problem like customer
    service presents additional challenges. First, generating scenarios involves interactive
    dialogues between the teacher and a customer to capture diverse conversational
    paths. However, during deployment, the student cannot perfectly replicate the
    teacher’s quality of interactions, leading to deviations. In multi-step settings
    such as conversations, even minor quality differences from the teacher’s responses
    at a given stage can lead to more negative customer response, and this negativity
    can cumulatively amplify over the entire conversation. As such, a student-customer
    conversation can veer into unencountered scenarios if the library is constructed
    based on only teacher-customer conversation scenarios. This phenomenon, known
    as distribution shift, poses a significant challenge when the student’s training
    data is generated exclusively from the teacher. Second, the strategies in the
    library must be adaptable to each student and specific scenario. These strategies
    should guide the student’s behavior by considering the student’s current capabilities
    and providing targeted suggestions. Finally, a third important issue is that agents
    must adhere to firm policies, business requirements, and cost constraints when
    addressing customer requests. Thus, these policies and constraints must be integrated
    into the strategy development process to ensure the agent’s solutions are practical
    and compliant with firm policies.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个针对多步骤、目标导向的对话问题（如客户服务）的库会带来额外的挑战。首先，生成场景涉及教师与客户之间的互动对话，以捕捉多样的对话路径。然而，在部署过程中，学生无法完美地复制教师的互动质量，从而导致偏差。在多步骤的设置中，如对话阶段，即使是教师在某一阶段的回应中的小质量差异也会导致客户反应更为负面，这种负面情绪可能会在整个对话中累积放大。因此，如果库仅基于教师-客户对话场景构建，那么学生-客户对话可能会偏离未遇到的场景。这种现象被称为分布转移，当学生的训练数据完全来源于教师时，这会带来显著的挑战。其次，库中的策略必须能够适应每个学生和具体场景。这些策略应通过考虑学生的当前能力并提供针对性的建议来指导学生的行为。最后，第三个重要问题是，代理在处理客户请求时必须遵守严格的政策、业务要求和成本限制。因此，这些政策和限制必须融入到策略开发过程中，以确保代理的解决方案既实用又符合公司政策。
- en: We design a novel method to address the aforementioned challenges. Our method
    is an iterative process where, in each iteration, a new batch of scenarios and
    their corresponding strategies are added to the library. To generate scenarios,
    both the student and teacher interact with the environment (e.g., a donor in a
    persuasion task or a customer in customer service) to produce conversations and
    sample scenarios. To address the issue of distribution shift, we ensure that enough
    interactions involve the student by gradually increasing the probability of selecting
    the student for scenario generation, so that the student eventually dominates
    this process. In the strategy learning phase, strategies are generated and refined
    iteratively. The teacher LLM evaluates both its own and the student’s responses,
    providing targeted strategies for the student to follow. These strategies are
    incorporated into prompts for subsequent refinement rounds, progressively honing
    the student’s ability to mimic the teacher. The library grows iteratively, with
    the teacher monitoring the student’s progress and deciding when to terminate the
    process based on the need for further feedback or significant improvement. The
    output is a customized library of scenarios and strategies, containing context-specific
    knowledge from the teacher that is most relevant to the student and the task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一种新颖的方法来解决上述挑战。我们的方法是一个迭代过程，在每次迭代中，新的场景及其对应的策略会被添加到库中。为了生成场景，学生和教师与环境进行互动（例如，在说服任务中的捐赠者或客户服务中的顾客），以生成对话和样本场景。为了解决分布转移的问题，我们通过逐步增加选择学生生成场景的概率来确保足够的互动，使学生最终主导这一过程。在策略学习阶段，策略被迭代生成和完善。教师LLM评估自己和学生的回答，提供针对性的策略供学生遵循。这些策略被纳入提示中用于后续的完善轮次，逐步提高学生模仿教师的能力。库会迭代增长，教师监控学生的进展，并根据是否需要进一步反馈或显著改善来决定何时终止过程。输出是一个包含教师针对学生和任务最相关的上下文特定知识的定制化场景和策略库。
- en: Our interpretable knowledge distillation approach offers several advantages
    over fine-tuning. First, by teaching strategies rather than responses, our method
    enables LLMs to understand how to handle different scenarios at a strategic level,
    providing a global view of the problem rather than simply mimicking lexical choices.
    Second, our method ensures easy transferability across LLMs and contexts. The
    strategy library allows LLMs to adapt to new, unseen situations, broadening their
    problem-solving capabilities. This library can be used by other student LLMs or
    for related customer service problems, potentially enhancing performance without
    direct training the student LLMs or on specific tasks. Third, the interpretability
    of our approach significantly enhances AI safety. By extracting explicit strategies,
    domain experts can review and understand the LLMs’ decision-making processes.
    This transparency facilitates trust and provides safeguards against misuse, errors,
    or adversarial influences. Finally, as our method does not require access to or
    modification of the student or teacher LLMs’ parameters (both can be queried solely
    via black-box access, such as APIs), it distinguishes itself from traditional
    knowledge distillation methods such as prompt tuning and fine-tuning, which require
    internal model access or parameter updates. This makes it particularly suitable
    for environments where direct model manipulation is impractical or restricted.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可解释知识蒸馏方法相较于微调具有若干优势。首先，通过教授策略而非回答，我们的方法使LLMs能够在战略层面上理解如何处理不同场景，提供了问题的全局视角，而不仅仅是模仿词汇选择。其次，我们的方法确保了在LLMs和环境中的易迁移性。策略库使LLMs能够适应新的、未见过的情况，拓宽其问题解决能力。这个库可以被其他学生LLMs使用或用于相关的客户服务问题，可能提升性能而无需直接训练学生LLMs或针对特定任务。第三，我们方法的可解释性显著增强了AI安全性。通过提取明确的策略，领域专家可以审查和理解LLMs的决策过程。这种透明性促进了信任，并提供了防止误用、错误或对抗性影响的保障。最后，由于我们的方法不需要访问或修改学生或教师LLMs的参数（两者仅通过黑箱访问，如API进行查询），它区别于传统的知识蒸馏方法，如提示调优和微调，这些方法需要内部模型访问或参数更新。这使得它特别适用于直接模型操控不切实际或受限的环境。
- en: 'Based on our empirical application, i.e., multi-turn conversations in customer
    service, our key findings are as follows: First, teaching strategy is more effective
    than teaching responses for multi-turn generation. Second, context-specific strategies
    are more effective than global strategies, since the former can provide more targeted
    strategies for different scenarios. Third, even though the library is learned
    for a particular student LLM and specific contexts, it contains common knowledge
    that is transferrable across models and across contexts.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的实证应用，即客户服务中的多轮对话，我们的主要发现如下：首先，教学策略比教学响应对于多轮生成更为有效。其次，特定情境的策略比全球策略更为有效，因为前者可以为不同场景提供更有针对性的策略。第三，即使库是为特定的学生LLM和特定情境而学习的，它仍然包含可在模型和情境之间转移的常识。
- en: 'The rest of this paper is organized as follows: §[2](#S2 "2 Related Work")
    discusses the related literature on LLM and knowledge distillation. §[3](#S3 "3
    Library-based Interpretable Knowledge Distillation") presents the proposed method,
    while §[4](#S4 "4 Experiments") evaluates it with an extensive set of experiments.
    §[5](#S5 "5 Conclusion") concludes the paper. More analyses are included in the
    online appendix.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：§[2](#S2 "2 相关工作") 讨论了关于LLM和知识蒸馏的相关文献。§[3](#S3 "3 基于库的可解释知识蒸馏")
    介绍了所提出的方法，而§[4](#S4 "4 实验") 则通过一系列广泛的实验对其进行了评估。§[5](#S5 "5 结论") 总结了全文。更多分析内容请参见在线附录。
- en: 2 Related Work
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Our work contributes to the recent but growing literature on marketing applications
    using LLMs. This body of research has typically focused on how LLMs can be used
    for market research and the study of human behavior, while also highlighting attendant
    challenges (Gui and Toubia [2023](#bib.bib7), Qiu et al. [2023](#bib.bib23), Horton
    [2023](#bib.bib11))). Specific marketing research applications include perceptual
    maps (e.g., Li et al. [2024](#bib.bib18)) and conjoint analysis (e.g., Brand et al.
    [2023](#bib.bib4), Gui and Toubia [2023](#bib.bib7)). In contrast, this paper
    is focused on how to effectively adapt and engineer an LLM for marketing tasks
    such as customer service. We will next discuss how our method relates to existing
    literature on knowledge distillation in LLMs and advances the literature on goal-oriented
    dialogs, particularly in multi-turn interactions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作对使用LLM的市场营销应用的最新而不断增长的文献做出了贡献。这些研究通常关注LLM如何用于市场调研和人类行为研究，同时也突出了相关的挑战（Gui和Toubia
    [2023](#bib.bib7)，Qiu等人 [2023](#bib.bib23)，Horton [2023](#bib.bib11)）。具体的市场研究应用包括感知图（例如，Li等人
    [2024](#bib.bib18)）和联合分析（例如，Brand等人 [2023](#bib.bib4)，Gui和Toubia [2023](#bib.bib7)）。相比之下，本文重点讨论如何有效地调整和工程化LLM以用于市场营销任务，例如客户服务。接下来，我们将讨论我们的方法与现有LLM知识蒸馏文献的关系，并推动了目标导向对话的文献，特别是在多轮互动中的进展。
- en: 2.1 Knowledge Distillation for LLM
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM的知识蒸馏
- en: The concept of utilizing a superior model to enhance a less powerful one is
    known as knowledge distillation. This technique was first introduced by Hinton
    et al. ([2015](#bib.bib10)) in the context of supervised learning and has been
    adapted for use with language models in recent years (Sanh et al. [2019](#bib.bib26),
    Sun et al. [2019](#bib.bib28)). In the realm of language models, knowledge distillation
    involves using a larger and more capable LLM to generate data that trains specialized,
    smaller models. Existing research on knowledge distillation for language models
    typically employs objective functions that either maximize the likelihood of high-probability
    sequences generated by the teacher model (Kim and Rush [2016](#bib.bib15)) or
    guide the student model to mimic the token-level probability distributions provided
    by the teacher (Sanh et al. [2019](#bib.bib26)). Some recent work also proposes
    to teach the student the rationale for solving a task (Hsieh et al. [2023](#bib.bib12),
    Magister et al. [2022](#bib.bib20)). However, all these methods require training
    the student model and updating its parameters. In our approach, however, we distill
    the knowledge into an external library that the student can query during inference,
    without the need for training the student model. The teacher’s knowledge is utilized
    by the student through retrieval-augmented generation (RAG), another popular technique
    in LLM. In doing so, our method requires only black-box access to the student
    model, such as through an API, which is not feasible with existing knowledge distillation
    techniques.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 利用更强大的模型来增强较弱模型的概念被称为知识蒸馏。这一技术最早由Hinton et al. ([2015](#bib.bib10)) 在监督学习的背景下提出，并在近年来被调整用于语言模型（Sanh
    et al. [2019](#bib.bib26)、Sun et al. [2019](#bib.bib28)）。在语言模型领域，知识蒸馏涉及使用更大、更强的LLM生成数据，以训练专门的、更小的模型。现有的语言模型知识蒸馏研究通常采用的目标函数是最大化教师模型生成的高概率序列的似然（Kim
    and Rush [2016](#bib.bib15)），或指导学生模型模仿教师提供的令牌级概率分布（Sanh et al. [2019](#bib.bib26)）。一些近期的研究还提出了教学生模型解决任务的理由（Hsieh
    et al. [2023](#bib.bib12)、Magister et al. [2022](#bib.bib20)）。然而，所有这些方法都需要训练学生模型并更新其参数。我们的方式则不同，我们将知识蒸馏到一个外部库中，学生模型可以在推理期间进行查询，无需训练学生模型。教师的知识通过检索增强生成（RAG），另一种在LLM中流行的技术，被学生模型利用。在这样做的过程中，我们的方法仅需对学生模型进行黑箱访问，例如通过API，这在现有的知识蒸馏技术中并不可行。
- en: 2.2 Goal-Oriented Dialogues
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 以目标为导向的对话
- en: Recent advancements in LLMs have significantly improved their application in
    complex goal-oriented dialogues (e.g., Ham et al. [2020b](#bib.bib9), Li et al.
    [2023](#bib.bib19), Snell et al. [2022](#bib.bib27)), but challenges and limitations
    remain. First, smaller LLMs often lack a strategic understanding of overall dialogue
    progression and fail to achieve dialogue objectives through multi-turn interactions
    (Cheng et al. [2024](#bib.bib5), Deng et al. [2023](#bib.bib6)). Second, the multi-step
    nature of goal-oriented dialogues fundamentally differs from one-step tasks like
    text classification and summarization as fine-tuning at each utterance level overlooks
    the interdependence of multi-turn utterances and the high-level strategy. Zhang
    et al. ([2023](#bib.bib33)) proposes an “Ask an Expert” solution, where a lesser
    model seeks advice from a better LLM for generating utterances, but this increases
    inference (i.e., response) time. Finally, some approaches rely on dialogues with
    specifically annotated strategies for training (e.g., Zhang et al. [2022](#bib.bib32),
    [2023](#bib.bib33), Joshi et al. [2021](#bib.bib14)), but the dependence on labeled
    datasets creates a significant barrier for practical application.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大语言模型（LLMs）方面的进展显著提升了它们在复杂的目标导向对话中的应用（例如，Ham et al. [2020b](#bib.bib9)、Li
    et al. [2023](#bib.bib19)、Snell et al. [2022](#bib.bib27)），但仍然存在挑战和局限性。首先，小型的LLMs通常缺乏对整体对话进展的战略理解，并且无法通过多轮交互实现对话目标（Cheng
    et al. [2024](#bib.bib5)、Deng et al. [2023](#bib.bib6)）。其次，目标导向对话的多步骤性质与文本分类和摘要等单步骤任务根本不同，因为在每次发言级别进行微调忽略了多轮发言之间的相互依赖和高级策略。Zhang
    et al. ([2023](#bib.bib33)) 提出了一个“请教专家”的解决方案，其中较小的模型寻求更优秀的LLM的建议来生成发言，但这增加了推理（即响应）时间。最后，一些方法依赖于具有特定标注策略的对话进行训练（例如，Zhang
    et al. [2022](#bib.bib32)、[2023](#bib.bib33)、Joshi et al. [2021](#bib.bib14)），但对标注数据集的依赖为实际应用创造了重大障碍。
- en: 3 Library-based Interpretable Knowledge Distillation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于库的可解释知识蒸馏
- en: Given a student LLM, denoted as $\mathcal{S}(\cdot)$ (e.g., LlaMa 2 or GPT-3.5),
    our method involves creating a library consisting of a set of representative scenarios,
    paired with corresponding strategies constructed by a teacher LLM $\mathcal{T}(\cdot)$
    optimized for instructing the students on *how* to respond in those scenarios.
    We first set up the learning environment and then describe the algorithm. Then,
    we will show how the library is used during deployment and explain the benefits
    of our method.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个学生语言模型，记作$\mathcal{S}(\cdot)$（例如LlaMa 2或GPT-3.5），我们的方法涉及创建一个库，包含一组代表性场景，以及由教师语言模型$\mathcal{T}(\cdot)$优化构建的相应策略，用于指导学生在这些场景中*如何*回应。我们首先建立学习环境，然后描述算法。接着，我们将展示库在部署过程中的使用方式，并解释我们方法的好处。
- en: 3.1 Learning Environment
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 学习环境
- en: We set up a learning environment where the student attempts to improve its performance
    by mimicking the teacher. For each input, the teacher compares the student’s output
    with its own output to help the student respond like itself. To facilitate effective
    learning, we simulate a customer using GPT-4, which we denote as $\mathcal{C}(\cdot)$.
    The learning of the student relies on the interaction with the customer LLM. This
    design is motivated by recent research that advocates using LLMs to simulate human
    responses to reduce costs and improve efficiency (Li et al. [2024](#bib.bib18)),
    compared to conducting real human studies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立了一个学习环境，让学生通过模仿教师来提高其表现。对于每个输入，教师将学生的输出与自己的输出进行比较，以帮助学生像自己一样回应。为了促进有效学习，我们使用GPT-4模拟客户，称之为$\mathcal{C}(\cdot)$。学生的学习依赖于与客户语言模型的互动。这一设计受到近期研究的启发，研究建议使用语言模型来模拟人类响应，以降低成本并提高效率（Li
    et al. [2024](#bib.bib18)），与进行真实的人类研究相比。
- en: '3.1.1 Simulating Customers:'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 模拟客户：
- en: To simulate the customer, we describe the task in the prompt where we request
    the LLM to role-play as a customer calling an airline company to request customer
    service. Here, we focus on a specific context where the customer bought a restricted
    ticket (non-changeable and non-refundable) and requests to cancel it without penalty.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟客户，我们在提示中描述任务，请求语言模型扮演一个打电话给航空公司请求客户服务的客户角色。在这里，我们专注于一个具体的情境，即客户购买了一张受限票（不可更改且不可退款），并请求在没有罚款的情况下取消它。
- en: 'To increase the heterogeneity of the customers and the richness of the conversations,
    we vary the customer’s social styles, initial emotions, and difficulty. For social
    styles, we use four types based on the classification from Merrill and Reid ([1981](#bib.bib21)).
    (i) Driver: results-driven, confident, and assertive; (ii) Analytical: detail-oriented,
    systematic, and logical; (iii) Amiable: cooperative, empathetic, and relationship-focused;
    and (iv) Expressive: enthusiastic, creative, and spontaneous²²2We provide more
    detailed description of each type in the prompt for simulating customers with
    GPT-4.. For initial emotions, we set four different customer emotions when initiating
    the call: calm, confused, concerned, and frustrated, leading to varied dialogue
    developments. Additionally, we vary the difficulty level of the customer by including/not
    including the keyword “demanding” in the customer role description to the LLM.
    We observe that including “demanding” changes customer behavior, making customers
    more persistent with their requests. We use $q(s,e,d)$ to represent the prompt
    for the customer LLM, parameterized by the social style $s$, initial emotion $e$,
    and difficulty level $d$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加客户的异质性和对话的丰富性，我们变化客户的社交风格、初始情感和难度。社交风格方面，我们使用四种类型，这些类型基于Merrill和Reid（[1981](#bib.bib21)）的分类。（i）驱动型：结果导向、自信且果断；（ii）分析型：注重细节、系统且逻辑；（iii）友善型：合作、富有同情心且以关系为重点；（iv）表达型：热情、富有创意且即兴²²2我们在模拟客户的提示中提供了每种类型的更详细描述。对于初始情感，我们设定了四种不同的客户情感：冷静、困惑、关切和沮丧，导致对话发展的变化。此外，我们通过在客户角色描述中包含/不包含关键词“demanding”来变化客户的难度水平。我们观察到包含“demanding”会改变客户的行为，使客户在请求时更加坚持。我们使用$q(s,e,d)$来表示客户语言模型的提示，参数化为社交风格$s$、初始情感$e$和难度水平$d$。
- en: Given the four social styles, four initial emotions, and two difficulty levels,
    we create a total of 32 types of customers. When simulating customers, each of
    the 32 types generates many different conversations because we set the temperature
    of the LLM to non-zero.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于四种社交风格、四种初始情感和两个难度等级，我们创建了总共32种类型的客户。在模拟客户时，由于我们将语言模型的温度设置为非零，每种类型的客户会生成许多不同的对话。
- en: '3.1.2 Simulating Teacher:'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 模拟教师：
- en: 'We then describe how to prompt a teacher LLM to act as an agent. Here we choose
    the state-of-the-art LLM, GPT-4, as the teacher. We define a base prompt instructing
    GPT-4 to role-play as a customer service agent. We denote the prompt as $p_{\text{base}}$,
    which contains three key components: role, goal, and constraints.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们描述了如何提示教师 LLM 扮演代理角色。在这里，我们选择最先进的 LLM GPT-4 作为教师。我们定义了一个基础提示，指示 GPT-4 扮演客服代理的角色。我们将这个提示表示为
    $p_{\text{base}}$，它包含三个关键组件：角色、目标和约束。
- en: The role specifies the position or function that GPT-4 needs to assume. For
    example, prompting GPT-4 to “role-play as a customer service agent” sets the context
    for the interaction, guiding the model to generate responses suitable for customer
    service scenarios. The goal establishes the desired outcome and ultimate objective
    of the interaction, guiding GPT-4 to adjust its strategies accordingly. For instance,
    if the goal is to achieve high customer satisfaction, GPT-4 will tailor its responses
    to be more empathetic, helpful, and solution-focused. Including the goal in the
    prompt is essential because it provides clear direction for the LLM, ensuring
    that its responses align with the intended results. The constraint establishes
    the limitations and boundaries that the LLM needs to follow, and as explained
    earlier, it helps to ensure that the LLM accounts for business constraints and
    firm policy. As such, the agent is instructed to adhere to these constraints while
    aiming to achieve high customer satisfaction. This ensures that all agents follow
    the same company policy, differing only in their communication strategies, which
    the teacher aims to teach the student.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 角色指定了 GPT-4 需要担任的位置或职能。例如，提示 GPT-4 “扮演客服代理” 设置了互动的背景，指导模型生成适合客服场景的回应。目标确立了互动的期望结果和最终目标，指导
    GPT-4 根据需要调整其策略。例如，如果目标是实现高客户满意度，GPT-4 将调整其回应，使其更加富有同情心、有帮助，并以解决问题为导向。在提示中包含目标是至关重要的，因为它为
    LLM 提供了明确的方向，确保其回应与预期结果一致。约束确定了 LLM 需要遵循的限制和边界，如前所述，它有助于确保 LLM 考虑到业务限制和公司政策。因此，代理被指示遵守这些约束，同时致力于实现高客户满意度。这确保了所有代理遵循相同的公司政策，仅在其沟通策略上有所不同，这正是教师旨在教授学生的内容。
- en: The base prompt is fixed for the entire task and included in the prompt for
    both the teacher and student.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基础提示在整个任务中是固定的，并且包含在教师和学生的提示中。
- en: 3.2 Knowledge Distillation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 知识蒸馏
- en: 'Our approach iterates over three steps: scenario generation, strategy teaching,
    and goal evaluation, progressively building up a customized library consisting
    of scenarios and strategies for handling them. We call each execution of the three
    steps as one iteration of the algorithm. We use $\mathcal{L}^{(t)}=\{(\mathbf{s}^{(t)}_{i},p_{\text{strategy}}(\mathbf{s}^{(t)}_{i}))_{i=1}^{n_{t}}\}$
    to represent the library at the end of iteration $t$, where $\mathbf{s}^{(t)}_{i}$
    represents a scenario indexed by $i$, $p_{\text{strategy}}(\mathbf{s}^{(t)}_{i})$
    is the corresponding prompt, and $n_{t}$ is the total number of scenarios in the
    library at iteration $t$. As $t$ increases, the library grows larger. Below, we
    detail each step.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括三个步骤：场景生成、策略教学和目标评估，逐步构建一个包含处理场景和策略的定制化库。我们将三步骤的每次执行称为算法的一次迭代。我们使用 $\mathcal{L}^{(t)}=\{(\mathbf{s}^{(t)}_{i},p_{\text{strategy}}(\mathbf{s}^{(t)}_{i}))_{i=1}^{n_{t}}\}$
    来表示第 $t$ 次迭代结束时的库，其中 $\mathbf{s}^{(t)}_{i}$ 代表按 $i$ 索引的场景，$p_{\text{strategy}}(\mathbf{s}^{(t)}_{i})$
    是相应的提示，$n_{t}$ 是第 $t$ 次迭代时库中的场景总数。随着 $t$ 的增加，库会不断扩大。下面，我们详细介绍每一步。
- en: '3.2.1 Scenario Generation:'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 场景生成：
- en: 'In this step, the LLM agent interacts with the customer $\mathcal{C}(\cdot)$
    to generate conversations, denoted as $\mathbf{x}^{(t,l)}$, where $t$ is the iteration
    index and $l$ is the conversation index. $\mathbf{x}^{(t,l)}$ consists of a sequence
    of utterances from the customer $\mathcal{C}(\cdot)$ and the customer service
    agent LLM (e.g., $\mathcal{T}$ or $\mathcal{S}$):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，LLM 代理与客户 $\mathcal{C}(\cdot)$ 进行互动，生成对话，表示为 $\mathbf{x}^{(t,l)}$，其中
    $t$ 是迭代索引，$l$ 是对话索引。$\mathbf{x}^{(t,l)}$ 包含来自客户 $\mathcal{C}(\cdot)$ 和客服代理 LLM（例如，$\mathcal{T}$
    或 $\mathcal{S}$）的一系列话语：
- en: '|  | $\mathbf{x}^{(t,l)}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\mathbf{a}^{(t,l)}_{2}\cdots),$
    |  | (1) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}^{(t,l)}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\mathbf{a}^{(t,l)}_{2}\cdots),$
    |  | (1) |'
- en: where $\mathbf{a}^{(t,l)}_{k}$ represents the agent’s utterance at the $k$-th
    turn and $\mathbf{c}^{(t,l)}_{k}$ represents the customer’s utterance at the $k$-the
    turn. Without loss of generality, we assume a conversation always starts with
    the agent’s utterance, for example, “Hello, how may I help you?”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{a}^{(t,l)}_{k}$表示第$k$轮的代理发言，$\mathbf{c}^{(t,l)}_{k}$表示第$k$轮的客户发言。为简单起见，我们假设对话总是以代理的发言开始，例如，“您好，我能帮您什么？”
- en: Since the generation of an utterance depends on all previous exchanges, we define
    a subconversation with the first $k$ turns, ending with the customer’s utterance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于发言的生成依赖于所有之前的交流，我们定义了一个以第$k$轮结束的客户发言的子对话。
- en: '|  | $\mathbf{x}^{(t,l)}_{[:k]}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\cdots,\mathbf{a}^{(t,l)}_{k},\mathbf{c}^{(t,l)}_{k}).$
    |  | (2) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}^{(t,l)}_{[:k]}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\cdots,\mathbf{a}^{(t,l)}_{k},\mathbf{c}^{(t,l)}_{k}).$
    |  | (2) |'
- en: 'The generation of the teacher’s utterance at the $k$-th turn is based on the
    prior conversation $x^{(t,l)}_{[:k]}$ as well as the base prompt $p_{\text{base}}$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在第$k$轮生成教师发言是基于之前的对话$x^{(t,l)}_{[:k]}$以及基础提示$p_{\text{base}}$：
- en: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{T}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}}).$
    |  | (3) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{T}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}}).$
    |  | (3) |'
- en: 'The customer’s utterance is determined by the prior conversation and the parameterized
    prompt $q(s,e,d)$:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 客户的发言由之前的对话和参数化提示$q(s,e,d)$决定：
- en: '|  | $\mathbf{c}^{(t,l)}_{k}=\mathcal{C}(\mathbf{x}^{(t,l)}_{[:k-1]},\mathbf{a}^{(t,l)}_{k}&#124;q(s,e,d)).$
    |  | (4) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{c}^{(t,l)}_{k}=\mathcal{C}(\mathbf{x}^{(t,l)}_{[:k-1]},\mathbf{a}^{(t,l)}_{k}&#124;q(s,e,d)).$
    |  | (4) |'
- en: For each iteration $t$, we generate a set of conversations, denoted as $\mathbf{X}^{(t)}$,
    by varying the customer prompt parameters $s$(social styles), $e$(initial emotions)
    and $d$ (difficulty) and running multiple times for each set of parameters with
    a non-zero temperature.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代$t$，我们通过改变客户提示参数$s$(社交风格)、$e$(初始情绪)和$d$ (难度)，并对每组参数运行多次生成一组对话，记作$\mathbf{X}^{(t)}$。
- en: Subconversations, defined in Equation (2), are randomly sampled from each conversation
    in $\mathbf{X}^{(t)}$. We call these subconversations scenarios. Each scenario
    includes the entire prior conversation ending with the customer’s utterance. We
    use $\mathbf{S}^{(t)}$ to represent the set of scenarios sampled from $\mathbf{X}^{(t)}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 子对话，如方程（2）所定义，从$\mathbf{X}^{(t)}$中的每个对话中随机抽取。我们称这些子对话为场景。每个场景包括以客户发言结束的整个之前对话。我们用$\mathbf{S}^{(t)}$表示从$\mathbf{X}^{(t)}$中抽取的场景集合。
- en: In a naive solution, the conversations in $\mathbf{X}^{(t)}$ are generated by
    $\mathcal{T}$ interacting with $\mathcal{C}$. However, each scenario extracted
    from $\mathbf{X}^{(t)}$ reflects the specific dynamics and decisions made by the
    teacher $\mathcal{T}$, not necessarily those a student would encounter or make
    when interacting independently in similar contexts. This leads to compounding
    errors because the scenarios the student encounters during deployment will differ
    from those it observed during training. In a multi-step decision-making process,
    even a small deviation in one step can compound over subsequent steps, leading
    to significant differences at the conversation level. This mismatch between training
    and deployment scenarios is a significant for challenge multi-step imitations,
    often referred to as distribution shift (Pomerleau [1991](#bib.bib22), Ross and
    Bagnell [2010](#bib.bib24)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简单的解决方案中，$\mathbf{X}^{(t)}$中的对话是通过$\mathcal{T}$与$\mathcal{C}$的交互生成的。然而，从$\mathbf{X}^{(t)}$中提取的每个场景反映了教师$\mathcal{T}$所做的具体动态和决策，而不一定是学生在类似环境下独立互动时会遇到或做出的决策。这导致了累积错误，因为学生在部署过程中遇到的场景将不同于训练过程中观察到的场景。在多步骤决策过程中，即使是一步的小偏差也会在后续步骤中累积，从而导致对话级别上的显著差异。这种训练与部署场景之间的差异是多步骤模仿中的一个重要挑战，通常被称为分布偏移（Pomerleau
    [1991](#bib.bib22)，Ross和Bagnell [2010](#bib.bib24)）。
- en: To overcome this challenge, we let the student participate in the scenario generation
    step. We assign a probability $p$ for the teacher to be selected to interact with
    the customer for conversation generation and $1-p$ for the student to be selected.
    Initially, $p$ is set to 1 in iteration 1 and gradually decreases in subsequent
    iterations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，我们让学生参与场景生成步骤。我们为教师与客户互动生成对话分配概率$p$，为学生选择分配$1-p$。最初，$p$在第1次迭代中设置为1，并在后续迭代中逐渐减少。
- en: 'In each iteration, when the student is selected to interact with the customer
    LLM, it will produce an output based on the scenarios and strategies retrieved
    from the current library:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，当学生被选择与客户LLM互动时，它将根据从当前库中检索到的场景和策略生成输出：
- en: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{S}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}},\mathcal{L}^{(t)}).$
    |  | (5) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{S}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}},\mathcal{L}^{(t)}).$
    |  | (5) |'
- en: We will explain how to retrieve the relevant strategy from the library in Section
    [3.3](#S3.SS3 "3.3 Deployment ‣ 3 Library-based Interpretable Knowledge Distillation").
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第[3.3](#S3.SS3 "3.3 Deployment ‣ 3 Library-based Interpretable Knowledge
    Distillation")节中解释如何从库中检索相关策略。
- en: As the iterations progress, more scenarios will be accumulated into $\mathcal{L}^{(t)}$,
    covering a more diverse set of situations the student could encounter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代的进行，更多的场景将被累积到$\mathcal{L}^{(t)}$中，覆盖学生可能遇到的更多样化的情况。
- en: '3.2.2 Strategy Teaching:'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 策略教学：
- en: 'In this step, the teacher iteratively generates and refines strategies for
    each scenario until the student accurately mimics the teacher’s output for that
    scenario. See Figure [1](#S3.F1 "Figure 1 ‣ 3.2.2 Strategy Teaching: ‣ 3.2 Knowledge
    Distillation ‣ 3 Library-based Interpretable Knowledge Distillation") for an illustration
    of the process.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '在此步骤中，教师迭代生成和细化每个场景的策略，直到学生准确模仿教师在该场景中的输出。有关该过程的说明，请参见图[1](#S3.F1 "Figure 1
    ‣ 3.2.2 Strategy Teaching: ‣ 3.2 Knowledge Distillation ‣ 3 Library-based Interpretable
    Knowledge Distillation")。'
- en: '![Refer to caption](img/03c2080736cf4fa24fa106e83dd76814.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/03c2080736cf4fa24fa106e83dd76814.png)'
- en: 'Figure 1: An iterative process of strategy teaching'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：策略教学的迭代过程
- en: We call the prompt generated at this step the *strategy prompt*. The strategy
    prompt will be combined with the base prompt to instruct the student *how* to
    behave like the teacher.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称此步骤生成的提示为*策略提示*。策略提示将与基础提示结合，以指导学生*如何*像教师一样行为。
- en: Specifically, for each scenario, denoted as $\mathbf{s}$, both $\mathcal{T}$
    and $\mathcal{S}$ generate a response. These responses are then evaluated by $\mathcal{T}$,
    which identifies discrepancies and proposes updates to the strategy. The updates
    are suggestions made to the student instructing what they should or should not
    do, based on their current output. They can vary from general, such as “use a
    more empathetic tone” to highly nuanced, such as “address the customer by their
    first name”, tailored to the specific demands of each scenario and the difference
    between the student and the teacher.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，对于每个场景，记作$\mathbf{s}$，$\mathcal{T}$和$\mathcal{S}$都生成一个响应。然后，这些响应由$\mathcal{T}$进行评估，识别差异并提出对策略的更新。这些更新是对学生的建议，指示他们应该或不应该做什么，基于他们当前的输出。更新可以是一般性的，例如“使用更具同理心的语气”，也可以是高度细致的，例如“用名字称呼客户”，根据每个场景的具体要求和学生与教师之间的差异进行调整。
- en: These updates are incorporated into the strategy prompt, denoted as $p_{\text{strategy}}(\mathbf{s})$.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更新被纳入到策略提示中，记作$p_{\text{strategy}}(\mathbf{s})$。
- en: '|  | teacher’s output: | $\displaystyle\mathbf{a}=\mathcal{T}(\mathbf{s}&#124;p_{\text{base}}).$
    |  | (6) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 教师的输出： | $\displaystyle\mathbf{a}=\mathcal{T}(\mathbf{s}&#124;p_{\text{base}}).$
    |  | (6) |'
- en: '|  | student’s output: | $\displaystyle\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\mathbf{s})).$
    |  | (7) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 学生的输出： | $\displaystyle\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\mathbf{s})).$
    |  | (7) |'
- en: 'This process of iterative refinement of $p_{\text{strategy}}(\mathbf{s})$ continues
    until there are no further updates from the teacher or a pre-defined maximum number
    of refinements is reached. By progressively refining its responses based on learned
    strategies and feedback, the student model becomes more adept at handling the
    scenarios, aligning its behavior with the teacher’s expertise and improving overall
    performance³³3A detailed example of updating the strategies and improving the
    response is shown in Appendix [8](#S8 "8 How Teacher LLM Iteratively Updates Strategies
    for Student LLM: Example")..'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '$p_{\text{strategy}}(\mathbf{s})$的迭代细化过程将持续进行，直到没有进一步的教师更新或达到预定义的最大细化次数。通过根据学习到的策略和反馈逐步改进其响应，学生模型在处理场景时变得更加熟练，使其行为与教师的专业知识一致，并提高整体表现³³一个详细的策略更新和响应改进的示例见附录[8](#S8
    "8 How Teacher LLM Iteratively Updates Strategies for Student LLM: Example")。'
- en: '3.2.3 Termination Mechanism:'
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 终止机制：
- en: Our main algorithm is an iterative process that gradually grows the library
    by adding a new batch of scenarios and their corresponding strategy prompts at
    each iteration. The thorough coverage of the data space by the scenarios is crucial
    for maintaining the robustness and reliability of the strategy prompts during
    deployment. To determine when the sufficiency is reached, we incorporate a goal
    evaluation step in the iterative process, where the student LLM is evaluated,
    based on the interactions with the customer LLM, after each new batch is added
    to the library, to determine whether the student LLM has achieved satisfactory
    performance using the current library.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要算法是一个迭代过程，通过在每次迭代中添加一批新的场景及其对应的策略提示来逐步扩展库。场景对数据空间的彻底覆盖对于保持策略提示在部署期间的鲁棒性和可靠性至关重要。为了确定何时达到充分性，我们在迭代过程中引入了一个目标评估步骤，其中学生LLM在每次新的批次被添加到库中后，会根据与客户LLM的互动进行评估，以确定学生LLM是否在使用当前库的情况下达到了令人满意的性能。
- en: We choose to use LLMs for the evaluation, motivated by recent research showing
    the potential of using LLMs to substitute human evaluations in various tasks,
    particularly in assessing other LLMs, with findings of up to 80% agreement with
    human judgment. Leveraging this advancement, our approach employs the teacher,
    a state-of-the-art LLM, to directly evaluate the outputs of the student LLM utilizing
    the current library. This direct evaluation is especially coherent since the student
    has been trained on the teacher’s strategies, making the teacher an ideal evaluator
    due to its understanding of the intricacies and subtleties required in the responses.
    We let the student interact with the environment to generate a validation set
    and let the teacher score it. This score is recorded and compared with the score
    from the previous iteration. The algorithm stops if there is no (significant)
    improvement after a pre-defined number of iterations (we set it to 2).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用LLMs进行评估，受到近期研究的启发，这些研究显示LLMs在各种任务中代替人工评估的潜力，特别是在评估其他LLMs方面，发现与人工判断的符合度高达80%。利用这一进展，我们的方法使用教师，一个最先进的LLM，直接评估学生LLM利用当前库生成的输出。这种直接评估尤其一致，因为学生已经在教师的策略下进行过训练，使得教师成为理想的评估者，因为它了解回答中所需的复杂性和细微之处。我们让学生与环境互动生成验证集，并让教师对其进行评分。记录下这个评分，并与之前迭代中的评分进行比较。如果在预定义的迭代次数（我们设置为2）之后没有（显著的）改进，算法将停止。
- en: 3.3 Deployment
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 部署
- en: The training algorithm generates a library comprising a set of scenarios and
    their associated strategy prompts. We will now discuss how this library is utilized
    during deployment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法生成一个包含一组场景及其相关策略提示的库。我们现在将讨论这个库在部署期间如何使用。
- en: 'As shown in Equation ([7](#S3.E7 "Equation 7 ‣ 3.2.2 Strategy Teaching: ‣ 3.2
    Knowledge Distillation ‣ 3 Library-based Interpretable Knowledge Distillation")),
    the student’s output is determined by the scenario, the base prompt, and the strategy
    prompt optimized for that specific scenario during training. However, during deployment,
    the prior conversations differ from the training scenarios, and there is no predefined
    $p_{\text{strategy}}$ for the test input. To address this, we employ a method
    akin to retrieval-augmented generation (Lewis et al. [2020](#bib.bib17)), where
    we identify the most similar scenario(s) from a library based on their embeddings.
    The corresponding strategy from these similar scenarios is then applied to the
    new, unseen input, i.e.,'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '如公式 ([7](#S3.E7 "公式 7 ‣ 3.2.2 策略教学: ‣ 3.2 知识蒸馏 ‣ 3 基于库的可解释知识蒸馏")) 所示，学生的输出由场景、基础提示和在训练过程中为该特定场景优化的策略提示决定。然而，在部署过程中，之前的对话与训练场景不同，且测试输入没有预定义的
    $p_{\text{strategy}}$。为了解决这个问题，我们采用类似于检索增强生成的方法（Lewis et al. [2020](#bib.bib17)），通过其嵌入从库中识别出最相似的场景。然后将这些相似场景中的策略应用于新的、未见过的输入，即，'
- en: '|  | student’s output: | $\displaystyle\quad\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\tilde{\mathbf{s}})),\
    \text{where}$ | $\displaystyle\tilde{\mathbf{s}}=\argmin_{\mathbf{s}^{\prime}\in\mathcal{L}}d(\mathbf{s},\mathbf{s}^{\prime}).$
    |  | (8) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 学生的输出： | $\displaystyle\quad\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\tilde{\mathbf{s}})),\
    \text{其中}$ | $\displaystyle\tilde{\mathbf{s}}=\argmin_{\mathbf{s}^{\prime}\in\mathcal{L}}d(\mathbf{s},\mathbf{s}^{\prime}).$
    |  | (8) |'
- en: By leveraging scenario embeddings, the library can efficiently match new, unseen
    inputs to the most similar scenarios, thereby providing a tailored strategy prompt.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用场景嵌入，库可以高效地将新的、未见过的输入匹配到最相似的场景，从而提供量身定制的策略提示。
- en: Notably, the use of the library is flexible. When retrieving strategies, it
    is not necessary to limit retrieval to a single closest scenario; instead, one
    can retrieve $k></math> scenarios from the library, following the <math   alttext=$-nearest
    neighbors approach. We provide a detailed analysis in Appendix [6.2](#S6.SS2 "6.2
    Using kNN to Retrieve Guidelines to Improve Performance ‣ 6 Robustness Checks")
    and find that tuning $k$ can further enhance performance.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，库的使用非常灵活。在检索策略时，不必将检索限制为单一的最接近场景；相反，可以从库中检索$k$个场景，遵循<math alttext=$-nearest
    neighbors方法。我们在附录[6.2](#S6.SS2 "6.2 使用kNN检索指南以提升性能 ‣ 6 鲁棒性检查")中提供了详细分析，并发现调整$k$可以进一步提升性能。
- en: '![Refer to caption](img/ec84ea690bd8819cb5b251b6c5831765.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ec84ea690bd8819cb5b251b6c5831765.png)'
- en: 'Figure 2: How the library is used during deployment'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：库在部署过程中的使用方式
- en: 3.4 Method Summary and Discussion
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 方法总结与讨论
- en: 'Intuitively, the library consists of a set of representative and prototypical
    “reference” points within the data space. These reference points must correctly
    and adequately cover the data space to ensure that when encountering a new scenario,
    there is a high likelihood that an existing strategy can be effectively applied.
    The correct coverage necessitates the algorithm to prevent distribution shift,
    motivating the design of increasingly letting the student to generate scenarios
    in Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Scenario Generation: ‣ 3.2 Knowledge Distillation
    ‣ 3 Library-based Interpretable Knowledge Distillation"). The termination mechanism
    is in place to enforce adequate coverage, where the teacher LLM monitors the performance
    of the student and only stops when no improvement is observed, indicating the
    sufficiency of the coverage.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，库由数据空间内的一组具有代表性和原型性的“参考”点组成。这些参考点必须正确而充分地覆盖数据空间，以确保在遇到新场景时，现有策略能够有效应用。正确的覆盖需要算法防止分布偏移，这促使在[3.2.1](#S3.SS2.SSS1
    "3.2.1 场景生成 ‣ 3.2 知识蒸馏 ‣ 3 基于库的可解释知识蒸馏")节中设计越来越多地让学生生成场景。终止机制用于强制执行充分的覆盖，教师LLM监控学生的表现，只有在没有观察到改进时才停止，表明覆盖的充分性。
- en: A key reason for the success of this method is the ability of the LLM to extrapolate,
    adapt, and apply a strategy designed for one scenario to a slightly different
    scenario. This capability allows the LLM to bridge gaps between similar but distinct
    situations, ensuring that the strategies remain effective even when faced with
    novel inputs. The LLM’s inherent understanding of the nuances in language and
    context further strengthens its ability to generalize from the library’s reference
    points, making it a powerful tool for dynamic and diverse real-world applications.
    This ability further allows the library to be used in a slightly different context
    where it is not trained on, as we will demonstrate in Section [4.3](#S4.SS3 "4.3
    Transferrability ‣ 4 Experiments").
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法成功的一个关键原因是LLM能够外推、适应并将为一个场景设计的策略应用于稍微不同的场景。这种能力使LLM能够弥合类似但不同情况之间的差距，确保策略在面对新输入时仍然有效。LLM对语言和上下文细微差别的固有理解进一步增强了其从库中参考点进行概括的能力，使其成为动态和多样化实际应用中的强大工具。这种能力还允许在LLM未经过训练的稍微不同的上下文中使用库，如我们将在[4.3](#S4.SS3
    "4.3 可迁移性 ‣ 4 实验")节中展示的。
- en: Interpretability Benefits.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可解释性好处。
- en: 'One of the defining features of our methodology is its interpretability: the
    teacher’s knowledge is distilled into scenarios and prompts that are easily understood
    by humans. This interpretability offers several advantages over fine-tuning-based
    methods. First, domain experts can verify the library before deployment, which
    is especially important when the teacher is an external LLM over which the company
    has no control, including its model and training data. In fine-tuning methods,
    a student model can be easily contaminated under adversarial attacks, as it is
    unclear what knowledge has been distilled into the student. In contrast, our method
    keeps the student LLM’s parameters unchanged, storing external knowledge in an
    interpretable library where each piece of “knowledge” can be scrutinized and verified.
    This design makes our method more resilient to adversarial attacks. Second, a
    library-based knowledge distillation is easily editable. Suboptimal or outdated
    strategies can be quickly updated by human experts. Meanwhile, when new scenarios
    arise during deployment and the existing library lacks relevant strategies, new
    entries can be seamlessly added with strategies devised by either the teacher
    or domain experts. This flexibility ensures that the library remains current and
    effective across various real-world applications.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法论的一个定义性特征是其可解释性：教师的知识被提炼成易于人类理解的场景和提示。这种可解释性相较于基于微调的方法具有几个优势。首先，领域专家可以在部署前验证库，这在教师是外部语言模型、公司无法控制其模型和训练数据的情况下尤为重要。在微调方法中，学生模型容易受到对抗性攻击的污染，因为无法明确知道什么知识已被提炼到学生模型中。相比之下，我们的方法保持学生语言模型的参数不变，将外部知识存储在一个可解释的库中，每一条“知识”都可以被审查和验证。这种设计使我们的方法对对抗性攻击更具抵抗力。其次，基于库的知识提炼易于编辑。次优或过时的策略可以由人类专家快速更新。同时，当部署过程中出现新场景而现有库中缺乏相关策略时，可以无缝添加新条目，这些策略可以由教师或领域专家制定。这种灵活性确保了库在各种现实应用中保持最新和有效。
- en: 4 Experiments
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We evaluate our method on four student models, LlaMa-2 7b, LlaMa-2 13b, LlaMa-2
    70b and GPT-3.5\. We assess LlaMa models because they are open source and free
    except for storage and computing cost. We assess GPT-3.5 because it can only be
    queried by API and the cost of use is only 5% of GPT-4 at $0.50 per 1M input tokens.
    This selection of student models also covers a large range of model size, from
    7 billion (LlaMa-2 7b) to 175 billion (GPT-3.5).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在四个学生模型上评估了我们的方法，分别是 LlaMa-2 7b、LlaMa-2 13b、LlaMa-2 70b 和 GPT-3.5。我们评估 LlaMa
    模型，因为它们是开源的，除了存储和计算成本外是免费的。我们评估 GPT-3.5，因为它只能通过 API 查询，其使用成本仅为 GPT-4 的 5%，即每百万个输入标记
    0.50 美元。这些学生模型的选择还涵盖了从 70 亿（LlaMa-2 7b）到 1750 亿（GPT-3.5）的广泛模型规模。
- en: With our experiments, we investigate the following research questions related
    to our proposed knowledge distillation method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的实验，我们调查了以下与我们提出的知识提炼方法相关的研究问题。
- en: Q1. Is teaching strategy more effective than teaching responses? To answer this
    question, for each task, we compare our proposed strategy teaching approach against
    fine-tuning student LLMs using the same amount of data. Fine-tuning directly trains
    the student on the scenario - response pairs without explicitly teaching strategies
    like our method does.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Q1. 教授策略是否比教授回应更有效？为了回答这个问题，对于每个任务，我们将我们提出的策略教学方法与使用相同数据量对学生语言模型进行微调进行了比较。微调直接在场景-回应对上训练学生模型，而不是像我们的方法那样明确教授策略。
- en: Q2. Is a context-specific dynamic use of strategies provided by the advanced
    LLM better than global strategies that are invariant across contexts? For this,
    we compare our dynamic approach against global strategies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Q2. 高级语言模型提供的上下文特定动态策略是否比在各种上下文中保持不变的全球策略更有效？为此，我们将动态方法与全球策略进行了比较。
- en: Q3. How effective is our solution to fixing the distribution shift challenge?
    To answer this question, we create a baseline that works the same as ours with
    the only difference being that the scenarios are all generated by the teacher
    interacting with the customer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Q3. 我们解决分布偏移挑战的方案有多有效？为了回答这个问题，我们创建了一个与我们的方案功能相同的基准，其唯一不同之处在于所有场景都由教师与客户互动生成。
- en: Q4. Are learnt strategies transferable across multiple student LLMs and other
    task contexts? We assess whether a strategy learnt from teaching one student LLM
    works for a different LLM and whether strategies learnt for one customer service
    context improves performance in another context.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Q4. 学到的策略是否可以在多个学生LLM和其他任务背景中转移？我们评估从教导一个学生LLM中学到的策略是否适用于不同的LLM，以及为一个客户服务背景学到的策略是否能在另一个背景中提高表现。
- en: 4.1 Main Results
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果
- en: Baseline Methods.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线方法。
- en: To answer question Q1 above, we compare our approach with traditional knowledge
    distillation via fine-tuning, where we first use the teacher LLM to interact with
    the customer LLM to generate a set of conversations, and then extract subconversations
    from it, just like how we sampled scenarios. The difference is that the student
    does not participate in the data generation and no explicit strategies are provided.
    The scenarios are used as the input and then we use the teacher’s response as
    the output, creating a training dataset for fine-tuning. To answer Q2, we compare
    with a set of guidelines manually constructed, through iterations between the
    authors and GPT-4\. These guidelines are global instructions that are used for
    every input the student encounters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答上面的问题Q1，我们将我们的方法与传统的知识蒸馏进行比较，其中我们首先使用教师LLM与客户LLM互动生成一组对话，然后从中提取子对话，就像我们如何抽样场景一样。不同之处在于学生不参与数据生成，并且没有提供明确的策略。这些场景作为输入，然后我们使用教师的回应作为输出，创建一个用于微调的训练数据集。为了回答Q2，我们比较了一组手动构建的指南，通过作者与GPT-4之间的迭代。这些指南是全球性的指令，用于学生遇到的每个输入。
- en: Evaluation Approach.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估方法。
- en: We evaluate the LLMs at the conversation level in terms of the achievement of
    the pre-defined goal - customer satisfaction. Motivated by recent research (Fu
    et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), we perform
    two sets of evaluations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从对话层面评估LLMs，以实现预定义的目标——客户满意度。受最近研究（Fu et al., 2023; Chiang et al., 2023; Geng
    et al., 2023; Sun et al., 2023）的启发，我们进行两组评估。
- en: We first use advanced LLMs as an evaluator to rate each conversation from 1
    (very dissatisfied) to 5 (very satisfied), using few-shot prompt where examples
    of rating 5 conversations are provided by the teacher, GPT-4, interacting with
    the customer LLM. We then use human evaluation where each participant does a blind
    comparison of GPT-4 and a student LLM, based on their conversations with the same
    customer LLM.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用先进的LLMs作为评估者，将每个对话从1（非常不满意）到5（非常满意）进行评分，使用少量示例提示，其中由教师GPT-4提供评分为5的对话示例。然后，我们使用人工评估，每个参与者对GPT-4和一个学生LLM进行盲对比，基于它们与相同客户LLM的对话。
- en: LLM as Evaluators.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM作为评估者。
- en: We use advanced LLMs to evaluate the conversations generated by different LLM
    agents (teacher or student) interacting with the customer LLM. Here we use GPT-4,
    since it is the state-of-the-art LLM available today. We use few-shot learning
    when evaluating these conversations - we include examples in the prompt that are
    generated by the teacher (GPT-4) and the customer. Since different examples may
    lead to different ratings, for robust evaluation, we perform the evaluation twice,
    using examples generated by the teacher interacting with “demanding” customers
    and “non-demanding” customers, respectively. We report the average ratings of
    four student models trained by different methods in Table [1](#S4.T1 "Table 1
    ‣ LLM as Evaluators. ‣ 4.1 Main Results ‣ 4 Experiments").
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用先进的LLMs来评估不同LLM代理（教师或学生）与客户LLM互动生成的对话。这里我们使用GPT-4，因为它是目前最先进的LLM。我们在评估这些对话时使用了少量示例学习——在提示中包含由教师（GPT-4）和客户生成的示例。由于不同的示例可能导致不同的评分，为了进行稳健的评估，我们进行了两次评估，分别使用与“苛刻”客户和“非苛刻”客户互动的教师生成的示例。我们在表[1](#S4.T1
    "Table 1 ‣ LLM as Evaluators. ‣ 4.1 Main Results ‣ 4 Experiments")中报告了四个通过不同方法训练的学生模型的平均评分。
- en: Our strategy imitation method performs consistently better than the baselines.
    First, compared to global guidelines, our method completely dominates this baseline,
    performing better or equally in all experiments. Compared with fine-tuning, our
    method is better or equal except on the smallest LLM, LlaMa-2 7b. This is because
    smaller LLMs do not follow instructions as well as larger LLMs. Thus the knowledge
    distilled from the teacher may not be executed by the student as well as a larger
    student. This observation is consistent with the findings from the literature
    that prompt based approaches are more effective for the LLM with more parameters
    (Lester et al. [2021](#bib.bib16)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略模仿方法在基准测试中表现 consistently 更好。首先，与全球准则相比，我们的方法完全主导了这个基准，在所有实验中表现更好或相当。与微调相比，我们的方法在除最小的LLM（LlaMa-2
    7b）外，表现更好或相当。这是因为较小的LLM不如较大的LLM能很好地遵循指令。因此，从教师那里提炼出的知识可能不会像较大的学生那样被学生很好地执行。这一观察结果与文献中的发现一致，即基于提示的方法对具有更多参数的LLM更有效（Lester
    et al. [2021](#bib.bib16)）。
- en: Note that when prompts use conversations with demanding customers as examples
    of a rating of 5, they tend to yield higher scores across all evaluation tables.
    This occurs because when we label these challenging interactions as a 5 in the
    prompt, we communicate to the LLM that even when a customer is persistent and
    difficult to console, the conversation still merits a high score. This effectively
    sets a low bar for evaluation, leading to inflated ratings. Conversely, if examples
    feature non-demanding customers where positive interactions result in a rating
    of 5, the LLM is more likely to focus on the customer’s reactions when evaluating
    conversations. This establishes a higher standard for ratings.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当提示使用要求高的客户对话作为评分5的例子时，它们往往会在所有评估表中获得更高的分数。这是因为当我们在提示中将这些具有挑战性的互动标记为5时，我们向LLM传达了即使客户固执且难以安抚，对话仍然值得高分。这实际上设定了较低的评估标准，导致评分膨胀。相反，如果示例中包含要求低的客户，在积极互动中得分为5，LLM在评估对话时更可能关注客户的反应。这建立了更高的评分标准。
- en: Different advanced LLMs can be used for evaluation. To assess robustness, we
    also use LlaMa-3 70B as an evaluator. See Appendix [6.1](#S6.SS1 "6.1 Alternative
    LLM for Evaluation ‣ 6 Robustness Checks") for details. In addition, we also evaluated
    retrieving more than 1 most similar scenarios to use their strategies during test.
    The results are shown in Appendix [6.2](#S6.SS2 "6.2 Using kNN to Retrieve Guidelines
    to Improve Performance ‣ 6 Robustness Checks").
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用不同的高级LLM进行评估。为了评估鲁棒性，我们还使用了LlaMa-3 70B作为评估器。有关详细信息，请参见附录 [6.1](#S6.SS1 "6.1
    评估的替代LLM ‣ 6 鲁棒性检查")。此外，我们还评估了检索超过1个最相似的场景以在测试过程中使用它们的策略。结果见附录 [6.2](#S6.SS2 "6.2
    使用kNN检索准则以提高性能 ‣ 6 鲁棒性检查")。
- en: 'Table 1: Average Customer Satisfaction Evaluated by GPT-4'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GPT-4评估的客户满意度平均值
- en: '| Methods | Demanding Customer Conversation Baseline | Non-Demanding Customer
    Conversation Baseline |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 要求高的客户对话基准 | 要求低的客户对话基准 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 3.91 | 4.81 | 4.88 | 4.91 | 2.84 | 3.59 | 4.03 | 4.03 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 基础 LLM | 3.91 | 4.81 | 4.88 | 4.91 | 2.84 | 3.59 | 4.03 | 4.03 |'
- en: '| Strategy Imitation | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
- en: '| Fine-Tuning | 4.63 | 4.94 | 5.0 | 4.68 | 3.75 | 3.47 | 4.06 | 3.81 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 4.63 | 4.94 | 5.0 | 4.68 | 3.75 | 3.47 | 4.06 | 3.81 |'
- en: '| Global Guidelines | 4.44 | 4.5 | 4.75 | 5.0 | 3.31 | 3.44 | 3.88 | 3.75 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 全球准则 | 4.44 | 4.5 | 4.75 | 5.0 | 3.31 | 3.44 | 3.88 | 3.75 |'
- en: Human Evaluation.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工评估。
- en: We conduct a blind comparison of the teacher and student via human evaluation.
    We design a survey and show the two conversations, from the student and teacher,
    each interacting with the same customer. We then ask the participant to choose
    the better agent based on the conversations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过人工评估进行教师与学生的盲比对。我们设计了一项调查，展示了学生和教师的两个对话，每个对话都与相同的客户互动。然后，我们要求参与者根据对话选择更好的代理。
- en: We report the percentage of times each student model is selected. If a student
    is comparable to the teacher, the selection rate should be around 50%. Thus, the
    larger the number, the better the model. Results show that the base models for
    each student is consistently worse than the teacher, especially for LlaMa-2 7b,
    which is selected only 3% of the time. After the knowledge distillation, we see
    a consistent improvement in the performance of student LLMs, with weaker LLMs
    benefiting more from learning from the teacher LLM. While the student still performs
    slightly worse than the teacher, the precentages significantly increase.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了每个学生模型被选择的百分比。如果一个学生与教师相当，则选择率应接近 50%。因此，数字越大，模型越好。结果显示，每个学生的基础模型始终逊色于教师，尤其是
    LlaMa-2 7b，仅被选择 3% 的时间。在知识蒸馏之后，我们看到学生 LLM 的性能有了一致的提升，较弱的 LLM 从教师 LLM 的学习中受益更多。虽然学生仍然略逊于教师，但百分比显著增加。
- en: 'Table 2: The percentage of times the student agent is selected over GPT-4 by
    human subjects'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：学生代理比 GPT-4 被人类受试者选择的百分比
- en: '| Methods | LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 0.03 | 0.25 | 0.38 | 0.22 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 基础 LLM | 0.03 | 0.25 | 0.38 | 0.22 |'
- en: '| Strategy Imitation | 0.34 | 0.41 | 0.41 | 0.41 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 0.34 | 0.41 | 0.41 | 0.41 |'
- en: Discussion.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论。
- en: Overall, our method performs better than the global guidelines because it provides
    strategies customized for each scenario and for each student. These strategies
    are developed based on specific scenarios, such that dynamic instructions are
    provided to the student, in contrast with the fixed strategies from a global guideline.
    Consequently, vastly different, even contradictory strategies may exist. We shown
    an example in Figure [3](#S4.F3 "Figure 3 ‣ Discussion. ‣ 4.1 Main Results ‣ 4
    Experiments"). In a scenario where a customer initiates a conversation by asking
    for help, the student is instructed to avoid over-explaining policies. In contrast,
    in a different scenario where the customer is engaged and seeks information, the
    student is instructed to provide clear and detailed information. Conversely, the
    global guidelines remain fixed, lacking such flexibility and adaptability.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的方法比全球指南表现更好，因为它为每个场景和每个学生提供了定制的策略。这些策略基于具体场景制定，为学生提供动态的指示，与全球指南中的固定策略形成对比。因此，可能存在截然不同甚至相互矛盾的策略。我们在图
    [3](#S4.F3 "Figure 3 ‣ Discussion. ‣ 4.1 Main Results ‣ 4 Experiments") 中展示了一个例子。在客户通过请求帮助开始对话的场景中，学生被指示避免过度解释政策。相反，在客户参与并寻求信息的不同场景中，学生被指示提供清晰而详细的信息。相反，全球指南保持固定，缺乏这种灵活性和适应性。
- en: '![Refer to caption](img/9b28e9a7bed5e890aad0ab3d8bdcdb5b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b28e9a7bed5e890aad0ab3d8bdcdb5b.png)'
- en: 'Figure 3: Different strategies are learned for different scenarios.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同场景下学习的不同策略。
- en: In addition, strategies are customized for different students. Strategies are
    constructed via proposing changes to the students’ current output. In other words,
    the strategies are “delta” to the students’ output, which is targeted to fix the
    problem of the student. Different student LLMs behave differently, and thus will
    receive different strategies from the teacher. For example, LlaMa2-7b tends to
    generate longer responses than GPT-3, the average response length is about 50%
    more than that of GPT-3.5\. We calculate the percentage of strategies that contain
    keywords “concise”, “brevity” or “excessive” in it, which means the teacher instructs
    the student to provide shorter responses. We find that 15.4% of the strategies
    for GPT-3.5 contain such keywords while 22.1% of strategies for LlaMa 2-7b contain
    such keywords, a 50% increase.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，策略针对不同的学生进行定制。这些策略通过对学生当前输出提出修改建议来构建。换句话说，这些策略是针对学生输出的“delta”，旨在解决学生的问题。不同的学生
    LLM 表现不同，因此将会从教师那里收到不同的策略。例如，LlaMa2-7b 生成的响应通常比 GPT-3 更长，平均响应长度约为 GPT-3.5 的 50%
    多。我们计算了包含关键词“简洁”、“简短”或“过多”的策略百分比，这意味着教师指示学生提供更简短的响应。我们发现，15.4% 的 GPT-3.5 策略包含这些关键词，而
    22.1% 的 LlaMa 2-7b 策略包含这些关键词，增加了 50%。
- en: Therefore, our method design ensures the strategies are customized to each student
    as well as each scenario the student might encouter during deployment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的方法设计确保了策略根据每个学生以及学生在部署过程中可能遇到的每个场景进行定制。
- en: 4.2 Evaluating the Solution to Distribution Shift
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估分布偏移的解决方案
- en: To answer question Q3, we compare our method with a baseline solution where
    everything else remains the same as in our proposed method, except that we do
    not address the distribution shift. In the baseline, which we call “Strategy Imitation
    (ablated)”, all scenarios are generated by only the teacher interacting with the
    customer, rather than allowing the student to participate in the generation process.
    To ensure a fair comparison, we generate the same number of scenarios as the original
    algorithm. In this case, a distribution shift exists because the training data
    is generated by the teacher, while during deployment, the scenarios are generated
    by the student.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答问题Q3，我们将我们的方法与基线解决方案进行比较，基线方案中所有其他条件与我们提出的方法相同，只是我们没有处理分布转移。在基线中，我们称之为“策略模仿（削减）”，所有场景仅由老师与客户互动生成，而不是允许学生参与生成过程。为了确保公平比较，我们生成与原始算法相同数量的场景。在这种情况下，存在分布转移，因为训练数据是由老师生成的，而在部署期间，场景是由学生生成的。
- en: To assess the distribution shift quantitatively, we report the distance between
    a new scenario the student encounters during deployment and the closest scenario
    retrieved from the library. The strategy from the retrieved scenario will be used
    to instruct the student on handling the new scenario. Therefore, the distance
    between the new and retrieved scenario determines the applicability of the strategy
    to the new scenario. A small distance indicates that the retrieved scenario is
    very similar to the new scenario, making the strategy highly relevant and applicable.
    Conversely, a larger distance suggests that the scenario is less relevant, leading
    to a lower quality of the corresponding strategy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量评估分布转移，我们报告了学生在部署期间遇到的新场景与从库中检索到的最接近场景之间的距离。检索场景中的策略将用于指导学生如何处理新场景。因此，新场景与检索场景之间的距离决定了策略对新场景的适用性。小距离表明检索场景与新场景非常相似，使得策略高度相关且适用。相反，大距离表明场景不太相关，从而导致相应策略的质量较低。
- en: 'Table 3: Average scenario distances'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：平均场景距离
- en: '| Methods | LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 |'
- en: '| Strategy Imitation | 0.179 | 0.172 | 0.181 | 0.169 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 0.179 | 0.172 | 0.181 | 0.169 |'
- en: '| Strategy Imitation (ablated) | 0.192 | 0.192 | 0.191 | 0.174 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿（削减） | 0.192 | 0.192 | 0.191 | 0.174 |'
- en: '| % Increase | 7.3% | 11.6% | 5.5% | 3.0% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| % 增加 | 7.3% | 11.6% | 5.5% | 3.0% |'
- en: Table [3](#S4.T3 "Table 3 ‣ 4.2 Evaluating the Solution to Distribution Shift
    ‣ 4 Experiments") presents the average distances between new scenarios and retrieved
    scenarios for all conversations in the test set, compared to those of the original
    algorithm. The results show that the average distance increased by up to 12%,
    indicating that the retrieved scenarios are relatively less similar to the test
    cases compared to the original algorithm. This is because the distribution of
    the test data deviates more from the training data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](#S4.T3 "Table 3 ‣ 4.2 Evaluating the Solution to Distribution Shift ‣ 4
    Experiments")展示了测试集中所有对话中新场景与检索场景之间的平均距离，并与原始算法进行比较。结果显示，平均距离增加了最多12%，这表明与原始算法相比，检索到的场景与测试案例的相似性相对较低。这是因为测试数据的分布与训练数据的偏差更大。
- en: Table [4](#S4.T4 "Table 4 ‣ 4.2 Evaluating the Solution to Distribution Shift
    ‣ 4 Experiments") evaluates the performance of the ablated algorithm using GPT-4.
    Without handling the distribution shift, the ablated algorithm performs worse
    than the original algorithm. Occasionally, the ablated algorithm also performs
    worse than the base LLM, as the strategies become less relevant when the distances
    increase, which means the student follows bad advice on how to respond to customers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4](#S4.T4 "Table 4 ‣ 4.2 Evaluating the Solution to Distribution Shift ‣ 4
    Experiments")评估了使用GPT-4的削减算法的性能。如果不处理分布转移，削减算法的表现比原始算法更差。偶尔，削减算法的表现也比基础LLM差，因为当距离增加时，策略变得不那么相关，这意味着学生按照不良建议响应客户。
- en: 'Table 4: Average Customer Satisfaction Evaluated by GPT-4'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：GPT-4评估的平均客户满意度
- en: '| Methods | Demanding Prompt | Non-Demanding Prompt |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 需求提示 | 非需求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 3.91 | 4.81 | 4.88 | 4.91 | 2.84 | 3.59 | 4.03 | 4.03 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 基础LLM | 3.91 | 4.81 | 4.88 | 4.91 | 2.84 | 3.59 | 4.03 | 4.03 |'
- en: '| Strategy Imitation | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
- en: '| Strategy Imitation (ablated) | 4.5 | 4.75 | 4.63 | 4.81 | 3.25 | 3.56 | 4.06
    | 4.38 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿（去除） | 4.5 | 4.75 | 4.63 | 4.81 | 3.25 | 3.56 | 4.06 | 4.38 |'
- en: 'We provide more analyses to show the effect of distribution shift in Appendix
    [7](#S7 "7 Distribution Shift Drives Drop in Performance: Evidence").'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了更多分析以显示分布变化的影响，见附录 [7](#S7 "7 分布变化驱动性能下降：证据")。
- en: 4.3 Transferrability
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 迁移性
- en: We have shown that the library is customized for each student and takes into
    account their specific characteristics and current ability in responding to the
    customers. Therefore, the library should contain different strategies for different
    students. On the other hand, we also hypothesize that there may exist some knowledge
    that is common to all students, even to other different but related scenarios.
    To study this question, we test the transferrability of the library.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了库是针对每个学生定制的，并考虑了他们在响应客户时的具体特点和当前能力。因此，库应包含针对不同学生的不同策略。另一方面，我们也假设可能存在一些对所有学生都通用的知识，即使是对其他不同但相关的情境。为研究这个问题，我们测试了库的迁移性。
- en: An important advantage of our method is that the library is highly portable
    and reusable; other student LLMs can directly use it without training. Therefore,
    we let all LLMs use the library built for LlaMa-2 7b, the LLM with the lowest
    cost, instead of training their own library.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的一个重要优势是库具有很高的可移植性和重用性；其他学生 LLM 可以直接使用它而无需训练。因此，我们让所有 LLM 使用为 LlaMa-2 7b（最低成本的
    LLM）构建的库，而不是训练自己的库。
- en: 'Table 5: Model Transferability Assessment (Evaluated by GPT-4)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：模型迁移性评估（由 GPT-4 评估）
- en: '| Methods | Demanding Prompt | Non-Demanding Prompt |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 高要求提示 | 低要求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 3.91 | 4.81 | 4.88 | 4.91 | 2.84 | 3.59 | 4.03 | 4.03 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 基础 LLM | 3.91 | 4.81 | 4.88 | 4.91 | 2.84 | 3.59 | 4.03 | 4.03 |'
- en: '| Strategy Imitation | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
- en: '| LlaMa-2 7b Library | 4.88 | 5.0 | 4.88 | 5.0 | 3.44 | 3.91 | 3.94 | 4.44
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b 库 | 4.88 | 5.0 | 4.88 | 5.0 | 3.44 | 3.91 | 3.94 | 4.44 |'
- en: Results show that the transferred library improves performance from the base
    LLM, even though it is not directly trained for other students. This indicates
    the student LLMs may lack some common communication skills and thus benefit from
    strategies constructed for other LLMs.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，即使转移的库没有直接为其他学生训练，它也能提升基础 LLM 的性能。这表明学生 LLM 可能缺乏一些共同的沟通技能，从而受益于为其他 LLM
    构建的策略。
- en: The transferrability of the library is not only across LLMs, but also across
    contexts. Though the library is built for a specific context of “canceling a restricted
    ticket,” we hypothesize that some aspects of the knowledge on how to interact
    with customers should also apply to other contexts within the general domain of
    customer service.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 库的迁移性不仅存在于 LLM 之间，也存在于不同的上下文之间。尽管库是为“取消受限票”这一特定上下文构建的，但我们假设有关如何与客户互动的某些知识也应该适用于客户服务一般领域中的其他上下文。
- en: Therefore, we evaluate the performance of our approach in a new context outside
    of the training. Specifically, we consider a context, where customers call the
    airline to resolve a “lost luggage issue,” where the customer seeks $10,000 compensation
    for their loss. For this experiment, we again compare performance using our approach
    with fine-tuning and global guidelines. For fine-tuning, we directly use the LLM
    fine-tuned on the ticket problem to solve the luggage problem without retraining.
    For the global guidelines, we use the same global guidelines constructed for the
    ticket problem. The results are shown in Table [6](#S4.T6 "Table 6 ‣ 4.3 Transferrability
    ‣ 4 Experiments").
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在训练之外的新上下文中评估我们方法的性能。具体而言，我们考虑一个情境，客户拨打航空公司电话解决“行李丢失问题”，客户寻求 $10,000 的赔偿。在这个实验中，我们再次使用我们的办法与微调和全球指导进行性能比较。对于微调，我们直接使用针对票务问题微调的
    LLM 来解决行李问题，而无需重新训练。对于全球指导，我们使用为票务问题构建的相同全球指导。结果如表 [6](#S4.T6 "表 6 ‣ 4.3 迁移性 ‣
    4 实验") 所示。
- en: 'Table 6: Context Transferability Assessment Evaluated by GPT-4'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：上下文迁移性评估（由 GPT-4 评估）
- en: '| Methods | Demanding Prompt | No-Demanding Prompt |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 高要求提示 | 低要求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 4.5 | 5.0 | 5.0 | 4.91 | 3.13 | 4.25 | 4.5 | 4.63 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 基础LLM | 4.5 | 5.0 | 5.0 | 4.91 | 3.13 | 4.25 | 4.5 | 4.63 |'
- en: '| Strategy Imitation | 4.5 | 5.0 | 5.0 | 5.0 | 4.0 | 4.63 | 4.5 | 5.0 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 4.5 | 5.0 | 5.0 | 5.0 | 4.0 | 4.63 | 4.5 | 5.0 |'
- en: '| Fine-tuning | 5.0 | 5.0 | 4.88 | 5.0 | 4.5 | 4.63 | 4.63 | 4.75 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 5.0 | 5.0 | 4.88 | 5.0 | 4.5 | 4.63 | 4.63 | 4.75 |'
- en: '| Global Guidelines | 4.75 | 5.0 | 4.88 | 5.0 | 3.88 | 4.25 | 4.63 | 5.0 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 全球指南 | 4.75 | 5.0 | 4.88 | 5.0 | 3.88 | 4.25 | 4.63 | 5.0 |'
- en: In addition, we also test using strategies from more than one most similar scenario,
    specifically $k=5$, and we find the results are even better than $k=1$, as discussed
    in Appendix [6.2.1](#S6.SS2.SSS1 "6.2.1 How use of kNN to Retrieve Guideliness
    Improve Context Transferability ‣ 6.2 Using kNN to Retrieve Guidelines to Improve
    Performance ‣ 6 Robustness Checks").
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还测试了使用多个最相似场景的策略，具体为 $k=5$，我们发现结果甚至比 $k=1$ 更好，如附录 [6.2.1](#S6.SS2.SSS1
    "6.2.1 使用 kNN 检索指南提高上下文可转移性 ‣ 6.2 使用 kNN 检索指南提高性能 ‣ 6 鲁棒性检查") 中讨论的那样。
- en: From the analyses, we find that the library is transferable across contexts,
    providing superior performance. The transferability is better than fine-tuning
    when the student LLM has more parameters (e.g., LlaMa-2 70B and GPT-3.5). For
    smaller LLMs, fine-tuning performs better. We hypothesize that this is because
    larger LLMs are better at extrapolating and may inherently adapt the strategies
    to their own context, even if the strategies initially appear less relevant.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从分析中，我们发现库在不同上下文中具有可转移性，提供了更优的性能。当学生LLM具有更多参数（例如，LlaMa-2 70B 和 GPT-3.5）时，可转移性优于微调。对于较小的LLM，微调表现更好。我们推测这是因为较大的LLM
    更擅长推断，并且可能会将策略自然适应到其自身的上下文中，即使这些策略最初看起来相关性较低。
- en: 5 Conclusion
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: Though advancement in LLMs have broadened their applicability in various domains,
    including customer service and content generation, deploying these models often
    faces constraints related to security, their size, cost, and operational feasibility
    on resource-constrained devices. Researchers have developed methods to enhance
    smaller, more economical models that require fewer computational resources for
    real-time deployment to address these constraints.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM的进步扩大了它们在各个领域的适用性，包括客户服务和内容生成，但部署这些模型通常面临与安全性、规模、成本和在资源受限设备上的操作可行性相关的限制。研究人员开发了方法来增强较小、更经济的模型，这些模型需要较少的计算资源以实现实时部署，以解决这些限制。
- en: This paper introduces “interpretable knowledge distillation,” a novel approach
    for transferring strategic knowledge from a large, sophisticated “teacher” LLM
    to a smaller “student” LLM. Unlike traditional methods that rely on direct imitation
    from teacher responses for learning, our approach focuses on strategy teaching.
    The teacher model provides strategies for handling various scenarios, which are
    compiled into a library. This library serves as a reference for the student model
    to apply appropriate strategies in real-world situations, functioning similarly
    to retrieval-augmented generation (RAG). This method does not require modifying
    the student model’s parameters, allowing it to benefit from the teacher’s knowledge
    through external, dynamic referencing, thus ensuring adaptability and scalability.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了“可解释的知识蒸馏”，这是一种将战略知识从大型、复杂的“教师”LLM 转移到较小的“学生”LLM 的新方法。与传统的直接模仿教师回应的学习方法不同，我们的方法专注于策略教学。教师模型提供处理各种场景的策略，这些策略被编入一个库。这个库作为参考，供学生模型在实际情况中应用适当的策略，类似于检索增强生成（RAG）。该方法不需要修改学生模型的参数，通过外部动态引用使其受益于教师的知识，从而确保适应性和可扩展性。
- en: Experimental results validate the method’s effectiveness. In a customer service
    context, student models equipped with the strategy library consistently outperformed
    those trained with traditional fine-tuning methods while gaining interpretability.
    They achieved higher customer satisfaction rates and effectively adapted to new,
    unseen scenarios. Transferability tests showed that strategies learned in one
    context (e.g., ticket cancellation) were beneficial in another (e.g., lost luggage
    claim), highlighting the method’s broad applicability. Moreover, the approach’s
    interpretability facilitates human oversight and ongoing refinement of the strategy
    library. The results suggest that smaller LLMs can achieve near parity with their
    larger counterparts in specific tasks, reducing the need for higher-cost, higher-complexity
    models. We hope the various advantages of our interpretable knowledge distillation
    approach facilitates greater adoption of LLMs across a range of challenging marketing
    tasks.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果验证了该方法的有效性。在客户服务环境中，配备策略库的学生模型始终优于使用传统微调方法训练的模型，同时具有可解释性。它们达到了更高的客户满意度，并有效地适应了新的、未见过的场景。转移性测试表明，在一个上下文中学到的策略（例如，票务取消）在另一个上下文中（例如，丢失行李索赔）是有益的，突显了该方法的广泛适用性。此外，该方法的可解释性促进了人工监督和策略库的持续优化。结果表明，较小的
    LLM 在特定任务中可以与较大的模型相媲美，从而减少了对高成本、高复杂度模型的需求。我们希望我们可解释的知识蒸馏方法的各种优势能够促进 LLM 在各种挑战性的市场任务中的更广泛应用。
- en: Funding and Competing Interests
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资助与竞争利益
- en: All authors certify that they have no affiliations with or involvement in any
    organization or entity with any financial interest or non-financial interest in
    the subject matter or materials discussed in this manuscript. The authors have
    no funding to report.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所有作者证明他们与任何在本文讨论的主题或材料上有财务或非财务利益的组织或实体没有任何关系或参与。作者没有资助报告。
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agarwal et al. (2024) Agarwal R, Vieillard N, Zhou Y, Stanczyk P, Garea SR,
    Geist M, Bachem O (2024) On-policy distillation of language models: Learning from
    self-generated mistakes. *The Twelfth International Conference on Learning Representations*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等 (2024) Agarwal R, Vieillard N, Zhou Y, Stanczyk P, Garea SR, Geist
    M, Bachem O (2024) 策略性蒸馏语言模型：从自生成的错误中学习。*第十二届国际学习表征会议*。
- en: Agarwal et al. (2023) Agarwal R, Vieillard N, Zhou Y, Stanczyk P, Ramos S, Geist
    M, Bachem O (2023) Generalized knowledge distillation for auto-regressive language
    models. *arXiv preprint arXiv:2306.13649* .
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等 (2023) Agarwal R, Vieillard N, Zhou Y, Stanczyk P, Ramos S, Geist
    M, Bachem O (2023) 针对自回归语言模型的广义知识蒸馏。*arXiv 预印本 arXiv:2306.13649*。
- en: Bordes et al. (2016) Bordes A, Boureau YL, Weston J (2016) Learning end-to-end
    goal-oriented dialog. *arXiv preprint arXiv:1605.07683* .
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bordes 等 (2016) Bordes A, Boureau YL, Weston J (2016) 学习端到端目标导向对话。*arXiv 预印本
    arXiv:1605.07683*。
- en: Brand et al. (2023) Brand J, Israeli A, Ngwe D (2023) Using gpt for market research.
    *Available at SSRN 4395751* .
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brand 等 (2023) Brand J, Israeli A, Ngwe D (2023) 使用 GPT 进行市场研究。*可在 SSRN 4395751
    获取*。
- en: 'Cheng et al. (2024) Cheng Y, Liu W, Wang J, Leong CT, Ouyang Y, Li W, Wu X,
    Zheng Y (2024) Cooper: Coordinating specialized agents towards a complex dialogue
    goal. *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38,
    17853–17861.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 (2024) Cheng Y, Liu W, Wang J, Leong CT, Ouyang Y, Li W, Wu X, Zheng
    Y (2024) Cooper: 协调专门化代理以实现复杂对话目标。*美国人工智能协会会议论文集*，第38卷，17853–17861。'
- en: 'Deng et al. (2023) Deng Y, Liao L, Liang C, Hongru W, Lei W, Chua TS (2023)
    Prompting and evaluating large language models for proactive dialogues: Clarification,
    target-guided, and non-collaboration. *The 2023 Conference on Empirical Methods
    in Natural Language Processing*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2023) Deng Y, Liao L, Liang C, Hongru W, Lei W, Chua TS (2023) 提示与评估大型语言模型以进行主动对话：澄清、目标引导和非合作。*2023年自然语言处理经验方法会议*。
- en: 'Gui and Toubia (2023) Gui G, Toubia O (2023) The challenge of using llms to
    simulate human behavior: A causal inference perspective. *arXiv preprint arXiv:2312.15524*
    .'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui 和 Toubia (2023) Gui G, Toubia O (2023) 使用大型语言模型模拟人类行为的挑战：因果推断视角。*arXiv 预印本
    arXiv:2312.15524*。
- en: 'Ham et al. (2020a) Ham D, Lee JG, Jang Y, Kim KE (2020a) End-to-end neural
    pipeline for goal-oriented dialogue systems using GPT-2\. Jurafsky D, Chai J,
    Schluter N, Tetreault J, eds., *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics*, 583–592 (Online: Association for Computational
    Linguistics), URL [http://dx.doi.org/10.18653/v1/2020.acl-main.54](http://dx.doi.org/10.18653/v1/2020.acl-main.54).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham 等 (2020a) Ham D, Lee JG, Jang Y, Kim KE (2020a) 基于 GPT-2 的目标导向对话系统的端到端神经管道。Jurafsky
    D, Chai J, Schluter N, Tetreault J 编，*第 58 屆计算语言学协会年会论文集*，583–592（在线：计算语言学协会），网址
    [http://dx.doi.org/10.18653/v1/2020.acl-main.54](http://dx.doi.org/10.18653/v1/2020.acl-main.54)。
- en: Ham et al. (2020b) Ham D, Lee JG, Jang Y, Kim KE (2020b) End-to-end neural pipeline
    for goal-oriented dialogue systems using gpt-2\. *Proceedings of the 58th annual
    meeting of the association for computational linguistics*, 583–592.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham 等 (2020b) Ham D, Lee JG, Jang Y, Kim KE (2020b) 基于 GPT-2 的目标导向对话系统的端到端神经管道。*第
    58 屆计算语言学协会年会论文集*，583–592。
- en: Hinton et al. (2015) Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge
    in a neural network. *arXiv preprint arXiv:1503.02531* .
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等 (2015) Hinton G, Vinyals O, Dean J (2015) 蒸馏神经网络中的知识。*arXiv 预印本 arXiv:1503.02531*。
- en: 'Horton (2023) Horton JJ (2023) Large language models as simulated economic
    agents: What can we learn from homo silicus? Technical report, National Bureau
    of Economic Research.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horton (2023) Horton JJ (2023) 大型语言模型作为模拟经济代理：我们能从“硅人”中学到什么？技术报告，国家经济研究局。
- en: 'Hsieh et al. (2023) Hsieh CY, Li CL, Yeh Ck, Nakhost H, Fujii Y, Ratner A,
    Krishna R, Lee CY, Pfister T (2023) Distilling step-by-step! outperforming larger
    language models with less training data and smaller model sizes. *Findings of
    the Association for Computational Linguistics: ACL 2023*, 8003–8017.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等 (2023) Hsieh CY, Li CL, Yeh Ck, Nakhost H, Fujii Y, Ratner A, Krishna
    R, Lee CY, Pfister T (2023) 逐步蒸馏！用更少的训练数据和更小的模型尺寸超越更大的语言模型。*计算语言学协会发现：ACL 2023*，8003–8017。
- en: 'Hu et al. (2021) Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L,
    Chen W (2021) Lora: Low-rank adaptation of large language models. *arXiv preprint
    arXiv:2106.09685* .'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2021) Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen
    W (2021) Lora：大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*。
- en: 'Joshi et al. (2021) Joshi R, Balachandran V, Vashishth S, Black A, Tsvetkov
    Y (2021) Dialograph: Incorporating interpretable strategy-graph networks into
    negotiation dialogues. *International Conference on Learning Representations (ICLR)*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等 (2021) Joshi R, Balachandran V, Vashishth S, Black A, Tsvetkov Y (2021)
    Dialograph：将可解释的策略图网络融入谈判对话中。*国际学习表征会议 (ICLR)*。
- en: Kim and Rush (2016) Kim Y, Rush AM (2016) Sequence-level knowledge distillation.
    *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*,
    1317–1327.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Rush (2016) Kim Y, Rush AM (2016) 序列级知识蒸馏。*2016 年自然语言处理实证方法会议论文集*，1317–1327。
- en: Lester et al. (2021) Lester B, Al-Rfou R, Constant N (2021) The power of scale
    for parameter-efficient prompt tuning. *Proceedings of the 2021 Conference on
    Empirical Methods in Natural Language Processing*, 3045–3059.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等 (2021) Lester B, Al-Rfou R, Constant N (2021) 参数高效的提示调优的规模效应。*2021
    年自然语言处理实证方法会议论文集*，3045–3059。
- en: Lewis et al. (2020) Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal
    N, Küttler H, Lewis M, Yih Wt, Rocktäschel T, et al. (2020) Retrieval-augmented
    generation for knowledge-intensive nlp tasks. *Advances in Neural Information
    Processing Systems* 33:9459–9474.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等 (2020) Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N,
    Küttler H, Lewis M, Yih Wt, Rocktäschel T 等 (2020) 用于知识密集型 NLP 任务的检索增强生成。*神经信息处理系统进展*
    33:9459–9474。
- en: 'Li et al. (2024) Li P, Castelo N, Katona Z, Sarvary M (2024) Frontiers: Determining
    the validity of large language models for automated perceptual analysis. *Marketing
    Science* .'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2024) Li P, Castelo N, Katona Z, Sarvary M (2024) 前沿：确定大型语言模型在自动感知分析中的有效性。*市场营销科学*。
- en: 'Li et al. (2023) Li Y, Zhang Y, Sun L (2023) Metaagents: Simulating interactions
    of human behaviors for llm-based task-oriented coordination via collaborative
    generative agents. *arXiv preprint arXiv:2310.06500* .'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Li Y, Zhang Y, Sun L (2023) Metaagents：通过协作生成代理模拟基于 LLM 的任务导向协调中的人类行为互动。*arXiv
    预印本 arXiv:2310.06500*。
- en: Magister et al. (2022) Magister LC, Mallinson J, Adamek J, Malmi E, Severyn
    A (2022) Teaching small language models to reason. *arXiv preprint arXiv:2212.08410*
    .
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magister 等 (2022) Magister LC, Mallinson J, Adamek J, Malmi E, Severyn A (2022)
    教授小型语言模型推理能力。*arXiv 预印本 arXiv:2212.08410*。
- en: Merrill and Reid (1981) Merrill DW, Reid RH (1981) *Personal styles & effective
    performance* (CRC Press).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merrill 和 Reid (1981) Merrill DW, Reid RH (1981) *个人风格与高效表现* (CRC Press)。
- en: Pomerleau (1991) Pomerleau DA (1991) Efficient training of artificial neural
    networks for autonomous navigation. *Neural computation* 3(1):88–97.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pomerleau (1991) Pomerleau DA (1991) 高效训练用于自主导航的人工神经网络。*神经计算* 3(1):88–97。
- en: Qiu et al. (2023) Qiu L, Singh PV, Srinivasan K (2023) How much should we trust
    llm results for marketing research? *Available at SSRN 4526072* .
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 (2023) Qiu L, Singh PV, Srinivasan K (2023) 我们应如何信任 LLM 结果用于市场研究？*可在 SSRN
    4526072 查阅*。
- en: Ross and Bagnell (2010) Ross S, Bagnell D (2010) Efficient reductions for imitation
    learning. *Proceedings of the thirteenth international conference on artificial
    intelligence and statistics*, 661–668 (JMLR Workshop and Conference Proceedings).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 和 Bagnell (2010) Ross S, Bagnell D (2010) 高效的模仿学习降维。*第十三届人工智能与统计国际会议论文集*，661–668
    (JMLR 研讨会和会议论文集)。
- en: 'Samad et al. (2022) Samad AM, Mishra K, Firdaus M, Ekbal A (2022) Empathetic
    persuasion: Reinforcing empathy and persuasiveness in dialogue systems. *Findings
    of the Association for Computational Linguistics: NAACL 2022*, 844–856.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samad 等 (2022) Samad AM, Mishra K, Firdaus M, Ekbal A (2022) 富有同理心的劝说：在对话系统中增强同理心和说服力。*计算语言学协会发现：NAACL
    2022*, 844–856。
- en: 'Sanh et al. (2019) Sanh V, Debut L, Chaumond J, Wolf T (2019) Distilbert, a
    distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint
    arXiv:1910.01108* .'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等 (2019) Sanh V, Debut L, Chaumond J, Wolf T (2019) DistilBERT，BERT 的精简版本：更小、更快、更便宜、更轻量。*arXiv
    预印本 arXiv:1910.01108*。
- en: 'Snell et al. (2022) Snell C, Yang S, Fu J, Su Y, Levine S (2022) Context-aware
    language modeling for goal-oriented dialogue systems. *Findings of the Association
    for Computational Linguistics: NAACL 2022*, 2351–2366.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell 等 (2022) Snell C, Yang S, Fu J, Su Y, Levine S (2022) 目标导向对话系统的上下文感知语言建模。*计算语言学协会发现：NAACL
    2022*, 2351–2366。
- en: Sun et al. (2019) Sun S, Cheng Y, Gan Z, Liu J (2019) Patient knowledge distillation
    for bert model compression. *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*, 4323–4332.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2019) Sun S, Cheng Y, Gan Z, Liu J (2019) 用于 BERT 模型压缩的患者知识蒸馏。*2019 年自然语言处理经验方法会议及第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，4323–4332。
- en: Tang et al. (2019) Tang R, Lu Y, Liu L, Mou L, Vechtomova O, Lin J (2019) Distilling
    task-specific knowledge from bert into simple neural networks. *arXiv preprint
    arXiv:1903.12136* .
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等 (2019) Tang R, Lu Y, Liu L, Mou L, Vechtomova O, Lin J (2019) 从 BERT
    中提取任务特定知识到简单神经网络中。*arXiv 预印本 arXiv:1903.12136*。
- en: 'Wang et al. (2019) Wang X, Shi W, Kim R, Oh Y, Yang S, Zhang J, Yu Z (2019)
    Persuasion for good: Towards a personalized persuasive dialogue system for social
    good. *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, 5635–5649.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019) Wang X, Shi W, Kim R, Oh Y, Yang S, Zhang J, Yu Z (2019) 为善劝说：朝着一个个性化的劝说对话系统迈进。*第57届计算语言学协会年会论文集*，5635–5649。
- en: 'Wei et al. (2018) Wei W, Le Q, Dai A, Li J (2018) Airdialogue: An environment
    for goal-oriented dialogue research. *Proceedings of the 2018 Conference on Empirical
    Methods in Natural Language Processing*, 3844–3854.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2018) Wei W, Le Q, Dai A, Li J (2018) AirDialogue：一个用于目标导向对话研究的环境。*2018
    年自然语言处理经验方法会议论文集*，3844–3854。
- en: Zhang et al. (2022) Zhang H, Zeng Z, Lu K, Wu K, Zhang S (2022) Efficient dialog
    policy learning by reasoning with contextual knowledge. *Proceedings of the AAAI
    Conference on Artificial Intelligence*, volume 36, 11667–11675.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022) Zhang H, Zeng Z, Lu K, Wu K, Zhang S (2022) 通过推理上下文知识高效学习对话策略。*AAAI
    人工智能大会论文集*，第36卷，11667–11675。
- en: 'Zhang et al. (2023) Zhang Q, Naradowsky J, Miyao Y (2023) Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    *Findings of the Association for Computational Linguistics: ACL 2023*, 6665–6694.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023) Zhang Q, Naradowsky J, Miyao Y (2023) 向专家请教：利用语言模型提高目标导向对话模型中的战略推理。*计算语言学协会发现：ACL
    2023*, 6665–6694。
- en: '{APPENDIX}'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '{附录}'
- en: 6 Robustness Checks
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 鲁棒性检查
- en: 6.1 Alternative LLM for Evaluation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 替代 LLM 评估
- en: 'In the main paper, we used GPT-4 to evaluate the model. To assess robustness
    with respect to the LLM that evaluates the agent, we use LlaMa 3-70b in place
    of GPT-4 and follow the same procedure in Section [4.1](#S4.SS1 "4.1 Main Results
    ‣ 4 Experiments"). The results are shown in Table [7](#S6.T7 "Table 7 ‣ 6.1 Alternative
    LLM for Evaluation ‣ 6 Robustness Checks"). The evaluations are consistent with
    those from GPT-4 in Table [1](#S4.T1 "Table 1 ‣ LLM as Evaluators. ‣ 4.1 Main
    Results ‣ 4 Experiments"): the average correlation between GPT-4 and LlaMa 3-70b
    evaluations is 0.81\. With both LLM evaluators, our proposed method performs consistently
    better than the baselines, only occasionally worse than fine-tuning, while gaining
    interpretability.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要论文中，我们使用了GPT-4来评估模型。为了评估与评估代理的LLM相关的鲁棒性，我们使用LlaMa 3-70b替代GPT-4，并按照[4.1](#S4.SS1
    "4.1 Main Results ‣ 4 Experiments")节中的相同程序进行。结果如表[7](#S6.T7 "Table 7 ‣ 6.1 Alternative
    LLM for Evaluation ‣ 6 Robustness Checks")所示。评估结果与表[1](#S4.T1 "Table 1 ‣ LLM as
    Evaluators. ‣ 4.1 Main Results ‣ 4 Experiments")中的GPT-4一致：GPT-4和LlaMa 3-70b评估之间的平均相关性为0.81\。使用这两种LLM评估者时，我们提出的方法表现始终优于基线，偶尔比微调效果差，同时具有更好的可解释性。
- en: 'Table 7: Average Customer Satisfaction Evaluated by Llama 3-70b'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 由Llama 3-70b评估的平均客户满意度'
- en: '| Methods | Demanding Prompt | Non-Demanding Prompt |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 高要求提示 | 低要求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 3.97 | 4.69 | 4.81 | 4.88 | 3.88 | 4.31 | 4.38 | 4.63 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 基础LLM | 3.97 | 4.69 | 4.81 | 4.88 | 3.88 | 4.31 | 4.38 | 4.63 |'
- en: '| Strategy Imitation | 4.69 | 4.94 | 4.94 | 5.0 | 4.13 | 4.5 | 4.31 | 4.72
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 4.69 | 4.94 | 4.94 | 5.0 | 4.13 | 4.5 | 4.31 | 4.72 |'
- en: '| Fine-Tuning | 4.63 | 4.72 | 4.88 | 4.94 | 4.44 | 4.5 | 4.5 | 4.41 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 4.63 | 4.72 | 4.88 | 4.94 | 4.44 | 4.5 | 4.5 | 4.41 |'
- en: '| Global Guidelines | 4.38 | 4.13 | 4.69 | 5.0 | 3.91 | 3.84 | 4.13 | 4.72
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 全球指南 | 4.38 | 4.13 | 4.69 | 5.0 | 3.91 | 3.84 | 4.13 | 4.72 |'
- en: 6.2 Using kNN to Retrieve Guidelines to Improve Performance
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 使用kNN检索指南以提高性能
- en: During deployment, our method uses the library by retrieving the most similar
    scenario and uses the corresponding strategy to instruct the student LLM to respond
    to the customer. This mechanism can be generalized following the idea of $k$-nearest
    neighbor. Instead of retrieving one most similar scenario, our method can be generalized
    to use $k$ most similar scenarios, allowing the algorithm to be further tuned
    to improve the performance. Here we perform additional experiments of using $k=5$
    and use GPT-4 to evaluate the conversations, to compare with the one generated
    by the original algorithm where $k=1$. The results are shown in Table [8](#S6.T8
    "Table 8 ‣ 6.2 Using kNN to Retrieve Guidelines to Improve Performance ‣ 6 Robustness
    Checks"). Note that when aggregating strategies from multiple scenarios (e.g.,
    when $k=5$), the list could can exceed the input token limit. In such cases, we
    use GPT4 to summarize the guidelines into a shorter list before being used by
    the student.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署过程中，我们的方法通过检索最相似的场景并使用相应的策略来指导学生LLM响应客户。这个机制可以按照$k$-最近邻的思想进行推广。我们的方法可以从检索一个最相似场景推广到检索$k$个最相似场景，从而允许算法进一步调整以提高性能。在这里，我们进行了额外的实验，使用$k=5$并使用GPT-4评估对话，以与原始算法生成的$k=1$的结果进行比较。结果如表[8](#S6.T8
    "Table 8 ‣ 6.2 Using kNN to Retrieve Guidelines to Improve Performance ‣ 6 Robustness
    Checks")所示。请注意，当从多个场景（例如，当$k=5$时）聚合策略时，列表可能会超出输入令牌限制。在这种情况下，我们使用GPT4将指南总结成一个更短的列表，然后由学生使用。
- en: 'Table 8: Evaluation of Conversations Generated by Retrieving Strategies of
    1 or 5 Most Similar Scenarios'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 通过检索1或5个最相似场景的策略评估生成的对话'
- en: '| Methods | Demanding Prompt | Non-Demanding Prompt |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 高要求提示 | 低要求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| $K=1$ | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| $K=1$ | 4.88 | 5.0 | 5.0 | 5.0 | 3.44 | 3.75 | 4.28 | 4.75 |'
- en: '| $K=5$ | 4.75 | 4.75 | 5.0 | 5.0 | 3.66 | 4.03 | 4.06 | 4.66 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| $K=5$ | 4.75 | 4.75 | 5.0 | 5.0 | 3.66 | 4.03 | 4.06 | 4.66 |'
- en: Results show that $k$ does have an impact on the performance of LLM as larger
    $k$ indicates a longer and richer list of guidelines. However, it is interesting
    that more instructions do not always improve performance; the improvement depends
    on the context and the LLM. Therefore, in practice, we recommend that $k$ is treated
    as a hyper-parameter and tuned via a validation set before deployment. Tuning
    of $k$ can be independent of the training step, and can be customized for different
    use of the library.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，$k$ 确实会影响 LLM 的性能，因为较大的 $k$ 表示更多的指导原则列表。然而，有趣的是，更多的指令并不总是能提高性能；这种提升依赖于上下文和
    LLM。因此，实际上，我们建议将 $k$ 视为超参数，并在部署之前通过验证集进行调整。$k$ 的调整可以独立于训练步骤，并可以根据库的不同用途进行定制。
- en: 6.2.1 How use of kNN to Retrieve Guideliness Improve Context Transferability
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 使用 kNN 检索指导原则如何提高上下文可转移性
- en: We also tested using a larger $k$, specifically $k=5$, when retrieving guidelines
    across different contexts. As shown in Table [9](#S6.T9 "Table 9 ‣ 6.2.1 How use
    of kNN to Retrieve Guideliness Improve Context Transferability ‣ 6.2 Using kNN
    to Retrieve Guidelines to Improve Performance ‣ 6 Robustness Checks"), the evaluation
    by GPT-4 indicates that a larger $k$ tends to lead to better performance for cross-context
    use of the library. This is because, in a new context, every scenario in the library
    is relatively distant from the input. Therefore, there is not much difference
    in relevance between the most similar (closest) scenario and the k-th closest
    scenario. Including more scenarios does not negatively impact relevance, but it
    increases the diversity of advice, which can ultimately benefit the model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测试了在不同上下文中使用较大的 $k$，具体为 $k=5$，来检索指导原则。如表[9](#S6.T9 "表9 ‣ 6.2.1 使用kNN检索指导原则如何提高上下文可转移性
    ‣ 6.2 使用kNN检索指导原则以提高性能 ‣ 6 鲁棒性检查")所示，GPT-4 的评估表明，较大的 $k$ 倾向于在库的跨上下文使用中获得更好的性能。这是因为在新的上下文中，库中的每个场景与输入相对较远。因此，最相似（最接近）的场景与第
    $k$ 个最接近的场景之间的相关性没有太大区别。包括更多的场景不会对相关性产生负面影响，但它增加了建议的多样性，这最终可能有利于模型。
- en: 'Table 9: Context Transferability Assessment Evaluated by GPT-4 for different
    K'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：GPT-4 对不同 $K$ 的上下文转移性评估
- en: '| Methods | Demanding Prompt | No-Demanding Prompt |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 需求提示 | 非需求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Base LLM | 4.5 | 5.0 | 5.0 | 4.91 | 3.13 | 4.25 | 4.5 | 4.63 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Base LLM | 4.5 | 5.0 | 5.0 | 4.91 | 3.13 | 4.25 | 4.5 | 4.63 |'
- en: '| K=1 | 4.5 | 5.0 | 5.0 | 5.0 | 4.0 | 4.63 | 4.5 | 5.0 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| K=1 | 4.5 | 5.0 | 5.0 | 5.0 | 4.0 | 4.63 | 4.5 | 5.0 |'
- en: '| K=5 | 4.88 | 5.0 | 5.0 | 5.0 | 4.0 | 4.6 | 4.75 | 5.0 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| K=5 | 4.88 | 5.0 | 5.0 | 5.0 | 4.0 | 4.6 | 4.75 | 5.0 |'
- en: '7 Distribution Shift Drives Drop in Performance: Evidence'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 分布偏移导致性能下降：证据
- en: In Section [4.2](#S4.SS2 "4.2 Evaluating the Solution to Distribution Shift
    ‣ 4 Experiments"), to demonstrate the value of preventing distribution shift in
    our algorithm, we ran an ablated version of our algorithm. In the ablated algorithm,
    we do not specifically tackle distribution shift - all scenarios are generated
    only by the teacher interacting with the customer LLM. The evaluation in Table
    [4](#S4.T4 "Table 4 ‣ 4.2 Evaluating the Solution to Distribution Shift ‣ 4 Experiments")
    shows that the ablated algorithm downgrades the performance significantly.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[4.2节](#S4.SS2 "4.2 评估分布偏移的解决方案 ‣ 4 实验")中，为了展示我们算法防止分布偏移的价值，我们运行了一个削减版本的算法。在削减的算法中，我们没有专门处理分布偏移——所有场景仅由教师与客户
    LLM 互动生成。第[4表](#S4.T4 "表4 ‣ 4.2 评估分布偏移的解决方案 ‣ 4 实验")中的评估显示，削减算法显著降低了性能。
- en: 'To show that the decrease of performance is indeed due to distribution shift,
    that the later scenario in the conversation, the more deviation there will be
    in the algorithm, we conduct another evaluation. We use GPT-4 to evaluate only
    the first half of the conversation generated by the original algorithm and the
    ablated algorithm, interacting with the same customer LLM with the same prompt.
    We then compute the drop of average rating in percentage in Table [10](#S7.T10
    "Table 10 ‣ 7 Distribution Shift Drives Drop in Performance: Evidence"), which
    is the difference in the rating devided by the rating of the original algorithm,
    denoted as $\Delta_{\text{half}}$ in the Table. We also compute the drop in rating
    from the full conversations evaluated in Table [4](#S4.T4 "Table 4 ‣ 4.2 Evaluating
    the Solution to Distribution Shift ‣ 4 Experiments"), denoted as $\Delta_{\text{full}}$.
    If there is distribution shift, then $\Delta_{\text{half}}$ should be a lot smaller
    than $\Delta_{\text{full}}$ since there should be more deviation at the end of
    the conversations than at the beginning of the conversations.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明性能下降确实是由于分布偏移，即对话的后期场景中的算法偏差更大，我们进行另一项评估。我们使用 GPT-4 仅评估原始算法和去除部分算法生成的对话的前半部分，与相同客户
    LLM 使用相同提示进行交互。然后我们计算表 [10](#S7.T10 "表 10 ‣ 7 分布偏移驱动性能下降：证据") 中的平均评分下降百分比，这即是评分的差异除以原始算法的评分，记作表中的
    $\Delta_{\text{half}}$。我们还计算了表 [4](#S4.T4 "表 4 ‣ 4.2 评估对分布偏移的解决方案 ‣ 4 实验") 中评估的完整对话的评分下降，记作
    $\Delta_{\text{full}}$。如果存在分布偏移，则 $\Delta_{\text{half}}$ 应该远小于 $\Delta_{\text{full}}$，因为对话结束时的偏差应大于对话开始时的偏差。
- en: Results show that $\Delta_{\text{half}}$ is always less than half of $\Delta_{\text{full}}$,
    indicating the deviation in performance does not happen evenly across a conversation
    but more towards the end.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，$\Delta_{\text{half}}$ 总是小于 $\Delta_{\text{full}}$ 的一半，这表明性能偏差并不是在对话中均匀发生的，而是更倾向于发生在对话的末尾。
- en: 'Table 10: Evaluation of Part of Conversations'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：部分对话的评估
- en: '| Methods | Demanding Prompt | Non-Demanding Prompt |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 高要求提示 | 低要求提示 |'
- en: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa-2 7b | LlaMa-2 13b | LlaMa-2 70b | GPT-3.5 | LlaMa-2 7b | LlaMa-2 13b
    | LlaMa-2 70b | GPT-3.5 |'
- en: '| Strategy Imitation | 4.88 | 5.0 | 5.0 | 5.0 | 4.85 | 4.91 | 4.97 | 4.88 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿 | 4.88 | 5.0 | 5.0 | 5.0 | 4.85 | 4.91 | 4.97 | 4.88 |'
- en: '| Strategy Imitation (ablated) | 4.91 | 4.88 | 4.88 | 4.97 | 4.72 | 4.88 |
    4.88 | 4.72 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 策略模仿（去除部分） | 4.91 | 4.88 | 4.88 | 4.97 | 4.72 | 4.88 | 4.88 | 4.72 |'
- en: '| $\Delta_{\text{half}}$ | -0.6% | -2.4% | -2.4% | -0.6% | -2.7% | -0.6% |
    -1.8% | -3.3% |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta_{\text{half}}$ | -0.6% | -2.4% | -2.4% | -0.6% | -2.7% | -0.6% |
    -1.8% | -3.3% |'
- en: '| $\Delta_{\text{full}}$ | -7.8% | -5.0% | -7.4% | -3.8% | -5.5% | -5.1% |
    -5.1% | -7.8% |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta_{\text{full}}$ | -7.8% | -5.0% | -7.4% | -3.8% | -5.5% | -5.1% |
    -5.1% | -7.8% |'
- en: '8 How Teacher LLM Iteratively Updates Strategies for Student LLM: Example'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 教师 LLM 如何迭代地更新学生 LLM 的策略：示例
- en: 'As shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.2.2 Strategy Teaching: ‣ 3.2 Knowledge
    Distillation ‣ 3 Library-based Interpretable Knowledge Distillation"), the strategies
    for each scenario are learned iteratively by comparing the student’s updated response
    with the teacher’s response and proposing changes to the strategies. As the strategies
    are updated, the student iteratively refines its response to become more and more
    similar to the teacher’s response. Below we illustrate the iterative process using
    an example.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [1](#S3.F1 "图 1 ‣ 3.2.2 策略教学： ‣ 3.2 知识蒸馏 ‣ 3 基于库的可解释知识蒸馏") 所示，每个场景的策略通过将学生更新后的响应与教师的响应进行比较，并提出对策略的修改，从而迭代地学习。当策略更新时，学生会迭代地调整其响应，使之越来越接近教师的响应。下面我们通过一个示例来说明这一迭代过程。
- en: In the example below, the scenario consists of two turns. This exchange represents
    the core of the conversation, where the initial pleasantries and information gathering
    are complete. At this stage, the agent transitions to the critical task of providing
    the customer with the necessary assistance and solutions.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，场景包含两个回合。这一交流代表了对话的核心部分，其中最初的寒暄和信息收集已完成。在此阶段，代理转向提供客户所需帮助和解决方案的关键任务。
- en: '<svg   height="124.34" overflow="visible" version="1.1" width="600"><g transform="translate(0,124.34)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 108.98)"><foreignobject
    width="556.69" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Scenario</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="77.49" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Agent: Hello, how can I help
    you? Customer: Hello, I recently booked a flight with your airline, and I need
    to cancel it. I tried to do it online, but I couldn’t find an option to cancel.
    Can you assist me with canceling my flight? Agent: Of course, I’d be happy to
    assist you with that. Could you please provide me with your booking reference
    number so I can pull up your reservation details? Customer: Sure, my booking reference
    number is ABC123.</foreignobject></g></g></svg>'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg   height="124.34" overflow="visible" version="1.1" width="600"><g transform="translate(0,124.34)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 108.98)"><foreignobject
    width="556.69" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Scenario</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="77.49" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Agent: Hello, how can I help
    you? Customer: Hello, I recently booked a flight with your airline, and I need
    to cancel it. I tried to do it online, but I couldn’t find an option to cancel.
    Can you assist me with canceling my flight? Agent: Of course, I’d be happy to
    assist you with that. Could you please provide me with your booking reference
    number so I can pull up your reservation details? Customer: Sure, my booking reference
    number is ABC123.</foreignobject></g></g></svg>'
- en: The student’s initial response (from the base LLM, without following any strategies
    from the teacher) is very different from the teacher’s response. The student does
    not provide any explanation of the policy or offer alternative solutions but directly
    expresses rejection. It overall sounds very cold and blunt.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 学生的初始回应（来自基础 LLM，没有遵循教师的任何策略）与教师的回应非常不同。学生没有提供任何政策解释或提供替代方案，而是直接表示拒绝。整体上听起来非常冷漠和直接。
- en: '<svg   height="174.58" overflow="visible" version="1.1" width="600"><g transform="translate(0,174.58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 156.37)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Teacher Response</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="124.88" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Teacher: Thank you for providing
    the booking reference number. Please allow me a moment to retrieve your booking
    details. [After a moment] I see that you have booked a restricted ticket. As per
    our policy, restricted tickets are non-refundable and non-changeable. This is
    because these tickets are offered at a lower cost compared to our flexible tickets.
    I understand this might not be the news you were hoping for, and I apologize for
    any inconvenience this may cause. However, we do have some alternative solutions
    that might be helpful. We can offer you travel credits for the value of your ticket,
    or the possibility of a one-time rescheduling, subject to applicable fees. Would
    you like to hear more about these options?</foreignobject></g></g></svg><svg   height="110.58"
    overflow="visible" version="1.1" width="600"><g transform="translate(0,110.58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 92.38)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Student Response</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="60.88" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Student (base LLM): Thank you
    for providing that information. I have pulled up your reservation, and I see that
    you have a restricted ticket, which is non-refundable and non-changeable. I understand
    that you wish to cancel your flight, but I must inform you that as per the purchase
    agreement, restricted tickets are not eligible for free cancellations or refunds.</foreignobject></g></g></svg>'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="174.58" overflow="visible" version="1.1" width="600"><g transform="translate(0,174.58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 156.37)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">教师回复</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="124.88" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">教师：感谢您提供了预订参考号码。请允许我稍等片刻以检索您的预订详情。[稍等片刻]
    我看到您预订的是一张限制票。根据我们的政策，限制票是不可以退还和更改的。因为这些票的价格低于我们的灵活票。我理解这可能不是您所希望得到的消息，对由此造成的不便我深表歉意。不过，我们有一些替代方案可能对您有帮助。我们可以为您提供相当于票面价值的旅行积分，或者提供一次性重新安排的可能性，但需支付相关费用。您是否想了解更多关于这些选项的信息？</foreignobject></g></g></svg><svg   height="110.58"
    overflow="visible" version="1.1" width="600"><g transform="translate(0,110.58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 92.38)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">学生回复</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="60.88" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">学生（基础 LLM）：感谢您提供的信息。我已经调出了您的预订记录，并看到您持有的是一张限制票，这张票不可退还也不可更改。我理解您希望取消航班，但我必须通知您，根据购买协议，限制票不符合免费取消或退款的条件。</foreignobject></g></g></svg>
- en: Based on these two responses, the teacher compares and then proposes changes
    to the student to improve, using the teacher’s response as the desired output.
    The teacher’s strategies at the first iteration point out the issues and offer
    suggestions for improvement. Incorporating the suggestions, the student’s response
    after iteration 1 improved significantly. The student offered an explanation of
    the policy and offered alternative solutions to the customer.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这两次回应，教师对比并提出改进建议，以教师的回应作为期望输出。教师在第一次迭代时指出问题并提供改进建议。结合这些建议，学生在迭代1后的回应显著改善。学生提供了政策解释并提出了替代解决方案。
- en: '<svg   height="276.63" overflow="visible" version="1.1" width="600"><g transform="translate(0,276.63)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 258.42)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Strategies and Updated Student Responses At Iteration 1</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="226.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Strategies: 1\. Acknowledge Customer’s Request: Acknowledge the
    customer’s request promptly and express willingness to assist. 2\. Provide Explanation:
    Offer clear and empathetic explanations of the situation, such as ticket restrictions.
    3\. Offer Alternatives: Demonstrate a proactive approach by providing alternative
    solutions or options to assist the customer. 4\. Avoid Blunt Statements: Avoid
    using blunt statements without context or empathy, as it can be dismissive to
    the customer. 5\. Avoid Lack of Explanation: Provide clear explanations to avoid
    leaving the customer confused or frustrated. 6\. Avoid Failure to Offer Solutions:
    Offer alternative solutions or options to prevent a negative customer experience.
    Student: Thank you for providing your booking reference number, ABC123\. After
    reviewing your reservation, I see that you have purchased a restricted ticket.
    I understand that you wish to cancel your flight; however, I must inform you that
    restricted tickets are non-refundable and non-changeable as per the purchase agreement.
    This policy allows us to offer these tickets at a lower cost.While I cannot provide
    a full refund for your ticket, I can offer you some alternatives. You may be eligible
    for travel credits or a one-time rescheduling of your flight, subject to applicable
    fees. Would you like to explore these options further?</foreignobject></g></g></svg>'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg   height="276.63" overflow="visible" version="1.1" width="600"><g transform="translate(0,276.63)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 258.42)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Strategies and Updated Student Responses At Iteration 1</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="226.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Strategies: 1\. Acknowledge Customer’s Request: Acknowledge the
    customer’s request promptly and express willingness to assist. 2\. Provide Explanation:
    Offer clear and empathetic explanations of the situation, such as ticket restrictions.
    3\. Offer Alternatives: Demonstrate a proactive approach by providing alternative
    solutions or options to assist the customer. 4\. Avoid Blunt Statements: Avoid
    using blunt statements without context or empathy, as it can be dismissive to
    the customer. 5\. Avoid Lack of Explanation: Provide clear explanations to avoid
    leaving the customer confused or frustrated. 6\. Avoid Failure to Offer Solutions:
    Offer alternative solutions or options to prevent a negative customer experience.
    Student: Thank you for providing your booking reference number, ABC123\. After
    reviewing your reservation, I see that you have purchased a restricted ticket.
    I understand that you wish to cancel your flight; however, I must inform you that
    restricted tickets are non-refundable and non-changeable as per the purchase agreement.
    This policy allows us to offer these tickets at a lower cost.While I cannot provide
    a full refund for your ticket, I can offer you some alternatives. You may be eligible
    for travel credits or a one-time rescheduling of your flight, subject to applicable
    fees. Would you like to explore these options further?</foreignobject></g></g></svg>'
- en: This process continues as the strategies are updated over iterations and the
    student further improves its response, until it sounds “close enough” to the teacher’s
    response. The final strategies and the student response are shown below. These
    strategies are then paired with the scenario and stored in the library, and included
    in the prompt during deployment.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程持续进行，策略在迭代中不断更新，学生的回应进一步改善，直到它听起来“足够接近”教师的回应。最终的策略和学生回应如下所示。这些策略随后与场景配对并存储在库中，在部署时包含在提示中。
- en: '<svg   height="475.88" overflow="visible" version="1.1" width="600"><g transform="translate(0,475.88)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 457.68)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Strategies and Student Responses at the Last Iteration</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="426.18" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Strategies:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="475.88" overflow="visible" version="1.1" width="600"><g transform="translate(0,475.88)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 457.68)"><foreignobject
    width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">最后一次迭代中的策略和学生回应</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="426.18" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">策略：
- en: '1\. Acknowledge Customer’s Request: Promptly acknowledge the customer’s request
    and express willingness to assist.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 确认客户请求：迅速确认客户的请求并表达愿意提供帮助。
- en: '2\. Provide Explanation: Offer clear and empathetic explanations of policies
    or restrictions that may affect the customer’s request.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 提供解释：清晰而富有同理心地解释可能影响客户请求的政策或限制。
- en: '3\. Offer Alternatives: Demonstrate a proactive approach by providing alternative
    solutions or options to assist the customer.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 提供替代方案：通过提供替代解决方案或选项，展示主动的处理方式，以帮助客户。
- en: '4\. Express Empathy and Understanding: Show empathy and understanding towards
    the customer’s situation.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 表达同理心和理解：对客户的情况表现出同理心和理解。
- en: '5\. Maintain Professional and Courteous Tone: Use polite language and maintain
    a professional tone throughout the conversation.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 保持专业和礼貌的语气：使用礼貌的语言，保持专业的语气贯穿整个对话。</foreignobject></g>
- en: '6\. Avoid Blunt or Overly Formal Language: Refrain from using blunt or overly
    formal language that may sound robotic or impersonal.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 避免生硬或过于正式的语言：避免使用可能显得机械或不近人情的生硬或过于正式的语言。
- en: '7\. Avoid Lack of Empathy: Ensure empathy and acknowledgment of customer disappointment
    in responses.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 避免缺乏同理心：在回应中确保表现出同理心并承认客户的失望。
- en: '8\. Avoid Transactional Communication: Be proactive in offering options and
    solutions rather than being too transactional in responses. Student: AGENT: Thank
    you for providing your booking reference number, ABC123\. After reviewing your
    reservation, I see that you have purchased a restricted ticket. I regret to inform
    you that restricted tickets are non-refundable and non-changeable as per the purchase
    agreement. This policy allows us to offer these tickets at a lower cost. I understand
    that you need to cancel your flight, and I empathize with your situation. While
    I cannot provide a full refund for the restricted ticket, I can offer you some
    alternative solutions. One option is to consider converting the value of your
    ticket into travel credits that you can use for future bookings with our airline.
    Additionally, if you need to reschedule your flight, we can explore the possibility
    of a one-time rescheduling, subject to applicable fees. I hope these alternatives
    can be helpful to you. Please let me know how you would like to proceed, and I
    will assist you accordingly.</foreignobject></g></g></svg>'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 避免交易性沟通：在回应中要主动提供选项和解决方案，而不是过于交易化。学生：代理：感谢您提供您的预订参考号，ABC123。经过审查您的预订，我发现您购买的是受限票。很遗憾地通知您，根据购买协议，受限票是不可退款和不可更改的。这一政策使我们能够以更低的价格提供这些票。我理解您需要取消航班，并且对您的情况表示同情。虽然我无法为受限票提供全额退款，但我可以为您提供一些替代方案。一个选项是考虑将您的票面值转换为旅行积分，以便您在未来的预订中使用。此外，如果您需要重新安排航班，我们可以探索一次性重新安排的可能性，但需支付相关费用。希望这些替代方案对您有所帮助。请告知您希望如何进行，我会相应地协助您。
