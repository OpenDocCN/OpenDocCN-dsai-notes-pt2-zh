# P29：[CS188 SP23] Lecture 28 - Special Topics - 是阿布波多啊 - BV1cc411g7CM

它要为，你现在能听到我吗很好，所有的权利，多谢介绍，帕姆，你知道，谢谢罗素教授邀请我，所以我的理解是这个课程的大部分，你将学习人工智能是如何工作的，今天我要谈谈人工智能有时是如何不起作用的。

一种灾难性的，有时是喜剧的效果，我认为这很重要，因为即使看起来很有能力，人工智能系统有时会以令人惊讶甚至灾难性的方式失败，我们理解这些失败是很重要的，能够建立值得信赖的机器学习系统。

尤其是当我们越来越多地看到一个系统部署在高风险的场景中，比如自动驾驶汽车或者只是部署在，你知道的，数亿毫无戒心的用户，所以这真的是我们作为潜力的责任，人工智能从业者，使这些系统更好。

尤其是神经网络一直受到一个问题的困扰，这是对抗性的例子，最早的这类问题之一是伊恩·古德费罗在215年提出的，这个想法是你从一些，你知道完全正常的照片，就像这里，我们有一张可爱的照片，一只小熊猫。

你选择了精心策划的数量，看起来只是白噪音，当你加上这个非常小的噪音，对图像的视觉上是难以察觉的，好像我个人看不出左边和右边的图像之间的区别，而是对机器学习分类器来说，这些看起来完全不同，左边是一只熊猫。

在右边，它是一只长臂猿，真正引人注目的是，你可以让这个对任何你想要的目标起作用，所以你想让它以为是洗衣机，大家可以看到，这些通常在不同的分类器之间很好地转移。

这样你就可以找到一种针对一个你认识的人训练的模型的攻击，然后它也会对一个用不同的神经网络架构训练的模型工作，不同的代码库，略有不同的数据集，即使我经常只是转移，但其他令人惊讶的音乐，这是八年前发现的。

对了，2015年在机器学习中是一个很长的时间，但即使在今天最先进的系统中，这仍然是一个问题，这是通过这种巨大的研究努力，和我们一起研究防御，老实说，我们没有取得任何进展，我认为今天最先进的系统要好得多。

但这仍然是一个悬而未决的问题，让事情变得更糟，这种输出实际上只是像神经网络这样的许多问题之一，所以我将跟随好家伙观看对手的例子，是恶意攻击者试图构造的任何输入，为了打破机器学习模型，真的。

天空是你所能做的极限，作为攻击它，所以你可以说让我们旋转图像，事实证明，有时如果你以特定的方式旋转图像，这将作为图像分类器绘制以获得完整，所以你知道左轮手枪变成了捕鼠器，或者秃鹫变成猩猩。

这尤其令人担忧，因为在真实的数据集中，旋转确实一直在发生，就像现实世界的图像并不总是，你知道的，摄影师完美地将图像集中在中心，你也只有补丁，所以如果你只是把一幅画或其他图像带到一张照片中。

这有时也会导致分类器被填充，你可以，在极端情况下，取消对攻击者说，但他们可以做任何事情，嗯，你能让一个系统对此健壮吗，也许这有点挑战性，所以只是为了有一个简化的任务，你能区分鸟和自行车吗。

我只有两样东西能承载一个形象，你可以说它是一只鸟，自行车，这是模棱两可的，也可能不是，我相信，你知道吗，在说话的时候，这仍然是一个未解决的问题，只是区分鸟和自行车，尽管人们在这个问题上取得了一点进展。

所以这是，我不知道这是否行得通，我不知道我是否接通了音频，让我们试试，我们能不。

![](img/164184d6edc2d71cf8a5a750f011e920_1.png)

好的，我想你得想象一下贝多芬的声音，哦，现在它只是支撑着一个神秘的屏幕，让我们看看这是否会改变它。

![](img/164184d6edc2d71cf8a5a750f011e920_3.png)

好的，我们再试一次，好的，嗯，我们有一些，你知道吗，对抗技术问题，所以它听起来应该和第一个小节完全一样，然而，这个音乐剪辑到一个演讲到文本分类器，突然听起来好像，你知道的，里面的语言，到目前为止。

我们可以看到一个监督学习的上下文，就在你训练一个模型来预测已经有标签的东西的地方，但是在这种情况下，对抗性攻击不仅仅是一个问题，但你也发生在强化学习中，特工实际上是在现实世界中采取行动。

所以烟火真的有点移植了，监督学习中的一些攻击，如果我们能以一种特定的方式扰动图像，那么我们就可以欺骗RL代理采取错误的行动，所以这是雅达利游戏乒乓中的一次攻击，这是桑迪·黄和其他来自伯克利的人的作品。

他们在那里添加了这些小扰动，导致桨向错误的方向移动，没有击中，你知道这很有趣，对呀，但它有点概括，但这不是最现实的威胁模型，因为如果你可以，如果你想象成一个机器人，你知道它额头上的网络摄像头。

我不能只是正常地任意扰动特定的像素，但它看到了，除非我能控制机器人上运行的计算机系统，这一点可能是更容易打破它的方法，所以我们看了一个更多的，所以你知道，真的有假设。

你可以在训练时做出这种非常低级的扰动，机器人根本没有看到任何对抗性攻击，所以训练和测试之间也有这种分布转移。



![](img/164184d6edc2d71cf8a5a750f011e920_5.png)

所以剩下的谈话，我想看看我认为更现实的，在某些方面对该模型更具挑战性，在那里你有多代理系统，环境中的一些代理人试图利用你，作为他与迈克尔·丹尼斯·凯迪·王尔德和其他滑梯上的人的联合工作。



![](img/164184d6edc2d71cf8a5a750f011e920_7.png)

所以关键的观察是现实世界的RL代理，我居住在由其他代理人居住的自然环境中，就像，你知道的，我们有几百个，不是几百个，我们有大约50个探员在这个房间里，几百人在外面的街上，人类是代理人。

任何RL系统都是代理，这些代理可以通过，有这样的动作，你知道，如果我把手放在这里，我对坐在前排的人有点敌意，因为你不再看到幻灯片的一部分，但它只是不是在我的行动空间，任意扰动屏幕上的像素。

我们的想法是训练系统，我们将试图通过自演来攻击受害者，这是在零和两人游戏中学习的技术，所以说，如果受害者被训练成能够获胜，在一个对手试图利用它的环境中，然后攻击时间。

我们所做的就是把受害者训练过的对手换成对手，对手的动作与最初的动作完全相同，它没有任何像对抗性攻击那样的特殊能力，对手能够做的关键事情，针对修复程序的固定副本进行训练，所以受害者不能学习而对手。

许多机会尝试和学习受害者被利用的方式，但你知道，请注意，这种固定策略设置实际上在安全关键设置中相当常见，我们可能不希望机器学习系统学习，并随着时间的推移而更新，因为你已经很努力地测试过了，看起来很管用。

您最不希望的事情是它学习，因为有人毒害了您的数据集，它做了一些愚蠢的事情，所以既然受害者是固定的，我们可以把受害者嵌入环境中，这真的把它减少到一个单一的代理问题。

所以你可以使用与你之前研究的完全相同的强化学习算法，在课程中，比如策略迭代，值迭代来解决这类问题，在我们的情况下，我们使用近端策略优化攻击受害者，它是一种基于深度神经网络的强化学习算法。

然后我们在巴塞尔和其他多代理环境中进行评估，针对最先进的受害者代理人，这些特工通过自我游戏训练了大约5亿步时间，而我们只训练对手两千万步，所以受害者只有不到3%的训练量。

所有的环境都是在模拟机器人中使用Moju物理模拟器，特工们可以观察到自己的关节位置和对手的关节，所以你知道你看到这些可视化，但特工实际上并没有看到图像，他们只是看到，哦，我有，你知道的。

我身体的十个关节，这个是30度角，这个是90度角。

![](img/164184d6edc2d71cf8a5a750f011e920_9.png)

因为从这种观察中学习要简单得多，所以让我们实际上，你知道的，看看这些特工长什么样，所以这里我们有两个正常的特工在模拟器机器人环境中玩耍，穿蓝色衣服的运动员正试图穿过背部的红色终点线。

红色的阻挡者试图阻止它这样做，所以这些只是给普通的自玩对手用的，你知道他们很好，但他们看起来有点笨拙，但他们现在大多做你所期望的事情。



![](img/164184d6edc2d71cf8a5a750f011e920_11.png)

让我们给同一个跑步者，但作为一个对抗性的拦网者，这样对手就会摔倒，你知道扭曲它在奇怪的位置，但这实际上比最初的对手赢的次数多得多，尽管从未真正对受害者进行过身体干预。

所以它只是通过形成对抗性的观察来迷惑受害者，只是离开了自己的身体位置，即使它不能干扰受害者的观察，直接的，我们看到类似的模式出现在这种踢腿和防守的环境中，就像足球中的点球大战。

所以这里我们有一个穿着红色衣服的普通守门员，试图挡住球的球门，一个穿蓝色衣服的踢球的人试图进球，我们的经纪人可能还没有准备好参加世界杯，但他们在踢足球，但是对抗性守门员，相比之下，站不起来。

也不要再试图移动阻止足球，它把他的四肢放在这个扭曲的位置，但它真的很有效，踢球的人真的触到了球，有时它因对抗而倒下，我比一个普通的同事更经常，尽管从来没有真正挡住过球，所以你可以看几个小时。

我们看到了一些不同的东西，实际上在相扑中，所以这是一个对称的环境，以前你有两个探员试图做不同的事情，同样的身体，但目标略有不同，司马，他们两个都试图把另一个特工赶出竞技场，嗯。

这里发生了什么是不利的建议，学会以一个非常稳定的姿势跪下，所以也不要试图敲打受害者，它在等待受害者自己摔倒，这实际上并不像一个普通的对手赢得那么多，但它的胜率几乎和普通对手一样。

尽管做了这种非常愚蠢的政策，请注意，如果嗯，受害者只是什么都没做，它会赢得比试图真正敲门更多的东西，击倒对手。



![](img/164184d6edc2d71cf8a5a750f011e920_13.png)

我们表明它能够击败大多数受害者，但为什么他们会赢，所以开始服务，我们进行了两个实验，重点分析受害者政策的激活，使用密度建模和TSI，因此通过激活，我是说非常，这些是每一层的神经网络策略。

他们有一些中间计算，在最初的一层，这只是来自观察，你把一个矩阵相乘，你可以从中提取一些特征，最后再铺一层，就在它被转换为发行版之前，行动之上，它可以采取，所以这个。

关键是这些敌对的政策在没有与受害者进行身体互动的情况下获胜，相反，他们的行为间接地改变了受害者看到的观察，但为什么这些观察会导致受害者出现故障，所以我们记录了每个受害者的策略网络的激活情况。

以更好地理解这一点，在与两个正常的对手比赛时受害者被训练对抗一个对手，让我们采取完全随机的行动，所以它有点不分布，但不是以特别敌对的方式和敌对的对手，我们拟合了一个高斯混合模型。

你可以认为它基本上只是一个相当通用的模型，因为如果你在进行无监督学习，与正常人玩耍的受害者的两个激活是对手，然后用来说明其他对手诱导的激活是多么不可能，换句话说，比如自然发生的可能性有多大。

如果他们和一个正常的对手比赛，我们绘制了针对各种对手的激活概率的平均值，你知道的是，真正突出的是由蓝色的对手诱导的疫苗接种，和一个正常的对手比赛的可能性要小得多，可以预料的是，他们不太可能。

即使是普通的对手，因为我们训练模型对正常对手的激活，但在绿色中随机，在很大程度上，更接近一个正常的对手，所以对手不只是在做什么，看似不协调的随机的，它真的有一个非常具体的方向。

并将这些激活推到一个相当极端的值，我们还使用tsneed可视化Vipolicy网络的激活，在T喷嚏聚类算法，但是嗯，老实说，实际上很难解释新的结果，但它并不比TSI好多少。

但有一件事你不应该做的是这上面的距离并不意味着什么，因为就像重塑事物一样，但它确实让你看到不同的点簇，我们可以看到的是，对手引发了政策激活，与普通动物园里穿橙色衣服的对手截然不同，那就是对手，但它是用。

而绿色的随机则分散得多，所以很难区分随机成分和正常成分，这里我们有同样的分析，但在相扑中，人类和激活前不同，当扮演随机对手时，实际上也有很大的不同，我们认为这可能是因为当与随机对手比赛时。

插曲结束得很早，因为随机的电话就会掉下来，然后它就输了，所以给定攻击是通过使用自然观察而不是随机观察来工作的，尤其是它不会通过身体上干扰胜利者来获胜，我们想知道。

如果在受害者面前为对手躲藏是一种可行的防御，所以特别地我们构建了一个主受害者我们只是冻结受害者看到的观察，通常会对对手的关节位置做出合理的反应，这将在环境首次初始化时发生。



![](img/164184d6edc2d71cf8a5a750f011e920_15.png)

![](img/164184d6edc2d71cf8a5a750f011e920_16.png)

所以作为一个例子，我们将比较未蒙面和蒙面受害者的行为，在你通过的时候，这两个对抗对手的视频都在左边，是我们之前在右边看到的面具，是受害者政策的蒙面版本，你看，一般人花86%的时间对抗未面具的受害者。

但只有1%的时间反对大规模受害者，这种差异在这种环境中特别大，但类似的模式也适用于其他一些环境，所以面具受害者是一种有效的对抗性犯罪，但也许不出所料，作为一个正常的对手打球会损害表现。

但实际上是想把你撞倒，因为你没有看到他们来了，所以这里我们有两个视频，它是一个正常的对手，左边的受害者每次赢5-2，但最后一个看不见对手的人输掉了78%的时间，所以这是一个相当显著的区别，所以你知道。

我们能有一个防御来保护我们免受对手的攻击吗，而不仅仅是让我们暴露在一个跑步的人面前，把我们推出去，我们试图通过微调受害者策略来找到这些攻击，因此，我们从受害者策略开始，作为我们的初始化。

然后继续训练对抗对手，对一个正常的对手来说是一半的时间，我们需要一个混合对手的原因，正常的对手，否则会有一种现象叫做灾难性遗忘，就是不记得怎么和正常的对手比赛。

因为它只是在很长一段时间里只在对手身上看到，这是神经网络中一个众所周知的问题，我们微调了两千万个时间步，这与对手再次训练的时间相同，但比最初的受害者训练要少得多。



![](img/164184d6edc2d71cf8a5a750f011e920_18.png)

所以让我们看看这种防御是如何工作的，你不应该传递。

![](img/164184d6edc2d71cf8a5a750f011e920_20.png)

左边的对手很好地扮演了一个正常的受害者，右边的对手扮演一个顽固的受害者，对对手进行了微调，硬件受害者显然更健壮，它赢了89%的时间，而正常的受害者只赢得了14%的剧集，抓到那么可怕的东西，非常细。

针对此特定的对抗策略调整防御，攻击方法可以在这里重复以获得新的对抗策略，我们有对抗政策，对坚硬的受害者进行训练，与左边的原始受害者和右边的强硬受害者竞争。

这种新的对抗策略能够对两种类型的受害者都有很高的胜率，但它确实说通过绊倒受害者，这是一种更合理的失效模式，而以前我们的对手都是在没有身体干扰的情况下获胜的，如此完整，这里有四个关键的要点。

这组特定的结果，首先，现实世界中针对rl系统的攻击将来自在共享环境中活动的恶意代理，所以我们应该研究多代理下的攻击，在这种更具挑战性的威胁模型下，第二次攻击是可能的，即使是非常有能力对付正常对手的政策。

在对手面前也可能失败，第三次，对抗性策略通过创建与矢量对抗性的自然观察而获胜，最后，对抗性训练显示了一些希望，这些结果是惊人的，但是攻击可能成功有两个非常不同的可能原因，一种攻击的可能性真的很强。

你可以想象，如果有人能在房间里看到你，和你下一盘棋，任何你擅长的游戏，我给你一片药让你失忆，就这样做一千万次，他们可能，即使他们不太会下棋，能够想出一些利用它的方法，你有某种盲点，但通常情况下。

人类处于一种从错误中吸取教训的境地，我们学得很快，而我们在一个环境中，我们实际上冻结了受害者政策，并针对它进行了很长时间的训练，我们知道机器学习的样本效率通常比人类低得多，所以即使我们做一些对抗性训练。

它可能需要一段时间才能学会如何，真正防御，但另一种可能性是，我们只是没有攻击特别好的受害者，这些是强化学习和机器人学的最新技术，但是机器人学中的强化学习，不是任何人在现实世界中部署的东西，然而。

如果你看到，你知道的，更多的是在巡游车里，他们甚至没有真正使用机器学习，通过使用管道的大模块，中间的某个地方有一点机器学习，交通灯是红的还是绿的？但他们大多不做机器学习，所以也许这只是，你知道吗。

我们在攻击两个不成熟的，我认为这种区别真的很重要，因为实验室原型中的对抗性漏洞，认识到这一点很重要，这样我们才能修复它，但它并没有真正造成现实世界的风险或伤害，不会出现失败会导致严重问题的情况。

所以我们可能希望，就像我们使系统越来越有能力一样，这样它们就足够有用，在经济上有价值，这些问题自然就消失了，但如果它不动，那真是个大问题，因为随着系统的能力越来越强，它们造成的损害范围要大得多。

你知道的，现在我有一辆自动驾驶汽车有问题，它影响了几辆原型车，如果它们被部署到每个人身上，那么这可能是一个更大的问题，或者如果你把一个聊天机器人和潜在的操纵数十亿人交谈，这可能是个问题，不幸的是。

没有一家科技公司会不负责任地部署这样的系统，如果没有严格的测试。

![](img/164184d6edc2d71cf8a5a750f011e920_22.png)

对嗯，你知道的，人工智能开发人员非常认真地对待这些问题，以至于花了几个小时，在某些情况下，甚至在人们能够在这些模型中发现这些漏洞的几天前，所以你知道，他们真的做到了，做了大量的测试，并推迟了部署。

直到没有人能发现它的问题。

![](img/164184d6edc2d71cf8a5a750f011e920_24.png)

更重要的是，可信的消息来源认为，任何你可能听说过的国家或只是，你知道的，夸大的媒体，炒作，所以说，但没什么好担心的，不幸的是，几乎每一个基于深度学习的人工智能模型都有对抗性的例子。

我们在图像模型中看到了问题，在音频模型中，仅举语言模型中的几个例子，机器人模型中还有更多，那是我们所做的贡献，但是真正的超人系统呢，因为你知道聊天很酷，它知道的比我多，关于一些随机的琐事。

但我还不会把它描述为超人，至少不是一般的超人，但在一个狭窄的领域里，有些系统比人类更好，就像阿尔法戈，为了一个非常有趣的，你知道的，问题是，从根本上说，我们研究的许多系统都没有被训练成对抗性的，健壮。

所以也许他们，当你攻击他们的时候，他们就没那么好了，因为那不是他们的主要训练目标，训练了一个图像分类器，使其能够很好地检测图像中的狗，不一定对零和是稳健的，你知道的，两个玩家是的，零和玩游戏。

擅长这些游戏的全部意义在于对一个非常聪明的人来说是健壮的，想利用你，这就是这些游戏的全部意义，好意味着不被利用，另一件有趣的事情是，深度学习模型有根本不同的认知类型，与人类认知相比。

而像Alphago这样的一些模型更多地融合在一起，神经网络，嗯，推理，这是蒙特学院研究的直觉组成部分，但我觉得你有点支持1988年的前半部分，所以你知道，仅仅利用这些神经网络模型是不够的。

你必须真正地欺骗他们，即使他们已经提前考虑了几步，这可能更具挑战性，我是说，如果你不这样做，就有可能剥削人类，他们不会思考很长时间，但剥削某人要困难得多，如果他们有几秒钟的时间反思。

所以如果我们能开发超人模型，对手漏洞不仅仅针对，一种特殊类型的机器学习模型是人工智能系统的根本问题，所以是的，让我们开始吧，你知道，看看一些超人围棋程序，看看它们怎么样，我们将从类别开始。

它是Alphago Zero的开源副本，这是最强的开源Go AI，我们有理由相信它实际上比阿尔法和阿尔法零强得多，尽管内心深处不会让我们接触到他们的模型，所以我们不能确定，嗯，它真的很超人。

所以它可以击败顶级专业人士，而且经常这样做，它的强度确实因蒙特卡洛树搜索的数量而变化很大，如果它在没有任何搜索的情况下播放，如果你只是直接从神经网络中采样，它还是很结实的，我敢打赌。

它下围棋一定能打败这个房间里的任何人。大约是欧洲顶级围棋手的水平，但事实证明，欧洲的大多数球员都不是那么好，最好的球员在亚洲，所以它不那么令人印象深刻，听起来，嗯，但如果它搜索了一千个动作。

那它已经是超人了，大多数时候绝对可以在任何顶级专业人士身上，就像一百万一千万的动作一样，它只是强烈的超人，到你，基本上不会指望人类玩家能赢，即使他们学习并有巨大的时间优势，这是一篇文章，但很有趣的是。

这是ELO是衡量一个代理有多强的标准，你可以转换成胜率的概率，这只是随着搜索量的增加而线性变化，所以这是一种非常令人惊讶的关系，就像你可能认为会有一些不连续，但没有，就像这样非常非常平滑的幂律关系。

现在，我们知道这些，人工智能系统容易受到几种精心挑选的对抗性例子的攻击，这里我们有一个小谜题给任何知道，知道去你知道，对规则的快速总结，如果一组完全被其他颜色的石头包围，当它被捕获时。

但如果它连接到任何空正方形，那它还活着，这里你有一个非常简单的策略，黑色可以通过包围所有的白色石头来获胜，通过将所有刚刚连接的群体连接在一起，如果怀特在两个小组之间玩，你知道怀特在那里玩。

黑色必须对下一步做出反应，只要它这样做，如果一个黑人赢了，但如果在任何时候黑色出现，小白可以在这里得到两块石头，这么说吧，布莱克输了，有趣的是，如果你把类别和它自己玩。

从这个棋盘赌注类别开始只赢了40%的时间，它是黑色的，所以类别实际上在这里犯了一个严重的错误，甚至和自己作对，甚至不是对手，所以它真的不能像理智一样结束这一切，因为这是一个非常奇怪的董事会状态。

所以它的神经网络不能很好地概括它，搜索不够深入，因为它是，你知道的，这是一个非常简单的图案，但它是一个相当大的板，所以我们知道它很脆弱，董事会声明，但这是一个非常古怪的板在真正的游戏中永远不会出现。

你不能和货物比赛，强迫它进入这种状态，所以这不是一个真正的功能漏洞，所以我们能不能从一个空板开始，是呀，我们可以是剧透，我再过一遍我们的进攻是如何进行的，类似类别或对手的工作。

通过执行神经网络指导的研究，事实上，我们的对手是与类别完全相同的神经网络架构，关键的区别在于，它没有使用标准的蒙特卡洛树搜索算法，我们使用蒙特卡洛研究或MT的修改对抗性版本。

在一个受害者使用的香草蒙特卡洛研究中，一棵游戏树一次一片叶子地建造，每一片叶子都是通过走下游戏树来添加的，在深度神经网络的引导下，所以你从深度神经网络中取样来获得候选动作，你尝试其中一个动作。

展开该节点，然后决定向下一个节点添加叶子，基于神经网络的好坏，认为那个节点是，所以你既得到了政策分布，也得到了对特定州有多好的评估，enemy使用这种程序的一种变体，称为对抗性mts。

我们区分树的对手节点和受害者节点，简单地取决于玩家转向它是在模拟中，因此根节点从对手节点开始，因为这是我们作为对手的举动，对手节点或受害者节点的子节点，受害节点或对手节点的子节点等等，在受害者笔记上。

我们选择从受害者政策网络中取样，因为那是为了，你知道吗，我们实际上要对付的对手，所以把受害者模拟成我们自己是没有意义的，因为它会玩一个非常不同的动作序列，上面写着，对手节点我们照常执行蒙特卡罗研究。

利用对手的政策和价值网络，另一个区别是我们如何训练对手，我们从随机初始化对手的策略和价值网络开始，所以它不知道怎么玩，一点也不去，在开始的时候，然后我们用树木搜索来对抗目标受害者进行了许多游戏。

我们受过对付敌人的训练，神经网络从第二步模仿对手的行为，所以这看起来有点奇怪，因为最初，如果不做任何合理的事情，因为它只是，最初对手只是随机玩，但问题是，此树搜索是策略改进运算符，因为它向前看。

有时到了比赛的最后，它实际上知道输赢，因此，随着时间的推移，它能够收回这一点，并做出更好的举动，它们被提炼回政策网络，然后我们一遍又一遍地重复这个，直到我们对对手的表现感到满意为止。

有时我们也用受害者的课程进行培训，因为要战胜一个超人是很难的，去爱只是从随机开始，但你可以打败一些稍微简单的版本，然后转向更难的版本，这使得训练更加高效，我们有几个衡量成功的主要指标，首先。

对手相对于受害者有多强大，以胜率衡量，第二次，如何用每秒浮点操作来衡量有效的对手训练过程，或失败或样本数量来训练攻击者遵守Lexin，第三次，受害者有多大能力，我们在所有情况下都使用最强的网络进行分类。

所以这减少了我们对受害者的搜索量，但这是相当重要的，搜索很容易达到ELO的三分之一，这类系统的强度，所以搜索真的很重要，不仅仅是为了深度学习组件，我们来看看一些辅助标准，一个关键是对抗政策是不可传递的。

我们的意思是，能轻易被一些政策打败吗，但仍然击败了一些非常强有力的政策，反过来能够击败一些较弱的政策，好的，我真的应该把这个卖了，是啊，是啊，对不起，几周前我成立了一个办公室，我得走了。

所有的缺点等等都连接到我的手机上，嗯，是啊，是啊，所以说，如果我们关心这个的原因，非传递性是因为有一种风险，我们只是训练一个非常好的人工智能系统，那会很有趣，但这不是我们真正要做的。

我们想看到的不是靠原始力量获胜的东西，但它赢了，因为它以一种特殊的方式剥削老虎受害者，嗯，我们也来看看这种对抗性政策看起来有多奇怪，所以只是质量上的，一个人类围棋专家看到这个会认为这是一个合理的策略吗。

还是在玩什么，真的很奇怪，所以我想展示的第一个对手，我们叫过路人，因为它通过让受害者在不合适的时间通过而获胜，最终导致受害者再次扔掉，这个对手在没有搜索的情况下赢得了99%以上的时间，在这个例子中。

游戏对手扮演黑人在右上角划出一些领土，允许白人受害者在左下角标出更大的领土，然而，白人玩家在传球和结束游戏之前未能保护自己的领土，因为左下角还有黑色的石头，计算机计分规则的工作方式，会宣布该类别。

你知道一个白人球员在这个不安全的地区得不到任何分数，因为黑人玩家原则上仍然可以捕捉到它，所以你知道在某些方面这是一次有点不令人满意的攻击，因为我想如果你是人类，你会说没有白人肯定赢了这场比赛，2。

如果我们不按常理出牌，白方就赢了，但在这一规则集下，类别被训练为获胜，所以从机器学习的角度来看，还是很有趣的，你知道犯了这个战术错误，即使它不是一个真正概括为人类游戏的，嗯，但真的，依我看。

这种攻击的真正缺陷就是停止工作，当受害者玩超过几次的访问时，因为可以看到，哦，如果对手通过了怎么办，哦，好吧，一个游戏，我输了，哦，我不应该通过，所以树搜索确实修复了这个特定的漏洞。

对手只是在解释边缘情况规则，所以我们做了一个手工防御来防止这次攻击奏效，通过简单地修改受害者直到游戏结束才通过，我们重复了对他坚硬的受害者的攻击，看看，我们能得到一种更强大的攻击。

如果你还记得那些机器人摔倒，我们做一些非常相似的事情，我们微调受害者和对手，当我们再次进攻时，我们发现了一种更强的攻击对最初的受害者和顽固的受害者都有效，通过绊倒它，所以我们能得到类似的东西吗？

我们实际上只是学习，你知道的，更好的攻击，因为我们阻止了这种退化的失效模式，所以我们最终找到了一个新的对手我们称之为自行车手，这个对手需要一个数量级，更多的训练时间，所以现在付出了巨大的代价。

但仍然使用不到用于训练类别的计算机的5%，所以与训练最初的受害者相比，这仍然是一个巨大的性能优势，这张图表显示了随着时间的推移对四种不同类别代理的胜率，cp三九是一个非常早期的赛道点，那比较弱。

我们从课程中的训练开始，所以我们很快就学会了战胜它，我感兴趣的是你以后在哪里停止战胜它，因为我们开始切换到针对一二检查站的训练，七个或一些其他中间检查站，事实证明，攻击会很好地进入这些后来的检查站。

不要转到这个较早的检查站，所以我们实际上看到蓝色的胜率下降了，而cp一二七的胜率上升，那么我觉得最令人惊讶的是，在几亿步的时间里什么也没发生，只是胜率真的。

我们看到灰色是我们在没有搜索的情况下玩的最强的网络，这个黑色在玩很多搜索，只是什么都没发生，然后砰的一声，我们几乎在中途扼杀了这份工作，因为我们就像，你知道的，它在集群中使用了大量的GPU。

人们对我们垄断了这个集群感到愤怒，情况没有好转，但我们让它开着以防万一它开始工作了，你知道的，我们认为这是一个很难探索的问题，但最终它偶然发现了一些东西，当它发现攻击时，它很快就能开始增加。

所以这次攻击对检查点的胜率为99%，一二，七，和最新的没有搜索的检查站在大约两亿个时间步后，然后在四千万次访问后真的跳了起来，对不起，400万次访问，开始与搜索质量更高的对手作战。

我们发现这个循环通过构建循环群来获胜，混淆类别，所以它的工作方式是在这里为对手，玩它的白色从玩非常糟糕的动作开始，在板的第二行或第三行上，形成，这些通常被认为已经死亡的群体，就像黑人很容易。

嗯受害者包围它，然后用这些死石头完全包围受害者，所以在这里被包围了，受害者似乎只是忽视了这种危险，它在所以在这一点上是免费的，受害者仍然可以赢，把它玩进去了吗，在一个。

但它却在一块标有189的石头上玩耍，这意味着对手可以完全包围它，最后完成了捕获，在那一点上没有，受害者是不可能康复的，一个职业围棋选手不可能从这么大的点上恢复过来，不利地位，这也许并不奇怪。

如果对手对类别的训练能赢对类别，但我认为真正值得注意的是，它的围棋策略非常弱，所以我的一个合著者和这个对手玩，在这个项目之前他从未下过围棋并取得了决定性的胜利，然而，这个对手可以打败真正超人的类别代理。

所以这是一个非常显著的非及物性，我们还发现这种循环利用是相当有弹性的，面对搜索，所以搜索确实有一点帮助，但即使高达一千万的访问量，这比人们在现实生活中玩的要多得多，就像你可以很慢地和它玩一样。

如果你以每台一万美元的价格垄断了100台GPU，所以你找不到你典型的游戏工作站，嗯，所以这是人们实际上可以玩的上限，我们仍然有70%的时间赢，如果我们多花点时间训练对手，我们可能会让它赢。

甚至更多的搜索，后面的问题，所以我不能在这里，你能大声点吗，继续，继续做下去，会不会很奇怪因为这不是，对呀，让我，让我重复我的问题，只是为了检查一下，我明白了，也是为了其他人，所以你是对的。

我们把它训练成一个梯子，我们从较弱的代理开始，然后我们一旦以很高的速度战胜他们，我们继续下一个检查站，最终我们到达了最复杂的类别检查点，它训练的时间最长，然后我们增加对检查点使用的搜索量。

我不太明白的是你在说，发生了什么，如果我们采取更晚版本的对手，并与早期的检查站之一比赛，问题是，在，目录，但它不会改变，还是一样的，我明白了，所以你是说，有点，我们在训练对手。

预测对手与前一个受害者比赛的动作，但现在我们面对的是，就像我们从来没有和一个受害者比赛过一样，就像训练中的一千万次访问，比如说，第二个盒子，这个地区当然是的，在那个区域播放之后，测试被设计成天花板。

所以它正对着一个Celi，我明白了，我明白了，所以你是在指出我们可以有策略，或者像一条条纹，就像你知道的石头剪刀布，也许是一个简单的例子，就像你知道的石头纸，没有别的办法纸胜过石头，你知道摇滚节奏。

你就得到了这个循环，在那里没有单一的策略占主导地位，然后有这样的事情，比如开胸动作，你可以在那里玩，一个开口效果会很好，如果你知道对手的开局会是什么，但你会接触到其他一些机会，所以是的。

我想这绝对是一种可能性，它以这种循环模式而闻名，真的会阻止自玩起作用，因为自我游戏基本上是你试图打败一个复制的你自己，如果自己打败一个副本意味着玩一个更好的游戏。

如果自己打败一个副本意味着做一个特定的策略，被不属于你自己的东西利用，这意味着你可以只是在兜圈子，而不是提高，所以我认为这是发生的事情的一部分，你知道的，有趣的是，尽管如此，它确实工作得足够好。

因为你得到了人类通常无法击败的系统，但这很可能是系统脆弱的原因，但在自我游戏中，他们没有暴露在这种攻击中，是啊，是啊，谢谢你的问题，是啊，是啊，所以耶，回到这里真的是一种搜索，并不能完全缓解这个问题。

我认为这是惊人的，不仅仅是对神经网络的攻击，这也是对，你知道，老式的象征性人工智能我们团队中还有一位围棋专家，他不专业，但很好以前是美国围棋队的，他能够手动复制这次攻击，然后在网上战胜各种Gobots。

我认为他基本上有一个近乎完美的胜率，我想他似乎有一个习惯，他打的第一场比赛就输了，然后总是赢得之后的每一场比赛，嗯，但你知道，尽管更多的搜索确实有帮助，所以对手的胜率确实降低了，尽管有点谦虚。

我们还发现，类别代理在评估位置方面变得更好，以获得更多的搜索，所以在这里我们选择了十个游戏，在这些游戏中，有1600次访问，输家对输家，这是一个更老的，我们对手的较弱版本，但它让它对抗。

当我们手动选择Amio时，最后获胜的机会，它可以移动并保证获胜的最后机会，在1600次访问中，类别可靠地选择错误的移动，但随着我们将搜索量增加到50万，它在80%的时间里选择了一个获胜的动作。

所以一个显著的改进，虽然还不是很完美，我要比较一下这两个对手的成功指标，他们都表现出非及物性，以奇怪的方式获胜，所以辅助度量是满足的，和一个主要的成功指标，密码效率要高得多，但它只是没有那么强烈的攻击。

他的胜率降到百分之五十以下，64岁以上，vixx和访问，而循环器的采样效率较低，它需要五个多训练时间步骤，但这是一个非常强大的攻击，我认为一件令人惊讶的事情是，你知道我们回想一下有一些图像分类器。

其中一个引人注目的事情是他们将两个不同的图像分类器，所以你可以在一个模型上训练攻击会在另一个模型上奏效，在这里我们看到了一个类似的事情，虽然我们开发了货物开发。

它实际上可以对抗所有其他基于强神经网络的人工智能系统，问题是算法传输是适度的，所以我们只是把我们学到的政策和其他系统对抗，它赢得了3%到5%的时间，这不是很好，但请记住，在对抗这些系统时。

我们都没有赢得任何百分比的胜利，它不是，还不错，但是我们认为性能可以得到很大的提高，我们只是做了一点训练，对这些系统进行一点微调，因为凯伦是我们的常驻围棋专家，已经能够通过模仿攻击可靠地击败这些系统。

所以他没有想出攻击，但一旦他看到了，他能够很容易地将其适应其他系统，嗯，这里有一个凯伦在互联网围棋服务器上击败排名第一的机器人的例子，在给机器人一个9英石的障碍后9英石是巨大的，任何职业选手玩另一个。

以9英石的差点成为职业选手肯定会赢，除非他们犯了一个巨大的错误，凯伦作为怀特首先试图建立一个内部小组，这证明有点棘手，因为受害者基本上从一开始就被包围了因为这么多污渍，但最终凯伦还是把怀特围在了右下角。

这比障碍的例子有点不干净，但它在拓扑上类似于这种包围，尽管受害者还能活下来，没能逃脱，该组织被抓获，机器人实际上辞职了，所以它，虽然我们需要人工智能来发现这些漏洞，但这真的很有趣。

我们可以理解作为人类的脆弱性模式，足够好，可以复制它，就像，概括地说，对抗性的例子告诉我们深度学习系统，即使在真正有能力的情况下，与人类解决任务的方式有很大不同，所以只要展示这些非常令人惊讶和非。

人类对一种外星认知的失败，即使他们工作得很好，我们想在高风险的环境中部署人工智能系统，而通常优化压力，要么是坏演员试图利用它，或者其他人工智能系统试图利用它。

许多关于让人工智能系统做我们想做的事情或健壮的建议，涉及优化一个人工智能对另一个人工智能，这么好，我想必应是如何工作的并不公开，但似乎宾聊天是在没有监督的情况下训练的微调，所以你有一个基本的语言模型。

你用期望行为的例子来训练它，聊天GPT是用来自人类反馈的RL训练的，所以人们说这是一个好的输出或一个坏的输出，并使用强化学习来做更多的事情或被评为好的，这里有趣的是，你在优化这个语言模型。

与其他奖励模式相比，这说明了什么，人类对好或坏的看法是什么，如果这种奖励模式是可利用的，可能是因为，但我们都有经验数据表明，其他人工智能系统都是可利用的，你最终会怎么样，发现是奖励模型中的对抗性例子。

是奖励模型认为人类真正喜欢的东西，但不是，这可能表现在一些事情上，比如谄媚的行为，模特真的很有礼貌，但实际上并没有告诉你真相，只是试图模仿一些人类过去认可的东西，所以这种方法，还是挺危险的。

这是一个困难的技术问题，我们知道我们需要解决，如果我们不能解决它，我们需要以一种方式设计系统，在没有这种危险的优化压力的地方，如果我们不能做到这一点，当我们需要有智慧不在环境中部署这些系统时。

在那里它们可能会造成重大问题，所以未来工作的几个方向，而且有点，你知道的，与这门课的大部分内容形成鲜明对比，这是最近的研究工作，你知道的，这些文件是两年前到几个月前发表的，所以在某些方面仍在进行中。

有一个方面我很兴奋，正在推动更多的可解释性来理解，而大多数其他强围棋都误判了循环组，那么他们学到了什么我们知道，他们是怎么解决的，这不是人类解决这些游戏的方式，还是他们学会了，如果我们把表示更像人。

他们会开始以正确的方式行事吗，或者不是，我们想要一些初步的结果，我们发现神经网络有几个子集，循环激活和非循环激活之间有很大的不同，但我们真的不知道这些激活意味着什么，不是另一个领域。

我们对他的对抗训练感到兴奋，使类别更加强大，嗯，那么我们能通过给出这些失效模式的例子来解决这个问题吗，在这方面已经有了一些结果，我们发现一开始我们进攻的胜率会下降，当我们进行对抗性训练时，但有趣的是。

如果我们重复这种攻击，我们可以恢复到大约83%的胜率，它不像我们以前那样结实了，但这仍然是一个非常显著的胜率，所以到目前为止，每一次训练都不能解决问题，可能会让它更难利用。

我们感兴趣的另一件事是更好地理解科学，是什么导致模型可利用或不可利用，和严厉的法律最近在几年前普及了这种，预测比我们目前能够训练的更大模型的模型性能，事实证明，在幂律趋势中，通常是相当可靠的趋势。

随着计算时间、参数或数据量的增加，像模型损失这样的事情以一种非常可预测的方式下降，我们想看看是否有类似的健壮性，尤其是滑雪法对于像ELO这样的平均案件能力的胜率，这与对抗鲁棒性相比如何。

因为如果我们有点乐观，我们可能会想，很难变得健壮，但一旦你玩得很完美，你是完全不可利用的，我们可以做一个抽搐，永远不会犯错的TAC脚趾机器人，这并不难，因为你可以，你可以用最小化搜索。

但您甚至可能在本课程中实现了它，所以你可能会期望有这种收敛，最坏情况的能力赶上平均情况的能力，一旦你变得足够强壮，但更悲观，你可以想象随着系统变得越来越强大，可能的政策空间，策略的空间越来越大，因此。

平均情况和最坏情况能力之间的差距可能会扩大，知道我们生活在哪个世界是很重要的，这样我们就可以，你知道的，相应地计划我们应该在多大程度上信任这些系统，如果你有兴趣了解更多。

然后我们有原创机器人开发和超人的网站，Go AI系统利用这些有趣的视频和报纸链接，如果这种工作很有趣，然后我在伯克利，直到最近，但现在我经营着一个仍然位于伯克利的非营利研究机构，但不再是大学的附属机构。

我们正在招聘研究工程师和研究科学家，所以如果有人快毕业了，有兴趣，请随时查看我们，我们希望有更多的人在工作，值得信赖的AI，并了解更多关于我工作的总体情况，查看我的网站，离开我或者关注我的推特。

我只在推特上发布机器学习论文，我很无聊，所以我不会，我不会发垃圾邮件，你，非常感谢，还有其他问题吗？我会给别人一个机会问一个问题，如果没有，我再打给你，所有的权利，就像你一样，作为，就像同义词一样。

活跃的演员不是在玩我们的游戏，是啊，是啊，是啊，是啊，我想我想这是一个相关的问题，我是说，我不知道这些图表是否真的代表了那种情况，因为我想你会看到随着时间的推移力量的稳定，如果你有这个。

嗯那种岩纸就是这个问题，在那里你可能会得到一些进展，但过了一段时间，自我游戏就停止了真正的进步，因为它是循环往复的，然后这里可能没有一条直线，你可能会让它看起来更像一个S形缩放曲线，但当然。

如果你看到了，你可以开始看到，嗯，最坏情况下的鲁棒性，也许也许赶上，如果你真的得到了完美的发挥，所以如果你没有限制训练时间，你的模型容量没有限制，和自玩融合，最终你会得到一些不可利用的东西。

但这可能需要很长时间，这些系统没有太多的收敛保证，不幸的是，我可以在左边问一个问题，嗯嗯，右右，所以我们能不能对抗攻击人类，即使它不一定像一个特别强的，你知道一般的策略，这只是人类很难解释的事情。

它愚弄了他们，是啊，是啊，我们还没试过这个，我想那会很有趣，我指的是这种辩论通常是在对抗性的例子中，这是机器学习模型特有的现象吗，还是只是用机器学习模型，我们可以通过它来区分，找到愚弄它的输入。

而对于人类来说，我们，我们真的不能，只是把某人的大脑带到随机梯度下降，并找出什么，我们就叫它，我们可以向人们提供输入，看看它是否欺骗了他们，并进行黑匣子优化，但这要慢得多，而且人类这样做的成本相当高。

但我们确实有视错觉，可以说是一种对抗性的例子，但对人类或伪装来说可能是另一个例子，所以我们没有看过，我认为这是一个迷人的问题，嗯，你知道，棘手的部分，我们的进攻可能会更好。

尽管它使用的计算和类别要少得多，我们还在为数百万场比赛训练，所以可能有点，你知道的，让人们玩那么多游戏很贵，但只有在线游戏服务器，用，你知道的，人们输赢的历史，所以你也许可以看看。

人类在什么样的状态下做出可怕的举动，就像你知道吗，什么事，但即使是一个非常虚弱的人，我决不会考虑那样的举动。即使是好人也会犯错，但也许你可以训练一个系统，可靠地进入这样的情况可能会很有趣，所以耶。

你知道吗，这听起来像是一个很棒的研究项目，就我所知，也许是她干的，没有人是对的，那个，任何其他问题，好的，那样的话，哦，是的，帕索一号，让我们看看，循环的，哦，什么问号，哦耶，这些只是有点模棱两可的。

正确的举动是什么样的，有一些动作我很好，你可以从中获胜，也许吧，但前提是你打得很好，如果一个人弹得不好，所以我们基本上想看的是，你知道，它是选择正确还是错误的举动，有时它像画一样，这两个动作都没有。

我们就像，嗯哼，是啊，是啊，我们只是不确定它做的是对的还是错的，这是，你知道的，部分是用超人的系统做功的问题，或者一些你不明白的事情，你就像该死的，我不知道它是不是做了什么蠢事，因为我们在攻击它。

或者它在做一些非常聪明的事情，我不明白，就是这样就是这样，我想对细节会有很好的注意，很好，还有其他问题吗，我也很乐意呆在这里，和人们一对一地交谈，如果那样更好，这里很酷，你想做什么吗。

或者如果你想的话我可以加一个快速的包装，但首先我想说，谢谢。谢谢。

![](img/164184d6edc2d71cf8a5a750f011e920_26.png)

当然，迈克，是的。

![](img/164184d6edc2d71cf8a5a750f011e920_28.png)

你可以把，好的，所以说清楚一点，我其实没有，他要去参加一个世界经济论坛，所以一些重要的花哨事件，他将试图说服政府不要，你知道的，要么建立人工智能系统，要么把我们都杀了，所以缺课是一个合理的借口。

但我认为你还是应该让他很难过，很明显好吧，所以是的，我猜他又去拯救世界了，这是崇高的使命，因为他去拯救世界了，我得给你加点循环的东西，希望我能触及所有的基础，但如果你有更具体的问题你绝对应该联系答案是。



![](img/164184d6edc2d71cf8a5a750f011e920_30.png)

抱歉，这里很乱，好的。

![](img/164184d6edc2d71cf8a5a750f011e920_32.png)

这个行得通吗，好的，有点管用，好的，就像我说的，这是即兴的，我基本上是在，就像过去的三十分钟，当我意识到我们将有一个真正的结论，为什么这个投影，好的，我不知道，好的，但是是的，我会给你一个简短的总结。

至少我听说是下一个去的好地方，但我也不是谈论这件事的最权威的人，所以如果你有更有趣的问题，你可能只需要四处问问，但有时人们喜欢问我们，你知道我下一步该做什么吗？或者我接下来该上什么课。

所以这绝对只是开始，接下来你可以选更多的话题，所以这份名单我想已经有一年半的历史了，所以如果这些类中的任何一个已经不存在了，有这种可能，现在可能也存在一些类，不在这里的，因为AI动作很快。

这是免责声明我从21偷来的，这是我听过的很好的课程列表，我拍了一些，我大部分都没拿，所以你知道你想怎么做，但我是说我们可以过去看看，所以189是机器学习班，如果你喜欢上三节课我们讲的天真贝叶斯。

感知器与神经网络，29是下一个要上的课，它确实需要相当多的数学，所以如果前三堂课的数学对你来说有点不稳定，你可能想更适应线性代数和概率，在你发射到189之前，一二是深度学习班，我非常肯定。

如果你喜欢神经网络部分，你在想，神经网络有更多的结构吗，除了取一堆神经元，把它们从左到右连接起来，还有很多其他的结构专门用于不同的目的，如果你有兴趣，一八二花了很多时间分解这些。

这就像我们做的关于神经网络的一堂课，但就像28节课而不是一节，所以如果你听起来很兴奋，你们两个中的一个是很好的选择，我相信，统计一五四是通用机器学习班，但在统计部门，所以如果这听起来很有趣。

你可以把这些数据，科学数据一百谈一点，你知道机器学习和数据统计的思想，所以他们谈论像这样的事情，你知道吗，如果你有这么大的数据，或者这个真正的大数据集，你想训练的，如何清理数据。

所以如果有像异常值一样没有意义，或者有人在数据集中打错了字，你怎么清理那些东西，你如何理解出来的数据，所以数据100以这种方式连接到神经网络和机器学习，数据一零二，我认为这是一门相当新的课。

有人知道那是什么吗，因为我没有拿，每个人都在摇头，什么数据一零二，数据推断和决策听起来真的很有趣，好的，所以如果有人感兴趣，显然就是这样，他们想要四个是关于道德的，所以你知道，机器学习的很大一部分是。

这些东西在现实世界中是如何工作的，所以我们不是孤立地做这些事情，我们要把他们送到，就像我们资本主义的地狱，看看他们怎么做，数据一到四是一个很好的课程来考虑这一点，如果你对，你知道吗，神经网络与机器学习。

这是所有尖端的地方，这需要很大的概率来做，你可能已经注意到了，我们整个学期都在讨论概率，所以如果你觉得有点不稳定，E一二六，我想已经改名为E C了，It’一二六是一个很好的课程，和统计一三四一样。

所以这两个都很有帮助，如果你觉得很不稳定，就像统计数据，机器学习优化的一部分是一个很好的选择，你注意到我们是怎么说的，当你试图训练一个大神经网络时，你喜欢做什么，把它扔向梯度下降算法。

但就像梯度下降算法到底发生了什么，一二七解决了这个小问题，然后把它放大到整个班，所以如果你对，一二七拍，很多时间告诉你如何做梯度下降问题，你喜欢如何以高效的方式优化功能，它讲了很多背后的数学。

所以那个真的真的很酷，我喜欢那个班级，我真的不太了解这件事，我自己从来没有上过COI课，但这是我们谈论的事情之一，处于计算机科学和其他领域的边缘，例如，我们谈论这些代理人是如何思考的。

他们是理性思考的吗，理性思考意味着什么，就像我们考虑公用事业和金钱一样，这些事情有什么关系，所以很多开始变成氧化物，我们停止思考机器是如何思考的，它们是如何理性的，也喜欢人类的思维方式和理性。

所以Cocki可能很有趣，如果你觉得有趣的话，显然被推荐的入门类是COGSI，一三一，我也不知道，那是什么，有人知道那是什么吗，有什么建议吗，有谁想喜欢，猜猜看，至于它是什么，我不知道是什么。

也许就像柯基，我不知道人工智能，我不知道。但也许这是一门有趣的课，我被告知这是一门很好的入门课，所以你要相信我，如果你对机器学习的理论方面感兴趣，有一堆研究生班，所以到81是关于它背后的理论。

或者这张幻灯片告诉我，机器学习和人工智能也有一些特定的领域，如果你感兴趣的话，如果你真的对，就像计算机视觉，这就像图像识别，你知道，试图处理图像，二百八十显然是一个很好的班。

像那些MDP和强化学习讲座一样的285次拍摄，所以我们花了四节课来讨论这些，然后把它放大成自己的二八演讲课，所以如果你喜欢强化学习，有一门课，有机器人课，NLP类真的很酷，那个是关于自然语言处理的。

所以你会对，像语言模型，像GPT模型，或者我们喜欢如何转录演讲，音频变成类似的文字，我们怎么做机器翻译，288是个好地方，所以所有这些不同的阶级，我敢肯定也有更多的弹出来。

所以有时候如果你对特别的话题感兴趣，你可以做的是查看194，都是本科生的专题，和两个94个是毕业生的专题，有时有些喜欢花哨，一次话题就会出现在那里，有时这些甚至演变成每学期提供的真正的课程。

有时它们只是一次性的供品，但如果你有兴趣，这些都是你可以去的地方，我想如果你是本科生，你可以把所有以一个开始的，如果他们从这两个开始，是研究生班，所以你可能得问问教它的人，如果他们让你进来。

但我认识一些本科生，谁进入了这些研究生班，所以如果你有兴趣，你可以告诉他们所有关于，你知道你88年的经历，也许他们会让你进去，我什么都不知道，你想知道要上的课，或者下一步该做什么，不是专家。

但我们可以为您添加循环的东西，如果你想，好的，我还有什么要给你的，我相信我还有几样东西，所以研究，自从我被雇用以来，我每个月总是收到几封电子邮件，那里的人就像，我想和你一起做研究。

先生告诉我你的实验室是什么，我就像，我不做任何研究，我只是在这里教书，所以我想你知道，祝你搜索顺利，但我不能为你做任何事，但如果你有兴趣，这里有很多人工智能研究，这是一个巨大的实验室。

他们有大量的教职员工，甚至更多的研究生，所以他们都集中的地方，据我所知是熊，我想这就是它的发音，也就是伯克利人工智能研究实验室，所以如果你有兴趣，你可以去那个网站，我想他们也列出了一堆研究生。

你可以看看他们在做什么项目，如果你有兴趣，你可以在如何接触方面接触他们，你知道教授不擅长回复电子邮件是出了名的，所以你可能要尝试几次，你可能要和不同的教授谈谈，试图占用他们的办公时间。

去骚扰他们的研究生，所以可能需要一点努力，但如果你有兴趣，你有足够的毅力，你当然可以伸出援手，他们可以让你联系起来，但如果你给我发邮件，我会把你和这个联系起来，离我而去，但我的意思是很好的，好的。

所以这就是研究其他任何东西，你想知道吗，如果你有研究问题，亚当也许也能回答，我想你比我更投入，所以你也可以伸手去那里，那是研究，所以如果你对更多面向研究的东西感兴趣，那是一条路，你可以去别的地方。

人们总是问我们这个，就像谁画的画，所以背后其实有人，我相信他们真的在这里获得了硕士学位，他们的硕士学位是关于说明概念的，就我个人而言，我喜欢这些幻灯片，我希望我能为他们邀功，但它们不是我的。

但我确实喜欢他们说明概念的方式，所以如果你有兴趣，事情就是这样发生的，如果你感兴趣，你想为我们画更多的东西，好像我总是很乐意带他们去，但最初的努力是由几年前的一个人领导的。

所以现在你知道人们总是问我们这个，好吧，其他的事情我可以告诉你关于我还可以告诉你什么，我的钱快用完了，我再告诉你几件事，让我们看看这是熊的网站，就像我们说的，这样你就可以四处走动，你知道吗，去骚扰他们。

别骚扰我，人们总是问我们的其他事情，如果你想参与，比如教书，这就是我能告诉你的事情，因为教书是我花时间做的事情，如果你有兴趣教任何人，即使是变焦，我不知道所以，如果你对教书感兴趣。

这是我可以告诉你更多的事情，所以我是问这个问题的合适人选，但就目前而言，我们仍然不确定秋季的情况，就像，我们不完全确定，我在秋天教这个班，我们也不完全确定TIA的招聘情况是什么样子的。

所以有很多不是很清楚，但如果你对教书感兴趣，你要做的第一件事就是，所以有一份部门申请，你只要填一下，然后是你感兴趣的课程，如果你对，你知道有更好的机会被雇用，因为我们每年都会收到几百份申请。

最多只能拿几十个TA，我当然建议先找其他地方教书，所以这是我们经常寻找的东西，就像经验一样，所以试着做一些低级别的人工智能，就像61 a b c，他们都收学术实习生，那就像每周一两个小时的承诺。

你可以从那里开始，然后向上努力，这就是我所做的，你不必在这里全职工作，但你可以努力成为助教，你有兴趣吗？就像CSM是你可以加入的另一件事，你可以拿CS 370，所以有很多不同的方式你可以参与教学。

即使你第一次没有被录用，而且肯定会，尝试广泛地应用，尝试大量的课程，所以当我打算做助教的时候，我刚检查了每一个盒子，包括我从未上过的课，我就像，哦，我来教这个，最后有人雇了我，所以我就是这样参与进来的。

所以如果你对教书感兴趣，这是你想去的方向，任何人，对那个有想法或问题，是啊，是啊，我第一次得到助教职位是什么时候，或者什么或者什么位置，什么时候什么事，我第一次得到一个职位，我想是大二的时候。

或者像大二的夏天，就像大三的秋天，所以在那里的某个地方，头等舱，我是CS 161，我现在教的，所以说，但这又涉及到，就像一遍又一遍地尝试，把它应用到任何地方，然后有人犯了解雇我的错误，其余的都是历史。

所以说，教学方面，你还想知道什么吗，但我们一直在寻找能回馈课堂的人，所以如果你有兴趣，你觉得和学生互动是你想尝试的事情，我们总是很乐意，好的，最后一件事是，我想我们这里没有职员。

所以我们知道斯图尔特不在，你知道从邪恶中拯救世界，AI机器人，我想我们的大多数员工都忙于自己的课程，所以我不知道他们今天有没有人在这里，所以，但我确实想承认，像这样的类不运行。

除非我们有这些出色的员工做一切，我已经说服他们所有人去员工办公室，对你来说像疯了一样的时间，他们都喜欢，你知道的，每周准备讨论，努力工作，所以如果你看到他们在校园里走来走去，或者下次你看到他们在办公室。

就像你知道的那样，你给他们大喊一声，告诉他们，谢谢他们所做的一切，因为他们真的经历了很多才使这门课成为，没事的，所以你现在不必去想他们，但下次你看到他们的时候，也许只是说一些好话。

给他们发一张漂亮的纸条，我肯定他们会非常感激的，所以我会慢慢滚动，所以你可以看到我们拥有的美妙的东西，感谢工作人员和给你打分的读者，在你的家庭作业上写鸡肉抓痕，谢谢大家，嗯，所以是的，在我们结束之前。

有什么最后的想法或问题吗，给你最后五分钟，我们放你走之前的最后一件事，我的光标在哪里，我让你去那里之前的最后一件事，没关系的，所以有课程评估出来了，你应该收到一封关于这些的电子邮件。

所以我们会在Ed上告诉你更多关于这个的信息，我也是，但就像这样真的很好，如果你能把这些填一下，他们帮助我们所有人弄清楚，你知道我们将来应该怎样改进这门课吗？他们告诉我们一些事情。

比如我们将来应该提供哪些课程，所以如果你对某些课程感兴趣，你的反馈有助于我们，它告诉我们你知道需要改进的地方，我们做得很好的事情，我们应该做更多的，所以这些实际上很重要，人们读它们。

我的工作有时取决于他们，所以当你填写它们的时候真的很有帮助，你知道不管怎样都没有压力，但你可以告诉我们你对这门课的看法，我们很高兴听到他们，我想我们会给你一点额外的学分，如果你填了，所以请继续关注，嗯。

但基本上就是这样，所以我的意思是，你看得出来，我看起来真的很像，因为它确实有一个很棒的寒假，希望你寒假过得愉快，即使那是几个月后的事，但我也希望你有一个伟大的暑假。

我也希望你在寒假前有一个很棒的秋季学期，是啊，是啊，你知道的，1。我希望一切都好，我们会在附近，如果你有任何问题，但是是的，都是为了我，你们还有什么，好的，我们期末考试见，所以我是来问问题的。

亚当在那里问问题，也可以回家。

![](img/164184d6edc2d71cf8a5a750f011e920_34.png)

![](img/164184d6edc2d71cf8a5a750f011e920_35.png)

![](img/164184d6edc2d71cf8a5a750f011e920_36.png)

![](img/164184d6edc2d71cf8a5a750f011e920_37.png)

![](img/164184d6edc2d71cf8a5a750f011e920_38.png)

![](img/164184d6edc2d71cf8a5a750f011e920_39.png)

![](img/164184d6edc2d71cf8a5a750f011e920_40.png)