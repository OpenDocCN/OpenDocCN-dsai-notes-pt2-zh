- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10774](https://ar5iv.labs.arxiv.org/html/2406.10774)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jiaming Tang    Yilong Zhao    Kan Zhu    Guangxuan Xiao    Baris Kasikci   
    Song Han
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As the demand for long-context large language models (LLMs) increases, models
    with context windows of up to 128K or 1M tokens are becoming increasingly prevalent.
    However, long-context LLM inference is challenging since the inference speed decreases
    significantly as the sequence length grows. This slowdown is primarily caused
    by loading a large KV cache during self-attention. Previous works have shown that
    a small portion of critical tokens will dominate the attention outcomes. However,
    we observe the criticality of a token highly depends on the query. To this end,
    we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track
    of the minimal and maximal Key values in KV cache pages and estimates the criticality
    of a given page using Query vectors. By only loading the Top-K critical KV cache
    pages for attention, Quest significantly speeds up self-attention without sacrificing
    accuracy. We show that Quest can achieve up to $7.03\times$ while performing well
    on tasks with long dependencies with negligible accuracy loss. Code is available
    at [https://github.com/mit-han-lab/Quest](https://github.com/mit-han-lab/Quest).
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid evolution of Large Language Models (LLMs) has shaped our daily lives.
    With the increasing demand for multi-round conversations and long document queries,
    the maximum context length of LLMs has dramatically grown from 2K to 1M (Liu et al.,
    [2024a](#bib.bib11); Peng et al., [2023](#bib.bib18); Tworkowski et al., [2023](#bib.bib23)).
    The 128k context length GPT-4 model has already been deployed in large-scale serving,
    which is equivalent to 300 pages of text (OpenAI, [2023](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: However, processing long-context requests is challenging. Due to the auto-regressive
    nature of LLMs, generating one token would require reading the entire KV cache.
    For Llama 7B model (Touvron et al., [2023](#bib.bib22)) with $32$% of the inference
    latency^*^**Tested with FP16 FlashInfer implementation on an RTX4090, limiting
    the overall throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the increasingly large size of the KV cache, previous works have shown
    that a small portion of the tokens can dominate the accuracy of token generation (Zhang
    et al., [2023b](#bib.bib28); Ge et al., [2024](#bib.bib5)). Therefore, we can
    dramatically reduce the inference latency by only loading the critical tokens,
    while still maintaining accuracy. Thus, it is essential to identify critical portions
    of the KV cache.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce9b808a46f0c7c01cc1c193c0be9b00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison between Dense Attention(a), Query-Agnostic Sparsity (b)
    and Quest’s Query-aware Sparsity (c). Quest significantly speeds up self-attention
    while maintaining high accuracy by dynamically determining the critical tokens
    based on the current query. $T$ represents the number of critical tokens for attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we further observe that the criticality of the tokens can change
    with different query tokens. As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Methodlogy
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"), the
    critical tokens vary a lot with different queries. Therefore, we need a dynamic
    and efficient approach to determine which portion of the KV cache needs to be
    attended to. To this end, we propose Quest, a query-aware criticality estimation
    algorithm for long-context LLM inference that efficiently and effectively identifies
    critical KV cache tokens and performs self-attention selectively on chosen tokens,
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the overhead of KV cache criticality estimation, Quest manages KV
    cache at page granularity (Kwon et al., [2023](#bib.bib9)). For each page, Quest
    utilizes maximum and minimum values of each feature dimension of the Key vector
    as the metadata to represent token information. During inference, Quest considers
    both the Query vector and the metadata to estimate each page’s criticality. Given
    all criticality scores of the pages, Quest chooses Top-K pages to perform approximate
    self-attention, where $K$ pages, Quest significantly accelerates inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate both the accuracy and efficiency of Quest. Since Quest dynamically
    decides the criticality of the tokens, Quest achieves better accuracy for a given
    degree of KV cache sparsity than baselines on PG19 dataset (Rae et al., [2019](#bib.bib19)),
    passkey retrieval task (Peng et al., [2023](#bib.bib18)), and LongBench (Bai et al.,
    [2023](#bib.bib2)) with $256$ inference speedup compared to FlashInfer (Ye et al.,
    [2024](#bib.bib26)) with 4-bit weight quantization. In summary, we make the following
    contribution:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An analysis of the self-attention mechanism that pinpoints the importance of
    query-aware sparsity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quest, an efficient and accurate KV cache acceleration algorithm, which exploits
    query-aware sparsity by dedicated operator designs and implementations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comprehensive evaluation of Quest, demonstrating up to $7.03\times$ end-to-end
    latency improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Long-context Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the demand for long-context models increases, many works have focused on
    extending the context window of LLMs. Currently, many models utilize Rotary Position
    Embeddings (RoPE) (Su et al., [2023](#bib.bib21)), and by different scaling methods
    of RoPE with fine-tuning, the window size of the original 4k Llama-2 has been
    expanded to 32k for LongChat (Li et al., [2023](#bib.bib10)) and 128k for Yarn-Llama-2 (Peng
    et al., [2023](#bib.bib18)). Through length extrapolation, the context windows
    of models reached beyond 1M (Liu et al., [2024b](#bib.bib12)). Beyond open-source
    models, GPT-4 Turbo supports lengths of up to 128k, while Claude-2 supports up
    to 200k (OpenAI, [2024](#bib.bib16); Anthropic, [2024](#bib.bib1)). With models
    increasingly capable of handling long input, this poses challenges for inference
    efficiency. Quest aims to boost long-context inference by exploiting query-aware
    KV cache sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 KV Cache Eviction Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For long-context LLM inference and serving scenarios, the huge size of the KV
    cache results in significant time and space overheads. Many previous efforts have
    been dedicated to compressing the size of the KV cache to accelerate attention
    and reduce memory usage. H2O (Zhang et al., [2023b](#bib.bib28)) retains a limited
    budget of the important KV cache based on the sum of historical attention scores.
    FastGen (Ge et al., [2024](#bib.bib5)) further refines the types of tokens, applying
    a more sophisticated strategy for selecting the KV cache to keep. TOVA (Oren et al.,
    [2024](#bib.bib17)) simplifies the policy by deciding which tokens to permanently
    discard based solely on the current query. StreamingLLM (Xiao et al., [2023](#bib.bib24))
    handles infinitely long texts with attention sinks and a finite KV cache. These
    methods decide which parts of the KV cache to discard based on historical information
    or current states, but discarded tokens might be important for future tokens,
    which may cause the loss of important information. To mitigate this issue, SparQ (Ribar
    et al., [2023](#bib.bib20)) computes approQximate attention scores by channel
    pruning and selects important tokens through them. However, this approach has
    not been widely validated for tasks with long dependencies, and the channel-level
    sparsity might pose challenges to practical acceleration. Therefore, we propose
    Quest, which retains all of the KV cache and selects part of the KV cache based
    on the current query to accelerate long-context self-attention without accuracy
    degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodlogy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a328c0a7f3631581b081d01a14e38185.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The attention map of prompt “A is B. C is D. A is”. Each row represents
    the attention scores of previous tokens queried by the tokens on the left. When
    queried with “D”, token “B” has a low attention score, showing “B” is not critical
    for generation. However, the “is” strongly attends to “B”. Therefore, the criticality
    of tokens strongly correlates with the current query token.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first motivate Quest by analyzing the breakdown of inference
    cost and self-attention properties. We then present the design of Quest and discuss
    its benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Long-context Inference Is Costly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM inference contains two stages, namely, the prefill stage and the decode
    stage. In the prefill stage, all the input tokens are transformed into embeddings
    and generate the Key ($K$) vectors. Both the Key and the Value vectors are saved
    in the KV cache for future use. The rest of the prefill stage includes self-attention
    and feed-forward network (FFN) layers, which produce the first response token.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24abcbe0104b1ad203d3d702a63a67e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The query aware sparsity for each layer in LongChat-7B model. We
    measure the sparsity by eliminating KV cache tokens while making sure the perplexity
    on PG19 increases less than 0.01\. For the first two layers, the sparsity is below
    10%, while for the rest of the layers, the sparsity is larger than 90%, showing
    great potential for optimization. Quest closely aligns with the oracle.'
  prefs: []
  type: TYPE_NORMAL
- en: In the decode stage, the model will take the last generated token to calculate
    its $K,Q,V$ and send to the FFN.
  prefs: []
  type: TYPE_NORMAL
- en: For one request, the prefill stage only happens once, while a decoding process
    is needed for every token in the response. Therefore, the decode stage dominates
    the inference time. For example, for $16$% of the time is spent on decode stages.
    Therefore, the decode stage performance is crucial for overall latency.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a long-context scenario significantly slows down the decode stage.
    In every decode stage, the $K$% of the time in a decode stage. Therefore, optimizing
    self-attention becomes a must for efficient long-context inference.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b321fd4230b520d0df59c740f5a87b77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Recall rate of tokens with Top-10 attention scores. Results are profiled
    with LongChat-7b-v1.5-32k model in passkey retrieval test of $10$K context length.
    Recall rate is the ratio of tokens selected by different attention methods to
    tokens selected by the full attention in each round of decoding. The average rate
    is shown in the figure, with various token budgets assigned.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e38a978e6ee647df16fce13ebd5c5980.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Quest performs self-attention in two stages. In stage 1, Quest estimates
    the criticality of pages by performing element-wise product between the current
    Query vector and both Min Key and Max Key vectors in each KV cache page. Quest
    gets the sum of the per-channel maximal value for each page as the page criticality
    estimation. In stage 2, only Top-K KV cache pages are loaded to perform sparse
    self-attention with the current Query.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Self-Attention Operation Features High Sparsity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Luckily, previous research has highlighted the inherent sparsity in self-attention (Zhang
    et al., [2023b](#bib.bib28); Ge et al., [2024](#bib.bib5)). Due to this property
    of self-attention, a small portion of tokens in the KV cache, called critical
    tokens, can accumulate sufficient attention scores, capturing the most important
    inter-token relationships. For example, as shown in Fig. [3](#S3.F3 "Figure 3
    ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference"), apart from the first two layers, less
    than 10% of the tokens are needed to achieve similar accuracy, which makes the
    attention on the rest of the tokens unnecessary. Therefore, if we can estimate
    the criticality of the tokens, we can only compute self-attention on critical
    KV cache tokens to greatly reduce the memory movement and thus improve efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Critical Tokens Depend on the Query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'However, the criticality of the tokens is dynamic and highly dependent on the
    query vector $Q$. Assuming the prompt is ”A is B. C is D. A is”, we demonstrate
    the attention map of a certain head in the 16th layer of Llama-2-7b in Fig.  [2](#S3.F2
    "Figure 2 ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity for Efficient Long-Context
    LLM Inference"). Since the output answer here should be ”B”, the token ”B” is
    critical to the current query ”is”. Thus, it has a high attention score. However,
    before the final token ”is”, ”B” is not critical for any previous query and has
    very low attention scores. In other words, the criticality of tokens is tightly
    related to the query token.'
  prefs: []
  type: TYPE_NORMAL
- en: We quantify this effect by profiling the average recall rate of tokens with
    Top-10 attention scores along the text generations. The original attention with
    full KV cache can maintain $100$ vectors for criticality estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Dynamically Estimating Token Criticality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To efficiently and accurately estimate the criticality of KV cache tokens,
    we propose Quest, an efficient and accurate algorithm that exploits query-aware
    context sparsity, which approximately selects the most potentially critical KV
    cache pages for the current query. We show the workflow of Quest in Fig. [5](#S3.F5
    "Figure 5 ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference"). To manage the overhead, Quest
    adopts PageAttention (Kwon et al., [2023](#bib.bib9)) and selects the KV cache
    pages at the granularity of pages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the criticality of the pages, Quest performs an approximate calculation
    of attention weights before the original attention operation, as shown in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.4 Dynamically Estimating Token Criticality ‣ 3 Methodlogy
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: Our insight is that in order not to miss critical tokens, we should select pages
    containing the token with the highest attention weights. However, for an efficient
    selection of pages, we should calculate an approximate attention score following
    this insight. We found that the upper bound attention weights within a page can
    be used to approximate the highest attention in the page. The upper bound of the
    attention weights can be calculated by the channel-wise minimal values ($m_{i}$
    for all tokens in this page regardless of the sign of $Q_{i}$, we get the upper
    bound of attention weights across all Key vectors on this page.
  prefs: []
  type: TYPE_NORMAL
- en: 'After deriving the upper bound attention weights, we choose the top $K$ is
    an arbitrarily defined hyper-parameter. To demonstrate the feasibility of Quest,
    we perform actual self-attention and gather Top-K per-page attention scores. As
    shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Long-context Inference Is Costly ‣ 3
    Methodlogy ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"),
    our query-aware sparsity mostly aligns with the oracle sparsity. Quest performs
    normal self-attention only on selected pages, which greatly reduces memory movement.
    We define the number of tokens in selected pages as the “Token Budget”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the low sparsity ratio for the first two layers (as shown in Fig. [3](#S3.F3
    "Figure 3 ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference")), we only apply Quest and
    all baselines on later layers to better preserve model accuracy. Note that whether
    to skip the first two layers or not is orthogonal to the KV cache selection algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Token Criticality Estimation
  prefs: []
  type: TYPE_NORMAL
- en: 'When inserting new token to KV cache:  Input: Key vector $K$     $m_{i}$ to
    $dim$  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Quest Reduces the Memory Movement of Self-Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of loading the whole KV cache, Quest only needs to load a fraction of
    the data, which leverages query-aware sparsity. Assume that every $K$ bytes. The
    whole KV cache is $2M*L$ of the total KV cache^‡^‡‡The top-K operator incurs negligible
    memory loading and execution time (5-10 us). Therefore, we do not include it in
    efficiency analysis. , which is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{1}{\text{Page Size}}+\frac{K}{\text{Page Num}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Assuming that we use $16$. Note that this memory load reduction is universal
    across all models and is compatible with existing quantization mechanisms (Zhao
    et al., [2024](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Method / Budget | 32 | 64 | 128 | 256 | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| H2O | 0% | 1% | 1% | 1% | 3% |'
  prefs: []
  type: TYPE_TB
- en: '| TOVA | 0% | 1% | 1% | 3% | 8% |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | 1% | 1% | 1% | 3% | 5% |'
  prefs: []
  type: TYPE_TB
- en: '| Quest (ours) | 65% | 99% | 99% | 99% | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| Method / Budget | 256 | 512 | 1024 | 2048 | 4096 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| H2O | 2% | 2% | 2% | 2% | 4% |'
  prefs: []
  type: TYPE_TB
- en: '| TOVA | 2% | 2% | 2% | 2% | 10% |'
  prefs: []
  type: TYPE_TB
- en: '| StreamingLLM | 1% | 1% | 1% | 2% | 4% |'
  prefs: []
  type: TYPE_TB
- en: '| Quest (ours) | 88% | 92% | 96% | 100% | 100% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: (i) Results of 10k length passkey retrieval test on LongChat-7b-v1.5-32k.
    (ii) Results of 100k length passkey retrieval test on Yarn-Llama-2-7b-128k. Quest
    can achieve nearly perfect accuracy with 64 and 1024 tokens KV cache budget, which
    is about 1% of the total sequence length, demonstrating that Quest can effectively
    preserve the model’s ability to handle long-dependency tasks. However, KV cache
    eviction algorithms such as H2O, TOVA, and StreamingLLM incorrectly discard the
    KV cache of the answer before receiving the question, thus failing to achieve
    ideal accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate Quest on the language modeling dataset PG19 (Rae et al., [2019](#bib.bib19)),
    passkey retrieval task (Peng et al., [2023](#bib.bib18)), and six datasets in
    LongBench (Bai et al., [2023](#bib.bib2)): NarrativeQA (Kočiský et al., [2018](#bib.bib8)),
    HotpotQA (Yang et al., [2018](#bib.bib25)), Qasper (Dasigi et al., [2021](#bib.bib4)),
    TrivialQA (Joshi et al., [2017](#bib.bib7)), GovReport (Huang et al., [2021](#bib.bib6)),
    MultifieldQA (Bai et al., [2023](#bib.bib2)). We choose two widely used long-context
    models for our evaluation: LongChat-v1.5-7b-32k (Li et al., [2023](#bib.bib10))
    and Yarn-Llama-2-7b-128k (Peng et al., [2023](#bib.bib18)). We compare our method
    against the KV cache eviction algorithm H2O (Zhang et al., [2023b](#bib.bib28)),
    TOVA (Oren et al., [2024](#bib.bib17)), and StreamingLLM (Xiao et al., [2023](#bib.bib24)).
    Note that we do not apply any Quest and other baseline algorithms to the first
    two layers of the model, as our analysis in Sec [3.4](#S3.SS4 "3.4 Dynamically
    Estimating Token Criticality ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity for
    Efficient Long-Context LLM Inference") indicates a low sparsity ratio for these
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Accuracy Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Language Modeling on PG19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first evaluate the language modeling perplexity on the PG19 test set, which
    is a dataset comprising 100 books with an average length of 70k tokens. We use
    the LongChat-7b-v1.5-32k model to test 32k tokens on PG19\. We feed the model
    with various numbers of tokens and evaluate the perplexity of generated tokens.
    We evaluate H2O, TOVA, and Quest with a token budget of 4096, which is approximately
    1/8 of the total token length. As indicated by the perplexity results in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.2.2 Results on long text passkey retrieval task ‣ 4.2 Accuracy Evaluation
    ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"),
    Quest’s accuracy closely matches the oracle baseline with a full KV cache.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Results on long text passkey retrieval task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since language modeling evaluation only involves local dependencies, models
    can achieve great performance by focusing on recent tokens. However, the ability
    to handle long-distance dependencies is crucial for long text reasoning. For KV
    cache eviction algorithms like H2O and TOVA, parts of KV caches that are important
    for distant future tokens may be discarded, thereby preventing the model from
    obtaining the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09ad0a3c2c4aad13fcb1ea8f4937e24c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Language modeling evaluation of Quest on PG19 dataset. We prompt
    the model with 0 to 32000 tokens from the PG19 test set and measure the perplexity
    of output tokens. H2O* and TOVA* indicate that for the first two layers of models,
    we do not apply these two algorithms to prune the KV Cache, as analyzed in Sec [3.4](#S3.SS4
    "3.4 Dynamically Estimating Token Criticality ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference"), which better preserves the
    model performance. Quest also uses a full cache in the first two layers of the
    model. Quest can closely match the performance of the full cache model.'
  prefs: []
  type: TYPE_NORMAL
- en: To show that Quest helps maintain the ability of models to handle longer dependency
    tasks, we evaluate it on the passkey retrieval task from Yarn (Peng et al., [2023](#bib.bib18)).
    This task measures a model’s ability to retrieve a simple passkey from a large
    amount of meaningless text. We put the answer in different depth ratios of the
    text and evaluate if the model can retrieve the correct answer with different
    KV cache token budgets. We evaluate LongChat-7b-v1.5-32k on 10k tokens test and
    Yarn-Llama-2-7b-128k on 100k tokens test.
  prefs: []
  type: TYPE_NORMAL
- en: Since H2O (Zhang et al., [2023b](#bib.bib28)) needs to calculate historical
    attention scores for KV cache pruning, it needs to compute the complete $O(n^{2})$
    attention map and thus is unable to use Flash-Attention (Dao et al., [2022](#bib.bib3))
    for long-context inference. Therefore, to enable H2O on long-context evaluation,
    we use Flash-Attention in the context stage for the 100k sequence length passkey
    retrieval test and start collecting historical attention scores for H2O in the
    decoding stage. For TOVA (Oren et al., [2024](#bib.bib17)) and StreamingLLM (Xiao
    et al., [2023](#bib.bib24)), we evaluated them on the 10k and 100k sequence lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the passkey retrieval test, we directly prefill the input text containing
    the passkey and texts to the model. However, to evaluate the impact of different
    methods on the model’s ability to handle long-dependency tasks in practical scenarios,
    we simulate decoding by feeding the task’s question and instruction to the model
    token by token. In this case, H2O and TOVA might mistakenly discard tokens critical
    for future tokens, such as the passkey that will be queried later. Similarly,
    StreamingLLM can only focus on the most recent text window, and if the passkey
    appears outside this window, it cannot provide the correct answer. Therefore,
    H2O, TOVA, and StreamingLLM cannot achieve ideal accuracy on the 10k and 100k
    length passkey retrieve test. However, Quest does not discard KV cache but instead
    uses a query-aware approach to identify critical tokens. As shown in Tab. [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for Efficient Long-Context
    LLM Inference"), Quest can achieve perfect accuracy with a minimal budget both
    on 10k and 100k sequence length tests.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Results on LongBench
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To validate that Quest can outperform baselines on general long-context datasets,
    we evaluate our method and baselines on six datasets in LongBench. We evaluate
    on LongChat-7b-v1.5-32k across a wide range of long-context datasets, including
    single-document QA: NarrativeQA, Qasper, MultiFieldQA; multi-document QA: HotpotQA;
    summarization: GovReport; few-shot learning: TriviaQA. We evaluate H2O, TOVA,
    StreamingLLM, and Quest with different KV cache budgets. For all datasets, we
    split the input into material and question/instruction. For the material part,
    we use Flash-Attention (Dao et al., [2022](#bib.bib3)) with the full KV cache
    to perform inference. For the question part, we simulate decoding by feeding them
    to the model token by token. Similar to the passkey retrieval test, to enable
    H2O to use Flash-Attention, we could not collect H2O’s historical attention scores
    during the context stage, thus starting from the decoding stage.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the Fig. LABEL:fig:longbench, Quest consistently outperforms all
    baselines across six long-context datasets with various KV cache budgets. Quest
    with a budget of $1$K tokens can achieve comparable performance as the model with
    full KV cache, while other baselines still exhibit a notable gap from full cache
    performance even with a larger budget. After considering the full cache used in
    the first two layers, Quest can achieve lossless performance on Qasper, HotpotQA,
    GovReport, TriviaQA, NarrativeQA, and MultifieldQA with KV cache sparsity of 1/6,
    1/6, 1/5, 1/10, 1/5, and 1/6, respectively. This demonstrates that Quest is capable
    of maintaining the model’s capabilities across different types of long-context
    tasks, as it does not lead to the generation of incorrect answers due to improper
    discarding of KV cache.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Efficiency evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the feasibility of Quest, we implement the entire framework
    with dedicated CUDA kernels based on FlashInfer (Ye et al., [2024](#bib.bib26)),
    a kernel library for LLM inference. We first evaluate Quest’s kernel-level efficiency
    under the configuration of Llama2-7B on an RTX4090 with CUDA 12.2 in Sec [4.3.1](#S4.SS3.SSS1
    "4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference"). Besides, we show
    the end-to-end speedup of Quest in text generation as shown in Sec [4.3.2](#S4.SS3.SSS2
    "4.3.2 End-to-End Evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference"). We compare Quest
    with a normal attention implementation from the original FlashInfer. To demonstrate
    the improvement, we qualitatively compare efficiency under the same accuracy between
    Quest and baselines in Sec [4.3.3](#S4.SS3.SSS3 "4.3.3 Comparison with Baselines
    ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for
    Efficient Long-Context LLM Inference"). Note that we use an Ada 6000 GPU (NVIDIA,
    [2023](#bib.bib13)) in end-to-end evaluations for longer context length.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Kernel evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the memory-bound nature of LLM inference, the speedup of Quest is proportional
    to the sparsity ratio (which is equivalent to memory movement reduction). We quantify
    this effect in Fig. LABEL:fig:kernel_efficiency, which evaluates per-kernel performance
    with NVIDIA’s benchmark tool NVBench (NVIDIA, [2024](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: Criticality estimation We evaluate the latency of criticality estimation in
    Quest under different sequence lengths and page sizes. At short sequence length,
    the memory bandwidth utilization of estimation is smaller than that of FlashInfer,
    as the total memory load size is not enough to fully utilize GPU memory bandwidth.
    As sequence length grows, the relative performance improves and approaches $1/\text{Page
    Size}$ since estimation only consumes one token per page. Note that techniques
    like quantization or larger page size can further reduce the additional memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64e8843a4c6da3c23f5ec7b548d92439.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Quest self-attention time breakdown compared to FlashInfer. At all
    sequence lengths, Quest significantly outperforms FlashInfer, as the memory movement
    is reduced. At sequence length $32$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88776e32788ba03b740beb1b3713d29f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: End-to-end latency of Quest. For all sequence lengths, Quest significantly
    outperforms FlashInfer. Increasing the sequence lengths only slightly changes
    the latency of Quest. At a given sequence length, Quest’s latency slightly increases
    as the token budget grows. With sequence length $32$.'
  prefs: []
  type: TYPE_NORMAL
- en: Top-K filtering We enable the Top-K filtering in Quest with a batched Top-K
    CUDA operator from a vector search kernel library RAFT (Zhang et al., [2023a](#bib.bib27)).
    We test the latency of Top-K filtering under different sequence lengths and token
    budgets. Since Criticality estimation reduces one entire token into one criticality
    score, Top-K filtering has limited memory movement compared to other operators,
    thus having a low latency overhead of 5-10 us for sequence length less than $128$k.
  prefs: []
  type: TYPE_NORMAL
- en: Approximate attention Since Quest is compatible with PageAttention, approximate
    attention can be easily implemented by feeding Top-K page indices as sparse loading
    indices. We compare Quest’s approximate attention with the original attention
    of FlashInfer under different sequence lengths and token budgets with a $16$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We further evaluate Quest’s attention mechanism, which combines Criticality
    estimation,Top-K filtering, and Approximate attention, on the Llama2-7B model
    using the PyTorch profiler. We show the time breakdown of Quest in Fig. [12](#S4.F12
    "Figure 12 ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference") on various
    sequence lengths. Quest reduce the self-attention time by $7.03\times$ token budget.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/db64526567bf2e11f0bb8ead0a9a6a3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Efficiency comparison of Quest with baselines under the same accuracy
    constraint. (a) Tokens budgets needed for comparable accuracy by different attention
    methods. Full denotes the original attention, which means the average context
    length of benchmarks. (b) Inference latency of different attention methods for
    comparable accuracy. Quest boosts $3.82\times$ speed on GovReport compared to
    TOVA.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 End-to-End Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To show the practical speedup of Quest, we deploy the framework into real-world
    single-batch scenarios. We measure the average latency of generating one token
    in the decode stage under different sequence lengths and token budgets. Note that
    we do not measure the sampling process since its execution time is smaller and
    depends on the setting. We compare Quest with a full KV cache baseline which is
    implemented by FlashInfer. As shown in Fig. [13](#S4.F13 "Figure 13 ‣ 4.3.1 Kernel
    evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference"), Quest outperforms FlashInfer at all
    sequence lengths. The latency of Quest grows significantly slower than FlashInfer
    when the sequence length increases, as Quest maintains similar token budgets.
    At sequence length $32$ with 4-bit quantized weight.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Comparison with Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To demonstrate the performance improvements of Quest, we compare the inference
    efficiency of different attention mechanisms under the same accuracy constraint,
    i.e. lossless accuracy of six tasks from LongBench. We show token budgets needed
    for the lossless accuracy target by different attention mechanisms in Fig [14](#S4.F14
    "Figure 14 ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference")(a). For
    example, NarrativeQA exhibits an average context length of $24$K tokens leading
    to much higher sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, none of the baselines included a kernel implementation of their proposed
    method. Consequently, we conduct a qualitative analysis of the baselines’ self-attention
    efficiency by utilizing the inference latency of FlashInfer, disregarding other
    runtime overheads (e.g., TOVA’s requirement to calculate history scores (Oren
    et al., [2024](#bib.bib17))). In contrast, Quest is evaluated in a practical setting
    with consideration of all operators. As shown in Fig. [14](#S4.F14 "Figure 14
    ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference")(b), Quest significantly
    surpasses all baselines in terms of self-attention latency due to the high query-aware
    sparsity. For GovReport and TriviaQA, Quest boosts the inference by $3.82\times$,
    respectively. Therefore, Quest can achieve higher efficiency while maintaining
    superior accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present Quest, an efficient and accurate KV cache selection algorithm that
    exploits query-aware sparsity. Quest dynamically estimates the criticality of
    tokens in KV cache based on the per-page metadata and the current query. It then
    performs self-attention only on the critical tokens with greatly reduced memory
    movement, providing high sparsity with negligible accuracy loss. Comprehensive
    evaluations demonstrate that Quest provides up to $7.03\times$ self-attention
    latency with the same accuracy target under long-context benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Zihao Ye for his insightful discussion, feedback, and useful advice
    on algorithm design and FlashInfer integration. This work was supported by generous
    gifts from Intel, Google, and the PRISM Research Center, a JUMP Center cosponsored
    by SRC and DARPA.
  prefs: []
  type: TYPE_NORMAL
- en: Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic (2024) Anthropic. Introducing the next generation of Claude. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family),
    2024. [Accessed 28-05-2024].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,
    Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench:
    A bilingual, multitask benchmark for long context understanding, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. Flashattention:
    Fast and memory-efficient exact attention with io-awareness, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasigi et al. (2021) Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A.,
    and Gardner, M. A dataset of information-seeking questions and answers anchored
    in research papers, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. (2024) Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.
    Model tells you what to discard: Adaptive kv cache compression for llms, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2021) Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L.
    Efficient attentions for long document summarization. In *Proceedings of the 2021
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pp.  1419–1436, Online, June 2021\.
    Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112.
    URL [https://aclanthology.org/2021.naacl-main.112](https://aclanthology.org/2021.naacl-main.112).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2017) Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA:
    A large scale distantly supervised challenge dataset for reading comprehension.
    In Barzilay, R. and Kan, M.-Y. (eds.), *Proceedings of the 55th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 
    1601–1611, Vancouver, Canada, July 2017\. Association for Computational Linguistics.
    doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kočiský et al. (2018) Kočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,
    K. M., Melis, G., and Grefenstette, E. The NarrativeQA reading comprehension challenge.
    *Transactions of the Association for Computational Linguistics*, 6:317–328, 2018.
    doi: 10.1162/tacl˙a˙00023. URL [https://aclanthology.org/Q18-1023](https://aclanthology.org/Q18-1023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H.,
    Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large
    language model serving with pagedattention. In *Proceedings of the ACM SIGOPS
    29th Symposium on Operating Systems Principles*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise
    on context length?, June 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024a) Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model
    on million-length video and language with blockwise ringattention, 2024a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024b) Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
    Scaling laws of rope-based extrapolation, 2024b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2023) NVIDIA. Nvidia ada lovelace professional gpu architecture. [https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf),
    2023. [Accessed 28-05-2024].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA (2024) NVIDIA. Nvbench: Nvidia’s benchmarking tool for gpus, 2024. Available
    online: [https://github.com/NVIDIA/nvbench](https://github.com/NVIDIA/nvbench).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2023) OpenAI. New models and developer products announced at devday.
    [https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI](https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI),
    November 2023. Accessed: 2024-01-31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2024) OpenAI. Introducing gpt-4o: our fastest and most affordable flagship
    model. [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models),
    2024. [Accessed 28-05-2024].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oren et al. (2024) Oren, M., Hassid, M., Adi, Y., and Schwartz, R. Transformers
    are multi-state RNNs, 2024. URL [https://arxiv.org/abs/2401.06104](https://arxiv.org/abs/2401.06104).
    arXiv:2401.06104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
    and Lillicrap, T. P. Compressive transformers for long-range sequence modelling.
    *arXiv preprint*, 2019. URL [https://arxiv.org/abs/1911.05507](https://arxiv.org/abs/1911.05507).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribar et al. (2023) Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C.,
    Luschi, C., and Orr, D. Sparq attention: Bandwidth-efficient llm inference, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2023) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
    Roformer: Enhanced transformer with rotary position embedding, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
    Michalewski, H., and Miłoś, P. Focused transformer: Contrastive training for context
    scaling, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient
    streaming language models with attention sinks. *arXiv*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2024) Ye, Z., Lai, R., Lu, R., Lin, C.-Y., Zheng, S., Chen, L.,
    Chen, T., and Ceze, L. Cascade inference: Memory bandwidth efficient shared prefix
    batch decoding. [https://flashinfer.ai/2024/01/08/cascade-inference.html](https://flashinfer.ai/2024/01/08/cascade-inference.html),
    Jan 2024. URL [https://flashinfer.ai/2024/01/08/cascade-inference.html](https://flashinfer.ai/2024/01/08/cascade-inference.html).
    Accessed on 2024-02-01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Zhang, J., Naruse, A., Li, X., and Wang, Y. Parallel top-k
    algorithms on gpu: A comprehensive study and new methods. In *Proceedings of the
    International Conference for High Performance Computing, Networking, Storage and
    Analysis*, SC ’23, New York, NY, USA, 2023a. Association for Computing Machinery.
    ISBN 9798400701092. doi: 10.1145/3581784.3607062. URL [https://doi.org/10.1145/3581784.3607062](https://doi.org/10.1145/3581784.3607062).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., Wang, Z., and Chen, B. H[2]o: Heavy-hitter
    oracle for efficient generative inference of large language models, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2024) Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng,
    S., Ceze, L., Krishnamurthy, A., Chen, T., and Kasikci, B. Atom: Low-bit quantization
    for efficient and accurate llm serving, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
