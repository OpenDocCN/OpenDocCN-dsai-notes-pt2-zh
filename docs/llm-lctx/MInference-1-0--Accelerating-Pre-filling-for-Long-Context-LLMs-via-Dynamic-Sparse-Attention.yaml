- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:56:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:56:08'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MInference 1.0: 通过动态稀疏注意力加速长上下文 LLM 的预填充'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
- en: Huiqiang Jiang¹¹1Equal contribution. ^◆Work during internship at Microsoft.,
    Yucheng Li^◆¹¹1Equal contribution. ^◆Work during internship at Microsoft., Chengruidong
    Zhang¹¹1Equal contribution. ^◆Work during internship at Microsoft., Qianhui Wu,
    Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Huiqiang Jiang¹¹1平等贡献。 ^◆在微软实习期间的工作。 Yucheng Li^◆¹¹1平等贡献。 ^◆在微软实习期间的工作。 Chengruidong
    Zhang¹¹1平等贡献。 ^◆在微软实习期间的工作。 Qianhui Wu, Xufang Luo,
- en: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
- en: Microsoft Corporation, ^◆University of Surrey {hjiang,chengzhang,yuqyang}@microsoft.com,yucheng.li@surrey.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司，^◆萨里大学 {hjiang,chengzhang,yuqyang}@microsoft.com, yucheng.li@surrey.ac.uk
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The computational challenges of Large Language Model (LLM) inference remain
    a significant barrier to their widespread deployment, especially as prompt lengths
    continue to increase. Due to the quadratic complexity of the attention computation,
    it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the
    pre-filling stage) on a single A100 GPU. Existing methods for speeding up pre-filling
    often fail to maintain acceptable accuracy or efficiency when applied to long-context
    LLMs. To address this gap, we introduce MInference (Million-tokens Inference),
    a sparse calculation method designed to accelerate pre-filling of long-sequence
    processing. Specifically, we identify three unique patterns in long-context attention
    matrices—the A-shape, Vertical-Slash, and Block-Sparse—that can be leveraged for
    efficient sparse computation on GPUs. We determine the optimal pattern for each
    attention head offline and dynamically build sparse indices based on the assigned
    pattern during inference. With the pattern and sparse indices, we perform efficient
    sparse attention calculations via our optimized GPU kernels to significantly reduce
    the latency in the pre-filling stage of long-context LLMs. Our proposed technique
    can be directly applied to existing LLMs without any modifications to the pre-training
    setup or additional fine-tuning. By evaluating on a wide range of downstream tasks,
    including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including
    LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that
    MInference effectively reduces inference latency by up to $10\times$ for pre-filling
    on an A100, while maintaining accuracy. Our code is available at [https://aka.ms/MInference](https://aka.ms/MInference).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）推理的计算挑战仍然是其广泛部署的一个重要障碍，尤其是当提示长度持续增加时。由于注意力计算的二次复杂度，一个 8B 的 LLM 在单个
    A100 GPU 上处理 1M 令牌的提示（即预填充阶段）需要 30 分钟。现有的加速预填充的方法在应用于长上下文 LLM 时，往往无法保持可接受的准确性或效率。为了解决这一问题，我们提出了
    MInference（百万令牌推理），这是一种稀疏计算方法，旨在加速长序列处理的预填充。具体而言，我们识别了长上下文注意力矩阵中的三种独特模式——A形、竖直斜杠和块稀疏——这些模式可以用于在
    GPU 上进行高效的稀疏计算。我们离线确定每个注意力头的最佳模式，并在推理过程中根据分配的模式动态构建稀疏索引。通过模式和稀疏索引，我们通过优化的 GPU
    内核执行高效的稀疏注意力计算，从而显著减少长上下文 LLM 预填充阶段的延迟。我们提出的技术可以直接应用于现有的 LLM，而无需对预训练设置进行任何修改或额外的微调。通过在包括
    InfiniteBench、RULER、PG-19 和大海捞针在内的广泛下游任务以及 LLaMA-3-1M、GLM-4-1M、Yi-200K、Phi-3-128K
    和 Qwen2-128K 等模型上进行评估，我们展示了 MInference 能够有效地将 A100 上的预填充推理延迟减少高达 $10\times$，同时保持准确性。我们的代码可以在
    [https://aka.ms/MInference](https://aka.ms/MInference) 获得。
- en: \doparttoc\faketableofcontents![Refer to caption](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents![请参见说明](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
- en: (a) Needle In A Haystack
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 大海捞针
- en: '![Refer to caption](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
- en: (b) Latency Speedup
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 延迟加速
- en: 'Figure 1: Attention weights, especially in long-context LLMs, exhibit up to
    96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic
    sparse attention to accelerate the pre-filling stage of long-context LLM inference.
    It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b),
    and matches or surpasses baselines, as demonstrated by Needle In A Haystack [[35](#bib.bib35)]
    in (a) on LLaMA-3-8B-1M [[24](#bib.bib24)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：注意力权重，尤其是在长上下文 LLMs 中，在 128K 上下文中表现出高达 96.8% 的稀疏性。我们提出了 MInference，利用动态稀疏注意力来加速长上下文
    LLM 推理的预填充阶段。正如图 (b) 所示，在单个 A100 上对于 1M 上下文实现了高达 10 倍的加速，并且与基准相匹配或超越，正如图 (a) 中
    Needle In A Haystack [[35](#bib.bib35)] 在 LLaMA-3-8B-1M [[24](#bib.bib24)] 上所示。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs) have entered the era of long-context processing,
    with some of them supporting context windows ranging from 128K to 10M tokens [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)].
    These extended context windows enable LLMs to unlock a multitude of complex real-world
    applications, such as repository-level code understanding [[7](#bib.bib7), [34](#bib.bib34),
    [58](#bib.bib58)], long-document question-answering [[9](#bib.bib9), [51](#bib.bib51)],
    extreme-label in-context learning [[51](#bib.bib51)], and long-horizon agent tasks [[79](#bib.bib79)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已进入长上下文处理时代，其中一些支持 128K 到 10M 标记的上下文窗口 [[24](#bib.bib24), [67](#bib.bib67),
    [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)]。这些扩展的上下文窗口使
    LLMs 能够解锁众多复杂的实际应用，如代码库级别的理解 [[7](#bib.bib7), [34](#bib.bib34), [58](#bib.bib58)]、长文档问答
    [[9](#bib.bib9), [51](#bib.bib51)]、极端标签的上下文学习 [[51](#bib.bib51)] 和长时间跨度的代理任务 [[79](#bib.bib79)]。
- en: 'However, due to the quadratic complexity of attention, it can take several
    minutes for the model to process the input prompt (i.e., the pre-filling stage)
    and then start to produce the first token, which leads to unacceptable Time To
    First Token experience, thus greatly hinders the wide application of long-context
    LLMs. As shown in Fig. [2a](#S2.F2.sf1 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), when serving LLaMA-3-8B on a single A100
    machine, the model would keep users waiting for 6 minutes to finish the pre-filling
    stage given a prompt of 300K tokens, and this number increases to 30 minutes for
    a prompt of 1M tokens. The overhead of self-attention computation exceeds 90%
    of the total pre-filling latency, which makes it the major bottleneck in long-context
    processing of LLMs. Previous research has shown that the attention matrices are
    highly sparse [[47](#bib.bib47), [17](#bib.bib17)], which has led to the development
    of fixed sparse attention methods such as Longformer [[6](#bib.bib6)] and BigBird [[87](#bib.bib87)].
    However, prior studies have also noted that attention distributions vary significantly
    across different inputs [[39](#bib.bib39), [47](#bib.bib47)]. This dynamic nature
    prevents prior sparse methods from being used directly on long-context LLMs without
    expensive training or fine-tuning. But if the dynamic sparse attention patterns
    could be efficiently predicted online, the pre-filling latency of long-context
    LLMs could be significantly reduced by calculating only the most important part
    of the attention weights.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于注意力机制的二次复杂度，模型处理输入提示（即预填充阶段）可能需要几分钟，然后才开始生成第一个标记，这导致了不可接受的首次标记时间体验，从而大大阻碍了长上下文
    LLMs 的广泛应用。如图 [2a](#S2.F2.sf1 "在图 2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLMs 的预填充")所示，当在单个 A100 机器上服务 LLaMA-3-8B 时，模型在处理 300K 标记的提示时需要让用户等待 6 分钟完成预填充阶段，对于
    1M 标记的提示，这个时间增加到 30 分钟。自注意力计算的开销超过了总预填充延迟的 90%，这使得它成为 LLMs 长上下文处理的主要瓶颈。以往研究表明，注意力矩阵高度稀疏
    [[47](#bib.bib47), [17](#bib.bib17)]，这导致了固定稀疏注意力方法的开发，如 Longformer [[6](#bib.bib6)]
    和 BigBird [[87](#bib.bib87)]。然而，先前的研究也指出，注意力分布在不同输入间差异显著 [[39](#bib.bib39), [47](#bib.bib47)]。这种动态特性阻止了之前的稀疏方法在长上下文
    LLMs 上的直接应用而无需昂贵的训练或微调。但如果能够有效地在线预测动态稀疏注意力模式，则通过仅计算最重要的注意力权重部分，可以显著减少长上下文 LLMs
    的预填充延迟。
- en: 'Building upon this idea, we present MInference, a technique that reduces 95%
    of FLOPs in the attention computation to significantly accelerate the pre-filling
    stage of long-context LLM inference via dynamic sparse attention. Unlike existing
    dynamic sparse attention methods that introduce large computational overhead to
    estimate attention patterns with low-rank hidden dimensions [[47](#bib.bib47),
    [63](#bib.bib63)], our method is designed specifically for long-context scenarios
    with minimal overhead in estimation. Specifically, we conduct extensive analysis
    and identify three general patterns of sparse attention in long-context LLMs:
    A-shape pattern, Vertical-Slash pattern, and Block-Sparse pattern. Based on these
    findings, we introduce a kernel-aware search method to assign the optimal attention
    pattern for each head. Importantly, instead of fixed attention masks in prior
    studies, we perform an efficient online approximation to build a dynamic sparse
    mask for each head according to their assigned pattern and particular inputs.
    For example, to build a dynamic sparse mask for a specific prompt on one Vertical-Slash
    head, we use a partial of attention weight consisting of the last last_q query
    and key vectors (i.e. $\bm{Q}_{[-\text{last\_q}:]}$) to estimate the most important
    indices of the vertical and slash lines globally on the attention matrix. For
    Block-Sparse heads, we perform mean pooling on both query and key vectors in blocks
    of 64 and calculate the block-level attention weights to determine the most important
    blocks and thereby obtain a block-sparse dynamic mask. After obtaining the dynamic
    sparse mask, three optimized GPU kernels are used, which we developed for the
    above three sparse patterns. These kernels are based on the dynamic sparse compilers
    PIT [[88](#bib.bib88)], Triton [[75](#bib.bib75)] and FlashAttention [[13](#bib.bib13)],
    which enable extremely efficient computation of dynamic sparse attention.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们提出了MInference，这是一种通过动态稀疏注意力技术减少95% FLOPs的注意力计算，以显著加速长上下文LLM推理的预填充阶段。与现有的动态稀疏注意力方法相比，这些方法引入了大量计算开销来估计低秩隐藏维度的注意力模式 [[47](#bib.bib47),
    [63](#bib.bib63)]，我们的方法专为长上下文场景设计，估计开销最小。具体而言，我们进行了广泛的分析，并识别出长上下文LLM中的三种一般稀疏注意力模式：A形模式、垂直斜线模式和块稀疏模式。基于这些发现，我们引入了一种内核感知搜索方法，为每个头分配最佳注意力模式。重要的是，与之前研究中的固定注意力掩码不同，我们通过高效的在线近似为每个头构建一个动态稀疏掩码，根据其分配的模式和特定输入进行调整。例如，为特定提示在一个垂直斜线头上构建动态稀疏掩码时，我们使用由最后一个查询和键向量组成的部分注意力权重（即$\bm{Q}_{[-\text{last\_q}:]}$）来估计注意力矩阵中垂直和斜线的最重要索引。对于块稀疏头，我们对查询和键向量进行64块的均值池化，并计算块级注意力权重，以确定最重要的块，从而获得块稀疏动态掩码。获得动态稀疏掩码后，使用三种优化的GPU内核，这些内核是我们为上述三种稀疏模式开发的。这些内核基于动态稀疏编译器PIT [[88](#bib.bib88)]、Triton [[75](#bib.bib75)]和FlashAttention [[13](#bib.bib13)]，实现了动态稀疏注意力的极高效计算。
- en: Extensive experiments are conducted on various Long-context LLMs, including
    LLaMA-3-8B-1M [[24](#bib.bib24)], GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)],
    across benchmarks with context lengths over 1M tokens, such as InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], Needle In A Haystack [[35](#bib.bib35)], and PG-19 [[65](#bib.bib65)].
    Needle In A Haystack was also tested on Phi-3-Mini-128K [[2](#bib.bib2)] and Qwen-2-7B-128K [[5](#bib.bib5)].
    Results show that MInference speeds up the pre-filling stage by up to $10\times$
    for 1M contexts with LLaMA-3-8B on a single A100, reducing latency from 30 minutes
    to 3 minutes per prompt, while maintaining or improving accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对各种长上下文LLM进行大量实验，包括LLaMA-3-8B-1M [[24](#bib.bib24)]、GLM-4-9B-1M [[26](#bib.bib26)]和Yi-9B-200K [[84](#bib.bib84)]，在上下文长度超过1M的基准测试中，如InfiniteBench [[86](#bib.bib86)]、RULER [[28](#bib.bib28)]、Needle
    In A Haystack [[35](#bib.bib35)]和PG-19 [[65](#bib.bib65)]。Needle In A Haystack还在Phi-3-Mini-128K [[2](#bib.bib2)]和Qwen-2-7B-128K [[5](#bib.bib5)]上进行了测试。结果显示，MInference使得1M上下文的预填充阶段加速高达$10\times$，在单个A100上使用LLaMA-3-8B，将延迟从每个提示30分钟减少到3分钟，同时保持或提高准确性。
- en: '2 Attention Heads: Dynamic, Sparse, and Characteristic'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力头：动态的、稀疏的和特征化的
- en: '![Refer to caption](img/10ccd929778db2d7ea98214ca2a9092c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10ccd929778db2d7ea98214ca2a9092c.png)'
- en: (a) Attention incurs heavy cost.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力开销很大。
- en: '![Refer to caption](img/84a510695ce9529c4982da1844ae2d16.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84a510695ce9529c4982da1844ae2d16.png)'
- en: (b) Attention is sparse.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是稀疏的。
- en: '![Refer to caption](img/804139198ed227c4ddf295cd89c1f3cd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/804139198ed227c4ddf295cd89c1f3cd.png)'
- en: (c) Sparsity of attention is dynamic.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力的稀疏性是动态的。
- en: 'Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention
    scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention
    scores are retrieved when reusing the top-k indices from another examples, indicating
    its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：（a）预填充阶段的延迟分解。（b）在128k上下文中，前k（k=4096）列可以覆盖多少注意力分数。（c）从另一个示例重用前k索引时，检索到的注意力分数较少，显示了其动态特性。可视化基于单个A100的LLaMa-3-8B。
- en: 2.1 Attention is Dynamically Sparse
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 注意力是动态稀疏的
- en: 'The sparsity of attention weights in pre-trained LLMs, especially in long-context
    scenarios, has been well-documented [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]. As shown in Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ 2 Attention
    Heads: Dynamic, Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), for an attention matrix
    of size $128k\times 128k$, retaining only the top 4k columns recalls 96.8% of
    the total attention. In other words, each token is attending to a limit number
    of tokens despite the long sequence it is processing.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练LLMs中的注意力权重稀疏性，特别是在长上下文场景中，已被充分记录 [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]。如图 [2b](#S2.F2.sf2 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")所示，对于大小为$128k\times
    128k$的注意力矩阵，仅保留前4k列即可回忆96.8%的总注意力。换句话说，尽管处理的是长序列，每个令牌只关注有限数量的令牌。
- en: 'On the other hand, although the sparse nature of attention matrices is shared
    across different inputs, the exact distributions of sparse pattern are highly
    dynamic. That is to say, a token at a given position only attends to a subset
    of the sequence in self-attention, and the exact tokens it attends to are highly
    context-dependent and vary significantly across different prompts. This dynamism
    has been mathematically demonstrated in prior studies [[39](#bib.bib39), [40](#bib.bib40)].
    As depicted in Fig. [2c](#S2.F2.sf3 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), if we take the top 4k columns found in Fig. [2b](#S2.F2.sf2
    "In Figure 2 ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and apply it on another prompt of 128k, the recall of attention would drop largely
    to 83.7%.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管不同输入间注意力矩阵的稀疏特性是一致的，但稀疏模式的具体分布是高度动态的。也就是说，给定位置的令牌在自注意力中仅关注序列的一个子集，而它关注的具体令牌高度依赖于上下文，并且在不同提示下变化显著。这种动态性在先前的研究中已被数学证明 [[39](#bib.bib39),
    [40](#bib.bib40)]。如图 [2c](#S2.F2.sf3 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")所示，如果我们取图 [2b](#S2.F2.sf2
    "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中的前4k列并应用于另一个128k的提示，注意力的回忆将大幅下降至83.7%。
- en: 2.2 Attention Sparsity Exhibits Patterns
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 注意力稀疏性展现模式
- en: 'Table 1: Comparison of different sparse patterns.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同稀疏模式的比较。
- en: '| Patterns | A-shape | Vertical-Slash | Block-Sparse | Top-K |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | A形 | 竖直斜杠 | 块稀疏 | Top-K |'
- en: '| Spatial Distribution | Static structured | Dynamic structured | Dynamic structured
    | Dynamic fine-grained |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 空间分布 | 静态结构 | 动态结构 | 动态结构 | 动态细粒度 |'
- en: '| Latency on GPU | Low | Medium | Low | High |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPU延迟 | 低 | 中 | 低 | 高 |'
- en: '| Time to build the index | Zero | Small | Small | High | ![Refer to caption](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| 建立索引的时间 | 零 | 小 | 小 | 高 | ![参见说明](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
- en: (a) Attention patterns
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力模式
- en: '![Refer to caption](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
- en: (b) Attention is spatial clustering
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是空间聚类
- en: '![Refer to caption](img/155b7edb1253cb7cf43bccc0db737600.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/155b7edb1253cb7cf43bccc0db737600.png)'
- en: (c) Attention pattern recall
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力模式回忆
- en: 'Figure 3: (a) Visualization of attention weights from different attention heads.
    For different prompts and tasks, the pattern of the same head is relatively consistent,
    but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest
    non-zero element in the attention matrix. (c) Attention recall distribution using
    our identified patterns, where FLOPs in the kernel refer to the real FLOPs required
    for sparse attention computing using on GPUs. Here, a 1x64 block size is used
    for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs.
    All visualization are based on LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: (a) 来自不同注意力头的注意力权重可视化。对于不同的提示和任务，相同头的模式相对一致，但稀疏索引动态变化。(b) 注意力矩阵中前10个非零元素的距离。(c)
    使用我们识别的模式的注意力召回分布，其中内核中的FLOPs指的是在GPU上进行稀疏注意力计算所需的实际FLOPs。这里，Vertical-Slash 模式使用
    1x64 的块大小，其他模式在GPU上使用 64x64 的块大小。所有可视化均基于 LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)]。'
- en: 'Although the sparsity distribution of attention matrix is dynamic, previous
    works [[82](#bib.bib82), [29](#bib.bib29)] have shown that they exhibit certain
    patterns in the two-dimensional space such as spatial clustering. Through our
    analysis of long-context prompts of various lengths and tasks, we have categorized
    such attention sparse patterns into the A-shape, Vertical-Slash (VS), and Block-Sparse
    patterns, as shown in Fig. [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Table [1](#S2.T1
    "Table 1 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") details the characteristics and differences
    between these three patterns.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管注意力矩阵的稀疏性分布是动态的，但之前的研究 [[82](#bib.bib82), [29](#bib.bib29)] 已经显示它们在二维空间中展现出某些模式，例如空间聚类。通过对各种长度和任务的长上下文提示进行分析，我们将这种注意力稀疏模式分类为A形、Vertical-Slash
    (VS) 和 Block-Sparse 模式，如图 [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    和图 [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention") 所示。表 [1](#S2.T1 "Table 1
    ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse,
    and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") 详细介绍了这三种模式的特征和差异。'
- en: A-shape pattern The attention weights of these types of heads are concentrated
    on initial tokens and local windows [[82](#bib.bib82), [29](#bib.bib29)], exhibiting
    relatively higher stability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: A-shape 模式这些头的注意力权重集中在初始标记和局部窗口上 [[82](#bib.bib82), [29](#bib.bib29)]，展现出相对较高的稳定性。
- en: Vertical-Slash (VS) pattern The attention weights are concentrated on specific
    tokens (vertical lines) and tokens at fixed intervals (slash lines). The positions
    of vertical and slash lines in this pattern dynamically change with the context
    content and exhibit a certain sparsity, making them difficult to be encompassed
    by local windows and A-shape patterns.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Vertical-Slash (VS) 模式中，注意力权重集中在特定的标记（垂直线）和固定间隔的标记（斜线）上。该模式中垂直线和斜线的位置会随着上下文内容动态变化，并展示出一定的稀疏性，使其难以被局部窗口和A形模式所涵盖。
- en: 'Block-Sparse pattern This sparsity pattern is the most dynamic, exhibiting
    a more dispersed distribution. Despite its dynamism, the attention weights maintain
    some characteristics of spatial clustering, which we identify as the block-sparse
    pattern. We analyzed the distances between non-zero attention weights and their
    top-k nearest non-zero neighbors within a 128k prompt as shown in Fig. [3b](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"). The results indicate that across layers and
    heads, the distances between nearest non-zero values are generally concentrated
    around 5, suggesting a strong spatial clustering of the attention weights.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Block-Sparse模式 这种稀疏模式最具动态性，表现出更分散的分布。尽管具有动态性，注意力权重仍然保持一些空间聚类的特征，我们将其识别为块稀疏模式。我们分析了在128k提示下非零注意力权重及其top-k最近非零邻居之间的距离，如图[3b](#S2.F3.sf2
    "在图3 ‣ 2.2 注意力稀疏性展示模式 ‣ 2 注意力头：动态的、稀疏的和特征性的 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")所示。结果表明，在层和头之间，最近的非零值之间的距离通常集中在5左右，表明注意力权重具有较强的空间聚类性。
- en: 'The point of these three patterns is that we can leverage them to perform highly
    efficient sparse computing for the attention matrix in long-context LLMs. In Fig. [3c](#S2.F3.sf3
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we test how efficient is our indentified
    patterns retrieving attention scores with limit computing cost on GPU (FLOPs).
    First, attention heads are labeled with one of the sparse pattern (detail see
    §[3.2](#S3.SS2 "3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
    ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")). Then we demonstrate our patterns are significantly
    more efficient compared to other sparse methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)]. Specifically, with the same amount of FLOPs, our patterns achieve
    a notable higher recall on attention scores, which can potentially lead to better
    accuracy. For example, previous Top-K methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)] struggle with the Block-Sparse pattern as they focus on specific
    tokens globally, while our pattern retrieves attention scores more efficiently
    and accurately. We example how we use these patterns on long-context LLMs and
    how we implement optimized GPU kernels for these patterns in §[3](#S3 "3 MInference
    1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种模式的要点是，我们可以利用它们对长上下文的LLM中的注意力矩阵进行高效的稀疏计算。在图[3c](#S2.F3.sf3 "在图3 ‣ 2.2 注意力稀疏性展示模式
    ‣ 2 注意力头：动态的、稀疏的和特征性的 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中，我们测试了我们识别的模式在GPU（FLOPs）上以有限计算成本检索注意力分数的效率。首先，注意力头被标记为一种稀疏模式（详细见§[3.2](#S3.SS2
    "3.2 通过动态稀疏注意力加速长上下文LLM推理 ‣ 3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")）。然后，我们展示了与其他稀疏方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]相比，我们的模式显著更高效。具体来说，在相同数量的FLOPs下，我们的模式在注意力分数上取得了显著更高的召回率，这可能导致更好的准确性。例如，之前的Top-K方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]在处理Block-Sparse模式时遇到困难，因为它们关注的是全局的特定令牌，而我们的模式更高效、准确地检索注意力分数。我们在§[3](#S3
    "3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中举例说明了如何在长上下文LLM上使用这些模式以及如何为这些模式实现优化的GPU内核。
- en: 3 MInference 1.0
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 MInference 1.0
- en: 'Following the analysis in §[2](#S2 "2 Attention Heads: Dynamic, Sparse, and
    Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs
    via Dynamic Sparse Attention"), we propose MInference to accelerate the pre-filling
    stage of long-context LLMs, consisting of three steps: 1) Offline attention pattern
    identification for each head; 2) Dynamic build of sparse indices w.r.t. the pattern;
    3) Sparse attention calculation with optimized GPU kernels.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据§[2](#S2 "2 注意力头：动态的、稀疏的和特征性的 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中的分析，我们提出了MInference来加速长上下文LLM的预填充阶段，包括三个步骤：1)
    离线注意力模式识别；2) 根据模式动态构建稀疏索引；3) 使用优化的GPU内核进行稀疏注意力计算。
- en: '![Refer to caption](img/717aebceb4eff85c06852353dc924b36.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/717aebceb4eff85c06852353dc924b36.png)'
- en: 'Figure 4: The three sparse methods in MInference.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MInference中的三种稀疏方法。
- en: 3.1 Problem Formulation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: 'When accelerating the pre-filling stage of long-context LLMs with sparse attention
    computing, the attention matrix can be formulated as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在加速长上下文LLMs的稀疏注意力计算的预填充阶段时，注意力矩阵可以表示为：
- en: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
- en: where $M_{i,j}\in\{0,1\}$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{i,j}\in\{0,1\}$。
- en: 'The goal of the dynamic sparse attention system is to achieve greater speedup
    with minimal overhead while retaining as much of the attention weights as possible.
    Formally, this can be expressed as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稀疏注意力系统的目标是以最小的开销实现更大的加速，同时尽可能保留更多的注意力权重。形式上，这可以表示为：
- en: '|  | $\displaystyle\min$ |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  | (2) |'
- en: '|  | $\displaystyle\min$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  |'
- en: where $t_{\text{sparse}}$ represent the time spent on dynamic sparse attention
    computation and estimation of the approximate dynamic sparse pattern, respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{\text{sparse}}$ 分别表示用于动态稀疏注意力计算和估算近似动态稀疏模式的时间。
- en: Algorithm 1 Kernel-Aware Sparse Pattern Search
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 内核感知稀疏模式搜索
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$     while $|t_{i}-t|></math> do     <math
    id=$'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$     当 $|t_{i}-t|$ 时
- en: 3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 通过动态稀疏注意力加速长上下文LLM推理
- en: Kernel-Aware Optimal Sparse Pattern Search
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内核感知最优稀疏模式搜索
- en: 'To achieve the best accuracy with limited FLOPs budget, we propose an offline
    Kernel-Aware Optimal Sparse Pattern Search method. In this step, we determine
    which sparse pattern will be used for each attention head, and the optimal setting
    for the pattern in real calculation (e.g., the number of vertical/slash lines
    in VS pattern; or the number of top-k blocks in BS patterns). As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.1 Problem Formulation ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"), we first create
    the search space based on a target FLOPs for each pattern, ensuring all potential
    candidates (i.e., different patterns with different settings) have similar computational
    cost. Kernel-aware here indicates the computational cost reflects the real FLOPs
    in GPU kernels, instead of conceptual estimations, which is crucial to achieve
    the optimal acceleration.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在有限的FLOPs预算下实现最佳精度，我们提出了一种离线内核感知最优稀疏模式搜索方法。在这一步中，我们确定每个注意力头使用哪种稀疏模式，以及实际计算中的模式最优设置（例如，VS模式中的垂直/斜线数量；或BS模式中的top-k块数量）。如算法 [1](#alg1
    "Algorithm 1 ‣ 3.1 Problem Formulation ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，我们首先根据每种模式的目标FLOPs创建搜索空间，确保所有潜在候选模式（即，具有不同设置的不同模式）的计算成本相似。这里的内核感知指计算成本反映了GPU内核中的实际FLOPs，而不是概念上的估算，这对于实现最佳加速至关重要。'
- en: Algorithm 2 Vertical-Slash Head
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 垂直-斜线头
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$  # Final dynamic
    sparse attention scores (only index block)  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{vs})/\sqrt{d}\right)$'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$  # 最终动态稀疏注意力得分（仅索引块）  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{vs})/\sqrt{d}\right)$'
- en: Algorithm 3 Block-Sparse Head
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 块稀疏头
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$  # Final dynamic
    sparse attention scores (only index block)  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{b})/\sqrt{d}\right)$'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$  # 最终动态稀疏注意力得分（仅索引块）  $\bm{A}\leftarrow\mathrm{softmax}\left(\mathrm{sparse}(\bm{Q}\bm{K}^{\top},\bm{i}_{b})/\sqrt{d}\right)$'
- en: Next, we go through the search space with a reference example to decide the
    optimal pattern and setting. Specifically, we use recall of the attention output
    as the objective criterion when searching for the best pattern. This approach
    leverages FlashAttention [[13](#bib.bib13)] to reduce GPU memory overhead and
    incorporates the information from the $\bm{V}$ matrix, enabling end-to-end selection
    of the best pattern, which further enhances performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过一个参考示例来遍历搜索空间，以决定最优模式和设置。具体而言，我们使用注意力输出的召回率作为搜索最佳模式的目标标准。这种方法利用FlashAttention [[13](#bib.bib13)]来减少GPU内存开销，并结合$\bm{V}$矩阵中的信息，实现端到端的最佳模式选择，从而进一步提升性能。
- en: Sparsity Indices Approximation and Dynamic Sparse Attention Calculation
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏指数近似和动态稀疏注意力计算
- en: 'During the inference stage, we will perform an online estimation on the attention
    matrix to dynamically determine the spatial distribution our sparse indices, based
    on the assigned patterns and the exact input. After that, we conduct the sparse
    attention computations with our optimized GPU kernels. The implementation details
    of our kernels can be found in Appendix [C.4](#A3.SS4 "C.4 Kernel Implementation
    ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"). Noted that the sparse mask is
    static for A-shape heads, so there is no overhead in building the dynamic masks,
    and only sparse calculation is required.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段，我们将对注意力矩阵进行在线估计，以动态确定稀疏指数的空间分布，基于分配的模式和确切输入。之后，我们使用优化的 GPU 内核进行稀疏注意力计算。我们内核的实现细节可以在附录
    [C.4](#A3.SS4 "C.4 内核实现 ‣ 附录 C 实验细节 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充")
    中找到。值得注意的是，对于 A 形头，稀疏掩码是静态的，因此在构建动态掩码时没有开销，仅需要进行稀疏计算。
- en: '(i) Vertical-Slash head. As shown in Algorithm [2](#alg2 "Algorithm 2 ‣ Kernel-Aware
    Optimal Sparse Pattern Search ‣ 3.2 Speedup of Long-context LLM Inference via
    Dynamic Sparse Attention ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), due to the continuity of
    vertical and slash lines, we matmul the last query vector $\bm{Q}_{[-\text{last\_q}:]}$.
    Using these sparse indices, we perform block-sparse calculations of the attention
    weights and attention output.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 垂直斜杠头。正如算法 [2](#alg2 "算法 2 ‣ 内核感知最佳稀疏模式搜索 ‣ 3.2 通过动态稀疏注意力加速长上下文 LLM 推理 ‣
    3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 所示，由于垂直和斜杠线的连续性，我们对最后的查询向量
    $\bm{Q}_{[-\text{last\_q}:]}$ 进行矩阵乘法。利用这些稀疏指数，我们进行注意力权重和注意力输出的块稀疏计算。
- en: '(ii) Block-Sparse head. Per Algorithm [3](#alg3 "Algorithm 3 ‣ Kernel-Aware
    Optimal Sparse Pattern Search ‣ 3.2 Speedup of Long-context LLM Inference via
    Dynamic Sparse Attention ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), mean pooling is applied
    on $\bm{Q}$ and use it to compute the sparse attention weights and attention output.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 块稀疏头。根据算法 [3](#alg3 "算法 3 ‣ 内核感知最佳稀疏模式搜索 ‣ 3.2 通过动态稀疏注意力加速长上下文 LLM 推理 ‣
    3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充")，对 $\bm{Q}$ 应用均值池化，并使用它来计算稀疏注意力权重和注意力输出。
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we investigate two questions: (i) How effective is MInference?
    We evaluate our method on three general long-context benchmarks: InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], and the Needle In A Haystack task [[35](#bib.bib35)],
    as well as the long-context language modeling task [[65](#bib.bib65)]. These benchmarks
    cover long-context QA, multi-hop QA, math reasoning, aggregation tasks, summarization,
    retrieval tasks, and code debugging, allowing us to assess MInference’s effectiveness
    across a wide range of long-context scenarios. (ii) How efficient is MInference?
    We delve into the end-to-end latency and its breakdown to evaluate the efficiency
    of MInference. Additional experimental, latency results, and analysis can be found
    in Appendix [D](#A4 "Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    [E](#A5 "Appendix E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), and [F](#A6 "Appendix F
    Sparsity in Kernel Distribution ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了两个问题：(i) MInference 的有效性如何？我们在三个通用长上下文基准上评估我们的方法：InfiniteBench [[86](#bib.bib86)]、RULER
    [[28](#bib.bib28)] 和 Needle In A Haystack 任务 [[35](#bib.bib35)]，以及长上下文语言建模任务 [[65](#bib.bib65)]。这些基准涵盖了长上下文
    QA、多跳 QA、数学推理、聚合任务、摘要、检索任务和代码调试，使我们能够评估 MInference 在广泛的长上下文场景中的有效性。(ii) MInference
    的效率如何？我们深入研究了端到端延迟及其分解，以评估 MInference 的效率。附加的实验、延迟结果和分析可以在附录 [D](#A4 "附录 D 附加实验结果
    ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充")、[E](#A5 "附录 E 模式分布 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLM 的预填充") 和 [F](#A6 "附录 F 内核分布中的稀疏性 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充")
    中找到。
- en: Implementation Details
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实现细节
- en: 'Our experiments use four state-of-the-art long-context LLMs: LLaMA-3-8B-Instruct-262k¹¹1https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k,
    LLaMA-3-8B-Instruct-1048k²²2https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k,
    GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)]. Additionally,
    we tested Needle in A Haystack [[35](#bib.bib35)] on Phi-3-Mini-128K [[2](#bib.bib2)]
    and Qwen2-7B-128K [[5](#bib.bib5)], as detailed in Appendix [D.1](#A4.SS1 "D.1
    Needle In A Haystack ‣ Appendix D Additional Experiment Results ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    To guarantee stable results, we use greedy decoding in all experiments. We provide
    a simple custom implementation of our method in PyTorch, built on FlashAttention [[13](#bib.bib13)],
    Triton [[75](#bib.bib75)], and the dynamic sparse compiler PIT [[88](#bib.bib88)].
    We set the target FLOPs $t$ in the Vertical-Slash and Block-Sparse patterns, respectively.
    The latency experiments are conducted on a single Nvidia A100 GPU in the bfloat16
    format. More details are provided in Appendix [C.2](#A3.SS2 "C.2 Additional Implementation
    Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验使用了四个最先进的长上下文LLMs：LLaMA-3-8B-Instruct-262k¹¹1https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k，LLaMA-3-8B-Instruct-1048k²²2https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k，GLM-4-9B-1M [[26](#bib.bib26)]，和Yi-9B-200K [[84](#bib.bib84)]。此外，我们还在Phi-3-Mini-128K [[2](#bib.bib2)]和Qwen2-7B-128K [[5](#bib.bib5)]上测试了Needle
    in A Haystack [[35](#bib.bib35)]，具体细节见附录 [D.1](#A4.SS1 "D.1 Needle In A Haystack
    ‣ Appendix D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention")。为保证结果稳定，我们在所有实验中使用贪婪解码。我们在PyTorch中提供了我们方法的简单自定义实现，该实现基于FlashAttention [[13](#bib.bib13)]、Triton [[75](#bib.bib75)]和动态稀疏编译器PIT [[88](#bib.bib88)]。我们在Vertical-Slash和Block-Sparse模式中分别设置了目标FLOPs
    $t$。延迟实验在单个Nvidia A100 GPU上以bfloat16格式进行。更多细节见附录 [C.2](#A3.SS2 "C.2 Additional
    Implementation Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")。'
- en: Dataset & Evaluation Metrics
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集与评估指标
- en: 'We use the provided metrics and scripts from the following benchmarks for evaluation.
    More details about dataset can be found in Appendix [C.1](#A3.SS1 "C.1 Dataset
    Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用以下基准提供的指标和脚本进行评估。关于数据集的更多细节可以在附录 [C.1](#A3.SS1 "C.1 Dataset Details ‣ Appendix
    C Experiment Details ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")中找到。'
- en: '(i) InfiniteBench [[86](#bib.bib86)]: This benchmark consists of 10 tasks,
    including retrieval tasks such as PassKey retrieval, Number retrieval, and KV
    retrieval, as well as representative realistic tasks like question-answering,
    coding, dialogue, and summarization. The average context length of InfiniteBench
    is about 214K tokens.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '(i) InfiniteBench [[86](#bib.bib86)]: 该基准包含10个任务，包括检索任务如PassKey检索、Number检索和KV检索，以及代表性的现实任务如问答、编码、对话和摘要。InfiniteBench的平均上下文长度约为214K
    tokens。'
- en: '(ii) RULER [[28](#bib.bib28)]: A challenging long-context benchmark consisting
    of 4 categories and 13 complex tasks, including retrieval, multi-hop tracing and
    aggregation, and QA tasks. It contains subsets with different prompt lengths up
    to 128k tokens, allowing us to determine the actual context window size of the
    model based on the results.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '(ii) RULER [[28](#bib.bib28)]: 一个具有挑战性的长上下文基准，包含4个类别和13个复杂任务，包括检索、多跳跟踪和聚合任务，以及问答任务。它包含不同提示长度的子集，最长可达128k
    tokens，这使我们能够根据结果确定模型的实际上下文窗口大小。'
- en: '(iii) Needle In A Haystack [[35](#bib.bib35)]: A long-context retrieval benchmark
    testing LLMs’ performance with context window sizes up to 1M tokens where information
    placed at various positions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '(iii) Needle In A Haystack [[35](#bib.bib35)]: 一个长上下文检索基准，测试LLMs在上下文窗口大小最长达1M
    tokens的情况下的表现，其中信息分布在各种位置。'
- en: '(iv) PG-19 [[65](#bib.bib65)]: Following StreamingLLM [[82](#bib.bib82)] and
    H2O [[89](#bib.bib89)], we use PG-19 for long-context language modeling tasks
    with prompts up to 100k tokens.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '(iv) PG-19 [[65](#bib.bib65)]: 继StreamingLLM [[82](#bib.bib82)]和H2O [[89](#bib.bib89)]之后，我们使用PG-19进行长上下文语言建模任务，提示长度最长可达100k
    tokens。'
- en: 'Table 2: Performance of different methods with different base models on InfiniteBench [[86](#bib.bib86)].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同基础模型在InfiniteBench [[86](#bib.bib86)]上的不同方法性能。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find | Retr.PassKey
    | Retr.Number | Retr.KV | 平均值 |'
- en: '| LLaMA-3-8B-262K | 20.2 | 12.4 | 67.3 | 6.0 | 12.9 | 22.1 | 26.6 | 100.0 |
    100.0 | 14.4 | 38.2 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B-262K | 20.2 | 12.4 | 67.3 | 6.0 | 12.9 | 22.1 | 26.6 | 100.0 |
    100.0 | 14.4 | 38.2 |'
- en: '| StreamingLLM | 21.0 | 8.2 | 40.2 | 10.0 | 10.4 | 25.9 | 30.0 | 86.8 | 5.1
    | 0.8 | 23.8 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 21.0 | 8.2 | 40.2 | 10.0 | 10.4 | 25.9 | 30.0 | 86.8 | 5.1
    | 0.8 | 23.8 |'
- en: '| StreamingLLM w/ dilated | 20.1 | 9.4 | 44.5 | 15.5 | 11.2 | 20.5 | 27.5 |
    5.0 | 87.5 | 0.5 | 24.2 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | 20.1 | 9.4 | 44.5 | 15.5 | 11.2 | 20.5 | 27.5 |
    5.0 | 87.5 | 0.5 | 24.2 |'
- en: '| StreamingLLM w/ strided | 17.3 | 8.2 | 27.5 | 14.5 | 11.2 | 19.5 | 27.5 |
    4.0 | 2.1 | 1.0 | 13.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | 17.3 | 8.2 | 27.5 | 14.5 | 11.2 | 19.5 | 27.5 |
    4.0 | 2.1 | 1.0 | 13.3 |'
- en: '| InfLLM | 24.1 | 7.8 | 45.0 | 6.0 | 11.4 | 19.5 | 32.9 | 100.0 | 100.0 | 1.2
    | 34.8 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 24.1 | 7.8 | 45.0 | 6.0 | 11.4 | 19.5 | 32.9 | 100.0 | 100.0 | 1.2
    | 34.8 |'
- en: '| Ours w/ static | 19.9 | 8.6 | 43.2 | 3.5 | 8.9 | 20.6 | 25.1 | 92.4 | 96.3
    | 0.2 | 31.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 w/ static | 19.9 | 8.6 | 43.2 | 3.5 | 8.9 | 20.6 | 25.1 | 92.4 | 96.3
    | 0.2 | 31.9 |'
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
- en: '| Yi-9B-200K | 8.2 | 10.6 | 64.2 | 1.0 | 17.3 | 21.3 | 23.4 | 99.8 | 100.0
    | 28.8 | 37.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 8.2 | 10.6 | 64.2 | 1.0 | 17.3 | 21.3 | 23.4 | 99.8 | 100.0
    | 28.8 | 37.5 |'
- en: '| StreamingLLM | 5.4 | 14.2 | 38.0 | 4.0 | 18.8 | 18.8 | 22.3 | 39.2 | 6.1
    | 1.6 | 16.8 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 5.4 | 14.2 | 38.0 | 4.0 | 18.8 | 18.8 | 22.3 | 39.2 | 6.1
    | 1.6 | 16.8 |'
- en: '| StreamingLLM w/ dilated | 5.7 | 4.2 | 15.0 | 0.0 | 18.2 | 0.0 | 2.9 | 0.0
    | 0.0 | 0.0 | 4.2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | 5.7 | 4.2 | 15.0 | 0.0 | 18.2 | 0.0 | 2.9 | 0.0
    | 0.0 | 0.0 | 4.2 |'
- en: '| StreamingLLM w/ strided | 6.1 | 4.5 | 9.8 | 0.0 | 16.9 | 0.0 | 3.1 | 1.5
    | 0.0 | 0.0 | 4.6 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | 6.1 | 4.5 | 9.8 | 0.0 | 16.9 | 0.0 | 3.1 | 1.5
    | 0.0 | 0.0 | 4.6 |'
- en: '| InfLLM | 6.3 | 13.0 | 45.9 | 2.5 | 21.5 | 20.6 | 34.6 | 85.3 | 88.1 | 1.4
    | 31.9 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 6.3 | 13.0 | 45.9 | 2.5 | 21.5 | 20.6 | 34.6 | 85.3 | 88.1 | 1.4
    | 31.9 |'
- en: '| Ours w/ static | 5.8 | 12.6 | 48.5 | 3.0 | 12.6 | 20.8 | 25.1 | 60.9 | 38.5
    | 1.0 | 22.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 w/ static | 5.8 | 12.6 | 48.5 | 3.0 | 12.6 | 20.8 | 25.1 | 60.9 | 38.5
    | 1.0 | 22.9 |'
- en: '| Ours | 7.9 | 11.2 | 64.2 | 1.0 | 17.9 | 24.1 | 23.1 | 99.5 | 100.0 | 27.6
    | 37.7 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 7.9 | 11.2 | 64.2 | 1.0 | 17.9 | 24.1 | 23.1 | 99.5 | 100.0 | 27.6
    | 37.7 |'
- en: '| GLM-4-9B-1M | 28.3 | 9.7 | 68.6 | 39.5 | 12.1 | 29.4 | 38.9 | 100.0 | 100.0
    | 41.0 | 46.7 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4-9B-1M | 28.3 | 9.7 | 68.6 | 39.5 | 12.1 | 29.4 | 38.9 | 100.0 | 100.0
    | 41.0 | 46.7 |'
- en: '| StreamingLLM | 27.7 | 6.4 | 40.2 | 12.5 | 10.8 | 27.7 | 21.1 | 97.1 | 25.6
    | 0.6 | 27.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 27.7 | 6.4 | 40.2 | 12.5 | 10.8 | 27.7 | 21.1 | 97.1 | 25.6
    | 0.6 | 27.0 |'
- en: '| InfLLM | 28.0 | 7.3 | 45.0 | 14.0 | 10.7 | 27.9 | 39.4 | 98.0 | 100.0 | 2.6
    | 37.3 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 28.0 | 7.3 | 45.0 | 14.0 | 10.7 | 27.9 | 39.4 | 98.0 | 100.0 | 2.6
    | 37.3 |'
- en: '| Ours | 28.8 | 9.6 | 68.6 | 38.5 | 12.0 | 30.7 | 39.1 | 100.0 | 100.0 | 43.0
    | 47.0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 28.8 | 9.6 | 68.6 | 38.5 | 12.0 | 30.7 | 39.1 | 100.0 | 100.0 | 43.0
    | 47.0 |'
- en: Baselines
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准
- en: 'We include five training-free sparse attention approaches as our baselines:
    1) StreamingLLM [[82](#bib.bib82)], which corresponds to the A-shape pattern.
    We use 1k global tokens and 4k local windows in all our experiments; 2) StreamingLLM
    w/ dilated [[6](#bib.bib6)], which sets dilated local windows with intervals in
    the local windows direction. We use 1k global tokens and 8k dilated attention
    windows with an interval of 1; 3) StreamingLLM w/ strided [[8](#bib.bib8)], which
    retains local windows while adding dilated attention. We use 1k global tokens,
    2k local windows, and 4k dilated attention windows with an interval of 1; 4) InfLLM [[83](#bib.bib83)],
    which uses a memory unit to process streaming long sequences. Following the paper,
    we set 128 global tokens and 8k local windows in all experiments; 5) Ours w/ static,
    which utilizes static sparse indices in the Vertical-Slash and Block-Sparse heads.
    For all baselines, we perform sparse computation only during the pre-filling stage,
    while retaining dense computation during the decoding stage.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括五种不需要训练的稀疏注意力方法作为我们的基准：1）StreamingLLM [[82](#bib.bib82)]，对应于 A 形模式。在所有实验中，我们使用
    1k 全局标记和 4k 局部窗口；2）StreamingLLM w/ dilated [[6](#bib.bib6)]，设置有间隔的扩张局部窗口。在所有实验中，我们使用
    1k 全局标记和 8k 扩张注意力窗口，间隔为 1；3）StreamingLLM w/ strided [[8](#bib.bib8)]，保留局部窗口同时添加扩张注意力。在所有实验中，我们使用
    1k 全局标记、2k 局部窗口和 4k 扩张注意力窗口，间隔为 1；4）InfLLM [[83](#bib.bib83)]，使用内存单元处理流式长序列。根据论文，我们在所有实验中设置
    128 个全局标记和 8k 局部窗口；5）我们的方法 w/ static，利用 Vertical-Slash 和 Block-Sparse 头中的静态稀疏索引。对于所有基准，我们仅在预填充阶段执行稀疏计算，同时在解码阶段保留密集计算。
- en: InfiniteBench
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: InfiniteBench
- en: 'Table 3: Performance (%) of different models and different methods on RULER [[28](#bib.bib28)]
    evaluated at lengths from 4k to 128k.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同模型和不同方法在 RULER [[28](#bib.bib28)] 上的性能（%），评估长度从 4k 到 128k。
- en: '| Methods | Claimed | Effective | 4K | 8K | 16K | 32K | 64K | 128K | Avg. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 声称 | 有效 | 4K | 8K | 16K | 32K | 64K | 128K | 平均 |'
- en: '| LLaMA-3-8B-262K | 262K | 16K | 97.2 | 91.8 | 87.3 | 80.8 | 77.4 | 72.2 |
    84.4 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B-262K | 262K | 16K | 97.2 | 91.8 | 87.3 | 80.8 | 77.4 | 72.2 |
    84.4 |'
- en: '| StreamingLLM | - | 4K | 97.2 | 38.1 | 37.5 | 17.2 | 14.2 | 9.4 | 35.0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | - | 4K | 97.2 | 38.1 | 37.5 | 17.2 | 14.2 | 9.4 | 35.0 |'
- en: '| StreamingLLM w/ dilated | - | <4K | 23.4 | 0.7 | 1.4 | 18.8 | 16.5 | 15.6
    | 12.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | - | <4K | 23.4 | 0.7 | 1.4 | 18.8 | 16.5 | 15.6
    | 12.7 |'
- en: '| StreamingLLM w/ strided | - | <4K | 2.0 | 0.7 | 0.6 | 0.6 | 0.7 | 1.3 | 1.0
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | - | <4K | 2.0 | 0.7 | 0.6 | 0.6 | 0.7 | 1.3 | 1.0
    |'
- en: '| InfLLM | - | 4K | 89.4 | 79.8 | 70.1 | 55.6 | 43.0 | 39.5 | 62.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | - | 4K | 89.4 | 79.8 | 70.1 | 55.6 | 43.0 | 39.5 | 62.9 |'
- en: '| Ours | - | 32K | 97.7 | 91.2 | 88.5 | 85.0 | 82.3 | 77.6 | 87.0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | 32K | 97.7 | 91.2 | 88.5 | 85.0 | 82.3 | 77.6 | 87.0 |'
- en: '| Yi-9B-200K | 200K | 8K | 91.9 | 90.2 | 78.8 | 76.3 | 68.1 | 62.9 | 78.1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 200K | 8K | 91.9 | 90.2 | 78.8 | 76.3 | 68.1 | 62.9 | 78.1 |'
- en: '| StreamingLLM | - | 4K | 91.9 | 37.8 | 33.9 | 18.6 | 13.0 | 12.8 | 34.3 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | - | 4K | 91.9 | 37.8 | 33.9 | 18.6 | 13.0 | 12.8 | 34.3 |'
- en: '| StreamingLLM w/ dilated | - | <4K | 44.8 | 42.8 | 38.5 | 29.8 | 26.8 | 23.9
    | 34.4 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | - | <4K | 44.8 | 42.8 | 38.5 | 29.8 | 26.8 | 23.9
    | 34.4 |'
- en: '| StreamingLLM w/ strided | - | <4K | 2.6 | 0.7 | 0.6 | 0.6 | 1.2 | 0.5 | 1.1
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | - | <4K | 2.6 | 0.7 | 0.6 | 0.6 | 1.2 | 0.5 | 1.1
    |'
- en: '| InfLLM | - | <4K | 80.3 | 83.9 | 60.7 | 45.2 | 38.6 | 30.2 | 56.5 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | - | <4K | 80.3 | 83.9 | 60.7 | 45.2 | 38.6 | 30.2 | 56.5 |'
- en: '| Ours | - | 8K | 92.3 | 89.7 | 79.0 | 73.8 | 64.7 | 56.9 | 74.7 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | 8K | 92.3 | 89.7 | 79.0 | 73.8 | 64.7 | 56.9 | 74.7 |'
- en: '| GLM-4-9B-1M | 1M | 64K | 93.8 | 91.6 | 89.3 | 87.4 | 85.2 | 80.8 | 88.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4-9B-1M | 1M | 64K | 93.8 | 91.6 | 89.3 | 87.4 | 85.2 | 80.8 | 88.0 |'
- en: '| StreamingLLM | - | 4K | 93.8 | 66.9 | 58.5 | 51.4 | 45.9 | 39.1 | 59.3 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | - | 4K | 93.8 | 66.9 | 58.5 | 51.4 | 45.9 | 39.1 | 59.3 |'
- en: '| InfLLM | - | 8K | 94.7 | 89.5 | 76.4 | 66.5 | 56.8 | 53.5 | 72.9 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | - | 8K | 94.7 | 89.5 | 76.4 | 66.5 | 56.8 | 53.5 | 72.9 |'
- en: '| Ours | - | 64K | 94.6 | 93.1 | 91.0 | 89.6 | 85.5 | 84.0 | 89.6 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | 64K | 94.6 | 93.1 | 91.0 | 89.6 | 85.5 | 84.0 | 89.6 |'
- en: 'As shown in Table [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), MInference achieves the best overall performance compared to baselines
    on InfiniteBench. Remarkably, MInference matches or even slightly surpasses the
    performance of the original full attention baseline on some tasks, despite the
    significant acceleration it provided. From the perspective of different tasks,
    our method not only performs well in natural language tasks such as summarization,
    QA, and code, but also maintains the original model’s performance on retrieval-related
    tasks. Baseline methods such as StreamingLLM, on the contrary, struggle with these
    retrieval tasks. Additionally, on tasks such as dialogue QA, using local attention
    mechanisms can better handle these tasks, while our performance is closer to the
    original results, indicating that our method is not solely based on local windows.
    Extending the local windows’ intervals in StreamingLLM, i.e., w/ dilated and w/
    strided, hardly affects the model’s performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，MInference在InfiniteBench上的整体表现优于基准方法。值得注意的是，尽管MInference提供了显著加速，但在一些任务上其表现与原始全注意力基准相当或略有超越。从不同任务的角度来看，我们的方法不仅在摘要、问答和代码等自然语言任务中表现良好，还保持了原始模型在检索相关任务中的表现。相对而言，像StreamingLLM这样的基准方法在这些检索任务中表现不佳。此外，在对话问答等任务上，使用局部注意机制可以更好地处理这些任务，而我们的性能更接近原始结果，这表明我们的方法不仅仅基于局部窗口。扩展StreamingLLM的局部窗口间隔，即使用扩张和步进策略，对模型性能几乎没有影响。'
- en: RULER
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RULER
- en: 'To further reveal the true potential of our method in long-context LLMs, we
    evaluate MInference with the state-of-the-art long-context challenge, RULER. As
    shown in Table [3](#S4.T3 "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    MInference effectively maintains the long-context performance even in complex
    multi-hop or aggregation tasks in RULER. It even outperforms the original full
    attention for testing lengths beyond 32K, achieving effective context windows
    of 32K and 64K (context with performance over 85% is considered effective [[28](#bib.bib28)])
    in LLaMA-3-8B-262K and GLM-4-9B-1M.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '为进一步揭示我们方法在长上下文 LLM 中的真实潜力，我们使用最先进的长上下文挑战 RULER 评估 MInference。如表[3](#S4.T3
    "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention")所示，MInference 即使在 RULER 的复杂多跳或聚合任务中也能有效保持长上下文性能。它甚至在测试长度超过
    32K 时超越了原始的全注意力，在 LLaMA-3-8B-262K 和 GLM-4-9B-1M 中实现了有效的上下文窗口为 32K 和 64K（上下文性能超过
    85% 被认为是有效的 [[28](#bib.bib28)]）。'
- en: Language Modeling
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言建模
- en: 'Following the approach of StreamingLLM [[82](#bib.bib82)] and H2O [[89](#bib.bib89)],
    we evaluate our methods against baselines on the language modeling task based
    on the PG-19 dataset [[65](#bib.bib65)]. As shown in [5](#S4.F5 "Figure 5 ‣ Language
    Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), our method yields best results compared to
    other sparse approaches, and exhibits minimal divergence compared to the full
    attention baseline. For prompts of 100K token, our perplexity is only 0.2 higher
    than the full attention, but lower than StreamingLLM for 0.25 and 0.75 on the
    Yi-9B-200K and LLaMA-3-262K models respectively.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '参照 StreamingLLM [[82](#bib.bib82)] 和 H2O [[89](#bib.bib89)] 的方法，我们在基于 PG-19
    数据集 [[65](#bib.bib65)] 的语言建模任务中评估了我们的方法与基准模型的表现。如[5](#S4.F5 "Figure 5 ‣ Language
    Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")所示，我们的方法相较于其他稀疏方法取得了最佳结果，并且与全注意力基线的偏差最小。对于
    100K token 的提示，我们的困惑度仅比全注意力高 0.2，但在 Yi-9B-200K 和 LLaMA-3-262K 模型上，分别比 StreamingLLM
    低 0.25 和 0.75。'
- en: '![Refer to caption](img/760645197ef39823c822f7e94434c307.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/760645197ef39823c822f7e94434c307.png)'
- en: (a) LLaMA-3-8B-Instruct-262K
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLaMA-3-8B-Instruct-262K
- en: (b) Yi-9B-200K
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Yi-9B-200K
- en: 'Figure 5: Perplexity results on PG-19 [[65](#bib.bib65)] using different models
    and methods.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用不同模型和方法在 PG-19 [[65](#bib.bib65)] 上的困惑度结果。
- en: '![Refer to caption](img/d8c4f5b0df6fa7dcf125d1ff1ed2fcb5.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d8c4f5b0df6fa7dcf125d1ff1ed2fcb5.png)'
- en: 'Figure 6: Results on Needle In A Haystack of StreamingLLM [[82](#bib.bib82)]
    in LLaMA-3-8B-1M.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在 LLaMA-3-8B-1M 中 StreamingLLM [[82](#bib.bib82)] 的“大海捞针”结果。
- en: Needle In A Haystack
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大海捞针
- en: 'Comparing Fig. [1a](#S0.F1.sf1 "In Figure 1 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") to [6](#S4.F6
    "Figure 6 ‣ Language Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), our method effectively retains
    the ability to process information at different positions across various context
    windows, ranging from 1k to 1M tokens. In contrast, methods like StreamingLLM,
    while effective in reducing latency, experience a rapid decline in performance
    once the critical information exceeds the range of global tokens and local windows.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '比较图[1a](#S0.F1.sf1 "In Figure 1 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention") 和 [6](#S4.F6 "Figure 6 ‣
    Language Modeling ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention")，我们的方法有效保持了在不同上下文窗口（从 1k 到 1M
    token）中处理信息的能力。相比之下，像 StreamingLLM 这样的算法虽然在减少延迟方面有效，但一旦关键的信息超出全球 token 和局部窗口的范围，性能迅速下降。'
- en: Ablation Study
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 消融研究
- en: 'To evaluate the contributions of different components in MInference, we introduce
    four variants for the ablation study: (1) Ours w/ static, which uses a static
    sparse mask in the Vertical-Slash and Block-Sparse patterns; (2) Ours w/ only
    A-shape, which is equivalent to StreamingLLM; (3) Ours w/ only block-sparse, which
    uses only the Block-Sparse pattern in the dynamic sparse calculation. (4) Ours
    w/ only vertical-slash, which uses only the Vertical-Slash pattern in the dynamic
    sparse calculation.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估 MInference 中不同组件的贡献，我们引入了四种变体进行消融研究：(1) Ours w/ static，使用 Vertical-Slash
    和 Block-Sparse 模式中的静态稀疏掩码；(2) Ours w/ only A-shape，相当于 StreamingLLM；(3) Ours w/
    only block-sparse，只使用动态稀疏计算中的 Block-Sparse 模式；(4) Ours w/ only vertical-slash，只使用动态稀疏计算中的
    Vertical-Slash 模式。
- en: 'Table 4: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K
    on InfiniteBench [[86](#bib.bib86)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在InfiniteBench上使用LLaMA-3-8B-Instruct-262K的不同消融方法的性能 [[86](#bib.bib86)]。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find | Retr.PassKey
    | Retr.Number | Retr.KV | 平均 |'
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
- en: '| Ours w/ only block-sparse | 12.4 | 3.4 | 5.7 | 6.0 | 3.1 | 12.2 | 24.0 |
    59.5 | 60.3 | 0.0 | 18.7 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 仅使用块稀疏的我们的 | 12.4 | 3.4 | 5.7 | 6.0 | 3.1 | 12.2 | 24.0 | 59.5 | 60.3 | 0.0
    | 18.7 |'
- en: '| Ours w/ only vertical-slash | 19.6 | 12.0 | 62.1 | 9.5 | 11.7 | 21.6 | 29.1
    | 100.0 | 100.0 | 5.0 | 37.1 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 仅使用垂直斜线的我们的 | 19.6 | 12.0 | 62.1 | 9.5 | 11.7 | 21.6 | 29.1 | 100.0 | 100.0
    | 5.0 | 37.1 |'
- en: 'Tables [2](#S4.T2 "Table 2 ‣ Dataset & Evaluation Metrics ‣ 4 Experiments ‣
    MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), [3](#S4.T3 "Table 3 ‣ InfiniteBench ‣ 4 Experiments ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"),
    and [4](#S4.T4 "Table 4 ‣ Ablation Study ‣ 4 Experiments ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") present the ablation
    results. It first proves that using static indices significantly degrades LLM
    performance, especially in highly dynamic tasks like KV retrieval, where accuracy
    nearly drops to zero. This highlight the necessity of our dynamic strategy and
    the effectiveness of our dynamically built sparse indices. Additionally, remove
    any pattern from the three leads to varying degrees of performance degradation.
    Specifically, "only A-shape" can only capture information within local windows.
    The "only block-sparse" variant using only the BS pattern, also results in significant
    performance declines. On the other hand, "only vertical-slash" manages to preserve
    most of the performance due to its balance between dynamicity and the StreamingLLM
    pattern, but still fall behind the full version of our method.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 数据集与评估指标 ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")、[3](#S4.T3
    "表 3 ‣ InfiniteBench ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")和[4](#S4.T4
    "表 4 ‣ 消融研究 ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")展示了消融结果。它首先证明使用静态索引会显著降低LLM性能，特别是在像KV检索这样的高动态任务中，准确率几乎降至零。这凸显了我们动态策略的必要性和我们动态构建稀疏索引的有效性。此外，从三种方法中移除任何模式都会导致不同程度的性能下降。特别是，“仅A形状”只能捕获局部窗口中的信息。“仅块稀疏”变体仅使用BS模式，也导致显著的性能下降。另一方面，“仅垂直斜线”由于其在动态性和StreamingLLM模式之间的平衡，能够保留大部分性能，但仍然落后于我们方法的完整版本。
- en: Latency
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 延迟
- en: 'Fig. [1b](#S0.F1.sf2 "In Figure 1 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention") and [10](#A4.F10 "Figure
    10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional Experiment Results ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    shows the latency and breakdown of MInference across different context windows
    on a single A100\. At 100K, 300K, 500K, and 1M tokens, our method achieves speedups
    of 1.8$\times$, respectively. It reduces the pre-filling latency from 30 mins
    to 3 mins on a single A100 for a prompt of 1M token. By further utilizing tensor
    parallel [[46](#bib.bib46)] and context parallel [[50](#bib.bib50), [31](#bib.bib31)],
    this latency can be reduced to 40 seconds on 8x A100 GPUs. This significantly
    lowers the deployment cost of long-context LLMs and enhances user experience.
    And since our kernel is implemented based on Triton, it can be easily ported to
    other devices and achieve similar speedups, such as on the H100. Additionally,
    analyzing the latency breakdown, we found about 5%-20% of the overhead is spent
    on dynamic sparse index building, while the remaining time is spent on dynamic
    sparse calculation.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1b](#S0.F1.sf2 "图 1 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 和 [10](#A4.F10
    "图 10 ‣ D.2 延迟分解 ‣ 附录 D 额外实验结果 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 显示了
    MInference 在单个 A100 上不同上下文窗口的延迟和分解情况。在 100K、300K、500K 和 1M 令牌下，我们的方法分别实现了 1.8$\times$
    的加速。它将单个 A100 上 1M 令牌的预填充延迟从 30 分钟减少到 3 分钟。通过进一步利用张量并行 [[46](#bib.bib46)] 和上下文并行
    [[50](#bib.bib50), [31](#bib.bib31)]，这一延迟可以在 8 台 A100 GPU 上减少到 40 秒。这显著降低了长上下文
    LLM 的部署成本，并提升了用户体验。由于我们的内核基于 Triton 实现，可以轻松移植到其他设备，并实现类似的加速，比如在 H100 上。此外，通过分析延迟分解，我们发现约
    5%-20% 的开销用于动态稀疏索引构建，而其余时间用于动态稀疏计算。
- en: Integrate with KV cache compression methods
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与 KV 缓存压缩方法的集成
- en: 'Table 5: Performance of different methods on InfiniteBench [[86](#bib.bib86)]
    using SnapKV [[43](#bib.bib43)] in the decoding stage.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同方法在解码阶段的 InfiniteBench 性能 [[86](#bib.bib86)]，使用 SnapKV [[43](#bib.bib43)]。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 英文摘要 | 英文问答 | 英文选择题 | 英文对话 | 中文问答 | 代码调试 | 数学查找 | 检索密码 | 检索编号 | 检索KV
    | 平均值 |'
- en: '| LLaMA-3 w/ SnapKV | 18.0 | 11.8 | 65.5 | 2.5 | 12.0 | 21.3 | 26.6 | 100.0
    | 100.0 | 1.8 | 36.0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 配合 SnapKV | 18.0 | 11.8 | 65.5 | 2.5 | 12.0 | 21.3 | 26.6 | 100.0
    | 100.0 | 1.8 | 36.0 |'
- en: '| Ours w/ SnapKV | 18.9 | 11.7 | 66.4 | 6.5 | 12.1 | 21.8 | 33.1 | 100.0 |
    100.0 | 2.0 | 37.3 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 我们配合 SnapKV | 18.9 | 11.7 | 66.4 | 6.5 | 12.1 | 21.8 | 33.1 | 100.0 | 100.0
    | 2.0 | 37.3 |'
- en: 'We also combined MInference with a state-of-the-art KV cache compression method
    SnapKV [[43](#bib.bib43)], as shown in Table [5](#S4.T5 "Table 5 ‣ Integrate with
    KV cache compression methods ‣ 4 Experiments ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"). This proves our method is
    compatible with KV cache compression techniques. For most tasks, performance remains
    nearly unchanged, with the average score even showing a slight increase, which
    further demonstrates the potential practical value of our method as an optimization
    for serving long-context LLMs.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将 MInference 与最先进的 KV 缓存压缩方法 SnapKV [[43](#bib.bib43)] 结合，如表 [5](#S4.T5 "表
    5 ‣ 与 KV 缓存压缩方法的集成 ‣ 4 实验 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 所示。这证明了我们的方法与
    KV 缓存压缩技术兼容。对于大多数任务，性能几乎保持不变，平均分数甚至略有增加，这进一步证明了我们的方法作为长上下文 LLM 服务优化的潜在实用价值。
- en: 5 Related Works
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Sparse Attention
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏注意力
- en: Due to the quadratic complexity of the attention mechanism, many previous works
    have focused on sparse attention to improve the efficiency of Transformers. These
    methods include static sparse patterns, cluster-based sparse approaches, and dynamic
    sparse attention. Static sparse patterns include techniques such as sliding windows [[30](#bib.bib30),
    [2](#bib.bib2)], dilated attention [[8](#bib.bib8), [72](#bib.bib72), [16](#bib.bib16)],
    and mixed sparse patterns [[6](#bib.bib6), [87](#bib.bib87), [38](#bib.bib38)].
    Cluster-based sparse methods include hash-based [[36](#bib.bib36)] and kNN-based [[68](#bib.bib68),
    [54](#bib.bib54)] methods. All of the above methods require pre-training the model
    from scratch, which makes them infeasible to be directly used as a plugin for
    reay-to-use LLMs. Recently, there has been work [[14](#bib.bib14), [85](#bib.bib85)]
    to unify state space models [[23](#bib.bib23), [22](#bib.bib22), [14](#bib.bib14)],
    and linear attention [[37](#bib.bib37), [70](#bib.bib70)] into structured masked
    attention. Additionally, some works [[81](#bib.bib81), [47](#bib.bib47), [63](#bib.bib63)]
    leverage the dynamic nature of attention to predict sparse patterns dynamically.
    However, these approaches often focus on low-rank hidden states during the dynamic
    pattern approximation or use post-statistical methods to obtain the sparse mask,
    which introduce substantial overhead in the estimation step, making them less
    useful for long-context LLMs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力机制的二次复杂性，许多早期工作集中于稀疏注意力以提高 Transformers 的效率。这些方法包括静态稀疏模式、基于簇的稀疏方法和动态稀疏注意力。静态稀疏模式包括滑动窗口[[30](#bib.bib30),
    [2](#bib.bib2)]、膨胀注意力[[8](#bib.bib8), [72](#bib.bib72), [16](#bib.bib16)]和混合稀疏模式[[6](#bib.bib6),
    [87](#bib.bib87), [38](#bib.bib38)]。基于簇的稀疏方法包括基于哈希[[36](#bib.bib36)]和基于 kNN[[68](#bib.bib68),
    [54](#bib.bib54)]的方法。上述所有方法都需要从头开始预训练模型，这使得它们不适合作为现成 LLM 的插件使用。最近，有工作[[14](#bib.bib14),
    [85](#bib.bib85)]试图将状态空间模型[[23](#bib.bib23), [22](#bib.bib22), [14](#bib.bib14)]和线性注意力[[37](#bib.bib37),
    [70](#bib.bib70)]统一为结构化的掩蔽注意力。此外，一些工作[[81](#bib.bib81), [47](#bib.bib47), [63](#bib.bib63)]利用注意力的动态特性来动态预测稀疏模式。然而，这些方法往往关注于动态模式逼近过程中的低秩隐状态，或使用后统计方法获得稀疏掩码，这在估计步骤中引入了大量开销，使其对于长上下文
    LLM 的应用效果不佳。
- en: Scaling Context Windows of LLMs
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 扩展 LLM 的上下文窗口
- en: 'Recent research has focused on expanding the context window of pre-trained
    LLMs, that enables LLMs to handle more complex real-life applications [[34](#bib.bib34),
    [58](#bib.bib58)]. These methods can be categorized into: 1) Staged pre-training [[55](#bib.bib55),
    [20](#bib.bib20)]; 2) Modifying or interpolating position embeddings [[61](#bib.bib61),
    [10](#bib.bib10), [60](#bib.bib60), [19](#bib.bib19)]; 3) Utilizing external memory
    modules for context storage [[4](#bib.bib4), [77](#bib.bib77), [83](#bib.bib83)];
    4) Expanding computations across multiple devices in a distributed manner [[50](#bib.bib50)].
    However, these methods do not alleviate the high inference costs in long-context
    processing.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究集中于扩展预训练 LLM 的上下文窗口，以使 LLM 能够处理更复杂的现实应用[[34](#bib.bib34), [58](#bib.bib58)]。这些方法可以分为：1)
    阶段性预训练[[55](#bib.bib55), [20](#bib.bib20)]; 2) 修改或插值位置嵌入[[61](#bib.bib61), [10](#bib.bib10),
    [60](#bib.bib60), [19](#bib.bib19)]; 3) 利用外部记忆模块进行上下文存储[[4](#bib.bib4), [77](#bib.bib77),
    [83](#bib.bib83)]; 4) 以分布式方式扩展计算至多个设备[[50](#bib.bib50)]。然而，这些方法并未减轻长上下文处理中的高推理成本。
- en: Long-Context LLM Inference
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长上下文 LLM 推理
- en: 'Recent studies [[21](#bib.bib21)] have tackled the high computational cost
    of attention and substantial KV cache storage in long-context scenarios from two
    angles: pre-filling and decoding. Pre-filling optimizations are primarily categorized
    as State Space Models [[23](#bib.bib23), [22](#bib.bib22)], linear attention methods [[70](#bib.bib70),
    [57](#bib.bib57)], memory-based methods [[53](#bib.bib53)], hybrid methods [[44](#bib.bib44),
    [27](#bib.bib27), [64](#bib.bib64)], and prompt compression methods [[41](#bib.bib41),
    [32](#bib.bib32), [33](#bib.bib33), [62](#bib.bib62)]. However, these approaches
    require training from scratch or additional overhead and are difficult to implement
    directly in pretrained long-context LLMs. Recently, some studies [[52](#bib.bib52),
    [83](#bib.bib83)] have focused on using kNN or cluster-based sparse attention
    to accelerate LLM inference. However, these methods often lead to reduced accuracy,
    limited speedup, or are restricted to CPU scenarios.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 [[21](#bib.bib21)] 从两个角度解决了长上下文场景中注意力的高计算成本和大量KV缓存存储问题：预填充和解码。预填充优化主要分为状态空间模型 [[23](#bib.bib23),
    [22](#bib.bib22)]，线性注意力方法 [[70](#bib.bib70), [57](#bib.bib57)]，基于内存的方法 [[53](#bib.bib53)]，混合方法 [[44](#bib.bib44),
    [27](#bib.bib27), [64](#bib.bib64)]，以及提示压缩方法 [[41](#bib.bib41), [32](#bib.bib32),
    [33](#bib.bib33), [62](#bib.bib62)]。然而，这些方法要求从头开始训练或附加开销，难以直接在预训练的长上下文LLM中实施。最近，一些研究 [[52](#bib.bib52),
    [83](#bib.bib83)] 专注于使用kNN或基于集群的稀疏注意力来加速LLM推理。然而，这些方法往往会导致准确性降低、加速有限，或仅限于CPU场景。
- en: 'In contrast, optimizations for the decoding stage are divided into: 1) Reusing
    attention KV to reduce KV cache storage [[73](#bib.bib73), [3](#bib.bib3), [71](#bib.bib71),
    [12](#bib.bib12)]; 2) Static KV cache compression patterns [[82](#bib.bib82),
    [29](#bib.bib29)]; 3) Dynamic KV cache compression patterns, including completely
    discarding the KV cache after compression [[89](#bib.bib89), [42](#bib.bib42),
    [25](#bib.bib25), [56](#bib.bib56)], and offloading-based methods [[63](#bib.bib63),
    [43](#bib.bib43), [15](#bib.bib15)]; 4) Cluster-based KV cache compression methods [[54](#bib.bib54),
    [78](#bib.bib78)]; 5) Methods for restoring performance loss due to KV cache compression [[1](#bib.bib1),
    [18](#bib.bib18)]; 6) Hierarchical speculative decoding methods [[69](#bib.bib69)].
    Nevertheless, these methods do not address the heavy computational burden of the
    attention in the pre-filling stage.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，解码阶段的优化可分为：1）重用注意力KV以减少KV缓存存储 [[73](#bib.bib73), [3](#bib.bib3), [71](#bib.bib71),
    [12](#bib.bib12)]; 2）静态KV缓存压缩模式 [[82](#bib.bib82), [29](#bib.bib29)]; 3）动态KV缓存压缩模式，包括在压缩后完全丢弃KV缓存 [[89](#bib.bib89),
    [42](#bib.bib42), [25](#bib.bib25), [56](#bib.bib56)]，以及基于卸载的方法 [[63](#bib.bib63),
    [43](#bib.bib43), [15](#bib.bib15)]; 4）基于集群的KV缓存压缩方法 [[54](#bib.bib54), [78](#bib.bib78)];
    5）恢复因KV缓存压缩而导致的性能损失的方法 [[1](#bib.bib1), [18](#bib.bib18)]; 6）层次化推测解码方法 [[69](#bib.bib69)]。然而，这些方法并没有解决预填充阶段注意力的高计算负担。
- en: 6 Conclusion
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'This paper addresses the expensive computational cost and the unacceptable
    latency of the attention calculations in the pre-filling stage of long-context
    LLMs. We propose MInference, a method that accelerates the pre-filling stage by
    leveraging dynamic sparse attention with spatial aggregation patterns. Specifically,
    we categorize attention heads into three types: A-shape, Vertical-Slash, and Block-Sparse.
    Using a kernel-aware optimal sparse pattern search method, we identify the optimal
    pattern for each head. Subsequently, we utilize a fast approximation approach
    to build dynamic sparse masks for different inputs, and then apply these mask
    to perform sparse attention calculations. Experimental results on benchmarks such
    as InfiniteBench, RULER, language modeling, and Needle In A Haystack demonstrate
    that our method effectively maintains the long-context capabilities of LLMs while
    achieving up to a 10x speedup, reducing the latency from 30 minutes to 3 minutes
    per prompt for 1 million token prompts on a single A100 GPU. Additionally, we
    have found that similar dynamic sparse attention patterns also exist in both multi-modal
    LLMs [[80](#bib.bib80)] and encoder-decoder LLMs [[66](#bib.bib66)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解决了长上下文LLMs预填充阶段计算成本高和延迟不可接受的问题。我们提出了MInference，这是一种通过利用具有空间聚合模式的动态稀疏注意力来加速预填充阶段的方法。具体而言，我们将注意力头分类为三种类型：A形、垂直斜线和块稀疏。使用内核感知的最优稀疏模式搜索方法，我们为每个头识别出最佳模式。随后，我们使用快速近似方法为不同输入构建动态稀疏掩模，然后应用这些掩模进行稀疏注意力计算。在InfiniteBench、RULER、语言建模和针在干草堆中等基准上的实验结果表明，我们的方法有效地保持了LLMs的长上下文能力，同时实现了最高10倍的加速，将延迟从每个提示30分钟减少到3分钟（对于1百万标记提示，在单个A100
    GPU上）。此外，我们发现类似的动态稀疏注意力模式也存在于多模态LLMs [[80](#bib.bib80)] 和编码器-解码器LLMs [[66](#bib.bib66)]
    中。使用MInference进行预填充阶段推理加速具有很大的潜力。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AAJ^+ [24] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya
    Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key
    tokens selection for efficient generative inference. Proceedings of Machine Learning
    and Systems, 6:114–127, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AAJ^+ [24] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya
    Soloveychik, 和 Purushotham Kamath。Keyformer：通过选择关键令牌减少Kv缓存以提高生成推理的效率。机器学习与系统会议论文集,
    6:114–127, 2024。
- en: 'AJA^+ [24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed
    Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del
    Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek
    Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie
    Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo
    Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen
    Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi,
    Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid
    Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,
    Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning
    Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua
    Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,
    Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna
    Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report:
    A highly capable language model locally on your phone. ArXiv, abs/2404.14219,
    2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AJA^+ [24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed
    Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie
    Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg,
    Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett,
    Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,
    Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi
    Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra,
    Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas
    Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy,
    Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital
    Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel
    Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav,
    Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,
    Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3技术报告：一种在手机上本地运行的高效语言模型。ArXiv,
    abs/2404.14219, 2024。
- en: 'ALTdJ^+ [23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,
    Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ALTdJ^+ [23] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,
    Federico Lebron 和 Sumit Sanghai. Gqa: 从多头检查点训练广义多查询变换器模型。2023年。'
- en: 'BANG [23] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley.
    Unlimiformer: Long-range transformers with unlimited length input. In Thirty-seventh
    Conference on Neural Information Processing Systems, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BANG [23] Amanda Bertsch, Uri Alon, Graham Neubig 和 Matthew R. Gormley. Unlimiformer:
    支持无限长度输入的长程变换器。在第三十七届神经信息处理系统大会，2023年。'
- en: BBC^+ [23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. Qwen technical report. ArXiv preprint, abs/2309.16609,
    2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BBC^+ [23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou 和 Tianhang Zhu. Qwen技术报告。ArXiv预印本，abs/2309.16609，2023年。
- en: 'BPC [20] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. ArXiv preprint, abs/2004.05150, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BPC [20] Iz Beltagy, Matthew E Peters 和 Arman Cohan. Longformer: 长文档变换器。ArXiv预印本，abs/2004.05150，2020年。'
- en: 'BSK^+ [23] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun
    Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok, and Shashank Shet. Codeplan:
    Repository-level coding using LLMs and planning. In NeurIPS 2023 Foundation Models
    for Decision Making Workshop, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BSK^+ [23] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun
    Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok 和 Shashank Shet. Codeplan:
    使用LLMs和规划进行仓库级编码。在NeurIPS 2023决策制定基础模型研讨会，2023年。'
- en: CGRS [19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. ArXiv preprint, abs/1904.10509, 2019.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGRS [19] Rewon Child, Scott Gray, Alec Radford 和 Ilya Sutskever. 使用稀疏变换器生成长序列。ArXiv预印本，abs/1904.10509，2019年。
- en: 'CPG^+ [23] Avi Caciularu, Matthew E Peters, Jacob Goldberger, Ido Dagan, and
    Arman Cohan. Peek across: Improving multi-document modeling via cross-document
    question-answering. In Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 1970–1989, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CPG^+ [23] Avi Caciularu, Matthew E Peters, Jacob Goldberger, Ido Dagan 和 Arman
    Cohan. Peek across: 通过跨文档问答改进多文档建模。在第61届计算语言学协会年会（第1卷：长篇论文）论文集中，第1970–1989页，2023年。'
- en: CWCT [23] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. ArXiv preprint,
    abs/2306.15595, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CWCT [23] Shouyuan Chen, Sherman Wong, Liangjian Chen 和 Yuandong Tian. 通过位置插值扩展大型语言模型的上下文窗口。ArXiv预印本，abs/2306.15595，2023年。
- en: 'CWW^+ [24] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing,
    Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up
    vision foundation models and aligning for generic visual-linguistic tasks. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 24185–24198, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CWW^+ [24] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing,
    Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu 等人. Internvl: 扩大视觉基础模型的规模，并对通用视觉语言任务进行对齐。在IEEE/CVF计算机视觉与模式识别会议论文集中，第24185–24198页，2024年。'
- en: 'DA [24] DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts
    language model, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DA [24] DeepSeek-AI. Deepseek-v2: 一个强大、经济高效的专家混合语言模型，2024年。'
- en: 'Dao [24] Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. In The Twelfth International Conference on Learning Representations,
    2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao [24] Tri Dao. Flashattention-2: 更快的注意力机制，具有更好的并行性和工作分配。在第十二届国际学习表征会议，2024年。'
- en: 'DG [24] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and
    efficient algorithms through structured state space duality. In Forty-first International
    Conference on Machine Learning, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DG [24] Tri Dao 和 Albert Gu. 变换器即SSMs: 通过结构化状态空间对偶性实现广义模型和高效算法。在第41届国际机器学习大会，2024年。'
- en: DHJ^+ [24] Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei
    Bi, and Shuming Shi. Sequence can secretly tell you what to discard. ArXiv preprint,
    abs/2404.15949, 2024.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DHJ^+ [24] Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei
    Bi, 和 Shuming Shi. 序列可以秘密地告诉你该丢弃什么。ArXiv 预印本，abs/2404.15949，2024。
- en: 'DMD^+ [23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang,
    Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. ArXiv preprint, abs/2307.02486, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMD^+ [23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, 和 Furu Wei. Longnet：将变压器扩展到 10 亿个标记。ArXiv 预印本，abs/2307.02486，2023。
- en: DSY [24] Yichuan Deng, Zhao Song, and Chiwun Yang. Attention is naturally sparse
    with gaussian distributed input. ArXiv preprint, abs/2404.02690, 2024.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSY [24] Yichuan Deng, Zhao Song, 和 Chiwun Yang. 注意力自然地稀疏于高斯分布的输入。ArXiv 预印本，abs/2404.02690，2024。
- en: 'DYZ^+ [24] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi,
    and Beidi Chen. Get more with LESS: Synthesizing recurrence with KV cache compression
    for efficient LLM inference. In Forty-first International Conference on Machine
    Learning, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DYZ^+ [24] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi,
    和 Beidi Chen. 用 LESS 获得更多：通过 KV 缓存压缩合成递归以提高 LLM 推理效率。发表于第四十一届国际机器学习会议，2024。
- en: 'DZZ^+ [24] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning
    Shang, Jiahang Xu, Fan Yang, and Mao Yang. LongroPE: Extending LLM context window
    beyond 2 million tokens. In Forty-first International Conference on Machine Learning,
    2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DZZ^+ [24] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning
    Shang, Jiahang Xu, Fan Yang, 和 Mao Yang. LongroPE：将 LLM 上下文窗口扩展到超过 200 万个标记。发表于第四十一届国际机器学习会议，2024。
- en: FPN^+ [24] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context.
    In Forty-first International Conference on Machine Learning, 2024.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPN^+ [24] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, 和 Hao Peng. 数据工程用于将语言模型扩展到 128k 上下文。发表于第四十一届国际机器学习会议，2024。
- en: 'Fu [24] Yao Fu. Challenges in deploying long-context transformers: A theoretical
    peak performance analysis. ArXiv preprint, abs/2405.08944, 2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu [24] Yao Fu. 部署长上下文变压器的挑战：理论峰值性能分析。ArXiv 预印本，abs/2405.08944，2024。
- en: 'GD [23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective
    state spaces. ArXiv preprint, abs/2312.00752, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GD [23] Albert Gu 和 Tri Dao. Mamba：具有选择性状态空间的线性时间序列建模。ArXiv 预印本，abs/2312.00752，2023。
- en: GGR [22] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long
    sequences with structured state spaces. In The Tenth International Conference
    on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GGR [22] Albert Gu, Karan Goel, 和 Christopher Ré. 使用结构化状态空间高效建模长序列。发表于第十届国际学习表征会议，ICLR
    2022，虚拟活动，2022年4月25日至29日，2022。
- en: Gra [24] Gradient. Llama-3 8b instruct gradient 4194k (v0.1), 2024.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gra [24] Gradient. Llama-3 8b 指令梯度 4194k (v0.1)，2024。
- en: 'GZL^+ [24] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and
    Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for
    llms. In The Twelfth International Conference on Learning Representations, 2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GZL^+ [24] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao. 模型告诉你该丢弃什么：针对 LLMS 的自适应 KV 缓存压缩。发表于第十二届国际学习表征会议，2024。
- en: 'GZX^+ [24] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin,
    Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of
    large language models from glm-130b to glm-4 all tools. ArXiv preprint, abs/2406.12793,
    2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GZX^+ [24] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin,
    Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai 等. Chatglm：从 glm-130b 到 glm-4
    全工具的一系列大型语言模型。ArXiv 预印本，abs/2406.12793，2024。
- en: 'HBK^+ [24] Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal
    Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local
    language modeling for fast inference. ArXiv preprint, abs/2406.02657, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBK^+ [24] Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal
    Schuster, Adam Fisch, James Thorne, 和 Se-Young Yun. Block transformer：全球到本地的语言建模以实现快速推理。ArXiv
    预印本，abs/2406.02657，2024。
- en: 'HSK^+ [24] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima
    Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What’s the real context
    size of your long-context language models? ArXiv preprint, abs/2404.06654, 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HSK^+ [24] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima
    Rekesh, Fei Jia, Yang Zhang, 和 Boris Ginsburg. Ruler：你的长上下文语言模型的真实上下文大小是多少？ArXiv
    预印本，abs/2404.06654，2024。
- en: 'HWP^+ [24] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language
    models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of
    the 2024 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3991–4008,
    Mexico City, Mexico, 2024\. Association for Computational Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HWP^+ [24] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji 和 Sinong
    Wang。LM-infinite：大型语言模型的零-shot极端长度泛化。编辑：Kevin Duh, Helena Gomez 和 Steven Bethard，发表于2024年北美计算语言学协会年会：人类语言技术（第1卷：长篇论文）会议论文集，页码3991–4008，墨西哥城，墨西哥，2024年。计算语言学协会。
- en: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. ArXiv preprint, abs/2310.06825,
    2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier 等。Mistral 7b。ArXiv预印本，abs/2310.06825，2023年。
- en: 'JTZ^+ [23] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang,
    Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations
    for enabling training of extreme long sequence transformer models. ArXiv preprint,
    abs/2309.14509, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JTZ^+ [23] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon
    Song, Samyam Rajbhandari 和 Yuxiong He。Deepspeed ulysses：用于极长序列变换器模型训练的系统优化。ArXiv预印本，abs/2309.14509，2023年。
- en: 'JWL^+ [23] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili
    Qiu. Llmlingua: Compressing prompts for accelerated inference of large language
    models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing, pages 13358–13376, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JWL^+ [23] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang 和 Lili Qiu。Llmlingua：压缩提示以加快大型语言模型的推理。发表于2023年自然语言处理实证方法会议论文集，页码13358–13376，2023年。
- en: 'JWL^+ [24] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin,
    Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long
    context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers). Association
    for Computational Linguistics, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JWL^+ [24] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin,
    Yuqing Yang 和 Lili Qiu。Longllmlingua：通过提示压缩加速和增强长上下文场景中的LLMs。发表于第62届计算语言学协会年会（第1卷：长篇论文）。计算语言学协会，2024年。
- en: 'JYW^+ [23] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin
    Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve
    real-world github issues? In The Twelfth International Conference on Learning
    Representations, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JYW^+ [23] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin
    Pei, Ofir Press 和 Karthik R Narasimhan。Swe-bench：语言模型能否解决现实世界的GitHub问题？发表于第十二届国际学习表征会议，2023年。
- en: Kam [23] Greg Kamradt. Needle in a haystack - pressure testing llms, 2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kam [23] Greg Kamradt。大海捞针——对LLMs进行压力测试，2023年。
- en: 'KKL [20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. In 8th International Conference on Learning Representations, ICLR
    2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KKL [20] Nikita Kitaev, Lukasz Kaiser 和 Anselm Levskaya。Reformer：高效的变换器。发表于第八届国际学习表征会议，ICLR
    2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日，2020年。
- en: 'KVPF [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François
    Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.
    In Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
    Research, pages 5156–5165\. PMLR, 2020.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KVPF [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas 和 François Fleuret。变换器是RNN：具有线性注意力的快速自回归变换器。发表于第37届国际机器学习会议，ICML
    2020，2020年7月13-18日，虚拟会议，机器学习研究论文集第119卷，页码5156–5165。PMLR，2020年。
- en: LCSR [21] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush.
    Block pruning for faster transformers. In Proceedings of the 2021 Conference on
    Empirical Methods in Natural Language Processing, pages 10619–10629, Online and
    Punta Cana, Dominican Republic, 2021\. Association for Computational Linguistics.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LCSR [21] François Lagunas, Ella Charlaix, Victor Sanh 和 Alexander Rush。块剪枝以加快变换器的速度。发表于2021年自然语言处理实证方法会议论文集，页码10619–10629，在线及多米尼加共和国蓬塔卡纳，2021年。计算语言学协会。
- en: LCW [21] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On
    the expressive power of self-attention matrices. ArXiv preprint, abs/2106.03764,
    2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LCW [21] Valerii Likhosherstov, Krzysztof Choromanski 和 Adrian Weller。《自注意力矩阵的表达能力》。ArXiv
    预印本，abs/2106.03764，2021。
- en: LCW [23] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On
    the expressive flexibility of self-attention matrices. Proceedings of the AAAI
    Conference on Artificial Intelligence, 37(7):8773–8781, 2023.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LCW [23] Valerii Likhosherstov, Krzysztof Choromanski 和 Adrian Weller。《自注意力矩阵的表达灵活性》。人工智能协会会议论文集，37(7):8773–8781，2023。
- en: LDGL [23] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context
    to enhance inference efficiency of large language models. In Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing, 2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDGL [23] Yucheng Li, Bo Dong, Frank Guerin 和 Chenghua Lin。《压缩上下文以增强大型语言模型的推理效率》。在
    2023 年自然语言处理实证方法会议论文集，2023。
- en: 'LDL^+ [24] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie,
    Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting
    the persistence of importance hypothesis for llm kv cache compression at test
    time. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LDL^+ [24] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie,
    Zhaozhuo Xu, Anastasios Kyrillidis 和 Anshumali Shrivastava。《Scissorhands: 在测试时利用重要性假设的持久性进行
    LLM KV 缓存压缩》。神经信息处理系统进展，36，2024。'
- en: 'LHY^+ [24] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli,
    Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what
    you are looking for before generation. ArXiv preprint, abs/2404.14469, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LHY^+ [24] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli,
    Hanchen Ye, Tianle Cai, Patrick Lewis 和 Deming Chen。《Snapkv: LLM 在生成前知道你在寻找什么》。ArXiv
    预印本，abs/2404.14469，2024。'
- en: 'LLB^+ [24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin,
    Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz,
    et al. Jamba: A hybrid transformer-mamba language model. ArXiv preprint, abs/2403.19887,
    2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LLB^+ [24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin,
    Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz
    等。《Jamba: 一种混合型变压器-蟒蛇语言模型》。ArXiv 预印本，abs/2403.19887，2024。'
- en: LLWL [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. Advances in neural information processing systems, 36, 2024.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLWL [24] Haotian Liu, Chunyuan Li, Qingyang Wu 和 Yong Jae Lee。视觉指令调优。《神经信息处理系统进展》，36，2024。
- en: 'LMZ^+ [24] Zhiqi Lin, Youshan Miao, Quanlu Zhang, Fan Yang, Yi Zhu, Cheng Li,
    Saeed Maleki, Xu Cao, Ning Shang, Yilei Yang, Weijiang Xu, Mao Yang, Lintao Zhang,
    and Lidong Zhou. nnscaler: Constraint-guided parallelization plan generation for
    deep learning training. In 18th USENIX Symposium on Operating Systems Design and
    Implementation (OSDI 24). USENIX Association, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LMZ^+ [24] Zhiqi Lin, Youshan Miao, Quanlu Zhang, Fan Yang, Yi Zhu, Cheng Li,
    Saeed Maleki, Xu Cao, Ning Shang, Yilei Yang, Weijiang Xu, Mao Yang, Lintao Zhang
    和 Lidong Zhou。《nnscaler: 针对深度学习训练的约束引导并行化计划生成》。在第18届 USENIX 操作系统设计与实现研讨会（OSDI
    24）。USENIX 协会，2024。'
- en: LQC^+ [22] Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding, and Yuan
    Xie. Dynamic sparse attention for scalable transformer acceleration. IEEE Transactions
    on Computers, 71(12):3165–3178, 2022.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LQC^+ [22] Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding 和 Yuan Xie。《用于可扩展变压器加速的动态稀疏注意力》。IEEE
    计算机学报，71(12):3165–3178，2022。
- en: 'LWD^+ [23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi
    Chen. Deja vu: Contextual sparsity for efficient LLMs at inference time. In Andreas
    Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
    Scarlett, editors, Proceedings of the 40th International Conference on Machine
    Learning, Proceedings of Machine Learning Research. PMLR, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LWD^+ [23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re 和 Beidi Chen。《Deja
    vu: 在推理时针对高效 LLM 的上下文稀疏性》。在 Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
    Engelhardt, Sivan Sabato 和 Jonathan Scarlett 编辑的《第40届国际机器学习会议论文集》，机器学习研究论文集。PMLR，2023。'
- en: LYZA [24] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model
    on million-length video and language with ringattention. ArXiv preprint, abs/2402.08268,
    2024.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LYZA [24] Hao Liu, Wilson Yan, Matei Zaharia 和 Pieter Abbeel。《具有环形注意力的百万长度视频和语言的世界模型》。ArXiv
    预印本，abs/2402.08268，2024。
- en: LZA [23] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise
    transformers for near-infinite context. In NeurIPS 2023 Foundation Models for
    Decision Making Workshop, 2023.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LZA [23] Hao Liu, Matei Zaharia 和 Pieter Abbeel。《用于近无限上下文的环形注意力与块状变压器》。在 NeurIPS
    2023 决策制定基础模型研讨会，2023。
- en: LZD^+ [24] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context
    llms struggle with long in-context learning. ArXiv preprint, abs/2404.02060, 2024.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LZD^+ [24] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue 和 Wenhu Chen。长上下文 LLMs
    在长时间上下文学习中表现挣扎。《ArXiv 预印本》，abs/2404.02060，2024年。
- en: 'MEL [24] Yuzhen Mao, Martin Ester, and Ke Li. Iceformer: Accelerated inference
    with long-sequence transformers on CPUs. In The Twelfth International Conference
    on Learning Representations, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MEL [24] Yuzhen Mao, Martin Ester 和 Ke Li。Iceformer：在 CPU 上加速长序列变换器的推理。第十二届国际学习表征会议，2024年。
- en: 'MFG [24] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave
    no context behind: Efficient infinite context transformers with infini-attention.
    ArXiv preprint, abs/2404.07143, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MFG [24] Tsendsuren Munkhdalai, Manaal Faruqui 和 Siddharth Gopal。无遗留上下文：具有无限注意力的高效无限上下文变换器。《ArXiv
    预印本》，abs/2404.07143，2024年。
- en: 'NŁC^+ [24] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan,
    and Edoardo Ponti. Dynamic memory compression: Retrofitting LLMs for accelerated
    inference. In Forty-first International Conference on Machine Learning, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NŁC^+ [24] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan 和
    Edoardo Ponti。动态内存压缩：为加速推理改造 LLMs。第41届国际机器学习会议，2024年。
- en: NXH^+ [23] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen
    Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report. ArXiv preprint,
    abs/2309.03450, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NXH^+ [23] Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen
    Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty 和 Caiming Xiong。Xgen-7b 技术报告。《ArXiv 预印本》，abs/2309.03450，2023年。
- en: OHAS [24] Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers
    are multi-state rnns. ArXiv preprint, abs/2401.06104, 2024.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OHAS [24] Matanel Oren, Michael Hassid, Yossi Adi 和 Roy Schwartz。变换器是多状态 RNN。《ArXiv
    预印本》，abs/2401.06104，2024年。
- en: 'PAA^+ [23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian
    Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan
    Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit
    Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,
    Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV:
    Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika
    Bali, editors, Findings of the Association for Computational Linguistics: EMNLP
    2023, pages 14048–14077, Singapore, 2023\. Association for Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PAA^+ [23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian
    Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan
    Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit
    Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,
    Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu 和 Rui-Jie Zhu。RWKV：为变换器时代重新发明
    RNN。《Houda Bouamor, Juan Pino 和 Kalika Bali（编辑）》，《计算语言学协会发现：EMNLP 2023》，第14048–14077页，新加坡，2023年。计算语言学协会。
- en: 'POC^+ [23] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User
    Interface Software and Technology, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POC^+ [23] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S. Bernstein。生成代理：人类行为的交互式模拟体。第36届年度 ACM 用户界面软件与技术研讨会论文集，2023年。
- en: PPJF [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret.
    Fast attention over long sequences with dynamic sparse flash attention. Advances
    in Neural Information Processing Systems, 36, 2024.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPJF [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi 和 François Fleuret。动态稀疏闪存注意力的长序列快速处理。《神经信息处理系统进展》，第36卷，2024年。
- en: 'PQFS [24] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn:
    Efficient context window extension of large language models. In The Twelfth International
    Conference on Learning Representations, 2024.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PQFS [24] Bowen Peng, Jeffrey Quesnelle, Honglu Fan 和 Enrico Shippole。Yarn：大语言模型的高效上下文窗口扩展。第十二届国际学习表征会议，2024年。
- en: 'PSL [22] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long:
    Attention with linear biases enables input length extrapolation. In The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29, 2022, 2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSL [22] Ofir Press, Noah A. Smith, 和 Mike Lewis。短训练，长测试：具有线性偏置的注意力使输入长度外推成为可能。见第十届国际学习表征会议，ICLR
    2022，虚拟活动，2022年4月25-29日，2022。
- en: 'PWJ^+ [24] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo,
    Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2:
    Data distillation for efficient and faithful task-agnostic prompt compression.
    In Findings of the Association for Computational Linguistics: ACL 2024. Association
    for Computational Linguistics, 2024.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PWJ^+ [24] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo,
    Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin 等。Llmlingua-2：用于高效和忠实任务无关提示压缩的数据提炼。见《计算语言学协会发现：ACL
    2024》。计算语言学协会，2024。
- en: 'RCHG^+ [24] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
    Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient LLM inference.
    In Forty-first International Conference on Machine Learning, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RCHG^+ [24] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
    Carlo Luschi, 和 Douglas Orr。Sparq attention：带宽高效的 LLM 推断。见《第四十一届国际机器学习会议》，2024。
- en: 'RLL^+ [24] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu
    Chen. Samba: Simple hybrid state space models for efficient unlimited context
    language modeling. ArXiv preprint, abs/2406.07522, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLL^+ [24] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, 和 Weizhu
    Chen。Samba：用于高效无限上下文语言建模的简单混合状态空间模型。ArXiv 预印本，abs/2406.07522，2024。
- en: RPJ^+ [20] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier,
    and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling.
    In 8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020, 2020.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RPJ^+ [20] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier,
    和 Timothy P. Lillicrap。压缩变压器用于长范围序列建模。见第八届国际学习表征会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日，2020。
- en: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu。探索统一的文本到文本变压器的迁移学习极限。《机器学习研究杂志》，21(140)：1-67，2020。
- en: 'RST^+ [24] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RST^+ [24] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser 等。Gemini 1.5：解锁跨越数百万标记上下文的多模态理解。ArXiv 预印本，abs/2403.05530，2024。
- en: RSVG [21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient
    content-based sparse attention with routing transformers. Transactions of the
    Association for Computational Linguistics, 9:53–68, 2021.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSVG [21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, 和 David Grangier。基于内容的稀疏注意力的高效路由变压器。
    《计算语言学协会学报》，9：53-68，2021。
- en: 'SCY^+ [24] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi
    Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical
    speculative decoding. ArXiv preprint, abs/2404.11912, 2024.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCY^+ [24] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, 和 Beidi Chen。Triforce：具有分层预测解码的无损加速长序列生成。ArXiv
    预印本，abs/2404.11912，2024。
- en: 'SDH^+ [23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer
    for large language models. ArXiv preprint, abs/2307.08621, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDH^+ [23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang, 和 Furu Wei。Retentive network：大语言模型的变压器继任者。ArXiv 预印本，abs/2307.08621，2023。
- en: 'SDZ^+ [24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming
    Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder
    architectures for language models. ArXiv preprint, abs/2405.05254, 2024.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDZ^+ [24] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma,
    Quanlu Zhang, Jianyong Wang, 和 Furu Wei。你只缓存一次：用于语言模型的解码器-解码器架构。ArXiv 预印本，abs/2405.05254，2024。
- en: 'SGR^+ [21] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo
    Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in
    self-attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th
    International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
    Event, volume 139 of Proceedings of Machine Learning Research, pages 9547–9557\.
    PMLR, 2021.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGR^+ [21] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo
    Li, 和 James Tin-Yau Kwok. Sparsebert：重新思考自注意力中的重要性分析。发表于Marina Meila 和 Tong Zhang
    编辑的第38届国际机器学习会议论文集，ICML 2021，2021年7月18-24日，虚拟会议，机器学习研究论文集第139卷，页码 9547–9557。PMLR，2021年。
- en: 'Sha [19] Noam Shazeer. Fast transformer decoding: One write-head is all you
    need. ArXiv preprint, abs/1911.02150, 2019.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sha [19] Noam Shazeer. 快速变压器解码：一个写头就足够了。ArXiv 预印本，abs/1911.02150，2019年。
- en: 'TDT^+ [23] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei,
    Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou,
    Neil Houlsby, and Donald Metzler. UL2: Unifying language learning paradigms. In
    The Eleventh International Conference on Learning Representations, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TDT^+ [23] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei,
    Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou,
    Neil Houlsby, 和 Donald Metzler. UL2：统一语言学习范式。发表于第十一届国际学习表征会议，2023年。
- en: 'TKC [19] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate
    language and compiler for tiled neural network computations. In Proceedings of
    the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages, pages 10–19, 2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TKC [19] Philippe Tillet, Hsiang-Tsung Kung, 和 David Cox. Triton：用于平铺神经网络计算的中间语言和编译器。发表于第3届ACM
    SIGPLAN国际机器学习与编程语言研讨会，页码 10–19，2019年。
- en: tri [23] Triton implementation of the flash attention v2 algorithm. Technical
    report, OpenAI, 2023.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tri [23] Flash Attention v2 算法的 Triton 实现。技术报告，OpenAI，2023年。
- en: 'TSP^+ [23] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu,
    Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training
    for context scaling. In Thirty-seventh Conference on Neural Information Processing
    Systems, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TSP^+ [23] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu,
    Henryk Michalewski, 和 Piotr Miłoś. 专注变压器：用于上下文扩展的对比训练。发表于第37届神经信息处理系统会议，2023年。
- en: 'TZZ^+ [24] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci,
    and Song Han. QUEST: Query-aware sparsity for efficient long-context LLM inference.
    In Forty-first International Conference on Machine Learning, 2024.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TZZ^+ [24] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci,
    和 Song Han. QUEST：用于高效长上下文 LLM 推理的查询感知稀疏性。发表于第41届国际机器学习会议，2024年。
- en: Wen [23] Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, 2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen [23] Lilian Weng. LLM 驱动的自主代理。lilianweng.github.io，2023年。
- en: 'WWL^+ [24] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng
    Jin, Longyue Wang, and Li Yuan. Look-m: Look-once optimization in kv cache for
    efficient multimodal long-context inference. ArXiv preprint, abs/2406.18139, 2024.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WWL^+ [24] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin,
    Longyue Wang, 和 Li Yuan. Look-m：用于高效多模态长上下文推理的 KV 缓存中的一次优化。ArXiv 预印本，abs/2406.18139，2024年。
- en: 'WZH [21] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse
    attention architecture with cascade token and head pruning. In 2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA), pages 97–110\. IEEE,
    2021.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WZH [21] Hanrui Wang, Zhekai Zhang, 和 Song Han. Spatten：具有级联标记和头部剪枝的高效稀疏注意力架构。发表于2021年IEEE国际高性能计算架构研讨会（HPCA），页码
    97–110。IEEE，2021年。
- en: XTC^+ [24] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
    Efficient streaming language models with attention sinks. In The Twelfth International
    Conference on Learning Representations, 2024.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XTC^+ [24] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis.
    具有注意力汇的高效流式语言模型。发表于第十二届国际学习表征会议，2024年。
- en: 'XZH^+ [24] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin,
    Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the
    intrinsic capacity of llms for understanding extremely long sequences with training-free
    memory. ArXiv preprint, abs/2402.04617, 2024.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XZH^+ [24] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan
    Zhang, Zhiyuan Liu, Song Han, 和 Maosong Sun. Infllm：揭示 llms 理解极长序列的内在能力，且无需训练。ArXiv
    预印本，abs/2402.04617，2024年。
- en: 'YCL^+ [24] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei
    Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation
    models by 01\. ai. ArXiv preprint, abs/2403.04652, 2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YCL^+ [24] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang,
    Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang，等。Yi：由 01\. ai 提供的开放基础模型。ArXiv
    预印本，abs/2403.04652，2024年。
- en: ZAW [24] Itamar Zimerman, Ameen Ali, and Lior Wolf. A unified implicit attention
    formulation for gated-linear recurrent sequence models. ArXiv preprint, abs/2405.16504,
    2024.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZAW [24] Itamar Zimerman, Ameen Ali 和 Lior Wolf. 针对门控线性递归序列模型的统一隐式注意力公式. ArXiv
    预印本, abs/2405.16504, 2024。
- en: 'ZCH^+ [24] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen,
    Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. $\infty$bench:
    Extending long context evaluation beyond 100k tokens. ArXiv preprint, abs/2402.13718,
    2024.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ZCH^+ [24] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen,
    Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu 等. $\infty$bench:
    将长上下文评估扩展至 100k 以上的标记. ArXiv 预印本, abs/2402.13718, 2024。'
- en: 'ZGD^+ [20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
    Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
    and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
    editors, Advances in Neural Information Processing Systems 33: Annual Conference
    on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ZGD^+ [20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
    Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
    和 Amr Ahmed. Big bird: Transformers for longer sequences. 在 Hugo Larochelle, Marc’Aurelio
    Ranzato, Raia Hadsell, Maria-Florina Balcan 和 Hsuan-Tien Lin 主编的《神经信息处理系统进展 33:
    神经信息处理系统年度会议 2020》, NeurIPS 2020, 2020年12月6-12日, 虚拟, 2020。'
- en: 'ZJZ^+ [23] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao
    Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, et al. Pit:
    Optimization of dynamic sparse deep learning models via permutation invariant
    transformation. In Proceedings of the 29th Symposium on Operating Systems Principles,
    pages 331–347, 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ZJZ^+ [23] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao
    Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang 等. Pit: 通过置换不变变换优化动态稀疏深度学习模型.
    在第29届操作系统原理研讨会论文集中, 页 331–347, 2023。'
- en: 'ZSZ^+ [24] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng,
    Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o:
    Heavy-hitter oracle for efficient generative inference of large language models.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ZSZ^+ [24] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng,
    Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett 等. H2o: 高效生成推理大语言模型的重型挑战者.
    神经信息处理系统进展, 36, 2024。'
- en: Appendix A Limitations
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 局限性
- en: As the context length decreases, the time required to build the dynamic index
    becomes more significant as attention computation time decreases. For example,
    with a 10k context, the time spent on building the index increases from 5% to
    30%, resulting in overall end-to-end latency approaching that of FlashAttention.
    However, this overhead proportion gradually decreases as the prompt lengthens.
    Additionally, when using a higher sparsity rate, the model performance may noticeably
    decline.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 随着上下文长度的减少，构建动态索引所需的时间变得更为重要，因为注意力计算时间减少。例如，对于一个 10k 上下文，构建索引所花费的时间从 5% 增加到
    30%，导致整体的端到端延迟接近 FlashAttention 的水平。然而，随着提示长度的增加，这一开销比例逐渐减少。此外，当使用较高的稀疏率时，模型性能可能会明显下降。
- en: Appendix B Broader Impacts
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更广泛的影响
- en: MInference effectively accelerates the inference of long-context LLMs, facilitating
    their deployment and application. By enabling lower latency, it can reduce the
    deployment costs of LLMs, especially for long-context LLMs, helping to democratize
    access to advanced AI. It also promotes further research and development in related
    fields.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: MInference 有效加速了长上下文 LLM 的推理，促进了其部署和应用。通过实现更低的延迟，它可以降低 LLM 的部署成本，尤其是对于长上下文 LLM，帮助普及先进的
    AI。它还促进了相关领域的进一步研究和发展。
- en: Appendix C Experiment Details
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 实验详细信息
- en: C.1 Dataset Details
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 数据集详细信息
- en: InfiniteBench [[86](#bib.bib86)]
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: InfiniteBench [[86](#bib.bib86)]
- en: includes 10 tasks designed to test various aspects of long-context processing.
    Specifically, these tasks cover entire novel summarization, open-form question
    answering based on novels, multiple-choice question answering on novels, question
    answering on long drama scripts, question answering on Chinese texts, debugging
    large code repositories, identifying the largest/smallest number in arrays, and
    retrieval tasks with varying pattern lengths. The average token length for these
    tasks is 214k, and they include 3,992 examples.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 包含 10 个任务，旨在测试长文本处理的各种方面。具体而言，这些任务涵盖了整个小说总结、基于小说的开放式问答、小说中的多项选择问答、长剧本中的问答、中文文本中的问答、大型代码库的调试、数组中最大/最小数的识别，以及具有不同模式长度的检索任务。这些任务的平均标记长度为
    214k，并包括 3,992 个示例。
- en: RULER [[28](#bib.bib28)]
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RULER [[28](#bib.bib28)]
- en: is a recent synthetic benchmark suite for long-context evaluation with 13 complex
    tasks across four categories. The retrieval category includes Single Needle-in-a-Haystack
    (S-NIAH), where a single key-value pair is inserted into noisy text, and the model
    must retrieve the value. Multi-keys Needle-in-a-Haystack (MK-NIAH) involves multiple
    keys, and the model retrieves one specific value among hard distractors. The Multi-values
    Needle-in-a-Haystack (MV-NIAH) task requires retrieving all values associated
    with a single key, while the Multi-queries Needle-in-a-Haystack (MQ-NIAH) task
    involves retrieving values for multiple keys. The Multi-hop Tracing category includes
    Variable Tracking (VT), where the model traces and returns all variable names
    pointing to the same value through variable bindings. The aggregation category
    introduces Common Words Extraction (CWE), where the model identifies the top-K
    common words from a mixture of common and uncommon words, and Frequent Words Extraction
    (FWE), where the model identifies the most frequent words from a Zeta distribution.
    The Question Answering (QA) category extends existing short-context QA datasets
    by adding distracting paragraphs, challenging the model to answer questions based
    on relevant information surrounded by distractors. These tasks provide a comprehensive
    evaluation of long-context modeling capabilities, covering multi-hop reasoning,
    aggregation, and complex question answering. Following [[28](#bib.bib28)], we
    test models on 4K, 8K, 16K, 32K, 64K, and 128K context lengths, including 2,600
    examples per length.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个最新的综合基准测试套件，用于长文本上下文评估，涵盖四个类别中的13个复杂任务。检索类别包括单一的 Needle-in-a-Haystack (S-NIAH)，其中一个键值对被插入到噪声文本中，模型必须检索该值。多键
    Needle-in-a-Haystack (MK-NIAH) 涉及多个键，模型在难以区分的干扰项中检索一个特定的值。Multi-values Needle-in-a-Haystack
    (MV-NIAH) 任务要求检索与单个键相关的所有值，而 Multi-queries Needle-in-a-Haystack (MQ-NIAH) 任务涉及为多个键检索值。Multi-hop
    Tracing 类别包括 Variable Tracking (VT)，模型需要追踪并返回指向相同值的所有变量名。聚合类别引入了 Common Words
    Extraction (CWE)，模型从常见和不常见的词混合中识别前K个常见词，以及 Frequent Words Extraction (FWE)，模型从
    Zeta 分布中识别最频繁的词。这些任务为长文本建模能力提供了全面的评估，涵盖了多跳推理、聚合和复杂的问答任务。参见 [[28](#bib.bib28)]，我们在
    4K、8K、16K、32K、64K 和 128K 上下文长度上测试模型，每个长度包括 2,600 个示例。
- en: Needle In A Haystack task [[35](#bib.bib35)]
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Needle In A Haystack 任务 [[35](#bib.bib35)]
- en: evaluates the performance of retrieval-augmented generation (RAG) systems by
    embedding specific, targeted information (the "needle") within a large, complex
    body of text (the "haystack"). The test assesses a language model’s ability to
    identify and utilize this specific piece of information amidst a vast amount of
    data. Both RULER and the needle test iterate over various context lengths and
    document depths (where the ground-truth is placed in the prompt) to measure the
    long-context performance. Here we scale the Needle In A Haystack task to 1M context
    length, including 750 examples.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将特定的、针对性的信息（“针”）嵌入到大量复杂文本（“干草堆”）中，评估检索增强生成（RAG）系统的性能。该测试评估语言模型在大量数据中识别和利用该特定信息的能力。RULER
    和针测试在各种上下文长度和文档深度（真实答案放置在提示中）上迭代，以测量长文本上下文的性能。在这里，我们将 Needle In A Haystack 任务扩展到
    1M 上下文长度，包括 750 个示例。
- en: PG-19 [[65](#bib.bib65)]
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PG-19 [[65](#bib.bib65)]
- en: The perplexity on long text is also often used by researchers to evaluate the
    language modeling performance of long-context LLMs. PG-19 is a suitable test set
    for this task, as it includes texts as long as 500K tokens. Perplexity is used
    as the metric indicating how well a model predicts the next token in a sequence.
    Our experiments are conducted on 1,000 random samples from PG-19 that are longer
    than 100K tokens.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对长文本的困惑度也常被研究人员用来评估长上下文LLM的语言建模性能。PG-19是一个适合此任务的测试集，因为它包含了长达500K标记的文本。困惑度作为一个指标，用来表示模型在序列中预测下一个标记的准确性。我们的实验是在PG-19中随机抽取的1,000个超过100K标记的样本上进行的。
- en: C.2 Additional Implementation Details
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 额外实现细节
- en: 'Our experiments are based on a number of state-of-the-art long-context LLMs:
    1) LLaMA-3-8B-Instruct-262k³³3https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k
    is a LLaMA-3 variant with further NTK-aware interpolation and minimal fine-tuning
    with Ring Attention, which achieved SOTA results on long-context assessments such
    as the Needle In A Haystack test; 2) LLaMA-3-8B-Instruct-1048k⁴⁴4https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k
    is similar to LLaMA-3-8B-Instruct-262k, but supports context lengths up to 1M
    tokens; 3) Yi-9B-200K [[84](#bib.bib84)] is a SOTA LLM that balances long-context
    performance with general capabilities; 4) Phi-3-Mini-128K [[2](#bib.bib2)] a small
    but powerful language model that offers capabilities equivalent to models ten
    times its size with up to 128K context window powered by LongRoPE [[19](#bib.bib19)];
    5) Qwen2-7B-128K [[5](#bib.bib5)] is a recently release update of Qwen series
    model with up to 128K context window that achieve superior or comparable performance
    compared to LLaMA-3; 6) GLM-4-9B-1M [[26](#bib.bib26)] has been improved from
    its predecessor in terms of a 1M context window, performance on downstream tasks
    and inference efficiency. To guarantee stable results, we use greedy decoding
    in all tests. Our kernel implementations are developed and optimized based on
    the dynamic sparse compiler PIT [[88](#bib.bib88)] in the Triton language [[75](#bib.bib75)].
    The latency experiments are done on a single Nvidia A100 GPU using bfloat16. We
    provide a simple custom implementation of attention in PyTorch, building on FlashAttention
    and Triton.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验基于多个最先进的长上下文LLM：1）LLaMA-3-8B-Instruct-262k³³3https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k
    是一个LLaMA-3变体，具有进一步的NTK感知插值和通过Ring Attention的最小微调， 在 Needle In A Haystack 测试等长上下文评估中取得了SOTA结果；2）LLaMA-3-8B-Instruct-1048k⁴⁴4https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k
    与LLaMA-3-8B-Instruct-262k类似，但支持最长达1M标记的上下文；3）Yi-9B-200K [[84](#bib.bib84)] 是一个SOTA
    LLM，兼顾了长上下文性能和一般能力；4）Phi-3-Mini-128K [[2](#bib.bib2)] 是一个小型但强大的语言模型，提供了相当于十倍于其大小的模型的能力，支持最长128K上下文窗口，由LongRoPE
    [[19](#bib.bib19)] 提供支持；5）Qwen2-7B-128K [[5](#bib.bib5)] 是Qwen系列模型的最新版本，支持最长128K上下文窗口，在性能上优于或可比LLaMA-3；6）GLM-4-9B-1M
    [[26](#bib.bib26)] 在1M上下文窗口、下游任务性能和推理效率方面相较前代有所提升。为了确保稳定结果，我们在所有测试中使用贪婪解码。我们的内核实现是基于Triton语言
    [[75](#bib.bib75)] 中的动态稀疏编译器PIT [[88](#bib.bib88)] 开发和优化的。延迟实验是在单个Nvidia A100
    GPU上使用bfloat16完成的。我们提供了一个基于FlashAttention和Triton的PyTorch自定义注意力实现。
- en: 'We set the target FLOPs $t$ to be the same as 1k global tokens and 4k local
    window tokens in the A-shape pattern. The step size of ChangeSpace is set to 50,
    with the corresponding search space shown in Table [6](#A3.T6 "Table 6 ‣ C.2 Additional
    Implementation Details ‣ Appendix C Experiment Details ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Additionally,
    we use only one sample as our validation set from KV retrieval synthetic data
    with 30k token inputs, which exhibits strong generalization and stability across
    different lengths and domains. The search time is approximately 15 minutes on
    a single A100. Additionally, we use the same optimal sparse pattern configuration
    for both the LLaMA-3-8B-Instruct-262K model and the LLaMA-3-8B-Instruct-1M model.
    The specific distribution is shown in Fig. [11](#A5.F11 "Figure 11 ‣ Appendix
    E Pattern Distribution ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention").'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将目标FLOPs $t$ 设置为与A形状模式中的1k全局令牌和4k局部窗口令牌相同。ChangeSpace的步长设置为50，对应的搜索空间见表[6](#A3.T6
    "表6 ‣ C.2 额外实施细节 ‣ 附录C 实验细节 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")。此外，我们仅使用一个样本作为来自30k令牌输入的KV检索合成数据的验证集，这些数据在不同长度和领域中展示了强大的泛化性和稳定性。搜索时间在单个A100上约为15分钟。此外，我们对LLaMA-3-8B-Instruct-262K模型和LLaMA-3-8B-Instruct-1M模型使用相同的最优稀疏模式配置。具体分布见图[11](#A5.F11
    "图11 ‣ 附录E 模式分布 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")。
- en: 'Table 6: Kernal-aware optimal head pattern search space. In this context, A-shape
    represents the global tokens and local window number, Vertical-Slash represents
    the Top-K number of vertical and diagonal lines, and Block-Sparse represents the
    Top-K number of blocks retained.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：内核感知最优头部模式搜索空间。在此上下文中，A形状表示全局令牌和局部窗口数量，Vertical-Slash表示垂直和对角线的Top-K数量，Block-Sparse表示保留的Top-K块数量。
- en: '| Patterns | Search Space |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 搜索空间 |'
- en: '| A-shape | $\{(1024,4096)\}$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| A-shape | $\{(1024,4096)\}$ |'
- en: '| Vertical-Slash | $\{(30,2048),(100,1800),(500,1500),(3000,200)\}$ |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Vertical-Slash | $\{(30,2048),(100,1800),(500,1500),(3000,200)\}$ |'
- en: '| Block-Sparse | $\{100\}$ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Block-Sparse | $\{100\}$ |'
- en: C.3 Single A100 Implementation Details
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 单个A100实施细节
- en: 'The original PyTorch implementation⁵⁵5https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py
    of the LLaMA model causes an out-of-memory error on a single A100 (80G) when the
    prompt exceeds 50k tokens. To enable running 1M prompt inference on a single A100,
    we implemented the following optimizations:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA模型的原始PyTorch实现⁵⁵5https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py
    在提示超过50k令牌时会导致单个A100（80G）上的内存不足错误。为了在单个A100上进行1M提示推理，我们实施了以下优化：
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Tensor Splitting: We split the Attention by head and the MLP by sequence dimension.
    In long-context scenarios, where computation is the bottleneck, this splitting
    keeps GPU utilization at 100%, and the overhead of splitting is negligible;'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 张量拆分：我们通过头部拆分注意力，通过序列维度拆分MLP。在计算是瓶颈的长上下文场景中，这种拆分保持GPU利用率在100%，而拆分的开销可以忽略不计；
- en: '2.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Reduction of Intermediate Variables: We minimized intermediate variable allocation
    by removing the attention mask and implementing causal mask logic directly within
    the kernel;'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少中间变量：通过移除注意力掩码并直接在内核中实现因果掩码逻辑，我们最小化了中间变量分配；
- en: '3.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Elimination of Unnecessary Computations: In long-context scenarios, only the
    logits corresponding to the last token in the prompt phase are meaningful. Thus,
    we only retain the computation of the LM Head Linear layer for the last token.'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消除不必要的计算：在长上下文场景中，仅与提示阶段最后一个令牌对应的logits是有意义的。因此，我们仅保留对最后一个令牌的LM Head Linear层的计算。
- en: C.4 Kernel Implementation
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 内核实现
- en: C.4.1 Block-Sparse Flash Attention
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.4.1 Block-Sparse Flash Attention
- en: Our Block-Sparse kernel implementation is based on the Triton version of the
    FlashAttention kernel [[76](#bib.bib76)]. With the selected block index as an
    additional input, each thread block loops through the top-K blocks in a row. As
    discussed in FlashAttention [[13](#bib.bib13)], the latency of the block-sparse
    FlashAttention kernel is linearly related to the number of blocks, and the speedup
    ratio (compared to the dense FlashAttention kernel) is approximately as,
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Block-Sparse 内核实现基于 Triton 版本的 FlashAttention 内核 [[76](#bib.bib76)]。通过选择的块索引作为额外输入，每个线程块循环遍历一行中的
    top-K 块。正如 FlashAttention [[13](#bib.bib13)] 中所讨论的，块稀疏 FlashAttention 内核的延迟与块的数量线性相关，速度提升比率（与密集
    FlashAttention 内核相比）大致为，
- en: '|  | $s_{p}=\frac{S}{2B\times k_{b}}$ |  | (3) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{p}=\frac{S}{2B\times k_{b}}$ |  | (3) |'
- en: C.4.2 Vertical-Slash Attention
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.4.2 Vertical-Slash 注意力
- en: 'The Vertical-Slash attention includes two custom kernels: the Vertical-Slash
    sparse index kernel and the Vertical-Slash sparse FlashAttention kernel.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Vertical-Slash 注意力包括两个自定义内核：Vertical-Slash 稀疏索引内核和 Vertical-Slash 稀疏 FlashAttention
    内核。
- en: '![Refer to caption](img/ff009019a243e049587bc52e961c7e21.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ff009019a243e049587bc52e961c7e21.png)'
- en: 'Figure 7: The dynamic sparse mask for the vertical-slash pattern using LLaMA-3-8B
    in the summarization task [[86](#bib.bib86)]. Yellow areas indicate the computed
    parts. Slash lines use $64\times 64$ blocks.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在总结任务中使用 LLaMA-3-8B 的垂直斜线模式的动态稀疏掩码 [[86](#bib.bib86)]。黄色区域表示已计算的部分。斜线使用
    $64\times 64$ 块。
- en: 'The Vertical-Slash sparse index kernel in Algorithm [4](#alg4 "Algorithm 4
    ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") builds the index for each row of blocks. Since
    a slash line segment can be masked by a square block, our attention mask is a
    mix of blocks and columns, as shown in Fig. [7](#A3.F7 "Figure 7 ‣ C.4.2 Vertical-Slash
    Attention ‣ C.4 Kernel Implementation ‣ Appendix C Experiment Details ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    We apply a point-range two-way merge algorithm where vertical indexes are treated
    as points and slash indexes are converted to ranges given the row index. The output
    consists of two parts: merged ranges and separate column indexes, where the ranges
    are represented by block indexes. The time complexity to build an index for a
    row is $O(k_{v}+k_{s})$.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [4](#alg4 "算法 4 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 中的
    Vertical-Slash 稀疏索引内核构建每行块的索引。由于斜线段可以被方块屏蔽，我们的注意力掩码是块和列的混合，如图 [7](#A3.F7 "图 7
    ‣ C.4.2 Vertical-Slash 注意力 ‣ C.4 内核实现 ‣ 附录 C 实验细节 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLM 的预填充") 所示。我们应用了一种点范围双向合并算法，其中垂直索引被视为点，斜线索引根据行索引转换为范围。输出由两个部分组成：合并的范围和单独的列索引，其中范围由块索引表示。为一行构建索引的时间复杂度为
    $O(k_{v}+k_{s})$。
- en: 'The Vertical-Slash sparse FlashAttention kernel in Algorithm [5](#alg5 "Algorithm
    5 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") is a mix of the block-sparse attention kernel
    and the PIT [[88](#bib.bib88)] sparse attention kernel. PIT is a technology that
    loads sparse data into dense compute blocks via a Permutation Invariant Transformation.
    A thread block first loops through the block indexes as described in the previous
    section (block part) and then loops through the column indexes grouped by block
    size (PIT part). The latency of this hybrid kernel is linearly related to the
    total area of blocks and columns.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [5](#alg5 "算法 5 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 中的
    Vertical-Slash 稀疏 FlashAttention 内核是块稀疏注意力内核和 PIT [[88](#bib.bib88)] 稀疏注意力内核的混合。PIT
    是一种通过排列不变变换将稀疏数据加载到密集计算块中的技术。线程块首先循环遍历前面部分描述的块索引（块部分），然后循环遍历按块大小分组的列索引（PIT 部分）。该混合内核的延迟与块和列的总面积线性相关。
- en: Appendix D Additional Experiment Results
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 额外实验结果
- en: D.1 Needle In A Haystack
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 Needle In A Haystack
- en: '![Refer to caption](img/39bb7b8f651893d7e6f68e4c5a961b46.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/39bb7b8f651893d7e6f68e4c5a961b46.png)'
- en: 'Figure 8: Results on Needle In A Haystack using InfLLM in LLaMA-3-8B-Instruct-1M.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 InfLLM 在 LLaMA-3-8B-Instruct-1M 上的 Needle In A Haystack 结果。
- en: 'In addition to the Needle In A Haystack results for LLaMA-3-Instruct-1M shown
    in §[4](#S4 "4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we also present the LLaMA-3-Instruct-1M using
    InfLLM results in Fig. [8](#A4.F8 "Figure 8 ‣ D.1 Needle In A Haystack ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"), and results for GLM-4-9B-1M,
    Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, shown in Fig. [9](#A4.F9 "Figure
    9 ‣ D.1 Needle In A Haystack ‣ Appendix D Additional Experiment Results ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention").
    Compared to Full Attention, using MInference has minimal impact on the ability
    to understand semantic information across different context windows and needle
    depths. There is even a slight performance improvement around the 100k context
    length using Yi-9B-200K and Phi-3-Mini-128K.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '除了在 §[4](#S4 "4 Experiments ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention") 中展示的 LLaMA-3-Instruct-1M 的 Needle
    In A Haystack 结果外，我们还展示了图 [8](#A4.F8 "Figure 8 ‣ D.1 Needle In A Haystack ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention") 中的 LLaMA-3-Instruct-1M 使用 InfLLM
    的结果，以及图 [9](#A4.F9 "Figure 9 ‣ D.1 Needle In A Haystack ‣ Appendix D Additional
    Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") 中展示的 GLM-4-9B-1M、Yi-9B-200K、Phi-3-Mini-128K
    和 Qwen2-7B-128K 的结果。与全注意力相比，使用 MInference 对理解不同上下文窗口和针头深度的语义信息的能力影响最小。使用 Yi-9B-200K
    和 Phi-3-Mini-128K 时，约 100k 上下文长度的性能甚至略有提升。'
- en: '![Refer to caption](img/ba5ebb2989e79685f9c2208bddfa185d.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ba5ebb2989e79685f9c2208bddfa185d.png)'
- en: (a) GLM-4-9B-1M
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GLM-4-9B-1M
- en: '![Refer to caption](img/52f83beb6a222be5d31f20f298066c2e.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/52f83beb6a222be5d31f20f298066c2e.png)'
- en: (b) GLM-4-9B-1M w/ MInference
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GLM-4-9B-1M 与 MInference
- en: '![Refer to caption](img/b938f1489e9142f62953ffa559352b51.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b938f1489e9142f62953ffa559352b51.png)'
- en: (c) Yi-9B-200K
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Yi-9B-200K
- en: '![Refer to caption](img/4cfe3b402cabfda105c1c3293ea50951.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4cfe3b402cabfda105c1c3293ea50951.png)'
- en: (d) Yi-9B-200K w/ MInference
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Yi-9B-200K 与 MInference
- en: '![Refer to caption](img/fd910638ecb8839985282ec15fd8c006.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fd910638ecb8839985282ec15fd8c006.png)'
- en: (e) Phi-3-Mini-128K
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Phi-3-Mini-128K
- en: '![Refer to caption](img/565de9e1d007245060ad8c490747a28a.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/565de9e1d007245060ad8c490747a28a.png)'
- en: (f) Phi-3-Mini-128K w/ MInference
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Phi-3-Mini-128K 与 MInference
- en: '![Refer to caption](img/32b3a37365e0176166e99b96732899d0.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/32b3a37365e0176166e99b96732899d0.png)'
- en: (g) Qwen2-7B-128K
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Qwen2-7B-128K
- en: '![Refer to caption](img/43463f60255f33abb96cf01e0ed331c7.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/43463f60255f33abb96cf01e0ed331c7.png)'
- en: (h) Qwen2-7B-128K w/ MInference
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Qwen2-7B-128K 与 MInference
- en: 'Figure 9: Needle In A Haystack [[35](#bib.bib35)] results using GLM-4-9B-1M [[26](#bib.bib26)],
    Yi-9B-200K [[84](#bib.bib84)], Phi-3-Mini-128K [[2](#bib.bib2)], and Qwen2-7B-128K [[5](#bib.bib5)].'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 使用 GLM-4-9B-1M [[26](#bib.bib26)]、Yi-9B-200K [[84](#bib.bib84)]、Phi-3-Mini-128K
    [[2](#bib.bib2)] 和 Qwen2-7B-128K [[5](#bib.bib5)] 的 Needle In A Haystack [[35](#bib.bib35)]
    结果。'
- en: D.2 Latency Breakdown
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 延迟分解
- en: '![Refer to caption](img/1b8f7add8e0f29d889bf7870f60b4586.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1b8f7add8e0f29d889bf7870f60b4586.png)'
- en: 'Figure 10: The latency breakdown of a single attention kernel for three patterns
    and FlashAttention [[13](#bib.bib13)] across different context windows in a single
    A100, including the index time for dynamic sparse approximation and building dynamic
    sparsity. At 10k tokens, the latency of the four kernels is very close and all
    are less than 1ms. At 1M tokens, the latency for A-shape is 164ms.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 单个注意力内核在三种模式和 FlashAttention [[13](#bib.bib13)] 下在单个 A100 上的延迟分解，包括动态稀疏近似和构建动态稀疏性的索引时间。在
    10k tokens 时，四个内核的延迟非常接近，均低于 1ms。在 1M tokens 时，A-shape 的延迟为 164ms。'
- en: 'Fig. [10](#A4.F10 "Figure 10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional
    Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") shows the micro-benchmark results of the three
    attention patterns proposed in this paper, as well as FlashAttention. It can be
    seen that Vertical-Slash is the slowest among the three patterns, but it still
    achieves a 13x speedup compared to FlashAttention under 1M context windows. A-shape
    is slightly faster than Vertical-Slash, but at 1M, A-shape is 50% slower than
    Vertical-Slash. Block-Sparse is the fastest, achieving a 30x speedup over FlashAttention
    under 1M context windows. The estimation and index-building time for the dynamic
    sparse pattern accounts for approximately 5%-15% and 25% of the total time for
    Vertical-Slash and Block-Sparse patterns, respectively. The index-building overhead
    is higher for Block-Sparse mainly due to the time-consuming MeanPooling and block-level
    matmul computations. Additionally, the memory overhead for sparse indexing is
    relatively small, remaining within 160MB for a LLaMA-3-8B model in 1M context.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '图[10](#A4.F10 "Figure 10 ‣ D.2 Latency Breakdown ‣ Appendix D Additional Experiment
    Results ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention")展示了本文提出的三种注意力模式的微基准结果，以及 FlashAttention。可以看出，在三种模式中，垂直-斜线是最慢的，但在
    1M 上相比于 FlashAttention 仍然实现了 13 倍的加速。A 形态略快于垂直-斜线，但在 1M 上，A 形态比垂直-斜线慢 50%。块稀疏是最快的，在
    1M 上实现了比 FlashAttention 快 30 倍。动态稀疏模式的估计和索引构建时间分别占垂直-斜线和块稀疏模式总时间的约 5%-15% 和 25%。块稀疏的索引构建开销较高，主要是由于耗时的均值池化和块级
    matmul 计算。此外，稀疏索引的内存开销相对较小，在 1M 上 LLaMA-3-8B 模型的内存保持在 160MB 内。'
- en: D.3 Additional Ablation Study
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 附加消融研究
- en: 'Table 7: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K
    on InfiniteBench [[86](#bib.bib86)]. It is important to note that due to kernel
    limitations, we must retain at least one vertical and one slash. Therefore, "ours
    w/ only vertical" retains the top-1 slash, and "ours w/ only slash" retains the
    top-1 vertical.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在 InfiniteBench 上使用 LLaMA-3-8B-Instruct-262K 的不同消融方法的性能[[86](#bib.bib86)]。需要注意的是，由于内核限制，我们必须保留至少一个垂直线和一个斜线。因此，“仅垂直线的方法”保留了
    top-1 斜线，而“仅斜线的方法”保留了 top-1 垂直线。
- en: '| Methods | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find
    | Retr.PassKey | Retr.Number | Retr.KV | Avg. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | En.Sum | En.QA | En.MC | En.Dia | Zh.QA | Code.Debug | Math.Find | Retr.PassKey
    | Retr.Number | Retr.KV | 平均值 |'
- en: '| Ours | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 20.5 | 12.9 | 65.9 | 7.5 | 12.5 | 22.3 | 33.1 | 100.0 | 100.0 | 12.8
    | 38.8 |'
- en: '| Ours w/ only vertical | 13.7 | 6.2 | 30.1 | 2.0 | 6.5 | 7.9 | 1.7 | 65.4
    | 52.7 | 0.0 | 18.6 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 仅垂直线的方法 | 13.7 | 6.2 | 30.1 | 2.0 | 6.5 | 7.9 | 1.7 | 65.4 | 52.7 | 0.0 |
    18.6 |'
- en: '| Ours w/ only slash | 18.4 | 11.5 | 60.1 | 3.0 | 11.4 | 22.1 | 28.4 | 100.0
    | 100.0 | 4.2 | 35.9 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 仅斜线的方法 | 18.4 | 11.5 | 60.1 | 3.0 | 11.4 | 22.1 | 28.4 | 100.0 | 100.0 |
    4.2 | 35.9 |'
- en: 'To further analyze the role of dynamic vertical and slash lines in the Vertical-Slash
    pattern for sparse computation, we introduce a new set of ablation studies as
    follows: 1) Ours w/ only vertical, which only uses vertical lines and the top-1
    slash line in Vertical-Slash pattern. 2) Ours w/ only slash, which only uses slash
    lines and the top-1 vertical line in Vertical-Slash pattern. The corresponding
    top-K quantities are set after converting based on FLOPs in kernel.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析动态垂直线和斜线在稀疏计算的垂直-斜线模式中的作用，我们引入了一组新的消融研究，如下所示：1）仅垂直线的方法，仅使用垂直线和垂直-斜线模式中的
    top-1 斜线。2）仅斜线的方法，仅使用斜线和垂直-斜线模式中的 top-1 垂直线。相应的 top-K 数量是在根据内核中的 FLOPs 转换后设定的。
- en: 'As shown in Table [7](#A4.T7 "Table 7 ‣ D.3 Additional Ablation Study ‣ Appendix
    D Additional Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for
    Long-Context LLMs via Dynamic Sparse Attention"), using only vertical lines results
    in a significant performance drop, especially in retrieval tasks, where performance
    is similar to only using block-sparse. In contrast, using only slash lines retains
    most of the performance, but in highly dynamic tasks such as KV retrieval, performance
    further decreases, with an average performance drop of 2.9% compared to Ours.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[7](#A4.T7 "Table 7 ‣ D.3 Additional Ablation Study ‣ Appendix D Additional
    Experiment Results ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")所示，使用仅垂直线会导致性能显著下降，特别是在检索任务中，性能类似于仅使用块稀疏。相比之下，使用仅斜线保持了大部分性能，但在诸如
    KV 检索等高度动态任务中，性能进一步下降，较“我们的”平均性能下降了 2.9%。'
- en: Appendix E Pattern Distribution
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 模式分布
- en: '![Refer to caption](img/5985fc55f8aafd48b024a5fae9e1aa72.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5985fc55f8aafd48b024a5fae9e1aa72.png)'
- en: (a) LLaMA-3-8B-Instruct-262K/1M
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLaMA-3-8B-Instruct-262K/1M
- en: '![Refer to caption](img/ee027e611a81c5a58fd894a1de67b87e.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee027e611a81c5a58fd894a1de67b87e.png)'
- en: (b) Yi-9B-200K
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Yi-9B-200K
- en: 'Figure 11: Distribution of three sparse head patterns in different models.
    We use the same optimal sparse pattern configuration for both LLaMA-3-8B-Instruct-262K
    and LLaMA-3-8B-Instruct-1M.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：不同模型中三种稀疏头模式的分布。我们对LLaMA-3-8B-Instruct-262K和LLaMA-3-8B-Instruct-1M使用了相同的最佳稀疏模式配置。
- en: 'Fig. [11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    shows the distribution of the optimal head configuration obtained through our
    search. Firstly, most of the patterns are the Vertical-Slash pattern (>90%). However,
    according to the ablation study, using only the Vertical-Slash pattern significantly
    impacts performance in highly dynamic tasks like KV retrieval. Secondly, the Block-Sparse
    pattern is primarily distributed in several intermediate to later layers, while
    the A-shape pattern is found in the middle layers. Although the optimal patterns
    vary slightly across different models, they generally align with these observations.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")展示了通过我们搜索得到的最佳头配置的分布。首先，大多数模式是Vertical-Slash模式（>90%）。然而，根据消融研究，仅使用Vertical-Slash模式在像KV检索这样高度动态的任务中显著影响性能。其次，Block-Sparse模式主要分布在几个中层到后层，而A-shape模式则出现在中层。尽管不同模型的最佳模式略有不同，但它们通常与这些观察结果一致。'
- en: Additionally, we used the same configuration for two versions of LLaMA in our
    experiments, and the results show that the 1M model also performs very well, with
    nearly perfect results in the Needle In A Haystack task. This demonstrates the
    generalizability of the optimal sparse pattern.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在实验中对两个版本的LLaMA使用了相同的配置，结果表明，1M模型也表现非常好，在Needle In A Haystack任务中几乎达到了完美的结果。这展示了最佳稀疏模式的普遍性。
- en: Appendix F Sparsity in Kernel Distribution
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 内核分布中的稀疏性
- en: '![Refer to caption](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
- en: 'Figure 12: The distribution of sparsity in the kernel across different context
    windows refers to the proportion of the kernel that is actually computed after
    block coverage, compared to the sparsity rate when using FlashAttention with a
    causal mask.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：内核在不同上下文窗口中的稀疏分布指的是在块覆盖后实际计算的内核比例，与使用带有因果掩码的FlashAttention时的稀疏率相比。
- en: 'As shown in Fig. [12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), the sparsity distribution of the three patterns during the actual
    kernel computation process is displayed. It can be seen that when the context
    windows exceed 200k, the actual sparsity of all three patterns surpasses 90%.
    Even considering a 20% index-building overhead, this ensures that the kernel achieves
    a speedup of over 8$\times$.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，实际内核计算过程中三种模式的稀疏分布情况被展示出来。可以看到，当上下文窗口超过200k时，三种模式的实际稀疏度都超过了90%。即便考虑20%的索引构建开销，这也确保了内核获得了超过8$\times$的加速。'
- en: Appendix G Does This Dynamic Sparse Attention Pattern Exist Only in Auto-Regressive
    LLMs or RoPE-Based LLMs?
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 这种动态稀疏注意模式仅存在于自回归LLMs或基于RoPE的LLMs中吗？
- en: '![Refer to caption](img/6b22e182a2b67446c71bc01d753a7101.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b22e182a2b67446c71bc01d753a7101.png)'
- en: 'Figure 13: The sparse pattern in T5-style Encoder Attention using Flan-UL2 [[74](#bib.bib74)]
    on the Summarization dataset [[86](#bib.bib86)].'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用Flan-UL2[[74](#bib.bib74)]在Summarization数据集[[86](#bib.bib86)]上进行T5风格Encoder
    Attention的稀疏模式。
- en: 'Similar vertical and slash line sparse patterns have been discovered in BERT [[72](#bib.bib72)]
    and multi-modal LLMs [[80](#bib.bib80)]. Additionally, as shown in Fig. [13](#A7.F13
    "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern Exist Only
    in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), we analyzed the distribution
    of attention patterns in T5 across different heads. It is evident that there are
    vertical and slash sparse patterns even in bidirectional attention.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '在 BERT [[72](#bib.bib72)] 和多模态 LLMs [[80](#bib.bib80)] 中已经发现了类似的垂直和斜线稀疏模式。此外，如图
    [13](#A7.F13 "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern
    Exist Only in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") 所示，我们分析了 T5 在不同头部中的注意力模式分布。显然，即使在双向注意力中也存在垂直和斜线稀疏模式。'
- en: Recent studies [[80](#bib.bib80)] have analyzed sparse attention patterns in
    multi-modal LLMs, revealing the presence of vertical and slash patterns in models
    like LLaVA [[45](#bib.bib45)] and InternVL [[11](#bib.bib11)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 [[80](#bib.bib80)] 分析了多模态 LLMs 中的稀疏注意力模式，揭示了 LLaVA [[45](#bib.bib45)]
    和 InternVL [[11](#bib.bib11)] 等模型中存在垂直和斜线模式。使用 MInference 进行预填充阶段的推理加速具有很大前景。
- en: Appendix H Case Study
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 案例研究
- en: 'Table [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") presents a comparison
    of the generation performance for various methods on the EN.SUM task (200K input
    length) from InfiniteBench based on the LLaMA-3-8B-262K model. The original summary
    provides a comprehensive and coherent narrative, detailing the Bronwyn family’s
    trip to the Kindergarten and touching on themes such as nostalgia, loss, and the
    passage of time. StreamingLLM’s summary, although looks coherent, introduces elements
    that are not present in the original story, leading to serious factual errors.
    For example, it mentions a boat trip to a school for boys and specific details
    like fishermen, sandwiches, and a spot where men were drowned. These details deviate
    from the original story, which is about the Bronwyn family preparing for a trip
    to the Kindergarten. In addition, the summaries generated by StreamingLLM with
    dilated and strided techniques are largely incoherent, consisting primarily of
    repetitive and nonsensical characters, indicating a failure to produce meaningful
    content. In stark contrast, the summary generated by our proposed method offers
    a detailed and coherent narrative, comparable to the original, with a clear depiction
    of the story’s main events and themes. This includes the preparation of the Bronwyn
    family for their trip, the characterization of family members and guests, and
    the exploration of deeper themes such as love, marriage, and the search for meaning.
    The results demonstrate the superiority of our proposed method in generating high-quality,
    human-like summaries over the baseline methods.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") 展示了基于 LLaMA-3-8B-262K
    模型的 InfiniteBench 上 EN.SUM 任务（200K 输入长度）的各种方法生成性能的比较。原始摘要提供了一个全面且连贯的叙述，详细描述了 Bronwyn
    家庭去幼儿园的旅行，并涉及怀旧、失落和时间流逝等主题。尽管 StreamingLLM 的摘要看起来连贯，但引入了原故事中不存在的元素，导致严重的事实错误。例如，它提到了前往男孩学校的船行以及具体的细节，如渔民、三明治和男人溺水的地方。这些细节偏离了原故事，原故事讲的是
    Bronwyn 家庭准备前往幼儿园的旅行。此外，使用扩张和步幅技术生成的 StreamingLLM 摘要大多不连贯，主要由重复和无意义的字符组成，显示出未能生成有意义的内容。相比之下，我们提出的方法生成的摘要提供了详细且连贯的叙述，与原文相当，清晰地描绘了故事的主要事件和主题。这包括
    Bronwyn 家庭为旅行做准备、家庭成员和客人的刻画，以及探索爱、婚姻和寻找意义等更深层次的主题。结果展示了我们提出的方法在生成高质量、人性化摘要方面优于基线方法。'
- en: 'Table [9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") compares the
    performance of various methods on the Retrieve.KV task (200K input length) using
    the LLaMA-3-8B-262K model. The original method demonstrates perfect retrieval,
    correctly predicting the exact strings of the ground truth for both examples.
    StreamingLLM, again, generates predictions that looks coherent and real, but factually
    incorrect. In addition, StreamingLLM with dilated and strided techniques, and
    our method with a static pattern, fail significantly, producing outputs that are
    either repetitive sequences of characters or nonsensical strings, indicating their
    inability to accurately retrieve the required key-value pairs. Our method, however,
    performs on par with the original, accurately retrieving and predicting the exact
    key-value pairs for both examples. This demonstrates the superior capability of
    our method in handling KV retrieval tasks, providing precise and reliable outputs
    consistent with the ground truth. The results highlight our method’s effectiveness
    and robustness compared to the baselines, making it a reliable choice for such
    tasks.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [9](#A8.T9 "表 9 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 比较了使用
    LLaMA-3-8B-262K 模型在 Retrieve.KV 任务（200K 输入长度）上的各种方法的性能。原始方法表现出完美的检索，准确预测了两个示例的地面真实值的确切字符串。StreamingLLM
    再次生成了看起来连贯和真实的预测，但事实却不正确。此外，使用膨胀和步长技术的 StreamingLLM 以及我们使用静态模式的方法都显著失败，生成了重复的字符序列或无意义的字符串，表明它们无法准确检索所需的键值对。然而，我们的方法与原始方法表现相当，准确检索并预测了两个示例的确切键值对。这展示了我们的方法在处理
    KV 检索任务中的优越能力，提供了与地面真实值一致的精确和可靠的输出。结果突显了我们的方法相较于基线的有效性和鲁棒性，使其成为此类任务的可靠选择。
- en: Algorithm 4 Vertical-Slash Index
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 竖直斜线索引
- en: 'Input: vertical indexes $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$, column count $\bm{c}_{\text{col}}\mathbb{N}^{N}$  #
    Merge points (vertical indexes) and ranges (slash indexes)     while $s_{v}\leq
    k_{s}$  # Update the range           if $(i-1)\times B-\bm{i}_{s}^{j_{s}}></math>           else              #
    Extend the range              <math id=$'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：竖直索引 $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$，列数 $\bm{c}_{\text{col}}\mathbb{N}^{N}$  #
    合并点（竖直索引）和范围（斜线索引）      while $s_{v}\leq k_{s}$  # 更新范围           if $(i-1)\times
    B-\bm{i}_{s}^{j_{s}}></math>           else              # 扩展范围              <math
    id=$'
- en: Algorithm 5 Vertical-Slash Flash Attention
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 竖直斜线闪光注意力
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ to $N$Load $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$  end for'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ 到 $N$ 载入 $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$  end for
- en: 'Table 8: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in summarization task [[86](#bib.bib86)].'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：使用 LLaMA-3-8B-Instruct-262K 进行摘要任务的不同方法生成结果的比较 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 |'
- en: '| Original | The story follows the Bronwyn family and their guests as they
    prepare for a day trip to the Kindergarten, a mysterious destination that holds
    great significance for them. The story is told from multiple perspectives, including
    that of Pete Lucy, who is staying with the family and is struggling to find her
    place in their lives. As the day progresses, the family members prepare for their
    trip, and Pete reflects on her own past experiences with the Bronwyns. The story
    ends with the family setting off on their journey, leaving Pete alone on the lawn
    to contemplate her own feelings and the meaning of life. The Kindergarten is a
    symbol of the family’s past and their memories, and the story explores themes
    of nostalgia, loss, and the passage of time. Ultimately, the story suggests that
    even though the past cannot be changed, it can still be celebrated and remembered.
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 故事讲述了布朗温家族及其宾客在准备前往一个对他们具有重大意义的神秘目的地——幼儿园的一日游的过程。故事从多个视角叙述，包括正在与家人同住并努力融入他们生活的皮特·露西。随着一天的进展，家人们为出行做准备，皮特回忆起自己与布朗温家族的过去经历。故事在家人们启程的过程中结束，皮特独自留在草坪上，思索自己的感受和生命的意义。幼儿园是家族过去和回忆的象征，故事探讨了怀旧、失落和时间流逝的主题。最终，故事暗示尽管过去无法改变，但仍可以庆祝和铭记。
    |'
- en: '| StreamingLLM | The story follows a family on a boat trip to the Kindergarten,
    a school for boys. The father, Mr Bronwyn, is reading a book and seems to be in
    a trance. The children, Seb and Annalisa, are excited about the trip but also
    a little scared. They pass by a group of fishermen and Mr Bronwyn shares out sandwiches.
    They come across a spot where three men were drowned and Mr Bronwyn takes out
    his watch, making a calculation. They reach the Kindergarten and Mr Bronwyn jumps
    out of the boat, holding a parcel. The story ends with Pete Lucy, who has been
    watching the family from the shore, feeling relieved that the trip is over and
    that Mr Bronwyn has reached his destination. The story is told from multiple perspectives,
    including Pete Lucy’s, who is watching the family from the shore and reflecting
    on his own life and relationships. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 故事讲述了一个家庭乘船前往幼儿园的旅行，幼儿园是一所男孩学校。父亲布朗温先生正在读一本书，看起来神情恍惚。孩子们，塞布和安娜莉莎，对这次旅行感到兴奋但也有些害怕。他们经过一群渔民，布朗温先生分发三明治。他们遇到了一个曾有三人溺亡的地方，布朗温先生拿出手表，进行计算。他们到达幼儿园，布朗温先生跳下船，拿着一个包裹。故事以皮特·露西的视角结束，她在岸边观察这个家庭，对旅行结束和布朗温先生达到目的地感到释然。故事从多个视角叙述，包括皮特·露西，她在岸边观察这个家庭，反思自己的生活和人际关系。'
- en: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( |'
- en: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | 图形单元36.OOAKA新 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( |'
- en: '| Ours w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 我们的静态 | def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
- en: '| Ours | The story follows the Bronwyn family and their guests as they prepare
    for a trip to the Kindergarten, a remote island off the coast of Scotland. The
    family is made up of Mr. Bronwyn, a philosopher, and his wife, Mrs. Bronwyn, who
    is known for her beauty and her ability to surround and protect her family. The
    guests include Kevin Emma, a young man who is studying for a fellowship, and Castor
    Kiki, a botanist who is also a widower. The story explores themes of love, marriage,
    and the search for meaning in life. It also touches on the idea of the other,
    or the person who is different from oneself and cannot be understood or judged
    by conventional standards. Ultimately, the story is about the search for truth
    and the desire to connect with others on a deeper level. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 故事讲述了布朗温一家及其客人在准备前往幼儿园的旅程中，他们去的是一个位于苏格兰海岸的偏远小岛。这个家庭由哲学家布朗温先生和以美丽和保护家人而著称的布朗温夫人组成。客人包括凯文·艾玛，一个正在为获得奖学金而学习的年轻人，以及卡斯特·基基，一个也是鳏夫的植物学家。这个故事探讨了爱情、婚姻以及对生活意义的追寻。它还涉及了“他者”的概念，即与自己不同的人，无法通过传统标准来理解或评判。最终，这个故事是关于寻求真理和渴望与他人建立更深层次联系的。'
- en: 'Table 9: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in KV Retrieval task [[86](#bib.bib86)].'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：使用 LLaMA-3-8B-Instruct-262K 在 KV 检索任务中不同方法生成结果的比较 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 |'
- en: '| Original | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '| 原始 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。'
- en: '| StreamingLLM | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant 2\. Ground Truth: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: 2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant
    2\. 实际值: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
- en: 'Prediction: "d829ce18-6339-4f77-8c04-31fc7ec33619". |'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "d829ce18-6339-4f77-8c04-31fc7ec33619"。'
- en: '| StreamingLLM w/ dilated | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: 5 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 带扩张 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: 5 (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( 2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    |'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 |'
- en: '| StreamingLLM w/ strided | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 带有跨步 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 2\. 实际值:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
- en: '| Ours w/ static | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "def solverome2 def solverome2 def solverome2 def solverome2 def 2\. Ground Truth:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的 带静态 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "def solverome2
    def solverome2 def solverome2 def solverome2 def 2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    |'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "def solverome2 def solverome2 def solverome2 def solverome2 def |'
- en: '| Ours | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。'
