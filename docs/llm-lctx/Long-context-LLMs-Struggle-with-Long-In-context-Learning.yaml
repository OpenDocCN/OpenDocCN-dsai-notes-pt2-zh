- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Long-context LLMs Struggle with Long In-context Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.02060](https://ar5iv.labs.arxiv.org/html/2404.02060)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ^(♠,♣)Tianle Li, ^(♠,♣)Ge Zhang, ^♠Quy Duc Do, ^†Xiang Yue, ^(♠,♣)Wenhu Chen
  prefs: []
  type: TYPE_NORMAL
- en: ^♠University of Waterloo
  prefs: []
  type: TYPE_NORMAL
- en: ^†Carnegie Mellon University
  prefs: []
  type: TYPE_NORMAL
- en: ^♣Vector Institute, Toronto
  prefs: []
  type: TYPE_NORMAL
- en: '{t29li,wenhuchen}@uwaterloo.ca'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/TIGER-AI-Lab/LongICLBench](https://github.com/TIGER-AI-Lab/LongICLBench)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have made significant strides in handling long
    sequences exceeding 32K tokens. However, their performance evaluation has largely
    been confined to metrics like perplexity and synthetic tasks, which may not fully
    capture their abilities in more nuanced, real-world scenarios. This study introduces
    a specialized benchmark (LongICLBench) focusing on long in-context learning within
    the realm of extreme-label classification. We meticulously selected six datasets
    with a label range spanning 28 to 174 classes covering different input (few-shot
    demonstration) lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend
    the entire input to recognize the massive label spaces to make correct predictions.
    We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context
    LLMs perform relatively well on less challenging tasks with shorter demonstration
    lengths by effectively utilizing the long context window. However, on the most
    challenging task Discovery with 174 labels, all the LLMs struggle to understand
    the task definition, thus reaching a performance close to zero. This suggests
    a notable gap in current LLM capabilities for processing and understanding long,
    context-rich sequences. Further analysis revealed a tendency among models to favor
    predictions for labels presented toward the end of the sequence. Their ability
    to reason over multiple pieces in the long sequence is yet to be improved. Our
    study reveals that long context understanding and reasoning is still a challenging
    task for the existing LLMs. We believe LongICLBench could serve as a more realistic
    evaluation for the future long-context LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70a9501233104da3b2ddb39a43b00124.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: LLM performance on long in-context benchmark across different lengths.
    We curate datasets with different difficulty levels. As we increase the difficulty
    of the dataset, LLMs struggle to understand the task definition and suffer from
    significant performance degradation. On the most difficult Discovery dataset,
    none of the LLMs is able to understand the long demonstration, leading to zero
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models have already entered the long context era. Myriad of LLMs
    has been released to support long context windows from 32K to 2M tokens. These
    methods (Hao et al., [2022](#bib.bib19); Chen et al., [2023a](#bib.bib10); Peng
    et al., [2023b](#bib.bib33); Ratner et al., [2023](#bib.bib36); Xiao et al., [2024](#bib.bib48))
    can unlock lots of complex real-world applications from long-document question-answering,
    multi-document summarization, long-horizon agent tasks, repo-level code understanding.
  prefs: []
  type: TYPE_NORMAL
- en: One line of research is based on AliBi (Press et al., [2022](#bib.bib35)) and
    RoPE (Su et al., [2024](#bib.bib40)) embedding, which allows us to train Transformers
    with short sequences and subsequently apply them to longer sequences during inference.
    Recently, different approaches (Xiong et al., [2023](#bib.bib49); Fu et al., [2024](#bib.bib17);
    Liu et al., [2024](#bib.bib26)) help the model to extrapolate to 128K window size
    with continued pre-training. Later on, LongRoPE (Ding et al., [2024](#bib.bib15))
    was proposed to further extend the context window to 2M tokens. Another line of
    research also utilizes methodologies like context window sliding and segmentation
    to overcome the issue of the limited context window in original Transformers (Hao
    et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib36)). Furthermore, architectural
    innovations, transitioning from traditional Transformer-based designs to recurrent
    models or state space models, have shown promise in facilitating long-range computations
    naturally Orvieto et al. ([2023](#bib.bib31)); Gu & Dao ([2023](#bib.bib18));
    Peng et al. ([2023a](#bib.bib32)). These techniques have been incorporated into
    several current open-source LLMs to enhance long sequence understanding capability (Chen
    et al., [2023b](#bib.bib11); Tworkowski et al., [2023](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10418bb602a94c7d0594a747ff935e00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison extreme-label ICL with the existing evaluation tasks.
    Passkey Retrieval is a synthetic task. Long-document Question-answering does not
    require reading the entire document to find the answer. In extreme-label ICL,
    the model needs to scan through the entire demonstration to understand the whole
    label space to make the correct prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These long-context models are primarily evaluated on three types of evaluations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. language model perplexity over long documents, which is used by most papers.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. passkey retrieval (Mohtashami & Jaggi, [2023](#bib.bib29); Chen et al.,
    [2023a](#bib.bib10); Li et al., [2023a](#bib.bib22)) or needle-in-a-haystack (Team
    et al., [2023](#bib.bib42); Fu et al., [2024](#bib.bib17)), which requires reciting
    a randomly inserted information in a long sequence. Several LLMs achieve 99%+
    on this synthetic task.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. long-document question-answer or summarization over Qasper (Dasigi et al.,
    [2021](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations (1) and (2) only provide a minimum bar for LLMs to pass, but their
    results cannot reflect LLMs’ true ability to deal with realistic long-sequence
    tasks. Evaluation (3) provides a more realistic metric, however, these tasks are
    more focused on retrieving correct information from the long input. In question
    answering, LLMs can take a shortcut to read a short snippet to predict the answer
    without reading the entire document as demonstrated in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Long-context LLMs Struggle with Long In-context Learning")
    case (b). Similarly, summarization also suffers from the strong position bias,
    where LLMs can utilize the few leading sentences (Nallapati et al., [2017](#bib.bib30))
    to achieve high performance. Therefore, these metrics are insufficient to measure
    LLMs’ ability to comprehend and reason over the entire input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose to adopt in-context learning (ICL) on extreme-label
    classification tasks (Anil et al., [2022](#bib.bib4); Milios et al., [2023](#bib.bib28))
    to evaluate long-context LLMs. Unlike the prior tasks, in-context learning requires
    LLMs to recognize the task by scanning over the entire input to understand the
    label space. This task necessitates LLMs’ ability to comprehend the entire input
    to make predictions. Due to the massive label space, the task demonstration could
    easily become a long sequence. For example, Discovery (Sileo et al., [2019](#bib.bib38))
    encompasses 174 classes with each example taking an average of 61 tokens. Therefore,
    the minimum demonstration for 1 shot/class already exceeds 10K tokens. Normally,
    LLMs demand more than 1 shot/class to understand the nuances of different fine-grained
    labels. Thus, this task becomes a natural testbed for long-context understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fbeec7c5caa7ca870132f02e924665ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Results for representative models across different evaluation datasets.
    The performance greatly decreases as the task becomes more challenging. Some models
    even decay linearly w.r.t the demonstration length.'
  prefs: []
  type: TYPE_NORMAL
- en: To systematically assess how these extended input capabilities affect model
    performance in the realm of fine-grained text classification with in-context learning,
    we have compiled a benchmark, i.e. LongICLBench, consisting of six carefully-selected
    tasks with different difficulty levels in terms of context length and label space.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate the performance of 13 long-context LLMs and find that the performance
    of the models uniformly dips as the task becomes more complex (e.g. requiring
    longer demonstration) as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Long-context LLMs Struggle with Long In-context Learning"). Some models like
    Qwen and Mistral even degrade linearly w.r.t the input length. Simultaneously,
    most of the models can benefit from the extensive demonstration if the length
    is within a certain range. As the input grows longer, it either hurts or makes
    the performance fluctuate as shown in Figure [1](#S0.F1 "Figure 1 ‣ Long-context
    LLMs Struggle with Long In-context Learning"). Moreover, we make further analysis
    on the distribution of label position to investigate the factors that affect the
    long in-context learning capability of these models. It is shown that the position
    distribution of instances in the prompt can dramatically influence the performance
    of some of the evaluated models including GPT4-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, our contributions to this work can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '- We have developed LongICLBench, dedicated to assessing long in-context learning
    tasks for large language models. This benchmark serves as a complement to earlier
    benchmarks that concentrated on tasks like long document summarization, question
    answering (QA), or retrieval, focusing instead on long in-context learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '- We evaluate a line of recent long-context LLMs on LongICLBench and reveal
    their performances with gradually changed difficulty levels. Simultaneously, we
    find the sensitivity of some of the long-context LLMs regarding instance position
    in the prompt. We hope the evaluation results can provide more insights for the
    improvement of the design of long-context large language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Long In-context Learning on LLMs As pre-trained language models continue to
    grow in size, in-context learning (ICL) has emerged as a favored approach for
    addressing a wide array of tasks without the need for extensive fine-tuning  (Dong
    et al., [2023](#bib.bib16)). A body of research has established that increasing
    the number of example demonstrations can enhance ICL performance  (Liu et al.,
    [2022](#bib.bib25); Wu et al., [2023](#bib.bib47)). Nonetheless, there are studies
    indicating that longer input prompts can actually diminish performance  (Liu et al.,
    [2023](#bib.bib27)), with the effectiveness of prior large language models (LLMs)
    being constrained by the maximum sequence length encountered during their training.
    It is also claimed in previous works that LLM+ICL falls short on specification-heavy
    tasks due to inadequate long-text understanding ability  (Peng et al., [2023c](#bib.bib34)).
    To counter this issue, various works have introduced memory augmentation and extrapolation
    techniques to support ICL with an extensive set of demonstrations  (Li et al.,
    [2023c](#bib.bib24); Wang et al., [2023](#bib.bib46)).
  prefs: []
  type: TYPE_NORMAL
- en: Long Context Techniques over LLMs The effectiveness of Transformer-based models
    is hindered by the quadratic increase in computational cost relative to sequence
    length, particularly in handling long context inputs. Recent efforts have explored
    various strategies to address this challenge. Some studies have pursued continued
    fine-tuning of the LLM with longer context inputs, aiming to adapt the model to
    extended sequences  (Rozière et al., [2024](#bib.bib37); Tworkowski et al., [2023](#bib.bib44)).
    Others have leveraged techniques such as position extrapolation and interpolation,
    building upon relative rotary positional embedding  (Su et al., [2021](#bib.bib39)),
    to extend input length beyond the training phase  (Press et al., [2022](#bib.bib35);
    Chen et al., [2023a](#bib.bib10)). Additionally, a range of approaches has been
    proposed to mitigate computational issues, including sliding memory window and
    chunk segmentation methods  (Hao et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib36);
    Zhu et al., [2024](#bib.bib54)). Furthermore, alternative architectures beyond
    the Transformer have been explored to handle long inputs more naturally, such
    as selective-state-spaces models, which represent a variation of recurrent neural
    networks  Peng et al. ([2023a](#bib.bib32)); Gu & Dao ([2023](#bib.bib18)). These
    diverse approaches claim that they can enhance the capabilities of LLMs in processing
    long context inputs more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Long Context Evaluation Due to the imperious demands for the support of long-range
    LLMs, there is a series of benchmarks focusing on long context evaluation. Long-Range
    Arena  (Tay et al., [2021](#bib.bib41)) includes tasks consisting of sequences
    ranging from 1K to 16K tokens to evaluate variations of fast Transformers. LongBench
     (Bai et al., [2023b](#bib.bib6)) comprises 21 bilingual datasets within 6 types
    of tasks with an average length of around 6k words, which have been processed
    in a unified format to enable effortless evaluation. L-Eval Benchmark  (An et al.,
    [2023](#bib.bib3)) supports 20 sub-tasks with input lengths of 3K to 200K tokens.
    LooGLE  (Li et al., [2023b](#bib.bib23)) focuses on summarization and four types
    of long dependency QA tasks with test instances exceeding 100k words. Most recently,
    $\infty$Bench  (Zhang et al., [2024](#bib.bib52)) encompasses 12 tasks, collecting
    from realistic, auto-generated, and human-annotated datasets with an average length
    of 200K tokens. Another recent work explores the impact of extending input lengths
    on the capabilities of Large Language Models, especially on reasoning tasks  (Levy
    et al., [2024](#bib.bib21)). Versatile as these benchmarks, none of them focus
    on exploring the capability of LLMs confronted with long in-context learning with
    extreme label space, which is quite different from the tasks of long-document
    understanding or synthetic needle in a haystack. Thus, our LongICLBench is proposed
    to fill the niche and make a more comprehensive long-context evaluation for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Extreme-label Classification Extreme-label Classification involves categorizing
    data into one of an extremely large number of labels, and finds application across
    a variety of real-world domains such as emotion classification from text, named
    entity recognition, and biological function prediction, each requiring precise
    differentiation among vast label spaces (Zhang et al., [2017](#bib.bib53); Sileo
    et al., [2019](#bib.bib38); Demszky et al., [2020](#bib.bib13); Ding et al., [2021](#bib.bib14)).
    Existing methods to tackle Extreme-label Classification tasks range from embedding-based
    approaches to fine-tuned retrievals (Bhatia et al., [2015](#bib.bib7); Vulić et al.,
    [2021](#bib.bib45)), focusing on efficiently managing and leveraging the large
    label space. However, integrating this task with long-context large language models
    presents unique challenges. The sheer scale of the label space in extreme-label
    classification complicates the in-context learning process, where LLMs are expected
    to discern fine-grained differences among labels based on extensive context  (Milios
    et al., [2023](#bib.bib28)). These challenges make the proposed LongICLBench with
    a range of difficulty levels a good testing scenario to evaluate the capability
    of long-context large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Long In-context Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Dataset | Task Type | # Classes | # Tokens/Shot | # Total Tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotion | Emotion Classification | 28 | 28 | [1K, 4K] |'
  prefs: []
  type: TYPE_TB
- en: '| BANKING77 | Intent Classification | 77 | 28 | [2K, 11K] |'
  prefs: []
  type: TYPE_TB
- en: '| TacRED | Relation Extraction | 41 | 80 | [4K, 18K] |'
  prefs: []
  type: TYPE_TB
- en: '| Few-NERD | Entity Recognition | 66 | 61 | [5K, 23K] |'
  prefs: []
  type: TYPE_TB
- en: '| DialogRE | Relation Extraction | 36 | 226 | [8K, 32K] |'
  prefs: []
  type: TYPE_TB
- en: '| Discovery | Discourse Marker Classification | 174 | 61 | [10K, 50K] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistics of the collected sub-dataset in LongICLBench. We evaluate
    from 1-shot/label to 5-shot/label, which results in the shown #total token range.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Long In-context Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To support the evaluation of long in-context learning on extreme-label classification
    tasks in different domains and various difficulty levels, we collect six datasets
    containing context length from short to long. In order to balance the sequence
    token length within each dataset and the goal of evaluation for long in-context
    learning, we keep a subset of the classes among all the classes to format evaluation
    sets around 1 round, 2 rounds, 3 rounds, 4 rounds, and 5 rounds correspondingly,
    where each round represent a complete set of examples containing all unique chosen
    labels. We sample the number of instances from each of the classes evenly to reduce
    the bias resulting from the label distribution. The statistics of the datasets
    are described in detail in Table [1](#S3.T1 "Table 1 ‣ 3 Long In-context Evaluation
    ‣ Long-context LLMs Struggle with Long In-context Learning") and Appendix [A.1](#A1.SS1
    "A.1 Additional Datasets ‣ Appendix A Appendix ‣ Long-context LLMs Struggle with
    Long In-context Learning").
  prefs: []
  type: TYPE_NORMAL
- en: BANKING77  (Casanueva et al., [2020](#bib.bib9)) is a banking-domain intent
    detection dataset comprising 13,083 annotated examples over 77 intents. We keep
    all of the types of intents, and each of the instances contains around 28 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: TacRED  (Zhang et al., [2017](#bib.bib53)) is a large-scale relation extraction
    dataset with 106,264 examples built over news and web text from the corpus used
    in the yearly TAC Knowledge Base Population. Only one relation is labeled for
    each of the sentences in the dataset. It covers 41 relation types in total, with
    an average length of 80 tokens for each example.
  prefs: []
  type: TYPE_NORMAL
- en: DialogRE  (Yu et al., [2020](#bib.bib50)) is a human-annotated dialogue-based
    relation extraction dataset composed of 1788 dialogues from a famous American
    television comedy, Friends, with 36 possible relation types existing between an
    argument pair in a dialogue. Each example contains around 226 tokens on average.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery  (Sileo et al., [2019](#bib.bib38)) automatically discovers sentence
    pairs with relevant discourse markers and curates a large dataset containing 174
    discourse markers with at least 10K examples each. Each example contains around
    61 tokens. There are 174 types of discourse markers. This dataset is the most
    difficult task with fine-grained labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Model and Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the exploration of in-context learning for extreme-label classification,
    we conduct a comprehensive evaluation for a series of recent open-source long-context
    language models of size around 7B parameters. We also include the SoTA models
    like Gemini and GPT-4-turbo. [Table 2](#S3.T2 "Table 2 ‣ 3.2 Model and Experimental
    Setup ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long In-context
    Learning") provides an overview of the models investigated, highlighting the innovations
    in their architecture specifically for dealing with long context. We can observe
    that there are multiple strategies adopted to extend the context window. Some
    of the models support the training context window size while some models support
    length extrapolation. RWKV (Peng et al., [2023a](#bib.bib32)) and Mamba (Gu &
    Dao, [2023](#bib.bib18)) are the two new RNN-like architectures to decrease attention
    complexity, which would allow the model to easily extrapolate to much longer inputs
    with linear time/memory complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Initialization | Strategy | Train | Support |'
  prefs: []
  type: TYPE_TB
- en: '| [Gemma-7B-base](#bib.bib43) | 7B | Gemma | RoPE + LF | 8K | 8K |'
  prefs: []
  type: TYPE_TB
- en: '| [LLaMA-2-7B-32K](#bib.bib27) | 7B | LLaMA-2 | Position Interpolation | 32K
    | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| [ChatGLM3-6B-32K](#bib.bib51) | 6B | ChatGLM | Position Encoding Scheme |
    32K | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| [Qwen-1.5-7B-base](#bib.bib5) | 7B | Qwen | NTK-Aware Interpolation | 32K
    | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| [Mistral-7B-v0.2-base](#bib.bib20) | 7B | Mistral | LF | 32K | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| [LLaMA-2-7B-LongLora](#bib.bib11) | 7B | LLaMA-2 | Shifted Short Attention
    | 100K | 100K |'
  prefs: []
  type: TYPE_TB
- en: '| [Yi-6B-200K](#bib.bib2) | 6B | Yi | Position Interpolation +LF | 200K | 200K
    |'
  prefs: []
  type: TYPE_TB
- en: '| [InternLM2-7B-base](#bib.bib8) | 7B | InternLM | Dynamic NTK | 32K | 200K
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Long-LLaMA-code-7B](#bib.bib44) | 7B | LLaMA-2 | Focused Transformer | 8K
    | 256K |'
  prefs: []
  type: TYPE_TB
- en: '| [RWKV-5-World](#bib.bib32) | 3B | RWKV | Attention-free Model | 4K | $\infty$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Mamba-2.8B](#bib.bib18) | 2.8B | Mamba | State Space Model | 2K | $\infty$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Gemini-1.0-Pro](#bib.bib42) | - | Gemini | Ring Attention | 32K | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| [GPT4-turbo](#bib.bib1) | - | GPT-4 | - | - | 128K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The overview of the evaluated models. We utilize base models before
    instruction-tuning except Gemini and GPT4-turbo. LF means fine-tuning the model
    on longer-context corpus after pre-training.'
  prefs: []
  type: TYPE_NORMAL
- en: We construct a prompt following the template as shown in [A.2](#A1.SS2 "A.2
    Prompting Template ‣ Appendix A Appendix ‣ Long-context LLMs Struggle with Long
    In-context Learning") for each of the datasets. To fairly evaluate the open-source
    and API-based models with a series of input lengths, we sample the same example
    set for all the models with labels distributed evenly to ensure an unbiased distribution
    for the in-context demonstration. For instance, an input of one round will include
    one set of examples traversing all the types, and 5 rounds will contain instances
    from each of the labels 5 times. For testing, we sample 500 examples from the
    test set of each dataset, simultaneously ensuring an even distribution in terms
    of the type of labels. All the open-source models are loaded from the weights
    in HuggingFace¹¹1[https://huggingface.co](https://huggingface.co), while the API-based
    models are called with the scripts in the official documentations ²²2[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api),
    [https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Experiment Result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main evaluation results are demonstrated in Table [4](#S3.T4 "Table 4 ‣
    3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle
    with Long In-context Learning"), Table [4](#S3.T4 "Table 4 ‣ 3.3 Experiment Result
    ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long In-context
    Learning"), Table [6](#S3.T6 "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context
    Evaluation ‣ Long-context LLMs Struggle with Long In-context Learning") and Table [6](#S3.T6
    "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context
    LLMs Struggle with Long In-context Learning"). For the entity recognition and
    relationship extraction dataset, we use the F1 score as the evaluation metric,
    and Accuracy is utilized for the other datasets. From the presented results, generally,
    we can find that models of Transformer-based architecture perform consistently
    better than the RNN-based ones in all the evaluated datasets. However, both of
    them are still falling behind the powerful API-based models, especially GPT4-turbo.
    For a relatively simple task like BANKING77, whose context length from 1 round
    to 5 rounds is 2K to 14 K, most of the models can benefit from the extensive context
    with more demonstrations. As shown in Figure [1](#S0.F1 "Figure 1 ‣ Long-context
    LLMs Struggle with Long In-context Learning") and Table [4](#S3.T4 "Table 4 ‣
    3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle
    with Long In-context Learning"), from 2K to 4K, there is either a huge increase
    nearly doubling the accuracy, or a complete failure for most of the open-source
    models. After 3 rounds, limited performance gain can be achieved by adding more
    examples. When it comes to more complicated tasks like TacRED and DialogueRE in
    Table [4](#S3.T4 "Table 4 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation
    ‣ Long-context LLMs Struggle with Long In-context Learning") and Table [6](#S3.T6
    "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context
    LLMs Struggle with Long In-context Learning"), which are more urgently requiring
    the capability of long-context comprehension, the overall performance of all the
    few-shot models drops compared to BANKING77\. As shown in the middle plot of Figure [1](#S0.F1
    "Figure 1 ‣ Long-context LLMs Struggle with Long In-context Learning"), only GPT4-turbo
    can consistently benefit from more demonstrations, all of the other models reach
    their peak at the middle with context length around 20K.
  prefs: []
  type: TYPE_NORMAL
- en: For the most challenging Discovery dataset, which has an extremely large label
    space including 174 classes, one round of traversing for all the label possibilities
    has already made up a context length of 10K. In this extreme case, all of the
    models, including GPT4-turbo, fail to tell the difference among the fine-grained
    types, leading to a score of 0\. The results across different datasets reveal
    the models’ capability to understand different types of tasks. Our initial hypothesis
    suggests that the strongest LLMs like GPT-4-turbo are capped at a certain complexity
    level between DialogRE and Discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting observation we have is that some LLMs’ performance on the
    extreme-label ICL seems highly predictable. According to [Figure 3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ Long-context LLMs Struggle with Long In-context Learning"),
    the performance of Qwen and Mistral are almost linear w.r.t the demonstration
    length. This reveals that there might be an underlying mathematical relation between
    performance and the task complexity for ICL.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 2K | 4K | 7K | 9K | 14K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32K | 30.2 | 70.4 | 72.0 | 75.6 | 77.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32K | 16.6 | 23.2 | 22.4 | 22.8 | 8.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32K | 21.6 | 52.8 | 61.4 | 66.0 | 67.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 29.8 | 43.6 | 66.4 | 67.8 | 64.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200K | 25.8 | 0 | 0 | 0 | 1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200K | 5.6 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 3.0 | 19.4 | 28.0 | 31.6 | 32.6 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 4K | 8.6 | 21.2 | 0.4 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.0-Pro | N/A | 32K | 33.4 | 41.4 | 40.6 | 45.6 | 50.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128K | 73.5 | 80.5 | 82.0 | 83.5 | 84.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SoTA (RoBERTA + ICDA) | N/A | - | 94.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: BANKING77 result with respect to increasing context length. 1R represents
    one round of traversing all the instances with unique label.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 4K | 7K | 10K | 14K | 18K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8K | 0.4 | 0.4 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0.4 | 0.4 | 0.8 | 0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32K | 29.7 | 36.1 | 38.9 | 40.1 | 25.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32K | 38.7 | 47.3 | 45.2 | 43.6 | 40.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 53.3 | 53.1 | 51.6 | 48.0 | 42.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200K | 5.6 | 1.9 | 8.0 | 9.5 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200K | 29.6 | 27.2 | 15.5 | 10.7 | 8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 3.8 | 7.1 | 4.1 | 6.6 | 4.9 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 1K | 2.3 | 2.6 | 1.0 | 0 | 1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.0-Pro | N/A | 32K | 71.4 | 77.8 | 78.2 | 77.4 | 76.8 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128K | 74.4 | 76.5 | 79.5 | 80.4 | 84.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SoTA (DeepStruct) | N/A | - | 76.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: TacRED result with respect to increasing context length.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 8K | 13K | 19K | 25K | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8K | 16.3 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32K | 6.9 | 13.9 | 6.3 | 5.7 | 5.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32K | 5.1 | 8.9 | 8.8 | 12.4 | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32K | 14.4 | 18.4 | 15.5 | 16.4 | 13.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 24.3 | 23.2 | 23.4 | 22.3 | 21.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200K | 0 | 0 | 0.8 | 0.8 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200K | 12.2 | 13.4 | 6.4 | 2.1 | 1.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 4.0 | 3.8 | 3.0 | 6.4 | 2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 4K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.0-Pro | N/A | 32K | 23.6 | 29.2 | 33.2 | 26.1 | 17.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128K | 43.5 | 48.8 | 53.6 | 60.2 | 60.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SoTA (HiDialog) | N/A | - | 77.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: DialogRE result with respect to increasing context length.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 10K | 20K | 30K | 40K | 50K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0 | 0 | 0 | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32k | 0 | 1.0 | 0 | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200k | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 4K | 0 | 0.2 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.0-Pro | N/A | 32K | 0 | 0 | 0 | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128K | 1.5 | 0.5 | 0.5 | 0.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| SoTA (MTL) | N/A | - | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Discovery result with respect to increasing context length.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Exploratory Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by the Lost in the Middle phenomenon  Liu et al. ([2023](#bib.bib27)),
    we take analysis experiments to explore whether the position distribution of the
    instances will make a difference in the performance for long in-context learning
    with extreme-label classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Scattered Distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e1eba4535480ba098856774d54e32f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Visualization of accuracy for every class when instances from the
    same class are scattered V.S. grouped in the demonstration prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: In our investigation, we conducted pilot experiments on TacRED, a medium-complexity
    dataset, with each label type demonstrated three times, resulting in a total of
    123 distinct instances (calculated as $41\times 3$). Within these experiments,
    instances bearing the same labels are distributed randomly to form a scattered
    configuration. For each instance, we track its relative position within the prompt
    alongside its corresponding label, thereafter computing the accuracy for each
    label class. As illustrated in the first row of Figure [4](#S4.F4 "Figure 4 ‣
    4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle
    with Long In-context Learning"), the visualization delineates the accuracy of
    each label, aligned with its position within the prompt, where diverse colors
    symbolize various label types. In scenarios where class instances are scattered,
    certain models, such as InternLM2-7B-base, demonstrate acceptable performances—approximately
    60% accuracy merely on specific labels, as highlighted by a red circle in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context
    LLMs Struggle with Long In-context Learning"), regardless of the instance placements.
    Conversely, other models, like ChatGLM3-6B-32K, exhibit robust performance across
    a broad spectrum of labels. Remarkably, the GPT4-turbo model consistently surpasses
    an 80% accuracy threshold for the majority of label types, with only a minimal
    count of exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Grouped Distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To facilitate a clear comparison between scattered and grouped distributions,
    we organize instances of the same class to be adjacent within the demonstration
    prompts. The impact of this reorganization on model performance, both pre and
    post-grouping, is presented in Table [7](#S4.T7 "Table 7 ‣ 4.2 Grouped Distribution
    ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle with Long In-context Learning").
    A pronounced trend emerges, highlighting a general decline in performance across
    most models after grouping instances by class. Notably, models such as Mistral-7B-v0.2-base
    and InternLM2-7B-base exhibit significant performance drops, underscoring a pronounced
    sensitivity to instance grouping. In an effort to delve deeper into this phenomenon,
    we visualize the accuracy of grouped labels in relation to their positions within
    the prompt, as illustrated in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Scattered Distribution
    ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle with Long In-context Learning").
    This visualization reveals that instances of the same class, denoted by dots of
    the same color, are positioned nearby. It became evident that some models, like
    InternLM2-7B-base, demonstrate high sensitivity to the distribution of instances,
    only handling instances with labels positioned at the end of the prompt. Conversely,
    other open-source models such as ChatGLM3-6B-32K, with a modest 3.3% drop in accuracy,
    proved to be more resilient to changes in instance positioning, maintaining high
    performance across varied positions. Surprisingly, even the GPT4-turbo is not
    immune to the challenges posed by grouped distributions, experiencing a notable
    decline in performance by 20.3%. This observed decrease in performance is consistent
    across models, unaffected by the specific positions of the labels within the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | Scatter | Grouped | $\Delta$ |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 10K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0.4 | 3.0 | +2.6 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32K | 38.9 | 35.6 | -3.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32K | 45.2 | 33.0 | -12.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 51.6 | 5.1 | -46.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200K | 8.0 | 0 | -8 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200K | 15.5 | 4.8 | -9.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 4.1 | 0 | -4.1 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 4K | 1.0 | 3.6 | +2.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128K | 79.5 | 59.2 | -20.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Exploratory Result on TacRED 3 Round. Grouped means forcing the same-typed
    demonstration examples near by each other instead of randomly distributing in
    the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In summary, our research explores the capability of large language models on
    long in-context learning tasks, particularly in extreme-label classification scenarios.
    We curate a dataset LongICLBench consisting of long in-context learning tasks
    with different difficulty levels with respect to the context length. Through our
    study, we have discovered that while LLMs show promising performance on inputs
    up to 20K tokens, their ability to process and understand longer sequences significantly
    decreases. Our exploratory experiments further highlight the impact of the distribution
    of examples within prompts on model performance. We hope LongICLBench and our
    findings contribute to the ongoing efforts to enhance LLMs’ understanding of long
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI et al. (2024) 01\. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,
    Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open
    foundation models by 01.ai, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai
    Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized
    evaluation for long context language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz,
    Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer,
    and Behnam Neyshabur. Exploring length generalization in large language models.
    In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), *Advances
    in Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=zSkYVeX7bC4](https://openreview.net/forum?id=zSkYVeX7bC4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. Qwen technical report, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023b) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai
    Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong,
    Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long
    context understanding, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatia et al. (2015) Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma,
    and Prateek Jain. Sparse local embeddings for extreme multi-label classification.
    In *Neural Information Processing Systems*, 2015. URL [https://api.semanticscholar.org/CorpusID:11419932](https://api.semanticscholar.org/CorpusID:11419932).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2024) Zheng Cai, Maosong Cao, Haojiong Chen, …, Yu Qiao, and Dahua
    Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Casanueva et al. (2020) Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew
    Henderson, and Ivan Vulić. Efficient intent detection with dual sentence encoders.
    In Tsung-Hsien Wen, Asli Celikyilmaz, Zhou Yu, Alexandros Papangelis, Mihail Eric,
    Anuj Kumar, Iñigo Casanueva, and Rushin Shah (eds.), *Proceedings of the 2nd Workshop
    on Natural Language Processing for Conversational AI*, pp.  38–45, Online, July
    2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.nlp4convai-1.5.
    URL [https://aclanthology.org/2020.nlp4convai-1.5](https://aclanthology.org/2020.nlp4convai-1.5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *ArXiv*, abs/2306.15595, 2023a. URL [https://api.semanticscholar.org/CorpusID:259262376](https://api.semanticscholar.org/CorpusID:259262376).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A
    Smith, and Matt Gardner. A dataset of information-seeking questions and answers
    anchored in research papers. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pp.  4599–4610, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Demszky et al. (2020) Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko,
    Alan Cowen, Gaurav Nemade, and Sujith Ravi. GoEmotions: A dataset of fine-grained
    emotions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.),
    *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pp.  4040–4054, Online, July 2020\. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.acl-main.372. URL [https://aclanthology.org/2020.acl-main.372](https://aclanthology.org/2020.acl-main.372).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2021) Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han,
    Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-NERD: A few-shot named entity
    recognition dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli
    (eds.), *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pp.  3198–3213, Online, August 2021\. Association for
    Computational Linguistics. doi: 10.18653/v1/2021.acl-long.248. URL [https://aclanthology.org/2021.acl-long.248](https://aclanthology.org/2021.acl-long.248).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2024) Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan
    Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context
    window beyond 2 million tokens. *arXiv preprint arXiv:2402.13753*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context.
    *arXiv preprint arXiv:2402.10171*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling
    with selective state spaces. *arXiv preprint arXiv:2312.00752*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2022) Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and
    Furu Wei. Structured prompting: Scaling in-context learning to 1, 000 examples.
    *ArXiv*, abs/2212.06713, 2022. URL [https://api.semanticscholar.org/CorpusID:254591686](https://api.semanticscholar.org/CorpusID:254591686).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more
    tokens: the impact of input length on the reasoning performance of large language
    models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length
    of open-source LLMs truly promise? In *NeurIPS 2023 Workshop on Instruction Tuning
    and Instruction Following*, 2023a. URL [https://openreview.net/forum?id=LywifFNXV5](https://openreview.net/forum?id=LywifFNXV5).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle:
    Can long-context language models understand long contexts?, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023c) Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang,
    Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples,
    2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Eneko
    Agirre, Marianna Apidianaki, and Ivan Vulić (eds.), *Proceedings of Deep Learning
    Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration
    for Deep Learning Architectures*, pp.  100–114, Dublin, Ireland and Online, May
    2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10.
    URL [https://aclanthology.org/2022.deelio-1.10](https://aclanthology.org/2022.deelio-1.10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang,
    Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et al. E^ 2-llm: Efficient
    and extreme length extension of large language models. *arXiv preprint arXiv:2401.06951*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *Transactions of the Association for Computational Linguistics*,
    12:157–173, 2023. URL [https://api.semanticscholar.org/CorpusID:259360665](https://api.semanticscholar.org/CorpusID:259360665).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Milios et al. (2023) Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. In-context
    learning for text classification with many labels, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mohtashami & Jaggi (2023) Amirkeivan Mohtashami and Martin Jaggi. Landmark
    attention: Random-access infinite context length for transformers. In *Workshop
    on Efficient Systems for Foundation Models@ ICML2023*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nallapati et al. (2017) Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner:
    A recurrent neural network based sequence model for extractive summarization of
    documents. In *Proceedings of the AAAI conference on artificial intelligence*,
    volume 31, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orvieto et al. (2023) Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando,
    Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks
    for long sequences. *ArXiv*, abs/2303.06349, 2023. URL [https://api.semanticscholar.org/CorpusID:257496654](https://api.semanticscholar.org/CorpusID:257496654).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023a) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel
    Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski,
    et al. Rwkv: Reinventing rnns for the transformer era. In *Findings of the Association
    for Computational Linguistics: EMNLP 2023*, pp.  14048–14077, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023b) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2023c) Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia
    Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, and Juanzi Li. When does
    in-context learning fall short and why? a study on specification-heavy tasks,
    2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test
    long: Attention with linear biases enables input length extrapolation. In *International
    Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    Parallel context windows for large language models. In Anna Rogers, Jordan Boyd-Graber,
    and Naoaki Okazaki (eds.), *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  6383–6402, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.352.
    URL [https://aclanthology.org/2023.acl-long.352](https://aclanthology.org/2023.acl-long.352).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
    Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,
    Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas
    Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sileo et al. (2019) Damien Sileo, Tim Van De Cruys, Camille Pradel, and Philippe
    Muller. Mining discourse markers for unsupervised sentence representation learning.
    In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 
    3477–3486, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1351. URL [https://aclanthology.org/N19-1351](https://aclanthology.org/N19-1351).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.
    Roformer: Enhanced transformer with rotary position embedding. *ArXiv*, abs/2104.09864,
    2021. URL [https://api.semanticscholar.org/CorpusID:233307138](https://api.semanticscholar.org/CorpusID:233307138).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara
    Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
    Long range arena : A benchmark for efficient transformers. In *International Conference
    on Learning Representations*, 2021. URL [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive
    training for context scaling, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vulić et al. (2021) Ivan Vulić, Pei-Hao Su, Samuel Coope, Daniela Gerz, Paweł
    Budzianowski, Iñigo Casanueva, Nikola Mrkšić, and Tsung-Hsien Wen. ConvFiT: Conversational
    fine-tuning of pretrained language models. In Marie-Francine Moens, Xuanjing Huang,
    Lucia Specia, and Scott Wen-tau Yih (eds.), *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pp.  1151–1168, Online and
    Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.emnlp-main.88. URL [https://aclanthology.org/2021.emnlp-main.88](https://aclanthology.org/2021.emnlp-main.88).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023.
    URL [https://openreview.net/forum?id=BryMFPQ4L6](https://openreview.net/forum?id=BryMFPQ4L6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    Self-adaptive in-context learning: An information compression perspective for
    in-context example selection and ordering, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2024) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks. In *The
    Twelfth International Conference on Learning Representations*, 2024. URL [https://openreview.net/forum?id=NG7sS51zVF](https://openreview.net/forum?id=NG7sS51zVF).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, et al. Effective long-context scaling of foundation models. *arXiv preprint
    arXiv:2309.16039*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2020) Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. Dialogue-based
    relation extraction. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault
    (eds.), *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pp.  4927–4940, Online, July 2020\. Association for Computational
    Linguistics. doi: 10.18653/v1/2020.acl-main.444. URL [https://aclanthology.org/2020.acl-main.444](https://aclanthology.org/2020.acl-main.444).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open
    bilingual pre-trained model. In *The Eleventh International Conference on Learning
    Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. $\infty$bench: Extending long context evaluation beyond 100k tokens, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and
    Christopher D. Manning. Position-aware attention and supervised data improve slot
    filling. In *Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2017)*, pp.  35–45, 2017. URL [https://nlp.stanford.edu/pubs/zhang2017tacred.pdf](https://nlp.stanford.edu/pubs/zhang2017tacred.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2024) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu
    Wei, and Sujian Li. PoSE: Efficient context window extension of LLMs via positional
    skip-wise training. In *The Twelfth International Conference on Learning Representations*,
    2024. URL [https://openreview.net/forum?id=3Z1gxuAQrA](https://openreview.net/forum?id=3Z1gxuAQrA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Additional Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We list a few additional datasets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: GoEmotions  (Demszky et al., [2020](#bib.bib13)) is the largest manually annotated
    dataset of 58k English comments from Reddit, which is labeled into 27 emotion
    categories or Neutral. There are 27 types of emotion types and drop the rare ones
    with few examples. Each selected example contains 28 tokens on average.
  prefs: []
  type: TYPE_NORMAL
- en: Few-NERD  (Ding et al., [2021](#bib.bib14)) is a large-scale human-annotated
    name entity recognition dataset with a hierarchy of 8 coarse-grained and 66 fine-grained
    entity types. Each of the instances is a paragraph with approximately 61 tokens
    on average and contains one or multiple entity names as the ground truth answer.
    There are 66 types of entities in the collection.
  prefs: []
  type: TYPE_NORMAL
- en: The performance for the two tasks is demonstrated in Table [8](#A1.T8 "Table
    8 ‣ A.1 Additional Datasets ‣ Appendix A Appendix ‣ Long-context LLMs Struggle
    with Long In-context Learning") and Table [9](#A1.T9 "Table 9 ‣ A.1 Additional
    Datasets ‣ Appendix A Appendix ‣ Long-context LLMs Struggle with Long In-context
    Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 0.8K | 1.6K | 2.4K | 3.2K | 4K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0 | 0 | 0.2 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32K | 22.0 | 17.0 | 15.0 | 12.6 | 10.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32K | 14.8 | 18.2 | 18.6 | 19.0 | 14.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 2.6 | 11.4 | 7.4 | 11.6 | 12.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200K | 0 | 0 | 0.8 | 4.0 | 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 0 | 0 | 0 | 0.2 | 0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 4K | 8.8 | 7.4 | 4.6 | 5.2 | 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.0-Pro | N/A | 32K | 20.3 | 21.4 | 22.4 | 24.4 | 24.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128K | 36.5 | 34.4 | 35.0 | 33.3 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SoTA (BERT) | N/A | - | 58.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: GoEmotion Result.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  prefs: []
  type: TYPE_TB
- en: '| Context Tokens |  |  | 5K | 9K | 14K | 19K | 24K |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-7B-base | 7B | 8k | 44.0 | 44.2 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-32K | 7B | 32k | 36.9 | 40.8 | 41.1 | 41.6 | 41.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32K | 6B | 32k | 24.1 | 9.3 | 23.6 | 10.4 | 1.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5-7B-base | 7B | 32k | 40.0 | 46.4 | 47.6 | 47.3 | 47.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 42.2 | 47.4 | 48.9 | 50.0 | 50.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-6B-200K | 6B | 200k | 34.3 | 40.2 | 44.8 | 42.3 | 43.2 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7B-base | 7B | 200k | 43.6 | 46.2 | 46.5 | 47.8 | 48.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Long-LLaMA-code-7B | 7B | 256K | 22.3 | 25.5 | 26.5 | 29.4 | 27.0 |'
  prefs: []
  type: TYPE_TB
- en: '| RWKV-5-World | 7B | 1k | 13.9 | 0 | 0 | 0.7 | 9.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Mamba-2.8B | 2.8B | 2k | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.0-Pro | N/A | 32k | 36.8 | 26.1 | 28.5 | 27.4 | 28.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | N/A | 128k | 53.4 | 55.3 | 56.2 | 55.6 | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SoTA (PL-Marker) | N/A | - | 70.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Few-NERD Result.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompting Template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Dataset | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotion | Given a comment, please predict the emotion category of this
    comment. The prediction answer must come from the demonstration examples with
    the exact format. The examples are as follows: {comment: ”…comment…”'
  prefs: []
  type: TYPE_NORMAL
- en: 'emotion category: ”…emotion…”'
  prefs: []
  type: TYPE_NORMAL
- en: '} $\times$ repeat n times |'
  prefs: []
  type: TYPE_NORMAL
- en: '| BANKING77 | Given a customer service query, please predict the intent of
    the query. The predicted answer must come from the demonstration examples with
    the exact format. The examples are as follows: {service query: ”…service…”'
  prefs: []
  type: TYPE_NORMAL
- en: 'intent category: ”…intent…”'
  prefs: []
  type: TYPE_NORMAL
- en: '} $\times$ repeat n times |'
  prefs: []
  type: TYPE_NORMAL
- en: '| TacRED | Given a sentence and a pair of subject and object entities within
    the sentence, please predict the relation between the given entities. The examples
    are as follows: {sentence: ”…sentence…'
  prefs: []
  type: TYPE_NORMAL
- en: the subject is ”…subject…”
  prefs: []
  type: TYPE_NORMAL
- en: the object is ”…object…”
  prefs: []
  type: TYPE_NORMAL
- en: 'the relation between the two entities is: ”…relation…”'
  prefs: []
  type: TYPE_NORMAL
- en: '} $\times$ repeat n times |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Few-NERD | Given the sentence, please find the name entities in the sentence
    and their corresponding entity types in the strict format of the given examples
    as following (Entity: EntityType): {”…entity…”: ”…entity type…”'
  prefs: []
  type: TYPE_NORMAL
- en: '} $\times$ repeat n times |'
  prefs: []
  type: TYPE_NORMAL
- en: '| DialogRE | Given the dialogue, please find the name pair entities in the
    dialogue and their corresponding relation types in the strict format of given
    examples as following (note that the number of entities has to strictly have the
    same value as the number of respective relation): {Dialogue:'
  prefs: []
  type: TYPE_NORMAL
- en: ”…dialogue…”
  prefs: []
  type: TYPE_NORMAL
- en: The list of entity pairs are ”…(subject1, object1), (subject2, object2), etc…
  prefs: []
  type: TYPE_NORMAL
- en: 'The ”…number of pairs…” respective relations between each entity pair are:
    ”…relation, relation2, etc…'
  prefs: []
  type: TYPE_NORMAL
- en: '} $\times$ repeat n times |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Discovery | Given two sentence1 and sentence2, please predict the conjunction
    word between the two sentences. The predicted answer must come from the demonstration
    examples with the exact format. The examples are as follows: {”…sentence1…” (
    ) ”…sentence2…”'
  prefs: []
  type: TYPE_NORMAL
- en: the conjunction word in ( ) is ”…conjunction…”
  prefs: []
  type: TYPE_NORMAL
- en: '} $\times$ repeat n times |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: The data prompt format of each dataset. Each dataset has a unique
    prompt format to effectively utilize the context and format of its respective
    data to get the best output response.'
  prefs: []
  type: TYPE_NORMAL
- en: The prompting template for each of the datasets is presented at Table [10](#A1.T10
    "Table 10 ‣ A.2 Prompting Template ‣ Appendix A Appendix ‣ Long-context LLMs Struggle
    with Long In-context Learning")
  prefs: []
  type: TYPE_NORMAL
