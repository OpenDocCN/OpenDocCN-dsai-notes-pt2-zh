- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:00:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window
    Does Not Mean LLMs Can Analyze Long Sequences Flawlessly'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01866](https://ar5iv.labs.arxiv.org/html/2408.01866)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peyman Hosseini¹, Ignacio Castro¹, Iacopo Ghinassi¹, Matthew Purver^(1, 2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹School of EECS, Queen Mary University of London, London, UK
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia
  prefs: []
  type: TYPE_NORMAL
- en: '{s.hosseini, i.castro, i.ghinassi m.purver}@qmul.ac.uk'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have demonstrated remarkable capabilities in comprehending
    and analyzing lengthy sequential inputs, owing to their extensive context windows
    that allow processing millions of tokens in a single forward pass. However, this
    paper uncovers a surprising limitation: LLMs fall short when handling long input
    sequences. We investigate this issue using three datasets and two tasks (sentiment
    analysis and news categorization) across various LLMs, including Claude 3, Gemini
    Pro, GPT 3.5 Turbo, Llama 3 Instruct, and Mistral Instruct models. To address
    this limitation, we propose and evaluate ad-hoc solutions that substantially enhance
    LLMs’ performance on long input sequences by up to 50%, while reducing API cost
    and latency by up to 93% and 50%, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window
    Does Not Mean LLMs Can Analyze Long Sequences Flawlessly'
  prefs: []
  type: TYPE_NORMAL
- en: Peyman Hosseini¹, Ignacio Castro¹, Iacopo Ghinassi¹, Matthew Purver^(1, 2) ¹School
    of EECS, Queen Mary University of London, London, UK ²Department of Knowledge
    Technologies, Jožef Stefan Institute, Ljubljana, Slovenia {s.hosseini, i.castro,
    i.ghinassi m.purver}@qmul.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have demonstrated remarkable capabilities in natural language understanding
    and generation tasks. Leveraging extensive pretraining on massive text corpora,
    the new generation of LLMs can perform a wide range of language tasks with minimal
    task-specific fine-tuning. Additionally, these LLMs are equipped with behemothic
    context windows that enable them to analyze inputs spanning up to tens or hundreds
    of pages in one forward pass. In this paper, we study the performance of Claude
    3 Haiku Anthropic ([2024](#bib.bib1)), GPT3.5-Turbo OpenAI ([2022](#bib.bib19)),
    Gemini-1.0-pro Team et al. ([2023](#bib.bib20)), Llama 3 8b Instruct FacebookResearch
    ([2024](#bib.bib7)), and Mistral 7b Instruct Jiang et al. ([2023](#bib.bib12)).
    These are equipped with context windows supporting up to 200,000, 16,000, 32,000,
    160,000, and 32,000 tokens respectively on long-form text inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Related Work.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Despite being equipped with context windows capable of supporting a huge snippet
    of text, the performance of LLMs on lengthy input sequences has been a subject
    of ongoing research Li et al. ([2023](#bib.bib14), [2024](#bib.bib15)). Different
    prompting strategies have emerged as a promising avenue for improving LLM performance
    by providing concise and informative input(Liu et al., [2023](#bib.bib16); Brown
    et al., [2020](#bib.bib5)). These strategies involve extracting key information
    from the input text and presenting it to the LLM in a structured manner.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization techniques play a crucial role in many natural language processing
    tasks by condensing lengthy inputs into more manageable snippets. Extractive Summarization
    methods such as TextRank Mihalcea and Tarau ([2004](#bib.bib18)) are widely used
    to identify and extract the most significant sentences from a document for different
    purposes from summarizing dialogues Feng et al. ([2022](#bib.bib8)) and scientific
    documents Cachola et al. ([2020](#bib.bib6)) to assessing content credibility
    Balcerzak et al. ([2014](#bib.bib2)) and creating automatic writing tools Wang
    et al. ([2020](#bib.bib21)). In this paper, we design and use summarisation pipelines
    as well as text truncation techniques to boost LLMs’ performance by optimizing
    the input while reducing their load.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06340ef66357ccc61d7bdea35cda1db6.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Pure Extractive Summarization Pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/329a4dfdab2c0a4fe107bb92442ab5a8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Diverse Summarization Pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The summarization pipelines for summarising information. The diverse
    summarization approach builds on top of the purely extractive approach but gives
    higher priority to lexical diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is a body of research dedicated to studying the limitations of LLMs on
    long sequences and proposing mitigations at both architecture-level Beltagy et al.
    ([2020](#bib.bib3)); Bertsch et al. ([2024](#bib.bib4)) as well as prompt-level
    Wei et al. ([2022](#bib.bib22)). These studies often involve defining and exploring
    overly complex problems such as those about extreme-label classification Li et al.
    ([2024](#bib.bib15)) or “Needle In a Haystack" Machlab and Battle ([2024](#bib.bib17)).
    However, a systematic study of LLM capabilities and limitations on long-form analysis
    tasks such as news categorization or sentiment analysis of long reviews which
    require a common general understanding of the input context is still lacking.
    Furthermore, the emphasis on approaches involving prompt-tuning has diverted attention
    away from optimizing and streamlining the information fed to LLMs. This study
    serves to fill these gaps by showcasing the failure of LLMs on canonical NLP tasks
    when dealing with long sequences and to ignite a spark of interest in the research
    community to explore the untapped potential of optimizing and condensing the information
    fed to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Contribution.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We systematically study the performance of state-of-the-art LLMs on sentiment
    analysis and news categorization tasks, revealing their limitations in processing
    long-form text effectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose and evaluate ad-hoc solutions using extractive and diverse summarization
    as well as selective truncation to condense input text, which substantially improves
    LLM performance by up to 50%, reduces API costs by as much as 93% and significantly
    reduces latency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present comprehensive empirical and ablation studies examining the relationship
    between input length, summarization strategies, and model performance, providing
    insights into optimal summarization approaches for LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first present our methodology on two pipelines for summarising information
    before prompting the LLM. Then we discuss all the scenarios we examine to analyze
    LLMs’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Summarization Methodology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We study two different summarization approaches for extracting key information
    from the documents and providing the input for prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Pure Extractive Summarization:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As shown in [Fig. 1(a)](#S1.F1.sf1 "In Fig. 1 ‣ Related Work. ‣ 1 Introduction
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly"), we use TextRank (Mihalcea
    and Tarau, [2004](#bib.bib18)), a well-known unsupervised extractive summarization
    algorithm, to select the most important sentences. TextRank uses a graph-based
    ranking model to measure the similarity between sentences and their centrality
    within the graph. We then write the instruction to the LLM (i.e., categorize or
    rate) and append the extracted sentences as input.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Diverse Summarization:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Build on top of the previous approach. We discard the least relevant sentences
    using TextRank ranking. Then we use TF-IDF to represent the sentences as vectors
    and calculate the diversity scores based on the dissimilarity between sentences
    using cosine similarity. The top N sentences with the highest diversity scores
    are chosen as the input used in prompting (see [§ A.2.1](#A1.SS2.SSS1 "A.2.1 Diverse
    Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly")). This extension maximises the diversity of
    the information for the LLMs instead of providing the LLMs with a concise summary.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Prompting Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To investigate the performance of LLMs on sentiment analysis and news categorization
    tasks involving long input sequences, we employ 7 prompting strategies and evaluate
    their effectiveness on three datasets, which we introduce in the next section.
    These prompting strategies include:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Full Context: The entire lengthy review is provided as input for analysis (Motiv.:
    the baseline approach for comparison with other methods).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Full Context + Summary: The $N$-sentence summary extracted using [Fig. 1(a)](#S1.F1.sf1
    "In Fig. 1 ‣ Related Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly") pipeline is appended to the lengthy review. (Motiv.: how does emphasizing
    a selected summary with repetition affect the performance?)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First Sentences: We crop the initial $N$ sentences from the text and provide
    it as input. (Motiv.: how does choosing the ‘opening’ section of a lengthy review
    affect the performance?)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Last Sentences: We crop the ending $N$ sentences from the text and provide
    it as input. (Motiv.: how does choosing the ‘ending’ section of a lengthy review
    affect the performance?)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Summary: We provide the extracted $N$ sentence summary ([Fig. 1(a)](#S1.F1.sf1
    "In Fig. 1 ‣ Related Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly")) as input. (Motiv.: how does choosing a summary affect performance?)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Diverse Summary: We provide the extracted $N$-sentence summary ([Fig. 1(b)](#S1.F1.sf2
    "In Fig. 1 ‣ Related Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly")) as input. (Motiv.: how does giving more priority to lexical diversity
    in the summary affect performance?)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Random Sampling: We randomly select $N$ sentences from the document. (Motiv.:
    how does randomly choosing a short snippet perform in comparison to providing
    the full context?)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now introduce the datasets used in this paper (see [§ A](#A1 "Appendix A
    Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly") for more details).
    For each dataset, as our interest is in LLM performance with long inputs, we use
    only the subsets of the data that exceed a minimum length. We report the average
    length of the studied subset in terms of the number of tokens in [Tabs. 6](#A1.T6
    "In A.2.1 Diverse Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly"), [4](#A1.T4 "Tab. 4 ‣ BBC
    News Archive. ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly") and [5](#A1.T5 "Tab. 5 ‣ BBC News Archive.
    ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly").'
  prefs: []
  type: TYPE_NORMAL
- en: 'GameSpot Reviews GameSpot ([2024](#bib.bib9)):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: more than 12,000 long game reviews with a sentiment score assigned by the author
    ranging from 1 to 100.
  prefs: []
  type: TYPE_NORMAL
- en: '20 Newsgroups Lang ([1995](#bib.bib13)):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: nearly 20,000 news documents belonging to 20 different topic categories. High-level
    topics include politics, religion, sports, and computers.
  prefs: []
  type: TYPE_NORMAL
- en: 'BBC News Archive Greene and Cunningham ([2006](#bib.bib11)):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2,225 BBC articles covering business, entertainment, politics, sport, and tech.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluated the performance of Claude 3 Haiku, Gemini-1.0-Pro, GPT-3.5 Turbo,
    Llama 3 8b Instruct, and Mistral 7b Instruct on the datasets and tasks detailed
    in [§ 3.1](#S3.SS1 "3.1 Datasets ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly"), using the prompting scenarios discussed in [§ 2.2](#S2.SS2 "2.2
    Prompting Scenarios ‣ 2 Methodology ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly").
    The summarized results are available in [Tabs. 1](#S3.T1 "In 3.2.1 Sentiment Analysis
    ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"),
    [2](#S3.T2 "Tab. 2 ‣ 3.2.2 News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly") and [3](#S3.T3 "Tab. 3 ‣
    3.2.2 News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly"). More detailed analysis for each LLM is available
    in [Tabs. 6](#A1.T6 "In A.2.1 Diverse Summarization ‣ A.2 News Categorization
    ‣ Appendix A Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs:
    Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"),
    [4](#A1.T4 "Tab. 4 ‣ BBC News Archive. ‣ A.2 News Categorization ‣ Appendix A
    Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly") and [5](#A1.T5
    "Tab. 5 ‣ BBC News Archive. ‣ A.2 News Categorization ‣ Appendix A Appendix ‣
    Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly") in [§ A](#A1 "Appendix A
    Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Sentiment Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Scenario | MSE | MAE | Accuracy | Avg. Lat. | Inp. Len. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 272.8 (6) | 11.7 (6) | 36.0 (7) | 1.27 (6) | 2120 (6) |'
  prefs: []
  type: TYPE_TB
- en: '| Full+Sum. | 403.1 (7) | 13.5 (7) | 38.2 (6) | 1.33 (7) | 2450 (7) |'
  prefs: []
  type: TYPE_TB
- en: '| First Sent. | 169.1 (5) | 9.9 (5) | 41.2 (5) | 0.82 (1) | 320 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Last Sent. | 99.6 (1) | 7.9 (1) | 50.7 (2) | 0.82 (1) | 320 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Sum. | 124.2 (2) | 8.8 (4) | 48.2 (4) | 0.82 (1) | 320 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Div. Sum. | 133.7 (4) | 8.6 (2) | 54.0 (1) | 0.82 (1) | 320 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Rand. Samp. | 129.6 (3) | 8.7 (3) | 50.0 (3) | 0.82 (1) | 320 (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Average performance of different LLMs for Sentiment Analysis on GameSpot
    over 5 runs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Tab. 1](#S3.T1 "In 3.2.1 Sentiment Analysis ‣ 3.2 Experiments
    ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"), both Full and
    Full+Sum approaches failed to perform favourably in predicting the article scores.
    However, extracting a subset of the input text, and providing it in the prompt,
    even through randomly sampling sentences, yielded superior performance. The ‘Last
    Sent.’ scenario performs the best in both loss metrics while the ‘Div Sum.’ achieves
    the highest accuracy by a substantial margin, performing 50% better than when
    providing the LLM with Full Context. Detailed analysis of the performance for
    each LLM on the GameSpot dataset is available in [Tab. 6](#A1.T6 "In A.2.1 Diverse
    Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 News Categorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluated the performance of LLMs on two news categorization datasets. [Tabs. 2](#S3.T2
    "In 3.2.2 News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly") and [3](#S3.T3 "Tab. 3 ‣ 3.2.2 News Categorization
    ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly")
    summarize these results across different prompting scenarios. Detailed analyses
    for each LLM for both experiments are available in [Tabs. 4](#A1.T4 "In BBC News
    Archive. ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly") and [5](#A1.T5 "Tab. 5 ‣ BBC News Archive.
    ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly") in [§ A](#A1 "Appendix A Appendix ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/996b00b9bcbbd8517359b996e543c9b5.png)![Refer to caption](img/d2d2463722598a9432f38ff1f4577ff2.png)![Refer
    to caption](img/686f7df516d501cd64167a52c8e819de.png)![Refer to caption](img/383fad1446ffb9fe6e8cc1fdf87e25bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Ablation study on the length of the selected truncation/summary for
    different scenarios using Claude 3 Haiku over 5 runs with 85% Confidence Intervals.
    The results show the efficacy of approaches optimizing LLMs’ input. ‘Full’ context
    performs poorly on all metrics. Additionally, after the length of input surpasses
    10 sentences, less meaningful improvement in the performance of all scenarios
    is observed. A similar trend is seen for all LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 0.27 (7) | 35.2 (7) | 1.58 (6) | 3450 (6) |'
  prefs: []
  type: TYPE_TB
- en: '| Full+Sum. | 0.30 (3) | 38.2 (5) | 1.94 (7) | 3700 (7) |'
  prefs: []
  type: TYPE_TB
- en: '| First Sent. | 0.30 (3) | 39.1 (3) | 0.79 (1) | 240 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Last Sent. | 0.29 (5) | 39.1 (3) | 0.79 (1) | 240 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Sum. | 0.31 (1) | 39.4 (2) | 0.79 (1) | 240 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Div. Sum. | 0.31 (1) | 39.5 (1) | 0.79 (1) | 240 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Rand. Samp. | 0.29 (5) | 37.6 (6) | 0.79 (1) | 240 (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Average performance of different LLMs for news categorization on 20
    NewsGroup over 5 runs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the 20 NewsGroup dataset (see [Tab. 2](#S3.T2 "In 3.2.2 News Categorization
    ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"))
    both ‘Sum.’ and ‘Div. Sum.’ approaches achieve the highest F1 scores. With a 39.5%
    accuracy, ‘Div. Sum’ outperforms all other scenarios in this metric. Importantly,
    all approaches achieve better results than providing the full context to the LLM,
    showing the effectiveness of summarising the information provided to the LLM for
    this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 0.51 (6) | 54.0 (6) | 1.07 (6) | 1150 (6) |'
  prefs: []
  type: TYPE_TB
- en: '| Full+Sum. | 0.50 (7) | 53.7 (7) | 1.72 (7) | 1400 (7) |'
  prefs: []
  type: TYPE_TB
- en: '| First Sent. | 0.61 (1) | 64.5 (1) | 0.78 (1) | 230 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Last Sent. | 0.53 (4) | 57.9 (4) | 0.78 (1) | 230 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Sum. | 0.56 (3) | 59.8 (3) | 0.78 (1) | 230 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Div. Sum. | 0.58 (2) | 60.9 (2) | 0.78 (1) | 230 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Rand. Samp. | 0.53 (4) | 57.6 (5) | 0.78 (1) | 230 (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Average performance of different LLMs for news categorization task
    on BBC News over 5 runs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3a62e3b116d69fabde4171d4b7264465.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Performance gain (% accuracy boost) vs. normalized ARP score for
    each summarization/truncation scenario compared to the ‘Full’ baseline. Lower
    ARP scores (more cohesive corpora) generally yield higher performance gains across
    scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As summarized in [Tab. 3](#S3.T3 "In 3.2.2 News Categorization ‣ 3.2 Experiments
    ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"), we observe
    a similar trend for the BBC dataset: all approaches except ‘Full+Sum.’ outperform
    ‘Full’ scenario in all metrics. This emphasizes the importance of selectively
    summarising the information provided to LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Ablation Studies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To further investigate the performance of our models under different conditions,
    we conducted two ablation studies. First, we varied the truncation/summary lengths
    from 3 to 15 sentences for all models and evaluated their sentiment analysis performance
    on the GameSpot dataset ([Fig. 2](#S3.F2 "In 3.2.2 News Categorization ‣ 3.2 Experiments
    ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly") and [Fig. 5](#A1.F5
    "In A.2.1 Diverse Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly")). Our solutions consistently
    outperformed the baseline across different summary lengths. Second, we studied
    the effect of temperature on the Mistral Instruct and Llama 3 Instruct models
    for news categorization on the 20 NewsGroup dataset ([Fig. 4](#A1.F4 "In A.2.1
    Diverse Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient
    Solutions For An Intriguing Failure of LLMs: Long Context Window Does Not Mean
    LLMs Can Analyze Long Sequences Flawlessly")). Again, the summarization approaches
    demonstrated superior performance compared to the baseline across different temperature
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study’s findings have significant implications for developing and deploying
    LLMs. Despite their long context windows, state-of-the-art LLMs still struggle
    to effectively process long text sequences, a critical limitation under-examined
    by prior research for common NLP tasks relying on contextual understanding. Our
    results highlight the need for more research into optimizing lengthy text inputs
    to enhance LLM performance. Future work should explore the generalizability of
    these findings across diverse optimization techniques, NLP tasks, and data domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'We conducted a preliminary analysis of the correlation between document cohesiveness
    and performance gains for each corpus. Using Average Relative Proximity (ARP)
    score (Ghinassi et al., [2023](#bib.bib10)) with average cosine similarity scoring
    over 2-sentence segments, we found that more cohesive corpora (lower ARP) had
    higher average performance gains from summarization ([Fig. 3](#S3.F3 "In 3.2.2
    News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For
    An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze
    Long Sequences Flawlessly")). Future studies could establish these findings further.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper examined the performance of lengthy inputs on various LLMs (Claude
    3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct, and Mistral Instruct). We found
    that the longest inputs resulted in the worst performance. Our results were consistent
    for three datasets and two tasks. We proposed several ad-hoc solutions to substantially
    enhance LLMs’ performance (up to 50%) on long input sequences and succeeded in
    reducing API cost (by up to 93%) while reducing the average latency (up to 50%).
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rapid change of model specifications.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, we examined a diverse set of state-of-the-art large language models,
    all of which are among the most commonly used LLMs. However, the rapidly evolving
    nature of this field means these findings may not be fully generalized to future
    LLMs with different architectures and training paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks requiring general context understanding.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Second, although we evaluated performance on two core NLP tasks (sentiment analysis
    and news categorization) across three datasets, further research is needed to
    determine if our conclusions hold for a wider range of natural language understanding
    tasks and domains. The datasets we used, while lengthy, may not fully capture
    the types of long-form content that LLMs will need to process in other real-world
    applications. However, our studies here laid the foundation to show the limitations
    of LLMs when dealing with long sequences even in canonical NLP tasks as an underexplored
    problem. We believe our ad-hoc solutions are applicable to a wide variety of tasks
    that require a general understanding of the input sequence rather than a detailed
    understanding of the whole context.
  prefs: []
  type: TYPE_NORMAL
- en: Societal Impact
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In terms of societal impact, we believe our findings can help enable the more
    effective and efficient application of LLMs to tasks involving longer documents,
    which has the potential to unlock significant value in domains like business analytics,
    legal contract review, and scientific literature mining. In addition, our ad-hoc
    solutions are a step forward for democratizing access to AI. Even though using
    LLM APIs is becoming more affordable, our approaches reduce the API cost for users
    by up to 93% and this further enables more accessibility across different sections
    of society. At the same time, the ability to extract key information from lengthy
    privacy policies, terms of service, and other consumer agreements could be misused
    in ways that fail to represent the full context. As LLMs achieve greater summarization
    capabilities, extra care will be needed to ensure these summaries are accurate,
    unbiased and not misused for deceptive purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while our work provides important empirical insights into the limitations
    of current LLMs on long sequence tasks and highlights promising directions for
    overcoming these challenges, we see it as a motivating starting point rather than
    a conclusive result. We encourage the research community to further test and expand
    on our findings to drive the development of more capable and robust prompting
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Anthropic (2024) Anthropic. 2024. Claude. [https://www.anthropic.com](https://www.anthropic.com).
    Version claude-3-haiku-20240307\. Accessed: 2024-04-05 to 2024-06-11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balcerzak et al. (2014) Bartomiej Balcerzak, Wojciech Jaworski, and Adam Wierzbicki.
    2014. Application of textrank algorithm for credibility assessment. In *2014 IEEE/WIC/ACM
    International Joint Conferences on Web Intelligence (WI) and Intelligent Agent
    Technologies (IAT)*, volume 1, pages 451–454\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bertsch et al. (2024) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew
    Gormley. 2024. Unlimiformer: Long-range transformers with unlimited length input.
    *Advances in Neural Information Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cachola et al. (2020) Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld.
    2020. TLDR: Extreme summarization of scientific documents. In *Findings of the
    Association for Computational Linguistics: EMNLP 2020*, pages 4766–4777, Online.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FacebookResearch (2024) FacebookResearch. 2024. Llama3. [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3).
    Accessed: 2024-05-05 to 2024-06-11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2022) Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2022. A survey
    on dialogue summarization: Recent advances and new frontiers. In *Proceedings
    of the Thirty-First International Joint Conference on Artificial Intelligence,
    IJCAI-22*, pages 5453–5460\. International Joint Conferences on Artificial Intelligence
    Organization. Survey Track.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GameSpot (2024) GameSpot. 2024. [Gamespot reviews](https://www.gamespot.com/games/reviews/).
    Accessed: 2024-02-05 to 2024-06-11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghinassi et al. (2023) Iacopo Ghinassi, Lin Wang, Chris Newell, and Matthew
    Purver. 2023. Comparing neural sentence encoders for topic segmentation across
    domains: not your typical text similarity task. *PeerJ Computer Science*, 9:e1593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greene and Cunningham (2006) Derek Greene and Pádraig Cunningham. 2006. Practical
    solutions to the problem of diagonal dominance in kernel document clustering.
    In *Proc. 23rd International Conference on Machine learning (ICML’06)*, pages
    377–384\. ACM Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lang (1995) Ken Lang. 1995. Newsweeder: Learning to filter netnews. In *Proceedings
    of the Twelfth International Conference on Machine Learning*, pages 331–339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How long can context
    length of open-source llms truly promise? In *NeurIPS 2023 Workshop on Instruction
    Tuning and Instruction Following*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024) Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen.
    2024. Long-context llms struggle with long in-context learning. *arXiv preprint
    arXiv:2404.02060*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Comput. Surv.*,
    55(9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machlab and Battle (2024) Daniel Machlab and Rick Battle. 2024. Llm in-context
    recall is prompt dependent. *arXiv preprint arXiv:2404.08865*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mihalcea and Tarau (2004) Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing
    order into text. In *Proceedings of the 2004 Conference on Empirical Methods in
    Natural Language Processing*, pages 404–411\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) OpenAI. 2022. Gpt-3.5-turbo. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
    Accessed: 2024-03-10 to 2024-06-11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Hei-Chia Wang, Wei-Ching Hsiao, and Sheng-Han Chang. 2020.
    Automatic paper writing based on a rnn and the textrank algorithm. *Applied Soft
    Computing*, 97:106767.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Datasets and Main Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We used three datasets in our evaluations. Here, we provide a more detailed
    explanation of each dataset and task.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.1 Seniment Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GameSpot.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The GameSpot Reviews dataset contains over 12,000 lengthy video game reviews
    with author-assigned sentiment scores ranging from 1 to 100\. Almost all the reviews
    in this dataset are quite lengthy and by using a minimum threshold of 45 sentences,
    there are still thousands of reviews available in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the sentiment analysis experiments, we asked each LLM to give a rating on
    the sentiment from 1 to 100 for each document. Most labels in the data were multiples
    of 10 (i.e., 10, 20, 30, …, 90, 100). However, sometimes the labels had other
    values like 95, or 85 as well. To this end and to cover even the corner cases,
    we asked the LLM to predict the label as an integer from 1 to 100\. The calculation
    of MSE and MAE metrics is straightforward and according to the standard definition.
    For calculating the accuracy, we considered a prediction as accurate if it was
    within 5 scoring points of the label. For example, if the label has a value of
    70, a predicted label between 65-75 range is considered an accurate prediction
    and any prediction outside this range is considered not accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the temperature used in this study as well as other studies, we tried
    different temperature values for the LLMs but no significant change or decrease
    was observed by doing this and summarisation/truncation methods always showed
    superior performance. The results reported in this experiment are the average
    over 5 runs and where applicable we have reported the 85% Confidence Interval
    as well. The temperature in the experiments reported here was set to 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 News Categorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 20 NewsGroup.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The 20 Newsgroups dataset features nearly 20,000 documents across 20 topic categories
    like politics, religion, sports and computers. We focused on a subset surpassing
    60 sentences, averaging 3450 tokens per document when tokenized by the NLTK word
    tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: BBC News Archive.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The BBC News Archive, consisting of 2,225 articles covering business, entertainment,
    politics, sports and tech. We focused our study on the subset surpassing 35 sentences,
    averaging 1150 tokens per document when tokenized by the NLTK word tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we explained in the main body of the paper as well as the results shown
    in [fig. 4](#A1.F4 "In A.2.1 Diverse Summarization ‣ A.2 News Categorization ‣
    Appendix A Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long
    Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"), when
    trying different temperatures, there were no meaningful changes in the results
    and order of the approaches in terms of their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.54 | 67.8 | 1.24 | 3450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.58 | 71.2 | 1.78 | 3700 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.50 | 66.4 | 0.72 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.49 | 64.4 | 0.72 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.56 | 69.6 | 0.72 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.52 | 65.5 | 0.72 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku | Rand. Samp. | 0.50 | 64.6 | 0.72 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.38 | 47.1 | 1.19 | 3450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.41 | 53.2 | 1.95 | 3700 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.42 | 45.2 | 0.33 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.40 | 42.8 | 0.33 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.38 | 43.4 | 0.33 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.39 | 44.3 | 0.33 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 3.5 Turbo | Rand. Samp. | 0.41 | 43.1 | 0.33 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.30 | 34.6 | 3.48 | 3450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.36 | 35.2 | 3.88 | 3700 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.36 | 43.2 | 1.12 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.45 | 46.4 | 1.12 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.39 | 40.4 | 1.12 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.38 | 40.8 | 1.12 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini Pro | Rand. Samp. | 0.36 | 40.2 | 1.12 | 240 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.01 | 1.1 | 0.96 | 3450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.01 | 2.3 | 0.99 | 3650 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.03 | 11.5 | 0.87 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.04 | 12.2 | 0.87 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.05 | 12.3 | 0.87 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.04 | 12.6 | 0.87 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7b | Rand. Samp. | 0.05 | 12.2 | 0.87 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.15 | 25.6 | 1.02 | 3450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.16 | 29.3 | 1.11 | 3650 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.17 | 29.1 | 0.89 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.17 | 29.8 | 0.89 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.19 | 31.2 | 0.89 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.21 | 34.1 | 0.89 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 8b | Rand. Samp. | 0.16 | 27.8 | 0.89 | 180 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The performance of different LLMs for Categorization task on 20 NewsGroup
    dataset. $N$ parameter for summarization and truncation is set to 7 for the Mistral
    and Llama models and 10 for the others.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.63 | 63.8 | 0.69 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.67 | 67.1 | 1.45 | 1400 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.69 | 70.4 | 0.65 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.56 | 56.9 | 0.65 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.61 | 61.5 | 0.65 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.64 | 63.8 | 0.65 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku | Rand. Samp. | 0.60 | 61.2 | 0.65 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.75 | 83.8 | 0.54 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.67 | 76.4 | 1.08 | 1400 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.80 | 86.3 | 0.49 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.69 | 78.4 | 0.49 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.72 | 79.9 | 0.49 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.69 | 78.7 | 0.49 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 3.5 Turbo | Rand. Samp. | 0.68 | 77.7 | 0.49 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.65 | 61.2 | 2.26 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.69 | 66.6 | 2.61 | 1400 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.63 | 59.7 | 1.04 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.64 | 58.4 | 1.04 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.61 | 57.7 | 1.04 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.61 | 56.8 | 1.04 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini Pro | Rand. Samp. | 0.61 | 56.2 | 1.04 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.13 | 22.1 | 0.97 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.03 | 11.7 | 1.67 | 1330 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.32 | 37.1 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.23 | 32.1 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.33 | 38.5 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.35 | 39.9 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7b | Rand. Samp. | 0.28 | 33.5 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 0.38 | 39.2 | 0.89 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 0.42 | 46.6 | 1.81 | 1330 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 0.62 | 68.8 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 0.53 | 63.5 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 0.54 | 61.2 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 0.59 | 65.3 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 8b | Rand. Samp. | 0.51 | 59.2 | 0.85 | 170 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The performance of different LLMs for Categorization task on BBC News
    dataset. $N$ parameter for summarization and truncation is set to 7 for the Mistral
    and Llama models and 10 for the others.'
  prefs: []
  type: TYPE_NORMAL
- en: The results reported in this paper in the tables for the news categorization
    task are averaged over 5 runs and we have reported 85% Confidence Interval when
    applicable. The temperature of the models was set to 0.0 for the results reported
    in the tables.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.1 Diverse Summarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here we provide more explanation about the diverse summarization approach and
    how the green-coloured component (in [Fig. 1(b)](#S1.F1.sf2 "In Fig. 1 ‣ Related
    Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing Failure of LLMs:
    Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"))
    which is the diversity selector in our algorithm works.'
  prefs: []
  type: TYPE_NORMAL
- en: To write equations describing what the green component is doing, we can focus
    on the main functions in this component and their inputs and outputs. Let’s denote
    the input text as $T$.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tokenize Sentences:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $S=\mathrm{sent\_tokenize}(T)$ |  | (1) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate sentence embeddings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $E=\mathrm{TfidfVectorizer}(S)$ |  | (2) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $E$ is the TF-IDF matrix representing the embeddings of the sentences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate diversity scores:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D$ |  | (3) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle D_{\mathrm{sum}}$ |  | (4) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $D$ is the sum of dissimilarity scores for each sentence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select top N diverse sentences:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $S_{\mathrm{top_{N}}}=\arg\max_{N}(D_{\mathrm{sum}})$ |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $S_{\mathrm{top_{N}}}$ sentences with the highest diversity scores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate the final summary by joining the sentences:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathrm{summary}=\mathrm{join}(S_{\mathrm{top_{N}}})$ |  | (6) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where the final summary is the concatenation of the selected top $N$ diverse
    sentences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The main steps denoted in the above equations can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the input text $T$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the TF-IDF embedding matrix $E$ for the sentences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the dissimilarity matrix $D$ contains the pairwise dissimilarity
    scores between all sentences. In [Eqn. 4](#A1.E4 "In Item 3\. ‣ A.2.1 Diverse
    Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly"), $D\_sum$ is a vector where each element represents
    the total dissimilarity score for a particular sentence compared to all other
    sentences.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the top $N$ with the highest diversity scores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concatenate the selected sentences to form the final summary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Model | Scenario | MSE | MAE | Accuracy | Avg. Lat. | Inp. Len. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 112.3 (6) | 8.50 (6) | 43.8 (7) | 1.04 | 2120 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 80.2 (2) | 7.17 (2) | 50.6 (5) | 1.05 | 2450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 131.8 (7) | 8.87 (7) | 44.4 (6) | 0.77 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 77.0 (1) | 6.58 (1) | 60.2 (2) | 0.77 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 97.6 (3) | 7.55 (4) | 56.8 (4) | 0.77 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 106.0 (5) | 7.21 (3) | 65.2 (1) | 0.77 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku | Rand. Samp. | 104.2 (4) | 7.56 (5) | 59.0 (3) | 0.77 | 320
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 134.9 (4) | 9.76 (6) | 40.4 (7) | 0.59 | 2120 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 110.8 (1) | 8.75 (1) | 46.6 (5) | 0.61 | 2450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 177.3 (7) | 10.81 (7) | 41.0 (6) | 0.47 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 119.1 (3) | 8.78 (2) | 55.6 (2) | 0.47 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 117.8 (2) | 8.94 (3) | 53.0 (3) | 0.47 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 141.7 (6) | 9.11 (4) | 59.2 (1) | 0.47 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 3.5 Turbo | Rand. Samp. | 135.3 (5) | 9.27 (5) | 51.8 (4) | 0.47 | 320
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 130.8 (6) | 9.89 (7) | 43.8 (7) | 2.74 | 2120 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 117.6 (6) | 9.14 (6) | 53.4 (6) | 2.96 | 2450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 88.0 (4) | 7.15 (4) | 61.6 (5) | 1.15 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 83.3 (3) | 7.14 (3) | 63.8 (4) | 1.08 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 73.3 (1) | 6.80 (1) | 67.4 (2) | 1.08 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 94.2 (5) | 7.23 (5) | 69.6 (1) | 1.08 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini Pro | Rand. Samp. | 78.8 (2) | 7.05 (2) | 66.2 (3) | 1.08 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 811.6 (6) | 19.54 (6) | 13.1 (6) | 1.01 | 2120 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 1532.1 (7) | 30.68 (7) | 8.9 (7) | 1.04 | 2450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 236.4 (4) | 10.81 (5) | 29.0 (4) | 0.91 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 93.5 (1) | 7.96 (1) | 38.0 (1) | 0.91 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 172.5 (4) | 10.86 (4) | 23.1 (5) | 0.91 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 155.1 (2) | 9.56 (2) | 35.8 (2) | 0.91 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7b | Rand. Samp. | 164.1 (3) | 9.57 (3) | 34.6 (3) | 0.91 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | 174.7 (5) | 10.62 (5) | 39.1 (3) | 0.95 | 2120 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full+Sum. | 192.4 (6) | 11.79 (7) | 31.3 (6) | 0.99 | 2450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Sent. | 212.1 (7) | 11.76 (6) | 30.1 (7) | 0.90 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Last Sent. | 125.4 (1) | 9.33 (1) | 36.1 (5) | 0.90 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sum. | 159.7 (2) | 10.06 (2) | 40.5 (1) | 0.90 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Div. Sum. | 171.6 (4) | 10.15 (4) | 40.0 (2) | 0.90 | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 8b | Rand. Samp. | 165.5 (3) | 10.10 (3) | 38.2 (4) | 0.90 | 320
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The performance of different LLMs for Sentiment Analysis task on GameSpot
    dataset. $N$ parameter for summarization and truncation is set to 7 for all the
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4bab81501a23069b39da99365313f559.png)![Refer to caption](img/a3e6d10ee5524636efb1b2cb31c57567.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Mistral 7b Instruct F1-Temperature Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d05cd4cea8e5cb8c72425836b2a94514.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mistral 7b Instruct Accuracy-Temperature Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/27524b9ba25ada7a0b11aba4136530bf.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Llama3 8b Instruct F1-Temperature Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb369df10a818584f5b0eb4c008eeeb9.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Llama3 8b Instruct Accuracy-Temperature Curve
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Ablation Study on temperature in the news categorization task on
    20 newsgroup dataset. The results show the performance of the models does not
    experience much difference as we change the temperature from 0 to 0.1 aside from
    a slight increase in the confidence interval. These results are over 5 runs. The
    width of the 85% Confidence Interval for the ‘Random Sampling’ scenario is much
    bigger due to the randomness introduced by selecting the sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73e096ec16266303377e892485f78364.png)![Refer to caption](img/be8b9f9ef6c347249796e8f7012693f6.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Claude3 Haiku Accuracy Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/975ce12b8089f953650fc63ed290adaa.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Claude3 Haiku MAE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37533dbaad122255e893586e15ad8e48.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Claude3 Haiku MSE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5a2854b90da8bb003eb4da1c72e7853.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Gemini Pro Accuracy Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81d3a4c6a41caead96552e7631c53138.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Gemini Pro MAE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f97986a29c64171fc461e5d1071544cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Gemini Pro MSE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73d8d3c42a542c095f273227a0e0dd19.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) GPT3.5 Turbo Accuracy Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0896fb83eced9ef259870b44e4b330d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) GPT3.5 Turbo MAE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa39e8525886a7a51bc8426a5092d0ce.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) GPT3.5 Turbo MSE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a8c04677026581e514bc8ac2e1f06d36.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) Mistral 7b Instruct Accuracy Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35364c6b794c46854768797d069fafa8.png)'
  prefs: []
  type: TYPE_IMG
- en: (k) Mistral 7b Instruct MAE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0353617dfbbd437036538e8e6b0c3974.png)'
  prefs: []
  type: TYPE_IMG
- en: (l) Mistral 7b Instruct MSE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f40e7b9f6e5d8b9059eac5b56cb87e35.png)'
  prefs: []
  type: TYPE_IMG
- en: (m) Llama3 8b Instruct Accuracy Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/429328ad9148b4c4fe4b41332d64ea04.png)'
  prefs: []
  type: TYPE_IMG
- en: (n) Llama3 8b Instruct MAE Curve
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0037b1ee225ac76991849c4b4ab2d76e.png)'
  prefs: []
  type: TYPE_IMG
- en: (o) Llama3 8b Instruct MSE Curve
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: LLMs performance over 5 runs on sentiment analysis on GameSpot dataset.
    The “Full Text” scenario is a horizontal line since it always contains the full
    text, not summary/truncated text and we have included it as a horizontal line
    here as baseline'
  prefs: []
  type: TYPE_NORMAL
