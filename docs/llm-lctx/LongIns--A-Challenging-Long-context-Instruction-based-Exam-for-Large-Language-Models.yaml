- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:02:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:02:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LongIns: A Challenging Long-context Instruction-based Exam for Large Language
    Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LongIns：一个挑战性的长上下文基于指令的考试
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17588](https://ar5iv.labs.arxiv.org/html/2406.17588)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17588](https://ar5iv.labs.arxiv.org/html/2406.17588)
- en: ¹Shawn Gavin¹¹1These authors contributed equally., ¹ ² ³Tuney Zheng¹¹1These
    authors contributed equally., ¹Jiaheng Liu, ¹Quehry Que,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Shawn Gavin¹¹1这些作者贡献相同。, ¹ ² ³Tuney Zheng¹¹1这些作者贡献相同。, ¹Jiaheng Liu, ¹Quehry
    Que,
- en: ¹ ³Noah Wang, ¹Jian Yang, ¹Chenchen Zhang,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ ³Noah Wang, ¹Jian Yang, ¹Chenchen Zhang,
- en: ³Wenhao Huang, ¹ ²Wenhu Chen²²2Corresponding Authors., ¹ ² ³Ge Zhang²²2Corresponding
    Authors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³Wenhao Huang, ¹ ²Wenhu Chen²²2通讯作者。, ¹ ² ³Ge Zhang²²2通讯作者。
- en: ¹M-A-P, ²University of Waterloo, ³01.ai
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹M-A-P, ²滑铁卢大学, ³01.ai
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The long-context capabilities of large language models (LLMs) have been a hot
    topic in recent years. To evaluate LLMs’ performance in different scenarios, various
    assessment benchmarks have emerged. However, as most of these benchmarks focus
    on identifying key information to answer questions, which mainly requires the
    retrieval ability of LLMs, these benchmarks can partially represent the reasoning
    performance of LLMs from large amounts of information. Meanwhile, although LLMs
    often claim to have context windows of 32k, 128k, 200k, or even longer, these
    benchmarks fail to reveal the actual supported length of these LLMs. To address
    these issues, we propose the LongIns benchmark dataset, a challenging long-context
    instruction-based exam for LLMs, which is built based on the existing instruction
    datasets. Specifically, in our LongIns, we introduce three evaluation settings:
    Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST),
    and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive
    evaluations on existing LLMs and have the following important findings: (1). The
    top-performing GPT-4 with 128k context length performs poorly on the evaluation
    context window of 16k in our LongIns. (2). For the multi-hop reasoning ability
    of many existing LLMs, significant efforts are still needed under short context
    windows (<4k).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的长上下文能力近年来成为热门话题。为了评估LLMs在不同场景中的表现，出现了各种评估基准。然而，由于这些基准大多集中于识别关键信息以回答问题，这主要要求LLMs的检索能力，这些基准只能部分代表LLMs在大量信息中的推理表现。同时，尽管LLMs常常声称具有32k、128k、200k甚至更长的上下文窗口，但这些基准未能揭示这些LLMs的实际支持长度。为了解决这些问题，我们提出了LongIns基准数据集，这是一个具有挑战性的基于指令的长上下文考试，建立在现有的指令数据集基础上。具体来说，在我们的LongIns中，我们引入了三种评估设置：全局指令与单任务（GIST）、局部指令与单任务（LIST）和局部指令与多任务（LIMT）。基于LongIns，我们对现有的LLMs进行了全面评估，并得出了以下重要发现：（1）表现最佳的GPT-4在128k上下文长度下，在我们的LongIns中的16k评估上下文窗口上表现较差。（2）对于许多现有LLMs的多跳推理能力，在短上下文窗口（<4k）下仍需进行大量努力。
- en: 'LongIns: A Challenging Long-context Instruction-based Exam'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LongIns：一个挑战性的长上下文基于指令的考试
- en: for Large Language Models
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: ¹Shawn Gavin¹¹1These authors contributed equally., ¹ ² ³Tuney Zheng¹¹1These
    authors contributed equally., ¹Jiaheng Liu, ¹Quehry Que, ¹ ³Noah Wang, ¹Jian Yang,
    ¹Chenchen Zhang, ³Wenhao Huang, ¹ ²Wenhu Chen²²2Corresponding Authors., ¹ ² ³Ge
    Zhang²²2Corresponding Authors. ¹M-A-P, ²University of Waterloo, ³01.ai
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Shawn Gavin¹¹1这些作者贡献相同。, ¹ ² ³Tuney Zheng¹¹1这些作者贡献相同。, ¹Jiaheng Liu, ¹Quehry
    Que, ¹ ³Noah Wang, ¹Jian Yang, ¹Chenchen Zhang, ³Wenhao Huang, ¹ ²Wenhu Chen²²2通讯作者。,
    ¹ ² ³Ge Zhang²²2通讯作者。 ¹M-A-P, ²滑铁卢大学, ³01.ai
- en: '![Refer to caption](img/3f7c47b9ad0afa700fa06ff4aed92c36.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f7c47b9ad0afa700fa06ff4aed92c36.png)'
- en: 'Figure 1: (a) Long Document Q&A is one of the common types of questions in
    mainstream long-context benchmarks. (b) Needle in a Haystack is an evaluation
    paradigm that tests the retrieval capabilities of LLMs in long contexts by inserting
    key information into long texts and asking questions based on that. (c) Our LongIns
    consists of (1) GIST mode, Each sample is composed of the same task type concatenated
    together, and the task instruction is given only once at the beginning globally.
    (2) LIST mode, which is similar to GIST in the composition of samples, but instructions
    are provided for each question within the sample; (3) consists of questions from
    different task types concatenated together.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（a）长文档问答是主流长上下文基准中常见的题型之一。（b）大海捞针是一种评估范式，通过将关键信息插入长文本中，并基于这些信息提问，来测试LLMs在长上下文中的检索能力。（c）我们的LongIns包括（1）GIST模式，每个样本由相同的任务类型串联而成，任务指令仅在开始时全球给出一次。（2）LIST模式，与GIST模式在样本组成上类似，但每个问题都提供了指令；（3）由不同任务类型的问题串联在一起。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The topic of extending the context window length in large language models (LLMs)
    remains a focal point in current research. Existing LLMs can handle context lengths
    ranging from 32k to 200k tokens (Wang et al., [2024](#bib.bib29)), while some
    LLMs achieving capacities up to 10M tokens (Liu et al., [2024a](#bib.bib19); Bai
    et al., [2023a](#bib.bib7)). The capacity of length expansions is crucial for
    enhancing long document comprehension and forms the basis for a series of advanced
    applications, including repository-level code. Comprehension/Editing and the dependence
    of agent-based tasks on long-term memory. Despite these advancements, the predominant
    evaluation metrics continue to prioritize the retrieval performance of LLMs, overlooking
    the actual window length that the model can understand when faced with long texts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展大型语言模型（LLMs）中的上下文窗口长度的话题仍然是当前研究的重点。现有的LLMs可以处理从32k到200k个标记的上下文长度（Wang et al.,
    [2024](#bib.bib29)），而一些LLMs的处理能力可达到10M个标记（Liu et al., [2024a](#bib.bib19)；Bai
    et al., [2023a](#bib.bib7)）。长度扩展的能力对于增强长文档的理解至关重要，并且为一系列高级应用奠定了基础，包括存储库级代码的理解/编辑以及基于代理的任务对长期记忆的依赖。尽管有这些进展，主流的评估指标仍然优先考虑LLMs的检索性能，忽视了在面对长文本时模型能够理解的实际窗口长度。
- en: The capabilities of LLMs in scenarios involving long-context , such as cross-document
    aggregation, localization, and context tracking, is paramount for ensuring a seamless
    interaction between users and LLMs. Users often expect LLMs to read and understand
    the entire documents. However, existing benchmarks measure the proficiency of
    LLMs based on their capacity to retrieve and understand only key information from
    the inputs, thereby bypassing the necessity for an exhaustive understanding of
    the entire text. This does not align well with the real-world expectations from
    users who seek in-depth processing of long texts. Datasets commonly used to evaluate
    the long-text capabilities of LLMs, such as L-eval  (An et al., [2023](#bib.bib6))
    or longBench  (Bai et al., [2023b](#bib.bib8)), are insufficient for assessing
    the understanding of long sequences. Therefore, how to truly evaluate the capabilities
    of LLMs in handling long-context tasks of the realistic scenarios still requires
    further exploration.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在涉及长上下文的场景中的能力，如跨文档聚合、定位和上下文跟踪，对于确保用户与LLMs之间的无缝互动至关重要。用户通常期望LLMs能够阅读和理解整个文档。然而，现有的基准测试根据LLMs从输入中检索和理解关键信息的能力来衡量其熟练程度，从而绕过了对整个文本的全面理解的必要性。这与用户期望LLMs深入处理长文本的现实需求不符。常用于评估LLMs长文本能力的数据集，如L-eval（An
    et al., [2023](#bib.bib6)）或longBench（Bai et al., [2023b](#bib.bib8)），不足以评估对长序列的理解。因此，如何真正评估LLMs在处理现实场景中的长上下文任务中的能力仍需进一步探索。
- en: To bridge this gap, we introduce LongIns, a benchmark tailored to critically
    assess the proficiency of LLMs in understanding extensive sequences. LongIns incorporates
    a contextual learning approach where the input involves more substantial key information
    segments, and is poised to ensure that correctly answering the culminating question
    necessitates a deep comprehension of the entire lengthy input sequence. This approach
    mandates that LLMs must truly excel in long-sequence understanding capabilities,
    beyond just the retrieval of key information from extended texts. We evaluate
    the performance of 20 different LLMs under LongIns, including GPT-4o. We observe
    that LLMs generally perform worse on tasks requiring understanding of complete
    long sequences compared to retrieval tasks of the same length. As the total length
    increases, the performance gap becomes more pronounced. Additionally, the models’
    performance is largely independent of their advertised context window length,
    leading us to believe that the advertised context window length for most models
    should be understood as the "maximum acceptable sequence length" rather than the
    "maximum comprehensible sequence length."
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一空白，我们引入了 LongIns，一个旨在批判性评估 LLM 在理解广泛序列中的能力的基准。LongIns 采用了一种上下文学习方法，其中输入包含更多的关键内容片段，并且需要深刻理解整个长输入序列才能正确回答最终问题。这种方法要求
    LLM 在长序列理解能力上必须真正出色，而不仅仅是从扩展文本中检索关键信息。我们在 LongIns 下评估了 20 种不同的 LLM，包括 GPT-4o。我们观察到，与相同长度的检索任务相比，LLM
    在需要理解完整长序列的任务上的表现通常较差。随着总长度的增加，性能差距变得更加明显。此外，模型的性能在很大程度上与其宣传的上下文窗口长度无关，这使我们相信，大多数模型的宣传上下文窗口长度应理解为“最大可接受序列长度”而非“最大可理解序列长度”。
- en: 'Moreover, by controlling the distribution of incorrect answers and response
    situations, we analyze the distribution of attention changes at different text
    positions. Further analysis on the density of key information within the same
    total length shows that, except for GPT-4 and GPT-4o, the accuracy of most models
    rapidly declines as the density of key information increases. In summary, the
    primary contributions can be summarized as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过控制错误答案和响应情况的分布，我们分析了在不同文本位置的注意力变化分布。进一步对相同总长度内关键信息密度的分析表明，除了 GPT-4 和 GPT-4o
    外，大多数模型在关键信息密度增加时准确率迅速下降。总的来说，主要贡献可以总结为：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide the LongIns benchmark test, specifically designed to evaluate the
    actual understanding and processing capabilities of LLMs on long sequences. Unlike
    other benchmarks based on retrieval tasks, we focus more on assessing the actual
    comprehensible window length of the models.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了 LongIns 基准测试，专门设计用于评估 LLM 在长序列上的实际理解和处理能力。与其他基于检索任务的基准测试不同，我们更注重评估模型的实际可理解窗口长度。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In our LongIns, we introduce three evaluation settings: Global Instruction
    & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction
    & Multiple Tasks (LIMT) to comprehensively evaluate the long-context abilities
    of existing LLMs.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的 LongIns 中，我们引入了三种评估设置：全球指令与单任务（GIST）、局部指令与单任务（LIST）以及局部指令与多任务（LIMT），以全面评估现有
    LLM 的长上下文能力。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We evaluate a series of long-context LLMs using this benchmark and find that
    most models fail to achieve high scores when the critical information length is
    only 8k. Even GPT-4 and GPT-4o score poorly at 16k length. This result is significantly
    different from the commonly recognized long context lengths (128k or longer),
    indicating that current LLMs still have considerable shortcomings in performing
    such tasks. We hope these results can provide a reference for research in the
    field of long-context LLMs.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用这一基准评估了一系列长上下文 LLM，发现当关键内容长度仅为 8k 时，大多数模型未能获得高分。即使是 GPT-4 和 GPT-4o 在 16k
    长度下的得分也很低。这一结果与常见的长上下文长度（128k 或更长）有显著不同，表明目前的 LLM 在执行此类任务时仍存在相当大的不足。我们希望这些结果能为长上下文
    LLM 领域的研究提供参考。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Long-context LLM
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长上下文 LLM
- en: The computational cost of processing sequences in Transformer-based models increases
    quadratically with sequence length, resulting in higher resource consumption and
    performance issues when handling long context inputs. Many studies explore various
    strategies to address this challenge, including the use of new positional encodings
    to achieve position interpolation or extrapolation Peng et al. ([2023](#bib.bib24));
    Chen et al. ([2023](#bib.bib10)); Liu et al. ([2024b](#bib.bib21)). For example,
    Rope (Su et al., [2021](#bib.bib27)) extends the positional knowledge learned
    during the pre-training phase to longer sequence lengths through extrapolation,
    including various variants such as NTK-RoPE (NormXU, [2021](#bib.bib23)). Alibi (Press
    et al., [2021](#bib.bib25)), on the other hand, maps long sequences to within
    recognizable lengths through interpolation. Some studies attempt to fine-tune
    LLMs to give the model a longer context window. LongLoRA  (Chen et al., [2024](#bib.bib11))is
    an efficient fine-tuning approach that significantly extends the context sizes
    of pre-trained LLMs with limited computational cost by using sparse local attention
    and improved parameter-efficient fine-tuning techniques. RingAttention (Liu et al.,
    [2023a](#bib.bib20)) can reduce the memory requirements of the Transformer model,
    allowing it to train sequences over 500 times longer than previous memory-efficient
    methods, without needing to approximate the attention mechanism. Additionally,
    traditional engineering techniques such as sliding windows or RAG (Lewis et al.,
    [2020](#bib.bib16)) are also solutions for addressing long context scenarios in
    LLMs. These methods improve the performance of LLMs in handling long contexts
    in certain aspects.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 基于模型处理序列的计算成本随着序列长度的增加而呈二次方增长，这导致在处理长上下文输入时资源消耗增加并出现性能问题。许多研究探讨了应对这一挑战的各种策略，包括使用新的位置编码实现位置插值或外推，如
    Peng 等人 ([2023](#bib.bib24))、Chen 等人 ([2023](#bib.bib10)) 和 Liu 等人 ([2024b](#bib.bib21))。例如，Rope
    (Su 等人, [2021](#bib.bib27)) 通过外推将预训练阶段学到的位置知识扩展到更长的序列长度，包括各种变体，如 NTK-RoPE (NormXU,
    [2021](#bib.bib23))。另一方面，Alibi (Press 等人, [2021](#bib.bib25)) 通过插值将长序列映射到可识别的长度。一些研究尝试微调
    LLMs，以赋予模型更长的上下文窗口。LongLoRA (Chen 等人, [2024](#bib.bib11)) 是一种高效的微调方法，通过使用稀疏的局部注意力和改进的参数高效微调技术，显著扩展了预训练
    LLMs 的上下文大小，同时计算成本有限。RingAttention (Liu 等人, [2023a](#bib.bib20)) 可以减少 Transformer
    模型的内存需求，使其能够训练比以前内存高效方法长 500 倍的序列，而无需近似注意力机制。此外，传统的工程技术如滑动窗口或 RAG (Lewis 等人, [2020](#bib.bib16))
    也是解决 LLMs 长上下文场景的方案。这些方法在处理长上下文方面在某些方面提高了 LLMs 的性能。
- en: Long-context Benchmark
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长上下文基准测试
- en: To evaluate the performance of different LLMs on long texts, various benchmarks
    are often used to test different aspects of LLMs’ capabilities. For instance,
    Zeroscrolls (Shaham et al., [2023](#bib.bib26)) is a zero-shot benchmark for natural
    language understanding over long texts, which contains only test and small validation
    sets, without training data. LongBench (Bai et al., [2023b](#bib.bib8)) is a bilingual,
    multi-task benchmark for long context understanding, which also provides a subset
    with uniformly distributed data called LongBench-E. L-Eval (An et al., [2023](#bib.bib6))
    constructs a long context evaluation benchmark containing multiple tasks and domains,
    including multiple-choice questions, with data that has undergone strict manual
    screening to ensure high quality. LooGLE (Li et al., [2023](#bib.bib18)), as a
    benchmark with longer samples, effective for evaluating the ability of LLMs to
    understand short-term and long-term dependencies in the content. M4LE (Kwan et al.,
    [2023](#bib.bib15)) provides additional tasks and datasets in both Chinese and
    English, covering a wider range of domains. $\infty$BENCH is the first benchmark
    with more than 100K tokens, with the longest test items reaching up to 2000K tokens.
    Needle in a Haystack (Doe and Smith, [2022](#bib.bib13)) employs Paul Graham’s
    218 essays as the “haystack” inserting a single sentence at various depths within
    the documents to statistically analyze the model’s accuracy in identifying the
    locations of various “needles” and thereby evaluate its performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估不同 LLM 在长文本上的性能，通常使用各种基准来测试 LLM 的不同能力。例如，Zeroscrolls (Shaham et al., [2023](#bib.bib26))
    是一个针对长文本自然语言理解的零样本基准，它只包含测试和小规模验证集，没有训练数据。LongBench (Bai et al., [2023b](#bib.bib8))
    是一个用于长上下文理解的双语、多任务基准，还提供了一个数据均匀分布的子集，称为 LongBench-E。L-Eval (An et al., [2023](#bib.bib6))
    构建了一个包含多个任务和领域的长上下文评估基准，包括多项选择题，其数据经过严格人工筛选，以确保高质量。LooGLE (Li et al., [2023](#bib.bib18))
    作为一个样本较长的基准，有效评估 LLM 理解短期和长期依赖性的能力。M4LE (Kwan et al., [2023](#bib.bib15)) 提供了中文和英文的额外任务和数据集，涵盖了更广泛的领域。$\infty$BENCH
    是第一个超过 100K tokens 的基准，最长的测试项达到了 2000K tokens。Needle in a Haystack (Doe and Smith,
    [2022](#bib.bib13)) 使用 Paul Graham 的 218 篇文章作为“干草堆”，在文档的不同深度插入单句，以统计分析模型识别各种“针”的位置的准确性，从而评估其性能。
- en: '| Model | Size | Strategy | Support |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 策略 | 支持 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-4o | \faLock | \faLock | 128k |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | \faLock | \faLock | 128k |'
- en: '| GPT-4-Turbo | \faLock | \faLock | 128k |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | \faLock | \faLock | 128k |'
- en: '| GPT-3.5-Turbo | \faLock | \faLock | 16K |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | \faLock | \faLock | 16K |'
- en: '| ERNIE-Speed | \faLock | \faLock | 8k |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE-Speed | \faLock | \faLock | 8k |'
- en: '| Qwen-Long | \faLock | NTK+LNS+WAttn | 10M |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-Long | \faLock | NTK+LNS+WAttn | 10M |'
- en: '| Deepseek-Chat | \faLock | YaRN | 32k |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-Chat | \faLock | YaRN | 32k |'
- en: '| Moonshot-v1 | \faLock | \faLock | 128k |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Moonshot-v1 | \faLock | \faLock | 128k |'
- en: '| Yi-Large-Turbo | \faLock | SeqPara+DistrAttn | 200k |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Large-Turbo | \faLock | SeqPara+DistrAttn | 200k |'
- en: '| Yi-Spark | \faLock | SeqPara+DistrAttn | 200k |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Spark | \faLock | SeqPara+DistrAttn | 200k |'
- en: '| GLM-4 | \faLock | \faLock | 128k |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4 | \faLock | \faLock | 128k |'
- en: '| \hdashline |  |  |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |'
- en: '| ChatGLM2-6B | 6B | PI | 32k |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B | 6B | PI | 32k |'
- en: '| Deepseek-LLM-Chat | 7B | - | 4k |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-LLM-Chat | 7B | - | 4k |'
- en: '| Lwm-Text-Chat | 7B | RingAttn | 128k |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Lwm-Text-Chat | 7B | RingAttn | 128k |'
- en: '| LongChat-7B | 7B | Rope | 16k |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7B | 7B | Rope | 16k |'
- en: '| MAP-Neo-Ins-v0.1 | 7B | - | 8k |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| MAP-Neo-Ins-v0.1 | 7B | - | 8k |'
- en: '| Mistral-7B-Ins-v0.2 | 7B | Rope | 32k |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Ins-v0.2 | 7B | Rope | 32k |'
- en: '| Longpalca-7B | 7B | LongLoRA | 32k |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Longpalca-7B | 7B | LongLoRA | 32k |'
- en: '| Qwen1.5-7B-Chat | 7B | Rope+Sliding Window | 32k |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-7B-Chat | 7B | Rope+滑动窗口 | 32k |'
- en: '| Internlm2-Chat-7B | 7B | Dynamic NTK | 200k |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Internlm2-Chat-7B | 7B | 动态 NTK | 200k |'
- en: '| Llama3-8B-Ins | 8B | RoPE | 8K |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Ins | 8B | RoPE | 8K |'
- en: '| \hdashline |  |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |'
- en: '| LongChat-13B | 13B | Rope | 16k |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-13B | 13B | Rope | 16k |'
- en: '| Baichuan2-13B-Chat | 13B | - | 4k |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B-Chat | 13B | - | 4k |'
- en: 'Table 1: Details on the evaluated models. “Ins” indicates “Instruct”. “LNS”
    indicates “LogNScaling”. “WAttn” indicates “WindowAttention”'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：评估模型的详细信息。“Ins”表示“指令”。“LNS”表示“LogNScaling”。“WAttn”表示“WindowAttention”
- en: '|  | GIST | LIST | LIMT |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | GIST | LIST | LIMT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Number of Questions | Q-Density | Number of Questions | Q-Density | Number
    of Questions | Q-Density |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 问题数量 | Q-密度 | 问题数量 | Q-密度 | 问题数量 | Q-密度 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| QA | 265 $\times$ 7 | 1.39 | - | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| QA | 265 $\times$ 7 | 1.39 | - | - |'
- en: '| Classif | 229 $\times$ 7 | 1.65 | - | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 229 $\times$ 7 | 1.65 | - | - |'
- en: '| RC | 142 $\times$ 7 | 1.71 | - | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| RC | 142 $\times$ 7 | 1.71 | - | - |'
- en: '| NLI | 140 $\times$ 7 | 1.70 | - | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| NLI | 140 $\times$ 7 | 1.70 | - | - |'
- en: '| MT | 432 $\times$ 7 | 1.09 | - | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| MT | 432 $\times$ 7 | 1.09 | - | - |'
- en: '| NER | 96 $\times$ 7 | 2.27 | - | - |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| NER | 96 $\times$ 7 | 2.27 | - | - |'
- en: '| CSR | 105 $\times$ 7 | 2.64 | - | - |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| CSR | 105 $\times$ 7 | 2.64 | - | - |'
- en: '| Total | 1409 $\times$ 7 | 1.40 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Total | 1409 $\times$ 7 | 1.40 |'
- en: 'Table 2: The distribution of task types in the LongBBH dataset. “QA”, “Classif”,
    “RC”, “NLI”, “MT”, “NER”, “CSR” represent Question Answering, Classification,
    Reading Comprehension, Natural Language Inference, Machine Translation, Named
    Entity Recognition, and Common Sense Reasoning, respectively. “Q-Density” represents
    the number of questions per 100 tokens, reflecting question density in each paper.
    Samples in LIMT consist of a mix of various task types, so the information is
    not presented by category.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LongBBH 数据集中任务类型的分布。 “QA”、“Classif”、“RC”、“NLI”、“MT”、“NER”、“CSR”分别代表问答、分类、阅读理解、自然语言推断、机器翻译、命名实体识别和常识推理。
    “Q-Density”代表每 100 个标记的问题数量，反映每篇论文中的问题密度。 LIMT 中的样本由各种任务类型的混合组成，因此信息未按类别呈现。
- en: 3 LongIns Benchmark
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LongIns 基准
- en: 'To support the evaluation of various types and lengths of tasks, we collect
    a diverse range of questions from the Super-NaturalInstructions (SNI) and BIG-bench
    datasets, covering different areas of natural language processing. This ensures
    comprehensive coverage of task types and difficulty levels. To address the issue
    of insufficient samples in some categories, we generate additional questions.
    As detailed in [Table 2](#S2.T2 "Table 2 ‣ Long-context Benchmark ‣ 2 Related
    Work ‣ LongIns: A Challenging Long-context Instruction-based Exam for Large Language
    Models"), our dataset, named LongIns, includes seven different context lengths:
    256, 512, 1024, 2048, 4096, 8192, and 16384 tokens, with 1409 questions for each
    length. These questions divide into seven task types, including question answering,
    classification, reading comprehension, natural language inference, translation,
    named entity recognition, and commonsense reasoning.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '为支持对各种任务类型和长度的评估，我们从 Super-NaturalInstructions (SNI) 和 BIG-bench 数据集中收集了多样化的问题，涵盖自然语言处理的不同领域。这确保了任务类型和难度水平的全面覆盖。为了解决某些类别样本不足的问题，我们生成了额外的问题。如[表2](#S2.T2
    "Table 2 ‣ Long-context Benchmark ‣ 2 Related Work ‣ LongIns: A Challenging Long-context
    Instruction-based Exam for Large Language Models")中详细说明，我们的数据集名为 LongIns，包括七种不同的上下文长度：256、512、1024、2048、4096、8192
    和 16384 个标记，每种长度有 1409 个问题。这些问题分为七种任务类型，包括问答、分类、阅读理解、自然语言推断、翻译、命名实体识别和常识推理。'
- en: 'Specifically, we construct our dataset by concatenating questions to the specified
    context lengths and modifying some of the questions’ answers to incorrect ones
    (we control the overall error rate at around 10% but ensure at least one incorrect
    answer for shorter lengths). The aim is to reflect the long-context performance
    of LLMs based on their efficiency in identifying incorrect answers after reading
    multiple questions simultaneously. Based on the above concept, we construct a
    dataset using the Global Instruction & Single Task (GIST) mode. Additionally,
    to explore the impact of different factors on the evaluation results, we create
    two other datasets: Local Instruction & Single Task (LIST) and Local Instruction
    & Multiple Tasks (LIMT).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们通过将问题拼接到指定的上下文长度并修改一些问题的答案为错误答案来构建数据集（我们控制整体错误率在大约 10% 左右，但确保较短长度中至少有一个错误答案）。其目的是反映
    LLM 在同时阅读多个问题后识别错误答案的效率。基于上述概念，我们使用全球指令与单一任务（GIST）模式构建数据集。此外，为了探索不同因素对评估结果的影响，我们创建了另外两个数据集：局部指令与单一任务（LIST）和局部指令与多任务（LIMT）。
- en: Global Instruction & Single Task
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全球指令与单一任务
- en: 'The GIST dataset serves as our core benchmark. GIST evaluates by concatenating
    the same type of questions to the specified lengths, constructing prompts for
    each sample according to the template shown in [Figure 6](#A3.F6 "Figure 6 ‣ C.1
    Template of GIST ‣ Appendix C Prompt Template ‣ LongIns: A Challenging Long-context
    Instruction-based Exam for Large Language Models"). To assess the model’s attention
    capabilities across different lengths and positions, we ensure an even distribution
    of incorrect answers to maintain statistical significance. During testing, for
    open-source models, we use the official example system prompt; if no official
    example is provided, we default to standard configurations. For closed-source
    models, we utilize examples from the official documentation.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'GIST 数据集作为我们的核心基准。GIST 通过将相同类型的问题拼接到指定长度，根据[图 6](#A3.F6 "Figure 6 ‣ C.1 Template
    of GIST ‣ Appendix C Prompt Template ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models")所示的模板为每个样本构造提示来进行评估。为了评估模型在不同长度和位置上的注意力能力，我们确保错误答案的均匀分布以保持统计显著性。在测试过程中，对于开源模型，我们使用官方示例系统提示；如果没有提供官方示例，我们默认为标准配置。对于闭源模型，我们利用官方文档中的示例。'
- en: Local Instruction & Multiple Tasks
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地指令与多任务
- en: 'Each piece of sample in GIST is composed of questions of the same task type,
    sharing a common task description. Therefore, the task description (i.e., Instruction)
    is placed at the beginning globally. However, this positioning might result in
    the subsequent questions being far from the Instruction, potentially affecting
    accuracy. To explore this issue, we construct a dataset in LIST mode for ablation
    experiments, as illustrated in [Figure 7](#A3.F7 "Figure 7 ‣ C.2 Template of LIST
    ‣ Appendix C Prompt Template ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'GIST 中的每个样本由相同任务类型的问题组成，共享一个通用的任务描述。因此，任务描述（即 Instruction）被全局放在开头。然而，这种定位可能导致后续问题远离
    Instruction，可能会影响准确性。为了解决这个问题，我们构建了一个 LIST 模式的数据集用于消融实验，如[图 7](#A3.F7 "Figure
    7 ‣ C.2 Template of LIST ‣ Appendix C Prompt Template ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models")所示。'
- en: Local Instruction & Multiple Tasks
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地指令与多任务
- en: 'Both GIST and LIST benchmarks are composed of questions of the same task type.
    When the context content is similar, the model’s in-context learning ability becomes
    more significant. To explore the role of the model’s in-context learning ability
    in the evaluation results, we design a subset called LIMT. LIMT includes 7 lengths,
    with each length containing 200 test items formed by mixing various task types.
    The construction template prompt for LIMT is similar to [Figure 7](#A3.F7 "Figure
    7 ‣ C.2 Template of LIST ‣ Appendix C Prompt Template ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models"), with the only
    difference being that each question within the sample corresponds to a different
    instruction.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'GIST 和 LIST 基准均由相同任务类型的问题组成。当上下文内容相似时，模型的上下文学习能力变得更为重要。为了探索模型的上下文学习能力在评估结果中的作用，我们设计了一个名为
    LIMT 的子集。LIMT 包含 7 个长度，每个长度包含 200 个由各种任务类型混合形成的测试项。LIMT 的构建模板提示类似于[图 7](#A3.F7
    "Figure 7 ‣ C.2 Template of LIST ‣ Appendix C Prompt Template ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models")，唯一的区别是样本中的每个问题对应不同的指令。'
- en: 4 Experiment
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this chapter, we present the experimental methods as well as the main findings
    and results obtained through these experiments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了实验方法以及通过这些实验获得的主要发现和结果。
- en: '| Model | Size | Support | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Model | Size | Support | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4o | \faLock | 128k | 70.94 | 59.14 | 60.58 | 55.43 | 52.92 | 43.81 |
    31.53 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | \faLock | 128k | 70.94 | 59.14 | 60.58 | 55.43 | 52.92 | 43.81 |
    31.53 |'
- en: '| GPT-4-Turbo | \faLock | 128k | 69.59 | 63.13 | 64.21 | 59.08 | 57.52 | 50.73
    | 40.91 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | \faLock | 128k | 69.59 | 63.13 | 64.21 | 59.08 | 57.52 | 50.73
    | 40.91 |'
- en: '| GPT-3.5-Turbo | \faLock | 16K | 54.61 | 45.38 | 41.68 | 34.81 | 26.27 | 18.81
    | 12.23 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | \faLock | 16K | 54.61 | 45.38 | 41.68 | 34.81 | 26.27 | 18.81
    | 12.23 |'
- en: '| ERNIE-Speed | \faLock | 8k | 44.16 | 35.99 | 24.88 | 17.29 | 12.22 | 1.13
    | - |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE-Speed | \faLock | 8k | 44.16 | 35.99 | 24.88 | 17.29 | 12.22 | 1.13
    | - |'
- en: '| Qwen-Long | \faLock | 10M | 61.58 | 54.60 | 42.07 | 34.24 | 25.99 | 18.71
    | 10.33 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-Long | \faLock | 10M | 61.58 | 54.60 | 42.07 | 34.24 | 25.99 | 18.71
    | 10.33 |'
- en: '| Deepseek-Chat | \faLock | 32k | 45.71 | 40.86 | 32.41 | 19.32 | 11.33 | 5.42
    | 3.10 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-Chat | \faLock | 32k | 45.71 | 40.86 | 32.41 | 19.32 | 11.33 | 5.42
    | 3.10 |'
- en: '| Moonshot-v1 | \faLock | 128k | 69.45 | 61.22 | 52.35 | 52.94 | 34.89 | 24.29
    | 15.10 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Moonshot-v1 | \faLock | 128k | 69.45 | 61.22 | 52.35 | 52.94 | 34.89 | 24.29
    | 15.10 |'
- en: '| Yi-Large-Turbo | \faLock | 200k | 57.05 | 42.22 | 33.81 | 28.60 | 19.82 |
    11.69 | 6.01 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Large-Turbo | \faLock | 200k | 57.05 | 42.22 | 33.81 | 28.60 | 19.82 |
    11.69 | 6.01 |'
- en: '| Yi-Spark | \faLock | 200k | 51.61 | 39.04 | 35.53 | 24.44 | 15.97 | 5.08
    | 0.88 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Spark | \faLock | 200k | 51.61 | 39.04 | 35.53 | 24.44 | 15.97 | 5.08
    | 0.88 |'
- en: '| GLM-4 | \faLock | 128k | 69.20 | 64.66 | 56.62 | 56.11 | 33.50 | 23.72 |
    12.01 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4 | \faLock | 128k | 69.20 | 64.66 | 56.62 | 56.11 | 33.50 | 23.72 |
    12.01 |'
- en: '| \hdashline |  |  |  |  |  |  |  |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |  |  |  |  |  |  |'
- en: '| ChatGLM2-6B | 6B | 32k | 20.75 | 17.00 | 14.08 | 10.81 | 6.38 | 2.93 | 0.87
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B | 6B | 32k | 20.75 | 17.00 | 14.08 | 10.81 | 6.38 | 2.93 | 0.87
    |'
- en: '| Deepseek-LLM-Chat | 7B | 4k | 29.08 | 24.01 | 16.75 | 12.99 | 3.61 | - |
    - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-LLM-Chat | 7B | 4k | 29.08 | 24.01 | 16.75 | 12.99 | 3.61 | - |
    - |'
- en: '| Lwm-Text-Chat | 7B | 128k | 22.19 | 15.20 | 10.77 | 8.80 | 3.18 | 0.73 |
    - |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Lwm-Text-Chat | 7B | 128k | 22.19 | 15.20 | 10.77 | 8.80 | 3.18 | 0.73 |
    - |'
- en: '| LongChat-7B | 7B | 16k | 24.58 | 20.85 | 17.48 | 14.35 | 9.62 | 6.31 | 2.54
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7B | 7B | 16k | 24.58 | 20.85 | 17.48 | 14.35 | 9.62 | 6.31 | 2.54
    |'
- en: '| MAP-Neo-Ins-v0.1 | 7B | 8k | 41.22 | 34.17 | 28.09 | 21.96 | 14.72 | 3.28
    | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| MAP-Neo-Ins-v0.1 | 7B | 8k | 41.22 | 34.17 | 28.09 | 21.96 | 14.72 | 3.28
    | - |'
- en: '| Mistral-7B-Ins-v0.2 | 7B | 32k | 40.53 | 37.00 | 29.15 | 20.37 | 0 | 0 |
    0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Ins-v0.2 | 7B | 32k | 40.53 | 37.00 | 29.15 | 20.37 | 0 | 0 |
    0 |'
- en: '| Longalpaca-7B | 7B | 32k | 31.75 | 25.37 | 20.08 | 14.94 | 9.68 | 4.74 |
    1.95 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Longalpaca-7B | 7B | 32k | 31.75 | 25.37 | 20.08 | 14.94 | 9.68 | 4.74 |
    1.95 |'
- en: '| Qwen1.5-7B-Chat | 7B | 32k | 33.48 | 29.88 | 24.32 | 19.50 | 16.72 | 11.57
    | 5.22 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-7B-Chat | 7B | 32k | 33.48 | 29.88 | 24.32 | 19.50 | 16.72 | 11.57
    | 5.22 |'
- en: '| Internlm2-Chat-7B | 7B | 200k | 52.14 | 45.23 | 36.84 | 26.88 | 18.19 | 11.98
    | 6.06 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Internlm2-Chat-7B | 7B | 200k | 52.14 | 45.23 | 36.84 | 26.88 | 18.19 | 11.98
    | 6.06 |'
- en: '| Llama3-8B-Ins | 8B | 8k | 52.93 | 46.35 | 40.63 | 32.06 | 22.48 | 10.09 |
    - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Ins | 8B | 8k | 52.93 | 46.35 | 40.63 | 32.06 | 22.48 | 10.09 |
    - |'
- en: '| \hdashline |  |  |  |  |  |  |  |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |  |  |  |  |  |  |'
- en: '| LongChat-13B | 13B | 16k | 27.96 | 25.52 | 22.19 | 18.06 | 14.30 | 8.21 |
    3.15 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-13B | 13B | 16k | 27.96 | 25.52 | 22.19 | 18.06 | 14.30 | 8.21 |
    3.15 |'
- en: '| Baichuan2-13B-Chat | 13B | 4k | 27.99 | 23.96 | 21.37 | 17.19 | 0 | - | -
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B-Chat | 13B | 4k | 27.99 | 23.96 | 21.37 | 17.19 | 0 | - | -
    |'
- en: '| Yi-34B | 34B | 200k | 26.98 | 22.14 | 17.00 | 13.24 | 8.13 | 4.02 | 1.19
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34B | 34B | 200k | 26.98 | 22.14 | 17.00 | 13.24 | 8.13 | 4.02 | 1.19
    |'
- en: 'Table 3: The evaluation results on GIST Subset. We report scores of multiple
    open-source and closed-source models across different lengths. “Ins” indicates
    “Instruct”.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: GIST 子集上的评估结果。我们报告了多个开源和闭源模型在不同长度下的得分。“Ins”表示“Instruct”。'
- en: 4.1 Experimental Setup
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'In this paper, we conduct a comprehensive evaluation of recent open-source
    and closed-source LLMs (Bai et al., [2023a](#bib.bib7); AI et al., [2024](#bib.bib4);
    Zeng et al., [2023](#bib.bib30); DeepSeek-AI et al., [2024](#bib.bib12); Du et al.,
    [2022](#bib.bib14); Liu et al., [2024a](#bib.bib19); Li* et al., [2023](#bib.bib17);
    Zhang et al., [2024](#bib.bib31); Team, [2023](#bib.bib28); AI@Meta, [2024](#bib.bib5);
    Baichuan, [2023](#bib.bib9); Achiam et al., [2023](#bib.bib3); [Moo,](#bib.bib2)
    ; [ERN,](#bib.bib1) ). Since the core idea of this benchmark is “long key information”
    rather than the total length of the text, we include some models that only retain
    the original 4k length window. [Table 1](#S2.T1 "Table 1 ‣ Long-context Benchmark
    ‣ 2 Related Work ‣ LongIns: A Challenging Long-context Instruction-based Exam
    for Large Language Models") provides basic information about the models we include.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们对最近的开源和闭源LLM进行了全面评估（Bai等，[2023a](#bib.bib7)；AI等，[2024](#bib.bib4)；Zeng等，[2023](#bib.bib30)；DeepSeek-AI等，[2024](#bib.bib12)；Du等，[2022](#bib.bib14)；Liu等，[2024a](#bib.bib19)；Li*等，[2023](#bib.bib17)；Zhang等，[2024](#bib.bib31)；Team，[2023](#bib.bib28)；AI@Meta，[2024](#bib.bib5)；Baichuan，[2023](#bib.bib9)；Achiam等，[2023](#bib.bib3)；[Moo,](#bib.bib2)；[ERN,](#bib.bib1)）。由于该基准的核心思想是“长关键内容”而非文本的总长度，我们包含了一些仅保留原始4k长度窗口的模型。[表
    1](#S2.T1 "表 1 ‣ 长文档基准 ‣ 2 相关工作 ‣ LongIns: 针对大型语言模型的具有挑战性的长文档指令考试")提供了我们包含的模型的基本信息。'
- en: 'The goal of this work is to construct a benchmark where LLMs must read and
    understand the entire context word by word to answer correctly. To achieve this,
    we employ an examination method requiring LLMs to carefully read all questions
    in a long text to correctly identify the “numbers of questions with incorrect
    answers.” A specific example can be found in [Table 7](#A1.T7 "Table 7 ‣ Appendix
    A Example Prompt for GIST with a Length of 256 Tokens ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models") in the appendix.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '本工作的目标是构建一个基准，在该基准下，LLMs必须逐字阅读和理解整个上下文才能正确回答。为此，我们采用了一种考试方法，要求LLMs仔细阅读长文本中的所有问题，以正确识别“错误答案的问题编号”。具体示例可在附录中的[表
    7](#A1.T7 "Table 7 ‣ Appendix A Example Prompt for GIST with a Length of 256 Tokens
    ‣ LongIns: A Challenging Long-context Instruction-based Exam for Large Language
    Models")中找到。'
- en: We also conduct experiments on the accuracy distribution of LLMs when incorrect
    questions are located at different depths for different text lengths. This approach
    reflects focus ability of LLMs at various positions and lengths. Additionally,
    since the length of questions affects LLM performance for the same total length,
    we conduct further analysis on question density under the same total length.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对在不同文本长度的不同深度位置的错误问题的准确性分布进行了实验。这种方法反映了LLMs在各种位置和长度下的专注能力。此外，由于问题长度影响LLM在相同总长度下的表现，我们对相同总长度下的问题密度进行了进一步分析。
- en: 4.2 Results on GIST Subset
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 GIST 子集的结果
- en: 'The overall evaluation results of our study are presented in [Table 3](#S4.T3
    "Table 3 ‣ 4 Experiment ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models"). Our inputs are sample created by concatenating
    multiple questions of the same type, with some answers intentionally altered to
    be incorrect. We generate prompt words using appropriate templates to guide the
    model, which serves as a grading tool, to identify all incorrectly answered questions.
    We use the F1 score between the list of actual incorrect question numbers and
    predicted incorrect question numbers predicted by LLMs. The results in the table
    indicate that closed-source models generally outperform open-source models, especially
    in fields requiring longer key information. Notably, GPT-4-turbo and GPT-4o maintain
    scores of 40.91 and 31.53, respectively, even with an exam length of 16,000 tokens,
    whereas other models are practically unusable at this length. Although a 256-token
    exam length is considered relatively short in the context of LLM interactions,
    certain LLMs still perform poorly at this length. This suggests that LLM performance
    declines when facing slightly longer texts that require sustained attention to
    key information. These findings highlight certain shortcomings in current LLMs,
    particularly in their multi-hop compositional reasoning abilities.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究的总体评估结果呈现在[表 3](#S4.T3 "Table 3 ‣ 4 Experiment ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models")中。我们的输入是通过将多个相同类型的问题连接起来创建的样本，部分答案被故意修改为错误。我们使用适当的模板生成提示词来引导模型，作为评分工具，以识别所有回答错误的问题。我们使用实际错误问题编号列表与LLMs预测的错误问题编号之间的F1分数。表中的结果表明，闭源模型通常优于开源模型，尤其是在需要较长关键信息的领域。值得注意的是，即使在16,000个令牌的考试长度下，GPT-4-turbo和GPT-4o的得分分别保持在40.91和31.53，而其他模型在此长度下几乎不可用。尽管在LLM交互的背景下，256个令牌的考试长度被认为相对较短，但某些LLM在此长度下仍表现不佳。这表明，面对稍长的文本时，LLM的表现会下降，因为这需要持续关注关键信息。这些发现突显了当前LLMs在多跳组合推理能力上的某些不足。'
- en: '| Model | Size | Support | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 支持 | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4o | \faLock | 128k | 76.31 | 74.65 | 71.16 | 66.55 | 58.83 | 56.23 |
    51.15 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | \faLock | 128k | 76.31 | 74.65 | 71.16 | 66.55 | 58.83 | 56.23 |
    51.15 |'
- en: '| GPT-4-Turbo | \faLock | 128k | 73.89 | 69.01 | 65.16 | 60.22 | 59.63 | 51.49
    | 44.22 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | \faLock | 128k | 73.89 | 69.01 | 65.16 | 60.22 | 59.63 | 51.49
    | 44.22 |'
- en: '| GPT-3.5-Turbo | \faLock | 16K | 51.60 | 50.12 | 42.04 | 32.07 | 18.58 | 19.67
    | 7.64 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | \faLock | 16K | 51.60 | 50.12 | 42.04 | 32.07 | 18.58 | 19.67
    | 7.64 |'
- en: '| ERNIE-Speed | \faLock | 8k | 42.34 | 36.77 | 33.52 | 23.67 | 16.86 | 4.18
    | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE-Speed | \faLock | 8k | 42.34 | 36.77 | 33.52 | 23.67 | 16.86 | 4.18
    | - |'
- en: '| Qwen-Long | \faLock | 10M | 59.00 | 54.45 | 53.87 | 47.34 | 38.29 | 35.22
    | 23.97 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-Long | \faLock | 10M | 59.00 | 54.45 | 53.87 | 47.34 | 38.29 | 35.22
    | 23.97 |'
- en: '| Deepseek-Chat | \faLock | 32k | 69.08 | 67.56 | 59.19 | 47.28 | 44.67 | 41.56
    | 35.23 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-Chat | \faLock | 32k | 69.08 | 67.56 | 59.19 | 47.28 | 44.67 | 41.56
    | 35.23 |'
- en: '| Yi-Large-Turbo | \faLock | 200k | 50.03 | 44.43 | 38.53 | 33.89 | 27.43 |
    26.19 | 17.38 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Large-Turbo | \faLock | 200k | 50.03 | 44.43 | 38.53 | 33.89 | 27.43 |
    26.19 | 17.38 |'
- en: '| Yi-Spark | \faLock | 200k | 43.93 | 38.19 | 34.39 | 29.86 | 26.66 | 21.26
    | 15.92 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Spark | \faLock | 200k | 43.93 | 38.19 | 34.39 | 29.86 | 26.66 | 21.26
    | 15.92 |'
- en: '| GLM-4 | \faLock | 128k | 58.52 | 53.00 | 48.58 | 44.64 | 42.15 | 41.02 |
    38.74 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4 | \faLock | 128k | 58.52 | 53.00 | 48.58 | 44.64 | 42.15 | 41.02 |
    38.74 |'
- en: '| \hdashline |  |  |  |  |  |  |  |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |  |  |  |  |  |  |'
- en: '| MAP-Neo-Ins-v0.1 | 7B | 8k | 41.96 | 36.95 | 27.87 | 23.02 | 16.04 | 6.37
    | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| MAP-Neo-Ins-v0.1 | 7B | 8k | 41.96 | 36.95 | 27.87 | 23.02 | 16.04 | 6.37
    | - |'
- en: '| ChatGLM2-6B | 6B | 32k | 20.75 | 17.00 | 14.08 | 10.81 | 6.38 | 2.93 | 0.87
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B | 6B | 32k | 20.75 | 17.00 | 14.08 | 10.81 | 6.38 | 2.93 | 0.87
    |'
- en: '| Deepseek-LLM-Chat | 7B | 4k | 29.08 | 24.01 | 16.75 | 12.99 | 3.61 | - |
    - |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-LLM-Chat | 7B | 4k | 29.08 | 24.01 | 16.75 | 12.99 | 3.61 | - |
    - |'
- en: '| Mistral-7B-Ins-v0.2 | 7B | 32k | 40.53 | 37.00 | 29.15 | 20.37 | 0 | 0 |
    0 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Ins-v0.2 | 7B | 32k | 40.53 | 37.00 | 29.15 | 20.37 | 0 | 0 |
    0 |'
- en: '| Qwen1.5-7B-Chat | 7B | 32k | 33.48 | 29.88 | 24.32 | 19.50 | 16.72 | 11.57
    | 5.22 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-7B-Chat | 7B | 32k | 33.48 | 29.88 | 24.32 | 19.50 | 16.72 | 11.57
    | 5.22 |'
- en: '| Internlm2-Chat-7B | 7B | 200k | 52.14 | 45.23 | 36.84 | 26.88 | 18.19 | 11.98
    | 6.06 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Internlm2-Chat-7B | 7B | 200k | 52.14 | 45.23 | 36.84 | 26.88 | 18.19 | 11.98
    | 6.06 |'
- en: '| Llama3-8B-Ins | 8B | 8k | 52.93 | 46.35 | 40.63 | 32.06 | 22.48 | 10.09 |
    - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Ins | 8B | 8k | 52.93 | 46.35 | 40.63 | 32.06 | 22.48 | 10.09 |
    - |'
- en: '| \hdashline |  |  |  |  |  |  |  |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |  |  |  |  |  |  |'
- en: '| LongChat-13B | 13B | 16k | 29.02 | 25.67 | 22.58 | 19.26 | 12.97 | 9.31 |
    5.51 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-13B | 13B | 16k | 29.02 | 25.67 | 22.58 | 19.26 | 12.97 | 9.31 |
    5.51 |'
- en: '| Baichuan2-13B-Chat | 13B | 4k | 30.77 | 25.10 | 23.95 | 20.52 | 0 | - | -
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B-Chat | 13B | 4k | 30.77 | 25.10 | 23.95 | 20.52 | 0 | - | -
    |'
- en: 'Table 4: The result of Local Instruction & Multiple Tasks. “Ins” indicates
    “Instruct”'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：局部指令与多任务的结果。“Ins”表示“指令”
- en: 4.3 Results on LIST Subset
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LIST 子集的结果
- en: 'To investigate the effect of LLMs’ dependence on instruction positioning, we
    conduct an ablation experiment by changing the instructions from global instructions
    to local instructions. The specific prompts are shown in [Figure 7](#A3.F7 "Figure
    7 ‣ C.2 Template of LIST ‣ Appendix C Prompt Template ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models"). For the GIST
    subset, the instructions are given only at the beginning, but for the LIST subset,
    instructions are provided before each question to explore the model’s dependency
    performance at different distances between the instructions and inputs. The results,
    as shown in [Table 5](#S5.T5 "Table 5 ‣ 5.1 Effect of the Position of Key Information
    ‣ 5 Discussion ‣ LongIns: A Challenging Long-context Instruction-based Exam for
    Large Language Models"), indicate that compared to the evaluation results on the
    GIST subset, models generally achieve higher scores on the LIST subset. Changing
    the position of instructions leads to higher model scores, indicating that most
    models are sensitive to the distance of instruction dependency. When instructions
    and inputs are separated by a greater distance, the performance of most models
    rapidly degrades. This is common across various models such as GPT-4-Turbo, including
    those that perform well in other tasks. And this issue generally affects the model’s
    reliance on the early memories in multi-turn conversations. But as a trivial solution,
    repeating the important instructions would significantly increase inference costs.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究LLMs对指令位置的依赖性，我们通过将指令从全局指令更改为局部指令来进行消融实验。具体的提示见[图7](#A3.F7 "图7 ‣ C.2 LIST模板
    ‣ 附录C 提示模板 ‣ LongIns：对大语言模型的挑战性长文本指令考试")。对于GIST子集，指令仅在开始时给出，而对于LIST子集，则在每个问题之前提供指令，以探索模型在指令与输入之间不同距离下的依赖性表现。结果如[表5](#S5.T5
    "表5 ‣ 5.1 关键信息位置的影响 ‣ 5 讨论 ‣ LongIns：对大语言模型的挑战性长文本指令考试")所示，与GIST子集的评估结果相比，模型在LIST子集上的得分普遍更高。改变指令位置会导致模型得分提高，表明大多数模型对指令依赖的距离非常敏感。当指令与输入之间的距离更大时，大多数模型的性能迅速下降。这在各种模型中都很常见，例如GPT-4-Turbo，包括那些在其他任务中表现良好的模型。这个问题通常影响模型在多轮对话中的早期记忆依赖。然而，作为一种简单的解决方案，重复重要的指令会显著增加推理成本。
- en: 4.4 Results on LIMT Subset
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 LIMT 子集的结果
- en: 'To explore the long-context comprehension ability of LLMs outside the in-context
    learning paradigm, we also concatenate the questions of different task types to
    the same length and evaluate on them. We extract a mini-mix dataset comprising
    50 samples for each of the 7 task types, totaling 350 samples. The evaluation
    results are shown in [Table 4](#S4.T4 "Table 4 ‣ 4.2 Results on GIST Subset ‣
    4 Experiment ‣ LongIns: A Challenging Long-context Instruction-based Exam for
    Large Language Models").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探索 LLM 在上下文学习范式之外的长上下文理解能力，我们还将不同任务类型的问题拼接到相同长度并进行评估。我们提取了一个迷你混合数据集，其中包含
    7 种任务类型的每种 50 个样本，共计 350 个样本。评估结果见[表 4](#S4.T4 "Table 4 ‣ 4.2 Results on GIST
    Subset ‣ 4 Experiment ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models")。'
- en: 'We observe that models generally score higher under the LIMT setting compared
    to the GIST setting. This is because questions from different types of tasks have
    different instructions, and therefore each output needs to be given separately.
    As a result, the distance between the question and the instructions is closer,
    and the dependence on the instructions becomes more apparent, leading to higher
    scores. In contrast, when compared with the evaluation results in [Table 5](#S5.T5
    "Table 5 ‣ 5.1 Effect of the Position of Key Information ‣ 5 Discussion ‣ LongIns:
    A Challenging Long-context Instruction-based Exam for Large Language Models"),
    it is evident that the LIMT setting are more challenging than the LIST setting
    under the same input format. This indicates that most models still fall short
    in tasks outside of the in-context learning paradigm.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，与 GIST 设置相比，模型在 LIMT 设置下的得分通常较高。这是因为不同类型任务的问题有不同的指令，因此每个输出需要单独处理。因此，问题与指令之间的距离更近，对指令的依赖性变得更加明显，从而导致更高的得分。相比之下，与[表
    5](#S5.T5 "Table 5 ‣ 5.1 Effect of the Position of Key Information ‣ 5 Discussion
    ‣ LongIns: A Challenging Long-context Instruction-based Exam for Large Language
    Models")中的评估结果相比，LIMT 设置在相同输入格式下比 LIST 设置更具挑战性。这表明大多数模型在上下文学习范式之外的任务中仍然存在不足。'
- en: 5 Discussion
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: 'During the analysis, we discover several noteworthy phenomena:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析过程中，我们发现了几个值得注意的现象：
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Yi-spark model, small-parameter model, achieves results close to those of
    larger parameter models, suggesting that the ability to focus on long key information
    is not strictly dependent on the number of parameters.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Yi-spark 模型，小参数模型，取得了接近大参数模型的结果，表明关注长关键信息的能力并不完全依赖于参数的数量。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The ERNIE-Speed model has a relatively low score because its strict review mechanism
    results in many refusals to answer.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ERNIE-Speed 模型得分较低，因为其严格的审查机制导致许多拒绝回答。
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some open-source models, such as baichuan2-13B-Chat and mistral-7b-32k, exhibit
    a sharp degradation when the length of key information approaches but does not
    exceed their context window limit. For instance, when the test length jumps from
    2k to 4k tokens, these models suddenly fail to follow instructions correctly and
    start producing nonsensical outputs. We believe this implies the true length that
    some models can handle effectively.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些开源模型，如 baichuan2-13B-Chat 和 mistral-7b-32k，当关键信息的长度接近但不超过其上下文窗口限制时，表现出急剧的性能下降。例如，当测试长度从
    2k 跃升至 4k 令牌时，这些模型突然无法正确执行指令，开始生成无意义的输出。我们认为这表明了一些模型有效处理的实际长度。
- en: 5.1 Effect of the Position of Key Information
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 关键信息的位置效果
- en: “Needle in a Haystack”   (Doe and Smith, [2022](#bib.bib13)) and “Lost In the
    Middle”   (Liu et al., [2023b](#bib.bib22)) indicate LLMs exhibit varying levels
    of attention when processing different positions within long texts. Therefore,
    we analyze the LLMs’ ability to handle key information located at different positions
    by controlling the distribution of error question.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: “针在干草堆中”（Doe 和 Smith，[2022](#bib.bib13)）和“迷失在中间”（Liu 等，[2023b](#bib.bib22)）表明
    LLM 在处理长文本中不同位置时表现出不同程度的注意力。因此，我们通过控制错误问题的分布来分析 LLM 处理位于不同位置的关键信息的能力。
- en: 'Specifically, We analyze the model’s sensitivity to position when dealing with
    lengthy key information by controlling the depth of incorrect questions within
    papers of various lengths and recording the model’s accuracy at those positions.
    The results are shown in [Figure 3](#S5.F3 "Figure 3 ‣ 5.2 Effect of the Density
    of Key Information ‣ 5 Discussion ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models").'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们通过控制试卷中错误问题的深度，分析模型在处理长篇关键信息时对位置的敏感性，并记录模型在这些位置的准确性。结果显示在[图3](#S5.F3
    "Figure 3 ‣ 5.2 Effect of the Density of Key Information ‣ 5 Discussion ‣ LongIns:
    A Challenging Long-context Instruction-based Exam for Large Language Models")中。'
- en: 'Through the above experiment, we can clearly observe the performance of models
    when faced with incorrect answers at different depths and lengths of test papers.
    Using the GPT series and Yi series as examples, both exhibit the same characteristics:
    the models demonstrate better recognition ability at the beginning of the test
    paper for the same length, with performance gradually declining as depth increases.
    The longer the overall length of the test paper at the same depth, the worse the
    performance. This is generally in line with our expectations. However, it is noteworthy
    that, unlike needle-in-a-haystack or other long-text benchmarks, LongIns only
    requires length of 16k to challenge the current state-of-the-art GPT-4 series
    models, while conventional closed-source models (such as the Yi series models)
    only require 8k to basically reach their performance limit. More depth-length
    two-dimensional diagrams for additional models can be found in [Figure 5](#A2.F5
    "Figure 5 ‣ Appendix B HotMap of More Models ‣ LongIns: A Challenging Long-context
    Instruction-based Exam for Large Language Models") in the appendix.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '通过上述实验，我们可以清楚地观察到模型在面对不同深度和长度的试卷时的表现。以GPT系列和Yi系列为例，它们都展示了相同的特征：对于相同长度的试卷，模型在试卷开始时具有更好的识别能力，但随着深度的增加，表现逐渐下降。在相同深度下，试卷的整体长度越长，性能越差。这通常符合我们的预期。然而，值得注意的是，与needle-in-a-haystack或其他长文本基准测试不同，LongIns只需长度为16k即可挑战当前最先进的GPT-4系列模型，而传统的闭源模型（如Yi系列模型）只需8k即可基本达到其性能极限。更多模型的深度-长度二维图可以在附录中的[图5](#A2.F5
    "Figure 5 ‣ Appendix B HotMap of More Models ‣ LongIns: A Challenging Long-context
    Instruction-based Exam for Large Language Models")中找到。'
- en: '| Model | Size | Support | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Model | Size | Support | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4o | \faLock | 128k | 74.89 | 74.62 | 70.43 | 66.98 | 61.82 | 59.07 |
    54.30 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | \faLock | 128k | 74.89 | 74.62 | 70.43 | 66.98 | 61.82 | 59.07 |
    54.30 |'
- en: '| GPT-4-Turbo | \faLock | 128k | 71.62 | 67.47 | 61.11 | 59.36 | 56.62 | 51.01
    | 44.91 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | \faLock | 128k | 71.62 | 67.47 | 61.11 | 59.36 | 56.62 | 51.01
    | 44.91 |'
- en: '| GPT-3.5-Turbo | \faLock | 16K | 53.03 | 44.28 | 40.16 | 33.80 | 26.55 | 19.81
    | 14.03 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | \faLock | 16K | 53.03 | 44.28 | 40.16 | 33.80 | 26.55 | 19.81
    | 14.03 |'
- en: '| ERNIE-Speed | \faLock | 8k | 51.29 | 40.38 | 31.86 | 25.90 | 16.24 | 2.61
    | - |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE-Speed | \faLock | 8k | 51.29 | 40.38 | 31.86 | 25.90 | 16.24 | 2.61
    | - |'
- en: '| Qwen-Long | \faLock | 10M | 64.85 | 56.32 | 44.46 | 36.11 | 29.05 | 23.16
    | 15.44 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-Long | \faLock | 10M | 64.85 | 56.32 | 44.46 | 36.11 | 29.05 | 23.16
    | 15.44 |'
- en: '| Deepseek-Chat | \faLock | 32k | 67.71 | 63.13 | 56.99 | 51.04 | 46.78 | 42.00
    | 37.25 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-Chat | \faLock | 32k | 67.71 | 63.13 | 56.99 | 51.04 | 46.78 | 42.00
    | 37.25 |'
- en: '| Yi-Large-Turbo | \faLock | 200k | 60.05 | 53.78 | 42.11 | 36.09 | 27.20 |
    21.66 | 10.75 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Large-Turbo | \faLock | 200k | 60.05 | 53.78 | 42.11 | 36.09 | 27.20 |
    21.66 | 10.75 |'
- en: '| Yi-Spark | \faLock | 200k | 48.24 | 40.48 | 34.35 | 26.37 | 19.04 | 8.34
    | 1.01 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Spark | \faLock | 200k | 48.24 | 40.48 | 34.35 | 26.37 | 19.04 | 8.34
    | 1.01 |'
- en: '| GLM-4 | \faLock | 128K | 60.25 | 54.11 | 49.79 | 46.05 | 41.32 | 37.88 |
    34.15 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4 | \faLock | 128K | 60.25 | 54.11 | 49.79 | 46.05 | 41.32 | 37.88 |
    34.15 |'
- en: '| \hdashline |  |  |  |  |  |  |  |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |  |  |  |  |  |  |'
- en: '| MAP-Neo-Ins-v0.1 | 7B | 8k | 39.34 | 33.71 | 28.45 | 23.82 | 16.04 | 6.83
    | - |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| MAP-Neo-Ins-v0.1 | 7B | 8k | 39.34 | 33.71 | 28.45 | 23.82 | 16.04 | 6.83
    | - |'
- en: '| Deepseek-LLM-Chat | 7B | 4k | 17.70 | 12.93 | 8.42 | 7.47 | 1.16 | - | -
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-LLM-Chat | 7B | 4k | 17.70 | 12.93 | 8.42 | 7.47 | 1.16 | - | -
    |'
- en: '| Qwen1.5-7B-Chat | 7B | 32k | 25.93 | 24.71 | 20.81 | 14.71 | 12.35 | 11.05
    | 10.69 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-7B-Chat | 7B | 32k | 25.93 | 24.71 | 20.81 | 14.71 | 12.35 | 11.05
    | 10.69 |'
- en: '| Llama3-8B-Ins | 8B | 8k | 46.84 | 40.97 | 37.41 | 27.46 | 21.11 | 16.08 |
    - |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Ins | 8B | 8k | 46.84 | 40.97 | 37.41 | 27.46 | 21.11 | 16.08 |
    - |'
- en: '| \hdashline |  |  |  |  |  |  |  |  |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline |  |  |  |  |  |  |  |  |  |'
- en: '| LongChat-13B | 13B | 16k | 28.19 | 25.13 | 23.00 | 19.27 | 15.32 | 9.22 |
    6.45 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-13B | 13B | 16k | 28.19 | 25.13 | 23.00 | 19.27 | 15.32 | 9.22 |
    6.45 |'
- en: '| Baichuan2-13B-Chat | 13B | 4k | 31.28 | 26.33 | 23.70 | 20.87 | 0 | - | -
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B-Chat | 13B | 4k | 31.28 | 26.33 | 23.70 | 20.87 | 0 | - | -
    |'
- en: 'Table 5: The result of Local Instruction & Single Task. “Ins” indicates “Instruct”'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：本地指令与单任务的结果。“Ins”表示“指令”
- en: 5.2 Effect of the Density of Key Information
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 关键信息密度的影响
- en: '![Refer to caption](img/cf9c2a807fbf2b172dd00efe65f81fb3.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cf9c2a807fbf2b172dd00efe65f81fb3.png)'
- en: 'Figure 2: The impact of question density on model performance. The horizontal
    axis represents the number of questions per 100 tokens in the paper; the higher
    this value, the more questions are contained within the same total length. The
    vertical axis represents the model’s score at that question density.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：问题密度对模型性能的影响。横轴表示每100个token中问题的数量；这个值越高，相同总长度中包含的问题就越多。纵轴表示在该问题密度下模型的得分。
- en: In our current setup, we concatenate questions of the same type to form a test
    paper, maintaining a constant length. However, due to variations in the lengths
    of the questions, the number of questions in papers of the same length varies.
    Therefore, we analyze the accuracy of multiple models with varying numbers of
    questions (i.e., question density) under the same test length, aiming to explore
    the impact of key information density on LLMs. We track the scoring results of
    test papers with different question densities and plot a graph of scores varying
    with question density.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的设置中，我们将同类型的问题串联成一份测试卷，保持长度不变。然而，由于问题长度的变化，相同长度的试卷中的问题数量有所不同。因此，我们分析了在相同测试长度下，具有不同数量问题（即问题密度）的多个模型的准确性，旨在探索关键信息密度对LLMs的影响。我们跟踪不同问题密度的测试卷的评分结果，并绘制出得分随问题密度变化的图表。
- en: '![Refer to caption](img/75001b667a3996564e00e39369eb5044.png)![Refer to caption](img/8a70e6513ba21fc61a759cac8544ffb4.png)![Refer
    to caption](img/9497b893217f3320372987dcd6277a67.png)![Refer to caption](img/976c7cc8a717946dd263fc563982eb64.png)![Refer
    to caption](img/f059e79ab87c29e49043a000c0671bcc.png)![Refer to caption](img/4c4efe0e5c0b09fc3646c4352e915d76.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75001b667a3996564e00e39369eb5044.png)![参考说明](img/8a70e6513ba21fc61a759cac8544ffb4.png)![参考说明](img/9497b893217f3320372987dcd6277a67.png)![参考说明](img/976c7cc8a717946dd263fc563982eb64.png)![参考说明](img/f059e79ab87c29e49043a000c0671bcc.png)![参考说明](img/4c4efe0e5c0b09fc3646c4352e915d76.png)'
- en: 'Figure 3: The impact of positional distribution and overall length of the questions
    on model performance is illustrated, with GPT series models on the left and Yi
    series models on the right. The horizontal axis represents the total length of
    the questions, while the vertical axis indicates the position of incorrect answers
    within the questions, where 0 denotes the beginning and 9 denotes the end. The
    greener the color, the higher the model’s recognition accuracy at that position.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：展示了问题的位置信息分布和总体长度对模型性能的影响，左侧为GPT系列模型，右侧为Yi系列模型。横轴表示问题的总长度，纵轴表示问题中错误答案的位置，0表示开始，9表示结束。颜色越绿，表示模型在该位置的识别准确率越高。
- en: 'As illustrated in [Figure 2](#S5.F2 "Figure 2 ‣ 5.2 Effect of the Density of
    Key Information ‣ 5 Discussion ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models"), it is evident that most models exhibit a performance
    degradation as the number of questions increases under the same test paper length.
    This includes high-scoring models such as Qwen. This demonstrates most models
    are more sensitive to the number of questions when the total length is the same.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图 2](#S5.F2 "图 2 ‣ 5.2 关键信息密度的影响 ‣ 5 讨论 ‣ LongIns: 大型语言模型挑战性长上下文指令考试")所示，可以明显看出，大多数模型在相同测试卷长度下，随着问题数量的增加，表现会有所下降。这包括像Qwen这样的高分模型。这表明，在总长度相同的情况下，大多数模型对问题数量更为敏感。'
- en: This also indicates that besides the length of the sample, the density of key
    information is also an important factor affecting performance. However, GPT-4-turbo
    and GPT-4o maintain good performance even at higher question densities, which
    to some extent indicates their robustness to changes in key information density.
    They can still maintain high-density information processing capabilities in long
    contexts.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这也表明，除了样本长度之外，关键信息的密度也是影响性能的重要因素。然而，即使在较高的问题密度下，GPT-4-turbo和GPT-4o仍能保持良好的性能，这在某种程度上表明它们对关键信息密度变化的鲁棒性。它们仍能在长上下文中保持高密度信息处理能力。
- en: '| Model | Accuracy |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 |'
- en: '| --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-4-Turbo | 95.7 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | 95.7 |'
- en: '| GPT-3.5-Turbo | 87.0 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 87.0 |'
- en: '| Deepseek-v1 | 88.2 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-v1 | 88.2 |'
- en: '| Yi-Large-Turbo | 87.8 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Yi-Large-Turbo | 87.8 |'
- en: '| Llama3-8B-Instruct | 81.0 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Instruct | 81.0 |'
- en: '| ERNIE-Speed | 81.5 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE-Speed | 81.5 |'
- en: '| GLM-4 | 90.1 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GLM-4 | 90.1 |'
- en: '| Qwen-Long | 83.6 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-Long | 83.6 |'
- en: 'Table 6: Directly present the QA of a single question to the model and have
    it determine whether the answer is correct, controlling the ratio of true labels
    to be 1:1\. The accuracy of the LLMs is then evaluated.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 直接向模型呈现单个问题的问答，并让其判断答案是否正确，控制真实标签的比例为1:1。然后评估LLMs的准确率。'
- en: 5.3 Ablation Experiment of Single Question
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 单问题消融实验
- en: 'LongIns aims to assess the ability of LLMs to maintain focus over longer key
    information by identifying incorrect question numbers through a full-text review.
    However, LLMs might make recognition errors not due to its lack of contextual
    capabilities but because it inherently lacks the ability to answer questions.
    Therefore, we set up this ablation experiment to ensure a true evaluation of the
    long-context abilities of LLMs. We conduct experiments using single questions
    as test papers for several models. The results, as shown in [Table 6](#S5.T6 "Table
    6 ‣ 5.2 Effect of the Density of Key Information ‣ 5 Discussion ‣ LongIns: A Challenging
    Long-context Instruction-based Exam for Large Language Models"), indicate that
    the accuracy of the models can generally exceed 80%, with some, like GPT-4-turbo,
    reaching up to 95.7%. These results suggest that LongIns provides a reliable evaluation
    of the actual contextual length performance of LLMs.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'LongIns旨在通过全面审查来识别不正确的问题编号，评估LLMs在长时间保持对关键信息关注的能力。然而，LLMs可能出现识别错误并非因为其缺乏上下文能力，而是因为其固有的回答问题能力的不足。因此，我们设置了这个消融实验，以确保对LLMs的长上下文能力进行真实评估。我们使用单个问题作为测试卷，对多个模型进行实验。结果如[表
    6](#S5.T6 "Table 6 ‣ 5.2 Effect of the Density of Key Information ‣ 5 Discussion
    ‣ LongIns: A Challenging Long-context Instruction-based Exam for Large Language
    Models")所示，模型的准确率通常超过80%，一些模型如GPT-4-turbo甚至达到了95.7%。这些结果表明，LongIns提供了对LLMs实际上下文长度表现的可靠评估。'
- en: 5.4 Effect of Different Task Types
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 不同任务类型的影响
- en: 'We also explore the performance of LLMs when using test papers composed of
    different task types., as shown in [Figure 4](#S5.F4 "Figure 4 ‣ 5.4 Effect of
    Different Task Types ‣ 5 Discussion ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models").'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还探讨了使用不同任务类型组成的测试卷时LLMs的表现，如[图 4](#S5.F4 "Figure 4 ‣ 5.4 Effect of Different
    Task Types ‣ 5 Discussion ‣ LongIns: A Challenging Long-context Instruction-based
    Exam for Large Language Models")所示。'
- en: A notable trend is that the model generally performs poorly in the common sense
    category but achieves higher scores in tasks such as NER and Reading Comprehension.
    By analyzing the actual responses and the questions, we find that despite our
    efforts to standardize the format of questions within the same test paper, there
    are still significant distribution disparities among different task types. The
    questions in the common sense category tend to vary widely within the same test
    paper, whereas tasks like NER and Reading Comprehension have almost consistent
    formats, which can stimulate the model’s in-context learning ability. When the
    variation is large, the advantage of in-context learning is almost nonexistent,
    leading to the model’s generally poor performance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著的趋势是，模型在常识类别中的表现通常较差，但在NER和阅读理解等任务中得分较高。通过分析实际回应和问题，我们发现尽管我们努力标准化同一测试卷中的问题格式，但不同任务类型之间仍存在显著的分布差异。常识类别中的问题在同一测试卷内变化较大，而NER和阅读理解等任务的格式几乎一致，这可以激发模型的上下文学习能力。当变化较大时，上下文学习的优势几乎不存在，导致模型的表现普遍较差。
- en: '![Refer to caption](img/ef3207580a355e216486536cf072d14b.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ef3207580a355e216486536cf072d14b.png)'
- en: 'Figure 4: The average performance of the model across different task types.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 模型在不同任务类型下的平均表现。'
- en: 6 Conclusion
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduce LongIns Bench, a benchmark designed to evaluate
    the long-context processing capabilities of LLMs by assessing their true reading
    window length. LongIns consists of over 1400 task types with varying lengths and
    controlled distributions of incorrect answers. Our findings highlight that models
    perform better with closer instruction distances, degrade with increasing length
    and density of key information, and exhibit varied performance across different
    task types. Notably, GPT-4-turbo and GPT-4o demonstrate robustness to high-density
    information. LongIns provides a reliable framework for assessing LLMs’ proficiency
    in understanding extensive sequences, offering valuable insights for future research
    and development.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 LongIns Bench，这是一个旨在通过评估 LLM 的实际阅读窗口长度来评估长上下文处理能力的基准。LongIns 包含超过
    1400 种任务类型，具有不同的长度和控制的错误答案分布。我们的发现突出显示了模型在接近指令距离时表现更好，随着关键信息的长度和密度增加而表现下降，并且在不同任务类型之间表现各异。值得注意的是，GPT-4-turbo
    和 GPT-4o 在高密度信息下表现出较强的稳健性。LongIns 提供了一个可靠的框架来评估 LLM 在理解广泛序列方面的能力，为未来的研究和开发提供了有价值的见解。
- en: Limitations
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Despite the comprehensive evaluation framework proposed by LongIns, there are
    several limitations to our study. The test set in LongIns is constructed using
    synthetic data, which, while controlled and consistent, may not entirely reflect
    the complexity and variability of natural language inputs. Additionally, our evaluation
    methodology emphasizes identifying incorrect answers within long texts, which
    may not cover other critical aspects of LLM performance such as creativity, user
    engagement, and adaptability to unforeseen inputs.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LongIns 提出了全面的评估框架，但我们的研究仍存在若干局限性。LongIns 的测试集使用合成数据构建，尽管受控且一致，但可能未能完全反映自然语言输入的复杂性和变异性。此外，我们的评估方法强调在长文本中识别错误答案，这可能未涵盖
    LLM 性能的其他关键方面，如创造力、用户参与度以及对不可预见输入的适应性。
- en: Ethics Statement
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This research adheres to ethical guidelines for AI development. We aim to enhance
    the capabilities of LLMs while acknowledging potential risks such as bias, misuse,
    and privacy concerns. To mitigate these, we advocate for transparency, rigorous
    bias testing, robust security measures, and human oversight in AI applications.
    Our goal is to contribute positively to the field and to encourage responsible
    AI development and deployment.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究遵循 AI 发展的伦理指南。我们旨在增强 LLM 的能力，同时承认潜在的风险，如偏见、误用和隐私问题。为减轻这些风险，我们倡导透明度、严格的偏见测试、强大的安全措施以及对
    AI 应用的人工监督。我们的目标是积极贡献于该领域，并鼓励负责任的 AI 发展和部署。
- en: References
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Ernie-speed. [https://qianfan.cloud.baidu.com/](https://qianfan.cloud.baidu.com/).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Ernie-speed. [https://qianfan.cloud.baidu.com/](https://qianfan.cloud.baidu.com/)。
- en: (2) Moonshot. [https://www.moonshot.cn/](https://www.moonshot.cn/).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Moonshot. [https://www.moonshot.cn/](https://www.moonshot.cn/)。
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等。2023. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: 'AI et al. (2024) 01\. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,
    Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. [Yi:
    Open foundation models by 01.ai](https://arxiv.org/abs/2403.04652). *Preprint*,
    arXiv:2403.04652.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AI et al. (2024) 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge
    Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, 和 Zonghong Dai. 2024. [Yi:
    Open foundation models by 01.ai](https://arxiv.org/abs/2403.04652). *预印本*, arXiv:2403.04652。'
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai
    Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. [L-eval: Instituting standardized
    evaluation for long context language models](https://arxiv.org/abs/2307.11088).
    *Preprint*, arXiv:2307.11088.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An et al. (2023) 陈欣·安、尚尚·龚、明中·钟、兴建·赵、木开·李、俊·张、灵鹏·孔和西鹏·丘。2023年。 [L-eval: 为长上下文语言模型制定标准化评估](https://arxiv.org/abs/2307.11088)。
    *预印本*，arXiv:2307.11088。'
- en: Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023a. [Qwen technical report](https://arxiv.org/abs/2309.16609).
    *Preprint*, arXiv:2309.16609.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023a) 金泽·白、帅·白、云飞·楚、泽宇·崔、凯·邓、晓东·邓、杨·范、文彬·葛、余·韩、飞·黄、滨远·惠、罗·纪、梅·李、俊扬·林、润吉·林、大一恒·刘、高·刘、程强·卢、柯名·卢、建新·马、瑞·门、兴章·任、轩成·任、传启·谭、思楠·谭、建宏·涂、彭·王、世杰·王、伟·王、圣广·吴、本锋·徐、晋·徐、安·杨、浩·杨、健·杨、书生·杨、杨·姚、博文·余、鸿义·袁、郑·袁、建伟·张、兴轩·张、一昌·张、珍如·张、常·周、敬人·周、晓欢·周和天航·朱。2023a。
    [Qwen技术报告](https://arxiv.org/abs/2309.16609)。 *预印本*，arXiv:2309.16609。
- en: 'Bai et al. (2023b) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai
    Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong,
    Jie Tang, and Juanzi Li. 2023b. Longbench: A bilingual, multitask benchmark for
    long context understanding. *arXiv preprint arXiv:2308.14508*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023b) 余士·白、辛·吕、佳杰·张、洪畅·吕、建凯·唐、智点·黄、郑孝·杜、小·刘、敖汉·曾、磊·侯、余晓·董、杰·唐和娟子·李。2023b。Longbench:
    一个用于长上下文理解的双语多任务基准。 *arXiv预印本 arXiv:2308.14508*。'
- en: 'Baichuan (2023) Baichuan. 2023. [Baichuan 2: Open large-scale language models](https://arxiv.org/abs/2309.10305).
    *arXiv preprint arXiv:2309.10305*.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baichuan (2023) 百川。2023年。 [Baichuan 2: 开放的大规模语言模型](https://arxiv.org/abs/2309.10305)。
    *arXiv预印本 arXiv:2309.10305*。'
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. Extending context window of large language models via positional interpolation.
    *CoRR*, abs/2306.15595.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) 首元·陈、谢尔曼·黄、梁剑·陈和远东·田。2023年。通过位置插值扩展大语言模型的上下文窗口。 *CoRR*，abs/2306.15595。
- en: 'Chen et al. (2024) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. 2024. [Longlora: Efficient fine-tuning of long-context
    large language models](https://arxiv.org/abs/2309.12307). *Preprint*, arXiv:2309.12307.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024) 余康·陈、盛聚·钱、浩天·唐、辛·赖、志坚·刘、宋·韩和佳雅·贾。2024年。 [Longlora: 高效微调长上下文大语言模型](https://arxiv.org/abs/2309.12307)。
    *预印本*，arXiv:2309.12307。'
- en: 'DeepSeek-AI et al. (2024) DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan
    Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo,
    Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo
    Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui
    Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong
    Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai
    Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao,
    Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang,
    Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu
    Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi
    Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng
    Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng,
    T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen
    Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang,
    Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen,
    Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan
    Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei,
    Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li,
    Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying
    Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo,
    Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang
    You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang,
    Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu
    Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. 2024. [Deepseek-v2: A strong,
    economical, and efficient mixture-of-experts language model](https://arxiv.org/abs/2405.04434).
    *Preprint*, arXiv:2405.04434.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepSeek-AI 等人（2024）DeepSeek-AI、刘爱新、冯贝、王斌、王冰轩、刘博、赵成刚、邓程琪、阮冲、戴大迈、郭大雅、杨德建、陈德丽、季冬杰、李尔杭、林方云、罗福莉、郝光波、陈冠亭、李国伟、张宏、徐汉伟、杨浩、张浩炜、丁红辉、辛华建、高华佐、李辉、曲慧、蔡锦龙、梁建、郭建中、倪佳琦、李佳士、陈晋、袁晶阳、邱俊杰、宋俊晓、董凯、高凯歌、关康、王梁、张乐聪、徐磊、夏乐怡、赵亮、张莉月、李萌、王苗君、张铭川、张铭华、唐铭辉、李铭铭、田宁、黄盼盼、王培义、张鹏、朱启浩、陈钦瑜、杜启曙、陈睿祺、金如熙、卢尚浩、周尚彦、陈善黄、吴少青、叶盛丰、马世荣、王诗宇、周顺丰、郑大小、王天、裴天、袁天、孙天宇、肖伟、曾望鼎、安伟、刘文、梁文峰、高文俊、张文涛、李曦、金翔岳、王宇博、刘小兵、刘晓东、王晓寒、沈晓金、陈晓康、陈晓莎、聂晓涛、孙晓文、王晓祥、刘鑫、谢鑫、余兴凯、宋新南、周欣怡、杨新宇、任泽辉、任泽辉、沙张力、傅哲、黄真、张真、谢振达、郝哲文、邵志红、温之牛、徐忠宇、李卓舒、王子慧、李紫伟、谢子维。2024年。[Deepseek-v2:
    A strong, economical, and efficient mixture-of-experts language model](https://arxiv.org/abs/2405.04434)。*预印本*，arXiv:2405.04434。'
- en: 'Doe and Smith (2022) John Doe and Jane Smith. 2022. Needle in a haystack: Benchmark
    for extremely imbalanced data. In *Proceedings of the International Conference
    on Machine Learning (ICML)*.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doe 和 Smith（2022）约翰·多伊 和 简·史密斯。2022年。针尖上的大海：极度不平衡数据的基准。在 *国际机器学习会议（ICML）论文集*
    中。
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. [Glm: General language model pretraining with
    autoregressive blank infilling](https://arxiv.org/abs/2103.10360). *Preprint*,
    arXiv:2103.10360.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人（2022）郑效杜、钱玉洁、刘晓、丁鸣、邱杰钟、杨志林、唐杰。2022年。[Glm: General language model pretraining
    with autoregressive blank infilling](https://arxiv.org/abs/2103.10360)。*预印本*，arXiv:2103.10360。'
- en: 'Kwan et al. (2023) Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou
    Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong. 2023. M4LE: A Multi-Ability Multi-Range
    Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwan 等人（2023）关伟忠、曾兴山、王宇飞、孙宇森、李亮友、尚丽峰、刘群、黄锦辉。2023年。M4LE：一个多能力、多范围、多任务、多领域的长上下文评估基准，用于大型语言模型。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. In *Advances in Neural Information Processing Systems*, volume 33,
    pages 9459–9474.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, 等. 2020. 检索增强生成用于知识密集型NLP任务。在*神经信息处理系统进展*，第33卷，第9459–9474页。
- en: Li* et al. (2023) Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianming Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. [How long can
    open-source llms truly promise on context length?](https://lmsys.org/blog/2023-06-29-longchat)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li* et al. (2023) Dacheng Li*，Rulin Shao*，Anze Xie，Ying Sheng，Lianming Zheng，Joseph
    E. Gonzalez，Ion Stoica，Xuezhe Ma 和 Hao Zhang. 2023. [开源LLM在上下文长度上的真实承诺有多长？](https://lmsys.org/blog/2023-06-29-longchat)
- en: 'Li et al. (2023) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.
    Loogle: Can long-context language models understand long contexts? *arXiv preprint
    arXiv:2311.04939*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Jiaqi Li, Mengmeng Wang, Zilong Zheng, 和 Muhan Zhang. 2023.
    Loogle：长上下文语言模型能否理解长上下文？*arXiv 预印本 arXiv:2311.04939*。
- en: Liu et al. (2024a) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.
    [World model on million-length video and language with blockwise ringattention](https://arxiv.org/abs/2402.08268).
    *Preprint*, arXiv:2402.08268.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024a) Hao Liu, Wilson Yan, Matei Zaharia, 和 Pieter Abbeel. 2024a.
    [百万长度视频和语言的世界模型与分块环注意力](https://arxiv.org/abs/2402.08268)。*预印本*，arXiv:2402.08268。
- en: Liu et al. (2023a) Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023a. [Ring attention
    with blockwise transformers for near-infinite context](https://arxiv.org/abs/2310.01889).
    *Preprint*, arXiv:2310.01889.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Hao Liu, Matei Zaharia, 和 Pieter Abbeel. 2023a. [环形注意力与分块变换器用于近无限上下文](https://arxiv.org/abs/2310.01889)。*预印本*，arXiv:2310.01889。
- en: 'Liu et al. (2024b) Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang,
    Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et al. 2024b.
    E2-llm: Efficient and extreme length extension of large language models. *arXiv
    preprint arXiv:2401.06951*.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024b) Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu
    Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, 等. 2024b. E2-llm：大规模语言模型的高效和极限长度扩展。*arXiv
    预印本 arXiv:2401.06951*。
- en: 'Liu et al. (2023b) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. [Lost in the middle:
    How language models use long contexts](https://api.semanticscholar.org/CorpusID:259360665).
    *Transactions of the Association for Computational Linguistics*, 12:157–173.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, 和 Percy Liang. 2023b. [迷失在中间：语言模型如何使用长上下文](https://api.semanticscholar.org/CorpusID:259360665)。*计算语言学协会会刊*，12:157–173。
- en: NormXU (2021) NormXU. 2021. An experiment on dynamic ntk scaling rope. [https://github.com/NormXU/Consistent-DynamicNTKRoPE](https://github.com/NormXU/Consistent-DynamicNTKRoPE).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NormXU (2021) NormXU. 2021. 动态NTK缩放绳实验。 [https://github.com/NormXU/Consistent-DynamicNTKRoPE](https://github.com/NormXU/Consistent-DynamicNTKRoPE)。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023. Yarn: Efficient context window extension of large language models. *CoRR*,
    abs/2309.00071.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, 和 Enrico Shippole.
    2023. Yarn：大规模语言模型的高效上下文窗口扩展。*CoRR*，abs/2309.00071。
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Omer Levy. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. In
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 3672–3683.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2021) Ofir Press, Noah A Smith, 和 Omer Levy. 2021. 短期训练，长期测试：具有线性偏差的注意力机制实现输入长度外推。在*计算语言学协会发现：ACL-IJCNLP
    2021*，第3672–3683页。
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. 2023. [Zeroscrolls: A zero-shot benchmark for long text understanding](https://arxiv.org/abs/2305.14196).
    *Preprint*, arXiv:2305.14196.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, 和 Omer
    Levy. 2023. [Zeroscrolls：一个零样本基准测试用于长文本理解](https://arxiv.org/abs/2305.14196)。*预印本*，arXiv:2305.14196。
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, 和
    Yunfeng Liu. 2021. Roformer：增强的变换器与旋转位置嵌入。*arXiv 预印本 arXiv:2104.09864*。
- en: 'Team (2023) InternLM Team. 2023. Internlm: A multilingual language model with
    progressively enhanced capabilities. [https://github.com/InternLM/InternLM-techreport](https://github.com/InternLM/InternLM-techreport).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队（2023）InternLM团队。2023。Internlm：一款多语言模型，逐步增强的能力。[https://github.com/InternLM/InternLM-techreport](https://github.com/InternLM/InternLM-techreport)。
- en: 'Wang et al. (2024) Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi
    Rezagholizadeh, and Armaghan Eshaghi. 2024. Beyond the limits: A survey of techniques
    to extend the context length in large language models. *arXiv preprint arXiv:2402.02244*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2024）王欣迪，马赫莎·萨尔马尼，帕尔萨·奥米迪，向宇·任，梅赫迪·雷扎戈利扎德，和阿尔马干·艾沙基。2024。超越极限：扩展大型语言模型上下文长度的技术综述。*arXiv预印本
    arXiv:2402.02244*。
- en: 'Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2023. GLM-130B:
    an open bilingual pre-trained model. In *The Eleventh International Conference
    on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曾等人（2023）曾傲涵，刘晓，郑霄杜，紫涵·王，汉宇·赖，明丁，卓怡·杨，逸凡·徐，文迪·郑，肖霞等。2023。GLM-130B：一个开放的双语预训练模型。在*第十一届国际学习表示会议，ICLR
    2023，卢旺达基加利，2023年5月1日至5日*。
- en: 'Zhang et al. (2024) Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua
    Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan,
    Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma,
    Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu,
    Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu
    Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang,
    Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang,
    and Wenhu Chen. 2024. Map-neo: Highly capable and transparent bilingual large
    language model series. *arXiv preprint arXiv: 2405.19327*.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等人（2024）张戈，斯科特·屈，贾恒·刘，陈晨·张，程华·林，周亮·余，丹尼·潘，艾斯特·程，杰·刘，群书·林，拉文·袁，托尼·郑，魏磅，辛润·杜，易名·梁，英浩·马，易智·李，自扬·马，比尔·林，埃曼努伊尔·贝内托斯，欢·杨，君廷·周，开静·马，明浩·刘，莫里·牛，诺亚·王，奎瑞·奎，瑞博·刘，斯内·刘，肖恩·郭，索伦·高，王春树·周，欣月·张，易智·周，宇博·王，月林·白，雨涵·张，宇翔·张，泽尼思·王，振珠·杨，自建·赵，佳俊·张，万里·欧阳，文豪·黄，和文虎·陈。2024。Map-neo：高能力和透明的双语大型语言模型系列。*arXiv预印本
    arXiv: 2405.19327*。'
- en: Appendix A Example Prompt for GIST with a Length of 256 Tokens
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A GIST示例提示，长度为256个令牌
- en: '| Prompt Example (256) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 提示示例（256） |'
- en: '| --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| You are an excellent grading teacher, and you will be given many completed
    questions. The requirements for these questions are as follows: |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位优秀的评分老师，你将会收到许多完成的问题。这些问题的要求如下： |'
- en: '| [In this task, you need to answer the given multiple-choice question on the
    physics. Classify your answers into ’a’, ’b’, ’c’, ’d’, and ’e’.] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [在这个任务中，你需要回答给定的物理选择题。将你的答案分类为’a’，’b’，’c’，’d’，和’e’。] |'
- en: '| Please identify all the questions that have been answered incorrectly according
    to the requirements. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 请识别所有按照要求回答错误的问题。 |'
- en: '| [0:Problem: how many seconds does sandy take to cover a distance of 700 meters
    , if sandy runs at a speed of 18 km / hr? Options: a.100 , b.120 , c.140 , d.160
    , e.180 Answer: d] |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [0:问题：如果桑迪以18公里/小时的速度跑步，桑迪跑完700米需要多少秒？选项：a.100，b.120，c.140，d.160，e.180 答案：d]
    |'
- en: '| [1:Problem: a train 300 m long takes 9 sec to cross a man walking at 3 kmph
    in a direction opposite to that of the train . find the speed of the train? Options:
    a.100 kmph , b.90 kmph , c.120 kmph , d.117 kmph , e.25 kmph Answer: d] |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [1:问题：一列300米长的火车在9秒内通过一个以3公里/小时速度走路的人的时候，火车的速度是多少？选项：a.100公里/小时，b.90公里/小时，c.120公里/小时，d.117公里/小时，e.25公里/小时
    答案：d] |'
- en: '| [2:Problem: a 300 m long train crosses a platform in 39 sec while it crosses
    a signal pole in 9 sec . what is the length of the platform ? Options: a.389m
    , b.350m , c.289m , d.799m , e.1000m Answer: e] |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| [2:问题：一列300米长的火车在39秒内通过一个站台，而通过一个信号杆需时9秒。站台的长度是多少？选项：a.389米，b.350米，c.289米，d.799米，e.1000米
    答案：e] |'
- en: '| The above text contains a description of the task for this group, as well
    as many question-answer pairs. Each enclosed in brackets. You are requested to
    identify which answer(s) are incorrect and to directly provide the question number(s)
    of the incorrect answer(s) enclosed in brackets. Please note, I need the numbers
    of the questions that are incorrect, do not provide the numbers of the questions
    with correct answers. Additionally, ensure to output the question numbers in the
    specified standard format. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 上述文本包含了该组任务的描述以及多个问题-答案对。每个都用括号括起来。请找出哪些答案是不正确的，并直接提供不正确答案的题号，用括号括起来。请注意，我需要的是错误答案的题号，不要提供正确答案的题号。此外，请确保以指定的标准格式输出题号。
    |'
- en: '| Answer: |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 答案： |'
- en: 'Table 7: An example of a complete prompt under the length of 256 tokens.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：一个长度为 256 个标记的完整提示示例。
- en: Appendix B HotMap of More Models
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更多模型的热图
- en: The 2D length-depth distribution map for more models. The horizontal axis represents
    the total length of the samples, while the vertical axis indicates the position
    of incorrect answers within the questions, where 0 denotes the beginning and 9
    denotes the end. The greener the color, the higher the model’s recognition accuracy
    at that position.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 更多模型的二维长度-深度分布图。水平轴表示样本的总长度，而垂直轴表示问题中错误答案的位置，其中 0 表示开始，9 表示结束。颜色越绿色，模型在该位置的识别准确率越高。
- en: '![[Uncaptioned image]](img/157c0c92824c515393ea666bdbaccd32.png)![[Uncaptioned
    image]](img/cecc6a273fe715a0f039f07bf00ef6d0.png)![Refer to caption](img/2c53672852cd66fede9add6015d05f6c.png)![Refer
    to caption](img/24ab9d1efe883aae8f7422e531c833a6.png)![Refer to caption](img/a4eaa6971d177bc0ea3cb9ac7bde5bc3.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![[未加标题的图像]](img/157c0c92824c515393ea666bdbaccd32.png)![[未加标题的图像]](img/cecc6a273fe715a0f039f07bf00ef6d0.png)![参见标题](img/2c53672852cd66fede9add6015d05f6c.png)![参见标题](img/24ab9d1efe883aae8f7422e531c833a6.png)![参见标题](img/a4eaa6971d177bc0ea3cb9ac7bde5bc3.png)'
- en: 'Figure 5: Expanding the Model’s Heatmap.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：扩展模型的热图。
- en: Appendix C Prompt Template
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示模板
- en: C.1 Template of GIST
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 GIST 模板
- en: '<svg id="A3.SS1.1.1.pic1" class="ltx_picture" height="254.58" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,254.58) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 237.65)"><foreignobject width="559.25"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0
    1.0 20.38 12.5)"><foreignobject width="559.25" height="208.71" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an excellent grading
    teacher, and you will be given many completed questions. The requirements for
    these questions are as follows: [The Task Description] Please identify all the
    questions that have been answered incorrectly according to the requirements. [Test
    Paper] The above text contains a description of the task for this group, as well
    as many question-answer pairs. Each enclosed in brackets. You are requested to
    identify which answer(s) are incorrect and to directly provide the question number(s)
    of the incorrect answer(s) enclosed in brackets. Please note, I need the numbers
    of the questions that are incorrect, do not provide the numbers of the questions
    with correct answers. Additionally, ensure to output the question numbers in the
    specified standard format. Answer:</foreignobject></g></g></svg>'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A3.SS1.1.1.pic1" class="ltx_picture" height="254.58" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,254.58) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 237.65)"><foreignobject width="559.25"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0
    1.0 20.38 12.5)"><foreignobject width="559.25" height="208.71" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an excellent grading
    teacher, and you will be given many completed questions. The requirements for
    these questions are as follows: [The Task Description] Please identify all the
    questions that have been answered incorrectly according to the requirements. [Test
    Paper] The above text contains a description of the task for this group, as well
    as many question-answer pairs. Each enclosed in brackets. You are requested to
    identify which answer(s) are incorrect and to directly provide the question number(s)
    of the incorrect answer(s) enclosed in brackets. Please note, I need the numbers
    of the questions that are incorrect, do not provide the numbers of the questions
    with correct answers. Additionally, ensure to output the question numbers in the
    specified standard format. Answer:</foreignobject></g></g></svg>'
- en: 'Figure 6: The prompt template used for GIST in LongIns.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LongIns 中用于 GIST 的提示模板。
- en: C.2 Template of LIST
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 LIST 模板
- en: <svg id="A3.SS2.1.1.1.p1.pic1" class="ltx_picture" height="254.58" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,254.58) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 237.65)"><foreignobject width="559.25"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0
    1.0 20.38 12.5)"><foreignobject width="559.25" height="208.71" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an excellent grading
    teacher, and you will be given many completed questions. [The Task Description,Question
    1] [The Task Description,Question 2] [The Task Description,Question 3] [The Task
    Description, ........ ] [The Task Description,Question N] The above text contains
    a description of the task for this group, as well as many question-answer pairs.
    Each enclosed in brackets. You are requested to identify which answer(s) are incorrect
    and to directly provide the question number(s) of the incorrect answer(s) enclosed
    in brackets. Please note, I need the numbers of the questions that are incorrect,
    do not provide the numbers of the questions with correct answers. Additionally,
    ensure to output the question numbers in the specified standard format. Answer:</foreignobject></g></g></svg>
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="A3.SS2.1.1.1.p1.pic1" class="ltx_picture" height="254.58" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,254.58) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 237.65)"><foreignobject width="559.25"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0
    1.0 20.38 12.5)"><foreignobject width="559.25" height="208.71" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an excellent grading
    teacher, and you will be given many completed questions. [The Task Description,Question
    1] [The Task Description,Question 2] [The Task Description,Question 3] [The Task
    Description, ........ ] [The Task Description,Question N] The above text contains
    a description of the task for this group, as well as many question-answer pairs.
    Each enclosed in brackets. You are requested to identify which answer(s) are incorrect
    and to directly provide the question number(s) of the incorrect answer(s) enclosed
    in brackets. Please note, I need the numbers of the questions that are incorrect,
    do not provide the numbers of the questions with correct answers. Additionally,
    ensure to output the question numbers in the specified standard format. Answer:</foreignobject></g></g></svg>
- en: 'Figure 7: The prompt template used for LIST in LongIns.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：LongIns 中用于 LIST 的提示模板。
