- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:01:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01370](https://ar5iv.labs.arxiv.org/html/2407.01370)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Philippe Laban^†^†thanks:   Equal contribution  Alexander R. Fabbri^($*$)  Caiming
    Xiong  Chien-Sheng Wu'
  prefs: []
  type: TYPE_NORMAL
- en: Salesforce AI Research
  prefs: []
  type: TYPE_NORMAL
- en: '{plaban afabbri cxiong wu.jason}@salesforce.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs and RAG systems are now capable of handling millions of input tokens or
    more. However, evaluating the output quality of such systems on long-context tasks
    remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this
    work, we argue that summarization can play a central role in such evaluation.
    We design a procedure to synthesize Haystacks of documents, ensuring that specific
    insights repeat across documents. The “Summary of a Haystack” (SummHay) task then
    requires a system to process the Haystack and generate, given a query, a summary
    that identifies the relevant insights and precisely cites the source documents.
    Since we have precise knowledge of what insights should appear in a haystack summary
    and what documents should be cited, we implement a highly reproducible automatic
    evaluation that can score summaries on two aspects – Coverage and Citation. We
    generate Haystacks in two domains (conversation, news), and perform a large-scale
    evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate
    that SummHay is an open challenge for current systems, as even systems provided
    with an Oracle signal of document relevance lag our estimate of human performance
    (56%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like
    GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also
    be used to study enterprise RAG systems and position bias in long-context models.
    We hope future systems can equal and surpass human performance on SummHay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary of a Haystack:'
  prefs: []
  type: TYPE_NORMAL
- en: A Challenge to Long-Context LLMs and RAG Systems
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent progress in efficient attention mechanisms has led to the expansion of
    the context length of large language models Beltagy et al. ([2020](#bib.bib4));
    Su et al. ([2024](#bib.bib49)). Previous state-of-the-art models such as T5 Raffel
    et al. ([2020](#bib.bib43)) and BART Lewis et al. ([2019](#bib.bib32)) were limited
    to input contexts of 512 or 1024 tokens, while the latest models such as Claude-3
    or Gemini-1.5-pro Reid et al. ([2024](#bib.bib45)) can process sequences of hundreds
    of thousands or millions of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Another paradigm, Retrieval Augmented Generation (RAG) Lewis et al. ([2020](#bib.bib33));
    Guu et al. ([2020](#bib.bib17)), has emerged as an alternative to these long-context
    LLMs, proposing a pipelined approach in which a retriever dynamically selects
    the context relevant to a given input query, alleviating the need for the generator
    to process long contexts directly.
  prefs: []
  type: TYPE_NORMAL
- en: Although both RAG and long-context LLMs offer to solve the common problem of
    answering queries over a large corpus of text, a direct comparison on a common
    task is still lacking, and evaluating such systems is an open challenge. Some
    recent work has popularized tests such as the Needle-in-a-Haystack task Kamradt
    ([2023](#bib.bib23)), which requires models to identify a small piece of information
    in a large document. However, these tasks do not offer the complexity needed to
    differentiate the capabilities of the latest generation of large-language models,
    with several state-of-the-art models achieving near-perfect performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we propose to leverage the task of summarization as a testbed
    for evaluating long-context models and RAG systems. Summarization requires reasoning
    over a long context and a careful understanding of the relative importance of
    content. However, most prior work on summarization evaluation, particularly in
    evaluating the relevance of summaries, has focused on single-document summarization
    or tasks in which the input content is on the order of 1,000-2,000 tokens Laban
    et al. ([2020](#bib.bib28)); Fabbri et al. ([2021a](#bib.bib12)); Bhandari et al.
    ([2020](#bib.bib5)); Liu et al. ([2022](#bib.bib37)). Apart from Chang et al.
    ([2023](#bib.bib6)), which focuses on summary coherence for 100k-token books,
    other evaluation work on longer conversational and multi-document news summarization
    is still often limited to around 10k tokens Zhong et al. ([2021](#bib.bib58));
    Huang et al. ([2023](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: A central problem is that summarization evaluation often relies on low-quality
    reference summaries and automatic metrics that do not correlate well with human
    judgments. Within reference-based evaluation, a candidate summary is compared
    to a gold-standard reference summary, with the optics that a higher overlap between
    the candidate and reference summary indicates higher quality. This paradigm may
    limit evaluation reliability, due to the lack of gold-standard references, particularly
    in long-context settings where obtaining high-quality summaries would be prohibitively
    expensive. Furthermore, automatic metrics may still fail to correlate well with
    human judgments with respect to these references; despite the human-validated
    pipeline of Huang et al. ([2023](#bib.bib20)), the best automatic metric for content
    coverage in that study has a correlation of just 0.37 with human judgment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we address these limitations through synthetic data generation.
    An overview of our Framework is found in Figure [1](#S2.F1 "Figure 1 ‣ 2 Related
    Work ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems").
    We propose data synthesis programs to generate a large corpus of documents (the
    “Haystack”) on a given topic. By enforcing that certain units of information (“insights”),
    categorized according to various subtopics, repeat within Haystack documents,
    and precisely controlling which insights occur in which documents, we can automatically
    derive the relevant insights within the Haystack for a given search query. A system
    completing the Summary of a Haystack (SummHay) task must then summarize insights
    relevant to a search query and cite the source documents of each insight. These
    summaries can be evaluated based on whether they cover the expected reference
    insights, and cite precisely and thoroughly the source documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first contribution is a procedure for generating Haystacks in two domains:
    conversations and news articles. Section [3](#S3 "3 Summary in a Haystack Framework
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") details
    the carefully-designed pipeline to ensure the feasibility and validity of the
    task. A Haystack typically contains 100 documents on a topic, totaling approximately
    100k tokens. We generate a total of 10 Haystacks, each coupled with roughly 10
    queries, for a total of 92 SummHay tasks. Our pipeline can be scaled and applied
    to other domains.'
  prefs: []
  type: TYPE_NORMAL
- en: Our second contribution develops SummHay’s evaluation protocol, centering on
    evaluating system outputs on their Coverage of reference insights, and the quality
    of their Citation. A manual annotation confirms strong reproducibility of the
    protocol among knowledgeable annotators (0.77 correlation). We then experiment
    with LLM-based evaluation, finding that although the level of correlation is slightly
    lower (0.71), evaluation cost is reduced by a factor of almost 50.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our third contribution is an estimate of human performance on SummHay and a
    large-scale evaluation of 50 RAG systems and 10 long-context LLMs. Our findings
    indicate that: (1) SummHay is challenging for all systems we evaluate, with all
    models significantly below our estimate of human performance, even when given
    oracle signals of document relevance; (2) non-trivial trade-offs exist when choosing
    between a RAG pipeline and a long-context LLM, with RAG systems typically improving
    citation quality, at the cost of insight coverage, (3) using advanced RAG components
    (e.g., Cohere’s Rerank3) leads to end-to-end performance boosts on the task, confirming
    that SummHay is a viable option for holistic RAG evaluation, (4) a positional
    bias experiment on SummHay confirms the lost in the middle phenomenon, demonstrating
    that most LLMs are biased towards information at the top or bottom of the context
    window.'
  prefs: []
  type: TYPE_NORMAL
- en: We open-source our dataset and evaluation methodology¹¹1[https://github.com/salesforce/summary-of-a-haystack](https://github.com/salesforce/summary-of-a-haystack).
    A system that achieves a high score on SummHay can reliably reason over large
    corpora of documents, detect and summarize insights, and accurately cite its sources.
    We anticipate that although our findings indicate that human performance is still
    out of reach, future systems can achieve and surpass such performance, providing
    more reliable and trustworthy answer engines.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91a0a815cdf22590fbe3d8bf4445cbfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Diagram illustrating the steps to synthesize a Haystack of documents
    given an input scenario: subtopic and insight creation followed by document generation.
    Once a Haystack is synthesized, it can be used to benchmark LLMs / RAG systems
    on query-focused summarization tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Summarization Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing work in summarization relevance, or coverage, evaluation has largely
    focused on the short-input, single-document setting Gao and Wan ([2022](#bib.bib15));
    Fabbri et al. ([2021b](#bib.bib13)). Extending to long input evaluation, recent
    work has performed meta-evaluation on coherence in book summarization Chang et al.
    ([2023](#bib.bib6)) and faithfulness across several domains Krishna et al. ([2023](#bib.bib25));
    Min et al. ([2023](#bib.bib41)); Zhang et al. ([2024a](#bib.bib56)). For coverage
    evaluation, recent work has studied content selection for book summarization Kim
    et al. ([2024](#bib.bib24)), evaluated a two-step extract-evaluate framework Wu
    et al. ([2023](#bib.bib53)), and compared the correlation of LLM metrics in coverage
    Huang et al. ([2023](#bib.bib20)). We leverage summarization relevance as a test-bed
    for long-context evaluation, and we focus on our synthetic creation pipeline and
    the simplified relevance evaluation that results in a high-correlation automatic
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Long-Context LLM Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Needle-in-a-haystack Kamradt ([2023](#bib.bib23)) was proposed to assess the
    long-context recall ability of LLMs. Subsequent work has analyzed the effect of
    needle placement Machlab and Battle ([2024](#bib.bib39)) and multi-needle LangChain
    ([2024](#bib.bib31)) and multi-modal variations Song et al. ([2024](#bib.bib48));
    Reid et al. ([2024](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, several long-context evaluation benchmarks have been created,
    for example by building upon and revising existing tasks and datasets Bai et al.
    ([2023](#bib.bib3)); An et al. ([2023](#bib.bib2)) Some work proposes ways to
    extend the context length of shorter-context datasets; Kuratov et al. ([2024](#bib.bib26));
    Kwan et al. ([2023](#bib.bib27)), while other work addresses data contamination
    in long-context settings Ni et al. ([2024](#bib.bib42)); Dong et al. ([2023](#bib.bib11)).
    Several papers introduce synthetic data in additional to existing tasks Shaham
    et al. ([2023](#bib.bib47)); Zhang et al. ([2024b](#bib.bib57)), which can prove
    to be more difficult for current models, as seen in Bai et al. ([2023](#bib.bib3)).
    Our benchmark focuses on synthetic data on the scale of 100k input tokens, and
    as opposed to existing synthetic tasks centered largely around retrieving, counting,
    or sorting, our summarization task requires aggregating and non-trivial reasoning
    over the long-context.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Attribution Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several benchmarks have emerged to study the ability of LLMs to ground their
    generation with citations Li et al. ([2023a](#bib.bib34)). AttributionBench Li
    et al. ([2024](#bib.bib36)) aggregates 7 existing attribution datasets, including
    Hagrid Kamalloo et al. ([2023](#bib.bib22)), consisting of generative answers
    to questions annotated by humans for attribution, and AttrEval-GenSearch Yue et al.
    ([2023](#bib.bib54)), which categorizes attribution into three levels of support.
    Attribution evaluation has also been performed along sources beyond documents
    such as knowledge graphs Hu et al. ([2024](#bib.bib19)); Li et al. ([2023b](#bib.bib35))
    and for tasks such as long-form question-answering Chen et al. ([2023b](#bib.bib8)).
    Specific to summarization, Seahorse Clark et al. ([2023](#bib.bib10)) collects
    annotations for summary attribution in the short-context setting. In this paper
    we study attribution, or citation, in the context of long-input summarization.
    Due to our synthetically generated data, we can trace reference insights to their
    sources and directly evaluate summary citations.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Summary in a Haystack Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/888c338caa169426aa72aa71f9332527.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example evaluation of a candidate summary (right) for its coverage
    of reference insights (left). Each reference insight is assigned a Coverage Score
    by mapping it to a single candidate bullet. A mapped bullet’s citations are used
    to calculate the Citation Score. The total score is the average across reference
    insights. See Appendix [A.7](#A1.SS7 "A.7 Additional Output Examples ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    for four additional examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems") illustrates the process of synthesizing
    Haystack data, and the task, which we now detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Preliminaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In SummHay, as in the needle-in-a-haystack task, the LLM responds to a query,
    but here it must generate a long-form answer (200-300 words) that requires identifying
    and summarizing insights that repeat across documents and citing source documents.
    The task resembles long-form question-answering Fan et al. ([2019](#bib.bib14))
    and query-focused summarization Zhong et al. ([2021](#bib.bib58)); Vig et al.
    ([2022](#bib.bib51)).
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we describe the Haystack synthesis, the steps taken
    to ensure the quality of our benchmark, and the task framing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Haystack Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Subtopic and Insight Generation (Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), left)
    One of the main motivations behind synthetically generating documents is to precisely
    control information distribution in the documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Haystack centers around a topic (e.g., “three students discuss strategies
    for an upcoming exam”). The first step generates a list of potential subtopics
    that can occur in documents about the topic. Subtopics are generic (e.g., students
    discussing study techniques, or how to manage stress). There are two subtopic
    requirements: (1) each subtopic should be distinctive and unique, such that no
    two subtopics overlap thematically, and (2) subtopics should be expandable into
    at least three distinct insights that are specific to the subtopic. Appendix [A.1](#A1.SS1
    "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems") goes over the quality assurance
    to ensure the satisfaction of these requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a second step, each subtopic gets instantiated into a list of specific insights,
    or facts that will be placed into the documents of the Haystack. Insights are
    defined as statements that contain specific information that may appear in a document
    about a given subtopic. Insights are expected to mention a number, a date, or
    an entity. For example, in the “Managing stress” subtopic, an insight can be:
    “a student explaining what the 25-5 Pomodoro technique is to the others”. Crucially,
    insights should be specific, independent of each other, and solely relevant to
    a single subtopic. Appendix [A.1.2](#A1.SS1.SSS2 "A.1.2 Insight Verification ‣
    A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems") goes over the quality assurance
    to ensure insight quality.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of breaking down documents into smaller information units has proven
    beneficial in recent work for both automatic and human evaluation Min et al. ([2023](#bib.bib41));
    Liu et al. ([2022](#bib.bib37)). Concretely, we use an LLM to generate subtopics
    and insights, optionally include context documents to provide seed ideas to the
    LLM, and aim to generate 10 subtopics, each with about 5-10 insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Document Generation (Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), center)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Haystack is synthesized one document at a time. For each document, we randomly
    select a set of insights across subtopics, and instruct an LLM to generate a document
    that must include all selected insights. The number of insights to include per
    document varies based on the domain, targeting 750 words of content per document
    (or roughly 1,000 tokens). By generating 100 documents, a Haystack totals on the
    order of 100k tokens. Appendix [A.1](#A1.SS1 "A.1 Haystack Synthesis Details ‣
    Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") details quality assurance that ensures documents are realistic
    and unique, and that each insight occurs within 5+ documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation of the SummHay task relies on precise knowledge of the mapping between
    insights and documents in the Haystack. We implement over five domain-specific
    verification processes during the synthesis of the Haystack to ensure that the
    expected mapping is sound. Manual inspection and the high performance of human
    annotators on the final task, shown in Section [5.3](#S5.SS3 "5.3 Estimating Human
    Performance ‣ 5 Results ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") provide evidence of the quality of the resulting Haystacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.3 Haystack Summarization (Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣
    Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), right)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having generated Haystacks following the above protocol, we can now leverage
    them for the Summary of a Haystack task. Using an LLM, we transform each subtopic
    into a query (e.g., “What do the students discuss regarding stress management?”).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each system completing the task is instructed to generate a summary to answer
    the query (which focuses on a single subtopic), which must be in bullet-point
    format. Crucially, we instruct the LLM on the number of bullet points that the
    summary should contain, which matches the number of insights of the subtopic.
    Although this can appear as a simplifying assumption, this important design choice
    allows us to control for the length of generated summaries, which are a known
    confounding factor in summarization evaluation Liu et al. ([2022](#bib.bib37)).
    We find in Section [5](#S5 "5 Results ‣ Summary of a Haystack: A Challenge to
    Long-Context LLMs and RAG Systems") that this choice effectively controls the
    length of generated summaries. The prompt also instructs the system to cite source
    documents in each of its bullet points, in a specific bracketed format (e.g.,
    [1,2]), using document identifiers provided in the Haystack. The bullet-point
    structure and specific citation format are the foundation for our evaluation,
    detailed in Section [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack: A
    Challenge to Long-Context LLMs and RAG Systems").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 SummHay Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We instantiate the above protocol across two domains: conversations and news,
    as these two domains are common test beds for summarization Hermann et al. ([2015](#bib.bib18));
    Gliwa et al. ([2019](#bib.bib16)). For each domain, we generate 5 Haystacks, and
    the average Haystack length is 93k tokens. Each Haystack consists of on average
    9.20 subtopics, each averaging 6.75 insights, for a total of 62 insights per topic.
    For the news domain, we leverage the documents from Huang et al. ([2023](#bib.bib20))
    as the seed context documents. Regarding LLM choice, we rely on a combination
    of GPT-3.5 and GPT-4o and specify additional details in Appendix [A.1](#A1.SS1
    "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Cov. Corr. | Link Acc. | Cost ($) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Manual Annot. | 0.770 | 95.0 | $325 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-pro | 0.751 | 89.3 | $15.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o (9FS) | 0.719 | 89.2 | $26.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 0.716 | 88.9 | $6.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | 0.677 | 87.9 | $23.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku | 0.498 | 87.7 | $0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5 | 0.495 | 86.7 | $1.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Reproducibility and cost of manual and automated evaluation for SummHay.
    We compute coverage correlation, linking accuracy, and evaluation cost.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first define task metrics (illustrated in Figure [2](#S3.F2 "Figure 2 ‣
    3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems")), establish the reproducibility of manual annotation, and
    then assess the quality of automated evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coverage Metric Given a candidate subtopic summary, we extract each bullet
    point (split on line breaks) and want to measure the overlap between these bullets
    and the subtopic’s reference insights. To do so, we iterate over each reference
    insight and assess whether the reference insight is fully, partially, or not covered
    in any of the candidate bullet points. For each insight, the summary receives
    a score of 100 for full coverage, 50 for partial coverage, and 0 otherwise. The
    final coverage score of a summary is the average coverage on all the insights
    of the subtopic, such that it ranges from 0 to 100\. In Figure [2](#S3.F2 "Figure
    2 ‣ 3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge to
    Long-Context LLMs and RAG Systems"), the top reference insight is fully covered
    by the second candidate bullet, the second insight is partially covered by the
    first candidate bullet, and the third insight is not covered in the summary. The
    Coverage Score is: $(100+50+0)/3=50$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Citation Metric Because documents of the Haystack are synthesized, each reference
    insight can be traced to a gold-standard set of documents that contain the insight.
    When a summary’s bullet covers a reference insight, we can compare generated citations
    to this reference set of cites. For each partially or fully covered reference
    insight, cited documents are extracted from the paired summary bullet point using
    a regular expression ([.*]), and we measure the precision and recall between the
    generated and gold-standard cites. The Citation Score of a reference insight is
    calculated as the F1 score of the precision and recall, giving equal weight to
    both. In short, a system must be both precise and thorough in its citing to achieve
    a high Citation Score. The Citation Score of an entire summary is then the average
    insight Citation Score of all reference insights that were covered by the system.
    In Figure 2, the average Citation F1 of the two covered bullets is: $(29+73)/2=51$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Joint Metric The Joint Metric pieces Coverage and Citation Scores together,
    measuring whether a candidate summary both covers the expected insights and cites
    documents appropriately. The Joint Score of a summary is calculated by iterating
    over each reference insight and multiplying its coverage score and citation scores
    (assigning a Citation Score of 0 in case the insight is not covered). The Joint
    Score of a summary ranges from 0 to 100\. In Figure [2](#S3.F2 "Figure 2 ‣ 3 Summary
    in a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems"), the summary’s Joint Score is: $(100*0.29+50*0.73+0*0)/3=21.8$.
    Appendix [A.7](#A1.SS7 "A.7 Additional Output Examples ‣ Appendix A Appendix ‣
    Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") provides
    four additional examples on the same subtopic with added details.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   width 10pt | Coverage Score ($\uparrow$) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Summarizer  width 10pt | Rand | Vect | LongE | KWs | RR3 | Orac   width 5pt
    | Full   width 10pt | Rand | Vect | LongE | KWs | RR3 | Orac   width 5pt | Full
      width 10pt | Rand | Vect | LongE | Kws | RR3 | Orac   width 5pt | Full | #$W_{b}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5  width 10pt | 36.2 | 45.8 | 46.0 | 48.4 | 51.9 | 56.2   width 5pt
    | –   width 10pt | 9.3 | 15.2 | 15.0 | 15.9 | 16.8 | 23.0   width 5pt | –   width
    10pt | 3.6 | 7.3 | 7.2 | 7.9 | 9.0 | 13.2   width 5pt | – | 28.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku  width 10pt | 49.9 | 64.9 | 62.3 | 63.4 | 66.6 | 72.1   width
    5pt | 62.3   width 10pt | 13.4 | 25.1 | 25.5 | 26.5 | 28.8 | 35.6   width 5pt
    | 14.1   width 10pt | 7.1 | 17.4 | 17.2 | 17.7 | 20.1 | 26.8   width 5pt | 9.2
    | 31.9 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo  width 10pt | 49.4 | 61.0 | 56.7 | 61.2 | 61.8 | 67.1   width
    5pt | 57.9   width 10pt | 17.9 | 28.6 | 28.1 | 31.1 | 31.8 | 41.4   width 5pt
    | 5.5   width 10pt | 9.6 | 18.7 | 16.9 | 20.1 | 20.6 | 28.9   width 5pt | 3.2
    | 37.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Command-r  width 10pt | 47.0 | 54.8 | 53.5 | 56.0 | 55.2 | 60.4   width 5pt
    | 50.3   width 10pt | 17.7 | 34.6 | 34.9 | 37.5 | 40.4 | 53.8   width 5pt | 30.9
      width 10pt | 8.9 | 19.6 | 19.6 | 21.9 | 23.6 | 33.9   width 5pt | 16.2 | 33.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-flash  width 10pt | 49.7 | 58.1 | 58.9 | 61.8 | 62.6 | 65.1   width
    5pt | 59.4   width 10pt | 17.4 | 31.9 | 31.8 | 34.2 | 43.6 | 51.7   width 5pt
    | 32.8   width 10pt | 9.2 | 19.4 | 20.0 | 22.0 | 28.7 | 34.9   width 5pt | 21.0
    | 31.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Command-r +  width 10pt | 44.2 | 56.4 | 53.1 | 56.2 | 58.9 | 61.0   width
    5pt | 44.5   width 10pt | 20.4 | 41.7 | 41.7 | 43.1 | 46.8 | 60.2   width 5pt
    | 19.9   width 10pt | 9.6 | 24.7 | 24.0 | 25.7 | 29.3 | 38.3   width 5pt | 9.7
    | 25.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Sonnet  width 10pt | 55.8 | 70.6 | 69.7 | 72.1 | 73.1 | 77.7   width
    5pt | 73.6   width 10pt | 18.0 | 34.9 | 36.6 | 37.3 | 41.1 | 51.7   width 5pt
    | 23.5   width 10pt | 11.0 | 26.1 | 27.2 | 28.5 | 31.4 | 41.2   width 5pt | 18.3
    | 33.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus  width 10pt | 56.5 | 72.4 | 69.6 | 72.5 | 76.5 | 81.4   width
    5pt | 76.2   width 10pt | 17.7 | 34.3 | 35.8 | 37.3 | 39.4 | 50.7   width 5pt
    | 22.3   width 10pt | 11.1 | 26.5 | 26.7 | 28.6 | 31.9 | 42.5   width 5pt | 18.0
    | 29.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o  width 10pt | 54.0 | 67.1 | 67.8 | 66.6 | 70.4 | 76.6   width 5pt
    | 66.1   width 10pt | 21.9 | 38.4 | 38.0 | 38.6 | 41.3 | 54.6   width 5pt | 16.2
      width 10pt | 12.6 | 27.3 | 27.6 | 27.3 | 30.8 | 43.4   width 5pt | 11.4 | 36.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-pro  width 10pt | 53.0 | 63.5 | 64.9 | 63.6 | 68.4 | 67.6   width
    5pt | 70.0   width 10pt | 21.9 | 43.1 | 44.5 | 46.6 | 49.7 | 64.1   width 5pt
    | 51.0   width 10pt | 12.3 | 28.6 | 31.0 | 30.8 | 36.0 | 44.6   width 5pt | 37.8
    | 30.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Human Perf.  width 10pt | – | – | – | – | – | 74.5   width 5pt | –   width
    10pt | – | – | – | – | – | 73.9   width 5pt | –   width 10pt | – | – | – | – |
    – | 56.1   width 5pt | – | 29.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Summary of a Haystack results of human performance, RAG systems, and
    Long-Context LLMs. Results are reported using three metrics: Coverage (left),
    Citation (center), and Joint (right) scores. Full corresponds to model performance
    when inputting the entire Haystack, whereas Rand, Vect, LongE, KWs, RR3, Orac
    correspond to retrieval components RAG systems. Models ranked by Oracle Joint
    Score. For each model, #$W_{b}$ report the average number of words per bullet
    point.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Annotation Reproducibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To establish the reproducibility of the evaluation protocol, two authors of
    the paper and two professional annotators independently annotated a common subset
    of 35 summaries, annotating for the coverage of 240 insights within the summaries.
    The Manual Annotation row of Table [1](#S3.T1 "Table 1 ‣ 3.4 SummHay Benchmark
    ‣ 3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") reports the inter-annotator agreement levels on the 240
    coverage judgments. Coverage Score averaged a correlation of 0.77 across pairs
    of annotators, indicating a strong level of agreement between participants. When
    annotators agree that a reference insight is covered, they agree on which candidate
    bullet originates the coverage in 95% of cases (Linking Accuracy). In short, annotators
    have strong agreement on which reference insights are covered by a given candidate
    summary and also agree on the specific bullets in the candidate summary that cover
    each insight.'
  prefs: []
  type: TYPE_NORMAL
- en: Annotating a single summary takes 4 minutes on average. To reduce evaluation
    costs and scale experiments, we next investigate LLM-based evaluation as an alternative
    to annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Automatic Metric Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recruited two professional annotators to annotate 200 candidate summaries
    (100 for each of the news and conversational domains) paired with reference insights.
    Annotators were compensated at $25/hour, and annotation required a total of 13
    hours of work, for a total of $325.
  prefs: []
  type: TYPE_NORMAL
- en: 'We prepared a prompt that contains task instructions and an example summary
    which has a fully covered, a partially covered, and an uncovered insight (see
    Appendix [A.2](#A1.SS2 "A.2 Evaluation Prompt ‣ Appendix A Appendix ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")). We evaluated
    5 LLMs as evaluators: GPT3.5, Claude 3 Haiku, Opus, GPT-4o, and Gemini-1.5-pro.
    In Table [1](#S3.T1 "Table 1 ‣ 3.4 SummHay Benchmark ‣ 3 Summary in a Haystack
    Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"),
    we report evaluator performance in terms of correlation on the insight-level coverage
    scores, linking accuracy, which measures whether LLMs can attribute the coverage
    to the correct bullet point, and the cost of evaluating the 200 summaries. We
    find that two models, GPT-4o and Gemini-1.5-pro achieve a strong positive correlation
    (0.7+) with the human annotation. We select the GPT-4o model as our evaluator,
    as it achieves high evaluation correlation at a fraction of the cost of Gemini-1.5-pro,
    and does not have strict rate limits in place. We attempt one improvement by preparing
    a prompt with 9 few-shot examples (three of fully, partially, and uncovered insights
    each), and report the result as GPT-4o (9FS). Although the increased number of
    examples does lead to minor correlation improvements, it comes at a large cost
    increase, and thus we finalize the auto-evaluation setting as using the original
    prompt and the GPT-4o evaluator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Appendix [A.3](#A1.SS3 "A.3 Automatic Results Bias ‣ Appendix A Appendix
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), we
    assess whether automatic evaluation using GPT-4o could cause two types of biases:
    first, whether it could systematically favor or disfavor summaries generated by
    a family of models (such as GPTs), and second whether it could be partial to summaries
    of a certain length. We find no sign of systematic bias in the LLM-based evaluation
    in the case of our protocol.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As illustrated in Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary of
    a Haystack: A Challenge to Long-Context LLMs and RAG Systems") (right), we evaluate
    both long-context LLMs that directly access the full Haystack and RAG systems
    where a retriever filters the Haystack down to documents it perceives as relevant,
    which get passed to a generator (LLM). By default, documents in the Haystack are
    ordered in a single arbitrary order.'
  prefs: []
  type: TYPE_NORMAL
- en: Full-Context Summarization We test a range of recent LLMs with context lengths
    longer than an individual Haystack, including Cohere’s Command-R and Command-R+,
    Google’s Gemini-1.5-pro and Gemini-1.5-flash Reid et al. ([2024](#bib.bib45)),
    OpenAI’s GPT4-turbo and GPT-4o, and Anthropic’s Claude3 models (haiku, sonnet,
    and opus). We also include GPT-3.5 exclusively in the RAG setting, as its context
    length is 16k tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented Summarization We evaluate RAG systems to reduce the Haystack
    input size. All retrieval models receive the query and all Haystack documents
    and must produce a query relevance score for each document. We sort the documents
    in reverse order according to the query relevance score and select the first 15k
    worth of document tokens. We chose 15k enables us to experiment with generators
    that have a 16k context window (GPT-3.5-turbo). We experiment with a total of
    six retrievers, each implemented as a separate query relevance score function.
    Under KWs, the document score is the number of overlapping keywords, extracted
    by NLTK, between the document and the subtopic query. We compare embedding methods
    that compute the cosine similarity between each document and the subtopic query,
    Vect, a SentenceTransformers Reimers and Gurevych ([2019](#bib.bib46)) embedder
    and LongE Zhu et al. ([2024](#bib.bib59)), which extends standard embedders to
    cover longer input contexts, and include the Rerank3 (RR3) model from Cohere Inc.
    ([2024](#bib.bib21)). We also include a Rand baseline that randomly assigns relevance
    scores, and an oracle setting ranker Orac, whose score is the number of subtopic
    insights that appear in a given document. The last two provide lower- and upper-bound
    retrieval quality estimates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b4fd2c52b6829c733891bd5c9a46a58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Estimates of human performance on the SummHay task, plotted over
    time as participants complete the task in the Oracle setting during two-hour sessions.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Benchmark Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") summarizes
    the SummHay results across long-context, in the Full column, and RAG settings
    for all 10 Summarizers included in our study.'
  prefs: []
  type: TYPE_NORMAL
- en: Coverage scores – which measure the presence of expected insights in a system’s
    output summary – range from 36.2% when using a random retriever and GPT3.5-turbo
    as the summarizer, to 81.4% using the Oracle retriever with the Claude 3 Opus
    summarizer. The choice of retriever impacts the Coverage score, with Random and
    Oracle retrievers leading to the best and worst scores, respectively, for almost
    all summarizers. Yet, top-performing LLMs like Claude3-opus achieve strong Coverage
    scores (70%+) under most retrieval settings, including Full context. In other
    words, achieving strong coverage of key insights in a large corpus of text does
    not require retrieval, given a sufficiently capable long-context LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Citation scores – which account for both the precision and the thoroughness
    of the model’s attribution back to source documents – present a complementary
    narrative. The lowest citation score often occurs in the full-context setting,
    with citation quality on par with random retrieval. On the other hand, as retrieval
    improves (for example from Random to RR3 to Oracle), citation scores increase.
    In a nutshell, for use-cases where citation quality is important, optimizing retrieval
    is paramount: it removes irrelevant documents from the summarizer’s context, narrowing
    and focusing options for citation. Gemini-1.5-pro stands out as an outlier, as
    it is the only model that achieves comparable Citation scores in RAG and long-context
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Taking Coverage and Citation into account, the Joint Score provides the complete
    system performance on SummHay. As expected, all Summarizers perform best with
    the Oracle retriever, an unrealistic setting intended to evaluate score ranges.
  prefs: []
  type: TYPE_NORMAL
- en: All models except for Gemini-1.5-pro achieve their realistic best performance
    in the RAG setting using the RR3 retriever. The higher relative performance of
    the more advanced RAG retriever RR3, developed for enterprise search and RAG settings,
    aligns with expectations compared to simpler retrievers. This result confirms
    the validity of our SummHay as a test-bed for holistic RAG evaluation; newer,
    more advanced RAGs can be benchmarked in an end-to-end fashion on SummHay, measuring
    direct impact on output quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We still observe two large gaps: all RAG systems underperform the Oracle setting,
    indicating ample room for improvements in RAG systems, and models achieve very
    low Joint scores in the full-context setting (10-20), indicating that SummHay
    is an unsolved task for long-context LLMs. Gemini-1.5-pro stands with strong ability
    to cite in the full-context setting, achieving the only realistic score above
    35 on the benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Rerank3 RAG setting, the top three models are neck and neck, with Claude3
    Opus, GPT-4o, and Gemini-1.5-pro all achieving Joint Scores between 30.8 and 36.0\.
    Yet these models achieve these performances through different trade-offs, with
    Claude3 Opus obtaining the highest Coverage, Gemini-1.5-pro the highest Citation,
    and GPT-4o at the mid-point. This confirms there is room to grow: a system that
    achieves the coverage of Claude3 Opus with the citation quality of Gemini-1.5-pro
    can exceed current score by 15-20%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-most column of Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics
    ‣ 4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") shows each system’s average bullet point length (number of tokens).
    Several systems (Gemini-1.5-pro and Claude 3 Opus) average 30 words per bullet,
    close human-written bullet points (29.7). Others (GPT-4o, GPT4-turbo) are more
    verbose, at 36-38 words per bullet point. In Appendix [A.3](#A1.SS3 "A.3 Automatic
    Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") we confirm that verbosity does not bias evaluation: succinct
    methods can achieve strong performance on SummHay.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix [A.5](#A1.SS5 "A.5 Citation Precision & Recall Analysis ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    breaks down Citation Scores into Precision and Recall. No system excels at either
    precision or recall but we do observe trade-offs. For example, Claude models generally
    achieve higher precision and lower recall, whereas Command-r + and GPT-4o favor
    recall over precision.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Estimating Human Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We estimate human performance on the task by recruiting two annotators to perform
    the task. We first define the setting in which annotation was conducted, then
    go over the results.
  prefs: []
  type: TYPE_NORMAL
- en: The participants performed the task in the Oracle setting, only viewing documents
    relevant to the query they are currently summarizing, as it reduces the volume
    of text they must read from about 100,000 tokens to 15,000 tokens. We assume this
    effectively reduces the amount of time required for annotation by a factor of
    5-6, but this remains unverified, as it is impractical to conduct human annotation
    on the full Haystack.
  prefs: []
  type: TYPE_NORMAL
- en: In total, two annotators participated in writing a total of 10 summaries, five
    for subtopics in the conversational domain, and five for subtopics in the news
    domain. Although this represents a subset of the 92 subtopics in the entire SummHay
    benchmark, we believe it represents an unbiased estimate of human performance
    on the benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S5.F3 "Figure 3 ‣ 5.1 Experimental Settings ‣ 5 Results ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") aggregates results
    across the ten annotation sessions. Overall, participants make steady progress
    during the sessions, with both Coverage and Citation rising rapidly in the first
    90 minutes, and then at a slower pace in the last 30 minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: The Citation Score corresponds to an F1 measure, and we also report on the Precision
    and Recalls of the Citations. We find that citation precision averages close to
    80.0 throughout the session, whereas recall rises steadily during the session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") includes
    the summarized final scores in contrast to system scores, showing that a human
    annotator can significantly outperform LLMs and RAG systems on the SummHay task,
    as the human joint score (56.1) is significantly higher than the best system performance
    (44.6). We caution the reader not to consider our estimate of human performance
    as an upper bound, as we believe that with more time and explicit instructions
    to double-check their work, annotators could further increase their scores. We
    solely intend the human performance to be a reference point for achievable performances
    on the benchmark, and we expect future systems to tie and surpass human performance
    on the task. Appendix [A.4](#A1.SS4 "A.4 Details on Establishing SummHay Human
    Performance ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") provides further detail on guidelines, recruitment, and
    task framing.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Document Order |  |'
  prefs: []
  type: TYPE_TB
- en: '| Summarizer | Top | Bottom | Random | Sensitivity |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 13.8 | 24.1 | 11.4 | 12.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude3 Opus | 20.4 | 28.0 | 18.0 | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-pro | 47.1 | 38.9 | 37.9 | 9.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Joint Scores of LLMs in the Full Context Setting, based on how documents
    are sorted. Documents can be in Random order or sorted such that relevant ones
    are at the Top or Bottom of the context window.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Position Bias Sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the Full Context experiment results (Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation
    Metrics ‣ 4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems"), Full columns), documents in the Haystack are ordered arbitrarily,
    with relevant documents in the top, middle, and bottom portions of the context
    window. Prior work Huang et al. ([2023](#bib.bib20)); Chang et al. ([2023](#bib.bib6));
    Chen et al. ([2023b](#bib.bib8)); Ravaut et al. ([2023](#bib.bib44)) has reported
    that models exhibit a position bias that leads them to put more importance on
    information in the context window’s extremities. SummHay offers a framework to
    study position bias systematically. In Table [3](#S5.T3 "Table 3 ‣ 5.3 Estimating
    Human Performance ‣ 5 Results ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems"), we report the results of the Position Bias experiment,
    in which we run the SummHay experiment with the top-3 performing models on sorted
    Haystacks, where relevant documents to a subtopic are either all at the Top or
    Bottom of the context window. Similar to prior work, we find that all three models
    exhibit position bias, with GPT-4o and Claude3 Opus performing better when relevant
    documents are at the bottom of the context window, and Gemini-1.5-pro favoring
    Top Haystacks. We compute a Position Sensitivity score as the maximum absolute
    difference in Joint Score between the Random ordering and Top and Bottom conditions.
    Future systems should strive to attain minimal sensitivity on SummHay, as document
    ordering is often arbitrary in real-world applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Task Upper Bound
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Section [3.2](#S3.SS2 "3.2 Haystack Generation ‣ 3 Summary in a Haystack
    Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    and Appendix [A.1](#A1.SS1 "A.1 Haystack Synthesis Details ‣ Appendix A Appendix
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), we
    detail our data pipeline and efforts to ensure the quality of our dataset. This
    includes insight subtopic verification and verifying the inclusion of only specified
    insights for each document. Despite our efforts to prevent overlap among insights
    and guarantee the presence of insights in the documents generated, errors may
    be introduced from using LLMs in scaling this data synthesis, making achieving
    a perfect joint score of 100 likely unachievable. Although we estimate human performance
    in a simplified setting, we do not determine the task upper bound. However, we
    show significant room for improvement between the realistic full-context and RAG
    settings and the Oracle setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying Assumptions in Data Synthesis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The assumptions made when generating Haystack documents likely introduce artificial
    signals that simplify the task. For example, in order to maximize control over
    the data synthesis process, Haystack documents are generated independently; no
    dependencies or cross-references exist among the documents. However, in a real-world
    multi-document summarization task, documents may link or refer to each other,
    and there may be temporal between documents. We believe the introduction of more
    realistic assumptions can further increase the difficulty of the task, and we
    hope that future work will take our synthesis processes as a starting point for
    such improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling for Verbosity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When generating summaries, we specify the desired number of insights for the
    LLM to generate. Furthermore, we do not control for or penalize the verbosity
    of the summaries, and summaries with longer insights may result in higher coverage.
    Not specifying the number of summary insights needed per query will result in
    a more difficult task, and we leave a study of the potential trade-offs between
    verbosity, human preference, and overall scores for future work.
  prefs: []
  type: TYPE_NORMAL
- en: Reliance on Automated Evaluation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Although we do not observe significant bias towards a particular family of
    models, as shown in Appendix [A.3](#A1.SS3 "A.3 Automatic Results Bias ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"),
    the results in Table [1](#S3.T1 "Table 1 ‣ 3.4 SummHay Benchmark ‣ 3 Summary in
    a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") demonstrate that there is room for improvement both in coverage
    and linking evaluation. Gemini-1.5-pro, in addition to being more costly than
    GPT-4o, had a rate limit which inhibited its use in our study. Although highly-cost
    effective, non-LLM based NLI and relevance metrics Chen and Eger ([2023](#bib.bib9));
    Liu et al. ([2023](#bib.bib38)) were not tested; Chen et al. ([2023b](#bib.bib8))
    found worse performance among smaller NLI models for the related task of unrelated
    sentence identification on long-form question answering.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Choice
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The generation models included in our study are all closed-source models. Although
    these closed-source models currently generally outperform open-sourced models,
    this performance comparison can be task-dependent Chen et al. ([2023a](#bib.bib7)).
    We exclude high-performing open-sourced models such as Llama-3 AI@Meta ([2024](#bib.bib1))
    as the original models cannot handle the minimal 16k context window necessary
    for our RAG experiments. Restricting the length of the retrieved documents to,
    for example, 8k would remove too many of the insights for a given subtopic; by
    allowing up to 15k tokens in the RAG setting, we find that the oracle citation
    F1 achievable with this context length is 0.84 averaged across insights, which
    we believe strikes a balance between the reduction of input size and the feasibility
    of the task. We leave an analysis of RAG systems across retrieved input lengths,
    as well as models specifically designed for output citation Menick et al. ([2022](#bib.bib40))
    for future work and encourage and benchmarking of longer-context, open-sourced
    models on SummHay.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we address the challenges of evaluating long-context LLMs and
    RAG systems by introducing the SummHay benchmark task, synthesized to assess the
    ability of systems to precisely summarize large sets of documents. The SummHay
    task requires generating summaries that accurately cover and cite insights relevant
    to a particular query. Our comprehensive evaluation reveals that current models
    struggle with this task; even in an oracle document setting, models lag behind
    human performance by more than 10 points. We believe that SummHay provides a robust
    framework for evaluating long-context systems and will encourage researchers to
    utilize SummHay to drive progress toward systems that can match or surpass human
    performance in long-context summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The models and datasets utilized in the project primarily reflect the culture
    of the English-speaking populace. Gender, age, race, and other socio-economic
    biases may exist in the data, and models trained on these datasets may propagate
    these biases. Text generation tasks such as summarization have previously been
    shown to contain these biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems") and Section [5](#S5 "5 Results ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), we recruited
    professional annotators to perform evaluation, or directly attempt the task. We
    ensured to remunerate the participants fairly ($25/hour). Participants could communicate
    with us to voice concerns, work at their own pace, and choose to stop working
    on the project at any time. Finally, we ensured to anonymize the annotations (annotator
    identity is instead marked as annotator1, annotator2, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: In our work, we relied on several datasets as well as pre-trained language models.
    We explicitly verified that all datasets and models are publicly released for
    research purposes and that we have proper permission to reuse and modify the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhandari et al. (2020) Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei
    Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2023) Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2023.
    Booookscore: A systematic exploration of book-length summarization in the era
    of llms. *arXiv preprint arXiv:2310.00785*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023a) Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu
    Ravaut, Ruochen Zhao, Caiming Xiong, and Shafiq Joty. 2023a. Chatgpt’s one-year
    anniversary: Are open-source large language models catching up? *arXiv preprint
    arXiv:2311.16989*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi.
    2023b. Understanding retrieval augmentation for long-form question answering.
    *arXiv preprint arXiv:2310.12150*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen and Eger (2023) Yanran Chen and Steffen Eger. 2023. Menli: Robust evaluation
    metrics from natural language inference. *Transactions of the Association for
    Computational Linguistics*, 11:804–825.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2023) Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua
    Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan
    Das, and Ankur P Parikh. 2023. Seahorse: A multilingual, multifaceted dataset
    for summarization evaluation. *arXiv preprint arXiv:2305.13194*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2023) Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong
    Wen. 2023. Bamboo: A comprehensive benchmark for evaluating long text modeling
    capacities of large language models. *arXiv preprint arXiv:2309.13345*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. (2021a) Alexander R Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021a. Summeval: Re-evaluating
    summarization evaluation. *Transactions of the Association for Computational Linguistics*,
    9:391–409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. (2021b) Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021b. [SummEval: Re-evaluating
    summarization evaluation](https://doi.org/10.1162/tacl_a_00373). *Transactions
    of the Association for Computational Linguistics*, 9:391–409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier,
    Jason Weston, and Michael Auli. 2019. [ELI5: Long form question answering](https://doi.org/10.18653/v1/P19-1346).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3558–3567, Florence, Italy. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao and Wan (2022) Mingqi Gao and Xiaojun Wan. 2022. [DialSummEval: Revisiting
    summarization evaluation for dialogues](https://doi.org/10.18653/v1/2022.naacl-main.418).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5693–5709,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander
    Wawer. 2019. [SAMSum corpus: A human-annotated dialogue dataset for abstractive
    summarization](https://doi.org/10.18653/v1/D19-5409). In *Proceedings of the 2nd
    Workshop on New Frontiers in Summarization*, pages 70–79, Hong Kong, China. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pages 3929–3938\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermann et al. (2015) Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. [Teaching
    machines to read and comprehend](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend).
    In *NIPS*, pages 1693–1701.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2024) Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong
    Wu, and Jeff Z Pan. 2024. Benchmarking large language models in complex question
    answering attribution using knowledge graphs. *arXiv preprint arXiv:2401.14640*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Kung-Hsiang Huang, Philippe Laban, Alexander R Fabbri,
    Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023.
    Embrace divergence for richer insights: A multi-document summarization benchmark
    and a case study on summarizing diverse information from news articles. *arXiv
    preprint arXiv:2309.09369*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inc. (2024) Cohere Inc. 2024. [Introducing rerank 3: The next generation of
    search relevance](https://cohere.com/blog/rerank-3). Accessed: 2024-06-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamalloo et al. (2023) Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur,
    and Jimmy Lin. 2023. Hagrid: A human-llm collaborative dataset for generative
    information-seeking with attribution. *arXiv preprint arXiv:2307.16883*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamradt (2023) Gregory Kamradt. 2023. [Needleinahaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2024) Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella,
    Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. Fables: Evaluating
    faithfulness and content selection in book-length summarization. *arXiv preprint
    arXiv:2404.01261*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishna et al. (2023) Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer,
    Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. Longeval: Guidelines for human
    evaluation of faithfulness in long-form summarization. *arXiv preprint arXiv:2301.13298*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuratov et al. (2024) Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin,
    Artyom Sorokin, and Mikhail Burtsev. 2024. In search of needles in a 10m haystack:
    Recurrent memory finds what llms miss. *arXiv preprint arXiv:2402.10790*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwan et al. (2023) Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou
    Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong. 2023. M4le: A multi-ability multi-range
    multi-task multi-domain long-context evaluation benchmark for large language models.
    *arXiv preprint arXiv:2310.19240*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laban et al. (2020) Philippe Laban, Andrew Hsi, John Canny, and Marti A Hearst.
    2020. The summary loop: Learning to write abstractive summaries without examples.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5135–5150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laban et al. (2022a) Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A
    Hearst. 2022a. Summac: Re-visiting nli-based models for inconsistency detection
    in summarization. *Transactions of the Association for Computational Linguistics*,
    10:163–177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laban et al. (2022b) Philippe Laban, Chien-Sheng Wu, Lidiya Murakhovs’ka, Xiang
    Chen, and Caiming Xiong. 2022b. Discord questions: A computational approach to
    diversity analysis in news coverage. In *Findings of the Association for Computational
    Linguistics: EMNLP 2022*, pages 5180–5194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain (2024) LangChain. 2024. [Multi-needle in a haystack](https://blog.langchain.dev/multi-needle-in-a-haystack/?ref=dailydev).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen,
    Baotian Hu, Aiguo Wu, and Min Zhang. 2023a. A survey of large language models
    attribution. *arXiv preprint arXiv:2311.03731*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Xinze Li, Yixin Cao, Liangming Pan, Yubo Ma, and Aixin Sun.
    2023b. Towards verifiable generation: A benchmark for knowledge-aware language
    model attribution. *arXiv preprint arXiv:2310.05634*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Yifei Li, Xiang Yue, Zeyi Liao, and Huan Sun. 2024. Attributionbench:
    How hard is automatic attribution evaluation? *arXiv preprint arXiv:2402.15089*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong
    Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, et al.
    2022. Revisiting the gold standard: Grounding summarization evaluation with robust
    human evaluation. *arXiv preprint arXiv:2212.07981*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Yixin Liu, Alexander R Fabbri, Yilun Zhao, Pengfei Liu, Shafiq
    Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023. Towards interpretable
    and efficient automatic reference-based summarization evaluation. *arXiv preprint
    arXiv:2303.03608*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machlab and Battle (2024) Daniel Machlab and Rick Battle. 2024. Llm in-context
    recall is prompt dependent. *arXiv preprint arXiv:2404.08865*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menick et al. (2022) Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, et al. 2022. Teaching language models to support answers with
    verified quotes. *arXiv preprint arXiv:2203.11147*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau
    Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
    Factscore: Fine-grained atomic evaluation of factual precision in long form text
    generation. *arXiv preprint arXiv:2305.14251*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ni et al. (2024) Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei
    Yin, and Piji Li. 2024. Xl bench: A benchmark for extremely long context understanding
    with long-range dependencies. *arXiv preprint arXiv:2404.05446*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravaut et al. (2023) Mathieu Ravaut, Shafiq Joty, Aixin Sun, and Nancy F Chen.
    2023. On context utilization in summarization with large language models. *arXiv
    e-prints*, pages arXiv–2310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-bert:
    Sentence embeddings using siamese bert-networks](http://arxiv.org/abs/1908.10084).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing*. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding.
    *arXiv preprint arXiv:2305.14196*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2024) Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu,
    Xiang Wan, and Benyou Wang. 2024. Milebench: Benchmarking mllms in long context.
    *arXiv preprint arXiv:2404.18532*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2024) Liyan Tang, Philippe Laban, and Greg Durrett. 2024. Minicheck:
    Efficient fact-checking of llms on grounding documents. *arXiv preprint arXiv:2404.10774*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vig et al. (2022) Jesse Vig, Alexander Fabbri, Wojciech Kryscinski, Chien-Sheng
    Wu, and Wenhao Liu. 2022. [Exploring neural models for query-focused summarization](https://doi.org/10.18653/v1/2022.findings-naacl.109).
    In *Findings of the Association for Computational Linguistics: NAACL 2022*, pages
    1455–1468, Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023) Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and
    Estevam Hruschka. 2023. Less is more for long document summary evaluation by llms.
    *arXiv preprint arXiv:2309.07382*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue et al. (2023) Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan
    Sun. 2023. Automatic evaluation of attribution by large language models. *arXiv
    preprint arXiv:2305.06311*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zha et al. (2023) Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.
    Alignscore: Evaluating factual consistency with a unified alignment function.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 11328–11348.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024a) Huajian Zhang, Yumo Xu, and Laura Perez-Beltrachini. 2024a.
    Fine-grained natural language inference based faithfulness evaluation for diverse
    summarisation tasks. *arXiv preprint arXiv:2402.17630*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024b) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. 2024b.
    Infinitybench: Extending long context evaluation beyond 100k tokens. *arXiv preprint
    arXiv:2402.13718*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma,
    Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and
    Dragomir Radev. 2021. [QMSum: A new benchmark for query-based multi-domain meeting
    summarization](https://doi.org/10.18653/v1/2021.naacl-main.472). In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 5905–5921, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2024) Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu
    Wei, and Sujian Li. 2024. Longembed: Extending embedding models for long context
    retrieval. *arXiv preprint arXiv:2404.12096*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Haystack Synthesis Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Below we specify additional details regarding our data synthesis pipelines.
    For the news domain, we leverage GPT-4o for all data synthesis steps. We found
    it necessary to leverage this high-performing LLM due to the longer seed context
    documents that subtopics and insights are generated from. For the conversation
    domain, we leverage GPT-4o to generate subtopics and insights, and for any LLM-based
    verification step. Conversation generation (conditioned on selected insights)
    is completed using GPT-3.5-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we employ verification steps to ensure that subtopics and insights
    are distinct and precisely mapped to Haystack documents. When generating documents
    given a set of insights, we do not want other insights to “leak” into the document,
    as that would reduce the quality of the Haystack and task. Below we list the verification
    steps taken across the conversation and news domain. Differences in domain characteristics
    and the seed used to generate Haystacks necessitate per-domain verification steps.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.1 Subtopic Verification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the news domain, to ensure distinct insights and subtopics we first prompt
    an LLM to identify any overlapping or duplicate subtopics and remove these subtopics.
    This helps ensure that when querying relevant insights for a subtopic, insights
    can only belong to one of the distinct subtopics generated.
  prefs: []
  type: TYPE_NORMAL
- en: In the conversation domain, we use manual inspection to verify the distinctness
    of the subtopics and regenerate subtopic candidates (at temperature $T=1$) until
    obtaining a list where each subtopic feels qualitatively unique.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Insight Verification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the news domain, we prompt the LLM to remove duplicate insights. After producing
    an initial set of insights for the subtopics, we take all insights and prompt
    an LLM to categorize each insight into one of the subtopics. As insights are initially
    generated for a particular subtopic, at this step we ensure that no insight can
    fit into another subtopic. We thus remove any insights for which the categorized
    subtopic differs from the one it was initially generated for.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the conversation domain, we iterate over subtopics sequentially and use
    a prompt to generate the list of insights for one subtopic. In the prompt, we
    provide not only the target subtopic but also all other subtopics and insights,
    instructing the LLM to avoid such subtopics and insights, and only propose insights
    that are distinct and unique in contrast to those. Manual inspection from the
    authors reveals that: as long as the subtopics are confirmed to be unique, and
    that the insights are enforced to be specific (and include entities), very little
    overlap occurs across subtopic insights.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.3 Document Verification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the news domain, to ensure a precise mapping of insights to documents in
    which an insight is present, we prompt an LLM to label whether any of the insights,
    across subtopics, other than those sampled for that document are found in the
    document. If any are found, we regenerate the document, asking the LLM to remove
    the sentence(s) containing the extraneous insights. This procedure is repeated
    until no extraneous insights are found, up until 5 iterations. For a similar purpose,
    we prompt an LLM to label whether the insights sampled for a given document are
    indeed found in the document. If not, we regenerate the document to add sentences
    that contain this insight, up until 5 iterations. We find that after 5 iterations
    of editing, discrepancies in insights by the LLM were primarily paraphrasing or
    partial detail upon manual inspection.
  prefs: []
  type: TYPE_NORMAL
- en: In the conversational domain, we generate the documents iteratively, one chapter
    at a time, where each chapter is intended to introduce a single insight. When
    expanding an insight into a chapter, we generate a candidate chapter and use GPT-4o
    to classify whether the candidate chapter indeed covers the expected subtopic
    and insight. If not, we regenerate the candidate chapter up to ten times, and
    otherwise, we accept the candidate chapter into the document and proceed with
    the next chapter. We find that in practice, the generation process requires 5
    iterations less than 1% of the time to generate a chapter that GPT-4o can correctly
    assign to the expected insight.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Evaluation Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Below, we list the prompt we use for automatic evaluation in the SummHay task,
    as described in Section [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems").'
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,WW91IGFyZSBnaXZlbiBhIGxpc3Qgb2YgYnVsbGV0IHBvaW50cyAoZWFjaCB3aXRoIGEgdW5pcXVlIG51bWJlciksIGFuZCBhIHNwZWNpZmljIHJlZmVyZW5jZSBpbnNpZ2h0LiBZb3VyIG9iamVjdGl2ZSBpcyB0byBkZXRlcm1pbmUgd2hldGhlciB0aGUgcmVmZXJlbmNlIGluc2lnaHQgaXMgY292ZXJlZCBpbiBhbnkgb2YgdGhlIGJ1bGxldCBwb2ludHMuIFlvdSBtdXN0IGZ1cnRoZXIgZGV0ZXJtaW5lIGlmIHRoZSBpbnNpZ2h0IGlzIHBhcnRpYWxseSBjb3ZlcmVkICgiUEFSVElBTF9DT1ZFUkFHRSIpIG9yIGZ1bGx5IGNvdmVyZWQgKCJGVUxMX0NPVkVSQUdFIikgYnkgdGhlIGJ1bGxldCBwb2ludHMuIElmIHRoZSBpbnNpZ2h0IGlzIG5vdCBjb3ZlcmVkIGF0IGFsbCwgeW91IG11c3QgcmV0dXJuICJOT19DT1ZFUkFHRSIuIFNlZSBleGFtcGxlcyBiZWxvdzoKCltbRkVXX1NIT1RfRVhBTVBMRVNdXQoKTm93IGNvbXBsZXRlIHRoZSB0YXNrIGZvciB0aGUgZm9sbG93aW5nIGluc2lnaHQgYW5kIGJ1bGxldCBwb2ludHM6CgpSZWZlcmVuY2UgSW5zaWdodDoKW1tJTlNJR0hUXV0KCkJ1bGxldCBQb2ludHM6CltbQlVMTEVUU11dCgpSZXF1aXJlbWVudHM6Ci0gRG8gbm90IGhhbGx1Y2luYXRlIHRoYXQgdGhlIGluc2lnaHQgaXMgY292ZXJlZCBieSB0aGUgYnVsbGV0IHBvaW50cyBpZiBpdCBpcyBub3QuCi0gWW91ciByZXNwb25zZSBzaG91bGQgb25seSBiZSB0aGUgSlNPTiBvdXRwdXQgaW4gdGhlIGZvcm1hdCBhYm92ZSwgc3VjaCB0aGF0IGl0IGNhbiBkaXJlY3RseSBwYXJzZWQgYnkgUHl0aG9uJ3MganNvbiBtb2R1bGUuIERPIE5PVCBPVVRQVVQgQU5ZIEVYUExBTkFUSU9OIE9SIEFOWVRISU5HIFRIQVQgSVMgTk9UIFRIRSBKU09OIFJFU1BPTlNFLgo=)You  are  given  a  list  of  bullet  points  (each  with  a  unique  number),  and  a  specific  reference  insight.  Your  objective  is  to  determine  whether  the  reference  insight  is  covered  in  any  of  the  bullet  points.  You  must  further  determine  if  the  insight  is  partially  covered  ("PARTIAL_COVERAGE")  or  fully  covered  ("FULL_COVERAGE")  by  the  bullet  points.  If  the  insight  is  not  covered  at  all,  you  must  return  "NO_COVERAGE".  See  examples  below:[[FEW_SHOT_EXAMPLES]]Now  complete  the  task  for  the  following  insight  and  bullet  points:Reference  Insight:[[INSIGHT]]Bullet  Points:[[BULLETS]]Requirements:-  Do  not  hallucinate  that  the  insight  is  covered  by  the  bullet  points  if  it  is  not.-  Your  response  should  only  be  the  JSON  output  in  the  format  above,  such  that  it  can  directly  parsed  by  Python’s  json  module.  DO  NOT  OUTPUT  ANY  EXPLANATION  OR  ANYTHING  THAT  IS  NOT  THE  JSON  RESPONSE.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Automatic Results Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Evaluator Model |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4o | Opus | Gem-1.5-Pro |'
  prefs: []
  type: TYPE_TB
- en: '| Summarizer Model Bias |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Sonnet | 0.027 | -0.001 | -0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-flash | 0.051 | 0.024 | -0.009 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5 | 0.009 | 0.050 | 0.048 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | 0.059 | 0.034 | 0.043 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-pro | 0.088 | 0.065 | 0.065 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | 0.075 | 0.091 | 0.056 |'
  prefs: []
  type: TYPE_TB
- en: '| Command-r + | 0.064 | 0.128 | 0.071 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku | 0.092 | 0.108 | 0.071 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 0.097 | 0.102 | 0.106 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 0.062 | 0.067 | 0.049 |'
  prefs: []
  type: TYPE_TB
- en: '| Summary Length Bias |'
  prefs: []
  type: TYPE_TB
- en: '| Length to Score Corr. | -0.122 | -0.174 | -0.178 |'
  prefs: []
  type: TYPE_TB
- en: '| Length to Delta Corr. | 0.02 | -0.051 | -0.081 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results of analysis of potential bias in automated evaluation. We
    explore potential bias caused by what model is used, which is reported in differences
    of scores with human annotation, and bias due to the length of the summary, which
    is reported as a correlation. An unbiased evaluator model should achieve a bias
    close to zero on both analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a concern that since we propose to use an LLM to automate the evaluation
    of the SummHay experiment, the choice of the evaluator model might affect the
    validity of the results if such a model has a systematic bias in its judgment.
    We evaluate the possible presence of two biases. First, whether the automatic
    evaluation could favor outputs of one model family over the other (e.g., GPT-4o
    systematically favoring outputs of the GPT* family).
  prefs: []
  type: TYPE_NORMAL
- en: 'To study this, we perform an automatic evaluation of score bias by calculating
    the difference $(\Delta)$ for each Summarizer model in our experiment, and inspect
    the bias of three evaluator models: GPT-4o, Claude3 Opus, and Gemini-1.5-pro.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The top portion of Table [4](#A1.T4 "Table 4 ‣ A.3 Automatic Results Bias ‣
    Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") summarizes the Summarizer model bias analysis results. First,
    we find that auto-evaluation results almost always have a positive bias, indicating
    that on average, the auto-evaluation overestimates Coverage by roughly 5 points
    across models. Second, we find that evaluator models tend to have a positive bias
    for top-performing Summarizer models (e.g., GPT-4o, Claude3 Opus, and Gemini-1.5-pro),
    but do not systematically prefer outputs from a specific model family. In fact,
    Claude3 Opus seems to be particularly critical of Claude3 model outputs (with
    biases very close to zero). All models have a strong positive bias towards GPT-4o
    outputs, but it does not translate to bias for a model family. Overall, we find
    no evidence of systematic bias of automatic evaluation that would favor one model
    family over the other. The analysis does reveal a pattern of overestimating coverage
    by an average of 5 points, which should be taken into account when interpreting
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: In a second analysis, we study whether automatic evaluation leads to favoring
    summaries based on their length.
  prefs: []
  type: TYPE_NORMAL
- en: To study length bias, we first see whether a summary’s score correlates with
    its length, as measured by the number of words divided by the number of bullet
    points. We divide by the number of bullet points as each query requires a different
    number of bullet points, which directly affects length of the summary in a way
    that is not controlled by the LLM. By measuring length bias based on the length
    of individual bullet points, we remove this confounding variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Length to Score Corr. row of Table [4](#A1.T4 "Table 4 ‣ A.3 Automatic
    Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems"), we find that there is a slight negative correlation (-0.12
    to -0.178) between a summary’s score and the number of words in its bullet points.
    This correlation could be explained by results from Section [5](#S5 "5 Results
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), which
    showed that several of the top-performing models (Claude3-opus, Gemini-1.5-pro)
    generate the shortest bullet points. In other words, short bullet points could
    achieve higher scores not because they are short, but because they were generated
    by better models. To remove this confounding variable, we measure whether automatic
    score $\Delta$ (computed above) correlates with bullet point length. This analysis,
    summarized in te Length to Delta Corr. row of Table [4](#A1.T4 "Table 4 ‣ A.3
    Automatic Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems"), indicates a non-existent correlation between
    bullet-point length, and whether the automatic evaluator was biased in its scoring
    (-0.081 to 0.02). In conclusion, we do not find evidence that using automatic
    evaluation with our evaluation protocol will cause length bias in our results,
    which would systems to generate shorter or longer bullets points.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Details on Establishing SummHay Human Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To establish task duration, we considered an average reading speed of 200 words
    per minute. Carefully reading the documents (roughly 12,000 words) would therefore
    require one hour. Accounting for the need to write the summary, and scan multiple
    times over documents to identify and cite insights, annotators were told they
    had a maximum of two hours to complete the task. Participants could take breaks
    (i.e., pause their work) and finish the task early if they felt the task was completed.
    After their initial sessions, participants were asked whether two hours seemed
    appropriate to complete the task without rushing, and both agreed. Participants
    sometimes used the entirety of the two hours, and in other cases completed the
    task in as little as 80 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: We relied on professional annotators known and trusted by our research group
    (based on performance on previous annotation work), and they were compensated
    at 25 USD per hour. One of the annotators participated in the annotation of automated
    summaries (and therefore had access to reference insights for certain subtopics),
    and we ensured that this annotator only performed the summarization task for subtopics
    and document sets they had not seen during that annotation, ensuring they had
    no prior knowledge of the documents in the Haystack.
  prefs: []
  type: TYPE_NORMAL
- en: Participants were given all documents in a shuffled order and were instructed
    to read them carefully and summarize any insight that seemed to be repeating across
    documents. Participants were told the number of present insights (similar to LLM
    prompt in our experiment). In practice, we found that participants chose to write
    slightly more bullet points than the number they were given.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding citation, participants were instructed to be as thorough as possible
    and to explicitly look for additional citations when they had identified an insight.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding tool use, participants were allowed to use string search (i.e., Ctrl+F),
    but were prohibited from copying the text from the documents, and were explicitly
    instructed they should not use ChatGPT or equivalent LLM-based interfaces in any
    way to assist them with the task. Because this cannot be strictly enforced practically,
    we rely on trusted professional annotators to complete the task.
  prefs: []
  type: TYPE_NORMAL
- en: Participants were instructed to gradually write their summary in a text box
    in the annotation interface, and we recorded the progress on the summary during
    each annotation summary. We then performed auto-evaluation using the same settings
    used in our benchmarking experiments on the final summary of the session, as well
    as a summary every 10 minutes during the session.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Citation Precision & Recall Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Precision | Recall | F1 (Citation) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Summarizer | Orac | Full | Orac | Full | Orac | Full |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5 | 46.7 | – | 17.9 | – | 23.0 | – |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Haiku | 49.3 | 24.7 | 31.6 | 24.2 | 35.6 | 14.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-turbo | 62.3 | 14.1 | 35.7 | 3.8 | 41.4 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | 66.0 | 30.7 | 45.8 | 24.0 | 50.7 | 22.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-flash | 57.8 | 38.2 | 54.5 | 44.2 | 51.7 | 32.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Sonnet | 67.3 | 42.2 | 47.2 | 19.9 | 51.7 | 23.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Command-r | 58.9 | 38.9 | 55.9 | 31.7 | 53.8 | 30.9 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 65.7 | 28.0 | 51.0 | 13.0 | 54.6 | 16.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Command-r + | 67.6 | 24.2 | 60.8 | 21.2 | 60.2 | 19.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-1.5-pro | 76.2 | 59.0 | 60.9 | 52.4 | 64.1 | 51.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Human Perf. | 78.8 | – | 82.4 | – | 76.7 | – |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Breakdown of Citation Precision and Recall of models on the SummHay
    Benchmark, combined into an F1 Score (Citation Score). Numbers are reported for
    the Full Context and the Oracle settings of SummHay.'
  prefs: []
  type: TYPE_NORMAL
- en: The Citation Score in the SummHay benchmark is an F1 calculation between the
    set of cites generated by a system in a given bullet point, and the expected cites
    of the matched reference insight, based on knowledge from the Haystack generation
    of what documents include the insight. F1 is chosen as a measure to ensure that
    systems balance between precise and thorough cites.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.5 Citation Precision & Recall Analysis ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    reports the precision and recall of all systems on the benchmark, as well as the
    F1 (i.e., the Citation score) to shed light on how different systems balance between
    precision and recall.'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Model Access Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For each model in our study, we specify its model card and how it was accessed.
  prefs: []
  type: TYPE_NORMAL
- en: We access the Google models Gemini-1.5-pro (gemini-1.5-pro-preview-0514) and
    Gemini-1.5-flash (gemini-1.5-flash-preview-0514) through Vertex AI ²²2[https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai).
  prefs: []
  type: TYPE_NORMAL
- en: 'We include three OpenAI models in our study: GPT-3.5-turbo (gpt-3.5-turbo-0125),
    GPT-4-turbo (gpt-4-turbo-2024-04-09), and GPT-4o (gpt-4o). All models were accessed
    through OpenAI’s official API³³3[https://github.com/openai/opeai-python](https://github.com/openai/opeai-python).'
  prefs: []
  type: TYPE_NORMAL
- en: Cohere summarizers Command-R (cohere.command-r-v1:0) and Command-R+ (cohere.command-r-plus-v1:0)
    were accessed through Amazon Bedrock⁴⁴4[https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/),
    while Rerank3 (rerank-english-v3.0) was accessed through Cohere’s official API⁵⁵5[https://docs.cohere.com/reference/rerank](https://docs.cohere.com/reference/rerank).
  prefs: []
  type: TYPE_NORMAL
- en: 'Anthropic models were also accessed through Amazon Bedrock: Claude 3 Haiku
    (anthropic.claude-3-haiku-20240307-v1:0), Claude 3 Sonnet (anthropic.claude-3-sonnet-20240229-v1:0),
    and Claude 3 Opus (anthropic.claude-3-opus-20240229-v1:0).'
  prefs: []
  type: TYPE_NORMAL
- en: The embedders Vect (sentence-transformers/all-mpnet-base-v2") and LongEmbed
    (dwzhu/e5-base-4k) were accessed through SentenceTransformers Reimers and Gurevych
    ([2019](#bib.bib46)) and huggingface’s transformers library Wolf et al. ([2019](#bib.bib52)),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: A.7 Additional Output Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Investigating Low Scores. ‣ A.8 Additional Discussion
    ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") provides four examples of real summary outputs from different
    RAG pipelines on a common subtopic related to managing stress when preparing to
    an exam. For each summary, we also report on the Coverage, Citation and Joint
    scores, as calculated by the LLM-based automatic evaluation. We add color coding
    and bolding to facilitate the interpretation of the evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: A.8 Additional Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We point to several additional areas for future work.
  prefs: []
  type: TYPE_NORMAL
- en: English-centric
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While our data pipeline can be extended to non-English languages with access
    to a seed scenario in a given language, our benchmark was developed only on English
    and may be more reliable in English. However, the task is language-agnostic, and
    future work can create a multilingual version of the SummHay task, similar to
    efforts such as Seahorse Clark et al. ([2023](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Relevance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We restrict our data synthesis process and analysis to target summarization
    relevance, but we believe that similar data procedures and evaluations could be
    applied to factual consistency. We leave an extension of our pipeline and analysis
    of model outputs along other dimensions such as coherence, efficiency (brevity),
    or factuality to future work.
  prefs: []
  type: TYPE_NORMAL
- en: Focus on Factoid-Style Insights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our Haystack synthesis process incentivizes the creation of specific insights
    that focus on a number or entity. Specificity helps simplify evaluation, and ensure
    we can achieve reproducible automatic evaluation. Yet real-world scenarios might
    have less clear-cut insights, with different documents only partially overlapping
    on insights, or with potentially disagreeing conclusions (e.g., some people like
    the Pomodoro Technique while others don’t). Prior work has shown that NLP methods
    struggle in such cases of coverage diversity Laban et al. ([2022b](#bib.bib30));
    Huang et al. ([2023](#bib.bib20)), and including such discord within the Haystack
    could yield more complex and realistic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating Low Scores.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Manual inspection reveals different failure modes in low-scoring summaries,
    including (1) retrieving insights that are not relevant to the query (other subtopics),
    (2) framing very high-level information as a specific insight (e.g., “all the
    participants are polite to each other”), (3) hallucinating insights not directly
    supported in the documents. We do not systematically evaluate the frequency of
    each failure, but future work can explore this more systematically, for example
    using efficient NLI-based alignment Laban et al. ([2022a](#bib.bib29)); Zha et al.
    ([2023](#bib.bib55)); Tang et al. ([2024](#bib.bib50)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b1c5d46d8833f97caef53c7c6ebabc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Examples of five insights within a subtopic, and four SummHay outputs
    from RAG systems, including the final Coverage, Citation, and Joint scores, as
    calculated by our LLM-based automatic evaluation. We add color coding and bolding
    to facilitate the interpretation of the evaluation.'
  prefs: []
  type: TYPE_NORMAL
