- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Structured Packing in LLM Training Improves Long Context Utilization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17296](https://ar5iv.labs.arxiv.org/html/2312.17296)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Konrad Staniszewski    Szymon Tworkowski    Yu Zhao    Sebastian Jaszczur   
    Henryk Michalewski    Łukasz Kuciński    Piotr Miłoś
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent developments in long-context large language models have attracted considerable
    attention. Yet, their real-world applications are often hindered by ineffective
    context information use. This work shows that structuring training data to increase
    semantic interdependence is an effective strategy for optimizing context utilization.
    To this end, we introduce Structured Packing for Long Context (SPLiCe), a method
    for creating training examples by using information retrieval methods to collate
    mutually relevant documents into a single training context. We empirically validate
    SPLiCe on large $3$B models, showing perplexity improvements and better long-context
    utilization on downstream tasks. Remarkably, already relatively short fine-tuning
    with SPLiCe is enough to attain these benefits. Additionally, the comprehensive
    study of SPLiCe reveals intriguing transfer effects such as training on code data
    leading to perplexity improvements on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, Language Models, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) (Brown et al., [2020](#bib.bib7); Chowdhery et al.,
    [2022](#bib.bib10); Lewkowycz et al., [2022](#bib.bib24); OpenAI, [2023b](#bib.bib30);
    Bai et al., [2023](#bib.bib3)) have transformed the field of AI. Recently, the
    field has observed the rise of Long-Context Language Models (LCLMs) that promise
    to unveil novel and powerful capabilities (Anthropic, [2023](#bib.bib1); OpenAI,
    [2023a](#bib.bib29)). However, their ability to process long context is not always
    as effective as one hopes for. Indeed, several studies have highlighted an important
    limitation: when processing prompts composed of multiple documents, LCLMs frequently
    encounter difficulties in accurately extracting relevant information (Tworkowski
    et al., [2023](#bib.bib41); Liu et al., [2023](#bib.bib28); Shi et al., [2023a](#bib.bib36)).
    Additionally, they typically find it challenging to utilize information from the
    middle of their inputs (Liu et al., [2023](#bib.bib28)), even on simple synthetic
    retrieval tasks (Li et al., [2023a](#bib.bib25)). Understanding these issues is
    vital for advancements in LCLM technologies and calls for systematic research.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we take a step towards better context utilization in LCLMs. We
    focus on training data, keeping other components, such as the architecture and
    training objectives, unchanged. The broad question is *how to organize training
    data to enhance long context capabilities*? Such perspective has received some
    attention recently (Levine et al., [2022](#bib.bib23); Chan et al., [2022](#bib.bib8);
    Shi et al., [2023b](#bib.bib37)), but the problem is far from being solved. The
    central finding of this work is that *structuring training data to increase semantic
    interdependence is an effective strategy towards better long context utilization*.
    We achieve this by introducing and evaluating Structured Packing for Long Context
    (SPLiCe), a method for creating training examples by using retrieval (e.g., BM25
    or repository structure) to collate mutually relevant documents into a single
    training context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86c9bc61bfe293d443645332d8aa00fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Training examples generated by the standard sampling procedure vs
    proposed by SPLiCe. Similar colors indicate related documents, which could be
    found using a retrieval method (e.g., BM25 or contriver) or metadata (e.g., repository
    structure for code).'
  prefs: []
  type: TYPE_NORMAL
- en: We empirically validate SPLiCe showing that fine-tuning of OpenLLaMA $3$B tokens
    already brings perplexity reduction. This reduction translates to substantial
    improvements in handling long-context information in downstream tasks that require
    retrieval and in-context learning. These tasks include TREC (Li & Roth, [2002](#bib.bib27);
    Hovy et al., [2001](#bib.bib18)), DBpedia (Lehmann et al., [2015](#bib.bib22)),
    Qasper (Shaham et al., [2022](#bib.bib35); Dasigi et al., [2021](#bib.bib11)),
    HotPotQA (Yang et al., [2018](#bib.bib45)), and the lost-in-the-middle benchmark
    (Liu et al., [2023](#bib.bib28)). We perform a comprehensive study of the design
    choices and properties of SPLiCe, showing that the acquired long-context capabilities
    transfer between modalities, such as code and text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We comprehensively show that structuring training data is a viable way of improving
    the long context utilization of LLMs. To this end, we introduce SPLiCe, a method
    for creating training examples by using retrieval to collate mutually relevant
    documents into a single training context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We fine-tune OpenLLaMA $3$Bv2 (Geng & Liu, [2023](#bib.bib15)) using SPLiCe,
    showing that it improves perplexity on long context evaluation, in-context learning
    ability, and ability to retrieve relevant data from the context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present a thorough analysis of the design choices behind SPLiCe, such as
    the number of retrieved documents and the order in which documents are merged
    into a training example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SPLiCe is a method for constructing training examples that enhances the LLMs
    long-context utilization. This translates to improved performance on a range of
    tasks like in-context learning, question answering, in-context information retrieval,
    and long context language modeling. We make the source code available to ensure
    the reproducibility of our results. (For this anonymous submission we share the
    zip file with our code.)
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 SPLiCe training example construction
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:    $D$  while $Q\neq\emptyset$ do        if $d_{i}\in D$'
  prefs: []
  type: TYPE_NORMAL
- en: Rationale and intuitions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Capturing long-range dependencies is believed to enhance language modeling and
    retrieval-augmentation (Borgeaud et al., [2022](#bib.bib6)). However, it is an
    open question how to achieve such benefits in pre-training or fine-tuning. The
    primary difficulty comes from the fact that long-range dependencies are rare in
    training data (de Vries, [2023](#bib.bib12)) and diminish with distance. It is
    thus unlikely that a model will learn to utilize long context without more explicit
    guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies indicate that structuring data, i.e., going beyond the i.i.d.
    paradigm, might be beneficial or even necessary to achieve good long-context performance.
    (Levine et al., [2022](#bib.bib23)) develops a theory showing that the trained
    model establishes stronger dependencies between text segments in the same training
    example. (Chan et al., [2022](#bib.bib8)) indicates that specific distributional
    properties of the training data have a significant impact on the model’s in-context
    performance. Finally, (Shi et al., [2023b](#bib.bib37)) shows that training on
    structured data improves the performance of LLMs on long-context tasks.
  prefs: []
  type: TYPE_NORMAL
- en: SPLiCe follows these intuitions. Namely, it constructs training examples by
    collating mutually relevant documents to increase the dependency density, thus
    allowing the model to learn to utilize long context. We provide additional insights
    into the distributional properties of SPLiCe created data at the end of this section
    and in Appendix [K](#A11 "Appendix K Data Distribution Property ‣ Structured Packing
    in LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: Structured Packing for Long Context (SPLiCe)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We propose SPLiCe, a method that creates training examples by building a tree
    of documents, see Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing
    in LLM Training Improves Long Context Utilization") and Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Structured Packing in LLM Training Improves Long Context
    Utilization") for a general overview.
  prefs: []
  type: TYPE_NORMAL
- en: SPLiCe starts by picking a random document from the dataset to create a single-node
    tree and continues in a breadth-first manner, each time appending top-$k$ similar
    documents from the corpus. The final sequence consists of the tree flattened according
    to a certain tree traversal strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameter $k$, SPLiCe produces examples that are similar to the ones
    used by retrieval augmented models, e.g., (Borgeaud et al., [2022](#bib.bib6)).
    Note that the dataset must be reasonably curated for SPLiCe to work efficiently.
    For example, a low duplication rate incentivizes the model to utilize long context
    beyond simple copying.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many possible retrieval methods can be used with SPLiCe (Retrieve function
    in Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing in LLM Training
    Improves Long Context Utilization")). In our experiments, we test the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SPLiCe Repo: based on additional meta-information about the data, that is the
    repository structure of the code (Repo): we concatenate files using a depth-first
    search algorithm on the directory structure, that is files from the same directory
    are grouped together. A similar method has been pioneered by (Wu et al., [2022](#bib.bib44))
    and proposed in (Shi et al., [2023b](#bib.bib37)) as an interesting future direction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SPLiCe BM25: based on BM25 (Robertson & Zaragoza, [2009](#bib.bib33); Bassani,
    [2023](#bib.bib4)), a standard retrieval method that uses a bag-of-words approach
    to rank documents based on their similarity to a query.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SPLiCe Cont: based on Contriever-MSMARCO (Cont) (Izacard et al., [2022](#bib.bib19)),
    a recently proposed retrieval method that uses a transformer model to rank documents
    based on their similarity to a query.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SPLiCe computational efficiency Given the dataset sizes used in training LLMs,
    the computational efficiency plays a non-trivial role. SPLiCe Repo is the fastest
    and easy to implement but requires additional directory structure, i.e., is not
    applicable to general web data. SPLiCe BM25 requires quadratic time index and
    needs dataset chunking to be applicable to large datasets, see Section [3.3](#S3.SS3
    "3.3 Design Choices Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"). SPLiCe Cont requires
    calculating embeddings for each document and retreval based on the cosine similarity.
    The latter step can be done efficiently using fast approximate inner-product search,
    e.g., Faiss (Johnson et al., [2017](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: Baseline The standard approach, commonly used in LLM training pipelines, is
    to randomly sample documents from the dataset and concatenate them to make training
    examples of a desired context. This is known as example packing (Brown et al.,
    [2020](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Burstiness SPLiCe conjecturally falls into the framework presented in (Chan
    et al., [2022](#bib.bib8)). This paper shows that the distributional properties
    of the training data affect the in-context capabilities of transformer models.
    In particular, it indicates the importance of ”burstiness”, i.e., a flatter frequency
    distribution with a relatively higher mass on the rare, long-tail tokens appearing
    in a sequence . In Appendix [K](#A11 "Appendix K Data Distribution Property ‣
    Structured Packing in LLM Training Improves Long Context Utilization"), we show
    that SPLiCe increases the burstiness of the training data (measured in terms of
    Zipf’s coefficient of token frequency) compared to the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments with Medium-Scale Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct a comprehensive examination of the impact of document
    packing on long-context performance, using medium-scale models (see Section [4](#S4
    "4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization") for large models results). We demonstrate that structuring data
    with SPLiCe is a generally applicable approach to improve long context performance.
    We study the performance in Section [3.2](#S3.SS2 "3.2 Experimental Results ‣
    3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization") and design choices of SPLiCe in Section [3.3](#S3.SS3
    "3.3 Design Choices Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we employ $270$K on a mixture of the original RedPajama data (TogetherComputer,
    [2023](#bib.bib39)) and long context data created using SPLiCe/Baseline. We employ
    the Focused Transformer (FoT) objective (Tworkowski et al., [2023](#bib.bib41))
    for context extension (unless stated otherwise). This approach is motivated by
    practical factors, viz. training with short context length expedites the process,
    while context scaling can be achieved by finetuning on a relatively small amount
    of tokens, as demonstrated by (Chen et al., [2023](#bib.bib9); Tworkowski et al.,
    [2023](#bib.bib41)). Loosely inspired by (Ouyang et al., [2022](#bib.bib31); Rozière
    et al., [2023](#bib.bib34)), in the latter phase, long context data (i.e., prepared
    with SPLiCe) constitutes half of the mixture. This aims to prevent the model from
    overfitting to artificially created long documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the evaluation, we measure perplexity on held-out portions of the arXiv (Azerbayev
    et al., [2022](#bib.bib2)) and StarCoder (Li et al., [2023b](#bib.bib26)) datasets
    employing a context length of $32$K tokens and truncate those exceeding this length.
  prefs: []
  type: TYPE_NORMAL
- en: We refer to Appendix [A](#A1 "Appendix A Architecture ‣ Structured Packing in
    LLM Training Improves Long Context Utilization") for a detailed description of
    the model architecture. For information regarding the training and evaluation
    data, see Appendix [G](#A7 "Appendix G Data Preparation ‣ Structured Packing in
    LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we compare SPLiCe against Baseline. We test two retrieval
    approaches for SPLiCe: BM25 (Robertson & Zaragoza, [2009](#bib.bib33); Bassani,
    [2023](#bib.bib4)), and Contriever-MSMARCO (Izacard et al., [2022](#bib.bib19)))
    denoted as SPLiCe BM25 and SPLiCe Cont respectively. In SPLiCe BM25 retrieval
    takes into account the whole content of the document, whereas in SPLiCe Cont we
    use the first $512$; see Section [3.3](#S3.SS3 "3.3 Design Choices Analysis ‣
    3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: The results are presented in Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results
    ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), and Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), from which we draw the following conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring data improves performance. We observe (see Table [3](#S3.T3 "Table
    3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")) that all variants
    of SPLiCe outperform Baseline, often significantly, regardless of the training
    and eval dataset. This indicates that the structure of the data is important for
    long-context training.
  prefs: []
  type: TYPE_NORMAL
- en: The positive effects transfer between domains. We observe (see Table [3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")) positive
    effects when training on code and evaluating on arXiv and when training on web
    data and evaluating on code. This indicates that the positive effects of SPLiCe
    transfer between domains. We conjecture that better data mixtures could bring
    further benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval method is of secondary importance. We observe (see Table [3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")) that the
    choice of retrieval method has a minor impact on the results. Typically, SPLiCe BM25
    is slightly better than SPLiCe Cont. On the code data, SPLiCe Repo is on par with
    SPLiCe BM25. We recall that SPLiCe Repo can be applied only to code data, whereas
    SPLiCe BM25 and SPLiCe Cont are more general. In practice, we often observe that
    SPLiCe Cont is faster than SPLiCe BM25 due to the use of Faiss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Perplexity ${}_{(\text{imporovment over {Baseline}})}$K context on
    a 50/50 mixture of RedPajama (organized in a standard way) and long-context data
    C, C#, Python, Wikipedia, StackExchange prepared using a method of choice (SPLiCe BM25,
    SPLiCe Cont, SPLiCe Repo, Baseline). Baseline denotes organizing long-context
    data in the same way as RedPajama. SPLiCe beats the Baseline often by a large
    margin. The variants of SPLiCe perform similarly, with SPLiCe BM25 being slightly
    better. For detailed results, see Appendix [B](#A2 "Appendix B Detailed Results
    For Medium-Size Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Long Context | Method | arXiv | Code | Code & |'
  prefs: []
  type: TYPE_TB
- en: '| Data |  |  | Haskell | Python | CUDA | All | arXiv |'
  prefs: []
  type: TYPE_TB
- en: '| C | SPLiCe BM25 | 5.46 [(.09)] | 3.20 [(.17)] | 2.81 [(.12)] | 2.22 [(.11)]
    | 2.94 [(.13)] | 3.10 [(.13)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 5.48 [(.07)] | 3.22 [(.15)] | 2.82 [(.11)] | 2.23 [(.10)] |
    2.96 [(.11)] | 3.11 [(.12)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 5.47 [(.08)] | 3.22 [(.15)] | 2.83 [(.10)] | 2.24 [(.09)] |
    2.96 [(.11)] | 3.12 [(.11)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Baseline | 5.55 | 3.37 | 2.93 | 2.33 | 3.07 | 3.23 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | SPLiCe BM25 | 5.52 [(.13)] | 3.33 [(.25)] | 2.90 [(.17)] | 2.46 [(.19)]
    | 3.11 [(.20)] | 3.26 [(.20)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 5.53[(.12)] | 3.35[(.23)] | 2.91[(.16)] | 2.48[(.17)] | 3.12[(.19)]
    | 3.27[(.19)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 5.53 [(.12)] | 3.35 [(.23)] | 2.91 [(.16)] | 2.49 [(.16)] |
    3.12 [(.19)] | 3.27 [(.19)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Baseline | 5.65 | 3.58 | 3.07 | 2.65 | 3.31 | 3.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | SPLiCe BM25 | 5.47 [(.10)] | 3.25 [(.21)] | 2.53 [(.09)] | 2.41
    [(.15)] | 3.02 [(.15)] | 3.17 [(.15)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 5.49 [(.08)] | 3.28 [(.18)] | 2.53 [(.09)] | 2.43 [(.13)] |
    3.03 [(.14)] | 3.19 [(.13)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 5.48 [(.09)] | 3.27 [(.19)] | 2.54 [(.08)] | 2.44 [(.12)] |
    3.03 [(.14)] | 3.18 [(.14)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Baseline | 5.57 | 3.46 | 2.62 | 2.56 | 3.17 | 3.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Wikipedia | SPLiCe BM25 | 5.64 [(.09)] | 3.82 [(.15)] | 3.26 [(.11)] | 2.87
    [(.13)] | 3.55 [(.13)] | 3.68 [(.13)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 5.65 [(.08)] | 3.87 [(.10)] | 3.30 [(.07)] | 2.92 [(.08)] |
    3.59 [(.09)] | 3.72 [(.09)] |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 5.73 | 3.97 | 3.37 | 3.00 | 3.68 | 3.81 |'
  prefs: []
  type: TYPE_TB
- en: '| StackExchange | SPLiCe BM25 | 5.07 [(.07)] | 3.88 [(.06)] | 3.32 [(.04)]
    | 2.89 [(.05)] | 3.60 [(.05)] | 3.69 [(.05)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 5.09 [(.05)] | 3.91 [(.03)] | 3.35 [(.01)] | 2.93 [(.01)] |
    3.63 [(.02)] | 3.73 [(.01)] |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 5.14 | 3.94 | 3.36 | 2.94 | 3.65 | 3.74 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Perplexity for training on a $50/50$ data mixture of RedPajama and
    C code. We perform three trainings using different training datesets and report
    the mean and standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Long Context | Method | arXiv | Code | Code & |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data |  |  | Python | All | arXiv |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C | SPLiCe BM25 | 5.463 $\pm$ .004 |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 5.477 $\pm$ .006 |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 5.474 $\pm$ .009 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Baseline | 5.550 $\pm$ .005 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Perplexity ${}_{(\text{imporovment over {Baseline}})}$K) using the
    method of CodeLlama (Rozière et al., [2023](#bib.bib34)), YaRN (Peng et al., [2023](#bib.bib32)),
    or left without changes (Naive), as opposed to FoT used in the other experiments.
    For details see Table [13](#A2.T13 "Table 13 ‣ Appendix B Detailed Results For
    Medium-Size Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization").'
  prefs: []
  type: TYPE_NORMAL
- en: '| RoPe scale | Training | Method | arXiv | Code | Code & |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| method | data |  |  | Haskell | Python | CUDA | All | arXiv |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naive | C# | SPLiCe BM25 | 6.25 [(.08)] | 4.84 [(.19)] | 3.55 [(.11)] | 2.84
    [(.12)] | 3.72 [(.13)] | 3.88 [(.12)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 6.25 [(.08)] | 4.87 [(.16)] | 3.56 [(.10)] | 2.85 [(.11)] |
    3.74 [(.11)] | 3.89 [(.11)] |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 6.33 | 5.03 | 3.66 | 2.96 | 3.85 | 4.00 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama | C# | SPLiCe BM25 | 5.74 [(.02)] | 4.28 [(.09)] | 3.22 [(.05)]
    | 2.53 [(.05)] | 3.34 [(.06)] | 3.49 [(.06)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 5.74 [(.02)] | 4.28 [(.09)] | 3.22 [(.05)] | 2.54 [(.04)] |
    3.35 [(.05)] | 3.50 [(.05)] |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 5.76 | 4.37 | 3.27 | 2.58 | 3.40 | 3.55 |'
  prefs: []
  type: TYPE_TB
- en: '| YaRN | C# | SPLiCe BM25 | 5.77 [(.02)] | 4.32 [(.10)] | 3.24 [(.05)] | 2.55
    [(.06)] | 3.37 [(.07)] | 3.52 [(.06)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 5.77 [(.02)] | 4.32 [(.10)] | 3.24 [(.05)] | 2.56 [(.05)] |
    3.38 [(.06)] | 3.53 [(.05)] |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 5.79 | 4.42 | 3.29 | 2.61 | 3.44 | 3.58 |'
  prefs: []
  type: TYPE_TB
- en: SPLiCe works with various context extension results Our main experiments use
    the Focused Transformer (FoT) approach (Tworkowski et al., [2023](#bib.bib41))
    for context extension. FoT extends the context only in a couple of attention layers
    and does not change the positional embeddings of the remaining layers. This allows
    for computationally efficient long-context fine-tuning. To show that our method
    works more generally, we have also evaluated it using more standard approaches.
    That is, we checked the effectiveness of the method when context extension is
    done in all layers, and parameters of Rotary Positional Encodings are either adjusted
    as in CodeLlama (Rozière et al., [2023](#bib.bib34)), adjusted with YaRN (Peng
    et al., [2023](#bib.bib32)), or left without changes. Table [3](#S3.T3 "Table
    3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") shows the results.
  prefs: []
  type: TYPE_NORMAL
- en: Training is stable. We prepared three subsets of the C code dataset and ran
    the tested methods on each of them. In Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), we report the mean perplexity and its standard
    deviation. We observe that the differences between the subsets are minimal, which
    indicates training stability and confirms the statistical significance of our
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Design Choices Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Choice of k As already mentioned in the previous section, the choice of the
    retrieval method (BM25 or Contriver) plays a minor role. In Appendix [C.1](#A3.SS1
    "C.1 SPLiCe Parameters ‣ Appendix C Ablations ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), we study the role of $k$ being the best choice.
    We leave for further work a more detailed analysis of this hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Document ordering We discovered that the ordering of the documents collected
    by the retrieval procedure might be a subtle issue. We consider three choices
    of ORDER function in Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"): identity (do not
    permute the documents), reverse (reverse the order of documents) and a random
    shuffle. In Appendix [C.1](#A3.SS1 "C.1 SPLiCe Parameters ‣ Appendix C Ablations
    ‣ Structured Packing in LLM Training Improves Long Context Utilization"), we present
    results for $270$M models showing negligible differences. However, we observed
    differences for large models, elaborated in Section [4.2.4](#S4.SS2.SSS4 "4.2.4
    Shuffling ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"), in which random shuffling
    is the best choice for the CodeLlama context extension method.'
  prefs: []
  type: TYPE_NORMAL
- en: Corpus chunking Performing retrieval from the whole corpus might be cumbersome
    due to its scale. However, we find that chunking the corpus into smaller parts
    still brings improvements. In particular, for the experiments with English Wikipedia
    and BM25 presented in Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results ‣ 3
    Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization"), we used a manageable size of $2$GB.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Large-Scale Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we show that SPLiCe can improve the long context performance
    of large-scale language models. To this end, we use $3$B parameter models and
    tasks that test in-context learning (Section [4.2.1](#S4.SS2.SSS1 "4.2.1 In-Context
    Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization")), question answering (Section
    [4.2.2](#S4.SS2.SSS2 "4.2.2 Question Answering ‣ 4.2 Experimental Results ‣ 4
    Large-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization")), and in-context information retrieval capabilities (Section [4.2.3](#S4.SS2.SSS3
    "4.2.3 Key Retrieval Performance ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization")). We
    provide perplexity results in Appendix [E](#A5 "Appendix E Perplexity Improvements
    ‣ Structured Packing in LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: We also emphasize that those improvements occur during a relatively short, compared
    to pre-training, fine-tuning. To be more precise, $3$T tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For $3$B v2 on 75/25/25 mixture of RedPajama (50) prepared in the standard way,
    StackExchange (25) and C (25) prepared using SPLiCe BM25. StackExchange is part
    of RedPajama (TogetherComputer, [2023](#bib.bib39)), and C data come from StarCoder
    (Li et al., [2023b](#bib.bib26)).
  prefs: []
  type: TYPE_NORMAL
- en: We train with $32$ with linear warmup and cosine decay, following (Geng & Liu,
    [2023](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the context extensions method we use FoT (Tworkowski et al., [2023](#bib.bib41)),
    and the CodeLlama (CL) (Rozière et al., [2023](#bib.bib34)) approach. More explicitly,
    we test six models:'
  prefs: []
  type: TYPE_NORMAL
- en: $\{$,
  prefs: []
  type: TYPE_NORMAL
- en: where Baseline denotes the standard data preparation method. Hyperparameter
    details are located in Appendix [A](#A1 "Appendix A Architecture ‣ Structured
    Packing in LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present perplexity measurements in Appendix [E](#A5 "Appendix E Perplexity
    Improvements ‣ Structured Packing in LLM Training Improves Long Context Utilization").
    In particular, following (Anthropic, [2023](#bib.bib1)), we examine perplexity
    improvements in the function of the token position in the document. Our findings
    reveal that SPLiCe models demonstrate superior perplexity across all distances.
    Notably, the improvement in perplexity is more pronounced for tokens positioned
    later in the document, indicating enhanced language modeling capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this section, we focus on testing the in-context learning,
    question answering and information retrieval abilities on our models using downstream
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 In-Context Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To evaluate the improvements of in-context learning abilities, we use TREC (Li
    & Roth, [2002](#bib.bib27); Hovy et al., [2001](#bib.bib18)) and DBpedia (Lehmann
    et al., [2015](#bib.bib22)), which are text classification tasks. For TREC, to
    test the long context utilization, we vary the number of in-context examples in
    $\{190,380,780,1560\}$K context length). We sample these examples multiple times
    and measure classification accuracy on test set. In Table [4](#S4.T4 "Table 4
    ‣ 4.2.1 In-Context Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization"), we observe
    that SPLiCe improves the average performance across different context lengths
    and model sizes. We follow a very similar protocol for DBpedia, see Table [5](#S4.T5
    "Table 5 ‣ 4.2.1 In-Context Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization"),
    confirming that SPLiCe improves the context utilization. In Appendix [I](#A9 "Appendix
    I Detailed Accuracy Improvements ‣ Structured Packing in LLM Training Improves
    Long Context Utilization"), we study the distribution of improvements with respect
    to the choice of the in-context examples, showing the domination of SPLiCe.
  prefs: []
  type: TYPE_NORMAL
- en: In Table [4](#S4.T4 "Table 4 ‣ 4.2.1 In-Context Learning ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization") and Table [5](#S4.T5 "Table 5 ‣ 4.2.1 In-Context Learning
    ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM
    Training Improves Long Context Utilization"), we report mean improvement $\Delta$
    bootstrap confidence intervals. See Appendix [I](#A9 "Appendix I Detailed Accuracy
    Improvements ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    for related histograms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Average classification performance on TREC (Li & Roth, [2002](#bib.bib27);
    Hovy et al., [2001](#bib.bib18)). For TREC, we average across $50$ bootstrap confidence
    intervals (see Appendix [I](#A9 "Appendix I Detailed Accuracy Improvements ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")).'
  prefs: []
  type: TYPE_NORMAL
- en: '| TREC |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Context &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (#examples) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Baseline | SPLiCe | $\Delta$ [conf interv] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| $8$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| $8$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| $8$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Average performance on DBpedia (Lehmann et al., [2015](#bib.bib22)).
    We average results across $40$ elements of the evaluation set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DBpedia |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Context &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (#examples) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Baseline | SPLiCe | $\Delta$ [conf interv] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Average performance on HotPotQA (Yang et al., [2018](#bib.bib45)).
    We average results across $7$ bootstrap confidence intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: '| HotPotQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Context &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (#examples) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Baseline | SPLiCe | $\Delta$ [conf interv] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: 4.2.2 Question Answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We also show that SPLiCe models have improved question answering capabilities.
    To this end, we use Qasper (Dasigi et al., [2021](#bib.bib11); Shaham et al.,
    [2022](#bib.bib35)) and HotPotQA (Yang et al., [2018](#bib.bib45)). Qasper contains
    questions regarding research papers provided as a part of the input context. In
    Table [7](#S4.T7 "Table 7 ‣ 4.2.2 Question Answering ‣ 4.2 Experimental Results
    ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization"), we observe that SPLiCe improves the two-shot performance. Similarly,
    we observe improvements on HotPotQA, see Table [6](#S4.T6 "Table 6 ‣ 4.2.1 In-Context
    Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"). We stress that in both cases,
    the results measure in-context capabilities as neither of the datasets was used
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Two-shot F1 performance on the validation subset of Qasper (Dasigi
    et al., [2021](#bib.bib11)). For Qasper, we use the implementation from Language
    Model Evaluation Harness (Gao et al., [2021](#bib.bib13)). Note that in the $3$B
    model case, despite using SPLiCe for code data only, we still have improvements
    in non-code tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| QASPER |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Context length &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (#examples) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Baseline | SPLiCe |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $3$K (2) | 22.1 | 22.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $7$K (2) | 22.8 | 23.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $7$K (2) | 29.0 | 29.7 |'
  prefs: []
  type: TYPE_TB
- en: 4.2.3 Key Retrieval Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key retrieval task introduced in (Liu et al., [2023](#bib.bib28)) has become
    a routine diagnostic tool to study context capabilities. Specifically, the model
    is presented with a list of key-value pairs, which are $128$-bit UUIDs, and is
    tasked to retrieve the value for a given key. Even though very simple, the task
    proved to be challenging for many open-source language models, see Appendix [D](#A4
    "Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training Improves
    Long Context Utilization") for the task details.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [2](#S4.F2 "Figure 2 ‣ 4.2.3 Key Retrieval Performance ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization"), we present the key retrieval performance of $7$B models
    trained with CodeLlama (Rozière et al., [2023](#bib.bib34)) context extension
    method. We note that despite relatively short tuning, SPLiCe significantly helps
    on hard-to-retrieve positions. We provide the results for FoT models in Appendix [D](#A4
    "Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training Improves
    Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5a76b85c8d01c548e5cdc928c46a535.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Key-value retrieval performance on a dictionary of $300$ examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Shuffling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our experiments with large $7$B CodeLlama models, we have noted slight improvements
    in handling long inputs when shuffling documents inside the context, that is,
    setting Order in the Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") to be a random shuffle.
    We provide a comparison on TREC in Table [17](#A6.T17 "Table 17 ‣ Appendix F Shuffling
    ‣ Structured Packing in LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an increasing number of works aiming to study the role of data in LLM
    training in detail. For instance, (Levine et al., [2022](#bib.bib23)) developed
    a theory and demonstrated empirically that incorporating non-adjacent but semantically
    related sentences in training examples leads to better sentence embeddings and
    improves open-domain question-answering performance. Another work (Gu et al.,
    [2023](#bib.bib16)) introduced a pretraining framework grounded on the idea that
    text documents often include intrinsic tasks. They showed that this approach substantially
    boosts in-context learning. Additionally, there is existing work on training long-context
    language models using repository-level code data, such as (Wu et al., [2022](#bib.bib44)).
    A recent work (Chan et al., [2022](#bib.bib8)) identifies the training data’s
    distributional properties that affect transformer models’ in-context capabilities.
    Similarly, (Han et al., [2023](#bib.bib17)) constructs small-scale data using
    an iterative gradient approach and shows that such data improve in-context performance.
  prefs: []
  type: TYPE_NORMAL
- en: Our methodology diverges from these works in several key ways. Firstly, we focus
    on the document-level context during the training phase, as opposed to sentence-level
    (Levine et al., [2022](#bib.bib23)) or paragraph-level (Gu et al., [2023](#bib.bib16))
    granularity. We demonstrate the efficacy of this approach in large-scale language
    modeling, specifically with OpenLLaMA $3$B. Secondly, we construct a tree structure
    of related documents through BM25/Contriever-MSMARCO retrieval and linearize this
    structure to form long-context examples. This allows for greater control over
    example coherence compared to relying solely on natural data structures like repository-level
    code. The gradient-based method presented in (Han et al., [2023](#bib.bib17))
    can be broadly associated with retrieval used in our work; however, it differs
    in scale and granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently to our research, (Shi et al., [2023b](#bib.bib37)) showed a similar
    method for preparing the training data. However, in our work, we focus on tuning
    the models for long context, up to $32$K tokens. In addition to that, we provide
    ablations regarding context extension methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We show that structuring the training data is a viable way of improving the
    model’s performance. The presented method can viewed as a general framework for
    organizing the documents into training examples. This opens multiple further research
    avenues.
  prefs: []
  type: TYPE_NORMAL
- en: Degree of relevance In this work, SPLiCe constructed training examples by concatenating
    most matching documents (according to BM25/Cont/Repo). However, recent results
    of (Tworkowski et al., [2023](#bib.bib41)) show that introducing unrelated data
    to the context may help the model learn better representations. We leave the study
    of how the choice of retriever (in particular, the ratio of related and unrelated
    documents) affects performance for future work. Note that as BM25 is a syntactic
    retriever, it is not guaranteed that the documents returned by it are semantically
    related. Moreover, we have not tuned Contriever-MSMARCO, and it was provided only
    with a $512$ token long prefix of a document.
  prefs: []
  type: TYPE_NORMAL
- en: In order to prepare the training data, SPLiCe uses each document exactly once
    (we mask out each used document for further retrieval). However, it is possible
    that allowing some high-quality documents to occur more than once may be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval granularity Another avenue for future work is to study the granularity
    of the pieces from which the training examples are constructed. In this work,
    we focus on the document-level granularity. However, it is possible to construct
    training examples from smaller pieces, such as paragraphs or sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Our method requires further studies to understand its scaling properties
    beyond the range presented in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Other context extension methods In most cases, our models were trained for a
    long context using the methodology presented in (Tworkowski et al., [2023](#bib.bib41)).
    Additionally, we tested three popular context extension methods on a medium scale
    (Naive, YaRN, and CodeLlama) and tuned a large $7$B model using the CodeLlama
    approach, preliminary confirming the applicability of SPLiCe. However, we leave
    more detailed studies of the design choices of SPLiCe with other context extension
    methods to future work.
  prefs: []
  type: TYPE_NORMAL
- en: Other data sources One of the approaches to training long-context language models
    is to use conversational data (Li et al., [2023a](#bib.bib25)). This is complementary
    to our method. SPLiCe can utilize data that already exists in vast quantities
    and can be easily applied to different types of text (like code, Wikipedia articles,
    StackExchange questions, answers, etc.) to further increase the number of long-context
    examples. We leave researching how SPLiCe integrates with other methods for preparing
    the long-context data as future work.
  prefs: []
  type: TYPE_NORMAL
- en: Data curation Using highly correlated samples has the potential to result in
    training instability. However, we noted no performance degradation during our
    experiments. We leave the study of how SPLiCe integrates with different data types
    for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we present SPLiCe, a novel method of constructing training examples
    for long-context language models, which utilizes BM25/Contriever-MSMARCO to find
    relevant documents and feed them to the model in a structured manner. We show
    that SPLiCe improves the perplexity of the language modeling task and performance
    on downstream tasks. We further show that SPLiCe can be used to improve long-context
    utilization of large-scale models using only short fine-tuning. We believe that
    our work indicates multiple interesting research directions for improving the
    performance of long-context language models with structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to improve the long context utilization
    ability of language models through structuring training data. It bears standard
    risks associated with advancing the field of LLMs (such as misuse and generation
    of malicious/misleading content). For a detailed overview, we refer to (Bender
    et al., [2021](#bib.bib5); Weidinger et al., [2021](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic (2023) Anthropic. Model card and evaluations for claude models. Technical
    report, Anthropic, 2023. URL [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azerbayev et al. (2022) Azerbayev, Z., Piotrowski, B., and Avigad, J. Proofnet:
    A benchmark for autoformalizing and formally proving undergraduate-level mathematics
    problems. In *Advances in Neural Information Processing Systems 35, 2nd MATH-AI
    Workshop at NeurIPS’22*, 2022. URL [https://mathai2022.github.io/papers/20.pdf](https://mathai2022.github.io/papers/20.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2023) Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,
    Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu,
    D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan,
    S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang,
    H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang,
    X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical
    report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bassani (2023) Bassani, E. retriv: A Python Search Engine for the Common Man,
    May 2023. URL [https://github.com/AmenRa/retriv](https://github.com/AmenRa/retriv).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bender et al. (2021) Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell,
    S. On the dangers of stochastic parrots: Can language models be too big? In *Proceedings
    of the 2021 ACM Conference on Fairness, Accountability, and Transparency*, FAccT
    ’21, pp.  610–623, New York, NY, USA, 2021\. Association for Computing Machinery.
    ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas,
    D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones,
    C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero,
    S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models
    by retrieving from trillions of tokens. In *International Conference on Machine
    Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA*, volume 162 of
    *Proceedings of Machine Learning Research*, pp.  2206–2240\. PMLR, 2022. URL [https://proceedings.mlr.press/v162/borgeaud22a.html](https://proceedings.mlr.press/v162/borgeaud22a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., and Amodei, D. Language models are few-shot learners. *CoRR*, abs/2005.14165,
    2020. URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chan et al. (2022) Chan, S., Santoro, A., Lampinen, A. K., Wang, J., Singh,
    A., Richemond, P. H., McClelland, J. L., and Hill, F. Data distributional properties
    drive emergent in-context learning in transformers. In *NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with
    pathways, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasigi et al. (2021) Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A.,
    and Gardner, M. A dataset of information-seeking questions and answers anchored
    in research papers, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de Vries (2023) de Vries, H. In the long (context) run, 2023. URL [https://www.harmdevries.com/post/context-length](https://www.harmdevries.com/post/context-length).
    Accessed: 2023-09-28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, sep 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng (2023) Geng, X. Easylm: A simple and scalable training framework for large
    language models, March 2023. URL [https://github.com/young-geng/EasyLM](https://github.com/young-geng/EasyLM).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng & Liu (2023) Geng, X. and Liu, H. Openllama: An open reproduction of llama,
    May 2023. URL [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2023) Gu, Y., Dong, L., Wei, F., and Huang, M. Pre-training to learn
    in context. In *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pp.  4849–4870, Toronto, Canada,
    July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.267.
    URL [https://aclanthology.org/2023.acl-long.267](https://aclanthology.org/2023.acl-long.267).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2023) Han, X., Simig, D., Mihaylov, T., Tsvetkov, Y., Celikyilmaz,
    A., and Wang, T. Understanding in-context learning via supportive pretraining
    data, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hovy et al. (2001) Hovy, E., Gerber, L., Hermjakob, U., Lin, C.-Y., and Ravichandran,
    D. Toward semantics-based answer pinpointing. In *Proceedings of the First International
    Conference on Human Language Technology Research*, 2001. URL [https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard et al. (2022) Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,
    P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive
    learning. *Trans. Mach. Learn. Res.*, 2022, 2022. URL [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2017) Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity
    search with gpus, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2019) Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity
    search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lehmann et al. (2015) Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas,
    D., Mendes, P. N., Hellmann, S., Morsey, M., van Kleef, P., Auer, S., and Bizer,
    C. Dbpedia - A large-scale, multilingual knowledge base extracted from wikipedia.
    *Semantic Web*, 6(2):167–195, 2015. doi: 10.3233/SW-140134. URL [https://doi.org/10.3233/SW-140134](https://doi.org/10.3233/SW-140134).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. (2022) Levine, Y., Wies, N., Jannai, D., Navon, D., Hoshen, Y.,
    and Shashua, A. The inductive bias of in-context learning: Rethinking pretraining
    example design. In *The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=lnEaqbTJIRz](https://openreview.net/forum?id=lnEaqbTJIRz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E.,
    Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo,
    T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning
    problems with language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,
    K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=IFXTZERXdM7](https://openreview.net/forum?id=IFXTZERXdM7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise
    on context length?, June 2023a. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov,
    D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii,
    E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro,
    J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M., Umapathi, L. K.,
    Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., V, R. M., Stillerman, J., Patel,
    S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Moustafa-Fahmy, N., Bhattacharyya,
    U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero,
    M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J.,
    Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B.,
    Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M.,
    Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may
    the source be with you! *CoRR*, abs/2305.06161, 2023b. doi: 10.48550/arXiv.2305.06161.
    URL [https://doi.org/10.48550/arXiv.2305.06161](https://doi.org/10.48550/arXiv.2305.06161).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Roth (2002) Li, X. and Roth, D. Learning question classifiers. In *COLING
    2002: The 19th International Conference on Computational Linguistics*, 2002. URL
    [https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,
    M., Petroni, F., and Liang, P. Lost in the middle: How language models use long
    contexts. *CoRR*, abs/2307.03172, 2023. doi: 10.48550/arXiv.2307.03172. URL [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023a) OpenAI. New models and developer products. OpenAI Blog, 2023a.
    URL [https://blog.salesforceairesearch.com/xgen-7b/](https://blog.salesforceairesearch.com/xgen-7b/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023b) OpenAI. Gpt-4 technical report, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., and Lowe, R. Training language models to follow instructions
    with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,
    K., and Oh, A. (eds.), *Advances in Neural Information Processing Systems 35:
    Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022,
    New Orleans, LA, USA, November 28 - December 9, 2022*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robertson & Zaragoza (2009) Robertson, S. E. and Zaragoza, H. The probabilistic
    relevance framework: BM25 and beyond. *Found. Trends Inf. Retr.*, 3(4):333–389,
    2009. doi: 10.1561/1500000019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2023) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov,
    I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez,
    A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
    Synnaeve, G. Code llama: Open foundation models for code, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaham et al. (2022) Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O.,
    Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., and Levy, O. Scrolls: Standardized
    comparison over long language sequences, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023a) Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi,
    E. H., Schärli, N., and Zhou, D. Large language models can be easily distracted
    by irrelevant context. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B.,
    Sabato, S., and Scarlett, J. (eds.), *International Conference on Machine Learning,
    ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA*, volume 202 of *Proceedings
    of Machine Learning Research*, pp.  31210–31227\. PMLR, 2023a. URL [https://proceedings.mlr.press/v202/shi23a.html](https://proceedings.mlr.press/v202/shi23a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023b) Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., James, R.,
    Lin, X. V., Smith, N. A., Zettlemoyer, L., Yih, S., and Lewis, M. In-context pretraining:
    Language modeling beyond document boundaries, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2021) Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced
    transformer with rotary position embedding. *CoRR*, abs/2104.09864, 2021. URL
    [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TogetherComputer (2023) TogetherComputer. Redpajama: An open source recipe
    to reproduce llama training dataset, April 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
    Michalewski, H., and Milos, P. Focused transformer: Contrastive training for context
    scaling. *NeurIPS 2023*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. *CoRR*,
    abs/1706.03762, 2017. URL [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weidinger et al. (2021) Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato,
    J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z.,
    Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell,
    L., Hendricks, L. A., Isaac, W., Legassick, S., Irving, G., and Gabriel, I. Ethical
    and social risks of harm from language models, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing
    transformers. In *The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J.
    (eds.), *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pp.  2369–2380\.
    Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL
    [https://doi.org/10.18653/v1/d18-1259](https://doi.org/10.18653/v1/d18-1259).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of our models is based on LLaMA (Touvron et al., [2023](#bib.bib40)),
    and the architectural details can be found in Table [8](#A1.T8 "Table 8 ‣ Appendix
    A Architecture ‣ Structured Packing in LLM Training Improves Long Context Utilization").
    Briefly speaking, our architecture is similar to the one introduced in (Vaswani
    et al., [2017](#bib.bib42)) with a few standard changes. First, we use only the
    decoder without the encoder part. Secondly, we perform RMSNorm before the input
    of both the attention and feed-forward modules. Thirdly, we use the LLaMA FeedForward
    module. Additionally, we use Rotary Position Embedding (Su et al., [2021](#bib.bib38)).
    For context extension, we use Focused Transformer (FoT)(Tworkowski et al., [2023](#bib.bib41)),
    CodeLlama (Rozière et al., [2023](#bib.bib34)) (CL) and YaRN (Peng et al., [2023](#bib.bib32)).
    Table [9](#A1.T9 "Table 9 ‣ Appendix A Architecture ‣ Structured Packing in LLM
    Training Improves Long Context Utilization") presents the details about both standard
    and long-context pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Architecture details. Focused Transformer context extension is applied
    in continued pretraining for $32$K context and evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter/Model Size | 270M | 3B | 7B |'
  prefs: []
  type: TYPE_TB
- en: '| Vocab Size | 32000 | 32000 | 32000 |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding Size | 1024 | 3200 | 4096 |'
  prefs: []
  type: TYPE_TB
- en: '| Num Attention Layers | 12 | 26 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Num Attention Heads | 8 | 32 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Head Size | 128 | 100 | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP Hidden Size | 4096 | 8640 | 11008 |'
  prefs: []
  type: TYPE_TB
- en: '| FoT Context Extension Layers | [6, 9] | [6, 12, 18] | [8, 16, 24] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Training details. We pretrain a custom $270$B parameter OpenLLaMAv2
    model (Geng, [2023](#bib.bib14)). Subscript denotes that parameter was specific
    for a context extension method with FoT referring to (Tworkowski et al., [2023](#bib.bib41))
    and no-FoT to other methods (Naive, YaRN (Peng et al., [2023](#bib.bib32)), CodeLlama
    (Rozière et al., [2023](#bib.bib34))).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stage | Parameter/Model Size | 270M | 3B | 7B |'
  prefs: []
  type: TYPE_TB
- en: '| Pretraining | Context | 2K | 2K | 2K |'
  prefs: []
  type: TYPE_TB
- en: '| Tokens | 6.3B | 1T | 1T |'
  prefs: []
  type: TYPE_TB
- en: '| Long Context Pretraining | Context | 32K[FoT], 16K[no-FoT] | 32K | 32K |'
  prefs: []
  type: TYPE_TB
- en: '| Batch Size | 128[FoT], 16[no-FoT] | 128 | 128[FoT], 32[no-FoT] |'
  prefs: []
  type: TYPE_TB
- en: '| Start Learning Rate | 5e-5 | 1.5e-5 | 1.5e-5 |'
  prefs: []
  type: TYPE_TB
- en: '| End Learning Rate | 5e-6 | 1.5e-6 | 1.5e-6 |'
  prefs: []
  type: TYPE_TB
- en: '| Warmup Steps | 250 | 1000 | 1000[FoT], 200[no-FoT] |'
  prefs: []
  type: TYPE_TB
- en: '| Tokens | 1B | 5.4B | 2B |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Detailed Results For Medium-Size Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we extend results presented in Section [3](#S3 "3 Experiments
    with Medium-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization"). The training methodology is described in Section [3.1](#S3.SS1
    "3.1 Experimental Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"). Details about the
    number of tokens used for evaluation can be found in Table [18](#A7.T18 "Table
    18 ‣ G.1 Evaluation Data ‣ Appendix G Data Preparation ‣ Structured Packing in
    LLM Training Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: Tables [10](#A2.T10 "Table 10 ‣ Appendix B Detailed Results For Medium-Size
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    and [11](#A2.T11 "Table 11 ‣ Appendix B Detailed Results For Medium-Size Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") show
    the results of training $270$K context on a 50/50 mixture of RedPajama data (organized
    in a standard way) and code data organized using a specified method. Table [10](#A2.T10
    "Table 10 ‣ Appendix B Detailed Results For Medium-Size Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization") contains detailed results
    from training on C# and Python. Table [11](#A2.T11 "Table 11 ‣ Appendix B Detailed
    Results For Medium-Size Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization") contains results on C averaged across three different subsets
    of C (for details about the construction of those subsets, see Appendix [G](#A7
    "Appendix G Data Preparation ‣ Structured Packing in LLM Training Improves Long
    Context Utilization")). Both tables show that SPLiCe outperforms the Baseline
    by a significant margin.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of SPLiCe BM25/SPLiCe Cont over the SPLiCe Repo approach
    is that it can be used also for non-structured data. Table [12](#A2.T12 "Table
    12 ‣ Appendix B Detailed Results For Medium-Size Models ‣ Structured Packing in
    LLM Training Improves Long Context Utilization") shows the detailed results of
    applying the SPLiCe on non-code data. Note that training on non-code data allows
    us to improve the model perplexity on the arXiv dataset in comparison to the model
    trained on code.
  prefs: []
  type: TYPE_NORMAL
- en: Table [13](#A2.T13 "Table 13 ‣ Appendix B Detailed Results For Medium-Size Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") considers
    models trained with YaRN (Peng et al., [2023](#bib.bib32)), CodeLlama (Rozière
    et al., [2023](#bib.bib34)) and Naive (no adjustment to RoPE) context extension
    methods. Table [14](#A2.T14 "Table 14 ‣ Appendix B Detailed Results For Medium-Size
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    shows that a simple artificial extension of example length via random concatenation
    of documents does not help.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Perplexity results comparing different ways of organizing the same
    data. All runs started from the same $270M$K context on a 50/50 mixture of RedPajama
    (organized in a standard way) and code organized in the mentioned ways. For details
    about training please refer to Section [3.1](#S3.SS1 "3.1 Experimental Setup ‣
    3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Altered Train Data: C# |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.52 | 5.53 | 5.65 | 5.53 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.39 | 2.40 | 2.50 | 2.40 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.60 | 2.61 | 2.74 | 2.62 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.46 | 2.48 | 2.65 | 2.49 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 1.82 | 1.82 | 1.90 | 1.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 3.41 | 3.44 | 3.72 | 3.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.14 | 2.16 | 2.31 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 8.41 | 8.46 | 8.85 | 8.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 2.80 | 2.80 | 2.95 | 2.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 3.68 | 3.71 | 4.05 | 3.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 1.94 | 1.95 | 2.06 | 1.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 3.01 | 3.03 | 3.25 | 3.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 3.33 | 3.35 | 3.58 | 3.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.09 | 2.10 | 2.22 | 2.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 3.61 | 3.62 | 3.80 | 3.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 2.90 | 2.91 | 3.07 | 2.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.26 | 3.27 | 3.46 | 3.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Altered Train Data: Python |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.47 | 5.49 | 5.57 | 5.48 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.38 | 2.39 | 2.45 | 2.39 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.61 | 2.63 | 2.71 | 2.63 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.41 | 2.43 | 2.56 | 2.44 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 1.98 | 1.99 | 2.06 | 2.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 3.23 | 3.28 | 3.47 | 3.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.15 | 2.17 | 2.27 | 2.17 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 7.94 | 7.98 | 8.28 | 7.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 2.70 | 2.71 | 2.82 | 2.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 3.42 | 3.46 | 3.72 | 3.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 1.95 | 1.96 | 2.03 | 1.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 2.97 | 2.99 | 3.13 | 2.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 3.25 | 3.28 | 3.46 | 3.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.12 | 2.12 | 2.20 | 2.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 3.57 | 3.58 | 3.73 | 3.58 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 2.53 | 2.53 | 2.62 | 2.54 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.17 | 3.19 | 3.32 | 3.18 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: To check the statistical significance of our results, we prepare
    three subsets of C (details in Appendix [G](#A7 "Appendix G Data Preparation ‣
    Structured Packing in LLM Training Improves Long Context Utilization") and train
    the models on a 50/50 mixture of RedPajama data (organized in the standard way)
    and C data organized using one of the methods. Note that the standard deviation
    is much lower than the perplexity improvements from using SPLiCe.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Altered Train Data: C |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.463 $\pm$ 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.126 $\pm$ 0.020 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.396 $\pm$ 0.006 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.219 $\pm$ 0.004 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 1.939 $\pm$ 0.002 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 2.994 $\pm$ 0.102 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.141 $\pm$ 0.002 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 7.857 $\pm$ 0.019 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 2.665 $\pm$ 0.003 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 3.306 $\pm$ 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 1.910 $\pm$ 0.006 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 3.009 $\pm$ 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 3.198 $\pm$ 0.008 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.075 $\pm$ 0.005 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 3.492 $\pm$ 0.014 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 2.810 $\pm$ 0.006 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.100 $\pm$ 0.009 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Perplexity results comparing different ways of organizing the same
    data. All runs started from the same $270M$K context on a 50/50 mixture of RedPajama
    (organized in a standard way) and other data organized using one of the methods.
    For details about training, please refer to Section [3.1](#S3.SS1 "3.1 Experimental
    Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"). Note that the model trained with SPLiCe on
    StackExchange outperforms the one trained on code on arXiv evaluation, showing
    the benefits of SPLiCe’s applicability to non-code data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Altered Train Data: StackExchange |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.07 | 5.09 | 5.14 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.68 | 2.69 | 2.70 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 3.02 | 3.04 | 3.06 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.89 | 2.93 | 2.94 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 2.27 | 2.28 | 2.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 4.02 | 4.06 | 4.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.58 | 2.60 | 2.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 9.55 | 9.67 | 9.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 3.13 | 3.16 | 3.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 4.28 | 4.34 | 4.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 2.24 | 2.25 | 2.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 3.62 | 3.66 | 3.68 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 3.88 | 3.91 | 3.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.43 | 2.45 | 2.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 4.08 | 4.11 | 4.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 3.32 | 3.35 | 3.36 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.69 | 3.73 | 3.74 |'
  prefs: []
  type: TYPE_TB
- en: '| Altered Train Data: Wikipedia |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.64 | 5.65 | 5.73 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.65 | 2.67 | 2.71 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.98 | 3.01 | 3.07 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.87 | 2.92 | 3.00 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 2.22 | 2.24 | 2.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 3.87 | 3.96 | 4.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.51 | 2.55 | 2.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 9.38 | 9.45 | 9.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 3.13 | 3.16 | 3.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 4.23 | 4.32 | 4.49 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 2.18 | 2.21 | 2.26 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 3.49 | 3.55 | 3.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 3.82 | 3.87 | 3.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.39 | 2.41 | 2.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 4.32 | 4.23 | 4.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 3.26 | 3.30 | 3.37 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.68 | 3.72 | 3.81 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Perplexity results comparing different ways of organizing the same
    data for non-FoT models. All runs started from the same $270M$K context on a 50/50
    mixture of RedPajama (organized in a standard way) and C# code is organized in
    one of three ways.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Altered Train Data: C# |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Context $16$K: CodeLlama |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | Baseline | SPLiCe Repo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.74 | 5.76 | 5.74 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.66 | 2.70 | 2.66 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.79 | 2.83 | 2.79 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.53 | 2.58 | 2.54 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 1.91 | 1.93 | 1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 3.78 | 3.85 | 3.79 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.28 | 2.33 | 2.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 8.29 | 8.41 | 8.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 3.57 | 3.64 | 3.58 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 3.93 | 4.01 | 3.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 1.99 | 2.03 | 2.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 2.95 | 3.01 | 2.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 4.28 | 4.37 | 4.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.31 | 2.35 | 2.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 3.67 | 3.72 | 3.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 3.22 | 3.27 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.49 | 3.55 | 3.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Context $16$K: YaRN |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | Baseline | SPLiCe Repo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.77 | 5.79 | 5.77 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.68 | 2.72 | 2.68 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.81 | 2.85 | 2.81 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.55 | 2.61 | 2.56 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 1.92 | 1.94 | 1.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 3.83 | 3.92 | 3.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.30 | 2.36 | 2.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 8.37 | 8.52 | 8.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 3.60 | 3.67 | 3.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 3.97 | 4.06 | 3.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 2.00 | 2.04 | 2.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 2.98 | 3.03 | 2.98 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 4.32 | 4.42 | 4.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.33 | 2.37 | 2.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 3.69 | 3.75 | 3.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 3.24 | 3.29 | 3.24 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.52 | 3.58 | 3.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Context $16$K: Naive |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | Baseline | SPLiCe Repo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 6.25 | 6.33 | 6.25 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.88 | 2.96 | 2.89 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 3.04 | 3.13 | 3.05 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.84 | 2.96 | 2.85 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 2.04 | 2.08 | 2.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 4.40 | 4.56 | 4.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.50 | 2.60 | 2.51 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 9.25 | 9.46 | 9.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 3.98 | 4.10 | 4.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 4.56 | 4.79 | 4.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 2.14 | 2.21 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 3.27 | 3.39 | 3.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 4.84 | 5.03 | 4.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.52 | 2.60 | 2.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 4.05 | 4.20 | 4.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 3.55 | 3.66 | 3.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.88 | 4.00 | 3.89 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Perplexity results comparing different ways of organizing the same
    data. All runs started from the same $270M$K context on a 50/50 mixture of RedPajama
    (organized in a standard way) and C code is organized in one of three ways. For
    details about training please refer to Section [3.1](#S3.SS1 "3.1 Experimental
    Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Altered Train Data: C |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Eval/Method | SPLiCe BM25 | Baseline | Random |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 5.46 | 5.55 | 5.55 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2.13 | 2.17 | 2.18 |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 2.40 | 2.47 | 2.47 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 2.22 | 2.33 | 2.33 |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 1.94 | 2.02 | 2.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 2.99 | 3.20 | 3.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 2.14 | 2.27 | 2.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 7.86 | 8.10 | 8.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 2.67 | 2.77 | 2.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 3.31 | 3.55 | 3.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 1.91 | 2.00 | 2.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 3.01 | 3.15 | 3.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 3.20 | 3.37 | 3.37 |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 2.07 | 2.16 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 3.49 | 3.62 | 3.64 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 2.81 | 2.93 | 2.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | 3.10 | 3.23 | 3.23 |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Ablations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 SPLiCe Parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two important design choices related to SPLiCe. First, how many related
    documents are retrieved in each step (the parameter $k$M models. We use ’standard’,
    as ordered by Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"), the reversed order, and random
    shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 15: Ablation of SPLiCe hyper-parameters. For each ablation, we have trained
    the same $270M$ is reversed, shuffle – examples are shuffled.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Code |  |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Top-$k$ | Order | C++ | Haskell | Python | CUDA | All |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe BM25 | top 1 | standard | 2.38 | 3.20 | 2.82 | 2.23 | 2.93 |'
  prefs: []
  type: TYPE_TB
- en: '| reverse | 2.38 | 3.21 | 2.82 | 2.23 | 2.93 |'
  prefs: []
  type: TYPE_TB
- en: '| top 2 | standard | 2.39 | 3.23 | 2.84 | 2.24 | 2.95 |'
  prefs: []
  type: TYPE_TB
- en: '| reverse | 2.39 | 3.24 | 2.84 | 2.24 | 2.95 |'
  prefs: []
  type: TYPE_TB
- en: '| top 3 | standard | 2.39 | 3.24 | 2.84 | 2.25 | 2.97 |'
  prefs: []
  type: TYPE_TB
- en: '| reverse | 2.39 | 3.25 | 2.84 | 2.25 | 2.96 |'
  prefs: []
  type: TYPE_TB
- en: '| shuffle | 2.40 | 3.24 | 2.84 | 2.25 | 2.96 |'
  prefs: []
  type: TYPE_TB
- en: We also evaluated the influence of BOS and EOS tokens on the performance of
    trained models. As both Repo and SPLiCe methods concatenate documents to create
    training examples, they effectively slightly decrease the number of separating
    tokens compared to the Baseline. However, in Appendix [C.2](#A3.SS2 "C.2 Importance
    of Separating Tokens ‣ Appendix C Ablations ‣ Structured Packing in LLM Training
    Improves Long Context Utilization") we included experiments showing that this
    has no impact on model performance.
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Importance of Separating Tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also evaluate the influence of BOS and EOS tokens on the performance. To
    be more precise, in all setups, training examples are separated by BOS and EOS
    tokens. As SPLiCe methods concatenate documents to create training examples, they
    effectively increase the average example length and decrease the number of separating
    tokens. To check whether those methods do not simply benefit from the reduction
    in the number of BOS and EOS tokens, we have trained a model on data prepared
    similarly as in SPLiCe, but instead of most matching documents $\texttt{RETRIEVE}(d,k)$
    returned random documents from the dataset (sampling without replacement). The
    results are shown in Table [16](#A3.T16 "Table 16 ‣ C.2 Importance of Separating
    Tokens ‣ Appendix C Ablations ‣ Structured Packing in LLM Training Improves Long
    Context Utilization"). We note that the difference between the Baseline and the
    random concatenation approach is small, and the random concatenation approach
    does not result in significant perplexity gains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 16: Perplexity evaluation of two methods of organizing the data. Baseline
    – document equals training example. Random – concatenate documents into examples
    of length bounded by $120$M parameter model. We performed three runs on different
    subsets of C to provide mean and standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Long Context | Method | arXiv | Code | Code & |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data |  |  | Python | All | arXiv |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C | Random | 5.554 $\pm$ 0.005 |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 5.550 $\pm$ 0.005 |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Key-Value Retrieval Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4.2.3 Key Retrieval Performance ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization") shows how training on SPLiCe organized data improves the
    performance on the key-value retrieval task proposed in (Liu et al., [2023](#bib.bib28)).
    This is a zero-shot task in which a model is prompted with a JSON dictionary and
    asked to retrieve a value corresponding to a specified key. The structure of the
    input is showcased below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the value corresponding to the specified key in the JSON object below.
    JSON data: {"4ef217b7-6bc0-48c6-af35-2765f1e730f3": "068192b7-16b1-40e0-8495-61c63f979d50",
     "cd6b8bdc-bc6c-4490-acb4-bc187a2dccba": "7364a26e-289f-4968-93d3-b273e882bdee",
     "7d057372-4ab8-4811-8110-658c3f19fff4": "3ad075c5-b567-4201-85a7-cb31a0c91540",
     "c62e192d-45e6-4646-bb88-1529c73256c9": "f0411644-1f6d-42a6-8af8-f06da66efc77",
     "06134e93-e158-490e-a66c-8e3b98e12735": "50a26a36-d832-450c-8d6e-a4cc3d0ec0ab",
     "3286f978-4270-4b54-8bfa-540d7e0772e6": "075cc716-1836-4f90-9be3-53e3d4ec6585",
     "4701aa05-c523-4b89-9700-64ab9c37c537": "49d86354-74c4-4256-9b3a-35e6e2b80d00",
     "c8895805-e574-4f13-9fe5-89da1d8c4748": "cc91af7f-8509-4bdc-bad7-2646af68e6d2"}
     "4701aa05-c523-4b89-9700-64ab9c37c537":'
  prefs: []
  type: TYPE_NORMAL
- en: We noted that FoT-trained models struggle with this task. This is probably due
    to the fact that they extend context only in a couple of layers, and the key-value
    retrieval task requires looking up and extracting a long sequence of letters and
    digits. Because of that, we evaluate FoT models with shorter dictionaries consisting
    of $75$B CL model with this context length and show the results in Figure [3](#A4.F3
    "Figure 3 ‣ Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training
    Improves Long Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d86d45736b83e67735eea4e182fcf1b1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce9e476f1707efb102fd9116f5e754c1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e5caa25e547c15b811a204e94cdd1fb.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Performance on a smaller version of key-value retrieval task from
    (Liu et al., [2023](#bib.bib28)). We note that FoT models (a), (b) generally struggle
    to retrieve tokens that are only visible to a subset of layers with extended context.
    For comparison, we show the results with a model that has extended context in
    all layers (c) using CodeLlama (Rozière et al., [2023](#bib.bib34)) method of
    context extension. Each position was evaluated using $500$ examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Perplexity Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figure [4](#A5.F4 "Figure 4 ‣ Appendix E Perplexity Improvements ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") we present perplexity
    improvements of $3$, and then average within the buckets. We average perplexity
    across arXiv, CUDA, Haskell, and CommonCrawl datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a27f94ba011deb4034f9268f5377400.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Perplexity improvement with SPLiCe against the Baseline of the final
    models (after $21$k training steps). We bucket tokens by their positions in the
    document and calculate the average. Each dot is the difference of the averages
    of the SPLiCe and Baseline models. We observe that SPLiCe has smaller perplexity,
    and the improvements tend to be larger for tokens further in the document.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/559373c3b9764e91df25c47b407ccd2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Evolution of the perplexity with SPLiCe, as the model is trained
    on more tokens. See [4](#A5.F4 "Figure 4 ‣ Appendix E Perplexity Improvements
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") for the
    difference with the baseline. As expected, SPLiCe significantly improves perplexity
    for tokens whose positions are very distant in the sequence. Perplexity for more
    distant tokens improves more significantly compared to tokens in the beginning,
    early in the training.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Shuffling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 17: Average classification performance on TREC (Li & Roth, [2002](#bib.bib27);
    Hovy et al., [2001](#bib.bib18)). We compare $7$ to denote the standard deviation.
    We decided to stick with the model trained with random shuffling as it has slightly
    better long-context performance and lower standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TREC |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Context length &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (# of examples) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SPLiCe-no-shuf | SPLiCe-shuf |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $7$1.9 |'
  prefs: []
  type: TYPE_TB
- en: '| $8$1.8 |'
  prefs: []
  type: TYPE_TB
- en: Appendix G Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: G.1 Evaluation Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have taken a random subset of arXiv from Proof-pile. For StarCoder data,
    we have downloaded up to $64$GB of each of the mentioned language subsets and
    performed a random 85/15 split for languages that we train on.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating the perplexity of the model, we skip documents that are shorter
    than the model context and truncate documents that are longer than that. Table
    [18](#A7.T18 "Table 18 ‣ G.1 Evaluation Data ‣ Appendix G Data Preparation ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") shows the number of
    tokens over which the perplexity was calculated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 18: Number of evaluation tokens in each of the considered datasets. For
    each context length $c$ tokens prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Eval/Method | 16K | 32K | 64K |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ArXiv | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| C | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| C++ | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 16M | 14M | 8M |'
  prefs: []
  type: TYPE_TB
- en: '| C# | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Common Lisp | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Dart | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Emacs Lisp | 16M | 15M | 9M |'
  prefs: []
  type: TYPE_TB
- en: '| Erlang | 16M | 16M | 12M |'
  prefs: []
  type: TYPE_TB
- en: '| Fortran | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Go | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Groovy | 11M | 4M | 2M |'
  prefs: []
  type: TYPE_TB
- en: '| Haskell | 16M | 16M | 15M |'
  prefs: []
  type: TYPE_TB
- en: '| Java | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 16M | 16M | 16M |'
  prefs: []
  type: TYPE_TB
- en: G.2 Train Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The StackExchange data was taken from the Proof-pile. To prepare the code train
    data, we take the StarCoder train splits mentioned in Section [G.1](#A7.SS1 "G.1
    Evaluation Data ‣ Appendix G Data Preparation ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), shuffle them, group the documents by the
    repository (documents from the same repository occur one after another), and split
    them into smaller packs. We also split repos larger than $25$M characters. The
    character filtering is consistent with our method as we aim to improve the performance
    in a scenario that lacks high-quality long-context data. For C# and Python, only
    one pack is used to organize the data. For C, we have performed a run on three
    packs and provided results and standard deviation in Table [3](#S3.T3 "Table 3
    ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"). For large models,
    we run the methods on several packs and concatenate the results into a single
    dataset. For natural language datasets, we extract a random subset of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Faiss Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our experiments with SPLiCe Cont utilize Faiss (Johnson et al., [2019](#bib.bib21))
    for fast approximate inner-product search. To be more precise, we use the ”IVF8192,Flat”
    index that we train on 262144 examples coming from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Detailed Accuracy Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of in-context learning depends much on the choice of the in-context
    examples. To study this in more detail we study the following random variable
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Delta(c)=\text{ACC}_{\textsc{{SPLiCe}}}(c)-\text{ACC}_{\textsc{Baseline}}(c),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\text{ACC}_{\textsc{{SPLiCe}}}(c),\text{ACC}_{\textsc{Baseline}}(c)$ [confidence
    interval].
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6](#A9.F6 "Figure 6 ‣ Appendix I Detailed Accuracy Improvements ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") shows details about
    accuracy improvements on TREC when considering different numbers of in-context
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20b6dfc34c6a4077402995ea00026928.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Examples: $190$K'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3acd2eb5ef4d419990c03dbe654d4068.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Examples: $380$K'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/faf3e44aac2e8209fb8d8ab3a1c68d45.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) Examples: $780$K'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/431fe0d2273e4e9395fb4d6da6b3f24f.png)'
  prefs: []
  type: TYPE_IMG
- en: '(d) Examples: $1560$K'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Histograms of accuracy improvement of SPLiCe BM25 over Baseline on
    TREC question classification task. The results are obtained by comparing the accuracy
    on the test set of TREC of the $3$ sets of in-context examples. Each set of in-context
    examples consists of elements randomly sampled (without replacement) from the
    training subset of TREC. Note that the model trained with SPLiCe is almost always
    better than the Baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56e3b7d4d17dfc2625897079fbd0bfda.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Examples: $190$B FoT'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e97a84e1e85951f82ef31437e3c55ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Examples: $380$B FoT'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc3d1082807495227440348d4285b872.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) Examples: $190$B FoT'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/960f4fc58aad2e18b8d584d95496fb42.png)'
  prefs: []
  type: TYPE_IMG
- en: '(d) Examples: $380$B FoT'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5fd2c76773a25b158a3ab47c220d2ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: '(e) Examples: $190$B CL'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a54e7314313da1c906735763c04c13e.png)'
  prefs: []
  type: TYPE_IMG
- en: '(f) Examples: $380$B CL'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Histograms of accuracy improvement of SPLiCe BM25 over Baseline on
    DBPedia. We sample $40$ element subset of the DBPedia test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix J Longer Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Table [19](#A10.T19 "Table 19 ‣ Appendix J Longer Context ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"), we present the perplexity
    results for $270$K.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 19: Perplexity ${}_{(\text{imporovment over {Baseline}})}$K context.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Long Context | Method | arXiv | Code | Code & |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data |  |  | Haskell | Python | CUDA | All | arXiv |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C# | SPLiCe BM25 | 4.86 [(.15)] | 2.60 [(.19)] | 2.66 [(.16)] | 2.32 [(.19)]
    | 2.75 [(.19)] | 2.88 [(.19)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Cont | 4.88 [(.13)] | 2.62 [(.17)] | 2.67 [(.15)] | 2.34 [(.17)] |
    2.77 [(.17)] | 2.90 [(.17)] |'
  prefs: []
  type: TYPE_TB
- en: '| SPLiCe Repo | 4.88 [(.13)] | 2.62 [(.17)] | 2.68 [(.14)] | 2.35 [(.16)] |
    2.77 [(.17)] | 2.90 [(.17)] |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 5.01 | 2.79 | 2.82 | 2.51 | 2.94 | 3.07 |'
  prefs: []
  type: TYPE_TB
- en: Appendix K Data Distribution Property
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chan et al. ([2022](#bib.bib8)); Han et al. ([2023](#bib.bib17)) shows that
    the ”burstiness” property of training data benefits a model’s generalisation.
    Specifically, ”burstiness” presents a flatter frequency distribution, with a relatively
    higher mass on the rare, long-tail tokens appearing in a sequence, which results
    in a lower Zipf’s coefficient of token frequency. We also observe a burstiness
    property of data in SPLiCe, as shown in Table [20](#A11.T20 "Table 20 ‣ Appendix
    K Data Distribution Property ‣ Structured Packing in LLM Training Improves Long
    Context Utilization").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 20: Zipf’s coefficient of token frequency on Baseline and SPLiCe. A lower
    Zipf’s coefficient represents a more significant burstiness property.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Data | Context Length | Method | Zipf’s Coefficient |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C | 2K | SPLiCe BM25 | ${1.667}_{(0.139)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | ${1.717}_{(0.152)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| StackEx | 2K | SPLiCe BM25 | ${1.836}_{(0.088)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | ${1.865}_{(0.074)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| C | 32K | SPLiCe BM25 | ${1.512}_{(0.055)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | ${1.593}_{(0.025)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| StackEx | 32K | SPLiCe BM25 | ${1.643}_{(0.026)}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | ${1.664}_{(0.013)}$ |'
  prefs: []
  type: TYPE_TB
