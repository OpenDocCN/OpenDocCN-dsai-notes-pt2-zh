- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06480](https://ar5iv.labs.arxiv.org/html/2404.06480)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chonghua Wang², Haodong Duan^(1†), Songyang Zhang¹, Dahua Lin¹, Kai Chen^(1‡)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Shanghai AI Laboratory
  prefs: []
  type: TYPE_NORMAL
- en: ²Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: philipwang@sjtu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: duanhaodong@pjlab.org.cn   The work was done during an internship at Shanghai
    AI Laboratory; ^† Project Lead; ^‡ Corresponding Author.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, the large language model (LLM) community has shown increasing interest
    in enhancing LLMs’ capability to handle extremely long documents. As various long-text
    techniques and model architectures emerge, the precise and detailed evaluation
    of models’ long-text capabilities has become increasingly important. Existing
    long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text
    test sets based on open-source datasets, focusing mainly on QA and summarization
    tasks. These datasets include test samples of varying lengths (from 2k to 32k+)
    entangled together, making it challenging to assess model capabilities across
    different length ranges. Moreover, they do not cover the ultralong settings (100k+
    tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval,
    a length-adaptable benchmark for evaluating the long-context understanding of
    LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which
    enable a more reliable evaluation of LLMs’ long context capabilities. These benchmarks
    support intricate manipulation of the length of test cases, and can easily produce
    text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API
    models and 6 open-source models with Ada-LEval. The evaluation results demonstrate
    the limitations of current LLMs, especially in ultra-long-context settings. Our
    code is available at [https://github.com/open-compass/Ada-LEval](https://github.com/open-compass/Ada-LEval).
  prefs: []
  type: TYPE_NORMAL
- en: 'Ada-LEval: Evaluating long-context LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: with length-adaptable benchmarks
  prefs: []
  type: TYPE_NORMAL
- en: 'Chonghua Wang²^†^†thanks:   The work was done during an internship at Shanghai
    AI Laboratory; ^† Project Lead; ^‡ Corresponding Author. , Haodong Duan^(1†),
    Songyang Zhang¹, Dahua Lin¹, Kai Chen^(1‡) ¹Shanghai AI Laboratory ²Shanghai Jiao
    Tong University philipwang@sjtu.edu.cn duanhaodong@pjlab.org.cn'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f259e09eb633fc40af6852f4b48f6db9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The demonstration of two tasks: TSort and BestAnswer introduced in
    Ada-LEval. Understanding and reasoning over the full text are required to solve
    these two tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs), typically based on large transformers trained
    on vast corpus, have shown exceptional abilities in memorization, comprehension,
    and reasoning (OpenAI, [2023](#bib.bib24); Touvron et al., [2023](#bib.bib32);
    Zheng et al., [2023](#bib.bib35)). A critical factor that affects LLM performance
    is the ‘context window’ - the number of tokens an LLM can process simultaneously.
    This window’s size is pivotal in handling lengthy texts. Since the debut of ChatGPT
    with a 2,000-token window in November 2022, significant efforts have been made
    in this domain, including more efficient attention mechanisms (Dao et al., [2022a](#bib.bib8);
    Zaheer et al., [2020](#bib.bib33); Ding et al., [2023](#bib.bib12)), scalable
    position embeddings (Su et al., [2021](#bib.bib28); Sun et al., [2022](#bib.bib30)),
    and quantization techniques (Frantar et al., [2022](#bib.bib13); Dettmers et al.,
    [2022](#bib.bib11)). As of December 2023, several LLMs claim to achieve context
    windows up to hundreds of thousands of tokens. This includes both proprietary
    models like GPT-4 Turbo (128,000 tokens), Claude-2.1 (200,000 tokens), and Moonshot
    AI (200,000 Chinese characters), and open-source models such as ChatGLM-32k (Zeng
    et al., [2022](#bib.bib34)) and LongChat-32k (Li* et al., [2023](#bib.bib20)).
    This expansion significantly enhances the potential for processing extensive documents.
    Nevertheless, the effectiveness of these long-context LLMs in managing long texts
    remains an area ripe for exploration and assessment.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside the evolution of LLMs, a wide range of benchmarks have emerged for
    capability assessment (Hendrycks et al., [2020](#bib.bib15); Suzgun et al., [2022](#bib.bib31);
    Cobbe et al., [2021](#bib.bib6); Huang et al., [2023](#bib.bib17)). Most of those
    benchmarks utilize short questions or instructions, making them unsuitable for
    evaluating LLMs’ long-context capabilities. While a few benchmarks do focus on
    assessing specific long-context abilities like summarization, question-answering
    (QA), and continue writing (Huang et al., [2021](#bib.bib16); Liu et al., [2023b](#bib.bib22);
    Dasigi et al., [2021](#bib.bib10)), comprehensive long-document evaluations have
    been limited. Recent benchmarks such as SCROLLS (Shaham et al., [2022](#bib.bib26)),
    L-Eval (An et al., [2023](#bib.bib1)) and LongBench (Bai et al., [2023](#bib.bib2))
    have started to address this gap by including a suite of long-document tasks,
    aiming for a more holistic assessment of LLMs’ long-context understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite these advancements, three significant limitations persist in existing
    benchmarks: Firstly, the ultra-long setting (32,000 tokens or longer) is scarcely
    represented, limiting insights into LLM performance in extreme context lengths.
    Secondly, the integration of test samples of varying lengths within these benchmarks
    complicates the evaluation of LLMs across different length ranges. Lastly, the
    focus on traditional tasks such as question-answering and summarization often
    does not necessitate comprehensive content understanding by the LLMs, as many
    questions in these tasks do not require full-text comprehension. This highlights
    the need for more targeted benchmarks that can rigorously evaluate the deep and
    complete understanding of long-form content by LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we introduce Ada-LEval, a pioneering benchmark to assess the long-context
    capabilities with length-adaptable questions. Ada-LEval comprises two challenging
    tasks: TSort, which involves arranging text segments in the correct order, and
    BestAnswer, which requires choosing the best answer of a question among multiple
    candidates. Both tasks feature the following advantages: 1\. Controllable Test
    Cases: The length of each test case can be finely tuned - by adjusting the number
    and length of text segments in TSort and altering the number of distractor options
    in BestAnswer. 2\. Necessity for Full-Text Comprehension: Successful completion
    of both tasks mandates complete reading and understanding of the provided text.
    3\. Precise Accuracy Measurement: The design of these tasks allows for unambiguous
    accuracy calculation. TSort has a definitive ‘correct’ order, whereas in BestAnswer,
    the annotated responses by the questioner serve as definitive answers.'
  prefs: []
  type: TYPE_NORMAL
- en: Our experiments on these tasks reveal critical insights. We observe a noteworthy
    decline in the performance of existing LLMs as text length increases, particularly
    in ultra-long scenarios. Furthermore, our ablation study uncovers several shortcomings
    in current LLMs, including limited instruction following over extended texts and
    pronounced input order bias. Additionally, we explore various scalable position
    embedding techniques aimed at enlarging the context window of LLMs. Our findings
    indicate that models equipped with those techniques show improved performance
    over the standard models, and the performance is comparable to their counterparts
    trained on longer contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Long-Context Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To address the complexities introduced by the increased text length in language
    models, researchers have developed a range of innovative techniques. These methodologies
    primarily focus on the following key areas: more efficient attention mechanisms,
    divide-and-conquer paradigms, and scalable position embedding techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Attention Mechanisms. Notable advancements in attention mechanisms
    within Transformers have been achieved by several studies (Zaheer et al., [2020](#bib.bib33);
    Guo et al., [2021](#bib.bib14); Dao et al., [2022b](#bib.bib9); Ding et al., [2023](#bib.bib12)).
    A key development in this area is Flash Attention (Dao et al., [2022a](#bib.bib8)),
    which streamlines the attention process by circumventing the need to read and
    write the attention matrix across different memory tiers. This approach results
    in faster processing and reduced memory usage compared to traditional attention
    methods. In LongNet, Ding et al. ([2023](#bib.bib12)) introduces Dilated Attention,
    which reduces the computation complexity of attention to nearly linear and scales
    to 1 billion tokens. However, Liu et al. ([2023a](#bib.bib21)) identified a limitation
    where these mechanisms tend to falter with the middle portions of long texts.
  prefs: []
  type: TYPE_NORMAL
- en: Divide-and-Conquer. In exploring alternatives to conventional long-text modeling,
    several studies have adopted a segmented approach to manage extensive content.
    WebGPT (Nakano et al., [2021](#bib.bib23)) addresses long-form QA by interacting
    with a text-based web-browsing environment. PEARL (Sun et al., [2023](#bib.bib29))
    introduces a framework that prompts LLMs to generate and execute plans for tackling
    complex long-text reasoning tasks. Chen et al. ([2023a](#bib.bib4)) constructs
    a memory tree with the summarization of document segments and navigates on the
    memory tree to answer the original question.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable Position Embeddings. Scalable position embeddings have been instrumental
    in extending the context window of LLMs. RoPE (Su et al., [2021](#bib.bib28))
    utilizes a rotation matrix to enhance positional information, integrating explicit
    relative position dependencies into the self-attention mechanism. ALiBi (Press
    et al., [2021](#bib.bib25)) does not add position embeddings to word embeddings,
    instead applying a linearly decreasing penalty to attention scores based on key-query
    distances. Position Interpolation (Chen et al., [2023b](#bib.bib5)) adopts a different
    strategy by linearly scaling down input position indices to align with preset
    context window sizes, requiring few fine-tuning steps. NTK-aware Scaled RoPE¹¹1[https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
    and ReRoPE (Su, [2023](#bib.bib27)) further combine the benefits of position interpolation
    and length extrapolation methods without any fine-tuning steps.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Long-Context Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building on advancements in long-context techniques, several long-context LLMs
    are developed and released. Llama 2 (Touvron et al., [2023](#bib.bib32)) integrates
    RoPE to expand its context window to 4,000 tokens. Vicuna-v1.5 (Zheng et al.,
    [2023](#bib.bib35)) further extends this capability by fine-tuning Llama 2 on
    high-quality, extensive conversations, successfully increasing the context window
    to 16,000 tokens. Longchat (Li* et al., [2023](#bib.bib20)) models condense RoPE
    to utilize model weights learned in the pretraining stage. ChatGLM2-32k (Zeng
    et al., [2022](#bib.bib34)) is trained on a 32,000-token context length using
    position interpolation, showcasing the scalability of this technique.
  prefs: []
  type: TYPE_NORMAL
- en: The domain of proprietary language models has seen even more significant advancements
    in long-context modeling, stepped into the ultra-long context field. GPT-4-Turbo OpenAI
    ([2023](#bib.bib24)) notably extends its context window to an impressive 128,000
    tokens. In a similar vein, Claude-2 and Claude-2.1 have achieved context lengths
    of 100,000 and 200,000 tokens respectively. This expansion allows them to process
    vast quantities of information, such as hundreds of pages of technical documentation
    or entire books. Kimi Chat, developed by Moonshot.ai, claims to handle up to 200,000
    Chinese characters. However, no existing dataset can evaluate the capability in
    tackling such long texts.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Long-Context Benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Efforts to evaluate the long-context capabilities of language models have been
    intensifying, with a focus primarily on traditional question-answering (QA) and
    summarization tasks. NarrativeQA (Kočiskỳ et al., [2018](#bib.bib18)) offers a
    question-answering dataset built on the entire books from Project Gutenberg and
    movie transcripts. GovReport (Huang et al., [2021](#bib.bib16)) provides a dataset
    comprising national policy issues, each accompanied by an expert-written summary,
    thus testing models’ ability to distill complex, lengthy documents into concise
    summaries. Based on existing long-context benchmarks, SCROLLS(Shaham et al., [2022](#bib.bib26))
    introduces a suite of datasets that requires models to process and reason over
    long contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently, L-Eval (An et al., [2023](#bib.bib1)) and LongBench (Bai et al.,
    [2023](#bib.bib2)) are designed for comprehensive evaluation of long-context capabilities
    of LLMs. L-Eval offers a collection of long documents across different domains
    and provides both close-ended and open-ended tasks. LongBench is a bilingual long
    context benchmark covering six task categories. Most tasks in these benchmarks
    are traditional QA and summarization with fixed document, questions and answers.
    They are inflexible on text length (up to $\sim$32,000 tokens), which fall short
    of adapting to ultra-long context evaluation. Additionally, LongBench uses mostly
    open-ended tasks with traditional F1 and ROUGE metric that may not align well
    with human judgments. In contrast, our benchmarks support length-adaptable evaluation,
    provide sufficient cases and evaluate models using accuracy metrics, avoiding
    inconsistencies with human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Ada-LEval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we outline the construction process of Ada-LEval, detailing
    both the collection methodology of our source data and the building procedure
    of our test cases. [Table 1](#S3.T1 "In 3 Ada-LEval ‣ Ada-LEval: Evaluating long-context
    LLMs with length-adaptable benchmarks") demonstrates the data statistics of Ada-LEval.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TSort |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Setting | Total #Cases Built | Max #Tokens | Avg #Tokens |'
  prefs: []
  type: TYPE_TB
- en: '| 2k | 5123 | 2000 | 1816 |'
  prefs: []
  type: TYPE_TB
- en: '| 4k | 5451 | 4000 | 3724 |'
  prefs: []
  type: TYPE_TB
- en: '| 8k | 5324 | 8000 | 7663 |'
  prefs: []
  type: TYPE_TB
- en: '| 16k | 4957 | 16000 | 15662 |'
  prefs: []
  type: TYPE_TB
- en: '| 32k | 2206 | 32000 | 31226 |'
  prefs: []
  type: TYPE_TB
- en: '| 64k | 1658 | 64000 | 62407 |'
  prefs: []
  type: TYPE_TB
- en: '| 128k | 782 | 127800 | 121488 |'
  prefs: []
  type: TYPE_TB
- en: '| BestAnswer |'
  prefs: []
  type: TYPE_TB
- en: '| Setting | Total #Cases Built | Max #Tokens | Avg #Tokens |'
  prefs: []
  type: TYPE_TB
- en: '| 1k | 7526 | 1128 | 955 |'
  prefs: []
  type: TYPE_TB
- en: '| 2k | 7526 | 2154 | 1983 |'
  prefs: []
  type: TYPE_TB
- en: '| 4k | 7526 | 4215 | 3994 |'
  prefs: []
  type: TYPE_TB
- en: '| 6k | 7526 | 6268 | 6012 |'
  prefs: []
  type: TYPE_TB
- en: '| 8k | 7526 | 7790 | 7518 |'
  prefs: []
  type: TYPE_TB
- en: '| 12k | 7526 | 12389 | 12091 |'
  prefs: []
  type: TYPE_TB
- en: '| 16k | 7526 | 15964 | 15646 |'
  prefs: []
  type: TYPE_TB
- en: '| 32k | 200 | 32974 | 32329 |'
  prefs: []
  type: TYPE_TB
- en: '| 64k | 200 | 64216 | 63274 |'
  prefs: []
  type: TYPE_TB
- en: '| 128k | 200 | 127059 | 126098 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The data statistics of TSort and BestAnswer. We adopt the GPT-4 tokenizer
    CL100K to calculate token numbers. We use a subset of all built cases for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Task Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TSort. TSort provides LLMs with $\mathbf{N}$ shuffled text segments, extracted
    from contiguous chapters of a long book. The task for models is to sort these
    segments into their original sequence. A response is regarded accurate only if
    it precisely reinstates the segments’ initial order. To simplify the challenge
    and minimize possible confusion, we supply LLMs with adjacent paragraphs from
    before and after the specified chapters to serve as contextual hints.
  prefs: []
  type: TYPE_NORMAL
- en: BestAnswer. Each test case in BestAnswer contains one question and a large amount
    of possible answers to this question. We consider the answer designated by the
    original inquirer as the most helpful answer, while LLMs are required to identify
    this optimal answer among all possible candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Source Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TSort. For TSort, we sourced our initial data from Booksum (Kryściński et al.,
    [2021](#bib.bib19)), a text summarization dataset derived from the Project Gutenberg,
    a public book repository consisting of over 60,000 free eBooks spanning various
    literary genres including novels, plays, short stories, and more. Genres like
    epistolary literature and poetry are excluded in the construction of TSort benchmark
    due to their non-sequential nature. To prevent LLMs from exploiting superficial
    cues, we meticulously remove identifiers such as chapter numbers and annotations
    from the content.
  prefs: []
  type: TYPE_NORMAL
- en: BestAnswer. The BestAnswer benchmark is constructed using threads from Stack
    Overflow, a platform renowned for its extensive range of programming-related questions
    and answers. Stack Overflow questions are categorized by multiple tags, indicating
    the thematic similarity of questions within each tag. To ensure the quality and
    diversity of our benchmark, we choose 23 different tags, including javascript,
    python, C++, *etc.*, and collect top 2500 questions from each tag based on popularity.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Test Case Building
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For both tasks, we construct test cases according to their token length (measured
    by GPT-4 tokenizer). We regard token lengths between 1,000 to 16,000 as long-context
    settings and text lengths exceed 16,000 as ultra-long-context settings.
  prefs: []
  type: TYPE_NORMAL
- en: Under long-context settings, TSort cases span test cases with 2k, 4k, 8k, and
    16k tokens. For each length, we fix the segment number N=4 and the length upper
    limit for each text segment and adjacent paragraphs before and after these contiguous
    chapters. We ensure that each text segment contains complete paragraphs thus no
    paragraph is sliced in the middle. To build test cases with different contents,
    we set stride between beginning paragraphs of test cases during construction.
    After prepending the instructions, we further filter test cases that exceed the
    token upper bound.
  prefs: []
  type: TYPE_NORMAL
- en: For BestAnswer, we generate test cases with 1k, 2k, 4k, 6k, 8k, 12k, and 16k
    tokens under long-context settings. Test cases contain the distractor answers
    under corresponding question and adaptable number of distractor answers from other
    similar questions under each length setting. To make evaluation results directly
    comparable across different length settings in long context scenarios, we ensure
    that the questions within the BestAnswer benchmark remain unchanged, regardless
    of the case length. In BestAnswer, we define the most helpful answer as the answer
    explicitly accepted by the inquirer, and adopt it as the ‘groundtruth answer’²²2
    We do not choose the answer with the highest number of votes, since the vote number
    can be influenced by factors such as the answer posting time and the identity
    of the respondent, in addition to its quality.. For integrity reasons, we exclude
    all questions where the corresponding most helpful answer is not text-only. When
    choosing the distractors, we only consider answers that are provided prior to
    the accepted answer under corresponding question. Besides, we incorporate answers
    from other questions with similar tags to the original question to serve as distractor
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: Under ultra-long-context settings, we build test cases with 32k, 64k, and 128k
    tokens for both tasks. The construction paradigm is similar to the long-context
    setting. For BestAnswer, since the number of similar questions and the corresponding
    answers are limited, we relax tag similarity constraints and allow answers of
    questions with less similar tags to serve as the distractor answers.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate the following LLMs under long-context settings: 4 proprietary models:
    (1) GPT-4-Turbo-0125, (2) GPT-4-Turbo-1106 (3) GPT-3.5-Turbo-1106, (4) Claude-2;
    and 6 open-source models: (5) LongChat-7b-v1.5-32k(Zheng et al., [2023](#bib.bib35)),
    (6) ChatGLM2-6B-32k(Zeng et al., [2022](#bib.bib34)), (7) ChatGLM3-6B-32k(Zeng
    et al., [2022](#bib.bib34)), (8) Vicuna-7b-v1.5-16k(Zheng et al., [2023](#bib.bib35)),
    (9) Vicuna-13b-v1.5-16k(Zheng et al., [2023](#bib.bib35)), (10) InternLM2-7b(Cai
    et al., [2024](#bib.bib3)). Due to the inferior performance of open-source LLMs
    under long-context settings, only models with good performance (GPT-4-Turbo, Claude-2,
    *etc.*) are evaluated under ultra-long-context settings.'
  prefs: []
  type: TYPE_NORMAL
- en: For open-source LLMs, we sample a 1000-testcase subset for evaluation under
    each length setting. Due to the costly API of state-of-the-art proprietary models
    (GPT-4-Turbo, Claude-2, *etc.*), we adopt 200-testcase subset (sampled from the
    1000-testcase set) for evaluation under long-context settings, and a 50-testcase
    subset for evaluation under ultra-long-context settings. All experiments are conducted
    using the open-source LLM evaluation platform OpenCompass  (Contributors, [2023](#bib.bib7)).
    We adopt the zero-shot setting for all evaluation, and provide a ‘random guess’
    baseline. We also measure the instruction following rate and the copy instruction
    rate³³3Instruction following rate denotes if the LLM outputs follow the pre-defined
    format. Copy instruction rate measures if the LLM outputs the same answer as in-context
    example provides. on both tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Long-Context Evaluation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| TSort | 2k | 4k | 8k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-0125 | 15.5 | 16.5 | 8.5 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 18.5 | 15.5 | 7.5 | 3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo-1106 | 4.0 | 4.5 | 4.5 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 5.0 | 5.0 | 4.5 | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 5.3 | 5.0 | 3.1 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 0.9 | 0.7 | 0.2 | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 2.3 | 2.4 | 2.0 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 5.3 | 2.2 | 2.3 | 1.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 5.4 | 5.0 | 2.4 | 3.1 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7b | 5.1 | 3.9 | 5.1 | 4.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Guess | 4.2 | 4.2 | 4.2 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: TSort results under long-context settings. We fix the number of segments
    $\mathbf{N}=4$ for TSort evaluation, thus random guess accuracy is roughly 4.2%
    (1 / 24).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TSort. [Table 2](#S4.T2 "In 4.2 Long-Context Evaluation Results ‣ 4 Evaluation
    Results ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")
    displays the test accuracy of various LLMs on the TSort task. This evaluation
    underscores the complexity of TSort, highlighting its intricate nature that necessitates
    a comprehensive understanding and reasoning across long text. Under settings from
    2,000 to 8,000 tokens, only the most powerful proprietary model GPT-4-Turbo outputs
    the correct order of texts with a significant higher probability compared to the
    random baseline. When the context window expands to 16,000, the quality of GPT-4-Turbo’s
    predictions also deteriorates to the random guess level. Other LLMs, encompassing
    both proprietary models and open-source models, all displaying similar performance
    compared to random guess (even under the relative short 2k setting). The results
    indicate that the TSort task posts a severe challenge to existing LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| BestAnswer | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-0125 | 73.5 | 73.5 | 65.5 | 63.0 | 56.5 | 52.0 | 44.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 74.0 | 73.5 | 67.5 | 59.5 | 53.5 | 49.5 | 44.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo-1106 | 61.5 | 48.5 | 41.5 | 29.5 | 17.0 | 2.5 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 65.0 | 43.5 | 23.5 | 15.0 | 17.0 | 12.0 | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 32.4 | 10.7 | 5.7 | 3.1 | 1.9 | 1.6 | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 31.2 | 10.9 | 4.5 | 1.6 | 1.6 | 0.0 | 0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 39.8 | 18.8 | 9.0 | 5.0 | 3.4 | 0.9 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 37.0 | 11.1 | 5.8 | 3.2 | 1.8 | 1.9 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 53.4 | 29.2 | 13.1 | 4.3 | 2.2 | 1.4 | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| InternLM2-7b | 58.6 | 49.5 | 33.9 | 12.3 | 13.4 | 2.0 | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Guess | 26.7 | 10.1 | 4.5 | 3.0 | 2.3 | 1.4 | 1.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: BestAnswer results under long-context settings. For a question with
    $\mathbf{N}$. The random guess accuracy over a long-context setting is the average
    of random guess accuracy for all questions within the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BestAnswer. [Table 3](#S4.T3 "In 4.2 Long-Context Evaluation Results ‣ 4 Evaluation
    Results ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")
    presents the test accuracy of LLMs on BestAnswer. GPT-4-Turbo establishes the
    state-of-the-art on the BestAnswer benchmark. It achieves an outstanding 44.5%
    accuracy under the 16k long-context setting, where around 100 distractor answers
    exist for each question. Among other proprietary models, Claude-2 achieves the
    second best accuracy 11% under the 16k setting. GPT-3.5-Turbo-1106, while outperforming
    Claude-2 under some relative short settings (2k, 4k, 6k), demonstrates performance
    similar to random guess under the 16k setting. There is a considerable performance
    gap between proprietary models and open-source models on BestAnswer. Although
    some models like Vicuna-13b-v1.5-16k and InternLM2-7b perform well under short
    settings, a dramatic accuracy decline can be observed when text length becomes
    larger.'
  prefs: []
  type: TYPE_NORMAL
- en: '| CopyInst Rate | 2k | 4k | 8k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 25.0 | 22.0 | 10.5 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo-1106 | 30.0 | 25.5 | 64.5 | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 99.5 | 95.0 | 97.4 | 96.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Expectation | 5.0 | 5.0 | 5.0 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 100.0 | 99.8 | 99.1 | 100.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 11.3 | 13.8 | 10.5 | 81.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 21.6 | 54.8 | 88.0 | 88.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 100.0 | 100.0 | 59.4 | 33.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 96.6 | 99.0 | 12.2 | 3.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Expectation | 5.3 | 5.0 | 5.4 | 5.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The copy instruction rate of LLMs on TSort under long-context settings.
    Expectation means the ratio of test cases for which the in-context example answer
    is exactly the correct one.'
  prefs: []
  type: TYPE_NORMAL
- en: '| CopyInst Rate | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 12.5 | 8.5 | 5.0 | 5.5 | 6.0 | 2.0 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo-1106 | 16.5 | 22.5 | 18.5 | 16.0 | 11.5 | 2.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 21.5 | 25.5 | 40.5 | 41.0 | 42.5 | 49.0 | 55.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Expectation | 13.0 | 7.0 | 3.0 | 2.0 | 2.5 | 1.5 | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 67.4 | 94.7 | 89.5 | 57.8 | 70.6 | 49.4 | 13.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 36.5 | 43.7 | 35.8 | 27.2 | 24.4 | 35.5 | 44.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 47.9 | 66.1 | 33.3 | 30.4 | 22.5 | 24.8 | 16.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 63.1 | 96.2 | 91.8 | 57.9 | 66.6 | 27.8 | 17.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 27.8 | 45.8 | 55.3 | 19.8 | 3.4 | 5.6 | 11.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Expectation | 14.4 | 10.0 | 5.1 | 2.3 | 1.7 | 1.3 | 1.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The copy instruction rate of LLMs on BestAnswer under long-context
    settings. Expectation means the ratio of test cases for which the in-context example
    answer is exactly the correct one.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/631b3e436e4cfd2164fbcccb6f29d5ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The instruction following rate of LLMs on TSort (Left) and BestAnswer
    (Right) under long-context settings. GPT-4-Turbo on TSort and all proprietary
    models on BestAnswer achieve 100% instruction following rate across all long-context
    settings, thus not displayed.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Error Breakdown
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We further analyze the error instances on TSort and BestAnswer, and find that
    most errors can be attributed to two categories: 1\. The LLM fails to follow the
    provided instruction and does not output a valid answer⁴⁴4A valid answer contains
    a permutation of N segment numbers on TSort and at least one designation of answers
    on BestAnswer.; 2\. The LLM does output a valid answer. However, it simply copies
    the example answer we provide in the in-context example. [Figure 2](#S4.F2 "In
    4.2 Long-Context Evaluation Results ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating
    long-context LLMs with length-adaptable benchmarks") display instruction following
    rate on TSort and BestAnswer. [Tables 4](#S4.T4 "In 4.2 Long-Context Evaluation
    Results ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") and [5](#S4.T5 "Table 5 ‣ 4.2 Long-Context Evaluation
    Results ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") provide detailed statistics about the copy instruction
    rate on TSort and BestAnswer.'
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art GPT-4-Turbo maintains a relatively low copy instruction
    rate and impeccable instruction following rate on both tasks. Error instances
    of Claude-2, LongChat and Vicuna models are predominantly due to elevated Copy
    Instruction Rate, while ChatGLM models suffer from low instruction following rate.
    It is worth noting that all models, with the sole exception of GPT-4-Turbo, find
    it more difficult to follow the instruction on both tasks as text length increases.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Ultra-Long-Context Evaluation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate the following proprietary models under ultra-long-context settings.
    (1) GPT-4-Turbo-0125 (2) GPT-4-Turbo-1106 (3) Claude-2\. (4) Claude-2.1\. We also
    evaluate InternLM2-7b on BestAnswer benchmark under ultra-long-context settings.
    Due to high API calling expense, we test 50 samples under each ultra-long context
    setting. Table [6](#S4.T6 "Table 6 ‣ 4.4 Ultra-Long-Context Evaluation Results
    ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable
    benchmarks") demonstrates the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | Model | 32k | 64k | 128k |'
  prefs: []
  type: TYPE_TB
- en: '| TSort | GPT-4-Turbo-0125 | 2.0 | 4.0 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 6.0 | 6.0 | 6.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 0.0 | 0.0 | / |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2.1 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Random Guess | 4.2 | 4.2 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| BestAnswer | GPT-4-Turbo-0125 | 30.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 16.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 4.0 | 0.0 | / |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2.1 | 4.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | InternLM2-7b | 0.5 | 0.5 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Random Guess | 0.6 | 0.3 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Results of LLMs on TSort and BestAnswer benchmarks in ultra-long context
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Though the evaluated models claim that they can understand long text up to 100,000+
    tokens (a whole book with hundreds of pages, *e.g.*), they suffer from a dramatic
    decline on their performance under ultra-long-context settings, comparing to their
    long-context performance. For the TSort task, GPT-4-Turbo is able to achieve a
    random guess level accuracy, while Claude fails to give any correct answers. For
    BestAnswer, the performance of all three models fall sharply from 16k to 32k text
    length. Meanwhile, they can not give any correct answer when the text length is
    greater than 32k.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.5.1 Perplexity Evaluation on TSort
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Perplexity (PPL) evaluation is frequently adopted to assess the capability
    of LLMs. During inference, models compute the perplexity of multiple candidates
    and the one with the lowest perplexity is selected as the inference result. For
    TSort, we create 24 candidates for perplexity computation, each candidate is a
    permutation of the 4 text segments. We conduct PPL-based evaluation for open-source
    LLMs on 2k, 4k and 8k text length settings. [Table 7](#S4.T7 "In 4.5.1 Perplexity
    Evaluation on TSort ‣ 4.5 Ablation Study ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating
    long-context LLMs with length-adaptable benchmarks") exhibits the PPL-Eval result
    on TSort. When text segments are arranged in the correct order, a significantly
    lower perplexity score can usually be observed⁵⁵5 One potential cause is that
    the chapters have been used for pretraining., resulting in the high TSort accuracy.
    However, when the sorting task is presented as QAs where LLMs are asked to directly
    output the correct order, the performance significantly deteriorates, indicating
    the limited instruction following capabilities of existing LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TSort (PPL Eval) | 2k | 4k | 8k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 60.9 | 68.3 | 77.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 40.5 | 53.5 | 57.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 50.1 | 57.0 | 59.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 70.1 | 78.3 | 77.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 79.3 | 86.7 | 89.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Guess | 4.2 | 4.2 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Perplexity Evaluation Results on TSort for open-source LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Position Bias in BestAnswer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To study the position bias of existing LLMs, in BestAnswer, we keep questions
    and answer candidates the same and alter the position of groundtruth answers.
    Specifically, we manually set the groundtruth answer at the beginning, in the
    middle, or at the rear of all answers and then perform the evaluation. [Table 8](#S4.T8
    "In 4.5.2 Position Bias in BestAnswer ‣ 4.5 Ablation Study ‣ 4 Evaluation Results
    ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks") displays
    the evaluation results. All models demonstrate significant position bias in choosing
    the most helpful answer. Most models achieve much better accuracy when the most
    helpful answer presents at the beginning. Claude-2 has some unique behaviors.
    It performs the best when the groundtruth is positioned at the rear across 4 of
    5 different settings. As the input length increases, the position bias becomes
    more obvious. For instance, Vicuna-7b-v1.5-16k demonstrates relatively uniform
    accuracy under the 1k setting. However, when the input length extends to 16k tokens,
    the model’s performance remains stable only when the best answer is at the front.'
  prefs: []
  type: TYPE_NORMAL
- en: '| BestAnswer | Pos | 1k | 2k | 4k | 8k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | front | 76.5 | 82.5 | 86.5 | 90.0 | 82.0 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 74.5 | 68.0 | 60.0 | 38.0 | 38.5 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 57.5 | 46.6 | 44.0 | 40.5 | 26.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo-1106 | front | 77.0 | 80.5 | 77.0 | 46.5 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 64.5 | 48.5 | 32.0 | 9.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 37.5 | 19.0 | 8.5 | 6.0 | 3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | front | 34.0 | 19.0 | 14.5 | 50.0 | 6.0 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 49.0 | 35.5 | 21.5 | 13.0 | 5.0 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 59.0 | 36.5 | 26.0 | 11.0 | 9.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | front | 24.1 | 5.0 | 12.1 | 33.6 | 29.0 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 32.7 | 13.6 | 0.2 | 0.2 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 29.8 | 1.9 | 0.0 | 0.1 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | front | 30.0 | 31.5 | 46.2 | 10.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 27.7 | 10.4 | 1.0 | 0.1 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 28.5 | 12.4 | 2.6 | 4.1 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | front | 48.9 | 34.3 | 37.6 | 35.8 | 19.0 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 41.9 | 22.3 | 5.3 | 0.9 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 28.8 | 5.4 | 3.7 | 8.8 | 2.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | front | 29.3 | 8.9 | 14.0 | 37.6 | 25.4 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 32.8 | 13.6 | 0.0 | 0.0 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 34.2 | 2.1 | 0.0 | 0.0 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | front | 52.5 | 51.4 | 58.6 | 81.7 | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '| mid | 64.5 | 29.2 | 1.5 | 0.5 | 0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| rear | 34.2 | 2.4 | 0.0 | 0.0 | 13.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Results of LLMs on BestAnswer where the best answer is set at the
    front, in the middle and at the rear of all answers. Pos denotes the position
    of the best answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 Scalable Position Embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scalable position embeddings have shown their value in extending context window
    while requiring minimal or no fine-tuning steps. Existing position embedding methods
    for context window extension can be categorized into two major categories: position
    interpolation and length extrapolation. NTK-aware Scaled RoPE utilizes the advantage
    of both methods by changing the base of RoPE. ReRoPE and Leaky ReRoPE (Su, [2023](#bib.bib27))
    design a window size to control the application of scalable position embeddings
    directly. We conduct our study on Vicuna-v1.5 models (Zheng et al., [2023](#bib.bib35)),
    which are Llama 2 fine-tuned with 4k context window. We adopt original models
    (4k context window) as the baseline across all settings. [Table 9](#S4.T9 "In
    4.5.3 Scalable Position Embeddings ‣ 4.5 Ablation Study ‣ 4 Evaluation Results
    ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks") shows
    the result of different position embedding methods on the BestAnswer benchmark.
    Our findings indicate that scalable position embeddings do improve the long-context
    modeling capability. All methods enhance the accuracy under the 8k setting, which
    is beyond the original context window. Concurrently, the model performance under
    short settings (1k, *e.g.*) is basically retained. NTK-aware Scaled RoPE diminishes
    performance on 1k context length, but outperforms other two methods on longer
    context. The advantage of these methods is more obvious on Vicuna-13b-v1.5. Moreover,
    comparing to their 16k versions, which utilize Flash Attention and are further
    trained on high-quality 16k length conversation data, advanced scalable position
    embeddings still achieve comparable performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vicuna-7b-v1.5 | 1k | 2k | 4k | 8k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ReRoPE | 39.6/39.6 | 11.6/11.6 | 4.7/5.4 | 2.3/3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Leaky ReRoPE | 39.9/39.9 | 11.2/11.2 | 5.1/5.7 | 1.3/2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| NTK | 32.5/32.5 | 10.7/10.7 | 5.8/5.8 | 3.9/3.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Original(4k) | 39.5/39.5 | 9.8/11.0 | 4.2/5.5 | 0.0/0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Original(16k) | 37.0/39.5 | 11.1/11.1 | 5.8/5.8 | 2.5/2.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5 | 1k | 2k | 4k | 8k |'
  prefs: []
  type: TYPE_TB
- en: '| ReRoPE | 49.2/49.2 | 22.5/22.5 | 9.2/10.0 | 1.5/2.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Leaky ReRoPE | 49.3/49.3 | 23.8/23.8 | 8.7/9.8 | 1.3/2.6 |'
  prefs: []
  type: TYPE_TB
- en: '| NTK | 43.8/43.8 | 23.0/23.0 | 11.1/11.1 | 2.3/2.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Original(4k) | 49.1/49.1 | 17.7/17.7 | 5.9/5.9 | 0.1/1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Original(16k) | 53.4/53.4 | 29.2/29.2 | 13.1/13.5 | 2.6/2.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Results of Vicuna-v1.5 with different context window extrapolation
    methods on BestAnswer. ‘Original (4k) / (16k)’ denotes the original Vicuna model
    trained with 4k / 16k context lengths. In the reported ‘X/Y’, X indicates the
    accuracy while Y indicates the accuracy which cases failed to follow the instruction
    are excluded.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.4 Comparison with Other Long-Context Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We compare Ada-LEval with other long-context benchmarks to validate that our
    benchmarks require much overall text understanding to complete the task than traditional
    long-context benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: We regard a task requires models to understand text comprehensively if the performances
    of models decrease sharply when the text is truncated. TSort task meets this requirement
    since truncating any segment will lead to an incorrect answer.
  prefs: []
  type: TYPE_NORMAL
- en: To exhibit the BestAnswer requires more comprehensive text understanding than
    traditional QA and summarization tasks, we conduct an experiment on BestAnswer(16k
    version) and 2 classic long-context datasets, NarrativeQA(LongBench subset, QA
    task) and GovReport(LongBench subset, summarization task) respectively. The metric
    for NarrativeQA is F1 score and metric for GovReport is Rouge-L. We evaluate the
    performance of GPT-4-Turbo-1106 on all 3 datasets. Each test case is truncated
    into 2k, 4k and 8k version as the input. We also provide its full version for
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | 2k | 4k | 8k | Full | Avg #tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BestAnswer | 11.0 | 20.0 | 31.5 | 44.0 | 15646 |'
  prefs: []
  type: TYPE_TB
- en: '| NarrativeQA | 24.7 | 25.6 | 29.7 | 33.1 | 10276 |'
  prefs: []
  type: TYPE_TB
- en: '| GovReport | 30.7 | 32.4 | 33.6 | 30.9 | 29872 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Results of GPT-4-Turbo on different long-context benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the table [10](#S4.T10 "Table 10 ‣ 4.5.4 Comparison with Other Long-Context
    Benchmarks ‣ 4.5 Ablation Study ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating
    long-context LLMs with length-adaptable benchmarks"), the performance of GPT-4-Turbo
    on BestAnswer decreases more dramatically than NarrativeQA and GovReport when
    text is truncated. Notably, the performance on GovReport even increases when text
    is truncated into 4k and 8k. Therefore, our benchmarks require more full-text
    comprehension than traditional QA and summarization tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce Ada-LEval, a length-adaptable dataset to assess
    long-context capability of LLMs. We conduct comprehensive experiments on multiple
    LLMs and find that all open-source models still lag significantly behind state-of-the-art
    proprietary models in terms of long context capability. When the input length
    scales to 4,000 tokens, most open-source models rapidly deteriorates to random
    guess level. In the meanwhile, the capability of proprietary models is also severely
    limited, When it comes to the ultra-long setting (32,000+ tokens), no proprietary
    model notably outperforms the random baseline. Ada-LEval is the first benchmark
    that evaluates LLMs under the ultra-long setting, and we hope that the limitations
    pointed out by this benchmarks can serve as valuable references for future developments
    of long-context LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement. This project is supported by the National Key R&D Program of
    China No.2022ZD0161600 and the Shanghai Postdoctoral Excellence Program (No.2023023).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ada-LEval is a challenging benchmark, requiring strong understanding and reasoning
    capabilities over long text. Due to the poor instruction following rate and copy
    instruction rate of open-source LLMs, Ada-LEval can hardly distinguish their long
    context capability through the accuracy metric.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as text length increases, the difficulty of Ada-LEval rises sharply
    under ultra-long-context settings. Even state-of-the-art proprietary models are
    not able to achieve an ideal performance, which further constrains its applicability
    to current LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2024) Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen,
    Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan,
    Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo,
    Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang
    Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining
    Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu,
    Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke
    Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song,
    Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang,
    Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu,
    Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen
    Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,
    Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang,
    Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou,
    Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024.
    [Internlm2 technical report](http://arxiv.org/abs/2403.17297).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023a) Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli
    Celikyilmaz. 2023a. Walking down the memory maze: Beyond context limit through
    interactive reading. *arXiv preprint arXiv:2310.05029*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023b. Extending context window of large language models via positional
    interpolation. *arXiv preprint arXiv:2306.15595*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contributors (2023) OpenCompass Contributors. 2023. Opencompass: A universal
    evaluation platform for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022a) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. 2022a. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022b) Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri
    Rudra, and Christopher Ré. 2022b. Hungry hungry hippos: Towards language modeling
    with state space models. *arXiv preprint arXiv:2212.14052*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A
    Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and
    answers anchored in research papers. *arXiv preprint arXiv:2105.03011*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000
    tokens. *arXiv preprint arXiv:2307.02486*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2021) Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021. Longt5: Efficient text-to-text
    transformer for long sequences. *arXiv preprint arXiv:2112.07916*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and
    Lu Wang. 2021. Efficient attentions for long document summarization. *arXiv preprint
    arXiv:2104.02112*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan
    Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al.
    2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation
    models. *arXiv preprint arXiv:2305.08322*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa
    reading comprehension challenge. *Transactions of the Association for Computational
    Linguistics*, 6:317–328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kryściński et al. (2021) Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal,
    Caiming Xiong, and Dragomir Radev. 2021. Booksum: A collection of datasets for
    long-form narrative summarization. *arXiv preprint arXiv:2105.08209*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li* et al. (2023) Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, , and Hao Zhang. 2023. [How long can
    open-source llms truly promise on context length?](https://lmsys.org/blog/2023-06-29-longchat)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle:
    How language models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Tianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench:
    Benchmarking repository-level code auto-completion systems. *arXiv preprint arXiv:2306.03091*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with
    human feedback. *arXiv preprint arXiv:2112.09332*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran,
    Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022.
    Scrolls: Standardized comparison over long language sequences. *arXiv preprint
    arXiv:2201.03533*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su (2023) Jianlin Su. 2023. Rectified rotary position embeddings. [https://github.com/bojone/rerope](https://github.com/bojone/rerope).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit
    Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions
    over long documents. *arXiv preprint arXiv:2305.14564*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable
    transformer. *arXiv preprint arXiv:2212.10554*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can
    solve them. *arXiv preprint arXiv:2210.09261*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. 2020. Big bird: Transformers for longer sequences. *Advances in
    neural information processing systems*, 33:17283–17297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
    An open bilingual pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Test Case Building Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that for each case length on Tsort task, we set the length upper limit
    for each text segment and the neighboring paragraphs before and after these contiguous
    chapters. We also set stride between beginning paragraphs. [Table 11](#A2.T11
    "In Appendix B Evaluation Setups ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") demonstrates the detail statistics on the length
    upper limit and the stride.'
  prefs: []
  type: TYPE_NORMAL
- en: On BestAnswer task, two questions are regarded as similar questions when they
    have 40% tags in common. Under ultra-long-context settings, both questions should
    contain at least 1 tag in common.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Evaluation Setups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation Hyperparameters. For open-source LLMs, we adopt their default hyperparameters
    during evaluation on Ada-LEval. For proprietary models including GPT-4-Turbo,
    GPT-3.5-Turbo-1106, we set the temperature to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Budget. Our experiments for open-source LLMs are conducted on
    NVIDIA A100 80GB GPU. The entire evaluation consumes around 800 GPU-hours.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark Instructions. We present instructions of both tasks within Ada-LEval.
    To ensure that models know what to do, we contain the sample input and output
    format that models need to follow in solving problems. The instructions are shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Validity of 200-testcase subset. Our experiments on long-context settings adopt
    200-testcase subset for proprietary models and 1000-testcase subset for open-source
    LLMs. To ensure that evaluation results on 200-testcase subset is valid, [Table 12](#A2.T12
    "In Appendix B Evaluation Setups ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") and [Table 13](#A2.T13 "In Appendix B Evaluation
    Setups ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")
    display results on 200-testcase subset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | Before | Segments | After | Stride |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2k | 200 | 350 | 200 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| 4k | 300 | 800 | 300 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| 8k | 400 | 1750 | 400 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| 16k | 500 | 3700 | 500 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| 32k | 500 | 7700 | 500 | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| 64k | 500 | 15700 | 500 | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| 128k | 500 | 31700 | 500 | 128 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: The length upper limit of text segments and stride between beginning
    paragraphs on TSort.'
  prefs: []
  type: TYPE_NORMAL
- en: '| TSort (200-testcase) | 2k | 4k | 8k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-0125 | 15.5 | 16.5 | 8.5 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 18.5 | 15.5 | 7.5 | 3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo-1106 | 4.0 | 4.5 | 4.5 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 5.0 | 5.0 | 4.5 | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 5.0 | 5.0 | 2.5 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 1.0 | 0.5 | 0.5 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 3.5 | 3.0 | 1.0 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 5.0 | 1.5 | 1.0 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 5.0 | 5.0 | 3.0 | 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Guess | 4.2 | 4.2 | 4.2 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: TSort results under long-context settings(200-testcase subset).'
  prefs: []
  type: TYPE_NORMAL
- en: '| BestAnswer (200-testcase) | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-0125 | 73.5 | 73.5 | 65.5 | 63.0 | 56.5 | 52.0 | 44.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo-1106 | 74.0 | 73.5 | 67.5 | 59.5 | 53.5 | 49.5 | 44.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo-1106 | 61.5 | 48.5 | 41.5 | 29.5 | 17.0 | 2.5 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | 65.0 | 43.5 | 23.5 | 15.0 | 17.0 | 12.0 | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-7b-v1.5-32k | 32.5 | 8.0 | 3.5 | 3.0 | 2.5 | 1.5 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2-6B-32k | 36.0 | 10.5 | 3.0 | 0.5 | 1.5 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3-6B-32k | 37.0 | 15.5 | 5.5 | 4.0 | 5.5 | 0.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b-v1.5-16k | 32.5 | 8.5 | 2.5 | 3.5 | 3.0 | 0.5 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13b-v1.5-16k | 52.0 | 29.0 | 11.0 | 4.0 | 1.5 | 1.0 | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Guess | 26.7 | 10.1 | 4.5 | 3.0 | 2.3 | 1.4 | 1.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: BestAnswer results under long-context settings(200-testcase subset).'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A2.p5.pic1" class="ltx_picture" height="207.44" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,207.44) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="179.88" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">TSort:
    You are an AI assistant. Your job is to sort multiple book sections into the correct
    order. Each time, you will be provided with 4 pieces of text. These texts form
    a continuous part of a book, but are provided in random order. You need to find
    the correct order and return the answer in a string. For example, if you output
    [4, 1, 3, 2], that means the correct order is: Part 4 -> Part 1 -> Part 3 -> Part
    2. You will also be provided with the neighboring paragraphs before and after
    the 4 pieces of texts. The case sample is shown below and you should give me the
    answer in the format exactly the same as the sample. However, you should NOT focus
    on the content of sample answer. Please do NOT output any extra content. Sample
    Input (format only): Before: XXX (Text before the continuous book part) Part 1:
    XXX Part 2: XXX Part 3: XXX Part 4: XXX After: XXX (Text after the continuous
    book part) Sample Output (format only): Answer: [4, 1, 3, 2]</foreignobject></g></g></svg><svg
    id="A2.p6.pic1" class="ltx_picture" height="224.04" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,224.04) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="196.49" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">BestAnswer: You are an AI assistant.
    Your job is to find out the most helpful answer to a given question. Each time,
    you will be provided with a question and n answers to this question. Each answer
    begins with an ’A’ and a number(e.g. A4), which represents its designation. You
    need to determine which answer is the most helpful one to the question. The case
    sample is shown below and you should give me the answer in the format exactly
    the same as the sample. However, you should NOT focus on the content of sample
    answer. Sample Input (format only): The question is given below. XXX(The content
    of question) Possible answers are given below. A1: XXX(The content of answer 1)
    A2: XXX(The content of answer 2) . . . An: XXX(The content of answer n) Now the
    answers are over, please decide which answer is the most helpful one to the question.
    You must give me only the designation of the MOST helpful answer. Sample Output
    (format only): Answer: The designation of the most helpful answer.(e.g. A4 means
    answer 4 is the most helpful answer)</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
