- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:02:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11629](https://ar5iv.labs.arxiv.org/html/2406.11629)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mingyang Song, Mao Zheng, Xuan Luo
  prefs: []
  type: TYPE_NORMAL
- en: Tencent Hunyuan
  prefs: []
  type: TYPE_NORMAL
- en: nickmysong@tencent.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Leveraging Large Language Models (LLMs) as judges for judging the performance
    of LLMs has recently garnered attention. However, this type of approach is affected
    by the potential biases in LLMs, raising concerns about the reliability of the
    evaluation results. To mitigate this issue, we propose and study two versions
    of many-shot in-context prompts, which rely on two existing settings of many-shot
    ICL for helping GPT-4o-as-a-Judge in single answer grading to mitigate the potential
    biases in LLMs, Reinforced ICL and Unsupervised ICL. Concretely, the former utilizes
    in-context examples with model-generated rationales, and the latter without. Based
    on the designed prompts, we investigate the impact of scaling the number of in-context
    examples on the consistency and quality of the judgment results. Furthermore,
    we reveal the symbol bias hidden in the pairwise comparison of GPT-4o-as-a-Judge
    and propose a simple yet effective approach to mitigate it. Experimental results
    show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot
    regime than in the zero-shot regime. Meanwhile, the experimental results further
    verify the effectiveness of the symbol bias mitigation approach. The code and
    data are released in [https://github.com/nick7nlp/SeeMoreJudgeBetter](https://github.com/nick7nlp/SeeMoreJudgeBetter).
  prefs: []
  type: TYPE_NORMAL
- en: Can Many-Shot In-Context Learning Help Long-Context LLM Judges?
  prefs: []
  type: TYPE_NORMAL
- en: See More, Judge Better!
  prefs: []
  type: TYPE_NORMAL
- en: Mingyang Song, Mao Zheng, Xuan Luo Tencent Hunyuan nickmysong@tencent.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs such as GPT-4 OpenAI ([2023](#bib.bib16)), Gemini 1.5 Reid et al. ([2024](#bib.bib18)),
    and Claude3 Anthropic ([2024](#bib.bib2)) have demonstrated remarkable capabilities
    across a wide range of Natural Language Processing (NLP) tasks, becoming integral
    tools in various applications. The rapid advancement of LLMs Chowdhery et al.
    ([2023](#bib.bib7)) underscores the critical need to evaluate their alignment
    with human intent in generated responses. Therefore, evaluation has emerged as
    a crucial research area pivotal to the success of LLMs Chang et al. ([2023](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: LLMs like GPT-4 have shown exceptional performance across various tasks, leading
    to their wide adoption as both evaluators Wang et al. ([2023a](#bib.bib21)); Fu
    et al. ([2023a](#bib.bib9)); Wang et al. ([2023c](#bib.bib23)); Zheng et al. ([2023](#bib.bib27));
    Wang et al. ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6)) and annotators
    Peng et al. ([2023](#bib.bib17)). However, the reliability of LLMs as evaluators
    remains uncertain, given their sensitivity to textual instructions Xu et al. ([2023](#bib.bib25));
    [Turpin et al.](#bib.bib20) and potential judgment biases Wang et al. ([2023b](#bib.bib22));
    Zheng et al. ([2023](#bib.bib27)); Chen et al. ([2024](#bib.bib6)). To this end,
    researchers, such as Wang et al. ([2023b](#bib.bib22)), focus on proposing various
    methods to solve the potential biases that exist when LLMs act as judges, such
    as the positional bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b41e07e8701ed94d6cc2ea4f187723d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Consistency between different versions of judgment results by adopting
    GPT-4o as a zero-shot judge. $\hat{v}_{1}$, and $v_{3}$ vs. $v_{2}$ denotes the
    comparison between the three versions of evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: When human and LLM judges grade answers involving model-generated chain-of-thought
    rationales, consistently and precisely assigning the same score in 1-10 across
    multiple judgments is challenging, even if a reference correct answer is provided
    and each score in 1-10 has a particular and comprehensive description. Meanwhile,
    it is impossible to cover all situations in the descriptions of scores. Nonetheless,
    existing LLM-as-a-Judge approaches only adopt prompts with several aspects of
    constraints, which is undoubtedly very difficult because LLMs do not know exactly
    what kind of answer corresponds to each score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Newly expanded context windows of LLMs allow researchers to investigate ICL
    with more shots than the zero-shot and few-shot regimes, namely many-shot ICL.
    To fully investigate the many-shot ICL, Agarwal et al. ([2024](#bib.bib1)) propose
    two settings of many-shot ICL (i.e., Reinforced ICL and Unsupervised ICL) to explore
    scaling in-context learning to hundreds or thousands of in-context examples and
    find performance improvements across multiple tasks. More importantly, they show
    that using the many-shot in-context examples with chain-of-thought rationales
    generated through the zero-shot regime is effective, and the many-shot ICL may
    overcome the pre-training biases of LLMs, whereas few-shot ICL struggles. Therefore,
    the intuitive idea is to use the many-shot ICL, allowing LLM-as-a-Judge to see
    the zero-shot evaluations of similar questions and answers first and then scoring
    examples before scoring. Therefore, an interesting question arises:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can many-shot in-context learning help long-context LLM judges, such as GPT-4o?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Motivated by prior findings and the above issue, we verify the consistency of
    the widely used prompts of LLM-as-a-Judge Zheng et al. ([2023](#bib.bib27)), as
    shown in Table [3](#A1.T3 "Table 3 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can Many-Shot
    In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!"). Concretely,
    the consistency experiments are based on the entire test set of GSM8K Cobbe et al.
    ([2021](#bib.bib8)), where the inference answers are obtained based on LLaMA3-70B
    (The pipeline as illustrated as in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") and details in $\S$ [3](#S3 "3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!")). Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!") presents that the consistency between the
    two versions of evaluations is low, with nearly half of the ratings being inconsistent.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the above phenomena and issues, in this paper, we investigate whether
    many-shot in-context learning helps long-context LLM judges. Motivated by Agarwal
    et al. ([2024](#bib.bib1)), we introduce two versions of many-shot ICL prompts
    via Reinforced ICL and Unsupervised ICL for LLM-as-a-Judge. The former utilizes
    in-context examples with model-generated rationales, and the latter removes rationales
    used in the former, details in $\S$ [A.1](#A1.SS1 "A.1 Many-Shot Unsupervised
    ICL ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!"). Meanwhile, we reveal a novel symbol bias
    in GPT-4o-as-a-Judge and explore a simple approach for mitigating this issue.
    Experiments show that many-shot ICL can help GPT-4o-as-a-Jduge obtain higher quality
    and consistency evaluation results. As the number of in-context examples increases,
    the quality and consistency of evaluation improves significantly. Furthermore,
    we further verify the effectiveness of the proposed simple yet effective approach
    for mitigating the symbol bias in pairwise comparison of GPT-4o-as-a-Jduge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions can be summarized as the following three-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firstly, we propose investigating many-shot ICL to help long-context LLM judges,
    such as GPT-4o. To the best of our knowledge, we are the first to explore and
    introduce many-shot in-context learning to assist long-context LLMs as judges
    in single answer grading of GPT-4o-as-a-Judge and scale the number of in-context
    examples to verify the effectiveness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, inspired by the Counting-Stars benchmark Song et al. ([2024](#bib.bib19)),
    we reveal a novel potential bias in pairwise comparison of GPT-4o-as-a-Judge,
    namely symbol bias, and propose a simple yet effective approach to mitigate it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, through experimental results, many-shot ICL can help GPT-4o-as-a-Jduge
    obtain higher quality and consistency evaluation results. Meanwhile, we demonstrate
    the effectiveness of the symbol bias mitigation method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/552ddaed0fbb3bc6345fcafef0dae7aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The pipeline of the experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a52535dc54c7abd5e3da2a46e0653e9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Consistency between two versions of judgment results. Concretely,
    the bar corresponding to "0" on the x-axis represents the number of samples with
    consistent and inconsistent ratings in comparing evaluation results obtained twice
    using GPT-4o as the judge in the zero-shot regime. In addition, the zero-shot
    generated rationales are used for Reinforced ICL and appended to the prompt for
    Unsupervised ICL. The bar corresponding to "$2^{n}$" on the x-axis represents
    the consistency of using the GPT-4o as a judge in the many-shot Reinforced ICL
    regime.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 GPT-4o as A Long-Context LLM Judge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we briefly describe the background of many-shot ICL, recall
    judgment biases, and introduce two variants of designed prompts for GPT-4o-as-a-Judge
    in this study.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Background of Many-Shot ICL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs excel at few-shot in-context learning, which involves learning from a few
    input-output demonstrations (“shots”) provided in context at inference without
    weight updates Brown et al. ([2020](#bib.bib3)). Newly expanded long-context LLMs
    allow us to investigate ICL with hundreds of in-context examples Li et al. ([2023](#bib.bib12));
    Agarwal et al. ([2024](#bib.bib1)). Due to some limitations, we only examine GPT-4o-as-a-Judge
    in a many-shot regime in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Recalling Judgment Biases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM judges possess potential biases, which have been widely explored Wang et al.
    ([2023b](#bib.bib22)); Wu and Aji ([2023](#bib.bib24)); Zheng et al. ([2023](#bib.bib27));
    Chen et al. ([2024](#bib.bib6)). For example, positional bias refers to the phenomenon
    where, during pairwise comparisons, LLM judges tend to favor one side of a pair
    regardless of the actual quality of the answers. In this paper, we investigate
    leveraging many-shot ICL to help GPT-4o as a better Judge, maybe through reducing
    judgment biases.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Two GPT-4o-as-a-Judge Variations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inspired by Zheng et al. ([2023](#bib.bib27)), we introduce two GPT-4o-as-a-Judge
    variations: single answer grading and pairwise comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Single Answer Grading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this variation, the GPT-4o Judge is required to assign a score with a reason
    to a single answer directly. Inspired by the recent work Agarwal et al. ([2024](#bib.bib1)),
    we adopt the Reinforced ICL, which uses model-generated rationales as the in-context
    examples. Meanwhile, we also design the prompt within the Unsupervised ICL. The
    4-shot Reinforced ICL and 1-shot Unsupervised ICL append 4-shots with prompts
    are presented in Table [1](#A1.T1 "Table 1 ‣ A.3 Dataset ‣ Appendix A Appendix
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") and Table [5](#A1.T5 "Table 5 ‣ A.3 Dataset ‣ Appendix A Appendix ‣
    Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!").
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Pairwise Comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GPT-4o Judge is presented with a question and two answers and tasked with
    determining which is better or declaring a tie. The prompt used for this scenario
    is given in Table [3](#A1.T3 "Table 3 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can
    Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!").
    In this work, we adopt the pairwise comparison of GPT-4o-as-a-Judge to evaluate
    the quality of the GPT-4o judgments results in the zero-shot regime.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/142ccb7534f9b2d9f47f61022785c0fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Consistency between two versions of judgment results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fca1badbaf70b719df9d5e91cddef560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Consistency between two versions of judgment results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1faa874689a6fbac8e12dd1ac835da5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Consistency between two versions of judgment results.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the experimental settings, consistency evaluation
    in single answer grading of GPT-4o-as-a-Jduge, quality evaluation via pairwise
    comparison of GPT-4o-as-a-Jduge, and reveal the symbol bias in GPT-4o-as-a-Jduge.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To analyze the GPT-4o judge in the many-shot regime, we use LLaMA3-70B to generate
    answers for each question in GSM8K Cobbe et al. ([2021](#bib.bib8)) with a temperature
    of 0.7\. Details of the used benchmark are provided in $\S$ examples (all examples
    in the training set).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, inspired by Counting-Stars Song et al. ([2024](#bib.bib19)), even
    if too many shots are provided, the GPT-4o may not be able to utilize all of them.
    Because, in the Counting-Stars benchmark, when the number of pieces of evidence
    reaches 32, long-context LLMs (e.g., GPT-4 Turbo, Gemini 1.5 Pro, and Claude3
    Opus) may no longer accurately obtain all of them. Therefore, adding more many-shot
    in-context examples is probably not captured by LLMs for learning as a reference
    to judge. However, the many-shot regime and Counting-Stars are substantially different,
    so there is no noise from "haystack". Hence, we set the maximum number of the
    in-context examples to 128, i.e., 128-shots.
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments, we use GPT-4o with public API access, and the specific endpoint
    is “gpt-4o-2024-05-13”. In addition, for the convenience of introduction, when
    comparing zero-shot and many-shot regimes, we uniformly refer to the few-shot
    and many-shot regimes as the many-shot regime.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8e03fdbe463a16f349b668ad0a10260f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Win rate compared between zero-shot and many-shot regimes. Here,
    A and A^† represent the results of the GPT-4o judge in the many-shot regime, and
    B and B^† represent the results of the GPT-4o judge in the zero-shot regime. The
    designed prompts are shown in Table [2](#A1.T2 "Table 2 ‣ A.3 Dataset ‣ Appendix
    A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See
    More, Judge Better!"). Notice that "[[C]]" denotes a tie.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca86ccba9d7dcfe0bb3f594f20a5d252.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparison evaluation results after mitigating the biases via different
    approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Consistency Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We investigate the consistency between different versions of evaluation results
    generated by the GPT-4o judge. Here, the single answer grading evaluation results
    can be used to compare the consistency between different versions. As shown in
    Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!"), the bar corresponding
    to "$2^{n}$" on the x-axis represents the number of samples with consistent and
    inconsistent ratings in comparing evaluation results obtained twice using GPT-4o-as-a-Judge
    in the many-shot Reinforced ICL regime. From the results, consistency improves
    as we increase the number of shots provided as in-context examples during inference.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies Wang et al. ([2023b](#bib.bib22)); Zheng et al. ([2023](#bib.bib27));
    Chen et al. ([2024](#bib.bib6)) have shown that the judgment performance of GPT-4
    as a judge is highly in agreement with those of humans. However, both human and
    LLM judgments are subject to potential biases Chen et al. ([2024](#bib.bib6)).
    By analyzing prior studies, we suppose it unnecessary to obtain utterly accurate
    evaluation results (because this is difficult) by using LLMs as judges. It is
    only required to ensure that the evaluation results are highly consistent multiple
    times so that the single answer grading of GPT-4o-as-a-Judge may be effective.
    From all results in this work, we find that the many-shot ICL examples help the
    judgment of LLMs more consistently, which is essential. We consider that the main
    reason may be that the many-shot in-context examples mitigate the potential biases
    of the GPT-4o judge in the zero-shot regime.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, we also implemented experiments in the many-shot Unsupervised ICL,
    but the results show that this regime may be unsuitable in the scenario of acting
    as a judge because the problem of each sample is the same, but the scored questions
    and answers are different. From the results in Figure [4](#S2.F4 "Figure 4 ‣ 2.3.2
    Pairwise Comparison ‣ 2.3 Two GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context
    LLM Judge ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See
    More, Judge Better!"), Figure [5](#S2.F5 "Figure 5 ‣ 2.3.2 Pairwise Comparison
    ‣ 2.3 Two GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context LLM Judge
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!"), and Figure [6](#S2.F6 "Figure 6 ‣ 2.3.2 Pairwise Comparison ‣ 2.3 Two
    GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context LLM Judge ‣ Can Many-Shot
    In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!"), it
    can be seen that the consistency corresponds to the number of appended in-context
    samples with model-generated rationales but nearly not to the number of in-context
    examples without model-generated rationales.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, both the few-shot and many-shot regimes are sensitive to the selection
    and order of in-context examples. In the experimental setting of this paper, almost
    no test data will have the same in-context examples. Even under this condition,
    the evaluation results also show a high consistency, demonstrating the effectiveness
    of many-shot ICL in helping GPT-4o-as-a-Judge.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Quality Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the consistency evaluation, an important question arises about: Does
    a high consistency refer to high-quality judgments? Around this issue, we pair-wise
    compare model-generated judgment rationales between zero-shot and many-shot regimes
    using the designed prompts shown in Table [2](#A1.T2 "Table 2 ‣ A.3 Dataset ‣
    Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context LLM
    Judges? See More, Judge Better!").'
  prefs: []
  type: TYPE_NORMAL
- en: From Figure [7](#S3.F7 "Figure 7 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") Compare(A, B), we can find that the evaluations obtained in the many-shot
    regime are significantly better than those in the zero-shot regime. However, as
    mentioned before, the positional bias of GPT-4o-as-a-Judge may cause these results,
    so we performed a second comparison by swapping the positions. As shown in Figure [7](#S3.F7
    "Figure 7 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!") Compare(B, A),
    we observe that as in-context examples increase, the judgment results in the many-shot
    regime gradually turn around, that is, a higher winning rate. To obtain fairness
    results, we integrate the above two results (Compare(A, B) and Compare(B, A)),
    as shown in Figure [8](#S3.F8 "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!"). It can be seen that after mitigating the positional bias, the judgment
    quality in the many-shot regime is still better than that in the zero-shot regime.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Revealing Symbol Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inspired by Song et al. ([2024](#bib.bib19)), LLMs revealed a preference for
    specific symbols under stress testing (details are described in the Appendix).
    Therefore, an interesting question arises about: Does the GPT-4o prefer to choose
    the answer with the symbol A rather than B? To verify this conjecture, we swap
    the answers corresponding to symbols A and B, as shown in Figure [2](#A1.T2 "Table
    2 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help
    Long-Context LLM Judges? See More, Judge Better!"). From Figure [7](#S3.F7 "Figure
    7 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!") Compare(A^†, B^†) and
    Compare(B^†, A^†), it can be seen that the results are different from the above
    experiments. Actually, the results of Compare(A, B) and Compare(A^†, B^†) should
    be similar, and the results of Compare(B, A) and Compare(B^†, A^†) should be similar.
    This phenomenon shows that symbol bias does exist when adopting GPT-4o-as-a-Judge.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent research Wang et al. ([2023b](#bib.bib22)) integrates the evaluation
    results of Compare(A, B) and Compare(B, A) to mitigate the positional bias, which
    motivates us to incorporate the evaluation results of Compare(A^†, B^†) and Compare(B^†,
    A^†) to reduce symbol bias. As presented in Figure [8](#S3.F8 "Figure 8 ‣ 3.1
    Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context Learning Help
    Long-Context LLM Judges? See More, Judge Better!"), it can be seen that as in-context
    examples increase, the higher the win rate of the many-shot regime, which further
    verifies the effectiveness of the many-shot regime in helping GPT-4o-as-a-Judge.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there is an interesting hypothesis that the phenomenon of the positional
    bias we think is not actually caused by the positions of answers, instead of the
    symbols of answers. In other words, there may be no positional bias in using LLMs
    as judges in pairwise comparison but rather the symbol bias. The experimental
    results from Figure [8](#S3.F8 "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") Compare(A, B) and Compare(A^†, B^†) can demonstrate this hypothesis
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have exhibited remarkable general generation capabilities, positioning
    themselves as powerful assistants Zhao et al. ([2023](#bib.bib26)); OpenAI ([2023](#bib.bib16)).
    With the rapid progression of LLMs, evaluating their proficiency in adhering to
    human instructions is imperative. Given the advanced capabilities of LLMs, researchers
    have begun adopting these models to judge the performance of LLMs in following
    human instructions Koo et al. ([2023](#bib.bib11)); Liusie et al. ([2023](#bib.bib14));
    Liu et al. ([2023](#bib.bib13)); Zhu et al. ([2023](#bib.bib28)); Lu et al. ([2023](#bib.bib15));
    Fu et al. ([2023b](#bib.bib10)); Zheng et al. ([2023](#bib.bib27)); Wang et al.
    ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6)). Notably, the evaluation
    paradigm introduced by Zheng et al. ([2023](#bib.bib27)) has gained widespread
    adoption. However, LLMs as judges are revealed to have potential biases Wang et al.
    ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6)), which leads to higher
    uncertainty and inconsistency during the evaluation using LLMs, questioning the
    validity of LLM-as-a-Judge.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we mainly study whether many-shot ICL helps long-context LLMs
    as judges, such as GPT-4o. To this end, we designed several experimental prompts,
    e.g., many-shot Reinforced and Unsupervised ICL. Experiments show that many-shot
    ICL can help the GPT-4o judge improve the consistency and quality of evaluation.
    Meanwhile, we also revealed another bias when LLMs act as judges, symbol bias,
    and further proposed mitigation approaches. In summary, we preliminarily verified
    the effectiveness of using many-shot ICL to assist the GPT-4o judge in single
    answer grading.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering the trade-off between costs and benefits, we do not verify too many
    in-context examples in the experiments, such as thousands of examples. Combining
    Figures [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!") and Figure [8](#S3.F8
    "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!"), it is not difficult
    to see that when the number of in-context examples increases to 64 and 128, although
    the consistency no longer improves, the evaluation quality has increased significantly.
    In addition, we consider that using GPT-4o-as-a-Judge in the many-shot regime
    is another evolution of the weak-to-strong strategy Burns et al. ([2023](#bib.bib4)),
    which uses many zero-shot judgment results to help GPT-4o generate a better one.
    As the long-context capabilities of LLMs improve, adding more in-context examples
    may reveal more valuable phenomena for studying long-context LLMs as judges in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agarwal et al. (2024) Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet,
    Stephanie C. Y. Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes,
    Eric Chu, Feryal M. P. Behbahani, Aleksandra Faust, and Hugo Larochelle. 2024.
    [Many-shot in-context learning](https://doi.org/10.48550/ARXIV.2404.11018). *CoRR*,
    abs/2404.11018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anthropic (2024) Anthropic. 2024. The claude 3 model family: Opus, sonnet,
    haiku. *Technical Report*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burns et al. (2023) Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen
    Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar,
    Jan Leike, Ilya Sutskever, and Jeff Wu. 2023. [Weak-to-strong generalization:
    Eliciting strong capabilities with weak supervision](https://doi.org/10.48550/ARXIV.2312.09390).
    *CoRR*, abs/2312.09390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu,
    Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,
    Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2023. [A survey on evaluation
    of large language models](https://doi.org/10.48550/ARXIV.2307.03109). *CoRR*,
    abs/2307.03109.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2024) Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang,
    and Benyou Wang. 2024. [Humans or llms as the judge? A study on judgement biases](https://doi.org/10.48550/ARXIV.2402.10669).
    *CoRR*, abs/2402.10669.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. [Palm: Scaling language
    modeling with pathways](http://jmlr.org/papers/v24/22-1144.html). *J. Mach. Learn.
    Res.*, 24:240:1–240:113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. [Training verifiers to solve
    math word problems](https://arxiv.org/abs/2110.14168). *CoRR*, abs/2110.14168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023a) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023a. [Gptscore: Evaluate as you desire](https://doi.org/10.48550/ARXIV.2302.04166).
    *CoRR*, abs/2302.04166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023b) Xue-Yong Fu, Md. Tahmid Rahman Laskar, Cheng Chen, and Shashi Bhushan
    TN. 2023b. [Are large language models reliable judges? A study on the factuality
    evaluation capabilities of llms](https://doi.org/10.48550/ARXIV.2311.00681). *CoRR*,
    abs/2311.00681.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koo et al. (2023) Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung
    Kim, and Dongyeop Kang. 2023. [Benchmarking cognitive biases in large language
    models as evaluators](https://doi.org/10.48550/ARXIV.2309.17012). *CoRR*, abs/2309.17012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang,
    Zhiyong Wu, and Lingpeng Kong. 2023. [In-context learning with many demonstration
    examples](https://doi.org/10.48550/ARXIV.2302.04931). *CoRR*, abs/2302.04931.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023. [Llms
    as narcissistic evaluators: When ego inflates evaluation scores](https://doi.org/10.48550/ARXIV.2311.09766).
    *CoRR*, abs/2311.09766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liusie et al. (2023) Adian Liusie, Potsawee Manakul, and Mark J. F. Gales. 2023.
    [Zero-shot NLG evaluation through pairware comparisons with llms](https://doi.org/10.48550/ARXIV.2307.07889).
    *CoRR*, abs/2307.07889.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng
    Tao. 2023. [Error analysis prompting enables human-like translation evaluation
    in large language models: A case study on chatgpt](https://doi.org/10.48550/ARXIV.2303.13809).
    *CoRR*, abs/2303.13809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [GPT-4 technical report](https://doi.org/10.48550/ARXIV.2303.08774).
    *CoRR*, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. 2023. [Instruction tuning with GPT-4](https://doi.org/10.48550/ARXIV.2304.03277).
    *CoRR*, abs/2304.03277.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
    Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,
    Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin
    Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael
    Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk,
    Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens
    Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher,
    Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri,
    Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener,
    and et al. 2024. [Gemini 1.5: Unlocking multimodal understanding across millions
    of tokens of context](https://doi.org/10.48550/ARXIV.2403.05530). *CoRR*, abs/2403.05530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2024) Mingyang Song, Mao Zheng, and Xuan Luo. 2024. [Counting-stars:
    A multi-evidence, position-aware, and scalable benchmark for evaluating long-context
    large language models](https://arxiv.org/abs/2403.11802). *Preprint*, arXiv:2403.11802.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(20) Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. [Language
    models don’t always say what they think: Unfaithful explanations in chain-of-thought
    prompting](http://papers.nips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. [Is chatgpt a good NLG evaluator?
    A preliminary study](https://doi.org/10.48550/ARXIV.2303.04048). *CoRR*, abs/2303.04048.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. [Large language models
    are not fair evaluators](https://doi.org/10.48550/ARXIV.2305.17926). *CoRR*, abs/2305.17926.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang
    Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun
    Zhang, and Yue Zhang. 2023c. [Pandalm: An automatic evaluation benchmark for LLM
    instruction tuning optimization](https://doi.org/10.48550/ARXIV.2306.05087). *CoRR*,
    abs/2306.05087.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu and Aji (2023) Minghao Wu and Alham Fikri Aji. 2023. [Style over substance:
    Evaluation biases for large language models](https://doi.org/10.48550/ARXIV.2307.03025).
    *CoRR*, abs/2307.03025.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023.
    [Baize: An open-source chat model with parameter-efficient tuning on self-chat
    data](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.385). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023,
    Singapore, December 6-10, 2023*, pages 6268–6278\. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. [A survey
    of large language models](https://doi.org/10.48550/ARXIV.2303.18223). *CoRR*,
    abs/2303.18223.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E Gonzalez, and Ion Stoica. 2023. [Judging llm-as-a-judge with mt-bench
    and chatbot arena](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf).
    In *Advances in Neural Information Processing Systems*, volume 36, pages 46595–46623.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. [Judgelm:
    Fine-tuned large language models are scalable judges](https://doi.org/10.48550/ARXIV.2310.17631).
    *CoRR*, abs/2310.17631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Many-Shot Unsupervised ICL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to Agarwal et al. ([2024](#bib.bib1)), the default prompt of many-shot
    Unsupervised ICL is composed of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A preamble, such as, “You will be provided questions similar to the ones below:”.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of unsolved inputs or problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few-shot prompt with outputs for the desired output format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To require the GPT-4o judge outputs the desired output format, we append 4-shot
    in-context examples with answers for many-shot Unsupervised ICL. The context length
    of a single test sample with $K$-shot in-context examples is presented in Table [4](#A1.T4
    "Table 4 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!"). As mentioned before,
    considering the trade-off between resources and effects, in the experiments of
    this paper, we do not introduce too many in-context examples, such as thousands
    of examples.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Symbol Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous work Song et al. ([2024](#bib.bib19)) has found that when an LLM is
    stress-tested, it is easy to output some wrong information, which may be an increasing
    array related to the test data or an English alphabet sequence starting with "A".
    This phenomenon shows that LLMs favor answers with the symbol "A" rather than
    the symbol "B" (or the symbol "1" instead of the symbol "2").
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GSM8K¹¹1[https://huggingface.co/datasets/openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k)
    is previously introduced by Cobbe et al. ([2021](#bib.bib8)), which comprises
    8.5K high-quality grade school math problems meticulously crafted by human problem
    writers. This dataset is divided into 7.5K training and 1K test problems. Specifically,
    each problem typically requires between 2 to 8 steps to solve, primarily involving
    a sequence of elementary calculations using basic arithmetic operations (addition,
    subtraction, multiplication, and division). The problems are designed so that
    a proficient middle school student can solve each one. Furthermore, the problem-solving
    task requires models to solve problems with model-generated rationales, which
    may be challenging to evaluate. We use LLaMA3-70B²²2[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)
    to create model-generated rationales for GSM8K.
  prefs: []
  type: TYPE_NORMAL
- en: '| Now, I am going to give you a series of demonstrations of Problems and Solutions.
    When you respond, respond only with the Solution of the final Problem, thinking
    step by step. Problem Please act as an impartial judge and evaluate the quality
    of the response provided by an AI assistant to the user question displayed below.
    Your evaluation should consider factors such as the helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of the response. Please rate the response
    on a scale of 1 to 10 by strictly following this JSON format: {"rating":"", "reason":""}.
    The "rating" should be as objective as possible. The "reason" denotes a comprehensive
    explanation of your rating, which should consider factors such as helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of the response should
    also be considered. Only return the JSON results and do not give any explanation.
    Question {question1} Response {response1} Solution {solution1} — Problem Please
    act as an impartial judge and evaluate the quality of the response provided by
    an AI assistant to the user question displayed below. Your evaluation should consider
    factors such as the helpfulness, relevance, accuracy, depth, creativity, and level
    of detail of the response. Please rate the response on a scale of 1 to 10 by strictly
    following this JSON format: {"rating":"", "reason":""}. The "rating" should be
    as objective as possible. The "reason" denotes a comprehensive explanation of
    your rating, which should consider factors such as helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of the response should also be considered.
    Only return the JSON results and do not give any explanation. Question {question2}
    Response {response2} Solution {solution2} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question3}
    Response {response3} Solution {solution3} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question4}
    Response {response4} Solution {solution4} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {final_question}
    Response {final_response} Solution |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Example Prompt with 4-shot in-context examples used for Reinforced
    ICL.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Please act as an impartial judge and evaluate the quality of the responses
    provided by two AI assistants to the user question displayed below. You should
    choose the assistant that follows the user’s instructions and answers the user’s
    question better. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their responses.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Avoid any position biases and ensure that the order in which the responses were
    presented does not influence your decision. Do not allow the length of the responses
    to influence your evaluation. Do not favor certain names of the assistants. Be
    as objective as possible. After providing your explanation, output your final
    verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]"
    if assistant B is better, and "[[C]]" for a tie. |'
  prefs: []
  type: TYPE_TB
- en: '| Problem |'
  prefs: []
  type: TYPE_TB
- en: '| Please act as an impartial judge and evaluate the quality of the response
    provided by an AI assistant to the user question displayed below. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of the response. Please rate the response on a scale of 1
    to 10 by strictly following this JSON format: {"rating":"", "reason":""}. The
    "rating" should be as objective as possible. The "reason" denotes a comprehensive
    explanation of your rating, which should consider factors such as helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of the response should
    also be considered. Only return the JSON results and do not give any explanation.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Question |'
  prefs: []
  type: TYPE_TB
- en: '| {question} |'
  prefs: []
  type: TYPE_TB
- en: '| Response |'
  prefs: []
  type: TYPE_TB
- en: '| {response} |  | The Assistant A’s Answer | The Assistant A’s Answer | The
    Assistant B’s Answer | The Assistant A’s Answer |'
  prefs: []
  type: TYPE_TB
- en: '| {Answer-A} | {Answer-B} | {Answer-A} | {Answer-B} |'
  prefs: []
  type: TYPE_TB
- en: '| The Assistant B’s Answer | The Assistant B’s Answer | The Assistant A’s Answer
    | The Assistant B’s Answer |'
  prefs: []
  type: TYPE_TB
- en: '| {Answer-B} | {Answer-A} | {Answer-B} | {Answer-A} |'
  prefs: []
  type: TYPE_TB
- en: '| Compare(A, B) | Compare(B, A) | Compare(A^†, B^†) | Compare(B^†, A^†) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The prompts used for pairwise comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt(A). The zero-shot prompt for single answer grading in Zheng et al.
    ([2023](#bib.bib27)). |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Please act as an impartial judge and evaluate the quality of the response
    provided by an AI assistant to the user question displayed below. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of the response. Begin your evaluation by providing a short
    explanation. Be as objective as possible. After providing your explanation, please
    rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]",
    for example: "Rating: [[5]]". |'
  prefs: []
  type: TYPE_TB
- en: '| Question |'
  prefs: []
  type: TYPE_TB
- en: '| {question} |'
  prefs: []
  type: TYPE_TB
- en: '| Response |'
  prefs: []
  type: TYPE_TB
- en: '| {response} |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt(B). The zero-shot prompt for single answer grading in this paper.
    Please act as an impartial judge and evaluate the quality of the response provided
    by an AI assistant to the user question displayed below. Your evaluation should
    consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of the response. Please rate the response on a scale of 1
    to 10 by strictly following this JSON format: {"rating":"", "reason":""}. The
    "rating" should be as objective as possible. The "reason" denotes a comprehensive
    explanation of your rating, which should consider factors such as helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of the response should
    also be considered. Only return the JSON results and do not give any explanation.
    Question {question} Response {response} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Two versions of zero-shot prompts are used for evaluation in this
    paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $2^{0}$ | $2^{4}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Average Context Length in Tokens of A Single Test Sample |'
  prefs: []
  type: TYPE_TB
- en: '| Reinforced ICL | 0.7K | 1.1K | 2.2K | 3.6K | 6.5K | 13.6K | 26.3K | 50.7K
    |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised ICL (Append 4-shots) | 2.2K | 2.6K | 3.1K | 4.6K | 6.9K | 11.7K
    | 20.9K | 40.8K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Context length in tokens for $K$-shot Reinforced and Unsupervised
    ICL.'
  prefs: []
  type: TYPE_NORMAL
- en: '| You will be provided Problems similar to the ones below: Problem Please act
    as an impartial judge and evaluate the quality of the response provided by an
    AI assistant to the user question displayed below. Your evaluation should consider
    factors such as the helpfulness, relevance, accuracy, depth, creativity, and level
    of detail of the response. Please rate the response on a scale of 1 to 10 by strictly
    following this JSON format: {"rating":"", "reason":""}. The "rating" should be
    as objective as possible. The "reason" denotes a comprehensive explanation of
    your rating, which should consider factors such as helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of the response should also be considered.
    Only return the JSON results and do not give any explanation. Question {question1}
    Response {response1} Now, I am going to give you a series of demonstrations of
    Problems and Solutions. When you respond, respond only with the Solution of the
    final Problem, thinking step by step. Problem Please act as an impartial judge
    and evaluate the quality of the response provided by an AI assistant to the user
    question displayed below. Your evaluation should consider factors such as the
    helpfulness, relevance, accuracy, depth, creativity, and level of detail of the
    response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question1}
    Response {response1} Solution {solution1} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question2}
    Response {response2} Solution {solution2} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question3}
    Response {response3} Solution {solution3} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question4}
    Response {response4} Solution {solution4} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {final_question}
    Response {final_response} Solution |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Example Prompt with 1-shot in-context examples used for Unsupervised
    ICL.'
  prefs: []
  type: TYPE_NORMAL
