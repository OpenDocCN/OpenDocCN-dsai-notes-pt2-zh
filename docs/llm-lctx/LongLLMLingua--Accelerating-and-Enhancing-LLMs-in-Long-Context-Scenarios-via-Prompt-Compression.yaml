- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:04:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:04:42'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LongLLMLingua: 通过提示压缩加速和增强LLMs在长上下文场景中的性能'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
- en: Huiqiang Jiang, Qianhui Wu, Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 慧强 蒋, 乾辉 吴, 旭芳 罗,
- en: Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 东升 李, Chin-Yew Lin, 玉清 杨, 丽丽 邱
- en: Microsoft Corporation {hjiang,qianhuiwu,xufang.luo}@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司 {hjiang,qianhuiwu,xufang.luo}@microsoft.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In long context scenarios, large language models (LLMs) face three main challenges:
    higher computational/financial cost, longer latency, and inferior performance.
    Some studies reveal that the performance of LLMs depends on both the density and
    the position of the key information (question relevant) in the input prompt. Inspired
    by these findings, we propose LongLLMLingua for prompt compression towards improving
    LLMs’ perception of the key information to simultaneously address the three challenges.
    We conduct evaluation on a wide range of long context scenarios including single-/multi-document
    QA, few-shot learning, summarization, synthetic tasks, and code completion. The
    experimental results show that LongLLMLingua compressed prompt can derive higher
    performance with much less cost. The latency of the end-to-end system is also
    reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance
    boost of up to 17.1% over the original prompt with $\sim$10k tokens at a compression
    rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.
    ¹¹1Our code is available at [https://aka.ms/LLMLingua](https://aka.ms/LLMLingua).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在长上下文场景中，大型语言模型（LLMs）面临三个主要挑战：更高的计算/经济成本、更长的延迟和较差的性能。一些研究表明，LLMs的性能依赖于输入提示中关键信息（与问题相关）的密度和位置。受这些发现的启发，我们提出了LongLLMLingua用于提示压缩，以提高LLMs对关键信息的感知，从而同时解决这三个挑战。我们在包括单文档/多文档问答、少样本学习、摘要生成、合成任务和代码补全在内的广泛长上下文场景中进行了评估。实验结果表明，LongLLMLingua压缩的提示能够在成本大幅降低的情况下获得更高的性能。端到端系统的延迟也有所减少。例如，在NaturalQuestions基准测试中，LongLLMLingua在压缩率为2x-10x的情况下，比原始提示提高了高达17.1%的性能，同时可以将端到端延迟提高1.4x-3.8x。¹¹1我们的代码可在[https://aka.ms/LLMLingua](https://aka.ms/LLMLingua)获取。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: ChatGPT and other large language models (LLMs) have revolutionized user-oriented
    language technologies and are serving as crucial components in more and more applications.
    Carefully designing prompts is necessary to achieve better performance in specific
    downstream tasks. The commonly used technologies such as In-Context Learning (ICL) (Dong
    et al., [2023](#bib.bib8)), Retrieval Augment Generation (RAG) (Lewis et al.,
    [2020](#bib.bib17)), and Agent (Park et al., [2023](#bib.bib24)) are driving prompts
    to be increasingly longer, even reaching thousands of tokens. Scenarios such as
    multi-document question answering, code completion, and document summarization
    also necessitate the processing of long contexts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT和其他大型语言模型（LLMs）已经彻底改变了面向用户的语言技术，并且在越来越多的应用中扮演着重要角色。精心设计提示是实现特定下游任务更好性能的必要条件。常用技术如上下文学习（ICL）（Dong
    et al., [2023](#bib.bib8)）、检索增强生成（RAG）（Lewis et al., [2020](#bib.bib17)）和智能体（Park
    et al., [2023](#bib.bib24)）正在推动提示变得越来越长，甚至达到数千个标记。多文档问答、代码补全和文档摘要等场景也需要处理长上下文。
- en: 'There are three main challenges when LLMs are used in long context scenarios:
    (1) The higher computational and financial cost required to run these models or
    to call APIs from companies providing LLM services. This can be a significant
    barrier for individuals or smaller organizations with limited resources. (2) The
    longer latency associated with LLMs, which can cause delays in generating responses
    or predictions and is particularly problematic in real-time scenarios where users
    expect quick and accurate responses. (3) The inferior performance caused by the
    extended window size of LLMs (Xiong et al., [2023](#bib.bib33)), and the low density
    as well as the less sensitive position of the question-relevant key information
    in the prompt. Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    shows that LLMs’ performance in downstream tasks may decrease as the noisy information
    in the prompt increases (Shi et al., [2023](#bib.bib29)). Moreover, the purple
    curve in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    indicates that LLMs’ ability to capture the relevant information depends on their
    positions in the prompt (Liu et al., [2023](#bib.bib20)): they achieve the highest
    performance when relevant information occurs at the beginning or end of the input
    context, and significantly degrades if relevant information is located in the
    middle of long contexts.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '当 LLM 被应用于长上下文场景时，主要面临三个挑战：(1) 运行这些模型或调用提供 LLM 服务的公司 API 所需的更高计算和财务成本。这对于资源有限的个人或较小的组织来说可能是一个重要障碍。
    (2) 与 LLM 相关的更长延迟，这可能导致生成响应或预测的延迟，并在需要快速准确响应的实时场景中特别成问题。 (3) 由于 LLM 的扩展窗口大小（Xiong
    等，[2023](#bib.bib33)）以及提示中与问题相关的关键信息的低密度和较低的敏感位置导致的性能下降。图 [1a](#S1.F1.sf1 "图 1
    ‣ 1 介绍 ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的 LLM") 显示，LLM 在下游任务中的表现可能会随着提示中的噪声信息增加而下降（Shi
    等，[2023](#bib.bib29)）。此外，图 [1b](#S1.F1.sf2 "图 1 ‣ 1 介绍 ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的
    LLM") 中的紫色曲线表明，LLM 捕捉相关信息的能力取决于其在提示中的位置（Liu 等，[2023](#bib.bib20)）：当相关信息出现在输入上下文的开始或结束时，它们的表现最佳，如果相关信息位于长上下文的中间则显著下降。'
- en: '![Refer to caption](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
- en: (a) Performance v.s. Document Number
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 性能 vs. 文档数量
- en: '![Refer to caption](img/808a8cc98f1873ee2c344faffaffa60d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/808a8cc98f1873ee2c344faffaffa60d.png)'
- en: (b) Performance v.s. Key Information Position
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 性能 vs. 关键信息位置
- en: 'Figure 1: (a) LLMs’ performance in downstream tasks may decrease as the noisy
    information in the prompt increases. In this case, we keep $k$ implies more noise
    introduced into the prompt. To improve the key information density in the prompt,
    we present question-aware coarse-to-fine compression. (b) LLMs’ ability to capture
    the relevant information depends on their positions in the prompt. To reduce information
    loss in the middle, we introduce a document reordering mechanism.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: (a) 当提示中的噪声信息增加时，LLM 在下游任务中的表现可能会下降。在这种情况下，我们保持$k$意味着提示中引入了更多噪声。为了提高提示中的关键信息密度，我们提出了基于问题感知的粗到细压缩方法。
    (b) LLM 捕捉相关信息的能力取决于其在提示中的位置。为了减少中间的信息丢失，我们引入了文档重排序机制。'
- en: 'Inspired by these observations, we propose LongLLMLingua to address the three
    challenges. Specifically, we use the advanced while efficient LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) as our backbone framework for prompt compression
    to address the first two challenges, i.e., reduce cost and latency. However, in
    the case of long contexts, the distribution of question-relevant key information
    in the prompt is generally sparse. Existing prompt compression methods like LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) and Selective-Context (Li, [2023](#bib.bib19)) that
    do not consider the content of the question during compression may retain too
    much noisy information in the compressed results, leading to inferior performance.
    In this paper, LongLLMLingua is designed to enhance LLM’s perception of key information
    (relevant to the question) in the prompt, so that the third challenge of inferior
    performance in long context scenarios could be addressed. Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression") is an example. The underlying
    principle of LongLLMLingua is that small language models are inherently capable
    of capturing the distribution of key information relevant to a given question.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这些观察结果的启发，我们提出了 LongLLMLingua 来解决这三个挑战。具体而言，我们使用先进且高效的 LLMLingua （Jiang 等，[2023a](#bib.bib13)）作为我们的主框架进行提示压缩，以解决前两个挑战，即降低成本和延迟。然而，在长上下文的情况下，提示中与问题相关的关键信息的分布通常是稀疏的。现有的提示压缩方法如
    LLMLingua （Jiang 等，[2023a](#bib.bib13)）和 Selective-Context （Li，[2023](#bib.bib19)）在压缩过程中不考虑问题的内容，可能会在压缩结果中保留过多噪声信息，导致性能下降。在本文中，LongLLMLingua
    旨在增强 LLM 对提示中与问题相关的关键信息的感知，从而解决长上下文场景中性能下降的第三个挑战。图 [1b](#S1.F1.sf2 "图1 ‣ 1 引言
    ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLM") 是一个例子。LongLLMLingua 的基本原理是小型语言模型本质上能够捕捉与给定问题相关的关键信息的分布。
- en: 'Our main contributions are five-fold: (1) We propose a question-aware coarse-to-fine
    compression method to improve the key information density in the prompt (Sec.
    [4.1](#S4.SS1 "4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")); (2) We introduce a document reordering mechanism to reduce
    information loss in the middle. (Sec. [4.2](#S4.SS2 "4.2 How to reduce information
    loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression")); (3) We present dynamic
    compression ratios to bridge the coarse-grained compression and fine-grained compression
    for adaptive granular control (Sec. [4.3](#S4.SS3 "4.3 How to achieve adaptive
    granular control during compression? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression")); (4) We
    propose a post-compression subsequence recovery strategy to improve the integrity
    of the key information ([4.4](#S4.SS4 "4.4 How to improve the integrity of key
    information? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression")). (5) We evaluate LongLLMLingua
    on three benchmarks, i.e., NaturalQuestions (Liu et al., [2023](#bib.bib20)),
    LongBench (Bai et al., [2023](#bib.bib1)), and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)).
    Experimental results demonstrate that compared with original prompts, LongLLMLingua
    compressed prompts can achieve higher performance with much lower costs. The latency
    of the end-to-end system is also reduced.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的主要贡献有五点：（1）我们提出了一种基于问题的粗到细压缩方法，以提高提示中的关键信息密度（见 [4.1](#S4.SS1 "4.1 如何提高提示中的关键信息密度？
    ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（2）我们引入了一种文档重排机制，以减少中间的信息丢失（见
    [4.2](#S4.SS2 "4.2 如何减少中间的信息丢失？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（3）我们提出了动态压缩比，以在粗粒度压缩和细粒度压缩之间建立桥梁，实现自适应粒度控制（见
    [4.3](#S4.SS3 "4.3 如何在压缩过程中实现自适应粒度控制？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（4）我们提出了一种后压缩子序列恢复策略，以提高关键信息的完整性（见
    [4.4](#S4.SS4 "4.4 如何提高关键信息的完整性？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（5）我们在三个基准上评估了LongLLMLingua，即NaturalQuestions（刘等，[2023](#bib.bib20)），LongBench（白等，[2023](#bib.bib1)），和ZeroSCROLLS（Shaham等，[2023](#bib.bib28)）。实验结果表明，与原始提示相比，LongLLMLingua压缩提示可以实现更高的性能，成本却低得多。端到端系统的延迟也有所减少。'
- en: 2 Problem Formulation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题定义
- en: 'Following LLMLingua (Jiang et al., [2023a](#bib.bib13)), we use $\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$
    can be any additional materials that users append to the prompt to get a better
    response from LLMs for $\mathbf{x}^{\text{que}}$. The objective of a prompt compression
    system can be formulated as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参考LLMLingua（蒋等，[2023a](#bib.bib13)），我们使用$\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$，这些可以是用户附加到提示中的任何额外材料，以获得更好的LLMs响应。提示压缩系统的目标可以表述为：
- en: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
- en: where $\widetilde{\mathbf{x}}$ and $\widetilde{\mathbf{y}}$ for joint optimization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\widetilde{\mathbf{x}}$和$\widetilde{\mathbf{y}}$用于联合优化。
- en: '3 Preliminary: LLMLingua'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步研究：LLMLingua
- en: LLMLingua (Jiang et al., [2023a](#bib.bib13)) uses a small language model $\mathcal{M}_{S}$
    used for prompt compression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua（蒋等，[2023a](#bib.bib13)）使用一个小型语言模型$\mathcal{M}_{S}$用于提示压缩。
- en: 4 LongLLMLingua
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LongLLMLingua
- en: '![Refer to caption](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
- en: 'Figure 2: Framework of LongLLMLingua. Gray Italic content: As in LLMLingua.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LongLLMLingua的框架。灰色斜体内容：如LLMLingua所示。
- en: LongLLMLingua is developed upon the framework of LLMLingua towards prompt compression
    in long context scenarios. The primary challenge in long context scenarios is
    how to enhance LLM’s perception of key information relevant to the question in
    the prompt. LongLLMLingua addresses this challenge from three perspectives, and
    further applies a subsequence recovery strategy to improve the accuracy and reliability
    of the information provided to users. We elaborate on each component in this section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LongLLMLingua是在LLMLingua框架的基础上开发的，旨在长上下文场景中的提示压缩。长上下文场景中的主要挑战是如何增强LLM对提示中与问题相关的关键信息的感知。LongLLMLingua从三个方面解决这一挑战，并进一步应用子序列恢复策略，以提高提供给用户的信息的准确性和可靠性。本节将详细阐述每个组成部分。
- en: 4.1 How to improve key information density in the prompt?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 如何提高提示中的关键信息密度？
- en: Question-Aware Coarse-Grained Compression
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题感知粗粒度压缩
- en: In coarse-grained compression, we aim to figure out a metric $r_{k}$ as the
    intermediate compressed results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在粗粒度压缩中，我们旨在找出一个指标 $r_{k}$ 作为中间压缩结果。
- en: 'LLMLingua uses document-level perplexity to represent the importance of documents:
    $r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$
    and instead become noise, reducing key information density in the compressed results
    and bringing difficulties for LLM to output correct answers. As shown in Figure [3a](#S4.F3.sf1
    "In Figure 3 ‣ Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve
    key information density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"), the recall@16
    of LLMLingua only reaches 50%, indicating its incompetence in retaining key information
    during compression.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua使用文档级困惑度来表示文档的重要性：$r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log
    p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$，这会变成噪声，降低压缩结果中的关键信息密度，并使LLM输出正确答案变得困难。如图[3a](#S4.F3.sf1
    "在图3中 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")所示，LLMLingua的recall@16仅达到50%，这表明其在压缩过程中保留关键信息的能力不足。
- en: Retrieval-based methods are also feasible here. We can use $\mathbf{x}^{\text{que}}$75%
    accuracy in recall@5, which implies that the final accuracy upper bound of LLMs
    with 4x compression is only 75%.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的方法在这里也是可行的。我们可以在 recall@5 中达到 $\mathbf{x}^{\text{que}}$ 的75%准确率，这意味着具有4倍压缩的LLMs的最终准确率上限仅为75%。
- en: '![Refer to caption](img/2e6955b6f2f232c285024fef939bddac.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e6955b6f2f232c285024fef939bddac.png)'
- en: (a) Recall Distribution
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 召回分布
- en: '![Refer to caption](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
- en: (b) Perplexity Distribution
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 困惑度分布
- en: 'Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset.
    (b) Comparison between perplexities and contrastive perplexities of tokens in
    the prompt from Multi-documemnt QA dataset. The document with the ground truth
    is located on the left side of the dashed line.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3： (a) 在NaturalQuestions多文档问答数据集上的召回对比。(b) Multi-documemnt QA数据集中提示中令牌的困惑度和对比困惑度的比较。地面真实值的文档位于虚线的左侧。
- en: 'One approach to improve key information density in the compressed results is
    to calculate document-level perplexity conditioned on the question $\mathbf{x}^{\text{que}}$.
    It can be regarded as a regularization term that mitigates the impact of hallucinations.
    This can be formulated as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提高压缩结果中关键信息密度的一种方法是计算以问题 $\mathbf{x}^{\text{que}}$ 为条件的文档级困惑度。这可以视为一种正则化项，用以缓解幻觉的影响。这可以表示为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $x^{\text{que},\text{restrict}}_{i}$ in the number of tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x^{\text{que},\text{restrict}}_{i}$ 为令牌的数量。
- en: 'Figure [3a](#S4.F3.sf1 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") demonstrates that our coarse-level compression approach achieves
    the highest recall with different numbers of retained documents, suggesting that
    it preserves the most key information from the documents $(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$
    in the compressed results.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3a](#S4.F3.sf1 "在图3 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣
    LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")展示了我们的粗粒度压缩方法在保留不同数量的文档时达到了最高的召回率，表明它在压缩结果中保留了来自文档$(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$的最关键信息。
- en: Question-Aware Fine-Grained Compression
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题感知细粒度压缩
- en: In fine-grained compression, we assess the importance of each token in the instruction
    $\mathbf{x}^{\text{ins}}$, so that the compressed results could contain more question-relevant
    key information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度压缩中，我们评估指令$\mathbf{x}^{\text{ins}}$中每个标记的重要性，以便压缩结果能够包含更多与问题相关的关键信息。
- en: 'A straightforward solution for the awareness of $\mathbf{x}^{\text{que}}$ can
    be formulated as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$\mathbf{x}^{\text{que}}$的意识，可以制定一个直接的解决方案：
- en: '|  | $s_{i}=\text{perplexity}(x_{i}&#124;x_{<i})-\text{perplexity}(x_{i}&#124;x^{\text{que}},x_{<i}).$
    |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=\text{perplexity}(x_{i}&#124;x_{<i})-\text{perplexity}(x_{i}&#124;x^{\text{que}},x_{<i}).$
    |  | (3) |'
- en: 'Figure [3b](#S4.F3.sf2 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") illustrates the difference between perplexities and contrastive
    perplexities. We can see that tokens of high perplexities are widely distributed
    in all documents. However, tokens with high contrastive perplexities concentrate
    more on the left side of the dashed line, which corresponds to the document that
    contains the answer to the question. This suggests that the proposed contrastive
    perplexity can better distinguish tokens relevant to the question, thus improving
    the key information density in the compressed results.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3b](#S4.F3.sf2 "在图3 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣
    LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")展示了困惑度和对比困惑度之间的差异。我们可以看到，高困惑度的标记在所有文档中广泛分布。然而，高对比困惑度的标记更多集中在虚线的左侧，这对应于包含问题答案的文档。这表明，所提出的对比困惑度可以更好地区分与问题相关的标记，从而提高压缩结果中的关键信息密度。
- en: 4.2 How to reduce information loss in the middle?
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 如何减少中间信息的丢失？
- en: 'As demonstrated in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"),
    LLM achieves the highest performance when relevant information occurs at the beginning
    and significantly degrades if relevant information is located in the middle of
    long contexts. After the coarse-grained compression, we have obtained a set of
    documents $\{\mathbf{x}^{\text{doc}}_{k}\}_{k=1}^{K^{\prime}}$. Therefore, we
    reorder documents using their importance scores to better leverage LLMs’ information
    perception difference in positions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1b](#S1.F1.sf2 "在图1 ‣ 1 介绍 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")所示，当相关信息出现在开头时，LLM的性能最佳；如果相关信息位于长上下文的中间，性能显著下降。经过粗粒度压缩后，我们获得了一组文档$\{\mathbf{x}^{\text{doc}}_{k}\}_{k=1}^{K^{\prime}}$。因此，我们根据文档的重要性评分重新排序，以更好地利用LLMs在位置上的信息感知差异。
- en: '|  | $1$2 |  | (4) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 4.3 How to achieve adaptive granular control during compression?
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 如何在压缩过程中实现自适应粒度控制？
- en: In fine-grained compression, LLMLingua applies the save compression ratio over
    all documents obtained from coarse-grained compression. However, the key information
    density of different documents is different. The more relevant to the question
    a document is, the more budget (i.e., lower compression ratio) we should allocate
    to it. Therefore, we bridge coarse-grained compression to fine-grained compression
    and use the importance scores $\{r_{k}\}_{k=1}^{K^{\prime}}$ obtained from coarse-grained
    compression to guide the budget allocation in fine-grained compression. In this
    way, we can achieve adaptive granular control on the whole.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度压缩中，LLMLingua对从粗粒度压缩中获得的所有文档应用保存的压缩比。然而，不同文档的关键信息密度不同。文档与问题的相关性越高，我们应该分配的预算（即更低的压缩比）就越多。因此，我们将粗粒度压缩与细粒度压缩相结合，并使用从粗粒度压缩中获得的重要性评分$\{r_{k}\}_{k=1}^{K^{\prime}}$来指导细粒度压缩中的预算分配。通过这种方式，我们可以对整体进行自适应的粒度控制。
- en: 'Specifically, we first determine the initial budget for the retained documents
    $\tau^{\text{doc}}$ can be formulated as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们首先确定保留文档的初始预算 $\tau^{\text{doc}}$ 可以表述为：
- en: '|  | $\displaystyle\tau_{i}$ |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tau_{i}$ |  | (5) |'
- en: '|  | $\displaystyle\tau_{k}^{\text{doc}}$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tau_{k}^{\text{doc}}$ |  |'
- en: where $N_{d}$ is a hyper-parameter that controls the overall budget for dynamic
    allocation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{d}$ 是一个超参数，用于控制动态分配的总体预算。
- en: 4.4 How to improve the integrity of key information?
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 如何提高关键信息的完整性？
- en: 'Certain tokens of key entities may be discarded during the fine-grained token-wise
    compression. For example, the time entity “2009” in the original prompt might
    be compressed to “209” and the name entity “Wilhelm Conrad Röntgen” might be compressed
    to “Wilhelmgen”. This can cause problems for fact-based tasks like document QA,
    where language models tend to replicate information from the prompt, as shown
    in Figure [4](#S4.F4 "Figure 4 ‣ 4.4 How to improve the integrity of key information?
    ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度逐词压缩过程中，某些关键实体的标记可能会被丢弃。例如，原始提示中的时间实体“2009”可能被压缩为“209”，名称实体“Wilhelm Conrad
    Röntgen”可能被压缩为“Wilhelmgen”。这可能会对基于事实的任务（如文档问答）造成问题，在这些任务中，语言模型往往会复制提示中的信息，如图[4](#S4.F4
    "图4 ‣ 4.4 如何提高关键信息的完整性？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")所示。
- en: '![Refer to caption](img/ac2035d4d90754564ce58c0b540f1d0f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ac2035d4d90754564ce58c0b540f1d0f.png)'
- en: 'Figure 4: The example of Subsequence Recovery, the red text represents the
    original text, and the blue text is the result after using the LLaMA 2-7B tokenizer.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：子序列恢复的示例，红色文本表示原始文本，蓝色文本是使用LLaMA 2-7B分词器后的结果。
- en: 'To improve the accuracy and reliability of the information provided to users,
    we propose a subsequence recovery method to restore the original content from
    LLMs’ responses. This method relies on the subsequence relationship among tokens
    in the original prompt, compressed prompt, and LLMs’ response. The overall procedure
    includes: i) Iterate through tokens $y_{l}$ from the original prompt. For more
    details, please refer to Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Token-level
    Subsquence Recovery Details ‣ LongLLMLingua: Accelerating and Enhancing LLMs in
    Long Context Scenarios via Prompt Compression").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高提供给用户的信息的准确性和可靠性，我们提出了一种子序列恢复方法，以从LLMs的响应中恢复原始内容。该方法依赖于原始提示、压缩提示和LLMs响应中标记的子序列关系。总体过程包括：
    i) 遍历原始提示中的标记 $y_{l}$。有关更多细节，请参阅算法[1](#alg1 "算法 1 ‣ 附录 A 逐词子序列恢复细节 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")。
- en: 5 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'Here, we investigate: (1) How effective is LongLLMLingua? (2) How efficient
    is LongLLMLingua?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们调查： (1) LongLLMLingua的效果如何？ (2) LongLLMLingua的效率如何？
- en: Implementation Details
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施细节
- en: 'In this paper, we use GPT-3.5-Turbo-0613⁴⁴4For experiments with original prompts
    exceeding 4k tokens, we utilize GPT-3.5-Turbo-16k-0613. and LongChat-13B-16k as
    the target LLMs, both accessible via OpenAI⁵⁵5https://platform.openai.com and
    HuggingFace⁶⁶6https://huggingface.co/lmsys/longchat-13b-16k. To ensure stable
    and reproducible results, we employ greedy decoding and set the temperature to
    0 in all experiments. For the small language models used for compression, we apply
    LLaMA-2-7B-Chat⁷⁷7https://ai.meta.com/llama/, which has been aligned by supervised
    fine-tuning and RLHF. We implement our approach with PyTorch 1.13.1 and HuggingFace
    Transformers. We set up hyperparameters following LLMLingua except for the segment
    size used in iterative token-level compression set to 200 here. More details are
    provided in Appendix [B](#A2 "Appendix B Experiment Details ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们使用GPT-3.5-Turbo-0613⁴⁴4，针对超过4k标记的原始提示进行实验时，我们利用GPT-3.5-Turbo-16k-0613。和LongChat-13B-16k作为目标LLM，这些模型都可以通过OpenAI⁵⁵5https://platform.openai.com和HuggingFace⁶⁶6https://huggingface.co/lmsys/longchat-13b-16k访问。为了确保结果的稳定性和可重复性，我们在所有实验中采用贪婪解码，并将温度设置为0。对于用于压缩的小型语言模型，我们使用了LLaMA-2-7B-Chat⁷⁷7https://ai.meta.com/llama/，它通过监督微调和RLHF进行了对齐。我们使用PyTorch
    1.13.1和HuggingFace Transformers实现了我们的方法。我们按照LLMLingua设置超参数，除了在迭代的标记级压缩中将段大小设置为200。更多细节见附录[B](#A2
    "附录 B 实验细节 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")。
- en: Dataset & Evaluation Metric
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集与评估指标
- en: We use NaturalQuestions for the multi-document QA task, and use LongBench and
    ZeroSCROLLS for general long context scenarios.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用NaturalQuestions进行多文档问答任务，并使用LongBench和ZeroSCROLLS进行一般长上下文场景的评估。
- en: '(i) NaturalQuestions (Liu et al., [2023](#bib.bib20)): This benchmark is similar
    to the retrieval-augmented generation setup in commercial search and question-answering
    scenarios like Bing Chat. Specifically, each question has 20 related documents
    in the original prompt. One of them contains the correct answer and there are
    five different ground truth document position settings in the prompt: 1st, 5th,
    10th, 15th, and 20th. Following Liu et al. ([2023](#bib.bib20)), we use accuracy
    as the evaluation metric.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (i) NaturalQuestions (Liu et al., [2023](#bib.bib20))：该基准类似于商业搜索和问答场景中的检索增强生成设置，如Bing
    Chat。具体而言，每个问题在原始提示中有20个相关文档。其中一个包含正确答案，并且在提示中有五种不同的真实文档位置设置：第1、第5、第10、第15和第20位。根据Liu
    et al. ([2023](#bib.bib20))，我们使用准确率作为评估指标。
- en: '(ii) LongBench (Bai et al., [2023](#bib.bib1)): This benchmark consists of
    six task types: single-document QA, multi-document QA, summarization, few-shot
    learning, code completion, and synthetic tasks. We used the English portion that
    covers 16 datasets for evaluation. We use the metrics and scripts provided along
    with the benchmark for evaluation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) LongBench (Bai et al., [2023](#bib.bib1))：该基准包含六种任务类型：单文档问答、多文档问答、总结、少样本学习、代码补全和合成任务。我们使用了涵盖16个数据集的英语部分进行评估。我们使用基准提供的指标和脚本进行评估。
- en: '(iii) ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)): This benchmark consists
    of four task types: summarization, QA, sentiment classification, and reordering,
    covering 10 datasets. We used the validation set for evaluation. We use the provided
    metrics and scripts for evaluation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) ZeroSCROLLS (Shaham et al., [2023](#bib.bib28))：该基准包含四种任务类型：总结、问答、情感分类和重新排序，涵盖了10个数据集。我们使用验证集进行评估。我们使用提供的指标和脚本进行评估。
- en: Baselines
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: 'We include two sets of baselines in following experiments:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下实验中包含了两组基线：
- en: '(i) Retrieval-based Methods. We measure the association between the question
    and the documents in the prompt using five SoTA retrieval methods: BM25, Gzip (Jiang
    et al., [2023b](#bib.bib14)), SentenceBERT (Reimers & Gurevych, [2019](#bib.bib27)),
    OpenAI Embedding, and the important metric $r_{k}$ used in LongLLMLingua coarse-grained
    compression. We discard sentences or paragraphs with low association until the
    compression constraint is met while keeping the original document order unchanged.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 基于检索的方法。我们使用五种最先进的检索方法来测量问题与提示中的文档之间的关联：BM25、Gzip (Jiang et al., [2023b](#bib.bib14))、SentenceBERT
    (Reimers & Gurevych, [2019](#bib.bib27))、OpenAI Embedding，以及在LongLLMLingua粗粒度压缩中使用的重要指标$r_{k}$。我们丢弃低关联的句子或段落，直到满足压缩约束，同时保持原始文档顺序不变。
- en: '(ii) Compression-based Methods. We compare our approach with two state-of-art
    methods for prompt compression, i.e., Selective Context (Li, [2023](#bib.bib19))
    and LLMLingua (Jiang et al., [2023a](#bib.bib13)). Both methods employ LLaMA-2-7B-Chat
    as the small language model for compression. In LLMLingua, a coarse-to-fine approach
    is used to handle constraints of compression ratio: the original prompt is first
    compressed to $k$ is the granular control coefficient; token-level is then performed
    to reach the overall constraint. Our method follows the same coarse-to-fine logic
    to achieve the constraint.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 基于压缩的方法。我们将我们的方法与两种最先进的提示压缩方法进行比较，即选择性上下文（Li, [2023](#bib.bib19)）和 LLMLingua（Jiang
    等, [2023a](#bib.bib13)）。这两种方法均采用 LLaMA-2-7B-Chat 作为小型语言模型进行压缩。在 LLMLingua 中，使用粗到细的方法来处理压缩比例的约束：首先将原始提示压缩到
    $k$，即粒度控制系数；然后进行 token 级别的操作以达到总体约束。我们的方法遵循相同的粗到细逻辑以实现约束。
- en: '| Methods | GPT3.5-Turbo | LongChat-13b | Length |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GPT3.5-Turbo | LongChat-13b | 长度 |'
- en: '| 1st | 5th | 10th | 15th | 20th | Reorder | 1st | 5th | 10th | 15th | 20th
    | Reorder | Tokens | $1/\tau$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1st | 5th | 10th | 15th | 20th | 重新排序 | 1st | 5th | 10th | 15th | 20th |
    重新排序 | Tokens | $1/\tau$ |'
- en: '| 2x constraint |  |  |  |  |  |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2x 限制 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 53.7 | 49.3 | 47.9 | 49.9 | 46.9 | 50.3 | 50.9 | 44.9 | 44.1 | 42.9
    | 43.2 | 46.0 | 1,545 | 1.9x |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 53.7 | 49.3 | 47.9 | 49.9 | 46.9 | 50.3 | 50.9 | 44.9 | 44.1 | 42.9
    | 43.2 | 46.0 | 1,545 | 1.9x |'
- en: '| Gzip | 64.6 | 63.8 | 60.5 | 58.3 | 57.3 | 64.4 | 61.9 | 55.7 | 52.7 | 50.8
    | 50.9 | 59.3 | 1,567 | 1.9x |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Gzip | 64.6 | 63.8 | 60.5 | 58.3 | 57.3 | 64.4 | 61.9 | 55.7 | 52.7 | 50.8
    | 50.9 | 59.3 | 1,567 | 1.9x |'
- en: '| SBERT | 72.5 | 67.9 | 63.3 | 65.0 | 66.2 | 68.7 | 65.8 | 57.5 | 54.9 | 53.4
    | 55.7 | 61.4 | 1,549 | 1.9x |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 72.5 | 67.9 | 63.3 | 65.0 | 66.2 | 68.7 | 65.8 | 57.5 | 54.9 | 53.4
    | 55.7 | 61.4 | 1,549 | 1.9x |'
- en: '| OpenAI | 73.0 | 65.6 | 66.5 | 65.4 | 65.5 | 69.9 | 65.9 | 57.5 | 56.2 | 54.2
    | 55.7 | 61.7 | 1,550 | 1.9x |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 73.0 | 65.6 | 66.5 | 65.4 | 65.5 | 69.9 | 65.9 | 57.5 | 56.2 | 54.2
    | 55.7 | 61.7 | 1,550 | 1.9x |'
- en: '| LongLLMLingua $r_{k}$ | 73.9 | 67.7 | 68.7 | 66.0 | 65.6 | 74.3 | 68.5 |
    59.1 | 56.8 | 55.3 | 56.9 | 65.2 | 1,548 | 1.9x |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 73.9 | 67.7 | 68.7 | 66.0 | 65.6 | 74.3 | 68.5 |
    59.1 | 56.8 | 55.3 | 56.9 | 65.2 | 1,548 | 1.9x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |  |  |'
- en: '| Selective-Context | 45.4 | 39.0 | 33.8 | 33.5 | 41.5 | - | 53.2 | 26.3 |
    25.4 | 24.2 | 33.3 | - | 1,478 | 2.0x |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 选择性上下文 | 45.4 | 39.0 | 33.8 | 33.5 | 41.5 | - | 53.2 | 26.3 | 25.4 | 24.2
    | 33.3 | - | 1,478 | 2.0x |'
- en: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 | 41.5 | 38.7 | 37.3 | 35.7
    | 34.1 | 37.5 | 37.1 | 1,410 | 2.1x |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 | 41.5 | 38.7 | 37.3 | 35.7
    | 34.1 | 37.5 | 37.1 | 1,410 | 2.1x |'
- en: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 | 76.2 | 68.7 | 59.4 | 57.3
    | 55.9 | 58.4 | 66.1 | 1,429 | 2.1x |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 | 76.2 | 68.7 | 59.4 | 57.3
    | 55.9 | 58.4 | 66.1 | 1,429 | 2.1x |'
- en: '| 4x constraint |  |  |  |  |  |  |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 4x 限制 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 40.6 | 38.6 | 38.2 | 37.4 | 36.6 | 36.3 | 39.5 | 37.5 | 36.8 | 36.4
    | 35.5 | 37.7 | 798 | 3.7x |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 40.6 | 38.6 | 38.2 | 37.4 | 36.6 | 36.3 | 39.5 | 37.5 | 36.8 | 36.4
    | 35.5 | 37.7 | 798 | 3.7x |'
- en: '| Gzip | 63.1 | 61.0 | 59.8 | 61.1 | 60.1 | 62.3 | 57.6 | 52.9 | 51.0 | 50.1
    | 50.4 | 57.2 | 824 | 3.6x |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Gzip | 63.1 | 61.0 | 59.8 | 61.1 | 60.1 | 62.3 | 57.6 | 52.9 | 51.0 | 50.1
    | 50.4 | 57.2 | 824 | 3.6x |'
- en: '| SBERT | 66.9 | 61.1 | 59.0 | 61.2 | 60.3 | 64.4 | 62.6 | 56.6 | 55.1 | 53.9
    | 55.0 | 59.1 | 808 | 3.6x |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 66.9 | 61.1 | 59.0 | 61.2 | 60.3 | 64.4 | 62.6 | 56.6 | 55.1 | 53.9
    | 55.0 | 59.1 | 808 | 3.6x |'
- en: '| OpenAI | 63.8 | 64.6 | 65.4 | 64.1 | 63.7 | 63.7 | 61.2 | 56.0 | 55.1 | 54.4
    | 55.0 | 58.8 | 804 | 3.7x |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 63.8 | 64.6 | 65.4 | 64.1 | 63.7 | 63.7 | 61.2 | 56.0 | 55.1 | 54.4
    | 55.0 | 58.8 | 804 | 3.7x |'
- en: '| LongLLMLingua $r_{k}$ | 71.1 | 70.7 | 69.3 | 68.7 | 68.5 | 71.5 | 67.8 |
    59.4 | 57.7 | 57.7 | 58.6 | 64.0 | 807 | 3.7x |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 71.1 | 70.7 | 69.3 | 68.7 | 68.5 | 71.5 | 67.8 |
    59.4 | 57.7 | 57.7 | 58.6 | 64.0 | 807 | 3.7x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |  |  |'
- en: '| Selective-Context | 31.4 | 19.5 | 24.7 | 24.1 | 43.8 | - | 38.2 | 17.2 |
    15.9 | 16.0 | 27.3 | - | 791 | 3.7x |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 选择性上下文 | 31.4 | 19.5 | 24.7 | 24.1 | 43.8 | - | 38.2 | 17.2 | 15.9 | 16.0
    | 27.3 | - | 791 | 3.7x |'
- en: '| LLMLingua | 25.5 | 27.5 | 23.5 | 26.5 | 30.0 | 27.0 | 32.1 | 30.8 | 29.9
    | 28.9 | 32.4 | 30.5 | 775 | 3.8x |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 25.5 | 27.5 | 23.5 | 26.5 | 30.0 | 27.0 | 32.1 | 30.8 | 29.9
    | 28.9 | 32.4 | 30.5 | 775 | 3.8x |'
- en: '| LongLLMLingua | 75.0 | 71.8 | 71.2 | 71.2 | 74.7 | 75.5 | 68.7 | 60.5 | 59.3
    | 58.3 | 61.3 | 66.7 | 748 | 3.9x |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 75.0 | 71.8 | 71.2 | 71.2 | 74.7 | 75.5 | 68.7 | 60.5 | 59.3
    | 58.3 | 61.3 | 66.7 | 748 | 3.9x |'
- en: '| Original Prompt | 75.7 | 57.3 | 54.1 | 55.4 | 63.1 | - | 68.6 | 57.4 | 55.3
    | 52.5 | 55.0 | - | 2,946 | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | 75.7 | 57.3 | 54.1 | 55.4 | 63.1 | - | 68.6 | 57.4 | 55.3 | 52.5 |
    55.0 | - | 2,946 | - |'
- en: '| Zero-shot |  |  | 56.1 |  |  |  |  | 35.0 |  |  | 15 | 196x |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot |  |  | 56.1 |  |  |  |  | 35.0 |  |  | 15 | 196x |'
- en: 'Table 1: Performance of different methods with different compression ratios
    on NaturalQuestions (20 documents) (Liu et al., [2023](#bib.bib20)). Reorder:
    we reorder the documents with relevance metrics of different baselines as our
    document reordering strategy described in Sec. [4.2](#S4.SS2 "4.2 How to reduce
    information loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"). In the
    case of OpenAI, it corresponds to LongContextReorder in the LangChain framework (Chase,
    [2022](#bib.bib4)). For results reported under 1st to 20th, we do not use the
    reordering strategy for all methods.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同方法在自然问题（20份文档）上的不同压缩比性能（Liu等， [2023](#bib.bib20)）。重新排序：我们按照不同基准的相关性指标重新排序文档，如第[4.2](#S4.SS2
    "4.2 如何减少中间的信息损失？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强LLMs在长上下文场景中的表现")节所述的文档重新排序策略。在OpenAI的情况下，这对应于LangChain框架中的LongContextReorder（Chase，[2022](#bib.bib4)）。对于1到20名的结果，我们没有对所有方法使用重新排序策略。
- en: '⁷⁷footnotetext: https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/
    long_context_reorder'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷⁷脚注文本：https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/
    long_context_reorder
- en: Main Results
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结果
- en: 'Table [1](#S5.T1 "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") and [3](#S5.T3
    "Table 3 ‣ Main Results ‣ 5 Experiments ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression") present the performance
    of various methods under different compression constraints. There are multiple
    observations and conclusions: (1) Our LongLLMLingua achieves the best performance
    across different tasks and constraints of compression ratios. Compared to the
    original prompt, our compressed prompt can derive higher performance with much
    less cost. For example, LongLLMLingua gains a performance boost of 17.1% on NaturalQuestions
    with the ground-true document at the 10th position, while the number of tokens
    input to GPT3.5-Turbo is $\sim$, LongLLMLingua even achieves a little performance
    gain. We mainly owe this to the question-aware coarse-to-fine compression, which
    can better figure out the key information and reach a higher key information density
    with a higher compression rate. (5) The proposed document reordering strategy
    helps in not only our approach but also other baselines as shown in Table [1](#S5.T1
    "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression"), well demonstrating its
    effectiveness.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S5.T1 "表1 ‣ 基准 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强LLMs在长上下文场景中的表现")和[3](#S5.T3
    "表3 ‣ 主要结果 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强LLMs在长上下文场景中的表现")展示了在不同压缩约束下各种方法的表现。有多个观察和结论：（1）我们的LongLLMLingua在不同任务和压缩比约束下表现最佳。与原始提示相比，我们的压缩提示能够在成本更低的情况下获得更高的性能。例如，LongLLMLingua在第10名的真实文档上，在NaturalQuestions上的性能提升了17.1%，而输入到GPT3.5-Turbo的令牌数为$\sim$，LongLLMLingua甚至实现了稍微的性能提升。我们主要归功于问题感知的粗到细压缩，它可以更好地找出关键信息，并以更高的压缩率达到更高的关键信息密度。（5）所提出的文档重新排序策略不仅对我们的方法有帮助，也对其他基准方法如表[1](#S5.T1
    "表1 ‣ 基准 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强LLMs在长上下文场景中的表现")所示，充分证明了其有效性。
- en: '|  | 1st | 5th | 10th | 15th | 20th |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 第1名 | 第5名 | 第10名 | 第15名 | 第20名 |'
- en: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 |'
- en: '| - w/o Question-aware Coarse-grained | 42.1 | 40.3 | 39.7 | 40.1 | 40.3 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| - 无问题感知的粗粒度 | 42.1 | 40.3 | 39.7 | 40.1 | 40.3 |'
- en: '| - w/ SBERT | 73.2 | 68.5 | 65.7 | 66.1 | 66.7 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| - 有SBERT | 73.2 | 68.5 | 65.7 | 66.1 | 66.7 |'
- en: '| - w/o Question-aware Fine-grained | 75.8 | 71.0 | 68.9 | 68.4 | 69.3 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| - 无问题感知的细粒度 | 75.8 | 71.0 | 68.9 | 68.4 | 69.3 |'
- en: '| - w/o Dynamic Compression Ratio | 74.4 | 70.7 | 68.7 | 67.9 | 68.1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| - 无动态压缩比 | 74.4 | 70.7 | 68.7 | 67.9 | 68.1 |'
- en: '| - w/o Subsequence Recovery | 76.7 | 71.7 | 69.4 | 69.3 | 69.7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| - 无子序列恢复 | 76.7 | 71.7 | 69.4 | 69.3 | 69.7 |'
- en: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 |'
- en: '| - w/ Subsequence Recovery | 43.8 | 44.1 | 43.5 | 43.3 | 44.4 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| - 有子序列恢复 | 43.8 | 44.1 | 43.5 | 43.3 | 44.4 |'
- en: 'Table 2: Ablation study on NaturalQuestions with 2x constraint using GPT-3.5-Turbo.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用GPT-3.5-Turbo进行2倍约束的自然问题的消融研究。
- en: '| Methods | LongBench | ZeroSCROLLS |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LongBench | ZeroSCROLLS |'
- en: '| SingleDoc | MultiDoc | Summ. | FewShot | Synth. | Code | AVG | Tokens | $1/\tau$
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 单文档 | 多文档 | 总结 | 少量示例 | 合成 | 代码 | 平均 | 令牌 | $1/\tau$ |'
- en: '| 3,000 tokens constraint |  |  |  |  |  |  |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 3,000 tokens 约束 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 32.3 | 34.3 | 25.3 | 57.9 | 45.1 | 48.9 | 40.6 | 3,417 | 3x | 19.8
    | 3,379 | 3x |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 32.3 | 34.3 | 25.3 | 57.9 | 45.1 | 48.9 | 40.6 | 3,417 | 3x | 19.8
    | 3,379 | 3x |'
- en: '| SBERT | 35.3 | 37.4 | 26.7 | 63.4 | 51.0 | 34.5 | 41.4 | 3,399 | 3x | 24.0
    | 3,340 | 3x |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 35.3 | 37.4 | 26.7 | 63.4 | 51.0 | 34.5 | 41.4 | 3,399 | 3x | 24.0
    | 3,340 | 3x |'
- en: '| OpenAI | 34.5 | 38.6 | 26.8 | 63.4 | 49.6 | 37.6 | 41.7 | 3,421 | 3x | 22.4
    | 3,362 | 3x |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 34.5 | 38.6 | 26.8 | 63.4 | 49.6 | 37.6 | 41.7 | 3,421 | 3x | 22.4
    | 3,362 | 3x |'
- en: '| LongLLMLingua $r_{k}$ | 37.6 | 42.9 | 26.9 | 68.2 | 49.9 | 53.4 | 46.5 |
    3,424 | 3x | 29.3 | 3,350 | 3x |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 37.6 | 42.9 | 26.9 | 68.2 | 49.9 | 53.4 | 46.5 |
    3,424 | 3x | 29.3 | 3,350 | 3x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |'
- en: '| Selective-Context | 23.3 | 39.2 | 25.0 | 23.8 | 27.5 | 53.1 | 32.0 | 3,328
    | 3x | 20.7 | 3,460 | 3x |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Selective-Context | 23.3 | 39.2 | 25.0 | 23.8 | 27.5 | 53.1 | 32.0 | 3,328
    | 3x | 20.7 | 3,460 | 3x |'
- en: '| LLMLingua | 31.8 | 37.5 | 26.2 | 67.2 | 8.3 | 53.2 | 37.4 | 3,421 | 3x |
    30.7 | 3,366 | 3x |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 31.8 | 37.5 | 26.2 | 67.2 | 8.3 | 53.2 | 37.4 | 3,421 | 3x |
    30.7 | 3,366 | 3x |'
- en: '| LongLLMLingua | 40.7 | 46.2 | 27.2 | 70.6 | 53.0 | 55.2 | 48.8 | 3,283 |
    3x | 32.8 | 3,412 | 3x |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 40.7 | 46.2 | 27.2 | 70.6 | 53.0 | 55.2 | 48.8 | 3,283 |
    3x | 32.8 | 3,412 | 3x |'
- en: '| 2,000 tokens constraint |  |  |  |  |  |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2,000 tokens 约束 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 30.1 | 29.4 | 21.2 | 19.5 | 12.4 | 29.1 | 23.6 | 1,985 | 5x | 20.1
    | 1,799 | 5x |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 30.1 | 29.4 | 21.2 | 19.5 | 12.4 | 29.1 | 23.6 | 1,985 | 5x | 20.1
    | 1,799 | 5x |'
- en: '| SBERT | 33.8 | 35.9 | 25.9 | 23.5 | 18.0 | 17.8 | 25.8 | 1,947 | 5x | 20.5
    | 1,773 | 6x |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 33.8 | 35.9 | 25.9 | 23.5 | 18.0 | 17.8 | 25.8 | 1,947 | 5x | 20.5
    | 1,773 | 6x |'
- en: '| OpenAI | 34.3 | 36.3 | 24.7 | 32.4 | 26.3 | 24.8 | 29.8 | 1,991 | 5x | 20.6
    | 1,784 | 5x |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 34.3 | 36.3 | 24.7 | 32.4 | 26.3 | 24.8 | 29.8 | 1,991 | 5x | 20.6
    | 1,784 | 5x |'
- en: '| LongLLMLingua $r_{k}$ | 37.8 | 41.7 | 26.9 | 66.3 | 53.0 | 52.4 | 46.3 |
    1,960 | 5x | 24.9 | 1,771 | 6x |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 37.8 | 41.7 | 26.9 | 66.3 | 53.0 | 52.4 | 46.3 |
    1,960 | 5x | 24.9 | 1,771 | 6x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |'
- en: '| Selective-Context | 16.2 | 34.8 | 24.4 | 15.7 | 8.4 | 49.2 | 24.8 | 1,925
    | 5x | 19.4 | 1,865 | 5x |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Selective-Context | 16.2 | 34.8 | 24.4 | 15.7 | 8.4 | 49.2 | 24.8 | 1,925
    | 5x | 19.4 | 1,865 | 5x |'
- en: '| LLMLingua | 22.4 | 32.1 | 24.5 | 61.2 | 10.4 | 56.8 | 34.6 | 1,950 | 5x |
    27.2 | 1,862 | 5x |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 22.4 | 32.1 | 24.5 | 61.2 | 10.4 | 56.8 | 34.6 | 1,950 | 5x |
    27.2 | 1,862 | 5x |'
- en: '| LongLLMLingua | 39.0 | 42.2 | 27.4 | 69.3 | 53.8 | 56.6 | 48.0 | 1,809 |
    6x | 32.5 | 1,753 | 6x |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 39.0 | 42.2 | 27.4 | 69.3 | 53.8 | 56.6 | 48.0 | 1,809 |
    6x | 32.5 | 1,753 | 6x |'
- en: '| Original Prompt | 39.7 | 38.7 | 26.5 | 67.0 | 37.8 | 54.2 | 44.0 | 10,295
    | - | 32.5 | 9,788 | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | 39.7 | 38.7 | 26.5 | 67.0 | 37.8 | 54.2 | 44.0 | 10,295 | - | 32.5
    | 9,788 | - |'
- en: '| Zero-shot | 15.6 | 31.3 | 15.6 | 40.7 | 1.6 | 36.2 | 23.5 | 214 | 48x | 10.8
    | 32 | 306x |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot | 15.6 | 31.3 | 15.6 | 40.7 | 1.6 | 36.2 | 23.5 | 214 | 48x | 10.8
    | 32 | 306x |'
- en: 'Table 3: Performance of different methods under different compression ratios
    onLongBench (Bai et al., [2023](#bib.bib1)) and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28))
    using GPT-3.5-Turbo. Considering the dataset structure, we do not use the reordering
    strategy here.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同方法在LongBench（Bai等，[2023](#bib.bib1)）和ZeroSCROLLS（Shaham等，[2023](#bib.bib28)）上，使用GPT-3.5-Turbo的性能，考虑到数据集结构，我们在此不使用重新排序策略。
- en: Ablation Study
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融研究
- en: 'To evaluate the contributions of different components in LongLLMLingua, we
    introduce six variants of it for ablation study: (1) Ours w/o Question-aware Coarse-grained,
    which calculates question-text relevance $r_{k}$. (3) Ours w/o Question-aware
    Fine-grained, which disregards Eq. ([3](#S4.E3 "In Question-Aware Fine-Grained
    Compression ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")) and only applies Iterative Token-level Prompt Compression
    as LLMLingua. (4) Ours w/o Dynamic Compression Ratio, where all documents share
    the same compression ratio in fine-grained compression. (5) Ours w/o and (6) LLMLingua
    w/ Subsequence Recovery, which either removes or adds the post-processing subsequence
    recovery strategy.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估LongLLMLingua中不同组件的贡献，我们引入了六种变体进行消融研究：（1）不使用Question-aware Coarse-grained，我们计算问题文本的相关性$r_{k}$。（3）不使用Question-aware
    Fine-grained，忽略了公式 ([3](#S4.E3 "在Question-Aware Fine-Grained Compression ‣ 4.1
    如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs"))，仅应用LLMLingua的迭代令牌级提示压缩。（4）不使用Dynamic
    Compression Ratio，在细粒度压缩中所有文档共享相同的压缩比。（5）不使用和（6）LLMLingua w/ Subsequence Recovery，移除或添加后处理子序列恢复策略。'
- en: 'Table [2](#S5.T2 "Table 2 ‣ Main Results ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") shows the
    results of the ablation study. In summary, removing any component proposed for
    LongLLMLingua will lead to a performance drop regardless of the position of the
    ground-truth answer. This well validates the necessity and effectiveness of the
    proposed question-aware mechanism during coarse-to-fine compression, the dynamic
    compression ratio, and the subsequence recovery strategy. It also shows that applying
    SBERT for coarse-grained compression will result in inferior performance, which
    implies the superiority of our question-aware importance metric in Eq. [2](#S4.E2
    "In Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve key information
    density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression") over SBERT. Moreover,
    our subsequence recovery strategy can also bring performance gains for LLMLingua.
    However, without our question-aware mechanism, results from LLMLingua are still
    less satisfactory. For more detailed cases, please go to Appendix [C](#A3 "Appendix
    C Ablation Analysis ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S5.T2 "表 2 ‣ 主要结果 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景下的 LLM") 显示了消融研究的结果。总的来说，去除任何为
    LongLLMLingua 提出的组件都会导致性能下降，无论真实答案的位置如何。这很好地验证了在粗到精压缩过程中提出的问题感知机制、动态压缩比和子序列恢复策略的必要性和有效性。它还显示了将
    SBERT 应用于粗粒度压缩会导致较差的性能，这表明我们在公式 [2](#S4.E2 "在问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣
    4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景下的 LLM") 中的问题感知重要性度量优于 SBERT。此外，我们的子序列恢复策略也能为
    LLMLingua 带来性能提升。然而，如果没有我们的问题感知机制，LLMLingua 的结果仍然不够令人满意。有关更详细的案例，请参见附录 [C](#A3
    "附录 C 消融分析 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景下的 LLM")。
- en: Latency Evaluation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 延迟评估
- en: '| $1/\tau$ | 2x | 5x | 10x |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $1/\tau$ | 2x | 5x | 10x |'
- en: '| --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| E2E w/o Compression | 15.6 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| E2E 无压缩 | 15.6 |'
- en: '| --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| E2E w/ LLMLingua | 10.5 (1.5x) | 6.0 (2.6x) | 3.9 (4.0x) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| E2E 使用 LLMLingua | 10.5 (1.5x) | 6.0 (2.6x) | 3.9 (4.0x) |'
- en: '| E2E w/ LongLLMLingua | 11.4 (1.4x) | 6.3 (2.5x) | 4.1 (3.8x) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| E2E 使用 LongLLMLingua | 11.4 (1.4x) | 6.3 (2.5x) | 4.1 (3.8x) |'
- en: '| LongLLMLingua | 2.9 | 1.6 | 1.2 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 2.9 | 1.6 | 1.2 |'
- en: 'Figure 5: Latency (s) on LongBench.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 LongBench 上的延迟（秒）。
- en: 'We conduct testing on a V100-32G GPU, using the prompts from LongBench with
    $\sim$10K tokens on average and setting the response length to 200 tokens in the
    API call. In Table [5](#S5.F5 "Figure 5 ‣ Latency Evaluation ‣ 5 Experiments ‣
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression"), E2E denotes the latency from both the prompt compression system
    and the black-box API, while LongLLMLingua denotes the prompt compression latency
    only. It is shown that our prompt compression system does accelerate the overall
    inference. As the compression rate increases, the acceleration effect becomes
    more pronounced. It is worth mentioning that in scenarios with longer API cost
    time, the actual absolute time saved by LongLLMLingua can be more significant.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 V100-32G GPU 上进行测试，使用来自 LongBench 的提示，平均有 $\sim$10K 个标记，并在 API 调用中将响应长度设置为
    200 个标记。在表 [5](#S5.F5 "图 5 ‣ 延迟评估 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景下的 LLM")
    中，E2E 表示提示压缩系统和黑箱 API 的延迟，而 LongLLMLingua 仅表示提示压缩的延迟。结果显示，我们的提示压缩系统确实加速了整体推理。随着压缩率的增加，加速效果变得更加明显。值得一提的是，在
    API 成本时间较长的场景中，LongLLMLingua 实际节省的绝对时间可能会更显著。
- en: 6 Related Works
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: 'Long Context for LLMs. Recent research has focused on expanding the window
    size of LLMs. Main approaches include: (1) Staged pre-training (Nijkamp et al.,
    [2023](#bib.bib23)) which gradually increases the context window; (2) Modifying (Press
    et al., [2022](#bib.bib26)) or interpolating position embeddings (Chen et al.,
    [2023](#bib.bib5); Peng et al., [2023](#bib.bib25); Han et al., [2023](#bib.bib11));
    (3) Using linear or sparse attention mechanisms (Ding et al., [2023](#bib.bib7);
    Sun et al., [2023](#bib.bib30)); (4) Utilizing external memory modules for context
    storage (Bertsch et al., [2023](#bib.bib2); Tworkowski et al., [2023](#bib.bib31)).
    While these methods address context window expansion, their impact on downstream
    task performance has yet to be discussed.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文用于LLMs。近期研究重点关注扩大LLM的窗口大小。主要方法包括：（1）阶段性预训练（Nijkamp et al., [2023](#bib.bib23)），逐渐增加上下文窗口；（2）修改（Press
    et al., [2022](#bib.bib26)）或插值位置嵌入（Chen et al., [2023](#bib.bib5); Peng et al.,
    [2023](#bib.bib25); Han et al., [2023](#bib.bib11)）；（3）使用线性或稀疏注意力机制（Ding et al.,
    [2023](#bib.bib7); Sun et al., [2023](#bib.bib30)）；（4）利用外部记忆模块进行上下文存储（Bertsch
    et al., [2023](#bib.bib2); Tworkowski et al., [2023](#bib.bib31)）。虽然这些方法解决了上下文窗口扩展的问题，但其对下游任务性能的影响尚待讨论。
- en: Information Distribution in Prompt. Recent empirical experiments have shown
    that LLM performance decreases with less effective information in a prompt (Bai
    et al., [2023](#bib.bib1); Li et al., [2023](#bib.bib18); Shi et al., [2023](#bib.bib29)).
    Moreover, the position of relevant information in a prompt has a significant impact
    on performance(Wu et al., [2022](#bib.bib32)). Liu et al. ([2023](#bib.bib20))
    suggests that LLMs have more difficulty comprehending information located in the
    middle of a prompt compared to those at the edges.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 提示中的信息分布。近期的实证实验表明，提示中信息的有效性降低会导致LLM性能下降（Bai et al., [2023](#bib.bib1); Li et
    al., [2023](#bib.bib18); Shi et al., [2023](#bib.bib29)）。此外，提示中相关信息的位置对性能有显著影响（Wu
    et al., [2022](#bib.bib32)）。Liu et al.（[2023](#bib.bib20)）建议，与提示边缘的信息相比，LLM在理解位于提示中间的信息时更为困难。
- en: Retrieval Methods can be categorized as dense or sparse retrieval methods. Sparse
    retrieval methods, like BM25, determine the relevance between queries and documents
    based on n-gram information. Conversely, dense retrieval methods assess the relevance
    between queries and documents in latent space using dense vectors, such as SentenceBERT (Reimers
    & Gurevych, [2019](#bib.bib27)) and OpenAI Embedding. Recently, Jiang et al. ([2023b](#bib.bib14)))
    proposed an unsupervised dense retrieval method that leverages traditional compression
    algorithms, such as gzip, and k-nearest neighbors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 检索方法可以分为稠密检索方法和稀疏检索方法。稀疏检索方法，如BM25，根据n-gram信息确定查询与文档之间的相关性。相反，稠密检索方法使用稠密向量在潜在空间中评估查询与文档之间的相关性，例如SentenceBERT（Reimers
    & Gurevych, [2019](#bib.bib27)）和OpenAI Embedding。最近，Jiang et al.（[2023b](#bib.bib14)）提出了一种无监督的稠密检索方法，利用传统的压缩算法，如gzip，以及k-最近邻。
- en: 'Prompt Compression Methods can be grouped into three main categories: (1) Token
    pruning (Goyal et al., [2020](#bib.bib10); Kim & Cho, [2021](#bib.bib15); Modarressi
    et al., [2022](#bib.bib21)) and token merging (Bolya et al., [2023](#bib.bib3)),
    which need model fine-tuning or intermediate results during inference and have
    been used with BERT-scale models. (2) Soft prompt tuning methods like GIST (Mu
    et al., [2023](#bib.bib22)), AutoCompressor (Chevalier et al., [2023](#bib.bib6)),
    and ICAE (Ge et al., [2023](#bib.bib9)), which require LLMs’ parameter fine-tuning,
    making them suitable for specific domains but not directly applicable to black-box
    LLMs. (3) Information-entropy-based approaches such as Selective Context (Li,
    [2023](#bib.bib19)) and LLMLingua (Jiang et al., [2023a](#bib.bib13)), which use
    a small language model to calculate the self-information or perplexity of each
    token in the original prompt and then remove tokens with lower perplexities.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 提示压缩方法可以分为三大类：（1）令牌剪枝（Goyal et al., [2020](#bib.bib10); Kim & Cho, [2021](#bib.bib15);
    Modarressi et al., [2022](#bib.bib21)）和令牌合并（Bolya et al., [2023](#bib.bib3)），这些方法需要模型微调或推理过程中的中间结果，已被用于BERT规模的模型。（2）软提示调整方法，如GIST（Mu
    et al., [2023](#bib.bib22)）、AutoCompressor（Chevalier et al., [2023](#bib.bib6)）和ICAE（Ge
    et al., [2023](#bib.bib9)），这些方法需要对LLM参数进行微调，适合特定领域但不能直接应用于黑箱LLM。（3）基于信息熵的方法，如Selective
    Context（Li, [2023](#bib.bib19)）和LLMLingua（Jiang et al., [2023a](#bib.bib13)），这些方法使用小型语言模型计算原始提示中每个令牌的自信息或困惑度，然后去除困惑度较低的令牌。
- en: 7 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We propose LongLLMLingua to address the three challenges, i.e., higher computational/financial
    cost, longer system latency, and inferior performance for LLMs in long context
    scenarios. We develop LongLLMLingua from the perspective of efficient prompt compression,
    thus reducing both computational/financial cost and the system latency. We further
    design four components, i.e., a question-aware coarse-to-fine compression method,
    a document reordering mechanism, dynamic compression ratios, and a post-compression
    subsequence recovery strategy to improve LLMs’ perception of the key information,
    with which LongLLMLingua demonstrate superior performance. Experiments on one
    multi-document QA benchmark and two long context benchmarks demonstrate that LongLLMLingua
    compressed prompt can derive higher performance than original prompts while both
    API costs for inference and the end-to-end system latency are largely reduced.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LongLLMLingua 来解决三个挑战，即更高的计算/财务成本、更长的系统延迟和在长上下文场景中LLMs的表现较差。我们从高效提示压缩的角度开发
    LongLLMLingua，从而减少计算/财务成本和系统延迟。我们进一步设计了四个组件，即一个问题感知的粗到精压缩方法、一个文档重新排序机制、动态压缩比和一个压缩后的子序列恢复策略，以提高LLMs对关键信息的感知，LongLLMLingua
    展现了卓越的性能。在一个多文档 QA 基准测试和两个长上下文基准测试上的实验表明，LongLLMLingua 压缩提示可以比原始提示获得更高的性能，同时推理的
    API 成本和端到端系统延迟都大幅减少。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench:
    A bilingual, multitask benchmark for long context understanding. *ArXiv preprint*,
    abs/2308.14508, 2023. URL [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, 等. Longbench：一个双语、多任务的长上下文理解基准。*ArXiv
    预印本*，abs/2308.14508，2023年。URL [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508).
- en: 'Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. Unlimiformer: Long-range transformers with unlimited length input. *ArXiv
    preprint*, abs/2305.01625, 2023. URL [https://arxiv.org/abs/2305.01625](https://arxiv.org/abs/2305.01625).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. Unlimiformer：具有无限长度输入的长距离变压器。*ArXiv 预印本*，abs/2305.01625，2023年。URL [https://arxiv.org/abs/2305.01625](https://arxiv.org/abs/2305.01625).
- en: 'Bolya et al. (2023) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
    Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster.
    In *The Eleventh International Conference on Learning Representations*, 2023.
    URL [https://openreview.net/forum?id=JroZRaRw7Eu](https://openreview.net/forum?id=JroZRaRw7Eu).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolya et al. (2023) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
    Christoph Feichtenhofer, and Judy Hoffman. 令牌合并：你的 vit 但更快。在 *第十一届国际学习表征会议*，2023年。URL
    [https://openreview.net/forum?id=JroZRaRw7Eu](https://openreview.net/forum?id=JroZRaRw7Eu).
- en: Chase (2022) Harrison Chase. LangChain, 2022. URL [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase (2022) Harrison Chase. LangChain, 2022. URL [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain).
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *ArXiv preprint*, abs/2306.15595, 2023. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 通过位置插值扩展大型语言模型的上下文窗口。*ArXiv 预印本*，abs/2306.15595，2023年。URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. Adapting language models to compress contexts. *ArXiv preprint*, abs/2305.14788,
    2023. URL [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. 适应语言模型以压缩上下文。*ArXiv 预印本*，abs/2305.14788，2023年。URL [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788).
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. *ArXiv preprint*, abs/2307.02486, 2023. URL [https://arxiv.org/abs/2307.02486](https://arxiv.org/abs/2307.02486).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, and Furu Wei. Longnet：将变压器扩展到1,000,000,000个标记。*ArXiv 预印本*，abs/2307.02486，2023年。URL
    [https://arxiv.org/abs/2307.02486](https://arxiv.org/abs/2307.02486).
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.
    *ArXiv preprint*, abs/2301.00234, 2023. URL [https://arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等（2023）Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang,
    Xu Sun, Jingjing Xu 和 Zhifang Sui. 上下文学习的调查。*ArXiv预印本*，abs/2301.00234，2023。URL
    [https://arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234)。
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context
    autoencoder for context compression in a large language model. *ArXiv preprint*,
    abs/2307.06945, 2023. URL [https://arxiv.org/abs/2307.06945](https://arxiv.org/abs/2307.06945).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge等（2023）Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen 和 Furu Wei. 上下文自动编码器用于大型语言模型中的上下文压缩。*ArXiv预印本*，abs/2307.06945，2023。URL
    [https://arxiv.org/abs/2307.06945](https://arxiv.org/abs/2307.06945)。
- en: 'Goyal et al. (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan T.
    Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating BERT
    inference via progressive word-vector elimination. In *Proceedings of the 37th
    International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
    Event*, volume 119 of *Proceedings of Machine Learning Research*, pp.  3690–3699\.
    PMLR, 2020. URL [http://proceedings.mlr.press/v119/goyal20a.html](http://proceedings.mlr.press/v119/goyal20a.html).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal等（2020）Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan
    T. Chakaravarthy, Yogish Sabharwal 和 Ashish Verma. Power-bert: 通过逐步词向量消除加速BERT推理。收录于*第37届国际机器学习大会(ICML
    2020)论文集，2020年7月13-18日，虚拟会议*，第119卷，*机器学习研究论文集*，第3690–3699页。PMLR，2020。URL [http://proceedings.mlr.press/v119/goyal20a.html](http://proceedings.mlr.press/v119/goyal20a.html)。'
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language
    models. *ArXiv preprint*, abs/2308.16137, 2023. URL [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han等（2023）Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji 和 Sinong Wang.
    Lm-infinite: 大型语言模型的简单即时长度泛化。*ArXiv预印本*，abs/2308.16137，2023。URL [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137)。'
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense
    information retrieval with contrastive learning. *Transactions on Machine Learning
    Research*, 2022. ISSN 2835-8856. URL [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard等（2022）Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel,
    Piotr Bojanowski, Armand Joulin 和 Edouard Grave. 使用对比学习的无监督密集信息检索。*机器学习研究杂志*，2022。ISSN
    2835-8856。URL [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0)。
- en: 'Jiang et al. (2023a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. In *Proceedings of the 2023 Conference on Empirical Methods in
    Natural Language Processing*. Association for Computational Linguistics, December
    2023a. URL [https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang等（2023a）Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang 和 Lili Qiu.
    Llmlingua: 压缩提示以加速大型语言模型的推理。收录于*2023年自然语言处理经验方法会议论文集*。计算语言学协会，2023年12月。URL [https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736)。'
- en: 'Jiang et al. (2023b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
    Tang, Yiqin Dai, and Jimmy Lin. “low-resource” text classification: A parameter-free
    classification method with compressors. In *Findings of the Association for Computational
    Linguistics: ACL 2023*, pp.  6810–6828, Toronto, Canada, 2023b. Association for
    Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.426. URL [https://aclanthology.org/2023.findings-acl.426](https://aclanthology.org/2023.findings-acl.426).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang等（2023b）Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang, Yiqin
    Dai 和 Jimmy Lin. “低资源”文本分类：一种无参数的分类方法与压缩器。收录于*计算语言学协会会议成果：ACL 2023*，第6810–6828页，加拿大多伦多，2023b。计算语言学协会。doi:
    10.18653/v1/2023.findings-acl.426。URL [https://aclanthology.org/2023.findings-acl.426](https://aclanthology.org/2023.findings-acl.426)。'
- en: 'Kim & Cho (2021) Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer:
    Train once with length drop, use anytime with search. In *Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pp. 
    6501–6511, Online, 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.508.
    URL [https://aclanthology.org/2021.acl-long.508](https://aclanthology.org/2021.acl-long.508).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim & Cho (2021) Gyuwan Kim 和 Kyunghyun Cho。长度自适应变换器：一次训练，随时使用。收录于 *第59届计算语言学协会年会和第11届国际自然语言处理联合会议论文集
    (第1卷：长篇论文)*，第 6501–6511 页，在线，2021。计算语言学协会。doi: 10.18653/v1/2021.acl-long.508。网址
    [https://aclanthology.org/2021.acl-long.508](https://aclanthology.org/2021.acl-long.508)。'
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions:
    A benchmark for question answering research. *Transactions of the Association
    for Computational Linguistics*, 7:452–466, 2019. doi: 10.1162/tacl˙a˙00276. URL
    [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwiatkowski 等人 (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le 和 Slav Petrov。自然问题：问答研究的基准。*计算语言学协会会刊*，7:452–466,
    2019。doi: 10.1162/tacl˙a˙00276。网址 [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026)。'
- en: 'Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation
    for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
    Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis 等人 (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel 和 Douwe Kiela。增强检索生成用于知识密集型 NLP 任务。收录于
    Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan 和 Hsuan-Tien
    Lin (编), *神经信息处理系统进展 33: 神经信息处理系统年会 2020, NeurIPS 2020, 2020年12月6-12日, 虚拟会议*，2020。网址
    [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)。'
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length?, 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph
    E. Gonzalez, Ion Stoica, Xuezhe Ma 和 Hao Zhang。开源 llms 在上下文长度上能真正承诺多长时间？，2023。网址
    [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat)。
- en: 'Li (2023) Yucheng Li. Unlocking context constraints of llms: Enhancing context
    efficiency of llms with self-information-based content filtering. *ArXiv preprint*,
    abs/2304.12102, 2023. URL [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li (2023) Yucheng Li。解锁 llms 的上下文限制：通过基于自信息的内容过滤提升 llms 的上下文效率。*ArXiv 预印本*，abs/2304.12102,
    2023。网址 [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102)。
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models
    use long contexts. *ArXiv preprint*, abs/2307.03172, 2023. URL [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni 和 Percy Liang。迷失在中间：语言模型如何使用长上下文。*ArXiv 预印本*，abs/2307.03172,
    2023。网址 [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)。
- en: 'Modarressi et al. (2022) Ali Modarressi, Hosein Mohebbi, and Mohammad Taher
    Pilehvar. AdapLeR: Speeding up inference by adaptive length reduction. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pp.  1–15, Dublin, Ireland, 2022\. Association for Computational
    Linguistics. doi: 10.18653/v1/2022.acl-long.1. URL [https://aclanthology.org/2022.acl-long.1](https://aclanthology.org/2022.acl-long.1).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '莫达雷西等（2022）阿里·莫达雷西、霍赛因·莫赫比和穆罕默德·塔赫尔·皮列瓦尔。AdapLeR：通过自适应长度缩减加速推理。在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*中，第1–15页，爱尔兰都柏林，2022年。计算语言学协会。doi:
    10.18653/v1/2022.acl-long.1。网址 [https://aclanthology.org/2022.acl-long.1](https://aclanthology.org/2022.acl-long.1)。'
- en: Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress
    prompts with gist tokens. *ArXiv preprint*, abs/2304.08467, 2023. URL [https://arxiv.org/abs/2304.08467](https://arxiv.org/abs/2304.08467).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穆等（2023）杰西·穆、李翔和诺亚·古德曼。学习使用要点令牌压缩提示。*ArXiv 预印本*，abs/2304.08467，2023年。网址 [https://arxiv.org/abs/2304.08467](https://arxiv.org/abs/2304.08467)。
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report. *ArXiv
    preprint*, abs/2309.03450, 2023. URL [https://arxiv.org/abs/2309.03450](https://arxiv.org/abs/2309.03450).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奈贾姆普等（2023）埃里克·奈贾姆普、谢天、广桥弘明、博·庞、肖聪颖、陈兴、杰西·维格、塞米赫·雅武兹、菲利普·拉班、本·克劳斯、森提尔·普鲁什瓦卡姆、汤·牛、沃伊切赫·克里斯金斯基、利迪亚·穆拉霍夫斯卡、普拉夫拉·库马尔·乔贝、亚历克斯·法布里、叶·刘、瑞·孟、李富·图、梅赫娜·巴特、吴建胜、希尔维奥·萨瓦雷斯、周英博、沙菲克·乔提和蔡明雄。Xgen-7b
    技术报告。*ArXiv 预印本*，abs/2309.03450，2023年。网址 [https://arxiv.org/abs/2309.03450](https://arxiv.org/abs/2309.03450)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. *ArXiv preprint*, abs/2304.03442, 2023. URL [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕克等（2023）郞·尚·帕克、约瑟夫·C·奥布赖恩、凯瑞·J·蔡、梅雷迪思·林格尔·莫里斯、珀西·梁和迈克尔·S·伯恩斯坦。生成代理：人类行为的互动模拟体。*ArXiv
    预印本*，abs/2304.03442，2023年。网址 [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442)。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models. *ArXiv preprint*,
    abs/2309.00071, 2023. URL [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2023）彭博文、杰弗里·克斯内尔、范宏路和恩里科·希波尔。Yarn：大语言模型的高效上下文窗口扩展。*ArXiv 预印本*，abs/2309.00071，2023年。网址
    [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071)。
- en: 'Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test
    long: Attention with linear biases enables input length extrapolation. In *International
    Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普雷斯等（2022）奥菲尔·普雷斯、诺亚·史密斯和迈克·刘易斯。训练短时间，测试长时间：具有线性偏差的注意力使输入长度外推成为可能。在*国际学习表征会议*，2022年。网址
    [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0)。
- en: 'Reimers & Gurevych (2019) Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence
    embeddings using Siamese BERT-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp.  3982–3992,
    Hong Kong, China, 2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410.
    URL [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '雷默斯和古雷维奇（2019）尼尔斯·雷默斯和伊琳娜·古雷维奇。Sentence-BERT：使用Siamese BERT网络的句子嵌入。在*2019年实证方法自然语言处理会议和第9届国际自然语言处理联合会议（EMNLP-IJCNLP）论文集*中，第3982–3992页，中国香港，2019年。计算语言学协会。doi:
    10.18653/v1/D19-1410。网址 [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410)。'
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. *ArXiv
    preprint*, abs/2305.14196, 2023. URL [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沙哈姆等（2023）乌里·沙哈姆、毛尔·伊夫吉、阿维亚·埃夫拉特、乔纳森·贝朗特和奥梅尔·莱维。Zeroscrolls：用于长文本理解的零样本基准。*ArXiv
    预印本*，abs/2305.14196，2023年。网址 [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196)。
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can
    be easily distracted by irrelevant context. In *International Conference on Machine
    Learning*, pp.  31210–31227\. PMLR, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, 和 Denny Zhou. 大型语言模型容易被无关上下文分散注意力。发表于*国际机器学习会议*，第
    31210–31227 页。PMLR，2023。
- en: 'Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia,
    Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer
    for large language models. *ArXiv preprint*, abs/2307.08621, 2023. URL [https://arxiv.org/abs/2307.08621](https://arxiv.org/abs/2307.08621).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia,
    Jilong Xue, Jianyong Wang, 和 Furu Wei. 保留网络：大型语言模型的变换器继任者。*ArXiv 预印本*，abs/2307.08621，2023。网址
    [https://arxiv.org/abs/2307.08621](https://arxiv.org/abs/2307.08621)。
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive
    training for context scaling. *ArXiv preprint*, abs/2307.03170, 2023. URL [https://arxiv.org/abs/2307.03170](https://arxiv.org/abs/2307.03170).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, 和 Piotr Miłoś. 聚焦变换器：用于上下文扩展的对比训练。*ArXiv 预印本*，abs/2307.03170，2023。网址
    [https://arxiv.org/abs/2307.03170](https://arxiv.org/abs/2307.03170)。
- en: 'Wu et al. (2022) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    Self-adaptive in-context learning: An information compression perspective for
    in-context example selection and ordering. *ArXiv preprint*, abs/2212.10375, 2022.
    URL [https://arxiv.org/abs/2212.10375](https://arxiv.org/abs/2212.10375).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, 和 Lingpeng Kong. 自适应上下文学习：一种信息压缩视角用于上下文示例选择和排序。*ArXiv
    预印本*，abs/2212.10375，2022。网址 [https://arxiv.org/abs/2212.10375](https://arxiv.org/abs/2212.10375)。
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela
    Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective
    long-context scaling of foundation models. *ArXiv preprint*, abs/2309.16039, 2023.
    URL [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela
    Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, 和 Hao Ma. 有效的基础模型长上下文扩展。*ArXiv
    预印本*，abs/2309.16039，2023。网址 [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039)。
- en: Appendix A Token-level Subsquence Recovery Details
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 令牌级子序列恢复细节
- en: Algorithm 1 Pseudo code of Token-level Subsquence Recovery.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 令牌级子序列恢复的伪代码。
- en: 'Input: The original prompt $\bm{x}$.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：原始提示 $\bm{x}$。
- en: 1:Set the final response list $\bm{y}_{\text{rec}}=\phi$ to the response $\bm{y}_{\text{rec}}$.11:     end if12:end while
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 1：将最终响应列表 $\bm{y}_{\text{rec}}=\phi$ 设置为响应 $\bm{y}_{\text{rec}}$。11：     结束 如果12：结束 当
- en: 'Output: The final response list $\bm{y}_{\text{rec}}$.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：最终的响应列表 $\bm{y}_{\text{rec}}$。
- en: Appendix B Experiment Details
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实验细节
- en: B.1 Dataset Details
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 数据集细节
- en: NaturalQuestions Multi-document QA
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NaturalQuestions 多文档问答
- en: 'A multi-document question-answering dataset, comprising 2,655 problems, was
    built by Liu et al. ([2023](#bib.bib20)) based on the NaturalQuestions dataset (Kwiatkowski
    et al., [2019](#bib.bib16)). This dataset provides a realistic retrieval-augmented
    generation setup that closely resembles commercial search and question-answering
    applications (e.g., Bing Chat). Each example in the dataset contains a question
    and k related documents, utilizing the Contriever retrieval system (Izacard et al.,
    [2022](#bib.bib12)), one of which includes a document with the correct answer.
    To perform this task, the model must access the document containing the answer
    within its input context and use it to answer the question. The dataset’s data
    is sourced from the NaturalQuestions dataset, which contains historical queries
    issued to the Google search engine and human-annotated answers extracted from
    Wikipedia. The average prompt token length in this benchmark is 2,946\. For our
    experiments, we used the version provided by Liu et al. ([2023](#bib.bib20)) that
    includes 20 documents⁸⁸8https://github.com/nelson-liu/lost-in-the-middle. The
    dataset comprises five different ground truth document position settings in the
    prompt: 1st, 5th, 10th, 15th, and 20th.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多文档问答数据集由Liu等人（[2023](#bib.bib20)）基于NaturalQuestions数据集（Kwiatkowski等，[2019](#bib.bib16)）构建，包含2,655个问题。该数据集提供了一个与商业搜索和问答应用（例如，Bing
    Chat）非常相似的真实检索增强生成设置。数据集中每个示例包含一个问题和k个相关文档，利用Contriever检索系统（Izacard等，[2022](#bib.bib12)），其中一个文档包含正确答案。为了完成这个任务，模型必须在输入上下文中访问包含答案的文档，并利用它来回答问题。数据集的数据来源于NaturalQuestions数据集，该数据集包含历史查询和从维基百科提取的人工标注答案。该基准的平均提示令牌长度为2,946。我们的实验使用了Liu等人（[2023](#bib.bib20)）提供的版本，包括20个文档⁸⁸8https://github.com/nelson-liu/lost-in-the-middle。数据集包含五种不同的真实文档位置设置：第1、第5、第10、第15和第20位置。
- en: LongBench
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LongBench
- en: A multi-task long context benchmark consists of 3,750 problems in English and
    includes six categories with a total of 16 tasks. These tasks encompass key long-text
    application scenarios, such as single-document QA, multi-document QA, summarization,
    few-shot learning, synthetic tasks, and code completion. The average prompt token
    length in this benchmark is 10,289\. For our experiments, we used the English
    dataset and evaluation scripts provided by Bai et al. ([2023](#bib.bib1)) for
    this benchmark⁹⁹9https://github.com/THUDM/LongBench.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多任务长文本基准包含3,750个英文问题，涵盖六个类别，共16个任务。这些任务涵盖了关键的长文本应用场景，如单文档问答、多文档问答、摘要、少样本学习、合成任务和代码补全。该基准的平均提示令牌长度为10,289。我们的实验使用了Bai等人（[2023](#bib.bib1)）为此基准提供的英文数据集和评估脚本⁹⁹9https://github.com/THUDM/LongBench。
- en: ZeroSCROLLS
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeroSCROLLS
- en: The multi-task long context benchmark consists of 4,378 problems, including
    four categories with a total of 10 tasks. These tasks cover summarization, question
    answering, aggregated sentiment classification, and information reordering. The
    average prompt token length in this benchmark is 9,788\. For our experiments,
    we used the validation set and evaluation scripts provided by Shaham et al. ([2023](#bib.bib28))
    for this dataset^(10)^(10)10https://www.zero.scrolls-benchmark.com/.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务长文本基准由4,378个问题组成，包括四个类别，共10个任务。这些任务涵盖了摘要、问答、汇总情感分类和信息重新排序。该基准的平均提示令牌长度为9,788。我们的实验使用了Shaham等人（[2023](#bib.bib28)）为此数据集提供的验证集和评估脚本^(10)^(10)10https://www.zero.scrolls-benchmark.com/。
- en: B.2 Other Implementation Details
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 其他实现细节
- en: 'All experiments were conducted using a Tesla V100 (32GB). We use tiktoken^(11)^(11)11https://github.com/openai/tiktoken
    and GPT-3.5-Turbo model to count all the tokens. We set the granular control coefficient
    $k$ used in dynamic compression ratio is set to 0.25. For a fair comparison, we
    only used reordering in the NaturalQuestions Multi-document QA and noted this
    in Table [1](#S5.T1 "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"). We use
    “We can get the answer to this question in the given documents.” as the guideline
    sentence in Equation ([3](#S4.E3 "In Question-Aware Fine-Grained Compression ‣
    4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua ‣
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression")).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '所有实验均使用了 Tesla V100 (32GB)。我们使用了 tiktoken^(11)^(11)11https://github.com/openai/tiktoken
    和 GPT-3.5-Turbo 模型来统计所有的 token。我们将用于动态压缩比的粒度控制系数 $k$ 设置为 0.25。为了公平比较，我们仅在 NaturalQuestions
    多文档 QA 中使用了重新排序，并在表格 [1](#S5.T1 "表 1 ‣ 基线 ‣ 5 实验 ‣ LongLLMLingua: 在长上下文场景中通过提示压缩加速和增强
    LLM") 中注明了这一点。我们在方程 ([3](#S4.E3 "在问题感知的细粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: 在长上下文场景中通过提示压缩加速和增强 LLM") 中使用了“我们可以在给定的文档中找到这个问题的答案。”作为指导句。'
- en: For the baselines experiment, we use the currently recommended strongest model,
    all-mpnet-base-v2^(12)^(12)12https://www.sbert.net/docs/pretrained_models.html,
    as the dense representation model for SentenceBERT. We use the recommended “text-embedding-ada-002”
    as the embedding model for OpenAI Embedding^(13)^(13)13https://platform.openai.com/docs/guides/embeddings/.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线实验，我们使用了当前推荐的最强模型，all-mpnet-base-v2^(12)^(12)12https://www.sbert.net/docs/pretrained_models.html，作为
    SentenceBERT 的稠密表示模型。我们使用推荐的“text-embedding-ada-002”作为 OpenAI Embedding^(13)^(13)13https://platform.openai.com/docs/guides/embeddings/
    的嵌入模型。
- en: Appendix C Ablation Analysis
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 消融分析
- en: '<svg id="A3.F6.pic1" class="ltx_picture" height="491.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,491.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 298.92)"><foreignobject width="556.69"
    height="178.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="261.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to <math id="A3.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math
    ltx_framed ltx_framed_rectangle" display="block"></math>, of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen</foreignobject></g></g></svg>'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A3.F6.pic1" class="ltx_picture" height="491.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,491.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 298.92)"><foreignobject width="556.69"
    height="178.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="261.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to <math id="A3.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math
    ltx_framed ltx_framed_rectangle" display="block"></math>, of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen</foreignobject></g></g></svg>'
- en: 'Figure 6: Comparing the compressed prompt and LLMs’ response before and after
    using Question-aware Fine-grained Compression and Subsequence Recovery($1/\tau$=30x,
    high compression ratio setting) from NaturalQuestions Multi-document QA (Liu et al.,
    [2023](#bib.bib20)) using GPT-3.5-Turbo.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 比较使用问题感知细粒度压缩和子序列恢复($1/\tau$=30x，高压缩比设置)前后的压缩提示和 LLM 的响应，来自 NaturalQuestions
    多文档 QA (Liu et al., [2023](#bib.bib20))，使用 GPT-3.5-Turbo。'
- en: Appendix D Economic Cost
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 经济成本
- en: '|  | Multi-document QA | LongBench | ZeroScolls |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 多文档 QA | LongBench | ZeroScrolls |'
- en: '| --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Original | 4.6 | 31.5 | 30.6 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 4.6 | 31.5 | 30.6 |'
- en: '| Ours | 1.3 | 3.0 | 3.2 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 1.3 | 3.0 | 3.2 |'
- en: 'Figure 7: The inference costs(per 1,000 samples $) for various datasets using
    GPT-3.5-Turbo.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 使用 GPT-3.5-Turbo 的各种数据集的推理成本（每 1,000 个样本 $）。'
- en: 'Table [7](#A4.F7 "Figure 7 ‣ Appendix D Economic Cost ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") presents
    the estimated per 1,000 samples inference costs for various datasets, encompassing
    input prompts and generated output text, based on GPT-3.5-Turbo pricing^(14)^(14)14https://openai.com/pricing.
    Our approach demonstrates substantial savings in computational resources and monetary
    expenses, particularly in long context situations. Cost reductions of $3.3, $28.5,
    and $27.4 per 1,000 samples are observed for Multi-document QA, LongBench, and
    ZeroScrolls, respectively.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [7](#A4.F7 "图 7 ‣ 附录 D 经济成本 ‣ LongLLMLingua: 在长上下文场景中通过提示压缩加速和增强 LLM") 显示了各种数据集每
    1,000 个样本的推理成本，包括输入提示和生成的输出文本，基于 GPT-3.5-Turbo 定价^(14)^(14)14https://openai.com/pricing。我们的方法在计算资源和货币开支方面表现出显著的节省，特别是在长上下文情况下。对于
    Multi-document QA、LongBench 和 ZeroScrolls，每 1,000 个样本的成本分别减少了 $3.3、$28.5 和 $27.4。'
- en: Appendix E Cases Study
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 案例研究
- en: '<svg id="A5.F8.pic1" class="ltx_picture" height="800.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,800.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="772.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in <math id="A5.F8.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"
    class="ltx_Math ltx_framed ltx_framed_rectangle" display="block"></math>. Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019</foreignobject></g></g></svg>'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A5.F8.pic1" class="ltx_picture" height="800.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,800.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="772.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in <math id="A5.F8.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"
    class="ltx_Math ltx_framed ltx_framed_rectangle" display="block"></math>. Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019</foreignobject></g></g></svg>'
- en: 'Figure 8: Cases study on NaturalQuestions Multi-document QA dataset (Liu et al.,
    [2023](#bib.bib20)) in 4x constraint using GPT-3.5-Turbo.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 在 4x 约束下使用 GPT-3.5-Turbo 对 NaturalQuestions 多文档 QA 数据集 (Liu et al., [2023](#bib.bib20))
    进行的案例研究。'
- en: '<svg id="A5.F9.pic1" class="ltx_picture" height="340.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please complete the code given below.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="A5.F9.pic1" class="ltx_picture" height="340.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">压缩提示：请完成下面给出的代码。
- en: '[PRE0]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next line of code: LLMs’ Response:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码：LLMs 的响应：
- en: '[PRE1]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Ground Truth:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 真实情况：
- en: '[PRE2]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Zero-shot LLMs’ Response:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本 LLMs 的响应：
- en: '[PRE3]</foreignobject></g></g></svg>'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</foreignobject></g></g></svg>'
- en: 'Figure 9: Cases study on lcc code completion task in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在 LongBench 基准测试中对 lcc 代码补全任务的案例研究 (Bai et al., [2023](#bib.bib1))，使用 GPT-3.5-Turbo
    和 2,000 个约束条件。
- en: '<svg id="A5.F10.pic1" class="ltx_picture" height="903.29" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,903.29) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="875.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something</foreignobject></g></g></svg>'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A5.F10.pic1" class="ltx_picture" height="903.29" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,903.29) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="875.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something</foreignobject></g></g></svg>'
- en: 'Figure 10: Cases study on trec few-show learning in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在 LongBench 基准测试中对 trec 少样本学习的案例研究 (Bai et al., [2023](#bib.bib1))，使用 GPT-3.5-Turbo
    和 2,000 个约束条件。
