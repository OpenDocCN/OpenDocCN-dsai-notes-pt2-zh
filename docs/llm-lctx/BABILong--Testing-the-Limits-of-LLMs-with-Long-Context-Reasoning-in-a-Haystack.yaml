- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:03:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:03:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'BABILong: 测试大型语言模型在长上下文推理中的极限'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10149](https://ar5iv.labs.arxiv.org/html/2406.10149)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10149](https://ar5iv.labs.arxiv.org/html/2406.10149)
- en: Yuri Kuratov^∗ ^(1,2)   Aydar Bulatov^∗ ²   Petr Anokhin¹   Ivan Rodkin² \ANDDmitry
    Sorokin¹   Artyom Sorokin¹   Mikhail Burtsev³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuri Kuratov^∗ ^(1,2)   Aydar Bulatov^∗ ²   Petr Anokhin¹   Ivan Rodkin² \ANDDmitry
    Sorokin¹   Artyom Sorokin¹   Mikhail Burtsev³
- en: ¹AIRI, Moscow, Russia   ²Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny,
    Russia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹AIRI, 俄罗斯莫斯科   ²MIPT神经网络与深度学习实验室, 俄罗斯多尔戈普鲁德内
- en: ³London Institute for Mathematical Sciences, London, UK
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³伦敦数学科学研究所, 英国伦敦
- en: '{yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, the input context sizes of large language models (LLMs) have
    increased dramatically. However, existing evaluation methods have not kept pace,
    failing to comprehensively assess the efficiency of models in handling long contexts.
    To bridge this gap, we introduce the BABILong benchmark, designed to test language
    models’ ability to reason across facts distributed in extremely long documents.
    BABILong includes a diverse set of 20 reasoning tasks, including fact chaining,
    simple induction, deduction, counting, and handling lists/sets. These tasks are
    challenging on their own, and even more demanding when the required facts are
    scattered across long natural text. Our evaluations show that popular LLMs effectively
    utilize only 10-20% of the context and their performance declines sharply with
    increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
    Generation methods achieve a modest 60% accuracy on single-fact question answering,
    independent of context length. Among context extension methods, the highest performance
    is demonstrated by recurrent memory transformers, enabling the processing of lengths
    up to 11 million tokens. The BABILong benchmark is extendable to any length to
    support the evaluation of new upcoming models with increased capabilities, and
    we provide splits up to 1 million token lengths.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）的输入上下文大小显著增加。然而，现有的评估方法未能跟上这一进步，未能全面评估模型在处理长上下文时的效率。为弥补这一不足，我们引入了BABILong基准测试，旨在测试语言模型在极长文档中跨事实推理的能力。BABILong包含一套多样化的20个推理任务，包括事实链、简单归纳、演绎推理、计数和处理列表/集合。这些任务本身具有挑战性，当所需的事实散布在长自然文本中时，挑战更大。我们的评估表明，流行的LLMs有效利用的上下文仅为10-20%，且随着推理复杂性的增加，其表现急剧下降。在上下文推理的替代方案中，检索增强生成方法在单一事实问答中取得了适度的60%准确率，与上下文长度无关。在上下文扩展方法中，最高性能由递归记忆变换器展示，能够处理长度达到1100万个标记。BABILong基准测试可以扩展到任何长度，以支持对具有更大能力的新模型的评估，我们提供了最长达到100万标记长度的分割。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Today, large language models (LLMs) and neural architectures are continually
    evolving and achieving remarkable improvements, particularly in their ability
    to handle longer contexts (OpenAI, [2023](#bib.bib45); Reid et al., [2024](#bib.bib53);
    Anthropic, [2024](#bib.bib5)). The ability of these models to process and generate
    text based on rich contextual information is crucial for several reasons. For
    example, longer contexts provide more information for the model to condition its
    outputs, leading to more accurate, contextually relevant, and up-to-date responses.
    Furthermore, long-context capabilities can enhance in-context learning by providing
    more in-context examples, instructions to follow, or example trajectories in context
    of reinforcement learning (Chevalier et al., [2023](#bib.bib14); Agarwal et al.,
    [2024](#bib.bib2); Lee et al., [2024](#bib.bib34)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，大型语言模型（LLMs）和神经网络架构不断发展，取得了显著的进步，特别是在处理更长上下文的能力上 (OpenAI, [2023](#bib.bib45);
    Reid et al., [2024](#bib.bib53); Anthropic, [2024](#bib.bib5))。这些模型基于丰富的上下文信息处理和生成文本的能力至关重要。举例来说，更长的上下文为模型提供了更多信息以调整其输出，从而产生更准确、与上下文相关和最新的回应。此外，长上下文能力可以通过提供更多上下文示例、需要遵循的指令或在强化学习的上下文中示例轨迹来增强上下文学习 (Chevalier
    et al., [2023](#bib.bib14); Agarwal et al., [2024](#bib.bib2); Lee et al., [2024](#bib.bib34))。
- en: Despite these advances in models capabilities, the benchmarks used to evaluate
    them have not kept pace. For example, current benchmarks such as Longbench (Bai
    et al., [2023](#bib.bib6)) and L-Eval An et al. ([2023](#bib.bib4)) scale only
    up to 40,000 tokens, while models are capable of hundreds of thousands and millions
    of tokens (OpenAI, [2023](#bib.bib45); Bulatov et al., [2024](#bib.bib11); Gu
    & Dao, [2023](#bib.bib24); Anthropic, [2024](#bib.bib5); Reid et al., [2024](#bib.bib53);
    Liu et al., [2024a](#bib.bib41)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型能力有所提升，但用于评估它们的基准测试却没有跟上。例如，当前的基准测试如 Longbench （Bai 等，[2023](#bib.bib6)）和
    L-Eval An 等 （[2023](#bib.bib4)）的规模仅达到 40,000 个 tokens，而模型能够处理数十万甚至数百万个 tokens （OpenAI，[2023](#bib.bib45)；Bulatov
    等，[2024](#bib.bib11)；Gu & Dao，[2023](#bib.bib24)；Anthropic，[2024](#bib.bib5)；Reid
    等，[2024](#bib.bib53)；Liu 等，[2024a](#bib.bib41)）。
- en: '![Refer to caption](img/c7b4877c270df2d7cab800f25d82e385.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7b4877c270df2d7cab800f25d82e385.png)'
- en: 'Figure 1: a) Generation of BABILong dataset. Facts relevant for the question
    are hidden inside a larger background texts from PG19\. b) Recurrent transformers
    answer questions about facts from very long texts when retrieval augmented generation
    fails. Common RAG method fails to answer questions because order of facts matters.
    GPT-4 LLM effectively uses only about 10% of the full 128K window. Small LMs,
    RMT with GPT-2 (137M) and Mamba (130M) fine-tuned for the task are able to solve
    it, with recurrent memory transformer scoring well up to record 11 111 111 tokens.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：a) BABILong 数据集的生成。与问题相关的事实被隐藏在来自 PG19 的更大背景文本中。b) 当检索增强生成失败时，递归变换器回答有关来自非常长文本的事实的问题。常见的
    RAG 方法因事实顺序的影响而无法回答问题。GPT-4 LLM 实际上只使用了完整 128K 窗口的约 10%。小型 LMs，如针对该任务进行微调的 GPT-2
    (137M) 和 Mamba (130M) 的 RMT，能够解决该问题，递归记忆变换器在记录 11 111 111 tokens 时表现良好。
- en: Creating natural and comprehensive long-context benchmarks that are human labeled
    is very challenging. As a consequence, synthetic benchmarks focusing on variations
    of "needle-in-a-haystack" tasks have become increasingly common (Zhang et al.,
    [2024b](#bib.bib77); Liu et al., [2024a](#bib.bib41); Song et al., [2024b](#bib.bib60);
    Hsieh et al., [2024](#bib.bib29)). One widely used needle-in-a-haystack task involves
    finding specific "needles with magic numbers" in a haystack of Paul Graham’s essays¹¹1[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
    However, the widespread use of this approach has highlighted its limitations -
    it is overly simplistic, and novel long context models often achieve perfect performance,
    as usually demonstrated by fully green heatmaps (Reid et al., [2024](#bib.bib53);
    Cohere, [2024](#bib.bib15); Liu et al., [2024a](#bib.bib41); Wang et al., [2024c](#bib.bib67)).
    This shows that while it serves well as a basic verification tool, it is not a
    rigorous benchmark that can effectively challenge and differentiate advanced long-context
    models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自然且全面的长文本基准测试，并由人工标注，是非常具有挑战性的。因此，聚焦于“针在干草堆里”任务变体的合成基准测试变得越来越普遍 （Zhang 等，[2024b](#bib.bib77)；Liu
    等，[2024a](#bib.bib41)；Song 等，[2024b](#bib.bib60)；Hsieh 等，[2024](#bib.bib29)）。一个广泛使用的“针在干草堆里”任务涉及在
    Paul Graham 的论文中找到特定的“带有魔法数字的针”¹¹1[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。然而，这种方法的广泛使用突出了它的局限性——它过于简单，新的长文本模型通常会实现完美的性能，如通常通过完全绿色的热图来展示 （Reid
    等，[2024](#bib.bib53)；Cohere，[2024](#bib.bib15)；Liu 等，[2024a](#bib.bib41)；Wang
    等，[2024c](#bib.bib67)）。这表明，虽然它作为基本验证工具效果良好，但它不是一个可以有效挑战和区分先进长文本模型的严格基准测试。
- en: To bridge this gap, we introduce the BABILong benchmark, designed to test language
    models’ ability to reason across facts distributed in extremely long documents.
    BABILong includes a diverse set of 20 reasoning tasks, including fact chaining,
    simple induction, deduction, counting, and handling lists/sets, that were designed
    as prerequisites for any system that aims to be capable of conversing with a human (Weston
    et al., [2016](#bib.bib68)). As a source of long natural documents we use books
    from PG19 corpora (Rae et al., [2020](#bib.bib50)). In this way, BABILong allows
    the construction of tasks of almost arbitrary length, in order to adapt them to
    the evaluation of new, more powerful models in an extensible and controllable
    way. We provide sets of predefined lengths with splits up to 1 million tokens,
    and we evaluate models on samples with up to 11 million tokens.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一差距，我们引入了BABILong基准，它旨在测试语言模型在极长文档中跨事实推理的能力。BABILong包括20种推理任务，包括事实链、简单归纳、演绎推理、计数和处理列表/集合，这些任务是任何旨在能够与人类对话的系统的先决条件（Weston等，[2016](#bib.bib68)）。作为长自然文档的来源，我们使用PG19语料库中的书籍（Rae等，[2020](#bib.bib50)）。通过这种方式，BABILong允许构建几乎任意长度的任务，以便以可扩展和可控的方式将其适应于对新、更多强大模型的评估。我们提供了预定义长度的集合，分割长度达到100万个标记，并且我们在样本上评估模型，长度可达1100万个标记。
- en: We find that popular LLMs effectively use only 10-20% of the context, with performance
    declining sharply as length and task complexity increase. Retrieval-Augmented
    Generation methods achieve a modest 60% accuracy in answering single-fact questions,
    regardless of context length. Among other methods, Mamba and Recurrent Memory
    Transformers (RMT) show the highest performance, with RMT capable of processing
    lengths up to 11 million tokens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现流行的LLM仅有效利用了10-20%的上下文，随着长度和任务复杂性的增加，性能急剧下降。检索增强生成方法在回答单一事实问题时取得了60%的适中准确率，与上下文长度无关。在其他方法中，Mamba和递归记忆变换器（RMT）表现最佳，其中RMT能够处理长度高达1100万个标记。
- en: 'The main contributions of our work are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献如下：
- en: 1\. We introduce BABILong, a novel scalable generative multi-task benchmark
    for evaluating the performance of NLP models in processing arbitrarily long documents
    with distributed facts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了BABILong，这是一个新颖的可扩展生成多任务基准，用于评估NLP模型在处理任意长文档中的分布事实的性能。
- en: 2\. We evaluate over 20 recent long-input language models with various sizes,
    architectures, and context extension methods on BABILong.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在BABILong上评估了20多个近期长输入语言模型，涵盖了各种尺寸、架构和上下文扩展方法。
- en: 3\. We find that popular LLMs effectively utilize only 10-20% of the context,
    with performance degrading sharply as reasoning complexity increases. Retrieval
    augmented generation fails to demonstrate good scores but fine-tuning for specific
    task helps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，流行的LLM仅有效利用了10-20%的上下文，随着推理复杂性的增加，性能急剧下降。检索增强生成未能表现出良好的得分，但针对特定任务的微调有所帮助。
- en: 4\. We demonstrate successful in domain single fact question answering with
    the recurrent memory transformer on input texts up to 11 million tokens, setting
    a new record for the sequence size processed by a single model, extending the
    known capabilities of neural networks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在输入文本长达1100万个标记的情况下使用递归记忆变换器，成功展示了领域单一事实问答的能力，创下了单个模型处理序列大小的新纪录，扩展了神经网络的已知能力。
- en: The BABILong benchmark data and code for evaluation are available on GitHub
    ²²2[https://github.com/booydar/babilong](https://github.com/booydar/babilong).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong基准数据和评估代码可在GitHub ²²2[https://github.com/booydar/babilong](https://github.com/booydar/babilong)
    上获取。
- en: 2 The BABILong Benchmark for Long Context Processing
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BABILong基准：长上下文处理
- en: 'Table 1: The first ten tasks of BABILong with the number of supporting and
    distracting facts. The last column displays the performance of LLMs on each task
    in the absence of background text. Each dot represents one of the selected models,
    while the blue bars indicate the median accuracy across tested models.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：BABILong的前十个任务及其支持和干扰事实的数量。最后一列显示了LLM在没有背景文本的情况下对每个任务的表现。每个点代表所选模型之一，而蓝色条形图表示测试模型的中位准确率。
- en: '| Task | Name | facts | relevant facts | LLMs answer accuracy |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 名称 | 事实 | 相关事实 | LLM的回答准确性 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | per task | per task | without background text (0K) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 每个任务 | 每个任务 | 无背景文本 (0K) |'
- en: '| qa1 | single supporting fact | 2-10 | 1 | ![[Uncaptioned image]](img/e30c6ce2910a48cdbcdd1928c6cce429.png)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| qa1 | 单一支持性事实 | 2-10 | 1 | ![[无标题图片]](img/e30c6ce2910a48cdbcdd1928c6cce429.png)
    |'
- en: '| qa2 | two supporting facts | 2-68 | 2 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| qa2 | 两个支持性事实 | 2-68 | 2 |'
- en: '| qa3 | three supporting facts | 4-320 | 3 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| qa3 | 三个支持性事实 | 4-320 | 3 |'
- en: '| qa4 | two arg relations | 2 | 1 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| qa4 | 两个论证关系 | 2 | 1 |'
- en: '| qa5 | three arg relations | 2-126 | 1 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| qa5 | 三个论证关系 | 2-126 | 1 |'
- en: '| qa6 | yes-no questions | 2-26 | 1 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| qa6 | 是非问题 | 2-26 | 1 |'
- en: '| qa7 | counting | 2-52 | 1-10 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| qa7 | 计数 | 2-52 | 1-10 |'
- en: '| qa8 | lists-sets | 2-50 | 1-8 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| qa8 | 列表-集合 | 2-50 | 1-8 |'
- en: '| qa9 | simple negation | 2-10 | 1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| qa9 | 简单否定 | 2-10 | 1 |'
- en: '| qa10 | indefinite knowledge | 2-10 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| qa10 | 不确定知识 | 2-10 | 1 |'
- en: 'The fundamental concept behind the Benchmark for Artificial Intelligence for
    Long-context evaluation is to extend the length of existing tasks to evaluate
    the ability of generative models to efficiently handle long contexts. Solving
    tasks with a long context size requires the model to distinguish important information
    from large amounts of irrelevant details. To simulate this behavior we "hide"
    the sentences of the original task between the sentences of irrelevant text that
    is drawn from another closely related distribution (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")a).
    Examples are constructed by gradually adding new sentences from the background
    dataset in their natural order until the augmented sample reaches the desired
    length. This way, we are not bound by the length of the original task itself,
    making it possible to assess even the longest available models with context sizes
    up to millions of tokens. For background text we use books from the PG19 dataset
     (Rae et al., [2020](#bib.bib50)) due to the substantial book lengths and naturally
    occurring long contexts. The model is required first to distinguish the sentences
    related to the original task, then memorize and subsequently utilize them to generate
    the correct solution.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '基准测试的基本概念是将现有任务的长度扩展，以评估生成模型高效处理长上下文的能力。解决具有长上下文的任务要求模型从大量无关的细节中区分出重要信息。为了模拟这种行为，我们将原始任务的句子“隐藏”在从另一个相关分布中提取的无关文本句子之间（参见图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ BABILong: 测试LLMs在长上下文推理中的极限")a）。通过逐渐将来自背景数据集的新句子按自然顺序添加到示例中，直到增强样本达到所需的长度。这样，我们不受原始任务长度的限制，可以评估甚至长达数百万标记的最长模型。我们使用
    PG19 数据集中的书籍作为背景文本（Rae 等，[2020](#bib.bib50)），因为其书籍长度大且自然存在长上下文。模型首先需要区分与原始任务相关的句子，然后记住并随后利用这些句子生成正确的解决方案。'
- en: 'In this work we extend the bAbI benchmark (Weston et al., [2016](#bib.bib68)),
    which consists of 20 tasks designed to evaluate basic aspects of reasoning. These
    tasks are generated by simulating interactions among characters and objects across
    various locations, each represented as a fact, such as "Mary traveled to the office."
    The challenge is to answer questions based on the facts generated in the current
    simulation, such as "Where is Mary?" The tasks in bAbI vary in the number of facts,
    question complexity, and the reasoning skills they assess, including spatial and
    temporal reasoning, deduction, and coreference resolution. In our paper, we label
    these tasks from ’QA1’ to ’QA20’. The first ten tasks, as shown in Table [1](#S2.T1
    "Table 1 ‣ 2 The BABILong Benchmark for Long Context Processing ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack") demonstrate that
    current LLMs exhibit mixed performance even without distractor texts, indicating
    that the BABILong tasks span a broad spectrum of difficulty and allow for testing
    models across various performance dimensions. Details and performance metrics
    for the bAbI tasks, along with examples of BABILong samples generated using our
    pipeline, can be found in Appendix [L](#A12 "Appendix L BABILong Task Examples
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们扩展了bAbI基准测试（Weston等，[2016](#bib.bib68)），它包含了20个任务，用于评估推理的基本方面。这些任务通过模拟角色和对象在不同位置之间的互动生成，每个互动被表示为一个事实，例如“玛丽去了办公室。”挑战在于根据当前模拟中生成的事实回答问题，例如“玛丽在哪里？”bAbI中的任务在事实数量、问题复杂性和评估的推理技能（包括空间和时间推理、推理和共指解析）方面有所不同。在我们的论文中，我们将这些任务标记为’QA1’到’QA20’。如表[1](#S2.T1
    "Table 1 ‣ 2 The BABILong Benchmark for Long Context Processing ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack")所示，前十个任务表明，即使没有干扰文本，当前的LLMs也表现出混合的性能，这表明BABILong任务涵盖了广泛的难度范围，并允许在各种性能维度上测试模型。bAbI任务的详细信息和性能指标，以及使用我们管道生成的BABILong样本的示例，可以在附录[L](#A12
    "Appendix L BABILong Task Examples ‣ BABILong: Testing the Limits of LLMs with
    Long Context Reasoning-in-a-Haystack")中找到。'
- en: As evident in the following sections, these seemingly simple tasks pose significant
    challenges to language models. Although filtering facts from background text might
    be straightforward, models encounter next challenges of finding supporting facts
    among distractors and performing types of reasoning such as counting that are
    especially difficult for LLMs. Additionally, most NLP benchmarks are vulnerable
    to data leakage to training sets of modern large language models (Sainz et al.,
    [2023](#bib.bib55)). Generated benchmarks, such as bAbI and BABILong, are immune
    to this type of contamination.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如以下部分所示，这些看似简单的任务对语言模型提出了重大挑战。尽管从背景文本中过滤事实可能很简单，但模型面临着从干扰项中找到支持事实和执行如计数等推理类型的下一步挑战，这些都是LLMs特别困难的。此外，大多数NLP基准测试易受数据泄漏到现代大型语言模型训练集的影响（Sainz等，[2023](#bib.bib55)）。生成的基准测试，如bAbI和BABILong，对这种污染类型具有免疫力。
- en: In this work we deliberately employ simple algorithmic tasks to underscore the
    fundamental limitations of current models in collecting evidence over long contexts
    even for basic reasoning. The brevity and similarity of the task sentences also
    enable the model distinguish them from seemingly close background text with the
    assistance of few-shot examples. This difference in distributions enables the
    scalability of BABILong to large amounts of diverse noise. Nevertheless, the BABILong
    approach can be applied to incorporate more complex tasks, using the same strategy
    of mixing task sentences with background text.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们故意使用简单的算法任务来强调当前模型在处理长期上下文中的基本局限性，即使是对于基本推理任务也是如此。任务句子的简洁性和相似性还使得模型能够通过少量示例将它们与看似相近的背景文本区分开。这种分布上的差异使得BABILong能够扩展到大量的多样化噪声。尽管如此，BABILong方法也可以应用于更复杂的任务，使用将任务句子与背景文本混合的相同策略。
- en: 3 Benchmarking Results
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基准测试结果
- en: 'Table 2: BABILong is a challenging benchmark for current long-context models.
    Even models that claim to support 128K tokens experience degradation beyond 10%
    of their input capacity. RAG methods do not help, while fine-tuning of small scale
    models (RMT 137M and Mamba 130M) shows that the tasks are solvable. Values represent
    average accuracy over QA1-QA5 tasks from BABILong.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：BABILong 是当前长上下文模型的一个挑战性基准。即使是声称支持 128K 标记的模型，其输入容量超过 10% 后也会出现性能下降。RAG
    方法无济于事，而小规模模型 (RMT 137M 和 Mamba 130M) 的微调显示任务是可解的。数值表示 BABILong 的 QA1-QA5 任务的平均准确率。
- en: '![[Uncaptioned image]](img/2a38835e3275cd9ec1c61f151ccf2d0a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/2a38835e3275cd9ec1c61f151ccf2d0a.png)'
- en: 'To maximize value for the research community, we have included models with
    the highest number of monthly downloads from the Hugging Face platform in our
    evaluation such as LLama-3 (AI@Meta, [2024](#bib.bib3)); 32k-64k – Mistral (Jiang
    et al., [2023](#bib.bib31)), Mixtral (Jiang et al., [2024](#bib.bib32)); 128k
    – ChatGLM3 (Du et al., [2022](#bib.bib20)), Phi-3 (Abdin et al., [2024](#bib.bib1)),
    Command-R (Cohere, [2024](#bib.bib15)); 200k – Yi (Young et al., [2024](#bib.bib72));
    including long-context fine-tuning: LongChat (Li et al., [2023a](#bib.bib37)),
    LLama-2-7b-32k ³³3[https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct](https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct),
    LongAlpaca (Chen et al., [2023](#bib.bib13)); long-context adaptation methods:
    Yarnv2 Mistral (Peng et al., [2023b](#bib.bib47)), Mistral and LLama-2 with Activation
    Beacons (Zhang et al., [2024a](#bib.bib76)). As a reference, we included GPT-4
    (gpt-4-0125-preview), currently the most powerful model available. Retrieval-augmented
    generation was also tested, as it represents a common solution for long document
    QA. As alternatives to traditional architectures, we considered the Mamba (Gu
    & Dao, [2023](#bib.bib24)), Jamba (Lieber et al., [2024](#bib.bib40)) and Recurrent
    Memory Transformer (RMT) (Bulatov et al., [2022](#bib.bib10)). Summary of evaluation
    results is presented in the Table [2](#S3.T2 "Table 2 ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").
    The table reports average accuracy of models on the first 5 tasks (QA1-QA5) of
    BABILong for different context sizes. For evaluation details for each task see
    Table [3](#A4.T3 "Table 3 ‣ Appendix D Detailed LLM evaluation on BABILong tasks
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    and Appendix [C](#A3 "Appendix C Details on RMT and Mamba fine-tuning on BABILong
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '为了最大化对研究社区的价值，我们在评估中包括了 Hugging Face 平台上每月下载量最高的模型，例如 LLama-3 (AI@Meta, [2024](#bib.bib3))；32k-64k
    – Mistral (Jiang et al., [2023](#bib.bib31)), Mixtral (Jiang et al., [2024](#bib.bib32))；128k
    – ChatGLM3 (Du et al., [2022](#bib.bib20)), Phi-3 (Abdin et al., [2024](#bib.bib1)),
    Command-R (Cohere, [2024](#bib.bib15))；200k – Yi (Young et al., [2024](#bib.bib72))；包括长上下文微调：LongChat
    (Li et al., [2023a](#bib.bib37)), LLama-2-7b-32k [https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct](https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct),
    LongAlpaca (Chen et al., [2023](#bib.bib13))；长上下文适应方法：Yarnv2 Mistral (Peng et
    al., [2023b](#bib.bib47)), Mistral 和 LLama-2 配合激活信标 (Zhang et al., [2024a](#bib.bib76))。作为参考，我们还包括了当前最强大的模型
    GPT-4 (gpt-4-0125-preview)。检索增强生成也进行了测试，因为它代表了长文档问答的常见解决方案。作为传统架构的替代方案，我们考虑了 Mamba
    (Gu & Dao, [2023](#bib.bib24)), Jamba (Lieber et al., [2024](#bib.bib40)) 和递归记忆变换器
    (RMT) (Bulatov et al., [2022](#bib.bib10))。评估结果的总结见表 [2](#S3.T2 "Table 2 ‣ 3 Benchmarking
    Results ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")。表中报告了模型在
    BABILong 的前 5 个任务 (QA1-QA5) 中的平均准确性，按不同上下文大小分列。每个任务的评估细节请参见表 [3](#A4.T3 "Table
    3 ‣ Appendix D Detailed LLM evaluation on BABILong tasks ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack") 和附录 [C](#A3 "Appendix
    C Details on RMT and Mamba fine-tuning on BABILong ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")。'
- en: 3.1 Evaluation of Effective Context Size
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 有效上下文大小的评估
- en: '![Refer to caption](img/b9ae6da4ece3a69b21ea035c49de96d3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9ae6da4ece3a69b21ea035c49de96d3.png)'
- en: 'Figure 2: LLMs struggle to answer questions about facts in texts larger than
    10,000 tokens. The plots demonstrate how the performance of selected leading models
    deteriorates with increasing context size. For single supporting fact questions
    (QA1), the majority of models perform well up to 4,000 tokens. However, when a
    correct response requires two (QA2) or three (QA3) facts, LLMs fail to achieve
    satisfactory accuracy.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLMs 在处理超过 10,000 个标记的文本中的事实性问题时表现不佳。图示展示了选定的领先模型在上下文大小增加时表现的恶化情况。对于单一支持事实的问题
    (QA1)，大多数模型在 4,000 个标记以内表现良好。然而，当正确的回答需要两个 (QA2) 或三个 (QA3) 事实时，LLMs 无法达到令人满意的准确率。
- en: One of the most important questions regarding performance of long-context models
    is how effectively they utilize the input context. Ideally, a model should maintain
    uniformly high performance regardless of the input size. For instance, if an LLM
    can process 128K tokens, it is expected to use all of this context in addressing
    the user’s task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于长上下文模型性能的一个重要问题是它们如何有效地利用输入上下文。理想情况下，模型应保持均匀的高性能，无论输入大小如何。例如，如果一个LLM可以处理128K个标记，预计它能够在处理用户任务时使用所有这些上下文。
- en: We evaluated the performance of models on question-answering tasks with varying
    numbers of supporting facts (QA1-QA3) to study how LLMs utilize the available
    context. Here, we distinguish between a QA task, which requires a single correct
    answer, and an information retrieval task, which should generate a list of relevant
    facts or references to information sources. We consider performance satisfactory
    if the accuracy of an answer exceeds 85% and a complete failure if it is below
    30%. ⁴⁴4This definition of satisfactory performance is not universal and should
    be adapted to the specific task at hand.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了模型在不同数量支持事实（QA1-QA3）下的问答任务表现，以研究LLMs如何利用可用上下文。在这里，我们区分了需要单一正确答案的问答任务和需要生成相关事实或信息源引用的检索任务。如果回答的准确率超过85%，我们认为表现令人满意；如果低于30%，则视为完全失败。⁴⁴4这一令人满意的表现定义并非普遍适用，应根据具体任务进行调整。
- en: 'Our benchmarking results show that current LLMs do not efficiently use their
    full context (Fig.[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation of Effective Context Size
    ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack")). Only 15 out of 24 tested LLMs were able to correctly
    answer 85% or more of the questions for any of QA1-QA3 tasks in a baseline setting
    without any background distractor text. Even for the the simplest task involving
    a single supporting fact (QA1), the best models are able to use only up to 4K
    tokens efficiently, except for GPT-4, which performs well till 16K. The range
    of full context utilization varies from 5% to 25%. When two supporting facts are
    required for an answer, only GPT-4 can solve the task without background text.
    When facts are embedded within texts, all tested LLMs fall below 85% performance(Fig.[2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation of Effective Context Size ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")QA2).
    The task with three supporting facts proves extremely challenging to current LLMs,
    with the best scores falling below 70% (Fig.[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation
    of Effective Context Size ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")QA3).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的基准测试结果显示，目前的LLMs无法有效地利用其全部上下文（图[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation of
    Effective Context Size ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")）。在没有任何背景干扰文本的基准设置中，24个测试LLMs中只有15个能够正确回答85%或更多的问题，即QA1-QA3任务。即使是涉及单一支持事实的最简单任务（QA1），最好的模型也只能有效利用最多4K个标记，除了GPT-4，其表现良好直至16K。完整上下文的利用范围从5%到25%不等。当需要两个支持事实来回答时，只有GPT-4能够在没有背景文本的情况下解决任务。当事实嵌入文本中时，所有测试的LLMs表现都低于85%（图[2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation of Effective Context Size ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")QA2）。三项支持事实的任务对当前LLMs来说极具挑战，最佳分数低于70%（图[2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation of Effective Context Size ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")QA3）。'
- en: 'Going deeper in performance of specific models presented in the Table [2](#S3.T2
    "Table 2 ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs with
    Long Context Reasoning-in-a-Haystack") we found the following. Yarn fails to extend
    to longer contexts despite showing stable results in long-context language modeling (Peng
    et al., [2023b](#bib.bib47)). LongChat, LongAlpaca, and both LLama2-7B-32K and
    LLama2-7B-32K-instruct models, even when fine-tuned on 32K lengths, failed to
    perform well on 32K tokens. Activation Beacon performed better than Yarn context
    extension method for Mistral 7B, but still achieved low results (< 40%) on 32K
    contexts. In contrast, Mistral and Mixtral, trained on lengths up to 32K, performed
    well on these lengths. Yi-9B-200k, trained on sequences up to 200K, shows less
    than 30% on 64K tokens and more. Yi-34B-200k shows very promising and stable results
    on lengths up to 64K, but unfortunately we were not able to run it on 128K tokens.
    Phi-3-Mini drops significantly from 64K to 128K, reaching less than 10%, while
    Phi-3-Medium maintains 30% at 128K. Jamba-v1 and Phi-3-Mini show close results,
    but Jamba-v1 does not have drop at 128K and shows 34% on this length. Command-R
    and Phi-3-Medium are the most robust to longer contexts, but start to lose performance
    more sharply at 128K. Phi-3-Medium and Command-R show results very close to GPT-4
    at 32K+ contexts.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地分析表格[2](#S3.T2 "表2 ‣ 3 基准测试结果 ‣ BABILong：测试长上下文推理的LLMs极限")中展示的特定模型的性能，我们发现以下内容。尽管在长上下文语言建模中表现稳定（Peng
    et al., [2023b](#bib.bib47)），Yarn未能扩展到更长的上下文。LongChat、LongAlpaca以及LLama2-7B-32K和LLama2-7B-32K-instruct模型，即使在32K长度上进行微调，也未能在32K令牌上表现良好。Activation
    Beacon在Mistral 7B上的表现优于Yarn的上下文扩展方法，但在32K上下文中仍然取得了较低的结果（< 40%）。相比之下，Mistral和Mixtral在长度达到32K时表现良好。Yi-9B-200k在处理高达200K的序列时，64K令牌及以上的表现不到30%。Yi-34B-200k在长度高达64K时表现出非常有希望且稳定的结果，但遗憾的是我们未能在128K令牌上进行测试。Phi-3-Mini在64K到128K时显著下降，低于10%，而Phi-3-Medium在128K时保持30%。Jamba-v1和Phi-3-Mini的结果接近，但Jamba-v1在128K时没有下降，且在该长度上显示34%。Command-R和Phi-3-Medium对更长的上下文最为稳健，但在128K时开始更急剧地丧失性能。Phi-3-Medium和Command-R在32K+上下文中显示出非常接近GPT-4的结果。
- en: 3.2 Retrieval-Augmented Generation Does Not Perform Well on BABILong
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 检索增强生成在BABILong上的表现不佳
- en: '![Refer to caption](img/5cdcc5790950fa29b16e6f7cd27aefe9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5cdcc5790950fa29b16e6f7cd27aefe9.png)'
- en: 'Figure 3: Fine-tuning but not RAG solves BABILong. a) RAG on QA1 task. Retrieval
    by chunks with size 512 tokens (RAG-C) fails to improve GPT-4 and Llama-3 performance
    on long-context tasks. RAG-S, which retrieves by sentences achieves better results,
    but further increasing the number of retrieved sentences from top-5 to top-20
    does not help. b) Task specific fine-tuning. Finetuned Mamba achieves the best
    overall results, greatly outperforming RAG models. However, processing sequences
    longer than 128k is extremely slow due to technical limitations. On the other
    hand, RMT shines on extremely long sequences, managing to keep high accuracy up
    to 10M tokens.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：微调而非RAG解决BABILong问题。a) RAG在QA1任务中的表现。通过512个令牌大小的块进行检索（RAG-C）未能改善GPT-4和Llama-3在长上下文任务中的表现。RAG-S通过句子进行检索，取得了更好的结果，但进一步将检索的句子数量从前5个增加到前20个并没有帮助。b)
    任务特定的微调。微调后的Mamba取得了最佳的整体结果，远远超过了RAG模型。然而，由于技术限制，处理超过128k的序列非常缓慢。另一方面，RMT在极长序列上表现出色，能够保持高准确率，直到10M令牌。
- en: Retrieval-Augmented Generation (RAG) is a popular solution for language models
    to handle large amounts of text. In RAG relevant parts of texts are retrieved
    from a large dataset on the first stage. Then, the language model uses input augmented
    with retrieved texts to generate the final response. In the case of BABILong,
    we expect RAG to extract all the facts relevant to a question from a long input
    text and then place them in the context of the model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种流行的解决方案，用于让语言模型处理大量文本。在RAG中，相关的文本部分在第一阶段从大型数据集中检索出来。然后，语言模型使用包含检索文本的输入生成最终响应。在BABILong的情况下，我们期望RAG从长输入文本中提取与问题相关的所有事实，然后将它们置于模型的上下文中。
- en: 'We experiment with two options: (1) retrieval by chunks of size 512 tokens,
    denoted RAG-C and (2) retrieval by sentences, called RAG-S. For details of evaluation
    and RAG pipelines with GPT4 and Llama-3 please refer to Appendix [G](#A7 "Appendix
    G Details of the RAG Pipeline ‣ BABILong: Testing the Limits of LLMs with Long
    Context Reasoning-in-a-Haystack"). The findings from the QA1 task, depicted in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Retrieval-Augmented Generation Does Not Perform
    Well on BABILong ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")a, indicate that retrieval performance
    using sentence chunks is superior to that of 512-token segments, with a notable
    decrease in accuracy observed already after 16k token context length. However,
    this superiority is task-specific and may not translate effectively to real-world
    applications due to the potential for information loss in smaller chunks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了两种选项：（1）按512个token大小的块进行检索，标记为RAG-C，以及（2）按句子进行检索，称为RAG-S。有关评估和RAG管道在GPT4和Llama-3中的详细信息，请参阅附录 [G](#A7
    "附录 G RAG管道详细信息 ‣ BABILong：测试LLMs在长上下文推理中的极限")。从QA1任务中得出的发现，如图 [3](#S3.F3 "图 3
    ‣ 3.2 基于检索的生成在BABILong上的表现不佳 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")a所示，使用句子块的检索性能优于512-token段，且在16k
    token上下文长度后准确性显著下降。然而，这种优越性是任务特定的，可能无法有效地转化为现实世界的应用，因为较小块的信息丢失潜力较大。
- en: 'The RAG pipeline with GPT-4-turbo shows scalable but weak performance on BABILong
    for sentence embeddings and poor scalability with chunk embeddings (see Fig. [3](#S3.F3
    "Figure 3 ‣ 3.2 Retrieval-Augmented Generation Does Not Perform Well on BABILong
    ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack")a). The weak performance of RAG might be attributable
    to the temporal dependencies inherent in the task, where the relevant fact is
    positioned at the end of the text. In QA2 and QA3, retrieval fails dramatically
    with accuracy plummeting below random guessing. This lack of success is attributable
    to the specific demands of these tasks, which require the retrieval of multiple
    (two or three) supporting facts to generate accurate responses. For example, in
    instances where the key facts are "Mary got the milk there." and "Mary travelled
    to the hallway.", with the query being "Where is the milk?", the retrieval system
    may successfully identify the first fact but fail to retrieve the second due to
    insufficient similarity between the question and the latter fact. The default
    retrieval algorithm’s lack of temporal consideration and limitations in the similarity
    function underscore the necessity for additional methods in tasks with multi-hop
    reasoning and temporal dynamics.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-4-turbo的RAG管道在BABILong中的句子嵌入上表现可扩展但较弱，对块嵌入的可扩展性差（见图 [3](#S3.F3 "图 3 ‣
    3.2 基于检索的生成在BABILong上的表现不佳 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")a）。RAG的弱性能可能归因于任务固有的时间依赖性，即相关事实位于文本末尾。在QA2和QA3中，检索失败显著，准确率跌至低于随机猜测。这一失败归因于这些任务的具体要求，需要检索多个（两个或三个）支持性事实以生成准确的响应。例如，在关键事实为“Mary
    got the milk there.” 和 “Mary travelled to the hallway.” 的情况下，如果查询为“Where is the
    milk?”，检索系统可能成功识别第一个事实，但由于问题与第二个事实之间相似度不足而未能检索到第二个事实。默认的检索算法缺乏时间考虑和相似度函数的局限性突显了在多跳推理和时间动态任务中需要额外方法的必要性。
- en: 3.3 Fine-Tuning Models on BABILong
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 在BABILong上微调模型
- en: 'We performed fine-tuning experiments with GPT-3.5-Turbo, Mistral-7B-Instruct-v0.2,
    RMT with GPT-2 (137M) backbone, and Mamba (130M) models. Fine-tuning results are
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Retrieval-Augmented Generation Does Not Perform
    Well on BABILong ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")b and Appendix [I](#A9 "Appendix I
    LLM fine-tuning ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    Figure [8](#A9.F8 "Figure 8 ‣ Appendix I LLM fine-tuning ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对GPT-3.5-Turbo、Mistral-7B-Instruct-v0.2、RMT与GPT-2 (137M)骨干以及Mamba (130M)模型进行了微调实验。微调结果见图 [3](#S3.F3
    "图 3 ‣ 3.2 基于检索的生成在BABILong上的表现不佳 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")b和附录 [I](#A9
    "附录 I LLM微调 ‣ BABILong：测试LLMs在长上下文推理中的极限") 图 [8](#A9.F8 "图 8 ‣ 附录 I LLM微调 ‣ BABILong：测试LLMs在长上下文推理中的极限")。
- en: 'RMT with a GPT-2 (Radford et al., [2019](#bib.bib49)) backbone model is trained
    on each task individually with a segment size of 512 and memory size of 16\. Train
    and evaluation splits of each task contain 10000 and 1000 samples, respectively,
    with a number of facts in each sample ranging from 2 to 320 depending on the task.
    A curriculum strategy is employed, initiating training on short sequences that
    fit in a single segment and then gradually introducing longer sequences once the
    training converges. During each curriculum stage $n$ segments. We provide details
    of training and evaluation procedures in Appendix [E](#A5 "Appendix E RMT Training
    and Evaluation Details ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 GPT-2 (Radford et al., [2019](#bib.bib49)) 作为骨干模型的 RMT 在每个任务上单独训练，段大小为 512，记忆大小为
    16。每个任务的训练和评估分割包含 10000 和 1000 个样本，样本中的事实数量从 2 到 320 不等，具体取决于任务。采用了课程策略，首先在适合单个段的短序列上开始训练，然后在训练收敛后逐步引入较长的序列。在每个课程阶段，$n$
    段。有关训练和评估程序的详细信息，请参见附录 [E](#A5 "附录 E RMT 训练和评估细节 ‣ BABILong: 测试 LLM 在长上下文推理中的极限")。'
- en: RMT model trained on 32 segments totalling in 16K tokens demonstrates strong
    performance on this length. Notably, RMT outperforms GPT-4 significantly, underscoring
    the efficiency of memory mechanism in processing long context. Even more importantly,
    the power of recurrent models extends to sequences longer than the training size.
    RMT shows consistent performance on longer sequences, up to 128k tokens, with
    only a marginal quality degradation. Surprisingly, with context sizes scaling
    to 1 million, 10 million tokens, and even 11.1 million tokens, which is over 600
    times of the training length, the recurrent model persistently outperform the
    larger counterparts utilizing RAG.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 32 个段、总计 16K 令牌上训练的 RMT 模型在这种长度上表现出色。值得注意的是，RMT 显著优于 GPT-4，突显了记忆机制在处理长上下文中的效率。更重要的是，递归模型的能力扩展到训练大小之外的序列。RMT
    在较长序列上表现稳定，最长可达 128k 令牌，质量仅有轻微下降。令人惊讶的是，当上下文大小扩展到 100 万、1000 万，甚至 1110 万令牌时，超过了训练长度的
    600 倍，递归模型仍然持续优于使用 RAG 的较大模型。
- en: Finetuned recurrent models, Mamba and RMT perform equally well on QA1, however
    due to the technical limitations of the Mamba implementation, the inference beyond
    128k was extremely slow, which makes it nearly impossible to process longer sequences.
    RMT greatly outperforms retrieval-augmented models and is able to process sequences
    up to 10M tokens much faster than Mamba. However, Mamba has an edge in complex
    tasks such as remembering a large number of facts in QA3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的递归模型，Mamba 和 RMT 在 QA1 上表现相当。然而，由于 Mamba 实现的技术限制，超过 128k 的推理速度极其缓慢，使得处理更长序列几乎不可能。RMT
    在处理长达 10M 令牌的序列时，比 Mamba 快得多，显著优于检索增强模型。然而，Mamba 在 QA3 中记住大量事实等复杂任务上具有优势。
- en: 'We evaluated GPT-3.5 and Mistral-7B models fine-tuned with 1000 samples from
    QA1 for 3 epochs. The evaluation results are shown in Appendix [I](#A9 "Appendix
    I LLM fine-tuning ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    Figure [8](#A9.F8 "Figure 8 ‣ Appendix I LLM fine-tuning ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack"). Fine-tuning dramatically
    improves performance for longer contexts making scores uniform across all input
    sizes. Still, these results are behind of fine-tuned Mamba and RMT.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对使用来自 QA1 的 1000 个样本进行 3 个周期微调的 GPT-3.5 和 Mistral-7B 模型进行了评估。评估结果见附录 [I](#A9
    "附录 I LLM 微调 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") 图 [8](#A9.F8 "图 8 ‣ 附录 I LLM 微调
    ‣ BABILong: 测试 LLM 在长上下文推理中的极限")。微调显著提高了对较长上下文的性能，使得所有输入大小的得分趋于一致。然而，这些结果仍落后于微调的
    Mamba 和 RMT。'
- en: 3.4 BABILong and Other Benchmarks
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 BABILong 和其他基准测试
- en: '![Refer to caption](img/33335ab64ef7267e2a81ae4e8c0e126b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/33335ab64ef7267e2a81ae4e8c0e126b.png)'
- en: 'Figure 4: BABILong is similar to MMLU (Hendrycks et al., [2020](#bib.bib27))
    on short lengths and captures differences in models behavior for longer contexts.
    MMLU is a relatively short benchmark, with samples up to 1k tokens in length.
    BABILong has a higher correlation with MMLU on short contexts (0K) than RULER (Hsieh
    et al., [2024](#bib.bib29)). However, RULER maintains a high correlation regardless
    of task length, with an even higher correlation at 64K, while BABILong’s correlation
    with MMLU decreases with length. This may indicate that BABILong is better at
    capturing differences in models behavior at different context lengths.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：BABILong在短长度上类似于MMLU (Hendrycks et al., [2020](#bib.bib27))，并在较长的上下文中捕捉模型行为的差异。MMLU是一个相对较短的基准测试，样本长度最多为1k
    tokens。BABILong在短上下文（0K）上的相关性高于RULER (Hsieh et al., [2024](#bib.bib29))。然而，RULER无论任务长度如何，都保持较高的相关性，在64K时相关性更高，而BABILong与MMLU的相关性则随着长度的增加而降低。这可能表明BABILong在不同上下文长度下更能捕捉模型行为的差异。
- en: Here, we analyze how models performance on BABILong benchmark differs from MMLU (Hendrycks
    et al., [2020](#bib.bib27)) and RULER (Hsieh et al., [2024](#bib.bib29)). The
    MMLU benchmark measures various branches of knowledge in LLMs, whereas RULER,
    a recently proposed long-context benchmark, shares a similar "needle-in-a-haystack"
    concept with BABILong. One notable difference is that RULER’s "needles" (such
    as adjectives, nouns, numbers, uuids) and long "haystack" contexts are more synthetic,
    consisting of randomly repeated sentences, except for tasks based on the SQuAD (Rajpurkar
    et al., [2016](#bib.bib51)) and HotPotQA (Yang et al., [2018](#bib.bib71)) datasets
    or using Paul Graham essays.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们分析了模型在BABILong基准测试上的表现如何与MMLU (Hendrycks et al., [2020](#bib.bib27))和RULER (Hsieh
    et al., [2024](#bib.bib29))不同。MMLU基准测试衡量了LLMs中各种知识领域，而RULER作为一个新提出的长上下文基准，与BABILong共享类似的“针在干草堆中”的概念。一个显著的不同是RULER的“针”（如形容词、名词、数字、uuid）和长“干草堆”上下文更加合成，由随机重复的句子组成，除了基于SQuAD (Rajpurkar
    et al., [2016](#bib.bib51))和HotPotQA (Yang et al., [2018](#bib.bib71))数据集的任务或使用Paul
    Graham的文章。
- en: 'We collect results from multiple models on MMLU, BABILong, and RULER at lengths
    ranging from 0K (BABILong without texts from PG19) to 128K tokens. In the upper-left
    part of Figure [4](#S3.F4 "Figure 4 ‣ 3.4 BABILong and Other Benchmarks ‣ 3 Benchmarking
    Results ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"),
    we show the correlation between scores on BABILong and RULER for different task
    lengths with those on the MMLU benchmark. At shorter lengths, BABILong exhibits
    a high correlation with MMLU, which diminishes as the length increases. Conversely,
    RULER shows a nearly constant correlation with MMLU, regardless of the length.
    The best correlated RULER lengths with MMLU are 64K and the average of all lengths
    (<=128K). In contrast, the highest correlation of BABILong scores with MMLU is
    at length 0K, which is expected since MMLU is a relatively short benchmark with
    examples up to 1K tokens. Comparing the correlations of BABILong with MMLU at
    the most correlated RULER lengths (<=128K and 64K) shows much lower values: $0.928$,
    respectively.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了多个模型在MMLU、BABILong和RULER上的结果，长度范围从0K（BABILong没有PG19文本）到128K tokens。在图[4](#S3.F4
    "图4 ‣ 3.4 BABILong与其他基准 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")的左上部分，我们展示了不同任务长度下BABILong和RULER的分数与MMLU基准测试的相关性。在较短的长度下，BABILong与MMLU的相关性很高，但随着长度的增加相关性降低。相反，RULER与MMLU的相关性几乎保持不变，与MMLU相关性最好的RULER长度是64K和所有长度（<=128K）的平均值。相比之下，BABILong与MMLU的最高相关性出现在长度为0K时，这是因为MMLU是一个相对较短的基准测试，样本最多为1K
    tokens。将BABILong与MMLU在最相关的RULER长度（<=128K和64K）下的相关性进行比较，结果要低得多：$0.928$。
- en: These results show that BABILong can detect differences in models behavior starting
    from lengths as small as 2K tokens, while RULER requires lengths of at least 128K
    tokens to show significant differentiation from relatively short MMLU benchmark.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，BABILong可以从2K tokens的小长度开始检测模型行为的差异，而RULER则需要至少128K tokens的长度才能显示出与相对较短的MMLU基准测试的显著差异。
- en: 4 Related Work on Long Context Benchmarks and Datasets
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 关于长上下文基准测试和数据集的相关工作
- en: Long Range Arena (LRA) (Tay et al., [2021](#bib.bib63)) was a one of the pioneering
    benchmarks for long context modeling. LRA is a set of tasks with lengths from
    1 to 16 thousand tokens. However, it mainly consists of very specific tasks such
    as ListOps (2k tokens), Byte-Level Text Classification (4k tokens) and Byte-Level
    Text Retrieval (8k tokens), and others that are less related to NLP. They are
    not well suited for evaluating of modern LLMs without fine-tuning on these tasks
    and cannot fully represent the capabilites of LLMs that can handle 100k+ tokens.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Long Range Arena (LRA) (Tay et al., [2021](#bib.bib63)) 是长上下文建模的开创性基准测试之一。LRA
    是一组长度从 1 到 16 千令牌的任务。然而，它主要由一些非常具体的任务组成，如 ListOps (2k 令牌)、字节级文本分类 (4k 令牌) 和字节级文本检索
    (8k 令牌) 等，与自然语言处理（NLP）关系不大。这些任务不适合评估现代 LLM，除非对这些任务进行微调，否则无法完全代表能够处理 100k+ 令牌的
    LLM 的能力。
- en: A new set of datasets and benchmarks specifically designed to test the ability
    of LLMs to handle long contexts has been proposed. The LongBench dataset (Bai
    et al., [2023](#bib.bib6)) contains 6 types of real and synthetic problems, ranging
    from summarization and multidoc QA to code completion. The average sample lengths
    in LongBench are 6k and 13k tokens for English and Chinese respectively, with
    40k tokens at max. Scrolls and ZeroSCROLLS (Shaham et al., [2022](#bib.bib56),
    [2023](#bib.bib57)) consist of QA, classification, summarization tasks and have
    higher average lengths ranging from 1.7k to 49.3k tokens. L-Eval (An et al., [2023](#bib.bib4))
    mostly combines 20 smaller long sequence datasets and adds 4 newly annotated tasks,
    with query-response pairs encompassing diverse question styles and domains. The
    average length of examples for L-Eval varies from 3 to 60 thousand tokens. Some
    of the benchmarks are focusing on evaluation of in-context learning and instruction
    following, such as LongAlign and LongBench-chat (Bai et al., [2024](#bib.bib7)),
    ZeroScrolls, LongICLBench (Li et al., [2024](#bib.bib39)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一组新的数据集和基准测试专门设计用于测试大型语言模型（LLMs）处理长上下文的能力已经被提出。LongBench 数据集 (Bai et al., [2023](#bib.bib6))
    包含 6 种真实和合成的问题类型，从摘要和多文档问答到代码补全。LongBench 的平均样本长度为英语 6k 和中文 13k 令牌，最大长度为 40k 令牌。Scrolls
    和 ZeroSCROLLS (Shaham et al., [2022](#bib.bib56), [2023](#bib.bib57)) 包括问答、分类、摘要任务，具有更高的平均长度，范围从
    1.7k 到 49.3k 令牌。L-Eval (An et al., [2023](#bib.bib4)) 主要结合了 20 个较小的长序列数据集，并添加了
    4 个新标注的任务，查询-响应对涵盖了多样的问题风格和领域。L-Eval 的示例平均长度从 3 到 60 千令牌不等。一些基准测试专注于评估上下文学习和指令跟随，如
    LongAlign 和 LongBench-chat (Bai et al., [2024](#bib.bib7))、ZeroScrolls、LongICLBench (Li
    et al., [2024](#bib.bib39))。
- en: There are other long-context datasets that primarily consist of QA and summarization
    tasks over texts from Wiki, arXiv, novels, movie and TV scripts, or other sources,
    e.g., InfinityBench (Zhang et al., [2024b](#bib.bib77)), Loogle (Li et al., [2023b](#bib.bib38)),
    Bamboo (Dong et al., [2023](#bib.bib18)), LVEval (Yuan et al., [2024](#bib.bib73)),
    NovelQA (Wang et al., [2024b](#bib.bib66)), Marathon (Zhang et al., [2023](#bib.bib75)),
    XL²-Bench (Ni et al., [2024](#bib.bib44)), DocFinQA (Reddy et al., [2024](#bib.bib52)),
    or ChapterBreak (Sun et al., [2022](#bib.bib62)), Ada-LEval (Wang et al., [2024a](#bib.bib65))
    that evaluate operations with text chunks, or move to multimodal tasks in MileBench (Song
    et al., [2024a](#bib.bib59)). These datasets vary in length, with maximum sample
    lengths of 636K tokens in ChapterBreak and average lengths reaching 200K tokens
    in InfinityBench, NovelQA, LVEval, and some subsets of XL²-Bench.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他主要由问答和摘要任务组成的长上下文数据集，这些任务来源于 Wiki、arXiv、小说、电影和电视脚本或其他来源，例如 InfinityBench (Zhang
    et al., [2024b](#bib.bib77))、Loogle (Li et al., [2023b](#bib.bib38))、Bamboo (Dong
    et al., [2023](#bib.bib18))、LVEval (Yuan et al., [2024](#bib.bib73))、NovelQA (Wang
    et al., [2024b](#bib.bib66))、Marathon (Zhang et al., [2023](#bib.bib75))、XL²-Bench (Ni
    et al., [2024](#bib.bib44))、DocFinQA (Reddy et al., [2024](#bib.bib52)) 或 ChapterBreak (Sun
    et al., [2022](#bib.bib62))、Ada-LEval (Wang et al., [2024a](#bib.bib65))，这些数据集评估文本块操作，或者转向
    MileBench (Song et al., [2024a](#bib.bib59)) 中的多模态任务。这些数据集的长度各异，ChapterBreak 的最大样本长度为
    636K 令牌，而 InfinityBench、NovelQA、LVEval 和 XL²-Bench 的某些子集的平均长度达到 200K 令牌。
- en: To further extend benchmarks lengths with real and human annotated data is very
    challenging. Therefore, "needle-in-a-haystack" inspired benchmarks arised. Following
    LLMTest ⁵⁵5[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
    with magic numbers as needles in Paul Graham essays as haystack, passkey and key-value
    retrieval tasks are part of InfinityBench (Zhang et al., [2024b](#bib.bib77)).
    Counting-Stars (Song et al., [2024b](#bib.bib60)) suggests to insert multiple
    sentences about little penguins that count stars into the same essays for English
    or The Story of the Stone for the Chinese language. The task is to answer questions
    based on these "needle" sentences. RULER (Hsieh et al., [2024](#bib.bib29)) extends
    "needle-in-a-haystack" with multiple types and amount of "needles". RULER and
    Counting-Start introduce new task categories such as multi-hop tracing and aggregation
    to test models beyond searching from context.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步扩展基准测试长度，使用真实和人工标注的数据是非常具有挑战性的。因此，受“针在干草堆中”启发的基准测试应运而生。跟随LLMTest ⁵⁵5[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)，以保罗·格雷厄姆的文章中的干草堆作为“针”的魔法数字为例，InfinityBench （张等，[2024b](#bib.bib77)）包括了密钥和键值检索任务。Counting-Stars （宋等，[2024b](#bib.bib60)）建议在相同的文章中插入关于小企鹅数星星的多句话，以英语或《石头记》为中文语言。任务是基于这些“针”句子回答问题。RULER （谢等，[2024](#bib.bib29)）通过引入多种类型和数量的“针”来扩展“针在干草堆中”的概念。RULER和Counting-Start引入了新的任务类别，如多跳追踪和聚合，以测试模型超越从上下文中搜索的能力。
- en: Some benchmarks have pre-defined length bins, such as LongBench (0-4k, 4k-8k,
    8k+), Ada-LEval (2k-128k), LVEval (16k, 32k, 64k, 128k, 256k), Bamboo (4k, 16k),
    S3Eval (2k, 4k, 8k, 16k) (Lei et al., [2023](#bib.bib35)). A number of benchmarks,
    including RULER, CountingStars, Ada-LEval, and S3Eval, can be generated at required
    lengths. All mentioned datasets are mostly in English with some of them covering
    Chinese language (LongBench, InfinityBench, LongAlign, Counting Stars, CLongEval (Qiu
    et al., [2024](#bib.bib48)), LV-Eval, XL²-Bench).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基准测试具有预定义的长度区间，例如LongBench（0-4k，4k-8k，8k+），Ada-LEval（2k-128k），LVEval（16k，32k，64k，128k，256k），Bamboo（4k，16k），S3Eval（2k，4k，8k，16k） （Lei等，[2023](#bib.bib35)）。包括RULER、CountingStars、Ada-LEval和S3Eval在内的一些基准测试可以生成所需的长度。所有提到的数据集主要是英语，其中一些涵盖了中文（LongBench、InfinityBench、LongAlign、Counting
    Stars、CLongEval （邱等，[2024](#bib.bib48)）、LV-Eval、XL²-Bench）。
- en: BABILong focuses on natural language reasoning over multiple facts distributed
    in very large textual corpora. Compared to existing approaches it provides more
    tasks and more natural and deceptive mixing of information into background documents.
    BABILong consists of diverse set of 20 tasks that cover different capabilities
    including multi-hop tracing, aggregation over needles and extending them with
    basic deduction and induction, time, positional, and size reasoning, and path
    finding. The benchmark goes with predefined splits up to unprecedented 1M token
    length. Lengths beyond 1M tokens could be generated and we test models up to 11M
    tokens. Furthermore, while we evaluate models on English-only tasks from bAbI (Weston
    et al., [2016](#bib.bib68)) adding new languages is straightforward.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong专注于在非常大的文本语料库中进行自然语言推理。与现有的方法相比，它提供了更多的任务，并且将信息更自然和具有欺骗性地混合到背景文档中。BABILong包括20个不同任务的多样化集合，涵盖了包括多跳追踪、在针上聚合以及使用基本推理和归纳、时间、位置、大小推理和路径寻找的不同能力。该基准测试具有预定义的划分，长度达到前所未有的1M标记。超过1M标记的长度可以生成，我们测试了最多11M标记的模型。此外，虽然我们在bAbI （Weston等，[2016](#bib.bib68)）上评估了仅英语的模型，添加新语言则很简单。
- en: Conclusions
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this work, we introduced the BABILong, a diverse and scalable benchmark designed
    to bridge the gap in evaluating large language models (LLMs) across extensive
    context lengths. Our experiments demonstrate that BABILong offers a more representative
    evaluation framework for long-context reasoning among the existing benchmarks.
    The analysis of correlation with other benchmarks further validates BABILong’s
    ability to pose a significant challenge for large language models to maintain
    performance as context lengths scale up. The BABILong benchmark offers algorithmically
    adaptable document length and facts placement, includes predefined sets of bins
    ranging from 0k to 1M tokens. Facts in BABILong could be generated making it leak-proof
    for future LLMs. It consists of a set of 20 diverse tasks covering reasoning tasks,
    including fact chaining, simple induction, deduction, counting, and handling lists/sets.
    Compared to other benchmarks, BABILong shows high correlation on short contexts
    with MMLU and diverges from it as lengths increases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了BABILong，一个多样且可扩展的基准，旨在填补评估大型语言模型（LLMs）在广泛上下文长度上的空白。我们的实验表明，BABILong提供了一个更具代表性的评估框架，用于现有基准中长上下文推理的评估。与其他基准的相关性分析进一步验证了BABILong在上下文长度扩展时给大型语言模型带来的显著挑战。BABILong基准提供了算法上可调整的文档长度和事实位置，包括从0k到1M
    tokens的预定义桶集。BABILong中的事实可以生成，使其对未来的LLMs具有防泄漏性。它由一组20个多样化的任务组成，涵盖推理任务，包括事实链条、简单归纳、演绎、计数和处理列表/集合。与其他基准相比，BABILong在短上下文上与MMLU显示出高度相关性，并且随着长度的增加而逐渐分歧。
- en: Our findings reveal limitations of popular open-source LLMs as well as GPT-4
    and RAG regarding effective long context utilization. Their performance heavily
    relies on the first 5-25 % of the input, highlighting the need for improved context
    processing mechanisms. Fine-tuning experiments show that tasks from BABILong are
    solvable even by relatively small models like RMT with GPT-2 (137M) and Mamba
    (130M). Fine-tuning improves the performance of GPT-3.5-Turbo and Mistral-7B,
    but their context lengths remain limited to 16K and 32K, respectively. Among the
    evaluated models, Mamba (130M) and RMT(137M) achieve the strongest results. However,
    Mamba is hard to infer on lengths more than 128K tokens, while RMT enables the
    processing of lengths up to 11 million tokens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究揭示了流行的开源LLMs（大语言模型）、GPT-4和RAG在有效利用长上下文方面的局限性。它们的性能严重依赖于输入的前5-25%，这突显了改进上下文处理机制的必要性。微调实验表明，BABILong中的任务即使是像GPT-2（137M）和Mamba（130M）这样的相对较小的模型也能解决。微调提高了GPT-3.5-Turbo和Mistral-7B的性能，但它们的上下文长度仍然限制在16K和32K。评估模型中，Mamba（130M）和RMT（137M）表现最强。然而，Mamba在处理超过128K
    tokens的长度时很困难，而RMT则可以处理长达1100万tokens的长度。
- en: Limitations
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The BABILong benchmark uses background texts to hide facts in them. In our experiments,
    we only tried PG19 and Wiki as background text sources. Other background texts
    may have a different effect on the results. PG19 and Wiki were chosen because
    they contain natural narratives and facts about people, in a way similar to bAbI
    tasks. Interference between similar facts in the background text can make the
    benchmark even more difficult.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong基准使用背景文本来隐藏其中的事实。在我们的实验中，我们仅尝试了PG19和Wiki作为背景文本来源。其他背景文本可能对结果有不同的影响。选择PG19和Wiki是因为它们包含自然叙事和有关人物的事实，类似于bAbI任务。背景文本中相似事实的干扰可能使基准测试变得更加困难。
- en: 'In GPT-4 and LLama-3 with RAG experiments, we do not optimize the retrieval
    component. We tried several prompts experiments with LLMs, but the ones that we
    selected could be suboptimal. We provide them in Appendix [J](#A10 "Appendix J
    Prompts Used to Benchmark GPT-4-Turbo and Mistral Models ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack") and in our GitHub repository.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '在GPT-4和LLama-3与RAG的实验中，我们没有优化检索组件。我们尝试了几种与LLMs的提示实验，但我们选择的提示可能不是最优的。我们将这些提示提供在附录[J](#A10
    "附录 J 用于基准测试 GPT-4-Turbo 和 Mistral 模型的提示 ‣ BABILong: 测试 LLMs 在长上下文推理中的极限")以及我们的GitHub仓库中。'
- en: The current version of the dataset reuses parameters for fact generation from
    the bAbI (Weston et al., [2016](#bib.bib68)). As the initial work used vocabularies
    of limited size, this results in a low variety of names and objects within the
    facts. This limitation makes the BABILong tasks easier for fine-tuned models,
    as they can quickly learn specific tokens that differentiate facts from background
    text. This issue is partially mitigated by generating distractor facts using the
    same vocabularies. Enhancing the dataset’s vocabulary in future versions could
    easily address this limitation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当前版本的数据集重新使用了bAbI（Weston等人，[2016](#bib.bib68)）的事实生成参数。由于初始工作使用了有限大小的词汇，这导致事实中的名字和对象种类较少。这一限制使得BABILong任务对于微调模型来说更容易，因为它们可以迅速学习区分事实和背景文本的特定标记。通过使用相同的词汇生成干扰事实，部分缓解了这个问题。未来版本中增强数据集的词汇量可以轻松解决这一限制。
- en: Although recurrent approaches, like RMT, are hindered by their sequential nature,
    resulting in reduced parallelizability, they compensate by constant memory requirements,
    but it is also their limitation as storage capacity of the model is finite. However,
    BABILong is solvable by this type of models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像RMT这样的递归方法受到其顺序性质的限制，导致并行性降低，但它们通过持续的内存需求来补偿这一点，但这也是它们的局限性，因为模型的存储容量是有限的。然而，BABILong可以通过这类模型解决。
- en: References
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abdin et al. (2024) Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah,
    A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., et al. Phi-3
    technical report: A highly capable language model locally on your phone. *arXiv
    preprint arXiv:2404.14219*, 2024.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdin等人（2024）Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A.,
    Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., 等人。Phi-3技术报告：一种在手机上本地运行的高能力语言模型。*arXiv预印本
    arXiv:2404.14219*，2024年。
- en: Agarwal et al. (2024) Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Chan,
    S., Anand, A., Abbas, Z., Nova, A., Co-Reyes, J. D., Chu, E., et al. Many-shot
    in-context learning. *arXiv preprint arXiv:2404.11018*, 2024.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等人（2024）Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Chan, S., Anand,
    A., Abbas, Z., Nova, A., Co-Reyes, J. D., Chu, E., 等人。多次上下文学习。*arXiv预印本 arXiv:2404.11018*，2024年。
- en: AI@Meta (2024) AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta（2024）AI@Meta. Llama 3模型卡。2024年。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'An et al. (2023) An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L.,
    and Qiu, X. L-eval: Instituting standardized evaluation for long context language
    models. *arXiv preprint arXiv:2307.11088*, 2023.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An等人（2023）An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., 和 Qiu,
    X. L-eval: 为长上下文语言模型建立标准化评估。*arXiv预印本 arXiv:2307.11088*，2023年。'
- en: Anthropic (2024) Anthropic. Introducing the next generation of claude, 2024.
    URL [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2024）Anthropic. 介绍下一代Claude，2024年。网址 [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)。
- en: 'Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,
    Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark
    for long context understanding. *arXiv preprint arXiv:2308.14508*, 2023.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等人（2023）Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z.,
    Liu, X., Zeng, A., Hou, L., 等人。Longbench: 一种用于长上下文理解的双语多任务基准。*arXiv预印本 arXiv:2308.14508*，2023年。'
- en: 'Bai et al. (2024) Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang,
    J., Dong, Y., and Li, J. Longalign: A recipe for long context alignment of large
    language models. *arXiv preprint arXiv:2401.18058*, 2024.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等人（2024）Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang, J., Dong,
    Y., 和 Li, J. Longalign: 一种用于大语言模型长上下文对齐的方案。*arXiv预印本 arXiv:2401.18058*，2024年。'
- en: 'Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy等人（2020）Beltagy, I., Peters, M. E., 和 Cohan, A. Longformer: 长文档变换器。*arXiv预印本
    arXiv:2004.05150*，2020年。'
- en: Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark,
    A., et al. Improving language models by retrieving from trillions of tokens. In
    *International conference on machine learning*, pp.  2206–2240\. PMLR, 2022.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud等人（2022）Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark,
    A., 等人。通过从数万亿个标记中检索来改善语言模型。发表于*国际机器学习会议*，第2206–2240页。PMLR，2022年。
- en: Bulatov et al. (2022) Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent memory
    transformer. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
    Oh, A. (eds.), *Advances in Neural Information Processing Systems*, volume 35,
    pp.  11079–11091\. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulatov 等（2022）Bulatov, A., Kuratov, Y., 和 Burtsev, M. 循环记忆变换器。在 Koyejo, S.,
    Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., 和 Oh, A.（编），*神经信息处理系统进展*，第35卷，第11079–11091页。Curran
    Associates, Inc.，2022年。网址 [https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf)。
- en: 'Bulatov et al. (2024) Bulatov, A., Kuratov, Y., Kapushev, Y., and Burtsev,
    M. Beyond attention: Breaking the limits of transformer context length with recurrent
    memory. *Proceedings of the AAAI Conference on Artificial Intelligence*, 38(16):17700–17708,
    Mar. 2024. doi: 10.1609/aaai.v38i16.29722. URL [https://ojs.aaai.org/index.php/AAAI/article/view/29722](https://ojs.aaai.org/index.php/AAAI/article/view/29722).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bulatov 等（2024）Bulatov, A., Kuratov, Y., Kapushev, Y., 和 Burtsev, M. 超越注意力：利用循环记忆突破变换器上下文长度的限制。*AAAI人工智能会议论文集*，38(16):17700–17708，2024年3月。doi:
    10.1609/aaai.v38i16.29722。网址 [https://ojs.aaai.org/index.php/AAAI/article/view/29722](https://ojs.aaai.org/index.php/AAAI/article/view/29722)。'
- en: Chase (2022) Chase, H. LangChain, October 2022. URL [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase（2022）Chase, H. LangChain，2022年10月。网址 [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)。
- en: 'Chen et al. (2023) Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S.,
    and Jia, J. Longlora: Efficient fine-tuning of long-context large language models.
    In *The Twelfth International Conference on Learning Representations*, 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023）Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., 和 Jia,
    J. Longlora：高效的长上下文大语言模型微调。在*第十二届国际学习表征会议*，2023年。
- en: 'Chevalier et al. (2023) Chevalier, A., Wettig, A., Ajith, A., and Chen, D.
    Adapting language models to compress contexts. In Bouamor, H., Pino, J., and Bali,
    K. (eds.), *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing*, pp.  3829–3846, Singapore, December 2023\. Association for
    Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL [https://aclanthology.org/2023.emnlp-main.232](https://aclanthology.org/2023.emnlp-main.232).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chevalier 等（2023）Chevalier, A., Wettig, A., Ajith, A., 和 Chen, D. 适应语言模型以压缩上下文。在
    Bouamor, H., Pino, J., 和 Bali, K.（编），*2023年自然语言处理实证方法会议论文集*，第3829–3846页，新加坡，2023年12月。计算语言学协会。doi:
    10.18653/v1/2023.emnlp-main.232。网址 [https://aclanthology.org/2023.emnlp-main.232](https://aclanthology.org/2023.emnlp-main.232)。'
- en: 'Cohere (2024) Cohere. Command r: Retrieval-augmented generation at production
    scale, March 2024. URL [https://cohere.com/blog/command-r](https://cohere.com/blog/command-r).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cohere（2024）Cohere. Command r: 在生产规模上进行检索增强生成，2024年3月。网址 [https://cohere.com/blog/command-r](https://cohere.com/blog/command-r)。'
- en: 'Didolkar et al. (2022) Didolkar, A., Gupta, K., Goyal, A., Gundavarapu, N. B.,
    Lamb, A. M., Ke, N. R., and Bengio, Y. Temporal latent bottleneck: Synthesis of
    fast and slow processing mechanisms in sequence learning. *Advances in Neural
    Information Processing Systems*, 35:10505–10520, 2022.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Didolkar 等（2022）Didolkar, A., Gupta, K., Goyal, A., Gundavarapu, N. B., Lamb,
    A. M., Ke, N. R., 和 Bengio, Y. 时间潜在瓶颈：序列学习中快速与慢速处理机制的综合。*神经信息处理系统进展*，35:10505–10520，2022年。
- en: 'Ding et al. (2023) Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang,
    W., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. *arXiv
    preprint arXiv:2307.02486*, 2023.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2023）Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., 和 Wei,
    F. Longnet：将变换器扩展到 1,000,000,000 个标记。*arXiv预印本 arXiv:2307.02486*，2023年。
- en: 'Dong et al. (2023) Dong, Z., Tang, T., Li, J., Zhao, W. X., and Wen, J.-R.
    Bamboo: A comprehensive benchmark for evaluating long text modeling capacities
    of large language models. *arXiv preprint arXiv:2309.13345*, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）Dong, Z., Tang, T., Li, J., Zhao, W. X., 和 Wen, J.-R. Bamboo：评估大语言模型长文本建模能力的综合基准。*arXiv预印本
    arXiv:2309.13345*，2023年。
- en: Douze et al. (2024) Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy,
    G., Mazaré, P.-E., Lomeli, M., Hosseini, L., and Jégou, H. The faiss library.
    2024.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Douze 等（2024）Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré,
    P.-E., Lomeli, M., Hosseini, L., 和 Jégou, H. faiss 库。2024年。
- en: 'Du et al. (2022) Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and
    Tang, J. Glm: General language model pretraining with autoregressive blank infilling.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  320–335, 2022.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等（2022）Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., 和 Tang, J.
    Glm: 自回归空白填充的通用语言模型预训练。在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，第320–335页，2022年。'
- en: Fan et al. (2020) Fan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar,
    S. Addressing some limitations of transformers with feedback memory. *arXiv preprint
    arXiv:2002.09402*, 2020.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2020）Fan, A., Lavril, T., Grave, E., Joulin, A., 和 Sukhbaatar, S. 通过反馈记忆解决变换器的一些局限性。*arXiv
    预印本 arXiv:2002.09402*，2020年。
- en: Gebru et al. (2021) Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W.,
    Wallach, H., Iii, H. D., and Crawford, K. Datasheets for datasets. *Communications
    of the ACM*, 64(12):86–92, 2021.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gebru 等（2021）Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach,
    H., Iii, H. D., 和 Crawford, K. 数据集说明书。*ACM 通讯*，64(12):86–92，2021年。
- en: Graves et al. (2014) Graves, A., Wayne, G., and Danihelka, I. Neural turing
    machines. *arXiv preprint arXiv:1410.5401*, 2014.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 等（2014）Graves, A., Wayne, G., 和 Danihelka, I. 神经图灵机。*arXiv 预印本 arXiv:1410.5401*，2014年。
- en: 'Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with
    selective state spaces. *arXiv preprint arXiv:2312.00752*, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu & Dao（2023）Gu, A. 和 Dao, T. Mamba: 具有选择性状态空间的线性时间序列建模。*arXiv 预印本 arXiv:2312.00752*，2023年。'
- en: Gu et al. (2021) Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences
    with structured state spaces. In *International Conference on Learning Representations*,
    2021.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2021）Gu, A., Goel, K., 和 Re, C. 高效建模长序列与结构化状态空间。在*国际学习表征会议*，2021年。
- en: Guu et al. (2020) Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval
    augmented language model pre-training. In *International conference on machine
    learning*, pp.  3929–3938\. PMLR, 2020.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等（2020）Guu, K., Lee, K., Tung, Z., Pasupat, P., 和 Chang, M. 检索增强语言模型预训练。在*国际机器学习会议*，第3929–3938页。PMLR，2020年。
- en: Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2020.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2020）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 测量大规模多任务语言理解。在*国际学习表征会议*，2020年。
- en: 'Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term
    memory. *Neural Comput.*, 9(8):1735–1780, November 1997. ISSN 0899-7667. doi:
    10.1162/neco.1997.9.8.1735. URL [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hochreiter & Schmidhuber（1997）Hochreiter, S. 和 Schmidhuber, J. 长短期记忆。*神经计算*，9(8):1735–1780，1997年11月。ISSN
    0899-7667。doi: 10.1162/neco.1997.9.8.1735。网址 [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)。'
- en: 'Hsieh et al. (2024) Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh,
    D., Jia, F., and Ginsburg, B. Ruler: What’s the real context size of your long-context
    language models? *arXiv preprint arXiv:2404.06654*, 2024.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsieh 等（2024）Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia,
    F., 和 Ginsburg, B. Ruler: 你长期上下文语言模型的真实上下文大小是多少？*arXiv 预印本 arXiv:2404.06654*，2024年。'
- en: Hutchins et al. (2022) Hutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur,
    B. Block-recurrent transformers. In Oh, A. H., Agarwal, A., Belgrave, D., and
    Cho, K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL
    [https://openreview.net/forum?id=uloenYmLCAo](https://openreview.net/forum?id=uloenYmLCAo).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutchins 等（2022）Hutchins, D., Schlag, I., Wu, Y., Dyer, E., 和 Neyshabur, B.
    块递归变换器。在 Oh, A. H., Agarwal, A., Belgrave, D., 和 Cho, K.（编辑），*神经信息处理系统进展*，2022年。网址
    [https://openreview.net/forum?id=uloenYmLCAo](https://openreview.net/forum?id=uloenYmLCAo)。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等。Mistral
    7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F.,
    et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2024）Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B.,
    Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., 等。Mixtral
    of experts。*arXiv 预印本 arXiv:2401.04088*，2024年。
- en: 'Khandelwal et al. (2019) Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,
    L., and Lewis, M. Generalization through memorization: Nearest neighbor language
    models. In *International Conference on Learning Representations*, 2019.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khandelwal 等（2019）Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., 和
    Lewis, M. 通过记忆实现泛化：最近邻语言模型。在 *国际学习表示会议*，2019。
- en: Lee et al. (2024) Lee, J., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum,
    O., and Brunskill, E. Supervised pretraining can learn in-context reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36, 2024.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2024）Lee, J., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum, O.,
    和 Brunskill, E. 监督预训练可以在上下文中学习强化学习。*神经信息处理系统进展*，36，2024。
- en: 'Lei et al. (2023) Lei, F., Liu, Q., Huang, Y., He, S., Zhao, J., and Liu, K.
    S3eval: A synthetic, scalable, systematic evaluation suite for large language
    models. *arXiv preprint arXiv:2310.15147*, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lei 等（2023）Lei, F., Liu, Q., Huang, Y., He, S., Zhao, J., 和 Liu, K. S3eval:
    一套用于大型语言模型的合成、可扩展、系统化评估工具。*arXiv 预印本 arXiv:2310.15147*，2023。'
- en: 'Lei et al. (2020) Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T. L., and Bansal,
    M. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning,
    2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lei 等（2020）Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T. L., 和 Bansal, M. Mart:
    用于连贯视频段落描述的记忆增强递归变换器，2020。'
- en: Li et al. (2023a) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J., Stoica, I., Ma, X., and Zhang, H. How long can context length of open-source
    llms truly promise? In *NeurIPS 2023 Workshop on Instruction Tuning and Instruction
    Following*, 2023a.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023a）Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J., Stoica,
    I., Ma, X., 和 Zhang, H. 开源 LLM 的上下文长度到底能承诺多长？在 *NeurIPS 2023 指令调整与指令跟随研讨会*，2023a。
- en: 'Li et al. (2023b) Li, J., Wang, M., Zheng, Z., and Zhang, M. Loogle: Can long-context
    language models understand long contexts? *arXiv preprint arXiv:2311.04939*, 2023b.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2023b）Li, J., Wang, M., Zheng, Z., 和 Zhang, M. Loogle: 长上下文语言模型能否理解长上下文？*arXiv
    预印本 arXiv:2311.04939*，2023b。'
- en: Li et al. (2024) Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Long-context
    llms struggle with long in-context learning. *arXiv preprint arXiv:2404.02060*,
    2024.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2024）Li, T., Zhang, G., Do, Q. D., Yue, X., 和 Chen, W. 长上下文的LLMs在长时间上下文学习中挣扎。*arXiv
    预印本 arXiv:2404.02060*，2024。
- en: 'Lieber et al. (2024) Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos,
    I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz, S., et al. Jamba: A
    hybrid transformer-mamba language model. *arXiv preprint arXiv:2403.19887*, 2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lieber 等（2024）Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos,
    I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz, S., 等. Jamba: 一种混合的变换器-曼巴语言模型。*arXiv
    预印本 arXiv:2403.19887*，2024。'
- en: Liu et al. (2024a) Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model
    on million-length video and language with blockwise ringattention. *arXiv preprint
    arXiv:2402.08268*, 2024a.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024a）Liu, H., Yan, W., Zaharia, M., 和 Abbeel, P. 世界模型在百万长度的视频和语言中使用块状环形注意力。*arXiv
    预印本 arXiv:2402.08268*，2024a。
- en: 'Liu et al. (2024b) Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro,
    B. Chatqa: Building gpt-4 level conversational qa models. *arXiv preprint arXiv:2401.10225*,
    2024b.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2024b）Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., 和 Catanzaro, B.
    Chatqa: 构建 GPT-4 级别的对话问答模型。*arXiv 预印本 arXiv:2401.10225*，2024b。'
- en: Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay
    regularization. In *International Conference on Learning Representations*, 2019.
    URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter（2019）Loshchilov, I. 和 Hutter, F. 解耦权重衰减正则化。在 *国际学习表示会议*，2019。URL
    [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Ni et al. (2024) Ni, X., Cai, H., Wei, X., Wang, S., Yin, D., and Li, P. XL²
    Bench: A benchmark for extremely long context understanding with long-range dependencies.
    *arXiv preprint arXiv:2404.05446*, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ni 等（2024）Ni, X., Cai, H., Wei, X., Wang, S., Yin, D., 和 Li, P. XL² Bench:
    一个用于理解极长上下文和长距离依赖的基准测试。*arXiv 预印本 arXiv:2404.05446*，2024。'
- en: OpenAI (2023) OpenAI. New models and developer products announced at devday,
    2023. URL [https://openai.com/index/new-models-and-developer-products-announced-at-devday/](https://openai.com/index/new-models-and-developer-products-announced-at-devday/).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 在 devday 2023 上宣布的新模型和开发者产品。URL [https://openai.com/index/new-models-and-developer-products-announced-at-devday/](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)。
- en: 'Peng et al. (2023a) Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,
    S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing
    rnns for the transformer era. *arXiv preprint arXiv:2305.13048*, 2023a.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023a）Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S.,
    Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., 等。Rwkv：为变换器时代重新定义 RNNs。*arXiv
    预印本 arXiv:2305.13048*，2023a。
- en: 'Peng et al. (2023b) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023b.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023b）Peng, B., Quesnelle, J., Fan, H., 和 Shippole, E. Yarn：大语言模型的高效上下文窗口扩展。在*第十二届国际学习表示会议*，2023b。
- en: 'Qiu et al. (2024) Qiu, Z., Li, J., Huang, S., Zhong, W., and King, I. Clongeval:
    A chinese benchmark for evaluating long-context large language models. *arXiv
    preprint arXiv:2403.03514*, 2024.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等（2024）Qiu, Z., Li, J., Huang, S., Zhong, W., 和 King, I. Clongeval：评估长上下文大型语言模型的中文基准。*arXiv
    预印本 arXiv:2403.03514*，2024。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    and Sutskever, I. Language models are unsupervised multitask learners. 2019.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., 和 Sutskever,
    I. 语言模型是无监督的多任务学习者。2019。
- en: Rae et al. (2020) Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
    and Lillicrap, T. P. Compressive transformers for long-range sequence modelling.
    In *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SylKikSYDH](https://openreview.net/forum?id=SylKikSYDH).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2020）Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., 和 Lillicrap,
    T. P. 用于长距离序列建模的压缩变换器。在*国际学习表示会议*，2020。URL [https://openreview.net/forum?id=SylKikSYDH](https://openreview.net/forum?id=SylKikSYDH)。
- en: 'Rajpurkar et al. (2016) Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
    Squad: 100,000+ questions for machine comprehension of text. In *Proceedings of
    the 2016 Conference on Empirical Methods in Natural Language Processing*, pp. 
    2383–2392, 2016.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等（2016）Rajpurkar, P., Zhang, J., Lopyrev, K., 和 Liang, P. Squad：100,000+
    个问题用于机器理解文本。在*2016 年自然语言处理方法学会议论文集*，第 2383–2392 页，2016。
- en: 'Reddy et al. (2024) Reddy, V., Koncel-Kedziorski, R., Lai, V. D., and Tanner,
    C. Docfinqa: A long-context financial reasoning dataset. *arXiv preprint arXiv:2401.06915*,
    2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddy 等（2024）Reddy, V., Koncel-Kedziorski, R., Lai, V. D., 和 Tanner, C. Docfinqa：一个长上下文的金融推理数据集。*arXiv
    预印本 arXiv:2401.06915*，2024。
- en: 'Reid et al. (2024) Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap,
    T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J.,
    et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens
    of context. *arXiv preprint arXiv:2403.05530*, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid 等（2024）Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap,
    T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J.,
    等。Gemini 1.5：解锁跨越数百万令牌的多模态理解。*arXiv 预印本 arXiv:2403.05530*，2024。
- en: Rubin & Berant (2023) Rubin, O. and Berant, J. Long-range language modeling
    with self-retrieval. *arXiv preprint arXiv:2306.13421*, 2023.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin & Berant（2023）Rubin, O. 和 Berant, J. 通过自我检索进行长距离语言建模。*arXiv 预印本 arXiv:2306.13421*，2023。
- en: 'Sainz et al. (2023) Sainz, O., Campos, J., García-Ferrero, I., Etxaniz, J.,
    de Lacalle, O. L., and Agirre, E. NLP evaluation in trouble: On the need to measure
    LLM data contamination for each benchmark. In Bouamor, H., Pino, J., and Bali,
    K. (eds.), *Findings of the Association for Computational Linguistics: EMNLP 2023*,
    pp.  10776–10787, Singapore, December 2023\. Association for Computational Linguistics.
    doi: 10.18653/v1/2023.findings-emnlp.722. URL [https://aclanthology.org/2023.findings-emnlp.722](https://aclanthology.org/2023.findings-emnlp.722).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sainz 等（2023）Sainz, O., Campos, J., García-Ferrero, I., Etxaniz, J., de Lacalle,
    O. L., 和 Agirre, E. NLP 评估遇到麻烦：关于需要对每个基准测量 LLM 数据污染的问题。在 Bouamor, H., Pino, J.,
    和 Bali, K.（编辑），*计算语言学协会会议发现：EMNLP 2023*，第 10776–10787 页，新加坡，2023 年 12 月。计算语言学协会。doi:
    10.18653/v1/2023.findings-emnlp.722。URL [https://aclanthology.org/2023.findings-emnlp.722](https://aclanthology.org/2023.findings-emnlp.722)。'
- en: 'Shaham et al. (2022) Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O.,
    Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., et al. Scrolls: Standardized
    comparison over long language sequences. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing*, pp.  12007–12021, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaham 等（2022）Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv,
    A., Gupta, A., Xiong, W., Geva, M., Berant, J., 等。Scrolls：对长语言序列的标准化比较。在*2022
    年自然语言处理方法学会议论文集*，第 12007–12021 页，2022。
- en: 'Shaham et al. (2023) Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy,
    O. Zeroscrolls: A zero-shot benchmark for long text understanding. *arXiv preprint
    arXiv:2305.14196*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham等人（2023）Shaham, U., Ivgi, M., Efrat, A., Berant, J., 和 Levy, O. Zeroscrolls:
    一种用于长文本理解的零样本基准。*arXiv预印本 arXiv:2305.14196*，2023。'
- en: 'Shi et al. (2023) Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis,
    M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language
    models. *arXiv preprint arXiv:2301.12652*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi等人（2023）Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer,
    L., 和 Yih, W.-t. Replug: 检索增强的黑箱语言模型。*arXiv预印本 arXiv:2301.12652*，2023。'
- en: 'Song et al. (2024a) Song, D., Chen, S., Chen, G. H., Yu, F., Wan, X., and Wang,
    B. Milebench: Benchmarking mllms in long context. *arXiv preprint arXiv:2404.18532*,
    2024a.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song等人（2024a）Song, D., Chen, S., Chen, G. H., Yu, F., Wan, X., 和 Wang, B. Milebench:
    长上下文中的MLLM基准。*arXiv预印本 arXiv:2404.18532*，2024a。'
- en: 'Song et al. (2024b) Song, M., Zheng, M., and Luo, X. Counting-stars: A multi-evidence,
    position-aware, and scalable benchmark for evaluating long-context large language
    models, 2024b.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song等人（2024b）Song, M., Zheng, M., 和 Luo, X. Counting-stars: 一种多证据、位置感知且可扩展的基准，用于评估长上下文大型语言模型，2024b。'
- en: 'Sorokin et al. (2022) Sorokin, A., Buzun, N., Pugachev, L., and Burtsev, M.
    Explain my surprise: Learning efficient long-term memory by predicting uncertain
    outcomes. *Advances in Neural Information Processing Systems*, 35:36875–36888,
    2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorokin等人（2022）Sorokin, A., Buzun, N., Pugachev, L., 和 Burtsev, M. 解释我的惊讶：通过预测不确定结果来学习高效的长期记忆。*神经信息处理系统进展*，35:36875–36888，2022年。
- en: 'Sun et al. (2022) Sun, S., Thai, K., and Iyyer, M. Chapterbreak: A challenge
    dataset for long-range language models. In *Proceedings of the 2022 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, pp.  3704–3714, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等人（2022）Sun, S., Thai, K., 和 Iyyer, M. Chapterbreak: 长距离语言模型的挑战数据集。在*2022年北美计算语言学协会人类语言技术会议论文集*，第3704-3714页，2022年。'
- en: 'Tay et al. (2021) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham,
    P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark
    for efficient transformers. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay等人（2021）Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P.,
    Rao, J., Yang, L., Ruder, S., 和 Metzler, D. 长距离竞技场：高效变压器的基准。在*国际学习表征会议*，2021年。网址
    [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k)。
- en: 'Voelker et al. (2019) Voelker, A., Kajić, I., and Eliasmith, C. Legendre memory
    units: Continuous-time representation in recurrent neural networks. *Advances
    in neural information processing systems*, 32, 2019.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voelker等人（2019）Voelker, A., Kajić, I., 和 Eliasmith, C. Legendre记忆单元：递归神经网络中的连续时间表征。*神经信息处理系统进展*，32，2019年。
- en: 'Wang et al. (2024a) Wang, C., Duan, H., Zhang, S., Lin, D., and Chen, K. Ada-leval:
    Evaluating long-context llms with length-adaptable benchmarks. *arXiv preprint
    arXiv:2404.06480*, 2024a.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2024a）Wang, C., Duan, H., Zhang, S., Lin, D., 和 Chen, K. Ada-leval:
    使用长度可调基准评估长上下文LLMs。*arXiv预印本 arXiv:2404.06480*，2024a。'
- en: 'Wang et al. (2024b) Wang, C., Ning, R., Pan, B., Wu, T., Guo, Q., Deng, C.,
    Bao, G., Wang, Q., and Zhang, Y. Novelqa: A benchmark for long-range novel question
    answering. *arXiv preprint arXiv:2403.12766*, 2024b.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2024b）Wang, C., Ning, R., Pan, B., Wu, T., Guo, Q., Deng, C., Bao, G.,
    Wang, Q., 和 Zhang, Y. Novelqa: 长距离新颖问题回答的基准。*arXiv预印本 arXiv:2403.12766*，2024b。'
- en: 'Wang et al. (2024c) Wang, S., Bai, Y., Zhang, L., Zhou, P., Zhao, S., Zhang,
    G., Wang, S., Chen, R., Xu, H., and Sun, H. Xl3m: A training-free framework for
    llm length extension based on segment-wise inference. *arXiv preprint arXiv:2405.17755*,
    2024c.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2024c）Wang, S., Bai, Y., Zhang, L., Zhou, P., Zhao, S., Zhang, G., Wang,
    S., Chen, R., Xu, H., 和 Sun, H. Xl3m: 一种基于分段推理的无训练框架，用于LLM长度扩展。*arXiv预印本 arXiv:2405.17755*，2024c。'
- en: 'Weston et al. (2016) Weston, J., Bordes, A., Chopra, S., and Mikolov, T. Towards
    ai-complete question answering: A set of prerequisite toy tasks. In Bengio, Y.
    and LeCun, Y. (eds.), *4th International Conference on Learning Representations,
    ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings*,
    2016. URL [http://arxiv.org/abs/1502.05698](http://arxiv.org/abs/1502.05698).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston等人（2016）Weston, J., Bordes, A., Chopra, S., 和 Mikolov, T. 朝向ai-complete问题回答：一组先决的玩具任务。在Bengio,
    Y. 和 LeCun, Y.（主编），*第4届国际学习表征会议，ICLR 2016，圣胡安，波多黎各，2016年5月2-4日，会议记录*，2016年。网址
    [http://arxiv.org/abs/1502.05698](http://arxiv.org/abs/1502.05698)。
- en: 'Wu et al. (2022a) Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., and Yu,
    Z. Memformer: A memory-augmented transformer for sequence modeling. In *Findings
    of the Association for Computational Linguistics: AACL-IJCNLP 2022*, pp.  308–318,
    Online only, November 2022a. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-aacl.29](https://aclanthology.org/2022.findings-aacl.29).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2022a) Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., 和 Yu, Z. Memformer:
    一种用于序列建模的记忆增强变换器. 在 *计算语言学协会发现：AACL-IJCNLP 2022*，第 308–318 页，仅在线，2022a 年 11 月.
    计算语言学协会. 网址 [https://aclanthology.org/2022.findings-aacl.29](https://aclanthology.org/2022.findings-aacl.29).'
- en: Wu et al. (2022b) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing
    transformers. In *International Conference on Learning Representations*, 2022b.
    URL [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2022b) Wu, Y., Rabe, M. N., Hutchins, D., 和 Szegedy, C. 记忆变换器. 在 *国际学习表征会议*，2022b.
    网址 [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-).
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pp.  2369–2380, 2018.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov,
    R., 和 Manning, C. D. Hotpotqa: 一个用于多跳问答的多样化、可解释数据集. 在 *2018年自然语言处理经验方法会议论文集*，第
    2369–2380 页，2018.'
- en: 'Young et al. (2024) Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang,
    G., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi: Open foundation models by
    01\. ai. *arXiv preprint arXiv:2403.04652*, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Young 等人 (2024) Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G.,
    Li, H., Zhu, J., Chen, J., Chang, J., 等. Yi: 由 01\. ai 开放的基础模型. *arXiv 预印本 arXiv:2403.04652*，2024.'
- en: 'Yuan et al. (2024) Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang,
    M., Tan, Z., Yao, Z., Lin, D., Li, B., et al. Lv-eval: A balanced long-context
    benchmark with 5 length levels up to 256k. *arXiv preprint arXiv:2402.05136*,
    2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等人 (2024) Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang, M.,
    Tan, Z., Yao, Z., Lin, D., Li, B., 等. Lv-eval: 一个平衡的长上下文基准，有 5 个长度级别，最长达 256k.
    *arXiv 预印本 arXiv:2402.05136*，2024.'
- en: 'Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J.,
    Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed,
    A. Big bird: Transformers for longer sequences. In Larochelle, H., Ranzato, M.,
    Hadsell, R., Balcan, M., and Lin, H. (eds.), *Advances in Neural Information Processing
    Systems*, volume 33, pp.  17283–17297\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等人 (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti,
    C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., 和 Ahmed, A. Big bird:
    适用于更长序列的变换器. 在 Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H.
    (编), *神经信息处理系统进展*，第 33 卷，第 17283–17297 页. Curran Associates, Inc., 2020. 网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf).'
- en: 'Zhang et al. (2023) Zhang, L., Li, Y., Liu, Z., Liu, J., Yang, M., et al. Marathon:
    A race through the realm of long context with large language models. *arXiv preprint
    arXiv:2312.09542*, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2023) Zhang, L., Li, Y., Liu, Z., Liu, J., Yang, M., 等. Marathon:
    一场穿越长上下文领域的大语言模型竞赛. *arXiv 预印本 arXiv:2312.09542*，2023.'
- en: 'Zhang et al. (2024a) Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou,
    Z. Soaring from 4k to 400k: Extending llm’s context with activation beacon. *arXiv
    preprint arXiv:2401.03462*, 2024a.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2024a) Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., 和 Dou, Z.
    从 4k 到 400k 的飞跃: 用激活信标扩展 LLM 的上下文. *arXiv 预印本 arXiv:2401.03462*，2024a.'
- en: 'Zhang et al. (2024b) Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,
    Han, X., Thai, Z. L., Wang, S., Liu, Z., et al. $\infty$ bench: Extending long
    context evaluation beyond 100k tokens. *arXiv preprint arXiv:2402.13718*, 2024b.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2024b) Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,
    Han, X., Thai, Z. L., Wang, S., Liu, Z., 等. $\infty$ bench: 将长上下文评估扩展到 100k 令牌以上.
    *arXiv 预印本 arXiv:2402.13718*，2024b.'
- en: Appendix A Code and Data Availability
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 代码和数据可用性
- en: Code for generating data and evaluating models is available at [https://github.com/booydar/babilong](https://github.com/booydar/babilong).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数据和评估模型的代码可在 [https://github.com/booydar/babilong](https://github.com/booydar/babilong)
    获得。
- en: 'We also provide pre-generated evaluation data hosted on HuggingFace datasets.
    The evaluation sets include 100 samples per length and per task, with lengths
    from 0k (no background text from PG-19) to 10 million tokens: [https://huggingface.co/datasets/RMT-team/babilong](https://huggingface.co/datasets/RMT-team/babilong)
    and 1000 samples per length and per task with lengths from 0k to 128k tokens:
    [https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples](https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了托管在 HuggingFace 数据集上的预生成评估数据。评估集包括每种长度和任务的 100 个样本，长度从 0k（没有 PG-19 的背景文本）到
    1000 万个标记：[https://huggingface.co/datasets/RMT-team/babilong](https://huggingface.co/datasets/RMT-team/babilong)
    和每种长度和任务的 1000 个样本，长度从 0k 到 128k 个标记：[https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples](https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples)。
- en: 'The croissant metadata for both evaluation sets is provided by HuggingFace:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 两个评估集的 croissant 元数据由 HuggingFace 提供：
- en: '[https://huggingface.co/api/datasets/RMT-team/babilong/croissant](https://huggingface.co/api/datasets/RMT-team/babilong/croissant)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/api/datasets/RMT-team/babilong/croissant](https://huggingface.co/api/datasets/RMT-team/babilong/croissant)'
- en: '[https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant](https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant](https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant)。'
- en: Our code is released under the Apache 2.0 License. We use data from the PG-19
    corpora (Rae et al., [2020](#bib.bib50)) (Apache 2.0 License⁶⁶6[https://github.com/google-deepmind/pg19](https://github.com/google-deepmind/pg19))
    and the bAbI dataset (Weston et al., [2016](#bib.bib68)) (BSD License⁷⁷7[https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md](https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码在 Apache 2.0 许可证下发布。我们使用来自 PG-19 语料库的数据（Rae 等， [2020](#bib.bib50)）（Apache
    2.0 许可证⁶⁶6[https://github.com/google-deepmind/pg19](https://github.com/google-deepmind/pg19)）和
    bAbI 数据集（Weston 等，[2016](#bib.bib68)）（BSD 许可证⁷⁷7[https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md](https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md)）。
- en: A.1 Reproducibility
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 可重复性
- en: 'Our code includes data generation, metrics, and the evaluation pipeline used
    to benchmark models. Additionally, we release the predictions of all models used
    in our study to ensure that all reported results can be reproduced and verified:
    [https://github.com/booydar/babilong/tree/predictions_06_2024](https://github.com/booydar/babilong/tree/predictions_06_2024).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码包括数据生成、指标和用于基准测试模型的评估管道。此外，我们发布了我们研究中使用的所有模型的预测，以确保所有报告的结果可以被重复和验证：[https://github.com/booydar/babilong/tree/predictions_06_2024](https://github.com/booydar/babilong/tree/predictions_06_2024)。
- en: Appendix B Related Work on Long Context Models
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 长文本模型的相关工作
- en: Approaches to long context processing
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理长上下文的方法
- en: In retrieval augmented generation (RAG), a language model is combined with a
    separate module, called a retriever. Given a specific request, the retriever finds
    a set of relevant parts from a dedicated data storage. Then parts selected by
    the retriever along with the input are incorporated by the language model to make
    predictions. Many different implementations of the retrieval mechanism have been
    proposed (Guu et al., [2020](#bib.bib26); Borgeaud et al., [2022](#bib.bib9);
    Shi et al., [2023](#bib.bib58)). Some works focus on directly retrieving predictions (Khandelwal
    et al., [2019](#bib.bib33)). Other works retrieve individual input tokens or text
    segments and add them to the LM input (Guu et al., [2020](#bib.bib26); Borgeaud
    et al., [2022](#bib.bib9)). For example, in REALM (Guu et al., [2020](#bib.bib26))
    whole text segments are retrieved and appended to the input to improve masked
    language modeling. In Memorizing Transformer (Wu et al., [2022b](#bib.bib70)),
    the retriever returns cached (key, value) pairs saved from previous training steps
    of the language model. In Retrieval-Pretrained Transformer (Rubin & Berant, [2023](#bib.bib54)),
    an LM component processes long documents in chunks and a retriever finds relevant
    chunks. Representations of retrieved chunks are fused with current chunk in the
    LM component, and both the LM and retrieval parts are trained jointly. AutoCompressor (Chevalier
    et al., [2023](#bib.bib14)) combines RMT-like (Bulatov et al., [2022](#bib.bib10))
    approach with retrieval from external corpora. AutoCompressor is first used to
    produce memory tokens (or summary vectors) for text chunks. Next, off-the-shelf
    retriever is used and corresponding chunk’s memory tokens are added to the context
    of the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索增强生成（RAG）中，语言模型与一个独立的模块——检索器相结合。给定特定的请求，检索器从专门的数据存储中找到一组相关的部分。然后，检索器选择的部分与输入一起被语言模型用于进行预测。已经提出了许多不同的检索机制实现方式（Guu
    et al., [2020](#bib.bib26)；Borgeaud et al., [2022](#bib.bib9)；Shi et al., [2023](#bib.bib58)）。一些研究集中于直接检索预测结果（Khandelwal
    et al., [2019](#bib.bib33)）。其他研究则检索单独的输入标记或文本段，并将它们添加到语言模型的输入中（Guu et al., [2020](#bib.bib26)；Borgeaud
    et al., [2022](#bib.bib9)）。例如，在 REALM（Guu et al., [2020](#bib.bib26)）中，检索整个文本段并将其附加到输入中，以改善掩码语言建模。在
    Memorizing Transformer（Wu et al., [2022b](#bib.bib70)）中，检索器返回从语言模型之前训练步骤中保存的缓存（key,
    value）对。在 Retrieval-Pretrained Transformer（Rubin & Berant, [2023](#bib.bib54)）中，语言模型组件将长文档分成块处理，检索器找到相关的块。检索到的块的表示与当前块在语言模型组件中融合，语言模型和检索部分一起训练。AutoCompressor（Chevalier
    et al., [2023](#bib.bib14)）结合了类似 RMT（Bulatov et al., [2022](#bib.bib10)）的方法与从外部语料库中检索。AutoCompressor
    首先用于生成文本块的记忆标记（或摘要向量）。接下来，使用现成的检索器，将相应块的记忆标记添加到模型的上下文中。
- en: In this work, we augment the Recurrent Memory Transformer (Bulatov et al., [2024](#bib.bib11))
    with the ability to retrieve its own past memory tokens. As far as we know, this
    is the first combination of a recurrent transformer with a trainable retrieval
    mechanism.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们增强了递归记忆变换器（Bulatov et al., [2024](#bib.bib11)），使其能够检索自己过去的记忆标记。就我们所知，这是递归变换器与可训练检索机制的首次结合。
- en: Recurrence is another mechanism to deal with long context (Graves et al., [2014](#bib.bib23);
    Voelker et al., [2019](#bib.bib64); Sorokin et al., [2022](#bib.bib61)). Instead
    of processing the entire context, a recurrent model breaks it down into smaller
    segments. The recurrent hidden state acts as an aggregator of information from
    past segments of the sequence. Attending to a memory state is much cheaper than
    to all contexts. Many different architectures adding recurrence to transformers
    have been proposed (Wu et al., [2022a](#bib.bib69); Lei et al., [2020](#bib.bib36);
    Fan et al., [2020](#bib.bib21)). For example, Compressive Transformer (Rae et al.,
    [2020](#bib.bib50)) updates recurrent memory by compressing hidden activation’s
    from the previous segment to a smaller set of representations. Recurrent Memory
    Transformer (Bulatov et al., [2022](#bib.bib10)) recurrently passes states of
    special memory tokens added to the input of Transformer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 递归是另一种处理长上下文的机制（Graves et al., [2014](#bib.bib23)；Voelker et al., [2019](#bib.bib64)；Sorokin
    et al., [2022](#bib.bib61)）。与处理整个上下文不同，递归模型将其拆分为更小的片段。递归隐藏状态充当来自序列过去片段的信息聚合器。关注记忆状态比关注所有上下文要便宜得多。已经提出了许多不同的将递归添加到变换器中的架构（Wu
    et al., [2022a](#bib.bib69)；Lei et al., [2020](#bib.bib36)；Fan et al., [2020](#bib.bib21)）。例如，Compressive
    Transformer（Rae et al., [2020](#bib.bib50)）通过将先前片段的隐藏激活压缩为更小的一组表示来更新递归记忆。递归记忆变换器（Bulatov
    et al., [2022](#bib.bib10)）递归地传递添加到变换器输入中的特殊记忆标记的状态。
- en: 'Activation Beacon (Zhang et al., [2024a](#bib.bib76)) compresses activations
    from prior segments using separate parameters and integrates a sliding window
    mechanism, handling up to 400k tokens. Temporal Latent Bottleneck (Didolkar et al.,
    [2022](#bib.bib16)) Transformer splits computation into two streams: recurrent
    slow stream and fast stream with self-attention between tokens. Block-Recurrent
    Transformer (Hutchins et al., [2022](#bib.bib30)) employs LSTM-style (Hochreiter
    & Schmidhuber, [1997](#bib.bib28)) gates to update its recurrent state.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Activation Beacon （Zhang 等人，[2024a](#bib.bib76)）使用单独的参数压缩来自先前段的激活，并整合了滑动窗口机制，处理高达
    400k 的 token。Temporal Latent Bottleneck （Didolkar 等人，[2022](#bib.bib16)）Transformer
    将计算分为两个流：递归的慢流和带有 token 之间自注意力的快流。Block-Recurrent Transformer （Hutchins 等人，[2022](#bib.bib30)）采用
    LSTM 风格的 （Hochreiter & Schmidhuber，[1997](#bib.bib28)）门来更新其递归状态。
- en: We use RMT in our experiments because of its simplicity, plug-and-play compatibility
    with pre-trained transformer-based language models, and promising scaling capabilities (Bulatov
    et al., [2024](#bib.bib11)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用 RMT，因其简单性、与预训练的基于 Transformer 的语言模型的即插即用兼容性以及令人期待的扩展能力 （Bulatov 等人，[2024](#bib.bib11)）。
- en: Big Bird (Zaheer et al., [2020](#bib.bib74)), Longformer (Beltagy et al., [2020](#bib.bib8)),
    LongNet (Ding et al., [2023](#bib.bib17)) help extend context length for Transformers
    by switching from full self-attention to sparse self-attention mechanisms with
    linear complexity. Works like RWKV (Peng et al., [2023a](#bib.bib46)), S4 (Gu
    et al., [2021](#bib.bib25)), Mamba (Gu & Dao, [2023](#bib.bib24)), take another
    approach and focus on advancing recurrent networks to reach high parallelism levels
    available to Transformers while retaining the linear complexity of RNN. These
    works show promising results on long sequences but are still lagging behind the
    best transformer models in natural language processing tasks. Mamba, however,
    seeks to bridge this gap.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Big Bird （Zaheer 等人，[2020](#bib.bib74)）、Longformer （Beltagy 等人，[2020](#bib.bib8)）、LongNet （Ding
    等人，[2023](#bib.bib17)）通过从完整自注意力机制转为具有线性复杂度的稀疏自注意力机制，帮助扩展 Transformer 的上下文长度。像
    RWKV （Peng 等人，[2023a](#bib.bib46)）、S4 （Gu 等人，[2021](#bib.bib25)）、Mamba （Gu & Dao，[2023](#bib.bib24)）这样的工作采取了另一种方法，专注于推动递归网络达到与
    Transformer 相匹配的高并行性水平，同时保留 RNN 的线性复杂度。这些工作在长序列上显示出有希望的结果，但在自然语言处理任务中仍落后于最好的 Transformer
    模型。然而，Mamba 旨在弥补这一差距。
- en: Appendix C Details on RMT and Mamba fine-tuning on BABILong
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 详细信息：RMT 和 Mamba 在 BABILong 上的微调
- en: '![Refer to caption](img/e9b0eec812105b7c08ce29b98041e3a6.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9b0eec812105b7c08ce29b98041e3a6.png)'
- en: 'Figure 5: RMT performance on five BABILong tasks varies between training seeds.
    The plot represents average performance and standard deviation across three training
    runs.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：RMT 在五个 BABILong 任务上的性能因训练种子而异。图表表示三个训练运行的平均性能和标准差。
- en: 'We used the GPT-2 (Radford et al., [2019](#bib.bib49)) (137M) model as the
    backbone for RMT. The segment size was fixed at 512 tokens, and the model was
    augmented with 16 memory tokens. Finetuning was conducted on BABILong using a
    curriculum schedule with progressively increasing sequence lengths: 1, 2, 4, 6,
    8, 16 and 32 segments. For each curriculum step $n$ for every batch to prevent
    overfitting to a certain context size. We maintained a fixed batch size of 64
    and the AdamW Loshchilov & Hutter ([2019](#bib.bib43)) optimizer with learning
    rate 1e-05, a linear schedule and 1000 warmup steps. Each curriculum stage had
    a maximum of 10,000 steps, although checkpoints were frequently finalized earlier
    based on the best validation exact match metric. The weight decay value was set
    to 0.01, and no gradient truncation was used. Training was performed on 1-4 Nvidia
    A100 or H100 GPUs with the duration of each curriculum stage ranging from 40 minutes
    to 20 hours.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 GPT-2 （Radford 等人，[2019](#bib.bib49)）（137M）模型作为 RMT 的骨干。段大小固定为 512 tokens，模型增加了
    16 个记忆 token。微调是在 BABILong 上使用逐步增加序列长度的课程计划进行的：1、2、4、6、8、16 和 32 个段。为了防止对某一上下文大小的过拟合，每个课程步骤
    $n$ 对每个批次进行调整。我们保持了固定的批量大小 64，并使用了 AdamW Loshchilov & Hutter（[2019](#bib.bib43)）优化器，学习率为
    1e-05，线性计划和 1000 个预热步骤。每个课程阶段最多有 10,000 步，尽管根据最佳验证精确匹配指标，检查点通常会更早完成。权重衰减值设置为 0.01，且未使用梯度截断。训练在
    1-4 个 Nvidia A100 或 H100 GPU 上进行，每个课程阶段的持续时间从 40 分钟到 20 小时不等。
- en: 'For each experiment we conducted three runs with different memory intitalizations
    and dataset shuffles. As shown in Figure [5](#A3.F5 "Figure 5 ‣ Appendix C Details
    on RMT and Mamba fine-tuning on BABILong ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack"), performance on context lengths, exceeding
    ones seen during training, may vary across different runs. This suggests that
    the early stopping criterion based on short-context accuracy may not be optimal.
    To reduce deviations between runs and enhance overall performance, techniques
    such as improving early stopping, gradient truncation and training on longer sequences
    can be employed.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个实验，我们进行了三次实验，分别使用了不同的内存初始化和数据集洗牌。如图[5](#A3.F5 "图 5 ‣ 附录 C 关于 RMT 和 Mamba
    在 BABILong 上的微调细节 ‣ BABILong: 测试具有长上下文推理的 LLM 的极限")所示，在上下文长度超出训练时所见的范围时，性能可能会因不同的实验而有所不同。这表明，基于短上下文准确率的早停准则可能不是最佳的。为了减少实验之间的偏差并提升整体性能，可以采用如改善早停、梯度截断和在更长序列上训练等技术。'
- en: To fine-tune mamba-130m, we used the exact same curriculum approach, with a
    randomly selected number of segments that gradually increased. Throughout every
    curriculum step, the batch size remained constant at 128\. We employed the AdamW
    optimizer with a linear schedule, weight decay of 2.0, gradient clipping of 1.0,
    learning rate of 3e–4, and a warmup step count of 10% of the total training steps.
    The model was trained for 10,000 steps in each curriculum stage except for the
    last one, which had 32 segments, where it was trained for 15,000 steps. 4 NVidia
    H100 GPUs were used for the training, and the overall training process for every
    task from BABILong took 2 to 3 days to complete.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了微调 mamba-130m，我们使用了完全相同的课程方法，随机选择了逐渐增加的段数。在每个课程步骤中，批量大小保持不变为 128。我们使用了带有线性计划的
    AdamW 优化器，权重衰减为 2.0，梯度裁剪为 1.0，学习率为 3e–4，预热步数为总训练步数的 10%。模型在每个课程阶段训练了 10,000 步，除了最后一个阶段，最后一个阶段有
    32 个段，在其中训练了 15,000 步。训练使用了 4 个 NVidia H100 GPU，每个 BABILong 任务的整体训练过程耗时 2 到 3
    天。
- en: Appendix D Detailed LLM evaluation on BABILong tasks
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 关于 BABILong 任务的详细 LLM 评估
- en: '![[Uncaptioned image]](img/ae8fb00b7fdbd759c265fb8452479940.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/ae8fb00b7fdbd759c265fb8452479940.png)'
- en: 'Table 3: Results of LLM evaluation on the first five tasks of BABILong. Rows
    correspond to sequence lengths, columns denote models, and each section represents
    a separate task from QA1 to QA5\. Each number indicates the average accuracy of
    the model at a given sequence length, calculated over 1000 samples for lengths
    up to 32k tokens, and over 100 samples for longer lengths.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：BABILong 前五个任务的 LLM 评估结果。行对应序列长度，列表示模型，每个部分代表一个单独的任务，从 QA1 到 QA5。每个数字表示模型在给定序列长度下的平均准确率，长度为
    32k 标记的样本数为 1000，长度更长的样本数为 100。
- en: 'Here we present the complete results of LLM evaluation. Table [3](#A4.T3 "Table
    3 ‣ Appendix D Detailed LLM evaluation on BABILong tasks ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack") showcases the performance
    of 27 models across the first five tasks. Comparing the tasks in the table makes
    evident the difference in task complexity for language models. QA1 and QA5 are
    the easiest, with most models achieving over 70% accuracy for the 0k split. QA4
    is significantly more challenging, and only 5 models can reach this level of performance.
    QA2 and QA3 pose even greater challenges for most models.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '这里我们展示了 LLM 评估的完整结果。表[3](#A4.T3 "表 3 ‣ 附录 D 关于 BABILong 任务的详细 LLM 评估 ‣ BABILong:
    测试具有长上下文推理的 LLM 的极限")展示了27个模型在前五个任务中的表现。比较表中的任务，可以清楚地看出语言模型任务复杂性的差异。QA1 和 QA5
    是最简单的，大多数模型在 0k 切分上达到了 70% 以上的准确率。QA4 则明显更具挑战性，仅有 5 个模型能够达到这个性能水平。QA2 和 QA3 对大多数模型提出了更大的挑战。'
- en: The number of parameters positively impacts accuracy on the shortest 0k split.
    Among not-finetuned models, GPT-4, Phi-3-medium, Jamba, Command-R, Yi-34B and
    Mixtral 8x22B consistently outperform smaller models. Notably, RWKV and Mamba-2.8B
    also demonstrate strong performance on QA2 and QA3\. However, as the context length
    increases, some of the largest models lose their advantage over smaller ones.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量对最短 0k 切分上的准确性有积极影响。在未微调的模型中，GPT-4、Phi-3-medium、Jamba、Command-R、Yi-34B 和
    Mixtral 8x22B 一直优于较小的模型。特别地，RWKV 和 Mamba-2.8B 在 QA2 和 QA3 上也表现出色。然而，随着上下文长度的增加，一些最大模型相对于较小模型的优势逐渐减小。
- en: Retreival-augmented Llama-3 has a strong advantage of being able to perform
    on any context length up to 10M tokens. On QA4 and QA5 retrieval allows to match
    and even surpass weaker competitors on longer context sizes. However, on QA2 and
    QA3 this approach fails dramatically. The reason for this performance drop lies
    in inability of retrieval to maintain the order of found sentences, complicating
    the task for the underlying Llama. Additionally, relevant sentences in these tasks
    are not always semantically similar to the question, preventing the model from
    retrieving all necessary facts for correct reasoning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 增强检索的Llama-3在任何上下文长度高达10M标记的情况下具有强大的优势。在QA4和QA5中，检索可以匹配甚至超越较弱的竞争对手，在较长的上下文尺寸上表现出色。然而，在QA2和QA3中，这种方法会显著失效。性能下降的原因在于检索无法维持找到的句子的顺序，从而使底层Llama任务变得复杂。此外，这些任务中的相关句子并不总是与问题在语义上相似，阻碍了模型检索所有必要的事实以进行正确推理。
- en: It is important to note, that all BABILong tasks are in practice solvable even
    with smaller models. Finetuned RMT and Mamba achieve outstanding scores across
    most sequence lengths, significantly outperforming LLMs despite having up to 100
    times ewer parameters. Mamba has an advantage on medium-length sequences, but
    RMT excells in processing much larger sequences up to 10M tokens.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，所有BABILong任务实际上即使在较小模型下也可以解决。微调的RMT和Mamba在大多数序列长度上取得了卓越的成绩，尽管参数最多多达100倍，但它们显著超越了LLMs。Mamba在中等长度序列上具有优势，但RMT在处理长度高达10M标记的序列时表现出色。
- en: Appendix E RMT Training and Evaluation Details
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E RMT训练和评估细节
- en: 'For RMT we use curriculum training with sequentially increasing number of segments.
    RMT uses the following schedule for number of segments: 1-2-4-6-8-16-32\. During
    each curriculum stage $n$ and train for {5000, 10000} steps with early stopping
    if metrics stop increasing. For the backbone transformer we use the pretrained
    GPT-2 137M from HuggingFace: [https://huggingface.co/GPT-2](https://huggingface.co/GPT-2).
    We used up to 4 Nvidia A100 80Gb per experiment.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RMT，我们使用递增的课程训练，序列数逐步增加。RMT采用以下序列数量安排：1-2-4-6-8-16-32。在每个课程阶段$n$中训练{5000,
    10000}步，并在指标停止增长时进行提前停止。对于基础变换器，我们使用HuggingFace提供的预训练GPT-2 137M：[https://huggingface.co/GPT-2](https://huggingface.co/GPT-2)。每个实验中使用了最多4个Nvidia
    A100 80Gb。
- en: 'We evaluate models on the BABILong benchmark. We use full test set for sequences
    up to 1M tokens, for 10M tokens we only provide results for 100 samples. As shown
    in Table [4](#A5.T4 "Table 4 ‣ Appendix E RMT Training and Evaluation Details
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"),
    evaluation time grows linearly with context length. We fix a random seed used
    to sample background texts from PG19 for the test set. However, the seed was not
    fixed to avoid overfitting to specific sampled texts during training.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在BABILong基准上评估模型。对于长度最多为1M标记的序列，我们使用完整的测试集；对于10M标记的序列，我们仅提供100个样本的结果。如表[4](#A5.T4
    "Table 4 ‣ Appendix E RMT Training and Evaluation Details ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack")所示，评估时间与上下文长度呈线性增长。我们固定了用于从PG19中抽取背景文本的随机种子。然而，为了避免在训练过程中对特定采样文本的过拟合，种子并没有固定。'
- en: 'Table 4: Time required for processing 1000 BABILong samples with RMT using
    a single A100 80Gb GPU, including input data processing.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：使用单个A100 80Gb GPU处理1000个BABILong样本所需的时间，包括输入数据处理。
- en: '| Context size | 4k | 32k | 128k | 1M |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 上下文大小 | 4k | 32k | 128k | 1M |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Processing time, minutes | 4 | 30 | 80 | 315 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 处理时间，分钟 | 4 | 30 | 80 | 315 |'
- en: Appendix F BABILong Dataset Statistics
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F BABILong数据集统计
- en: 'The proposed benchmark includes 20 diverse tasks, ranging from simple "needle
    in a haystack" scenarios with distractor facts to more complex tasks that require
    counting, logical reasoning, or spatial reasoning. The Figure [6](#A6.F6 "Figure
    6 ‣ Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack") evaluates the complexity of the base
    short versions of these tasks. Tasks such as QA1, QA5, and QA10 are generally
    easier for most models, whereas QA7, QA15, and QA19 are the most challenging.
    The plot clearly shows that the number of facts needed for reasoning significantly
    impacts task complexity, as performance gradually declines from QA1 to QA2 and
    QA3, which differ in the number of supporting facts. The distribution of task
    labels is shown in Table [6](#A6.T6 "Table 6 ‣ Appendix F BABILong Dataset Statistics
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '提出的基准包括20个多样化的任务，从带有干扰事实的简单“针在干草堆中”场景到需要计数、逻辑推理或空间推理的更复杂任务。图[6](#A6.F6 "Figure
    6 ‣ Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")评估了这些任务的基础短版本的复杂性。QA1、QA5和QA10等任务对于大多数模型来说通常较容易，而QA7、QA15和QA19则是最具挑战性的。图中清楚地显示了推理所需的事实数量显著影响任务复杂性，因为从QA1到QA2和QA3，性能逐渐下降，这些任务在支持事实的数量上有所不同。任务标签的分布如表[6](#A6.T6
    "Table 6 ‣ Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")所示。'
- en: '![Refer to caption](img/7ddd1004ec8e6f97351f741bc593b00a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7ddd1004ec8e6f97351f741bc593b00a.png)'
- en: 'Figure 6: The performance of LLMs on the bAbI (BABILong without distractor
    text) depends significantly on the task complexity. Each dot represents the average
    accuracy of the model on one thousand samples of the given task. The median accuracy
    across all models is denoted by black stars.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：LLMs在bAbI（没有干扰文本的BABILong）上的表现显著依赖于任务的复杂性。每个点代表模型在给定任务的一千个样本上的平均准确率。所有模型的中位准确率由黑色星星表示。
- en: 'BABILong is a generative benchmark, designed to be scalable with increasing
    length of language models. The same bAbI task can be scaled to any desired length
    in tokens by adding a sufficient number of distractor sentences. For reproducibility,
    we pre-generate dataset splits for several fixed lengths: 0k (tasks with no distractor
    sentences), 4k, 8k, 16k, 32k, 64k, 128k, 512k, 1M and 10M tokens. The length in
    tokens is measured using the classic GPT-2 tokenizer, which is close in fertility
    to the popular GPT-4 tokenizer. As shown in Table [5](#A6.T5 "Table 5 ‣ Appendix
    F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs with Long
    Context Reasoning-in-a-Haystack"), the number of tokens for tokenizers of different
    models may differ for samples in the same split. However considering the trade-off
    between the sequence length and embedding layer size, we believe the comparison
    remains fair.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'BABILong是一个生成性基准，旨在随着语言模型长度的增加而具有可扩展性。通过添加足够数量的干扰句子，可以将相同的bAbI任务扩展到所需的任意长度。为了确保可重复性，我们为多个固定长度预生成了数据集拆分：0k（没有干扰句子的任务）、4k、8k、16k、32k、64k、128k、512k、1M和10M
    tokens。token的长度使用经典的GPT-2分词器进行测量，其在繁殖性方面接近于流行的GPT-4分词器。如表[5](#A6.T5 "Table 5 ‣
    Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")所示，不同模型的分词器在相同拆分中的样本的token数量可能会有所不同。然而，考虑到序列长度和嵌入层大小之间的权衡，我们认为比较仍然公平。'
- en: 'Table 5: Token count for various models across selected tasks. We measure the
    length of BABILong samples using the conservative GPT-2 tokenizer. Actual token
    sizes may vary depending on the model tokenizer.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：各种模型在选定任务中的token计数。我们使用保守的GPT-2分词器来测量BABILong样本的长度。实际的token大小可能会根据模型分词器有所不同。
- en: '| Model | 0k | 4k | 16k | 64k | 128k |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 0k | 4k | 16k | 64k | 128k |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4 | 120 | 3544 | 15071 | 61343 | 123367 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 120 | 3544 | 15071 | 61343 | 123367 |'
- en: '| GPT-2 | 120 | 3700 | 15699 | 63698 | 127695 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 120 | 3700 | 15699 | 63698 | 127695 |'
- en: '| Llama-2 | 135 | 3942 | 16757 | 68110 | 137222 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 135 | 3942 | 16757 | 68110 | 137222 |'
- en: '| Mistral | 128 | 3863 | 16438 | 66862 | 134592 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 128 | 3863 | 16438 | 66862 | 134592 |'
- en: '| Words | 98 | 2548 | 10789 | 44180 | 88592 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 词汇 | 98 | 2548 | 10789 | 44180 | 88592 |'
- en: '| Symbols | 561 | 14507 | 61452 | 251947 | 507598 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 561 | 14507 | 61452 | 251947 | 507598 |'
- en: 'Table 6: The distribution of labels in first five BABILong tasks, % of all
    samples.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：前五个BABILong任务中的标签分布，占所有样本的百分比。
- en: '|  | label1 | label2 | label3 | label4 | label5 | label6 | label7 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | label1 | label2 | label3 | label4 | label5 | label6 | label7 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| qa1 | 15.4 | 14.9 | 15.7 | 18.7 | 18.2 | 17.1 |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| qa1 | 15.4 | 14.9 | 15.7 | 18.7 | 18.2 | 17.1 |  |'
- en: '| qa2 | 15.9 | 18.7 | 16.7 | 16.5 | 17.5 | 14.6 |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| qa2 | 15.9 | 18.7 | 16.7 | 16.5 | 17.5 | 14.6 |  |'
- en: '| qa3 | 13.3 | 18.4 | 21.5 | 14.6 | 15.4 | 16.7 |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| qa3 | 13.3 | 18.4 | 21.5 | 14.6 | 15.4 | 16.7 |  |'
- en: '| qa4 | 15.6 | 17.7 | 16.6 | 17.1 | 15.3 | 17.6 |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| qa4 | 15.6 | 17.7 | 16.6 | 17.1 | 15.3 | 17.6 |  |'
- en: '| qa5 | 9.5 | 18.8 | 12.9 | 16.4 | 13.6 | 18.9 | 9.8 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| qa5 | 9.5 | 18.8 | 12.9 | 16.4 | 13.6 | 18.9 | 9.8 |'
- en: Appendix G Details of the RAG Pipeline
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G RAG 流水线详细信息
- en: 'For the GPT4-RAG pipelines, we employed the FAISS (Douze et al., [2024](#bib.bib19))
    vector database, using Langchain (Chase, [2022](#bib.bib12)), for our experimental
    RAG setup. We utilized the ’text-embedding-ada-002’ model for generating text
    embeddings. Our methodology encompassed two distinct approaches for text chunking:
    firstly, segmentation by sentences utilizing the NLTK library, and secondly, division
    into segments of 512 tokens each. We adopted a binary metric for evaluating retrieval
    accuracy, where the criterion was the presence or absence of relevant facts (singular
    or multiple, based on the specific task) within the retrieved text chunks. This
    retrieval accuracy was quantified for the top 5 chunks. Additionally, we assessed
    the performance of GPT-4-turbo in conjunction with the retrieved facts, specifically
    focusing on the ’QA1’ task. Our experimental scope spanned various context lengths,
    including 8k, 64k, and 128k tokens for tasks ’QA1’ through ’QA5’ of the BABILong
    dataset, with added 4k, 16k, 32k, 500k, 1M and 10M token length for an in-depth
    analysis of the ’QA1’ task. Additionally, we assessed the performance of RAG on
    the ’QA1’ task, utilizing precomputed Wikipedia embeddings⁸⁸8[https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings](https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings)
    instead of pg-19 with an average embedding size of 250 tokens. This evaluation
    aimed to determine the influence of embedding size and noise characteristics on
    model performance. For each task, we maintained a consistent sample size of 50
    across different context lengths. For the Llama3 + RAG pipeline we used the ’nvidia/Llama3-ChatQA-1.5-8B’
    as the language model Liu et al. ([2024b](#bib.bib42)) and the ’nvidia/dragon-multiturn-query-encoder’
    for context embedding. Another difference is that we did not use any caching and
    Wikipedia embeddings unlike with GPT-4.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT4-RAG 流水线，我们使用了 FAISS (Douze et al., [2024](#bib.bib19)) 向量数据库，并利用 Langchain
    (Chase, [2022](#bib.bib12)) 进行实验性的 RAG 设置。我们使用了’text-embedding-ada-002’模型生成文本嵌入。我们的方法包括两种不同的文本分块方式：首先，利用
    NLTK 库按句子进行分割；其次，划分为每个512个标记的段落。我们采用了二元指标来评估检索准确性，标准是检索文本块中相关事实的存在或缺失（根据具体任务，可能是单个或多个）。此检索准确性是对前5个块进行量化的。此外，我们评估了
    GPT-4-turbo 与检索到的事实结合的性能，特别关注于’QA1’任务。我们的实验范围涵盖了不同的上下文长度，包括 8k、64k 和 128k 标记，涉及
    BABILong 数据集的’QA1’至’QA5’任务，并增加了 4k、16k、32k、500k、1M 和 10M 标记长度，以深入分析’QA1’任务。此外，我们评估了
    RAG 在’QA1’任务上的表现，使用了预计算的 Wikipedia 嵌入⁸⁸8[https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings](https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings)
    替代了 pg-19，平均嵌入大小为 250 个标记。这项评估旨在确定嵌入大小和噪声特征对模型性能的影响。对于每个任务，我们在不同上下文长度下保持了 50 的一致样本大小。对于
    Llama3 + RAG 流水线，我们使用了’nvidia/Llama3-ChatQA-1.5-8B’作为语言模型 Liu et al. ([2024b](#bib.bib42))
    和’nvidia/dragon-multiturn-query-encoder’用于上下文嵌入。另一个不同之处是我们没有像 GPT-4 那样使用任何缓存和
    Wikipedia 嵌入。
- en: Appendix H Recurrent Memory Transformer Analysis
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 递归记忆变换器分析
- en: 'To understand how recurrent models consistently retain their performance over
    extremely long sequences, we analyze the RMT memory states and attention patterns
    on the QA1 task. We evaluate RMT trained on 32 segments or approximately 16k tokens
    on a single sample with two facts, see Figure [7](#A8.F7 "Figure 7 ‣ Appendix
    H Recurrent Memory Transformer Analysis ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack") (a) and (b). For both sequence lengths
    16k and 128k the memory states exhibit a consistent pattern. In the absence of
    fact in input, the memory remains similar to its initial states, but the introduction
    of fact leads to visible change in the memory state. This indicates that the model
    learned to distinguish important facts from the background text and preserving
    them in memory until a question appears. The operations with memory are represented
    by distinctive patterns on attention maps, specifically, the process of writing
    a new fact to memory Figure [7](#A8.F7 "Figure 7 ‣ Appendix H Recurrent Memory
    Transformer Analysis ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack") (c) and reading from memory to answer a question (d).
    This visual demonstration supports the intuition of learning distinct memory operations
    when dealing with information scattered across extended contextual spans.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解递归模型如何在极长序列上持续保持其性能，我们分析了 RMT 在 QA1 任务上的记忆状态和关注模式。我们评估了在单个样本上训练了 32 个段或大约
    16k 个标记的 RMT，包含两个事实，见图 [7](#A8.F7 "图 7 ‣ 附录 H 递归记忆变换器分析 ‣ BABILong: 测试 LLM 在长上下文推理中的极限")
    (a) 和 (b)。对于 16k 和 128k 的序列长度，记忆状态表现出一致的模式。在输入中没有事实的情况下，记忆保持与其初始状态相似，但事实的引入导致记忆状态发生明显变化。这表明模型学会了将重要事实与背景文本区分开来，并将其保存在记忆中，直到出现问题。记忆操作通过注意图上的独特模式表示，具体来说，就是将新事实写入记忆的过程，见图
    [7](#A8.F7 "图 7 ‣ 附录 H 递归记忆变换器分析 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") (c)，以及从记忆中读取以回答问题
    (d)。这种视觉演示支持在处理分散于扩展上下文跨度的信息时学习不同记忆操作的直观感受。'
- en: '![Refer to caption](img/8cbb567df39d312fa2cd5e1903938826.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8cbb567df39d312fa2cd5e1903938826.png)'
- en: 'Figure 7: RMT learns to detect and store relevant facts using memory. Heatmaps
    (a) and (b) represent pairwise distances between memory states on QA1 with context
    size 16k (a) and 128k (b). Distant states are marked with blue color and similar
    ones with red. Changes in memory mainly occurs when the model meets a new fact,
    which indicates model adaptation to distinguishing and storing facts in memory.
    Memory attention maps (c) and (d) when RMT writes the fact to memory (c) and then
    reads it when answering the question (d). The intensity of red color corresponds
    to the amount of attention between the query on the left and key on the top.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：RMT 学会使用记忆来检测和存储相关事实。热图 (a) 和 (b) 表示在上下文大小为 16k (a) 和 128k (b) 的 QA1 上，记忆状态之间的成对距离。距离较远的状态用蓝色标记，类似的状态用红色标记。当模型遇到新事实时，记忆的变化主要发生，这表明模型在适应于区分和存储记忆中的事实。记忆关注图
    (c) 和 (d) 显示了 RMT 在将事实写入记忆时 (c) 和回答问题时读取记忆的情况 (d)。红色的强度对应于左侧查询和顶部键之间的关注量。
- en: Appendix I LLM fine-tuning
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I LLM 微调
- en: 'Results for GPT-3.5 and Mistral-7B fine-tuning are shown on the Fig.[8](#A9.F8
    "Figure 8 ‣ Appendix I LLM fine-tuning ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack").'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-3.5 和 Mistral-7B 微调的结果如图 [8](#A9.F8 "图 8 ‣ 附录 I LLM 微调 ‣ BABILong: 测试 LLM
    在长上下文推理中的极限") 所示。'
- en: '![Refer to caption](img/cfa45a247ec7e49453b4d4ad2f13081b.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cfa45a247ec7e49453b4d4ad2f13081b.png)'
- en: 'Figure 8: LLM fine-tuning makes full context effective. a) After fine-tuning
    both GPT-3.5 and Mistral-7B significantly improved their scores along context
    lengths achieving 90% + accuracy on QA1 task. b) GPT-3.5 fine-tuned for QA1 task
    shows improved performance on QA2-QA5 tasks. c) Full fine-tuning of smaller Mistral-7B
    on QA1 results in degraded scores for other tasks (QA2-QA5). No distractor text
    for b) and c).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLM 微调使得完整上下文有效。a) 微调后，GPT-3.5 和 Mistral-7B 的得分在上下文长度上显著提高，在 QA1 任务中达到了 90%
    以上的准确率。b) 针对 QA1 任务微调的 GPT-3.5 在 QA2-QA5 任务上表现出改进的性能。c) 在 QA1 上完全微调的小型 Mistral-7B
    对其他任务 (QA2-QA5) 的得分结果有所下降。b) 和 c) 中没有干扰文本。
- en: Appendix J Prompts Used to Benchmark GPT-4-Turbo and Mistral Models
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 用于基准测试 GPT-4-Turbo 和 Mistral 模型的提示
- en: We used the same prompts to evaluate GPT-4-Turbo and Mistral models in our tasks.
    Each prompt starts with the description of the task followed by several examples
    inside the $<$ tags. The next section inside $<$ tags contains an instance of
    the task. We additionally duplicate the question with the QUESTION mark, in order
    for the model recognize the question in the large input prompts. The last sentences
    specify the required response format.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的提示来评估 GPT-4-Turbo 和 Mistral 模型在我们的任务中的表现。每个提示都以任务描述开始，后面跟着几个在 $<$ 标签内的示例。接下来的部分在
    $<$ 标签内包含任务的一个实例。我们额外重复了带有 QUESTION 标记的问题，以便模型能够识别大输入提示中的问题。最后的句子指定了所需的响应格式。
- en: <svg id="A10.p2.pic1" class="ltx_picture" height="432.45" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,432.45) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 414.24)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA1
    task</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 13.78)"><foreignobject width="556.69" height="382.75" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">I will give you context with the facts about positions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts. If a person was in different locations, use the latest location to answer the question.
    <example> Charlie went to the hallway. Judith come back to the kitchen. Charlie travelled to balcony. Where is Charlie?
    Answer: The most recent location of Charlie is balcony. </example> <example> Alan moved to the garage. Charlie went to the beach. Alan went to the shop. Rouse travelled to balcony. Where is Alan?
    Answer: The most recent location of Alan is shop. </example> <context> {QA1 query with noise}
    </context> QUESTION: {QA1 question} Always return your answer in the following format: The most recent location of ’person’ is ’location’. Do not write anything else after that.<svg
    id="A10.p3.pic1" class="ltx_picture" height="484.08" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,484.08) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 465.88)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA2 task</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="434.38" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">I give you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.
    If a person got an item in the first location and travelled to the second location the item is also in the second location.
    If a person dropped an item in the first location and moved to the second location the item remains in the first location.
    <example> Charlie went to the kitchen. Charlie got a bottle. Charlie moved to the balcony. Where is the bottle?
    Answer: The bottle is in the balcony. </example> <example> Alan moved to the garage. Alan got a screw driver. Alan moved to the kitchen. Where is the screw driver?
    Answer: The screw driver is in the kitchen. </example> <context> {QA2 query with noise}
    </context> QUESTION: {QA2 question} Always return you answer in the following format: The ’item’ is in ’location’. Do not write anything else after that.<svg
    id="A10.p4.pic1" class="ltx_picture" height="538.3" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,538.3) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 520.1)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA3 task</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="488.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">I give you context with the facts about locations and actions of different persons hidden in some random text and a question.
    You need to answer the question based only on the information from the facts.
    If a person got an item in the first location and travelled to the second location the item is also in the second location.
    If a person dropped an item in the first location and moved to the second location the item remains in the first location
    <example> John journeyed to the bedroom.Mary grabbed the apple. Mary went back to the bathroom. Daniel journeyed to the bedroom. Daniel moved to the garden. Mary travelled to the kitchen. Where was the apple before the kitchen?
    Answer: Before the kitchen the apple was in the bathroom. </example> <example>
    John went back to the bedroom. John went back to the garden. John went back to the kitchen. Sandra took the football. Sandra travelled to the garden. Sandra journeyed to the bedroom. Where was the football before the bedroom?
    Answer: Before the kitchen the football was in the garden. </example> <context>
    {QA3 query with noise} </context> QUESTION: {QA3 question} Always return you answer in the following format: Before the $location_1& the $item$ was in the $location_2$. Do not write anything else after that.<svg
    id="A10.p5.pic1" class="ltx_picture" height="413.15" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,413.15) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 394.95)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA4 task</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="363.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">I will give you context with the facts about different people, their location and actions, hidden in some random text and a question.
    You need to answer the question based only on the information from the facts.
    <example> The hallway is south of the kitchen. The bedroom is north of the kitchen. What is the kitchen south of?
    Answer: bedroom </example> <example> The garden is west of the bedroom. The bedroom is west of the kitchen. What is west of the bedroom?
    Answer: garden </example> <context> {QA4 query with noise} </context> QUESTION: {QA4 question}
    Your answer should contain only one word - location. Do not write anything else after that<svg
    id="A10.p6.pic1" class="ltx_picture" height="508.9" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,508.9) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 490.69)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA5 task</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="459.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">I will give you context with the facts about locations and their relations hidden in some random text and a question. You need to answer the question based only on the information from the facts.
    <example> Mary picked up the apple there. Mary gave the apple to Fred. Mary moved to the bedroom. Bill took the milk there. Who did Mary give the apple to?
    Answer: Fred </example> <example> 1 Jeff took the football there. Jeff passed the football to Fred. Jeff got the milk there. Bill travelled to the bedroom. Who gave the football?
    Answer: Jeff </example> <example> Fred picked up the apple there. Fred handed the apple to Bill. Bill journeyed to the bedroom. Jeff went back to the garden. What did Fred give to Bill?
    Answer: apple </example> <context> {QA5 query with noise} </context> QUESTION: {QA5 question}
    Your answer should contain only one word. Do not write anything else after that. Do not explain your answer.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="A10.p2.pic1" class="ltx_picture" height="432.45" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,432.45) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 414.24)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA1任务</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="382.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">我将提供有关不同人物位置的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。如果一个人曾经在不同的位置，请使用最新的位置来回答问题。<example>
    查理去了走廊。朱迪回到厨房。查理去阳台了。查理现在在哪里？ 答案：查理最新的位置是阳台。</example> <example> 艾伦去了车库。查理去了海滩。艾伦去了商店。罗斯去了阳台。艾伦现在在哪里？
    答案：艾伦最新的位置是商店。</example> <context> {QA1查询和噪声} </context> 问题：{QA1问题} 请始终以以下格式返回你的答案：’person’最新的位置是’location’。不要在后面写任何其他内容。<svg
    id="A10.p3.pic1" class="ltx_picture" height="484.08" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,484.08) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 465.88)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA2任务</foreignobject></g> <g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="434.38" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">我提供有关不同人物的位置和行动的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。如果一个人在第一个位置拿到了一件物品，并移动到第二个位置，那么物品也在第二个位置。如果一个人在第一个位置丢下了物品，并移动到第二个位置，那么物品仍然在第一个位置。<example>
    查理去了厨房。查理拿到了一个瓶子。查理移到了阳台。瓶子现在在哪里？ 答案：瓶子在阳台。</example> <example> 艾伦去了车库。艾伦拿到了一把螺丝刀。艾伦去了厨房。螺丝刀现在在哪里？
    答案：螺丝刀在厨房。</example> <context> {QA2查询和噪声} </context> 问题：{QA2问题} 请始终以以下格式返回你的答案：’item’在’location’。不要在后面写任何其他内容。<svg
    id="A10.p4.pic1" class="ltx_picture" height="538.3" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,538.3) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 520.1)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA3任务</foreignobject></g> <g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="488.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">我提供有关不同人物的位置和行动的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。如果一个人在第一个位置拿到了一件物品，并移动到第二个位置，那么物品也在第二个位置。如果一个人在第一个位置丢下了物品，并移动到第二个位置，那么物品仍然在第一个位置。<example>
    约翰去了卧室。玛丽拿到了苹果。玛丽回到浴室。丹尼尔去了卧室。丹尼尔去了花园。玛丽去了厨房。苹果在厨房之前在哪里？ 答案：在厨房之前，苹果在浴室。</example>
    <example> 约翰回到卧室。约翰回到花园。约翰回到厨房。桑德拉拿到了足球。桑德拉去了花园。桑德拉去了卧室。足球在卧室之前在哪里？ 答案：在卧室之前，足球在花园。</example>
    <context> {QA3查询和噪声} </context> 问题：{QA3问题} 请始终以以下格式返回你的答案：在$location_1$之前，$item$在$location_2$。不要在后面写任何其他内容。<svg
    id="A10.p5.pic1" class="ltx_picture" height="413.15" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,413.15) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 394.95)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA4任务</foreignobject></g> <g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="363.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">我将提供有关不同人物、他们的位置和行动的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。<example>
    走廊在厨房的南边。卧室在厨房的北边。厨房在南边是什么？ 答案：卧室</example> <example> 花园在卧室的西边。卧室在厨房的西边。卧室的西边是什么？
    答案：花园</example> <context> {QA4查询和噪声} </context> 问题：{QA4问题} 你的答案应仅包含一个词 - 位置。不要在后面写任何其他内容。<svg
    id="A10.p6.pic1" class="ltx_picture" height="508.9" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,508.9) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 490.69)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">QA5任务</foreignobject></g> <g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)
- en: Appendix K Analysis of LLM Performance for Different Locations of the Supporting
    Facts
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 K 对不同支持事实位置的LLM性能分析
- en: 'Fig. [9](#A11.F9 "Figure 9 ‣ Appendix K Analysis of LLM Performance for Different
    Locations of the Supporting Facts ‣ BABILong: Testing the Limits of LLMs with
    Long Context Reasoning-in-a-Haystack") shows the evaluation result of the GPT-4-Turbo
    model when all the facts in task are located in the same quarter of the input
    query. It is seen that the performance of the model is not the same for different
    locations of the supporting facts. The most difficult location to identify the
    facts is in the middle of context which corresponds to the depts = 50 in the Fig. [9](#A11.F9
    "Figure 9 ‣ Appendix K Analysis of LLM Performance for Different Locations of
    the Supporting Facts ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack").'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [9](#A11.F9 "图 9 ‣ 附录 K 对不同支持事实位置的LLM性能分析 ‣ BABILong：测试LLMs在长上下文推理中的极限") 显示了当任务中的所有事实位于输入查询的同一象限时GPT-4-Turbo模型的评估结果。可以看出，模型在支持事实的不同位置上的表现并不相同。识别事实最困难的位置是上下文中间，这对应于图 [9](#A11.F9
    "图 9 ‣ 附录 K 对不同支持事实位置的LLM性能分析 ‣ BABILong：测试LLMs在长上下文推理中的极限")中的depts = 50。
- en: '![Refer to caption](img/bf6e3974fe95470319cba405afad377d.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf6e3974fe95470319cba405afad377d.png)'
- en: 'Figure 9: Evaluation results for GPT-4-Turbo with different locations of the
    facts in the QA1 task.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：GPT-4-Turbo在QA1任务中对不同位置事实的评估结果。
- en: Appendix L BABILong Task Examples
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 L BABILong 任务示例
- en: This section contains samples of the final BABILong dataset for first five tasks.
    First part of each example displays facts needed to solve the task, second part
    shows the example with background text with total length up to 512 tokens, and
    the final part contains question and the desired answer. The tasks differ by the
    number of facts and the task complexity, testing the ability for multiple reasoning
    aspects.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含前五个任务的最终BABILong数据集示例。每个示例的第一部分显示了解决任务所需的事实，第二部分展示了背景文本的示例，总长度达到512个token，最后一部分包含问题和期望的答案。任务根据事实数量和任务复杂性有所不同，测试多个推理方面的能力。
- en: '| QA1 single-supporting-fact |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| QA1 单一支持事实 |'
- en: '| --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Facts: Sandra moved to the kitchen. Sandra went back to the garden. Sandra
    journeyed to the office. Mary moved to the office. Sandra journeyed to the bathroom.
    Daniel moved to the office. Daniel went back to the kitchen. Mary moved to the
    hallway. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 事实：桑德拉去了厨房。桑德拉回到了花园。桑德拉前往办公室。玛丽去了办公室。桑德拉前往卫生间。丹尼尔去了办公室。丹尼尔回到了厨房。玛丽去了走廊。 |'
- en: '| Input: Now this loss of the sense of proportion in human affairs, Sir, is
    a very bad sign, and a well-nigh infallible indicator of nerve-strain and general
    overpressure. Sandra moved to the kitchen. But I find a yet more unmistakable
    evidence in support of my contention in the extraordinary emotional sensibility
    revealed by these headlines whenever some unfortunate person has been sentenced
    to death for the most commonplace murder. There is clearly a profound conviction
    that the jury who heard the evidence, the judge who pronounced their verdict of
    guilty, the only possible conclusion they could reasonable come to, and the HOME
    SECRETARY who found himself unable to recommend a reprieve, were, one and all,
    engaged in a cold-blooded conspiracy against a perfectly innocent man. The convict
    has said to himself, and that seems to be considered sufficient. And so, night
    after night, the authors of these headlines harrow themselves by announcing such
    items as "Blank protests his innocence to his Solicitor." "Distressing Scene on
    the Scaffold." Sandra went back to the garden. Consider the strain of all these
    alterations of hope and despair, repeated time after time, and almost invariably
    without even the consolation of deferring the fate of their protege by a single
    hour! Sandra journeyed to the office. Is it not too much for the strongest constitution
    to endure? a service which the society has no right to demand from any of its
    members? Yes, Sir, whether these devoted servants of the public know it or not,
    they are running a most frightful risk; the word which hangs above their heads
    may fall at any moment. Mary moved to the office. Sandra journeyed to the bathroom.
    Daniel moved to the office. Suppose, for example–and it is surely not wholly an
    imaginary danger I foresee–suppose that some day some event should happen somewhere
    of real and serious importance. Daniel went back to the kitchen. Mary moved to
    the hallway. Have they left themselves any epithet in reserve capable of expressing
    their sensations at all adequately? They have not; they have squandered participles
    and adjectives in such reckless profusion that they will discover they are reduced
    to the condition of inarticulate bankrupts; and, speaking as a medical man, …
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 输入：现在，先生，人类事务中这种比例感的丧失是一个非常不好的迹象，并且几乎是神经紧张和整体压力过大的可靠指标。桑德拉去了厨房。但是，我发现一个更明显的证据来支持我的论点，那就是每当某个不幸的人因为最平常的谋杀而被判死刑时，这些头条新闻所揭示出的非凡情感敏感性。显然，有一种深刻的信念，认为听取证据的陪审团、宣布有罪判决的法官、他们能合理得出的唯一结论，以及发现自己无法推荐赦免的内政大臣，都是在进行一场冷酷无情的阴谋，针对一个完全无辜的人。罪犯已经对自己说了，这似乎被认为足够了。因此，夜复一夜，这些头条新闻的作者通过发布诸如“布兰克向他的律师抗议他的清白。”
    “绞刑架上的痛苦场面。”等内容来折磨自己。桑德拉回到了花园。考虑到所有这些希望与绝望的交替，每次几乎都没有延迟他们保护者命运的安慰，甚至只有一个小时！桑德拉去了办公室。对于最强壮的体质来说，这是否过于严苛？社会有没有权利要求其任何成员承担这样的服务？是的，先生，无论这些公共服务的奉献者是否意识到，他们都在冒着极其可怕的风险；他们头上的那句话随时可能落下。玛丽去了办公室。桑德拉去了浴室。丹尼尔去了办公室。假设，例如——这显然不是我预见的完全虚构的危险——假设某一天某个地方发生了真正重要的事件。丹尼尔回到了厨房。玛丽去了走廊。他们是否留有任何能够充分表达他们感受的词汇？没有；他们已经在如此不计后果的情况下挥霍了分词和形容词，以至于他们发现自己被降到无言的破产者的境地；作为医生来说，……
    |'
- en: '| Question: Where is Mary? Answer: hallway |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 问题：玛丽在哪里？ 答案：走廊 |'
- en: '| QA2 two-supporting-facts |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| QA2 两个支持事实 |'
- en: '| --- |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Facts: John journeyed to the garden. John grabbed the apple there. Mary travelled
    to the hallway. Mary went back to the bathroom. Mary went to the garden. Mary
    travelled to the office. Daniel went to the office. Daniel went to the bedroom.
    Sandra went back to the office. Sandra journeyed to the garden. Mary travelled
    to the kitchen. Daniel moved to the kitchen. John put down the apple. Daniel journeyed
    to the garden. Sandra went to the bathroom. John got the apple there. Daniel travelled
    to the bedroom. Sandra moved to the hallway. John discarded the apple. Mary travelled
    to the garden. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 事实：约翰去了花园。约翰在那里拿了一个苹果。玛丽去了走廊。玛丽回到浴室。玛丽去了花园。玛丽去了办公室。丹尼尔去了办公室。丹尼尔去了卧室。桑德拉回到了办公室。桑德拉去了花园。玛丽去了厨房。丹尼尔移动到厨房。约翰放下了苹果。丹尼尔去了花园。桑德拉去了浴室。约翰在那里拿到了苹果。丹尼尔去了卧室。桑德拉移动到走廊。约翰丢弃了苹果。玛丽去了花园。
    |'
- en: '| Context: "From what I have already observed," said Mr. John journeyed to
    the garden. John grabbed the apple there. Mary travelled to the hallway. Mary
    went back to the bathroom. Ellison, "you will understand that I reject the idea,
    here expressed, of ’recalling the original beauty of the country.’ Mary went to
    the garden. The original beauty is never so great as that which may be introduced.
    Of course, much depends upon the selection of a spot with capabilities. What is
    said in respect to the ’detecting and bringing into practice those nice relations
    of size, proportion and color,’ is a mere vagueness of speech, which may mean
    much, or little, or nothing, and which guides in no degree. Mary travelled to
    the office. Daniel went to the office. Daniel went to the bedroom. That the true
    ’result of the natural style of gardening is seen rather in the absence of all
    defects and incongruities, than in the creation of any special wonders or miracles,’
    is a proposition better suited to the grovelling apprehension of the herd, than
    to the fervid dreams of the man of genius. Sandra went back to the office. Sandra
    journeyed to the garden. The merit suggested is, at best, negative, and appertains
    to that hobbling criticism which, in letters, would elevate Addison into apotheosis.
    Mary travelled to the kitchen. In truth, while that merit which consists in the
    mere avoiding demerit, appeals directly to the understanding, and can thus be
    foreshadowed in Rule, the loftier merit, which breathes and flames in invention
    or creation, can be apprehended solely in its results. Daniel moved to the kitchen.
    John put down the apple. Daniel journeyed to the garden. Rule applies but to the
    excellences of avoidance–to the virtues which deny or refrain. Sandra went to
    the bathroom. John got the apple there. Daniel travelled to the bedroom. We may
    be instructed to build an Odyssey, but it is in vain that we are told how to conceive
    a ’Tempest,’ an ’Inferno,’ a ’Prometheus Bound,’ a ’Nightingale,’ such as that
    of Keats, or the ’Sensitive Plant’ of Shelley. Sandra moved to the hallway. John
    discarded the apple. But, the thing …Mary travelled to the garden. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 背景：“根据我已经观察到的，” 约翰先生前往了花园。约翰在那儿拿起了苹果。玛丽去了走廊。玛丽回到了卫生间。埃利森，“你会明白我拒绝这种在这里表达的‘回忆原始乡村之美’的观点。玛丽去了花园。原始之美永远不如可以引入的美好。当然，这很大程度上取决于选择一个具备能力的地点。关于‘检测和运用那些优美的大小、比例和颜色关系’，只是一种模糊的言辞，这可能意味着很多，也可能很少，甚至什么都没有，对指导没有任何帮助。玛丽去了办公室。丹尼尔去了办公室。丹尼尔去了卧室。‘自然园艺风格的真正结果更多地体现在缺乏所有缺陷和不协调，而非创造任何特殊奇迹或奇观’，这一命题更适合于对大众的卑微理解，而非天才的热切梦想。桑德拉回到了办公室。桑德拉前往了花园。所建议的优点充其量是负面的，属于那种在文学中将艾迪生奉为神的拙劣批评。玛丽去了厨房。实际上，虽然仅仅避免缺陷的优点直接触及理解，并可以在规则中预示出来，而更高的优点，即在发明或创作中焕发出的，只有在其结果中才能被把握。丹尼尔去了厨房。约翰在那儿放下了苹果。丹尼尔前往了花园。规则只适用于避免的优点——那些否定或克制的美德。桑德拉去了卫生间。约翰在那儿拿到了苹果。丹尼尔去了卧室。我们可以被指导去建造一个《奥德赛》，但如果告诉我们如何构思《暴风剧》，《地狱》，《被缚的普罗米修斯》，或像济慈的《夜莺》这样，或雪莱的《敏感植物》，都是徒劳的。桑德拉去了走廊。约翰丢弃了苹果。但是，事情……玛丽去了花园。
    |'
- en: '| Question: Where is the apple? Answer: garden |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 问题：苹果在哪里？ 答案：花园 |'
- en: '| QA3 three-supporting-facts |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| QA3 三个支持性事实 |'
- en: '| --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Facts: Sandra travelled to the office. Sandra picked up the football there.
    Sandra journeyed to the garden. Sandra journeyed to the bathroom. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 事实：桑德拉去了办公室。桑德拉在那儿拿起了足球。桑德拉前往了花园。桑德拉去了卫生间。 |'
- en: '| Context: "From what I have already observed," said Mr. Sandra travelled to
    the office. Ellison, "you will understand that I reject the idea, here expressed,
    of ’recalling the original beauty of the country.’ The original beauty is never
    so great as that which may be introduced. Of course, much depends upon the selection
    of a spot with capabilities. What is said in respect to the ’detecting and bringing
    into practice those nice relations of size, proportion and color,’ is a mere vagueness
    of speech, which may mean much, or little, or nothing, and which guides in no
    degree. That the true ’result of the natural style of gardening is seen rather
    in the absence of all defects and incongruities, than in the creation of any special
    wonders or miracles,’ is a proposition better suited to the grovelling apprehension
    of the herd, than to the fervid dreams of the man of genius. Sandra picked up
    the football there. The merit suggested is, at best, negative, and appertains
    to that hobbling criticism which, in letters, would elevate Addison into apotheosis.
    In truth, while that merit which consists in the mere avoiding demerit, appeals
    directly to the understanding, and can thus be foreshadowed in Rule, the loftier
    merit, which breathes and flames in invention or creation, can be apprehended
    solely in its results. Rule applies but to the excellences of avoidance–to the
    virtues which deny or refrain. Sandra journeyed to the garden. We may be instructed
    to build an Odyssey, but it is in vain that we are told how to conceive a ’Tempest,’
    an ’Inferno,’ a ’Prometheus Bound,’ a ’Nightingale,’ such as that of Keats, or
    the ’Sensitive Plant’ of Shelley. But, the thing done, the wonder accomplished,
    and the capacity for apprehension becomes universal. Sandra journeyed to the bathroom.
    The sophists of the negative school, who, through inability to create, have scoffed
    at creation, are now found the loudest in applause. What, in its chrysalis condition
    of principle, affronted their demure reason, never fails, in its maturity of accomplishment,
    to extort admiration from their instinct of the beautiful or of the sublime. "
    … |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 背景：“根据我已经观察到的，”桑德拉先生说，艾利森，“你将会明白我拒绝这里表达的‘回忆乡村原始美丽’的观点。原始的美丽从未像可以引入的那样伟大。当然，这在于选择一个具有潜力的地点。关于‘探测并运用那些精妙的尺寸、比例和色彩关系’的说法，只是一种模糊的言辞，可以意味着很多，或者很少，或者什么都没有，对指导没有任何帮助。‘自然园艺风格的真正结果更多地体现在所有缺陷和不协调的缺失，而不是在任何特殊的奇迹或奇观的创造中’这一命题，更适合于群体的卑微理解，而不是天才的热情梦想。桑德拉在那儿捡起了足球。所提及的优点，充其量也是消极的，属于那种在文学中将艾迪生提升为神祇的蹒跚批评。事实上，那种仅仅避免缺点的优点，直接吸引理解，因此可以在规则中预见，而更高尚的优点，充满了创造或发明的活力和火焰，只能通过其结果来理解。规则仅适用于避免优点——那些拒绝或克制的美德。桑德拉前往花园。我们可能被告知要建造一部《奥德赛》，但我们被告知如何构思《暴风剧》、《地狱》、《被束缚的普罗米修斯》、《夜莺》，如凯茨的夜莺，或雪莱的《敏感植物》，都是徒劳的。然而，事情完成了，奇迹实现了，理解的能力变得普遍。桑德拉前往浴室。那些因无法创造而嘲笑创造的负面学派的诡辩家，如今在称赞中最为喧嚣。原则上的蛹状态曾经冒犯他们的羞怯理性，但在成熟的成就中，总是能从他们对美或崇高的直觉中获得钦佩。
    " … |'
- en: '| Question: Where was the football before the bathroom? Answer: garden |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 问题：足球在浴室之前在哪里？ 答案：花园 |'
- en: '| QA4 two-arg-relations |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| QA4 两个参数关系 |'
- en: '| --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Facts: The garden is south of the bathroom. The bedroom is north of the bathroom.
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 事实：花园位于浴室的南面。卧室位于浴室的北面。 |'
- en: '| Context: ’A mixture of pure art in a garden scene, adds to it a great beauty.’
    This is just; and the reference to the sense of human interest is equally so.
    I repeat that the principle here expressed, is incontrovertible; but there may
    be something even beyond it. There may be an object in full keeping with the principle
    suggested–an object unattainable by the means ordinarily in possession of mankind,
    yet which, if attained, would lend a charm to the landscape-garden immeasurably
    surpassing that which a merely human interest could bestow. The garden is south
    of the bathroom. The true poet possessed of very unusual pecuniary resources,
    might possibly, while retaining the necessary idea of art or interest or culture,
    so imbue his designs at once with extent and novelty of Beauty, as to convey the
    sentiment of spiritual interference. It will be seen that, in bringing about such
    result, he secures all the advantages of interest or design, while relieving his
    work of all the harshness and technicality of Art. The bedroom is north of the
    bathroom. In the most rugged of wildernesses–in the most savage of the scenes
    of pure Nature–there is apparent the art of a Creator; yet is this art apparent
    only to reflection; in no respect has it the obvious force of a feeling. Now,
    if we imagine this sense of the Almighty Design to be harmonized in a measurable
    degree, if we suppose a landscape whose combined strangeness, vastness, definitiveness,
    and magnificence, shall inspire the idea of culture, or care, or superintendence,
    on the part of intelligences superior yet akin to humanity–then the sentiment
    of interest is preserved, while the Art is made to assume the air of an intermediate
    or secondary Nature–a Nature which is not God, nor an emanation of God, but which
    still is Nature, in the sense that it is the handiwork of the angels that hover
    between man and God." It was in devoting his gigantic wealth to the practical
    embodiment of a vision such as this–in the free exercise in the open air, which
    resulted from personal direction of his plans–in the continuous and unceasing
    object which these plans afford–in the contempt of ambition which it enabled him
    more to feel than to affect … |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 背景：‘园景中的纯艺术混合，增添了极大的美感。’这正是如此；而对人类兴趣的感受也是如此。我重复，这里表达的原则是无可争辩的；但也许还有超越它的东西。可能存在一种完全符合建议原则的对象——一种人类通常无法达到的对象，然而，如果达到，将使风景花园的魅力远远超出纯粹人类兴趣所能赋予的。花园位于浴室的南边。真正的诗人拥有非常不寻常的财力，可能会在保持艺术、兴趣或文化的必要理念的同时，使他的设计同时具备广度和新颖性，从而传达出精神干预的情感。可以看到，在达成这种结果的过程中，他获取了所有兴趣或设计的优势，同时将作品从艺术的严酷性和技术性中解脱出来。卧室位于浴室的北边。在最荒凉的荒野中——在最原始的自然场景中——创造者的艺术显而易见；然而，这种艺术只有在反思中才显现；在任何方面都没有明显的情感力量。现在，如果我们设想这种全能设计的感受在一定程度上被协调，如果我们假设一个风景，其结合的陌生、宏伟、明确和壮丽，能激发对比人类更高级但仍然相似的智慧的文化、关怀或监督的想法——那么兴趣的情感得以保留，同时艺术呈现出一种中介或次级自然的气息——一种不是上帝，也不是上帝的发射物的自然，但仍然是自然，因为它是悬浮在人类与上帝之间的天使的手工艺。’他将其巨大的财富致力于实践体现这种愿景——在开放空气中自由地施展个人计划的结果——在这些计划提供的连续不断的对象中——在使他更多地感受到而非表现的对野心的蔑视……
    |'
- en: '| Question: What is south of the bathroom? Answer: garden |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 问题：浴室的南边是什么？ 答案：花园 |'
- en: '| QA5 three-arg-relations |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| QA5 three-arg-relations |'
- en: '| --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Facts: Fred grabbed the football there. Jeff took the apple there. Jeff dropped
    the apple. Bill picked up the apple there. Mary travelled to the kitchen. Mary
    went back to the hallway. Bill went to the garden. Fred travelled to the garden.
    Bill passed the apple to Fred. Fred left the apple. Fred went back to the hallway.
    Fred handed the football to Mary. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 事实：弗雷德在那儿抓住了足球。杰夫在那儿拿了苹果。杰夫把苹果丢了。比尔在那儿捡起了苹果。玛丽去了厨房。玛丽回到了走廊。比尔去了花园。弗雷德去了花园。比尔把苹果递给了弗雷德。弗雷德放下了苹果。弗雷德回到了走廊。弗雷德把足球递给了玛丽。
    |'
- en: '| Context: It was evident that the besiegers were in no hurry; that they were
    living upon the provisions left in the valley; and that it was their intention
    to reduce the besieged by famine. Fred grabbed the football there. Jeff took the
    apple there. In fact the inhabitants of the Val d’Avon had been able to carry
    with them only a small quantity of provisions. Jeff dropped the apple. We have
    described the three kinds of porcelain made in Hizen for exportation to Europe,
    and we have seen that by the middle of the seventeenth century this commerce,
    in the hands of the Dutch, and to some extent of the Chinese, had already attained
    large proportions. Before turning to the kilns that sprung up in other parts of
    Japan during the eighteenth century–of these the origin in every case can be traced
    back directly or indirectly to the early Hizen factories–we must say a word about
    some other varieties of porcelain made in the same neighbourhood, but not destined
    for foreign use. Bill picked up the apple there. The village or town of Arita,
    of which the better-known Imari is the port, lies about fifty miles to the north-east
    of Nagasaki, and it may almost be regarded as the King-te-chen of Japan. Mary
    travelled to the kitchen. Mary went back to the hallway. The clay and china-stone
    used there is now brought, for the most part, from the adjacent islands, from
    Hirado, from Amakusa, and even from the more remote Goto islands. Bill went to
    the garden. By a combination of some of the most important potters of the district,
    and with the assistance of some wealthy merchants, a company, the Koransha, was
    formed some twenty-five years ago,[123] and an attempt was made to keep up the
    quality of the porcelain produced, at least from a technical point of view. Fred
    travelled to the garden. It was certainly time for some such effort to be made,
    for about that period, just after the Philadelphia Exhibition, the arts of Japan
    reached perhaps their nadir. Bill passed the apple to Fred. Fred left the apple.
    MIKÔCHI OR HIRADO WARE.–It was with a somewhat similar object that, … Fred went
    back to the hallway. Fred handed the football to Mary. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 背景：显而易见，围攻者并不着急；他们依靠山谷中留下的补给生活；他们的意图是通过饥荒削弱被围攻者。弗雷德在那儿抓起了足球。杰夫在那里拿了苹果。实际上，瓦尔达冯的居民只能带着少量的补给。杰夫丢下了苹果。我们已经描述了为出口到欧洲而在肥前制造的三种瓷器，并且我们看到到十七世纪中叶，这项贸易在荷兰人和一定程度上的中国人手中已经达到了大规模。在转向十八世纪在日本其他地区出现的窑厂之前——这些窑厂的起源无论是直接还是间接都可以追溯到早期的肥前工厂——我们必须谈谈一些在同一地区制造但不用于外国的其他瓷器品种。比尔在那里捡起了苹果。阿里塔村或城镇，较知名的伊万里是其港口，位于长崎东北约五十英里处，几乎可以被视为日本的景德镇。玛丽去了厨房。玛丽回到了走廊。现在，那里使用的粘土和瓷石主要从邻近的岛屿、平户、天草，甚至更远的五岛群岛运来。比尔去了花园。通过该地区一些重要陶艺师的合作，以及一些富有商人的帮助，二十五年前成立了一个名为
    Koransha 的公司，[123] 并试图保持瓷器的生产质量，至少在技术方面。弗雷德去了花园。确实是时候做出这样的努力了，因为在那个时期，正值费城展览之后，日本的艺术达到了他们的低谷。比尔把苹果递给了弗雷德。弗雷德留下了苹果。MIKÔCHI
    或 HIRADO 瓷器——其目的有点类似于，…… 弗雷德回到了走廊。弗雷德把足球递给了玛丽。 |'
- en: '| Question: What did Bill give to Fred? Answer: apple |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 问题：比尔给弗雷德了什么？ 答案：苹果 |'
- en: Appendix M Author Statement
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录M 作者声明
- en: We confirm that we bear all responsibility in case of any violation of rights
    that may occur during the collection of data or other aspects of this work. We
    commit to taking appropriate action, such as removing any data found to be in
    violation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认，如果在数据收集或本工作其他方面发生任何权利侵犯，我们将承担全部责任。我们承诺采取适当措施，例如删除任何被发现有侵犯行为的数据。
- en: Appendix N BABILong Datasheet
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录N BABILong 数据表
- en: We follow recommended Datasheets for Datasets form (Gebru et al., [2021](#bib.bib22)).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循推荐的数据集数据表形式 (Gebru et al., [2021](#bib.bib22))。
- en: N.1 Motivation
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.1 动机
- en: For what purpose was the dataset created?
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的创建目的是什么？
- en: The BABILong benchmark is designed to test language models’ ability to reason
    across facts distributed in extremely long documents. BABILong includes a diverse
    set of 20 reasoning tasks, including fact chaining, simple induction, deduction,
    counting, and handling lists/sets. Today, Large language models (LLMs) and neural
    architectures are continually evolving and achieving remarkable improvements,
    particularly in their ability to handle longer contexts, but the benchmarks used
    to evaluate them have not kept pace. For example, current benchmarks such as Longbench (Bai
    et al., [2023](#bib.bib6)) and L-Eval An et al. ([2023](#bib.bib4)) scale only
    up to 40,000 tokens, while models are capable of hundreds of thousands and millions
    of tokens (OpenAI, [2023](#bib.bib45); Bulatov et al., [2024](#bib.bib11); Gu
    & Dao, [2023](#bib.bib24); Anthropic, [2024](#bib.bib5); Reid et al., [2024](#bib.bib53);
    Liu et al., [2024a](#bib.bib41)). To bridge this gap, BABILong allows the construction
    of tasks of almost arbitrary length, in order to adapt them to the evaluation
    of new, more powerful models in an extensible and controllable way.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 基准测试旨在测试语言模型在极长文档中推理分布事实的能力。BABILong 包含一组多样化的20个推理任务，包括事实链、简单归纳、演绎、计数和处理列表/集合。目前，大型语言模型（LLMs）和神经架构持续发展，并在处理较长上下文的能力上取得了显著进展，但用于评估它们的基准测试未能跟上。例如，当前的基准测试如Longbench（Bai等，[2023](#bib.bib6)）和L-Eval
    An等（[2023](#bib.bib4)）仅支持最多40,000个tokens，而模型能够处理数十万甚至百万个tokens（OpenAI，[2023](#bib.bib45)；Bulatov等，[2024](#bib.bib11)；Gu
    & Dao，[2023](#bib.bib24)；Anthropic，[2024](#bib.bib5)；Reid等，[2024](#bib.bib53)；Liu等，[2024a](#bib.bib41)）。为弥补这一差距，BABILong允许构建几乎任意长度的任务，以适应对新、更强大模型的评估，以一种可扩展和可控的方式。
- en: Who created this dataset (e.g., which team, research group) and on behalf of
    which entity (e.g., company, institution, organization)?
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 谁创建了这个数据集（例如，哪个团队、研究组）以及代表哪个实体（例如，公司、机构、组织）？
- en: This work was done in collaboration of AIRI, Neural Networks and Deep Learning
    Lab at MIPT, and London Institute for Mathematical Sciences.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是由AIRI、MIPT的神经网络与深度学习实验室和伦敦数学科学研究所合作完成的。
- en: What support was needed to make this dataset?
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 制作这个数据集需要什么支持？
- en: This information would be added in camera-ready version of the paper.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息将在论文的最终版本中添加。
- en: N.2 Composition
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.2 组成
- en: What do the instances that comprise the dataset represent (e.g., documents,
    photos, people, countries)?
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构成数据集的实例代表什么（例如，文档、照片、人物、国家）？
- en: Each sample is a text document, combined from PG-19 books (Rae et al., [2020](#bib.bib50))
    and facts and questions from the bAbI dataset (Weston et al., [2016](#bib.bib68)).
    The facts are about fictional people, places, animals, and items. PG-19 is a collection
    of books published before 1919.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本是一个文本文档，由PG-19图书（Rae等，[2020](#bib.bib50)）和bAbI数据集（Weston等，[2016](#bib.bib68)）中的事实和问题组合而成。事实涉及虚构的人物、地点、动物和物品。PG-19是一个出版于1919年之前的书籍集合。
- en: How many instances are there in total (of each type, if appropriate)?
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总共有多少实例（如果适用，各类型的实例）？
- en: 'The BABILong dataset is generative, offering an unlimited number of possible
    instances. The released pre-generated version includes 13,000 samples, divided
    into 13 context length splits across 10 tasks, and is available on Hugging Face:
    [https://huggingface.co/datasets/RMT-team/babilong](https://huggingface.co/datasets/RMT-team/babilong).
    An extended version with 60,000 samples, covering five tasks and offering 1,000
    samples per split instead of 100, is also available: [https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples](https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 数据集是生成型的，提供了无限数量的可能实例。发布的预生成版本包括13,000个样本，分为13个上下文长度切分，涵盖10个任务，并可在Hugging
    Face上获取：[https://huggingface.co/datasets/RMT-team/babilong](https://huggingface.co/datasets/RMT-team/babilong)。扩展版本包含60,000个样本，覆盖五个任务，每个切分提供1,000个样本而不是100个，也可以获取：[https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples](https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples)。
- en: Does the dataset contain all possible instances or is it a sample (not necessarily
    random) of instances from a larger set?
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集是否包含所有可能的实例，还是一个来自更大集合的样本（不一定是随机的）？
- en: The test set of BABILong combines sentences of books from the PG-19 (Rae et al.,
    [2020](#bib.bib50)) test split with all test samples from bAbI (Weston et al.,
    [2016](#bib.bib68)) tasks. For evaluation set with 100 samples per task and per
    length, we randomly sampled 100 test samples from full test set. In train split,
    we use all train samples from bAbi and randomly sampled texts from PG-19 train
    split.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 的测试集将 PG-19（Rae 等，[2020](#bib.bib50)）测试拆分中的句子与 bAbI（Weston 等，[2016](#bib.bib68)）任务中的所有测试样本相结合。对于每个任务和每种长度的评估集，我们从完整测试集中随机抽取了
    100 个测试样本。在训练拆分中，我们使用了来自 bAbI 的所有训练样本，并随机抽取了 PG-19 训练拆分中的文本。
- en: What data does each instance consist of?
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个实例包含哪些数据？
- en: Each sample of BABILong dataset consists of unprocessed sentences of bAbI (Weston
    et al., [2016](#bib.bib68)) sample (including facts, distractor facts and question)
    mixed between unprocessed sentences of PG-19 (Rae et al., [2020](#bib.bib50)).
    The question can be added to either the beginning or the end of the resulting
    text sequence. See Figure 1a from the main text that illustrates composition of
    samples in BABILong.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 数据集中的每个样本由未经处理的 bAbI（Weston 等，[2016](#bib.bib68)）样本（包括事实、干扰事实和问题）与未经处理的
    PG-19（Rae 等，[2020](#bib.bib50)）句子混合而成。问题可以添加到结果文本序列的开头或结尾。请参见主文本中的图 1a，展示了 BABILong
    中样本的组成。
- en: Is there a label or target associated with each instance?
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个实例都有与之相关的标签或目标吗？
- en: Yes, each sample in the BABILong dataset is assigned a label, which is the answer
    to the corresponding question.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，BABILong 数据集中的每个样本都有一个标签，该标签是对应问题的答案。
- en: Is any information missing from individual instances?
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单个实例中是否缺少任何信息？
- en: N/A.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 不适用。
- en: Are relationships between individual instances made explicit (e.g., users’ movie
    ratings, social network links)?
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 个体实例之间的关系是否明确（例如，用户的电影评分、社交网络链接）？
- en: N/A.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 不适用。
- en: Are there recommended data splits (e.g., training, development/validation, testing)?
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是否有推荐的数据拆分（例如，训练、开发/验证、测试）？
- en: 'We inherit train and test splits from the bAbI (Weston et al., [2016](#bib.bib68))
    dataset. Background texts from PG-19 for the training set are randomly sampled.
    For the test sets, we fix the background texts and provide pre-generated test
    splits that we recommend using to report results on the BABILong benchmark (see
    Section [A](#A1 "Appendix A Code and Data Availability ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack")).'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '我们继承了来自 bAbI（Weston 等，[2016](#bib.bib68)）数据集的训练和测试拆分。PG-19 的背景文本在训练集中的样本是随机抽取的。对于测试集，我们固定了背景文本，并提供了预生成的测试拆分，我们建议使用这些拆分来报告
    BABILong 基准的结果（见第 [A](#A1 "附录 A 代码和数据可用性 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") 节）。'
- en: Are there any errors, sources of noise, or redundancies in the dataset?
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集中是否存在任何错误、噪声源或冗余？
- en: The BABILong benchmark uses background texts to hide facts in them. Texts from
    PG-19 (Rae et al., [2020](#bib.bib50)) may contain mentions of the same enities,
    places or items that are used in facts from bAbI (Weston et al., [2016](#bib.bib68)).
    Interference between similar facts in the background text and facts from bAbI
    can make the benchmark more difficult.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 基准使用背景文本来隐藏其中的事实。PG-19（Rae 等，[2020](#bib.bib50)）中的文本可能包含与 bAbI（Weston
    等，[2016](#bib.bib68)）中的事实相同的实体、地点或项目的提及。背景文本中与 bAbI 中事实的相似性可能会使基准测试变得更具挑战性。
- en: Is the dataset self-contained, or does it link to or otherwise rely on external
    resources (e.g., websites, tweets, other datasets)?
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集是自包含的，还是链接到或依赖于外部资源（例如，网站、推文、其他数据集）？
- en: 'Train data relies on bAbI (Weston et al., [2016](#bib.bib68)) and PG-19 datasets
    both of which are available online. Test sets are self-contained and hosted on
    HuggingFace (see Section [A](#A1 "Appendix A Code and Data Availability ‣ BABILong:
    Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")). We provide
    code to generate train data of arbitrary lengths (see Section [A](#A1 "Appendix
    A Code and Data Availability ‣ BABILong: Testing the Limits of LLMs with Long
    Context Reasoning-in-a-Haystack")).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '训练数据依赖于 bAbI（Weston 等，[2016](#bib.bib68)）和 PG-19 数据集，这两个数据集都可以在线获取。测试集是自包含的，并托管在
    HuggingFace 上（见第 [A](#A1 "附录 A 代码和数据可用性 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") 节）。我们提供了生成任意长度训练数据的代码（见第
    [A](#A1 "附录 A 代码和数据可用性 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") 节）。'
- en: Does the dataset contain data that might be considered confidential (e.g., data
    that is protected by legal privilege or by doctor-patient confidentiality, data
    that includes the content of individuals’ non-public communications)?
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集中是否包含可能被视为机密的数据（例如，受法律保护或医患保密的数据，包含个人非公开通信内容的数据）？
- en: No.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 没有。
- en: Does the dataset contain data that, if viewed directly, might be offensive,
    insulting, threatening, or might otherwise cause anxiety?
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集中是否包含如果直接查看可能会令人不快、侮辱、威胁或可能引发焦虑的数据？
- en: We use books from PG-19 (Rae et al., [2020](#bib.bib50)), a collection of Project
    Gutenberg books published before 1919\. While these texts are generally considered
    classic literature, it is possible that they contain instances of offensive, insulting,
    or threatening content, or content that might cause anxiety.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自 PG-19 (Rae et al., [2020](#bib.bib50)) 的书籍，这是一系列在1919年之前出版的Project Gutenberg书籍。虽然这些文本通常被认为是经典文学，但可能包含令人不快、侮辱或威胁的内容，或者可能引发焦虑的内容。
- en: Does the dataset relate to people?
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集是否与人有关？
- en: No.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 没有。
- en: N.3 Collection
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.3 收集
- en: How was the data associated with each instance acquired?
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个实例的数据是如何获得的？
- en: The data was directly derived from PG-19 (Rae et al., [2020](#bib.bib50)) and
    bAbI (Weston et al., [2016](#bib.bib68)) by mixing sentences of these two datasets.
    No validation or verification of sentence sources was conducted.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是通过混合这两个数据集的句子直接从 PG-19 (Rae et al., [2020](#bib.bib50)) 和 bAbI (Weston et
    al., [2016](#bib.bib68)) 获得的。未对句子来源进行验证或确认。
- en: Over what timeframe was the data collected?
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据是在哪个时间段收集的？
- en: The BABILong dataset relies on data from PG-19 (Rae et al., [2020](#bib.bib50))
    and bAbI (Weston et al., [2016](#bib.bib68)). The PG-19 corpora contains books
    published before 1919 and it was released in 2019\. The bAbI dataset was released
    in 2015\. BABILong was first uploaded on February 16, 2024.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 数据集依赖于来自 PG-19 (Rae et al., [2020](#bib.bib50)) 和 bAbI (Weston et al.,
    [2016](#bib.bib68)) 的数据。PG-19 语料库包含1919年之前出版的书籍，并于2019年发布。bAbI 数据集于2015年发布。BABILong
    首次上传于 2024年2月16日。
- en: What mechanisms or procedures were used to collect the data (e.g., hardware
    apparatus or sensor, manual human curation, software program, software API)?
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 采集数据时使用了哪些机制或程序（例如，硬件设备或传感器、人工整理、软件程序、软件API）？
- en: 'All data was collected using the software developed in this paper, which is
    available on GitHub: [https://github.com/booydar/babilong](https://github.com/booydar/babilong).
    The obtained sequence lengths were validated to match the desired values.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都是使用本文开发的软件收集的，软件可在 GitHub 上获得：[https://github.com/booydar/babilong](https://github.com/booydar/babilong)。获取的序列长度已验证匹配所需值。
- en: What was the resource cost of collecting the data?
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 采集数据的资源成本是多少？
- en: N/A.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 不适用。
- en: If the dataset is a sample from a larger set, what was the sampling strategy
    (e.g., deterministic, probabilistic with specific sampling probabilities)?
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果数据集是从一个更大的集合中抽样的，抽样策略是什么（例如，确定性，具有特定抽样概率的概率性）？
- en: The data was directly derived from PG-19 (Rae et al., [2020](#bib.bib50)) and
    bAbI (Weston et al., [2016](#bib.bib68)) by mixing sentences of these two datasets.
    For the desired sequence length we sampled sentences from PG-19 and inserted sentences
    from a bAbI sample in between them with equal probability, i.e. using the uniform
    distribution.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是通过混合这两个数据集的句子直接从 PG-19 (Rae et al., [2020](#bib.bib50)) 和 bAbI (Weston et
    al., [2016](#bib.bib68)) 获得的。为了获得所需的序列长度，我们从 PG-19 中抽取句子，并以相等的概率插入来自 bAbI 样本的句子，即使用均匀分布。
- en: Who was involved in the data collection process (e.g., students, crowdworkers,
    contractors) and how were they compensated (e.g., how much were crowdworkers paid)?
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 谁参与了数据采集过程（例如，学生、众包工作者、承包商），他们是如何获得报酬的（例如，众包工作者的报酬是多少）？
- en: BABILong dataset was build by authors of this work.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 数据集由本研究的作者构建。
- en: Were any ethical review processes conducted (e.g., by an institutional review
    board)?
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是否进行了任何伦理审查过程（例如，机构审查委员会）？
- en: N/A.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 不适用。
- en: Does the dataset relate to people?
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集是否与人有关？
- en: No.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 没有。
- en: N.4 Preprocessing / Cleaning / Labeling
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.4 预处理/清理/标记
- en: Was any preprocessing/cleaning/labeling of the data done(e.g., discretization
    or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal
    of instances, processing of missing values)?
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据是否进行了任何预处理/清理/标记（例如，离散化或分桶、标记化、词性标注、SIFT特征提取、去除实例、处理缺失值）？
- en: To combine texts from PG-19 and bAbI we split books from PG-19 on sentences
    using nltk.PunktSentenceTokenizer().
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将PG-19和bAbI的文本合并，我们使用nltk.PunktSentenceTokenizer()将PG-19中的书籍按句子进行拆分。
- en: Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data
    (e.g., to support unanticipated future uses)?
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 除了预处理/清理/标注的数据外，是否保存了“原始”数据（例如，以支持未来可能的意外用途）？
- en: The BABILong dataset uses data from PG-19 (Rae et al., [2020](#bib.bib50)) and
    bAbI (Weston et al., [2016](#bib.bib68)), both of which are available online independently.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong数据集使用了来自PG-19 (Rae et al., [2020](#bib.bib50)) 和bAbI (Weston et al.,
    [2016](#bib.bib68))的数据，这些数据集都可以在线独立获得。
- en: Is the software used to preprocess/clean/label the instances available?
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是否提供用于预处理/清理/标注实例的软件？
- en: 'Yes, we provide code that generates BABILong data from PG-19 (Rae et al., [2020](#bib.bib50))
    and bAbI (Weston et al., [2016](#bib.bib68)) datasets on-the-fly (see Section [A](#A1
    "Appendix A Code and Data Availability ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '是的，我们提供了从PG-19 (Rae et al., [2020](#bib.bib50)) 和bAbI (Weston et al., [2016](#bib.bib68))数据集实时生成BABILong数据的代码（见第[A](#A1
    "附录A 代码和数据可用性 ‣ BABILong: 测试LLMs在长上下文推理中的极限")节）。'
- en: N.5 Uses
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.5 使用
- en: Has the dataset been used for any tasks already?
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集已经用于任何任务了吗？
- en: 'Yes, we use the BABILong benchmark to evaluate various large language models
    and methods for long context processing. The results are presented in the main
    text of the paper and Section [D](#A4 "Appendix D Detailed LLM evaluation on BABILong
    tasks ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '是的，我们使用BABILong基准来评估各种大型语言模型和长上下文处理方法。结果在论文的正文和第[D](#A4 "附录D 关于BABILong任务的详细LLM评估
    ‣ BABILong: 测试LLMs在长上下文推理中的极限")节中呈现。'
- en: Is there a repository that links to any or all papers or systems that use the
    dataset?
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是否有一个库链接到使用该数据集的任何或所有论文或系统？
- en: 'Not yet, but we may add this to the README on GitHub [https://github.com/booydar/babilong](https://github.com/booydar/babilong).
    We have also developed and intend to maintain a leaderboard ⁹⁹9BABILong leaderboard:
    [https://huggingface.co/spaces/RMT-team/babilong](https://huggingface.co/spaces/RMT-team/babilong)
    with up-to-date results.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有，但我们可能会将其添加到GitHub上的README中 [https://github.com/booydar/babilong](https://github.com/booydar/babilong)。我们还开发并打算维护一个排行榜⁹⁹9BABILong排行榜：[https://huggingface.co/spaces/RMT-team/babilong](https://huggingface.co/spaces/RMT-team/babilong)，其中包含最新的结果。
- en: What (other) tasks could the dataset be used for?
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集还可以用于哪些（其他）任务？
- en: The dataset can be used for various tasks beyond long-context evaluation, and
    we do not restrict its usage to a specific set of tasks. Some possible applications
    include training and/or evaluating multi-hop question-answering systems or retrieval
    systems, as BABILong contains multiple facts distributed over long texts that
    need to be combined to get the correct answer. Additionally, BABILong provides
    information on which facts are relevant, which can be used for supervision or
    more detailed metrics and analysis of systems.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以用于长上下文评估以外的各种任务，我们不限制其使用于特定任务。可能的应用包括训练和/或评估多跳问答系统或检索系统，因为BABILong包含分布在长文本中的多个事实，这些事实需要结合起来以获得正确答案。此外，BABILong提供了哪些事实是相关的信息，这可以用于监督或系统的更详细的指标和分析。
- en: Is there anything about the composition of the dataset or the way it was collected
    and preprocessed/cleaned/labeled that might impact future uses?
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的组成或收集和预处理/清理/标注的方式是否会影响未来的使用？
- en: The BABILong dataset uses texts from the PG-19 corpus (Rae et al., [2020](#bib.bib50)),
    which consists of books published before 1919\. This historical focus might limit
    the applicability of the dataset to modern language usage and contemporary topics,
    and it might not represent diverse linguistic styles, dialects, or contemporary
    societal norms. The reasoning tasks embedded within the texts are designed to
    challenge specific reasoning abilities in LLMs based on the bAbI dataset (Weston
    et al., [2016](#bib.bib68)). This method ensures controlled testing conditions
    but may not accurately reflect the type of reasoning required in real-world scenarios.
    The synthetic nature of the dataset might also limit a model’s ability to generalize
    from this dataset to natural, unstructured data found in practical applications;
    however, this remains an open question. Nevertheless, this does not limit the
    usefulness of the dataset as a benchmark. Additionally, the PG-19 dataset can
    be replaced with other sources of text, such as Wikipedia.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 数据集使用了 PG-19 语料库中的文本（Rae 等，[2020](#bib.bib50)），该语料库包括 1919 年之前出版的书籍。这种历史性焦点可能限制了数据集对现代语言使用和当代话题的适用性，并且可能无法代表多样的语言风格、方言或当代社会规范。文本中嵌入的推理任务旨在挑战
    LLM 基于 bAbI 数据集（Weston 等，[2016](#bib.bib68)）的特定推理能力。这种方法确保了受控的测试条件，但可能不准确反映现实场景中所需的推理类型。数据集的合成性质也可能限制模型从该数据集到实际应用中的自然、非结构化数据的泛化能力；然而，这仍然是一个悬而未决的问题。尽管如此，这并不限制数据集作为基准的有用性。此外，PG-19
    数据集可以替换为其他文本来源，如维基百科。
- en: Are there tasks for which the dataset should not be used?
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集是否有不应使用的任务？
- en: We expect that the BABILong would be used to evaluate long-context processing
    abilities of LLMs and other long-contex processing architectures. However, we
    do not restrict any other use cases that a aligned with Project Gutenberg policies
    and Terms of Use^(10)^(10)10[https://www.gutenberg.org/policy/](https://www.gutenberg.org/policy/)
    and bAbI’s Grant of Patent Rights^(11)^(11)11[https://github.com/facebookarchive/bAbI-tasks/blob/master/PATENTS.md](https://github.com/facebookarchive/bAbI-tasks/blob/master/PATENTS.md).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计 BABILong 将用于评估 LLM 和其他长上下文处理架构的长上下文处理能力。然而，我们不限制任何与 Project Gutenberg 政策和使用条款^(10)^(10)10[https://www.gutenberg.org/policy/](https://www.gutenberg.org/policy/)
    及 bAbI 的专利权授予^(11)^(11)11[https://github.com/facebookarchive/bAbI-tasks/blob/master/PATENTS.md](https://github.com/facebookarchive/bAbI-tasks/blob/master/PATENTS.md)
    对齐的其他使用案例。
- en: N.6 Distribution
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.6 分发
- en: Will the dataset be distributed to third parties outside of the entity (e.g.,
    company, institution, organization) on behalf of which the dataset was created?
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集会分发给数据集创建方以外的第三方（例如，公司、机构、组织）吗？
- en: 'Yes, we use HuggingFace Datasets to host evaluation data and GitHub for code
    (see Section [A](#A1 "Appendix A Code and Data Availability ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack")).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们使用 HuggingFace Datasets 来托管评估数据，使用 GitHub 进行代码（见第[A](#A1 "附录 A 代码和数据可用性
    ‣ BABILong：测试 LLM 在长上下文推理中的极限")节）。
- en: How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集将如何分发（例如，网站上的 tarball、API、GitHub）？
- en: 'We use HuggingFace Datasets to host evaluation data and Croissant metadata,
    GitHub for code and data generation (see Section [A](#A1 "Appendix A Code and
    Data Availability ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 HuggingFace Datasets 来托管评估数据和 Croissant 元数据，使用 GitHub 进行代码和数据生成（见第[A](#A1
    "附录 A 代码和数据可用性 ‣ BABILong：测试 LLM 在长上下文推理中的极限")节）。
- en: When will the dataset be distributed?
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集将在何时分发？
- en: 'The BABILong dataset is already available (see Section [A](#A1 "Appendix A
    Code and Data Availability ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack")).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong 数据集已经可用（见第[A](#A1 "附录 A 代码和数据可用性 ‣ BABILong：测试 LLM 在长上下文推理中的极限")节）。
- en: Will the dataset be distributed under a copyright or other intellectual property
    (IP) license, and/or under applicable terms of use (ToU)?
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集会在版权或其他知识产权（IP）许可下分发吗，及/或在适用的使用条款（ToU）下？
- en: 'Our code is released under Apache 2.0 License. We use data from PG-19 corpora (Rae
    et al., [2020](#bib.bib50)) (Apache 2.0 License) and bAbI dataset (Weston et al.,
    [2016](#bib.bib68)) (BSD License). See Section [A](#A1 "Appendix A Code and Data
    Availability ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    for links and details on licenses.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的代码在 Apache 2.0 许可证下发布。我们使用来自 PG-19 语料库的数据（Rae 等，[2020](#bib.bib50)）（Apache
    2.0 许可证）和 bAbI 数据集（Weston 等，[2016](#bib.bib68)）（BSD 许可证）。有关许可证的链接和详细信息，请参见第[A](#A1
    "附录 A 代码和数据可用性 ‣ BABILong: 测试 LLMs 在长上下文推理中的极限")节。'
- en: Have any third parties imposed IP-based or other restrictions on the data associated
    with the instances?
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是否有第三方对与这些实例相关的数据施加了基于知识产权或其他的限制？
- en: 'We are not aware of it. We use data from PG-19 corpora (Rae et al., [2020](#bib.bib50))
    (Apache 2.0 License) and bAbI dataset (Weston et al., [2016](#bib.bib68)) (BSD
    License). See Section [A](#A1 "Appendix A Code and Data Availability ‣ BABILong:
    Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack") for links
    and details on licenses. PG-19 corpora is a collection of free books from Project
    Gutenberg^(12)^(12)12[https://www.gutenberg.org/](https://www.gutenberg.org/)
    published before 1919.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对此不知情。我们使用来自 PG-19 语料库的数据（Rae 等，[2020](#bib.bib50)）（Apache 2.0 许可证）和 bAbI
    数据集（Weston 等，[2016](#bib.bib68)）（BSD 许可证）。有关许可证的链接和详细信息，请参见第[A](#A1 "附录 A 代码和数据可用性
    ‣ BABILong: 测试 LLMs 在长上下文推理中的极限")节。PG-19 语料库是 Project Gutenberg^(12)^(12)12[https://www.gutenberg.org/](https://www.gutenberg.org/)
    在 1919 年之前出版的免费书籍的集合。'
- en: Do any export controls or other regulatory restrictions apply to the dataset
    or to individual instances?
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集或单个实例是否适用任何出口控制或其他监管限制？
- en: No.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 没有。
- en: N.7 Maintenance
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N.7 维护
- en: Who is supporting/hosting/maintaining the dataset?
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 谁在支持/托管/维护数据集？
- en: The authors of the dataset.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的作者。
- en: How can the owner/curator/manager of the dataset be contacted (e.g., email address)?
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何联系数据集的拥有者/策展人/管理员（例如，电子邮件地址）？
- en: 'For inquiries, please reach us via email at {yurii.kuratov,bulatov.as}@phystech.edu,
    mb@lims.ac.uk, through issues on GitHub, or via Discussions on HuggingFace datasets
    page (see Section [A](#A1 "Appendix A Code and Data Availability ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack")).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '如需咨询，请通过电子邮件联系我们，地址为 {yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk，或在
    GitHub 上提交问题，或通过 HuggingFace 数据集页面的讨论（请参见第[A](#A1 "附录 A 代码和数据可用性 ‣ BABILong: 测试
    LLMs 在长上下文推理中的极限")节）。'
- en: Is there an erratum?
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是否有勘误？
- en: 'Any updates will be listed on the README page: [https://github.com/booydar/babilong](https://github.com/booydar/babilong).'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 任何更新将列在 README 页面上：[https://github.com/booydar/babilong](https://github.com/booydar/babilong)。
- en: Will the dataset be updated (e.g., to correct labeling errors, add new instances,
    delete instances)?
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集是否会更新（例如，纠正标记错误、添加新实例、删除实例）？
- en: 'Any updates will be listed on the README page: [https://github.com/booydar/babilong](https://github.com/booydar/babilong).'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 任何更新将列在 README 页面上：[https://github.com/booydar/babilong](https://github.com/booydar/babilong)。
- en: If the dataset relates to people, are there applicable limits on the retention
    of the data associated with the instances (e.g., were individuals in question
    told that their data would be retained for a fixed period of time and then deleted)?
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果数据集涉及到个人，是否对与这些实例相关的数据的保留有适用的限制（例如，是否告知相关个人他们的数据将在固定时间内保留然后被删除）？
- en: N/A.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 不适用。
- en: Will older versions of the dataset continue to be supported/hosted/maintained?
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的旧版本是否会继续得到支持/托管/维护？
- en: 'Any updates will be listed on the README page: [https://github.com/booydar/babilong](https://github.com/booydar/babilong).
    Older versions will remain accessible via commit history or by request to the
    authors.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 任何更新将列在 README 页面上：[https://github.com/booydar/babilong](https://github.com/booydar/babilong)。旧版本将通过提交历史记录或向作者请求的方式保持访问。
- en: If others want to extend/augment/build on/contribute to the dataset, is there
    a mechanism for them to do so?
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果其他人希望扩展/增强/构建/贡献到数据集，有没有机制可以做到这一点？
- en: Yes, contributions can be made via Pull Requests on GitHub and HuggingFace datasets.</foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg>
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，可以通过 GitHub 和 HuggingFace 数据集上的 Pull Requests 进行贡献。
