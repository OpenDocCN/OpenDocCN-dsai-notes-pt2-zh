- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:56:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13282](https://ar5iv.labs.arxiv.org/html/2406.13282)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meizhi Zhong    Chen Zhang    Yikun Lei    Xikai Liu    Yan Gao    Yao Hu   
    Kehai Chen    Min Zhang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Enabling LLMs to handle lengthy context is currently a research hotspot. Most
    LLMs are built upon rotary position embedding (RoPE), a popular position encoding
    method. Therefore, a prominent path is to extrapolate the RoPE trained on comparably
    short texts to far longer texts. A heavy bunch of efforts have been dedicated
    to boosting the extrapolation via extending the formulations of the RoPE, however,
    few of them have attempted to showcase their inner workings comprehensively. In
    this paper, we are driven to offer a straightforward yet in-depth understanding
    of RoPE extensions from an attention perspective and on two benchmarking tasks.
    A broad array of experiments reveals several valuable findings: 1) Maintaining
    attention patterns to those at the pretrained length improves extrapolation; 2)
    Large attention uncertainty leads to retrieval errors; 3) Using longer continual
    pretraining lengths for RoPE extensions could reduce attention uncertainty and
    significantly enhance extrapolation.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, Long Context, Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) (Radford et al., [2018](#bib.bib18); Touvron et al.,
    [2023](#bib.bib22); Zhang et al., [2023](#bib.bib24); Brown et al., [2020](#bib.bib4))
    have accommodated a wide range of natural language processing applications, such
    as code completion (Rozière et al., [2023](#bib.bib19)) and question answering
    (Kamalloo et al., [2023](#bib.bib12); Jiang et al., [2021](#bib.bib10); Su et al.,
    [2019](#bib.bib20)). However, a notable challenge limiting further customization
    is possibly the inability of LLMs to utilize context beyond the pretrained length
    (Minaee et al., [2024](#bib.bib16); Chen et al., [2023b](#bib.bib6)) due to the
    inherent flaw of rotary position embedding (RoPE) being used. Fortunately, RoPE
    extensions emerge as key ingredients to enabling LLMs to leverage extended context
    that exceeds pretrained scope (Chen et al., [2023b](#bib.bib6); Peng et al., [2023](#bib.bib17);
    Liu et al., [2023](#bib.bib14); Han et al., [2023](#bib.bib8); Rozière et al.,
    [2023](#bib.bib19)). These RoPE extensions focus on improving performance on long
    texts, yet frustratingly, only a few of them (Liu et al., [2023](#bib.bib14);
    Han et al., [2023](#bib.bib8); Men et al., [2024](#bib.bib15)) have delved into
    the underlying mechanisms in a complicated way.
  prefs: []
  type: TYPE_NORMAL
- en: This demands us to systematically analyze common RoPE extensions more straightforwardly,
    from the perspective of attention (Vaswani et al., [2017](#bib.bib23)). We include
    three widely-used RoPE extensions, i.e., position interpolation (Chen et al.,
    [2023b](#bib.bib6)), YaRN (Peng et al., [2023](#bib.bib17)), and NTK-Aware interpolation (Rozière
    et al., [2023](#bib.bib19)). To our best knowledge, there is simply no research
    in understanding RoPE extensions for long-context models thoroughly from an attention
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: As a start, we strive to primarily study these methods on a long-context perplexity
    test (PPL), and empirically compare their corresponding attention patterns. We
    discover that finetuning LLMs with these RoPE-extension methods accordingly in
    the pretrained length principally lifts their extrapolation performance. Particularly
    with the NTK-Aware interpolation method, one can extrapolate up to 32$\times$
    beyond the pretrained length. To unleash the reasons behind the successes of these
    methods, we collect the attention scores respectively distributed in 2K and 8K
    lengths during inference. The results demonstrate that these methods maintain
    attention patterns consistent with those observed at the pretrained length. In
    contrast, the attention patterns of the RoPE are substantially deviated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, following literature (Hu et al., [2024](#bib.bib9); Fu et al., [2024](#bib.bib7)),
    we examine these RoPE extensions on a more challenging long-context test called
    Needle-in-a-Haystack (Needle) (Kamradt, [2023](#bib.bib13)). We find that the
    RoPE extensions could pass more tests than the RoPE does. Nonetheless, as the
    context length increased, the RoPE extensions could hardly locate the needles.
    We associate the observation with attention uncertainty, specifically in the form
    of attention entropy. We uncover that large uncertainty leads to retrieval errors:
    the positions that incur large attention entropy are exactly where the incorrect
    answers are borrowed from.'
  prefs: []
  type: TYPE_NORMAL
- en: We further hypothesize that this large attention uncertainty stems from a mismatch
    between the context lengths in training and inference. Inspired by the conjecture,
    a natural way to ease the mismatch is to directly train on longer texts. Experimental
    results exhibit that, with the same amount of training tokens consumed, using
    examples with longer contexts largely alleviates uncertainty. Thereby, the ability
    to digest long texts is promoted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our key contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We study various RoPE extensions for length extrapolation in perplexity testing
    and find that the effectiveness could be yielded from maintaining the original
    attention patterns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyze these methods using advanced Needle testing and observe that they
    may fail to extrapolate to regions where large attention uncertainty persists.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We hypothesize that large attention uncertainty stems from a context length
    mismatch between training and inference. It is possible to reduce this large uncertainty
    by minimizing the mismatch through continual training with lengths closer to those
    in inference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Backgroud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Target LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We consider LLaMa series at different sizes to conduct experiments, including
    MiniMA-2-3B (Zhang et al., [2023](#bib.bib24)), LLaMa-2-7B, and LLaMa-2-13B (Touvron
    et al., [2023](#bib.bib22)). All these mentioned LLMs consistently use rotary
    position embeddings to take position information into consideration. Owing to
    space limitation, we only present the experimental results for LLaMa-2-7B, and
    the results for MiniMA-2-3B and LLaMa-2-13B, share similar trends with those for
    LLaMa-2-7B, as shown in Appendix [A](#A1 "Appendix A Experimental Results on MiniMA-2-3B
    ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective")
    and [B](#A2 "Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 RoPE and Its Extensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rotary Position Embedding (RoPE).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into RoPE extensions, we first briefly describe RoPE itself.
    The use of RoPE (Su et al., [2021](#bib.bib21)) has become pervasive in contemporary
    LLMs (Touvron et al., [2023](#bib.bib22); Bai et al., [2023](#bib.bib2); Bi et al.,
    [2024](#bib.bib3)). RoPE encodes the position information of tokens with a rotation
    tensor that naturally incorporates explicit relative position dependency. To illustrate,
    given a hidden vector ${\bm{h}}=[h_{0},h_{1},...,h_{d-1}]$, RoPE operates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  $$f({\bm{h}},m)=\begin{pmatrix}h_{0}\\ h_{1}\\'
  prefs: []
  type: TYPE_NORMAL
- en: h_{2}\\
  prefs: []
  type: TYPE_NORMAL
- en: h_{3}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: h_{d-2}\\
  prefs: []
  type: TYPE_NORMAL
- en: h_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{0}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{0}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{d/2-1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{d/2-1}}\end{pmatrix}+\begin{pmatrix}-h_{1}\\
  prefs: []
  type: TYPE_NORMAL
- en: h_{0}\\
  prefs: []
  type: TYPE_NORMAL
- en: -h_{3}\\
  prefs: []
  type: TYPE_NORMAL
- en: h_{2}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: -h_{d-1}\\
  prefs: []
  type: TYPE_NORMAL
- en: h_{d-2}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{0}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{0}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{d/2-1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{d/2-1}}\end{pmatrix}$$  |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\theta_{j}=b^{-2j/d},j\in\{0,1,...,d/2-1\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Position Interpolation (PI). As described in Chen et al. ([2023a](#bib.bib5))
    and Kaiokendev ([2023](#bib.bib11)), PI involves proportionally downscaling the
    position index $m$ in Equation [1](#S2.E1 "Equation 1 ‣ 2.2 RoPE and Its Extensions
    ‣ 2 Backgroud ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective").'
  prefs: []
  type: TYPE_NORMAL
- en: NTK-Aware Interpolation (NTK). NTK (Rozière et al., [2023](#bib.bib19)) assumes
    that interpolating all dimensions equally, as done by PI, may result in the loss
    of high-frequency information. Therefore, NTK introduces a nonlinear interpolation
    strategy by adjusting the base frequency $b$.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another RoPE extensioN (YaRN). Unlike PI and NTK, which treat each dimension
    of RoPE uniformly, YaRN (Peng et al., [2023](#bib.bib17)) employs a ramp function
    to combine PI and NTK at varying proportions across different dimensions. Additionally,
    it introduces a temperature factor to mitigate the distribution shift of the attention
    caused by long inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Following the default settings of the original papers (Chen et al., [2023b](#bib.bib6);
    Peng et al., [2023](#bib.bib17); Liu et al., [2023](#bib.bib14)), we adjust $\alpha$
    from 10,000 to 1,000,000 for NTK in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Long-Context Evaluations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following existing works (Chen et al., [2023b](#bib.bib6); Peng et al., [2023](#bib.bib17);
    Fu et al., [2024](#bib.bib7)), we use the perplexity test (dubbed PPL) as the
    primary evaluation and the Needle-in-a-Haystack test as a more challenging evaluation.
    The perplexity is a primary measure that reflects a model’s ability to handle
    long texts. The Needle-in-a-Haystack test (dubbed Needle) (Kamradt, [2023](#bib.bib13))
    requires LLMs to accurately recall a specific sentence (the Needle) embedded at
    an arbitrary location within a long document (the haystack). We obtain the perplexity
    using the Proof-pile (Azerbayev et al., [2022](#bib.bib1)) dataset. We follow
    the standard described in Fu et al. ([2024](#bib.bib7)) for the Needle-in-a-Haystack
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3 RoPE Extensions on PPL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We preliminarily study the RoPE extensions from a primary long-context perplexity
    test. From the test, as illustrated in Figure [1](#S3.F1 "Figure 1 ‣ 3 RoPE Extensions
    on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective"), we identify that NTK can extrapolate from 4K to 128K, whereas PI
    and YaRN can extrapolate to 62K. We observe similar results in both the smaller
    model MiniMA-2-3B and the larger model LLaMa-2-13B, as illustrated in Figures
    [4](#A1.F4 "Figure 4 ‣ Appendix A Experimental Results on MiniMA-2-3B ‣ Understanding
    the RoPE Extensions of Long-Context LLMs: An Attention Perspective") and [7](#A2.F7
    "Figure 7 ‣ Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective"). To recognize
    why these RoPE extensions enable train-short-and-test-long properties in PPL,
    we collect the attention scores on 10 sequences in 2K and 8K and visualize their
    attention distributions. The followings are a few key takeaways from the attention
    perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dea263eb01a934713c49c20988d9c125.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Perplexity on Proof-pile dataset (Lower is better).'
  prefs: []
  type: TYPE_NORMAL
- en: RoPE extensions maintain the original attention patterns.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Figure [2](#S3.F2 "Figure 2 ‣ RoPE extensions maintain the original
    attention patterns. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions
    of Long-Context LLMs: An Attention Perspective"), similar to the findings from
    Chen et al. ([2023b](#bib.bib6)), we acknowledge that the attention patterns fluctuate
    when the RoPE is tested on 8K sequences (exceeding the training length). However,
    with RoPE extensions, the attention distributions, as illustrated in Figures [2](#S3.F2
    "Figure 2 ‣ RoPE extensions maintain the original attention patterns. ‣ 3 RoPE
    Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An
    Attention Perspective")(c-e), revert to the original pattern seen in Figure [2(a)](#S3.F2.sf1
    "Figure 2(a) ‣ Figure 2 ‣ RoPE extensions maintain the original attention patterns.
    ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective") when tested on 8K sequences. Similar observations
    are seen in both LLaMa-2-13B and MiniMA-2-3B, as illustrated in Figures [5](#A1.F5
    "Figure 5 ‣ Appendix A Experimental Results on MiniMA-2-3B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective") and [8](#A2.F8
    "Figure 8 ‣ Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Jensen–Shannon (JS) divergence of mean attention distributions between
    different models at lengths of 2048 (top row) and 8192 (bottom row). A lower JS
    divergence indicates that the two attention distributions are similar.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PI | YaRN | NTK | RoPE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMa-2 | 1.29 | 0.05 | 0.06 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMa-3 | 2.29 | 1.72 | 1.68 | 2.57 | ![Refer to caption](img/2d972eb579c40207d44ef0e4b10b787d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) RoPE on 2K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bff7d1516c73c51979e87e19d4539920.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) RoPE on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98842027febf301b4c470d55afbefd90.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) PI on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/44cd668320a03c862c6c182bed6afdb2.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) YaRN on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bf5ecaf5ab4ec9c3f05f1c804b91952.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) NTK on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Attention distributions of RoPE, PI, YaRN, and NTK methods on 2K
    and 8K sequences. The red line represents the mean attention scores across all
    heads, layers, and examples. The other lines indicate the attention scores for
    each head in each layer of the examples.'
  prefs: []
  type: TYPE_NORMAL
- en: RoPE extensions closely resemble the attention patterns of models trained on
    longer cotnext.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To further verify whether RoPE extensions maintain the original attention patterns,
    we aim to directly quantify the Jensen–Shannon (JS) divergence between different
    attention distributions. We choose LLaMa-2 and LLaMa-3 as bases. As illustrated
    in the bottom row of Table [1](#S3.T1 "Table 1 ‣ RoPE extensions maintain the
    original attention patterns. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE
    Extensions of Long-Context LLMs: An Attention Perspective"), the JS divergence
    between the RoPE extensions and LLaMa-3 is smaller than between the RoPE and LLaMa-3.
    This indicates that the attention patterns of RoPE extensions resemble those of
    models directly trained in a longer context.'
  prefs: []
  type: TYPE_NORMAL
- en: NTK and YaRN do not affect the attention patterns within the pretrained length.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some RoPE extensions can degrade performance within the original pretrained
    length (Peng et al., [2023](#bib.bib17); Zhang et al., [2024](#bib.bib25)). To
    verify whether RoPE extensions alter the attention patterns within the pretrained
    length, we also calculate the JS divergence among these models’ attention distributions
    at a 2K length. As illustrated on the top row of Table [1](#S3.T1 "Table 1 ‣ RoPE
    extensions maintain the original attention patterns. ‣ 3 RoPE Extensions on PPL
    ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective"),
    the JS divergence for the NTK and YaRN is very low, almost zero, indicating minimal
    impact on attention distribution. On the contrary, the JS divergence for the PI
    is significantly higher. Therefore, we conclude that the NTK and YaRN methods
    do not affect attention patterns within the pretrained length.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c81bfeefa0c4b04f3a264c43c58187b5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RoPE
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f721fb423675cc3e3778ce25aeaf72d8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Finetuning with PI
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/463c41d0bd0b28b9bf3604282c5d4601.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Finetuning with YaRN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c10848cc2996799a1354c62d298a2e8.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Finetuning with NTK
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1167af8241242f3544cc1a8f7a7ac336.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Finetuning on 4K with NTK from (d)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3bea87cd7fdd0dabad0e1847e4ef6949.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Finetuning on 32K with NTK from (d)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Performance comparison for the Needle-in-a-Haystack Test. The x-axis
    represents the length of the document, while the y-axis indicates the position
    where the needle is located. A red cell indicates that the model fails to recall
    the information in the needle, whereas a green cell indicates success. A white
    dashed line denotes the model’s continual pretrain length. Each value in the cells
    signifies the mean attention entropy, with higher values reflecting more dispersed
    attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 RoPE Extensions on Needle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand the performance and behavior of the RoPE extensions on more challenging
    long-context tasks, we conduct Needle testing (Fu et al., [2024](#bib.bib7)).
    As shown in Figure [3](#S3.F3 "Figure 3 ‣ NTK and YaRN do not affect the attention
    patterns within the pretrained length. ‣ 3 RoPE Extensions on PPL ‣ Understanding
    the RoPE Extensions of Long-Context LLMs: An Attention Perspective")(a-d), LLaMa-2-7B
    with RoPE extensions can pass more needle tests than the RoPE. However, as the
    context length increases, some tests fail, resulting in retrieval needle errors.
    Eventually, almost all fail in extremely long contexts. We also conduct Needle
    testing on the MiniMA-2-3B and LLaMa-2-13B models with RoPE and PI. Unlike the
    LLaMa-2-7B, the PI method shows a more significant improvement in the LLaMa-2-13B,
    as depicted in Figure [9](#A2.F9 "Figure 9 ‣ Appendix B Experimental Results on
    LLaMa-2-13B ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective"). In contrast, on the MiniMA-2-3B, PI passes only a few needle tests
    at longer lengths, as illustrated in Figure [6](#A1.F6 "Figure 6 ‣ Appendix A
    Experimental Results on MiniMA-2-3B ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective"). We attribute these observations to the impact
    of model size. Below are key takeaways from the attention perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: Attention uncertainty leads to more retrieval needle errors.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To acquire the reason behind the retrieval needle errors, we calculate the
    entropy of attention for each length and depth, as illustrated in Figures [3](#S3.F3
    "Figure 3 ‣ NTK and YaRN do not affect the attention patterns within the pretrained
    length. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective"). Our findings demonstrate that the locations
    of retrieval needle errors often coincide with high attention entropy. For example,
    at the same depth, the positions with errors are among the top-k in entropy; similarly,
    at the same length, the error positions also have high entropy. We hypothesize
    that the increase in attention entropy with longer test lengths is due to the
    train-short-and-test-long setting. During inference, the number of tokens handled
    by the self-attention mechanism far exceeds that during training. More tokens
    lead to more dispersed attention, i.e., higher uncertainty, causing a mismatch
    between training and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: A natural approach to lower attention uncertainty for enhancing extrapolation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A direct solution is to train on longer contexts, thereby increasing the number
    of attention tokens during training and reducing attention uncertainty. To validate
    our hypothesis, we finetune models on 4K and 32K training lengths with the same
    tokens on NTK. As shown in Figures [3(e)](#S3.F3.sf5 "Figure 3(e) ‣ Figure 3 ‣
    NTK and YaRN do not affect the attention patterns within the pretrained length.
    ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective") and [3(f)](#S3.F3.sf6 "Figure 3(f) ‣ Figure 3
    ‣ NTK and YaRN do not affect the attention patterns within the pretrained length.
    ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective"), compared to models trained in short contexts,
    models trained in more extended contexts exhibited significantly lower attention
    uncertainty. For example, at length 63938, the attention entropy is generally
    below 5. The Needle test pass rates improved significantly, especially in longer
    testing contexts. Conversely, models trained with the same number of tokens but
    shorter context sizes showed little to no change in attention entropy, remaining
    similar to the original one ([3(d)](#S3.F3.sf4 "Figure 3(d) ‣ Figure 3 ‣ NTK and
    YaRN do not affect the attention patterns within the pretrained length. ‣ 3 RoPE
    Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An
    Attention Perspective")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper provides the first thorough understanding of RoPE extensions for
    long-context LLMs from an attention perspective, evaluated on two widely-used
    benchmarks: Perplexity and Needle-in-a-Haystack. Extensive experiments demonstrate
    some valuable findings: 1) Compared to direct extrapolation, RoPE extensions can
    maintain the original training length attention patterns. 2) Large attention uncertainty
    leads to retrieval errors in needle testing in RoPE extensions. 3) Using longer
    continual pretraining lengths for RoPE extensions can reduce attention uncertainty
    and significantly enhance extrapolation in target LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azerbayev et al. (2022) Azerbayev, Z., Ayers, E., , and Piotrowski, B. Proof-pile,
    2022. URL [https://github.com/zhangir-azerbayev/proof-pile](https://github.com/zhangir-azerbayev/proof-pile).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2023) Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,
    Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. *ArXiv preprint*,
    abs/2309.16609, 2023. URL [https://arxiv.org/abs/2309.16609](https://arxiv.org/abs/2309.16609).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bi et al. (2024) Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding,
    H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm: Scaling open-source language
    models with longtermism. *ArXiv preprint*, abs/2401.02954, 2024. URL [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (eds.), *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation. *ArXiv preprint*,
    abs/2306.15595, 2023a. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation. *ArXiv preprint*,
    abs/2306.15595, 2023b. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2024) Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y.,
    and Peng, H. Data engineering for scaling language models to 128k context. *ArXiv
    preprint*, abs/2402.10171, 2024. URL [https://arxiv.org/abs/2402.10171](https://arxiv.org/abs/2402.10171).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang,
    S. Lm-infinite: Simple on-the-fly length generalization for large language models.
    *ArXiv preprint*, abs/2308.16137, 2023. URL [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2024) Hu, Y., Huang, Q., Tao, M., Zhang, C., and Feng, Y. Can perplexity
    reflect large language model’s ability in long text understanding? *ArXiv preprint*,
    abs/2405.06105, 2024. URL [https://arxiv.org/abs/2405.06105](https://arxiv.org/abs/2405.06105).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021) Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can
    we know when language models know? on the calibration of language models for question
    answering. *Transactions of the Association for Computational Linguistics*, 9:962–977,
    2021. doi: 10.1162/tacl˙a˙00407. URL [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaiokendev (2023) Kaiokendev. Things i’m learning while training superhot. [https://kaiokendev.github.io/til#extending-context-to-8k](https://kaiokendev.github.io/til#extending-context-to-8k),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamalloo et al. (2023) Kamalloo, E., Dziri, N., Clarke, C. L., and Rafiei, D.
    Evaluating open-domain question answering in the era of large language models.
    *ArXiv preprint*, abs/2305.06984, 2023. URL [https://arxiv.org/abs/2305.06984](https://arxiv.org/abs/2305.06984).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamradt (2023) Kamradt, G. Needle in a haystack - pressure testing llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
    Scaling laws of rope-based extrapolation. *ArXiv preprint*, abs/2310.05209, 2023.
    URL [https://arxiv.org/abs/2310.05209](https://arxiv.org/abs/2310.05209).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Men et al. (2024) Men, X., Xu, M., Wang, B., Zhang, Q., Lin, H., Han, X., and
    Chen, W. Base of rope bounds context length. *ArXiv preprint*, abs/2405.14591,
    2024. URL [https://arxiv.org/abs/2405.14591](https://arxiv.org/abs/2405.14591).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minaee et al. (2024) Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher,
    R., Amatriain, X., and Gao, J. Large language models: A survey. *ArXiv preprint*,
    abs/2402.06196, 2024. URL [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models. *ArXiv preprint*,
    abs/2309.00071, 2023. URL [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever,
    I., et al. Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2023) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov,
    I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez,
    A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
    Synnaeve, G. Code Llama: Open foundation models for code, 2023. arXiv: 2308.12950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2019) Su, D., Xu, Y., Winata, G. I., Xu, P., Kim, H., Liu, Z., and
    Fung, P. Generalizing question answering system with pre-trained language model
    fine-tuning. In *Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering*, pp.  203–211, Hong Kong, China, 2019\. Association for Computational
    Linguistics. doi: 10.18653/v1/D19-5827. URL [https://aclanthology.org/D19-5827](https://aclanthology.org/D19-5827).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2021) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
    Roformer: Enhanced transformer with rotary position embedding. *ArXiv preprint*,
    abs/2104.09864, 2021. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *ArXiv preprint*, abs/2307.09288,
    2023. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need.
    In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan,
    S. V. N., and Garnett, R. (eds.), *Advances in Neural Information Processing Systems
    30: Annual Conference on Neural Information Processing Systems 2017, December
    4-9, 2017, Long Beach, CA, USA*, pp.  5998–6008, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, C., Song, D., Ye, Z., and Gao, Y. Towards the law
    of capacity gap in distilling language models. *ArXiv preprint*, abs/2311.07052,
    2023. URL [https://arxiv.org/abs/2311.07052](https://arxiv.org/abs/2311.07052).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024) Zhang, Y., Li, J., and Liu, P. Extending llms’ context window
    with 100 samples. *ArXiv preprint*, abs/2401.07004, 2024. URL [https://arxiv.org/abs/2401.07004](https://arxiv.org/abs/2401.07004).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Results on MiniMA-2-3B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e4da0a2b893fe54417660e7c63094b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Validation Perplexity on Proof-pile (Azerbayev et al., [2022](#bib.bib1))
    of MiniMA-2-3B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb6b222b3e4d02122c4378fc11fbe6f3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RoPE on 2K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9d43fb32dbe27052a13e794bf5a226a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) RoPE on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/731fe0cdec03b8fc610d255e2027ef65.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) PI on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d893f193a8ffd197320da85fe1ca3804.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) YaRN on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84c24ce690ff085e2450f52672d7c8f2.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) NTK on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Attention distributions of RoPE, PI, YaRN, and NTK methods on 2K
    and 8K sequences on MiniMA-2-3B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/346407a9ea9222629540a19e2e66a55d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RoPE
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70418268e8737cca5c9f143a5919bb43.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Finetuning with PI
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Performance comparison for the Needle-in-a-Haystack Test of MiniMa-2-3B.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Experimental Results on LLaMa-2-13B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8c08dc80bf0f25385eb229c9bc421602.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Validation Perplexity on Proof-pile of LLaMa-2-13B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c6ea41072581efbc4722313265104109.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RoPE on 2K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7a4b72e9c712b67dce2d31df02756cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) RoPE on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/521d2d91c5da0056efc97f02bab2602d.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) PI on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3b8c194122311ae6402d904ca37456d.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) YaRN on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/326771bb643d0037eb6615d8ea4857ea.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) NTK on 8K sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Attention distributions of RoPE, PI, YaRN, and NTK methods on 2K
    and 8K sequences on LLaMA-2-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/275a23ce0f7afa5f8c4188003ba134af.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RoPE
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a0cb7db3324762728cd34c2849defe0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Finetuning with PI
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Performance comparison for the Needle-in-a-Haystack Test of LLaMa-2-13B.'
  prefs: []
  type: TYPE_NORMAL
