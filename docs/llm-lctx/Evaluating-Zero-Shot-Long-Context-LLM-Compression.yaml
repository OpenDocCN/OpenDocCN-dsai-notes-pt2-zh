- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:56:22'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Zero-Shot Long-Context LLM Compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06773](https://ar5iv.labs.arxiv.org/html/2406.06773)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: spacing=nonfrench
  prefs: []
  type: TYPE_NORMAL
- en: Chenyu Wang
  prefs: []
  type: TYPE_NORMAL
- en: cw9420@princeton.edu    Yihan Wang
  prefs: []
  type: TYPE_NORMAL
- en: yihanw_mems@princeton.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This study evaluates the effectiveness of zero-shot compression techniques on
    large language models (LLMs) under long-context. We identify the tendency for
    computational errors to increase under long-context when employing certain compression
    methods. We propose a hypothesis to explain the varied behavior of different LLM
    compression techniques and explore remedies to mitigate the performance decline
    observed in some techniques under long-context. This is a course report for COS
    598D Machine Learning and Systems by Prof. Kai Li at Princeton University. Due
    to limited computational resources, our experiments were conducted only on LLaMA-2-7B-32K.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) excel in processing and generating human-like text,
    yet they confront significant challenges.[[16](#bib.bib16), [4](#bib.bib4), [36](#bib.bib36),
    [26](#bib.bib26), [2](#bib.bib2), [44](#bib.bib44)] One major challenge is computational
    efficiency; LLMs demand considerable resources during both training and inference
    stages. Another challenge is their limited context length, which is the maximum
    number of tokens the model can process at once. Inputs that exceed this context
    length can result in unreasonable responses from the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: To address the computational efficiency challenge of LLMs, many researchers
    have proposed leveraging model compression techniques. These LLM compression techniques
    commonly operate under the assumption that both the weights and activations of
    LLMs can be compressed with minimal impact on computational error[[41](#bib.bib41),
    [35](#bib.bib35), [18](#bib.bib18)]. On the other hand, researchers are also working
    on extending the context length of LLMs[[1](#bib.bib1), [17](#bib.bib17)]. The
    context length of LLMs have been expanding exponentially in recent years. This
    significant increase in context length allows LLMs to handle increasingly complex
    tasks, such as analyzing multiple books and documents simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the literature, we have observed that the majority of studies
    on LLM compression focus on models with relatively short context lengths (e.g.,
    4K). However, the effectiveness of these compression techniques on models with
    extensive context lengths (e.g., 32K) remains under-evaluated. In this project,
    we aim to evaluate zero-shot LLM compression under long-context. We begin by conducting
    both qualitative theoretical analysis and empirical evaluations of long-context
    LLM compression. Subsequently, we attempt to develop appropriate solutions to
    address the performance decline observed in some LLM compression techniques under
    long-context.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contribution of this project can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct a theoretical analysis of long-context LLM compression. We find that
    as the context length increases in compressed LLMs, computational errors tend
    to accumulate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We perform an empirical evaluation of various LLM compression techniques to
    assess computational errors in extended contexts. Our findings indicate diverse
    behaviors across different compression methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a hypothesis to explain the varied responses of different LLM compression
    techniques and explore remedies to mitigate the performance decline observed in
    some techniques under long-context scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The rest of this report is organized as follows. In Section [2](#S2 "2 Related
    Works ‣ Evaluating Zero-Shot Long-Context LLM Compression"), we will introduces
    previous research related to this project. In Section [4](#S4 "4 Evaluation Details
    ‣ Evaluating Zero-Shot Long-Context LLM Compression"), we will expand the technical
    details of our project. In Section [5](#S5 "5 Conclusion ‣ Evaluating Zero-Shot
    Long-Context LLM Compression"), we will conclude our project. In Section 5, we
    will introduce the future work.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Long-Context LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent times, there has been a push towards expanding the context length
    of Language Models (LLMs) efficiently through continuous pretraining or fine-tuning.
    One approach involves enhancing Rotary Position Embeddings (RoPE) [[33](#bib.bib33)],
    which has led to longer contexts of up to 128k  [[6](#bib.bib6), [7](#bib.bib7),
    [29](#bib.bib29)]. Another line of research, exemplified by Mistral [[16](#bib.bib16)],
    introduces sliding window attention mechanisms that focus only on a portion of
    tokens from the preceding layer, thereby reducing computational demands and facilitating
    pretraining with longer contexts of up to 30k. However, due to the memory-intensive
    nature of autoregressive generation in LLMs [[20](#bib.bib20)], the storage of
    Key-Value (KV) caches for longer contexts slows down inference processes and necessitates
    GPUs with large VRAM capacities.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization in LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Quantization is a commonly employed method for model compression. [[14](#bib.bib14),
    [34](#bib.bib34), [37](#bib.bib37)] Researchers investigate two distinct settings
    for Large Language Model (LLM) quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W8A8 quantization, where both activations and weights are quantized to INT8 [[9](#bib.bib9),
    [41](#bib.bib41), [40](#bib.bib40), [39](#bib.bib39), [32](#bib.bib32)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-bit weight-only quantization (e.g., W4A16), where only weights are quantized
    into low-bit integers [[12](#bib.bib12), [10](#bib.bib10), [31](#bib.bib31), [32](#bib.bib32),
    [27](#bib.bib27)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This work concentrates on the second setting, as it not only reduces the hardware
    requirements by necessitating a smaller memory size but also accelerates token
    generation, thus alleviating memory-bound workloads. Besides, we require the algorithm
    to be zero-shot, which is relatively low-cost and non-task-specific.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning in LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pruning, a well-established technique for compressing neural networks, involves
    eliminating weights to create sparse networks [[21](#bib.bib21)]. It can be broadly
    classified into structured and unstructured approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Structural pruning involves removing entire filters from the neural network,
    making it more conducive to hardware implementation. Various methods exist for
    implementing structural pruning, such as l1-dependent pruning [[14](#bib.bib14),
    [42](#bib.bib42)], first-order importance estimation [[24](#bib.bib24)], hessian-based
    estimation [[19](#bib.bib19), [38](#bib.bib38)], or the optimal brain surgeon [[21](#bib.bib21),
    [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured methods [[14](#bib.bib14), [13](#bib.bib13), [28](#bib.bib28),
    [35](#bib.bib35)] like magnitude pruning operate at the individual weight level,
    maintaining performance even at higher sparsity levels. However, existing pruning
    methods usually require modifications to the training procedure [[30](#bib.bib30)],
    retraining the pruned networks to regain accuracy [[23](#bib.bib23)], or a computationally
    intensive iterative retraining process [[3](#bib.bib3), [11](#bib.bib11)]. Yet,
    scaling these techniques to LLMs with billions of parameters poses a challenge,
    as the necessary training process demands significant computational resources [[15](#bib.bib15),
    [45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We focus on unstructured pruning methods in this work as they are more fundamental
    and flexible than structural pruning. Specifically, we choose two representative
    methods: magnitude pruning and Wanda [[35](#bib.bib35)]. Although other model
    compression methods such as neural architecture search exist [[46](#bib.bib46),
    [5](#bib.bib5)], this study focuses exclusively on pruning and quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Concurrent Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our review of the literature, we noted a related study on the quantization
    of long-context LLMs detailed in Li et al. (2024) [[22](#bib.bib22)]. This research
    conducted two experiments to assess the performance of LLMs under extended contexts.
    It reported a decline in performance when applying weight-only quantization, weight-activation
    quantization, and KV cache quantization. However, it did not delve into further
    analysis to explore the underlying causes of this observed performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are typically built on transformer architecture, which operates as a ’sequence
    model’. In the transformer architecture, each newly generated token calculates
    its attention scores against the hidden states of all preceding tokens. Regarding
    LLM compression techniques, while they can accelerate inference, they also introduce
    computational errors in both the output and hidden states of LLMs. Consequently,
    in compressed LLMs, each new token is computed based on an increasingly large
    number of preceding tokens, with each token contributing its own computational
    error. The formulas below provide a detailed description of this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the transformer architecture processing a sequence where each token
    $t$ for token $t$ are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textbf{a}_{t}=\text{softmax}\left(\frac{\textbf{q}_{t}[k_{1}^{T}\cdots
    k_{t}^{T}]}{\sqrt{d_{k}}}\right)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $d_{k}$ is the dimensionality of the key vectors, ensuring proper scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden state $\mathbf{h}_{t}$ is computed by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{h}_{t}=[v_{1}^{T},\cdots,v_{t}^{T}]\textbf{a}_{t}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{v}_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'When LLMs undergo compression, noise $\epsilon\sim\mathcal{N}(0,\sigma^{2})$
    is added to each key and value vector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{h}_{t}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\approx\sum_{i=1}^{t}\text{softmax}\left(\frac{\textbf{q}_{i}k_{i}}{\sqrt{d_{k}}}\right)v_{i}+$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: for simplicity, in this step we assume that all the vectors involved are one
    dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65439f945fa9c1cc525a26ea36b23046.png)![Refer to caption](img/2d2d7b77bf911faaf4fd05589dfa6d80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Pruning algorithms are robust to context lengths. The KL divergence
    of output logits between the uncompressed model and the pruned models does not
    change much with respect to different context lengths. The pruning ratio will
    only affect the variance of KL divergence values measured in different context
    lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: This formula indicates that for compressed LLMs, the computation of the $t$,
    indicating increased computation error in longer sequences.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to the complexity of LLMs, there remains uncertainty regarding
    how this theoretical analysis will be reflected in the final output. In the following
    sub-section, we will discuss the empirical evaluation of the computation error
    of compressed LLMs under long-context.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Empirical Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess the computational error in the final output, we compute the Kullback-Leibler
    (KL) divergence between the outputs of the compressed and uncompressed versions
    of the same model, as indicated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D_{\text{KL}}(p\parallel q)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $q$ represents the compressed model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: We have selected LLaMA-2-7B-32K as our model to evaluate four distinct LLM compression
    techniques across two categories[[8](#bib.bib8)]. For pruning, we implement magnitude
    pruning, which represents the simplest pruning technique, and Wanda pruning, which
    represents the state-of-the-art method. For quantization, we choose to evaluate
    weight-only and weight-activation quantization implemented by [[22](#bib.bib22)].
    Due to its utilization of ’per-group’ weight quantization and ’per-token’ activation
    quantization, the method described in [[22](#bib.bib22)] represents a robust quantization
    technique. It achieves minimal computational error compared to other methods.
    We sampled texts from WikiText dataset [[25](#bib.bib25)], and calculate the KL
    divergence between the models’ output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5bdca2261ee774b2c7b6c046e7b997a.png)![Refer to caption](img/7b025fc39ac05b5847e4819810e87bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Only about 2% weights are sensitive to low-bit quantizations. We
    can use 8-bit quantization instead of 3/4-bit quantization for these weights to
    make the compressed models less sensitive to context lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We plot out experiments using pruning in Figure [1](#S4.F1 "Figure 1 ‣ 4.1
    Theoretical Analysis ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context
    LLM Compression") and experiments using quantization in Figure [3](#S4.F3 "Figure
    3 ‣ 4.2 Empirical Evaluation ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context
    LLM Compression") and Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Empirical Evaluation ‣
    4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context LLM Compression"). Counter-intuitively,
    the performance of pruning and quantization varies. For pruning, the KL-divergence
    of output between the uncompressed model and the pruned models does not change
    much with respect to different context lengths. The pruning ratio will only affect
    the variance of KL-divergence values measured in different context lengths. For
    quantization, especially when we use low-bit ($\leq 4$) weight quantization, the
    performance of compressed models becomes more sensitive to context lengths: the
    output of compressed models becomes more different from that of the uncompressed
    model as the context length increases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b1daf65e6ea6d26bf81241130dd6801.png)![Refer to caption](img/d09ceefd9efa0daec6156116a91ad166.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: When we use low-bit ($\leq 4$) weight quantization, the performance
    of compressed models becomes more sensitive to context lengths: the output of
    compressed models become more different from the of the uncompressed model when
    the context length increases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/406ad9a716cd77553d5c1aa6b641bd90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: When choosing 3-bit quantization, it is very obvious that the output
    of compressed models become more different from the of the uncompressed model
    when the context length increases.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Hypothesis on the Varied Behaviors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we attempt to discuss the reason why pruning remains robust
    as context length increases while quantization suffers from increasing computational
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Our intuition is that for different pruning methods, the weights with larger
    magnitudes are always left untouched, while for quantization methods, they typically
    quantize all the weights regardless of their magnitude. As we observed in Section [4.2](#S4.SS2
    "4.2 Empirical Evaluation ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context
    LLM Compression"), pruning stays robust to context length. Thus, it is reasonable
    to hypothesize that LLMs’ long-range dependency only relies on a small part of
    weights, and for LLaMA-2-7B-32K, the weights with larger magnitude are more sensitive
    in longer contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Our experimental results support our intuition. As is shown in Figure [2](#S4.F2
    "Figure 2 ‣ 4.2 Empirical Evaluation ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot
    Long-Context LLM Compression"), if we select about 2% of weight groups with large
    magnitude, and quantize them to 8 bits, while quantizing other groups to 3 or
    4 bits; we observe that for both weight-only quantization and weight-activation
    quantization, the increasing KL divergence under long-context disappears. However,
    the overall KL divergence level doesn’t change much. In this way, we remedy the
    performance drop of low-bit quantized LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Another experiment results to support our hypothesis is that for pruning, as
    is shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Hypothesis on the Varied Behaviors
    ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context LLM Compression"),
    if we randomly prune out only 10% of the weights, we can observe an almost linearly
    increasing KL divergence as context length increases.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e6c16ef8ff18cf4b5a114c0327b3db2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: If we randomly prune out only 10% weights, we can observe an almost-linearly
    increasing KL divergence as context length increases.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work explored the computation errors in zero-shot LLM compression within
    long-context settings, uncovering that computation errors escalate with increased
    context length in certain compression settings, while remaining stable in others.
    Our hypothesis suggests that specific weights are particularly sensitive to compression
    under long-context scenarios, highlighting a critical area for future research
    to optimize LLM performance effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this project, we explore this question within a relatively simple setting.
    In the future, we plan to evaluate additional long-context LLMs, such as Llama
    3 and Mistral [[43](#bib.bib43), [16](#bib.bib16)], and conduct experiments across
    a broader range of tasks. Additionally, we acknowledge that our hypothesis requires
    further validation. We aim to deepen our research to solidify these initial findings.
    We can also summarize our research as a new LLM compression methods if possible.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We extend our heartfelt gratitude to Prof. Kai Li and the teaching assistants
    for their invaluable guidance and support throughout this project. We also express
    our appreciation to Hao Kang, a PhD student at the Georgia Institute of Technology,
    for his insightful discussions that enriched our work. Chenyu is also grateful
    for Prof. Xuefei Ning and Shiyao Li from Tsinghua University for their work on
    LLM quantization evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu,
    X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for
    long context understanding. arXiv preprint arXiv:2308.14508 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L.,
    He, H., Leahy, C., McDonell, K., Phang, J., et al. Gpt-neox-20b: An open-source
    autoregressive language model. arXiv preprint arXiv:2204.06745 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Bondarenko, Y., Nagel, M., and Blankevoort, T. Understanding and overcoming
    the challenges of efficient transformer quantization. In Proceedings of the 2021
    Conference on Empirical Methods in Natural Language Processing (Online and Punta
    Cana, Dominican Republic, Nov. 2021), Association for Computational Linguistics,
    pp. 7947–7969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language
    models are few-shot learners. In Advances in Neural Information Processing Systems
    (2020), H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33,
    Curran Associates, Inc., pp. 1877–1901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Cai, Y., Wang, C., Ning, X., Zhou, Z., Niu, D., Yang, H., and Wang, Y.
    Deepguiser: Learning to disguise neural architectures for impeding adversarial
    transfer attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of
    large language models via positional interpolation. arXiv preprint arXiv: 2306.15595
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. LongloRA:
    Efficient fine-tuning of long-context large language models. In The Twelfth International
    Conference on Learning Representations (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Computer, T. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K),
    2024. Accessed: yyyy-mm-dd.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit
    matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Dettmers, T., and Zettlemoyer, L. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Frankle, J., and Carbin, M. The lottery ticket hypothesis: Finding sparse,
    trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate
    post-training quantization for generative pre-trained transformers. arXiv preprint
    arXiv:2210.17323 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding. In ICLR
    (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections
    for efficient neural network. Advances in neural information processing systems
    28 (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
    E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal
    large language models. arXiv preprint arXiv:2203.15556 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S.,
    Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral
    7b. arXiv preprint arXiv:2310.06825 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L.
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression, Oct. 2023. arXiv:2310.06839 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Kang, H., Zhang, Q., Kundu, S., Jeong, G., Liu, Z., Krishna, T., and Zhao,
    T. Gear: An efficient kv cache compression recipefor near-lossless generative
    inference of llm. arXiv preprint arXiv:2403.05527 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B.,
    Goin, M., and Alistarh, D. The optimal bert surgeon: Scalable and accurate second-order
    pruning for large language models. arXiv preprint arXiv:2203.07259 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez,
    J., Zhang, H., and Stoica, I. Efficient memory management for large language model
    serving with pagedattention. In Proceedings of the 29th Symposium on Operating
    Systems Principles (2023), pp. 611–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. Advances in
    neural information processing systems 2 (1989).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Li, S., Ning, X., Wang, L., Liu, T., Shi, X., Yan, S., Dai, G., Yang,
    H., and Wang, Y. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining
    approach. arXiv preprint arXiv:1907.11692 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord,
    O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought
    chains for science question answering. Advances in Neural Information Processing
    Systems 35 (2022), 2507–2521.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture
    models, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] OpenAI. Gpt-4 technical report, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuqmm:
    Quantized matmul for efficient inference of large-scale generative language models.
    arXiv preprint arXiv:2206.09557 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Paul, M., Chen, F., Larsen, B. W., Frankle, J., Ganguli, S., and Dziugaite,
    G. K. Unmasking the lottery ticket hypothesis: What’s encoded in a winning ticket’s
    mask? arXiv preprint arXiv:2210.03044 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context
    window extension of large language models. In The Twelfth International Conference
    on Learning Representations (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted
    training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Sheng, Y., Cao, S., Li, D., Hooper, C., Lee, N., Yang, S., Chou, C., Zhu,
    B., Zheng, L., Keutzer, K., Gonzalez, J. E., and Stoica, I. S-LoRA: Serving Thousands
    of Concurrent LoRA Adapters, Nov. 2023. arXiv:2311.03285 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, D. Y., Xie,
    Z., Chen, B., Barrett, C., Gonzalez, J. E., et al. High-throughput generative
    inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer
    with rotary position embedding. NEUROCOMPUTING (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Sun, H., Wang, C., Zhu, Z., Ning, X., Dai, G., Yang, H., and Wang, Y.
    Gibbon: Efficient co-exploration of nn model and processing-in-memory architecture.
    In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE) (2022),
    IEEE, pp. 867–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning
    approach for large language models. arXiv preprint arXiv:2306.11695 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open
    foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Wang, C., Dong, Z., Zhou, D., Zhu, Z., Wang, Y., Feng, J., and Keutzer,
    K. Epim: Efficient processing-in-memory accelerators based on epitome. arXiv preprint
    arXiv:2311.07620 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Wang, C., Grosse, R., Fidler, S., and Zhang, G. Eigendamage: Structured
    pruning in the kronecker-factored eigenbasis. In International conference on machine
    learning (2019), PMLR, pp. 6566–6575.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Wei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., and Liu, X.
    Outlier suppression+: Accurate quantization of large language models by equivalent
    and optimal shifting and scaling. arXiv preprint arXiv:2304.09145 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F.,
    and Liu, X. Outlier suppression: Pushing the limit of low-bit transformer language
    models. arXiv preprint arXiv:2209.13325 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Accurate
    and efficient post-training quantization for large language models. arXiv preprint
    arXiv:2211.10438 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Zafrir, O., Larey, A., Boudoukh, G., Shen, H., and Wasserblat, M. Prune
    once for all: Sparse pre-trained language models. arXiv preprint arXiv:2111.05754
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Zhang, P., Shao, N., Liu, Z., Xiao, S., Qian, H., Ye, Q., and Dou, Z.
    Extending llama-3’s context ten-fold overnight. arXiv preprint arXiv:2404.19553
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P.,
    and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init
    attention. arXiv preprint arXiv:2303.16199 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan,
    C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language
    models. arXiv preprint arXiv:2205.01068 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Zoph, B., and Le, Q. V. Neural architecture search with reinforcement
    learning. arXiv preprint arXiv:1611.01578 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
