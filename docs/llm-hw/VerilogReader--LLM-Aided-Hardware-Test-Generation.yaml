- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'VerilogReader: LLM-Aided Hardware Test Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.04373](https://ar5iv.labs.arxiv.org/html/2406.04373)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ruiyang Ma¹, Yuxin Yang¹, Ziqian Liu², Jiaxi Zhang¹, Min Li³, Junhua Huang³,
    Guojie Luo¹ ¹School of Computer Science, Peking University; ²School of Information,
    Renmin University of China; ³Noah’s Ark Lab, Huawei ruiyang@stu.pku.edu.cn, {yxyang,
    zhangjiaxi, gluo}@pku.edu.cn, liuziqian@ruc.edu.cn, minli.amoy@gmail.com, huang.hjh@outlook.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Test generation has been a critical and labor-intensive process in hardware
    design verification. Recently, the emergence of Large Language Model (LLM) with
    their advanced understanding and inference capabilities, has introduced a novel
    approach. In this work, we investigate the integration of LLM into the Coverage
    Directed Test Generation (CDG) process, where the LLM functions as a Verilog Reader.
    It accurately grasps the code logic, thereby generating stimuli that can reach
    unexplored code branches. We compare our framework with random testing, using
    our self-designed Verilog benchmark suite. Experiments demonstrate that our framework
    outperforms random testing on designs within the LLM’s comprehension scope. Our
    work also proposes prompt engineering optimizations to augment LLM’s understanding
    scope and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Automatic Test Generation, LLM, Verilog^†^† This work was partly supported
    by the National Natural Science Foundation of China (Grant No. 62090021) and the
    National Key R&D Program of China (Grant No. 2022YFB4500500). Corresponding author:
    Guojie Luo. 979-8-3503-7608-1/24$31.00 ©2024 IEEE'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As hardware complexity surges, the importance of hardware verification in the
    development process intensifies. Undetected hardware bugs can result in substantial
    repercussions and considerable economic losses. To address the risk of design
    flaws in hardware, engineers employ two primary verification methodologies: formal
    verification and dynamic verification.'
  prefs: []
  type: TYPE_NORMAL
- en: Formal methods employs mathematical techniques to prove or disprove the correctness
    of a system with respect to a certain formal specification or property [[1](#bib.bib1)].
    On the other hand, dynamic verification, generates diverse test cases to simulate
    the Design Under Test (DUT), offering more flexibility and scalability than formal
    verification [[2](#bib.bib2)]. Coverage targets, including code and functional
    coverage, serve as benchmarks for determining the thoroughness of tests. The attainment
    of these targets necessitates high-quality test inputs, which imposes a considerable
    labor burden on verification engineers.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the need for human intervention, Coverage Directed Test Generation
    (CDG) has emerged as a pivotal technique in automatic hardware test generation [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]. This method leverages
    heuristic approaches to explore the input space, with coverage states serving
    as basic feedback for the generation of new test cases. In situations with hard-to-reach
    coverpoints, supplementary circuit structural information (e.g., control/data
    flow graph, module connectivity graph) are utilized to guide directed test generation [[7](#bib.bib7),
    [4](#bib.bib4), [8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the impressive capabilities of LLM in comprehension and inference
    have been highlighted. Previous studies have shown LLM’s versatility in multiple
    hardware tasks, such as RTL writing [[9](#bib.bib9), [10](#bib.bib10)], assertion
    generation [[11](#bib.bib11), [12](#bib.bib12)] and bug fixing [[13](#bib.bib13)].
    The advanced competencies of LLM present a compelling opportunity for their deployment
    in the field of hardware test generation. Zhang et al. have pioneered the initial
    step towards verifying the functional points of DUT [[14](#bib.bib14)]. A description
    of functional coverpoints is provided, following which the LLM generates input
    sequences. Their experimental results demonstrate a significant improvement in
    performance over random testing on various DUTs. Their research substantiates
    the capability of LLM to comprehend the high-level description of input principles
    and functional testpoints in the task of hardware verification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57f4320c8bf33296243ea97bece0ff7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: LLM-Aided Hardware Test Generation Workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: While our research adopts a distinct perspective. Complementing with previous
    work, we have pioneered the use of LLM to specifically improve the hardware code
    coverage, which is a more fundamental testing target and is intrinsically linked
    to the Verilog code itself. This approach necessitates the shift in LLM’s focus
    from the high-level functional testplan descriptions to the in-depth understanding
    of basic Verilog code logic and coverage status. That is, we repositioned the
    LLM as a VerilogReader, facilitating its role as a hardware verifier to read codes
    and write test cases for uncovered lines or branches, consequently reducing the
    manual effort required for code analysis and test generation.
  prefs: []
  type: TYPE_NORMAL
- en: ^†^† ¹https://github.com/magicYang1573/llm-hardware-test-generation
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our paper makes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We open-source a framework that integrates LLM into the CDG process¹. For the
    first time, LLM is used as a VerilogReader to understand Verilog code and coverage,
    aiming to generate tests for code coverage closure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose Coverage Explainer and DUT Explainer to enrich the prompt, thereby
    enhancing LLM’s comprehension of the design and our testing intentions. These
    modules also augment the extensibility of our framework.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create a benchmark suite including 24 Verilog designs of simple, medium,
    and complex levels. Our experiments show that our framework outperforms random
    testing on simple- and medium-level DUTs. We also delineate the maximum Verilog
    reading capabilities of current LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Basic Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our study integrates LLM into the Coverage Directed Test Generation (CDG) process,
    as depicted in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ VerilogReader: LLM-Aided
    Hardware Test Generation"). In each iteration, the LLM generates multi-cycle inputs
    in JSON format. These inputs are subsequently decoded by the Input Decoder into
    hardware stimuli. Upon completion of simulation, the Coverage Monitor provides
    current code coverage information to LLM, guiding the generation of extra input
    stimuli.'
  prefs: []
  type: TYPE_NORMAL
- en: To generate test inputs, the LLM requires a comprehensive understanding of the
    Verilog DUT and the current coverage status. Given that these data are initially
    in non-natural-language formats, they must be transformed into a format conducive
    to the LLM. To this end, we have introduced two explainer modules. The Coverage
    Explainer module reformats the original simulator coverage report into a more
    LLM-readable format, while the DUT Explainer module enriches the DUT code with
    a natural language description or guidance. These modules collectively enhances
    the LLM’s comprehension of test intentions and the DUT’s functionality. Following
    this, the Prompt Generator integrates these outputs to create the final prompt.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Prompt Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To encourage a step-by-step thought process in the LLM, the Prompt Generator
    facilitates two rounds of question-and-answer sessions in each iteration of the
    CDG process, thereby generating the hardware input stimulus, as depicted in Figure [2](#S2.F2
    "Figure 2 ‣ II-B Prompt Generator ‣ II Approach ‣ VerilogReader: LLM-Aided Hardware
    Test Generation"). In the first round, the LLM is informed of our objective to
    generate tests for unexplored code lines, incorporating details about the DUT
    from the DUT Explainer and the current coverage data from the Coverage Explainer.
    The LLM responds in natural language, typically mirroring its cognitive process.
    In the second round, we instruct the LLM to reformulate its initial response into
    a standardized JSON format for subsequent input decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6aa9021c6525947d4aa8e4bc068fd95a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example of prompts and LLM answers.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Coverage Explainer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/adcee66e410add2bdff70fe019f7cdc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparison of three coverage report formats.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enhance LLM’s comprehension of the current DUT coverage, we introduce the
    Coverage Explainer module, which translates the intricate coverage report into
    a more comprehensible format. As shown in Figure [3](#S2.F3 "Figure 3 ‣ II-C Coverage
    Explainer ‣ II Approach ‣ VerilogReader: LLM-Aided Hardware Test Generation")(a),
    the original Verilator coverage report format includes each coverpoint represented
    by a unique identifier string and a hit count. This format is cryptic and poses
    readability challenges for both human users and LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple alternative involves using Verilator-provided verilator_coverage tool
    to create an annotated coverage report, as depicted in Figure [3](#S2.F3 "Figure
    3 ‣ II-C Coverage Explainer ‣ II Approach ‣ VerilogReader: LLM-Aided Hardware
    Test Generation")(b). This format, which correlates coverage status with DUT source
    code, is more interpretable. The left-side number in each code line indicates
    the hit count of the line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the improvements of annotated coverage format, it still presents challenges
    for LLM, as LLM must identify uncovered lines, thereby increasing the complexity.
    To mitigate this, we suggest an advanced LLM-readable coverage report, specifically
    designed for our test generation task, as depicted in Figure [3](#S2.F3 "Figure
    3 ‣ II-C Coverage Explainer ‣ II Approach ‣ VerilogReader: LLM-Aided Hardware
    Test Generation")(c). This report introduces a ‘TO BE COVERED’ flag for lines
    that remain uncovered. The application of natural language to flag only the uncovered
    lines could facilitate a more straightforward inference process for LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: II-D DUT Explainer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To augment LLM’s comprehension of the DUT, we introduce the DUT Explainer module.
    Given that the LLM’s comprehension of Verilog code for test generation tasks is
    not fully optimized, this module aims to provide additional digestible information
    about the DUT, thereby facilitating more efficient test generation. The DUT Explainer
    module is designed to serve two main functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Design Description, provides the LLM with a natural language explanation of
    the DUT’s functionalities and internal logic, mitigating the LLM’s incapacity
    to interpret Verilog code. This description can be acquired either by the LLM
    or manually. When acquired by the LLM, the test generation task is split into
    two stages: DUT understanding and input logic inference, thus alleviating LLM’s
    workload in each phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Test Guidance, enriches the LLM with supplementary information for creating
    tests for specific DUT. This could involve fundamental test logic rules or advice
    for some hard-to-cover points. For instance, when generating tests for a Finite
    State Machine (FSM) circuit, LLM is guided to first consider the transition to
    each state and then discern conditions to address any uncovered points within
    that state. Additionally, it could be endowed with some knowledge on reaching
    challenging states, thereby reducing the analytical burden on the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: III Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We evaluate our framework on our synthetic benchmark suite, detailed in Section [III-A](#S3.SS1
    "III-A Benchmark Suite ‣ III Evaluation ‣ VerilogReader: LLM-Aided Hardware Test
    Generation"). For each design, we use Pyverilog [[15](#bib.bib15)] to extract
    input signals and automatically generate testbench interface with our framework.
    Verilator [[16](#bib.bib16)] serves as our simulator. The language models used
    in our experiments include OpenAI’s GPT-4 and GPT-4-Turbo-0125 [[17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Benchmark Suite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We created 24 Verilog designs in our benchmark suite and assigns three difficulty
    levels for these designs.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Simple
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This level involves 10 basic combinational logic circuits (s01-s10), such as
    multiplexer and ALU. The direct influence of inputs on the coverage path within
    the same cycle offers a straightforward inference scenario for LLM. These designs
    are used to assess LLM’s understanding of Verilog syntax, including constructs
    like always, case, assign, etc.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Medium
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This level consists of 8 sequential logic circuits (m01-m08), such as FSMs,
    counters and arbiters. The coverage path of the current cycle is influenced by
    inputs from several preceding cycles. These designs aim to demonstrate LLM’s cross-cycle
    inference capabilities in test generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Complex
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This level encompasses 6 large-scale FSM circuits (c01-c06), ranging from 16
    to 128 states, and two transition branches per state. It serves as a benchmark
    category to evaluate the upper limit of the current LLM’s comprehensive ability
    in hardware test generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Comparison of Coverage Explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [II-C](#S2.SS3 "II-C Coverage Explainer ‣ II Approach ‣ VerilogReader:
    LLM-Aided Hardware Test Generation"), we present an LLM-readable coverage report,
    designed to enhance LLM’s comprehension of current coverage status. To validate
    the utility of our coverage explanation method, we contrast it with the original
    and annotated coverage reports from Verilator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiments were carried out on medium-level DUTs using GPT-4 as the language
    model. The comparison metric was the total length of input stimulus (measured
    in clock cycles) required to achieve full line coverage. Given the stochastic
    behavior of LLM, each experiment was replicated five times. The results are represented
    as box (25%ile) and whisker (75%ile) plots, along with median lines for each DUT,
    as shown in Figure [4](#S3.F4 "Figure 4 ‣ III-B Comparison of Coverage Explanations
    ‣ III Evaluation ‣ VerilogReader: LLM-Aided Hardware Test Generation"). The figure
    clearly indicates that the original unreadable coverage report poses the greatest
    challenge for LLM, whereas our LLM-readable coverage report demonstrates superior
    performance compared to the other two Verilator-provided reports.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad2eec38685dd4332648f241f4341bce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of coverage explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Comparison against Random Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to evaluate the efficacy of LLM for hardware test generation, we contrast
    our framework with random testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We conducted experiments on simple- and medium-level DUTs, utilizing GPT-4
    and GPT-4-Turbo as language models. We also performed five trials for each experiment.
    As illustrated in Figure [5](#S3.F5 "Figure 5 ‣ III-C Comparison against Random
    Testing ‣ III Evaluation ‣ VerilogReader: LLM-Aided Hardware Test Generation")
    (log scale), LLM achieved 100% coverage using significantly fewer inputs than
    random testing. The limitations of random testing became especially apparent in
    sequential designs with elusive branches, often failing to achieve full coverage
    within one-minute timeframe. In contrast, LLM could expediently reach these branches
    with their capacity for circuit logic analysis. Interestingly, despite GPT-4-Turbo’s
    purported superiority, it demonstrated a similar capability to GPT-4 in hardware
    test generation tasks in our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6ec78b76749ac99d33c6704b5670089.png)![Refer to caption](img/43595dd29273760c910ac68929fbf957.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of LLM-aided test generation and random testing.'
  prefs: []
  type: TYPE_NORMAL
- en: III-D DUT Explanation Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [II-D](#S2.SS4 "II-D DUT Explainer ‣ II Approach ‣ VerilogReader:
    LLM-Aided Hardware Test Generation"), we introduce two optimization methods in
    DUT Explainer module that aim to improve LLM’s understanding of hardware design.
    Beyond providing LLM with the original Verilog code, we can supplement this with
    Design Description or Test Guidance. The former is generated by GPT-4 in our experiment,
    while the latter is manually written. These resources can be accessed in our open-source
    project.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We carried out experiments on medium-level DUTs using GPT-4, with each experiment
    conducted five times. The results, presented in Figure [6](#S3.F6 "Figure 6 ‣
    III-D DUT Explanation Optimization ‣ III Evaluation ‣ VerilogReader: LLM-Aided
    Hardware Test Generation"), indicate that the inclusion of a LLM-generated Design
    Description in the prompt improved LLM’s understanding of the design during test
    generation. However, the impact of Test Guidance was not uniformly beneficial.
    In designs like m05 and m06, the guidance inadvertently reduced the diversity
    of LLM-generated input, causing an over-reliance on our guidance and consequently
    stifling its capacity for self-exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ab460ac72a1becc666c1c4d27869fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Effect of design description and test guidance.'
  prefs: []
  type: TYPE_NORMAL
- en: III-E LLM Reading Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our previous experiment, we evaluated LLM’s proficiency in generating tests
    for simple- and medium-level hardware designs, with the most complex designs consisting
    of around 100 lines of Verilog code. To explore the upper bounds of current LLM’s
    capabilities for test generation, we introduced a complex level in our benchmark
    and employed FSMs with varying numbers of states as DUTs. Given that the largest
    design exceeded 500 lines of code and surpassed GPT-4’s input length limitation,
    we chose GPT-4-Turbo for this experiment, conducting three trials and calculating
    the average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S3.F7 "Figure 7 ‣ III-E LLM Reading Scalability ‣ III Evaluation
    ‣ VerilogReader: LLM-Aided Hardware Test Generation") illustrates the outcome
    of the experiment. It is evident that as the DUT scalability escalates, the quality
    of test generation precipitously declines. For an FSM with 16 states, nearly 100%
    line coverage was achieved after 20 iterations of LLM calls. However, for larger
    FSM designs with over 64 states, the coverage cannot exceed 50%. This reveals
    the LLM’s inadequacies in directly processing large Verilog designs and performing
    intricate inferences for test generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8cce43a28f6117536e59bc849b9d4fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: LLM’s performance in test generation for large-scale FSMs.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While LLM demonstrates competence in understanding simple- and medium-level
    DUTs, their performance diminishes with complex-level benchmarks and industry-scale
    hardware designs. The aspiration to employ LLM in an end-to-end manner for such
    designs is challenging. A substantial journey lies ahead before LLM can surpass
    a human hardware expert, especially in the context of Verilog code comprehension
    and its subsequent application in diverse EDA tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One potential solution to enable the application of LLM in real-world hardware
    verification is to enhance our DUT Explainer. By providing a more comprehensive
    high-level abstraction of the design and the verification intentions, we can guide
    LLM to view test generation tasks from a more macroscopic perspective. Our LLM-aided
    framework offers the opportunity for users to seamlessly incorporate help information
    during the hardware CDG process. LLM could facilitate the translation of these
    guidance information from natural language into actual hardware stimuli, thereby
    alleviating the workload of hardware verification engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, future research could focus on merging LLM with other structural
    AI techniques. Verilog’s highly structured nature, characterized by a multitude
    of concurrent always blocks and module hierarchies, presents a significant challenge
    for LLM’s decipherment. However, these structures may be more easily understood
    by a Graph Neural Network (GNN) [[8](#bib.bib8), [18](#bib.bib18), [19](#bib.bib19)].
    Therefore, the combination of LLM for regional semantic interpretation and GNN
    for structural interpretation could present a promising strategy to enhance the
    scalability of AI hardware understanding capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our research primarily investigates the application of LLM in understanding
    Verilog designs and generating test inputs to achieve code coverage closure. We
    have constructed a suite of benchmarks comprising basic combinational and sequential
    circuits to assess our framework’s efficacy. To enhance LLM’s comprehension of
    a given DUT and the test generation task, we have introduced Coverage Explainer
    and DUT Explainer to enrich the prompt. Experimental results demonstrate that
    the LLM is capable of generating inputs and achieves full code coverage for DUTs
    of simple and medium complexity in our benchmarks. Future research could focus
    on enhancing the abstraction level of guidance information provided to LLM, or
    integrating LLM with GNN to capture both semantic and structural information of
    DUTs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. Kern and M. R. Greenstreet, “Formal verification in hardware design:
    a survey,” *TODAES*, vol. 4, no. 2, pp. 123–193, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] W. K. Lam, *Hardware design verification: simulation and formal method-based
    approaches*.   Prentice Hall PTR, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. Fine and A. Ziv, “Coverage directed test generation for functional verification
    using Bayesian networks,” in *DAC*, Jun 2003, pp. 286–291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Li, K. Gent, and M. S. Hsiao, “Design validation of RTL circuits using
    evolutionary swarm intelligence,” in *ITC*, Nov 2012, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Wang, H. Zhu, P. Popli, Y. Xiao, P. Bodgan, and S. Nazarian, “Accelerating
    coverage directed test generation for functional verification: A neural network-based
    framework,” in *GLSVLSI*, May 2018, pp. 207–212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] K. Laeufer, J. Koenig, D. Kim, J. Bachrach, and K. Sen, “RFUZZ: Coverage-directed
    fuzz testing of RTL on FPGAs,” in *ICCAD*, 2018, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. Canakci, L. Delshadtehrani, and F. Eris, “DirectFuzz: Automated test
    generation for RTL designs using directed graybox fuzzing,” in *DAC*, 2021, pp.
    529–534.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Vasudevan, W. J. Jiang, D. Bieber, R. Singh, C. R. Ho, C. Sutton *et al.*,
    “Learning semantic representations to verify hardware designs,” in *NeurIPS*,
    vol. 34, 2021, pp. 23 491–23 504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri, “AutoChip:
    Automating HDL generation using LLM feedback,” *arXiv preprint arXiv:2311.04887*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and
    S. Garg, “VeriGen: A large language model for Verilog code generation,” *TODAES*,
    vol. 29, no. 3, pp. 1–31, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt, S. Thakur, R. Karri, and
    J. Rajendran, “LLM-assisted generation of hardware assertions,” *arXiv preprint
    arXiv:2306.14027*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] W. Fang, M. Li, M. Li, Z. Yan, S. Liu, H. Zhang, and Z. Xie, “AssertLLM:
    Generating and evaluating hardware verification assertions from design specifications
    via multi-LLMs,” *arXiv preprint arXiv:2402.00386*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] B. Ahmad, S. Thakur, B. Tan, R. Karri, and H. Pearce, “Fixing hardware
    security bugs with large language models,” *arXiv preprint arXiv:2302.01215*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Zhang, G. Chadwick, H. McNally, Y. Zhao, and R. Mullins, “LLM4DV: Using
    large language models for hardware test stimuli generation,” *arXiv preprint arXiv:2310.04535*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Takamaeda-Yamazaki, “Pyverilog: A Python-based hardware design processing
    toolkit for Verilog HDL,” in *ARC*.   Springer International Publishing, Apr 2015,
    pp. 451–460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. Snyder, “Verilator,” https://www.veripool.org/wiki/verilator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “GPT-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Shi, H. Pan, S. Khan, M. Li, Y. Liu, J. Huang, H.-L. Zhen, M. Yuan,
    Z. Chu, and Q. Xu, “Deepgate2: Functionality-aware circuit representation learning,”
    in *ICCAD*, 2023, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] B. Jin, G. Liu, C. Han, M. Jiang, H. Ji, and J. Han, “Large language models
    on graphs: A comprehensive survey,” *arXiv preprint arXiv:2312.02783*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
