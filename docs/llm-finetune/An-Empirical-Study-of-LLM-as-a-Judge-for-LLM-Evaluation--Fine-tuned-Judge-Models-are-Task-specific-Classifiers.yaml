- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02839](https://ar5iv.labs.arxiv.org/html/2403.02839)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hui Huang¹²²2Contribution during internship at Baidu Inc., Yingqi Qu², Jing
    Liu², Muyun Yang¹³³3Corresponding Authors., Tiejun Zhao¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Faculty of Computing, Harbin Institute of Technology, Harbin, China
  prefs: []
  type: TYPE_NORMAL
- en: ²Baidu Inc., Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: huanghui@stu.hit.edu.cn, {quyingqi, liujing46}@baidu.com,
  prefs: []
  type: TYPE_NORMAL
- en: '{yangmuyun, tjzhao}@hit.edu.cn;'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, there has been a growing trend of utilizing Large Language Model (LLM)
    to evaluate the quality of other LLMs. Many studies have employed proprietary
    close-source models, especially GPT4, as the evaluator. Alternatively, other works
    have fine-tuned judge models based on open-source LLMs as the evaluator. In this
    study, we conduct an empirical study of different judge models on their evaluation
    capability. Our findings indicate that although the fine-tuned judge models achieve
    high accuracy on in-domain test sets, even surpassing GPT4, they are inherently
    task-specific classifiers, and their generalizability and fairness severely underperform
    GPT4.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, the evaluation for Large-scale Language Models (LLMs) has drawn considerate
    attention of research community Liang et al. ([2022](#bib.bib8)); Chang et al.
    ([2023](#bib.bib2)). As the capabilities of LLMs continue to develop across various
    tasks, it is essential to evaluate them from comprehensive perspectives Qin et al.
    ([2023](#bib.bib11)). However, traditional evaluation metrics for generative models,
    such as BLEU and ROUGE, only capture limited aspects of a model’s performance
    Mathur et al. ([2020](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3bb320e41d4aa9c44b6487826c1399c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The general training and inference procedure of fine-tuned judge
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Some research has proposed LLM-as-a-Judge Li et al. ([2023b](#bib.bib7)); Zheng
    et al. ([2023](#bib.bib19)), namley utilizing proprietary LLMs, especially GPT4
    Achiam et al. ([2023](#bib.bib1)), to evaluate the LLM’s response. By defining
    evaluation schemes in the prompt template, LLMs can leverage their instruction-following
    ability to provide reliable evaluation. For example, Li et al. ([2023b](#bib.bib7))
    constructed a test set containing 805 questions and used the win rate compared
    with text-davinci-003 as the evaluation result, which is determined by GPT4\.
    Zheng et al. ([2023](#bib.bib19)) developed 80 multi-round test questions covering
    eight common areas, and then automatically scored the model’s answers using GPT4\.
    Their results reveal that strong LLM-based evaluators can achieve a high agreement
    rate among human experts, establishing a foundation for LLM-as-a-Judge.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Foundation | Instruction | Response | Annotation | Evaluation Scheme
    | Testset |'
  prefs: []
  type: TYPE_TB
- en: '| JudgeLM |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhu et al., [2023b](#bib.bib21)) | Vicuna | Instruct Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| (Alpaca-GPT4, |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly-15K…) | 11 models |'
  prefs: []
  type: TYPE_TB
- en: '| (Alpaca,Vicuna…) | GPT4 | Pairwise Grading | GPT4 |'
  prefs: []
  type: TYPE_TB
- en: '| PandaLM |'
  prefs: []
  type: TYPE_TB
- en: '| (Wang et al., [2024](#bib.bib16)) | LLaMA | Alpaca 52K | 5 models |'
  prefs: []
  type: TYPE_TB
- en: '| (LLaMA, Bloom…) | GPT3.5 | Pairwise Selection | Human |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-J |'
  prefs: []
  type: TYPE_TB
- en: '| (Li et al., [2023a](#bib.bib6)) | LLaMA2-chat | Preference Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| (Chatbot Arena, |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI WebGPT…) | Preference Datasets | Human | Pairwise Selection |'
  prefs: []
  type: TYPE_TB
- en: '| Pointwise Grading | Human |'
  prefs: []
  type: TYPE_TB
- en: '| Prometheus |'
  prefs: []
  type: TYPE_TB
- en: '| (Kim et al., [2023](#bib.bib5)) | LLaMA2-chat | GPT4 Generated | GPT4 Generated
    | GPT4 | Pointwise Grading | GPT4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Detailed statistics of the four fine-tuned judge models, which is
    the foundation of our cross-validation. All the four models are open-source, with
    their training and test data also publicly released.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, relying on external API for evaluation may introduce consideration
    about privacy leakage, and the opacity of API models also challenges the evaluation
    reproducibility. To address these issues, several fine-tuned judge models are
    proposed, relying on open-source foundation models and data constructed from either
    GPT4 or human annotation, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers"). For instance, PandaLM Wang et al. ([2024](#bib.bib16))
    constructs data based on Alpaca instructions and GPT3.5 annotation, and then fine-tunes
    LLaMA-7B Touvron et al. ([2023](#bib.bib14)) as a judge model. JudgeLM Zhu et al.
    ([2023b](#bib.bib21)) constructs data from GPT4 annotations and fine-tunes a scalable
    judge model. Auto-J Li et al. ([2023a](#bib.bib6)) constructs judgement data upon
    multiple scenarios to train a generative judge model, which can provide both judgement
    and critic. Prometheus Kim et al. ([2023](#bib.bib5)) defines thousands of evaluation
    criteria and finetunes a fine-grained judge model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we conduct an empirical study of the evaluation capabilities
    of judge models. We conduct extrapolated evaluations among available judge models
    and meta-evaluation testsets. Experiment results indicate that while the fine-tuned
    judge models achieve superior accuracy on their respective in-domain test sets,
    they still exhibit limitations in the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuned judge model is inherently a classification model;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuned judge model is overfitted to specific evaluation schemes;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuned judge model is biased towards superficial quality;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To draw a conclusion, the fine-tuned judge models should be used only in similar
    evaluation scenarios, and can not serve as a general substitution for GPT4 in
    terms of LLM evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 2 How Far can Fine-tuned Judges Go?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The typical process for finetuning a judge model consists of the following
    three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Data Collection. The training data generally comprises three components:
    instructions, responses and evaluations. The instructions are typically obtained
    from instruction datasets, with the responses generated by various representative
    models, and the evaluations can be derived from either GPT4 or human annotation;'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Prompt Designing. The prompt template can be structured in various
    ways depending on the evaluation scheme, such as pairwise selection (which aims
    to select the better one from a pair of responses), pointwise grading (which aims
    to score a single reference), etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Model Fine-tuning. Using the designed prompt and collected data, the
    training process of the judge model typically follows the instruction fine-tuning
    paradigm Ouyang et al. ([2022](#bib.bib10)). The model is fed with a instruction
    alongside answer(s) for yielding output comprising evaluation results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While current judge models mostly self-claim exceeding the evaluation capability
    of GPT4, in this work, we make an cross validation based on four representative
    works as listed in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ An Empirical Study
    of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific
    Classifiers"). Our findings are presented in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Train | JudgeLM-test | PandaLM-test | Auto-J-test | Prometheus-test
    |'
  prefs: []
  type: TYPE_TB
- en: '| accuracy | F1 | accuracy | F1 | agreement | pearson-ind | pearson-ood |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 3.5 | 73.83 | 52.85 | 62.96 | 58.20 | 42.7 | 0.636 | 0.563 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 4-0613 | 85.28 | 76.87 | 78.68 | 73.24 | 56.3 | 0.742 | 0.743 |'
  prefs: []
  type: TYPE_TB
- en: '| Released Models^† | 79.02 | 71.87 | 67.57 | 57.49 | 54.6 | 0.864 | 0.869
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B | generation^‡ | 82.44 | 71.77 | 72.37 | 60.78 | 47.6 | 0.826 |
    0.815 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B | classification^‡ | 82.16 | 70.07 | 70.87 | 60.34 | 46.8 | 0.846
    | 0.831 |'
  prefs: []
  type: TYPE_TB
- en: '| DeBERTa | classification^‡ | 81.30 | 68.34 | 72.27 | 51.75 | 31.7 | 0.835
    | 0.813 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison of evaluators trained with different architectures. Results
    with ^† are from evaluating the four publicly released models on their respective
    testsets, and results with ^‡ are from evaluating models trained by us. Notice
    all our LLM-based evaluators are trained from Vicuna-7B Chiang et al. ([2023](#bib.bib3)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Inherently a Classification Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we do not consider critic generation (which is seldom used in system-level
    evaluation), the LLM evaluation is inherently a classification (or regression)
    task. In contrast to the current judge models trained in a generation-style, we
    train four classification (regression) models based the four groups of data in
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers")¹¹1The
    training settings and prompt templates are presented in Appendix [A.1](#A1.SS1
    "A.1 Training Settings ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") due
    to space limitation.. We also train four classification models based on DeBERTaV3-large
    He et al. ([2023](#bib.bib4)) on the same data, which is 20 times smaller than
    the 7B version of LLaMA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [2](#S2.T2 "Table 2 ‣ 2 How Far can Fine-tuned Judges Go?
    ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers"), the classification model performs equally well
    as the generation model for both pairwise selection and pointwise grading. The
    powerful generation ability of LLMs hardly brings any improvement to the evaluation
    accuracy, as they are trained on the same data for the same objective. Moreover,
    the DeBERTa-based evaluator achieves comparable performance with the LLM-based
    evaluators²²2The only exception is on Auto-J-test, which is possibly due to a
    large proportion of the test data exceeds 512 (the maximum context length of DeBERTa).,
    which might be argued for that the encoder-only architecture is more suitable
    for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also analyze the correlation between different predictions made by different
    evaluators. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Inherently a Classification
    Model ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") and
    [3](#S2.F3 "Figure 3 ‣ 2.1 Inherently a Classification Model ‣ 2 How Far can Fine-tuned
    Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
    Judge Models are Task-specific Classifiers"), the correlation among different
    classification models is much closer than their correlation with GPT4\. Different
    as they are in architectures, all three models are inherently classifiers fitting
    to the same set of supervision, leading to similar evaluation outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79767ba21d1f44665a1bdd6ee354c7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The F1 score between the predictions of different evaluators on JudgeLM
    testset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1487d8392bc7307cd990606a7dd6163.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The pearson coefficient between the predictions of different evaluators
    on Prometheus testset.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Overfitting to Evaluation Scheme
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | JudgeLM-test | PandaLM-test | Auto-J-test | Average |'
  prefs: []
  type: TYPE_TB
- en: '| accuracy | F1 | accuracy | F1 | agreement |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 3.5 | 73.83 | 52.85 | 62.96 | 58.20 | 42.7 | 59.83 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 4-0613 | 85.28 | 76.87 | 78.68 | 73.24 | 56.3 | 73.42 |'
  prefs: []
  type: TYPE_TB
- en: '| JudgeLM-7B | 79.02 | 71.87 | 70.97 | 67.59 | 46.6 | 65.53 |'
  prefs: []
  type: TYPE_TB
- en: '| PandaLM-7B | 65.24 | 47.42 | 67.57 | 57.49 | 40.0 | 57.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-J-13B | 72.86 | 57.60 | 71.47 | 61.01 | 54.6 | 66.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Prometheus-13B | 54.24 | 50.04 | 45.25 | 43.58 | 47.8 | 49.10 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o trans | 24.58 | 23.39 | 29.03 | 27.92 | 16.2 | 23.26 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Results of evaluators on pairwise selection. Notice Prometheus can
    be transformed for pairwise selection by grading two answers twice and compare
    the scores, therefore we release both results with and without transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Prometheus-test-ind | Prometheus-test-ood | Average |'
  prefs: []
  type: TYPE_TB
- en: '| pearson | kendalltau | spearman | pearson | kendalltau | spearman |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 3.5 | 0.636 | 0.536 | 0.617 | 0.563 | 0.453 | 0.521 | 0.600 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 4-0613 | 0.742 | 0.659 | 0.747 | 0.743 | 0.660 | 0.747 | 0.743 |'
  prefs: []
  type: TYPE_TB
- en: '| Prometheus-13B | 0.864 | 0.788 | 0.863 | 0.869 | 0.789 | 0.869 | 0.867 |'
  prefs: []
  type: TYPE_TB
- en: '| JudgeLM-7B | 0.649 | 0.647 | 0.739 | 0.610 | 0.602 | 0.690 | 0.630 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o trans | 0.398 | 0.371 | 0.416 | 0.384 | 0.371 | 0.419 | 0.391 |'
  prefs: []
  type: TYPE_TB
- en: '| PandaLM-7B | 0.417 | 0.368 | 0.423 | 0.386 | 0.333 | 0.383 | 0.402 |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-J-13B | 0.614 | 0.526 | 0.608 | 0.591 | 0.504 | 0.580 | 0.603 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results of evaluators on pointwise grading. Notice JudgeLM can be
    transformed for pointwise grading by adding the reference as the first answer,
    therefore we release both results with and without transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | MTBench |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| accuracy | precision | recall | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 4-0613 | 66.9 | 63.8 | 62.2 | 61.9 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| JudgeLM-7B | 48.7 | 52.0 | 49.7 | 48.7 |'
  prefs: []
  type: TYPE_TB
- en: '| PandaLM-7B | 55.2 | 52.6 | 49.4 | 46.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-J-13B | 51.7 | 50.2 | 46.8 | 43.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Prometheus-13B | 53.2 | 49.6 | 48.4 | 47.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Results of evaluators on multi-turn evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most appealing attribute of LLMs is their generalization ability,
    enabling them to execute various task defined by various instructions Zhu et al.
    ([2023a](#bib.bib20)). Under the case of LLM evaluation, the instruction can also
    be formed in various schemes: pairwise selection, pointwise grading, etc. Since
    different judge models are fine-tuned on different schemes, we could verify their
    evaluation capability under the scheme defined by others. Specifically, we cross-validate
    the judge models on each other’s testset³³3We applying their publicly released
    checkpoints with predefined prompts. For details please refer to Appendix [A.2](#A1.SS2
    "A.2 Prompt Templates ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers")..'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [3](#S2.T3 "Table 3 ‣ 2.2 Overfitting to Evaluation Scheme
    ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for
    LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") and [4](#S2.T4
    "Table 4 ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges
    Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
    Models are Task-specific Classifiers"), all four models perform the best on their
    own training settings, respectivly, with results comparable with GPT4\. However,
    if we employ a model on evaluation schemes where it is not trained on⁴⁴4For example,
    using a pairwise model (such as PandaLM or JudgeLM) for pointwise grading (such
    as Promethues testset), or using a pointwise model (such as Promethues) for pairwise
    selection (such as PandaLM or JudgeLM testsets)., the evaluation performance would
    drop by a large margin. On the contrary, GPT4 consistently exhibits superior performance
    across various evaluation schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also validate the judge models on MT-bench Zheng et al. ([2023](#bib.bib19)),
    which is a multi-turn meta-evaluation dataset. As can be seen in Table [5](#S2.T5
    "Table 5 ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges
    Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
    Models are Task-specific Classifiers"), while the four models are all trained
    for single-turn evaluation, they underperforms GPT4 on MT-bench by a large margin.
    This demonstrates that the finetuned judge models are overfitted to their respective
    evaluation schemes and lack generalibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Biased Towards Superficial Quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, there has been a few researches on the bias of LLM-based evaluators,
    namely the evaluator would favor more verbose answers, or answers with similar
    format Wang et al. ([2023](#bib.bib15)); Saito et al. ([2023](#bib.bib13)). To
    address this issue, Zeng et al. ([2023](#bib.bib18)) proposed LLMBar as a testbed
    for the fairness of evaluators. It comprises one natural testset (Natural) and
    four adversarial testsets (Neighbor, Manual, GPTOut, GPTInst), and the adversarial
    testsets consist of paired outputs with a correct answer and a smooth, coherent
    but inconsistent answer. We evaluate the judge models on LLMBar\@footnotemark,
    and the results are shown in Table [6](#S2.T6 "Table 6 ‣ 2.3 Biased Towards Superficial
    Quality ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers").'
  prefs: []
  type: TYPE_NORMAL
- en: It seems that the fine-tuned judge models are severely biased to superficial
    quality such as formality or verbosity, while neglecting crucial properties such
    as instruction following, resulting an accuracy even worse than random guess on
    the adversarial testsets. On the other hand, GPT4 does not over-rely on the superficial
    features and achieves decent accuracy on all the testsets. This also verifies
    that the fine-tuned judge models are inherently classifiers overfitted to the
    training data, relying on spurious statistical features for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: It deserves noticing that the DeBERTa-based evaluator also outperforms the LLM-based
    evaluator by a large margin in terms of fairness. This inspires us that the bias
    of LLM-based evaluator may come from the casual language modeling process. While
    the model is trained to generate fluent and verbose responses, it also tends to
    prefer fluent and verbose response when employed for evaluation, even if it is
    not aligned with the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | LLMBar |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Natu. | Neig. | GPTI. | GPTO. | Manu. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT 4-0613 | 93.5 | 64.2 | 76.6 | 76.6 | 75.0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| JudgeLM-7B | 62.0 | 23.1 | 26.1 | 46.8 | 28.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PandaLM-7B | 59.0 | 16.5 | 21.7 | 42.6 | 26.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-J-13B | 70.0 | 20.9 | 21.7 | 46.8 | 23.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Prometheus-7B | 53.0 | 22.4 | 17.4 | 27.7 | 32.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DeBERTa | 62.0 | 26.9 | 42.4 | 55.3 | 34.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Accuracy of evaluators on bias evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we empirically study on the judge models for LLM evaluation. As
    revealed in our experiments, despite achieving superior evaluation performance
    on the in-domain testset, the fine-tuned judge models underperforms GPT4 in terms
    of generalibility and fairness by a large margin, which we believe cannot be amended
    by simply finetuning on more evaluation data. It is advisable to exercise caution
    when leveraging fine-tuned judge models for evaluation in real applications, watching
    for the overlap between the evaluation scenario and the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our work still has some limitations: 1) Due to the lack of related work, we
    primarily relied on cross-validation to assess the generalizability of the four
    fine-tuned judge models. To conduct a more thorough evaluation of the generalizability,
    it would be beneficial to incorporate additional independent testsets encompassing
    a broader range of evaluation schemes, such as reference augmentation and domain-specific
    evaluation. 2) The work of Zeng et al. ([2023](#bib.bib18)) is only a general
    assessment of evaluator bias, and we did not include fine-grained assessment for
    different biases, such as formality bias, verbosity bias, etc. 3) Due to time
    and resource constraints, we did not incorporate manual inspection into the meta-evaluation
    process. Including human evaluators would enhance the credibility of our claims.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2023) Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. [DeBERTav3:
    Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled
    embedding sharing](https://openreview.net/forum?id=sE7-XhLxHA). In *The Eleventh
    International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre,
    Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023.
    Prometheus: Inducing fine-grained evaluation capability in language models. *arXiv
    preprint arXiv:2310.08491*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint
    arXiv:2310.05470*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathur et al. (2020) Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020.
    [Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation
    evaluation metrics](https://doi.org/10.18653/v1/2020.acl-main.448). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 4984–4997, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing
    task solver? *arXiv preprint arXiv:2302.06476*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saito et al. (2023) Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto.
    2023. Verbosity bias in preference labeling by large language models. *arXiv preprint
    arXiv:2310.10076*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are
    not fair evaluators. *ArXiv*, abs/2305.17926.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang
    Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun
    Zhang, and Yue Zhang. 2024. Pandalm: An automatic evaluation benchmark for llm
    instruction tuning optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-art natural language
    processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2023) Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal,
    and Danqi Chen. 2023. Evaluating large language models at evaluating instruction
    following. *arXiv preprint arXiv:2310.07641*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023a) Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing
    Xie. 2023a. Promptbench: A unified library for evaluation of large language models.
    *arXiv preprint arXiv:2312.07910*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023b) Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023b. Judgelm:
    Fine-tuned large language models are scalable judges. *arXiv preprint arXiv:2310.17631*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Training Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in Section [2.1](#S2.SS1 "2.1 Inherently a Classification Model
    ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for
    LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"), we fine-tune
    our own judge models based on the four groups of data (JudgeLM Zhu et al. ([2023b](#bib.bib21)),
    PandaLM Wang et al. ([2024](#bib.bib16)), Auto-J Li et al. ([2023a](#bib.bib6)),
    Prometheus Kim et al. ([2023](#bib.bib5))), both in generation-style and in classification-style,
    for the purpose of comparison. We train all the models on NVIDIA A100-80GB GPUs
    with Huggingface-transformers Wolf et al. ([2020](#bib.bib17)) and DeepSpeed Rasley
    et al. ([2020](#bib.bib12)). Detailed hyper-parameters are presented in Table
    [7](#A1.T7 "Table 7 ‣ A.2 Prompt Templates ‣ Appendix A Appendix ‣ An Empirical
    Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific
    Classifiers"). Notice when comparing generation and classification models, we
    adopt the same prompt template and same hyper-parameters, with the only difference
    lies in the prediction method. For generation model, the prediction head reused
    the pretrained language model head, and is trained akin to the process of language
    modeling. For classification (regression) model, the prediction head is newly
    initialized as a linear projection layer, and is decoupled from the language modeling
    process⁵⁵5Please refer to the class AutoModelForSequence Classification in Huggingface
    library for more details., as shown in Figure [4](#A1.F4 "Figure 4 ‣ A.2 Prompt
    Templates ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge for LLM
    Evaluation: Fine-tuned Judge Models are Task-specific Classifiers").'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompt Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in Section [2.1](#S2.SS1 "2.1 Inherently a Classification Model
    ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for
    LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"), [2.2](#S2.SS2
    "2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges Go? ‣
    An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers") and [2.3](#S2.SS3 "2.3 Biased Towards Superficial
    Quality ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"), we
    take the publicly released checkpoints of the four models and validate their performance.
    We use the same prompt templates as defined by the four open-source models, as
    presented from Figure [5](#A1.F5 "Figure 5 ‣ A.2 Prompt Templates ‣ Appendix A
    Appendix ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
    Judge Models are Task-specific Classifiers") to Figure [12](#A1.F12 "Figure 12
    ‣ A.2 Prompt Templates ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"). For
    JudgeLM and PandaLM, their predefined prompts are in the form of pairwise selection,
    and we make slight modifications to apply them on pointwise grading. For Prometheus,
    the predefined prompt is in the form of pointwise grading, and we make slight
    modifications to apply it on pairwise selection. For Auto-J, they predefined prompts
    both for pairwise selection and pointwise grading.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Configuration | Vicuna-based | DeBERTa-based |'
  prefs: []
  type: TYPE_TB
- en: '| base model | Vicuna-7B | DeBERTaV3-large |'
  prefs: []
  type: TYPE_TB
- en: '| max length | 2048 | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| learning rate | 2e-5 | 2e-5 |'
  prefs: []
  type: TYPE_TB
- en: '| learning rate schedule | cosine decay | cosine decay |'
  prefs: []
  type: TYPE_TB
- en: '| optimizer | AdamW | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '| AdamW beta1 | 0.9 | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| AdamW beta2 | 0.999 | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| weight decay | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPU nums | 8 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| training epochs | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| batch size | 128 | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| warmup ratio | 0.003 | 0.003 |'
  prefs: []
  type: TYPE_TB
- en: '| numerical precision | bf16, tf32 | fp16 |'
  prefs: []
  type: TYPE_TB
- en: '| ZeRO optimizer | stage 2 | None |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Configurations of the judge models fine-tuned by us in Section [2.1](#S2.SS1
    "2.1 Inherently a Classification Model ‣ 2 How Far can Fine-tuned Judges Go? ‣
    An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers"). Both classification and generation judge models
    leverage the same group of configs based on their foundation model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b879619b9d6774d8bef4568a3e5ec0de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The architecture of classification-based judge model. The major difference
    lies in the prediction head, where a new classification (regression) head is initialized
    for predicting the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d845900076de53da237e0f85bc760a90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Prompt template for JudgeLM applied for pairwise selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22c367ebf8e4d8e173ead67fd45c638d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Prompt template for JudgeLM applied for pointwise grading.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5f5b575d89ef89b5be8ddd3cc0b765f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Prompt template for PandaLM applied for pairwise selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d589becaf202d696cc55f94d8f145a38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Prompt template for PandaLM applied for pointwise grading.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ee3823c2c3e1615e0307d9360f9fea0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Prompt template for Auto-J applied for pairwise selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/28cd1c709a6c10de05edd83b86183864.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Prompt template for Auto-J applied for pointwise grading.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dcdbee053c57d6607ec9c4c12b46f80b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Prompt template for Prometheus applied for pairwise selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5bc2fd1f3f1300dae9acdda4dc7f35b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Prompt template for Prometheus applied for pointwise grading.'
  prefs: []
  type: TYPE_NORMAL
