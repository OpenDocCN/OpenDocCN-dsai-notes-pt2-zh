- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:34:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年9月8日 18:34:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从基础到突破的最终指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13296](https://ar5iv.labs.arxiv.org/html/2408.13296)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13296](https://ar5iv.labs.arxiv.org/html/2408.13296)
- en: Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and Arsalan
    Shahid
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, 和 Arsalan Shahid
- en: '@ CeADAR Connect Group CeADAR: Ireland’s Centre for AI, University College
    Dublin, Belfield, Dublin, Ireland'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '@ CeADAR Connect Group CeADAR: 爱尔兰都柏林大学人工智能中心，贝尔菲尔德，都柏林，爱尔兰'
- en: '{ venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid } @ ucd.ie(  August
    2024)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{ venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid } @ ucd.ie(
    2024年8月)'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This technical report thoroughly examines the process of fine-tuning Large Language
    Models (LLMs), integrating theoretical insights and practical applications. It
    begins by tracing the historical development of LLMs, emphasising their evolution
    from traditional Natural Language Processing (NLP) models and their pivotal role
    in modern AI systems. The analysis differentiates between various fine-tuning
    methodologies, including supervised, unsupervised, and instruction-based approaches,
    underscoring their respective implications for specific tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术报告全面审视了大型语言模型（LLMs）的微调过程，整合了理论洞察和实际应用。报告首先追溯了LLMs的历史发展，强调了它们从传统自然语言处理（NLP）模型的演变以及在现代人工智能系统中的关键作用。分析区分了各种微调方法，包括监督、无监督和基于指令的方法，突出了它们在特定任务中的影响。
- en: A structured seven-stage pipeline for LLM fine-tuning is introduced, covering
    the complete lifecycle from data preparation to model deployment. Key considerations
    include data collection strategies, handling of imbalanced datasets, model initialisation,
    and optimisation techniques, with a particular focus on hyperparameter tuning.
    The report also highlights parameter-efficient fine-tuning methods such as Low-Rank
    Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with
    optimal model performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍了一种结构化的七阶段LLM微调流程，涵盖了从数据准备到模型部署的完整生命周期。关键考虑因素包括数据收集策略、处理不平衡数据集、模型初始化和优化技术，特别关注超参数调优。报告还强调了如低秩适应（LoRA）和半微调等参数高效的微调方法，这些方法在资源限制与模型性能优化之间取得了平衡。
- en: The exploration extends to advanced fine-tuning techniques and configurations
    like memory fine-tuning, Mixture of Experts (MoE) and Mixture of Agents (MoA),
    demonstrating how these methods harness specialised networks and multi-agent collaboration
    for improved outcomes. Proximal Policy Optimisation (PPO) and Direct Preference
    Optimisation (DPO) are discussed as innovative approaches to aligning models with
    human preferences, while the benefits of pruning and routing optimisations are
    examined for enhancing efficiency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 探索扩展到先进的微调技术和配置，如记忆微调、专家混合（MoE）和代理混合（MoA），展示了这些方法如何利用专门的网络和多代理协作以提高结果。讨论了近端策略优化（PPO）和直接偏好优化（DPO）作为将模型与人类偏好对齐的创新方法，同时探讨了修剪和路由优化的好处，以提高效率。
- en: In the latter sections, the report delves into validation frameworks, post-deployment
    monitoring, and optimisation techniques for inference. It also addresses the deployment
    of LLMs on distributed and cloud-based platforms. Additionally, cutting-edge topics
    such as multimodal LLMs and fine-tuning for audio and speech processing are covered,
    alongside emerging challenges related to scalability, privacy, and accountability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续部分，报告深入探讨了验证框架、部署后的监控以及推理优化技术。还涉及了在分布式和基于云的平台上部署LLMs。此外，报告涵盖了前沿主题，如多模态LLMs以及针对音频和语音处理的微调，同时讨论了与可扩展性、隐私和问责制相关的新兴挑战。
- en: This report aims to serve as a comprehensive guide for researchers and practitioners,
    offering actionable insights into fine-tuning LLMs while navigating the challenges
    and opportunities inherent in this rapidly evolving field.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告旨在为研究人员和从业人员提供全面的指南，提供有关微调LLMs的可操作洞察，同时应对这一快速发展领域中的挑战和机遇。
- en: Contents
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#Ch1 "In The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](#Ch1 "从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本 1.0）")'
- en: '[1.1 Background of Large Language Models (LLMs)](#Ch1.S1 "In Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.1 大型语言模型 (LLMs) 背景](#Ch1.S1 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.2 Historical Development and Key Milestones](#Ch1.S2 "In Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.2 历史发展与关键里程碑](#Ch1.S2 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs](#Ch1.S3
    "In Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3 从传统NLP模型到最先进的LLMs](#Ch1.S3 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.1 Statistical Language Models (SLMs)](#Ch1.S3.SS1 "In 1.3 Evolution from
    Traditional NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.1 统计语言模型 (SLMs)](#Ch1.S3.SS1 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.2 Neural Language Models (NLMs)](#Ch1.S3.SS2 "In 1.3 Evolution from Traditional
    NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.2 神经语言模型 (NLMs)](#Ch1.S3.SS2 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.3 Pre-trained Language Models (PLMs)](#Ch1.S3.SS3 "In 1.3 Evolution from
    Traditional NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.3 预训练语言模型 (PLMs)](#Ch1.S3.SS3 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.4 Large Language Models (LLMs)](#Ch1.S3.SS4 "In 1.3 Evolution from Traditional
    NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.4 大型语言模型 (LLMs)](#Ch1.S3.SS4 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.4 Overview of Current Leading LLMs](#Ch1.S4 "In Chapter 1 Introduction ‣
    The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.4 当前领先LLMs概述](#Ch1.S4 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.5 What is Fine-Tuning?](#Ch1.S5 "In Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.5 什么是微调？](#Ch1.S5 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本 1.0）")'
- en: '[1.6 Types of LLM Fine-Tuning](#Ch1.S6 "In Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6 LLM微调类型](#Ch1.S6 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.6.1 Unsupervised Fine-Tuning](#Ch1.S6.SS1 "In 1.6 Types of LLM Fine-Tuning
    ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6.1 无监督微调](#Ch1.S6.SS1 "在1.6 LLM微调类型 ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.6.2 Supervised Fine-Tuning (SFT)](#Ch1.S6.SS2 "In 1.6 Types of LLM Fine-Tuning
    ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6.2 有监督微调（SFT）](#Ch1.S6.SS2 "在1.6 LLM微调类型 ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.6.3 Instruction Fine-Tuning via Prompt Engineering](#Ch1.S6.SS3 "In 1.6
    Types of LLM Fine-Tuning ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6.3 通过提示工程进行指令微调](#Ch1.S6.SS3 "在1.6 LLM微调类型 ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.7 Pre-training vs Fine-tuning](#Ch1.S7 "In Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.7 预训练与微调](#Ch1.S7 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.8 Importance of Fine-Tuning LLMs](#Ch1.S8 "In Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.8 微调LLM的重要性](#Ch1.S8 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9 Retrieval Augmented Generation (RAG)](#Ch1.S9 "In Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9 检索增强生成（RAG）](#Ch1.S9 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.1 Traditional RAG Pipeline and Steps](#Ch1.S9.SS1 "In 1.9 Retrieval Augmented
    Generation (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.1 传统RAG流程与步骤](#Ch1.S9.SS1 "在1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.2 Benefits of Using RAG](#Ch1.S9.SS2 "In 1.9 Retrieval Augmented Generation
    (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.2 使用RAG的好处](#Ch1.S9.SS2 "在1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.3 Challenges and Considerations in Serving RAG](#Ch1.S9.SS3 "In 1.9 Retrieval
    Augmented Generation (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.3 服务RAG的挑战与考虑](#Ch1.S9.SS3 "在1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.4 Use Cases and Examples](#Ch1.S9.SS4 "In 1.9 Retrieval Augmented Generation
    (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.4 用例和示例](#Ch1.S9.SS4 "在 1.9 检索增强生成（RAG） ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.9.5 Considerations for Choosing Between RAG and Fine-Tuning](#Ch1.S9.SS5
    "In 1.9 Retrieval Augmented Generation (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.5 选择 RAG 和微调的考虑因素](#Ch1.S9.SS5 "在 1.9 检索增强生成（RAG） ‣ 第 1 章 引言 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[1.10 Objectives of the Report](#Ch1.S10 "In Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10 报告目标](#Ch1.S10 "在第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.10.1 Goals and Scope](#Ch1.S10.SS1 "In 1.10 Objectives of the Report ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10.1 目标和范围](#Ch1.S10.SS1 "在 1.10 报告目标 ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.10.2 Key Questions and Issues Addressed](#Ch1.S10.SS2 "In 1.10 Objectives
    of the Report ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10.2 关键问题和解决的议题](#Ch1.S10.SS2 "在 1.10 报告目标 ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.10.3 Overview of the Report Structure](#Ch1.S10.SS3 "In 1.10 Objectives
    of the Report ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10.3 报告结构概述](#Ch1.S10.SS3 "在 1.10 报告目标 ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2 Seven Stage Fine-Tuning Pipeline for LLM](#Ch2 "In The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 七阶段 LLM 微调流程](#Ch2 "在从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[2.1 Stage 1: Dataset Preparation](#Ch2.S1 "In Chapter 2 Seven Stage Fine-Tuning
    Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 第一阶段：数据集准备](#Ch2.S1 "在第 2 章 七阶段 LLM 微调流程 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2.2 Stage 2: Model Initialisation](#Ch2.S2 "In Chapter 2 Seven Stage Fine-Tuning
    Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 第二阶段：模型初始化](#Ch2.S2 "在第 2 章 七阶段 LLM 微调流程 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2.3 Stage 3: Training Environment Setup](#Ch2.S3 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.3 第三阶段：训练环境设置](#Ch2.S3 "在第 2 章 七阶段 LLM 微调流程 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2.4 Stage 4: Partial or Full Fine-Tuning](#Ch2.S4 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.4 第4阶段：部分或完全微调](#Ch2.S4 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[2.5 Stage 5: Evaluation and Validation](#Ch2.S5 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.5 第5阶段：评估与验证](#Ch2.S5 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[2.6 Stage 6: Deployment](#Ch2.S6 "In Chapter 2 Seven Stage Fine-Tuning Pipeline
    for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.6 第6阶段：部署](#Ch2.S6 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[2.7 Stage 7: Monitoring and Maintenance](#Ch2.S7 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.7 第7阶段：监控与维护](#Ch2.S7 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3 Stage 1: Data Preparation](#Ch3 "In The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 第1阶段：数据准备](#Ch3 "在《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1 Steps Involved in Data Preparation](#Ch3.S1 "In Chapter 3 Stage 1: Data
    Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 数据准备的步骤](#Ch3.S1 "在第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.1 Data Collection](#Ch3.S1.SS1 "In 3.1 Steps Involved in Data Preparation
    ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.1 数据收集](#Ch3.S1.SS1 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.2 Data Preprocessing and Formatting](#Ch3.S1.SS2 "In 3.1 Steps Involved
    in Data Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.2 数据预处理与格式化](#Ch3.S1.SS2 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.3 Handling Data Imbalance](#Ch3.S1.SS3 "In 3.1 Steps Involved in Data
    Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.3 处理数据不平衡](#Ch3.S1.SS3 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.4 Splitting Dataset](#Ch3.S1.SS4 "In 3.1 Steps Involved in Data Preparation
    ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.4 数据集拆分](#Ch3.S1.SS4 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.2 Existing and Potential Research Methodologies](#Ch3.S2 "In Chapter 3 Stage
    1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 现有和潜在的研究方法](#Ch3.S2 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.2.1 Data Annotation](#Ch3.S2.SS1 "In 3.2 Existing and Potential Research
    Methodologies ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.1 数据标注](#Ch3.S2.SS1 "在3.2 现有和潜在的研究方法 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.2.2 Data Augmentation](#Ch3.S2.SS2 "In 3.2 Existing and Potential Research
    Methodologies ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.2 数据增强](#Ch3.S2.SS2 "在3.2 现有和潜在的研究方法 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.2.3 Synthetic Data Generation using LLMs](#Ch3.S2.SS3 "In 3.2 Existing and
    Potential Research Methodologies ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.3 使用LLMs生成合成数据](#Ch3.S2.SS3 "在3.2 现有和潜在的研究方法 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.3 Challenges in Data Preparation for Fine-Tuning LLMs](#Ch3.S3 "In Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 数据准备中的挑战](#Ch3.S3 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.4 Available LLM Fine-Tuning Datasets](#Ch3.S4 "In Chapter 3 Stage 1: Data
    Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.4 可用的LLM微调数据集](#Ch3.S4 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5 Best Practices](#Ch3.S5 "In Chapter 3 Stage 1: Data Preparation ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5 最佳实践](#Ch3.S5 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5.1 High-Quality Data Collection](#Ch3.S5.SS1 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.1 高质量数据收集](#Ch3.S5.SS1 "在3.5 最佳实践 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5.2 Effective Data Preprocessing](#Ch3.S5.SS2 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.2 有效的数据预处理](#Ch3.S5.SS2 "在3.5 最佳实践 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5.3 Managing Data Imbalance](#Ch3.S5.SS3 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.3 数据不平衡管理](#Ch3.S5.SS3 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[3.5.4 Augmenting and Annotating Data](#Ch3.S5.SS4 "In 3.5 Best Practices ‣
    Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.4 数据增强与注释](#Ch3.S5.SS4 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[3.5.5 Ethical Data Handling](#Ch3.S5.SS5 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.5 伦理数据处理](#Ch3.S5.SS5 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[3.5.6 Regular Evaluation and Iteration](#Ch3.S5.SS6 "In 3.5 Best Practices
    ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.6 定期评估与迭代](#Ch3.S5.SS6 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4 Stage 2: Model Initialisation](#Ch4 "In The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 第2阶段：模型初始化](#Ch4 "在《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.1 Steps Involved in Model Initialisation](#Ch4.S1 "In Chapter 4 Stage 2:
    Model Initialisation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 模型初始化的步骤](#Ch4.S1 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.2 Tools and Libraries for Model Initialisation](#Ch4.S2 "In Chapter 4 Stage
    2: Model Initialisation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 模型初始化的工具和库](#Ch4.S2 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.3 Challenges in Model Initialisation](#Ch4.S3 "In Chapter 4 Stage 2: Model
    Initialisation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 模型初始化中的挑战](#Ch4.S3 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.4 Tutorials](#Ch4.S4 "In Chapter 4 Stage 2: Model Initialisation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 教程](#Ch4.S4 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[5 Stage 3: Training Setup](#Ch5 "In The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 第3阶段：训练设置](#Ch5 "在《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[5.1 Steps Involved in Training Setup](#Ch5.S1 "In Chapter 5 Stage 3: Training
    Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 训练设置中的步骤](#Ch5.S1 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.2 Setting up Training Environment](#Ch5.S2 "In Chapter 5 Stage 3: Training
    Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 设置训练环境](#Ch5.S2 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.3 Defining Hyperparameters](#Ch5.S3 "In Chapter 5 Stage 3: Training Setup
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 定义超参数](#Ch5.S3 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.3.1 Methods for Hyperparameter Tuning](#Ch5.S3.SS1 "In 5.3 Defining Hyperparameters
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.1 超参数调优方法](#Ch5.S3.SS1 "在 5.3 定义超参数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4 Initialising Optimisers and Loss Functions](#Ch5.S4 "In Chapter 5 Stage
    3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4 初始化优化器和损失函数](#Ch5.S4 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.1 Gradient Descent](#Ch5.S4.SS1 "In 5.4 Initialising Optimisers and Loss
    Functions ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.1 梯度下降](#Ch5.S4.SS1 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.2 Stochastic Gradient Descent (SGD)](#Ch5.S4.SS2 "In 5.4 Initialising
    Optimisers and Loss Functions ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.2 随机梯度下降（SGD）](#Ch5.S4.SS2 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.3 Mini-batch Gradient Descent](#Ch5.S4.SS3 "In 5.4 Initialising Optimisers
    and Loss Functions ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.3 小批量梯度下降](#Ch5.S4.SS3 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.4 AdaGrad](#Ch5.S4.SS4 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.4 AdaGrad](#Ch5.S4.SS4 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.5 RMSprop](#Ch5.S4.SS5 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.5 RMSprop](#Ch5.S4.SS5 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.4.6 AdaDelta](#Ch5.S4.SS6 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.6 AdaDelta](#Ch5.S4.SS6 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.4.7 Adam](#Ch5.S4.SS7 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.7 Adam](#Ch5.S4.SS7 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的 LLM
    微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.4.8 AdamW](#Ch5.S4.SS8 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.8 AdamW](#Ch5.S4.SS8 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.5 Challenges in Training Setup](#Ch5.S5 "In Chapter 5 Stage 3: Training
    Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5 训练设置中的挑战](#Ch5.S5 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[5.6 Best Practices](#Ch5.S6 "In Chapter 5 Stage 3: Training Setup ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.6 最佳实践](#Ch5.S6 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations](#Ch6
    "In The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 第 4 阶段：微调技术和适当模型配置的选择](#Ch6 "在从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6.1 Steps Involved in Fine-Tuning](#Ch6.S1 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1 微调涉及的步骤](#Ch6.S1 "在第 6 章 第 4 阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6.2 Fine-Tuning Strategies for LLMs](#Ch6.S2 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2 LLM 的微调策略](#Ch6.S2 "在第 6 章 第 4 阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6.2.1 Task-Specific Fine-Tuning](#Ch6.S2.SS1 "In 6.2 Fine-Tuning Strategies
    for LLMs ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.1 特定任务微调](#Ch6.S2.SS1 "第6.2节 微调策略 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.2.2 Domain-Specific Fine-Tuning](#Ch6.S2.SS2 "In 6.2 Fine-Tuning Strategies
    for LLMs ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.2 特定领域微调](#Ch6.S2.SS2 "第6.2节 微调策略 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques](#Ch6.S3 "In Chapter
    6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3 参数高效微调（PEFT）技术](#Ch6.S3 "第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.1 Adapters](#Ch6.S3.SS1 "In 6.3 Parameter-Efficient Fine-Tuning (PEFT)
    Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.1 适配器](#Ch6.S3.SS1 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.2 Low-Rank Adaptation (LoRA)](#Ch6.S3.SS2 "In 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.2 低秩适应（LoRA）](#Ch6.S3.SS2 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置
    ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.3 QLoRA](#Ch6.S3.SS3 "In 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.3 QLoRA](#Ch6.S3.SS3 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣
    微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.4 Weight-Decomposed Low-Rank Adaptation (DoRA)](#Ch6.S3.SS4 "In 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.4 权重分解低秩适应（DoRA）](#Ch6.S3.SS4 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置
    ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.5 Fine-Tuning with Multiple Adapters](#Ch6.S3.SS5 "In 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.5 多适配器微调](#Ch6.S3.SS5 "在6.3 参数高效微调（PEFT）技术 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置 ‣
    《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.4 Half Fine Tuning](#Ch6.S4 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4 半微调](#Ch6.S4 "在第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.4.1 Benefits of using Half Fine tuning](#Ch6.S4.SS1 "In 6.4 Half Fine Tuning
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4.1 使用半微调的好处](#Ch6.S4.SS1 "在6.4 半微调 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.4.2 Comparison between HFT and LoRA](#Ch6.S4.SS2 "In 6.4 Half Fine Tuning
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4.2 HFT与LoRA的比较](#Ch6.S4.SS2 "在6.4 半微调 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.5 Lamini Memory Tuning](#Ch6.S5 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.5 Lamini内存调优](#Ch6.S5 "在第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.5.1 Lamini-1 - A model architecture based on Lamini](#Ch6.S5.SS1 "In 6.5
    Lamini Memory Tuning ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.5.1 Lamini-1 - 基于Lamini的模型架构](#Ch6.S5.SS1 "在6.5 Lamini内存调优 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置
    ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.6 Mixture of Experts](#Ch6.S6 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.6 专家混合](#Ch6.S6 "在第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.6.1 Mixtral 8x7B Architecture and Performance](#Ch6.S6.SS1 "In 6.6 Mixture
    of Experts ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.6.1 Mixtral 8x7B架构与性能](#Ch6.S6.SS1 "在6.6 专家混合 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置
    ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.7 Mixture of Agents](#Ch6.S7 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7 智能体混合](#Ch6.S7 "在第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.7.1 Methodology](#Ch6.S7.SS1 "In 6.7 Mixture of Agents ‣ Chapter 6 Stage
    4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations ‣
    The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7.1 方法论](#Ch6.S7.SS1 "在6.7智能体混合 ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.7.2 Analogy with MoE](#Ch6.S7.SS2 "In 6.7 Mixture of Agents ‣ Chapter 6
    Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7.2 与MoE的类比](#Ch6.S7.SS2 "在6.7智能体混合 ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.7.3 What makes MoA works well?](#Ch6.S7.SS3 "In 6.7 Mixture of Agents ‣
    Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7.3 什么使MoA运作良好？](#Ch6.S7.SS3 "在6.7智能体混合 ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8 Proximal Policy Optimisation (PPO)](#Ch6.S8 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8 近端策略优化（PPO）](#Ch6.S8 "在第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8.1 Benefits of PPO](#Ch6.S8.SS1 "In 6.8 Proximal Policy Optimisation (PPO)
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8.1 PPO的好处](#Ch6.S8.SS1 "在6.8近端策略优化（PPO） ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8.2 Limitations of PPO](#Ch6.S8.SS2 "In 6.8 Proximal Policy Optimisation
    (PPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8.2 PPO的局限性](#Ch6.S8.SS2 "在6.8近端策略优化（PPO） ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8.3 Tutorial for training models using PPO technique](#Ch6.S8.SS3 "In 6.8
    Proximal Policy Optimisation (PPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8.3 使用PPO技术训练模型的教程](#Ch6.S8.SS3 "在6.8近端策略优化（PPO） ‣ 第6章第4阶段：微调技术和适当模型配置的选择
    ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.9 Direct Preference Optimisation (DPO)](#Ch6.S9 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9 直接偏好优化 (DPO)](#Ch6.S9 "在第6章 第4阶段: 精细调整技术与合适模型配置的选择 ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾
    (版本 1.0)")'
- en: '[6.9.1 Benefits of DPO](#Ch6.S9.SS1 "In 6.9 Direct Preference Optimisation
    (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.1 DPO的好处](#Ch6.S9.SS1 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.9.2 Best Practices for DPO](#Ch6.S9.SS2 "In 6.9 Direct Preference Optimisation
    (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.2 DPO的最佳实践](#Ch6.S9.SS2 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.9.3 Tutorial for training models using DPO technique](#Ch6.S9.SS3 "In 6.9
    Direct Preference Optimisation (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.3 使用DPO技术训练模型的教程](#Ch6.S9.SS3 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.9.4 Is DPO Superior to PPO for LLM Alignment?](#Ch6.S9.SS4 "In 6.9 Direct
    Preference Optimisation (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.4 DPO是否优于PPO用于LLM对齐？](#Ch6.S9.SS4 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.10 Optimised Routing and Pruning Operations (ORPO)](#Ch6.S10 "In Chapter
    6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10 优化路由和修剪操作 (ORPO)](#Ch6.S10 "在第6章 第4阶段: 精细调整技术与合适模型配置的选择 ‣ 从基础到突破的最终指南:
    技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.10.1 When to Prune AI Models?](#Ch6.S10.SS1 "In 6.10 Optimised Routing and
    Pruning Operations (ORPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10.1 何时修剪AI模型？](#Ch6.S10.SS1 "在6.10 优化路由和修剪操作 (ORPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.10.2 Benefits of Pruning](#Ch6.S10.SS2 "In 6.10 Optimised Routing and Pruning
    Operations (ORPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and
    Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10.2 剪枝的好处](#Ch6.S10.SS2 "在 6.10 优化路由和剪枝操作（ORPO） ‣ 第6章 第4阶段：微调技术和适当模型配置的选择
    ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[6.10.3 Challenges of Pruning](#Ch6.S10.SS3 "In 6.10 Optimised Routing and
    Pruning Operations (ORPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10.3 剪枝的挑战](#Ch6.S10.SS3 "在 6.10 优化路由和剪枝操作（ORPO） ‣ 第6章 第4阶段：微调技术和适当模型配置的选择
    ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[7 Stage 5: Evaluation and Validation](#Ch7 "In The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 第5阶段：评估和验证](#Ch7 "在从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[7.1 Steps Involved in Evaluating and Validating Fine-Tuned Models](#Ch7.S1
    "In Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.1 评估和验证微调模型的步骤](#Ch7.S1 "在第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.2 Setting Up Evaluation Metrics](#Ch7.S2 "In Chapter 7 Stage 5: Evaluation
    and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2 设置评估指标](#Ch7.S2 "在第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.2.1 Importance of Cross-Entropy for LLM Training and Evaluation](#Ch7.S2.SS1
    "In 7.2 Setting Up Evaluation Metrics ‣ Chapter 7 Stage 5: Evaluation and Validation
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.1 交叉熵在 LLM 训练和评估中的重要性](#Ch7.S2.SS1 "在 7.2 设置评估指标 ‣ 第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.2.2 Beyond Cross-Entropy: Advanced LLM Evaluation Metrics](#Ch7.S2.SS2 "In
    7.2 Setting Up Evaluation Metrics ‣ Chapter 7 Stage 5: Evaluation and Validation
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.2 超越交叉熵：高级 LLM 评估指标](#Ch7.S2.SS2 "在 7.2 设置评估指标 ‣ 第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.3 Understanding the Training Loss Curve](#Ch7.S3 "In Chapter 7 Stage 5:
    Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3 理解训练损失曲线](#Ch7.S3 "在第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.3.1 Interpreting Loss Curves](#Ch7.S3.SS1 "In 7.3 Understanding the Training
    Loss Curve ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3.1 解读损失曲线](#Ch7.S3.SS1 "在 7.3 理解训练损失曲线 ‣ 第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.3.2 Avoiding Overfitting](#Ch7.S3.SS2 "In 7.3 Understanding the Training
    Loss Curve ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3.2 避免过拟合](#Ch7.S3.SS2 "在7.3 理解训练损失曲线 ‣ 第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.3.3 Sources of Noisy Gradients](#Ch7.S3.SS3 "In 7.3 Understanding the Training
    Loss Curve ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3.3 噪声梯度来源](#Ch7.S3.SS3 "在7.3 理解训练损失曲线 ‣ 第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.4 Running Validation Loops](#Ch7.S4 "In Chapter 7 Stage 5: Evaluation and
    Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.4 运行验证循环](#Ch7.S4 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.5 Monitoring and Interpreting Results](#Ch7.S5 "In Chapter 7 Stage 5: Evaluation
    and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.5 监控和解读结果](#Ch7.S5 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.6 Hyperparameter Tuning and Other Adjustments](#Ch7.S6 "In Chapter 7 Stage
    5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.6 超参数调优及其他调整](#Ch7.S6 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.6.1 Data Size and Quality](#Ch7.S6.SS1 "In 7.6 Hyperparameter Tuning and
    Other Adjustments ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.6.1 数据规模和质量](#Ch7.S6.SS1 "在7.6 超参数调优及其他调整 ‣ 第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.7 Benchmarking Fine-Tuned LLMs](#Ch7.S7 "In Chapter 7 Stage 5: Evaluation
    and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.7 微调LLMs的基准测试](#Ch7.S7 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.8 Evaluating Fine-Tuned LLMs on Safety Benchmark](#Ch7.S8 "In Chapter 7
    Stage 5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.8 在安全基准上评估微调LLMs](#Ch7.S8 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9 Evaluating Safety of Fine-Tuned LLM using AI Models](#Ch7.S9 "In Chapter
    7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9 使用AI模型评估微调LLMs的安全性](#Ch7.S9 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9.1 Llama Guard](#Ch7.S9.SS1 "In 7.9 Evaluating Safety of Fine-Tuned LLM
    using AI Models ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9.1 Llama Guard](#Ch7.S9.SS1 "在7.9 使用AI模型评估微调LLM的安全性 ‣ 第7章 第5阶段：评估与验证 ‣
    《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9.2 Shield Gemma](#Ch7.S9.SS2 "In 7.9 Evaluating Safety of Fine-Tuned LLM
    using AI Models ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9.2 Shield Gemma](#Ch7.S9.SS2 "在7.9 使用AI模型评估微调LLM的安全性 ‣ 第7章 第5阶段：评估与验证 ‣
    《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9.3 WILDGUARD](#Ch7.S9.SS3 "In 7.9 Evaluating Safety of Fine-Tuned LLM using
    AI Models ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9.3 WILDGUARD](#Ch7.S9.SS3 "在7.9 使用AI模型评估微调LLM的安全性 ‣ 第7章 第5阶段：评估与验证 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8 Stage 6: Deployment](#Ch8 "In The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8 第6阶段：部署](#Ch8 "在《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.1 Steps Involved in Deploying the Fine-Tuned Model](#Ch8.S1 "In Chapter
    8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.1 微调模型部署的步骤](#Ch8.S1 "在第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.2 Cloud-Based Providers for LLM Deployment](#Ch8.S2 "In Chapter 8 Stage
    6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2 基于云的LLM部署提供商](#Ch8.S2 "在第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3 Techniques for Optimising Model Performance During Inference](#Ch8.S3
    "In Chapter 8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3 优化模型推理性能的技术](#Ch8.S3 "在第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3.1 Traditional On-Premises GPU-Based Deployments](#Ch8.S3.SS1 "In 8.3 Techniques
    for Optimising Model Performance During Inference ‣ Chapter 8 Stage 6: Deployment
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.1 传统的本地GPU部署](#Ch8.S3.SS1 "在8.3 优化模型推理性能的技术 ‣ 第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3.2 Distributed LLM: Torrent-Style Deployment and Parallel Forward Passes](#Ch8.S3.SS2
    "In 8.3 Techniques for Optimising Model Performance During Inference ‣ Chapter
    8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.2 分布式LLM：类似于种子的部署和并行前向传递](#Ch8.S3.SS2 "在8.3 优化模型推理性能的技术 ‣ 第8章 第6阶段：部署
    ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3.3 WebGPU-Based Deployment of LLM](#Ch8.S3.SS3 "In 8.3 Techniques for Optimising
    Model Performance During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.3 基于 WebGPU 的大型语言模型部署](#Ch8.S3.SS3 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署 ‣
    从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.3.4 LLM on WebGPU using WebLLM](#Ch8.S3.SS4 "In 8.3 Techniques for Optimising
    Model Performance During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.4 使用 WebLLM 在 WebGPU 上的 LLM](#Ch8.S3.SS4 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署
    ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.3.5 Quantised LLMs](#Ch8.S3.SS5 "In 8.3 Techniques for Optimising Model
    Performance During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.5 量化大型语言模型](#Ch8.S3.SS5 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.3.6 vLLMs](#Ch8.S3.SS6 "In 8.3 Techniques for Optimising Model Performance
    During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.6 vLLMs](#Ch8.S3.SS6 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.4 Key Considerations for Deployment of LLMs](#Ch8.S4 "In Chapter 8 Stage
    6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.4 部署大型语言模型（LLMs）的关键考虑因素](#Ch8.S4 "在第8章第6阶段：部署 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9 Stage 7: Monitoring and Maintenance](#Ch9 "In The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9 第7阶段：监控和维护](#Ch9 "在从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.1 Steps Involved in Monitoring and Maintenance of Deployed Fine-Tuned LLMs](#Ch9.S1
    "In Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.1 监控和维护已部署的微调大型语言模型的步骤](#Ch9.S1 "在第9章第7阶段：监控和维护 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.2 Continuous Monitoring of Model Performance](#Ch9.S2 "In Chapter 9 Stage
    7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2 模型性能的持续监控](#Ch9.S2 "在第9章第7阶段：监控和维护 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.2.1 Functional Monitoring](#Ch9.S2.SS1 "In 9.2 Continuous Monitoring of
    Model Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.1 功能监控](#Ch9.S2.SS1 "在9.2 模型性能的持续监控 ‣ 第9章第7阶段：监控和维护 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.2.2 Prompt Monitoring](#Ch9.S2.SS2 "In 9.2 Continuous Monitoring of Model
    Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.2 提示监控](#Ch9.S2.SS2 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.2.3 Response Monitoring](#Ch9.S2.SS3 "In 9.2 Continuous Monitoring of Model
    Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.3 响应监控](#Ch9.S2.SS3 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.2.4 Alerting Mechanisms and Thresholds](#Ch9.S2.SS4 "In 9.2 Continuous Monitoring
    of Model Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.4 警报机制和阈值](#Ch9.S2.SS4 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.2.5 Monitoring User Interface (UI)](#Ch9.S2.SS5 "In 9.2 Continuous Monitoring
    of Model Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.5 监控用户界面 (UI)](#Ch9.S2.SS5 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3 Updating LLM Knowledge](#Ch9.S3 "In Chapter 9 Stage 7: Monitoring and
    Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3 更新LLM知识](#Ch9.S3 "在第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3.1 Retraining Methods](#Ch9.S3.SS1 "In 9.3 Updating LLM Knowledge ‣ Chapter
    9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3.1 重新训练方法](#Ch9.S3.SS1 "在9.3 更新LLM知识 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3.2 Additional Methods](#Ch9.S3.SS2 "In 9.3 Updating LLM Knowledge ‣ Chapter
    9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3.2 额外方法](#Ch9.S3.SS2 "在9.3 更新LLM知识 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3.3 Key Considerations](#Ch9.S3.SS3 "In 9.3 Updating LLM Knowledge ‣ Chapter
    9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3.3 关键考虑因素](#Ch9.S3.SS3 "在9.3 更新LLM知识 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.4 The Future of LLM Updates](#Ch9.S4 "In Chapter 9 Stage 7: Monitoring and
    Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.4 LLM更新的未来](#Ch9.S4 "在第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[10 Industrial Fine-Tuning Platforms and Frameworks for LLMs](#Ch10 "In The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10 工业微调平台和框架](#Ch10 "在从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0
    版）")'
- en: '[10.1 Autotrain](#Ch10.S1 "In Chapter 10 Industrial Fine-Tuning Platforms and
    Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1 Autotrain](#Ch10.S1 "在第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第
    1.0 版）")'
- en: '[10.1.1 Steps Involved in Fine-Tuning Using Autotrain](#Ch10.S1.SS1 "In 10.1
    Autotrain ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.1 使用 Autotrain 进行微调的步骤](#Ch10.S1.SS1 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架
    ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.2 Best Practices of Using Autotrain](#Ch10.S1.SS2 "In 10.1 Autotrain
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.2 使用 Autotrain 的最佳实践](#Ch10.S1.SS2 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架
    ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.3 Challenges of Using Autotrain](#Ch10.S1.SS3 "In 10.1 Autotrain ‣ Chapter
    10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.3 使用 Autotrain 的挑战](#Ch10.S1.SS3 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架 ‣
    从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.4 When to Use Autotrain](#Ch10.S1.SS4 "In 10.1 Autotrain ‣ Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.4 何时使用 Autotrain](#Ch10.S1.SS4 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的微调
    LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.5 Tutorials](#Ch10.S1.SS5 "In 10.1 Autotrain ‣ Chapter 10 Industrial
    Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.5 教程](#Ch10.S1.SS5 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs
    的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.2 Transformers Library and Trainer API](#Ch10.S2 "In Chapter 10 Industrial
    Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.2 Transformers 库和 Trainer API](#Ch10.S2 "在第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs
    的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.2.1 Limitations of the Transformers Library and Trainer API](#Ch10.S2.SS1
    "In 10.2 Transformers Library and Trainer API ‣ Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.2.1 Transformers 库和 Trainer API 的局限性](#Ch10.S2.SS1 "在 10.2 Transformers
    库和 Trainer API ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第
    1.0 版）")'
- en: '[10.3 Optimum: Enhancing LLM Deployment Efficiency](#Ch10.S3 "In Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.3 Optimum：提升 LLM 部署效率](#Ch10.S3 "在第 10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）》")'
- en: '[10.3.1 Best Practices of Using Optimum](#Ch10.S3.SS1 "In 10.3 Optimum: Enhancing
    LLM Deployment Efficiency ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks
    for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.3.1 使用 Optimum 的最佳实践](#Ch10.S3.SS1 "在 10.3 Optimum：提升 LLM 部署效率 ‣ 第 10 章
    工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.3.2 Tutorials](#Ch10.S3.SS2 "In 10.3 Optimum: Enhancing LLM Deployment
    Efficiency ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.3.2 教程](#Ch10.S3.SS2 "在 10.3 Optimum：提升 LLM 部署效率 ‣ 第 10 章 工业级微调平台和框架 ‣
    《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4 Amazon SageMaker JumpStart](#Ch10.S4 "In Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4 亚马逊 SageMaker JumpStart](#Ch10.S4 "在第 10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM
    微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.1 Steps Involved in Using JumpStart](#Ch10.S4.SS1 "In 10.4 Amazon SageMaker
    JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.1 使用 JumpStart 的步骤](#Ch10.S4.SS1 "在 10.4 亚马逊 SageMaker JumpStart ‣ 第
    10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.2 Best Practices for Using JumpStart](#Ch10.S4.SS2 "In 10.4 Amazon SageMaker
    JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.2 使用 JumpStart 的最佳实践](#Ch10.S4.SS2 "在 10.4 亚马逊 SageMaker JumpStart ‣
    第 10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.3 Limitations of Using JumpStart](#Ch10.S4.SS3 "In 10.4 Amazon SageMaker
    JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.3 使用 JumpStart 的限制](#Ch10.S4.SS3 "在 10.4 亚马逊 SageMaker JumpStart ‣ 第
    10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.4 Tutorials](#Ch10.S4.SS4 "In 10.4 Amazon SageMaker JumpStart ‣ Chapter
    10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.4 教程](#Ch10.S4.SS4 "在 10.4 亚马逊 SageMaker JumpStart ‣ 第 10 章 工业级微调平台和框架
    ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.5 Amazon Bedrock](#Ch10.S5 "In Chapter 10 Industrial Fine-Tuning Platforms
    and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5 亚马逊 Bedrock](#Ch10.S5 "在第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.5.1 Steps Involved in Using Amazon Bedrock](#Ch10.S5.SS1 "In 10.5 Amazon
    Bedrock ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5.1 使用亚马逊 Bedrock 的步骤](#Ch10.S5.SS1 "在 10.5 亚马逊 Bedrock ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.5.2 Limitations of Using Amazon Bedrock](#Ch10.S5.SS2 "In 10.5 Amazon Bedrock
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5.2 使用亚马逊 Bedrock 的局限性](#Ch10.S5.SS2 "在 10.5 亚马逊 Bedrock ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.5.3 Tutorials](#Ch10.S5.SS3 "In 10.5 Amazon Bedrock ‣ Chapter 10 Industrial
    Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5.3 教程](#Ch10.S5.SS3 "在 10.5 亚马逊 Bedrock ‣ 第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.6 OpenAI’s Fine-Tuning API](#Ch10.S6 "In Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6 OpenAI 微调 API](#Ch10.S6 "在第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.6.1 Steps Involved in Using OpenAI’s Fine-Tuning API](#Ch10.S6.SS1 "In
    10.6 OpenAI’s Fine-Tuning API ‣ Chapter 10 Industrial Fine-Tuning Platforms and
    Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6.1 使用 OpenAI 微调 API 的步骤](#Ch10.S6.SS1 "在 10.6 OpenAI 微调 API ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.6.2 Limitations of OpenAI’s Fine-Tuning API](#Ch10.S6.SS2 "In 10.6 OpenAI’s
    Fine-Tuning API ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for
    LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6.2 OpenAI 微调 API 的局限性](#Ch10.S6.SS2 "在 10.6 OpenAI 微调 API ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.6.3 Tutorials](#Ch10.S6.SS3 "In 10.6 OpenAI’s Fine-Tuning API ‣ Chapter
    10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6.3 教程](#Ch10.S6.SS3 "在 10.6 OpenAI 微调 API ‣ 第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.7 NVIDIA NeMo Customizer](#Ch10.S7 "In Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7 NVIDIA NeMo 自定义工具](#Ch10.S7 "在第10章 工业级微调平台和框架 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.1 Key Features of NVIDIA NeMo](#Ch10.S7.SS1 "In 10.7 NVIDIA NeMo Customizer
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.1 NVIDIA NeMo 的主要特性](#Ch10.S7.SS1 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架
    ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.2 Components of NVIDIA NeMo](#Ch10.S7.SS2 "In 10.7 NVIDIA NeMo Customizer
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.2 NVIDIA NeMo 组件](#Ch10.S7.SS2 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架
    ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.3 Customising Large Language Models (LLMs)](#Ch10.S7.SS3 "In 10.7 NVIDIA
    NeMo Customizer ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for
    LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.3 自定义大型语言模型 (LLMs)](#Ch10.S7.SS3 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架
    ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.4 Tutorials](#Ch10.S7.SS4 "In 10.7 NVIDIA NeMo Customizer ‣ Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.4 教程](#Ch10.S7.SS4 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11 Multimodal LLMs and their Fine-tuning](#Ch11 "In The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11 多模态LLM及其微调](#Ch11 "在从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.1 Vision Language Model (VLMs)](#Ch11.S1 "In Chapter 11 Multimodal LLMs
    and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.1 视觉语言模型 (VLMs)](#Ch11.S1 "在第11章 多模态LLM及其微调 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.1.1 Architecture](#Ch11.S1.SS1 "In 11.1 Vision Language Model (VLMs) ‣
    Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.1.1 架构](#Ch11.S1.SS1 "在11.1 视觉语言模型 (VLMs) ‣ 第11章 多模态LLM及其微调 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.1.2 Contrastive Learning](#Ch11.S1.SS2 "In 11.1 Vision Language Model (VLMs)
    ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.1.2 对比学习](#Ch11.S1.SS2 "在11.1 视觉语言模型 (VLMs) ‣ 第11章 多模态LLM及其微调 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.2 Fine-tuning of multimodal models](#Ch11.S2 "In Chapter 11 Multimodal
    LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.2 多模态模型的微调](#Ch11.S2 "在第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.2.1 Full-parameter Fine-Tuning](#Ch11.S2.SS1 "In 11.2 Fine-tuning of multimodal
    models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.2.1 全参数微调](#Ch11.S2.SS1 "在 11.2 多模态模型的微调 ‣ 第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.2.2 Case study of fine-tuning MLLMs for Medical domain](#Ch11.S2.SS2 "In
    11.2 Fine-tuning of multimodal models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.2.2 微调 MLLMs 在医疗领域的案例研究](#Ch11.S2.SS2 "在 11.2 多模态模型的微调 ‣ 第 11 章 多模态 LLMs
    及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.3 Applications of Multimodal models](#Ch11.S3 "In Chapter 11 Multimodal
    LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.3 多模态模型的应用](#Ch11.S3 "在第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.4 Audio or Speech LLMs Or Large Audio Models](#Ch11.S4 "In Chapter 11 Multimodal
    LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4 音频或语音 LLMs 或大型音频模型](#Ch11.S4 "在第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.4.1 Tokenization and Preprocessing](#Ch11.S4.SS1 "In 11.4 Audio or Speech
    LLMs Or Large Audio Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.1 分词和预处理](#Ch11.S4.SS1 "在 11.4 音频或语音 LLMs 或大型音频模型 ‣ 第 11 章 多模态 LLMs 及其微调
    ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.4.2 Fine-Tuning Techniques](#Ch11.S4.SS2 "In 11.4 Audio or Speech LLMs
    Or Large Audio Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.2 微调技术](#Ch11.S4.SS2 "在 11.4 音频或语音 LLMs 或大型音频模型 ‣ 第 11 章 多模态 LLMs 及其微调
    ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.4.3 Fine-Tuning Whisper for Automatic Speech Recognition (ASR)](#Ch11.S4.SS3
    "In 11.4 Audio or Speech LLMs Or Large Audio Models ‣ Chapter 11 Multimodal LLMs
    and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.3 微调 Whisper 以进行自动语音识别 (ASR)](#Ch11.S4.SS3 "在 11.4 音频或语音 LLMs 或大型音频模型
    ‣ 第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.4.4 Case Studies and Applications](#Ch11.S4.SS4 "In 11.4 Audio or Speech
    LLMs Or Large Audio Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.4 案例研究与应用](#Ch11.S4.SS4 "在11.4 音频或语音LLM或大型音频模型 ‣ 第11章 多模态LLM及其微调 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12 Open Challenges and Research Directions](#Ch12 "In The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12 开放挑战与研究方向](#Ch12 "在从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1 Scalability Issues](#Ch12.S1 "In Chapter 12 Open Challenges and Research
    Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1 可扩展性问题](#Ch12.S1 "在第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1.1 Challenges in Scaling Fine-Tuning Processes](#Ch12.S1.SS1 "In 12.1
    Scalability Issues ‣ Chapter 12 Open Challenges and Research Directions ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1.1 微调过程中的扩展挑战](#Ch12.S1.SS1 "在12.1 可扩展性问题 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1.2 Research Directions for Scalable Solutions](#Ch12.S1.SS2 "In 12.1 Scalability
    Issues ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1.2 可扩展解决方案的研究方向](#Ch12.S1.SS2 "在12.1 可扩展性问题 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1.3 Hardware and Algorithm Co-Design](#Ch12.S1.SS3 "In 12.1 Scalability
    Issues ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1.3 硬件与算法协同设计](#Ch12.S1.SS3 "在12.1 可扩展性问题 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2 Ethical Considerations in Fine-Tuning LLMs](#Ch12.S2 "In Chapter 12 Open
    Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2 微调LLM的伦理考虑](#Ch12.S2 "在第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2.1 Bias and Fairness](#Ch12.S2.SS1 "In 12.2 Ethical Considerations in
    Fine-Tuning LLMs ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2.1 偏见与公平性](#Ch12.S2.SS1 "在12.2 微调LLM的伦理考虑 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2.2 Privacy Concerns](#Ch12.S2.SS2 "In 12.2 Ethical Considerations in Fine-Tuning
    LLMs ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2.2 隐私问题](#Ch12.S2.SS2 "在12.2 微调LLM的伦理考虑 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2.3 Security Risks](#Ch12.S2.SS3 "In 12.2 Ethical Considerations in Fine-Tuning
    LLMs ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2.3 安全风险](#Ch12.S2.SS3 "在12.2 微调大规模语言模型的伦理考虑 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3 Accountability and Transparency](#Ch12.S3 "In Chapter 12 Open Challenges
    and Research Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3 责任与透明度](#Ch12.S3 "在第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.1 The Need for Accountability and Transparency](#Ch12.S3.SS1 "In 12.3
    Accountability and Transparency ‣ Chapter 12 Open Challenges and Research Directions
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.1 责任与透明度的必要性](#Ch12.S3.SS1 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.2 Recent Research and Industry Practices](#Ch12.S3.SS2 "In 12.3 Accountability
    and Transparency ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.2 最近的研究与行业实践](#Ch12.S3.SS2 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.3 Promoting Accountability and Transparency](#Ch12.S3.SS3 "In 12.3 Accountability
    and Transparency ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.3 促进责任与透明度](#Ch12.S3.SS3 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.4 Proposed frameworks/techniques for Ethical Fine-Tuning](#Ch12.S3.SS4
    "In 12.3 Accountability and Transparency ‣ Chapter 12 Open Challenges and Research
    Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.4 伦理微调的提议框架/技术](#Ch12.S3.SS4 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.4 Integration with Emerging Technologies](#Ch12.S4 "In Chapter 12 Open
    Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.4 融合新兴技术](#Ch12.S4 "在第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.4.1 Opportunities](#Ch12.S4.SS1 "In 12.4 Integration with Emerging Technologies
    ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.4.1 机会](#Ch12.S4.SS1 "在12.4 融合新兴技术 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.4.2 Challenges](#Ch12.S4.SS2 "In 12.4 Integration with Emerging Technologies
    ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.4.2 挑战](#Ch12.S4.SS2 "在12.4与新兴技术的整合 ‣ 第12章开放挑战与研究方向 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[12.5 Future Research Areas](#Ch12.S5 "In Chapter 12 Open Challenges and Research
    Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.5 未来研究领域](#Ch12.S5 "在第12章开放挑战与研究方向 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[Glossary](#Chx1 "In The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[术语表](#Chx1 "在从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: Chapter 1 Introduction
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章 引言
- en: 1.1 Background of Large Language Models (LLMs)
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 大型语言模型（LLMs）的背景
- en: Large Language Models (LLMs) represent a significant leap in computational systems
    capable of understanding and generating human language. Building on traditional
    language models (LMs) like N-gram models [[1](#bib.bib1)], LLMs address limitations
    such as rare word handling, overfitting, and capturing complex linguistic patterns.
    Notable examples, such as GPT-3 and GPT-4 [[2](#bib.bib2)], leverage the self-attention
    mechanism within Transformer architectures to efficiently manage sequential data
    and understand long-range dependencies. Key advancements include in-context learning
    for generating coherent text from prompts and Reinforcement Learning from Human
    Feedback (RLHF) [[3](#bib.bib3)] for refining models using human responses. Techniques
    like prompt engineering, question-answering, and conversational interactions have
    significantly advanced the field of natural language processing (NLP) [[4](#bib.bib4)].
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）代表了在理解和生成自然语言方面的计算系统的重要飞跃。基于传统语言模型（LMs）如N-gram模型[[1](#bib.bib1)]，LLMs解决了诸如稀有词处理、过拟合和捕捉复杂语言模式等局限性。值得注意的例子如GPT-3和GPT-4[[2](#bib.bib2)]，利用Transformer架构中的自注意力机制高效地管理序列数据并理解长程依赖。关键进展包括通过上下文学习生成连贯文本的能力，以及利用人类反馈进行模型优化的强化学习（RLHF）[[3](#bib.bib3)]。技术如提示工程、问答和对话互动显著推动了自然语言处理（NLP）领域的发展[[4](#bib.bib4)]。
- en: 1.2 Historical Development and Key Milestones
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 历史发展与关键里程碑
- en: Language models are fundamental to natural language processing (NLP), leveraging
    mathematical techniques to generalise linguistic rules and knowledge for tasks
    involving prediction and generation. Over several decades, language modelling
    has evolved from early statistical language models (SLMs) to today’s advanced
    large language models (LLMs). This rapid advancement has enabled LLMs to process,
    comprehend, and generate text at a level comparable to human capabilities [[5](#bib.bib5),
    [6](#bib.bib6)].
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是自然语言处理（NLP）的基础，利用数学技术来概括语言规则和知识，用于预测和生成任务。在几十年间，语言建模已经从早期的统计语言模型（SLMs）发展到今天的先进大型语言模型（LLMs）。这一快速进步使得LLMs能够在与人类能力相当的水平上处理、理解和生成文本[[5](#bib.bib5),
    [6](#bib.bib6)]。
- en: '![Refer to caption](img/4da995f8c2ca7f2f1f2f0bd5194884bf.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4da995f8c2ca7f2f1f2f0bd5194884bf.png)'
- en: 'Figure 1.1: A chronological timeline showcasing the evolution of Large Language
    Models (LLMs) from 1990 to 2023\. This progression begins with early statistical
    models such as N-grams, transitions through neural language models like Word2Vec
    and RNN/LSTM, and advances into the era of pre-trained models with the introduction
    of transformers and attention mechanisms. The figure highlights significant milestones,
    including the development of BERT, GPT series, and recent innovations such as
    GPT-4 and ChatGPT, demonstrating the rapid advancements in LLM technology over
    time. (adapted from [[6](#bib.bib6)])'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：一个按时间顺序展示大语言模型（LLMs）从 1990 年到 2023 年演变的时间轴。这个过程从早期的统计模型如 N-grams 开始，经过神经语言模型如
    Word2Vec 和 RNN/LSTM，发展到预训练模型时代，介绍了变换器和注意力机制。该图突出显示了重要的里程碑，包括 BERT、GPT 系列和最近的创新，如
    GPT-4 和 ChatGPT，展示了 LLM 技术随时间的迅速进步。（改编自 [[6](#bib.bib6)])
- en: 'Figure [1.1](#Ch1.F1 "Figure 1.1 ‣ 1.2 Historical Development and Key Milestones
    ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)") shows the evolution
    of large language models from early statistical approaches to current advanced
    models.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.1](#Ch1.F1 "图 1.1 ‣ 1.2 历史发展与关键里程碑 ‣ 第1章 引言 ‣ 从基础到突破的大语言模型微调终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的全面回顾（版本
    1.0）") 展示了大型语言模型从早期统计方法到当前先进模型的发展历程。
- en: 1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 从传统 NLP 模型到最先进的大语言模型的演变
- en: Understanding LLMs requires tracing the development of language models through
    stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs),
    Pre-trained Language Models (PLMs), and LLMs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 理解大语言模型（LLMs）需要追溯语言模型的发展历程，包括统计语言模型（SLMs）、神经语言模型（NLMs）、预训练语言模型（PLMs）以及大语言模型（LLMs）。
- en: 1.3.1 Statistical Language Models (SLMs)
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1 统计语言模型（SLMs）
- en: 'Emerging in the 1990s, SLMs analyse natural language using probabilistic methods
    to determine the likelihood of sentences within texts. For instance, the probability
    $P(S)$ of the sentence “I am very happy” is given by:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 出现在 1990 年代，统计语言模型（SLMs）使用概率方法分析自然语言，以确定文本中句子的可能性。例如，句子 “I am very happy” 的概率
    $P(S)$ 为：
- en: '|  | $P(S)=P(\omega_{1},\omega_{2},\omega_{3},\omega_{4})=P(\text{I},\text{am},\text{very},\text{happy})$
    |  | (1.1) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(S)=P(\omega_{1},\omega_{2},\omega_{3},\omega_{4})=P(\text{I},\text{am},\text{very},\text{happy})$
    |  | (1.1) |'
- en: 'This probability can be calculated using conditional probabilities:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率可以通过条件概率来计算：
- en: '|  | $1$2 |  | (1.2) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1.2) |'
- en: 'Conditional probabilities are estimated using Maximum Likelihood Estimation
    (MLE):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率的估计使用最大似然估计（MLE）：
- en: '|  | $1$2 |  | (1.3) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1.3) |'
- en: 1.3.2 Neural Language Models (NLMs)
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.2 神经语言模型（NLMs）
- en: NLMs leverage neural networks to predict word sequences, overcoming SLM limitations.
    Word vectors enable computers to understand word meanings. Tools like Word2Vec
    [[7](#bib.bib7)] represent words in a vector space where semantic relationships
    are reflected in vector angles. NLMs consist of interconnected neurons organised
    into layers, resembling the human brain’s structure. The input layer concatenates
    word vectors, the hidden layer applies a non-linear activation function, and the
    output layer predicts subsequent words using the Softmax function to transform
    values into a probability distribution.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言模型（NLMs）利用神经网络来预测词序列，克服了统计语言模型的局限性。词向量使计算机能够理解词义。像 Word2Vec [[7](#bib.bib7)]
    这样的工具将词表示为向量空间中的点，其中语义关系反映在向量角度中。NLMs 由互联的神经元组成，这些神经元被组织成层，类似于人脑的结构。输入层将词向量串联在一起，隐藏层应用非线性激活函数，输出层使用
    Softmax 函数将值转换为概率分布，以预测后续词。
- en: '![Refer to caption](img/0dc20304e6f5cbed35413b812f841791.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0dc20304e6f5cbed35413b812f841791.png)'
- en: 'Figure 1.2: A schematic representation of Neural Language Models, showcasing
    the layered architecture where the input layer processes sequential data, the
    hidden layer captures dependencies, and the output layer generates predictions.
    The figure emphasises the flow of information through concatenation and matrix
    multiplications, culminating in a probability distribution via the softmax function.
    (adopted from [[6](#bib.bib6)])'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：神经语言模型的示意图，展示了分层架构，其中输入层处理顺序数据，隐藏层捕获依赖关系，输出层生成预测。该图强调了通过连接和矩阵乘法的信息流，最终通过
    softmax 函数得到概率分布。（采用自 [[6](#bib.bib6)]）
- en: 'Figure [1.2](#Ch1.F2 "Figure 1.2 ‣ 1.3.2 Neural Language Models (NLMs) ‣ 1.3
    Evolution from Traditional NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)") illustrates the structure of Neural Language
    Models, highlighting the layers and connections used to predict subsequent words.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.2](#Ch1.F2 "图 1.2 ‣ 1.3.2 神经语言模型 (NLMs) ‣ 1.3 从传统 NLP 模型到最先进 LLMs 的演变 ‣
    第 1 章 引言 ‣ 从基础到突破的 LLMs 微调终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第 1.0 版）") 说明了神经语言模型的结构，突出显示了用于预测后续词汇的层次结构和连接。
- en: 1.3.3 Pre-trained Language Models (PLMs)
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.3 预训练语言模型（PLMs）
- en: PLMs are initially trained on extensive volumes of unlabelled text to understand
    fundamental language structures (pre-training). They are then fine-tuned on a
    smaller, task-specific dataset. This ”pre-training and fine-tuning” paradigm,
    exemplified by GPT-2 [[8](#bib.bib8)] and BERT [[9](#bib.bib9)], has led to diverse
    and effective model architectures.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: PLMs 最初在大量未标记的文本上进行训练，以理解基本的语言结构（预训练）。然后，在较小的任务特定数据集上进行微调。这种“预训练和微调”范式，以 GPT-2
    [[8](#bib.bib8)] 和 BERT [[9](#bib.bib9)] 为例，导致了多样且有效的模型架构。
- en: 1.3.4 Large Language Models (LLMs)
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.4 大型语言模型（LLMs）
- en: 'LLMs like GPT-3, GPT-4, PaLM [[10](#bib.bib10)], and LLaMA [[11](#bib.bib11)]
    are trained on massive text corpora with tens of billions of parameters. LLMs
    undergo a two-stage process: initial pre-training on a vast corpus followed by
    alignment with human values. This approach enables LLMs to understand human commands
    and values better.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3、GPT-4、PaLM [[10](#bib.bib10)] 和 LLaMA [[11](#bib.bib11)] 这样的 LLMs 在包含数十亿参数的大规模文本语料库上进行训练。LLMs
    经过两个阶段的过程：首先是在广泛的语料库上进行初步预训练，然后与人类价值观对齐。这种方法使 LLMs 更好地理解人类的指令和价值观。
- en: 1.4 Overview of Current Leading LLMs
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 当前领先 LLMs 的概述
- en: LLMs are powerful tools in NLP, capable of performing tasks such as translation,
    summarisation, and conversational interaction. Advances in transformer architectures,
    computational power, and extensive datasets have driven their success. These models
    approximate human-level performance, making them invaluable for research and practical
    implementations. LLMs’ rapid development has spurred research into architectural
    innovations, training strategies, extending context lengths, fine-tuning techniques,
    and integrating multi-modal data. Their applications extend beyond NLP, aiding
    in human-robot interactions and creating intuitive AI systems. This highlights
    the importance of comprehensive reviews consolidating the latest developments
    [[12](#bib.bib12)].
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 是 NLP 中强大的工具，能够执行诸如翻译、摘要和对话互动等任务。变压器架构、计算能力和广泛数据集的进步推动了它们的成功。这些模型接近人类水平的性能，使它们在研究和实际应用中不可或缺。LLMs
    的快速发展推动了对架构创新、训练策略、扩展上下文长度、微调技术以及多模态数据集成的研究。它们的应用超越了 NLP，有助于人机交互和创建直观的 AI 系统。这突显了综合评审汇总最新发展的重要性
    [[12](#bib.bib12)]。
- en: '![Refer to caption](img/ab45c3d014aec95d74422eb9113d2392.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ab45c3d014aec95d74422eb9113d2392.png)'
- en: 'Figure 1.3: Mind map depicting various dimensions of Large Language Models
    (LLMs), covering aspects from pre-training and fine-tuning methodologies to efficiency,
    evaluation, inference, and application domains. Each dimension is linked to specific
    techniques, challenges, and examples of models that exemplify the discussed characteristics.
    This diagram serves as an overview of the multifaceted considerations in the development
    and deployment of LLMs. (adapted from [[13](#bib.bib13)])'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.3: 思维导图展示了大语言模型（LLMs）的各个维度，包括从预训练和微调方法到效率、评估、推理和应用领域的各个方面。每个维度都与特定的技术、挑战和示例模型相关联。这张图表作为
    LLMS 发展和部署中的多维考虑的概述。(改编自 [[13](#bib.bib13)])'
- en: 'Figure [1.3](#Ch1.F3 "Figure 1.3 ‣ 1.4 Overview of Current Leading LLMs ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") provides an overview of current leading
    LLMs, highlighting their capabilities and applications.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.3](#Ch1.F3 "图 1.3 ‣ 1.4 当前领先 LLM 概述 ‣ 第1章 引言 ‣ 《从基础到突破的终极指南：大语言模型微调的全面评审，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）》")
    提供了当前领先 LLM 的概述，突出了它们的能力和应用。
- en: 1.5 What is Fine-Tuning?
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 什么是微调？
- en: Fine-tuning uses a pre-trained model, such as OpenAI’s GPT series, as a foundation.
    The process involves further training on a smaller, domain-specific dataset. This
    approach builds upon the model’s pre-existing knowledge, enhancing performance
    on specific tasks with reduced data and computational requirements.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 微调使用预训练模型，如 OpenAI 的 GPT 系列，作为基础。这个过程涉及在较小的、领域特定的数据集上进一步训练。这种方法在模型现有知识的基础上进行，增强了在特定任务上的性能，同时减少了数据和计算需求。
- en: Fine-tuning transfers the pre-trained model’s learned patterns and features
    to new tasks, improving performance and reducing training data needs. It has become
    popular in NLP for tasks like text classification, sentiment analysis, and question-answering.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 微调将预训练模型的学习模式和特征转移到新任务上，提升性能并减少训练数据需求。它在自然语言处理（NLP）中变得非常流行，用于文本分类、情感分析和问答等任务。
- en: 1.6 Types of LLM Fine-Tuning
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6 LLM 微调类型
- en: 1.6.1 Unsupervised Fine-Tuning
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.6.1 无监督微调
- en: This method does not require labelled data. Instead, the LLM is exposed to a
    large corpus of unlabelled text from the target domain, refining its understanding
    of language. This approach is useful for new domains like legal or medical fields
    but is less precise for specific tasks such as classification or summarisation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不需要标注数据。相反，LLM 接触到目标领域的大量未标注文本，从而完善对语言的理解。这种方法适用于法律或医疗等新领域，但对于特定任务如分类或摘要则精确度较低。
- en: 1.6.2 Supervised Fine-Tuning (SFT)
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.6.2 有监督微调 (SFT)
- en: SFT involves providing the LLM with labelled data tailored to the target task.
    For example, fine-tuning an LLM for text classification in a business context
    uses a dataset of text snippets with class labels. While effective, this method
    requires substantial labelled data, which can be costly and time-consuming to
    obtain.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 涉及向 LLM 提供针对目标任务的标注数据。例如，在商业背景下对文本分类进行微调时，会使用带有类别标签的文本片段数据集。虽然有效，但这种方法需要大量标注数据，这可能既昂贵又耗时。
- en: 1.6.3 Instruction Fine-Tuning via Prompt Engineering
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.6.3 通过提示工程进行指令微调
- en: This method relies on providing the LLM with natural language instructions,
    useful for creating specialised assistants. It reduces the need for vast amounts
    of labelled data but depends heavily on the quality of the prompts.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法依赖于向 LLM 提供自然语言指令，这对于创建专用助手很有用。它减少了对大量标注数据的需求，但高度依赖于提示的质量。
- en: 1.7 Pre-training vs Fine-tuning
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7 预训练与微调
- en: 'Table [1.1](#Ch1.T1 "Table 1.1 ‣ 1.7 Pre-training vs Fine-tuning ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") provides a comparison between pre-training
    and fine-tuning, highlighting their respective characteristics and processes.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1.1](#Ch1.T1 "表格 1.1 ‣ 1.7 预训练与微调 ‣ 第1章 引言 ‣ 《从基础到突破的终极指南：大语言模型微调的全面评审，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）》")
    提供了预训练与微调的比较，突出了它们各自的特点和过程。
- en: '| Aspect | Pre-training | Fine-tuning |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 预训练 | 微调 |'
- en: '| --- | --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Definition | Training on a vast amount of unlabelled text data | Adapting
    a pre-trained model to specific tasks |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 定义 | 在大量未标记文本数据上训练 | 将预训练模型适应于特定任务 |'
- en: '| Data Requirement | Extensive and diverse unlabelled text data | Smaller,
    task-specific labelled data |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 数据要求 | 大量多样的未标记文本数据 | 较小的、特定任务的标记数据 |'
- en: '| Objective | Build general linguistic knowledge | Specialise model for specific
    tasks |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 建立通用语言知识 | 针对特定任务对模型进行专业化 |'
- en: '| Process | Data collection, training on large dataset, predict next word/sequence
    | Task-specific data collection, modify last layer for task, train on new dataset,
    generate output based on tasks |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 过程 | 数据收集，大数据集训练，预测下一个词/序列 | 特定任务的数据收集，修改最后一层以适应任务，在新数据集上训练，根据任务生成输出 |'
- en: '| Model Modification | Entire model trained | Last layers adapted for new task
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 模型修改 | 整个模型训练 | 对新任务进行最后一层的适配 |'
- en: '| Computational Cost | High (large dataset, complex model) | Lower (smaller
    dataset, fine-tuning layers) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 计算成本 | 高（大数据集，复杂模型） | 较低（小数据集，微调层） |'
- en: '| Training Duration | Weeks to months | Days to weeks |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 训练时间 | 数周到数月 | 数天到数周 |'
- en: '| Purpose | General language understanding | Task-specific performance improvement
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 通用语言理解 | 特定任务的性能提升 |'
- en: '| Examples | GPT, LLaMA 3 | Fine-tuning LLaMA 3 for summarisation |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | GPT，LLaMA 3 | 微调LLaMA 3以进行总结 |'
- en: 'Table 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large
    Language Models (LLMs). The table outlines key differences between the pre-training
    and fine-tuning phases across various aspects such as definition, data requirements,
    objectives, processes, model modification, computational costs, training duration,
    and their respective purposes, with examples highlighting specific models and
    tasks. Pre-training involves extensive training on vast amounts of unlabelled
    data to build general linguistic knowledge, while fine-tuning adapts the pre-trained
    models to specialised tasks using smaller, labelled datasets, focusing on task-specific
    performance improvements.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1：大型语言模型（LLMs）中预训练与微调的对比概述。该表格概述了预训练和微调阶段在定义、数据要求、目标、过程、模型修改、计算成本、训练时间及其各自目的等方面的关键差异，并通过示例突出特定模型和任务。预训练涉及在大量未标记数据上进行广泛训练，以建立通用语言知识，而微调则使用较小的标记数据集将预训练模型适应于专业任务，关注于任务特定的性能提升。
- en: 1.8 Importance of Fine-Tuning LLMs
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8 微调大型语言模型的重要性
- en: '1.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training,
    adapting it to specific tasks with reduced computation time and resources.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迁移学习：微调利用预训练过程中获得的知识，将其适应于特定任务，从而减少计算时间和资源。
- en: '2.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Reduced Data Requirements: Fine-tuning requires less labelled data, focusing
    on tailoring pre-trained features to the target task.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少数据要求：微调需要较少的标记数据，重点是将预训练的特征调整为目标任务。
- en: '3.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Improved Generalisation: Fine-tuning enhances the model’s ability to generalise
    to specific tasks or domains, capturing general language features and customising
    them.'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改善泛化能力：微调增强了模型对特定任务或领域的泛化能力，捕捉通用语言特征并进行定制。
- en: '4.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Efficient Model Deployment: Fine-tuned models are more efficient for real-world
    applications, being computationally efficient and well-suited for specific tasks.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的模型部署：微调后的模型在实际应用中更为高效，计算上更为高效，且适合特定任务。
- en: '5.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of
    tasks, performing well across various applications without task-specific architectures.'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对各种任务的适应性：微调后的LLMs能够适应广泛的任务，在各种应用中表现良好，无需特定任务架构。
- en: '6.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific
    tasks by adjusting to the nuances and vocabulary of the target domain.'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特定领域的性能：微调使模型能够在特定领域任务中表现出色，通过调整目标领域的细微差别和词汇。
- en: '7.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Faster Convergence: Fine-tuning usually achieves faster convergence, starting
    with weights that already capture general language features.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更快的收敛：微调通常实现更快的收敛，从已经捕捉到通用语言特征的权重开始。
- en: 1.9 Retrieval Augmented Generation (RAG)
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9 检索增强生成（RAG）
- en: 'A popular method to utilise your own data is by incorporating it into the prompt
    when querying the LLM model. This approach, known as Retrieval-Augmented Generation
    (RAG), involves retrieving relevant data and using it as additional context for
    the LLM. Instead of depending solely on knowledge from the training data, a RAG
    workflow pulls pertinent information, connecting static LLMs with real-time data
    retrieval. With RAG architecture, organisations can deploy any LLM model and enhance
    it to return relevant results by providing a small amount of their own data (see
    Figure[1.4](#Ch1.F4 "Figure 1.4 ‣ 1.9 Retrieval Augmented Generation (RAG) ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") for visual workflow). This process
    avoids the costs and time associated with fine-tuning or pre-training the model.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一种利用自己数据的流行方法是在查询LLM模型时将数据纳入提示中。这种方法称为检索增强生成（RAG），涉及检索相关数据并将其作为LLM的附加上下文。RAG工作流程不仅依赖于训练数据中的知识，还提取相关信息，将静态LLM与实时数据检索连接起来。使用RAG架构，组织可以部署任何LLM模型，并通过提供少量自己的数据来增强模型的相关结果（参见图[1.4](#Ch1.F4
    "图1.4 ‣ 1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调完全评审：技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")以获得可视化工作流程）。此过程避免了微调或预训练模型相关的成本和时间。
- en: '![Refer to caption](img/c3249a348733e733b4e38128d7d66e98.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c3249a348733e733b4e38128d7d66e98.png)'
- en: 'Figure 1.4: An illustration of the Traditional Retrieval-Augmented Generation
    (RAG) pipeline steps, depicting the sequential process from client query to response
    generation. The pipeline starts with the client’s question, followed by semantic
    search in a vector database, contextually enriching the data before generating
    a prompt for the large language model (LLM). The final response is post-processed
    and returned to the client.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：传统检索增强生成（RAG）流程步骤的示意图，展示了从客户端查询到响应生成的顺序过程。流程从客户端的问题开始，随后在向量数据库中进行语义搜索，语境丰富数据，然后为大型语言模型（LLM）生成提示。最终回应经过后处理并返回给客户端。
- en: 1.9.1 Traditional RAG Pipeline and Steps
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.1 传统RAG流程和步骤
- en: '1.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Data Indexing: Organise data efficiently for quick retrieval. This involves
    processing, chunking, and storing data in a vector database using indexing strategies
    like search indexing, vector indexing, and hybrid indexing.'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据索引：高效地组织数据以便快速检索。这包括使用搜索索引、向量索引和混合索引等策略处理、分块和存储数据到向量数据库中。
- en: '2.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Input Query Processing: Refine user queries to improve compatibility with indexed
    data. This can include simplification or vector transformation of queries for
    enhanced search efficiency.'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入查询处理：优化用户查询，以提高与索引数据的兼容性。这可能包括对查询进行简化或向量变换，以提高搜索效率。
- en: '3.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Searching and Ranking: Retrieve and rank data based on relevance using search
    algorithms such as TF-IDF, BM25, and deep learning models like BERT to interpret
    the query’s intent and context.'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索和排序：使用TF-IDF、BM25等搜索算法以及BERT等深度学习模型，根据相关性检索和排序数据，以解释查询的意图和上下文。
- en: '4.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Prompt Augmentation: Incorporate relevant information from the search results
    into the original query to provide the LLM with additional context, enhancing
    response accuracy and relevance.'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示增强：将搜索结果中的相关信息纳入原始查询，为LLM提供额外的上下文，提高回应的准确性和相关性。
- en: '5.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Response Generation: Use the augmented prompt to generate responses that combine
    the LLM’s knowledge with current, specific data, ensuring high-quality, contextually
    grounded answers.'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回应生成：使用增强的提示生成结合LLM知识和当前具体数据的回应，确保高质量、上下文扎实的答案。
- en: 1.9.2 Benefits of Using RAG
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.2 使用RAG的好处
- en: •
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Up-to-Date and Accurate Responses: Enhances the LLM’s responses with current
    external data, improving accuracy and relevance.'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最新和准确的回应：通过引入当前的外部数据来增强LLM的回应，提高准确性和相关性。
- en: •
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reducing Inaccurate Responses: Grounds the LLM’s output in relevant knowledge,
    reducing the risk of generating incorrect information.'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少不准确的回应：将LLM的输出基于相关知识，降低生成不正确信息的风险。
- en: •
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain-Specific Responses: Delivers contextually relevant responses tailored
    to an organisation’s proprietary data.'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域特定回应：提供针对组织专有数据量身定制的上下文相关回应。
- en: •
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising
    LLMs without extensive model fine-tuning.'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效率和成本效益：提供一种经济高效的定制LLM的方法，无需广泛的模型微调。
- en: 1.9.3 Challenges and Considerations in Serving RAG
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.3 服务RAG的挑战和考虑因素
- en: '1.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'User Experience: Ensuring rapid response times suitable for real-time applications.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户体验：确保快速响应时间适合实时应用。
- en: '2.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Cost Efficiency: Managing the costs associated with serving millions of responses.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本效益：管理提供数百万回应的成本。
- en: '3.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Accuracy: Ensuring outputs are accurate to avoid misinformation.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性：确保输出准确，以避免错误信息。
- en: '4.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Recency and Relevance: Keeping responses and content current with the latest
    data.'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 近期性和相关性：保持回应和内容与最新数据同步。
- en: '5.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Business Context Awareness: Aligning LLM responses with specific business contexts.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 商业背景意识：使LLM的回答与特定商业背景对齐。
- en: '6.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Service Scalability: Managing increased capacity while controlling costs.'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 服务可扩展性：在控制成本的同时管理增加的容量。
- en: '7.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Security and Governance: Implementing protocols for data security, privacy,
    and governance.'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全性和治理：实施数据安全、隐私和治理的协议。
- en: 1.9.4 Use Cases and Examples
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.4 使用案例和示例
- en: '1.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate
    answers from company documents, enhancing customer support.'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问答聊天机器人：将LLM与聊天机器人集成，从公司文档中生成准确答案，提升客户支持。
- en: '2.'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Search Augmentation: Enhance search engines with LLM-generated answers for
    more accurate informational queries.'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索增强：通过LLM生成的答案增强搜索引擎，以提高信息查询的准确性。
- en: '3.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Knowledge Engine: Use LLMs to answer questions related to internal functions,
    such as HR and compliance, using company data.'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识引擎：利用LLM回答与内部职能（如人力资源和合规性）相关的问题，使用公司数据。
- en: 1.9.5 Considerations for Choosing Between RAG and Fine-Tuning
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.5 选择RAG和微调的考虑因素
- en: 'When considering external data access, RAG is likely a superior option for
    applications needing to access external data sources. Fine-tuning, on the other
    hand, is more suitable if you require the model to adjust its behaviour, and writing
    style, or incorporate domain-specific knowledge. In terms of suppressing hallucinations
    and ensuring accuracy, RAG systems tend to perform better as they are less prone
    to generating incorrect information. If you have ample domain-specific, labelled
    training data, fine-tuning can result in a more tailored model behaviour, whereas
    RAG systems are robust alternatives when such data is scarce. RAG systems provide
    an advantage with dynamic data retrieval capabilities for environments where data
    frequently updates or changes. Additionally, it is crucial to ensure the transparency
    and interpret ability of the model’s decision-making process. In that case, RAG
    systems offer insight that is typically not available in models that are solely
    fine-tuned. Figure[1.5](#Ch1.F5 "Figure 1.5 ‣ 1.9.5 Considerations for Choosing
    Between RAG and Fine-Tuning ‣ 1.9 Retrieval Augmented Generation (RAG) ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") illustrates the visual representation
    alongside example use cases.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑外部数据访问时，RAG可能是需要访问外部数据源的应用的更优选择。另一方面，微调更适合于需要模型调整行为、写作风格或融入领域特定知识的情况。在抑制幻觉和确保准确性方面，RAG系统通常表现更好，因为它们较少生成错误信息。如果你有大量领域特定的标注训练数据，微调可以产生更具针对性的模型行为，而RAG系统在此类数据稀缺时则是稳健的替代方案。RAG系统在数据频繁更新或变化的环境中提供动态数据检索能力的优势。此外，确保模型决策过程的透明性和可解释性也至关重要。在这种情况下，RAG系统提供的洞察通常是仅依赖微调模型无法获得的。图[1.5](#Ch1.F5
    "图 1.5 ‣ 1.9.5 选择RAG和微调的考虑因素 ‣ 1.9 检索增强生成 (RAG) ‣ 第1章 引言 ‣ 从基础到突破的终极LLM微调指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评审（版本1.0)")
    展示了视觉表现和示例使用案例。
- en: '![Refer to caption](img/7c127a3142ffca33f4afc49e11879fe7.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c127a3142ffca33f4afc49e11879fe7.png)'
- en: 'Figure 1.5: Graph comparing the model adaptation required versus the level
    of external knowledge needed across different scenarios, highlighting the roles
    of Retrieval-Augmented Generation (RAG), Fine-Tuning, and their hybrid applications
    in various contexts such as Q&A systems, customer support automation, and summarisation
    tasks. (adapted from [[14](#bib.bib14)])'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：图表比较了在不同情境下模型适应所需的外部知识水平，突出了**检索增强生成**（RAG）、微调及其在问答系统、客户支持自动化和总结任务等各种背景中的混合应用的角色。（改编自[[14](#bib.bib14)]）
- en: 1.10 Objectives of the Report
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.10 报告目标
- en: 1.10.1 Goals and Scope
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.10.1 目标和范围
- en: The primary goal of this report is to conduct a comprehensive analysis of fine-tuning
    techniques for LLMs. This involves exploring theoretical foundations, practical
    implementation strategies, and challenges. The report examines various fine-tuning
    methodologies, their applications, and recent advancements.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告的主要目标是对LLM微调技术进行全面分析。这包括探讨理论基础、实际实施策略和挑战。报告考察了各种微调方法、它们的应用和近期的进展。
- en: 1.10.2 Key Questions and Issues Addressed
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.10.2 关键问题和讨论议题
- en: This report addresses critical questions surrounding fine-tuning LLMs, starting
    with foundational insights into LLMs, their evolution, and significance in NLP.
    It defines fine-tuning, distinguishes it from pre-training, and emphasises its
    role in adapting models for specific tasks. Key objectives include enhancing model
    performance for targeted applications and domains.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告解决了关于LLM微调的关键问题，从LLM的基础知识、演变和在自然语言处理中的重要性开始。它定义了微调，区分了预训练，并强调其在模型特定任务适应中的作用。关键目标包括提高模型在特定应用和领域中的表现。
- en: The report outlines a structured fine-tuning process, featuring a high-level
    pipeline with visual representations and detailed stage explanations. It covers
    practical implementation strategies, including model initialisation, hyperparameter
    definition, and fine-tuning techniques such as Parameter-Efficient Fine-Tuning
    (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation
    methods, deployment challenges, and recent advancements are also explored.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 报告概述了一个结构化的微调过程，包含一个高层次的流程图、视觉表现和详细的阶段解释。它涵盖了实际的实施策略，包括模型初始化、超参数定义以及如**参数高效微调**（PEFT）和**检索增强生成**（RAG）等微调技术。还探讨了行业应用、评估方法、部署挑战和近期的进展。
- en: 1.10.3 Overview of the Report Structure
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.10.3 报告结构概述
- en: The rest of the report provides a comprehensive understanding of fine-tuning
    LLMs. The main chapters include an in-depth look at the fine-tuning pipeline,
    practical applications, model alignment, evaluation metrics, and challenges. The
    concluding sections discuss the evolution of fine-tuning techniques, highlight
    ongoing research challenges, and provide insights for researchers and practitioners.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的其余部分提供了对LLM微调的全面理解。主要章节包括对微调流程的深入分析、实际应用、模型对齐、评估指标和挑战。结论部分讨论了微调技术的发展，强调了持续的研究挑战，并为研究人员和从业人员提供了见解。
- en: Chapter 2 Seven Stage Fine-Tuning Pipeline for LLM
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章 LLM的七阶段微调流程
- en: Fine-tuning a Large Language Model (LLM) is a comprehensive process divided
    into seven distinct stages, each essential for adapting the pre-trained model
    to specific tasks and ensuring optimal performance. These stages encompass everything
    from initial dataset preparation to the final deployment and maintenance of the
    fine-tuned model. By following these stages systematically, the model is refined
    and tailored to meet precise requirements, ultimately enhancing its ability to
    generate accurate and contextually appropriate responses. The seven stages include
    Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-Tuning,
    Evaluation and Validation, Deployment, and Monitoring and Maintenance.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLM）是一个全面的过程，分为七个独特的阶段，每个阶段对于将预训练模型适应特定任务并确保最佳性能至关重要。这些阶段涵盖了从初始数据集准备到最终部署和维护微调模型的所有内容。通过系统地遵循这些阶段，模型被精细化并调整以满足精确的要求，*最终提升其生成准确且上下文适当回应的能力*。七个阶段包括数据集准备、模型初始化、训练环境设置、微调、评估与验证、部署以及监控与维护。
- en: '![Refer to caption](img/b68b9db62519b7aac15496aeeb666712.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b68b9db62519b7aac15496aeeb666712.png)'
- en: 'Figure 2.1: A comprehensive pipeline for fine-tuning Large Language Models
    (LLMs), illustrating the seven essential stages: Dataset Preparation, Model Initialisation,
    Training Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment,
    and Monitoring and Maintenance. Each stage plays a crucial role in adapting the
    pre-trained model to specific tasks and ensuring optimal performance throughout
    its lifecycle.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：一个全面的微调大型语言模型（LLMs）流程图，展示了七个关键阶段：数据集准备、模型初始化、训练环境设置、微调、评估和验证、部署，以及监控和维护。每个阶段在将预训练模型适应特定任务并确保其在生命周期内的最佳性能中都发挥着至关重要的作用。
- en: 'Figure [2.1](#Ch2.F1 "Figure 2.1 ‣ Chapter 2 Seven Stage Fine-Tuning Pipeline
    for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") illustrates the comprehensive pipeline
    for fine-tuning LLMs, encompassing all necessary stages from dataset preparation
    to monitoring and maintenance.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2.1](#Ch2.F1 "图 2.1 ‣ 第 2 章 七阶段 LLM 微调流程 ‣ 微调 LLM 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本
    1.0）") 展示了 LLM 微调的全面流程，包括从数据集准备到监控和维护的所有必要阶段。
- en: '2.1 Stage 1: Dataset Preparation'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 阶段 1：数据集准备
- en: Fine-tuning a Large Language Model (LLM) starts with adapting the pre-trained
    model for specific tasks by updating its parameters using a new dataset. This
    involves cleaning and formatting the dataset to match the target task, such as
    instruction tuning, sentiment analysis, or topic mapping. The dataset is composed
    of $$<\text{input},\text{output}></math> pairs, demonstrating the desired behaviour
    for the model.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLM）从通过使用新数据集更新其参数来适应预训练模型于特定任务开始。这涉及清理和格式化数据集，以匹配目标任务，如指令调整、情感分析或主题映射。数据集由$$<\text{input},\text{output}></math>对组成，展示了模型期望的行为。
- en: 'For example, in instruction tuning, the dataset may look like:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在指令调整中，数据集可能看起来像这样：
- en: '[PRE0]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the ’Input Query’ is what the user asks, and the ’Generated Output’ is
    the model’s response. The structure and style of these pairs can be adjusted based
    on the specific needs of the task.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，‘输入查询’是用户提问的内容，而‘生成输出’是模型的响应。这些对的结构和风格可以根据任务的具体需求进行调整。
- en: '2.2 Stage 2: Model Initialisation'
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 阶段 2：模型初始化
- en: Model initialisation is the process of setting up the initial parameters and
    configurations of the LLM before training or deploying it. This step is crucial
    for ensuring the model performs optimally, trains efficiently, and avoids issues
    such as vanishing or exploding gradients.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 模型初始化是指在训练或部署 LLM 之前设置初始参数和配置的过程。这个步骤对确保模型最佳性能、有效训练以及避免诸如梯度消失或爆炸等问题至关重要。
- en: '2.3 Stage 3: Training Environment Setup'
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 阶段 3：训练环境设置
- en: Setting up the training environment for LLM fine-tuning involves configuring
    the necessary infrastructure to adapt a pre-existing model for specific tasks.
    This includes selecting relevant training data, defining the model’s architecture
    and hyperparameters, and running training iterations to adjust the model’s weights
    and biases. The aim is to enhance the LLM’s performance in generating accurate
    and contextually appropriate outputs tailored to specific applications, like content
    creation, translation, or sentiment analysis. Successful fine-tuning relies on
    careful preparation and rigorous experimentation.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 为 LLM 微调设置训练环境涉及配置必要的基础设施，以将现有模型调整为特定任务。这包括选择相关的训练数据、定义模型的架构和超参数，以及运行训练迭代来调整模型的权重和偏差。目标是提升
    LLM 在生成准确和上下文适当的输出方面的表现，以满足特定应用需求，如内容创作、翻译或情感分析。成功的微调依赖于细致的准备和严格的实验。
- en: '2.4 Stage 4: Partial or Full Fine-Tuning'
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 阶段 4：部分或完全微调
- en: This stage involves updating the parameters of the LLM using a task-specific
    dataset. Full fine-tuning updates all parameters of the model, ensuring comprehensive
    adaptation to the new task. Alternatively, Half fine-tuning (HFT) [[15](#bib.bib15)]
    or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter layers,
    can be employed to partially fine-tune the model. This method attaches additional
    layers to the pre-trained model, allowing for efficient fine-tuning with fewer
    parameters, which can address challenges related to computational efficiency,
    overfitting, and optimisation.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段涉及使用特定任务的数据集来更新 LLM 的参数。全面微调更新模型的所有参数，确保对新任务的全面适应。或者，可以采用半微调（HFT）[[15](#bib.bib15)]
    或参数高效微调（PEFT）方法，例如使用适配器层，来部分微调模型。这种方法将额外的层附加到预训练模型上，从而以更少的参数进行高效微调，这可以解决计算效率、过拟合和优化相关的挑战。
- en: '2.5 Stage 5: Evaluation and Validation'
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.5 阶段 5: 评估和验证'
- en: Evaluation and validation involve assessing the fine-tuned LLM’s performance
    on unseen data to ensure it generalises well and meets the desired objectives.
    Evaluation metrics, such as cross-entropy, measure prediction errors, while validation
    monitors loss curves and other performance indicators to detect issues like overfitting
    or underfitting. This stage helps guide further fine-tuning to achieve optimal
    model performance.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 评估和验证涉及在未见过的数据上评估微调后的 LLM 的表现，以确保其良好的泛化能力并达到预期目标。评估指标，如交叉熵，衡量预测误差，而验证则监控损失曲线和其他性能指标，以检测如过拟合或欠拟合的问题。这一阶段有助于指导进一步的微调，以实现最佳的模型性能。
- en: '2.6 Stage 6: Deployment'
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.6 阶段 6: 部署'
- en: Deploying an LLM means making it operational and accessible for specific applications.
    This involves configuring the model to run efficiently on designated hardware
    or software platforms, ensuring it can handle tasks like natural language processing,
    text generation, or user query understanding. Deployment also includes setting
    up integration, security measures, and monitoring systems to ensure reliable and
    secure performance in real-world applications.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 LLM 意味着使其在特定应用中可操作和可访问。这涉及将模型配置为在指定的硬件或软件平台上高效运行，确保它能够处理自然语言处理、文本生成或用户查询理解等任务。部署还包括设置集成、安全措施和监控系统，以确保在实际应用中的可靠和安全的性能。
- en: '2.7 Stage 7: Monitoring and Maintenance'
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.7 阶段 7: 监控和维护'
- en: Monitoring and maintaining an LLM after deployment is crucial to ensure ongoing
    performance and reliability. This involves continuously tracking the model’s performance,
    addressing any issues that arise, and updating the model as needed to adapt to
    new data or changing requirements. Effective monitoring and maintenance help sustain
    the model’s accuracy and effectiveness over time.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署后监控和维护 LLM 是至关重要的，以确保持续的性能和可靠性。这涉及不断跟踪模型的表现，解决出现的任何问题，并根据需要更新模型以适应新数据或变化的需求。有效的监控和维护有助于保持模型的准确性和有效性。
- en: 'Chapter 3 Stage 1: Data Preparation'
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '第三章 阶段 1: 数据准备'
- en: 3.1 Steps Involved in Data Preparation
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据准备步骤
- en: 3.1.1 Data Collection
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 数据收集
- en: 'The first step in data preparation is to collect data from various sources.
    These sources can be in any format such as CSV, web pages, SQL databases, S3 storage,
    etc. Python provides several libraries to gather the data efficiently and accurately.
    Table [3.1](#Ch3.T1 "Table 3.1 ‣ 3.1.1 Data Collection ‣ 3.1 Steps Involved in
    Data Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)") presents a selection of commonly used data formats along with the corresponding
    Python libraries used for data collection.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '数据准备的第一步是从各种来源收集数据。这些来源可以是任何格式，例如 CSV、网页、SQL 数据库、S3 存储等。Python 提供了多个库来高效、准确地收集数据。表
    [3.1](#Ch3.T1 "Table 3.1 ‣ 3.1.1 Data Collection ‣ 3.1 Steps Involved in Data
    Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")
    展示了一些常用的数据格式及其对应的 Python 库。'
- en: '| Data Format | Python Library | Description | Library Link |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 数据格式 | Python 库 | 描述 | 库链接 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| CSV Files | pandas | pandas is a powerful library for data manipulation and
    analysis. It provides the read_csv function for easy and efficient reading of
    CSV files into DataFrame objects. It also supports reading data in Excel, JSON,
    and more. | [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)
    |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| CSV 文件 | pandas | pandas 是一个强大的数据处理和分析库。它提供了 read_csv 函数，用于将 CSV 文件高效地读取到
    DataFrame 对象中。它还支持读取 Excel、JSON 等格式的数据。 | [pandas 文档](https://pandas.pydata.org/pandas-docs/stable/)
    |'
- en: '| Web Pages | BeautifulSoup and requests | BeautifulSoup is a library for parsing
    HTML and XML documents. Combined with requests for sending HTTP requests, it enables
    data extraction from web pages, essential for web scraping tasks. | [BeautifulSoup
    documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), [requests
    documentation](https://requests.readthedocs.io/en/latest/) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 网页 | BeautifulSoup 和 requests | BeautifulSoup 是一个用于解析 HTML 和 XML 文档的库。结合
    requests 进行 HTTP 请求，它能够从网页中提取数据，这对于网页抓取任务至关重要。 | [BeautifulSoup 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/),
    [requests 文档](https://requests.readthedocs.io/en/latest/) |'
- en: '| SQL Databases | SQLAlchemy | SQLAlchemy is a SQL toolkit and Object-Relational
    Mapping (ORM) library for Python, providing a full suite of enterprise-level persistence
    patterns. | [SQLAlchemy documentation](https://www.sqlalchemy.org/) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| SQL 数据库 | SQLAlchemy | SQLAlchemy 是一个 SQL 工具包和 Python 的对象关系映射（ORM）库，提供了一整套企业级持久性模式。
    | [SQLAlchemy 文档](https://www.sqlalchemy.org/) |'
- en: '| S3 Storage | boto3 | boto3 is the Amazon Web Services (AWS) SDK for Python,
    allowing developers to use services like Amazon S3 and EC2\. It enables interaction
    with AWS services, including uploading, downloading, and managing S3 bucket files.
    | [boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| S3 存储 | boto3 | boto3 是 Amazon Web Services (AWS) 的 Python SDK，允许开发人员使用 Amazon
    S3 和 EC2 等服务。它使与 AWS 服务的交互成为可能，包括上传、下载和管理 S3 桶中的文件。 | [boto3 文档](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    |'
- en: '| Data Integration | RapidMiner | RapidMiner is a comprehensive environment
    for data preparation, machine learning, and predictive analytics, allowing efficient
    processing and transformation of raw data into actionable insights. | [RapidMiner
    documentation](https://rapidminer.com/) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 数据集成 | RapidMiner | RapidMiner 是一个全面的数据准备、机器学习和预测分析环境，允许高效地处理和转换原始数据，提取有用的见解。
    | [RapidMiner 文档](https://rapidminer.com/) |'
- en: '| Data Cleaning | Trifacta Wrangler | Trifacta Wrangler focuses on simplifying
    and automating data wrangling processes, transforming raw data into clean and
    structured formats. | [Trifacta Wrangler documentation](https://www.trifacta.com/)
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 数据清洗 | Trifacta Wrangler | Trifacta Wrangler 专注于简化和自动化数据清洗过程，将原始数据转换为干净且结构化的格式。
    | [Trifacta Wrangler 文档](https://www.trifacta.com/) |'
- en: 'Table 3.1: Python libraries and tools for data collection and integration in
    various formats, providing an overview of commonly used libraries, their functions,
    and links to their official documentation for efficient data management and processing.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3.1: 各种格式的数据收集与集成的 Python 库和工具概述，提供了常用库、其功能及其官方文档链接，以便于高效的数据管理和处理。'
- en: 3.1.2 Data Preprocessing and Formatting
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 数据预处理与格式化
- en: 'Data preprocessing and formatting are crucial for ensuring high-quality data
    for fine-tuning. This step involves tasks such as cleaning the data, handling
    missing values, and formatting the data to match the specific requirements of
    the task. Several libraries assist with text data processing and Table [3.2](#Ch3.T2
    "Table 3.2 ‣ 3.1.2 Data Preprocessing and Formatting ‣ 3.1 Steps Involved in Data
    Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")
    contains some of the most commonly used data preprocessing libraries in python.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '数据预处理与格式化对于确保高质量的数据以进行微调至关重要。此步骤包括清理数据、处理缺失值以及格式化数据以符合任务的特定要求。几个库可以帮助处理文本数据，表
    [3.2](#Ch3.T2 "Table 3.2 ‣ 3.1.2 Data Preprocessing and Formatting ‣ 3.1 Steps
    Involved in Data Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)") 包含了一些在 Python 中最常用的数据预处理库。'
- en: '| Library Name | Data Preprocessing Options | Link |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 库名称 | 数据预处理选项 | 链接 |'
- en: '| --- | --- | --- |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| spaCy | spaCy provides robust capabilities for text preprocessing, including
    tokenization, lemmatization, and efficient sentence boundary detection. | [spaCy
    documentation](https://spacy.io/) |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| spaCy | spaCy 提供了强大的文本预处理功能，包括分词、词形还原和高效的句子边界检测。 | [spaCy documentation](https://spacy.io/)
    |'
- en: '| NLTK | NLTK offers a comprehensive set of tools for data preprocessing, such
    as tokenization, stemming, and stop word removal. | [NLTK documentation](https://www.nltk.org/)
    |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| NLTK | NLTK 提供了一整套数据预处理工具，如分词、词干提取和停用词移除。 | [NLTK documentation](https://www.nltk.org/)
    |'
- en: '| HuggingFace | HuggingFace provides extensive capabilities for text preprocessing
    through its transformers library, including functionalities for tokenization and
    support for various pre-trained models. | [HuggingFace documentation](https://huggingface.co/)
    |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| HuggingFace | HuggingFace 通过其 transformers 库提供了广泛的文本预处理功能，包括分词功能和对各种预训练模型的支持。
    | [HuggingFace documentation](https://huggingface.co/) |'
- en: '| KNIME | KNIME Analytics Platform allows visual workflow design for data integration,
    preprocessing, and advanced manipulations like text mining and image analysis.
    | [KNIME documentation](https://www.knime.com/) |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| KNIME | KNIME Analytics Platform 允许通过可视化工作流设计进行数据集成、预处理以及文本挖掘和图像分析等高级操作。
    | [KNIME documentation](https://www.knime.com/) |'
- en: 'Table 3.2: Outline of Python libraries commonly used for text data preprocessing,
    including spaCy, NLTK, HuggingFace, and KNIME. It details the specific preprocessing
    options offered by each library and provides links to their official documentation
    for users seeking more in-depth guidance on their use.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：概述了常用的文本数据预处理 Python 库，包括 spaCy、NLTK、HuggingFace 和 KNIME。详细介绍了每个库提供的特定预处理选项，并提供了其官方文档的链接，以便用户获取更深入的使用指导。
- en: 3.1.3 Handling Data Imbalance
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 处理数据不平衡
- en: 'Handling imbalanced datasets is crucial for ensuring balanced performance across
    all classes. Several techniques and strategies are employed:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集对于确保所有类别的性能平衡至关重要。采用了几种技术和策略：
- en: '1.'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Over-sampling and Under-sampling: Techniques like SMOTE (Synthetic Minority
    Over-sampling Technique) generate synthetic examples to achieve balance.'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过采样和欠采样：像SMOTE（合成少数类过采样技术）这样的技术通过生成合成示例来实现数据平衡。
- en: 'Python Library:  [imbalanced-learn](https://imbalanced-learn.org/stable/references/index.html)
    Description: imbalanced-learn provides various methods to deal with imbalanced
    datasets, including oversampling techniques like SMOTE.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库： [imbalanced-learn](https://imbalanced-learn.org/stable/references/index.html)
    描述：imbalanced-learn 提供了多种处理不平衡数据集的方法，包括像 SMOTE 这样的过采样技术。
- en: '2.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Adjusting Loss Function: Modify the loss function to give more weight to the
    minority class, setting class weights inversely proportional to the class frequencies.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调整损失函数：修改损失函数以给予少数类更多权重，将类别权重设置为与类别频率成反比。
- en: '3.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight
    easy examples and focus training on hard negatives.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 焦点损失：交叉熵损失的一种变体，添加了一个因子来降低简单示例的权重，并将训练重点放在困难的负例上。
- en: 'Python Library:  [focal_loss](https://pypi.org/project/focal-loss/)'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库： [focal_loss](https://pypi.org/project/focal-loss/)
- en: 'Description: The focal_loss package provides robust implementations of various
    focal loss functions, including BinaryFocalLoss and SparseCategoricalFocalLoss.'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：focal_loss 包提供了多种焦点损失函数的强大实现，包括 BinaryFocalLoss 和 SparseCategoricalFocalLoss。
- en: '4.'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Cost-sensitive Learning: Incorporating the cost of misclassifications directly
    into the learning algorithm, assigning a higher cost to misclassifying minority
    class samples.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本敏感学习：将误分类的成本直接纳入学习算法中，为误分类少数类样本分配更高的成本。
- en: '5.'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Ensemble Methods: Using techniques like bagging and boosting to combine multiple
    models and handle class imbalance.'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成方法：使用像集成和提升这样的技术来结合多个模型并处理类别不平衡。
- en: 'Python Library:  [sklearn.ensemble](https://scikit-learn.org/stable/modules/ensemble.html)'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库： [sklearn.ensemble](https://scikit-learn.org/stable/modules/ensemble.html)
- en: 'Description: scikit-learn provides robust implementations of various ensemble
    methods, including bagging and boosting.'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：scikit-learn 提供了各种集成方法的强大实现，包括集成和提升。
- en: '6.'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Stratified Sampling: Ensuring that each mini-batch during training contains
    an equal or proportional representation of each class.'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分层抽样：确保训练期间每个小批次都包含每个类别的相等或按比例代表。
- en: 'Python Library:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
- en: 'Description: scikit-learn offers tools for stratified sampling, ensuring balanced
    representation across classes.'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：scikit-learn 提供了分层抽样的工具，确保各类之间的平衡表示。
- en: '7.'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Data Cleaning: Removing noisy and mislabelled data, which can disproportionately
    affect the minority class.'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据清理：去除噪声和错误标记的数据，这些数据可能对少数类产生不成比例的影响。
- en: 'Python Library:  [pandas.DataFrame.sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [pandas.DataFrame.sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)'
- en: 'Description: pandas provides methods for sampling data from DataFrames, useful
    for data cleaning and preprocessing.'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：pandas 提供了从 DataFrames 中抽样数据的方法，适用于数据清理和预处理。
- en: '8.'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and
    Cohen’s Kappa are more informative than accuracy when dealing with imbalanced
    datasets.'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用适当的指标：像精确度-召回 AUC、F1 分数和 Cohen’s Kappa 等指标在处理不平衡数据集时，比准确率更具信息量。
- en: 'Python Library:  [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)'
- en: 'Description: scikit-learn offers a comprehensive set of tools for evaluating
    the performance of classification models, particularly with imbalanced datasets.'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：scikit-learn 提供了一套全面的工具来评估分类模型的性能，特别是在不平衡数据集上。
- en: 3.1.4 Splitting Dataset
  id: totrans-431
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 数据集拆分
- en: 'Splitting the dataset for fine-tuning involves dividing it into training and
    validation sets, typically using an 80:20 ratio. Different techniques include:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集进行精调时，涉及将其划分为训练集和验证集，通常采用 80:20 的比例。不同的技术包括：
- en: '1.'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Random Sampling: Selecting a subset of data randomly to create a representative
    sample.'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机抽样：随机选择数据子集以创建具有代表性的样本。
- en: 'Python Library:  [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)'
- en: '2.'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Stratified Sampling: Dividing the dataset into subgroups and sampling from
    each to maintain class balance.'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分层抽样：将数据集划分为子组，并从每个子组中抽样以保持类别平衡。
- en: 'Python Library:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
- en: '3.'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'K-Fold Cross Validation: Splitting the dataset into K folds and performing
    training and validation K times.'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: K 折交叉验证：将数据集拆分为 K 个折叠，进行 K 次训练和验证。
- en: 'Python Library:  [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)'
- en: '4.'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Leave-One-Out Cross Validation: Using a single data point as the validation
    set and the rest for training, repeated for each data point.'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 留一交叉验证：使用一个数据点作为验证集，其余数据用于训练，对每个数据点重复此过程。
- en: 'Python Library:  [sklearn.model_selection.LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)'
- en: Further details can be found in [scikit-learn’s documentation on model selection](https://scikit-learn.org/stable/api/sklearn.model_selection.html).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息请参阅 [scikit-learn 关于模型选择的文档](https://scikit-learn.org/stable/api/sklearn.model_selection.html)。
- en: 3.2 Existing and Potential Research Methodologies
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 现有和潜在的研究方法
- en: 3.2.1 Data Annotation
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 数据标注
- en: 'Data annotation involves labelling or tagging textual data with specific attributes
    relevant to the model’s training objectives. This process is crucial for supervised
    learning tasks and greatly influences the performance of the fine-tuned model.
    Recent research highlights various approaches to data annotation:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注涉及使用与模型训练目标相关的特定属性对文本数据进行标签或标记。这一过程对监督学习任务至关重要，并极大地影响了精调模型的性能。最近的研究强调了数据标注的各种方法：
- en: •
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Human Annotation: Manual annotation by human experts remains a gold standard
    due to its accuracy and context understanding. However, it is time-consuming and
    costly for large datasets [[16](#bib.bib16)]. Tools like Excel, Prodigy¹¹1[https://prodi.gy](https://prodi.gy),
    and Innodata²²2[https://innodata.com/](https://innodata.com/) facilitate this
    process.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工标注：由于其准确性和对上下文的理解，人工专家手动标注仍然是黄金标准。然而，对于大型数据集来说，这一过程既耗时又昂贵[[16](#bib.bib16)]。像
    Excel、Prodigy¹¹1[https://prodi.gy](https://prodi.gy) 和 Innodata²²2[https://innodata.com/](https://innodata.com/)
    等工具可以帮助完成这一过程。
- en: •
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semi-automatic Annotation: Combining machine learning algorithms with human
    review to create labelled datasets more efficiently. This approach balances efficiency
    and accuracy. Tools like Snorkel³³3[https://snorkel.ai/](https://snorkel.ai/)
    use weak supervision to generate initial labels, which are then refined by human
    annotators [[17](#bib.bib17)].'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半自动标注：将机器学习算法与人工审核相结合，以更高效地创建标记数据集。这种方法在效率和准确性之间取得平衡。像 Snorkel³³3[https://snorkel.ai/](https://snorkel.ai/)
    等工具使用弱监督来生成初步标签，然后由人工标注者进行细化[[17](#bib.bib17)]。
- en: •
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Automatic Annotation: Fully automated annotation leverages machine learning
    algorithms to label data without human intervention, offering scalability and
    cost-effectiveness. Services like Amazon SageMaker Ground Truth⁴⁴4[https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/)
    utilise machine learning to automate data labelling, although the accuracy may
    vary depending on the complexity of the task [[18](#bib.bib18)].'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动标注：完全自动化的标注利用机器学习算法对数据进行标记，无需人工干预，提供了可扩展性和成本效益。像 Amazon SageMaker Ground Truth⁴⁴4[https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/)
    等服务利用机器学习自动化数据标记，尽管准确性可能会根据任务的复杂性而有所不同[[18](#bib.bib18)]。
- en: 3.2.2 Data Augmentation
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 数据增强
- en: 'Data Augmentation (DA) techniques expand training datasets artificially to
    address data scarcity and improve model performance. Advanced techniques often
    used in NLP include:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（DA）技术通过人工扩展训练数据集，以解决数据稀缺问题并提高模型性能。在 NLP 中常用的高级技术包括：
- en: •
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words
    with their semantic equivalents, thereby generating new data instances [[19](#bib.bib19),
    [20](#bib.bib20)].'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 词嵌入：使用词嵌入技术，如 Word2Vec 和 GloVe，将词替换为其语义等价词，从而生成新的数据实例[[19](#bib.bib19), [20](#bib.bib20)]。
- en: •
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Back Translation: Translating text to another language and then back to the
    original language to create paraphrased data. This technique helps in generating
    diverse training samples [[21](#bib.bib21)]. Tools like Google Translate API⁵⁵5[https://translate.google.com/?sl=auto&tl=en&op=translate](https://translate.google.com/?sl=auto&tl=en&op=translate)
    are commonly used for this purpose.'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回译：将文本翻译成另一种语言，然后再翻译回原始语言，以创建同义改写数据。这一技术有助于生成多样化的训练样本[[21](#bib.bib21)]。像 Google
    Translate API⁵⁵5[https://translate.google.com/?sl=auto&tl=en&op=translate](https://translate.google.com/?sl=auto&tl=en&op=translate)
    等工具通常用于此目的。
- en: •
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial Attacks: Generating augmented data through adversarial examples
    that slightly modify the original text to create new training samples while preserving
    the original meaning [[22](#bib.bib22)]. Libraries like TextAttack⁶⁶6[https://github.com/QData/TextAttack](https://github.com/QData/TextAttack)
    provide frameworks for such augmentations.'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗攻击：通过对抗样本生成增强数据，这些样本稍微修改原始文本，以创建新的训练样本，同时保留原始含义[[22](#bib.bib22)]。像 TextAttack⁶⁶6[https://github.com/QData/TextAttack](https://github.com/QData/TextAttack)
    等库提供了这种增强的框架。
- en: •
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NLP-AUG⁷⁷7[https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug):
    This library offers a variety of augmenters for character, word, sentence, audio,
    and spectrogram augmentation, enhancing dataset diversity.'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NLP-AUG⁷⁷7[https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug)：该库提供了多种数据增强器，用于字符、词、句子、音频和频谱图增强，增强了数据集的多样性。
- en: 3.2.3 Synthetic Data Generation using LLMs
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 使用 LLMs 生成合成数据
- en: 'Large Language Models (LLMs) can generate synthetic data through innovative
    techniques such as:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以通过创新技术生成合成数据，例如：
- en: •
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating
    relevant and high-quality synthetic data [[23](#bib.bib23)].'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程：设计特定的提示以指导 LLMs，如 GPT-3，生成相关的高质量合成数据[[23](#bib.bib23)]。
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-Step Generation: Employing iterative generation processes where LLMs
    generate initial data that is refined through subsequent steps [[24](#bib.bib24)].
    This method can produce high-quality synthetic data for various tasks, including
    summarising and bias detection.'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多步骤生成：采用迭代生成过程，其中LLMs生成初步数据，并通过后续步骤进行精炼[[24](#bib.bib24)]。这种方法可以为各种任务生成高质量的合成数据，包括总结和偏差检测。
- en: It is crucial to verify the accuracy and relevance of synthetic data generated
    by LLMs before using them for fine-tuning processes [[25](#bib.bib25)].
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用LLMs生成的合成数据进行微调过程之前，验证其准确性和相关性是至关重要的[[25](#bib.bib25)]。
- en: 3.3 Challenges in Data Preparation for Fine-Tuning LLMs
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 微调LLMs的数据准备挑战
- en: 'Key challenges in data preparation include:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备中的关键挑战包括：
- en: '1.'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Domain Relevance: Ensuring that the data is relevant to the specific domain
    for accurate model performance. Mismatched domain data can lead to poor generalisation
    and inaccurate outputs [[26](#bib.bib26)].'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域相关性：确保数据与特定领域相关，以确保模型性能的准确性。不匹配的领域数据可能导致较差的泛化能力和不准确的输出[[26](#bib.bib26)]。
- en: '2.'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Data Diversity: Including diverse and well-balanced data to prevent model biases
    and improve generalisation. A lack of diversity can cause the model to perform
    poorly on underrepresented scenarios [[27](#bib.bib27)].'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据多样性：包含多样化和均衡的数据，以防止模型偏见并提高泛化能力。缺乏多样性可能导致模型在代表性不足的场景中表现不佳[[27](#bib.bib27)]。
- en: '3.'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Data Size: Managing and processing large datasets, with at least 1000 samples
    recommended for effective fine-tuning. However, large datasets pose challenges
    in terms of storage, computational requirements, and processing time.'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据规模：管理和处理大型数据集，建议至少使用1000个样本以实现有效的微调。然而，大型数据集在存储、计算要求和处理时间方面会带来挑战。
- en: '4.'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies
    are critical for providing clean inputs to the model. Poorly preprocessed data
    can degrade model performance significantly.'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据清洗和预处理：去除噪音、错误和不一致性对于向模型提供干净的输入至关重要。预处理不当的数据可能会显著降低模型性能。
- en: '5.'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Data Annotation: Ensuring precise and consistent labelling is essential for
    tasks requiring labelled data. Inconsistent annotation can lead to unreliable
    model predictions.'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据标注：确保精确和一致的标注对于需要标注数据的任务至关重要。不一致的标注可能导致模型预测的不可靠。
- en: '6.'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Handling Rare Cases: Adequately representing rare but important instances in
    the dataset to ensure the model can generalise to less frequent but critical scenarios.'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理稀有案例：充分代表数据集中的稀有但重要的实例，以确保模型能够泛化到较少出现但关键的场景。
- en: '7.'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Ethical Considerations: Scrutinising data for harmful or biased content to
    prevent unintended consequences. Ethical data handling includes removing biases
    and ensuring privacy [[28](#bib.bib28)].'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理考虑：审查数据以防止有害或偏见内容，以防止意外后果。伦理的数据处理包括去除偏见和确保隐私[[28](#bib.bib28)]。
- en: 3.4 Available LLM Fine-Tuning Datasets
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 可用的LLM微调数据集
- en: For a comprehensive list of datasets suitable for fine-tuning LLMs, refer to
    resources like [LLMXplorer](https://forms.gle/TNUbqHiCBsinD4Bu8), which provides
    domain and task-specific datasets.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取适合微调LLMs的数据集的全面列表，请参阅像[LLMXplorer](https://forms.gle/TNUbqHiCBsinD4Bu8)这样的资源，该资源提供领域和任务特定的数据集。
- en: 3.5 Best Practices
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 最佳实践
- en: 3.5.1 High-Quality Data Collection
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 高质量数据收集
- en: Ensuring high-quality, diverse, and representative data is critical. Leveraging
    curated sources and ensuring comprehensive coverage across different scenarios
    enhances model robustness [[29](#bib.bib29)]. Tools like DataRobot Paxata⁸⁸8[https://www.datarobot.com/platform/preparation/](https://www.datarobot.com/platform/preparation/)
    and KNIME Analytics Platform⁹⁹9[https://www.knime.com/](https://www.knime.com/)
    offer robust data profiling and transformation capabilities.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 确保高质量、多样化和代表性的数据至关重要。利用策划的资源并确保覆盖不同场景，可以增强模型的鲁棒性[[29](#bib.bib29)]。像DataRobot
    Paxata⁸⁸8[https://www.datarobot.com/platform/preparation/](https://www.datarobot.com/platform/preparation/)和KNIME
    Analytics Platform⁹⁹9[https://www.knime.com/](https://www.knime.com/)等工具提供了强大的数据分析和转换能力。
- en: 3.5.2 Effective Data Preprocessing
  id: totrans-493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 有效的数据预处理
- en: Proper data preprocessing is essential for model performance. Utilising libraries
    like spaCy, NLTK, and HuggingFace Transformers can streamline preprocessing tasks.
    Platforms like Trifacta Wrangler and RapidMiner automate data cleaning tasks,
    improving efficiency and ensuring consistency [[30](#bib.bib30)].
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的数据预处理对于模型性能至关重要。利用像spaCy、NLTK和HuggingFace Transformers这样的库可以简化预处理任务。像Trifacta
    Wrangler和RapidMiner这样的平台可以自动化数据清理任务，提高效率并确保一致性[[30](#bib.bib30)]。
- en: 3.5.3 Managing Data Imbalance
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 管理数据不平衡
- en: Addressing data imbalance is crucial. Techniques like over-sampling, under-sampling,
    and SMOTE help balance datasets. Libraries like imbalanced-learn and ensemble
    methods in scikit-learn provide robust tools for managing imbalanced datasets
    [[31](#bib.bib31)].
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据不平衡至关重要。过采样、欠采样和SMOTE等技术有助于平衡数据集。像imbalanced-learn这样的库以及scikit-learn中的集成方法提供了管理不平衡数据集的强大工具[[31](#bib.bib31)]。
- en: 3.5.4 Augmenting and Annotating Data
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 数据增强和标注
- en: Data augmentation and annotation improve model robustness. Tools like NLP-AUG,
    TextAttack, and Snorkel offer sophisticated capabilities for creating diverse
    and well-labelled datasets [[32](#bib.bib32), [33](#bib.bib33)].
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强和标注提高了模型的稳健性。像NLP-AUG、TextAttack和Snorkel这样的工具提供了创建多样且标注良好的数据集的先进能力[[32](#bib.bib32),
    [33](#bib.bib33)]。
- en: 3.5.5 Ethical Data Handling
  id: totrans-499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 伦理数据处理
- en: Ensuring ethical data handling involves thorough scrutiny for biases and privacy
    concerns. Implementing privacy-preserving techniques and filtering harmful content
    is critical. Services like Amazon SageMaker Ground Truth ensure scalable and secure
    data annotation [[34](#bib.bib34)].
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 确保伦理数据处理涉及对偏差和隐私问题的彻底审查。实施隐私保护技术和过滤有害内容至关重要。像Amazon SageMaker Ground Truth这样的服务确保可扩展和安全的数据标注[[34](#bib.bib34)]。
- en: 3.5.6 Regular Evaluation and Iteration
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.6 定期评估和迭代
- en: Continuous evaluation and iteration of the data preparation pipeline help maintain
    data quality and relevance. Leveraging feedback loops and performance metrics
    ensures ongoing improvements and adaptation to new data requirements.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 持续评估和迭代数据准备管道有助于维护数据质量和相关性。利用反馈循环和性能指标确保持续改进和适应新的数据需求。
- en: By integrating these best practices, researchers and practitioners can enhance
    the effectiveness of LLM fine-tuning, ensuring robust and reliable model performance.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合这些最佳实践，研究人员和从业者可以提高LLM微调的有效性，确保模型性能的可靠性和稳健性。
- en: 'Chapter 4 Stage 2: Model Initialisation'
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章 第二阶段：模型初始化
- en: 4.1 Steps Involved in Model Initialisation
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型初始化中的步骤
- en: '![Refer to caption](img/9cf39698e03417a076e40e2f3c6c5396.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9cf39698e03417a076e40e2f3c6c5396.png)'
- en: 'Figure 4.1: Sequential steps involved in Initialising a Large Language Model
    (LLM), illustrating the process from setting up the environment to executing tasks.
    Each step is critical for ensuring that the LLM is correctly configured and ready
    for operation. This includes installing necessary dependencies, importing libraries,
    selecting and downloading the appropriate language model from a repository, and
    finally, loading the model to perform specific tasks.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：初始化大型语言模型（LLM）中的顺序步骤，展示了从设置环境到执行任务的过程。每个步骤对于确保LLM正确配置并准备好运行至关重要。这包括安装必要的依赖项、导入库、从仓库中选择并下载适当的语言模型，最后加载模型以执行特定任务。
- en: '1.'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Set Up the Environment: Configure your environment, such as setting up GPU/TPU
    usage if available, which can significantly speed up model loading and inference.'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置环境：配置环境，例如设置GPU/TPU使用（如果可用），这可以显著加快模型加载和推理速度。
- en: '2.'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Install the Dependencies: Ensure that all necessary software and libraries
    are installed. This typically includes package managers like pip and frameworks
    like PyTorch or TensorFlow.'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装依赖项：确保所有必要的软件和库已安装。这通常包括像pip这样的包管理器以及像PyTorch或TensorFlow这样的框架。
- en: '3.'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Import the Libraries: Import the required libraries in your script or notebook.
    Common libraries include transformers from Hugging Face, torch for PyTorch, and
    other utility libraries.'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入库：在脚本或笔记本中导入所需的库。常见的库包括来自Hugging Face的transformers、PyTorch的torch以及其他实用库。
- en: '4.'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Choose the Language Model: Select the appropriate pre-trained language model
    based on your task requirements. This could be models like BERT, GPT-3, or others
    available on platforms like Hugging Face’s Model Hub.'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择语言模型：根据任务需求选择适当的预训练语言模型。这可能是像BERT、GPT-3这样的模型，或在Hugging Face的模型库中提供的其他模型。
- en: '5.'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Download the Model from the Repository: Use the chosen framework’s functions
    to download the pre-trained model from an online repository. For instance, using
    transformers, you might use AutoModel.from_pretrained(’model_name’).'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从仓库下载模型：使用所选框架的函数从在线仓库下载预训练模型。例如，使用 transformers，你可以使用 AutoModel.from_pretrained(’model_name’)。
- en: '6.'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Load the Model in the Memory: Load the model into memory, ready for inference
    or further fine-tuning. This step ensures the model weights are initialised and
    ready for use.'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将模型加载到内存中：将模型加载到内存中，准备进行推理或进一步微调。这一步骤确保模型权重被初始化并准备好使用。
- en: '7.'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Execute Tasks: Perform the desired tasks using the loaded model. This could
    involve making predictions, generating text, or fine-tuning the model on a new
    dataset.'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行任务：使用加载的模型执行所需的任务。这可能包括进行预测、生成文本或在新数据集上微调模型。
- en: 4.2 Tools and Libraries for Model Initialisation
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 模型初始化的工具和库
- en: 'Python offers a wide range of libraries for Initialising large language models,
    providing access to both open and closed-source models. Here are some notable
    libraries:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了广泛的库来初始化大型语言模型，提供对开源和闭源模型的访问。以下是一些值得注意的库：
- en: '1.'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Python Library:  [*HuggingFace*](https://huggingface.co/docs/transformers/en/index)'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库：[*HuggingFace*](https://huggingface.co/docs/transformers/en/index)
- en: 'Description: HuggingFace is renowned for its support of numerous pre-trained
    large language models, ranging from Phi-3 mini to Llama-3 70B. The transformers
    library, part of HuggingFace, enables users to access these models via classes
    such as AutoModelForCausalLM. This library supports loading fine-tuned models
    as well as 4-bit quantised models. Additionally, the transformers library includes
    the ”pipeline” feature, making it easy to use pre-trained models for various tasks
    [[35](#bib.bib35)].'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：HuggingFace 因其对大量预训练的大型语言模型的支持而闻名，从 Phi-3 mini 到 Llama-3 70B。HuggingFace
    旗下的 transformers 库使用户可以通过诸如 AutoModelForCausalLM 等类访问这些模型。该库支持加载微调后的模型以及 4 位量化模型。此外，transformers
    库还包括“pipeline”功能，使得使用预训练模型执行各种任务变得简单[[35](#bib.bib35)]。
- en: '2.'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Python Framework:  [*PyTorch*](https://pytorch.org/docs/stable/index.html)'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 框架：[*PyTorch*](https://pytorch.org/docs/stable/index.html)
- en: 'Description: PyTorch offers comprehensive tools and libraries for Initialising
    and fine-tuning large language models. It provides a flexible and efficient platform
    for building and deploying deep learning models. HuggingFace’s transformers library
    bridges the gap between PyTorch and other frameworks, enhancing its usability
    for state-of-the-art language models [[36](#bib.bib36)].'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：PyTorch 提供了全面的工具和库，用于初始化和微调大型语言模型。它提供了一个灵活高效的平台，用于构建和部署深度学习模型。HuggingFace
    的 transformers 库弥合了 PyTorch 与其他框架之间的差距，提高了其在最先进的语言模型中的可用性[[36](#bib.bib36)]。
- en: '3.'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Python Framework:  [*TensorFlow*](https://www.tensorflow.org/tutorials)'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 框架：[*TensorFlow*](https://www.tensorflow.org/tutorials)
- en: 'Description: TensorFlow also provides extensive tools and libraries for Initialising
    and fine-tuning large language models. Similar to PyTorch, it benefits from the
    HuggingFace transformers library, which provides a versatile and user-friendly
    API and interface for working with the latest advancements in large language models
    [[37](#bib.bib37)].'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：TensorFlow 同样提供了广泛的工具和库，用于初始化和微调大型语言模型。与 PyTorch 类似，它也受益于 HuggingFace transformers
    库，该库为处理最新的大型语言模型提供了多功能和用户友好的 API 和接口[[37](#bib.bib37)]。
- en: 4.3 Challenges in Model Initialisation
  id: totrans-533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模型初始化中的挑战
- en: '| Challenge | Description |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | 描述 |'
- en: '| --- | --- |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Alignment with the Target Task | It’s essential that the pre-trained model
    closely aligns with your specific task or domain. This initial alignment serves
    as a solid foundation for further fine-tuning efforts, leading to improved efficiency
    and results [[38](#bib.bib38)]. |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 与目标任务的一致性 | 预训练模型与特定任务或领域的紧密对接至关重要。这种初始对接为进一步的微调工作奠定了坚实的基础，从而提高了效率和结果[[38](#bib.bib38)]。
    |'
- en: '| Understanding the Pre-trained Model | Before making a selection, it’s crucial
    to thoroughly comprehend the architecture, capabilities, limitations, and the
    tasks the model was originally trained on. Without this understanding, fine-tuning
    efforts may not yield the desired outcomes [[23](#bib.bib23)]. |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 理解预训练模型 | 在做出选择之前，必须彻底理解模型的架构、能力、局限性以及模型最初训练的任务。如果没有这种理解，微调工作可能不会产生期望的结果[[23](#bib.bib23)]。
    |'
- en: '| Availability and Compatibility | Careful consideration of a model’s documentation,
    license, maintenance, and update frequency is necessary to avoid potential issues
    and ensure smooth integration into your application. |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 可用性和兼容性 | 需要仔细考虑模型的文档、许可证、维护和更新频率，以避免潜在问题，并确保顺利集成到您的应用中。 |'
- en: '| Model Architecture | Not all models excel at every task. Each model architecture
    has its strengths and weaknesses, so selecting one aligned with your specific
    task is essential for favourable outcomes [[39](#bib.bib39)]. |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 并非所有模型都擅长所有任务。每种模型架构都有其优点和缺点，因此选择一个与特定任务相匹配的模型对于获得良好的结果至关重要[[39](#bib.bib39)]。
    |'
- en: '| Resource Constraints | Loading pre-trained LLMs is resource-heavy and requires
    more computation. These models need high-performance CPUs and GPUs and a significant
    amount of disk space. For instance, the Llama 3 8B model requires a minimum of
    16GB of memory to load and run the inference. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 资源限制 | 加载预训练的 LLM 资源消耗很大，需要更多的计算。这些模型需要高性能的 CPU 和 GPU 以及大量的磁盘空间。例如，Llama
    3 8B 模型需要至少 16GB 的内存才能加载和运行推断。 |'
- en: '| Privacy | Privacy and confidentiality are crucial factors when selecting
    a large language model (LLM). Many businesses prefer not to share their data with
    external LLM providers. In such instances, hosting an LLM on local servers or
    using pre-trained LLMs available through private cloud providers can be viable
    solutions. These approaches ensure that data remains within the company’s premises,
    thereby preserving privacy and confidentiality. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 隐私 | 隐私和保密性在选择大型语言模型（LLM）时至关重要。许多企业希望不与外部 LLM 提供商分享其数据。在这种情况下，将 LLM 托管在本地服务器上或使用通过私人云提供商提供的预训练
    LLM 可以是可行的解决方案。这些方法确保数据保持在公司的场所内，从而保护隐私和保密性。 |'
- en: '| Cost and Maintenance | Hosting LLMs on local servers entails significant
    time and expense for setup and ongoing maintenance. Conversely, utilising cloud
    vendors alleviates concerns about resource maintenance but incurs monthly billing
    costs. These charges are typically based on factors such as model size and the
    volume of requests per minute. |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 成本和维护 | 在本地服务器上托管 LLM 需要大量的时间和费用来设置和维护。相比之下，使用云供应商可以缓解对资源维护的担忧，但会产生每月的账单费用。这些费用通常基于模型大小和每分钟请求量等因素。
    |'
- en: '| Model Size and Quantisation | utilising a pre-trained model with high memory
    consumption can still be viable by employing its quantised version. Through quantisation,
    pre-trained weights can be loaded with reduced precision, typically 4-bit or 8-bit
    floating point, substantially diminishing parameter volume while maintaining considerable
    accuracy [[40](#bib.bib40)]. |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小和量化 | 利用高内存消耗的预训练模型仍然可行，通过采用其量化版本。通过量化，预训练权重可以以较低的精度加载，通常为 4 位或 8 位浮点数，从而显著减少参数体积，同时保持相当的准确性[[40](#bib.bib40)]。
    |'
- en: '| Pre-training Datasets | Examine the datasets used for pre-training to gauge
    the model’s understanding of language. These are important as there are models
    available specifically for performing code generation, and we do not want to use
    those models for finance text classification [[41](#bib.bib41)]. |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 预训练数据集 | 检查用于预训练的数据集，以评估模型对语言的理解。这些数据集很重要，因为有些模型专门用于代码生成，我们不希望将这些模型用于金融文本分类[[41](#bib.bib41)]。
    |'
- en: '| Bias Awareness | Be vigilant regarding potential biases in pre-trained models,
    especially if unbiased predictions are required. The bias awareness can be evaluated
    by testing different models and backtracking the datasets used for pre-training
    [[42](#bib.bib42)]. |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 偏见意识 | 要警惕预训练模型中的潜在偏见，特别是当需要无偏预测时。通过测试不同的模型和追溯用于预训练的数据集来评估偏见意识[[42](#bib.bib42)]。'
- en: 'Table 4.1: Comprehensive Overview of Challenges in Initialising a Large Language
    Model (LLM). This table highlights critical considerations, such as the importance
    of aligning pre-trained models with specific tasks, understanding model architecture
    and compatibility, managing resource constraints, and ensuring data privacy. Additionally,
    it discusses the challenges related to cost, maintenance, and the complexities
    of model size, quantisation, and bias awareness. Each challenge is associated
    with specific references to ensure thorough understanding and proper model deployment.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：大型语言模型（LLM）初始化挑战的综合概述。此表突出了关键考虑因素，例如将预训练模型与特定任务对齐的重要性、理解模型架构和兼容性、管理资源约束以及确保数据隐私。此外，它还讨论了与成本、维护、模型大小、量化和偏差意识相关的挑战。每个挑战都与具体参考文献相关，以确保对模型部署的彻底理解。
- en: 4.4 Tutorials
  id: totrans-547
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 教程
- en: '1.'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[Summarisation using Llama 3](https://medium.com/@manuelescobar-dev/implementing-and-running-llama-3-with-hugging-faces-transformers-library-40e9754d8c80)'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 Llama 3 进行总结](https://medium.com/@manuelescobar-dev/implementing-and-running-llama-3-with-hugging-faces-transformers-library-40e9754d8c80)'
- en: '2.'
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '[HuggingFace tutorial for getting started with LLMs](https://huggingface.co/docs/transformers/en/llm_tutorial)'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[HuggingFace LLM 入门教程](https://huggingface.co/docs/transformers/en/llm_tutorial)'
- en: '3.'
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '[PyTorch tutorial for fine-tuning models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)'
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PyTorch 模型微调教程](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)'
- en: '4.'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: '[TensorFlow tutorial for transformer models](https://www.tensorflow.org/tutorials/text/transformer)'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[TensorFlow 变换器模型教程](https://www.tensorflow.org/tutorials/text/transformer)'
- en: 'Chapter 5 Stage 3: Training Setup'
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五章 第三阶段：训练设置
- en: 5.1 Steps Involved in Training Setup
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 训练设置中的步骤
- en: '1.'
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Setting up the training environment: When setting up the environment for training
    an LLM, it is crucial to configure high-performance hardware, such as GPUs or
    TPUs, and ensure proper installation of necessary software components like CUDA,
    cuDNN, and deep learning frameworks such as PyTorch or TensorFlow. Verify hardware
    recognition and compatibility with the software to leverage computational power
    effectively, reducing training time and improving model performance.'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置训练环境：在为训练大型语言模型（LLM）设置环境时，至关重要的是配置高性能硬件，如 GPU 或 TPU，并确保必要的软件组件（如 CUDA、cuDNN
    以及深度学习框架如 PyTorch 或 TensorFlow）正确安装。验证硬件识别和与软件的兼容性，以有效利用计算能力，缩短训练时间并提高模型性能。
- en: '2.'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Defining the Hyper-parameters: When defining hyperparameters for fine-tuning
    an LLM, it is essential to carefully tune key parameters such as learning rate,
    batch size, and epochs to optimise the model’s performance.'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义超参数：在为 LLM 微调定义超参数时，必须仔细调整关键参数，如学习率、批次大小和训练轮数，以优化模型的性能。
- en: '3.'
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Initialising Optimisers and Loss Functions: When initialising optimisers and
    loss functions for fine-tuning an LLM, it is crucial to select the appropriate
    optimiser to efficiently update the model’s weights and the correct loss function
    to measure model performance [[43](#bib.bib43)].'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化优化器和损失函数：在为 LLM 微调初始化优化器和损失函数时，必须选择合适的优化器以高效更新模型权重，以及选择正确的损失函数以测量模型性能 [[43](#bib.bib43)]。
- en: 5.2 Setting up Training Environment
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 设置训练环境
- en: When fine-tuning a large language model (LLM), the computational environment
    plays a crucial role in ensuring efficient training. To achieve optimal performance,
    it’s essential to configure the environment with high-performance hardware such
    as GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units). GPUs, such
    as the NVIDIA A100 or V100, are widely used for training deep learning models
    due to their parallel processing capabilities. For larger-scale operations, TPUs
    offered by Google Cloud can provide even greater acceleration [[44](#bib.bib44)].
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLM）时，计算环境在确保高效训练方面起着至关重要的作用。为实现最佳性能，必须使用高性能硬件，如 GPU（图形处理单元）或 TPU（张量处理单元）来配置环境。由于其并行处理能力，NVIDIA
    A100 或 V100 等 GPU 被广泛用于训练深度学习模型。对于大规模操作，谷歌云提供的 TPU 可以提供更大的加速 [[44](#bib.bib44)]。
- en: First, ensure that your system or cloud environment has the necessary hardware
    installed. For GPUs, this involves setting up CUDA¹¹1[https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)
    (Compute Unified Device Architecture) and cuDNN²²2[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)
    (CUDA Deep Neural Network library) from NVIDIA, which are essential for enabling
    GPU acceleration. For TPU usage, you would typically set up a Google Cloud environment
    with TPU instances, which includes configuring the TPU runtime in your training
    scripts.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你的系统或云环境安装了必要的硬件。对于GPU，这涉及到设置CUDA¹¹1[https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)（计算统一设备架构）和cuDNN²²2[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)（CUDA深度神经网络库），这些对于启用GPU加速至关重要。对于TPU使用，通常会在Google
    Cloud环境中设置TPU实例，包括在训练脚本中配置TPU运行时。
- en: Verify that your hardware is correctly recognised and utilised by your deep
    learning frameworks. In PyTorch, for instance, you can check GPU availability
    with torch.cuda.is_available(). Properly setting up and testing the hardware ensures
    that the training process can leverage the computational power effectively, reducing
    training time and improving model performance [[36](#bib.bib36)].
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 验证你的硬件是否被深度学习框架正确识别和利用。例如，在PyTorch中，你可以使用torch.cuda.is_available()检查GPU的可用性。正确设置和测试硬件可以确保训练过程能够有效利用计算能力，从而缩短训练时间并提高模型性能[[36](#bib.bib36)]。
- en: When fine-tuning an LLM, both software and hardware considerations are paramount
    to ensure a smooth and efficient training process. On the software side, you need
    a compatible deep learning framework like PyTorch or TensorFlow. These frameworks
    have extensive support for LLMs and provide utilities for efficient model training
    and evaluation. Installing the latest versions of these frameworks, along with
    any necessary dependencies, is crucial for leveraging the latest features and
    performance improvements [[45](#bib.bib45)].
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调LLM时，软件和硬件的考虑都至关重要，以确保训练过程顺利高效。在软件方面，你需要一个兼容的深度学习框架，比如PyTorch或TensorFlow。这些框架对LLM有广泛支持，并提供高效的模型训练和评估工具。安装这些框架的最新版本及任何必要的依赖项，对于利用最新特性和性能改进至关重要[[45](#bib.bib45)]。
- en: Additionally, use libraries like Hugging Face’s transformers to simplify the
    process of loading pre-trained models and tokenizers. This library is particularly
    well-suited for working with various LLMs and offers a user-friendly interface
    for model fine-tuning. Ensure that all software components, including libraries
    and dependencies, are compatible with your chosen framework and hardware setup
    [[35](#bib.bib35)].
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用像Hugging Face的transformers这样的库可以简化加载预训练模型和分词器的过程。这个库特别适合处理各种LLM，并提供了一个用户友好的模型微调接口。确保所有软件组件，包括库和依赖项，都与选择的框架和硬件设置兼容[[35](#bib.bib35)]。
- en: On the hardware side, consider the memory requirements of the model and your
    dataset. LLMs typically require substantial GPU memory, so opting for GPUs with
    higher VRAM (e.g., 16GB or more) can be beneficial. If your model is exceptionally
    large or if you are training with very large datasets, distributed training across
    multiple GPUs or TPUs might be necessary. This requires a careful setup of data
    parallelism or model parallelism techniques to efficiently utilise the available
    hardware [[46](#bib.bib46)].
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件方面，考虑模型和数据集的内存需求。LLM通常需要大量的GPU内存，因此选择具有更高VRAM（例如16GB或更多）的GPU可能会有益。如果你的模型非常大或者你正在使用非常大的数据集，可能需要在多个GPU或TPU上进行分布式训练。这需要仔细设置数据并行或模型并行技术，以高效利用可用硬件[[46](#bib.bib46)]。
- en: Lastly, ensure robust cooling and power supply for your hardware, as training
    LLMs can be resource-intensive, generating significant heat and requiring consistent
    power. Proper hardware setup not only enhances training performance but also prolongs
    the lifespan of your equipment [[47](#bib.bib47)].
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保你的硬件有足够的散热和电源供应，因为训练LLM可能会消耗大量资源，产生大量热量并需要稳定的电力供应。适当的硬件设置不仅能提高训练性能，还能延长设备的使用寿命[[47](#bib.bib47)]。
- en: 5.3 Defining Hyperparameters
  id: totrans-572
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 定义超参数
- en: 'Key hyperparameters like learning rate, batch size, epochs are crucial for
    enhancing the model’s performance and obtaining superior outcomes. This process
    entails adjusting hyperparameters and training settings to align with your particular
    use case. Below are the key hyperparameters:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 关键超参数如学习率、批量大小、训练周期对提升模型性能和获得更好结果至关重要。这个过程包括调整超参数和训练设置，以符合你的特定用例。以下是关键超参数：
- en: '1.'
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like
    stochastic gradient descent (SGD). This technique estimates the error gradient
    for the model’s current state using samples from the training dataset and subsequently
    updates the model’s weights via the backpropagation of errors algorithm. The learning
    rate dictates the speed at which the model adapts to the problem. Smaller learning
    rates necessitate more training due to the minimal weight adjustments per update,
    while larger learning rates lead to quicker changes to weights [[48](#bib.bib48)].'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率：微调LLM涉及使用优化算法如随机梯度下降（SGD）。这种技术使用来自训练数据集的样本来估计模型当前状态的误差梯度，然后通过误差反向传播算法更新模型的权重。学习率决定了模型适应问题的速度。较小的学习率需要更多的训练，因为每次更新时权重的调整幅度较小，而较大的学习率会导致权重迅速变化[[48](#bib.bib48)]。
- en: '2.'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Batch Size: A batch refers to a subset of the training data used to update
    a model’s weights during the training process. Batch training involves dividing
    the entire training set into smaller groups, updating the model after processing
    each batch. The batch size is a hyperparameter that determines the number of samples
    processed before the model parameters are updated.'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小：批量是指在训练过程中用于更新模型权重的训练数据的一个子集。批量训练涉及将整个训练集分成较小的组，在处理每个批量后更新模型。批量大小是一个超参数，决定在更新模型参数之前处理的样本数量。
- en: '3.'
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Epochs: Epoch refers to a full pass through the entire training dataset. This
    involves a complete forward and backward pass through the dataset. The dataset
    can be processed as a single batch or divided into multiple smaller batches. An
    epoch is considered complete once the model has processed all batches and updated
    its parameters based on the calculated loss.'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练周期：训练周期是指通过整个训练数据集的一次完整过程。这包括数据集的完整前向和反向传递。数据集可以作为一个批量处理，也可以分成多个较小的批量。一次训练周期被视为完成，当模型处理了所有批量并根据计算的损失更新了参数时。
- en: 5.3.1 Methods for Hyperparameter Tuning
  id: totrans-580
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 超参数调整的方法
- en: 'LLM hyperparameter tuning involves adjusting various hyperparameters during
    the training process to identify the optimal combination that yields the best
    output. This process often entails significant trial and error, meticulously tracking
    each hyperparameter adjustment, and recording the resulting performance. Conducting
    this manually can be highly time-consuming. To address this, automated hyperparameter
    tuning methods have been developed to streamline the process. The three most common
    methods of automated hyperparameter tuning are random search, grid search, and
    Bayesian optimisation:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: LLM超参数调整涉及在训练过程中调整各种超参数，以确定能产生最佳输出的最佳组合。这个过程通常需要大量的试验和错误，细致地跟踪每个超参数的调整，并记录结果性能。手动进行这一过程可能非常耗时。为了解决这个问题，已经开发了自动化超参数调整方法来简化这一过程。自动化超参数调整的三种最常见方法是随机搜索、网格搜索和贝叶斯优化：
- en: '1.'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Random Search: This method randomly selects and evaluates combinations of hyperparameters
    from a specified range. It is a straightforward and efficient approach capable
    of exploring a large parameter space. However, it may not always find the optimal
    combination of hyperparameters and can be computationally expensive [[49](#bib.bib49)].'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机搜索：这种方法随机选择和评估指定范围内的超参数组合。它是一种简单而高效的方法，能够探索较大的参数空间。然而，它可能不会总是找到最佳的超参数组合，并且计算成本较高[[49](#bib.bib49)]。
- en: '2.'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Grid Search: Unlike random search, grid search exhaustively evaluates every
    possible combination of hyperparameters from a given range. Although resource-intensive,
    this systematic approach ensures that the optimal set of hyperparameters is found
    [[50](#bib.bib50)].'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网格搜索：与随机搜索不同，网格搜索会穷举地评估给定范围内的每一种可能的超参数组合。尽管这种方法资源密集，但它通过系统化的方法确保找到最佳的超参数集[[50](#bib.bib50)]。
- en: '3.'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Bayesian Optimisation: This method uses a probabilistic model to predict the
    performance of different hyperparameters and selects the best ones accordingly.
    It is an efficient method that can handle large parameter spaces better and is
    less resource-intensive than grid search. However, it is more complex to set up
    and may be less reliable in identifying the optimal set of hyperparameters compared
    to grid search.'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯优化：该方法使用概率模型来预测不同超参数的性能，并据此选择最佳的超参数。它是一种高效的方法，能够更好地处理大参数空间，且比网格搜索资源消耗更少。然而，它在设置上更为复杂，可能在识别最佳超参数组合时不如网格搜索可靠。
- en: '4.'
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Automated hyperparameter tuning: This facilitates the development of multiple
    language models, each with a unique combination of hyperparameters. By training
    these models on the same dataset, it becomes possible to compare their outputs
    and determine which configuration is best suited for the desired use case. Additionally,
    models tuned with different sets of hyperparameters can be tailored to various
    specific applications.'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动超参数调优：这有助于开发多个语言模型，每个模型都有独特的超参数组合。通过在相同数据集上训练这些模型，可以比较它们的输出，并确定哪种配置最适合所需的用例。此外，使用不同超参数集进行调优的模型可以针对各种特定应用进行调整。
- en: 5.4 Initialising Optimisers and Loss Functions
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 初始化优化器和损失函数
- en: 'Choosing the right optimiser and loss function is crucial for training and
    fine-tuning LLMs. Below are descriptions of some commonly used optimisation algorithms,
    their advantages, disadvantages, and appropriate use cases:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的优化器和损失函数对于训练和微调大型语言模型至关重要。以下是一些常用优化算法的描述，包括它们的优缺点和适用情况：
- en: 5.4.1 Gradient Descent
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 梯度下降
- en: Gradient Descent is a fundamental optimisation algorithm used to minimise cost
    functions in machine learning models. It aims to find the optimal parameters for
    a neural network.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种基本的优化算法，用于最小化机器学习模型中的成本函数。它旨在找到神经网络的最佳参数。
- en: 'How it Works: Gradient Descent iteratively updates model parameters in the
    direction of the negative gradient of the cost function. It calculates gradients
    for each parameter and applies updates across all data points until convergence.
    This method utilises the entire dataset to calculate gradients, often requiring
    a fixed learning rate and being sensitive to the scale of data and learning rate
    choice.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：梯度下降通过沿着成本函数的负梯度方向迭代更新模型参数。它计算每个参数的梯度，并在所有数据点上应用更新，直到收敛。此方法利用整个数据集计算梯度，通常需要固定的学习率，并对数据规模和学习率的选择非常敏感。
- en: 'Pros:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Simple and easy to implement.
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单易于实现。
- en: •
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Intuitive and easy to understand.
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直观且易于理解。
- en: •
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Converges to the global minimum for convex functions.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于凸函数，能够收敛到全局最小值。
- en: •
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Suitable for small-scale problems.
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适用于小规模问题。
- en: 'Cons:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Computationally expensive on large datasets.
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大数据集上计算成本高。
- en: •
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: May get stuck in local minima.
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能会陷入局部最小值。
- en: •
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires a large number of iterations.
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要大量迭代。
- en: •
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensitive to the choice of learning rate.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对学习率的选择非常敏感。
- en: 'When to Use: Gradient Descent is best used for small datasets where gradient
    computation is cheap and simplicity and clarity are preferred.'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：梯度下降最适用于梯度计算便宜且更注重简洁性和清晰性的小数据集。
- en: 5.4.2 Stochastic Gradient Descent (SGD)
  id: totrans-614
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 随机梯度下降（SGD）
- en: Stochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses
    on reducing computation per iteration.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）是梯度下降的一种变体，侧重于减少每次迭代的计算量。
- en: 'How it Works: SGD updates parameters using a single or few data points at each
    iteration, introducing randomness in updates. It reduces the computational burden
    per iteration and often converges faster than batch Gradient Descent. However,
    it requires a smaller learning rate due to higher variance and benefits from momentum
    to stabilise updates.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：SGD在每次迭代时使用单个或少量数据点更新参数，引入更新中的随机性。它减少了每次迭代的计算负担，并且通常比批量梯度下降收敛更快。然而，由于方差较高，它需要更小的学习率，并且通过动量来稳定更新。
- en: 'Pros:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fast and handles large datasets well.
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 速度快，能很好地处理大数据集。
- en: •
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Efficient memory usage.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的内存使用。
- en: •
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Simple and easy to implement.
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单易于实现。
- en: •
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can escape local minima due to noise.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于噪声，可能会逃离局部最小值。
- en: 'Cons:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: High variance in updates can lead to instability.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新中的高方差可能导致不稳定。
- en: •
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can overshoot the minimum.
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能会超过最小值。
- en: •
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensitive to the choice of learning rate.
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对学习率的选择非常敏感。
- en: •
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can be slower to converge compared to batch methods.
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相较于批量方法，收敛速度可能较慢。
- en: 'When to Use: SGD is ideal for large datasets, incremental learning scenarios,
    and real-time learning environments where computational resources are limited.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：SGD非常适合大型数据集、增量学习场景以及计算资源有限的实时学习环境。
- en: 5.4.3 Mini-batch Gradient Descent
  id: totrans-636
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 Mini-batch梯度下降
- en: Mini-batch Gradient Descent combines the efficiency of SGD and the stability
    of batch Gradient Descent, offering a compromise between batch and stochastic
    approaches.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: Mini-batch梯度下降结合了SGD的效率和批量梯度下降的稳定性，提供了批量和随机方法之间的折中。
- en: 'How it Works: It splits data into small batches and updates parameters using
    gradients averaged over each mini-batch. This reduces variance compared to SGD
    and is more efficient than batch Gradient Descent, helping in generalising the
    updates.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：它将数据分成小批次，并使用每个小批次上梯度的平均值来更新参数。这相比SGD减少了方差，并且比批量梯度下降更高效，有助于更新的泛化。
- en: 'Pros:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Balances between efficiency and stability.
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在效率和稳定性之间取得平衡。
- en: •
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More generalisable updates.
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更具泛化性的更新。
- en: •
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reduces the variance of parameter updates.
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少参数更新的方差。
- en: •
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Provides a compromise between SGD and batch.
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了SGD和批量之间的折中。
- en: 'Cons:'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires tuning of batch size.
  id: totrans-650
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要调整批量大小。
- en: •
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can still be computationally expensive for very large datasets.
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于非常大的数据集仍然可能计算开销大。
- en: •
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More complex implementation.
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现较为复杂。
- en: •
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can require more iterations than full-batch Gradient Descent.
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能需要比全批次梯度下降更多的迭代次数。
- en: 'When to Use: Mini-batch Gradient Descent is suitable for most deep learning
    tasks, especially when working with moderate to large datasets.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：Mini-batch梯度下降适用于大多数深度学习任务，特别是在处理中等到大型数据集时。
- en: 5.4.4 AdaGrad
  id: totrans-658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4 AdaGrad
- en: Adaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional
    models, adjusting learning rates to improve performance on sparse data.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应梯度算法（AdaGrad）旨在处理稀疏数据和高维模型，通过调整学习率来提高稀疏数据上的表现。
- en: 'How it Works: AdaGrad adapts the learning rate for each parameter based on
    historical gradient information, accumulating squared gradients. This approach
    prevents large updates for frequent parameters and helps in dealing with sparse
    features.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：AdaGrad根据历史梯度信息为每个参数调整学习率，积累平方梯度。这种方法防止了频繁参数的大幅更新，并有助于处理稀疏特征。
- en: 'Pros:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapts learning rate for each parameter.
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个参数调整学习率。
- en: •
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Good for sparse data.
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对稀疏数据有效。
- en: •
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No need to manually tune learning rates.
  id: totrans-667
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无需手动调整学习率。
- en: •
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Works well with high-dimensional data.
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对高维数据表现良好。
- en: 'Cons:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning rate can diminish to zero, stopping learning.
  id: totrans-672
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率可能减少到零，从而停止学习。
- en: •
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: May require more tuning for convergence.
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能需要更多的调参以实现收敛。
- en: •
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Accumulation of squared gradients can lead to overly small learning rates.
  id: totrans-676
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平方梯度的积累可能导致学习率过小。
- en: •
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can slow down significantly.
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能显著减慢速度。
- en: 'When to Use: AdaGrad is useful for sparse datasets like text and images where
    learning rates need to adapt to feature frequency.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：AdaGrad适用于需要根据特征频率调整学习率的稀疏数据集，如文本和图像。
- en: 5.4.5 RMSprop
  id: totrans-680
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.5 RMSprop
- en: Root Mean Square Propagation (RMSprop) is an adaptive learning rate method designed
    to perform better on non-stationary and online problems.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根传播（RMSprop）是一种自适应学习率方法，旨在在非平稳和在线问题上表现更好。
- en: 'How it Works: RMSprop modifies AdaGrad by using a moving average of squared
    gradients to adapt learning rates based on recent gradient magnitudes. It maintains
    a running average of squared gradients to help in maintaining steady learning
    rates.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：RMSprop通过使用平方梯度的移动平均来修改AdaGrad，根据近期梯度的幅度调整学习率。它保持平方梯度的运行平均，以帮助保持稳定的学习率。
- en: 'Pros:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Addresses the diminishing learning rate problem of AdaGrad.
  id: totrans-685
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决了AdaGrad的学习率衰减问题。
- en: •
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapts learning rate based on recent gradients.
  id: totrans-687
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据近期梯度调整学习率。
- en: •
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Effective for recurrent neural networks.
  id: totrans-689
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对递归神经网络有效。
- en: •
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More robust against non-stationary targets.
  id: totrans-691
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对非平稳目标更为稳健。
- en: 'Cons:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can still get stuck in local minima on non-convex problems.
  id: totrans-694
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在非凸问题上仍可能陷入局部最小值。
- en: •
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires hyperparameter tuning.
  id: totrans-696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要调整超参数。
- en: •
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires careful tuning of the decay rate.
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要仔细调整衰减率。
- en: •
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can be sensitive to the initial learning rate.
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对初始学习率可能较为敏感。
- en: 'When to Use: RMSprop is best for non-convex optimisation problems, training
    RNNs and LSTMs, and dealing with noisy or non-stationary objectives.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：RMSprop 最适用于非凸优化问题、训练 RNN 和 LSTM 以及处理噪声或非平稳目标。
- en: 5.4.6 AdaDelta
  id: totrans-702
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.6 AdaDelta
- en: Adaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive
    learning rates without diminishing too quickly.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应 Delta（AdaDelta）在 AdaGrad 和 RMSprop 的基础上进行改进，专注于自适应学习率而不会过快衰减。
- en: 'How it Works: AdaDelta eliminates the need for a default learning rate by using
    a moving window of gradient updates. It adapts learning rates based on recent
    gradient magnitudes to ensure consistent updates even with sparse gradients.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：AdaDelta 通过使用梯度更新的移动窗口来消除默认学习率的需求。它根据最近的梯度幅度调整学习率，以确保即使在梯度稀疏的情况下也能保持一致的更新。
- en: 'Pros:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Eliminates the need to set a default learning rate.
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消除了设置默认学习率的需要。
- en: •
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Addresses the diminishing learning rate issue.
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决了学习率衰减问题。
- en: •
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does not require manual tuning of the learning rate.
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不需要手动调整学习率。
- en: •
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Handles gradient sparsity well.
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理梯度稀疏性较好。
- en: 'Cons:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More complex than RMSprop and AdaGrad.
  id: totrans-716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比 RMSprop 和 AdaGrad 更复杂。
- en: •
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can have slower convergence initially.
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初期可能会有较慢的收敛速度。
- en: •
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can require more iterations to converge.
  id: totrans-720
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能需要更多迭代才能收敛。
- en: •
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementation can be more complex.
  id: totrans-722
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现可能更复杂。
- en: 'When to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred
    when avoiding manual learning rate setting.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：AdaDelta 适用于类似 RMSprop 的场景，但在避免手动设置学习率时更为合适。
- en: 5.4.7 Adam
  id: totrans-724
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.7 Adam
- en: Adaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop,
    making it suitable for problems with large datasets and high-dimensional spaces.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应矩估计（Adam）结合了 AdaGrad 和 RMSprop 的优点，适用于大数据集和高维空间的问题。
- en: 'How it Works: Adam uses running averages of both gradients and their squared
    values to compute adaptive learning rates for each parameter. It includes bias
    correction and often achieves faster convergence than other methods.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：Adam 使用梯度及其平方值的运行平均值来计算每个参数的自适应学习率。它包括偏差校正，通常比其他方法实现更快的收敛。
- en: 'Pros:'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combines advantages of AdaGrad and RMSprop.
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结合了 AdaGrad 和 RMSprop 的优点。
- en: •
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adaptive learning rates.
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应学习率。
- en: •
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Includes bias correction.
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括偏差校正。
- en: •
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fast convergence.
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 快速收敛。
- en: •
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Works well with large datasets and high-dimensional spaces.
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大数据集和高维空间中表现良好。
- en: 'Cons:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires tuning of hyperparameters (though it often works well with defaults).
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要调整超参数（尽管默认设置通常效果良好）。
- en: •
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Computationally intensive.
  id: totrans-742
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算密集型。
- en: •
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can lead to overfitting if not regularised properly.
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有正确的正则化，可能导致过拟合。
- en: •
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires more memory.
  id: totrans-746
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要更多内存。
- en: 'When to Use: Adam is widely used in most deep learning applications due to
    its efficiency and effectiveness, particularly in complex neural network architectures.'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：由于其效率和效果，Adam 广泛用于大多数深度学习应用，特别是在复杂的神经网络架构中。
- en: 5.4.8 AdamW
  id: totrans-748
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.8 AdamW
- en: AdamW is an extension of Adam that includes weight decay regularisation to address
    overfitting issues present in Adam.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW 是 Adam 的扩展，包含权重衰减正则化，以解决 Adam 中存在的过拟合问题。
- en: 'How it Works: AdamW integrates L2 regularisation directly into the parameter
    updates, decoupling weight decay from the learning rate. This improves generalisation
    and is suitable for fine-tuning large models.'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：AdamW 将 L2 正则化直接集成到参数更新中，将权重衰减与学习率解耦。这提高了模型的泛化能力，并适用于大型模型的微调。
- en: 'Pros:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Includes weight decay for better regularisation.
  id: totrans-753
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括权重衰减以实现更好的正则化。
- en: •
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combines Adam’s adaptive learning rate with L2 regularisation.
  id: totrans-755
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 Adam 的自适应学习率与 L2 正则化结合。
- en: •
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Improves generalisation.
  id: totrans-757
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改善了泛化能力。
- en: •
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reduces overfitting compared to Adam.
  id: totrans-759
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 Adam 相比，减少了过拟合。
- en: 'Cons:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Slightly more complex than Adam.
  id: totrans-762
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比 Adam 稍微复杂。
- en: •
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires careful tuning of the weight decay parameter.
  id: totrans-764
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要仔细调整权重衰减参数。
- en: •
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Slightly slower than Adam due to additional computations.
  id: totrans-766
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于额外的计算，比 Adam 略慢。
- en: •
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires more memory.
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要更多内存。
- en: 'When to Use: AdamW is ideal for scenarios where regularisation is needed, such
    as preventing overfitting in large models and fine-tuning pre-trained models.'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：AdamW 适用于需要正则化的场景，如防止大型模型的过拟合和微调预训练模型。
- en: A comprehensive collection of optimisation algorithms implemented within the
    PyTorch library can be found in [here](https://pytorch.org/docs/stable/optim.html).
    The Hugging Face Transformers package also offers a variety of optimisers for
    initialising and fine-tuning language models, available [here](https://huggingface.co/docs/transformers/en/main_classes/optimiser_schedules).
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 在[这里](https://pytorch.org/docs/stable/optim.html)可以找到PyTorch库中实现的优化算法的全面集合。Hugging
    Face Transformers包也提供了多种用于初始化和微调语言模型的优化器，详见[这里](https://huggingface.co/docs/transformers/en/main_classes/optimiser_schedules)。
- en: 5.5 Challenges in Training Setup
  id: totrans-771
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 训练设置中的挑战
- en: '1.'
  id: totrans-772
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Ensuring compatibility and proper configuration of high-performance hardware
    like GPUs or TPUs can be complex and time-consuming.
  id: totrans-773
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保高性能硬件（如GPU或TPU）的兼容性和正确配置可能复杂且耗时。
- en: '2.'
  id: totrans-774
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Managing dependencies and versions of deep learning frameworks and libraries
    to avoid conflicts and leverage the latest features.
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管理深度学习框架和库的依赖项和版本，以避免冲突并利用最新功能。
- en: '3.'
  id: totrans-776
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Selecting an appropriate learning rate is critical, as too high a rate can cause
    suboptimal convergence, while too low a rate can make the training process excessively
    slow.
  id: totrans-777
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的学习率至关重要，因为过高的学习率可能导致次优的收敛，而过低的学习率则可能使训练过程过于缓慢。
- en: '4.'
  id: totrans-778
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Determining the optimal batch size that balances memory constraints and training
    efficiency, especially given the large memory requirements of LLMs.
  id: totrans-779
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确定平衡内存限制和训练效率的最佳批量大小，特别是在大型语言模型需要大量内存的情况下。
- en: '5.'
  id: totrans-780
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Choosing the right number of epochs to avoid underfitting or overfitting the
    model, requiring careful monitoring and validation.
  id: totrans-781
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的训练周期数以避免模型的欠拟合或过拟合，这需要仔细的监控和验证。
- en: '6.'
  id: totrans-782
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Selecting the most suitable optimiser for the specific training task to efficiently
    update the model’s weights.
  id: totrans-783
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择最适合特定训练任务的优化器，以高效地更新模型的权重。
- en: '7.'
  id: totrans-784
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Choosing the correct loss function to accurately measure model performance and
    guide the optimisation process.
  id: totrans-785
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择正确的损失函数，以准确衡量模型性能并指导优化过程。
- en: 5.6 Best Practices
  id: totrans-786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 最佳实践
- en: •
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to
    2e-4, to ensure stable convergence. A learning rate schedule, such as learning
    rate warm-up followed by a linear decay, can also be beneficial. This helps in
    initially stabilising the training and then allowing the model to converge more
    accurately.'
  id: totrans-788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳学习率：使用较低的学习率，通常在1e-4到2e-4之间，以确保稳定的收敛。学习率调度，例如学习率预热后跟线性衰减，也可能会有所帮助。这有助于最初稳定训练，然后允许模型更准确地收敛。
- en: •
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Batch Size Considerations: Opt for a batch size that balances memory constraints
    and training efficiency. Smaller batch sizes can help in achieving faster convergence
    but may require more frequent updates. Conversely, larger batch sizes can be more
    memory-intensive but may lead to more stable updates. Experiment with different
    batch sizes to find the optimal balance for your specific use case.'
  id: totrans-790
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小考虑：选择一个平衡内存限制和训练效率的批量大小。较小的批量大小可以帮助实现更快的收敛，但可能需要更频繁的更新。相反，较大的批量大小可能更消耗内存，但可能导致更稳定的更新。尝试不同的批量大小以找到适合你特定用例的最佳平衡。
- en: •
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Save Checkpoints Regularly: Regularly save model weights at various intervals
    across 5-8 epochs to capture optimal performance without overfitting. Implement
    early stopping mechanisms to halt training once the model performance starts to
    degrade on the validation set, thereby preventing overfitting [[51](#bib.bib51)].'
  id: totrans-792
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定期保存检查点：在5-8个周期内定期保存模型权重，以捕捉最佳性能而不至于过拟合。实施早停机制，一旦模型在验证集上的性能开始下降，便停止训练，从而防止过拟合[[51](#bib.bib51)]。
- en: •
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search,
    random search, and Bayesian optimisation to find the optimal set of hyperparameters.
    Tools such as Optuna, Hyperopt, and Ray Tune can automate this process and help
    in efficiently exploring the hyperparameter space [[49](#bib.bib49)].'
  id: totrans-794
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数调优：利用网格搜索、随机搜索和贝叶斯优化等超参数调优方法来寻找最佳的超参数组合。工具如Optuna、Hyperopt和Ray Tune可以自动化这一过程，并帮助高效地探索超参数空间[[49](#bib.bib49)]。
- en: •
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Parallelism and Model Parallelism: For large-scale training, consider
    using data parallelism or model parallelism techniques to distribute the training
    workload across multiple GPUs or TPUs. Libraries like Horovod and DeepSpeed can
    facilitate efficient distributed training, helping to reduce training time and
    manage memory usage effectively [[52](#bib.bib52), [53](#bib.bib53)].'
  id: totrans-796
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据并行和模型并行：对于大规模训练，考虑使用数据并行或模型并行技术，将训练负载分配到多个GPU或TPU上。像Horovod和DeepSpeed这样的库可以促进高效的分布式训练，帮助减少训练时间并有效管理内存使用[[52](#bib.bib52),
    [53](#bib.bib53)]。
- en: •
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Regular Monitoring and Logging: Implement robust monitoring and logging to
    track training metrics, resource usage, and potential bottlenecks. Tools like
    TensorBoard, Weights & Biases, and MLflow can provide real-time insights into
    the training process, allowing for timely interventions and adjustments.'
  id: totrans-798
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定期监控和日志记录：实施健全的监控和日志记录，以跟踪训练指标、资源使用情况和潜在瓶颈。像TensorBoard、Weights & Biases和MLflow这样的工具可以提供实时的训练过程洞察，允许及时干预和调整。
- en: •
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Handling Overfitting and Underfitting: Ensure that your model generalises well
    by implementing techniques to handle overfitting and underfitting. regularisation
    techniques such as L2 regularisation, dropout, and data augmentation can help
    prevent overfitting. Conversely, if your model is underfitting, consider increasing
    the model complexity or training for more epochs.'
  id: totrans-800
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理过拟合和欠拟合：通过实施处理过拟合和欠拟合的技术，确保模型能够很好地泛化。正则化技术如L2正则化、丢弃法和数据增强可以帮助防止过拟合。相反，如果模型欠拟合，可以考虑增加模型复杂度或训练更多轮次。
- en: •
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Use Mixed Precision Training: Mixed precision training involves using both
    16-bit and 32-bit floating-point types to reduce memory usage and increase computational
    efficiency. This technique can significantly speed up training and reduce the
    required memory footprint, especially when using large models. NVIDIA’s Apex and
    TensorFlow’s mixed precision API provide support for implementing mixed precision
    training [[54](#bib.bib54)].'
  id: totrans-802
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用混合精度训练：混合精度训练涉及同时使用16位和32位浮点类型，以减少内存使用并提高计算效率。这种技术可以显著加快训练速度并减少所需的内存占用，特别是在使用大型模型时。NVIDIA的Apex和TensorFlow的混合精度API提供了实现混合精度训练的支持[[54](#bib.bib54)]。
- en: •
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evaluate and Iterate: Continuously evaluate the model performance using a separate
    validation set and iterate on the training process based on the results. Regularly
    update your training data and retrain the model to keep it current with new data
    trends and patterns.'
  id: totrans-804
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估和迭代：使用独立的验证集持续评估模型性能，并根据结果对训练过程进行迭代。定期更新训练数据并重新训练模型，以保持其与新数据趋势和模式的同步。
- en: •
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Documentation and Reproducibility: Maintain thorough documentation of your
    training setup, including the hardware configuration, software environment, and
    hyperparameters used. Ensure reproducibility by setting random seeds and providing
    detailed records of the training process. This practice not only aids in debugging
    and further development but also facilitates collaboration and sharing of results
    with the broader research community.'
  id: totrans-806
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档记录和可重复性：保持对训练设置的详细文档，包括硬件配置、软件环境和使用的超参数。通过设置随机种子和提供训练过程的详细记录来确保可重复性。这种做法不仅有助于调试和进一步开发，还促进了与更广泛研究社区的合作和结果共享。
- en: 'Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations'
  id: totrans-807
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章 第4阶段：微调技术和适当模型配置的选择
- en: This chapter focuses on selecting appropriate fine-tuning techniques and model
    configurations that suit the specific requirements of various tasks. Fine-tuning
    is a crucial stage where pre-trained models are adapted to specific tasks or domains.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍选择适当的微调技术和模型配置，以满足各种任务的具体需求。微调是一个关键阶段，在此阶段，预训练模型会根据特定任务或领域进行调整。
- en: 6.1 Steps Involved in Fine-Tuning
  id: totrans-809
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 微调涉及的步骤
- en: The following steps outline the fine-tuning process, integrating advanced techniques
    and best practices.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了微调过程，整合了先进的技术和最佳实践。
- en: '1.'
  id: totrans-811
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained
    tokenizer and model. The tokenizer ensures that the input text is converted into
    a format the model can process, while the pre-trained model serves as the foundation
    for further adaptation. Depending on the task, select a model that has been pre-trained
    on relevant data to provide a strong starting point.'
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化预训练的分词器和模型：首先加载预训练的分词器和模型。分词器确保输入文本被转换成模型可以处理的格式，而预训练模型则作为进一步适配的基础。根据任务选择一个在相关数据上进行过预训练的模型，以提供一个良好的起点。
- en: '2.'
  id: totrans-813
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Modify the Model’s Output Layer: Adjust the model’s output layer to align with
    the specific requirements of the target task. This may involve modifying existing
    layers or adding new layers. For instance, tasks like classification may require
    a softmax layer with the appropriate number of classes, while text generation
    tasks might involve changes in the decoding mechanism.'
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改模型的输出层：调整模型的输出层以符合目标任务的具体要求。这可能涉及修改现有层或添加新层。例如，分类任务可能需要一个具有适当类别数的softmax层，而文本生成任务可能需要改变解码机制。
- en: '3.'
  id: totrans-815
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy
    that best fits the task and the model architecture. Some Options include:'
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的微调策略：选择最适合任务和模型架构的微调策略。一些选项包括：
- en: •
  id: totrans-817
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation,
    classification, and question answering, adapt the model using relevant datasets.'
  id: totrans-818
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定微调：对于文本摘要、代码生成、分类和问答等任务，使用相关数据集对模型进行调整。
- en: •
  id: totrans-819
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text
    relevant to specific domains, such as medical, financial, or legal fields.'
  id: totrans-820
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域特定微调：将模型调整为理解和生成与特定领域（如医学、金融或法律领域）相关的文本。
- en: •
  id: totrans-821
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters
    allow for fine-tuning with reduced computational costs by updating a small subset
    of model parameters.'
  id: totrans-822
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）：像LoRA、QLoRA和适配器等技术允许通过更新模型参数的一个小子集来进行低计算成本的微调。
- en: •
  id: totrans-823
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and
    learning new tasks by updating only half of the model’s parameters during each
    fine-tuning round.'
  id: totrans-824
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半微调（HFT）：通过在每轮微调中仅更新一半模型参数，在保留预训练知识和学习新任务之间取得平衡。
- en: '4.'
  id: totrans-825
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Set Up the Training Loop: Establish the training loop, incorporating the selected
    fine-tuning strategy. The loop should include data loading, loss computation,
    backpropagation, and parameter updates. When using PEFT methods, ensure that only
    the relevant parameters are updated to maximise efficiency. Implement techniques
    like dynamic learning rates and early stopping to enhance the training process.'
  id: totrans-826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置训练循环：建立训练循环，融入所选的微调策略。循环应包括数据加载、损失计算、反向传播和参数更新。使用PEFT方法时，确保仅更新相关参数以最大化效率。实施动态学习率和提前停止等技术以提高训练过程。
- en: '5.'
  id: totrans-827
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple
    tasks, consider strategies like fine-tuning with multiple adapters or leveraging
    Mixture of Experts (MoE) architectures. These methods allow a single model to
    handle various tasks by utilising specialised sub-networks or adapters for each
    task.'
  id: totrans-828
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 融合多任务处理技术：如果要进行多任务微调，可以考虑使用多适配器微调或利用专家混合（MoE）架构等策略。这些方法允许一个模型通过利用每个任务的专用子网络或适配器来处理各种任务。
- en: '6.'
  id: totrans-829
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Monitor Performance on a Validation Set: Regularly evaluate the model’s performance
    on a validation set to ensure it generalises well to unseen data. Adjust hyperparameters
    such as learning rate, batch size, and dropout rates based on the validation performance.
    Utilise advanced monitoring tools to track metrics like accuracy, loss, and overfitting.'
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在验证集上监控性能：定期评估模型在验证集上的表现，以确保其对未见数据的泛化能力。根据验证性能调整超参数，如学习率、批量大小和dropout率。利用先进的监控工具来跟踪准确率、损失和过拟合等指标。
- en: '7.'
  id: totrans-831
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Optimise Model Using Advanced Techniques: Employ techniques such as Proximal
    Policy Optimisation (PPO) for reinforcement learning scenarios, or Direct Preference
    Optimisation (DPO) for aligning model outputs with human preferences. These techniques
    are particularly useful in fine-tuning models for tasks requiring nuanced decision-making
    or human-like responses.'
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用先进技术优化模型：采用如近端策略优化（PPO）用于强化学习场景，或直接偏好优化（DPO）以使模型输出与人类偏好对齐。这些技术在微调需要细致决策或类似人类反应的任务时特别有用。
- en: '8.'
  id: totrans-833
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Prune and optimise the Model (if necessary): To deploy the model in resource-constrained
    environments, consider pruning techniques to reduce its size and complexity. This
    involves removing unnecessary parameters or components without significantly affecting
    performance. Utilise dynamic pruning methods during inference to optimise the
    model on-the-fly for different scenarios.'
  id: totrans-834
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝和优化模型（如有必要）：为在资源受限环境中部署模型，考虑剪枝技术以减少其大小和复杂性。这涉及去除不必要的参数或组件，而不显著影响性能。利用动态剪枝方法在推理过程中实时优化模型以适应不同场景。
- en: '9.'
  id: totrans-835
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Continuous Evaluation and Iteration: Continuously evaluate the model’s performance
    across various tasks using appropriate benchmarks. Iterate on the fine-tuning
    process, making adjustments based on performance metrics and real-world testing.
    This iterative approach helps in refining the model to meet specific performance
    criteria.'
  id: totrans-836
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续评估和迭代：通过适当的基准持续评估模型在各种任务中的表现。对微调过程进行迭代，根据性能指标和实际测试进行调整。这种迭代方法有助于将模型精炼以满足特定性能标准。
- en: 6.2 Fine-Tuning Strategies for LLMs
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 LLM的微调策略
- en: 6.2.1 Task-Specific Fine-Tuning
  id: totrans-838
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 任务特定微调
- en: Task-specific fine-tuning adapts large language models (LLMs) for particular
    downstream tasks using appropriately formatted and cleaned data. Below is a summary
    of key tasks suitable for fine-tuning LLMs, including examples of LLMs tailored
    to these tasks.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 任务特定的微调将大型语言模型（LLMs）调整为特定的下游任务，使用适当格式化和清洗的数据。以下是适合微调LLMs的关键任务摘要，包括针对这些任务调整的LLM示例。
- en: '| Task | Description | Key Models |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | 关键模型 |'
- en: '| --- | --- | --- |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Text Summarisation | Condensing long texts into coherent summaries while
    retaining key information. Approaches include Extractive (selecting key sentences)
    and Abstractive summarisation (generating new sentences). | BERTSUM, GPT-3, T5
    |'
  id: totrans-842
  prefs: []
  type: TYPE_TB
  zh: '| 文本总结 | 将长文本浓缩为连贯的摘要，同时保留关键信息。方法包括提取式（选择关键句子）和抽象式总结（生成新句子）。 | BERTSUM, GPT-3,
    T5 |'
- en: '| Code Generation | Automatically generating programming code based on natural
    language descriptions, partial code snippets, or structured data inputs. | Codex,
    GPT-3, CodeBERT |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成 | 基于自然语言描述、部分代码片段或结构化数据输入自动生成编程代码。 | Codex, GPT-3, CodeBERT |'
- en: '| Classification | Categorising text into predefined labels such as Sentiment
    Analysis, Topic Classification, and Entity Classification. | BERT, RoBERTa, GPT-4
    |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 将文本分类到预定义标签中，如情感分析、主题分类和实体分类。 | BERT, RoBERTa, GPT-4 |'
- en: '| Q&A | Understanding and generating accurate, contextually relevant answers
    to natural language questions. | BERT, GPT-3, T5 |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
  zh: '| 问答 | 理解和生成准确的、上下文相关的自然语言问题答案。 | BERT, GPT-3, T5 |'
- en: 'Table 6.1: Overview of tasks such as text summarisation, code generation, classification,
    and Q&A, along with their key LLMs and descriptions.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：任务概述，如文本总结、代码生成、分类和问答，以及它们的关键LLM和描述。
- en: 6.2.2 Domain-Specific Fine-Tuning
  id: totrans-847
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 领域特定微调
- en: Domain-specific fine-tuning focuses on tailoring the model to comprehend and
    produce text relevant to a specific domain or industry. By fine-tuning the model
    on a dataset derived from the target domain, it enhances the model’s contextual
    understanding and expertise in domain-specific tasks. Below are examples of domain-specific
    LLMs.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定的微调专注于调整模型，以理解和生成与特定领域或行业相关的文本。通过在来自目标领域的数据集上微调模型，可以提升模型在领域特定任务中的上下文理解和专业知识。以下是领域特定LLM的示例。
- en: Medical Domain
  id: totrans-849
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 医疗领域
- en: 'Model Description: Med-PaLM 2 is trained on meticulously curated medical datasets
    and is capable of accurately answering medical questions, achieving performance
    comparable to that of medical professionals [[55](#bib.bib55)].'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：Med-PaLM 2 在精心策划的医疗数据集上进行训练，能够准确回答医疗问题，性能与医疗专业人士相当 [[55](#bib.bib55)]。
- en: 'Base Model: PaLM 2'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：PaLM 2
- en: 'Fine-tuned Model Parameters: Not Known'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：未知
- en: 'Fine-Tuning Techniques Used: Instruction fine-tuning'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：指令微调
- en: 'Datasets Used:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MedQA
  id: totrans-856
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MedQA
- en: •
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MedMCQA
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MedMCQA
- en: •
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LiveQA
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LiveQA
- en: •
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MedicationQA
  id: totrans-862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MedicationQA
- en: •
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: HealthSearchQA
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HealthSearchQA
- en: 'Results: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating
    superior performance in handling complex medical knowledge and reasoning tasks.'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：Med-PaLM 2 在多个关键医学基准测试中优于 GPT-4，展示了在处理复杂医学知识和推理任务方面的卓越性能。
- en: Finance Domain
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 金融领域
- en: 'Model Description: FinGPT, an open-source LLM tailored for the financial sector,
    enhances financial research and cooperation by promoting data accessibility and
    handling finance-specific issues like data acquisition and quality [[56](#bib.bib56)].'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：FinGPT，是一款针对金融领域的开源 LLM，通过提升数据可获取性和处理金融特有问题（如数据获取和质量），增强金融研究和合作 [[56](#bib.bib56)]。
- en: 'Base Model: LlaMA, ChatGLM, and other Transformer Models'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：LlaMA、ChatGLM 和其他 Transformer 模型
- en: 'Fine-tuned Model Parameters: Not Known'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：未知
- en: 'Fine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：LoRA，股票价格的强化学习（RLSP）
- en: 'Datasets Used:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Financial News (Reuters, CNBC, Yahoo Finance)
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 财经新闻（路透社、CNBC、雅虎财经）
- en: •
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Social Media (Twitter, Facebook, Reddit, Weibo)
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 社交媒体（Twitter、Facebook、Reddit、微博）
- en: •
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regulatory Filings (e.g., SEC filings)
  id: totrans-877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监管文件（例如，SEC 文件）
- en: •
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Trends (Seeking Alpha, Google Trends)
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 趋势（Seeking Alpha、Google Trends）
- en: •
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Academic Datasets
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学术数据集
- en: 'Results: Not Applicable'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：不适用
- en: Legal Domain
  id: totrans-883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 法律领域
- en: 'Model Description: LAWGPT, the first open-source model specifically designed
    for Chinese legal applications, demonstrates superior capability in handling Chinese
    legal tasks [[57](#bib.bib57)].'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：LAWGPT，是首个专门为中国法律应用设计的开源模型，在处理中国法律任务方面表现出色 [[57](#bib.bib57)]。
- en: 'Base Model: Chinese Alpaca Plus 7B base model'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：Chinese Alpaca Plus 7B 基础模型
- en: 'Fine-tuned Model Parameters: Not Known'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：未知
- en: 'Fine-Tuning Techniques Used: LoRA with Alpaca template'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：使用 Alpaca 模板的 LoRA
- en: 'Datasets Used:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Open-source dataset: 200,000 examples containing crime type prediction and
    crime consultation tasks.'
  id: totrans-890
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开源数据集：包含犯罪类型预测和犯罪咨询任务的 200,000 个样本。
- en: •
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'JEC-QA dataset: 20,000 examples containing legal question answering tasks.'
  id: totrans-892
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEC-QA 数据集：包含 20,000 个法律问答任务的样本。
- en: •
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA
    datasets using ChatGPT.'
  id: totrans-894
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建的法律数据集：80,000 个样本，通过使用 ChatGPT 从开源和 JEC-QA 数据集中提炼而来。
- en: 'Results: LAWGPT demonstrates notable performance improvements over the LLaMA
    7B model in various legal tasks, but still trails behind proprietary models like
    GPT-3.5 Turbo and GPT-4.'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：LAWGPT 在各种法律任务中表现出显著的性能提升，优于 LLaMA 7B 模型，但仍落后于 GPT-3.5 Turbo 和 GPT-4 等专有模型。
- en: Pharmaceutical Domain
  id: totrans-896
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 制药领域
- en: 'Model Description: PharmaGPT, a suite of domain-specific large language models
    tailored to the biopharmaceutical and chemical industries, sets a new benchmark
    for precision in these fields [[58](#bib.bib58)].'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：PharmaGPT，一系列针对生物制药和化学工业的特定领域大语言模型，为这些领域的精确度设立了新标准 [[58](#bib.bib58)]。
- en: 'Base Model: LlaMA series'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：LlaMA 系列
- en: 'Fine-tuned Model Parameters: 13B and 70B'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：13B 和 70B
- en: 'Fine-Tuning Techniques Used: Instruction fine-tuning and RLHF'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：指令微调和 RLHF
- en: 'Datasets Used:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Specific-domain data from academic papers and clinical reports
  id: totrans-903
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自学术论文和临床报告的特定领域数据
- en: •
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Text data from NLP dataset formats (e.g., question answering, summarisation,
    dialogue)
  id: totrans-905
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自 NLP 数据集格式的文本数据（例如，问答、总结、对话）
- en: •
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Instruction fine-tuning dataset for multitask learning
  id: totrans-907
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多任务学习的指令微调数据集
- en: •
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RLHF dataset with human preference expert-annotated instructions
  id: totrans-909
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RLHF 数据集，含人工偏好专家注释的指令
- en: 'Results: PharmaGPT models demonstrated impressive performance on various pharmaceutical
    benchmarks, consistently outperforming GPT-3.5 Turbo.'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：PharmaGPT 模型在各种制药基准测试中表现出色，一直优于 GPT-3.5 Turbo。
- en: Finance Domain
  id: totrans-911
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 金融领域
- en: 'Model Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large
    language model specifically designed for the financial sector. [[59](#bib.bib59)]'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：Palmyra-Fin-70B-32K，由 Writer 开发，是一款专门为金融行业设计的领先大语言模型。[[59](#bib.bib59)]
- en: 'Base Model: LlaMA'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：LlaMA
- en: 'Fine-tuned Model Parameters: 70B'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：70B
- en: 'Fine-Tuning Techniques Used: Not Known'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：未知
- en: 'Datasets Used: Not Known'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：未知
- en: 'Results: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving
    leading results across various financial datasets and excelling in financial document
    analysis, market trend prediction, and risk assessment.'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：Palmyra-Fin-70B-32K 展现了最先进的性能，在各种金融数据集中取得了领先结果，并在金融文档分析、市场趋势预测和风险评估方面表现出色。
- en: 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques
  id: totrans-918
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 参数高效微调（PEFT）技术
- en: 'Parameter Efficient Fine Tuning [(PEFT)](https://github.com/huggingface/peft)
    is an impactful NLP technique that adeptly adapts pre-trained language models
    to various applications with remarkable efficiency. PEFT methods fine-tune only
    a small subset of (additional) model parameters while keeping most of the pre-trained
    LLM parameters frozen, thereby significantly reducing computational and storage
    costs. This approach mitigates the issue of catastrophic forgetting, a phenomenon
    where neural networks lose previously acquired knowledge and experience a significant
    performance decline on previously learned tasks when trained on new datasets.
    PEFT methods have demonstrated superior performance compared to full fine-tuning,
    particularly in low-data scenarios, and exhibit better generalisation to out-of-domain
    contexts. This technique is applicable to various modalities, such as financial
    sentiment classification and machine translation of medical terminologies. A taxonomy
    of PEFT-based fine-tuning approaches is provided in Figure[6.1](#Ch6.F1 "Figure
    6.1 ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage
    4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations ‣
    The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)"). We will further discuss a few key PEFT-based
    approaches in the following sections.'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调 [(PEFT)](https://github.com/huggingface/peft) 是一种有效的NLP技术，能够以显著的效率将预训练语言模型适配到各种应用中。PEFT方法仅微调模型参数的一个小子集，同时保持大多数预训练LLM参数不变，从而显著降低计算和存储成本。这种方法缓解了灾难性遗忘的问题，即神经网络在训练新数据集时丧失之前获得的知识，并在之前学过的任务上表现出显著的性能下降。与完全微调相比，PEFT方法在低数据场景下表现出更优越的性能，并在跨领域背景下展示了更好的泛化能力。这一技术适用于各种模态，如金融情感分类和医学术语的机器翻译。图[6.1](#Ch6.F1
    "图6.1 ‣ 6.3 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")
    提供了PEFT基础微调方法的分类。我们将在以下章节中进一步讨论一些关键的基于PEFT的方法。
- en: '![Refer to caption](img/e46d6577e1e8d21bc229e596602f5f83.png)'
  id: totrans-920
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e46d6577e1e8d21bc229e596602f5f83.png)'
- en: 'Figure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT)
    Methods for Large Language Models (LLMs). This figure categorises various PEFT
    techniques, highlighting their distinct approaches, from additive and selective
    fine-tuning to reparameterised and hybrid methods. It details specific strategies
    within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based Fine-Tuning,
    and their respective sub-techniques like LoRA and its derivatives, showcasing
    the diverse and evolving landscape of LLM fine-tuning. (adapted from [[60](#bib.bib60)])'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：大型语言模型（LLMs）参数高效微调（PEFT）方法的综合分类。这张图展示了各种PEFT技术的分类，突出了它们不同的方法，包括附加微调、选择性微调、重新参数化和混合方法。它详细说明了每个类别中的具体策略，如基于适配器的微调、基于软提示的微调，以及其各自的子技术，如LoRA及其衍生品，展示了LLM微调的多样化和不断发展的格局。（改编自[[60](#bib.bib60)]）
- en: 6.3.1 Adapters
  id: totrans-922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 适配器
- en: Adapter-based methods introduce additional trainable parameters after the attention
    and fully connected layers of a frozen pre-trained model, aiming to reduce memory
    usage and accelerate training. The specific approach varies depending on the adapter;
    it might involve adding an extra layer or representing the weight updates delta
    (W) as a low-rank decomposition of the weight matrix. Regardless of the method,
    adapters are generally small yet achieve performance comparable to fully fine-tuned
    models, allowing for the training of larger models with fewer resources.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 基于适配器的方法在冻结的预训练模型的注意力层和全连接层之后引入额外的可训练参数，旨在减少内存使用并加快训练。具体方法根据适配器的不同而有所不同；它可能涉及添加额外的层，或将权重更新增量（W）表示为权重矩阵的低秩分解。无论采用哪种方法，适配器通常较小，但能够实现与完全微调模型相媲美的性能，从而允许在资源较少的情况下训练更大的模型。
- en: '![Refer to caption](img/9d6c08f62e7040457a4281198f1fc5b3.png)'
  id: totrans-924
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9d6c08f62e7040457a4281198f1fc5b3.png)'
- en: 'Figure 6.2: Schematic representation of the Adapter Architecture used in LLMs.
    The diagram showcases the integration of adapters within the Transformer architecture,
    including the feed-forward up and down layers and their role in enabling efficient
    model adaptation by inserting additional parameters while maintaining the model’s
    core structure (adapted from [[61](#bib.bib61)])'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：用于 LLM 的适配器架构的示意图。图示展示了在 Transformer 架构中集成适配器的过程，包括前馈上层和下层及其在插入额外参数的同时保持模型核心结构的作用（改编自
    [[61](#bib.bib61)]）
- en: HuggingFace supports adapter configurations through the PEFT library. During
    fine-tuning, new adapters are integrated into the model using LoraConfig ¹¹1[https://huggingface.co/docs/peft/en/package_reference/lora](https://huggingface.co/docs/peft/en/package_reference/lora).
    HuggingFace uses PeftConfig to load existing pre-trained models and apply PEFT
    techniques. Additionally, HuggingFace provides built-in support to run the fine-tuning
    process across any distributed configuration using Accelerate²²2[https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index),
    making large-scale training and inference simple, efficient, and adaptable.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace 通过 PEFT 库支持适配器配置。在微调过程中，通过使用 LoraConfig ¹¹1[https://huggingface.co/docs/peft/en/package_reference/lora](https://huggingface.co/docs/peft/en/package_reference/lora)
    将新的适配器集成到模型中。HuggingFace 使用 PeftConfig 加载现有的预训练模型并应用 PEFT 技术。此外，HuggingFace 提供了内置支持，通过
    Accelerate²²2[https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index)
    在任何分布式配置中运行微调过程，使大规模训练和推理变得简单、高效且具有适应性。
- en: 6.3.2 Low-Rank Adaptation (LoRA)
  id: totrans-927
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 低秩适应（LoRA）
- en: Low-Rank Adaptation (LoRA)[[62](#bib.bib62)] is a technique designed for fine-tuning
    large language models, which modifies the fine-tuning process by freezing the
    original model weights and applying changes to a separate set of weights, added
    to the original parameters. LoRA transforms the model parameters into a lower-rank
    dimension, reducing the number of trainable parameters, speeding up the process,
    and lowering costs. This method is particularly useful in scenarios where multiple
    clients require fine-tuned models for different applications, allowing for the
    creation of specific weights for each use case without the need for separate models.
    By employing low-rank approximation methods, LoRA effectively reduces computational
    and resource requirements while preserving the pre-trained model’s adaptability
    to specific tasks or domains.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）[[62](#bib.bib62)] 是一种针对大规模语言模型进行微调的技术，该技术通过冻结原始模型权重并对一组单独的权重进行修改，从而改变微调过程。LoRA
    将模型参数转换为低秩维度，减少了可训练参数的数量，加快了过程并降低了成本。这种方法在需要多个客户端为不同应用程序进行微调模型的场景中特别有用，允许为每个用例创建特定的权重，而无需单独的模型。通过采用低秩近似方法，LoRA
    有效地减少了计算和资源需求，同时保留了预训练模型对特定任务或领域的适应性。
- en: '![Refer to caption](img/4bd5e9445d38406defc47d824480fb2c.png)'
  id: totrans-929
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4bd5e9445d38406defc47d824480fb2c.png)'
- en: 'Figure 6.3: A comparison between weight updates in regular fine-tuning and
    LoRA fine-tuning. In regular fine-tuning, the entire weight update matrix ($\Delta
    W$), significantly reducing the number of trainable parameters by leveraging the
    inner dimension (r), which is a hyperparameter. This method is more efficient
    in terms of memory and computation, making it ideal for fine-tuning large models.
    (adapted from [[63](#bib.bib63)])'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：常规微调和 LoRA 微调中的权重更新比较。在常规微调中，整个权重更新矩阵（$\Delta W$）显著减少了可训练参数的数量，通过利用内维度（r），这是一个超参数。这种方法在内存和计算方面更为高效，使其非常适合大规模模型的微调。（改编自
    [[63](#bib.bib63)]）
- en: Benefits of Using LoRA
  id: totrans-931
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 LoRA 的好处
- en: '1.'
  id: totrans-932
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Parameter Efficiency: LoRA significantly reduces the number of parameters that
    need to be trained by focusing only on the low-rank matrices, resulting in lower
    memory and storage requirements compared to full fine-tuning.'
  id: totrans-933
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数效率：LoRA 通过仅关注低秩矩阵显著减少了需要训练的参数数量，从而比完全微调需要更少的内存和存储需求。
- en: '2.'
  id: totrans-934
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Efficient Storage: The storage of the trained model is more efficient as it
    only requires storing the low-rank matrices instead of the full model weights.'
  id: totrans-935
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效存储：训练后的模型存储更为高效，因为只需要存储低秩矩阵而不是完整的模型权重。
- en: '3.'
  id: totrans-936
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Reduced Computational Load: Training with low-rank matrices requires fewer
    computational resources, making it faster and more scalable.'
  id: totrans-937
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 降低计算负担：使用低秩矩阵进行训练需要较少的计算资源，从而使其更快且更具可扩展性。
- en: '4.'
  id: totrans-938
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Lower Memory Footprint: Since fewer parameters are being updated, the memory
    footprint during training is reduced, enabling the use of larger batch sizes or
    more complex models within the same hardware constraints.'
  id: totrans-939
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 较低的内存占用：由于更新的参数较少，训练过程中的内存占用减少，使得在相同硬件限制下可以使用更大的批量大小或更复杂的模型。
- en: '5.'
  id: totrans-940
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Flexibility: LoRA can be easily integrated with existing pre-trained models
    without extensive modifications to the model architecture.'
  id: totrans-941
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活性：LoRA 可以轻松与现有的预训练模型集成，而无需对模型架构进行大量修改。
- en: '6.'
  id: totrans-942
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Compatibility: It can be used alongside other fine-tuning techniques, such
    as adapter layers or prompt-tuning, to further enhance performance.'
  id: totrans-943
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 兼容性：它可以与其他微调技术（如适配器层或提示调整）一起使用，以进一步提升性能。
- en: '7.'
  id: totrans-944
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Comparable Results: Despite the reduction in the number of trainable parameters,
    LoRA has been shown to achieve performance comparable to full fine-tuning in many
    tasks.'
  id: totrans-945
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相当的结果：尽管可训练参数数量减少，LoRA 已被证明在许多任务中能达到与完全微调相当的性能。
- en: '8.'
  id: totrans-946
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Task-Specific Adaptation: It effectively adapts the pre-trained model to specific
    tasks, leveraging the knowledge already embedded in the original model.'
  id: totrans-947
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定适配：它有效地将预训练模型适配到特定任务，利用原模型中已嵌入的知识。
- en: '9.'
  id: totrans-948
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating
    overfitting, especially when dealing with smaller task-specific datasets.'
  id: totrans-949
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 避免过拟合：通过专注于低秩更新，LoRA 有助于减轻过拟合，特别是在处理较小的任务特定数据集时。
- en: Limitations
  id: totrans-950
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 限制
- en: 'While LoRA demonstrates considerable power, it also presents challenges:'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LoRA 展示了相当的能力，但也存在一些挑战：
- en: •
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding
    substantial alterations to the pre-trained model’s internal representations.'
  id: totrans-953
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调范围：LoRA 在应用于需要对预训练模型内部表示进行大幅修改的任务时可能会遇到困难。
- en: •
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyperparameter Optimisation: Tuning the rank parameter ‘r’ requires meticulous
    adjustment for optimal performance.'
  id: totrans-955
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数优化：调整秩参数‘r’需要精细调整，以获得最佳性能。
- en: •
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ongoing Research: Despite its promise, LoRA is still in active research stages,
    and its long-term implications remain to be fully explored.'
  id: totrans-957
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 进行中的研究：尽管前景广阔，LoRA 仍处于活跃的研究阶段，其长期影响仍待全面探索。
- en: Despite these challenges, LoRA stands as a pioneering technique with vast potential
    to democratise access to the capabilities of LLMs. Continued research and development
    offer the prospect of overcoming current limitations and unlocking even greater
    efficiency and adaptability.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，LoRA 仍然是一项开创性的技术，具有广阔的潜力来普及大型语言模型的能力。持续的研究和开发有望克服当前的限制，并释放出更高的效率和适应性。
- en: Tutorial for Fine-Tuning LLM Using LoRA
  id: totrans-959
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 LoRA 微调大型语言模型（LLM）教程
- en: An open-source template for fine-tuning LLMs using the LoRA method with the
    Hugging Face library can be found [here](https://gitlab.com/CeADARIreland_Public/llm-resources).
    This template is designed specifically for adapting LLMs for instruction fine-tuning
    processes.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 [这里](https://gitlab.com/CeADARIreland_Public/llm-resources) 找到一个用于使用 Hugging
    Face 库通过 LoRA 方法微调大型语言模型的开源模板。此模板专为调整 LLM 以进行指令微调过程而设计。
- en: 6.3.3 QLoRA
  id: totrans-961
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 QLoRA
- en: 'QLoRA[[64](#bib.bib64)] is an extended version of LoRA designed for greater
    memory efficiency in large language models (LLMs) by quantising weight parameters
    to 4-bit precision. Typically, LLM parameters are stored in a 32-bit format, but
    QLoRA compresses them to 4-bit, significantly reducing the memory footprint. This
    allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also
    quantises the weights of the LoRA adapters from 8-bit to 4-bit, further decreasing
    memory and storage requirements (see Figure[6.4](#Ch6.F4 "Figure 6.4 ‣ 6.3.3 QLoRA
    ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")). Despite the reduction in bit precision, QLoRA maintains performance
    levels comparable to traditional 16-bit fine-tuning.'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 'QLoRA[[64](#bib.bib64)] 是 LoRA 的扩展版本，旨在通过将权重参数量化为 4 位精度来提高大型语言模型 (LLMs) 的内存效率。通常，LLM
    参数以 32 位格式存储，但 QLoRA 将其压缩为 4 位，显著减少内存占用。这使得在较弱的硬件上，包括消费级 GPU 上进行微调成为可能。QLoRA 还将
    LoRA 适配器的权重量化从 8 位减少到 4 位，进一步降低内存和存储需求（见图[6.4](#Ch6.F4 "Figure 6.4 ‣ 6.3.3 QLoRA
    ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")）。尽管减少了位精度，QLoRA 仍保持与传统 16 位微调相当的性能水平。'
- en: It achieves this by backpropagating gradients through a frozen, 4-bit quantised
    pre-trained language model into Low-Rank Adapters, making the fine-tuning process
    efficient while preserving model effectiveness. The QLoRA configuration is supported
    by HuggingFace via the PEFT library, utilising LoraConfig and BitsAndBytesConfig
    for quantisation. Innovations such as an optimal 4-bit data type, double quantisation
    of constants, and memory spike management enable QLoRA to reduce memory usage
    from 96 bits per parameter in traditional fine-tuning to 5.2 bits per parameter,
    an 18-fold reduction.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过将梯度通过冻结的 4 位量化预训练语言模型反向传播到低秩适配器中来实现这一点，使得微调过程高效，同时保持模型效果。QLoRA 配置由 HuggingFace
    通过 PEFT 库支持，利用 LoraConfig 和 BitsAndBytesConfig 进行量化。诸如最佳 4 位数据类型、常量的双重量化和内存峰值管理等创新，使得
    QLoRA 能够将传统微调中的每个参数的内存使用量从 96 位减少到 5.2 位，减少了 18 倍。
- en: '![Refer to caption](img/9359865e6fad00982fa64032a7095eaf.png)'
  id: totrans-964
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9359865e6fad00982fa64032a7095eaf.png)'
- en: 'Figure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This
    figure illustrates the QLoRA optimisation process, showing how the optimisation
    states, adapters, and the model interact during fine-tuning. It demonstrates the
    use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise the memory
    and computational efficiency during the fine-tuning of large language models (adapted
    from [[65](#bib.bib65)]).'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.4: 量化低秩适配 (QLoRA) 优化工作流程。该图展示了 QLoRA 优化过程，显示了在微调过程中优化状态、适配器和模型如何互动。它展示了使用不同位宽（32
    位、16 位和 4 位）来优化大型语言模型微调的内存和计算效率（改编自 [[65](#bib.bib65)]）。'
- en: Performance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit
    quantised models on benchmarks. Additionally, QLoRA enabled the fine-tuning of
    a high-quality 4-bit chatbot using a single GPU in 24 hours, achieving quality
    comparable to ChatGPT.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，QLoRA 超越了原始的 4 位量化，并在基准测试中匹配了 16 位量化模型。此外，QLoRA 使得使用单个 GPU 在 24 小时内微调出高质量的
    4 位聊天机器人，达到与 ChatGPT 相当的质量。
- en: This [tutorial](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
    explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the
    Phi-2 model.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 本 [教程](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
    讲解了在 Phi-2 模型上微调 QLoRA 的端到端步骤。
- en: 6.3.4 Weight-Decomposed Low-Rank Adaptation (DoRA)
  id: totrans-968
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4 权重分解低秩适配 (DoRA)
- en: In the context of optimising model fine-tuning, the pattern analysis of LoRA
    and Full Fine-Tuning (FT) reveals significant differences in learning behaviours
    and updates. LoRA, employing a strategy of incrementally updating pre-trained
    weights using the product of two low-rank matrices, maintains the original weights
    largely static during the fine-tuning process, which allows for efficient inference.
    Despite its computational efficiency, previous studies have suggested that LoRA’s
    limited number of trainable parameters might contribute to its performance discrepancies
    when compared to FT.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化模型微调的背景下，LoRA 和全微调（FT）的模式分析揭示了学习行为和更新的显著差异。LoRA 采用逐步更新预训练权重的低秩矩阵乘积策略，使得在微调过程中原始权重基本保持不变，从而实现高效的推理。尽管其计算效率高，但以往研究表明
    LoRA 的有限可训练参数可能导致其性能与 FT 相比存在差异。
- en: Weight-Decomposed Low-Rank Adaptation (DoRA) [[66](#bib.bib66)] is a novel fine-tuning
    methodology designed to optimise pre-trained models by decomposing their weights
    into magnitude and directional components. This approach leverages the efficiency
    of Low-Rank Adaptation (LoRA) for directional updates, facilitating substantial
    parameter updates without altering the entire model architecture. DoRA addresses
    the computational challenges associated with traditional full fine-tuning (FT)
    by maintaining model simplicity and inference efficiency, while simultaneously
    bridging the performance gap typically observed between LoRA and FT. Empirical
    and theoretical evaluations demonstrate that DoRA not only achieves learning outcomes
    comparable to FT across diverse tasks—including natural language processing and
    vision-language applications—but also consistently surpasses LoRA in performance,
    providing a robust solution for enhancing the adaptability and efficiency of large-scale
    models.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分解低秩适应（DoRA）[[66](#bib.bib66)]是一种新颖的微调方法，通过将预训练模型的权重分解为幅度和方向分量来优化模型。这种方法利用低秩适应（LoRA）在方向更新中的高效性，实现显著的参数更新，而无需改变整个模型架构。DoRA
    通过保持模型的简单性和推理效率，同时弥合 LoRA 和全微调（FT）之间通常存在的性能差距，解决了传统全微调（FT）相关的计算挑战。实证和理论评估表明，DoRA
    不仅在自然语言处理和视觉语言应用等多种任务中达到与 FT 相当的学习效果，而且在性能上始终超越 LoRA，提供了一种增强大规模模型适应性和效率的稳健解决方案。
- en: '![Refer to caption](img/772bfe082dddb41106b6ac962dcba452.png)'
  id: totrans-971
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/772bfe082dddb41106b6ac962dcba452.png)'
- en: 'Figure 6.5: An overview of DoRA (Decomposed Representations for Adaptation),
    which is a method for weight decomposed low-rank adaptation. The figure illustrates
    how pre-trained weights are decomposed and adapted for fine-tuning. In the left
    section, pre-trained weights are decomposed into a magnitude and direction. The
    right section shows how these decomposed weights are merged with trainable parameters
    during fine-tuning, resulting in updated weights that combine both frozen (blue)
    and trainable (green) components. The process emphasises efficient adaptation
    by focusing on the most significant directions in the parameter space, facilitating
    effective fine-tuning while maintaining the integrity of the original model (adapted
    from [[66](#bib.bib66)]).'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：DoRA（分解适应表示）的概述，这是一种权重分解低秩适应的方法。图示了如何将预训练权重分解并适应微调。在左侧部分，预训练权重被分解为幅度和方向。右侧部分显示了这些分解的权重如何与可训练参数合并进行微调，从而生成结合了冻结（蓝色）和可训练（绿色）组件的更新权重。该过程通过关注参数空间中最重要的方向，强调高效适应，同时保持原始模型的完整性（改编自
    [[66](#bib.bib66)]）。
- en: Python Library - DoRA is facilitated via the HuggingFace LoraConfig package.
    To incorporate DoRA into the fine-tuning process, it is essential to specify the
    ’use_dora = True’ parameter during the Lora configuration. Further information
    on initialisation can be found [here](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/lora).
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: Python 库 - DoRA 通过 HuggingFace 的 LoraConfig 包来实现。要将 DoRA 纳入微调过程，必须在 Lora 配置中指定
    'use_dora = True' 参数。有关初始化的更多信息，请参见 [这里](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/lora)。
- en: Benefits of DoRA
  id: totrans-974
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DoRA 的好处
- en: '1.'
  id: totrans-975
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling
    full fine-tuning (FT) by decomposing pre-trained weights into magnitude and directional
    components, allowing for more nuanced updates.'
  id: totrans-976
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强的学习能力：DoRA 通过将预训练权重分解为幅度和方向分量，获得了与全微调（FT）相似的学习能力，允许更细致的更新。
- en: '2.'
  id: totrans-977
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation
    (LoRA) for directional updates, DoRA enables efficient fine-tuning without altering
    the entire model architecture.'
  id: totrans-978
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效微调：通过利用低秩适应（LoRA）的结构优势进行方向性更新，DoRA实现了高效的微调，而无需改变整个模型架构。
- en: '3.'
  id: totrans-979
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'No Additional Inference Latency: Despite its improved learning capabilities,
    DoRA does not introduce any additional inference latency over LoRA, maintaining
    model simplicity and efficiency.'
  id: totrans-980
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无额外推理延迟：尽管具备改进的学习能力，DoRA并未引入额外的推理延迟，保持了模型的简单性和效率。
- en: '4.'
  id: totrans-981
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Superior Performance: Experimental results demonstrate that DoRA consistently
    outperforms LoRA across a wide range of tasks, including natural language processing
    (NLP), visual instruction tuning, and image/video-text understanding. For example,
    it shows significant improvements in commonsense reasoning and visual instruction
    tuning benchmarks.'
  id: totrans-982
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卓越表现：实验结果表明，DoRA在包括自然语言处理（NLP）、视觉指令调优和图像/视频文本理解等广泛任务中始终优于LoRA。例如，它在常识推理和视觉指令调优基准测试中显示了显著的改进。
- en: '5.'
  id: totrans-983
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Versatility Across Backbones: DoRA has been validated across various model
    backbones, including large language models (LLM) and vision-language models (LVLM),
    indicating its broad applicability and robustness in different domains.'
  id: totrans-984
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨骨干的多样性：DoRA已在包括大型语言模型（LLM）和视觉语言模型（LVLM）在内的各种模型骨干上得到验证，表明其在不同领域中的广泛适用性和鲁棒性。
- en: '6.'
  id: totrans-985
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Innovative Analysis: The introduction of a novel weight decomposition analysis
    helps uncover fundamental differences in the learning patterns of FT and various
    parameter-efficient fine-tuning (PEFT) methods, contributing to a deeper understanding
    of model fine-tuning dynamics.'
  id: totrans-986
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创新分析：引入了一种新颖的权重分解分析，有助于揭示FT和各种参数高效微调（PEFT）方法在学习模式上的根本差异，从而加深对模型微调动态的理解。
- en: Comparison between LoRA and DoRA
  id: totrans-987
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LoRA与DoRA的比较
- en: 'Low-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA)
    are both advanced techniques designed to improve the efficiency and effectiveness
    of fine-tuning large pre-trained models. While they share the common goal of reducing
    computational overhead, they employ different strategies to achieve this (see
    Table[6.2](#Ch6.T2 "Table 6.2 ‣ Comparison between LoRA and DoRA ‣ 6.3.4 Weight-Decomposed
    Low-Rank Adaptation (DoRA) ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")).'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）和权重分解低秩适应（DoRA）都是旨在提高大规模预训练模型微调效率和效果的先进技术。虽然它们有减少计算开销的共同目标，但采用了不同的策略来实现这一目标（参见表[6.2](#Ch6.T2
    "表 6.2 ‣ LoRA与DoRA的比较 ‣ 6.3.4 权重分解低秩适应（DoRA） ‣ 6.3 参数高效微调（PEFT）技术 ‣ 第6章第4阶段：微调技术选择及适当的模型配置
    ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")）。
- en: '| Criteria | LoRA (Low-Rank Adaptation) | DoRA (Weight-Decomposed Low-Rank
    Adaptation) |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | LoRA（低秩适应） | DoRA（权重分解低秩适应） |'
- en: '| --- | --- | --- |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Objective | Provide an efficient method for fine-tuning pre-trained models
    by using low-rank matrix products to update weights incrementally without increasing
    inference latency. | Improves learning capacity by closely mimicking the learning
    patterns of full fine-tuning, optimising magnitude and direction separately. |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 提供一种高效的预训练模型微调方法，通过使用低秩矩阵乘积逐步更新权重，而不会增加推理延迟。 | 通过密切模拟完全微调的学习模式，优化幅度和方向，提升学习能力。
    |'
- en: '| Approach | Implements a low-rank decomposition where the weight update is
    modelled as the product of two low-rank matrices (B and A), keeping the original
    weights static. | Uses weight decomposition analysis to reparameterise the weight
    matrix into separate magnitude and direction components for distinct updates.
    |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 实现了一种低秩分解，其中权重更新被建模为两个低秩矩阵（B和A）的乘积，保持原始权重不变。 | 使用权重分解分析将权重矩阵重新参数化为独立的幅度和方向组件，以便进行不同的更新。
    |'
- en: '| Model Architecture | Keeps the pre-trained weight matrix (W0) unchanged and
    applies updates using low-rank matrices (B and A). Matrix A is initialised with
    a uniform Kaiming distribution, while B is set to zero initially. | Restructures
    the weight matrix into magnitude and directional components, ensuring directional
    vectors are unit vectors for more detailed adjustments. |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 保持预训练权重矩阵（W0）不变，并使用低秩矩阵（B和A）进行更新。矩阵A初始化为均匀Kaiming分布，而B初始设置为零。 | 将权重矩阵重构为幅度和方向分量，确保方向向量为单位向量，以便进行更详细的调整。
    |'
- en: 'Table 6.2: A detailed comparison between LoRA (Low-Rank Adaptation) and DoRA
    (Weight-Decomposed Low-Rank Adaptation), highlighting their objectives, approaches,
    and the specific architectural strategies they employ for fine-tuning large language
    models.'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2：LoRA（低秩自适应）和DoRA（权重分解低秩自适应）之间的详细比较，突出了它们的目标、方法以及它们在微调大型语言模型时采用的具体架构策略。
- en: Tutorial for Fine-Tuning LLM using DoRA
  id: totrans-995
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用DoRA微调LLM的教程
- en: This [tutorial](https://www.kaggle.com/code/aisuko/dora-from-scratch) offers
    an in-depth guide and detailed explanation of the steps involved in implementing
    DoRA from scratch, as well as insights into the fine-tuning process essential
    for optimising performance.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[tutorial](https://www.kaggle.com/code/aisuko/dora-from-scratch)提供了一个深入的指南和实现DoRA从头开始的详细说明，以及优化性能所需的微调过程的见解。
- en: 6.3.5 Fine-Tuning with Multiple Adapters
  id: totrans-997
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5 多适配器微调
- en: 'During fine-tuning, we have explored the method of freezing the parameters
    of the LLM and focusing solely on fine-tuning a few million trainable parameters
    using LoRA. For example, fine-tuning an LLM for translation involves training
    a translation adapter with relevant data. This approach allows us to fine-tune
    separate adapters for each specific task we want the LLM to perform. However,
    a key question arises: can we consolidate multiple adapters into a unified multi-task
    adapter? For instance, if we have separate adapters for translation and summarisation
    tasks, can we merge them so that the LLM can proficiently handle both tasks? (Illustrated
    via Figure[6.6](#Ch6.F6 "Figure 6.6 ‣ 6.3.5 Fine-Tuning with Multiple Adapters
    ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")).'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: '在微调过程中，我们探索了冻结LLM参数的方法，并专注于使用LoRA微调少量的可训练参数。例如，微调用于翻译的LLM涉及用相关数据训练翻译适配器。这种方法允许我们为每个特定任务微调单独的适配器。然而，关键问题出现了：我们能否将多个适配器整合成一个统一的多任务适配器？例如，如果我们有用于翻译和摘要任务的单独适配器，我们能否将它们合并，使得LLM能够熟练地处理这两个任务？（通过图示[6.6](#Ch6.F6
    "Figure 6.6 ‣ 6.3.5 Fine-Tuning with Multiple Adapters ‣ 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")展示）。'
- en: 'The PEFT library simplifies the process of merging adapters with its add_weighted_adapter
    function ³³3[https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter),
    which offers three distinct methods:'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT库通过其add_weighted_adapter函数简化了合并适配器的过程³³3[https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter)，该函数提供了三种不同的方法：
- en: '1.'
  id: totrans-1000
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Concatenation: This straightforward method concatenates the parameters of the
    adapters. For instance, if two adapters each have a rank of 16, the resulting
    adapter will have a rank of 32\. This method is highly efficient.'
  id: totrans-1001
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连接：这种简单方法将适配器的参数进行连接。例如，如果两个适配器的秩均为16，则结果适配器的秩为32。这种方法效率极高。
- en: '2.'
  id: totrans-1002
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Linear Combination: Although less documented, this method appears to perform
    a weighted sum of the adapters’ parameters.'
  id: totrans-1003
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性组合：尽管文献中对此方法的记录较少，但它似乎执行了适配器参数的加权和。
- en: '3.'
  id: totrans-1004
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'SVD: The default method employs singular value decomposition through torch.linalg.svd.
    While versatile, it is notably slower than the other methods, particularly for
    adapters with high ranks (greater than 100), which can take several hours.'
  id: totrans-1005
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SVD：默认方法使用torch.linalg.svd进行奇异值分解。虽然方法多样，但相较于其他方法，其速度明显较慢，尤其是在处理高排名（大于100）的适配器时，可能需要几个小时。
- en: Each method allows for customising the combination by adjusting weights. For
    instance, when merging two adapters, X and Y, assigning more weight to X ensures
    that the resulting adapter prioritises behaviour similar to X over Y.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都允许通过调整权重来自定义组合。例如，当合并两个适配器 X 和 Y 时，为 X 分配更多的权重可以确保生成的适配器优先表现出与 X 相似的行为而非
    Y。
- en: This approach is particularly suited for consolidating a single LLM to handle
    multiple tasks rather than creating separate models for each task domain. By adopting
    this method, there is no longer a need to individually fine-tune a model for each
    task. Instead, a single adapter layer can be fine-tuned for each task, allowing
    queries to yield the desired responses efficiently.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法特别适合将单一 LLM 整合以处理多个任务，而不是为每个任务领域创建单独的模型。通过采用这种方法，不再需要为每个任务单独微调模型。相反，可以为每个任务微调一个适配器层，从而高效地获得所需的响应。
- en: '![Refer to caption](img/e419499ac781af2f2bdfafd996ac6163.png)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e419499ac781af2f2bdfafd996ac6163.png)'
- en: 'Figure 6.6: Overview of how multiple adapters can be used with a pre-trained
    LLM to fine-tune it for various specific tasks, such as summarisation, proofreading,
    sentiment analysis, and more. (adapted from [[67](#bib.bib67)])'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：概述如何使用多个适配器对预训练的 LLM 进行微调，以处理各种特定任务，如总结、校对、情感分析等。（改编自[[67](#bib.bib67)]）
- en: Steps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters
  id: totrans-1010
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 LoRA 微调 LLM 以处理多个任务和适配器的步骤
- en: '1.'
  id: totrans-1011
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks
    using different prompt formats or task-identifying tags (e.g., [translate_fren],
    [chat]).'
  id: totrans-1012
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适配器创建：创建多个适配器，每个适配器都经过微调，以适应特定任务，使用不同的提示格式或任务标识标签（例如，[translate_fren]，[chat]）。
- en: '2.'
  id: totrans-1013
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'LoRA Integration: Implement LoRA to efficiently integrate these adapters into
    the pre-trained LLM. Utilise LoRA’s methods such as concatenation, linear combination,
    or singular value decomposition (SVD) to combine adapters while minimising computational
    overhead and maintaining performance.'
  id: totrans-1014
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LoRA 集成：实现 LoRA，以高效地将这些适配器集成到预训练的 LLM 中。利用 LoRA 的方法，如连接、线性组合或奇异值分解（SVD），在最小化计算开销和保持性能的同时，组合适配器。
- en: '3.'
  id: totrans-1015
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Task-Specific Adaptation: Fine-tune each adapter with task-specific data to
    enhance performance for individual tasks. Ensure adapters are trained with data
    relevant to their respective tasks, optimising their ability to generate accurate
    responses.'
  id: totrans-1016
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定的适应：使用任务特定的数据对每个适配器进行微调，以提高其在各个任务中的性能。确保适配器使用与其各自任务相关的数据进行训练，从而优化其生成准确回应的能力。
- en: '4.'
  id: totrans-1017
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Behaviour Adjustment: Monitor the behaviour of combined adapters to identify
    any undesired inherited behaviours from individual adapters (e.g., short response
    generation from a translation adapter). Adjust the combination weights or types
    to modify adapter behaviour as needed, ensuring each adapter performs optimally
    for its intended task.'
  id: totrans-1018
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行为调整：监控组合适配器的行为，以识别任何来自个别适配器的非预期继承行为（例如，翻译适配器生成的短响应）。根据需要调整组合权重或类型，以修改适配器行为，确保每个适配器在其预期任务中表现最佳。
- en: '5.'
  id: totrans-1019
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Evaluation and Iteration: Evaluate the performance of the combined model across
    multiple tasks using validation datasets. Iterate on the fine-tuning process,
    making adjustments to adapter combinations and training parameters based on performance
    metrics and user feedback.'
  id: totrans-1020
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估和迭代：使用验证数据集评估组合模型在多个任务中的性能。根据性能指标和用户反馈，迭代微调过程，调整适配器组合和训练参数。
- en: Therefore, for optimal performance, it is advisable to combine adapters that
    have been fine-tuned with distinctly varied prompt formats. However, even when
    using adapters with different prompt formats, the resulting adapter may not exhibit
    desired behaviour. For example, a newly combined adapter designed for chatting
    may only generate short responses, inheriting this tendency from an adapter that
    was originally trained to halt after producing a single sentence. To adjust the
    behaviour of the combined adapter, one can prioritise the influence of a specific
    adapter during the combination process and/or modify the method of combination
    used.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了获得最佳性能，建议组合经过明显不同提示格式微调的适配器。然而，即使使用不同提示格式的适配器，生成的适配器也可能不表现出期望的行为。例如，专为聊天设计的新组合适配器可能仅生成短响应，继承了原本被训练成在生成单句后停止的适配器的倾向。要调整组合适配器的行为，可以在组合过程中优先考虑特定适配器的影响和/或修改使用的组合方法。
- en: An illustrative tutorial demonstrating the fine-tuning of large language models
    (LLMs) using multiple adapter layers for various tasks can be found [here](https://kaitchup.substack.com/p/combine-multiple-lora-adapters-for).
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 演示使用多个适配器层进行各种任务的语言模型（LLMs）微调的说明性教程可以在[这里](https://kaitchup.substack.com/p/combine-multiple-lora-adapters-for)找到。
- en: 6.4 Half Fine Tuning
  id: totrans-1023
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 半微调
- en: Half Fine-Tuning (HFT)[[68](#bib.bib68)] is a technique designed to balance
    the retention of foundational knowledge with the acquisition of new skills in
    large language models (LLMs). HFT involves freezing half of the model’s parameters
    during each fine-tuning round while updating the other half, allowing the model
    to retain pre-trained knowledge and enhance new task performance without altering
    the model architecture.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 半微调（HFT）[[68](#bib.bib68)]是一种旨在平衡大型语言模型（LLMs）中基础知识保留与新技能获取的技术。HFT涉及在每轮微调期间冻结模型的一半参数，同时更新另一半，从而允许模型保留预训练知识并提高新任务性能，而不改变模型架构。
- en: 'Each repetitive transformer layer is divided into three blocks: self-attention,
    feed-forward, and layernorm, with half of the parameters in each block updated
    and the other half frozen, varying with each round. This strategic parameter update
    helps maintain knowledge parity across training rounds and enhances scalability
    in successive training sessions.'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 每个重复的变换器层被划分为三个块：自注意力、前馈和层归一化，其中每个块的一半参数被更新，另一半保持冻结，每轮都会有所变化。这种战略参数更新有助于保持训练轮次之间的知识一致性，并增强在连续训练会话中的可扩展性。
- en: Research on models like LLAMA 2-7B demonstrated that HFT could significantly
    restore forgotten basic knowledge while preserving high general ability performance.
    This method’s robustness and efficiency make it applicable to various fine-tuning
    scenarios, including supervised fine-tuning, direct preference optimisation, and
    continual learning. Additionally, HFT’s ability to maintain the model architecture
    simplifies its implementation and ensures compatibility with existing systems,
    further promoting its practical adoption.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 对像LLAMA 2-7B这样的模型的研究表明，HFT可以显著恢复遗忘的基本知识，同时保持高水平的整体能力表现。这种方法的鲁棒性和效率使其适用于各种微调场景，包括监督微调、直接偏好优化和持续学习。此外，HFT保持模型架构的能力简化了其实施，并确保与现有系统的兼容性，进一步促进了其实际应用。
- en: '![Refer to caption](img/19a3f0078a3978fcd8766bef545cae81.png)'
  id: totrans-1027
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/19a3f0078a3978fcd8766bef545cae81.png)'
- en: 'Figure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as
    applied to LLAMA 2’s architecture. The diagram shows multiple stages of fine-tuning,
    where specific model parameters are selectively activated (orange) while others
    remain frozen (blue). This approach optimises training by reducing computational
    requirements while still effectively adapting the model to new tasks or data.
    (adapted from [[68](#bib.bib68)])'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：半微调（HFT）方法应用于LLAMA 2架构的示意图。该图显示了多个微调阶段，其中特定模型参数被选择性激活（橙色），而其他参数保持冻结（蓝色）。这种方法通过减少计算需求来优化训练，同时有效地将模型适应于新任务或数据。（改编自[[68](#bib.bib68)]）
- en: 6.4.1 Benefits of using Half Fine tuning
  id: totrans-1029
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 使用半微调的好处
- en: '1.'
  id: totrans-1030
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters
    to their pre-trained state, HFT effectively recovers a portion of the original
    knowledge, thereby mitigating catastrophic forgetting of previously acquired capabilities.'
  id: totrans-1031
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练知识的恢复：通过将一半微调参数回滚到其预训练状态，HFT有效地恢复了部分原始知识，从而减轻了对先前获得能力的灾难性遗忘。
- en: '2.'
  id: totrans-1032
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Enhanced Performance: Research experiments shows that HFT maintains or even
    surpasses the performance of full fine-tuning (FFT) on downstream tasks, demonstrating
    its effectiveness in balancing knowledge retention with task-specific learning.'
  id: totrans-1033
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能提升：研究实验表明，HFT在下游任务中的表现保持不变甚至超越了全面微调（FFT），展示了其在平衡知识保留与任务特定学习方面的有效性。
- en: '3.'
  id: totrans-1034
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Robustness: The method is robust to different selection strategies and the
    number of parameters chosen for updating, ensuring consistent performance across
    various configurations.'
  id: totrans-1035
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性：该方法对不同的选择策略和更新参数的数量具有鲁棒性，确保在各种配置下性能的一致性。
- en: '4.'
  id: totrans-1036
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Simplicity and Scalability: HFT does not alter the model architecture, which
    simplifies implementation and allows for scalable applications, particularly beneficial
    in successive fine-tuning scenarios.'
  id: totrans-1037
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单性和可扩展性：HFT不会改变模型架构，这简化了实现过程，并允许可扩展的应用，特别是在连续微调场景中尤为有利。
- en: '5.'
  id: totrans-1038
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Versatility: The technique has proven effective across diverse fine-tuning
    scenarios, including supervised fine-tuning, direct preference optimisation, and
    continual learning.'
  id: totrans-1039
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多功能性：该技术在各种精调场景中已证明有效，包括监督精调、直接偏好优化和持续学习。
- en: 6.4.2 Comparison between HFT and LoRA
  id: totrans-1040
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 HFT 与 LoRA 的比较
- en: '| Criteria | HFT | LoRA |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | HFT | LoRA |'
- en: '| --- | --- | --- |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Objective | The goal is to retain the foundational knowledge acquired during
    pre-training while learning new task-specific skills, thus balancing between maintaining
    existing capabilities and acquiring new ones. | LoRA aims to reduce computational
    and memory requirements during fine-tuning, making it more efficient and feasible
    to train large models on limited hardware resources. |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 目标是保留预训练过程中获得的基础知识，同时学习新的任务特定技能，从而在保持现有能力和获得新技能之间取得平衡。 | LoRA 旨在减少精调过程中对计算和内存的需求，使其在有限硬件资源上训练大型模型变得更高效、更可行。
    |'
- en: '| Approach | HFT involves freezing half of the model’s parameters during each
    fine-tuning round and updating only the other half. | LoRA reduces the number
    of trainable parameters by introducing low-rank decomposition into the weight
    matrices of the neural network. This involves injecting low-rank matrices into
    the model’s layers during fine-tuning. |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | HFT 涉及在每次精调轮次中冻结模型的一半参数，只更新另一半参数。 | LoRA 通过将低秩分解引入神经网络的权重矩阵来减少可训练参数的数量。这在精调过程中将低秩矩阵注入模型的层中。
    |'
- en: '| Model Architecture | HFT does not alter the model’s architecture or introduce
    new parameters, making it straightforward to apply without additional structural
    changes. | LoRA modifies the model by adding low-rank matrices, which changes
    the training dynamics and requires additional computations for the low-rank updates.
    |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | HFT 不改变模型的架构或引入新的参数，使其应用简单，无需额外的结构变更。 | LoRA 通过添加低秩矩阵来修改模型，这改变了训练动态，并需要额外的计算来更新低秩矩阵。
    |'
- en: '| Performance | Research has shown that HFT can restore forgotten basic knowledge
    while maintaining high performance in general abilities. | LoRA is designed to
    achieve competitive performance with full fine-tuning but with significantly fewer
    trainable parameters and lower computational costs. |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 研究表明，HFT 可以恢复遗忘的基本知识，同时在一般能力方面保持高性能。 | LoRA 旨在实现与完全精调相竞争的性能，但可训练参数显著减少，计算成本也较低。
    |'
- en: 'Table 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation
    (LoRA).'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.3：半精调（HFT）与低秩适应（LoRA）的比较分析。
- en: 6.5 Lamini Memory Tuning
  id: totrans-1048
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 Lamini 记忆精调
- en: Lamini [[69](#bib.bib69)] was introduced as a specialised approach to fine-tuning
    Large Language Models (LLMs), targeting the reduction of hallucinations. This
    development was motivated by the need to enhance the reliability and precision
    of LLMs in domains requiring accurate information retrieval. Traditional training
    methods typically consist of running stochastic gradient descent on vast datasets,
    which, despite fitting the training data well, often produce models that fail
    to generalise effectively and are prone to such errors.
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: Lamini [[69](#bib.bib69)] 被引入作为一种专门的精调方法，旨在减少幻觉现象。此发展是由于需要提高 LLM 在需要准确检索信息的领域中的可靠性和精度。传统的训练方法通常包括在大量数据集上运行随机梯度下降，尽管这些方法对训练数据的拟合较好，但往往会生成无法有效泛化且易于出现错误的模型。
- en: Foundation models often follow a training regimen similar to the Chinchilla
    recipe, which prescribes training for a single epoch on a massive corpus, such
    as training Llama 2 7B on about one trillion tokens. This approach results in
    substantial loss and is geared more towards enhancing generalisation and creativity
    where a degree of randomness in token selection is permissible. However, it falls
    short for tasks demanding high factual precision. In contrast, Lamini Memory Tuning
    delves deeper by analysing the loss of individual facts, significantly improving
    the accuracy of factual recall. By augmenting a model with additional parameters
    specifically for memory (e.g., an 8B parameter model with an extra 2B parameters
    for weights), Lamini enables the model to memorise and accurately recall a significant
    number of facts, closely aligning performance with LLM scaling laws without compromising
    on generalisation.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型通常遵循类似于Chinchilla配方的训练方案，该配方建议在大规模语料库上进行单次训练，例如在大约一万亿个标记上训练Llama 2 7B。这种方法导致显著的损失，更侧重于增强泛化和创造力，在标记选择中允许一定的随机性。然而，对于要求高度事实精确的任务，这种方法并不理想。相比之下，Lamini内存调优通过分析单个事实的损失，深入挖掘，显著提高了事实回忆的准确性。通过为内存特别增添额外参数（例如，一个8B参数模型增加2B参数用于权重），Lamini使模型能够记住并准确回忆大量事实，使性能与LLM规模法则紧密对齐，同时不妨碍泛化能力。
- en: 6.5.1 Lamini-1 - A model architecture based on Lamini
  id: totrans-1051
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1 Lamini-1 - 基于Lamini的模型架构
- en: 'Departing from traditional transformer-based designs, the Lamini-1 model architecture
    (Figure [6.8](#Ch6.F8 "Figure 6.8 ‣ 6.5.1 Lamini-1 - A model architecture based
    on Lamini ‣ 6.5 Lamini Memory Tuning ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)"))
    employs a massive mixture of memory experts (MoME). This system features a pre-trained
    transformer backbone augmented by adapters that are dynamically selected from
    an index using cross-attention mechanisms. These adapters function similarly to
    experts in MoE architectures, and the network is trained end-to-end while freezing
    the backbone. This setup allows for specific facts to be stored exactly in the
    selected experts.'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于传统的变换器设计，Lamini-1模型架构（图[6.8](#Ch6.F8 "图6.8 ‣ 6.5.1 Lamini-1 - 基于Lamini的模型架构
    ‣ 6.5 Lamini内存调优 ‣ 第6章 第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")）采用了大规模记忆专家混合（MoME）。该系统具有一个经过预训练的变换器骨干，增加了通过交叉注意机制从索引中动态选择的适配器。这些适配器类似于MoE架构中的专家，网络在冻结骨干的同时进行端到端训练。这种设置允许特定的事实被准确存储在选定的专家中。
- en: '![Refer to caption](img/ad279d3f8f14186d1f1e324a6046cb0b.png)'
  id: totrans-1053
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad279d3f8f14186d1f1e324a6046cb0b.png)'
- en: 'Figure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive
    Array of Memory Experts (MoME). This architecture integrates a pre-trained transformer
    backbone with dynamically selected adapters via cross-attention mechanisms. Each
    adapter, functioning as a memory expert, is capable of storing specific factual
    data. (adopted from [[69](#bib.bib69)])'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：Lamini-1模型架构示意图，展示了大规模记忆专家阵列（MoME）。该架构将预训练的变换器骨干与通过交叉注意机制动态选择的适配器集成在一起。每个适配器作为记忆专家，能够存储特定的事实数据。（改编自[[69](#bib.bib69)]）
- en: At inference time, only the relevant experts are retrieved from the index, enabling
    the LLM to store a large number of facts while maintaining low inference latency.
    Specialised GPU kernels written in Triton are used to accelerate the lookup of
    experts, optimising the system for quick access to stored knowledge.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时，仅从索引中检索相关的专家，从而使LLM能够存储大量的事实，同时保持低推理延迟。使用Triton编写的专业GPU内核加速专家的查找，优化系统以快速访问存储的知识。
- en: Systems Optimisations for Banishing Hallucinations
  id: totrans-1056
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 系统优化以消除幻觉
- en: The MoME architecture is designed to minimise the computational demand required
    to memorise facts. During training, a subset of experts, such as 32 out of a million,
    is selected for each fact. The weights of the backbone network and the cross attention
    used to select the expert are frozen, and gradient descent steps are taken until
    the loss is sufficiently reduced to memorise the fact. This approach prevents
    the same expert from being selected multiple times for different facts by first
    training the cross attention selection mechanism during a generalisation training
    phase, then freezing its weights.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: MoME 架构旨在最小化记忆事实所需的计算需求。在训练期间，为每个事实选择一个专家子集，例如从一百万个中选择 32 个。骨干网络的权重和用于选择专家的交叉注意力被冻结，并进行梯度下降步骤，直到损失被足够减少以记忆事实。这种方法通过首先在泛化训练阶段训练交叉注意力选择机制，然后冻结其权重，来防止相同的专家被多次选择用于不同的事实。
- en: This method ensures that computation scales with the number of training examples,
    not the total number of parameters, thereby significantly reducing the computation
    required for memory tuning. This optimised approach allows Lamini-1 to achieve
    near-zero loss in memory tuning on real and random answers efficiently, demonstrating
    its efficacy in eliminating hallucinations while improving factual recall.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法确保了计算的规模与训练示例的数量相关，而非参数的总数量，从而显著减少了内存调优所需的计算量。这种优化方法使 Lamini-1 能够在真实和随机答案上实现接近零的内存调优损失，展示了其在消除幻觉同时提高事实回忆方面的有效性。
- en: 6.6 Mixture of Experts
  id: totrans-1059
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 专家混合
- en: A mixture of experts (MoE) is an architectural design for neural networks that
    divides the computation of a layer or operation (e.g., linear layers, MLPs, or
    attention projection) into several specialised subnetworks, referred to as ”experts”.
    Each expert independently carries out its computation, and the results are aggregated
    to produce the final output of the MoE layer. MoE architectures can be categorised
    as either dense, where every expert is engaged for each input, or sparse, where
    only a subset of experts is utilised for each input.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）是一种神经网络的架构设计，将一层或操作（例如，线性层、MLP 或注意力投影）的计算分为几个专门的子网络，称为“专家”。每个专家独立执行其计算，然后将结果汇总以生成
    MoE 层的最终输出。MoE 架构可以分为密集型，其中每个输入都涉及所有专家，或稀疏型，其中每个输入只利用专家子集。
- en: 6.6.1 Mixtral 8x7B Architecture and Performance
  id: totrans-1061
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.1 Mixtral 8x7B 架构与性能
- en: 'Mixtral [[70](#bib.bib70)] 8x7B employs a Sparse Mixture of Experts (SMoE)
    architecture (Figure [6.9](#Ch6.F9 "Figure 6.9 ‣ 6.6.1 Mixtral 8x7B Architecture
    and Performance ‣ 6.6 Mixture of Experts ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")),
    mirroring the structure of Mistral 7B but incorporating eight feedforward blocks
    (experts) in each layer. For every token at each layer, a router network selects
    two experts to process the current state and combine their outputs. Although each
    token interacts with only two experts at a time, the selected experts can vary
    at each timestep. Consequently, each token has access to 47 billion parameters
    but utilises only 13 billion active parameters during inference. Mixtral 8x7B
    not only matches but often surpasses Llama 2 70B and GPT-3.5 across all evaluated
    benchmarks. Its performance is notably superior to Llama 2 70B in mathematics,
    code generation, and multilingual tasks.'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral [[70](#bib.bib70)] 8x7B 采用了稀疏专家混合（SMoE）架构（图 [6.9](#Ch6.F9 "图 6.9 ‣ 6.6.1
    Mixtral 8x7B 架构与性能 ‣ 6.6 专家混合 ‣ 第 6 章 第 4 阶段：微调技术的选择和适当模型配置 ‣ 从基础到突破的最终微调指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽评审（版本
    1.0）")），其结构与 Mistral 7B 相似，但在每层中加入了八个前馈块（专家）。对于每个层中的每个令牌，一个路由网络会选择两个专家来处理当前状态并结合它们的输出。尽管每个令牌每次只与两个专家互动，但所选的专家可以在每个时间步中有所不同。因此，每个令牌可以访问
    470 亿个参数，但在推理过程中只使用 130 亿个活动参数。Mixtral 8x7B 不仅与 Llama 2 70B 相匹配，而且在所有评估基准上通常优于
    GPT-3.5。它在数学、代码生成和多语言任务上的表现明显优于 Llama 2 70B。
- en: '![Refer to caption](img/e8a13f98a4bac63b294f5f8e49216e42.png)'
  id: totrans-1063
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e8a13f98a4bac63b294f5f8e49216e42.png)'
- en: 'Figure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture.
    The model is composed of a router network that dynamically selects the most relevant
    experts from a pool of eight transformer-based experts, each with 7 billion parameters.
    The experts are organised into transformer blocks, where the router directs data
    to the appropriate expert based on the input, optimising computational efficiency
    and model performance. This architecture allows for scalability and specialised
    processing within large language models. (adapted from [[71](#bib.bib71)])'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：Mixtral 8x7B 专家混合（MoE）模型架构图。该模型由一个路由网络组成，能够从八个基于变压器的专家池中动态选择最相关的专家，每个专家拥有
    70 亿个参数。专家被组织成变压器块，路由器根据输入将数据指向相应的专家，从而优化计算效率和模型性能。这种架构允许在大型语言模型中进行可扩展性和专门化处理。（改编自
    [[71](#bib.bib71)]）
- en: 6.7 Mixture of Agents
  id: totrans-1065
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 专家混合
- en: Despite the numerous LLMs and their notable accomplishments, they continue to
    encounter fundamental limitations regarding model size and training data. Scaling
    these models further is prohibitively expensive, often necessitating extensive
    retraining on multiple trillion tokens. Simultaneously, different LLMs exhibit
    distinct strengths and specialise in various aspects of tasks. A recent study
    has investigated leveraging the collective expertise of multiple LLMs to develop
    a more capable and robust model, a method known as Mixture of Agents (MoA) [[72](#bib.bib72)].
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有众多的 LLM 和其显著成就，但它们仍面临着模型规模和训练数据的基本限制。进一步扩展这些模型的成本极其昂贵，通常需要对多个万亿个标记进行广泛的再训练。同时，不同的
    LLM 展现出不同的优势，并专注于任务的各个方面。最近的一项研究探讨了利用多个 LLM 的集体专业知识来开发更强大和鲁棒的模型，这种方法被称为专家混合（MoA）[[72](#bib.bib72)]。
- en: 'MoA functions using a layered architecture, where each layer comprises multiple
    LLM agents (Figure  [6.10](#Ch6.F10 "Figure 6.10 ‣ 6.7 Mixture of Agents ‣ Chapter
    6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")). This structure reveals a phenomenon known
    as the “collaborativeness of LLMs.” The innovative MoA framework utilises the
    combined capabilities of several LLMs to enhance both reasoning and language generation
    proficiency. Research indicates that LLMs naturally collaborate, demonstrating
    improved response quality when incorporating outputs from other models, even if
    those outputs are not ideal.'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: MoA 使用分层架构，每层包含多个 LLM 代理（图 [6.10](#Ch6.F10 "图 6.10 ‣ 6.7 专家混合 ‣ 第六章 第四阶段：选择微调技术和适当的模型配置
    ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第 1.0 版）")）。这种结构揭示了一种被称为“LLMs 合作性”的现象。创新的
    MoA 框架利用多个 LLM 的综合能力来提升推理和语言生成的能力。研究表明，LLMs 自然会进行合作，当结合其他模型的输出时，即使这些输出不是理想的，也能提高响应质量。
- en: '![Refer to caption](img/94eb1e435da461a959f43a32f6fd68b7.png)'
  id: totrans-1068
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94eb1e435da461a959f43a32f6fd68b7.png)'
- en: 'Figure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The
    model consists of multiple layers, each incorporating several agents that process
    the input independently before concatenating their outputs to form an intermediate
    result. The process continues across layers, refining the output at each stage
    to generate the final output based on the given prompt (adapted from [[72](#bib.bib72)]).'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：Mixture of Agents (MoA) LLM 配置示意图。该模型由多层组成，每层包含多个独立处理输入的代理，然后将它们的输出串联成一个中间结果。这个过程在各层之间继续进行，每个阶段都细化输出，以生成基于给定提示的最终输出（改编自
    [[72](#bib.bib72)]）。
- en: 6.7.1 Methodology
  id: totrans-1070
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.7.1 方法论
- en: 'To enhance collaboration among multiple LLMs, it is essential to understand
    their individual strengths and classify them accordingly. The classification includes:'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强多个 LLM 之间的合作，必须了解它们的各自优势并进行分类。分类包括：
- en: '1.'
  id: totrans-1072
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Proposers: These models excel at generating valuable reference responses for
    other models. While they may not perform exceptionally on their own, they provide
    useful context and varied perspectives that improve the final output when utilised
    by an aggregator.'
  id: totrans-1073
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提议者：这些模型擅长为其他模型生成有价值的参考响应。尽管它们可能单独表现并不突出，但当被汇总器利用时，它们提供有用的背景和多样化的视角，从而改善最终输出。
- en: '2.'
  id: totrans-1074
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Aggregators: These models are adept at merging responses from various models
    into a single high-quality result. An effective aggregator should maintain or
    even enhance the quality of the final response, regardless of the quality of the
    individual inputs.'
  id: totrans-1075
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚合器：这些模型擅长将来自不同模型的响应合并为单一的高质量结果。一个有效的聚合器应当能够保持甚至提高最终响应的质量，无论单个输入的质量如何。
- en: 'The careful selection of LLMs for each MoA layer is crucial Performance metrics,
    such as average win rates in a given layer, help assess the suitability of models
    for subsequent layers, ensuring the production of higher-quality outputs. Diversity
    in model outputs is vital, as varied responses from different models contribute
    significantly more than homogeneous outputs from a single model. In MoA, given
    an input prompt, the output of the $i^{\text{th}}$ is calculated as follows:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 MoA 层 LLM 的精心选择至关重要。性能指标，如特定层的平均胜率，有助于评估模型在后续层的适用性，确保产生更高质量的输出。模型输出的多样性很重要，因为来自不同模型的多样化响应比单一模型的同质输出贡献更大。在
    MoA 中，给定输入提示，$i^{\text{th}}$ 的输出计算如下：
- en: '|  | $y_{i}=\bigoplus_{j=1}^{n}\left[A_{i,j}(x_{i})\right]+x_{1},\,x_{i+1}=y_{i}$
    |  | (6.1) |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}=\bigoplus_{j=1}^{n}\left[A_{i,j}(x_{i})\right]+x_{1},\,x_{i+1}=y_{i}$
    |  | (6.1) |'
- en: 6.7.2 Analogy with MoE
  id: totrans-1078
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.7.2 与 MoE 的类比
- en: 'Mixture-of-Experts (MoE) is a well-established machine learning technique where
    multiple expert networks, each with specialised skills, collaborate to address
    complex problems. This approach has demonstrated significant success across various
    applications and serves as the inspiration for the Mixture-of-Agents (MoA) method.
    In a typical MoE design, a stack of layers, known as MoE layers, consists of multiple
    expert networks, a gating network, and residual connections to improve gradient
    flow. The output for layer $y_{i}$ is calculated as follows:'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）是一种成熟的机器学习技术，其中多个具有专业技能的专家网络协作解决复杂问题。这种方法在各种应用中取得了显著成功，并作为专家混合体（MoA）方法的灵感。在典型的
    MoE 设计中，一组层（称为 MoE 层）包括多个专家网络、一个门控网络和残差连接以改善梯度流。层 $y_{i}$ 的输出计算如下：
- en: '|  | $y_{i}=\sum_{j=1}^{n}G_{i,j}(x_{i})E_{i,j}(x_{i})+x_{i}$ |  | (6.2) |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}=\sum_{j=1}^{n}G_{i,j}(x_{i})E_{i,j}(x_{i})+x_{i}$ |  | (6.2) |'
- en: The MoA framework advances the MoE concept by operating at the model level through
    prompt-based interactions rather than altering internal activations or weights.
    Instead of relying on specialised sub-networks within a single model, MoA utilises
    multiple full-fledged LLMs across different layers. In this approach, the gating
    and expert networks’ functions are integrated within an LLM, leveraging its ability
    to interpret prompts and generate coherent outputs without additional coordination
    mechanisms.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: MoA 框架通过基于提示的交互在模型层面上推进了 MoE 概念，而不是改变内部激活或权重。MoA 不依赖于单一模型中的专门子网络，而是利用多个完整的 LLM
    跨不同层。在这种方法中，门控和专家网络的功能集成在 LLM 内，利用其解释提示和生成连贯输出的能力，无需额外的协调机制。
- en: 6.7.3 What makes MoA works well?
  id: totrans-1082
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.7.3 什么使 MoA 表现良好？
- en: '1.'
  id: totrans-1083
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'MoA’s Superior Performance: MoA significantly outperforms LLM-based rankers,
    which select one answer from the proposals rather than generating new responses.
    This suggests that MoA’s approach of aggregating all generated responses provides
    more effective results than simply choosing from pre-existing options.'
  id: totrans-1084
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MoA 的优越性能：MoA 显著优于基于 LLM 的排名器，这些排名器从提案中选择一个答案，而不是生成新响应。这表明 MoA 通过聚合所有生成的响应所采用的方法比简单地从现有选项中选择要有效得多。
- en: '2.'
  id: totrans-1085
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Effective Incorporation of Proposals: The aggregator in MoA demonstrates a
    tendency to integrate the best proposed answers. This is supported by positive
    correlations between aggregator responses and various similarity metrics, such
    as BLEU scores, which measure n-gram overlaps. The use of alternative similarity
    measures also shows a consistent positive correlation with preference scores,
    indicating that the aggregator effectively utilises the proposed responses.'
  id: totrans-1086
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提案的有效整合：MoA 中的聚合器显示出整合最佳提案的倾向。这得到了聚合器响应与各种相似度指标之间的正相关支持，例如测量 n-gram 重叠的 BLEU
    分数。使用替代相似度度量也显示出与偏好分数的一致正相关，表明聚合器有效利用了提议的响应。
- en: '3.'
  id: totrans-1087
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Influence of Model Diversity and Proposer Count: Increasing the number of proposers
    improves output quality, highlighting the benefits of additional auxiliary information.
    Additionally, using a diverse set of LLMs as proposers consistently yields better
    results compared to using a single LLM. This suggests that both the number and
    diversity of LLM agents in each MoA layer contribute to enhanced performance,
    with potential for further improvement through scaling.'
  id: totrans-1088
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型多样性和提议者数量的影响：增加提议者数量可以提升输出质量，突出附加辅助信息的好处。此外，使用多样化的LLM作为提议者通常能比单一LLM产生更好的结果。这表明每个MoA层中的LLM代理的数量和多样性都对性能提升有所贡献，通过扩展可能会进一步改善。
- en: '4.'
  id: totrans-1089
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Model Specialisation: Analysis of model roles within the MoA ecosystem reveals
    that GPT-4o, Qwen, and LLaMA-3 are effective in both assisting and aggregating
    tasks. In contrast, WizardLM excels as a proposer but struggles with aggregating
    responses from other models.'
  id: totrans-1090
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型专业化：对MoA生态系统中模型角色的分析表明，GPT-4o、Qwen和LLaMA-3在协助和聚合任务中表现有效。相比之下，WizardLM在作为提议者方面表现出色，但在聚合其他模型的响应时表现较差。
- en: 6.8 Proximal Policy Optimisation (PPO)
  id: totrans-1091
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 近端策略优化（PPO）
- en: PPO [[73](#bib.bib73)] is a widely recognised reinforcement learning algorithm
    used for training agents to perform tasks in diverse environments. This algorithm
    leverages policy gradient methods, where policies—represented by neural networks—determine
    the actions taken by the agent based on the current state. PPO effectively handles
    the dynamic nature of training data generated through continuous agent-environment
    interactions, a feature that differentiates it from static datasets used in supervised
    learning.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: PPO [[73](#bib.bib73)] 是一种广泛认可的强化学习算法，用于训练代理在各种环境中执行任务。该算法利用策略梯度方法，其中策略——由神经网络表示——根据当前状态决定代理采取的行动。PPO有效地处理了通过持续的代理-环境交互生成的动态训练数据，这一特性使其区别于在监督学习中使用的静态数据集。
- en: The innovation of PPO lies in its ”surrogate” objective function, optimised
    via stochastic gradient ascent. This approach allows for multiple updates from
    the same batch of data, enhancing both training efficiency and stability over
    traditional policy gradient methods. Developed by OpenAI, PPO was designed to
    balance ease of implementation with the robust performance characteristics of
    more complex algorithms like Trust Region Policy Optimisation (TRPO), but without
    the associated computational complexity.
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的创新在于其“替代”目标函数，通过随机梯度上升进行优化。这种方法允许从同一批数据中进行多次更新，提高了训练效率和稳定性，相较于传统的策略梯度方法。由OpenAI开发，PPO旨在平衡实现的简易性与类似于信赖域策略优化（TRPO）等更复杂算法的强大性能特征，但没有相关的计算复杂度。
- en: PPO operates by maximising expected cumulative rewards through iterative policy
    adjustments that increase the likelihood of actions leading to higher rewards.
    A key feature of PPO is its use of a clipping mechanism in the objective function,
    which limits the extent of policy updates, thus preventing drastic changes and
    maintaining stability during training.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: PPO通过迭代调整策略来最大化期望累积奖励，从而提高导致更高奖励的行动的可能性。PPO的一个关键特性是其在目标函数中使用剪切机制，这限制了策略更新的幅度，从而防止了剧烈的变化，并在训练过程中保持稳定性。
- en: '![Refer to caption](img/59c1bc79768ee9fc99eb35f6d4026ab9.png)'
  id: totrans-1095
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/59c1bc79768ee9fc99eb35f6d4026ab9.png)'
- en: 'Figure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the
    context of Reinforcement Learning from Human Feedback (RLHF) for fine-tuning a
    Large Language Model (LLM). The process involves using a prompt dataset to train
    the LLM. The PPO algorithm adjusts the LLM’s policy based on rewards provided
    by the reward model, which is fine-tuned through human feedback. (adapted from
    [[73](#bib.bib73)])'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：近端策略优化（PPO）在来自人类反馈的强化学习（RLHF）上下文中应用于微调大型语言模型（LLM）的示意图。该过程涉及使用提示数据集来训练LLM。PPO算法根据奖励模型提供的奖励调整LLM的策略，奖励模型通过人类反馈进行微调。（改编自[[73](#bib.bib73)]）
- en: Python Library - HuggingFace Transformer Reinforcement Learning (TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index))
    package supports the PPO Trainer⁵⁵5[https://huggingface.co/docs/trl/main/en/ppo_trainer](https://huggingface.co/docs/trl/main/en/ppo_trainer)
    for training language models from the preference data.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: Python库 - HuggingFace Transformer 强化学习（TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index)）包支持PPO训练器⁵⁵5[https://huggingface.co/docs/trl/main/en/ppo_trainer](https://huggingface.co/docs/trl/main/en/ppo_trainer)用于从偏好数据中训练语言模型。
- en: The PPOTrainer expects to align a generated response with a query given the
    rewards obtained from the Reward model. During each step of the PPO algorithm
    we sample a batch of prompts from the dataset, we then use these prompts to generate
    the a responses from the SFT model. Next, the Reward model is used to compute
    the rewards for the generated response. Finally, these rewards are used to optimise
    the SFT model using the PPO algorithm. Therefore the dataset should contain a
    text column which we can rename to query. Each of the other data-points required
    to optimise the SFT model are obtained during the training loop.
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: PPOTrainer期望根据从奖励模型获得的奖励，使生成的响应与查询对齐。在PPO算法的每一步中，我们从数据集中采样一批提示，然后使用这些提示从SFT模型生成响应。接下来，使用奖励模型计算生成响应的奖励。最后，这些奖励用于通过PPO算法优化SFT模型。因此，数据集应包含一个文本列，我们可以将其重命名为查询。优化SFT模型所需的其他数据点在训练循环中获得。
- en: 6.8.1 Benefits of PPO
  id: totrans-1099
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.1 PPO的好处
- en: '1.'
  id: totrans-1100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable
    and reliable policy updates. The clipped surrogate objective function is central
    to this stability, as it limits policy updates to prevent large, potentially destabilising
    changes. This results in smoother and more consistent learning.'
  id: totrans-1101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性：近端策略优化（PPO）旨在确保稳定可靠的策略更新。裁剪的代理目标函数是这种稳定性的核心，因为它限制策略更新以防止大规模、潜在的不稳定变化。这导致学习过程更加平稳和一致。
- en: '2.'
  id: totrans-1102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively
    straightforward to implement. It avoids the need for second-order optimisation
    techniques, making it more accessible to less experienced practitioners.'
  id: totrans-1103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现的简便性：与高级算法TRPO相比，PPO相对容易实现。它避免了二阶优化技术，使得经验不足的实践者也能更容易上手。
- en: '3.'
  id: totrans-1104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Sample Efficiency: PPO achieves data efficiency through its use of the clipped
    surrogate objective. This mechanism regulates policy updates, ensuring stability
    while effectively reusing training data. Consequently, PPO tends to be more sample-efficient
    than other reinforcement learning algorithms, performing well with fewer samples,
    which is advantageous in scenarios where data collection is costly or time-consuming.'
  id: totrans-1105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例效率：PPO通过使用裁剪的代理目标实现了数据效率。这个机制调节策略更新，确保稳定性，同时有效地重用训练数据。因此，PPO往往比其他强化学习算法更具样本效率，在样本较少的情况下表现良好，这在数据收集成本高或耗时的场景中尤为有利。
- en: 6.8.2 Limitations of PPO
  id: totrans-1106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.2 PPO的局限性
- en: '1.'
  id: totrans-1107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves
    intricate policy and value networks, necessitating substantial computational resources
    for training. This complexity often results in extended training durations and
    increased operational expenses.'
  id: totrans-1108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂性和计算成本：近端策略优化（PPO）涉及复杂的策略和价值网络，需要大量计算资源进行训练。这种复杂性通常导致训练时间延长和运营成本增加。
- en: '2.'
  id: totrans-1109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Hyperparameter Sensitivity: PPO’s performance is highly dependent on several
    hyperparameters, such as the clipping range, learning rate, and discount factor.
    Achieving optimal performance requires meticulous tuning of these parameters.
    Incorrect settings can lead to suboptimal policy outcomes or instability during
    the learning process.'
  id: totrans-1110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数敏感性：PPO的性能高度依赖于几个超参数，如裁剪范围、学习率和折扣因子。要实现最佳性能，需要对这些参数进行精细调整。设置不正确可能导致策略结果不理想或学习过程中的不稳定。
- en: '3.'
  id: totrans-1111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Stability and Convergence Issues: Although PPO is designed to enhance stability
    compared to earlier methods, it can still encounter convergence issues, particularly
    in highly dynamic or complex environments. Maintaining stable policy updates remains
    a significant challenge.'
  id: totrans-1112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性和收敛性问题：尽管PPO设计上旨在增强稳定性，但在高度动态或复杂的环境中，它仍可能遇到收敛性问题。保持稳定的策略更新仍然是一个重大挑战。
- en: '4.'
  id: totrans-1113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Reward Signal Dependence: PPO’s effectiveness is heavily reliant on a well-defined
    reward signal to guide the learning process. In scenarios where designing an appropriate
    reward function is challenging or impractical, PPO may struggle to attain the
    desired results.'
  id: totrans-1114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励信号依赖：PPO的有效性在很大程度上依赖于一个明确定义的奖励信号来指导学习过程。在设计适当的奖励函数具有挑战性或不切实际的情况下，PPO可能难以达到预期的结果。
- en: 6.8.3 Tutorial for training models using PPO technique
  id: totrans-1115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.3 使用PPO技术训练模型的教程
- en: The tutorial for tuning GPT2 to generate positive movie reviews based on the
    IMDB dataset using PPO technique can be found [here.](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PPO 技术基于 IMDB 数据集对 GPT2 进行正面电影评论生成调优的教程可以在 [此处](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)
    查找。
- en: 6.9 Direct Preference Optimisation (DPO)
  id: totrans-1117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 直接偏好优化（DPO）
- en: Direct Preference Optimisation (DPO) [[74](#bib.bib74)] offers a streamlined
    approach to aligning language models (LMs) with human preferences, bypassing the
    complexity of reinforcement learning from human feedback (RLHF). Large-scale unsupervised
    LMs typically lack precise behavioural control, necessitating methods like RLHF
    that fine-tune models using human feedback. However, RLHF is intricate, involving
    the creation of reward models and the fine-tuning of LMs to maximise estimated
    rewards, which can be unstable and computationally demanding. DPO addresses these
    challenges by directly optimising LMs with a simple classification objective that
    aligns responses with human preferences. This approach eliminates the need for
    explicit reward modelling and extensive hyperparameter tuning, enhancing stability
    and efficiency. DPO optimises the desired behaviours by increasing the relative
    likelihood of preferred responses while incorporating dynamic importance weights
    to prevent model degeneration. Thus, DPO simplifies the preference learning pipeline,
    making it an effective method for training LMs to adhere to human preferences.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 直接偏好优化（DPO）[[74](#bib.bib74)] 提供了一种简化的方法，将语言模型（LMs）与人类偏好对齐，绕过了从人类反馈中进行强化学习（RLHF）的复杂性。大规模无监督的
    LMs 通常缺乏精确的行为控制，因此需要像 RLHF 这样的方式，通过人类反馈微调模型。然而，RLHF 复杂，包括奖励模型的创建以及 LMs 的微调，以最大化估计奖励，这可能不稳定且计算量大。DPO
    通过直接优化 LMs，采用简单的分类目标，使响应与人类偏好对齐，从而解决这些挑战。这种方法消除了显式奖励建模和广泛的超参数调整需求，提高了稳定性和效率。DPO
    通过增加偏好响应的相对可能性并结合动态重要性权重来防止模型退化，从而优化期望的行为。因此，DPO 简化了偏好学习管道，使其成为训练 LMs 以符合人类偏好的有效方法。
- en: '![Refer to caption](img/06654e643ba623d84aa22f7cf2bc6a18.png)'
  id: totrans-1119
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/06654e643ba623d84aa22f7cf2bc6a18.png)'
- en: 'Figure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure
    illustrates the Direct Preference Optimisation (DPO) technique used in fine-tuning
    large language models. The process begins with preference data (<math id=$$ represents
    less preferred outputs. Through a maximum likelihood estimation process, this
    preference data is used to optimise the model’s parameters, resulting in the final
    large language model (LLM). The method is designed to improve the alignment of
    model outputs with desired user preferences, enhancing the model’s effectiveness
    in specific tasks. (adapted from [[74](#bib.bib74)])'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12：直接偏好优化（DPO）过程流程。该图展示了用于微调大型语言模型的直接偏好优化（DPO）技术。该过程始于偏好数据（<math id=$$ 表示不太偏好的输出。通过最大似然估计过程，这些偏好数据用于优化模型的参数，最终得到大型语言模型（LLM）。该方法旨在改善模型输出与期望用户偏好的一致性，提高模型在特定任务中的有效性。（改编自
    [[74](#bib.bib74)]）
- en: 'Python Library - HuggingFace TRL package supports the DPO Trainer⁶⁶6[https://huggingface.co/docs/trl/main/en/dpo_trainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)
    for training language models from the preference data. The DPO training process
    requires a dataset formatted in a very specific manner. If you are utilising the
    default DPODataCollatorWithPadding data collator, your final dataset object must
    include three specific entries, which should be labelled as follows:'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: Python 库 - HuggingFace TRL 包支持 DPO Trainer⁶⁶6 [https://huggingface.co/docs/trl/main/en/dpo_trainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)
    用于根据偏好数据训练语言模型。DPO 训练过程需要以非常特定的方式格式化数据集。如果你使用的是默认的 DPODataCollatorWithPadding
    数据整理器，你的最终数据集对象必须包含三个特定条目，这些条目应标记如下：
- en: •
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prompt
  id: totrans-1123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: •
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Chosen
  id: totrans-1125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已选择
- en: •
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rejected
  id: totrans-1127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 被拒绝
- en: HuggingFace offers datasets compatible with DPO and can be accessed [here.](https://huggingface.co/datasets?other=dpo)
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace 提供了与 DPO 兼容的数据集，可在 [此处](https://huggingface.co/datasets?other=dpo)
    访问。
- en: 6.9.1 Benefits of DPO
  id: totrans-1129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.9.1 DPO 的好处
- en: '1.'
  id: totrans-1130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Direct Alignment with Human Preferences: DPO directly optimises models to generate
    responses that align with human preferences, thereby producing more favourable
    outputs.'
  id: totrans-1131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与人类偏好的直接对齐：DPO 直接优化模型以生成与人类偏好对齐的响应，从而产生更有利的输出。
- en: '2.'
  id: totrans-1132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Minimised Dependence on Proxy Objectives: In contrast to methods that rely
    on next-word prediction, DPO leverages explicit human preferences, resulting in
    responses that are more reflective of human behaviour.'
  id: totrans-1133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最小化对代理目标的依赖：与依赖下一个词预测的方法不同，DPO利用明确的人类偏好，导致生成的响应更能反映人类行为。
- en: '3.'
  id: totrans-1134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement,
    such as dialogue generation or creative writing, DPO excels in aligning the model
    with human preferences.'
  id: totrans-1135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在主观任务上的增强性能：对于需要主观判断的任务，如对话生成或创意写作，DPO在将模型与人类偏好对齐方面表现优异。
- en: 6.9.2 Best Practices for DPO
  id: totrans-1136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.9.2 DPO的最佳实践
- en: '1.'
  id: totrans-1137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'High-Quality Preference Data: The performance of the model is heavily influenced
    by the quality of preference data. Ensure the dataset includes clear and consistent
    human preferences.'
  id: totrans-1138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高质量的偏好数据：模型的性能受到偏好数据质量的重大影响。确保数据集包含清晰且一致的人类偏好。
- en: '2.'
  id: totrans-1139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Optimal Beta Value: Experiment with various beta values to manage the influence
    of the reference model. Higher beta values prioritise the reference model’s preferences
    more strongly.'
  id: totrans-1140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳Beta值：尝试不同的beta值以管理参考模型的影响。较高的beta值更强烈地优先考虑参考模型的偏好。
- en: '3.'
  id: totrans-1141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch
    size, and LoRA configuration to determine the best settings for your dataset and
    task.'
  id: totrans-1142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数调整：优化学习率、批量大小和LoRA配置等超参数，以确定适合数据集和任务的最佳设置。
- en: '4.'
  id: totrans-1143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Evaluation on Target Tasks: Continuously assess the model’s performance on
    the target task using appropriate metrics to monitor progress and ensure the achievement
    of desired results.'
  id: totrans-1144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标任务评估：使用适当的指标持续评估模型在目标任务上的性能，以监控进展并确保实现预期结果。
- en: '5.'
  id: totrans-1145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Ethical Considerations: Pay attention to potential biases in the preference
    data and take steps to mitigate them, preventing the model from adopting and amplifying
    these biases.'
  id: totrans-1146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理考虑：关注偏好数据中的潜在偏见，并采取措施减轻这些偏见，防止模型采纳和放大这些偏见。
- en: 6.9.3 Tutorial for training models using DPO technique
  id: totrans-1147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.9.3 使用DPO技术训练模型的教程
- en: The tutorial for DPO training, including the full source code of the training
    scripts for SFT and DPO, is available [here.](https://github.com/huggingface/blog/blob/main/dpo-trl.md)
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DPO训练的教程，包括SFT和DPO训练脚本的完整源代码，可在[此处](https://github.com/huggingface/blog/blob/main/dpo-trl.md)获取。
- en: 6.9.4 Is DPO Superior to PPO for LLM Alignment?
  id: totrans-1149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.9.4 DPO是否优于PPO用于LLM对齐？
- en: The recent study on DPO superior to PPO for LLM Alignment[[75](#bib.bib75)]
    investigates the efficacy of reward-based and reward-free methods within RLHF.
    Reward-based methods, such as those developed by OpenAI, utilise a reward model
    constructed from preference data and apply actor-critic algorithms like Proximal
    Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free
    methods, including Direct Preference Optimisation (DPO), RRHF, and PRO, forego
    an explicit reward function, with DPO focusing exclusively on policy optimisation
    through a logarithmic representation of the reward function.
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于DPO是否优于PPO用于LLM对齐的研究[[75](#bib.bib75)]探讨了在RLHF中基于奖励的方法和无奖励的方法的有效性。基于奖励的方法，例如OpenAI开发的方法，利用从偏好数据中构建的奖励模型，并应用像Proximal
    Policy Optimisation（PPO）这样的演员-评论家算法来优化奖励信号。相反，无奖励的方法，包括Direct Preference Optimisation（DPO）、RRHF和PRO，放弃了明确的奖励函数，其中DPO专注于通过奖励函数的对数表示来进行策略优化。
- en: One of the objectives of this study is to determine whether DPO is genuinely
    superior to PPO in the RLHF domain. The study combines theoretical and empirical
    analyses to uncover the inherent limitations of DPO and identify critical factors
    that enhance PPO’s practical performance in RLHF.
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的一个目标是确定DPO是否确实在RLHF领域优于PPO。该研究结合了理论和实证分析，以揭示DPO的固有局限性，并确定提升PPO在RLHF中实际表现的关键因素。
- en: Theoretical findings suggest that DPO may yield biased solutions by exploiting
    out-of-distribution responses. Empirical results indicate that DPO’s performance
    is notably affected by shifts in the distribution between model outputs and the
    preference dataset. Furthermore, the study highlights that while iterative DPO
    may offer improvements over static data training, it still fails to enhance performance
    in challenging tasks such as code generation. Ablation studies on PPO reveal essential
    components for optimal performance, including advantage normalisation, large batch
    sizes, and exponential moving average updates for the reference model’s parameters.
    These findings form the basis of practical tuning guidelines, demonstrating PPO’s
    robust effectiveness across diverse tasks and its ability to achieve state-of-the-art
    results in challenging code competition tasks. Specifically, on the CodeContest
    dataset, the PPO model with 34 billion parameters surpasses AlphaCode-41B, showing
    a significant improvement in performance metrics.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 理论研究表明，DPO可能通过利用分布外响应而产生偏倚的解决方案。实证结果表明，DPO的性能明显受到模型输出与偏好数据集之间分布变化的影响。此外，研究强调，尽管迭代DPO可能比静态数据训练提供改进，但在代码生成等具有挑战性的任务中仍未能提高性能。对PPO的消融研究揭示了实现最佳性能的关键组件，包括优势标准化、大批量样本以及参考模型参数的指数移动平均更新。这些发现形成了实际调优指南的基础，展示了PPO在各种任务中的强大有效性以及在具有挑战性的代码竞赛任务中实现最先进结果的能力。具体来说，在CodeContest数据集上，具有340亿参数的PPO模型超越了AlphaCode-41B，显示出性能指标的显著提升。
- en: 6.10 Optimised Routing and Pruning Operations (ORPO)
  id: totrans-1153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.10 优化路由和剪枝操作（ORPO）
- en: 'Pruning LLMs involves eliminating unnecessary or redundant components from
    a neural network to reduce its size and complexity, thereby enhancing its efficiency
    and performance. This process assists AI developers and engineers in addressing
    the challenges associated with deploying AI models in resource-limited environments,
    such as mobile devices, edge computing, or embedded systems. Pruning AI models
    can be achieved through various techniques, each suited to the type and structure
    of the neural network, the pruning objective, and the pruning criterion. The following
    are common approaches:'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝大模型（LLMs）涉及从神经网络中删除不必要或冗余的组件，以减小其规模和复杂性，从而提高效率和性能。此过程帮助AI开发人员和工程师解决在资源受限的环境中部署AI模型的挑战，例如移动设备、边缘计算或嵌入式系统。剪枝AI模型可以通过各种技术实现，每种技术都适合于神经网络的类型和结构、剪枝目标以及剪枝标准。以下是常见的方法：
- en: '1.'
  id: totrans-1155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Weight Pruning: Involves removing weights or connections with minimal magnitude
    or impact on the output. This method reduces the number of parameters and operations
    in the model, although it may not necessarily decrease memory footprint or latency.'
  id: totrans-1156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**权重剪枝**：涉及移除对输出影响最小的权重或连接。这种方法减少了模型中的参数数量和操作，尽管它不一定减少内存占用或延迟。'
- en: '2.'
  id: totrans-1157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Unit Pruning: Eliminates entire units or neurons with the lowest activation
    or contribution to the output. This technique can reduce the model’s memory footprint
    and latency but may require retraining or fine-tuning to maintain performance.'
  id: totrans-1158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**单元剪枝**：移除对输出激活或贡献最小的整个单元或神经元。这种技术可以减少模型的内存占用和延迟，但可能需要重新训练或微调以维持性能。'
- en: '3.'
  id: totrans-1159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Filter Pruning: Involves removing entire filters or channels in convolutional
    neural networks that have the least importance or relevance to the output. This
    strategy also reduces memory footprint and latency, though it may necessitate
    retraining or fine-tuning to preserve performance [[76](#bib.bib76)].'
  id: totrans-1160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**滤波器剪枝**：涉及移除卷积神经网络中对输出影响最小的整个滤波器或通道。这种策略还可以减少内存占用和延迟，但可能需要重新训练或微调以保持性能 [[76](#bib.bib76)]。'
- en: 6.10.1 When to Prune AI Models?
  id: totrans-1161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.10.1 何时剪枝AI模型？
- en: Pruning AI models can be conducted at various stages of the model development
    and deployment cycle, contingent on the chosen technique and objective.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝AI模型可以在模型开发和部署周期的各个阶段进行，具体取决于所选择的技术和目标。
- en: '1.'
  id: totrans-1163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Pre-Training Pruning: Leverages prior knowledge or heuristics to determine
    the optimal network structure before training begins. This approach can save time
    and resources during training but may necessitate careful design and experimentation
    to identify the best configuration.'
  id: totrans-1164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预训练剪枝**：利用先验知识或启发式方法来确定训练开始之前的最佳网络结构。这种方法可以节省训练时间和资源，但可能需要精心设计和实验以确定最佳配置。'
- en: '2.'
  id: totrans-1165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Post-Training Pruning: Involves using metrics or criteria to assess the importance
    or impact of each network component after training. This method helps maintain
    model performance but may require additional validation and testing to ensure
    quality and robustness.'
  id: totrans-1166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练后剪枝：涉及使用指标或标准来评估每个网络组件在训练后的重要性或影响。这种方法有助于保持模型性能，但可能需要额外的验证和测试以确保质量和鲁棒性。
- en: '3.'
  id: totrans-1167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Dynamic Pruning: Adjusts the network structure during inference or runtime
    based on feedback or signals. This approach can optimise the model for different
    scenarios or tasks but may involve higher computational overhead and complexity
    to implement and execute.'
  id: totrans-1168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态剪枝：在推理或运行时根据反馈或信号调整网络结构。这种方法可以针对不同场景或任务优化模型，但可能涉及更高的计算开销和复杂性。
- en: 6.10.2 Benefits of Pruning
  id: totrans-1169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.10.2 剪枝的好处
- en: '1.'
  id: totrans-1170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Reduced Size and Complexity: Pruning decreases the size and complexity of AI
    models, making them easier to store, transmit, and update.'
  id: totrans-1171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少大小和复杂性：剪枝减少了AI模型的大小和复杂性，使其更易于存储、传输和更新。
- en: '2.'
  id: totrans-1172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Improved Efficiency and Performance: Pruned models are faster, more energy-efficient,
    and more reliable.'
  id: totrans-1173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提高效率和性能：剪枝后的模型更快、更节能且更可靠。
- en: '3.'
  id: totrans-1174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Enhanced generalisation and Accuracy: Pruning can make models more robust,
    less prone to overfitting, and more adaptable to new data or tasks.'
  id: totrans-1175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强泛化能力和准确性：剪枝可以使模型更具鲁棒性，减少过拟合，更适应新数据或任务。
- en: 6.10.3 Challenges of Pruning
  id: totrans-1176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.10.3 剪枝的挑战
- en: '1.'
  id: totrans-1177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Balance Between Size Reduction and Performance: Achieving the optimal balance
    between reducing size and complexity and maintaining performance is challenging;
    excessive or insufficient pruning can degrade model quality and functionality.'
  id: totrans-1178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在减少大小和性能之间的平衡：在减少大小和复杂性与保持性能之间取得最佳平衡是具有挑战性的；过度或不足的剪枝可能会降低模型质量和功能。
- en: '2.'
  id: totrans-1179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Choosing Appropriate Techniques: Selecting the right pruning technique, criterion,
    and objective for the specific neural network type and structure is crucial, as
    different methods can produce varying effects and outcomes.'
  id: totrans-1180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的技术：为特定的神经网络类型和结构选择合适的剪枝技术、标准和目标至关重要，因为不同的方法可能会产生不同的效果和结果。
- en: '3.'
  id: totrans-1181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Evaluation and Validation: Pruned models need thorough evaluation and validation
    to ensure pruning has not introduced errors, biases, or vulnerabilities that could
    impact performance and robustness.'
  id: totrans-1182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估与验证：剪枝后的模型需要经过彻底的评估和验证，以确保剪枝没有引入错误、偏差或漏洞，这些因素可能会影响性能和鲁棒性。
- en: 'Chapter 7 Stage 5: Evaluation and Validation'
  id: totrans-1183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章第5阶段：评估与验证
- en: 7.1 Steps Involved in Evaluating and Validating Fine-Tuned Models
  id: totrans-1184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 评估和验证微调模型的步骤
- en: '1.'
  id: totrans-1185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy,
    to measure the difference between the predicted and actual distributions of the
    data.'
  id: totrans-1186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置评估指标：选择合适的评估指标，如交叉熵，以测量预测数据和实际数据分布之间的差异。
- en: '2.'
  id: totrans-1187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Interpret Training Loss Curve: Monitor and analyse the training loss curve
    to ensure the model is learning effectively, avoiding patterns of underfitting
    or overfitting.'
  id: totrans-1188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解读训练损失曲线：监控和分析训练损失曲线，以确保模型有效学习，避免出现欠拟合或过拟合的模式。
- en: '3.'
  id: totrans-1189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Run Validation Loops: After each training epoch, evaluate the model on the
    validation set to compute relevant performance metrics and track the model’s generalisation
    ability.'
  id: totrans-1190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行验证循环：在每个训练轮次后，对验证集上的模型进行评估，以计算相关的性能指标并跟踪模型的泛化能力。
- en: '4.'
  id: totrans-1191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Monitor and Interpret Results: Consistently observe the relationship between
    training and validation metrics to ensure stable and effective model performance.'
  id: totrans-1192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监控和解读结果：持续观察训练和验证指标之间的关系，以确保模型性能的稳定性和有效性。
- en: '5.'
  id: totrans-1193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning
    rate, batch size, and number of training epochs to optimise model performance
    and prevent overfitting.'
  id: totrans-1194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数调整：调整关键超参数，如学习率、批量大小和训练轮次，以优化模型性能并防止过拟合。
- en: 7.2 Setting Up Evaluation Metrics
  id: totrans-1195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 设置评估指标
- en: Cross-entropy is a key metric for evaluating LLMs during training or fine-tuning.
    Originating from information theory, it quantifies the difference between two
    probability distributions.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是训练或微调LLMs时评估的关键指标。它源于信息论，用于量化两个概率分布之间的差异。
- en: 7.2.1 Importance of Cross-Entropy for LLM Training and Evaluation
  id: totrans-1197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 交叉熵在LLM训练和评估中的重要性
- en: Cross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss
    function, guiding the model to produce high-quality predictions by minimising
    discrepancies between the predicted and actual data. In LLMs, each potential word
    functions as a separate class, and the model’s task is to predict the next word
    given the context. This task is inherently complex, requiring the model to understand
    syntax, semantics, and context deeply.
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵对于训练和微调 LLM 至关重要。它作为一种损失函数，通过最小化预测数据和实际数据之间的差异来指导模型生成高质量的预测。在 LLM 中，每个潜在单词作为一个独立的类别，模型的任务是根据上下文预测下一个单词。这个任务本质上很复杂，需要模型深入理解句法、语义和上下文。
- en: '7.2.2 Beyond Cross-Entropy: Advanced LLM Evaluation Metrics'
  id: totrans-1199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 超越交叉熵：高级 LLM 评估指标
- en: 'While cross-entropy remains fundamental, evaluating LLMs effectively necessitates
    additional metrics tailored to various aspects of model performance. Here are
    some advanced metrics employed in LLM evaluation:'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然交叉熵仍然是基础，但有效评估 LLM 需要额外的指标来针对模型性能的各个方面。以下是一些用于 LLM 评估的高级指标：
- en: Perplexity
  id: totrans-1201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 困惑度
- en: Perplexity measures how well a probability distribution or model predicts a
    sample. In the context of LLMs, it evaluates the model’s uncertainty about the
    next word in a sequence. Lower perplexity indicates better performance, as the
    model is more confident in its predictions.
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度衡量概率分布或模型对样本的预测能力。在 LLM 的背景下，它评估模型对序列中下一个单词的不确定性。较低的困惑度表明更好的性能，因为模型对其预测更有信心。
- en: Factuality
  id: totrans-1203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事实性
- en: Factuality assesses the accuracy of the information produced by the LLM. It
    is particularly important for applications where misinformation could have serious
    consequences. Higher factuality scores correlate with higher output quality.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: '**事实性**评估 LLM 生成的信息的准确性。对于可能产生严重后果的应用来说，这一点尤为重要。更高的事实性评分与更高的输出质量相关联。'
- en: LLM Uncertainty
  id: totrans-1205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 不确定性
- en: LLM uncertainty is measured using log probability, helping to identify low-quality
    generations. Lower uncertainty indicates higher output quality. This metric leverages
    the log probability of each generated token, providing insights into the model’s
    confidence in its responses.
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 不确定性使用对数概率来衡量，有助于识别低质量的生成内容。较低的不确定性表示更高的输出质量。该指标利用每个生成标记的对数概率，提供对模型响应信心的见解。
- en: Prompt Perplexity
  id: totrans-1207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示困惑度
- en: This metric evaluates how well the model understands the input prompt. Lower
    prompt perplexity indicates a clear and comprehensible prompt, which is likely
    to yield better model performance.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标评估模型对输入提示的理解程度。较低的提示困惑度表明提示清晰易懂，这通常会带来更好的模型性能。
- en: Context Relevance
  id: totrans-1209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文相关性
- en: In retrieval-augmented generation (RAG) systems, context relevance measures
    how pertinent the retrieved context is to the user query. Higher context relevance
    improves the quality of generated responses by ensuring that the model utilises
    the most relevant information.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索增强生成（RAG）系统中，**上下文相关性**衡量检索到的上下文对用户查询的相关性。更高的上下文相关性通过确保模型利用最相关的信息来提高生成响应的质量。
- en: Completeness
  id: totrans-1211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 完整性
- en: Completeness assesses whether the model’s response fully addresses the query
    based on the provided context. High completeness ensures that all relevant information
    is included in the response, enhancing its utility and accuracy.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整性**评估模型的响应是否充分解决了基于提供的上下文的查询。高完整性确保所有相关信息都包含在响应中，从而提高其实用性和准确性。'
- en: Chunk Attribution and Utilisation
  id: totrans-1213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 信息块归属和利用
- en: These metrics evaluate how effectively the retrieved chunks of information contribute
    to the final response. Higher chunk attribution and utilisation scores indicate
    that the model is efficiently using the available context to generate accurate
    and relevant answers.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标评估检索到的信息块对最终响应的贡献效果。更高的信息块归属和利用评分表明模型有效地使用了可用的上下文来生成准确且相关的回答。
- en: Data Error Potential
  id: totrans-1215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据错误潜力
- en: This metric quantifies the difficulty the model faces in learning from the training
    data. Higher data quality results in lower error potential, leading to better
    model performance.
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标量化模型在从训练数据中学习时面临的困难。较高的数据质量会导致较低的错误潜力，从而提高模型性能。
- en: Safety Metrics
  id: totrans-1217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安全指标
- en: Safety metrics ensure that the LLM’s outputs are appropriate and non-harmful.
    These are included in the final sections of the chapter.
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: 安全指标确保 LLM 的输出是适当且无害的。这些指标在章节的最后部分中包含。
- en: Integrating these advanced metrics provides a holistic view of LLM performance,
    enabling developers to fine-tune and optimise models more effectively. By employing
    a metrics-first approach, it is possible to ensure that LLMs not only produce
    accurate and high-quality outputs but also do so consistently and reliably across
    diverse applications¹¹1[https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation).
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 整合这些先进的指标提供了 LLM 性能的全面视图，使开发者能够更有效地微调和优化模型。通过采用以指标为先的方法，可以确保 LLM 不仅能够产生准确且高质量的输出，而且在各种应用中始终如一地可靠¹¹1[https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation)。
- en: 7.3 Understanding the Training Loss Curve
  id: totrans-1220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 理解训练损失曲线
- en: The training loss curve plots the loss value against training epochs and is
    essential for monitoring model performance.
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失曲线将损失值与训练周期进行绘图，对监控模型性能至关重要。
- en: 7.3.1 Interpreting Loss Curves
  id: totrans-1222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1 解释损失曲线
- en: 'An ideal training loss curve shows a rapid decrease in loss during initial
    stages, followed by a gradual decline and eventual plateau. Specific patterns
    to look for include:'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的训练损失曲线显示初期损失迅速下降，随后逐渐降低并最终平稳。需要注意的具体模式包括：
- en: '1.'
  id: totrans-1224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Underfitting: High loss value that does not decrease significantly over time,
    suggesting the model cannot learn the data.'
  id: totrans-1225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欠拟合：高损失值在一段时间内没有显著减少，表明模型无法学习数据。
- en: '2.'
  id: totrans-1226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Overfitting: Decreasing training loss with increasing validation loss, indicating
    the model memorises the training data.'
  id: totrans-1227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过拟合：训练损失降低而验证损失增加，表明模型记住了训练数据。
- en: '3.'
  id: totrans-1228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Fluctuations: Significant variations may indicate a high learning rate or noisy
    gradients.'
  id: totrans-1229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 波动：显著的变化可能表示学习率过高或噪声梯度。
- en: '![Refer to caption](img/d03cfe9ebed867545a17f1ffd276a588.png)'
  id: totrans-1230
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d03cfe9ebed867545a17f1ffd276a588.png)'
- en: 'Figure 7.1: Example training loss curve showing the decline in loss over iterations
    during the fine-tuning of Llama2 13B on a financial Q/A dataset. The curve illustrates
    the effectiveness of the fine-tuning process in reducing the loss and improving
    model performance.'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：示例训练损失曲线显示了在对 Llama2 13B 在金融问答数据集进行微调过程中损失的下降。该曲线展示了微调过程在减少损失和提高模型性能方面的有效性。
- en: 7.3.2 Avoiding Overfitting
  id: totrans-1232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 避免过拟合
- en: 'Techniques to prevent overfitting include:'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过拟合的技术包括：
- en: '1.'
  id: totrans-1234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Regularisation: Adds a penalty term to the loss function to encourage smaller
    weights.'
  id: totrans-1235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正则化：向损失函数添加惩罚项，以鼓励更小的权重。
- en: '2.'
  id: totrans-1236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Early Stopping: Stops training when validation performance no longer improves.'
  id: totrans-1237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提前停止：当验证性能不再提高时停止训练。
- en: '3.'
  id: totrans-1238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Dropout: Randomly deactivates neurons during training to reduce sensitivity
    to noise.'
  id: totrans-1239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dropout：在训练过程中随机停用神经元以减少对噪声的敏感性。
- en: '4.'
  id: totrans-1240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Cross-Validation: Splits data into multiple subsets for training and validation
    to assess model generalisation.'
  id: totrans-1241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交叉验证：将数据划分为多个子集进行训练和验证，以评估模型的泛化能力。
- en: '5.'
  id: totrans-1242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Batch Normalisation: Normalises inputs to each layer during training to stabilise
    the learning process.'
  id: totrans-1243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量归一化：在训练过程中对每一层的输入进行归一化，以稳定学习过程。
- en: '6.'
  id: totrans-1244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount
    of diverse data and batch sizes.'
  id: totrans-1245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更大的数据集和批量大小：通过增加多样化的数据量和批量大小来减少过拟合。
- en: 7.3.3 Sources of Noisy Gradients
  id: totrans-1246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.3 噪声梯度的来源
- en: 'Noisy gradients are common during the training of machine learning models,
    including LLMs. They arise from variability in gradient estimates due to stochastic
    gradient descent and its variants. Strategies to manage noisy gradients include:'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声梯度在机器学习模型的训练过程中，包括 LLMs，是很常见的。它们源于由于随机梯度下降及其变体引起的梯度估计的变异。管理噪声梯度的策略包括：
- en: '1.'
  id: totrans-1248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Learning Rate Scheduling: Gradually decreasing the learning rate during training
    can reduce the impact of noisy gradients.'
  id: totrans-1249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率调度：在训练过程中逐渐降低学习率可以减少噪声梯度的影响。
- en: '2.'
  id: totrans-1250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Gradient Clipping: Setting a threshold for gradient values prevents large updates
    that can destabilise training.'
  id: totrans-1251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度裁剪：为梯度值设置阈值，防止大的更新使训练不稳定。
- en: 7.4 Running Validation Loops
  id: totrans-1252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 运行验证循环
- en: 'Validation loops provide an unbiased evaluation of model performance. Typical
    steps include:'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 验证循环提供了对模型性能的无偏评价。典型步骤包括：
- en: '1.'
  id: totrans-1254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Split Data: Divide the dataset into training and validation sets.'
  id: totrans-1255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 划分数据：将数据集分为训练集和验证集。
- en: '2.'
  id: totrans-1256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Initialise Validation: Evaluate the model on the validation set at the end
    of each epoch.'
  id: totrans-1257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化验证：在每个周期结束时在验证集上评估模型。
- en: '3.'
  id: totrans-1258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Calculate Metrics: Compute relevant performance metrics, such as cross-entropy
    loss.'
  id: totrans-1259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算指标：计算相关性能指标，如交叉熵损失。
- en: '4.'
  id: totrans-1260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Record Results: Log validation metrics for each epoch.'
  id: totrans-1261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记录结果：记录每个周期的验证指标。
- en: '5.'
  id: totrans-1262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Early Stopping: Optionally stop training if validation loss does not improve
    for a predefined number of epochs.'
  id: totrans-1263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 早停：如果验证损失在预定义的周期内没有改善，可以选择停止训练。
- en: 7.5 Monitoring and Interpreting Results
  id: totrans-1264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 监控和解释结果
- en: 'Monitoring validation results involves analysing trends in validation metrics
    over epochs. Key aspects include:'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 监控验证结果涉及分析验证指标随周期的变化趋势。关键方面包括：
- en: '1.'
  id: totrans-1266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Consistent Improvement: Indicates good model generalisation if both training
    and validation metrics improve and plateau.'
  id: totrans-1267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一致的改善：如果训练和验证指标都改善并稳定，说明模型泛化良好。
- en: '2.'
  id: totrans-1268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Divergence: Suggests overfitting if training metrics improve while validation
    metrics deteriorate.'
  id: totrans-1269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发散：如果训练指标改善而验证指标恶化，可能表明过拟合。
- en: '3.'
  id: totrans-1270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Stability: Ensure validation metrics do not fluctuate significantly, indicating
    stable training.'
  id: totrans-1271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性：确保验证指标不显著波动，表明训练稳定。
- en: 7.6 Hyperparameter Tuning and Other Adjustments
  id: totrans-1272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 超参数调优及其他调整
- en: 'Fine-tuning involves adjusting key hyperparameters to achieve optimal performance.
    Important hyperparameters include:'
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 微调涉及调整关键超参数以实现最佳性能。重要的超参数包括：
- en: '1.'
  id: totrans-1274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Learning Rate: Determines the step size for updating model weights. A good
    starting point is 2e-4, but this can vary.'
  id: totrans-1275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率：决定更新模型权重的步长。一个好的起始点是2e-4，但这可能有所不同。
- en: '2.'
  id: totrans-1276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Batch Size: Larger batch sizes lead to more stable updates but require more
    memory.'
  id: totrans-1277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小：较大的批量大小会导致更新更稳定，但需要更多内存。
- en: '3.'
  id: totrans-1278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Number of Training Epochs: Balancing the number of epochs ensures the model
    learns sufficiently without overfitting or underfitting.'
  id: totrans-1279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练周期数：平衡周期数确保模型充分学习而不过拟合或欠拟合。
- en: '4.'
  id: totrans-1280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for
    large models.'
  id: totrans-1281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化器：如Paged ADAM的优化器优化内存使用，对大模型有利。
- en: Other tunable parameters include dropout rate, weight decay, and warmup steps.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可调参数包括丢弃率、权重衰减和预热步骤。
- en: 7.6.1 Data Size and Quality
  id: totrans-1283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.1 数据大小和质量
- en: The efficacy of LLMs is directly impacted by the quality of their training data.
    Ensuring that datasets are clean, relevant, and adequate is crucial. Data cleanliness
    refers to the absence of noise, errors, and inconsistencies within the labelled
    data. For example, having a phrase like “This article suggests…” multiple times
    in the training data can corrupt the response of LLMs and add a bias towards using
    this specific phrase more often and in inappropriate situations.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的效果直接受到其训练数据质量的影响。确保数据集干净、相关且充足至关重要。数据清洁度指标志数据中不存在噪音、错误和不一致。例如，训练数据中出现多次“这篇文章建议……”这样的短语可能会污染LLM的响应，并导致LLM在不适当的情况下更频繁地使用这个特定短语。
- en: 7.7 Benchmarking Fine-Tuned LLMs
  id: totrans-1285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7 基准测试微调后的LLMs
- en: 'Modern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE,
    HellaSwag, TruthfulQA, and MMLU (See Table [7.1](#Ch7.T1 "Table 7.1 ‣ 7.7 Benchmarking
    Fine-Tuned LLMs ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")). These benchmarks evaluate various capabilities and provide an
    overall view of LLM performance.'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLM通过标准化基准如GLUE、SuperGLUE、HellaSwag、TruthfulQA和MMLU进行评估（参见表[7.1](#Ch7.T1 "表7.1
    ‣ 7.7 基准测试微调后的LLMs ‣ 第7章 第5阶段：评估与验证 ‣ 微调LLMs的终极指南：从基础到突破的详尽技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")）。这些基准评估各种能力，并提供LLM性能的总体视图。
- en: '| Benchmark | Description | Reference URL |'
  id: totrans-1287
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 描述 | 参考网址 |'
- en: '| --- | --- | --- |'
  id: totrans-1288
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GLUE | Provides a standardised set of diverse NLP tasks to evaluate the effectiveness
    of different language models | [Source](https://gluebenchmark.com/) |'
  id: totrans-1289
  prefs: []
  type: TYPE_TB
  zh: '| GLUE | 提供一套标准化的多样化NLP任务，以评估不同语言模型的有效性 | [来源](https://gluebenchmark.com/)
    |'
- en: '| SuperGLUE | Compares more challenging and diverse tasks with GLUE, with comprehensive
    human baselines | [Source](https://super.gluebenchmark.com/) |'
  id: totrans-1290
  prefs: []
  type: TYPE_TB
  zh: '| SuperGLUE | 与GLUE相比，比较更具挑战性和多样化的任务，具有全面的人类基线 | [来源](https://super.gluebenchmark.com/)
    |'
- en: '| HellaSwag | Evaluates how well an LLM can complete a sentence | [Source](https://rowanzellers.com/hellaswag/)
    |'
  id: totrans-1291
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | 评估大语言模型完成句子的能力 | [来源](https://rowanzellers.com/hellaswag/) |'
- en: '| TruthfulQA | Measures truthfulness of model responses | [Source](https://github.com/sylinrl/TruthfulQA)
    |'
  id: totrans-1292
  prefs: []
  type: TYPE_TB
  zh: '| TruthfulQA | 测量模型响应的真实性 | [来源](https://github.com/sylinrl/TruthfulQA) |'
- en: '| MMLU | Evaluates how well the LLM can multitask | [Source](https://github.com/hendrycks/test)
    |'
  id: totrans-1293
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 评估大语言模型的多任务处理能力 | [来源](https://github.com/hendrycks/test) |'
- en: '| IFEval | Tests a model’s ability to follow explicit instructions, focusing
    on formatting adherence | [Source](https://github.com/google-research/google-research/tree/master/instruction_following_eval)
    |'
  id: totrans-1294
  prefs: []
  type: TYPE_TB
  zh: '| IFEval | 测试模型按照明确指示的能力，重点在于格式遵守 | [来源](https://github.com/google-research/google-research/tree/master/instruction_following_eval)
    |'
- en: '| BBH (Big Bench Hard) | 23 challenging tasks from the BigBench dataset to
    evaluate LLMs using objective metrics | [Source](https://github.com/suzgunmirac/BIG-Bench-Hard)
    |'
  id: totrans-1295
  prefs: []
  type: TYPE_TB
  zh: '| BBH (Big Bench Hard) | 来自BigBench数据集的23个挑战任务，用于使用客观指标评估大语言模型 | [来源](https://github.com/suzgunmirac/BIG-Bench-Hard)
    |'
- en: '| MATH | Compilation of high-school level competition problems formatted using
    LaTeX and Asymptote | [Source](https://github.com/hendrycks/apps) |'
  id: totrans-1296
  prefs: []
  type: TYPE_TB
  zh: '| MATH | 高中水平竞赛问题的汇编，格式使用LaTeX和Asymptote | [来源](https://github.com/hendrycks/apps)
    |'
- en: '| GPQA | Challenging knowledge dataset with questions crafted by PhD-level
    domain experts | [Source](https://github.com/idavidrein/gpqa) |'
  id: totrans-1297
  prefs: []
  type: TYPE_TB
  zh: '| GPQA | 挑战性的知识数据集，问题由博士级领域专家设计 | [来源](https://github.com/idavidrein/gpqa)
    |'
- en: '| MuSR | Dataset with complex problems requiring models to integrate reasoning
    with long-range context parsing | [Source](https://github.com/Zayne-Sprague/MuSR)
    |'
  id: totrans-1298
  prefs: []
  type: TYPE_TB
  zh: '| MuSR | 一个包含复杂问题的数据集，需要模型将推理与长范围上下文解析相结合 | [来源](https://github.com/Zayne-Sprague/MuSR)
    |'
- en: '| MMLU-PRO | Refined version of MMLU with higher quality and more challenging
    multiple-choice questions | [Source](https://github.com/TIGER-AI-Lab/MMLU-Pro)
    |'
  id: totrans-1299
  prefs: []
  type: TYPE_TB
  zh: '| MMLU-PRO | MMLU的改进版本，提供更高质量和更具挑战性的多项选择题 | [来源](https://github.com/TIGER-AI-Lab/MMLU-Pro)
    |'
- en: '| ARC | Measures machine reasoning with a dataset of grade-school science questions
    | [Source](https://allenai.org/data/arc) |'
  id: totrans-1300
  prefs: []
  type: TYPE_TB
  zh: '| ARC | 使用小学科学问题的数据集测量机器推理能力 | [来源](https://allenai.org/data/arc) |'
- en: '| COQA | A dataset for building conversational question-answering systems |
    [Source](https://stanfordnlp.github.io/coqa/) |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
  zh: '| COQA | 用于构建对话式问答系统的数据集 | [来源](https://stanfordnlp.github.io/coqa/) |'
- en: '| DROP | Evaluates the ability to perform discrete reasoning over paragraphs
    of text | [Source](https://allennlp.org/drop) |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
  zh: '| DROP | 评估在段落文本上进行离散推理的能力 | [来源](https://allennlp.org/drop) |'
- en: '| SQuAD | A reading comprehension dataset for evaluating models’ ability to
    answer questions based on passages of text | [Source](https://rajpurkar.github.io/SQuAD-explorer/)
    |'
  id: totrans-1303
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 一个用于评估模型基于文本段落回答问题能力的阅读理解数据集 | [来源](https://rajpurkar.github.io/SQuAD-explorer/)
    |'
- en: '| TREC | A benchmark for evaluating text retrieval methodologies | [Source](https://trec.nist.gov/)
    |'
  id: totrans-1304
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 评估文本检索方法的基准 | [来源](https://trec.nist.gov/) |'
- en: '| WMT | A dataset and benchmark for evaluating machine translation models |
    [Source](http://www.statmt.org/wmt20/) |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
  zh: '| WMT | 评估机器翻译模型的数据集和基准 | [来源](http://www.statmt.org/wmt20/) |'
- en: '| XNLI | A dataset for evaluating cross-lingual language understanding | [Source](https://cims.nyu.edu/%C2%A0sbowman/xnli/)
    |'
  id: totrans-1306
  prefs: []
  type: TYPE_TB
  zh: '| XNLI | 用于评估跨语言理解的数据集 | [来源](https://cims.nyu.edu/%C2%A0sbowman/xnli/) |'
- en: '| PiQA | A dataset for evaluating models’ understanding of physical interactions
    | [Source](https://yonatanbisk.com/piqa/) |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '| PiQA | 用于评估模型对物理交互理解的数据集 | [来源](https://yonatanbisk.com/piqa/) |'
- en: '| Winogrande | A large-scale dataset for evaluating commonsense reasoning |
    [Source](https://mosaic.allenai.org/projects/winogrande) |'
  id: totrans-1308
  prefs: []
  type: TYPE_TB
  zh: '| Winogrande | 用于评估常识推理的大规模数据集 | [来源](https://mosaic.allenai.org/projects/winogrande)
    |'
- en: 'Table 7.1: Detailed Overview of Benchmark Datasets Used for Evaluating Language
    Model Performance.'
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7.1: 用于评估语言模型性能的基准数据集详细概述。'
- en: As LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging
    current benchmarks and setting new standards in the domain. Given the diverse
    nature of LLMs and the tasks they can perform, the choice of benchmarks depends
    on the specific tasks the LLM is expected to handle. For generic applicability,
    various benchmarks for different downstream applications and reasoning should
    be utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant
    benchmarks like BigCodeBench for coding.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLMs）的发展，基准测试也在不断进化，例如 BigCodeBench 等新标准正在挑战当前的基准并在该领域设立新的标准。鉴于 LLMs
    的多样性及其可以执行的任务，基准测试的选择取决于 LLM 预计要处理的具体任务。对于通用适用性，应利用各种针对不同下游应用和推理的基准测试。对于领域/任务特定的
    LLMs，基准测试可以限定在如 BigCodeBench 这样的相关基准上。
- en: 7.8 Evaluating Fine-Tuned LLMs on Safety Benchmark
  id: totrans-1311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.8 在安全基准上评估微调后的 LLMs
- en: The safety aspects of Large Language Models (LLMs) are increasingly scrutinised
    due to their ability to generate harmful content when influenced by jailbreaking
    prompts. These prompts can bypass the embedded safety and ethical guidelines within
    the models, similar to code injection techniques used in traditional computer
    security to circumvent safety protocols. Notably, models like ChatGPT, GPT-3,
    and InstructGPT are vulnerable to such manipulations that remove content generation
    restrictions, potentially violating OpenAI’s guidelines. This underscores the
    necessity for robust safeguards to ensure LLM outputs adhere to ethical and safety
    standards.
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的安全性方面正受到越来越多的关注，因为这些模型在受到破解提示的影响时可能生成有害内容。这些提示可以绕过模型内嵌的安全和伦理指南，类似于在传统计算机安全中用于规避安全协议的代码注入技术。值得注意的是，像
    ChatGPT、GPT-3 和 InstructGPT 这样的模型容易受到此类操控，这些操控会移除内容生成的限制，可能违反 OpenAI 的指南。这凸显了确保
    LLM 输出符合伦理和安全标准的强大保障措施的必要性。
- en: 'DecodingTrust [[77](#bib.bib77)] provides a comprehensive evaluation of the
    trustworthiness of LLMs, notably comparing GPT-4 with GPT-3.5 (ChatGPT). This
    evaluation spans several critical areas:'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: DecodingTrust [[77](#bib.bib77)] 提供了对 LLMs 可信度的全面评估，特别是将 GPT-4 与 GPT-3.5（ChatGPT）进行了比较。这项评估涵盖了几个关键领域：
- en: '1.'
  id: totrans-1314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Toxicity: Optimisation algorithms and generative models are employed to create
    challenging prompts that test the model’s ability to avoid generating harmful
    content.'
  id: totrans-1315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有害内容：优化算法和生成模型用于创建具有挑战性的提示，以测试模型避免生成有害内容的能力。
- en: '2.'
  id: totrans-1316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Stereotype Bias: An array of demographic groups and stereotype topics are utilised
    to assess model bias, helping to understand and mitigate prejudiced responses.'
  id: totrans-1317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 刻板印象偏见：使用各种人口统计组和刻板印象话题来评估模型的偏见，帮助理解和减少带有偏见的回应。
- en: '3.'
  id: totrans-1318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Adversarial Robustness: The resilience of models against adversarial attacks
    is tested by challenging them with sophisticated algorithms intended to deceive
    or mislead.'
  id: totrans-1319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗鲁棒性：通过使用复杂的算法来测试模型对抗对抗性攻击的能力，这些算法旨在欺骗或误导模型。
- en: '4.'
  id: totrans-1320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability
    to handle inputs that differ significantly from their training data, such as poetic
    or Shakespearean styles.'
  id: totrans-1321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布外（OOD）鲁棒性：评估模型处理与其训练数据显著不同的输入的能力，例如诗意或莎士比亚风格的输入。
- en: '5.'
  id: totrans-1322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Robustness to Adversarial Demonstrations: Demonstrations that contain misleading
    information are used to test the model’s robustness across various tasks.'
  id: totrans-1323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对对抗性展示的鲁棒性：使用包含误导性信息的展示来测试模型在各种任务中的鲁棒性。
- en: '6.'
  id: totrans-1324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Privacy: Various levels of privacy evaluation assess how well models safeguard
    sensitive information during interactions and understand privacy-related contexts.'
  id: totrans-1325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私：评估模型在互动过程中如何保护敏感信息的隐私，并理解与隐私相关的背景。
- en: '7.'
  id: totrans-1326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Hallucination Detection: Identifies instances where the model generates information
    not grounded in the provided context or factual data. Lower hallucination rates
    improve the reliability and trustworthiness of the LLM’s outputs.'
  id: totrans-1327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 幻觉检测：识别模型生成的未基于提供的上下文或事实数据的信息的实例。较低的幻觉率提高了 LLM 输出的可靠性和可信度。
- en: '8.'
  id: totrans-1328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Tone Appropriateness: Assesses whether the model’s output maintains an appropriate
    tone for the given context. This is particularly important for applications in
    customer service, healthcare, and other sensitive areas.'
  id: totrans-1329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语气适当性：评估模型的输出是否在给定上下文中保持适当的语气。这在客户服务、医疗保健和其他敏感领域的应用中特别重要。
- en: '9.'
  id: totrans-1330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Machine Ethics: Ethical assessments involve testing models with scenarios that
    require moral judgments, using datasets like ETHICS and Jiminy Cricket.'
  id: totrans-1331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器伦理：伦理评估包括用需要道德判断的场景测试模型，使用像 ETHICS 和 Jiminy Cricket 这样的数据集。
- en: '10.'
  id: totrans-1332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: 'Fairness: The fairness of models is evaluated by generating tasks that vary
    protected attributes, ensuring equitable responses across different demographic
    groups.'
  id: totrans-1333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 公平性：通过生成涉及受保护属性的任务来评估模型的公平性，确保在不同人口群体中提供公平的回应。
- en: The dataset employed for evaluating the aforementioned eight safety dimensions
    can be found [here](https://github.com/AI-secure/DecodingTrust/tree/main/data).
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估上述八个安全维度的数据集可以在[这里](https://github.com/AI-secure/DecodingTrust/tree/main/data)找到。
- en: In partnership with HuggingFace, the [LLM Safety Leaderboard](https://huggingface.co/blog/leaderboard-decodingtrust)
    utilises DecodingTrust’s framework to provide a unified evaluation platform for
    LLM safety. This allows researchers and practitioners to better understand the
    capabilities, limitations, and risks associated with LLMs. Users are encouraged
    to submit their models to HuggingFace for evaluation, ensuring they meet the evolving
    standards of safety and reliability in the field.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 与 HuggingFace 合作的[LLM 安全排行榜](https://huggingface.co/blog/leaderboard-decodingtrust)利用了
    DecodingTrust 的框架，为 LLM 安全提供了一个统一的评估平台。这使研究人员和从业者能够更好地了解 LLM 的能力、局限性和风险。鼓励用户将他们的模型提交给
    HuggingFace 进行评估，以确保它们符合该领域不断发展的安全和可靠性标准。
- en: 7.9 Evaluating Safety of Fine-Tuned LLM using AI Models
  id: totrans-1336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.9 使用 AI 模型评估微调 LLM 的安全性
- en: 7.9.1 Llama Guard
  id: totrans-1337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.9.1 Llama Guard
- en: 'Llama Guard 2[[78](#bib.bib78)] is a safeguard model built on LLMs for managing
    risks in conversational AI applications. It effectively categorises both input
    prompts and responses from AI agents using a detailed safety risk taxonomy tailored
    to identify potential legal and policy risks in AI interactions. It utilises a
    detailed safety risk taxonomy designed to identify and manage potential legal
    and policy risks in interactions involving conversational AI. This taxonomy enables
    effective classification in areas such as:'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: Llama Guard 2[[78](#bib.bib78)] 是一个建立在 LLM 上的保护模型，用于管理对话 AI 应用中的风险。它使用详细的安全风险分类法有效地对
    AI 代理的输入提示和回应进行分类，以识别潜在的法律和政策风险。这种分类法旨在识别和管理与对话 AI 互动相关的潜在法律和政策风险，能够在以下领域进行有效分类：
- en: •
  id: totrans-1339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Violence & Hate, addressing content that could incite violent acts or discrimination.
  id: totrans-1340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 暴力与仇恨，处理可能煽动暴力行为或歧视的内容。
- en: •
  id: totrans-1341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sexual Content, targeting sexually explicit material or behaviour, especially
    involving minors.
  id: totrans-1342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性内容，针对性别明示的材料或行为，特别是涉及未成年人的。
- en: •
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Guns & Illegal Weapons, concerning the promotion or instruction of illegal armaments.
  id: totrans-1344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 枪支与非法武器，涉及非法武器的推广或指导。
- en: •
  id: totrans-1345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regulated or Controlled Substances, covering illegal drugs and other controlled
    substances.
  id: totrans-1346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 受管制或受控物质，涵盖非法药物及其他受控物质。
- en: •
  id: totrans-1347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Suicide & Self-Harm, aimed at content that could encourage self-destructive
    behaviour.
  id: totrans-1348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自杀与自残，旨在处理可能鼓励自毁行为的内容。
- en: •
  id: totrans-1349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Criminal Planning, for content that could assist in planning or executing criminal
    activities.
  id: totrans-1350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 犯罪计划，针对可能协助计划或执行犯罪活动的内容。
- en: The core of Llama Guard 2 is its robust framework that allows for both prompt
    and response classification, supported by a high-quality dataset that enhances
    its ability to monitor conversational exchanges. Operating on a Llama2-7b model,
    Llama Guard 2 has been instruction-tuned to deliver strong performance on benchmarks
    like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches
    or surpasses the capabilities of existing content moderation tools.
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: Llama Guard 2 的核心是其强大的框架，支持对提示和回应的分类，并通过高质量的数据集提升监控对话交流的能力。Llama Guard 2 基于
    Llama2-7b 模型，经过指令微调，在 OpenAI Moderation Evaluation 数据集和 ToxicChat 等基准测试中表现出色，其能力与现有内容审查工具相匹配或超越。
- en: The model supports multi-class classification and generates binary decision
    scores. Its instruction fine-tuning allows for extensive customisation of tasks
    and adaptation of output formats. This feature enables users to modify taxonomy
    categories to align with specific use cases and supports flexible prompting capabilities,
    including zero-shot and few-shot applications. The adaptability and effectiveness
    of Llama Guard make it a vital resource for developers and researchers. By making
    its model weights publicly available, Llama Guard 2 encourages ongoing development
    and customisation to meet the evolving needs of AI safety within the community.
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型支持多类分类并生成二元决策分数。其指令微调允许广泛定制任务和适应输出格式。此功能使用户能够修改分类类别以与特定用例对齐，并支持灵活的提示功能，包括零-shot
    和少-shot 应用。Llama Guard 的适应性和有效性使其成为开发者和研究人员的重要资源。通过公开模型权重，Llama Guard 2 鼓励持续发展和定制，以满足社区中
    AI 安全的不断变化需求。
- en: 'Llama Guard 3 represents the latest advancement over Llama Guard 2, having
    been fine-tuned on the Llama 3 8b model. The key difference between the two versions
    is that Llama Guard 3 expands upon the capabilities of Llama Guard 2 by introducing
    three new categories: Defamation, Elections, and Code Interpreter Abuse.'
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: Llama Guard 3 是 Llama Guard 2 的最新进展，经过 Llama 3 8b 模型的微调。两个版本之间的主要区别在于 Llama
    Guard 3 在 Llama Guard 2 的基础上引入了三个新类别：诽谤、选举和代码解释器滥用。
- en: 'Python Library: Llama Guard 3 is accessible via HuggingFace’s AutoModelForCausalLM.²²2[https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM)
    A detailed tutorial is available at this [link](https://huggingface.co/meta-llama/Llama-Guard-3-8B).
    Please note that access to the model requires submitting a request to Hugging
    Face with the user details. Additionally, the model weights can be downloaded
    from the Meta platform by providing user details, and the link can be found [here](https://llama.meta.com/llama-downloads).'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: Python 库：Llama Guard 3 可通过 HuggingFace 的 AutoModelForCausalLM 访问。²²2[https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM)
    详细教程可通过此[链接](https://huggingface.co/meta-llama/Llama-Guard-3-8B)获取。请注意，访问模型需要向
    Hugging Face 提交用户详细信息的请求。此外，模型权重可以通过提供用户详细信息从 Meta 平台下载，链接可在[这里](https://llama.meta.com/llama-downloads)找到。
- en: The prompt formats for these two models also differ, with the specific formats
    for Llama Guard 2 available [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-guard-2)
    and Llama Guard 3 is accessible [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama-guard-3).
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型的提示格式也有所不同，Llama Guard 2 的具体格式可在[这里](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-guard-2)查看，Llama
    Guard 3 的格式可在[这里](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama-guard-3)访问。
- en: 7.9.2 Shield Gemma
  id: totrans-1356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.9.2 Shield Gemma
- en: ShieldGemma [[79](#bib.bib79)] is an advanced content moderation model built
    on the Gemma2 platform, designed to enhance the safety and reliability of interactions
    between LLMs and users. It effectively filters both user inputs and model outputs
    to mitigate key harm types, including offensive language, hate speech, misinformation,
    and explicit content. The model’s scalability, with options ranging from 2B to
    27B parameters, allows for tailored applications that meet specific needs, such
    as reducing latency in online safety applications or enhancing performance in
    complex decision-making tasks.
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: ShieldGemma [[79](#bib.bib79)] 是一个基于 Gemma2 平台的高级内容审查模型，旨在提升 LLM 和用户之间互动的安全性和可靠性。它有效地过滤用户输入和模型输出，以减少关键危害类型，包括冒犯性语言、仇恨言论、虚假信息和色情内容。该模型的可扩展性，从
    2B 到 27B 参数选项，允许针对特定需求进行定制应用，例如减少在线安全应用中的延迟或提升复杂决策任务中的性能。
- en: A distinguishing feature of ShieldGemma is its novel approach to data curation.
    It leverages synthetic data generation techniques to create high-quality datasets
    that are robust against adversarial prompts and fair across diverse identity groups.
    This reduces the need for extensive human annotation, streamlining the data preparation
    process while ensuring the model’s effectiveness. Compared to existing content
    moderation tools like LlamaGuard and WildGuard, which typically offer fixed-size
    models and limited customisation, ShieldGemma’s flexible architecture and advanced
    data handling capabilities provide a more adaptable and efficient solution. These
    innovations position ShieldGemma as a significant advancement in LLM-based content
    moderation, offering developers and researchers a versatile tool that promotes
    safer and more reliable AI interactions across various platforms.
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: ShieldGemma 的一个显著特点是其新颖的数据策展方法。它利用合成数据生成技术创建高质量数据集，这些数据集对对抗性提示具有鲁棒性，并且在各种身份群体中都很公平。这减少了对大量人工注释的需求，简化了数据准备过程，同时确保模型的有效性。与现有的内容审核工具如
    LlamaGuard 和 WildGuard 相比，ShieldGemma 的灵活架构和先进的数据处理能力提供了更适应和高效的解决方案。这些创新使 ShieldGemma
    成为基于 LLM 的内容审核的重要进展，为开发者和研究人员提供了一种多功能工具，促进了在各种平台上更安全、更可靠的 AI 互动。
- en: 'Python Library: The ShieldGemma series is available on HuggingFace via AutoModelForCausalLM.
    The models can be accessed [here](https://huggingface.co/google). A tutorial for
    running ShieldGemma 2B on Google Colab can be found [here](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/responsible/docs/safeguards/shieldgemma_on_keras.ipynb).
    Similar to Llama Guard series, ShieldGemma series also has guidelines for prompting
    and it can be found [here](https://ai.google.dev/gemma/docs/shieldgemma/model_card).'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: Python 库：ShieldGemma 系列可通过 AutoModelForCausalLM 在 HuggingFace 上获得。模型可以在[这里](https://huggingface.co/google)访问。关于如何在
    Google Colab 上运行 ShieldGemma 2B 的教程可以在[这里](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/responsible/docs/safeguards/shieldgemma_on_keras.ipynb)找到。类似于
    Llama Guard 系列，ShieldGemma 系列也有提示指导，具体内容可以在[这里](https://ai.google.dev/gemma/docs/shieldgemma/model_card)找到。
- en: 7.9.3 WILDGUARD
  id: totrans-1360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.9.3 WILDGUARD
- en: 'WILDGUARD [[80](#bib.bib80)] is an innovative open-source tool developed to
    enhance the safety of interactions with large language models (LLMs). This tool
    addresses three critical moderation tasks: detecting harmful intent in user prompts,
    identifying safety risks in model responses, and determining when a model appropriately
    refuses unsafe requests. Central to its development is WILDGUARD MIX³³3[https://huggingface.co/datasets/allenai/wildguardmix](https://huggingface.co/datasets/allenai/wildguardmix),
    a meticulously curated dataset comprising 92,000 labelled examples that include
    both benign prompts and adversarial attempts to bypass safety measures. The dataset
    is divided into WILDGUARD TRAIN, used for training the model, and WILDGUARD TEST,
    consisting of high-quality human-annotated examples for evaluation.'
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: WILDGUARD [[80](#bib.bib80)] 是一款创新的开源工具，旨在提高与大型语言模型（LLM）交互的安全性。该工具解决了三个关键的审核任务：检测用户提示中的有害意图，识别模型响应中的安全风险，以及确定模型何时适当地拒绝不安全的请求。其开发的核心是
    WILDGUARD MIX³³3[https://huggingface.co/datasets/allenai/wildguardmix](https://huggingface.co/datasets/allenai/wildguardmix)，这是一个经过精心策划的数据集，包括
    92,000 个标记示例，其中包括无害提示和试图绕过安全措施的对抗性尝试。数据集分为 WILDGUARD TRAIN，用于训练模型，以及 WILDGUARD
    TEST，由高质量人工注释示例组成，用于评估。
- en: The WILDGUARD model itself is fine-tuned on the Mistral-7B language model using
    the WILDGUARD TRAIN dataset, enabling it to perform all three moderation tasks
    in a unified, multi-task manner. Results show that WILDGUARD surpasses existing
    open-source moderation tools in effectiveness, particularly excelling in handling
    adversarial prompts and accurately detecting model refusals. On many benchmarks,
    WILDGUARD’s performance is on par with or exceeds that of GPT-4, a much larger,
    closed-source model.
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: WILDGUARD 模型本身在 Mistral-7B 语言模型上进行了微调，使用 WILDGUARD TRAIN 数据集，使其能够以统一的多任务方式执行所有三个审核任务。结果表明，WILDGUARD
    在有效性方面超越了现有的开源审核工具，尤其是在处理对抗性提示和准确检测模型拒绝方面表现出色。在许多基准测试中，WILDGUARD 的表现与 GPT-4 这个更大、封闭源模型的表现相当或更好。
- en: The quick start guide and additional information on WILDGUARD are available
    in GitHub and it can be accessed [here](https://github.com/allenai/wildguard).
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 WILDGUARD 的快速入门指南和附加信息可以在 GitHub 上找到，访问[这里](https://github.com/allenai/wildguard)。
- en: 'Chapter 8 Stage 6: Deployment'
  id: totrans-1364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 8 章 第 6 阶段：部署
- en: 8.1 Steps Involved in Deploying the Fine-Tuned Model
  id: totrans-1365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 微调模型部署的步骤
- en: '1.'
  id: totrans-1366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Model Export: Save the fine-tuned model in a suitable format (e.g., ONNX, TensorFlow
    SavedModel, PyTorch) for deployment.'
  id: totrans-1367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型导出：将微调后的模型保存为适合部署的格式（例如 ONNX、TensorFlow SavedModel、PyTorch）。
- en: '2.'
  id: totrans-1368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Infrastructure Setup: Prepare the deployment environment, including necessary
    hardware, cloud services, and containerisation tools.'
  id: totrans-1369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础设施设置：准备部署环境，包括必要的硬件、云服务和容器化工具。
- en: '3.'
  id: totrans-1370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'API Development: Create APIs to allow applications to interact with the model,
    facilitating prediction requests and responses.'
  id: totrans-1371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: API 开发：创建 API 以允许应用程序与模型交互，便于预测请求和响应。
- en: '4.'
  id: totrans-1372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Deployment: Deploy the model to the production environment, making it accessible
    to end-users or applications.'
  id: totrans-1373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署：将模型部署到生产环境中，使其可以被最终用户或应用程序访问。
- en: 8.2 Cloud-Based Providers for LLM Deployment
  id: totrans-1374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 LLM 部署的云服务提供商
- en: Cloud-based large language model (LLM) inferencing frequently employs a pricing
    model based on the number of tokens processed. Users are charged according to
    the volume of text analysed or generated by the model. While this pricing structure
    can be cost-effective for sporadic or small-scale usage, it may not always be
    economical for larger or continuous workloads.
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的大型语言模型（LLM）推理通常采用基于处理的令牌数量的定价模型。用户根据模型分析或生成的文本量收费。虽然这种定价结构对于偶尔或小规模使用可能具有成本效益，但对于更大或持续的工作负载，它可能不总是经济的。
- en: In some scenarios, hosting an LLM solution in-house may offer better long-term
    cost savings, especially if there is consistent or high-volume usage. Managing
    your own infrastructure provides greater control over resource allocation and
    allows for cost optimisation based on specific needs. Additionally, self-hosting
    offers advantages in terms of data privacy and security, as sensitive information
    remains within your own environment.
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，自行托管 LLM 解决方案可能提供更好的长期成本节省，特别是当存在持续或高量使用时。管理自己的基础设施提供了对资源分配的更大控制，并允许根据具体需求进行成本优化。此外，自托管在数据隐私和安全性方面具有优势，因为敏感信息仍然保留在自己的环境中。
- en: However, it is crucial to carefully evaluate the total cost of ownership when
    comparing cloud-based solutions with self-hosted alternatives. This evaluation
    should consider factors such as hardware expenses, maintenance, and operational
    overheads. Ultimately, the decision should be informed by a comprehensive cost-benefit
    analysis, considering both short-term affordability and long-term sustainability.
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在比较基于云的解决方案与自托管替代方案时，至关重要的是仔细评估总体拥有成本。这种评估应考虑硬件费用、维护和运营开销等因素。*最终*，决策应基于全面的成本效益分析，考虑短期可承受性和长期可持续性。
- en: 'Several companies offer deployment services for large language models (LLMs),
    providing a range of tools and platforms to efficiently implement and manage these
    models. Here’s a detailed list of some prominent providers and their services:'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: 多家公司提供大型语言模型（LLM）的部署服务，提供各种工具和平台以有效实施和管理这些模型。以下是一些知名提供商及其服务的详细列表：
- en: •
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Amazon Web Services (AWS)
  id: totrans-1380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 亚马逊网络服务（AWS）
- en: –
  id: totrans-1381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Amazon Bedrock: This service offers a suite of foundation models including
    Amazon Titan, which supports various NLP tasks such as summarisation and text
    generation. Bedrock integrates seamlessly with other AWS services for scalable
    and secure deployment.'
  id: totrans-1382
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Amazon Bedrock：此服务提供了一系列基础模型，包括支持各种 NLP 任务的 Amazon Titan，如总结和文本生成。Bedrock 与其他
    AWS 服务无缝集成，实现可扩展和安全的部署。
- en: –
  id: totrans-1383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Amazon SageMaker: Provides an end-to-end machine learning service that includes
    tools for building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained
    models and step-by-step guides to simplify the deployment process.'
  id: totrans-1384
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Amazon SageMaker：提供端到端的机器学习服务，包括用于构建、训练和部署 LLM 的工具。SageMaker JumpStart 提供预训练模型和逐步指南，以简化部署过程。
- en: –
  id: totrans-1385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Tutorial:  [This tutorial](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-deploy.html)
    explains the deployment of LLM Agents on Amazon Bedrock. Another [tutorial](https://aws.amazon.com/blogs/machine-learning/fine-tune-and-deploy-language-models-with-amazon-sagemaker-canvas-and-amazon-bedrock/)
    explains end-to-end fine-tuning and deployment of LLMs with Sagemaker Canvas and
    Amazon Bedrock. [General guidelines of Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html)
    for LLM users can be found here.'
  id: totrans-1386
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教程： [此教程](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-deploy.html)
    讲解了在 Amazon Bedrock 上部署 LLM Agents 的过程。另一个 [教程](https://aws.amazon.com/blogs/machine-learning/fine-tune-and-deploy-language-models-with-amazon-sagemaker-canvas-and-amazon-bedrock/)
    讲解了使用 Sagemaker Canvas 和 Amazon Bedrock 进行端到端的 LLM 微调和部署。关于 LLM 用户的 [Amazon Bedrock
    一般指南](https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html)
    可以在这里找到。
- en: •
  id: totrans-1387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Microsoft Azure
  id: totrans-1388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Microsoft Azure
- en: –
  id: totrans-1389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Azure OpenAI Service: This service offers access to OpenAI’s powerful models
    like GPT-3.5 and Codex. It provides capabilities for embedding, image generation
    with DALL-E, and speech-to-text with Whisper. Azure’s integration with OpenAI
    models ensures robust deployment options for various applications.'
  id: totrans-1390
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务：该服务提供对 OpenAI 强大模型的访问，如 GPT-3.5 和 Codex。它提供嵌入、使用 DALL-E 生成图像和使用
    Whisper 进行语音转文本的功能。Azure 与 OpenAI 模型的集成确保了各种应用的强大部署选项。
- en: –
  id: totrans-1391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Azure Machine Learning: Supports the deployment of custom and pre-trained models,
    offering tools for model management, deployment, and monitoring. It integrates
    with Azure’s broader ecosystem for scalable and secure ML operations.'
  id: totrans-1392
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Azure 机器学习：支持自定义和预训练模型的部署，提供模型管理、部署和监控的工具。它与 Azure 的更广泛生态系统集成，支持可扩展且安全的机器学习操作。
- en: –
  id: totrans-1393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Tutorial:  [Here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal)
    is the tutorial for creating and deploying an Azure OpenAI Service in Microsoft
    Azure platform.'
  id: totrans-1394
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教程： [这里](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal)
    是关于在 Microsoft Azure 平台上创建和部署 Azure OpenAI 服务的教程。
- en: •
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Google Cloud Platform (GCP)
  id: totrans-1396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Google Cloud Platform (GCP)
- en: –
  id: totrans-1397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Vertex AI: This platform allows the deployment of large language models with
    tools for training, tuning, and serving models. Vertex AI supports models like
    BERT and GPT-3, providing extensive MLOps capabilities for end-to-end management.'
  id: totrans-1398
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Vertex AI：该平台允许部署大型语言模型，提供训练、调整和服务模型的工具。Vertex AI 支持如 BERT 和 GPT-3 等模型，提供广泛的
    MLOps 能力以支持端到端的管理。
- en: –
  id: totrans-1399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Cloud AI API: Offers APIs for NLP tasks such as translation, sentiment analysis,
    and entity recognition. These APIs are backed by Google’s powerful infrastructure,
    ensuring high performance and reliability.'
  id: totrans-1400
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 云 AI API：提供用于 NLP 任务的 API，如翻译、情感分析和实体识别。这些 API 得到了 Google 强大基础设施的支持，确保了高性能和可靠性。
- en: –
  id: totrans-1401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Tutorial:  [This document](https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction/train-and-deploy-model)
    contains a tutorial for training and deploying an LLM in GCP.'
  id: totrans-1402
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教程： [此文档](https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction/train-and-deploy-model)
    包含一个关于在 GCP 中训练和部署 LLM 的教程。
- en: •
  id: totrans-1403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hugging Face
  id: totrans-1404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hugging Face
- en: –
  id: totrans-1405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Inference API: This service allows users to deploy and manage LLMs hosted on
    Hugging Face’s infrastructure. It supports various models from the Transformers
    library and provides an easy-to-use API for integrating these models into applications.'
  id: totrans-1406
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理 API：该服务允许用户部署和管理托管在 Hugging Face 基础设施上的 LLM。它支持来自 Transformers 库的各种模型，并提供易于使用的
    API，以将这些模型集成到应用程序中。
- en: –
  id: totrans-1407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Spaces: A collaborative environment where users can deploy and share models
    using Hugging Face’s hosting platform. It supports deploying custom models and
    interactive demos.'
  id: totrans-1408
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Spaces：一个协作环境，用户可以在 Hugging Face 的托管平台上部署和共享模型。它支持部署自定义模型和互动演示。
- en: –
  id: totrans-1409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Tutorial:  [This document](https://huggingface.co/blog/inference-endpoints-llm)
    contains a tutorial for training and deploying an LLM using HuggingFace Inference
    API.'
  id: totrans-1410
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教程： [此文档](https://huggingface.co/blog/inference-endpoints-llm) 包含一个关于使用 HuggingFace
    推理 API 训练和部署 LLM 的教程。
- en: •
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Other Platforms
  id: totrans-1412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他平台
- en: –
  id: totrans-1413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'OpenLLM: Provides deployment solutions [here](https://github.com/bentoml/OpenLLM?ref=content.whylabs.ai).'
  id: totrans-1414
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenLLM：提供部署解决方案 [在这里](https://github.com/bentoml/OpenLLM?ref=content.whylabs.ai)。
- en: –
  id: totrans-1415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Deepseed: Offers deployment solutions [here](https://github.com/microsoft/DeepSpeed?ref=content.whylabs.ai).'
  id: totrans-1416
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Deepseed：提供部署解决方案 [在这里](https://github.com/microsoft/DeepSpeed?ref=content.whylabs.ai)。
- en: 8.3 Techniques for Optimising Model Performance During Inference
  id: totrans-1417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 推理过程中优化模型性能的技术
- en: Optimising model performance during inference is crucial for the efficient deployment
    of large language models (LLMs). The following advanced techniques offer various
    strategies to enhance performance, reduce latency, and manage computational resources
    effectively.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中优化模型性能对于大语言模型（LLMs）的高效部署至关重要。以下先进技术提供了各种策略，以提升性能、减少延迟，并有效管理计算资源。
- en: 8.3.1 Traditional On-Premises GPU-Based Deployments
  id: totrans-1419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1 传统的本地 GPU 部署
- en: 'This conventional approach to deploying large language models (LLMs) involves
    using Graphics Processing Units (GPUs) due to their parallel processing capabilities,
    which enable fast and efficient inference. However, this method requires upfront
    hardware investment and may not be suitable for applications with fluctuating
    demand or limited budgets. GPU-based deployments face several challenges:'
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: 这种传统的大语言模型（LLMs）部署方法涉及使用图形处理单元（GPUs），由于其并行处理能力，使得推理过程快速而高效。然而，这种方法需要前期硬件投资，并且可能不适合需求波动或预算有限的应用。基于
    GPU 的部署面临一些挑战：
- en: '1.'
  id: totrans-1421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Resource utilisation may suffer during periods of low demand due to idle servers.
  id: totrans-1422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在低需求期间，资源利用率可能会受到闲置服务器的影响。
- en: '2.'
  id: totrans-1423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Scaling up or down often requires physical hardware modifications, which can
    be time-consuming.
  id: totrans-1424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扩展或缩减通常需要物理硬件修改，这可能耗时。
- en: '3.'
  id: totrans-1425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Centralised servers can introduce single points of failure and scalability limitations.
  id: totrans-1426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集中式服务器可能会引入单点故障和可扩展性限制。
- en: To mitigate these issues, strategies such as load balancing between multiple
    GPUs, fallback routing, model parallelism, and data parallelism can be employed
    to achieve better results. Optimisation techniques like distributed inference
    using PartialState from accelerate can further enhance efficiency.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这些问题，可以采用诸如多 GPU 之间的负载均衡、备用路由、模型并行和数据并行等策略，以获得更好的结果。像 accelerate 中的 PartialState
    这样的优化技术可以进一步提高效率。
- en: 'Example use case: Large-Scale NLP Application'
  id: totrans-1428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例用例：大规模 NLP 应用
- en: For instance, a large e-commerce platform implemented traditional on-premises
    GPU-based deployment to handle millions of customer queries daily. By utilising
    load balancing and model parallelism, they were able to achieve a significant
    reduction in latency and improved customer satisfaction.
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个大型电子商务平台采用了传统的本地 GPU 部署来处理每天数百万个客户查询。通过利用负载均衡和模型并行，他们能够显著减少延迟并提高客户满意度。
- en: '8.3.2 Distributed LLM: Torrent-Style Deployment and Parallel Forward Passes'
  id: totrans-1430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2 分布式 LLM：类似于 Torrent 的部署和并行前向传递
- en: An innovative deployment strategy for large language models (LLMs) involves
    distributing them across multiple GPUs in a decentralised, torrent-style manner.
    Libraries like Petals¹¹1[https://github.com/bigscience-workshop/petals](https://github.com/bigscience-workshop/petals)
    can perform this task. Petals functions as a decentralised pipeline designed for
    rapid neural network inference by partitioning the model into distinct blocks
    or layers, which are distributed across multiple geographically dispersed servers.
    Users can connect their own GPUs to this network, acting as both contributors
    and clients who can access and apply the model to their data.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 一种创新的大语言模型（LLMs）部署策略涉及将它们以去中心化、类似 Torrent 的方式分布在多个 GPU 上。像 Petals¹¹1[https://github.com/bigscience-workshop/petals](https://github.com/bigscience-workshop/petals)
    这样的库可以执行此任务。Petals 作为一个去中心化的管道，通过将模型分割成不同的块或层，并将其分布在多个地理位置分散的服务器上，旨在快速进行神经网络推理。用户可以将自己的
    GPU 连接到这个网络，既作为贡献者又作为客户端，能够访问和应用模型到他们的数据上。
- en: When a client request is received, the network routes it through a series of
    servers optimised to minimise the total forward pass time. Each server dynamically
    selects the most optimal set of blocks, adapting to the current bottlenecks in
    the pipeline. This framework leverages decentralisation principles to distribute
    computational load across diverse regions, sharing computational resources and
    GPUs in a way that reduces the financial burden on individual organisations. This
    collaborative approach not only optimises resource utilisation but also fosters
    a global community dedicated to shared AI goals.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 当收到客户请求时，网络通过一系列优化以最小化总前向传递时间的服务器来路由请求。每个服务器动态选择最优的块集，适应当前管道中的瓶颈。该框架利用去中心化原则，将计算负载分布在不同区域，共享计算资源和
    GPU，从而减少对单个组织的财务负担。这种协作方法不仅优化了资源利用，还促进了致力于共享 AI 目标的全球社区。
- en: '![Refer to caption](img/9846676a869631c0750b0b245f6accd8.png)'
  id: totrans-1433
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/9846676a869631c0750b0b245f6accd8.png)'
- en: 'Figure 8.1: Conceptual Representation of Distributed LLM Deployment Using a
    Torrent-Style Approach. This figure illustrates the distributed deployment of
    a Large Language Model (LLM) using a torrent-style approach, where multiple GPT
    model layers (stacks) are distributed across different nodes (represented by chefs)
    and perform parallel forward passes. The process mimics the flow of orders from
    customers (input data) through restaurants (intermediate processing layers) to
    chefs (model layers), highlighting the efficiency of parallel processing and distributed
    computing in handling large-scale language models. This approach is essential
    for reducing inference latency and improving the scalability of LLMs across diverse
    computational environments. (adapted from [[81](#bib.bib81)])'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：使用类似 torrent 的方法分布式部署 LLM 的概念表示。该图展示了使用类似 torrent 的方法进行大型语言模型（LLM）的分布式部署，其中多个
    GPT 模型层（堆栈）分布在不同节点（由厨师表示）并执行并行前向传递。这个过程模拟了从顾客（输入数据）通过餐馆（中间处理层）到厨师（模型层）的订单流，突出了并行处理和分布式计算在处理大规模语言模型中的效率。这种方法对于减少推理延迟和提高
    LLM 在多样计算环境中的可扩展性至关重要。（改编自 [[81](#bib.bib81)]）
- en: 'Example use case: Global Research Collaboration'
  id: totrans-1435
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例用例：全球研究协作
- en: A consortium of research institutions implemented a distributed LLM using the
    Petals framework to analyse large datasets across different continents. By leveraging
    the decentralised nature of Petals, they achieved high efficiency in processing
    and collaborative model development.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由研究机构组成的联盟利用 Petals 框架实现了分布式 LLM，用于分析跨不同大陆的大型数据集。通过利用 Petals 的去中心化特性，他们在处理和协作模型开发中实现了高效率。
- en: 8.3.3 WebGPU-Based Deployment of LLM
  id: totrans-1437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.3 基于 WebGPU 的大语言模型（LLM）部署
- en: This deployment option for large language models (LLMs) involves utilising WebGPU,
    a web standard that provides a low-level interface for graphics and compute applications
    on the web platform. With WebGPU, organisations can harness the power of GPUs
    directly within web browsers, enabling efficient inference for LLMs in web-based
    applications. WebGPU enables high-performance computing and graphics rendering
    directly within the client’s web browser. It allows developers to utilise the
    client’s GPU for tasks such as rendering graphics, accelerating computational
    workloads, and performing parallel processing, all without the need for plugins
    or additional software installations. This capability permits complex computations
    to be executed efficiently on the client’s device, leading to faster and more
    responsive web applications.
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: 这种大语言模型（LLM）的部署选项涉及利用 WebGPU，一个为 web 平台上的图形和计算应用提供低级接口的网络标准。通过 WebGPU，组织可以直接在网页浏览器中利用
    GPU 的强大性能，使得 LLM 在基于 web 的应用程序中进行高效推理。WebGPU 使得高性能计算和图形渲染可以直接在客户端的网页浏览器中进行。它允许开发者利用客户端的
    GPU 执行如渲染图形、加速计算负载和执行并行处理等任务，而无需插件或额外的软件安装。这种能力允许复杂计算在客户端设备上高效执行，从而实现更快、更响应的 web
    应用程序。
- en: 8.3.4 LLM on WebGPU using WebLLM
  id: totrans-1439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.4 使用 WebLLM 在 WebGPU 上的 LLM
- en: Clients can access powerful large language models and chatbots directly in their
    browser, leveraging WebGPU acceleration. This approach eliminates server dependencies,
    providing users with exceptional performance and enhanced privacy. WebLLM facilitates
    the use of large language models directly in the client’s browser to perform tasks
    such as filtering out personally identifiable information (PII) or named entity
    recognition (NER) on data without transmitting it over the network. This ensures
    enhanced privacy and security by retaining sensitive information on the client
    side.
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以直接在浏览器中访问强大的大型语言模型和聊天机器人，利用 WebGPU 加速。这种方法消除了服务器依赖，为用户提供了卓越的性能和增强的隐私保护。WebLLM
    使得可以在客户端浏览器中直接使用大型语言模型，执行如过滤个人可识别信息（PII）或命名实体识别（NER）等任务，而无需通过网络传输数据。这通过将敏感信息保留在客户端来确保增强的隐私和安全性。
- en: '![Refer to caption](img/5e6827762f1e3d15e4074b74a2d129eb.png)'
  id: totrans-1441
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/5e6827762f1e3d15e4074b74a2d129eb.png)'
- en: 'Figure 8.2: WebGPU-Based Deployment of LLM: This diagram illustrates the architecture
    of deploying a large language model (LLM) using WebGPU technology. The CPU manages
    the distribution of prompt inferencing tasks to multiple GPUs, which then process
    these prompts in parallel, enhancing efficiency and scalability in LLM deployment
    across web-based platforms. (adapted from [[81](#bib.bib81)])'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：基于 WebGPU 的 LLM 部署：该图示例了使用 WebGPU 技术部署大型语言模型（LLM）的架构。CPU 管理将提示推理任务分配给多个
    GPU，这些 GPU 然后并行处理这些提示，提高了 LLM 在基于 Web 的平台上的效率和可扩展性。（改编自 [[81](#bib.bib81)])
- en: Additional Use Cases for WebLLM
  id: totrans-1443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: WebLLM 的其他用例
- en: '1.'
  id: totrans-1444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Language Translation: Enable real-time translation of text directly in the
    browser, allowing users to communicate across language barriers without transmitting
    their messages over the network.'
  id: totrans-1445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言翻译：实现浏览器中实时文本翻译，使用户能够跨越语言障碍进行沟通，而无需将信息传输到网络上。
- en: '2.'
  id: totrans-1446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Code Autocompletion: Develop code editors that provide intelligent autocompletion
    suggestions based on context, leveraging WebLLM to understand and predict code
    snippets.'
  id: totrans-1447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码自动补全：开发代码编辑器，提供基于上下文的智能自动补全建议，利用 WebLLM 理解和预测代码片段。
- en: '3.'
  id: totrans-1448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Customer Support Chatbots: Implement chatbots on websites to provide instant
    customer support and answer frequently asked questions without relying on external
    servers.'
  id: totrans-1449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 客户支持聊天机器人：在网站上实现聊天机器人，以提供即时客户支持并回答常见问题，而无需依赖外部服务器。
- en: '4.'
  id: totrans-1450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Data Analysis and Visualisation: Create browser-based tools for analysing and
    visualising data, with WebLLM assisting in data processing, interpretation, and
    generating insights.'
  id: totrans-1451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据分析与可视化：创建浏览器基础的数据分析和可视化工具，WebLLM 协助数据处理、解读和生成见解。
- en: '5.'
  id: totrans-1452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Personalised Recommendations: Develop recommendation engines that offer personalised
    product recommendations, content suggestions, or movie/music recommendations based
    on user preferences and behaviour.'
  id: totrans-1453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 个性化推荐：开发推荐引擎，根据用户的偏好和行为提供个性化的产品推荐、内容建议或电影/音乐推荐。
- en: '6.'
  id: totrans-1454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Privacy-Preserving Analytics: Develop analytics platforms that perform data
    analysis directly in the browser, ensuring that sensitive information remains
    on the client side and reducing the risk of data breaches.'
  id: totrans-1455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私保护分析：开发直接在浏览器中执行数据分析的分析平台，确保敏感信息保持在客户端，减少数据泄露的风险。
- en: 'Example use case: Privacy-Focused Web Application'
  id: totrans-1456
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例用例：隐私保护的 Web 应用程序
- en: A healthcare startup deployed an LLM using WebLLM to process patient information
    directly within the browser, ensuring data privacy and compliance with healthcare
    regulations. This approach significantly reduced the risk of data breaches and
    improved user trust.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 一家医疗保健初创公司使用 WebLLM 部署了一个 LLM 以直接在浏览器中处理患者信息，从而确保数据隐私和符合医疗保健法规。这种方法显著降低了数据泄露的风险，并提高了用户信任。
- en: 8.3.5 Quantised LLMs
  id: totrans-1458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.5 量化 LLMs
- en: Model quantisation is a technique utilised to reduce the size of an AI model
    by representing its parameters with fewer bits. In traditional machine learning
    models, each parameter (e.g., weights and biases in neural networks) is typically
    stored as a 32-bit floating-point number, necessitating significant memory and
    computational resources, particularly for large models. Quantisation aims to alleviate
    this by reducing the precision of these parameters. For instance, instead of storing
    each parameter as a 32-bit floating-point number, they may be represented using
    fewer bits, such as 8-bit integers. This compression reduces the memory footprint
    of the model, making it more efficient to deploy and execute, especially in resource-constrained
    environments like mobile devices or edge devices. QLoRA is a popular example of
    this quantisation for LLMs and can be used to deploy LLMs locally or host them
    on external servers.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化是一种通过使用更少的位数表示参数来减少 AI 模型大小的技术。在传统的机器学习模型中，每个参数（例如神经网络中的权重和偏置）通常以 32 位浮点数存储，需要大量的内存和计算资源，尤其对于大型模型。量化旨在通过降低这些参数的精度来缓解这一问题。例如，代替将每个参数存储为
    32 位浮点数，可以使用更少的位数表示，例如 8 位整数。这种压缩减少了模型的内存占用，使其在资源受限的环境（如移动设备或边缘设备）中更高效地部署和执行。QLoRA
    是一种流行的量化示例，用于 LLMs，并可用于本地部署 LLMs 或将其托管在外部服务器上。
- en: 'Example use case: Edge Device Deployment'
  id: totrans-1460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例用例：边缘设备部署
- en: A tech company used quantised LLMs to deploy advanced NLP models on mobile devices,
    enabling offline functionality for applications such as voice recognition and
    translation. This deployment significantly improved app performance and user experience
    by reducing latency and reliance on internet connectivity.
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: 一家科技公司使用量化 LLMs 在移动设备上部署先进的 NLP 模型，使得语音识别和翻译等应用可以离线功能。此部署通过减少延迟和对互联网连接的依赖显著提高了应用性能和用户体验。
- en: 8.3.6 vLLMs
  id: totrans-1462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.6 vLLMs
- en: The vLLM²²2[https://docs.vllm.ai/en/stable/](https://docs.vllm.ai/en/stable/)
    system efficiently handles requests by employing a block-level memory management
    method and preemptive request scheduling. It utilises the PagedAttention[[82](#bib.bib82)]
    algorithm to manage the key-value (KV) cache, thereby reducing memory waste and
    fragmentation. By batching requests and sharing physical blocks across multiple
    samples, vLLM optimises memory usage and enhances throughput. Performance tests
    indicate that vLLM surpasses other systems in various decoding scenarios. Consider
    a transformer-based model tasked with summarising a lengthy book. Traditional
    transformers process the entire book simultaneously, which can be both computationally
    and memory-intensive, especially for extended texts. With PagedAttention, the
    book is divided into smaller segments or pages. The model then focuses on summarising
    one page at a time, rather than the entire book simultaneously. This approach
    reduces computational complexity and memory requirements, making it more feasible
    to process and summarise lengthy texts efficiently.
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM²²2[https://docs.vllm.ai/en/stable/](https://docs.vllm.ai/en/stable/) 系统通过采用块级内存管理方法和预防性请求调度高效地处理请求。它利用
    PagedAttention[[82](#bib.bib82)] 算法来管理键值（KV）缓存，从而减少内存浪费和碎片化。通过批量请求和在多个样本之间共享物理块，vLLM
    优化了内存使用并提高了吞吐量。性能测试表明，vLLM 在各种解码场景中超越了其他系统。考虑一个基于 Transformer 的模型任务是总结一本冗长的书。传统的
    Transformer 同时处理整本书，这在计算和内存上可能非常耗费资源，尤其对于较长的文本。使用 PagedAttention 时，书籍被分成更小的段落或页面。模型一次只关注总结一页，而不是同时处理整本书。这种方法减少了计算复杂性和内存需求，使得处理和总结冗长文本更加高效。
- en: 'Example use case: High-Volume Content Generation'
  id: totrans-1464
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例用例：大规模内容生成
- en: A content marketing agency implemented vLLMs for generating large volumes of
    SEO-optimised content. By leveraging the efficient memory management of vLLMs,
    they were able to handle multiple concurrent requests, significantly increasing
    their content production rate while maintaining high quality.
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 一家内容营销机构实现了 vLLMs 以生成大量的 SEO 优化内容。通过利用 vLLMs 高效的内存管理，他们能够处理多个并发请求，显著提高了内容生产率，同时保持了高质量。
- en: 8.4 Key Considerations for Deployment of LLMs
  id: totrans-1466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 LLM 部署的关键考虑因素
- en: 'Deploying large language models (LLMs) effectively requires careful planning
    and consideration of various factors to ensure optimal performance, cost-efficiency,
    and security. Key considerations include:'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 有效部署大型语言模型（LLMs）需要仔细规划并考虑各种因素，以确保性能、成本效益和安全性的最佳化。关键考虑因素包括：
- en: •
  id: totrans-1468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Infrastructure Requirements:'
  id: totrans-1469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础设施要求：
- en: –
  id: totrans-1470
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Compute Resources: Ensure adequate CPU/GPU resources to handle the model’s
    computational demands. High-performance GPUs are typically required for efficient
    inference and training.'
  id: totrans-1471
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算资源：确保有足够的 CPU/GPU 资源来处理模型的计算需求。高性能 GPU 通常是高效推理和训练所必需的。
- en: –
  id: totrans-1472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Memory: LLMs, especially those with billions of parameters, require substantial
    memory. Memory management techniques such as quantisation and model parallelism
    can be employed to optimise usage.'
  id: totrans-1473
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存：LLMs，特别是那些具有数十亿参数的，要求大量内存。可以使用量化和模型并行等内存管理技术来优化使用。
- en: •
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scalability:'
  id: totrans-1475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可扩展性：
- en: –
  id: totrans-1476
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Horizontal Scaling: Plan for horizontal scaling to distribute the load across
    multiple servers, which can improve performance and handle increased demand.'
  id: totrans-1477
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 水平扩展：计划水平扩展，以在多个服务器之间分配负载，这可以提高性能并应对增加的需求。
- en: –
  id: totrans-1478
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Load Balancing: Implement load balancing strategies to ensure even distribution
    of requests and prevent any single point of failure.'
  id: totrans-1479
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 负载均衡：实施负载均衡策略，以确保请求的均匀分配，防止出现单点故障。
- en: •
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cost Management:'
  id: totrans-1481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本管理：
- en: –
  id: totrans-1482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Token-based Pricing: Understand the cost implications of token-based pricing
    models offered by cloud providers. This model charges based on the number of tokens
    processed, which can become expensive with high usage.'
  id: totrans-1483
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于令牌的定价：了解云提供商提供的基于令牌的定价模型的成本影响。这种模型根据处理的令牌数量收费，高使用率可能会变得昂贵。
- en: –
  id: totrans-1484
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Self-Hosting: Evaluate the costs and benefits of self-hosting versus cloud
    hosting. Self-hosting might offer long-term savings for consistent, high-volume
    usage but requires significant upfront investment in hardware and ongoing maintenance.'
  id: totrans-1485
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自托管：评估自托管与云托管的成本和收益。自托管可能在长期高频使用中提供节省，但需要在硬件和持续维护上进行重大前期投资。
- en: •
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance Optimisation:'
  id: totrans-1487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能优化：
- en: –
  id: totrans-1488
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Latency: Minimise latency to ensure real-time performance, particularly for
    applications requiring instant responses like chatbots and virtual assistants.'
  id: totrans-1489
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 延迟：最小化延迟以确保实时性能，特别是对于需要即时响应的应用程序，如聊天机器人和虚拟助手。
- en: –
  id: totrans-1490
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Throughput: Maximise throughput to handle a high volume of requests efficiently.
    Techniques like batching and efficient memory management (e.g., PagedAttention)
    can help.'
  id: totrans-1491
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 吞吐量：最大化吞吐量，以高效处理大量请求。诸如批处理和高效内存管理（例如PagedAttention）等技术可以提供帮助。
- en: •
  id: totrans-1492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Security and Privacy:'
  id: totrans-1493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全与隐私：
- en: –
  id: totrans-1494
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Security: Implement robust security measures to protect sensitive data,
    including encryption and secure access controls.'
  id: totrans-1495
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据安全：实施强有力的安全措施来保护敏感数据，包括加密和安全访问控制。
- en: –
  id: totrans-1496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Privacy: Ensure compliance with data privacy regulations by keeping sensitive
    data within your environment if self-hosting, or ensuring cloud providers comply
    with relevant privacy standards.'
  id: totrans-1497
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私：确保遵守数据隐私法规，如果自托管，保持敏感数据在你的环境内，或者确保云服务提供商遵守相关隐私标准。
- en: •
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Maintenance and Updates:'
  id: totrans-1499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 维护与更新：
- en: –
  id: totrans-1500
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Model Updates: Regularly update the model to incorporate new data and improve
    performance. Automate this process if possible to reduce manual effort.'
  id: totrans-1501
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型更新：定期更新模型以融入新数据并提升性能。如果可能，自动化此过程以减少人工操作。
- en: –
  id: totrans-1502
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'System Maintenance: Plan for regular maintenance of the infrastructure to prevent
    downtime and ensure smooth operation.'
  id: totrans-1503
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统维护：规划基础设施的定期维护，以防止停机并确保平稳运行。
- en: •
  id: totrans-1504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Flexibility and Customisation:'
  id: totrans-1505
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活性与定制化：
- en: –
  id: totrans-1506
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Fine-Tuning: Allow for model fine-tuning to adapt the LLM to specific use cases
    and datasets. Fine-tuning can improve accuracy and relevance in responses.'
  id: totrans-1507
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：允许对模型进行微调，以适应特定的使用案例和数据集。微调可以提高响应的准确性和相关性。
- en: –
  id: totrans-1508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'API Integration: Ensure the deployment platform supports easy integration with
    existing systems and workflows through APIs and SDKs.'
  id: totrans-1509
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: API集成：确保部署平台通过API和SDK支持与现有系统和工作流程的轻松集成。
- en: •
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'User Management:'
  id: totrans-1511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户管理：
- en: –
  id: totrans-1512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Access Control: Implement role-based access control to manage who can deploy,
    use, and maintain the LLM.'
  id: totrans-1513
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 访问控制：实施基于角色的访问控制来管理谁可以部署、使用和维护LLM。
- en: –
  id: totrans-1514
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Monitoring and Logging: Set up comprehensive monitoring and logging to track
    usage, performance, and potential issues. This helps in proactive troubleshooting
    and optimisation.'
  id: totrans-1515
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监控与日志记录：设置全面的监控和日志记录，以跟踪使用情况、性能和潜在问题。这有助于主动排查故障和优化。
- en: •
  id: totrans-1516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compliance:'
  id: totrans-1517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 合规：
- en: –
  id: totrans-1518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Regulatory Compliance: Ensure that the deployment adheres to all relevant regulatory
    and legal requirements, including data protection laws like GDPR, HIPAA, etc.'
  id: totrans-1519
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 法规遵从：确保部署符合所有相关的法规和法律要求，包括数据保护法律，如GDPR、HIPAA等。
- en: –
  id: totrans-1520
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Ethical Considerations: Implement ethical guidelines to avoid biases and ensure
    the responsible use of LLMs.'
  id: totrans-1521
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理考虑：实施伦理指南以避免偏见，确保LLM的负责任使用。
- en: •
  id: totrans-1522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Support and Documentation:'
  id: totrans-1523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持与文档：
- en: –
  id: totrans-1524
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Technical Support: Choose a deployment platform that offers robust technical
    support and resources.'
  id: totrans-1525
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术支持：选择提供强大技术支持和资源的部署平台。
- en: –
  id: totrans-1526
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Documentation: Provide comprehensive documentation for developers and users
    to facilitate smooth deployment and usage.'
  id: totrans-1527
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档：为开发者和用户提供全面的文档，以促进顺利部署和使用。
- en: 'Chapter 9 Stage 7: Monitoring and Maintenance'
  id: totrans-1528
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章 第7阶段：监控与维护
- en: 9.1 Steps Involved in Monitoring and Maintenance of Deployed Fine-Tuned LLMs
  id: totrans-1529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 部署微调LLM的监控和维护步骤
- en: 'Continuous monitoring and maintenance of fine-tuned LLMs are essential to ensure
    their optimal performance, accuracy, and security over time. Below are the key
    steps involved in this process:'
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: 对微调后的LLM进行持续监控和维护至关重要，以确保其性能、准确性和安全性。以下是此过程中的关键步骤：
- en: '1.'
  id: totrans-1531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Setup Initial Baselines: Establish initial performance baselines by evaluating
    the model on a comprehensive test dataset. Record metrics such as accuracy, latency,
    throughput, and error rates to serve as reference points for future monitoring.'
  id: totrans-1532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置初始基准：通过在全面的测试数据集上评估模型，建立初始性能基准。记录诸如准确性、延迟、吞吐量和错误率等指标，以作为未来监控的参考点。
- en: '2.'
  id: totrans-1533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Performance Monitoring: Implement systems to continuously track key performance
    metrics such as response time, server load, and token usage. Regularly compare
    these metrics against the established baselines to detect any deviations.'
  id: totrans-1534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能监控：实施系统以持续跟踪关键性能指标，如响应时间、服务器负载和令牌使用情况。定期将这些指标与既定基准进行比较，以检测任何偏差。
- en: '3.'
  id: totrans-1535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Accuracy Monitoring: Continuously evaluate the model’s predictions against
    a ground truth dataset. Use metrics like precision, recall, F1 score, and cross-entropy
    loss to ensure the model maintains high accuracy levels.'
  id: totrans-1536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精度监控：持续评估模型的预测结果与真实数据集的匹配情况。使用精度、召回率、F1 分数和交叉熵损失等指标，确保模型保持高精度水平。
- en: '4.'
  id: totrans-1537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Error Monitoring: Track and analyse errors, including runtime errors and prediction
    errors. Implement logging mechanisms to capture detailed information about each
    error for troubleshooting and improvement.'
  id: totrans-1538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 错误监控：跟踪和分析错误，包括运行时错误和预测错误。实施日志记录机制，以捕捉有关每个错误的详细信息，以便进行故障排除和改进。
- en: '5.'
  id: totrans-1539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Log Analysis: Maintain comprehensive logs for each prediction request and response,
    including input data, output predictions, response times, and encountered errors.
    Regularly review logs to identify patterns and areas for improvement.'
  id: totrans-1540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 日志分析：为每个预测请求和响应保持全面日志，包括输入数据、输出预测、响应时间和遇到的错误。定期审查日志，以识别模式和改进领域。
- en: '6.'
  id: totrans-1541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Alerting Mechanisms: Set up automated alerting systems to notify stakeholders
    of any anomalies or deviations from expected performance metrics. Integrate alerts
    with communication tools like Slack, PagerDuty, or email for timely responses.'
  id: totrans-1542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 警报机制：建立自动警报系统，通知利益相关者任何异常或与预期性能指标的偏差。将警报集成到如 Slack、PagerDuty 或电子邮件等通信工具中，以便及时响应。
- en: '7.'
  id: totrans-1543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Feedback Loop: Establish a feedback loop with end-users to gather insights
    on model performance and user satisfaction. Use this feedback to continuously
    refine and improve the model.'
  id: totrans-1544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反馈循环：与最终用户建立反馈循环，收集有关模型性能和用户满意度的见解。利用这些反馈不断改进和提升模型。
- en: '8.'
  id: totrans-1545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Security Monitoring: Implement robust security measures to monitor for threats,
    including unauthorised access, data breaches, and adversarial attacks. Use encryption,
    access control, and regular security audits to protect the model and data.'
  id: totrans-1546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全监控：实施强有力的安全措施来监控威胁，包括未经授权的访问、数据泄露和对抗性攻击。使用加密、访问控制和定期安全审计来保护模型和数据。
- en: '9.'
  id: totrans-1547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Drift Detection: Continuously monitor for data and concept drift using statistical
    tests and drift detectors. Regularly evaluate the model on holdout datasets to
    detect changes in input data distribution or model performance.'
  id: totrans-1548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 漂移检测：使用统计测试和漂移检测器持续监控数据和概念漂移。定期在保留数据集上评估模型，以检测输入数据分布或模型性能的变化。
- en: '10.'
  id: totrans-1549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: 'Model Versioning: Maintain version control for different iterations of the
    model. Track performance metrics for each version to ensure that the best-performing
    model is in production.'
  id: totrans-1550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型版本控制：对模型的不同迭代版本进行版本控制。跟踪每个版本的性能指标，以确保最佳性能模型在生产中使用。
- en: '11.'
  id: totrans-1551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: 'Documentation and Reporting: Keep detailed documentation of monitoring procedures,
    metrics, and findings. Generate regular reports to provide stakeholders with insights
    into the model’s performance and maintenance activities.'
  id: totrans-1552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档和报告：保持监控程序、指标和发现的详细文档。生成定期报告，向利益相关者提供有关模型性能和维护活动的见解。
- en: '12.'
  id: totrans-1553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: 'Periodic Review and Update: Regularly assess and update the monitoring processes
    to incorporate new techniques, tools, and best practices, ensuring the monitoring
    system remains effective and up-to-date.'
  id: totrans-1554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定期审查和更新：定期评估和更新监控过程，纳入新技术、新工具和最佳实践，确保监控系统保持有效和最新。
- en: 9.2 Continuous Monitoring of Model Performance
  id: totrans-1555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 模型性能的持续监控
- en: While large language model (LLM) applications undergo some form of evaluation,
    continuous monitoring remains inadequately implemented in most cases. This section
    outlines the components necessary to establish an effective monitoring programme
    aimed at safeguarding users and preserving brand integrity.
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLM）应用程序会进行某种形式的评估，但在大多数情况下，持续监控仍然实施不足。本节概述了建立有效监控程序所需的组件，旨在保护用户和维护品牌完整性。
- en: 9.2.1 Functional Monitoring
  id: totrans-1557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1 功能监控
- en: Initially, it is crucial to monitor fundamental metrics consistently. This includes
    tracking metrics such as request volume, response times, token utilisation, costs
    incurred, and error rates.
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，持续监控基本指标至关重要。这包括跟踪请求量、响应时间、令牌使用情况、发生的成本以及错误率等指标。
- en: 9.2.2 Prompt Monitoring
  id: totrans-1559
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2 提示监控
- en: Following functional metrics, attention should be directed towards monitoring
    user-generated prompts or inputs. Metrics like readability can provide valuable
    insights. LLM evaluators should be employed to detect potential toxicity in responses.
    Additionally, metrics such as embedding distances from reference prompts prove
    insightful, ensuring adaptability to varying user interactions over time.
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控功能性指标之后，应将注意力转向监控用户生成的提示或输入。诸如可读性等指标可以提供有价值的见解。应聘用 LLM 评估员来检测响应中的潜在毒性。此外，诸如与参考提示的嵌入距离等指标也很有洞察力，确保对随时间变化的用户交互的适应性。
- en: Introducing a new evaluation category involves identifying adversarial attempts
    or malicious prompt injections, often overlooked in initial evaluations. Comparison
    against reference sets of known adversarial prompts helps identify and flag malicious
    activities. Evaluative LLMs play a crucial role in classifying prompts as benign
    or malicious.
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: 引入新的评估类别涉及识别敌对尝试或恶意提示注入，这通常在初始评估中被忽视。与已知敌对提示的参考集进行比较有助于识别和标记恶意活动。评估 LLM 在将提示分类为良性或恶性方面发挥着关键作用。
- en: 9.2.3 Response Monitoring
  id: totrans-1562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.3 响应监控
- en: Monitoring responses involves several critical checks to ensure alignment with
    expected outcomes. Parameters such as relevance, coherence (hallucination), topical
    alignment, sentiment, and their evolution over time are essential. Metrics related
    to toxicity and harmful output require frequent monitoring due to their critical
    impact. Prompt leakage represents an adversarial tactic wherein sensitive prompt
    information is illicitly extracted from the application’s stored data. Monitoring
    responses and comparing them against the database of prompt instructions can help
    detect such breaches. Embedding distance metrics are particularly effective in
    this regard. Regular testing against evaluation datasets provides benchmarks for
    accuracy and highlights any performance drift over time. Tools capable of managing
    embeddings allow exportation of underperforming output datasets for targeted improvements.
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: 监控响应涉及多个关键检查，以确保其与预期结果的一致性。诸如相关性、一致性（幻觉）、主题对齐、情感及其随时间的演变等参数都是必不可少的。由于毒性和有害输出的关键影响，这些指标需要频繁监控。提示泄露代表一种敌对策略，即从应用程序存储的数据中非法提取敏感提示信息。监控响应并将其与提示指令的数据库进行比较可以帮助检测这些泄露。嵌入距离指标在这方面特别有效。定期对评估数据集进行测试提供了准确性的基准，并突出显示任何性能漂移。能够管理嵌入的工具允许导出表现不佳的输出数据集以进行针对性改进。
- en: 9.2.4 Alerting Mechanisms and Thresholds
  id: totrans-1564
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.4 警报机制和阈值
- en: Effective monitoring necessitates well-calibrated alerting thresholds to avoid
    excessive false alarms. Implementing multivariate drift detection and alerting
    mechanisms can enhance accuracy. Consideration of false alarm rates and best practices
    for setting thresholds is paramount for effective monitoring system design. Alerting
    features should include integration with communication tools such as Slack and
    PagerDuty. Some systems offer automated response blocking in case of alerts triggered
    by problematic prompts. Similar mechanisms can be employed to screen responses
    for personal identifiable information (PII), toxicity, and other quality metrics
    before delivery to users. Custom metrics tailored to specific application nuances
    or innovative insights from data scientists can significantly enhance monitoring
    efficacy. Flexibility to incorporate such metrics is essential to adapt to evolving
    monitoring needs and advancements in the field.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的监控需要良好校准的警报阈值，以避免过多的虚假警报。实施多变量漂移检测和警报机制可以提高准确性。考虑虚假警报率和设置阈值的最佳实践对有效监控系统设计至关重要。警报功能应包括与通信工具（如
    Slack 和 PagerDuty）的集成。一些系统提供自动响应阻止功能，以防出现由问题提示触发的警报。类似的机制也可以用来筛查响应中的个人身份信息（PII）、毒性和其他质量指标，然后再交付给用户。针对特定应用程序细微差别或数据科学家的创新见解量身定制的自定义指标可以显著提高监控效果。能够灵活地纳入这些指标对于适应不断发展的监控需求和领域的进步至关重要。
- en: 9.2.5 Monitoring User Interface (UI)
  id: totrans-1566
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.5 监控用户界面 (UI)
- en: The monitoring system’s UI is pivotal, typically featuring time-series graphs
    of monitored metrics. Differentiated UIs facilitate in-depth analysis of alert
    trends, aiding root cause analysis. Advanced UI capabilities may include visualisations
    of embedding spaces through clustering and projections, providing insights into
    data patterns and relationships. Mature monitoring systems categorise data by
    users, projects, and teams, ensuring role-based access control (RBAC) to protect
    sensitive information. Optimising alert analysis within the UI interface remains
    an area where improvements can significantly reduce false alarm rates and enhance
    operational efficiency.
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 监控系统的用户界面至关重要，通常展示被监控指标的时间序列图。差异化的用户界面有助于深入分析警报趋势，帮助根本原因分析。高级用户界面功能可能包括通过聚类和投影可视化嵌入空间，提供数据模式和关系的见解。成熟的监控系统将数据按用户、项目和团队进行分类，确保基于角色的访问控制（RBAC）以保护敏感信息。在用户界面中优化警报分析仍然是一个可以显著减少误报率和提高操作效率的改进领域。
- en: 9.3 Updating LLM Knowledge
  id: totrans-1568
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 更新 LLM 知识
- en: 'To improve the knowledge base of an LLM, continued pretraining is used to help
    LLM evolve with the latest knowledge and information. The world and language are
    constantly evolving. New information emerges, trends shift, and cultural references
    change. LLMs trained on static data can become outdated, leading to:'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高 LLM 的知识库，持续预训练被用来帮助 LLM 适应最新的知识和信息。世界和语言在不断发展。新信息不断出现，趋势发生变化，文化参考也在变化。基于静态数据训练的
    LLM 可能会变得过时，导致：
- en: •
  id: totrans-1570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Factual Errors: Outdated information can cause LLMs to provide inaccurate responses.'
  id: totrans-1571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事实错误：过时的信息可能导致 LLM 提供不准确的回答。
- en: •
  id: totrans-1572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Irrelevance: Models might miss the context of current events or use outdated
    references.'
  id: totrans-1573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不相关性：模型可能会错过当前事件的背景或使用过时的参考。
- en: •
  id: totrans-1574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bias Perpetuation: Biases present in training data can become entrenched if
    not addressed through updates.'
  id: totrans-1575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏见延续：如果不通过更新解决，训练数据中存在的偏见可能会变得根深蒂固。
- en: 9.3.1 Retraining Methods
  id: totrans-1576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1 重新训练方法
- en: •
  id: totrans-1577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Periodic Retraining: This involves refreshing the model’s knowledge base at
    regular intervals (weekly, monthly, yearly) with new data. This is a straightforward
    method but requires a steady stream of high-quality, unbiased data.'
  id: totrans-1578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定期重新训练：这涉及到以定期（每周、每月、每年）用新数据刷新模型的知识库。这是一种直接的方法，但需要稳定的高质量、无偏数据。
- en: •
  id: totrans-1579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Trigger-Based Retraining: This approach monitors the LLM’s performance. When
    metrics like accuracy or relevance fall below a certain threshold, a retraining
    process is triggered. This method is more dynamic but requires robust monitoring
    systems and clear performance benchmarks.'
  id: totrans-1580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 触发式重新训练：这种方法监控 LLM 的性能。当准确度或相关性等指标低于某个阈值时，触发重新训练过程。这种方法更具动态性，但需要强大的监控系统和明确的性能基准。
- en: 9.3.2 Additional Methods
  id: totrans-1581
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2 额外方法
- en: •
  id: totrans-1582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-Tuning: LLMs can be fine-tuned for specific tasks by training them on
    smaller, domain-specific datasets. This allows for specialisation without complete
    retraining.'
  id: totrans-1583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：LLM 可以通过在较小的、特定领域的数据集上进行训练来进行微调。这允许在不进行完全重新训练的情况下进行专业化。
- en: •
  id: totrans-1584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Active Learning: This approach involves selectively querying the LLM to identify
    areas where it lacks knowledge. The retrieved information is then used to update
    the model.'
  id: totrans-1585
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主动学习：这种方法涉及选择性地查询 LLM，以识别其知识缺乏的领域。检索到的信息然后用于更新模型。
- en: 9.3.3 Key Considerations
  id: totrans-1586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.3 关键考虑因素
- en: •
  id: totrans-1587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Quality and Bias: New training data must be carefully curated to ensure
    quality and mitigate bias. Techniques like human annotation and fairness checks
    are crucial.'
  id: totrans-1588
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据质量与偏见：新的训练数据必须经过仔细筛选，以确保质量并减轻偏见。人工注释和公平性检查等技术至关重要。
- en: •
  id: totrans-1589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computational Cost: Retraining LLMs can be computationally expensive, requiring
    significant resources. Optimisations like transfer learning (using pre-trained
    models as a starting point) can help reduce costs.'
  id: totrans-1590
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算成本：重新训练 LLM 可能会非常昂贵，需要大量资源。像迁移学习（以预训练模型作为起点）这样的优化可以帮助降低成本。
- en: •
  id: totrans-1591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Downtime: Retraining often takes time, leading to LLM downtime. Strategies
    like rolling updates or deploying multiple models can minimise service disruptions.'
  id: totrans-1592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 停机时间：重新训练通常需要时间，导致 LLM 停机。像滚动更新或部署多个模型这样的策略可以减少服务中断。
- en: •
  id: totrans-1593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Version Control: Tracking different versions of the LLM and their training
    data is essential for rollbacks in case of performance issues.'
  id: totrans-1594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 版本控制：跟踪 LLM 的不同版本及其训练数据对于在出现性能问题时进行回滚至关重要。
- en: 9.4 The Future of LLM Updates
  id: totrans-1595
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4 LLM 更新的未来
- en: Research is ongoing to develop more efficient and effective LLM update strategies.
    One promising area is continuous learning, where LLMs can continuously learn and
    adapt from new data streams without retraining from scratch. Continuous learning
    aims to reduce the need for frequent full-scale retraining by enabling models
    to update incrementally with new information. This approach can significantly
    enhance the model’s ability to remain current with evolving knowledge and language
    use, improving its long-term performance and relevance.
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 研究正在进行，以开发更高效、更有效的LLM更新策略。一个有前景的领域是**持续学习**，在这种情况下，LLMs可以从新的数据流中不断学习和适应，而无需从头开始重新训练。持续学习旨在通过使模型能够逐步更新新信息，减少频繁全规模重新训练的需求。这种方法可以显著提高模型跟上不断发展的知识和语言使用的能力，从而提升其长期表现和相关性。
- en: Innovations in transfer learning and meta-learning are also contributing to
    advancements in LLM updates. These techniques allow models to leverage pre-existing
    knowledge and adapt quickly to new tasks or domains with minimal additional training.
    By integrating these advanced learning methods, future LLMs can become more adaptable
    and efficient in processing and understanding new information.
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**和**元学习**的创新也在推动LLM更新的进步。这些技术使得模型能够利用已有的知识，并以最少的额外训练快速适应新任务或领域。通过整合这些先进的学习方法，未来的LLMs可以变得更加适应和高效于处理和理解新信息。'
- en: Furthermore, ongoing improvements in hardware and computational resources will
    support more frequent and efficient updates. As processing power increases and
    becomes more accessible, the computational burden of updating large models will
    decrease, enabling more regular and comprehensive updates.
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，硬件和计算资源的持续改进将支持更频繁和高效的更新。随着处理能力的提升和变得更加可获得，更新大型模型的计算负担将减少，从而使得更定期和全面的更新成为可能。
- en: Collaboration between academia and industry is vital in driving these advancements.
    By sharing research findings and best practices, the field can collectively move
    towards more robust and efficient LLM update methodologies, ensuring that models
    remain accurate, relevant, and valuable over time.
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界和工业界之间的合作对于推动这些进展至关重要。通过共享研究成果和最佳实践，领域可以集体朝着更强大、更高效的LLM更新方法论迈进，确保模型随着时间的推移保持准确、相关和有价值。
- en: Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
  id: totrans-1600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章 工业微调平台和框架用于LLMs
- en: The evolution of fine-tuning techniques has been propelled by leading tech companies
    and platforms that have introduced innovative frameworks and services. Companies
    like HuggingFace, Amazon Web Services (AWS), Microsoft Azure, and OpenAI have
    developed tools and platforms that simplify and democratise the fine-tuning process.
    These advancements have not only lowered the barrier to entry for leveraging state-of-the-art
    AI models but have also enabled a wide range of applications across various industries,
    from healthcare and finance to customer service and content creation. Each of
    these platforms offers unique capabilities that cater to different needs, whether
    it be through automated fine-tuning workflows, scalable cloud-based training environments,
    or accessible API interfaces for deploying custom models.
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术的发展得到了领先科技公司和平台的推动，这些公司和平台推出了创新的框架和服务。像**HuggingFace**、**亚马逊网络服务（AWS）**、**微软Azure**和**OpenAI**这样的公司开发了简化和民主化微调过程的工具和平台。这些进展不仅降低了利用最先进AI模型的门槛，还使得跨多个行业（从医疗保健和金融到客户服务和内容创作）的各种应用成为可能。这些平台中的每一个都提供了满足不同需求的独特功能，无论是通过自动化微调工作流程、可扩展的基于云的训练环境，还是可访问的API接口用于部署定制模型。
- en: HuggingFace, for example, has made significant strides with its Transformers
    library¹¹1[https://huggingface.co/docs/transformers/en/index/](https://huggingface.co/docs/transformers/en/index/)
    and tools like Autotrain²²2[https://huggingface.co/autotrain](https://huggingface.co/autotrain)
    and SetFit, which allow users to fine-tune models with minimal coding and data.
    Their platform provides a robust infrastructure that supports both the research
    community and industry practitioners, facilitating the rapid development and deployment
    of custom AI solutions. Similarly, AWS’s SageMaker³³3[https://huggingface.co/autotrain](https://huggingface.co/autotrain)
    and SetFit⁴⁴4[https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/)
    provides an extensive suite of services that cover the entire machine learning
    lifecycle, from data preparation and training to model deployment and optimisation,
    making it a comprehensive solution for enterprise-level applications.
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，HuggingFace在其Transformers库¹¹1[https://huggingface.co/docs/transformers/en/index/](https://huggingface.co/docs/transformers/en/index/)和像Autotrain²²2[https://huggingface.co/autotrain](https://huggingface.co/autotrain)及SetFit这样的工具方面取得了显著进展，这些工具允许用户以最少的编码和数据来微调模型。他们的平台提供了强大的基础设施，支持研究社区和行业从业者，加速定制人工智能解决方案的开发和部署。同样，AWS的SageMaker³³3[https://huggingface.co/autotrain](https://huggingface.co/autotrain)和SetFit⁴⁴4[https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/)提供了涵盖整个机器学习生命周期的广泛服务，从数据准备和训练到模型部署和优化，使其成为企业级应用的综合解决方案。
- en: On the other hand, Microsoft Azure integrates its fine-tuning capabilities with
    enterprise-grade tools and services, offering solutions like Azure Machine Learning
    and the Azure OpenAI Service that cater to large organisations looking to incorporate
    advanced AI into their operations. Azure’s focus on MLOps and seamless integration
    with other Azure services ensures that fine-tuned models can be efficiently deployed
    and maintained in production environments. Meanwhile, OpenAI has pioneered the
    concept of ”fine-tuning as a service” allowing businesses to leverage their powerful
    models like GPT-4 through a user-friendly API ⁵⁵5[https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations),
    enabling custom model adaptations without the need for in-house AI expertise or
    infrastructure.
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Microsoft Azure将其微调能力与企业级工具和服务集成，提供诸如Azure Machine Learning和Azure OpenAI
    Service等解决方案，满足大企业希望将先进人工智能融入其运营的需求。Azure对MLOps的关注以及与其他Azure服务的无缝集成确保了微调模型能够高效地在生产环境中部署和维护。与此同时，OpenAI开创了“微调即服务”的概念，使企业能够通过用户友好的API⁵⁵5[https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations)利用其强大的GPT-4模型，实现定制模型的适配，而无需内部人工智能专长或基础设施。
- en: 'The collective efforts of these tech companies have not only enhanced the efficiency
    and scalability of fine-tuning but also democratised access to sophisticated AI
    tools. By reducing the technical barriers and providing comprehensive, user-friendly
    platforms, these innovations have enabled a wider range of industries to deploy
    advanced AI models tailored to their specific needs. Tables [10.1](#Ch10.T1 "Table
    10.1 ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)") and [10.2](#Ch10.T2 "Table 10.2 ‣ Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)") offer a quick comparison of LLM fine-tuning tools and frameworks from different
    providers.'
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: 这些科技公司的集体努力不仅提高了微调的效率和可扩展性，还使复杂的人工智能工具变得更加普及。通过降低技术门槛并提供全面且用户友好的平台，这些创新使得更多行业能够部署针对其特定需求的先进人工智能模型。表格[10.1](#Ch10.T1
    "表格 10.1 ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的最终微调指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")和[10.2](#Ch10.T2
    "表格 10.2 ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的最终微调指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")提供了来自不同供应商的LLM微调工具和框架的快速比较。
- en: '| Parameter | NVIDIA NeMo | Hugging Face AutoTrain API | Amazon Bedrock | AWS
    SageMaker JumpStart | Hugging Face Trainer API |'
  id: totrans-1605
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | NVIDIA NeMo | Hugging Face AutoTrain API | Amazon Bedrock | AWS SageMaker
    JumpStart | Hugging Face Trainer API |'
- en: '| Primary Use Case | Custom fine-tuning of LLMs with advanced NVIDIA GPUs.
    | Fine-tuning and deployment of LLMs with minimal code. | Fine-tuning and deploying
    LLMs on AWS infrastructure. | Simplified fine-tuning and deployment within the
    AWS ecosystem. | Manual fine-tuning of LLMs with detailed control over training
    processes. |'
  id: totrans-1606
  prefs: []
  type: TYPE_TB
  zh: '| 主要使用案例 | 使用先进的NVIDIA GPU进行LLMs的定制微调。 | 使用最少的代码进行LLMs的微调和部署。 | 在AWS基础设施上微调和部署LLMs。
    | 在AWS生态系统中简化微调和部署。 | 手动微调LLMs，对训练过程有详细控制。 |'
- en: '| Model Support | Supports a variety of large, pre-trained models, including
    Megatron series. | Supports a wide range of pre-trained models from the Hugging
    Face model hub. | Supports various LLMs like Amazon Titan and third-party models.
    | Pre-trained models from AWS and partners; integration with custom models. |
    Supports a vast array of models from the Hugging Face model hub. |'
  id: totrans-1607
  prefs: []
  type: TYPE_TB
  zh: '| 模型支持 | 支持各种大型预训练模型，包括Megatron系列。 | 支持来自Hugging Face模型库的广泛预训练模型。 | 支持各种LLMs，如Amazon
    Titan和第三方模型。 | 来自AWS和合作伙伴的预训练模型；与自定义模型集成。 | 支持来自Hugging Face模型库的大量模型。'
- en: '| Data Handling | Users provide task-specific data for fine-tuning, processed
    using NVIDIA’s infrastructure. | Uploads datasets via a simple interface; AutoTrain
    handles preprocessing and model training. | Data is uploaded and managed within
    the AWS environment; integrates with AWS data services. | Uploads and processes
    data within AWS; supports various data formats. | Users manually preprocess data
    and manage training steps. |'
  id: totrans-1608
  prefs: []
  type: TYPE_TB
  zh: '| 数据处理 | 用户提供任务特定的数据进行微调，使用NVIDIA的基础设施处理。 | 通过简单的界面上传数据集；AutoTrain处理预处理和模型训练。
    | 数据在AWS环境中上传和管理；与AWS数据服务集成。 | 在AWS中上传和处理数据；支持各种数据格式。 | 用户手动预处理数据并管理训练步骤。 |'
- en: '| Customisation Level | High; extensive control over fine-tuning process and
    model parameters. | Moderate; automated process with some customisation options.
    | High; detailed configuration and integration with AWS services. | Moderate;
    pre-configured settings with some customisation available. | Very High; detailed
    control over every aspect of fine-tuning. |'
  id: totrans-1609
  prefs: []
  type: TYPE_TB
  zh: '| 定制化级别 | 高；对微调过程和模型参数有广泛控制。 | 中等；自动化过程，提供一些定制选项。 | 高；详细配置和与AWS服务的集成。 | 中等；预配置设置，提供一些定制选项。
    | 非常高；对微调的每个方面有详细控制。 |'
- en: '| Scalability | High; leverages NVIDIA’s GPU capabilities for efficient scaling.
    | High; scalable via Hugging Face’s cloud infrastructure. | Very High; scalable
    across AWS’s extensive cloud infrastructure. | High; scalable within the AWS cloud
    ecosystem. | High; scalability depends on the infrastructure used (e.g., local
    vs. cloud). |'
  id: totrans-1610
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 高；利用NVIDIA的GPU能力实现高效扩展。 | 高；通过Hugging Face的云基础设施实现扩展。 | 非常高；在AWS广泛的云基础设施中扩展。
    | 高；在AWS云生态系统中扩展。 | 高；可扩展性取决于所使用的基础设施（例如，本地与云）。 |'
- en: '| Deployment Options | On-premises or cloud deployment via NVIDIA infrastructure.
    | Deployed via Hugging Face’s cloud or can be exported for local deployment. |
    Integrated into AWS services, easily deployable across AWS’s global infrastructure.
    | AWS cloud deployment; integrates with other AWS services. | Deployable locally,
    in cloud, or exported to other platforms. |'
  id: totrans-1611
  prefs: []
  type: TYPE_TB
  zh: '| 部署选项 | 通过NVIDIA基础设施进行本地或云部署。 | 通过Hugging Face的云部署或可导出以便本地部署。 | 集成到AWS服务中，易于在AWS全球基础设施上部署。
    | AWS云部署；与其他AWS服务集成。 | 可在本地、云中部署，或导出到其他平台。 |'
- en: '| Integration with Ecosystem | Deep integration with NVIDIA tools (e.g., TensorRT)
    and GPU-based workflows. | Integrates well with the Hugging Face ecosystem and
    other ML tools. | Seamless integration with AWS services (e.g., S3, Lambda, SageMaker).
    | Strong integration with AWS services; easy to connect with data pipelines and
    analytics. | Integrates with Hugging Face ecosystem and other Python-based ML
    tools. |'
  id: totrans-1612
  prefs: []
  type: TYPE_TB
  zh: '| 生态系统集成 | 深度集成NVIDIA工具（例如，TensorRT）和基于GPU的工作流。 | 与Hugging Face生态系统及其他ML工具集成良好。
    | 与AWS服务（例如，S3、Lambda、SageMaker）无缝集成。 | 与AWS服务紧密集成；容易连接数据管道和分析。 | 与Hugging Face生态系统和其他基于Python的ML工具集成。
    |'
- en: '| Data Privacy | Users must ensure data privacy compliance; NVIDIA handles
    data during processing. | Data handled within Hugging Face’s environment; privacy
    depends on data handling practices. | Strong focus on data privacy within AWS
    environment; compliant with various standards. | Strong AWS privacy and security
    measures; compliant with industry standards. | User-managed; depends on where
    the models and data are hosted. |'
  id: totrans-1613
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私 | 用户必须确保数据隐私合规；NVIDIA 在处理过程中处理数据。 | 数据在 Hugging Face 环境内处理；隐私取决于数据处理实践。
    | 在 AWS 环境内对数据隐私有强烈关注；符合各种标准。 | AWS 具有强大的隐私和安全措施；符合行业标准。 | 用户管理；取决于模型和数据的托管位置。
    |'
- en: '| Target Users | Enterprises and developers needing advanced customisation
    and performance in LLM fine-tuning. | Developers and businesses looking for easy,
    automated LLM fine-tuning solutions. | Businesses and developers integrated into
    or seeking to leverage AWS cloud services. | Enterprises and developers seeking
    streamlined AI/ML solutions within AWS. | Researchers, developers, and ML engineers
    needing detailed control over training. |'
  id: totrans-1614
  prefs: []
  type: TYPE_TB
  zh: '| 目标用户 | 需要高级定制和性能的企业和开发者。 | 寻找简单、自动化 LLM 微调解决方案的开发者和企业。 | 已集成或希望利用 AWS 云服务的企业和开发者。
    | 寻求在 AWS 内部提供精简 AI/ML 解决方案的企业和开发者。 | 需要对训练进行详细控制的研究人员、开发者和 ML 工程师。 |'
- en: '| Limitations | High resource demand and potential costs; dependency on NVIDIA
    ecosystem. | Less control over fine-tuning specifics; cloud-based, may not suit
    all on-premises needs. | Dependency on AWS; potential vendor lock-in, cost management
    complexity. | Limited to AWS services; pre-configured options may limit deep customisation.
    | Requires technical expertise; more complex setup and management. |'
  id: totrans-1615
  prefs: []
  type: TYPE_TB
  zh: '| 局限性 | 高资源需求和潜在成本；依赖 NVIDIA 生态系统。 | 对微调细节的控制较少；基于云，可能不适合所有本地需求。 | 依赖 AWS；潜在的供应商锁定，成本管理复杂。
    | 仅限于 AWS 服务；预配置选项可能限制深入定制。 | 需要技术专长；设置和管理较复杂。 |'
- en: 'Table 10.1: Detailed Comparison of LLM Fine-Tuning Platforms (Part I). This
    table provides a comprehensive comparison of various fine-tuning tools for Large
    Language Models (LLMs), including NVIDIA NeMo, Hugging Face AutoTrain API, Amazon
    Bedrock, AWS SageMaker JumpStart, and Hugging Face Trainer API. It covers multiple
    aspects such as the primary use case, model support, data handling, customisation
    level, scalability, deployment options, integration with the ecosystem, data privacy,
    target users, and limitations for each tool.'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1：LLM 微调平台详细比较（第一部分）。该表提供了对各种大语言模型（LLMs）微调工具的全面比较，包括 NVIDIA NeMo、Hugging
    Face AutoTrain API、Amazon Bedrock、AWS SageMaker JumpStart 和 Hugging Face Trainer
    API。它涵盖了多个方面，例如主要用途、模型支持、数据处理、定制程度、可扩展性、部署选项、与生态系统的集成、数据隐私、目标用户以及每个工具的局限性。
- en: '| Parameter | OpenAI Fine-Tuning API | Google Vertex AI Studio | Microsoft
    Azure AI Studio | LangChain |'
  id: totrans-1617
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | OpenAI 微调 API | Google Vertex AI Studio | Microsoft Azure AI Studio
    | LangChain |'
- en: '| Primary Use Case | API-based fine-tuning for OpenAI models with custom datasets.
    | End-to-end ML model development and deployment within Google Cloud. | End-to-end
    AI development, fine-tuning, and deployment on Azure. | Building applications
    using LLMs with modular and customisable workflows. |'
  id: totrans-1618
  prefs: []
  type: TYPE_TB
  zh: '| 主要用途 | 基于 API 的 OpenAI 模型微调，使用自定义数据集。 | 在 Google Cloud 内进行端到端 ML 模型开发和部署。
    | 在 Azure 上进行端到端 AI 开发、微调和部署。 | 使用 LLM 构建具有模块化和可定制工作流的应用程序。 |'
- en: '| Model Support | Limited to OpenAI models like GPT-3 and GPT-4. | Supports
    Google’s pre-trained models and user-customised models. | Supports Microsoft’s
    models and custom models fine-tuned within Azure. | Supports integration with
    various LLMs and AI tools (e.g., OpenAI, GPT-4, Cohere). |'
  id: totrans-1619
  prefs: []
  type: TYPE_TB
  zh: '| 模型支持 | 限于 OpenAI 模型，如 GPT-3 和 GPT-4。 | 支持 Google 的预训练模型和用户定制模型。 | 支持微软的模型和在
    Azure 上微调的自定义模型。 | 支持与各种 LLM 和 AI 工具的集成（例如 OpenAI、GPT-4、Cohere）。 |'
- en: '| Data Handling | Users upload datasets via API; OpenAI handles preprocessing
    and fine-tuning. | Data managed within Google Cloud; supports multiple data formats.
    | Data integrated within Azure ecosystem; supports various formats and sources.
    | Data handling is flexible, dependent on the specific LLM and integration used.
    |'
  id: totrans-1620
  prefs: []
  type: TYPE_TB
  zh: '| 数据处理 | 用户通过 API 上传数据集；OpenAI 处理预处理和微调。 | 数据在 Google Cloud 内管理；支持多种数据格式。 |
    数据集成在 Azure 生态系统内；支持各种格式和来源。 | 数据处理灵活，取决于具体的 LLM 和使用的集成。 |'
- en: '| Customisation Level | Moderate; focuses on ease of use with limited deep
    customisation. | High; offers custom model training and deployment with detailed
    configuration. | High; extensive customisation options through Azure’s AI tools.
    | Very High; allows detailed customisation of workflows, models, and data processing.
    |'
  id: totrans-1621
  prefs: []
  type: TYPE_TB
  zh: '| 自定义水平 | 中等；专注于易用性，深度自定义有限。 | 高；提供自定义模型训练和部署，具有详细配置。 | 高；通过 Azure 的 AI 工具提供广泛的自定义选项。
    | 非常高；允许详细自定义工作流、模型和数据处理。 |'
- en: '| Scalability | High; scalable through OpenAI’s cloud infrastructure. | Very
    High; leverages Google Cloud’s infrastructure for scaling. | Very High; scalable
    across Azure’s global infrastructure. | High; scalability depends on the specific
    infrastructure and models used. |'
  id: totrans-1622
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 高；通过 OpenAI 的云基础设施进行扩展。 | 非常高；利用 Google Cloud 的基础设施进行扩展。 | 非常高；可扩展至
    Azure 的全球基础设施。 | 高；可扩展性取决于具体的基础设施和使用的模型。'
- en: '| Deployment Options | Deployed via API, integrated into applications using
    OpenAI’s cloud. | Deployed within Google Cloud; integrates with other GCP services.
    | Deployed within Azure; integrates with Azure’s suite of services. | Deployed
    within custom infrastructure; integrates with various cloud and on-premises services.
    |'
  id: totrans-1623
  prefs: []
  type: TYPE_TB
  zh: '| 部署选项 | 通过 API 部署，集成到使用 OpenAI 云的应用程序中。 | 部署在 Google Cloud 内；与其他 GCP 服务集成。
    | 部署在 Azure 内；与 Azure 的服务套件集成。 | 部署在自定义基础设施中；与各种云和本地服务集成。 |'
- en: '| Integration with Ecosystem | Limited to OpenAI ecosystem; integrates well
    with apps via API. | Seamless integration with Google Cloud services (e.g., BigQuery,
    AutoML). | Deep integration with Azure’s services (e.g., Data Factory, Power BI).
    | Flexible integration with multiple tools, APIs, and data sources. |'
  id: totrans-1624
  prefs: []
  type: TYPE_TB
  zh: '| 与生态系统的集成 | 限于 OpenAI 生态系统；通过 API 与应用程序良好集成。 | 与 Google Cloud 服务（例如 BigQuery、AutoML）无缝集成。
    | 深度集成 Azure 的服务（例如 Data Factory、Power BI）。 | 灵活集成多个工具、API 和数据源。 |'
- en: '| Data Privacy | Managed by OpenAI; users must consider data transfer and privacy
    implications. | Strong privacy and security measures within Google Cloud environment.
    | Strong privacy and security measures within Azure environment. | Dependent on
    the integrations and infrastructure used; users manage privacy. |'
  id: totrans-1625
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私 | 由 OpenAI 管理；用户必须考虑数据传输和隐私影响。 | Google Cloud 环境中的隐私和安全措施强大。 | Azure
    环境中的隐私和安全措施强大。 | 依赖于集成和使用的基础设施；用户管理隐私。 |'
- en: '| Target Users | Developers and enterprises looking for straightforward, API-based
    LLM fine-tuning. | Developers and businesses integrated into Google Cloud or seeking
    to leverage GCP. | Enterprises and developers integrated into Azure or seeking
    to leverage Azure’s AI tools. | Developers needing to build complex, modular LLM-based
    applications with custom workflows. |'
  id: totrans-1626
  prefs: []
  type: TYPE_TB
  zh: '| 目标用户 | 寻找简单、基于 API 的 LLM 微调的开发者和企业。 | 集成到 Google Cloud 中或寻求利用 GCP 的开发者和企业。
    | 集成到 Azure 中或寻求利用 Azure 的 AI 工具的企业和开发者。 | 需要构建复杂、模块化 LLM 基础应用程序的开发者，具有自定义工作流。
    |'
- en: '| Limitations | Limited customisation; dependency on OpenAI’s infrastructure;
    potential cost. | Limited to Google Cloud ecosystem; potential cost and vendor
    lock-in. | Limited to Azure ecosystem; potential cost and vendor lock-in. | Complexity
    in chaining multiple models and data sources; requires more setup. |'
  id: totrans-1627
  prefs: []
  type: TYPE_TB
  zh: '| 限制 | 自定义有限；依赖 OpenAI 的基础设施；可能的成本。 | 限于 Google Cloud 生态系统；可能的成本和供应商锁定。 | 限于
    Azure 生态系统；可能的成本和供应商锁定。 | 链接多个模型和数据源的复杂性；需要更多的设置。 |'
- en: 'Table 10.2: Detailed Comparison of LLM Fine-Tuning Platforms (Part II). This
    table continues the comparison of LLM fine-tuning tools, focusing on OpenAI Fine-Tuning
    API, Google Vertex AI Studio, Microsoft Azure AI Studio, and LangChain. It evaluates
    the tools based on the primary use case, model support, data handling, customisation
    level, scalability, deployment options, integration with the ecosystem, data privacy,
    target users, and limitations, offering a complete view of their capabilities
    and constraints.'
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2：LLM 微调平台的详细比较（第二部分）。本表继续比较 LLM 微调工具，重点介绍 OpenAI 微调 API、Google Vertex
    AI Studio、Microsoft Azure AI Studio 和 LangChain。它根据主要用例、模型支持、数据处理、自定义水平、可扩展性、部署选项、与生态系统的集成、数据隐私、目标用户和限制对工具进行评估，提供了它们的能力和限制的完整视图。
- en: 10.1 Autotrain
  id: totrans-1629
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 Autotrain
- en: Autotrain is HuggingFace’s innovative platform that automates the fine-tuning
    of large language models, making it accessible even to those with limited machine
    learning expertise. The complexity and resource demands of fine-tuning LLMs can
    be daunting, but Autotrain simplifies the process by handling the most challenging
    aspects, such as data preparation, model configuration, and hyperparameter optimisation.
    This automation is particularly valuable for small teams or individual developers
    who need to deploy custom LLMs quickly and efficiently.
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: Autotrain是HuggingFace创新的平台，自动化了大型语言模型的微调，即使是那些机器学习专业知识有限的用户也能使用。微调LLMs的复杂性和资源需求可能令人生畏，但Autotrain通过处理最具挑战性的方面，如数据准备、模型配置和超参数优化，简化了这一过程。这种自动化对需要快速高效部署自定义LLMs的小团队或个人开发者特别有价值。
- en: 10.1.1 Steps Involved in Fine-Tuning Using Autotrain
  id: totrans-1631
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1 使用Autotrain进行微调的步骤
- en: 'Following are the steps involved in fine-tuning LLMs using Autotrain. Figure
    [10.1](#Ch10.F1 "Figure 10.1 ‣ 10.1.1 Steps Involved in Fine-Tuning Using Autotrain
    ‣ 10.1 Autotrain ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks
    for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") represents the visual workflow.'
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Autotrain对LLMs进行微调的步骤。图[10.1](#Ch10.F1 "图 10.1 ‣ 10.1.1 使用Autotrain进行微调的步骤
    ‣ 10.1 Autotrain ‣ 第10章 工业微调平台和LLMs框架 ‣ 从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的全面回顾（第1.0版）")展示了可视化工作流程。
- en: '![Refer to caption](img/464aba4e2d868ae176d81023f8a82180.png)'
  id: totrans-1633
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/464aba4e2d868ae176d81023f8a82180.png)'
- en: 'Figure 10.1: Overview of the Autotrain Workflow. This diagram illustrates the
    step-by-step process within the Autotrain system, beginning with the upload of
    datasets and model selection by users. The workflow then moves to data preparation
    and model configuration, followed by automated hyperparameter tuning to optimise
    model performance. The fine-tuning phase adjusts the model based on the provided
    datasets, culminating in the deployment of the fully fine-tuned model for practical
    use.'
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：Autotrain工作流程概述。该图示例了Autotrain系统中的逐步过程，从用户上传数据集和选择模型开始。工作流程随后转向数据准备和模型配置，然后是自动超参数调整以优化模型性能。微调阶段根据提供的数据集调整模型，最终将完全微调的模型部署到实际应用中。
- en: •
  id: totrans-1635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset Upload and Model Selection:'
  id: totrans-1636
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集上传和模型选择：
- en: –
  id: totrans-1637
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Users begin by uploading their datasets to the Autotrain platform.
  id: totrans-1638
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户首先将数据集上传到Autotrain平台。
- en: –
  id: totrans-1639
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: They then select a pre-trained model from the extensive HuggingFace Model Hub.
  id: totrans-1640
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，他们从庞大的HuggingFace Model Hub中选择一个预训练模型。
- en: •
  id: totrans-1641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Preparation:'
  id: totrans-1642
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据准备：
- en: –
  id: totrans-1643
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Autotrain automatically processes the uploaded data, including tasks like tokenization
    to convert text into a format the LLM can understand.
  id: totrans-1644
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Autotrain自动处理上传的数据，包括将文本转化为LLM可以理解的格式的任务，如分词。
- en: •
  id: totrans-1645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Configuration:'
  id: totrans-1646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型配置：
- en: –
  id: totrans-1647
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: The platform configures the model for fine-tuning, setting up the training environment
    and necessary parameters.
  id: totrans-1648
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平台配置模型以进行微调，设置训练环境和必要的参数。
- en: •
  id: totrans-1649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Automated Hyperparameter Tuning:'
  id: totrans-1650
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动超参数调整：
- en: –
  id: totrans-1651
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Autotrain explores various hyperparameter configurations (such as learning rate,
    batch size, and sequence length) and selects the best-performing ones.
  id: totrans-1652
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Autotrain探索各种超参数配置（如学习率、批量大小和序列长度），并选择表现最佳的配置。
- en: •
  id: totrans-1653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-Tuning:'
  id: totrans-1654
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：
- en: –
  id: totrans-1655
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: The model is fine-tuned on the prepared data with the optimised hyperparameters.
  id: totrans-1656
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在准备好的数据上进行微调，并使用优化后的超参数。
- en: •
  id: totrans-1657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Deployment:'
  id: totrans-1658
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署：
- en: –
  id: totrans-1659
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Once fine-tuning is complete, the model is ready for deployment in various NLP
    applications, such as text generation, completion, and language translation.
  id: totrans-1660
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调完成后，模型可以在各种NLP应用中部署，例如文本生成、补全和语言翻译。
- en: 10.1.2 Best Practices of Using Autotrain
  id: totrans-1661
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.2 使用Autotrain的最佳实践
- en: •
  id: totrans-1662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Quality: Ensure high-quality, well-labelled data for better model performance.'
  id: totrans-1663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据质量：确保高质量、标注良好的数据以获得更好的模型性能。
- en: •
  id: totrans-1664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Selection: Choose pre-trained models that are well-suited to your specific
    task to minimize fine-tuning effort.'
  id: totrans-1665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择：选择适合特定任务的预训练模型，以减少微调工作量。
- en: •
  id: totrans-1666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyperparameter Optimisation: Leverage Autotrain’s automated hyperparameter
    tuning to achieve optimal performance without manual intervention.'
  id: totrans-1667
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数优化：利用Autotrain的自动超参数调整实现最佳性能，无需手动干预。
- en: 10.1.3 Challenges of Using Autotrain
  id: totrans-1668
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.3 使用 Autotrain 的挑战
- en: •
  id: totrans-1669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Privacy: Ensuring the privacy and security of sensitive data during the
    fine-tuning process.'
  id: totrans-1670
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据隐私：确保在微调过程中敏感数据的隐私和安全。
- en: •
  id: totrans-1671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Resource Constraints: Managing computational resources effectively, especially
    in environments with limited access to powerful hardware.'
  id: totrans-1672
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 资源限制：有效管理计算资源，尤其是在对强大硬件访问有限的环境中。
- en: •
  id: totrans-1673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Overfitting: Avoiding overfitting by ensuring diverse and representative
    training data and using appropriate regularization techniques.'
  id: totrans-1674
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型过拟合：通过确保数据多样性和代表性以及使用适当的正则化技术来避免过拟合。
- en: 10.1.4 When to Use Autotrain
  id: totrans-1675
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.4 何时使用 Autotrain
- en: '1.'
  id: totrans-1676
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Lack of Deep Technical Expertise: Ideal for individuals or small teams without
    extensive machine learning or LLM background who need to fine-tune models quickly
    and effectively.'
  id: totrans-1677
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺乏深度技术专长：适合那些没有广泛机器学习或 LLM 背景的个人或小团队，他们需要快速有效地微调模型。
- en: '2.'
  id: totrans-1678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Quick Prototyping and Deployment: Suitable for rapid development cycles where
    time is critical, such as proof-of-concept projects or MVPs.'
  id: totrans-1679
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 快速原型开发和部署：适合于时间关键的快速开发周期，如概念验证项目或 MVP。
- en: '3.'
  id: totrans-1680
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Resource-Constrained Environments: Useful for scenarios with limited computational
    resources or where a quick turnaround is necessary.'
  id: totrans-1681
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 资源受限环境：适用于计算资源有限或需要快速周转的场景。
- en: In summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning
    of LLMs for standard NLP tasks, especially in environments with limited resources
    or expertise. However, it may not be suitable for highly specialised applications
    or those requiring significant customisation and scalability.
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Autotrain 是一个出色的工具，适合于在资源或专业知识有限的环境中快速、用户友好地微调 LLMs 以进行标准 NLP 任务。然而，它可能不适用于高度专业化的应用或需要显著自定义和可扩展性的场景。
- en: 10.1.5 Tutorials
  id: totrans-1683
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.5 教程
- en: '1.'
  id: totrans-1684
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[How To Create HuggingFace Custom AI Models Using AutoTrain](https://cobusgreyling.medium.com/how-to-create-huggingface-custom-ai-models-using-autotrain-72d75484b82b)'
  id: totrans-1685
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[如何使用 AutoTrain 创建 HuggingFace 自定义 AI 模型](https://cobusgreyling.medium.com/how-to-create-huggingface-custom-ai-models-using-autotrain-72d75484b82b)'
- en: '2.'
  id: totrans-1686
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '[Finetune models with HuggingFace AutoTrain](https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain)'
  id: totrans-1687
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace AutoTrain 微调模型](https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain)'
- en: 10.2 Transformers Library and Trainer API
  id: totrans-1688
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 Transformers 库和 Trainer API
- en: The Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning
    large language models (LLMs) such as BERT, GPT-3, and GPT-4\. This comprehensive
    library offers a wide array of pre-trained models tailored for various LLM tasks,
    making it easier for users to adapt these models to specific needs with minimal
    effort. Whether you’re fine-tuning for tasks like sentiment analysis, text classification,
    or generating customer support responses, the library simplifies the process by
    allowing seamless model selection from the HuggingFace Model Hub and straightforward
    customisation through its high-level APIs.
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace 的 Transformers 库作为一个关键工具，在微调大型语言模型（LLMs）如 BERT、GPT-3 和 GPT-4 上表现突出。这个全面的库提供了各种预训练模型，适用于不同的
    LLM 任务，使用户能够以最小的努力将这些模型适应到特定需求中。无论你是在进行情感分析、文本分类还是生成客户支持响应，库都通过允许从 HuggingFace
    模型库中无缝选择模型并通过其高级 API 进行简单的自定义，简化了过程。
- en: Central to the fine-tuning process within the Transformers Library is the Trainer
    API. This API includes the Trainer class, which automates and manages the complexities
    of fine-tuning LLMs. After completing data preprocessing, the Trainer class streamlines
    the setup for model training, including data handling, optimisation, and evaluation.
    Users only need to configure a few parameters, such as learning rate and batch
    size, and the API takes care of the rest. However, it’s crucial to note that running
    Trainer.train() can be resource-intensive and slow on a CPU. For efficient training,
    a GPU or TPU is recommended. Platforms like Google Colab provide free access to
    these resources, making it feasible for users without high-end hardware to fine-tune
    models effectively.
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformers 库中，微调过程的核心是 Trainer API。这个 API 包含 Trainer 类，该类自动化并管理微调 LLMs 的复杂性。在完成数据预处理后，Trainer
    类简化了模型训练的设置，包括数据处理、优化和评估。用户只需配置少量参数，如学习率和批量大小，其余由 API 处理。然而，需要注意的是，运行 Trainer.train()
    可能会在 CPU 上消耗大量资源且速度较慢。为了高效训练，建议使用 GPU 或 TPU。像 Google Colab 这样的平台提供了免费访问这些资源的机会，使得没有高端硬件的用户也能有效地进行模型微调。
- en: The Trainer API also supports advanced features like distributed training and
    mixed precision, which are essential for handling the large-scale computations
    required by modern LLMs. Distributed training allows the fine-tuning process to
    be scaled across multiple GPUs or nodes, significantly reducing training time.
    Mixed precision training, on the other hand, optimises memory usage and computation
    speed by using lower precision arithmetic without compromising model performance.
    HuggingFace’s dedication to accessibility is evident in the extensive documentation
    and community support they offer, enabling users of all expertise levels to fine-tune
    LLMs. This democratisation of advanced NLP technology empowers developers and
    researchers to deploy sophisticated, fine-tuned models for a wide range of applications,
    from specialised language understanding to large-scale data processing.
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer API还支持分布式训练和混合精度等高级功能，这些功能对于处理现代LLMs所需的大规模计算至关重要。分布式训练允许将微调过程扩展到多个GPU或节点，显著减少训练时间。另一方面，混合精度训练通过使用较低精度的算术运算来优化内存使用和计算速度，而不会影响模型性能。HuggingFace致力于提高可访问性的努力在他们提供的广泛文档和社区支持中得到了体现，使所有专业水平的用户都能微调LLMs。这种高级NLP技术的民主化使开发者和研究人员能够为各种应用部署复杂的、经过精细调优的模型，从专业语言理解到大规模数据处理。
- en: 10.2.1 Limitations of the Transformers Library and Trainer API
  id: totrans-1692
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.1 Transformers Library和Trainer API的局限性
- en: •
  id: totrans-1693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited Customisation for Advanced Users: While the Trainer API simplifies
    many aspects of training, it might not offer the deep customisation that advanced
    users or researchers might need for novel or highly specialised applications.'
  id: totrans-1694
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级用户的定制化有限：尽管Trainer API简化了训练的许多方面，但它可能无法提供高级用户或研究人员在新颖或高度专业化应用中所需的深度定制化。
- en: •
  id: totrans-1695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning Curve: Despite the simplified API, there is still a learning curve
    associated with understanding and effectively using the Transformers Library and
    Trainer API, particularly for those new to NLP and LLM.'
  id: totrans-1696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习曲线：尽管API简化了许多操作，但仍然存在学习曲线，特别是对于那些刚接触NLP和LLM的用户，理解和有效使用Transformers Library和Trainer
    API仍然具有一定的挑战。
- en: •
  id: totrans-1697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integration Limitations: The seamless integration and ease of use are often
    tied to the HuggingFace ecosystem, which might not be compatible with all workflows
    or platforms outside their environment.'
  id: totrans-1698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成限制：无缝集成和易用性通常与HuggingFace生态系统紧密相关，这可能与其环境外的所有工作流程或平台不兼容。
- en: In summary, the Transformers Library and Trainer API provide robust, scalable
    solutions for fine-tuning LLMs across a range of applications, offering ease of
    use and efficient training capabilities. However, users must be mindful of the
    resource requirements and potential limitations in customisation and complexity
    management.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Transformers Library和Trainer API为各种应用中的LLM微调提供了强大且可扩展的解决方案，具有易用性和高效的训练能力。然而，用户必须注意资源需求以及在定制化和复杂性管理方面的潜在限制。
- en: '10.3 Optimum: Enhancing LLM Deployment Efficiency'
  id: totrans-1700
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3 Optimum：提升LLM部署效率
- en: 'Optimum⁶⁶6[https://huggingface.co/docs/optimum/en/index](https://huggingface.co/docs/optimum/en/index)
    is HuggingFace’s tool designed to optimise the deployment of large language models
    (LLMs) by enhancing their efficiency across various hardware platforms. As LLMs
    grow in size and complexity, deploying them in a cost-effective and performant
    manner becomes increasingly challenging. Optimum addresses these challenges by
    applying a range of hardware-specific optimisations, such as quantisation, pruning,
    and model distillation, which reduce the model’s size and improve inference speed
    without significantly affecting accuracy. The following are the key techniques
    supported by Optimum:'
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: '[Optimum](https://huggingface.co/docs/optimum/en/index)是HuggingFace设计的工具，旨在通过提升在各种硬件平台上的效率来优化大型语言模型（LLMs）的部署。随着LLMs的规模和复杂性增长，以成本效益高且性能良好的方式进行部署变得越来越具挑战性。Optimum通过应用一系列硬件特定的优化技术来应对这些挑战，例如量化、剪枝和模型蒸馏，这些技术在不显著影响准确性的情况下，减小了模型的大小并提高了推理速度。以下是Optimum支持的关键技术：'
- en: •
  id: totrans-1702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantisation: Quantisation is one of the key techniques supported by Optimum.
    This process involves converting the model’s weights from high-precision floating-point
    numbers to lower-precision formats, such as int8 or float16\. This reduction in
    precision decreases the model’s memory footprint and computational requirements,
    enabling faster execution and lower power consumption, especially on edge devices
    and mobile platforms. Optimum automates the quantisation process, making it accessible
    to users who may not have expertise in low-level hardware optimisation.'
  id: totrans-1703
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化：量化是 Optimum 支持的关键技术之一。这个过程涉及将模型的权重从高精度浮点数转换为较低精度的格式，例如 int8 或 float16。这种精度的降低减少了模型的内存占用和计算需求，从而实现更快的执行速度和更低的功耗，特别是在边缘设备和移动平台上。Optimum
    自动化了量化过程，使得即使没有低级硬件优化专长的用户也能使用。
- en: •
  id: totrans-1704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pruning: Pruning is another critical optimisation strategy offered by Optimum.
    It involves identifying and removing less significant weights from the LLM, reducing
    its overall complexity and size. This leads to faster inference times and lower
    storage needs, which are particularly beneficial for deploying models in environments
    with limited computational resources. Optimum’s pruning algorithms carefully eliminate
    these redundant weights while maintaining the model’s performance, ensuring that
    it continues to deliver high-quality results even after optimisation.'
  id: totrans-1705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝：剪枝是 Optimum 提供的另一项关键优化策略。它涉及识别和去除 LLM 中不重要的权重，从而减少模型的整体复杂性和大小。这将导致更快的推理时间和更低的存储需求，这在计算资源有限的环境中尤其有利。Optimum
    的剪枝算法会仔细去除这些冗余权重，同时保持模型的性能，确保即使在优化后仍能提供高质量的结果。
- en: •
  id: totrans-1706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Distillation: In addition to these techniques, Optimum supports model
    distillation, a process where a smaller, more efficient model is trained to replicate
    the behaviour of a larger, more complex model. This distilled model retains much
    of the knowledge and capabilities of the original while being significantly lighter
    and faster. Optimum provides tools to facilitate the distillation process, allowing
    users to create compact LLMs that are well-suited for real-time applications.
    By offering a comprehensive suite of optimisation tools, Optimum ensures that
    HuggingFace’s LLMs can be deployed effectively across a wide range of environments,
    from powerful cloud servers to resource-constrained edge devices.'
  id: totrans-1707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型蒸馏：除了这些技术，Optimum 还支持模型蒸馏，即训练一个更小、更高效的模型以复制一个更大、更复杂模型的行为。这个蒸馏模型保留了原始模型的大部分知识和能力，同时变得更轻便和更快。Optimum
    提供了促进蒸馏过程的工具，使用户能够创建适合实时应用的紧凑 LLM。通过提供全面的优化工具套件，Optimum 确保 HuggingFace 的 LLM 能够在从强大的云服务器到资源受限的边缘设备等广泛环境中有效部署。
- en: 10.3.1 Best Practices of Using Optimum
  id: totrans-1708
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1 使用 Optimum 的最佳实践
- en: •
  id: totrans-1709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Understand Hardware Requirements: Assess the target deployment environment
    (e.g., edge devices, cloud servers) to optimise model configuration accordingly.'
  id: totrans-1710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 了解硬件要求：评估目标部署环境（例如，边缘设备、云服务器），以相应地优化模型配置。
- en: •
  id: totrans-1711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Iterative Optimisation: Experiment with different optimisation techniques (quantisation
    levels, pruning thresholds) to find the optimal balance between model size, speed,
    and accuracy.'
  id: totrans-1712
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代优化：尝试不同的优化技术（量化水平、剪枝阈值），以找到模型大小、速度和准确性之间的最佳平衡。
- en: •
  id: totrans-1713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Validation and Testing: Validate optimised models thoroughly to ensure they
    meet performance and accuracy requirements across different use cases.'
  id: totrans-1714
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证和测试：彻底验证优化后的模型，以确保它们在不同使用案例中满足性能和准确性要求。
- en: •
  id: totrans-1715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Documentation and Support: Refer to HuggingFace’s resources for detailed guidance
    on using Optimum’s tools effectively, and leverage community support for troubleshooting
    and best practices sharing.'
  id: totrans-1716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档和支持：参考 HuggingFace 的资源以获取有关有效使用 Optimum 工具的详细指南，并利用社区支持进行故障排除和最佳实践共享。
- en: •
  id: totrans-1717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Continuous Monitoring: Monitor deployed models post-optimisation to detect
    any performance degradation and adjust optimisation strategies as needed to maintain
    optimal performance over time.'
  id: totrans-1718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续监控：在优化后监控部署的模型，以检测任何性能下降，并根据需要调整优化策略，以维持长期的最佳性能。
- en: 10.3.2 Tutorials
  id: totrans-1719
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2 教程
- en: '1.'
  id: totrans-1720
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[An Introduction to Using Transformers and Hugging Face](https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face)'
  id: totrans-1721
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 Transformers 和 Hugging Face 的简介](https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face)'
- en: 10.4 Amazon SageMaker JumpStart
  id: totrans-1722
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4 Amazon SageMaker JumpStart
- en: 'Amazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed
    to simplify and expedite the fine-tuning of large language models (LLMs). It provides
    users with a rich library of pre-built models and solutions that can be quickly
    customised for various use cases. This tool is particularly valuable for organisations
    looking to deploy NLP solutions efficiently without deep expertise in machine
    learning or the extensive computational resources typically required for training
    LLMs from scratch. The architecture depicted in Figure [10.2](#Ch10.F2 "Figure
    10.2 ‣ 10.4 Amazon SageMaker JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms
    and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)") outlines a comprehensive
    pipeline for the fine-tuning and deployment of large language models (LLMs) Utilising
    AWS services.'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker JumpStart 是 SageMaker 生态系统中的一个功能，旨在简化和加速大型语言模型（LLMs）的微调。它为用户提供了丰富的预构建模型和解决方案库，这些模型和解决方案可以快速定制以适应各种用例。这个工具对于那些希望高效部署
    NLP 解决方案的组织特别有价值，而无需深入的机器学习专业知识或通常需要的大量计算资源来从头训练 LLMs。图 [10.2](#Ch10.F2 "图 10.2
    ‣ 10.4 Amazon SageMaker JumpStart ‣ 第 10 章 大型语言模型的工业微调平台和框架 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的全面回顾
    (版本 1.0)") 描述了一个利用 AWS 服务的大型语言模型（LLMs）微调和部署的综合流程。
- en: '![Refer to caption](img/94effda76cc6c3853fc83d1a3013d020.png)'
  id: totrans-1724
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94effda76cc6c3853fc83d1a3013d020.png)'
- en: 'Figure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart
    process, starting from data preprocessing using EMR Serverless Spark to the fine-tuning
    of LLMs, and ending with model deployment on Amazon SageMaker Endpoints. (adapted
    from [[83](#bib.bib83)])'
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：展示 Amazon SageMaker JumpStart 过程的逐步工作流，从使用 EMR Serverless Spark 进行数据预处理开始，到
    LLMs 的微调，最后在 Amazon SageMaker Endpoints 上部署模型。（改编自 [[83](#bib.bib83)])
- en: 10.4.1 Steps Involved in Using JumpStart
  id: totrans-1726
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.1 使用 JumpStart 的步骤
- en: •
  id: totrans-1727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Preparation and Preprocessing:'
  id: totrans-1728
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据准备与预处理：
- en: –
  id: totrans-1729
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Storage: Begin by securely storing raw datasets in Amazon S3, AWS’s scalable
    object storage service.'
  id: totrans-1730
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据存储：首先将原始数据集安全存储在 Amazon S3，即 AWS 的可扩展对象存储服务中。
- en: –
  id: totrans-1731
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient
    data preprocessing. This step refines and prepares the raw data for subsequent
    model training and evaluation.'
  id: totrans-1732
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预处理：利用 EMR Serverless 框架与 Apache Spark 进行高效的数据预处理。此步骤对原始数据进行精炼和准备，以便进行后续的模型训练和评估。
- en: –
  id: totrans-1733
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing,
    ensuring accessibility and readiness for the next stages.'
  id: totrans-1734
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据精炼：在预处理后，将处理过的数据集存储回 Amazon S3，以确保数据的可访问性和为下一阶段做好准备。
- en: •
  id: totrans-1735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Fine-Tuning with SageMaker JumpStart:'
  id: totrans-1736
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 SageMaker JumpStart 微调模型：
- en: –
  id: totrans-1737
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Model Selection: Choose from a variety of pre-built models and solutions available
    through SageMaker JumpStart’s extensive library, tailored for tasks such as sentiment
    analysis, text generation, or customer support automation.'
  id: totrans-1738
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择：从 SageMaker JumpStart 广泛的模型和解决方案库中选择适合情感分析、文本生成或客户支持自动化等任务的预构建模型。
- en: –
  id: totrans-1739
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Fine-Tuning Execution: Utilise Amazon SageMaker’s capabilities, integrated
    with SageMaker JumpStart, to fine-tune the selected model. This involves adjusting
    parameters and configurations to optimise the model’s performance for specific
    use cases.'
  id: totrans-1740
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调执行：利用 Amazon SageMaker 与 SageMaker JumpStart 集成的能力来微调选定的模型。这涉及调整参数和配置，以优化模型在特定用例中的表现。
- en: –
  id: totrans-1741
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Workflow Simplification: Leverage pre-built algorithms and model templates
    provided by SageMaker JumpStart to streamline the fine-tuning workflow, reducing
    the time and effort required for deployment.'
  id: totrans-1742
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工作流简化：利用 SageMaker JumpStart 提供的预构建算法和模型模板来简化微调工作流，减少部署所需的时间和精力。
- en: •
  id: totrans-1743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Deployment and Hosting:'
  id: totrans-1744
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型部署与托管：
- en: –
  id: totrans-1745
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Deployment Setup: Deploy the fine-tuned model using Amazon SageMaker’s endpoint
    deployment capabilities. This setup ensures that the model is hosted in a scalable
    environment capable of handling real-time predictions efficiently.'
  id: totrans-1746
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署设置：使用 Amazon SageMaker 的端点部署功能来部署微调后的模型。此设置确保模型在可扩展的环境中托管，能够高效处理实时预测。
- en: –
  id: totrans-1747
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Scalability: Benefit from AWS’s infrastructure scalability, allowing seamless
    scaling of resources to accommodate varying workloads and operational demands.'
  id: totrans-1748
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '可扩展性：受益于 AWS 的基础设施可扩展性，实现资源的无缝扩展，以适应不同的工作负载和操作需求。  '
- en: –
  id: totrans-1749
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '–  '
- en: 'Efficiency and Accessibility: Ensure that the deployed model is accessible
    via SageMaker endpoints, enabling efficient integration into production applications
    for real-time inference tasks.'
  id: totrans-1750
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '效率与可访问性：确保部署的模型可以通过 SageMaker 端点访问，实现与生产应用的高效集成，适用于实时推断任务。  '
- en: 10.4.2 Best Practices for Using JumpStart
  id: totrans-1751
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '10.4.2 使用 JumpStart 的最佳实践  '
- en: •
  id: totrans-1752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Robust Data Management: Maintain secure and organised data storage practices
    in Amazon S3, facilitating efficient data access and management throughout the
    pipeline.'
  id: totrans-1753
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '强大的数据管理：在 Amazon S3 中维护安全有序的数据存储实践，促进整个流程中的高效数据访问和管理。  '
- en: •
  id: totrans-1754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Cost-Effective Processing: Utilise serverless computing frameworks like EMR
    Serverless with Apache Spark for cost-effective and scalable data preprocessing.'
  id: totrans-1755
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '成本效益处理：利用如 EMR Serverless 和 Apache Spark 的无服务器计算框架，实现具有成本效益和可扩展的数据预处理。  '
- en: •
  id: totrans-1756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Optimised Fine-Tuning: Capitalise on SageMaker JumpStart’s pre-built models
    and algorithms to expedite and optimise the fine-tuning process, ensuring optimal
    model performance without extensive manual configuration.'
  id: totrans-1757
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '优化微调：利用 SageMaker JumpStart 的预构建模型和算法，加速和优化微调过程，确保模型性能最佳，无需 extensive manual
    configuration。  '
- en: •
  id: totrans-1758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms
    post-deployment to track model performance metrics. This allows for timely optimisations
    and adjustments to maintain accuracy and efficiency over time.'
  id: totrans-1759
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '持续监控与优化：在部署后实施强有力的监控机制，以跟踪模型性能指标。这允许及时优化和调整，以保持长期的准确性和效率。  '
- en: •
  id: totrans-1760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Integration with AWS Services: Leverage AWS’s comprehensive suite of services
    and integration capabilities to create end-to-end pipelines that ensure reliable
    and scalable deployment of large-scale language models across diverse operational
    environments.'
  id: totrans-1761
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '与 AWS 服务的集成：利用 AWS 提供的全面服务和集成能力，创建端到端的管道，以确保大规模语言模型在多样化操作环境中的可靠和可扩展的部署。  '
- en: 10.4.3 Limitations of Using JumpStart
  id: totrans-1762
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '10.4.3 使用 JumpStart 的局限性  '
- en: •
  id: totrans-1763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Limited Customisation: While JumpStart simplifies the process for common use
    cases, it may offer limited flexibility for highly specialised or complex applications
    that require significant customisation beyond the provided templates and workflows.'
  id: totrans-1764
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '限制性定制：虽然 JumpStart 简化了常见用例的过程，但对于需要大量定制的高度专业化或复杂应用程序，它可能提供有限的灵活性。  '
- en: •
  id: totrans-1765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services,
    which may pose challenges for users who prefer or need to operate in multi-cloud
    environments or those with existing infrastructure outside of AWS.'
  id: totrans-1766
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 依赖 AWS 生态系统：JumpStart 与 AWS 服务紧密集成，这可能对那些希望或需要在多云环境中操作的用户或有现有 AWS 之外基础设施的用户构成挑战。
- en: •
  id: totrans-1767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '•  '
- en: 'Resource Costs: Utilising SageMaker’s scalable resources for fine-tuning LLMs,
    especially large models, can incur substantial costs, which might be a barrier
    for smaller organisations or those with limited budgets.'
  id: totrans-1768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '资源成本：利用 SageMaker 可扩展资源进行 LLM 微调，特别是大模型，可能会产生可观的费用，这可能成为小型组织或预算有限的组织的障碍。  '
- en: 10.4.4 Tutorials
  id: totrans-1769
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '10.4.4 教程  '
- en: '1.'
  id: totrans-1770
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.  '
- en: '[Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart](https://www.linkedin.com/pulse/fine-tuning-llama-2-amazon-sagemaker-jumpstart-elhousieny-phd%E1%B4%AC%E1%B4%AE%E1%B4%B0-8zp9c/)'
  id: totrans-1771
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 Amazon SageMaker JumpStart 微调 LLaMA 2](https://www.linkedin.com/pulse/fine-tuning-llama-2-amazon-sagemaker-jumpstart-elhousieny-phd%E1%B4%AC%E1%B4%AE%E1%B4%B0-8zp9c/)  '
- en: '2.'
  id: totrans-1772
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.  '
- en: '[LLM Agents Using AWS SageMaker JumpStart Foundation Models](https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/)'
  id: totrans-1773
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 AWS SageMaker JumpStart 基础模型的 LLM 代理](https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/)  '
- en: 10.5 Amazon Bedrock
  id: totrans-1774
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '10.5 Amazon Bedrock  '
- en: Amazon Bedrock⁷⁷7[https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/)
    is a fully managed service designed to simplify access to high-performing foundation
    models (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral
    AI, Stability AI, and Amazon. It provides a unified API that integrates these
    models and offers extensive capabilities for developing secure, private, and responsible
    generative AI applications. With Amazon Bedrock, users can effortlessly experiment
    with and assess leading FMs tailored to their specific needs. The service supports
    private customisation of models through fine-tuning and Retrieval Augmented Generation
    (RAG), enabling the creation of intelligent agents that leverage enterprise data
    and systems. Amazon Bedrock’s serverless architecture allows for quick deployment,
    seamless integration, and secure customisation of FMs without the burden of infrastructure
    management, Utilising AWS tools to deploy these models into applications efficiently
    and securely.
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock⁷⁷7[https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/)
    是一个完全托管的服务，旨在简化访问来自顶级 AI 创新者（如 AI21 Labs、Anthropic、Cohere、Meta、Mistral AI、Stability
    AI 和 Amazon）的高性能基础模型（FMs）。它提供了一个统一的 API 来集成这些模型，并提供广泛的功能以开发安全、私密和负责任的生成 AI 应用程序。通过
    Amazon Bedrock，用户可以轻松地尝试和评估适合他们特定需求的领先 FMs。该服务支持通过微调和检索增强生成（RAG）进行模型的私有定制，允许创建利用企业数据和系统的智能代理。Amazon
    Bedrock 的无服务器架构允许快速部署、无缝集成和安全定制 FMs，无需管理基础设施负担，利用 AWS 工具有效且安全地将这些模型部署到应用程序中。
- en: 10.5.1 Steps Involved in Using Amazon Bedrock
  id: totrans-1776
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.5.1 使用 Amazon Bedrock 的步骤
- en: 'Amazon Bedrock offers a streamlined workflow for deploying and fine-tuning
    LLMs, making it an ideal choice for businesses looking to quickly integrate advanced
    AI capabilities into their operations. Here’s a high-level overview of how Bedrock
    operates:'
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock 提供了一个精简的工作流程用于部署和微调 LLMs，使其成为希望快速将先进 AI 能力集成到业务中的理想选择。以下是 Bedrock
    操作的高级概述：
- en: •
  id: totrans-1778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Selection: Users start by choosing from a curated selection of foundation
    models available through Bedrock. These include models from AWS (like Amazon Titan)
    and third-party providers (such as Anthropic Claude and Stability AI).'
  id: totrans-1779
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择：用户首先从 Bedrock 提供的精心挑选的基础模型中选择。这些模型包括 AWS 的模型（如 Amazon Titan）和第三方提供商的模型（如
    Anthropic Claude 和 Stability AI）。
- en: •
  id: totrans-1780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-Tuning:'
  id: totrans-1781
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：
- en: –
  id: totrans-1782
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Once a model is selected, users can fine-tune it to better fit their specific
    needs. This involves feeding the model with domain-specific data or task-specific
    instructions to tailor its outputs.
  id: totrans-1783
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦选择了模型，用户可以对其进行微调，以更好地符合他们的特定需求。这涉及到向模型提供领域特定的数据或任务特定的指令，以调整其输出。
- en: –
  id: totrans-1784
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: The fine-tuning process is handled via simple API calls, eliminating the need
    for extensive setup or detailed configuration. Users provide their custom data,
    and Bedrock manages the training process in the background.
  id: totrans-1785
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调过程通过简单的 API 调用进行，无需广泛的设置或详细配置。用户提供自定义数据，Bedrock 在后台管理训练过程。
- en: •
  id: totrans-1786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Deployment:'
  id: totrans-1787
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署：
- en: –
  id: totrans-1788
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: After fine-tuning, Bedrock takes care of deploying the model in a scalable and
    efficient manner. This means that users can quickly integrate the fine-tuned model
    into their applications or services.
  id: totrans-1789
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在微调之后，Bedrock 会以可扩展和高效的方式处理模型的部署。这意味着用户可以快速将微调后的模型集成到他们的应用程序或服务中。
- en: –
  id: totrans-1790
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Bedrock ensures that the model scales according to demand and handles performance
    optimisation, providing a seamless user experience.
  id: totrans-1791
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Bedrock 确保模型根据需求进行扩展，并处理性能优化，提供无缝的用户体验。
- en: •
  id: totrans-1792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integration and Monitoring:'
  id: totrans-1793
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成与监控：
- en: –
  id: totrans-1794
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Bedrock integrates smoothly with other AWS services, allowing users to embed
    AI capabilities directly into their existing AWS ecosystem.
  id: totrans-1795
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Bedrock 与其他 AWS 服务无缝集成，使用户能够将 AI 能力直接嵌入到现有的 AWS 生态系统中。
- en: –
  id: totrans-1796
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Users can monitor and manage the performance of their deployed models through
    AWS’s comprehensive monitoring tools, ensuring that the models continue to perform
    optimally.
  id: totrans-1797
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户可以通过 AWS 的综合监控工具来监控和管理他们部署的模型的性能，确保模型持续保持最佳表现。
- en: 10.5.2 Limitations of Using Amazon Bedrock
  id: totrans-1798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.5.2 使用 Amazon Bedrock 的限制
- en: While Amazon Bedrock offers a robust suite of tools and services for addressing
    certain AI challenges, it is not a comprehensive solution for all AI needs. One
    key limitation is that it does not eliminate the requirement for human expertise.
    Organisations still need skilled professionals who understand the intricacies
    of AI technology to effectively develop, fine-tune, and optimise the models provided
    by Bedrock.
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Amazon Bedrock 提供了一套强大的工具和服务来应对某些 AI 挑战，但它并不是所有 AI 需求的全面解决方案。一个关键的局限性是，它并没有消除对人工专业知识的需求。组织仍然需要了解
    AI 技术复杂性的专业人士，以有效开发、微调和优化 Bedrock 提供的模型。
- en: Additionally, Amazon Bedrock is not designed to function as a standalone service.
    It relies on integration with other AWS services, such as Amazon S3 for data storage,
    AWS Lambda for serverless computing, and AWS SageMaker for machine learning model
    development. Therefore, businesses leveraging Amazon Bedrock will also need to
    use these complementary AWS services to fully realise its potential. This interconnectedness
    means that while Amazon Bedrock enhances the AI capabilities within an AWS ecosystem,
    it may present a steep learning curve and require significant infrastructure management
    for those new to AWS.
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Amazon Bedrock 并非设计为独立服务。它依赖于与其他 AWS 服务的集成，如用于数据存储的 Amazon S3、用于无服务器计算的 AWS
    Lambda 和用于机器学习模型开发的 AWS SageMaker。因此，利用 Amazon Bedrock 的企业还需要使用这些补充的 AWS 服务，以充分发挥其潜力。这种相互关联意味着，虽然
    Amazon Bedrock 在 AWS 生态系统内增强了 AI 能力，但对那些不熟悉 AWS 的人来说，可能会存在陡峭的学习曲线和显著的基础设施管理需求。
- en: 10.5.3 Tutorials
  id: totrans-1801
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.5.3 教程
- en: '1.'
  id: totrans-1802
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[Finetuning LLMs on Amazon Bedrock](https://medium.com/@abdullahiolaoye4/finetuning-llms-on-amazon-bedrock-887ebc547adc)'
  id: totrans-1803
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[Finetuning LLMs on Amazon Bedrock](https://medium.com/@abdullahiolaoye4/finetuning-llms-on-amazon-bedrock-887ebc547adc)'
- en: '2.'
  id: totrans-1804
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '[Amazon Bedrock for Generative AI](https://cloudnature.net/blog/the-complete-guide-to-amazon-bedrock-for-generative-ai)'
  id: totrans-1805
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[Amazon Bedrock for Generative AI](https://cloudnature.net/blog/the-complete-guide-to-amazon-bedrock-for-generative-ai)'
- en: 10.6 OpenAI’s Fine-Tuning API
  id: totrans-1806
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6 OpenAI 的微调 API
- en: OpenAI’s Fine-Tuning API is a comprehensive platform that facilitates the customisation
    of OpenAI’s pre-trained LLMs to cater to specific tasks and domains. This service
    is designed to be user-friendly, enabling a broad range of users, from businesses
    to individual developers, to harness the power of advanced AI without the complexities
    typically associated with model training and deployment.
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的微调 API 是一个全面的平台，便于将 OpenAI 预训练的 LLMs 自定义以适应特定任务和领域。该服务设计为用户友好，使从企业到个人开发者的广泛用户能够利用先进的
    AI 技术，而无需面对模型训练和部署通常带来的复杂性。
- en: 10.6.1 Steps Involved in Using OpenAI’s Fine-Tuning API
  id: totrans-1808
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.6.1 使用 OpenAI 的微调 API 的步骤
- en: •
  id: totrans-1809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Selection:'
  id: totrans-1810
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择：
- en: –
  id: totrans-1811
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAI’s
    extensive lineup. This includes powerful models like GPT-4, which offer a robust
    starting point for a wide range of language processing tasks.'
  id: totrans-1812
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择预训练模型：用户开始时从 OpenAI 的广泛模型系列中选择一个基础模型。这包括如 GPT-4 这样的强大模型，提供了一个强有力的起点，用于广泛的语言处理任务。
- en: –
  id: totrans-1813
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Customisable Base: These models come pre-trained with vast amounts of data,
    providing a solid foundation that can be further refined to suit specific requirements.'
  id: totrans-1814
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可定制的基础模型：这些模型经过大量数据的预训练，提供了一个坚实的基础，可以进一步调整以满足特定需求。
- en: •
  id: totrans-1815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Preparation and Upload:'
  id: totrans-1816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据准备和上传：
- en: –
  id: totrans-1817
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Curating Relevant Data: Users need to gather and prepare a dataset that reflects
    the specific task or domain they wish to fine-tune the model for. This data is
    crucial for teaching the model to perform the desired function more effectively.'
  id: totrans-1818
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 策划相关数据：用户需要收集和准备一个反映他们希望微调模型的特定任务或领域的数据集。这些数据对于教会模型更有效地执行所需功能至关重要。
- en: –
  id: totrans-1819
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Uploading Data to the API: The Fine-Tuning API facilitates easy data upload.
    Users can feed their curated datasets into the API through straightforward commands,
    making the process accessible even to those with limited technical backgrounds.'
  id: totrans-1820
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上传数据到 API：微调 API 使数据上传变得简单。用户可以通过简单的命令将他们策划的数据集输入 API，使即使是技术背景有限的人也能轻松完成这一过程。
- en: •
  id: totrans-1821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Initiating Fine-Tuning:'
  id: totrans-1822
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启动微调：
- en: –
  id: totrans-1823
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Automated Process: Once the data is uploaded, OpenAI’s infrastructure handles
    the fine-tuning process. The API adjusts the model’s parameters based on the new
    data to improve performance on the specified tasks.'
  id: totrans-1824
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动化过程：一旦数据上传完成，OpenAI 的基础设施会处理微调过程。API 根据新数据调整模型参数，以提高在指定任务上的性能。
- en: •
  id: totrans-1825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Deploying the Fine-Tuned Model:'
  id: totrans-1826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署微调后的模型：
- en: –
  id: totrans-1827
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'API Integration: The fine-tuned model can be accessed and deployed via OpenAI’s
    API. This allows for seamless integration into various applications, such as chatbots,
    automated content creation tools, or specialised customer service systems.'
  id: totrans-1828
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: API 集成：可以通过 OpenAI 的 API 访问和部署微调后的模型。这使得它能够无缝集成到各种应用程序中，例如聊天机器人、自动化内容创建工具或专业客户服务系统。
- en: 10.6.2 Limitations of OpenAI’s Fine-Tuning API
  id: totrans-1829
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.6.2 OpenAI 微调 API 的局限性
- en: •
  id: totrans-1830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pricing Models: Fine-tuning and using OpenAI’s models through the API can be
    costly, especially for large-scale deployments or continuous usage. This can be
    a significant consideration for smaller organisations or budget-constrained projects.'
  id: totrans-1831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定价模型：通过 API 微调和使用 OpenAI 的模型可能会很昂贵，特别是对于大规模部署或持续使用的情况。这对小型组织或预算有限的项目来说可能是一个重要的考虑因素。
- en: •
  id: totrans-1832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Privacy and Security: Users must upload their data to OpenAI’s servers
    for the fine-tuning process. This raises potential concerns about data privacy
    and the security of sensitive or proprietary information.'
  id: totrans-1833
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据隐私和安全：用户必须将数据上传到 OpenAI 的服务器以进行微调。这引发了关于数据隐私和敏感或专有信息安全的潜在担忧。
- en: •
  id: totrans-1834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dependency on OpenAI Infrastructure: The reliance on OpenAI’s infrastructure
    for model hosting and API access can lead to vendor lock-in, limiting flexibility
    and control over the deployment environment.'
  id: totrans-1835
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对 OpenAI 基础设施的依赖：依赖于 OpenAI 的基础设施进行模型托管和 API 访问可能会导致供应商锁定，限制了对部署环境的灵活性和控制。
- en: •
  id: totrans-1836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited Control Over Training Process: The fine-tuning process is largely automated
    and managed by OpenAI, offering limited visibility and control over the specific
    adjustments made to the model.'
  id: totrans-1837
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对训练过程的控制有限：微调过程主要由 OpenAI 自动化和管理，用户对模型的具体调整所做的更改了解和控制有限。
- en: 10.6.3 Tutorials
  id: totrans-1838
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.6.3 教程
- en: '1.'
  id: totrans-1839
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[Fine-Tuning GPT-3 Using the OpenAI API](https://www.datacamp.com/tutorial/fine-tuning-gpt-3-using-the-open-ai-api-and-python)'
  id: totrans-1840
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 OpenAI API 微调 GPT-3](https://www.datacamp.com/tutorial/fine-tuning-gpt-3-using-the-open-ai-api-and-python)'
- en: 10.7 NVIDIA NeMo Customizer
  id: totrans-1841
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.7 NVIDIA NeMo Customizer
- en: NVIDIA NeMo Customiser⁸⁸8[https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/](https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/)
    is part of the NeMo framework, a suite of tools and models designed by NVIDIA
    to facilitate the development and fine-tuning of LLM models. The Customiser focuses
    specifically on making it easier to fine-tune large language models (LLMs) for
    specialised tasks and domains. Like other fine-tuning tools, NeMo Customiser is
    geared toward users who want to adapt pre-trained models for specific applications,
    such as conversational AI, translation, or domain-specific text generation. It
    delivers enterprise-ready models by offering accurate data curation, extensive
    customisation options, retrieval-augmented generation (RAG), and improved performance
    features. The platform supports training and deploying generative AI models across
    diverse environments, including cloud, data center, and edge locations. It provides
    a comprehensive package with support, security, and reliable APIs as part of the
    NVIDIA AI Enterprise.
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA NeMo Customiser⁸⁸8[https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/](https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/)
    是 NeMo 框架的一部分，这是一套由 NVIDIA 设计的工具和模型，旨在促进 LLM 模型的开发和微调。Customiser 专注于简化大语言模型 (LLMs)
    的微调，以适应专业任务和领域。与其他微调工具类似，NeMo Customiser 针对那些希望将预训练模型适应于特定应用（如对话 AI、翻译或领域特定的文本生成）的用户。它通过提供准确的数据策划、广泛的定制选项、检索增强生成
    (RAG) 和改进的性能特性来交付企业级模型。该平台支持在云、数据中心和边缘位置等多种环境中训练和部署生成 AI 模型。作为 NVIDIA AI Enterprise
    的一部分，它提供了一个全面的套餐，包括支持、安全性和可靠的 API。
- en: 10.7.1 Key Features of NVIDIA NeMo
  id: totrans-1843
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.7.1 NVIDIA NeMo 的主要特性
- en: NVIDIA NeMo is designed to enhance AI projects with several standout features.[[84](#bib.bib84)]
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA NeMo 旨在通过多个突出的特性来增强 AI 项目。[[84](#bib.bib84)]
- en: •
  id: totrans-1845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like
    NeMo Curator for preparing large-scale, high-quality datasets. These tools facilitate
    efficient pretraining of generative AI models by leveraging thousands of compute
    cores, which significantly reduces training time and enhances the accuracy of
    large language models (LLMs).
  id: totrans-1846
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最先进的训练技术 NeMo 采用了如 NeMo Curator 这样的 GPU 加速工具来准备大规模的高质量数据集。这些工具通过利用数千个计算核心来促进生成
    AI 模型的高效预训练，这大大减少了训练时间，提高了大型语言模型 (LLMs) 的准确性。
- en: •
  id: totrans-1847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Advanced Customisation for LLMs The NeMo Customiser microservice allows for
    precise fine-tuning and alignment of LLMs for specific domains. It uses model
    parallelism to speed up training and supports scaling across multiple GPUs and
    nodes, enabling the fine-tuning of larger models.
  id: totrans-1848
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级定制化针对大型语言模型（LLMs） NeMo Customiser 微服务允许对 LLMs 进行精确的微调和对齐，适应特定领域。它使用模型并行化来加速训练，并支持跨多个
    GPU 和节点的扩展，实现对更大模型的微调。
- en: •
  id: totrans-1849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference
    Server to streamline AI inference at scale. This integration accelerates generative
    AI inference, ensuring confident deployment of AI applications both on-premises
    and in the cloud.
  id: totrans-1850
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化的 AI 推理与 NVIDIA Triton NeMo 包括 NVIDIA Triton 推理服务器，以简化大规模 AI 推理。此集成加速了生成型
    AI 推理，确保了 AI 应用在本地和云端的可靠部署。
- en: •
  id: totrans-1851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture
    that simplifies the development of conversational AI models. It supports comprehensive
    workflows from data processing to deployment and includes pre-trained models for
    automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech
    (TTS), which can be fine-tuned or used as-is.
  id: totrans-1852
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户友好的生成 AI 工具 NeMo 具有模块化、可重用的架构，简化了对话式人工智能模型的开发。它支持从数据处理到部署的全面工作流程，并包括用于自动语音识别（ASR）、自然语言处理（NLP）和文本到语音（TTS）的预训练模型，这些模型可以进行微调或直接使用。
- en: •
  id: totrans-1853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained
    models and training scripts, facilitating rapid application development or fine-tuning
    for specific tasks. Currently, NeMo supports models like Llama 2, Stable Diffusion,
    and NVIDIA’s Nemotron-3 8B family.
  id: totrans-1854
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳预训练模型 NeMo Collections 提供多种预训练模型和训练脚本，便于快速应用开发或针对特定任务的微调。目前，NeMo 支持如 Llama
    2、Stable Diffusion 和 NVIDIA 的 Nemotron-3 8B 系列等模型。
- en: •
  id: totrans-1855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance,
    low-latency information retrieval, enhancing generative AI applications with enterprise-grade
    retrieval-augmented generation (RAG) capabilities. This feature supports real-time
    business insights and data Utilisation.
  id: totrans-1856
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化的检索增强生成 NeMo Retriever 提供高性能、低延迟的信息检索，增强生成型人工智能应用的企业级检索增强生成（RAG）能力。此功能支持实时业务洞察和数据利用。
- en: 10.7.2 Components of NVIDIA NeMo
  id: totrans-1857
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.7.2 NVIDIA NeMo 的组件
- en: •
  id: totrans-1858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NeMo Core Provides essential elements like the Neural Module Factory for training
    and inference, streamlining the development of conversational AI models.
  id: totrans-1859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NeMo Core 提供了基本元素，如神经模块工厂，用于训练和推理，简化了对话式人工智能模型的开发。
- en: •
  id: totrans-1860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS,
    including pre-trained models and training scripts, making the platform versatile.
  id: totrans-1861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NeMo Collections 提供专门的模块和模型，用于自动语音识别（ASR）、自然语言处理（NLP）和文本到语音（TTS），包括预训练模型和训练脚本，使平台功能多样。
- en: •
  id: totrans-1862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Neural Modules Serve as the building blocks of NeMo, defining trainable components
    such as encoders and decoders, which can be connected to create comprehensive
    models.
  id: totrans-1863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经模块 作为 NeMo 的构建块，定义了可训练的组件，如编码器和解码器，这些组件可以连接在一起，创建综合模型。
- en: •
  id: totrans-1864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Application Scripts Simplify the deployment of conversational AI models with
    ready-to-use scripts, enabling quick training or fine-tuning on specific datasets
    for various AI applications.
  id: totrans-1865
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用脚本 简化了对话式人工智能模型的部署，提供即用的脚本，实现对特定数据集的快速训练或微调，适用于各种人工智能应用。
- en: 10.7.3 Customising Large Language Models (LLMs)
  id: totrans-1866
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.7.3 定制大型语言模型（LLMs）
- en: 'While general-purpose LLMs, enhanced with prompt engineering or light fine-tuning,
    have enabled organisations to achieve successful proof-of-concept projects, transitioning
    to production presents additional challenges. Figure [10.3](#Ch10.F3 "Figure 10.3
    ‣ 10.7.3 Customising Large Language Models (LLMs) ‣ 10.7 NVIDIA NeMo Customizer
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)") illustrates NVIDIA’s detailed LLM customisation lifecycle, offering
    valuable guidance for organisations that are preparing to deploy customised models
    in a production environment [[85](#bib.bib85)].'
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过提示工程或轻微微调增强的一般目的 LLMs 已使组织实现了成功的概念验证项目，但过渡到生产环境会带来额外的挑战。图 [10.3](#Ch10.F3
    "图 10.3 ‣ 10.7.3 大型语言模型（LLMs）的定制 ‣ 10.7 NVIDIA NeMo 定制器 ‣ 第 10 章 LLM 的工业微调平台和框架
    ‣ 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战和机遇的全面回顾（版本 1.0）") 说明了 NVIDIA 详细的 LLM 定制生命周期，为准备在生产环境中部署定制模型的组织提供了宝贵的指导
    [[85](#bib.bib85)]。
- en: '![Refer to caption](img/1404b9959d6a8456418abd2348463b61.png)'
  id: totrans-1868
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1404b9959d6a8456418abd2348463b61.png)'
- en: 'Figure 10.3: Nvidia NeMo Framework for Customising and Deploying LLMs. The
    Nvidia NeMo framework is designed for end-to-end customisation and deployment
    of large language models (LLMs). This diagram illustrates the process from data
    curation and distributed training of foundation models, through model customisation,
    to accelerated inference with guardrails. The platform enables AI developers to
    integrate in-domain, secure, and cited responses into enterprise applications,
    ensuring that LLMs are effectively tailored for specific tasks and industries.
    The NeMo framework, supported by Nvidia AI Enterprise, also offers robust support
    for various pre-trained foundation models like OpenAI’s GPT family, ensuring scalability
    and reliability in AI deployments. (adapted from [[85](#bib.bib85)])'
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.3: Nvidia NeMo 框架用于定制和部署大型语言模型（LLMs）。Nvidia NeMo 框架旨在实现大型语言模型（LLMs）的端到端定制和部署。该图示说明了从数据整理和基础模型的分布式训练，通过模型定制，到加速推断的全过程。该平台使
    AI 开发人员能够将领域内、安全且经过引用的响应集成到企业应用程序中，确保 LLMs 能够有效地针对特定任务和行业进行调整。NeMo 框架由 Nvidia
    AI Enterprise 支持，还提供对各种预训练基础模型的强大支持，如 OpenAI 的 GPT 系列，确保 AI 部署的可扩展性和可靠性。（改编自 [[85](#bib.bib85)]）'
- en: '1.'
  id: totrans-1870
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Model Selection or Development
  id: totrans-1871
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择或开发
- en: NVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and
    supports the integration of other open-source models of any size. Alternatively,
    users can develop their own models, starting with data curation, which includes
    selecting, labeling, cleansing, validating, and integrating data. This process,
    better termed data engineering, involves additional analysis, designing storage,
    evaluating model training results, and incorporating reinforcement learning with
    human feedback (RLHF). While building a custom foundation model is often costly,
    complex, and time-consuming, most enterprises opt to start with a pre-trained
    model and focus on customisation.
  id: totrans-1872
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NVIDIA 提供了从 8B 到 43B 参数范围的各种预训练模型，并支持集成任何规模的其他开源模型。或者，用户可以从数据整理开始开发自己的模型，包括选择、标记、清洗、验证和整合数据。这个过程，准确来说是数据工程，涉及额外的分析、设计存储、评估模型训练结果以及结合人类反馈的强化学习（RLHF）。虽然构建自定义基础模型通常成本高、复杂且耗时，但大多数企业选择从预训练模型开始，并专注于定制。
- en: '2.'
  id: totrans-1873
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Model Customisation
  id: totrans-1874
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型定制
- en: Model customisation involves optimising performance with task-specific datasets
    and adjusting model weights. NeMo offers recipes for customisation, and enterprises
    can choose models already tailored to specific tasks and then fine-tune them with
    proprietary data.
  id: totrans-1875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型定制涉及使用特定任务数据集来优化性能并调整模型权重。NeMo 提供了定制的配方，企业可以选择已经针对特定任务调整的模型，然后用专有数据进行微调。
- en: '3.'
  id: totrans-1876
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Inference
  id: totrans-1877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推断
- en: Inference refers to running models based on user queries. This phase involves
    considering hardware, architecture, and performance factors that significantly
    impact usability and cost in production.
  id: totrans-1878
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推断指的是基于用户查询运行模型。此阶段涉及考虑硬件、架构和性能因素，这些因素对生产中的可用性和成本有重大影响。
- en: '4.'
  id: totrans-1879
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Guardrails
  id: totrans-1880
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保护措施
- en: NVIDIA employs guardrails as intermediary services between models and applications.
    These services review incoming prompts for policy compliance, execute arbitration
    or orchestration steps, and ensure model responses adhere to policies. Guardrails
    help maintain relevance, accuracy, safety, privacy, and security.
  id: totrans-1881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NVIDIA使用保护措施作为模型和应用之间的中介服务。这些服务审核传入的提示以确保符合政策，执行仲裁或编排步骤，并确保模型响应符合政策。保护措施有助于保持相关性、准确性、安全性、隐私和安全。
- en: '5.'
  id: totrans-1882
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Applications
  id: totrans-1883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用
- en: NVIDIA’s framework presents enterprise applications as LLM-ready, though this
    is not always the case. Existing applications may be connected to LLMs to enable
    new features. However, creating assistants for knowledge access or task execution
    often involves designing new applications specifically for natural language interfaces.
  id: totrans-1884
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NVIDIA的框架将企业应用程序呈现为LLM就绪，但这并不总是如此。现有应用程序可能会连接到LLM以启用新功能。然而，为知识访问或任务执行创建助手通常涉及专门为自然语言接口设计新的应用程序。
- en: 10.7.4 Tutorials
  id: totrans-1885
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.7.4 教程
- en: '1.'
  id: totrans-1886
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[Introduction to NVIDIA NeMo — Tutorial and Example](https://medium.com/@khang.pham.exxact/introduction-to-nvidia-nemo-tutorial-example-478f6ba6b160)'
  id: totrans-1887
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[NVIDIA NeMo简介 — 教程和示例](https://medium.com/@khang.pham.exxact/introduction-to-nvidia-nemo-tutorial-example-478f6ba6b160)'
- en: '2.'
  id: totrans-1888
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '[How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/nmt-python-advanced-finetune-nmt-model-with-nemo.html)'
  id: totrans-1889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[如何使用Nvidia NeMo微调Riva NMT双语模型](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/nmt-python-advanced-finetune-nmt-model-with-nemo.html)'
- en: Chapter 11 Multimodal LLMs and their Fine-tuning
  id: totrans-1890
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章 多模态LLMs及其微调
- en: A multimodal model is a machine learning model that can process information
    from various modalities, such as images, videos, and text. For instance, Google’s
    multimodal model, Gemini[[86](#bib.bib86)], can analyse a photo of a plate of
    cookies and produce a written recipe in response, and it can perform the reverse
    as well.
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型是一种能够处理来自各种模态的信息的机器学习模型，如图像、视频和文本。例如，谷歌的多模态模型Gemini[[86](#bib.bib86)]可以分析一张饼干盘的照片，并生成一个书面食谱作为回应，同时也可以进行反向操作。
- en: The difference between Generative AI and Multimodal AI is that generative AI
    refers to the use of machine learning models to create new content, such as text,
    images, music, audio, and videos, typically from a single type of input. Multimodal
    AI extends these generative capabilities by processing information from multiple
    modalities, including images, videos, and text. This enables the AI to understand
    and interpret different sensory modes, allowing users to input various types of
    data and receive a diverse range of content types in return.
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI和多模态AI的区别在于，生成式AI指的是利用机器学习模型创建新内容，如文本、图像、音乐、音频和视频，通常来自单一类型的输入。多模态AI通过处理来自多种模态的信息，如图像、视频和文本，扩展了这些生成能力。这使得AI能够理解和解释不同的感官模式，使用户可以输入各种类型的数据，并获得多种类型的内容作为回应。
- en: '![Refer to caption](img/6ffde3048ded9e0311651b5561e63908.png)'
  id: totrans-1893
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6ffde3048ded9e0311651b5561e63908.png)'
- en: 'Figure 11.1: Timeline of Multimodal Model Developments — This figure illustrates
    the progression of significant multimodal models, highlighting key releases from
    major tech companies and research institutions from December 2023 to March 2024\.
    The timeline showcases models like Google’s TinyGPT-V and Gemini Nano, along with
    other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVA-Gemma, indicating
    the rapid advancement in multimodal AI technologies (adapted from [[87](#bib.bib87)]).'
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：多模态模型发展时间线 — 该图展示了显著的多模态模型的发展历程，突出显示了从2023年12月到2024年3月主要科技公司和研究机构的关键发布。时间线展示了像谷歌的TinyGPT-V和Gemini
    Nano这样的模型，以及MoE-LLAVA、DeepSeek-VL和LLAVA-Gemma等其他创新，表明了多模态AI技术的快速进步（改编自[[87](#bib.bib87)]）。
- en: 11.1 Vision Language Model (VLMs)
  id: totrans-1895
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1 视觉语言模型（VLMs）
- en: Vision language models encompass multimodal models capable of learning from
    both images and text inputs. They belong to the category of generative models
    that utilise image and text data to produce textual outputs. These models, especially
    at larger scales, demonstrate strong zero-shot capabilities, exhibit robust generalisation
    across various tasks, and effectively handle diverse types of visual data such
    as documents and web pages. Typical applications include conversational interactions
    involving images, image interpretation based on textual instructions, answering
    questions related to visual content, understanding documents, generating captions
    for images, and more. Certain advanced vision language models can also understand
    spatial attributes within images. They can generate bounding boxes or segmentation
    masks upon request to identify or isolate specific subjects, localise entities
    within images, or respond to queries regarding their relative or absolute positions.
    The landscape of large vision language models is characterised by considerable
    diversity in training data, image encoding techniques, and consequently, their
    functional capabilities.
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言模型包括能够从图像和文本输入中学习的多模态模型。它们属于生成模型的类别，利用图像和文本数据生成文本输出。这些模型，特别是在更大规模下，表现出强大的零-shot能力，展示了在各种任务中的鲁棒泛化，并有效处理多样的视觉数据类型，如文档和网页。典型应用包括涉及图像的对话互动、基于文本指令的图像解释、回答与视觉内容相关的问题、理解文档、为图像生成描述等。某些高级视觉语言模型还可以理解图像中的空间属性。它们可以根据请求生成边界框或分割掩膜，以识别或隔离特定对象、定位图像中的实体或回答有关它们相对或绝对位置的问题。大型视觉语言模型的特点是训练数据、图像编码技术的多样性以及因此而来的功能能力的差异。
- en: 11.1.1 Architecture
  id: totrans-1897
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.1 架构
- en: 'Vision-language models adeptly integrate both visual and textual information,
    leveraging three fundamental components:'
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语言模型巧妙地整合了视觉和文本信息，利用三个基本组件：
- en: •
  id: totrans-1899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Image Encoder: This component translates visual data (images) into a format
    that the model can process.'
  id: totrans-1900
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像编码器：这个组件将视觉数据（图像）转换为模型可以处理的格式。
- en: •
  id: totrans-1901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Text Encoder: Similar to the image encoder, this component converts textual
    data (words and sentences) into a format the model can understand.'
  id: totrans-1902
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本编码器：类似于图像编码器，这个组件将文本数据（单词和句子）转换为模型可以理解的格式。
- en: •
  id: totrans-1903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fusion Strategy: This component combines the information from both the image
    and text encoders, merging the two data types into a unified representation.'
  id: totrans-1904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 融合策略：这个组件将图像编码器和文本编码器的信息结合起来，将这两种数据类型合并为统一的表示。
- en: These elements work collaboratively, with the model’s learning process (loss
    functions) specifically tailored to the architecture and learning strategy employed.
    Although the concept of vision-language models is not new, their construction
    has evolved significantly. Early models used manually crafted image descriptions
    and pre-trained word vectors. Modern models, however, utilise transformers—an
    advanced neural network architecture—for both image and text encoding. These encoders
    can learn features either independently or jointly.
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素协同工作，模型的学习过程（损失函数）专门针对所采用的架构和学习策略进行定制。尽管视觉-语言模型的概念并不新颖，但它们的构建已显著演变。早期的模型使用手动制作的图像描述和预训练的词向量。然而，现代模型利用变换器——一种先进的神经网络架构——进行图像和文本编码。这些编码器可以独立或联合学习特征。
- en: A crucial aspect of these models is pre-training. Before being applied to specific
    tasks, the models are trained on extensive datasets using carefully selected objectives.
    This pre-training equips them with the foundational knowledge required to excel
    in various downstream applications. Following is one of the example architectures
    of VLMs.
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的一个关键方面是预训练。在应用于具体任务之前，模型会在广泛的数据集上进行训练，使用精心选择的目标。这个预训练使它们具备了在各种下游应用中表现出色的基础知识。以下是VLMs的一个示例架构。
- en: 11.1.2 Contrastive Learning
  id: totrans-1907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.2 对比学习
- en: Contrastive learning is a technique that focuses on understanding the differences
    between data points. It computes a similarity score between instances and aims
    to minimise contrastive loss, making it particularly useful in semi-supervised
    learning where a limited number of labelled samples guide the optimisation process
    to classify unseen data points.
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习是一种专注于理解数据点之间差异的技术。它计算实例之间的相似度得分，并旨在最小化对比损失，使其在半监督学习中特别有用，在这种学习中，有限的标注样本指导优化过程，以对未见数据点进行分类。
- en: How it works
  id: totrans-1909
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: For instance, to recognise a cat, contrastive learning compares a cat image
    with a similar cat image and a dog image. The model learns to distinguish between
    a cat and a dog by identifying features such as facial structure, body size, and
    fur. By determining which image is closer to the ”anchor” image, the model predicts
    its class.
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了识别一只猫，对比学习将猫图像与相似的猫图像和一只狗图像进行比较。模型通过识别如面部结构、身体大小和毛发等特征来区分猫和狗。通过确定哪个图像更接近“锚”图像，模型预测其类别。
- en: '![Refer to caption](img/2b5ac58769244fa43cc55965292fd8f9.png)'
  id: totrans-1911
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2b5ac58769244fa43cc55965292fd8f9.png)'
- en: 'Figure 11.2: Workflow of Contrastive Pre-Training for Multimodal Models. This
    figure illustrates the process of contrastive pre-training where text and image
    encoders are trained to align representations from both modalities. Step 1 involves
    contrastive pre-training by pairing text and image data, while Step 2 showcases
    the creation of a dataset classifier using label text encoded by the text encoder.
    Step 3 demonstrates the model’s application for zero-shot prediction by leveraging
    the pre-trained text and image encoders. This method enables the model to generalise
    across various tasks without requiring task-specific fine-tuning (adopted from
    [[88](#bib.bib88)]).'
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：多模态模型对比预训练的工作流程。此图展示了对比预训练的过程，其中文本和图像编码器被训练以对齐来自两种模态的表示。步骤 1 涉及通过配对文本和图像数据进行对比预训练，而步骤
    2 展示了使用文本编码器编码的标签文本创建数据集分类器。步骤 3 展示了模型如何通过利用预训练的文本和图像编码器进行零样本预测。这种方法使模型能够在各种任务中进行泛化，而无需特定任务的微调（采用自[[88](#bib.bib88)]）。
- en: 'CLIP is a model that utilises contrastive learning to compute similarity between
    text and image embeddings through textual and visual encoders. It follows a three-step
    process for zero-shot predictions:'
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 是一种利用对比学习来计算文本和图像嵌入之间相似性的模型，通过文本和视觉编码器完成。这一过程分为三个步骤用于零样本预测：
- en: •
  id: totrans-1914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pre-training: Trains a text and image encoder to learn image-text pairs.'
  id: totrans-1915
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练：训练文本和图像编码器以学习图像-文本对。
- en: •
  id: totrans-1916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Caption Conversion: Converts training dataset classes into captions.'
  id: totrans-1917
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标题转换：将训练数据集类转换为标题。
- en: •
  id: totrans-1918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-Shot Prediction: Estimates the best caption for a given input image based
    on learned similarities.'
  id: totrans-1919
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零样本预测：根据学习到的相似性估计给定输入图像的最佳标题。
- en: 11.2 Fine-tuning of multimodal models
  id: totrans-1920
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2 多模态模型的微调
- en: For fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such
    as LoRA and QLoRA can be utilised. The process of fine-tuning for multimodal applications
    is analogous to that for large language models, with the primary difference being
    the nature of the input data. In addition to LoRA, which employs matrix factorisation
    techniques to reduce the number of parameters, other tools such as LLM-Adapters
    and (IA)³[[89](#bib.bib89)] can be effectively used. LLM-Adapters integrate various
    adapter modules into the pre-trained model’s architecture, enabling parameter-efficient
    fine-tuning for diverse tasks by updating only the adapter parameters while keeping
    the base model parameters fixed. (IA)³, or Infused Adapters by Inhibiting and
    Amplifying Inner Activations, enhances performance by learning vectors to weight
    model parameters through activation multiplications, supporting robust few-shot
    performance and task mixing without manual adjustments. Moreover, dynamic adaptation
    techniques like DyLoRA[[90](#bib.bib90)] allow for the training of low-rank adaptation
    blocks across different ranks, optimising the learning process by sorting the
    representations during training. LoRA-FA[[91](#bib.bib91)], a variant of LoRA,
    optimises the fine-tuning process by freezing the first low-rank matrix after
    initialisation and using it as a random projection while training the other, thereby
    reducing the number of parameters by half without compromising performance.
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多模态大型语言模型（MLLM）的微调，可以使用诸如LoRA和QLoRA等PEFT技术。多模态应用的微调过程类似于大型语言模型，主要区别在于输入数据的性质。除了LoRA，它利用矩阵分解技术来减少参数数量，LLM-Adapters和（IA）³[[89](#bib.bib89)]等其他工具也可以有效使用。LLM-Adapters将各种适配器模块集成到预训练模型的架构中，通过只更新适配器参数而保持基础模型参数固定，从而实现对各种任务的参数高效微调。（IA）³，即通过抑制和放大内在激活的注入适配器，通过激活乘法学习向量来加权模型参数，增强了性能，支持强大的少样本性能和任务混合，而无需手动调整。此外，像DyLoRA[[90](#bib.bib90)]这样的动态适应技术允许在不同等级的低秩适应块上进行训练，通过在训练过程中对表示进行排序来优化学习过程。LoRA-FA[[91](#bib.bib91)]，作为LoRA的变体，通过在初始化后冻结第一个低秩矩阵，并将其用作随机投影，同时训练其他矩阵，从而优化了微调过程，在不妨碍性能的情况下将参数数量减少了一半。
- en: The Efficient Attention Skipping (EAS)[[92](#bib.bib92)] module introduces a
    novel parameter and computation-efficient tuning method for MLLMs, aiming to maintain
    high performance while reducing parameter and computation costs for downstream
    tasks. However, MemVP[[93](#bib.bib93)] critiques this approach, noting that it
    still increases the input length of language models. To address this, MemVP integrates
    visual prompts with the weights of Feed Forward Networks, thereby injecting visual
    knowledge to decrease training time and inference latency, ultimately outperforming
    previous PEFT methods.
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 高效注意力跳跃（EAS）[[92](#bib.bib92)]模块引入了一种新颖的参数和计算高效的调优方法，旨在保持高性能的同时，降低下游任务的参数和计算成本。然而，MemVP[[93](#bib.bib93)]批评了这种方法，指出它仍然增加了语言模型的输入长度。为了解决这一问题，MemVP将视觉提示与前馈网络的权重集成，从而注入视觉知识，以减少训练时间和推理延迟，最终超越了之前的PEFT方法。
- en: 11.2.1 Full-parameter Fine-Tuning
  id: totrans-1923
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1 全参数微调
- en: Methods such as those introduced by LOMO[[94](#bib.bib94)] and MeZO[[95](#bib.bib95)]
    provide alternative solutions by focusing on memory efficiency. LOMO utilises
    a low-memory optimisation technique derived from Stochastic Gradient Descent (SGD),
    reducing memory consumption typically associated with the ADAM optimiser. MeZO,
    on the other hand, offers a memory-efficient optimiser that requires only two
    forward passes to compute gradients, enabling comprehensive fine-tuning of large
    models with a memory footprint equivalent to inference [[87](#bib.bib87)].
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
  zh: 如LOMO[[94](#bib.bib94)]和MeZO[[95](#bib.bib95)]所介绍的方法，通过关注内存效率提供了替代解决方案。LOMO利用源自随机梯度下降（SGD）的低内存优化技术，减少了通常与ADAM优化器相关的内存消耗。另一方面，MeZO提供了一种内存高效的优化器，只需两次前向传播即可计算梯度，从而在内存占用相当于推理的情况下，能够对大型模型进行全面的微调[[87](#bib.bib87)]。
- en: 11.2.2 Case study of fine-tuning MLLMs for Medical domain
  id: totrans-1925
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2 医疗领域MLLM微调案例研究
- en: The following section provides a case study on fine-tuning MLLMs for the Visual
    Question Answering (VQA) task. In this example, we present a PEFT for fine-tuning
    MLLM specifically designed for Med-VQA applications. To ensure accurate performance
    measurement, human evaluations were conducted, demonstrating that the model achieves
    an overall accuracy of 81.9% and surpasses the GPT-4v model by a substantial margin
    of 26% in absolute accuracy on closed-ended questions.
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了一个关于为视觉问答（VQA）任务微调MLLMs的案例研究。在这个例子中，我们展示了一种针对Med-VQA应用特别设计的PEFT微调MLLM的方法。为了确保准确的性能测量，进行了人工评估，结果表明该模型的整体准确率为81.9%，在封闭式问题上超越GPT-4v模型的绝对准确率26%。
- en: 'The model consists of three components: the vision encoder, a pre-trained Large
    Language Model (LLM) for handling multimodal inputs and generating responses,
    and a single linear layer for projecting embeddings from the visual encoding space
    to the LLM space, as shown in figure [11.3](#Ch11.F3 "Figure 11.3 ‣ 11.2.2 Case
    study of fine-tuning MLLMs for Medical domain ‣ 11.2 Fine-tuning of multimodal
    models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)").'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: '该模型由三个组件组成：视觉编码器、用于处理多模态输入和生成响应的预训练大语言模型（LLM），以及一个单一的线性层，用于将视觉编码空间中的嵌入投影到LLM空间，如图[11.3](#Ch11.F3
    "Figure 11.3 ‣ 11.2.2 Case study of fine-tuning MLLMs for Medical domain ‣ 11.2
    Fine-tuning of multimodal models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")所示。'
- en: The Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual
    embeddings, with model weights remaining frozen during the fine-tuning process.
    The technique from MiniGPT-v2 is utilised, grouping four consecutive tokens into
    one visual embedding to efficiently reduce resource consumption by concatenating
    on the embedding dimension.
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer（ViT）类型的骨干网络EVA将图像令牌编码为视觉嵌入，模型权重在微调过程中保持冻结。采用了MiniGPT-v2中的技术，将四个连续的令牌组合成一个视觉嵌入，以通过在嵌入维度上拼接来高效减少资源消耗。
- en: These grouped visual tokens are then processed through the projection layer,
    resulting in embeddings (length 4096) in the LLM space. A multimodal prompt template
    integrates both visual and question information, which is input into the pre-trained
    LLM, LLaMA2-chat(7B), for answer generation. The low-rank adaptation (LoRA) technique
    is applied for efficient fine-tuning, keeping the rest of the LLM frozen during
    downstream fine-tuning. A beam search with a width of 1 is utilised.
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分组的视觉令牌随后通过投影层处理，生成在LLM空间中的嵌入（长度4096）。多模态提示模板整合了视觉和问题信息，这些信息被输入到预训练的LLM LLaMA2-chat(7B)中用于生成答案。应用了低秩适应（LoRA）技术以高效进行微调，在下游微调过程中保持LLM的其余部分冻结。使用了宽度为1的束搜索。
- en: '![Refer to caption](img/907c0f87093623583b24af4debd9c6a6.png)'
  id: totrans-1930
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/907c0f87093623583b24af4debd9c6a6.png)'
- en: 'Figure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained
    LLM with a Vision Encoder for medical visual question answering tasks. The architecture
    includes stages for processing images and generating contextually relevant responses,
    demonstrating the integration of vision and language models in a medical setting
    (adopted from [[96](#bib.bib96)]).'
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：Med VQA架构概述，集成了LoRA和一个与视觉编码器结合的预训练LLM，用于医学视觉问答任务。该架构包括处理图像和生成上下文相关响应的阶段，展示了在医学环境中视觉和语言模型的集成（采自[[96](#bib.bib96)]）。
- en: 'The multimodal prompt includes input images, questions, and a specific token
    for VQA tasks, following the MiniGPT-v2 template. In Figure [11.3](#Ch11.F3 "Figure
    11.3 ‣ 11.2.2 Case study of fine-tuning MLLMs for Medical domain ‣ 11.2 Fine-tuning
    of multimodal models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)"), the image features derived from linear projection
    are labelled as ImageFeature, with the corresponding questions serving as text
    instructions. The special token [VQA] is used as the task identifier, forming
    the complete multimodal instructional template:'
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态提示包括输入图像、问题和特定的VQA任务标记，遵循MiniGPT-v2模板。在图像[11.3](#Ch11.F3 "图11.3 ‣ 11.2.2
    医疗领域MLLM微调案例研究 ‣ 11.2 多模态模型的微调 ‣ 第11章 多模态LLMs及其微调 ‣ 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽评审（版本1.0）")中，从线性投影派生的图像特征标记为ImageFeature，相应的问题作为文本指令。特殊标记[VQA]用作任务标识符，形成完整的多模态指令模板：
- en: '[PRE1]'
  id: totrans-1933
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Model Training
  id: totrans-1934
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Weights from MiniGPT-v2, pre-trained on general domain datasets, are further
    fine-tuned using multimodal medical datasets in two stages. The LoRA technique
    is employed for efficient fine-tuning, updating only a small portion of the entire
    model, as detailed below:'
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: 从MiniGPT-v2获得的权重，在一般领域数据集上预训练后，使用多模态医疗数据集进行两阶段的进一步微调。采用LoRA技术进行高效微调，仅更新整个模型的一小部分，具体如下：
- en: •
  id: totrans-1936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning with image captioning: During this stage, the model is fine-tuned
    using the ROCO medical image-caption dataset, which contains medical image-caption
    pairs of varying lengths. The prompt template used is <Img><ImageHere></Img>[caption]
    <instruction>, with the instruction prompt randomly selected from a pool of four
    candidates, such as “Briefly describe this image.” During training, only the linear
    projection layer and the LoRA layer in the LLM are fine-tuned, while other parts
    of the model remain frozen.'
  id: totrans-1937
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有图像描述的微调：在此阶段，模型使用ROCO医疗图像-描述数据集进行微调，该数据集包含长度不同的医疗图像-描述对。使用的提示模板为<Img><ImageHere></Img>[caption]
    <instruction>，指令提示从四个候选项中随机选择，例如“简要描述此图像。”在训练过程中，仅微调LLM中的线性投影层和LoRA层，而模型的其他部分保持不变。
- en: •
  id: totrans-1938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA
    dataset, VQA-RAD, which contains triplets of images, questions, and answers. Following
    the instruction template proposed in MiniGPT-v2, the template used is: “[INST]
    <img><ImageFeature></img>[VQA] Instruction [/INST]”, where the instruction prompt
    is: “Based on the image, respond to this question with a short answer: question,”
    with question signifying the question corresponding to the given medical image.
    The motivation for generating short answers is to validate against the existing
    labelled data in VQA-RAD, where the answers are typically short in both open-ended
    and closed-ended QA pairs. Similar to the first stage, the vision encoder and
    the LLM remain frozen while only the linear projection and LoRA layers in the
    LLM are updated.'
  id: totrans-1939
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在VQA上的微调：在第二阶段，模型在包含图像、问题和答案三元组的Med-VQA数据集VQA-RAD上进行微调。根据MiniGPT-v2提出的指令模板，使用的模板是：“[INST]
    <img><ImageFeature></img>[VQA] 指令 [/INST]”，其中指令提示为：“根据图像，用简短的回答回答这个问题：问题”，其中问题指的是与给定医疗图像对应的问题。生成简短答案的动机是为了验证现有的VQA-RAD标注数据，其中答案通常在开放性和封闭性问答对中都很简短。与第一阶段类似，视觉编码器和LLM保持不变，只更新LLM中的线性投影和LoRA层。
- en: 11.3 Applications of Multimodal models
  id: totrans-1940
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3 多模态模型的应用
- en: '1.'
  id: totrans-1941
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Gesture Recognition - These models interpret and recognise human gestures, which
    is crucial for sign language translation. Multimodal models facilitate inclusive
    communication by processing gestures and converting them into text or speech.
  id: totrans-1942
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手势识别 - 这些模型解读和识别人的手势，这对于手语翻译至关重要。多模态模型通过处理手势并将其转换为文本或语音，促进了包容性沟通。
- en: '2.'
  id: totrans-1943
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Video Summarisation - Multimodal models can summarise lengthy videos by extracting
    key visual and audio elements. This capability streamlines content consumption,
    enables efficient content browsing, and enhances video content management platforms.
  id: totrans-1944
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频总结 - 多模态模型可以通过提取关键视觉和音频元素来总结长视频。这一能力简化了内容消费、实现了高效的内容浏览，并增强了视频内容管理平台。
- en: '3.'
  id: totrans-1945
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: DALL-E is a notable example of multimodal AI that generates images from textual
    descriptions. This technology expands creative possibilities in content creation
    and visual storytelling, with applications in art, design, advertising, and more.
  id: totrans-1946
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DALL-E 是一个值得注意的多模态AI示例，它可以从文本描述中生成图像。这项技术扩展了内容创作和视觉讲故事的创意可能性，应用于艺术、设计、广告等领域。
- en: '4.'
  id: totrans-1947
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Educational Tools - Multimodal models enhance learning experiences by providing
    interactive educational content that responds to both visual and verbal cues from
    students. They are integral to adaptive learning platforms that adjust content
    and difficulty based on student performance and feedback.
  id: totrans-1948
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教育工具 - 多模态模型通过提供响应学生视觉和语言提示的互动教育内容来增强学习体验。它们在适应性学习平台中不可或缺，这些平台根据学生的表现和反馈调整内容和难度。
- en: '5.'
  id: totrans-1949
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Virtual Assistants - Multimodal models power virtual assistants by understanding
    and responding to voice commands while processing visual data for comprehensive
    user interaction. They are essential for smart home automation, voice-controlled
    devices, and digital personal assistants.
  id: totrans-1950
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虚拟助手 - 多模态模型通过理解和响应语音命令，同时处理视觉数据以实现全面的用户互动，为虚拟助手提供动力。它们对于智能家居自动化、语音控制设备和数字个人助理至关重要。
- en: 11.4 Audio or Speech LLMs Or Large Audio Models
  id: totrans-1951
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4 音频或语音LLM或大型音频模型
- en: Audio or speech LLMs are models designed to understand and generate human language
    based on audio inputs. They have applications in speech recognition, text-to-speech
    conversion, and natural language understanding tasks. These models are typically
    pre-trained on large datasets to learn generic language patterns, which are then
    fine-tuned on specific tasks or domains to enhance performance.
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 音频或语音LLM是旨在理解和生成基于音频输入的人类语言的模型。它们在语音识别、文本转语音转换和自然语言理解任务中具有应用。这些模型通常在大规模数据集上进行预训练，以学习通用语言模式，然后在特定任务或领域上进行微调，以提升性能。
- en: Audio and Speech Large Language Models (LLMs) represent a significant advancement
    in the integration of language processing with audio signals. These models leverage
    a robust Large Language Model as a foundational backbone, which is enhanced to
    handle multimodal data through the inclusion of custom audio tokens. This transformation
    allows the models to learn and operate within a shared multimodal space, where
    both text and audio signals can be effectively processed.
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音大型语言模型（LLM）代表了语言处理与音频信号整合的重大进展。这些模型利用强大的大型语言模型作为基础框架，通过包含自定义音频令牌来增强以处理多模态数据。这一转变使得模型能够在一个共享的多模态空间中学习和操作，在这里文本和音频信号都能有效地被处理。
- en: 'Unlike text, which is inherently discrete, audio signals are continuous and
    need to be discretized into manageable audio tokens. Techniques like HuBERT[[97](#bib.bib97)]
    and wav2vec[[98](#bib.bib98)] are employed for this purpose, converting audio
    into a tokenized format that the LLM can process alongside text. The model, typically
    autoregressive and decoder-based, is pre-trained using a combination of self-supervised
    tasks, such as predicting masked tokens in interleaved text and audio, and supervised
    fine-tuning for specific tasks like transcription or sentiment analysis. This
    capability to handle and generate audio and text simultaneously allows for a wide
    range of applications, from audio question answering to speech-based sentiment
    detection, making Audio and Speech LLMs a versatile tool in multimodal AI. The
    figure [11.4](#Ch11.F4 "Figure 11.4 ‣ 11.4 Audio or Speech LLMs Or Large Audio
    Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)") illustrates an example of a multimodal Audio LM architecture. In this setup,
    a prompt provides instructions in both text and audio formats. The audio is tokenized
    using an audio tokenizer. The multimodal model then combines these text and audio
    tokens and generates spoken speech through a vocoder (also known as a voice decoder).'
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本不同，文本本质上是离散的，而音频信号是连续的，需要离散化为可管理的音频标记。像 HuBERT[[97](#bib.bib97)] 和 wav2vec[[98](#bib.bib98)]
    这样的技术被用来将音频转换为大模型可以处理的标记化格式，与文本一起处理。该模型通常是自回归的和基于解码器的，通过自监督任务（如预测交错文本和音频中的掩蔽标记）以及监督微调（如转录或情感分析）进行预训练。这种同时处理和生成音频与文本的能力使得应用范围非常广泛，从音频问答到基于语音的情感检测，使得音频和语音大模型在多模态
    AI 中成为一个多才多艺的工具。图 [11.4](#Ch11.F4 "图 11.4 ‣ 11.4 音频或语音大模型或大型音频模型 ‣ 第 11 章 多模态大模型及其微调
    ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾 (版本 1.0)") 说明了一个多模态音频语言模型架构的示例。在此设置中，提示以文本和音频格式提供指令。音频通过音频标记器进行标记化。多模态模型然后结合这些文本和音频标记，通过声码器（也称为语音解码器）生成语音。
- en: '![Refer to caption](img/eb896cee30b5a45918b541748e21af56.png)'
  id: totrans-1955
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/eb896cee30b5a45918b541748e21af56.png)'
- en: 'Figure 11.4: Multimodal Audio-Text Language Model architecture that integrates
    text and audio inputs for advanced multimodal processing. The architecture utilises
    text tokenizers and audio encoders/tokenizers to convert inputs into tokens, which
    are then processed by the audio-text LM. This model supports both discrete and
    continuous speech processing and enables tasks such as sentiment analysis and
    response generation in natural language. The audio tokens are further refined
    using a vocoder, while text tokens are detokenized to produce coherent text outputs
    (adapted from [[99](#bib.bib99)]).'
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：集成文本和音频输入以实现高级多模态处理的多模态音频-文本语言模型架构。该架构利用文本标记器和音频编码器/标记器将输入转换为标记，然后由音频-文本大模型处理。该模型支持离散和连续语音处理，并实现自然语言中的情感分析和响应生成等任务。音频标记通过声码器进一步精炼，而文本标记则被反标记化以生成连贯的文本输出（改编自
    [[99](#bib.bib99)]）。
- en: Audio and speech LLMs like AudioPaLM[[100](#bib.bib100)], AudioLM[[101](#bib.bib101)],
    and various adaptations of models like Whisper and LLaMA, integrate capabilities
    for understanding and generating audio data, including speech-to-text (STT), text-to-speech
    (TTS), and speech-to-speech (STS) translation. These models have shown that LLMs,
    initially designed for text, can be effectively adapted for audio tasks through
    sophisticated tokenization and fine-tuning techniques.
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音大模型（LLMs），如 AudioPaLM[[100](#bib.bib100)]、AudioLM[[101](#bib.bib101)]，以及
    Whisper 和 LLaMA 等模型的各种改编，整合了理解和生成音频数据的能力，包括语音转文本（STT）、文本转语音（TTS）和语音转语音（STS）翻译。这些模型表明，最初设计用于文本的大模型可以通过复杂的标记化和微调技术有效地适应音频任务。
- en: 11.4.1 Tokenization and Preprocessing
  id: totrans-1958
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.1 标记化和预处理
- en: A key aspect of adapting LLMs for audio is the tokenization of audio data into
    discrete representations that the model can process. For instance, AudioLM and
    AudioPaLM utilise a combination of acoustic and semantic tokens. Acoustic tokens
    capture the high-quality audio synthesis aspect, while semantic tokens help maintain
    long-term structural coherence in the generated audio. This dual-token approach
    allows the models to handle both the intricacies of audio waveforms and the semantic
    content of speech.
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: 适应音频的 LLM 的一个关键方面是将音频数据分词为模型可以处理的离散表示。例如，AudioLM 和 AudioPaLM 利用声学和语义标记的组合。声学标记捕捉高质量音频合成方面，而语义标记帮助保持生成音频的长期结构一致性。这种双标记方法使模型能够处理音频波形的复杂性和语音的语义内容。
- en: 11.4.2 Fine-Tuning Techniques
  id: totrans-1960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.2 微调技术
- en: 'Fine-tuning audio and speech LLMs typically involve several key strategies:'
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: 微调音频和语音 LLM 通常涉及几个关键策略：
- en: •
  id: totrans-1962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Full Parameter Fine-Tuning: This involves updating all the model’s parameters
    during fine-tuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters
    to adapt pre-trained text LLMs to various audio tasks, although this can be computationally
    expensive.'
  id: totrans-1963
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完全参数微调：这涉及在微调期间更新模型的所有参数。例如，LauraGPT 和 SpeechGPT 微调所有参数，以适应各种音频任务，尽管这可能会耗费较大的计算资源。
- en: •
  id: totrans-1964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update
    only specific layers or modules of the model. This method significantly reduces
    computational requirements while still allowing effective adaptation. Models like
    Qwen-Audio leverage LoRA to fine-tune pre-trained components for enhanced performance
    on speech recognition tasks.'
  id: totrans-1965
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层特定微调：像 LoRA（低秩适配）这样的技术只更新模型的特定层或模块。这种方法显著减少了计算需求，同时仍能有效适应。像 Qwen-Audio 这样的模型利用
    LoRA 对预训练组件进行微调，以提高语音识别任务的性能。
- en: •
  id: totrans-1966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper
    encoder, freeze certain parts of the model (like the speech encoder) and only
    fine-tune a linear projector or specific adapters to align the speech and text
    modalities. This approach simplifies the training process and enhances efficiency[[102](#bib.bib102)].'
  id: totrans-1967
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于组件的微调：最近的模型，例如那些集成 Whisper 编码器的模型，会冻结模型的某些部分（如语音编码器），仅对线性投影器或特定适配器进行微调，以对齐语音和文本模态。这种方法简化了训练过程并提高了效率[[102](#bib.bib102)]。
- en: •
  id: totrans-1968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning,
    starting with a text-based pre-training phase, followed by fine-tuning on a mixture
    of tasks that include both text and audio data. This staged approach leverages
    the strengths of pre-trained text models while adapting them for multimodal tasks.'
  id: totrans-1969
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多阶段微调：像 AudioPaLM 这样的模型进行多阶段微调，首先是基于文本的预训练阶段，然后在包含文本和音频数据的任务混合上进行微调。这种分阶段的方法利用了预训练文本模型的优势，同时使其适应多模态任务。
- en: 11.4.3 Fine-Tuning Whisper for Automatic Speech Recognition (ASR)
  id: totrans-1970
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.3 微调 Whisper 以进行自动语音识别（ASR）
- en: Whisper¹¹1[https://openai.com/index/whisper/](https://openai.com/index/whisper/)
    is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed
    to convert spoken language into text. Built upon the powerful Transformer architecture,
    Whisper excels at capturing and transcribing diverse speech patterns across various
    languages and accents. Unlike traditional ASR models that require extensive labelled
    data, Whisper leverages a vast dataset and self-supervised learning, enabling
    it to perform robustly in noisy environments and handle a wide range of speech
    variations. Its versatility and high accuracy make it an ideal choice for applications
    such as voice assistants, transcription services, and multilingual speech recognition
    systems.
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper¹¹1[https://openai.com/index/whisper/](https://openai.com/index/whisper/)
    是一个由 OpenAI 开发的先进自动语音识别（ASR）模型，旨在将口语转化为文本。基于强大的 Transformer 架构，Whisper 擅长捕捉和转录各种语言和口音的多样化语音模式。与需要大量标注数据的传统
    ASR 模型不同，Whisper 利用广泛的数据集和自监督学习，使其能够在噪声环境中稳定表现，并处理各种语音变体。它的多功能性和高准确性使其成为语音助手、转录服务和多语言语音识别系统等应用的理想选择。
- en: Why Fine-Tune Whisper?
  id: totrans-1972
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么微调 Whisper？
- en: Fine-tuning Whisper for specific ASR tasks can significantly enhance its performance
    in specialised domains. Although Whisper is pre-trained on a large and diverse
    dataset, it might not fully capture the nuances of specific vocabularies or accents
    present in niche applications. Fine-tuning allows Whisper to adapt to particular
    audio characteristics and terminologies, leading to more accurate and reliable
    transcriptions. This process is especially beneficial in industries with domain-specific
    jargon, like medical, legal, or technical fields, where the generic model might
    struggle with specialised vocabulary.
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: 对特定ASR任务进行Whisper的微调可以显著提升其在专业领域的表现。尽管Whisper已经在一个大规模且多样化的数据集上进行了预训练，但它可能未能完全捕捉到特定应用中的词汇或口音的细微差别。微调使Whisper能够适应特定的音频特征和术语，从而提供更准确和可靠的转录结果。这个过程在有领域特定术语的行业中尤其有益，例如医疗、法律或技术领域，其中通用模型可能会对专业词汇感到困难。
- en: Steps to Fine-Tune Whisper
  id: totrans-1974
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调Whisper的步骤
- en: •
  id: totrans-1975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Collection and Preparation: Gather a sizable dataset that matches the
    target domain or task. Ensure the dataset includes diverse examples with clear
    transcriptions. Clean and preprocess the audio files and transcripts, ensuring
    they are in a consistent format and aligned correctly. Tools like FFmpeg²²2[https://ffmpeg.org/ffmpeg.html](https://ffmpeg.org/ffmpeg.html)
    can help standardise audio formats and sample rates.'
  id: totrans-1976
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据收集和准备：收集一个与目标领域或任务相匹配的足够大的数据集。确保数据集中包括具有清晰转录的多样化示例。清理和预处理音频文件和转录，确保它们格式一致并正确对齐。像FFmpeg²²2[https://ffmpeg.org/ffmpeg.html](https://ffmpeg.org/ffmpeg.html)这样的工具可以帮助标准化音频格式和采样率。
- en: •
  id: totrans-1977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Augmentation: To improve robustness, augment the dataset with variations
    such as different noise levels, accents, or speeds. Techniques like adding background
    noise, altering pitch, or changing the tempo can help the model generalise better
    to real-world conditions.'
  id: totrans-1978
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据增强：为了提高鲁棒性，用不同的噪声水平、口音或速度对数据集进行增强。添加背景噪声、改变音调或改变节奏等技术可以帮助模型更好地适应实际条件。
- en: •
  id: totrans-1979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Preprocessing: Convert the audio files into a format suitable for Whisper,
    typically into mel spectrograms or another time-frequency representation. This
    transformation is crucial as Whisper relies on such representations to learn and
    transcribe speech effectively.'
  id: totrans-1980
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预处理：将音频文件转换为适合Whisper的格式，通常是mel频谱图或其他时频表示。这一转化是至关重要的，因为Whisper依赖这些表示来有效地学习和转录语音。
- en: •
  id: totrans-1981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Configuration: Initialise the Whisper model with pre-trained weights.
    Configure the model to accommodate the target language or domain-specific adjustments.
    This includes setting appropriate hyperparameters, like learning rate and batch
    size, tailored to the dataset’s size and complexity.'
  id: totrans-1982
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型配置：用预训练权重初始化Whisper模型。配置模型以适应目标语言或领域特定的调整。这包括设置适当的超参数，如学习率和批次大小，这些都应针对数据集的大小和复杂性量身定制。
- en: •
  id: totrans-1983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training: Fine-tune the Whisper model on the prepared dataset using a framework
    like PyTorch or TensorFlow. Ensure to monitor the model’s performance on a validation
    set to avoid overfitting. Techniques like gradient clipping, learning rate scheduling,
    and early stopping can help maintain training stability and efficiency.'
  id: totrans-1984
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练：使用像PyTorch或TensorFlow这样的框架对准备好的数据集进行Whisper模型的微调。确保监控模型在验证集上的表现，以避免过拟合。像梯度裁剪、学习率调度和早停等技术可以帮助保持训练的稳定性和效率。
- en: •
  id: totrans-1985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evaluation and Testing: After training, evaluate the model’s performance on
    a separate test set to assess its accuracy and generalisability. Metrics like
    Word Error Rate (WER) or Character Error Rate (CER) provide insights into how
    well the model transcribes audio compared to ground truth transcriptions.'
  id: totrans-1986
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估和测试：训练后，在一个单独的测试集上评估模型的表现，以评估其准确性和泛化能力。像词错误率（WER）或字符错误率（CER）这样的指标提供了模型转录音频相对于真实转录的效果。
- en: 11.4.4 Case Studies and Applications
  id: totrans-1987
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.4 案例研究和应用
- en: '1.'
  id: totrans-1988
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant
    improvements in transcribing doctor-patient interactions. Models like Whisper
    have been fine-tuned on medical terminologies, resulting in more accurate and
    reliable transcriptions.'
  id: totrans-1989
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗转录：在医疗数据上对语音LLM进行微调，已显著改善了医生与患者互动的转录。像Whisper这样的模型在医疗术语上进行了微调，结果是更准确和可靠的转录。
- en: '2.'
  id: totrans-1990
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Legal Document Processing: Legal firms have employed fine-tuned audio LLMs
    to transcribe court proceedings and legal discussions. Domain-specific fine-tuning
    has enhanced the models’ ability to recognise and accurately transcribe legal
    jargon.'
  id: totrans-1991
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 法律文档处理：法律事务所已经采用微调的音频 LLMs 来转录法庭程序和法律讨论。领域特定的微调提高了模型识别和准确转录法律术语的能力。
- en: '3.'
  id: totrans-1992
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Customer Service Automation: Companies are using fine-tuned speech models to
    automate customer service interactions. These models are trained on customer support
    data to understand and respond to queries more effectively, providing a more seamless
    user experience.'
  id: totrans-1993
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 客户服务自动化：公司正在使用微调的语音模型来自动化客户服务交互。这些模型通过对客户支持数据的训练来理解和回应查询，从而提供更加无缝的用户体验。
- en: Chapter 12 Open Challenges and Research Directions
  id: totrans-1994
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 12 章 开放挑战与研究方向
- en: 12.1 Scalability Issues
  id: totrans-1995
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 可扩展性问题
- en: 'The fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM¹¹1[https://ai.google/discover/palm2/](https://ai.google/discover/palm2/)
    , and T5²²2[https://huggingface.co/docs/transformers/en/model_doc/t5](https://huggingface.co/docs/transformers/en/model_doc/t5)
    has become a critical area of research, presenting several significant challenges
    and opening up new avenues for exploration, particularly in scaling these processes
    efficiently. This discussion focuses on the two main aspects: the challenges in
    scaling fine-tuning processes and potential research directions for scalable solutions.'
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的微调，如 GPT-4、PaLM¹¹1[https://ai.google/discover/palm2/](https://ai.google/discover/palm2/)
    和 T5²²2[https://huggingface.co/docs/transformers/en/model_doc/t5](https://huggingface.co/docs/transformers/en/model_doc/t5)，已成为一个关键的研究领域，面临几个重要挑战，并为高效扩展这些过程开辟了新的研究方向。本讨论集中在两个主要方面：扩展微调过程中的挑战以及可扩展解决方案的潜在研究方向。
- en: 12.1.1 Challenges in Scaling Fine-Tuning Processes
  id: totrans-1997
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1 扩展微调过程中的挑战
- en: '1.'
  id: totrans-1998
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Computational Resources: Large-scale models such as GPT-3 and PaLM require
    enormous computational resources for fine-tuning. For instance, fine-tuning a
    175-billion parameter model like GPT-3 necessitates high-performance GPUs or TPUs
    capable of handling vast amounts of data and complex operations. The sheer volume
    of parameters translates to extensive computational demands. Even a relatively
    smaller model, such as BERT-large with 340 million parameters, can be computationally
    intensive to fine-tune.'
  id: totrans-1999
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算资源：像 GPT-3 和 PaLM 这样的规模较大的模型需要巨大的计算资源进行微调。例如，微调一个拥有 1750 亿参数的模型，如 GPT-3，需要高性能的
    GPU 或 TPU 来处理大量数据和复杂操作。参数的庞大数量转化为广泛的计算需求。即使是相对较小的模型，如参数为 3.4 亿的 BERT-large，微调也可能计算密集。
- en: '2.'
  id: totrans-2000
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Memory Requirements: The memory footprint for fine-tuning LLMs is staggering.
    Each parameter in the model requires storage, and during training, additional
    memory is needed to store intermediate computations, gradients, and optimiser
    states. For example, loading a 7 billion parameter model (e.g., LLaMA 2) in FP32
    (4 bytes per parameter) requires approximately 28 GB of GPU memory, while fine-tuning
    demands around 112 GB of GPU memory[[103](#bib.bib103)]. This memory demand is
    beyond the capability of most consumer-grade hardware, making fine-tuning accessible
    primarily to well-funded organisations or research institutions.'
  id: totrans-2001
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存需求：微调 LLMs 的内存占用令人惊叹。模型中的每个参数都需要存储，并且在训练过程中，还需要额外的内存来存储中间计算、梯度和优化器状态。例如，加载一个
    70 亿参数的模型（如 LLaMA 2）在 FP32（每个参数 4 字节）中大约需要 28 GB 的 GPU 内存，而微调则需要大约 112 GB 的 GPU
    内存[[103](#bib.bib103)]。这种内存需求超出了大多数消费者级硬件的能力，使得微调主要对资金充裕的组织或研究机构可及。
- en: '3.'
  id: totrans-2002
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Data Volume: LLMs typically require vast amounts of training data to achieve
    state-of-the-art performance during fine-tuning. This data needs to be loaded,
    preprocessed, and fed into the model at high speeds to maintain efficient training.
    Managing large datasets can become a bottleneck, especially if the data is stored
    in a distributed fashion across multiple systems or if it needs to be fetched
    from remote storage.'
  id: totrans-2003
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据量：LLMs 通常需要大量的训练数据以在微调过程中实现最先进的性能。这些数据需要以高速加载、预处理并输入模型，以保持高效训练。管理大型数据集可能成为瓶颈，特别是当数据分布在多个系统中存储或需要从远程存储中获取时。
- en: '4.'
  id: totrans-2004
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs
    fully utilised. However, data pipelines can become bottlenecks if not properly
    optimised. For example, shuffling large datasets or loading them into memory quickly
    enough to keep up with the training process can be challenging. Techniques like
    data packing, where multiple small examples are combined into larger batches,
    help improve throughput but add complexity to data handling routines.[[104](#bib.bib104)]'
  id: totrans-2005
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 吞吐量和瓶颈：高吞吐量对保持GPU或TPU的充分利用至关重要。然而，数据管道如果没有得到适当优化，可能会成为瓶颈。例如，将大型数据集打乱或快速加载到内存中以跟上训练过程可能会面临挑战。数据打包等技术，将多个小示例合并为更大的批次，有助于提高吞吐量，但也增加了数据处理的复杂性。[[104](#bib.bib104)]
- en: '5.'
  id: totrans-2006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Efficient Use of Resources: The financial and environmental costs of fine-tuning
    large models are significant. Large-scale fine-tuning involves not just the direct
    cost of computational resources but also the indirect costs associated with energy
    consumption and infrastructure maintenance. Techniques such as mixed-precision
    training and gradient checkpointing can reduce these costs by optimising memory
    and computational efficiency.'
  id: totrans-2007
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 资源的高效使用：微调大型模型的财务和环境成本是显著的。大规模微调不仅涉及计算资源的直接成本，还包括与能源消耗和基础设施维护相关的间接成本。混合精度训练和梯度检查点等技术可以通过优化内存和计算效率来降低这些成本。
- en: The challenges in scaling the fine-tuning processes of LLMs are multifaceted
    and complex, involving significant computational, memory, and data handling constraints.
    Innovations in PEFT, data throughput optimisation, and resource-efficient training
    methods are critical for overcoming these challenges. As LLMs continue to grow
    in size and capability, addressing these challenges will be essential for making
    advanced AI accessible and practical for a wider range of applications.
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM的挑战是多方面且复杂的，涉及到显著的计算、内存和数据处理约束。在PEFT、数据吞吐量优化和资源高效训练方法方面的创新对于克服这些挑战至关重要。随着LLM的规模和能力不断增长，解决这些挑战对于使先进AI对更广泛的应用变得可及和实用将是关键的。
- en: 12.1.2 Research Directions for Scalable Solutions
  id: totrans-2009
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2 可扩展解决方案的研究方向
- en: Advanced PEFT Techniques and Sparse Fine-Tuning
  id: totrans-2010
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高级PEFT技术和稀疏微调
- en: Recent advancements in PEFT techniques, like LoRA and its variant, Quantised
    LoRA, are revolutionising the scalability of LLMs. LoRA reduces the computational
    burden by updating only a low-rank approximation of the parameters, significantly
    lowering memory and processing requirements. Quantised LoRA further optimises
    resource usage by applying quantisation to these low-rank matrices, maintaining
    high model performance while minimising the need for extensive hardware. This
    has enabled efficient fine-tuning of massive models, such as in Meta’s LLaMA project,
    where adapting a smaller set of influential parameters allowed the models to perform
    robustly across various tasks with less computational strain.
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在PEFT技术方面的进展，如LoRA及其变体Quantised LoRA，正在彻底改变LLM的可扩展性。LoRA通过仅更新参数的低秩近似来降低计算负担，显著降低内存和处理要求。Quantised
    LoRA通过对这些低秩矩阵进行量化，进一步优化了资源使用，在保持模型性能的同时最小化了对大量硬件的需求。这使得对大型模型进行高效微调成为可能，例如在Meta的LLaMA项目中，调整一小部分有影响力的参数使模型在各种任务中表现出色，同时减少了计算压力。
- en: Sparse fine-tuning techniques, such as SpIEL [[105](#bib.bib105)] complement
    these efforts by selectively updating only the most impactful parameters. SpIEL
    fine-tunes models by only changing a small portion of the parameters, which it
    tracks with an index. The process includes updating the parameters, removing the
    least important ones, and adding new ones based on their gradients or estimated
    momentum using an efficient optimiser.
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏微调技术，如SpIEL [[105](#bib.bib105)]，通过选择性地更新最具影响力的参数来补充这些努力。SpIEL通过仅更改小部分参数（用索引跟踪）来微调模型。该过程包括更新参数、删除不重要的参数，并根据梯度或估计的动量使用高效的优化器添加新参数。
- en: Data Efficient Fine-Tuning (DEFT)
  id: totrans-2013
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据高效微调（DEFT）
- en: To address the scalability challenges, recently the concept of DEFT has emerged.
    This novel approach introduces data pruning as a mechanism to optimise the fine-tuning
    process by focusing on the most critical data samples.
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对可扩展性挑战，最近出现了DEFT的概念。这种新颖的方法引入了数据剪枝作为优化微调过程的机制，专注于最关键的数据样本。
- en: DEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by
    selectively pruning the training data to identify the most influential and representative
    samples. This method leverages few-shot learning principles, enabling LLMs to
    adapt to new data with minimal samples while maintaining or even exceeding performance
    levels achieved with full datasets [[106](#bib.bib106)].
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
  zh: DEFT 旨在通过选择性地剪枝训练数据以识别最具影响力和代表性的样本，从而提高 LLM 微调的效率和效果。这种方法利用了少样本学习原理，使 LLM 能够在仅有少量样本的情况下适应新数据，同时保持或甚至超越使用完整数据集时所达到的性能水平[[106](#bib.bib106)]。
- en: Key Components of DEFT
  id: totrans-2016
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DEFT 的关键组成部分
- en: 'High Accuracy Through Influence Score: DEFT introduces the concept of an influence
    score to evaluate and rank the importance of each data sample in the context of
    LLM fine-tuning. The influence score estimates how removing a specific sample
    would impact the overall performance of the model. This approach allows for the
    selection of a small subset of data that is highly representative and influential,
    thereby enabling the model to maintain high accuracy with significantly fewer
    samples.'
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: 通过影响分数实现高准确度：DEFT 引入了影响分数的概念，以评估和排名每个数据样本在 LLM 微调过程中的重要性。影响分数估算了去除特定样本会对模型整体性能的影响。这种方法允许选择一个具有高度代表性和影响力的小数据子集，从而使模型在使用显著较少的样本的情况下保持高准确度。
- en: 'High Efficiency Through Effort Score and Surrogate Models: To address the cost
    and complexity of evaluating large datasets, DEFT employs a surrogate model—a
    smaller, computationally less intensive model—to approximate the influence scores.
    This surrogate model helps estimate the impact of each sample without the heavy
    computational burden associated with directly using the LLM. Additionally, DEFT
    introduces an effort score to identify and prioritise more challenging samples
    that may require special attention from the LLM. This dual-score system ensures
    that the fine-tuning process remains both efficient and effective.'
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
  zh: 通过努力分数和代理模型实现高效能：为了应对评估大数据集的成本和复杂性，DEFT 使用了一个代理模型——一个计算负担较小的模型——来近似影响分数。这个代理模型有助于估算每个样本的影响，而无需直接使用大型语言模型（LLM）所带来的高计算负担。此外，DEFT
    引入了一个努力分数，以识别和优先考虑可能需要 LLM 特别关注的更具挑战性的样本。这个双重分数系统确保了微调过程既高效又有效。
- en: Practical Implications and Use Cases
  id: totrans-2019
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实际应用与使用案例
- en: •
  id: totrans-2020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial
    for applications where models need to quickly adapt to new data with minimal samples.
    In scenarios such as personalised recommendations or adapting to sudden changes
    in user behaviour, DEFT allows for rapid fine-tuning, maintaining high performance
    with a fraction of the data typically required.'
  id: totrans-2021
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 少样本微调以实现快速适应：DEFT 对于需要模型快速适应新数据且样本较少的应用特别有益。在个性化推荐或适应用户行为突变等场景中，DEFT 允许快速微调，维持高性能的同时，所需的数据量仅为通常所需的一部分。
- en: •
  id: totrans-2022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reducing Computational Costs in Large-Scale Deployments: By focusing on the
    most influential data samples and using surrogate models, DEFT significantly reduces
    the computational resources needed for fine-tuning. This makes it feasible to
    maintain high-performing LLMs even in large-scale deployments where data volumes
    are substantial.'
  id: totrans-2023
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大规模部署中降低计算成本：通过关注最具影响力的数据样本和使用代理模型，DEFT 显著减少了进行微调所需的计算资源。这使得即使在数据量庞大的大规模部署中，也能保持高性能的
    LLM。
- en: Future Directions
  id: totrans-2024
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来方向
- en: 'The DEFT introduces a data pruning task for fine-tuning large language models
    (LLMs), setting the stage for new research into efficient LLM-based recommendation
    systems and presenting numerous opportunities for future exploration. Key areas
    for further investigation include:'
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: DEFT 引入了一个数据剪枝任务用于微调大型语言模型（LLMs），为基于 LLM 的推荐系统的高效研究奠定了基础，并为未来的探索提供了许多机会。进一步研究的关键领域包括：
- en: •
  id: totrans-2026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Applying the proposed DEALRec[[107](#bib.bib107)] approach to a broader range
    of LLM-based recommender models across diverse cross-domain datasets, thereby
    enhancing fine-tuning performance within resource constraints.
  id: totrans-2027
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将提出的 DEALRec[[107](#bib.bib107)] 方法应用于更广泛的 LLM 基于推荐模型的跨领域数据集，从而在资源限制下提高微调性能。
- en: •
  id: totrans-2028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Addressing the limited context window of LLMs by selectively focusing on the
    most informative items in user interaction sequences for fine-tuning purposes.
  id: totrans-2029
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过选择性地关注用户交互序列中最具信息量的项来解决 LLM 的有限上下文窗口问题，以进行微调。
- en: 12.1.3 Hardware and Algorithm Co-Design
  id: totrans-2030
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.3 硬件与算法协同设计
- en: Co-designing hardware and algorithms tailored for LLMs can lead to significant
    improvements in the efficiency of fine-tuning processes. Custom hardware accelerators
    optimised for specific tasks or types of computation can drastically reduce the
    energy and time required for model training and fine-tuning.
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 为LLMs量身定制的硬件与算法的协同设计可以显著提高微调过程的效率。为特定任务或计算类型优化的定制硬件加速器可以大幅减少模型训练和微调所需的能源和时间。
- en: •
  id: totrans-2032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Custom Accelerators: Developing hardware accelerators specifically for the
    sparse and low-precision computations often used in LLM fine-tuning can enhance
    performance. These accelerators are designed to efficiently handle the unique
    requirements of LLMs, such as the high memory bandwidth and extensive matrix multiplications
    involved in transformer architectures.'
  id: totrans-2033
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定制加速器：开发专门用于LLM微调中稀疏和低精度计算的硬件加速器可以提高性能。这些加速器设计用于有效处理LLMs的独特要求，例如涉及到的高内存带宽和广泛的矩阵乘法。
- en: •
  id: totrans-2034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation
    techniques, such as those that minimise data movement or leverage hardware-specific
    features (e.g., tensor cores for mixed-precision calculations), can further enhance
    the efficiency of fine-tuning processes.'
  id: totrans-2035
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法优化：将硬件创新与算法优化技术相结合，例如那些减少数据传输或利用硬件特定功能（例如用于混合精度计算的张量核心），可以进一步提高微调过程的效率。
- en: •
  id: totrans-2036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example: NVIDIA’s TensorRT³³3[https://docs.nvidia.com/tensorrt/index.html](https://docs.nvidia.com/tensorrt/index.html)
    is an example of hardware and algorithm co-design in action. It optimises deep
    learning models for inference by leveraging NVIDIA GPUs’ capabilities, significantly
    speeding up the process while reducing the resource requirements. TensorRT’s optimisations
    include support for mixed-precision and sparse tensor operations, making it highly
    suitable for fine-tuning large models.'
  id: totrans-2037
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：NVIDIA的TensorRT³³3[https://docs.nvidia.com/tensorrt/index.html](https://docs.nvidia.com/tensorrt/index.html)
    是硬件与算法协同设计的一个实际例子。它通过利用NVIDIA GPU的能力来优化深度学习模型的推理过程，大幅加快了处理速度，同时减少了资源需求。TensorRT的优化包括对混合精度和稀疏张量操作的支持，使其非常适合用于大模型的微调。
- en: As the scale of language models continues to grow, addressing the challenges
    of fine-tuning them efficiently becomes increasingly critical. Innovations in
    PEFT, sparse fine-tuning, data handling, and the integration of advanced hardware
    and algorithmic solutions present promising directions for future research. These
    scalable solutions are essential not only to make the deployment of LLMs feasible
    for a broader range of applications but also to push the boundaries of what these
    models can achieve.
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型规模的不断扩大，如何高效地微调这些模型成为越来越重要的挑战。PEFT、稀疏微调、数据处理以及先进硬件和算法解决方案的创新为未来研究提供了有希望的方向。这些可扩展的解决方案不仅对于使LLM的部署适用于更广泛的应用至关重要，而且也推动了这些模型能实现的最终边界。
- en: 12.2 Ethical Considerations in Fine-Tuning LLMs
  id: totrans-2039
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 LLMs微调中的伦理考虑
- en: 12.2.1 Bias and Fairness
  id: totrans-2040
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1 偏差与公平性
- en: When fine-tuning LLMs, the goal is often to optimise their performance for specific
    tasks or datasets. However, these datasets may inherently carry biases that get
    transferred to the model during the fine-tuning process. Biases can arise from
    various sources, including historical data, imbalanced training samples, and cultural
    prejudices embedded in language. For instance, an LLM fine-tuned on a dataset
    primarily sourced from English-speaking countries might underperform or make biased
    predictions when applied to text from other linguistic or cultural backgrounds.
    Google AI’s Fairness Indicators tool⁴⁴4[https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/](https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/)
    is a practical solution that allows developers to evaluate the fairness of their
    models by analysing performance metrics across different demographic groups. This
    tool can be integrated into the fine-tuning pipeline to monitor and address bias
    in real-time.
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLMs时，目标通常是优化模型在特定任务或数据集上的表现。然而，这些数据集可能固有地带有偏见，这些偏见会在微调过程中转移到模型中。偏见可能来源于各种渠道，包括历史数据、不平衡的训练样本以及语言中嵌入的文化偏见。例如，一个基于主要来自英语国家的数据集进行微调的LLM在应用于其他语言或文化背景的文本时，可能会表现不佳或做出有偏见的预测。谷歌AI的公平性指标工具⁴⁴4[https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/](https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/)
    是一个实用的解决方案，可以通过分析不同人口群体的性能指标来评估模型的公平性。该工具可以集成到微调流程中，以实时监控和处理偏见。
- en: Addressing Bias and Fairness
  id: totrans-2042
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理偏见和公平性
- en: •
  id: totrans-2043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse
    and representative of all user demographics can help mitigate bias.'
  id: totrans-2044
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样化和具有代表性的数据：确保微调数据集在所有用户人群中具有多样性和代表性，有助于减轻偏见。
- en: •
  id: totrans-2045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fairness Constraints: Incorporating fairness constraints, as suggested by the
    FairBERTa framework⁵⁵5[https://huggingface.co/facebook/FairBERTa](https://huggingface.co/facebook/FairBERTa),
    ensures that fine-tuned models maintain equitable performance across different
    groups.'
  id: totrans-2046
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 公平性约束：根据FairBERTa框架⁵⁵5[https://huggingface.co/facebook/FairBERTa](https://huggingface.co/facebook/FairBERTa)的建议，纳入公平性约束可以确保微调后的模型在不同群体中保持公平的表现。
- en: •
  id: totrans-2047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing
    conditions might initially be trained on data from predominantly white patients.
    Such a model could produce less accurate diagnoses for patients from other racial
    backgrounds. By using fairness-aware fine-tuning techniques, healthcare providers
    can develop models that perform more equitably across diverse patient populations.'
  id: totrans-2048
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例应用：在医疗领域，一个为了辅助诊断而微调的LLM可能最初是基于主要来自白人患者的数据进行训练的。这样的模型可能对其他种族背景的患者诊断不够准确。通过使用公平性意识的微调技术，医疗提供者可以开发在不同患者群体中表现更为公平的模型。
- en: 12.2.2 Privacy Concerns
  id: totrans-2049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2 隐私问题
- en: Fine-tuning often involves using sensitive or proprietary datasets, which poses
    significant privacy risks. If not properly managed, fine-tuned models can inadvertently
    leak private information from their training data. This issue is especially critical
    in domains like healthcare or finance, where data confidentiality is paramount.
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
  zh: 微调通常涉及使用敏感或专有的数据集，这会带来显著的隐私风险。如果管理不当，微调后的模型可能会无意中泄露训练数据中的私人信息。这个问题在医疗或金融等数据保密性至关重要的领域尤为严重。
- en: Ensuring Privacy During Fine-Tuning
  id: totrans-2051
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 确保微调过程中的隐私
- en: •
  id: totrans-2052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Differential Privacy⁶⁶6[https://privacytools.seas.harvard.edu/differential-privacy](https://privacytools.seas.harvard.edu/differential-privacy):
    Implementing differential privacy techniques during fine-tuning can prevent models
    from leaking sensitive information.'
  id: totrans-2053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 差分隐私⁶⁶6[https://privacytools.seas.harvard.edu/differential-privacy](https://privacytools.seas.harvard.edu/differential-privacy)：在微调过程中实施差分隐私技术可以防止模型泄露敏感信息。
- en: •
  id: totrans-2054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Federated Learning⁷⁷7[https://research.ibm.com/blog/what-is-federated-learning](https://research.ibm.com/blog/what-is-federated-learning):
    Utilising federated learning frameworks allows models to be fine-tuned across
    decentralised data sources, which enhances privacy by keeping data localised.'
  id: totrans-2055
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联邦学习⁷⁷7[https://research.ibm.com/blog/what-is-federated-learning](https://research.ibm.com/blog/what-is-federated-learning)：利用联邦学习框架可以在分散的数据源上对模型进行微调，从而通过保持数据本地化来增强隐私保护。
- en: •
  id: totrans-2056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example Application: In customer service applications, companies might fine-tune
    LLMs using customer interaction data. Employing differential privacy ensures that
    the model learns from these interactions without memorising and potentially leaking
    personal information, thus maintaining customer confidentiality.'
  id: totrans-2057
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例应用：在客户服务应用中，公司可能使用客户互动数据对LLM进行微调。采用差分隐私技术确保模型从这些互动中学习，而不会记住或泄露个人信息，从而维护客户的机密性。
- en: 12.2.3 Security Risks
  id: totrans-2058
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.3 安全风险
- en: •
  id: totrans-2059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible
    to security vulnerabilities, particularly from adversarial attacks. These attacks
    involve inputs designed to exploit model weaknesses, causing them to produce erroneous
    or harmful outputs. Such vulnerabilities can be more pronounced in fine-tuned
    models due to their specialised training data, which may not cover all possible
    input scenarios.'
  id: totrans-2060
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调模型中的安全漏洞：微调的LLM易受安全漏洞的影响，特别是对抗性攻击。这些攻击涉及设计用于利用模型弱点的输入，导致模型产生错误或有害的输出。由于微调模型可能使用了专门的训练数据，这些数据可能无法涵盖所有可能的输入场景，因此这些漏洞可能更加明显。
- en: •
  id: totrans-2061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recent Research and Industry Practices: Microsoft’s Adversarial ML Threat Matrix
    provides a comprehensive framework for identifying and mitigating adversarial
    threats during model development and fine-tuning. This matrix helps developers
    understand the potential attack vectors and implement defensive strategies accordingly.'
  id: totrans-2062
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最新研究和行业实践：微软的对抗性ML威胁矩阵提供了一个全面的框架，用于识别和缓解模型开发和微调过程中可能遇到的对抗性威胁。该矩阵帮助开发者了解潜在的攻击向量，并相应地实施防御策略。
- en: •
  id: totrans-2063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhancing Security in Fine-Tuning:'
  id: totrans-2064
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升安全性在微调中的作用：
- en: –
  id: totrans-2065
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Adversarial Training: Exposing models to adversarial examples during fine-tuning
    can enhance their robustness against attacks.'
  id: totrans-2066
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性训练：在微调过程中将模型暴露于对抗样本可以提高其对攻击的鲁棒性。
- en: –
  id: totrans-2067
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Security Audits: Regularly conducting security audits on fine-tuned models
    can help identify and address potential vulnerabilities.'
  id: totrans-2068
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全审计：定期对微调模型进行安全审计可以帮助识别和解决潜在的漏洞。
- en: 12.3 Accountability and Transparency
  id: totrans-2069
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 问责制和透明度
- en: 12.3.1 The Need for Accountability and Transparency
  id: totrans-2070
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.1 需要问责制和透明度
- en: Fine-tuning can significantly alter an LLM’s behaviour, making it crucial to
    document and understand the changes and their impacts. This transparency is essential
    for stakeholders to trust the model’s outputs and for developers to be accountable
    for its performance and ethical implications.
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以显著改变LLM的行为，因此记录和理解这些变化及其影响至关重要。这种透明度对于利益相关者信任模型输出以及开发者对其性能和道德影响负责是必要的。
- en: 12.3.2 Recent Research and Industry Practices
  id: totrans-2072
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.2 最新研究和行业实践
- en: Meta’s Responsible AI framework⁸⁸8[https://ai.meta.com/responsible-ai/](https://ai.meta.com/responsible-ai/)
    underscores the importance of documenting the fine-tuning process and its effects
    on model behaviour. This includes maintaining detailed records of the data used,
    the changes made during fine-tuning, and the evaluation metrics applied.
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: '[Meta的负责任AI框架](https://ai.meta.com/responsible-ai/)强调了记录微调过程及其对模型行为影响的重要性。这包括维护所使用数据的详细记录、微调过程中所做的更改以及应用的评估指标。'
- en: 12.3.3 Promoting Accountability and Transparency
  id: totrans-2074
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.3 促进问责制和透明度
- en: •
  id: totrans-2075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensive Documentation: Creating detailed documentation of the fine-tuning
    process and its impact on model performance and behaviour.'
  id: totrans-2076
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合文档：创建关于微调过程及其对模型性能和行为影响的详细文档。
- en: •
  id: totrans-2077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transparent Reporting: Utilising frameworks like Model Cards⁹⁹9[https://huggingface.co/docs/hub/en/model-cards](https://huggingface.co/docs/hub/en/model-cards)
    to report on the ethical and operational characteristics of fine-tuned models.'
  id: totrans-2078
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 透明报告：利用像[Model Cards](https://huggingface.co/docs/hub/en/model-cards)这样的框架报告微调模型的道德和操作特征。
- en: •
  id: totrans-2079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example Application: In content moderation systems, LLMs fine-tuned to identify
    and filter harmful content need clear documentation and reporting. This ensures
    that platform users and regulators understand how the model operates and can trust
    its moderation decisions.'
  id: totrans-2080
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例应用：在内容审核系统中，微调以识别和过滤有害内容的LLM需要清晰的文档和报告。这确保平台用户和监管机构理解模型的操作方式，并能信任其审核决定。
- en: 12.3.4 Proposed frameworks/techniques for Ethical Fine-Tuning
  id: totrans-2081
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.4 道德微调的建议框架/技术
- en: Frameworks for Mitigating Bias
  id: totrans-2082
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 减少偏见的框架
- en: Bias-aware fine-tuning frameworks aim to incorporate fairness into the model
    training process. FairBERTa, introduced by Facebook, is an example of such a framework
    that integrates fairness constraints directly into the model’s objective function
    during fine-tuning. This approach ensures that the model’s performance is balanced
    across different demographic groups.
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: 意识到偏见的微调框架旨在将公平性纳入模型训练过程中。由 Facebook 推出的 FairBERTa 就是一个这样的框架，它在微调过程中将公平性约束直接集成到模型的目标函数中。这种方法确保了模型在不同人口群体中的性能平衡。
- en: Organisations can adopt fairness-aware frameworks to develop more equitable
    AI systems. For instance, social media platforms can use these frameworks to fine-tune
    models that detect and mitigate hate speech while ensuring fair treatment across
    various user demographics.
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可以采用关注公平性的框架来开发更公平的 AI 系统。例如，社交媒体平台可以使用这些框架来微调检测和缓解仇恨言论的模型，同时确保对各种用户群体的公平对待。
- en: Techniques for Privacy Preservation
  id: totrans-2085
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 隐私保护技术
- en: Differential privacy and federated learning are key techniques for preserving
    privacy during fine-tuning. TensorFlow Privacy^(10)^(10)10[https://www.tensorflow.org/responsible_ai/privacy/guide](https://www.tensorflow.org/responsible_ai/privacy/guide),
    developed by Google, provides built-in support for differential privacy, allowing
    developers to fine-tune models securely without compromising data confidentiality.
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私和联邦学习是保护微调过程中的隐私的关键技术。Google 开发的 TensorFlow Privacy^(10)^(10)10[https://www.tensorflow.org/responsible_ai/privacy/guide](https://www.tensorflow.org/responsible_ai/privacy/guide)
    提供了内置的差分隐私支持，使开发者能够在不妨碍数据机密性的情况下安全地微调模型。
- en: LLMs are highly effective but face challenges when applied in sensitive areas
    where data privacy is crucial. To address this, researchers focus on enhancing
    Small Language Models (SLMs) tailored to specific domains. Existing methods often
    use LLMs to generate additional data or transfer knowledge to SLMs, but these
    approaches struggle due to differences between LLM-generated data and private
    client data. In response, a new Federated Domain-specific Knowledge Transfer (FDKT)[[108](#bib.bib108)]
    framework is introduced. FDKT leverages LLMs to create synthetic samples that
    mimic clients’ private data distribution using differential privacy. This approach
    significantly boosts SLMs’ performance by approximately 5% while maintaining data
    privacy with a minimal privacy budget, outperforming traditional methods relying
    solely on local private data.
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 非常有效，但在数据隐私至关重要的敏感领域应用时面临挑战。为了解决这一问题，研究人员专注于增强针对特定领域的小型语言模型（SLM）。现有方法通常使用
    LLM 生成额外数据或将知识转移到 SLM，但这些方法由于 LLM 生成的数据和私人客户数据之间的差异而面临困难。因此，引入了一种新的联邦领域特定知识转移（FDKT）[[108](#bib.bib108)]
    框架。FDKT 利用 LLM 创建模仿客户私人数据分布的合成样本，采用差分隐私方法。这种方法通过大约 5% 的提升显著提高了 SLM 的性能，同时以最小的隐私预算保持数据隐私，优于仅依赖本地私人数据的传统方法。
- en: In healthcare, federated fine-tuning can allow hospitals to collaboratively
    train models on patient data without transferring sensitive information. This
    approach ensures data privacy while enabling the development of robust, generalisable
    AI systems.
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健领域，联邦微调可以允许医院在不转移敏感信息的情况下协作训练模型。这种方法在确保数据隐私的同时，促进了稳健且具有广泛适应性的 AI 系统的开发。
- en: Frameworks for Enhancing Security
  id: totrans-2089
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强安全性的框架
- en: Adversarial training and robust security measures[[109](#bib.bib109)] are essential
    for protecting fine-tuned models against attacks. The adversarial training approach
    involves training models with adversarial examples to improve their resilience
    against malicious inputs. Microsoft Azure’s adversarial training tools provide
    practical solutions for integrating these techniques into the fine-tuning process,
    helping developers create more secure and reliable models.
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练和强健的安全措施[[109](#bib.bib109)] 对保护微调模型免受攻击至关重要。对抗训练方法包括使用对抗样本训练模型，以提高其抵御恶意输入的能力。Microsoft
    Azure 的对抗训练工具提供了将这些技术集成到微调过程中的实际解决方案，帮助开发者创建更安全可靠的模型。
- en: In cybersecurity, fine-tuned LLMs used for threat detection can benefit from
    adversarial training to enhance their ability to identify and respond to sophisticated
    attacks, thereby improving organisational security.
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全中，用于威胁检测的微调 LLM 可以通过对抗训练来提升其识别和响应复杂攻击的能力，从而改善组织的安全性。
- en: Frameworks for Ensuring Transparency
  id: totrans-2092
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 确保透明度的框架
- en: Transparency and accountability frameworks, such as Model Cards and AI FactSheets^(11)^(11)11[https://aifs360.res.ibm.com/](https://aifs360.res.ibm.com/),
    provide structured ways to document and report on the fine-tuning process and
    the resulting model behaviours. These frameworks promote understanding and trust
    among stakeholders by clearly outlining the model’s capabilities, limitations,
    and ethical considerations.
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
  zh: 透明性和问责制框架，例如模型卡和AI FactSheets^(11)^(11)11[https://aifs360.res.ibm.com/](https://aifs360.res.ibm.com/)，提供了结构化的方式来记录和报告微调过程及其结果模型行为。这些框架通过明确列出模型的能力、局限性和伦理考虑，促进了利益相关者之间的理解和信任。
- en: In government applications, where AI systems might be used for decision-making
    or public services, maintaining transparent documentation through frameworks like
    AI FactSheets ensures that these systems are accountable and their decisions can
    be audited and trusted by the public.
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: 在政府应用中，AI系统可能被用于决策或公共服务，通过像AI FactSheets这样的框架保持透明的文档记录，确保这些系统是有问责制的，其决策可以被公众审计和信任。
- en: Fine-tuning LLMs introduces several ethical challenges, including bias, privacy
    risks, security vulnerabilities, and accountability concerns. Addressing these
    requires a multifaceted approach that integrates fairness-aware frameworks, privacy-preserving
    techniques, robust security measures, and transparency and accountability mechanisms.
    By leveraging recent advancements in these areas, researchers and practitioners
    can develop and deploy LLMs that are not only powerful but also ethically sound
    and trustworthy.
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLMs引入了若干伦理挑战，包括偏见、隐私风险、安全漏洞和问责问题。解决这些问题需要多方面的方法，整合公平意识框架、隐私保护技术、强健的安全措施以及透明和问责机制。通过利用这些领域的最新进展，研究人员和实践者可以开发和部署不仅功能强大而且伦理上健全和值得信赖的LLMs。
- en: 12.4 Integration with Emerging Technologies
  id: totrans-2096
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4 与新兴技术的集成
- en: Integrating LLMs with emerging technologies such as IoT (Internet of Things)
    and edge computing presents numerous opportunities and challenges, reflecting
    advancements and insights from recent research and industry developments.
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLMs与物联网（IoT）等新兴技术和边缘计算集成，带来了许多机遇和挑战，反映了近期研究和行业发展的进展与见解。
- en: 12.4.1 Opportunities
  id: totrans-2098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.4.1 机会
- en: •
  id: totrans-2099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhanced Decision-Making and Automation: LLMs have the capability to analyse
    and derive insights from vast amounts of unstructured data generated by IoT devices.
    This data can range from sensor readings in manufacturing plants to environmental
    data in smart cities. By processing this data in real-time, LLMs can optimise
    decision-making processes and automate tasks that traditionally required human
    intervention. For example:'
  id: totrans-2100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强决策制定和自动化：LLMs能够分析和提取来自IoT设备生成的大量非结构化数据的洞察。这些数据可以包括制造工厂的传感器读数到智慧城市的环境数据。通过实时处理这些数据，LLMs可以优化决策过程并自动化传统上需要人工干预的任务。例如：
- en: –
  id: totrans-2101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing
    sensor data to predict equipment failures before they occur, thereby reducing
    downtime and maintenance costs.'
  id: totrans-2102
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工业应用：通过LLMs分析传感器数据来预测设备故障，从而减少停机时间和维护成本，可以提升预测性维护。
- en: –
  id: totrans-2103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Smart Cities: LLMs can analyse traffic patterns and environmental data from
    IoT sensors to optimise city infrastructure and improve urban planning decisions.'
  id: totrans-2104
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智慧城市：LLMs可以分析来自IoT传感器的交通模式和环境数据，以优化城市基础设施和改善城市规划决策。
- en: •
  id: totrans-2105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Personalised User Experiences: Integration with edge computing allows LLMs
    to process data locally on devices rather than relying solely on cloud-based servers.
    This enables LLMs to deliver highly personalised services based on real-time data
    and user preferences, enhancing user experiences across various domains:'
  id: totrans-2106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 个性化用户体验：与边缘计算的集成使得大型语言模型（LLMs）能够在设备上本地处理数据，而不是仅仅依赖于基于云的服务器。这使得LLMs能够基于实时数据和用户偏好提供高度个性化的服务，提升各个领域的用户体验：
- en: –
  id: totrans-2107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Healthcare: LLMs can provide personalised healthcare recommendations by analysing
    data from wearable devices and integrating it with medical records securely stored
    on edge devices.'
  id: totrans-2108
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗保健：LLMs可以通过分析来自可穿戴设备的数据，并将其与安全存储在边缘设备上的医疗记录整合，从而提供个性化的医疗建议。
- en: •
  id: totrans-2109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Improved Natural Language Understanding: IoT data integration enriches LLMs’
    ability to understand context and respond more intelligently to natural language
    queries. This can significantly improve user interactions with smart environments:'
  id: totrans-2110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改进的自然语言理解：IoT 数据集成丰富了 LLM 理解上下文的能力，并能更智能地响应自然语言查询。这可以显著改善用户与智能环境的互动：
- en: –
  id: totrans-2111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Smart Homes: LLMs integrated with IoT devices can understand and respond to
    voice commands more accurately, adjusting smart home settings based on real-time
    sensor data (e.g., adjusting lighting and temperature based on occupancy and environmental
    conditions).'
  id: totrans-2112
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智能家居：与 IoT 设备集成的 LLM 可以更准确地理解和响应语音命令，根据实时传感器数据调整智能家居设置（例如，根据占用情况和环境条件调整照明和温度）。
- en: 12.4.2 Challenges
  id: totrans-2113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.4.2 挑战
- en: •
  id: totrans-2114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Complexity and Integration: Integrating data from diverse IoT devices
    poses challenges related to data quality, interoperability, and scalability. LLMs
    need to effectively process and interpret this heterogeneous data to derive meaningful
    insights:'
  id: totrans-2115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据复杂性和集成：来自不同 IoT 设备的数据集成面临与数据质量、互操作性和可扩展性相关的挑战。LLM 需要有效处理和解释这些异构数据，以获得有意义的见解：
- en: –
  id: totrans-2116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Integration: Ensuring seamless integration of data streams from different
    IoT platforms and devices without compromising data integrity or performance.'
  id: totrans-2117
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集成：确保来自不同 IoT 平台和设备的数据流无缝集成，而不影响数据完整性或性能。
- en: –
  id: totrans-2118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency
    and reliability before feeding it into LLMs for analysis.'
  id: totrans-2119
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据预处理：清理和预处理 IoT 数据，以确保在将其输入 LLM 进行分析之前的一致性和可靠性。
- en: •
  id: totrans-2120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Privacy and Security: Edge computing involves processing sensitive data locally
    on devices, raising concerns about data privacy and security:'
  id: totrans-2121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私和安全：边缘计算涉及在设备上本地处理敏感数据，带来了数据隐私和安全问题：
- en: –
  id: totrans-2122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Privacy: Implementing robust encryption techniques and access control
    mechanisms to protect sensitive data processed by LLMs on edge devices.'
  id: totrans-2123
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据隐私：实施强大的加密技术和访问控制机制，以保护在边缘设备上由 LLM 处理的敏感数据。
- en: –
  id: totrans-2124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Secure Communication: Ensuring secure communication channels between IoT devices
    and LLMs to prevent data breaches or unauthorised access.'
  id: totrans-2125
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全通信：确保 IoT 设备与 LLM 之间的安全通信渠道，以防止数据泄露或未经授权的访问。
- en: •
  id: totrans-2126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real-Time Processing and Reliability: LLMs deployed in edge computing environments
    must operate with low latency and high reliability to support real-time applications:'
  id: totrans-2127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时处理和可靠性：部署在边缘计算环境中的 LLM 必须以低延迟和高可靠性运行，以支持实时应用：
- en: –
  id: totrans-2128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Latency: Optimising algorithms and processing capabilities of LLMs to handle
    real-time data streams efficiently without delays.'
  id: totrans-2129
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 延迟：优化 LLM 的算法和处理能力，以高效处理实时数据流而不会出现延迟。
- en: –
  id: totrans-2130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Reliability: Ensuring the accuracy and consistency of insights generated by
    LLMs in dynamic and unpredictable IoT environments.'
  id: totrans-2131
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可靠性：确保在动态和不可预测的 IoT 环境中生成的 LLM 见解的准确性和一致性。
- en: 12.5 Future Research Areas
  id: totrans-2132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5 未来研究领域
- en: •
  id: totrans-2133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Federated Learning and Edge Computing: Exploring federated learning techniques
    where LLMs can be trained collaboratively across edge devices without centralised
    data aggregation. This approach addresses privacy concerns and reduces communication
    overhead.'
  id: totrans-2134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联邦学习和边缘计算：探索联邦学习技术，使 LLM 可以在边缘设备上协作训练，而无需集中数据聚合。这种方法解决了隐私问题并减少了通信开销。
- en: •
  id: totrans-2135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real-Time Decision Support Systems: Developing LLM-based systems capable of
    real-time decision-making by integrating with edge computing infrastructure. This
    includes optimising algorithms for low-latency processing and ensuring reliability
    under dynamic environmental conditions.'
  id: totrans-2136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时决策支持系统：开发基于 LLM 的系统，能够通过与边缘计算基础设施集成实现实时决策。这包括优化低延迟处理的算法，并确保在动态环境条件下的可靠性。
- en: •
  id: totrans-2137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ethical and Regulatory Implications: Investigating the ethical implications
    of integrating LLMs with IoT and edge computing, particularly regarding data ownership,
    transparency, and fairness. This area requires frameworks for ethical AI deployment
    and governance.'
  id: totrans-2138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理和监管影响：研究将 LLM 与 IoT 和边缘计算集成的伦理影响，特别是数据所有权、透明度和公平性方面。这一领域需要伦理 AI 部署和治理的框架。
- en: Glossary
  id: totrans-2139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词汇表
- en: LLM
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: LLM
- en: Large Language Model – A type of AI model, typically with billions of parameters,
    trained on vast amounts of text data to understand and generate human-like text.
    They are primarily designed for tasks in natural language processing (NLP).
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型 – 一种AI模型，通常具有数十亿个参数，经过大量文本数据的训练以理解和生成类似人类的文本。它们主要设计用于自然语言处理（NLP）任务。
- en: NLP
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: NLP
- en: Natural Language Processing – A field of artificial intelligence that focuses
    on the interaction between computers and humans through natural language, including
    tasks like language generation, translation, and sentiment analysis.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理 – 一种人工智能领域，关注计算机与人类通过自然语言的交互，包括语言生成、翻译和情感分析等任务。
- en: LoRA
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA
- en: Low-Rank Adaptation – A parameter-efficient fine-tuning technique that adjusts
    only small low-rank matrices to adapt pre-trained models to specific tasks, thus
    preserving most of the original model’s parameters.
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应 – 一种参数高效的微调技术，仅调整小型低秩矩阵以将预训练模型适应特定任务，从而保留大部分原始模型的参数。
- en: DoRA
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: DoRA
- en: Weight-Decomposed Low-Rank Adaptation – A technique that decomposes model weights
    into magnitude and direction components, facilitating fine-tuning while maintaining
    inference efficiency.
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分解低秩适应 – 一种将模型权重分解为幅度和方向分量的技术，有助于在保持推理效率的同时进行微调。
- en: QLoRA
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA
- en: Quantised Low-Rank Adaptation – A variation of LoRA, specifically designed for
    quantised models, allowing for efficient fine-tuning in resource-constrained environments.
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: 量化低秩适应 – LoRA 的一种变体，专为量化模型设计，允许在资源受限的环境中进行高效的微调。
- en: PPO
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: PPO
- en: Proximal Policy Optimisation – A reinforcement learning algorithm that adjusts
    policies by balancing the exploration of new actions and exploitation of known
    rewards, designed for stability and efficiency in training.
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: 近端策略优化 – 一种强化学习算法，通过平衡新行动的探索和已知奖励的利用来调整策略，旨在训练中的稳定性和效率。
- en: DPO
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
  zh: DPO
- en: Direct Preference Optimisation – A method that directly aligns language models
    with human preferences through preference optimisation, bypassing reinforcement
    learning models like PPO.
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: 直接偏好优化 – 一种通过偏好优化直接将语言模型与人类偏好对齐的方法，绕过了像 PPO 这样的强化学习模型。
- en: MoE
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
  zh: MoE
- en: Mixture of Experts – A model architecture that employs multiple specialised
    subnetworks, called experts, which are selectively activated based on the input
    to improve model performance and efficiency.
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合 – 一种模型架构，采用多个专业的子网络，称为专家，根据输入有选择地激活，以提高模型性能和效率。
- en: MoA
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: MoA
- en: Mixture of Agents – A multi-agent framework where several agents collaborate
    during training and inference, leveraging the strengths of each agent to improve
    overall model performance.
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: 代理混合 – 一个多代理框架，其中多个代理在训练和推理过程中协作，利用每个代理的优势来提升整体模型性能。
- en: PEFT
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT
- en: Parameter-Efficient Fine-Tuning – A fine-tuning approach for large models that
    involves adjusting only a subset of model parameters, improving efficiency in
    scenarios with limited computational resources. This includes techniques like
    LoRA, QLoRA, and adapters.
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调 – 一种大模型的微调方法，只调整模型参数的一个子集，提高了在计算资源有限的情况下的效率。这包括 LoRA、QLoRA 和适配器等技术。
- en: Adapters
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器
- en: Small, trainable modules introduced into the layers of pre-trained language
    models, allowing efficient task-specific fine-tuning without modifying the core
    parameters of the original model. Techniques such as **AdapterFusion** and **AdapterSoup**
    fall under this category, facilitating the combination of multiple adapters for
    complex multitasking.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: 小型可训练模块引入到预训练语言模型的层中，允许在不修改原始模型核心参数的情况下进行高效的任务特定微调。**AdapterFusion** 和 **AdapterSoup**
    等技术属于此类别，促进了多个适配器的组合以实现复杂的多任务处理。
- en: Soft Prompt Tuning (SPT)
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示调整（SPT）
- en: A fine-tuning technique where a set of trainable prompt tokens are added to
    the input sequence to guide a pre-trained model towards task-specific performance
    without modifying internal model weights.
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种微调技术，通过在输入序列中添加一组可训练的提示令牌来引导预训练模型向特定任务的性能发展，而无需修改内部模型权重。
- en: Prefix-Tuning
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整
- en: A variation of soft prompt tuning where a fixed sequence of trainable vectors
    is prepended to the input layer at every layer of the model, enhancing task-specific
    adaptation.
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: 一种软提示调整的变体，其中将固定序列的可训练向量添加到模型的每一层的输入层，增强了任务特定的适应能力。
- en: Quantisation
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: 量化
- en: The process of reducing the precision of model weights and activations, often
    from 32-bit to lower-bit representations like 8-bit or 4-bit, to reduce memory
    usage and improve computational efficiency.
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型权重和激活精度的过程，通常从32位减少到8位或4位等低位表示，以减少内存使用并提高计算效率。
- en: Quantised LLMs
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: 量化语言模型（Quantised LLMs）
- en: Large Language Models that have undergone quantisation, a process that reduces
    the precision of model weights and activations, often from 32-bit to 8-bit or
    lower, to enhance memory and computational efficiency.
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: 经过量化的大型语言模型，这一过程减少了模型权重和激活的精度，通常从32位减少到8位或更低，以提高内存和计算效率。
- en: Pruning
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝（Pruning）
- en: A model optimisation technique that reduces the complexity of large language
    models by removing less significant parameters, enabling faster inference and
    lower memory usage.
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模型优化技术，通过去除不太重要的参数来减少大型语言模型的复杂性，从而实现更快的推理和更低的内存使用。
- en: Half Fine-Tuning (HFT)
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: 半微调（Half Fine-Tuning，HFT）
- en: A fine-tuning method where half of the model’s parameters are kept frozen while
    the other half are updated, helping to maintain pre-trained knowledge while adapting
    the model to new tasks.
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
  zh: 一种微调方法，其中模型的一半参数保持冻结，而另一半进行更新，帮助在适应新任务的同时保持预训练知识。
- en: Structured Masking
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化掩蔽（Structured Masking）
- en: A technique that masks entire layers, heads, or other structural components
    of a model to reduce complexity while fine-tuning for specific tasks.
  id: totrans-2175
  prefs: []
  type: TYPE_NORMAL
  zh: 一种掩蔽整个层、头或模型其他结构组件的技术，以在针对特定任务微调时减少复杂性。
- en: Unstructured Masking
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化掩蔽（Unstructured Masking）
- en: A technique where certain parameters of the model are masked out randomly or
    based on a pattern during fine-tuning, allowing for the identification of the
    most important model weights.
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在微调过程中随机或基于模式掩蔽模型某些参数的技术，从而识别出最重要的模型权重。
- en: GLUE
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE
- en: General Language Understanding Evaluation – A benchmark used to evaluate the
    performance of NLP models across a variety of language understanding tasks, such
    as sentiment analysis and natural language inference.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
  zh: 通用语言理解评估（General Language Understanding Evaluation）——一个用于评估NLP模型在各种语言理解任务（如情感分析和自然语言推理）表现的基准。
- en: SuperGLUE
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: 超级GLUE（SuperGLUE）
- en: Super General Language Understanding Evaluation – A more challenging extension
    of GLUE, consisting of harder tasks designed to test the robustness and adaptability
    of NLP models.
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
  zh: 超级通用语言理解评估（Super General Language Understanding Evaluation）——GLUE的一个更具挑战性的扩展，包含更难的任务，旨在测试NLP模型的鲁棒性和适应性。
- en: TruthfulQA
  id: totrans-2182
  prefs: []
  type: TYPE_NORMAL
  zh: 真实QA（TruthfulQA）
- en: A benchmark designed to measure the truthfulness of a language model’s output,
    focusing on factual accuracy and resistance to hallucination.
  id: totrans-2183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个旨在衡量语言模型输出真实性的基准，重点关注事实准确性和对虚假信息的抵抗力。
- en: IFEval
  id: totrans-2184
  prefs: []
  type: TYPE_NORMAL
  zh: IFEval
- en: Instruction Following Evaluation – A benchmark that assesses a model’s ability
    to follow explicit instructions across tasks, usually in the context of fine-tuning
    large models for adherence to specific instructions.
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
  zh: 指令遵循评估（Instruction Following Evaluation）——一个评估模型在各种任务中遵循明确指令能力的基准，通常是在微调大型模型以遵循特定指令的背景下进行评估。
- en: BBH
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
  zh: BBH
- en: Big Bench Hard – A subset of the Big Bench dataset, which consists of particularly
    difficult tasks aimed at evaluating the advanced reasoning abilities of large
    language models.
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: Big Bench Hard——Big Bench数据集的一个子集，包含特别困难的任务，旨在评估大型语言模型的高级推理能力。
- en: MATH
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: MATH
- en: A dataset created to evaluate a model’s ability to solve high-school level mathematical
    problems, presented in formal formats like LaTeX.
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个创建用于评估模型解决高中水平数学问题能力的数据集，问题以LaTeX等正式格式呈现。
- en: GPQA
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
  zh: GPQA
- en: General-Purpose Question Answering – A challenging dataset that features knowledge-based
    questions crafted by experts to assess deep reasoning and factual recall.
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: 通用问题回答（General-Purpose Question Answering）——一个具有挑战性的数据集，包含由专家设计的基于知识的问题，用于评估深度推理和事实回忆能力。
- en: MuSR
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: MuSR
- en: Multimodal Structured Reasoning – A dataset that involves complex problems requiring
    language models to integrate reasoning across modalities, often combining text
    with other forms of data such as images or graphs.
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态结构化推理（Multimodal Structured Reasoning）——一个涉及复杂问题的数据集，要求语言模型在不同模态之间整合推理，通常结合文本和图像或图表等其他数据形式。
- en: MMLU
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU
- en: Massive Multitask Language Understanding – A benchmark that evaluates a language
    model’s ability to perform various tasks across diverse domains, such as humanities,
    STEM, social sciences, and others, typically requiring high-level reasoning.
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模多任务语言理解（Massive Multitask Language Understanding）——一个评估语言模型在各种领域（如人文学科、STEM、社会科学等）执行多种任务能力的基准，通常要求高级推理能力。
- en: MMLU-PRO
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU-PRO
- en: A refined version of the MMLU dataset with a focus on more challenging, multi-choice
    problems, typically requiring the model to parse long-range context.
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU数据集的精炼版本，专注于更具挑战性的多选问题，通常要求模型解析长距离的上下文。
- en: ARC
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
  zh: ARC
- en: AI2 Reasoning Challenge – A benchmark for evaluating a language model’s reasoning
    capabilities using a dataset of multiple-choice science questions.
  id: totrans-2199
  prefs: []
  type: TYPE_NORMAL
  zh: AI2 推理挑战 – 一个评估语言模型推理能力的基准，使用的是多选科学问题的数据集。
- en: COQA
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: COQA
- en: Conversational Question Answering – A benchmark that evaluates how well a language
    model can understand and engage in back-and-forth conversation, especially in
    a question-answer format.
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
  zh: 对话式问答 – 一个评估语言模型在对话中理解和参与的能力的基准，特别是在问答格式下。
- en: DROP
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: DROP
- en: Discrete Reasoning Over Paragraphs – A benchmark that tests a model’s ability
    to perform discrete reasoning over text, especially in scenarios requiring arithmetic,
    comparison, or logical reasoning.
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
  zh: 段落上的离散推理 – 一个测试模型在文本上执行离散推理的能力的基准，特别是在需要算术、比较或逻辑推理的场景中。
- en: SQuAD
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD
- en: Stanford Question Answering Dataset – A popular dataset for evaluating a model’s
    ability to understand and answer questions based on passages of text.
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福问答数据集 – 一个流行的数据集，用于评估模型理解和回答基于文本段落的问题的能力。
- en: TREC
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: TREC
- en: Text REtrieval Conference – A benchmark that evaluates models on various text
    retrieval tasks, often focusing on information retrieval and document search.
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: 文本检索会议 – 一个评估模型在各种文本检索任务中的表现的基准，通常专注于信息检索和文档搜索。
- en: WMT
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
  zh: WMT
- en: Workshop on Machine Translation – A dataset and benchmark for evaluating the
    performance of machine translation systems across different language pairs.
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译研讨会 – 一个评估机器翻译系统在不同语言对间表现的数据集和基准。
- en: XNLI
  id: totrans-2210
  prefs: []
  type: TYPE_NORMAL
  zh: XNLI
- en: Cross-lingual Natural Language Inference – A dataset designed to evaluate a
    model’s ability to understand and infer meaning across multiple languages.
  id: totrans-2211
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言自然语言推理 – 一个设计用于评估模型在多语言中理解和推断含义的数据集。
- en: PiQA
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
  zh: PiQA
- en: Physical Interaction Question Answering – A dataset that measures a model’s
    understanding of physical interactions and everyday tasks.
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
  zh: 物理交互问答 – 一个测量模型理解物理交互和日常任务的数据集。
- en: Winogrande
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
  zh: Winogrande
- en: A large-scale dataset aimed at evaluating a language model’s ability to handle
    commonsense reasoning, typically through tasks that involve resolving ambiguous
    pronouns in sentences.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大规模数据集，旨在评估语言模型处理常识推理的能力，通常通过涉及解析句子中的模糊代词的任务来实现。
- en: RLHF
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF
- en: Reinforcement Learning from Human Feedback – A method where language models
    are fine-tuned based on human-provided feedback, often used to guide models towards
    preferred behaviours or outputs.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人类反馈的强化学习 – 一种通过人类提供的反馈对语言模型进行微调的方法，通常用于引导模型朝着期望的行为或输出。
- en: RAFT
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT
- en: Retrieval-Augmented Fine-Tuning – A method combining retrieval techniques with
    fine-tuning to enhance the performance of language models by allowing them to
    access external information during training or inference.
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强微调 – 一种结合检索技术和微调的方法，通过允许模型在训练或推理过程中访问外部信息来提升语言模型的性能。
- en: References
  id: totrans-2220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N-gram language models. [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf).
    [Accessed 01-07-2024].'
  id: totrans-2221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] N-gram 语言模型。 [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)。
    [访问日期 2024年01月07日]。'
- en: '[2] Anis Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown, 04 2023.'
  id: totrans-2222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Anis Koubaa. Gpt-4 与 gpt-3.5: 简明对比, 2023年4月。'
- en: '[3] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. A survey
    of reinforcement learning from human feedback, 2024.'
  id: totrans-2223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Timo Kaufmann, Paul Weng, Viktor Bengs, 和 Eyke Hüllermeier. 人类反馈强化学习综述,
    2024年。'
- en: '[4] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qian Yang, and Xingxu Xie. A survey on evaluation of large language
    models. ACM Transactions on Intelligent Systems and Technology, 15:1 – 45, 2023.'
  id: totrans-2224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qian Yang, 和 Xingxu Xie. 大语言模型评估综述. ACM 智能系统与技术汇刊, 15:1 – 45, 2023年。'
- en: '[5] Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid,
    Aafaq Iqbal Khan, and Arsalan Shahid. Building trust in conversational ai: A review
    and solution architecture using large language models and knowledge graphs. Big
    Data and Cognitive Computing, 8(6):70, 2024.'
  id: totrans-2225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid,
    Aafaq Iqbal Khan, 和 Arsalan Shahid。构建对话 AI 的信任：使用大型语言模型和知识图谱的综述和解决方案架构。《大数据与认知计算》，8(6):70，2024。'
- en: '[6] Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, and Wenbin Zhang.
    History, development, and principles of large language models-an introductory
    survey, 2024.'
  id: totrans-2226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, 和 Wenbin Zhang。大型语言模型的历史、发展及原理——入门调查，2024。'
- en: '[7] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space, 2013.'
  id: totrans-2227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tomas Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean。高效估计词向量空间中的词表示，2013。'
- en: '[8] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
    Sutskever. Language models are unsupervised multitask learners. 2019.'
  id: totrans-2228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, 和 Ilya Sutskever。语言模型是无监督的多任务学习者，2019。'
- en: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding, 2019.'
  id: totrans-2229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。Bert：用于语言理解的深度双向变换器预训练，2019。'
- en: '[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
    Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
    Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
    Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
    Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
    Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways, 2022.'
  id: totrans-2230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
    Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
    Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
    Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
    Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
    Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew
    M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, 和 Noah Fiedel。Palm：通过路径扩展语言建模，2022。'
- en: '[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models, 2023.'
  id: totrans-2231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和 Guillaume Lample。Llama：开放和高效的基础语言模型，2023。'
- en: '[12] The art of fine-tuning large language models, explained in depth — linkedin.com.
    [https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc](https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc).
    [Accessed 01-07-2024].'
  id: totrans-2232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 大型语言模型的微调艺术，深度解析——linkedin.com。 [https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc](https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc)。
    [访问日期：2024年01月07日]。'
- en: '[13] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
    Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A comprehensive overview of
    large language models, 2024.'
  id: totrans-2233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
    Usman, Naveed Akhtar, Nick Barnes, 和 Ajmal Mian。大型语言模型的综合概述，2024。'
- en: '[14] Jeff Li, MBA, PMP on LinkedIn: Fine-tuning versus RAG in Generative AI
    Applications Architecture — linkedin.com. [https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT](https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT).
    [Accessed 01-08-2024].'
  id: totrans-2234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jeff Li, MBA, PMP 在 LinkedIn 上：生成式 AI 应用架构中的精调与 RAG — linkedin.com。 [https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT](https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT)。
    [访问时间 2024-08-01]。'
- en: '[15] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, and Hua
    Wu. Hft: Half fine-tuning for large language models. arXiv preprint arXiv:2404.18466,
    2024.'
  id: totrans-2235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun 和 Hua Wu。Hft:
    大型语言模型的半精调。arXiv 预印本 arXiv:2404.18466，2024。'
- en: '[16] Rion Snow, Brendan O’Connor, Dan Jurafsky, and Andrew Y Ng. Cheap and
    fast—but is it good? evaluating non-expert annotations for natural language tasks.
    In Proceedings of the Conference on Empirical Methods in Natural Language Processing
    (EMNLP), pages 254–263, 2008.'
  id: totrans-2236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Rion Snow, Brendan O’Connor, Dan Jurafsky 和 Andrew Y Ng。便宜且快速——但是否好？评估非专家注释在自然语言任务中的有效性。在自然语言处理经验方法会议（EMNLP）会议录，第254–263页，2008。'
- en: '[17] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu,
    and Christopher Ré. Snorkel: Rapid training data creation with weak supervision.
    In Proceedings of the VLDB Endowment, volume 11, pages 269–282, 2017.'
  id: totrans-2237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu
    和 Christopher Ré。Snorkel: 利用弱监督快速创建训练数据。在 VLDB 基金会会议录，第11卷，第269–282页，2017。'
- en: '[18] Liang Ding, Philipp Gentner, Artur Duda, Vaibhav Sangtani, Dominik Ziegler,
    Max Hennen, Siddharth Jain, and Roland Werthschützky. Automatic data labeling
    for supervised learning with applications to visual inspection of mixed-plastic
    waste. Journal of Cleaner Production, 234:1033–1044, 2019.'
  id: totrans-2238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Liang Ding, Philipp Gentner, Artur Duda, Vaibhav Sangtani, Dominik Ziegler,
    Max Hennen, Siddharth Jain 和 Roland Werthschützky。用于监督学习的自动数据标注及其在混合塑料废料视觉检查中的应用。《清洁生产杂志》，234:1033–1044，2019。'
- en: '[19] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. In Proceedings of the International Conference
    on Learning Representations (ICLR), 2013.'
  id: totrans-2239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean。向量空间中词表示的高效估计。在国际学习表示会议（ICLR）会议录，2013。'
- en: '[20] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove:
    Global vectors for word representation. In Proceedings of the 2014 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,
    2014.'
  id: totrans-2240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Jeffrey Pennington, Richard Socher 和 Christopher D Manning。Glove: 全局词向量表示。在2014年自然语言处理经验方法会议（EMNLP）会议录，第1532–1543页，2014。'
- en: '[21] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine
    translation models with monolingual data. Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    86–96, 2016.'
  id: totrans-2241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Rico Sennrich, Barry Haddow 和 Alexandra Birch。利用单语数据改进神经机器翻译模型。第54届计算语言学协会年会（第1卷：长论文）会议录，第86–96页，2016。'
- en: '[22] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box
    adversarial examples for text classification. In Proceedings of the 56th Annual
    Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
    pages 31–36, 2017.'
  id: totrans-2242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Javid Ebrahimi, Anyi Rao, Daniel Lowd 和 Dejing Dou。Hotflip: 用于文本分类的白盒对抗示例。在第56届计算语言学协会年会（第2卷：短论文）会议录，第31–36页，2017。'
- en: '[23] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
    2020.'
  id: totrans-2243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。arXiv 预印本 arXiv:2005.14165，2020。'
- en: '[24] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models
    better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), pages 3816–3830, 2021.'
  id: totrans-2244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Tianyu Gao, Adam Fisch 和 Danqi Chen。使预训练语言模型更好地进行少样本学习。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长论文）会议录，第3816–3830页，2021。'
- en: '[25] Steven Feng, Varun Gangal, Jinjun Wei, Yashvardhan Chandrasekhar, Yichong
    Chen, Dani He, Shuyang Huang, Faisal Ladhak, Jiao Lee, Xinyi Li, et al. A survey
    of data augmentation approaches for nlp. arXiv preprint arXiv:2106.07499, 2021.'
  id: totrans-2245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Steven Feng、Varun Gangal、Jinjun Wei、Yashvardhan Chandrasekhar、Yichong
    Chen、Dani He、Shuyang Huang、Faisal Ladhak、Jiao Lee、Xinyi Li 等。NLP 数据增强方法综述。arXiv
    预印本 arXiv:2106.07499，2021年。'
- en: '[26] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,
    Doug Downey, and Noah A Smith. Don’t stop pretraining: Adapt language models to
    domains and tasks. In Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics, pages 8342–8360, 2020.'
  id: totrans-2246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Suchin Gururangan、Ana Marasović、Swabha Swayamdipta、Kyle Lo、Iz Beltagy、Doug
    Downey 和 Noah A Smith。不要停止预训练：将语言模型适应于领域和任务。第58届计算语言学协会年会论文集，第8342–8360页，2020年。'
- en: '[27] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
    Shmitchell. On the dangers of stochastic parrots: Can language models be too big?
    Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,
    pages 610–623, 2021.'
  id: totrans-2247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Emily M Bender、Timnit Gebru、Angelina McMillan-Major 和 Shmargaret Shmitchell。关于随机鹦鹉的危险：语言模型会不会过大？2021年
    ACM 公平性、问责性和透明度会议论文集，第610–623页，2021年。'
- en: '[28] Reuben Binns. Fairness in machine learning: Lessons from political philosophy.
    Proceedings of the 2018 Conference on Fairness, Accountability, and Transparency,
    pages 149–159, 2018.'
  id: totrans-2248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Reuben Binns。机器学习中的公平性：来自政治哲学的教训。2018年公平性、问责性和透明度会议论文集，第149–159页，2018年。'
- en: '[29] Sebastian Ruder. The stanford natural language inference (snli) corpus.
    arXiv preprint arXiv:1807.03519, 2021.'
  id: totrans-2249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Sebastian Ruder。斯坦福自然语言推理（SNLI）语料库。arXiv 预印本 arXiv:1807.03519，2021年。'
- en: '[30] Pradeep Rajan, Krishna Vyas, Rajiv Bansal, Ranjan Sharma, and Shubhranshu
    Mukherjee. Machine learning for data preprocessing. Journal of Big Data, 6(1):1–25,
    2019.'
  id: totrans-2250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Pradeep Rajan、Krishna Vyas、Rajiv Bansal、Ranjan Sharma 和 Shubhranshu Mukherjee。用于数据预处理的机器学习。《大数据期刊》，6(1):1–25，2019年。'
- en: '[31] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.
    Smote: synthetic minority over-sampling technique. Journal of Artificial Intelligence
    Research, 16:321–357, 2002.'
  id: totrans-2251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Nitesh V Chawla、Kevin W Bowyer、Lawrence O Hall 和 W Philip Kegelmeyer。SMOTE：合成少数类过采样技术。《人工智能研究期刊》，16:321–357，2002年。'
- en: '[32] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation
    for deep learning. Journal of Big Data, 6(1):1–48, 2019.'
  id: totrans-2252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Connor Shorten 和 Taghi M Khoshgoftaar。深度学习图像数据增强综述。《大数据期刊》，6(1):1–48，2019年。'
- en: '[33] Alexander Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and
    Christopher Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings
    of the VLDB Endowment, 11(3):269–282, 2020.'
  id: totrans-2253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Alexander Ratner、Henry Ehrenberg、Zeshan Hussain、Jared Dunnmon 和 Christopher
    Ré。Snorkel：利用弱监督快速创建训练数据。VLDB 促进会会议论文集，11(3):269–282，2020年。'
- en: '[34] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine
    learning: Lessons from political philosophy. In Proceedings of the 2017 ACM on
    Conference on Fairness, Accountability, and Transparency, pages 149–159, 2017.'
  id: totrans-2254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Solon Barocas、Moritz Hardt 和 Arvind Narayanan。机器学习中的公平性：来自政治哲学的教训。第2017届
    ACM 公平性、问责性和透明度会议论文集，第149–159页，2017年。'
- en: '[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers:
    State-of-the-art natural language processing. Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing: System Demonstrations, pages
    38–45, 2020.'
  id: totrans-2255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement Delangue、Anthony
    Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz 等。Transformers：最先进的自然语言处理。2020年自然语言处理经验方法会议：系统演示论文集，第38–45页，2020年。'
- en: '[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
    Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:
    An imperative style, high-performance deep learning library. Advances in Neural
    Information Processing Systems, 32, 2019.'
  id: totrans-2256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga 等。PyTorch：一种命令式风格的高性能深度学习库。《神经信息处理系统进展》，32，2019年。'
- en: '[37] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
    Tensorflow: Large-scale machine learning on heterogeneous distributed systems.
    arXiv preprint arXiv:1603.04467, 2015.'
  id: totrans-2257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Martín Abadi、Ashish Agarwal、Paul Barham、Eugene Brevdo、Zhifeng Chen、Craig
    Citro、Greg S Corrado、Andy Davis、Jeffrey Dean、Matthieu Devin 等。TensorFlow：异构分布式系统上的大规模机器学习。arXiv
    预印本 arXiv:1603.04467，2015年。'
- en: '[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-2258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器预训练。arXiv预印本
    arXiv:1810.04805，2018年。'
- en: '[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  id: totrans-2259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov。RoBERTa：一种强健优化的BERT预训练方法。arXiv预印本
    arXiv:1907.11692，2019年。'
- en: '[40] Sheng Shen, Zhewei Dong, Xiaocheng Ye, Linjian Ma, Zhewei Li, Zirui Wang,
    Samyam Rajbhandari, Yuxiong Wang, and Zhen Yang. Q-bert: Hessian based ultra low
    precision quantization of bert. Proceedings of the AAAI Conference on Artificial
    Intelligence, 34(05):8815–8821, 2020.'
  id: totrans-2260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Sheng Shen, Zhewei Dong, Xiaocheng Ye, Linjian Ma, Zhewei Li, Zirui Wang,
    Samyam Rajbhandari, Yuxiong Wang, 和 Zhen Yang。Q-bert：基于Hessian的超低精度BERT量化。AAAI人工智能会议论文集，34(05)：8815–8821，2020年。'
- en: '[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
    Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9,
    2019.'
  id: totrans-2261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, 和 Ilya
    Sutskever。语言模型是无监督的多任务学习者。OpenAI博客，1(8)：9，2019年。'
- en: '[42] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
    Hanna Wallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets. Communications
    of the ACM, 64(12):86–92, 2021.'
  id: totrans-2262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
    Hanna Wallach, Hal Daumé III, 和 Kate Crawford。数据集的数据表。ACM通讯，64(12)：86–92，2021年。'
- en: '[43] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-2263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Diederik P Kingma 和 Jimmy Ba。Adam：一种随机优化方法。arXiv预印本 arXiv:1412.6980，2014年。'
- en: '[44] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
    Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter
    performance analysis of a tensor processing unit. Proceedings of the 44th Annual
    International Symposium on Computer Architecture, pages 1–12, 2017.'
  id: totrans-2264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
    Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, 等人。在数据中心的张量处理单元性能分析。第44届国际计算机架构年会论文集，第1–12页，2017年。'
- en: '[45] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
    Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
    Tensorflow: A system for large-scale machine learning. 12th USENIX Symposium on
    Operating Systems Design and Implementation (OSDI 16), pages 265–283, 2016.'
  id: totrans-2265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
    Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 等人。Tensorflow：大规模机器学习系统。第12届USENIX操作系统设计与实现研讨会（OSDI
    16），第265–283页，2016年。'
- en: '[46] Mohammad Shoeybi, Mostofa Patwary, Raghavendra Puri, Patrick LeGresley,
    Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter
    language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.'
  id: totrans-2266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Mohammad Shoeybi, Mostofa Patwary, Raghavendra Puri, Patrick LeGresley,
    Jared Casper, 和 Bryan Catanzaro。Megatron-lm：利用模型并行训练数十亿参数的语言模型。arXiv预印本 arXiv:1909.08053，2019年。'
- en: '[47] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
    Bhojanapalli, Xiaodan Song, James Demmel, Cho-Jui Hsieh, and Payal Yadollahpour.
    Large batch optimization for deep learning: Training bert in 76 minutes. arXiv
    preprint arXiv:1904.00962, 2019.'
  id: totrans-2267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
    Bhojanapalli, Xiaodan Song, James Demmel, Cho-Jui Hsieh, 和 Payal Yadollahpour。深度学习的大批量优化：76分钟内训练BERT。arXiv预印本
    arXiv:1904.00962，2019年。'
- en: '[48] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. 2016.'
  id: totrans-2268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville。《深度学习》。2016年。'
- en: '[49] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization.
    Journal of Machine Learning Research, 13(2):281–305, 2012.'
  id: totrans-2269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] James Bergstra 和 Yoshua Bengio。超参数优化的随机搜索。机器学习研究杂志，13(2)：281–305，2012年。'
- en: '[50] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated Machine
    Learning: Methods, Systems, Challenges. Springer Nature, 2019.'
  id: totrans-2270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Frank Hutter, Lars Kotthoff, 和 Joaquin Vanschoren。自动化机器学习：方法、系统、挑战。Springer
    Nature，2019年。'
- en: '[51] Lutz Prechelt. Early stopping-but when? Neural Networks: Tricks of the
    trade, pages 55–69, 1998.'
  id: totrans-2271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Lutz Prechelt. 早期停止——但什么时候？《神经网络：实用技巧》，第55–69页，1998年。'
- en: '[52] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed
    deep learning in tensorflow. arXiv preprint arXiv:1802.05799, 2018.'
  id: totrans-2272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Alexander Sergeev 和 Mike Del Balso。Horovod：在TensorFlow中快速且简单的分布式深度学习。arXiv预印本
    arXiv:1802.05799，2018年。'
- en: '[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Deepspeed:
    Extreme-scale model training for everyone. arXiv preprint arXiv:2007.04822, 2020.'
  id: totrans-2273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, 和 Yuxiong He. Deepspeed：为所有人提供极大规模模型训练。
    arXiv预印本 arXiv:2007.04822，2020年。'
- en: '[54] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
    Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
    Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2018.'
  id: totrans-2274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
    Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
    Venkatesh, 等等. 混合精度训练。 arXiv预印本 arXiv:1710.03740，2018年。'
- en: '[55] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou,
    Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann,
    Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley
    Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong,
    Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado,
    Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards
    expert-level medical question answering with large language models, 2023.'
  id: totrans-2275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le
    Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann,
    Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley
    Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong,
    Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado,
    Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, 和 Vivek Natarajan. 面向专家级医学问答的大型语言模型，2023年。'
- en: '[56] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source
    financial large language models, 2023.'
  id: totrans-2276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Hongyang Yang, Xiao-Yang Liu, 和 Christina Dan Wang. Fingpt：开源金融大型语言模型，2023年。'
- en: '[57] Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe
    Guo, and Yu-Feng Li. Lawgpt: A chinese legal knowledge-enhanced large language
    model, 2024.'
  id: totrans-2277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe
    Guo, 和 Yu-Feng Li. Lawgpt：一个中文法律知识增强的大型语言模型，2024年。'
- en: '[58] Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao
    Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai,
    Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao,
    Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie
    Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Zhongkai Ye, Lidong Pei, and
    Changyang Tu. Pharmagpt: Domain-specific large language models for bio-pharmaceutical
    and chemistry, 2024.'
  id: totrans-2278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao
    Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai,
    Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao,
    Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie
    Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Zhongkai Ye, Lidong Pei, 和 Changyang
    Tu. Pharmagpt：用于生物制药和化学的领域特定大型语言模型，2024年。'
- en: '[59] Writer Engineering team. Palmyra-Fin-70B-32k: a powerful LLM designed
    for Finance. [https://dev.writer.com](https://dev.writer.com), 2024.'
  id: totrans-2279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Writer Engineering team. Palmyra-Fin-70B-32k：为金融设计的强大LLM。 [https://dev.writer.com](https://dev.writer.com)，2024年。'
- en: '[60] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient
    fine-tuning for large models: A comprehensive survey, 2024.'
  id: totrans-2280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, 和 Sai Qian Zhang. 大型模型的参数高效微调：综合调查，2024年。'
- en: '[61] Lin Tian, Xiuzhen Zhang, and Jey Han Lau. Metatroll: Few-shot detection
    of state-sponsored trolls with transformer adapters. In Proceedings of the ACM
    Web Conference 2023, WWW ’23\. ACM, April 2023.'
  id: totrans-2281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Lin Tian, Xiuzhen Zhang, 和 Jey Han Lau. Metatroll：使用变换器适配器进行的国家支持的恶意行为者的少样本检测。在ACM
    Web Conference 2023的论文集中，WWW ’23。 ACM，2023年4月。'
- en: '[62] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models, 2021.'
  id: totrans-2282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen. Lora：大型语言模型的低秩适配，2021年。'
- en: '[63] PhD Sebastian Raschka. Practical Tips for Finetuning LLMs Using LoRA (Low-Rank
    Adaptation) — magazine.sebastianraschka.com. [https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms).
    [Accessed 01-08-2024].'
  id: totrans-2283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] PhD Sebastian Raschka. 使用LoRA（低秩适配）微调LLMs的实用技巧 — magazine.sebastianraschka.com.
    [https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)。
    [访问时间 01-08-2024]。'
- en: '[64] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms, 2023.'
  id: totrans-2284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. Qlora：量化大语言模型的高效微调，2023年。'
- en: '[65] What is QLoRa? — Analytics Vidhya — community.analyticsvidhya.com. [https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora](https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora).
    [Accessed 01-08-2024].'
  id: totrans-2285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 什么是 QLoRa？ — Analytics Vidhya — community.analyticsvidhya.com. [https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora](https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora).
    [访问日期：2024年8月1日]。'
- en: '[66] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank
    Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation,
    2024.'
  id: totrans-2286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank
    Wang, Kwang-Ting Cheng, 和 Min-Hung Chen. Dora: 权重分解低秩适应, 2024。'
- en: '[67] Apple intelligence foundation language models, 2024.'
  id: totrans-2287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Apple intelligence foundation language models, 2024。'
- en: '[68] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, and Hua
    Wu. Hft: Half fine-tuning for large language models, 2024.'
  id: totrans-2288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, 和 Hua Wu.
    Hft: 大型语言模型的半精调, 2024。'
- en: '[69] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin
    Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, and
    Gregory Diamos. Banishing llm hallucinations requires rethinking generalization,
    2024.'
  id: totrans-2289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin
    Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, 和 Gregory
    Diamos. 消除 LLM 幻觉需要重新思考泛化, 2024。'
- en: '[70] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
    Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou
    Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard
    Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
    Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril,
    Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.'
  id: totrans-2290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
    Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma
    Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, 和 William El Sayed. Mixtral of experts,
    2024。'
- en: '[71] Applying Mixture of Experts in LLM Architectures — NVIDIA Technical Blog
    — developer.nvidia.com. [https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/).
    [Accessed 01-08-2024].'
  id: totrans-2291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 在 LLM 架构中应用专家混合 — NVIDIA 技术博客 — developer.nvidia.com. [https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/).
    [访问日期：2024年8月1日]。'
- en: '[72] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents
    enhances large language model capabilities, 2024.'
  id: totrans-2292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, 和 James Zou. Mixture-of-agents
    增强大型语言模型能力, 2024。'
- en: '[73] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms, 2017.'
  id: totrans-2293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg Klimov.
    近端策略优化算法, 2017。'
- en: '[74] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D.
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model, 2024.'
  id: totrans-2294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
    D. Manning, 和 Chelsea Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型, 2024。'
- en: '[75] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju
    Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive
    study, 2024.'
  id: totrans-2295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju
    Wang, Chao Yu, 和 Yi Wu. DPO 是否优于 PPO 进行 LLM 对齐？ 一项全面研究, 2024。'
- en: '[76] What are the most effective techniques for pruning ai models? — linkedin.com.
    [https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef](https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef).
    [Accessed 05-07-2024].'
  id: totrans-2296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] 最有效的 AI 模型剪枝技术是什么？ — linkedin.com. [https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef](https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef).
    [访问日期：2024年7月5日]。'
- en: '[77] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui
    Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
    Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
    Song, and Bo Li. Decodingtrust: A comprehensive assessment of trustworthiness
    in gpt models, 2024.'
  id: totrans-2297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui
    Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
    Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
    Song, 和 Bo Li. Decodingtrust: 对 GPT 模型可信度的全面评估, 2024。'
- en: '[78] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer,
    Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian
    Khabsa. Llama guard: Llm-based input-output safeguard for human-ai conversations,
    2023.'
  id: totrans-2298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] 哈坎·伊南、卡尔提克耶亚·乌帕萨尼、简丰·池、拉希·荣塔、克里提卡·艾耶尔、尤宁·毛、迈克尔·托恩切夫、青胡、布莱恩·富勒、达维德·特斯图吉内和马迪安·哈布萨。Llama
    guard：基于 LLM 的人类-人工智能对话输入输出保护，2023年。'
- en: '[79] Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza
    Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu,
    Olivia Sturman, and Oscar Wahltinez. Shieldgemma: Generative ai content moderation
    based on gemma, 2024.'
  id: totrans-2299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] 曾文君、刘宇池、瑞安·穆林斯、吕多维克·佩兰、乔·费尔南德斯、哈姆扎·哈科斯、卡尔提克·纳拉辛汉、德鲁·普劳德、皮尤什·库马尔、巴克提普里亚·拉达拉普、奥利维亚·斯特曼和奥斯卡·瓦尔蒂内斯。Shieldgemma：基于
    Gemma 的生成式人工智能内容审查，2024年。'
- en: '[80] Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin,
    Nathan Lambert, Yejin Choi, and Nouha Dziri. Wildguard: Open one-stop moderation
    tools for safety risks, jailbreaks, and refusals of llms, 2024.'
  id: totrans-2300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 韩胜久、卡维尔·拉奥、艾莉森·艾廷格、李伟·江、比尔·宇辰·林、内森·兰伯特、叶津·崔和诺哈·兹里。Wildguard：开放的一站式审查工具，针对安全风险、越狱和
    LLM 的拒绝，2024年。'
- en: '[81] Vishal Mysore. LLM Deployment Strategies : Its not Magic , Its Logic!
    — visrow. [https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4](https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4).
    [Accessed 07-08-2024].'
  id: totrans-2301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] 维沙尔·迈索尔。LLM 部署策略：这不是魔法，这是逻辑！— visrow。 [https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4](https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4)。
    [访问日期 2024年8月7日]。'
- en: '[82] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management
    for large language model serving with pagedattention, 2023.'
  id: totrans-2302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] 权宇硕、朱焕林、司源·庄、英盛、连敏·郑、科迪·郝宇、约瑟夫·E·冈萨雷斯、郝张和伊昂·斯托伊卡。大规模语言模型服务的高效内存管理与分页注意机制，2023年。'
- en: '[83] Preprocess and fine-tune llms quickly and cost-effectively using amazon
    emr serverless and amazon sagemaker — aws.amazon.com. [https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/](https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/).
    [Accessed 06-08-2024].'
  id: totrans-2303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] 使用 amazon emr serverless 和 amazon sagemaker 快速、经济地预处理和微调 llms — aws.amazon.com。
    [https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/](https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/)。
    [访问日期 2024年8月6日]。'
- en: '[84] Nvidia nemo build and customize your own llms (with tutorial) — run.ai.
    [https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo](https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo).
    [Accessed 07-08-2024].'
  id: totrans-2304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Nvidia nemo 构建和定制你自己的 llms（附教程）— run.ai。 [https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo](https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo)。
    [访问日期 2024年8月7日]。'
- en: '[85] Nvidia. What is nvidia nemo? [https://www.nvidia.com/en-us/ai-data-science/products/nemo/](https://www.nvidia.com/en-us/ai-data-science/products/nemo/).'
  id: totrans-2305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Nvidia。什么是 nvidia nemo？ [https://www.nvidia.com/en-us/ai-data-science/products/nemo/](https://www.nvidia.com/en-us/ai-data-science/products/nemo/)。'
- en: '[86] Gemini Team and Rohan Anil et al. Gemini: A family of highly capable multimodal
    models, 2024.'
  id: totrans-2306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Gemini 团队和罗汉·安尼尔等。Gemini：一系列高能力的多模态模型，2024年。'
- en: '[87] Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang
    He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang, and Lizhuang Ma.
    Efficient multimodal large language models: A survey, 2024.'
  id: totrans-2307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] 金宜章、李坚、刘业鑫、天俊·顾、凯·吴、郑凯·蒋、穆扬·赫、博·赵、辛·谭、振业·甘、亚彪·王、成杰·王和李壮·马。高效的多模态大规模语言模型：综述，2024年。'
- en: '[88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, and Ilya Sutskever. Learning transferable visual models from natural
    language supervision, 2021.'
  id: totrans-2308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] 亚历克·拉德福德、郑沃克·金、克里斯·哈拉西、阿迪提亚·拉梅什、加布里埃尔·戈、桑迪尼·阿加瓦尔、吉里什·萨斯特里、阿曼达·阿斯克尔、帕梅拉·米什金、杰克·克拉克、格雷琴·克鲁格和伊利亚·苏茨克维尔。通过自然语言监督学习可迁移的视觉模型，2021年。'
- en: '[89] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and
    cheaper than in-context learning, 2022.'
  id: totrans-2309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] 刘浩坤、德里克·谭、穆罕默德·穆基斯、杰伊·莫赫塔、滕浩·黄、莫希特·班萨尔和科林·拉费尔。少量样本参数高效微调优于上下文学习，更加经济，2022年。'
- en: '[90] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi.
    Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free
    low-rank adaptation, 2023.'
  id: totrans-2310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev 和 Ali Ghodsi. Dylora：通过动态无搜索低秩适应进行预训练模型的参数高效调优，2023。'
- en: '[91] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa:
    Memory-efficient low-rank adaptation for large language models fine-tuning, 2023.'
  id: totrans-2311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu 和 Bo Li. Lora-fa：大语言模型微调的内存高效低秩适应，2023。'
- en: '[92] Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, and Rongrong Ji. Not all
    attention is needed: Parameter and computation efficient transfer learning for
    multi-modal large language models, 2024.'
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun 和 Rongrong Ji. 并非所有注意力都是必要的：多模态大型语言模型的参数和计算高效转移学习，2024。'
- en: '[93] Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han, and Yunhe Wang.
    Memory-space visual prompting for efficient vision-language fine-tuning, 2024.'
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han 和 Yunhe Wang.
    记忆空间视觉提示用于高效的视觉语言微调，2024。'
- en: '[94] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng
    Qiu. Full parameter fine-tuning for large language models with limited resources,
    2024.'
  id: totrans-2314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo 和 Xipeng Qiu.
    在资源有限的情况下对大型语言模型进行完整参数微调，2024。'
- en: '[95] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee,
    Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes,
    2024.'
  id: totrans-2315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee,
    Danqi Chen 和 Sanjeev Arora. 仅通过前向传递进行语言模型微调，2024。'
- en: '[96] Gang Liu, Jinlong He, Pengfei Li, Genrong He, Zhaolin Chen, and Shenjun
    Zhong. Pefomed: Parameter efficient fine-tuning of multimodal large language models
    for medical imaging, 2024.'
  id: totrans-2316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Gang Liu, Jinlong He, Pengfei Li, Genrong He, Zhaolin Chen 和 Shenjun Zhong.
    Pefomed：用于医学影像的多模态大型语言模型的参数高效微调，2024。'
- en: '[97] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
    Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation
    learning by masked prediction of hidden units, 2021.'
  id: totrans-2317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
    Salakhutdinov 和 Abdelrahman Mohamed. Hubert：通过对隐藏单元的掩蔽预测进行自监督语音表征学习，2021。'
- en: '[98] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec
    2.0: A framework for self-supervised learning of speech representations, 2020.'
  id: totrans-2318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed 和 Michael Auli. wav2vec
    2.0：一个用于自监督语音表征学习的框架，2020。'
- en: '[99] Deepak Babu P R. Audio language models and multimodal architecture — prdeepak.babu.
    [https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac](https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac).
    [Accessed 19-07-2024].'
  id: totrans-2319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Deepak Babu P R. 音频语言模型与多模态架构 — prdeepak.babu. [https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac](https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac).
    [访问日期：2024年7月19日]。'
- en: '[100] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur
    Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei
    Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg,
    Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco
    Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu,
    Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas
    Zilka, and Christian Frank. Audiopalm: A large language model that can speak and
    listen, 2023.'
  id: totrans-2320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur
    Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei
    Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg,
    Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco
    Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu,
    Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas
    Zilka 和 Christian Frank. Audiopalm：一种能够说话和听的语言模型，2023。'
- en: '[101] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier
    Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco
    Tagliasacchi, and Neil Zeghidour. Audiolm: a language modeling approach to audio
    generation, 2023.'
  id: totrans-2321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier
    Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco
    Tagliasacchi 和 Neil Zeghidour. Audiolm：一种面向音频生成的语言建模方法，2023。'
- en: '[102] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,
    Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A comprehensive overview
    of large language models, 2024.'
  id: totrans-2322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,
    Muhammad Usman, Naveed Akhtar, Nick Barnes 和 Ajmal Mian. 大型语言模型的综合概述，2024。'
- en: '[103] Fine-tune llama 2 with lora: Customizing a large language model for question-answering
    — rocm.blogs.amd.com. [https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html).
    [Accessed 15-07-2024].'
  id: totrans-2323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] 使用Lora对Llama 2进行微调：为问答定制大型语言模型 — rocm.blogs.amd.com。 [https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html)。
    [访问日期：2024年7月15日]。'
- en: '[104] Aayush Mittal. Understanding llm fine-tuning: Tailoring large language
    models to your unique requirements — linkedin.com. [https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements](https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements).
    [Accessed 11-07-2024].'
  id: totrans-2324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Aayush Mittal。理解LLM微调：将大型语言模型定制为符合你独特需求的模型 — linkedin.com。 [https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements](https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements)。
    [访问日期：2024年7月11日]。'
- en: '[105] Alan Ansell, Ivan Vulić, Hannah Sterz, Anna Korhonen, and Edoardo M.
    Ponti. Scaling sparse fine-tuning to large language models, 2024.'
  id: totrans-2325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Alan Ansell、Ivan Vulić、Hannah Sterz、Anna Korhonen 和 Edoardo M. Ponti。将稀疏微调扩展到大型语言模型，2024年。'
- en: '[106] Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei,
    and Tat-Seng Chua. Data-efficient fine-tuning for llm-based recommendation, 2024.'
  id: totrans-2326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Xinyu Lin、Wenjie Wang、Yongqi Li、Shuo Yang、Fuli Feng、Yinwei Wei 和 Tat-Seng
    Chua。基于LLM的推荐系统的数据高效微调，2024年。'
- en: '[107] Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Wenliang Zhong, Xinwang
    Liu, Guannan Zhang, and Kejun Zhang. End-to-end learnable clustering for intent
    learning in recommendation, 2024.'
  id: totrans-2327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Yue Liu、Shihao Zhu、Jun Xia、Yingwei Ma、Jian Ma、Wenliang Zhong、Xinwang
    Liu、Guannan Zhang 和 Kejun Zhang。推荐系统中用于意图学习的端到端可学习聚类，2024年。'
- en: '[108] Haoran Li, Xinyuan Zhao, Dadi Guo, Hanlin Gu, Ziqian Zeng, Yuxing Han,
    Yangqiu Song, Lixin Fan, and Qiang Yang. Federated domain-specific knowledge transfer
    on large language models using synthetic data, 2024.'
  id: totrans-2328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Haoran Li、Xinyuan Zhao、Dadi Guo、Hanlin Gu、Ziqian Zeng、Yuxing Han、Yangqiu
    Song、Lixin Fan 和 Qiang Yang。使用合成数据对大型语言模型进行联邦领域特定知识转移，2024年。'
- en: '[109] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
    and Adrian Vladu. Towards deep learning models resistant to adversarial attacks,
    2019.'
  id: totrans-2329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Aleksander Madry、Aleksandar Makelov、Ludwig Schmidt、Dimitris Tsipras 和
    Adrian Vladu。针对对抗攻击的深度学习模型，2019年。'
