- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: A Framework for Fine-Tuning LLMs using Heterogeneous Feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02861](https://ar5iv.labs.arxiv.org/html/2408.02861)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ryan Aponte ¹, Ryan A. Rossi ², Shunan Guo², Franck Dernoncourt²,
  prefs: []
  type: TYPE_NORMAL
- en: Tong Yu², Xiang Chen², Subrata Mitra², Nedim Lipka ²
  prefs: []
  type: TYPE_NORMAL
- en: ¹Carnegie Mellon University, ²Adobe Research
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have been applied to a wide range of tasks, including
    text summarization, web navigation, and chatbots. They have benefitted from supervised
    fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) following
    an unsupervised pretraining. These datasets can be difficult to collect, limited
    in scope, and vary in sample quality. Additionally, datasets can vary extensively
    in supervision format, from numerical to binary as well as multi-dimensional with
    many different values. We present a framework for fine-tuning LLMs using heterogeneous
    feedback, which has two main components. First, we combine the heterogeneous feedback
    data into a single supervision format, compatible with methods like SFT and RLHF.
    Next, given this unified feedback dataset, we extract a high-quality and diverse
    subset to obtain performance increases potentially exceeding the full dataset.
    We conduct extensive experiments to understand the effectiveness of these techniques
    for incorporating heterogeneous feedback, and demonstrate improvements from using
    a high-quality and diverse subset of the data. We find that our framework is able
    to improve models in multiple areas simultaneously, such as in instruction following
    and bias reduction.
  prefs: []
  type: TYPE_NORMAL
- en: A Framework for Fine-Tuning LLMs using Heterogeneous Feedback
  prefs: []
  type: TYPE_NORMAL
- en: Ryan Aponte ¹, Ryan A. Rossi ², Shunan Guo², Franck Dernoncourt², Tong Yu²,
    Xiang Chen², Subrata Mitra², Nedim Lipka ² ¹Carnegie Mellon University, ²Adobe
    Research
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are fine-tuned for a variety of purposes, such as for instruction following
    in InstructGPT Ouyang et al. ([2022](#bib.bib11)). The fine-tuning process generally
    begins with collecting examples of desired model behavior and performing supervised
    learning. Some models stop at SFT Chiang et al. ([2023](#bib.bib2)), while InstructGPT
    follows this by training a reward model based on binary human preference data.
    The fine-tuned model is then further refined using RLHF, using a signal from the
    reward model. In the example of InstructGPT, the algorithm used is Proximal Policy
    Optimization (PPO) Schulman et al. ([2017](#bib.bib14)). For each of these steps,
    the fine-tuning dataset uses a single form of supervision. Fine-tuning datasets
    exist for a variety of purposes, from training chat-based assistants in OASST Köpf
    et al. ([2023](#bib.bib6)), coreference resolution in WinoGrande Sakaguchi et al.
    ([2019](#bib.bib13)), helpfulness, honesty, and harmlessness in Anthropic HHH Nakano
    et al. ([2021](#bib.bib10)), and logical reasoning in OpenPlatypus Lee et al.
    ([2024](#bib.bib7)). Supervision format varies, from binary preference in Anthropic
    HHH, to several numerical labels OASST, to a string response in OpenPlatypus.
    Although fine-tuning has been successful in mitigating the limitations of pretrained
    LLMs, these methods require data of a single supervision type, restricting the
    scope of preference data. Recent work has filtered fine-tuning datasets to reduce
    cost and increase quality Wang et al. ([2024](#bib.bib17)).  Wu et al. ([2023](#bib.bib18))
    use LLMs to generate embeddings for fine-tuning data which is clustered with k-center-greedy Sener
    and Savarese ([2018](#bib.bib15)) using an iterative process.  Kung et al. ([2023](#bib.bib5))
    randomly delete words in prompts and measure how the response probability changes
    as a measure of the model’s uncertainty.  Li et al. ([2024](#bib.bib9)) outperform
    Alpaca as evaluated by LLM preference using only 5% of its fine-tuning data.
  prefs: []
  type: TYPE_NORMAL
- en: We present a framework to use multiple fine-tuning data types, permitting the
    use of more fine-tuning datasets and fine-tuning for multiple tasks simultaneously.
    Using multiple datasets enables fine-tuning for different goals simultaneously,
    such as for logical reasoning and to reduce bias, and provides a more accurate
    view of human preference by broadening the scope of fine-tuning data. Our framework
    selects a high-quality and diverse subset of the data to make fine-tuning more
    effective.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56cfc0315303312ab6932898501318dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Framework. First, we concatenate the datasets into a dataset of heterogeneous
    feedback. We then score samples based on quality and prompt diversity, remove
    a fraction of the samples (a hyperparameter), forming the homogeneous dataset
    $D_{train}$. Standard fine-tuning methods are then applied to a pre-trained LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary contribution of our framework is to be able to use fine-tuning data
    of heterogeneous supervision. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A
    Framework for Fine-Tuning LLMs using Heterogeneous Feedback") includes a high-level
    overview. Our framework utilizes the simplest supervision, such as binary preference,
    and projects all remaining datasets into that format. Because some data may be
    redundant in the unified dataset, we filter for quality and diversity to generate
    $D_{train}$. For simplicity, we use this dataset for both the SFT and RLHF steps
    of fine-tuning, however this is not a requirement. This generates an LLM fine-tuned
    with high-quality and diverse data, LLaMA-HD.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Primary fine-tuning dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a dataset $\mathcal{D}$ of prompts with two responses using binary preference,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D}=\{(P_{i},A_{i,0},A_{i,1}\}_{i=1}^{M}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $P$ is the prompt, $A_{i,0}$ and $A_{i,1}$ are answers to the prompt,
    with $A_{i,0}$ defined as the preferred response to the prompt. This type of dataset
    takes the form of binary preference due to two example responses to a single prompt.
    Examples here do not convey a sense of quality, thus prohibiting ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Secondary fine-tuning dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given a dataset $\mathcal{D^{*}}$ of user-specific prompts and responses (question-answer
    tuples):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D^{*}}=\{(P_{i},A_{i},{\bf y}_{i})\}_{i=1}^{N}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $P_{i}$ and $A_{i}$ are the $i$th prompt and response pair, respectively,
    and ${\bf y}_{i}\in\mathbb{R}^{k}$ is the real-valued vector denoting the score
    of various labels for that pair. For a dataset of this type to be compatible with
    our method, it is necessary that there are multiple responses to the same prompt.
    For example,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(P_{i},A_{i^{\prime}},{\bf y}_{i^{\prime}})\in D^{*}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: can be the second response to the prompt. This process can be repeated for arbitrarily
    many datasets. A general method for one axis of supervision is included in Appendix [A.2](#A1.SS2
    "A.2 Converting Supervision ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning
    LLMs using Heterogeneous Feedback").
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Simple Unionization for Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We take $\mathcal{D^{*}}$ and create a dictionary with prompt as key and responses
    as a list of all responses to that prompt. This requires at least two responses
    for each prompt to be considered. We can conduct quality and diversity filtering
    on these prompts, and then select the preferred response pairs. Once we have tuples
    containing a prompt, preferred, and non-preferred response, our data from $D^{*}$
    are now in the same format as $D$, so we take the union.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Quality Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We infer example quality based on the numerical labels of responses. For datasets
    with multiple numerical labels, selection of the label is a hyperparameter likely
    motivated by the purpose of fine-tuning. For example, our experiment uses toxicity
    as this is related to our objective of reducing bias. For prompts with more than
    two responses, the highest quality pair of responses are those that vary most
    in the numerical label. Intuitively, these should give a strong signal to a reward
    model because one response is strongly preferred. Finally, we can rank and select
    prompts by the preference difference of their responses.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Diversity Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We select for prompt diversity by generating embeddings for each prompt, followed
    by clustering. Prompts with similar meaning can be considered redundant. We follow
    OpenPlatypus in using a sentence transformer to generate semantic embeddings to
    filter datasets Lee et al. ([2024](#bib.bib7)). Embeddings are then clustered
    using unsupervised methods like k-means. We select the top fraction of responses
    from each cluster. Both the number of clusters and fraction of each cluster are
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the training pipeline from StackLLaMA Beeching et al. ([2023](#bib.bib1)),
    which uses LLaMA-7B Touvron et al. ([2023](#bib.bib16)). First, we perform SFT
    on the base model. We then train a reward model using the fine-tuned model. This
    is followed by RLHF on the fine-tuned model using PPO. Low-Rank Adaptation is
    used to reduce memory usage and increase parallelization Hu et al. ([2021](#bib.bib4)).
    We select varying fractions of the training dataset, as well as omit filtering,
    to measure its influence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Quantitative Results. Bolded entries denote highest performance. -S
    indicates model was fine-tuned with SFT only, -R is SFT followed by RLHF. No filtering
    was performed on fine-tuning datasets for LLaMA-S and LLaMA-R.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Bias $\downarrow$ | Bias (Entropy) $\downarrow$ | Bias (Cluster)
    $\downarrow$ | Accuracy $\uparrow$ | Similarity $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-Base | 0.4585 | 0.0010 | 3.0393 | 0.9482 | 0.9482 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-S | 1.1721 | 0.1553 | 10.3180 | 0.5953 | 0.6553 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-R | 0.9247 | 0.0098 | 4.2139 | 0.9457 | 0.9457 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-0.2-S | 0.4436 | 0.0580 | 4.5856 | 0.9204 | 0.9204 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-0.4-S | 0.7798 | 0.0548 | 6.3741 | 0.8788 | 0.9394 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-0.6-S | 0.7947 | 0.0407 | 7.5564 | 0.8327 | 0.8927 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-1.0-S | 0.4117 | 0.0333 | 3.8851 | 0.9533 | 0.9533 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-0.2-R | 0.4330 | 0.0580 | 3.0892 | 0.9482 | 0.9482 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-0.4-R | 0.4287 | 0.0548 | 2.9852 | 0.9602 | 0.9508 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-0.6-R | 0.4727 | 0.0010 | 2.9472 | 0.9646 | 0.9571 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-HD-1.0-R | 0.3629 | 0.0068 | 3.1570 | 0.9583 | 0.9583 |'
  prefs: []
  type: TYPE_TB
- en: 3 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Heterogeneous Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use three datasets for our experiments: WinoGrande Sakaguchi et al. ([2019](#bib.bib13))
    (our primary dataset), OpenAssistant OASST Köpf et al. ([2023](#bib.bib6)) (our
    secondary dataset), and WinoGender Zhao et al. ([2018](#bib.bib19)) for testing
    the generalization of our method. WinoGrande is a coreference resolution dataset
    developed as a more challenging alternative to the Winograd Schema Challenge Levesque
    et al. ([2012](#bib.bib8)), as machine learning models exceeded 90% accuracy on
    the dataset. WinoGrande has been found to transfer to other wino-style schema
    challenges, including WinoGender. OASST is a conversation dataset consisting of
    over 10,000 conversation trees. This dataset has numerical supervision, providing
    an inherent measure of quality. WinoGender is a dataset testing gender bias in
    coreference resolution that involve a pair of sentences, one conforming to American
    gender biases and one against them. Differences in response indicate gender bias.'
  prefs: []
  type: TYPE_NORMAL
- en: We fine-tune using either WinoGrande alone with LLaMA-SFT and LLaMA-RLHF, or
    with a combination of WinoGrande and OASST using our framework. We fine-tune using
    several subsets of the data, in addition to the dataset without filtering. For
    SFT and training the reward model, we treat the pro-bias examples of WinoGender
    as negative and the anti-bias examples as positive. This follows from the intuition
    that language models learn human biases, so reductions in bias can be achieved
    by training models in the opposite direction.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Dataset Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the numerical score toxicity in OASST to measure prompt quality. Prompts
    are ranked based on the difference in toxicity between responses. By reducing
    toxicity, we may also be able to reduce gender bias. For prompts with more than
    two responses, we consider the largest difference. As WinoGrande does not have
    ordered scoring, these prompts are not filtered. The number following model name
    indicates the fraction of the dataset used, with 1.0 indicating no filtering was
    performed. Experimental details about quality and diversity selection are included
    in Appendix [A.3](#A1.SS3 "A.3 Hyperparameters ‣ Appendix A Appendix ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback").
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare our approach that learns from heterogeneous human feedback datasets
    to the following fundamental baselines: Pre-trained LLM (base), Pre-trained LLM
    with SFT using WinoGrande, and Pre-trained LLM with SFT and RLHF using WinoGrande.
    Our method uses the same heterogeneous dataset for SFT and RLHF. Eight Nvidia
    A100 GPUs are used for each step of the process. We test using LLaMA-7B, however
    our framework is naturally able to leverage any other state-of-the-art large language
    model. The hyperparameters and LoRA configuration are included in Appendix [A.3](#A1.SS3
    "A.3 Hyperparameters ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning LLMs
    using Heterogeneous Feedback").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use several metrics to measure change in gender bias, reported in Table [1](#S2.T1
    "Table 1 ‣ 2.6 Training ‣ 2 Framework ‣ A Framework for Fine-Tuning LLMs using
    Heterogeneous Feedback"). Our metrics use prompts based on the multiple choice
    format used in PaLM Chowdhery et al. ([2022](#bib.bib3)), and are included in
    Appendix [A.4](#A1.SS4 "A.4 Quantitative Prompt ‣ Appendix A Appendix ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback"). Bias takes the difference
    in log probabilities for the correct token in WinoBias for the pro-bias and anti-bias
    sentences. A model reflecting no gender bias would have a difference of 0\. Bias
    (Cluster) performs the same computation, except it considers the log probabilities
    for every word in the coreference cluster. This includes the pronoun used in the
    Bias metric, so its values are larger. Bias (Entropy) takes the relative entropy
    of the next token logits for the pro-bias and anti-bias sentences. This measures
    how different the model state is as a result of each prompt. An unbiased model
    would have a relative entropy of 0.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is computed in a generative context, where the model is asked to complete
    a sentence. Generation is stopped after 10 new tokens or a punctuation token,
    whichever is sooner. Accuracy is averaged over both the pro-bias and anti-bias
    prompts, so this is more a measure of utility. We complement this metric with
    Similarity, which uses the same generation. It is how often the result, either
    correct or incorrect, for a pair of WinoBias sentences is shared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we conduct a qualitative experiment. We ask the model to respond
    to several simple prompts. Evaluating models in a chatbot-like context gives another
    perspective on utility. The models are given single-sentence prompts such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Give me a list of top vacation destinations.\n"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Quantitative Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantitative results are reported in Table [1](#S2.T1 "Table 1 ‣ 2.6 Training
    ‣ 2 Framework ‣ A Framework for Fine-Tuning LLMs using Heterogeneous Feedback").
    Results are rounded to 4 digits after the decimal place. We find that our method
    is able to reduce bias by several metrics relative to all baselines, including
    a pre-trained model, while maintaining utility as measured by accuracy. We also
    see that using SFT and RLHF with our framework generally leads to lower bias than
    with SFT only. Based on the results for Bias (Entropy), Bias (Cluster), and Accuracy,
    we can get higher performance by filtering for data quality and diversity than
    with using the full fine-tuning dataset. We believe this result may be improved
    by examining more rigorous methods for measuring quality and diversity. The qualitative
    results also show that our framework permits the improvment on multiple measures,
    which are not necessarily correlated, simultaneously. With LLaMA-HD-0.6-R and
    LLaMA-HD-1.0-R, we achieve higher generative accuracy, a measure of utility, and
    higher generative similarity, a measure of bias, relative to the base model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Qualitative Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We find that the base and fine-tuned methods using WinoGrande consistently fail
    to follow the prompt. In many instances, the prompt is repeated indefinitely.
    With our method, we receive reasonable responses, likely as a result of our secondary
    fine-tuning dataset, OASST, including instruction-following examples. The qualitative
    task shows us that the method is able to train for multiple tasks at once, namely
    a reduction in bias and instruction following. An illustrative sample is included
    in Appendix [A.5](#A1.SS5 "A.5 Qualitative Example ‣ Appendix A Appendix ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback"). We observe that while the
    base model rarely answers the prompt, LLaMA-S does on occasion respond reasonably,
    even though it was not explicitly instruction fine-tuned. Using only 20% of the
    filtered dataset, we are able to achieve consistent instruction following (Appendix [A.5](#A1.SS5
    "A.5 Qualitative Example ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning LLMs
    using Heterogeneous Feedback")). The highest generative accuracy and lowest bias
    (entropy) was also obtained by a model using a fitlered dataset, demonstrating
    that filtering can simultaneously improve quality and reduce bias.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We find that combining datasets of heterogeneous supervision for fine-tuning
    can lead to performance increases beyond using only one dataset, even when the
    secondary dataset is less directly related to the task. We find that by varying
    the fraction of used data, we are able to achieve performance comparable to the
    full dataset, and sometimes exceed it. Most significantly, when the reduced bias
    result of the quantitative result are combined with the instruction-following
    seen in the qualitative result, we show that it is possible to fine-tune for multiple
    purposes simultaneously, even when the datasets include a different supervision
    format. Our framework can be used to improve both performance-oriented metrics,
    like instruction following, and to unwanted behavior like bias. This shows that
    it is possible to effectively fine-tune LLMs using heterogeneous supervision.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beeching et al. (2023) Edward Beeching, Younes Belkada, Kashif Rasul, Lewis
    Tunstall, Leandro von Werra, Nazneen Rajani, and Nathan Lambert. 2023. [Stackllama:
    An rl fine-tuned llama model for stack exchange question and answering](https://doi.org/10.57967/hf/0513).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kung et al. (2023) Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun
    Peng. 2023. [Active instruction tuning: Improving cross-task generalization by
    training on prompt sensitive tasks](https://arxiv.org/abs/2311.00288). *Preprint*,
    arXiv:2311.00288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Köpf et al. (2023) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver
    Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri,
    Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023.
    [Openassistant conversations – democratizing large language model alignment](https://arxiv.org/abs/2304.07327).
    *Preprint*, arXiv:2304.07327.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2024) Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2024. [Platypus:
    Quick, cheap, and powerful refinement of llms](https://arxiv.org/abs/2308.07317).
    *Preprint*, arXiv:2308.07317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levesque et al. (2012) Hector Levesque, Ernest Davis, and Leora Morgenstern.
    2012. The winograd schema challenge. In *Thirteenth international conference on
    the principles of knowledge representation and reasoning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen,
    Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024. [From quantity to
    quality: Boosting llm performance with self-guided data selection for instruction
    tuning](https://arxiv.org/abs/2308.12032). *Preprint*, arXiv:2308.12032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. [Webgpt: Browser-assisted
    question-answering with human feedback](https://arxiv.org/abs/2112.09332). *CoRR*,
    abs/2112.09332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155).
    *Preprint*, arXiv:2203.02155.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
    Sentence embeddings using siamese bert-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2019. [Winogrande: An adversarial winograd schema challenge at
    scale](https://arxiv.org/abs/1907.10641). *Preprint*, arXiv:1907.10641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. [Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347).
    *Preprint*, arXiv:1707.06347.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener and Savarese (2018) Ozan Sener and Silvio Savarese. 2018. [Active learning
    for convolutional neural networks: A core-set approach](https://arxiv.org/abs/1708.00489).
    *Preprint*, arXiv:1708.00489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971).
    *Preprint*, arXiv:2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024) Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, and
    Dianhui Chu. 2024. [A survey on data selection for llm instruction tuning](https://arxiv.org/abs/2402.05123).
    *Preprint*, arXiv:2402.05123.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023) Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and
    Chang Zhou. 2023. [Self-evolved diverse data sampling for efficient instruction
    tuning](https://arxiv.org/abs/2311.08182). *Preprint*, arXiv:2311.08182.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2018) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,
    and Kai-Wei Chang. 2018. [Gender bias in coreference resolution: Evaluation and
    debiasing methods](https://arxiv.org/abs/1804.06876). *Preprint*, arXiv:1804.06876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Heterogeneous Dataset Creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we include an example of heterogeneous dataset creation. The
    following example is from WinoGrande, our primary fine-tuning dataset. It format
    follows that of Section [2.1](#S2.SS1 "2.1 Primary fine-tuning dataset ‣ 2 Framework
    ‣ A Framework for Fine-Tuning LLMs using Heterogeneous Feedback"), where $A_{i,0}$
    is the correct response.
  prefs: []
  type: TYPE_NORMAL
- en: '$P_{i}$: ‘The box was still visible after James tried his best to manage the
    wrapper on it. James should have used the _ that is small.’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i,0}$: ‘box’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i,1}$: ‘wrapper’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The following is from OASST, our secondary fine-tuning dataset. It contains
    a varying number of responses to each prompt, as well as numerical labels. In
    the experiment, we use the score toxicity. For brevity, we only include part of
    the score for the first response, but each response has several numerical scores.
  prefs: []
  type: TYPE_NORMAL
- en: '$P_{i^{\prime}}$: ‘Which affordable GPU would you recommend to train a language
    model?’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i^{\prime},0}$: ‘It heavily depends on the size...’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i^{\prime},1}$: ‘It is difficult to say...’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i^{\prime},2}$: ...'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$y_{i,0}$: ‘toxicity’= 0.00038284, ‘spam’ = 0, ...'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To convert the data of type OASST into that of our primary dataset, we need
    to first choose two of the responses. If the prompt had only one response, it
    would be discarded. We select the most and least toxic responses assocaited with
    this prompt, and give the prompt a quality score based on that difference. For
    this example, we will consider this to be responses 0 and 1, with 1 the preferred
    response. After filtering for diversity and quality, the data entry for the unified
    dataset would have the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '$P_{i^{\prime\prime}}$: ‘Which affordable GPU would you recommend to train
    a language model?’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i^{\prime\prime},0}$: ‘It is diffucult to say...’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '$A_{i^{\prime\prime},1}$: ‘It heavily depends on the size...’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that the format is identical to that of our primary dataset, we take the
    union of the primary and secondary datasets to form the homogenized dataset. The
    same process is applied to any additional datasets. If separate datasets were
    used for SFT and RLHF, the process would be repeated.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Converting Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general, preference datasets can be supervised in one of three types, in
    order of increasing complexity: binary, ordinal, and numerical. As projecting
    a simpler supervision type can be noisy, we focus on simplifying supervision.
    In general, this is the recipe for converting a single axis of supervision:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: numerical $\rightarrow$ sort increasing or decreasing, based on application
    $\rightarrow$ ordinal
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ordinal $\rightarrow$ extract best and worst responses to prompt $\rightarrow$
    binary
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.3 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We include the hyperparameters and LoRA configuration in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quality and Diversity Selection: We use all-MiniLM-L6-v2, a sentence transformer
    designed to capture semantic information, to generate embeddings for each prompt Reimers
    and Gurevych ([2019](#bib.bib12)). The data are then separated into 10 clusters
    with 10 restarts using k-means. We select the top 20%, 40%, and 60% of prompts
    from each cluster, as well as use the full unfiltered dataset for LLaMA-HD-1.0\.
    We perform stratified random sampling to preserve the fraction of samples from
    each of the datasets used in the experiment, maintaining the importance of each
    dataset relative to the unfiltered model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SFT: Learning Rate: 1e-5, Maximum steps: 5000, Epochs: 1, Optimizer: Adamw,
    LR Scheduler: cosine, Maximum text length: 512, Batch size: 4, Grad accumulation
    steps: 1, weight decay: 0.05\. LoRA: Rank: 16, Alpha: 32, Dropout: 0.05.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reward Model: Learning rate: 2e-5, Epochs: 1, Optimizer: Adamw, LR Scheduler:
    linear, Maximum text length: 512, Batch size: 4, Gradient accumulation steps:
    1, weight decay: 0.001\. LoRA: Rank: 8, Alpha: 32, Dropout: 0.1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RLHF: Learning Rate: 1.41e-5, Maximum steps: 20000, Epochs: 4, Minimum generation
    length: 32, Maximum generation length: 128, PPO Minibatch size: 1, Batch size:
    32, Gradient accumulation steps: 4\. LoRA: Rank: 16, Alpha: 32, Dropout: 0.05.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Quantitative Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We list the prompt used for our qualitative experiment here. The format is based
    on the multiple choice one from PaLM Chowdhery et al. ([2022](#bib.bib3)). All
    quantitative metrics use this format. Details about scoring are included in Section [3.4](#S3.SS4
    "3.4 Metrics ‣ 3 Experimental Setup ‣ A Framework for Fine-Tuning LLMs using Heterogeneous
    Feedback").
  prefs: []
  type: TYPE_NORMAL
- en: '‘{sentence} "{pronoun}" refers to: ’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A.5 Qualitative Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We report LLaMA-HD-0.2-S because it uses the smallest fraction of OASST, yet
    consistently demonstrates instruction following.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt: ‘What can I do in Miami, FL in November?’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'LLaMA-SFT: ‘I’m going to the beach in the summer...’ LLaMA-HD-0.2-S: ‘ In November,
    you can enjoy the warm weather...’'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
