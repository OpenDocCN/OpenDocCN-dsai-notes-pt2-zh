- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'TrustLLM: Smelling and Reasoning Smart Contract Vulnerability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.16073](https://ar5iv.labs.arxiv.org/html/2403.16073)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Wei Ma1, Daoyuan Wu1, Yuqiang Sun1, Tianwen Wang2, Shangqing Liu1, Jian Zhang1,
    Yue Xue3, and Yang Liu1 Corresponding author: Daoyuan Wu. 1Nanyang Technological
    University, Singapore 2National University of Singapore, Singapore 3MetaTrust
    Labs, Singapore'
  prefs: []
  type: TYPE_NORMAL
- en: 'TrustLLM: Unified Fine-Tuning and LLM Agents for Smart Contract Auditing with
    Justifications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wei Ma1, Daoyuan Wu1, Yuqiang Sun1, Tianwen Wang2, Shangqing Liu1, Jian Zhang1,
    Yue Xue3, and Yang Liu1 Corresponding author: Daoyuan Wu. 1Nanyang Technological
    University, Singapore 2National University of Singapore, Singapore 3MetaTrust
    Labs, Singapore'
  prefs: []
  type: TYPE_NORMAL
- en: Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing
    with Justifications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wei Ma1, Daoyuan Wu1, Yuqiang Sun1, Tianwen Wang2, Shangqing Liu1, Jian Zhang1,
    Yue Xue3, and Yang Liu1 Corresponding author: Daoyuan Wu. 1Nanyang Technological
    University, Singapore 2National University of Singapore, Singapore 3MetaTrust
    Labs, Singapore'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Smart contracts are decentralized applications built atop blockchains like Ethereum.
    Recent research has shown that large language models (LLMs) have potential in
    auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can
    achieve only 30% precision (when both decision and justification are correct).
    This is likely because off-the-shelf LLMs were primarily pre-trained on a general
    text/code corpus and not fine-tuned on the specific domain of Solidity smart contract
    auditing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose TrustLLM, a general framework that combines fine-tuning
    and LLM-based agents for intuitive smart contract auditing with justifications.
    Specifically, TrustLLM is inspired by the observation that expert human auditors
    first perceive what could be wrong and then perform a detailed analysis of the
    code to identify the cause. As such, TrustLLM employs a two-stage fine-tuning
    approach: it first tunes a Detector model to make decisions and then tunes a Reasoner
    model to generate causes of vulnerabilities. However, fine-tuning alone faces
    challenges in accurately identifying the optimal cause of a vulnerability. Therefore,
    we introduce two LLM-based agents, the Ranker and Critic, to iteratively select
    and debate the most suitable cause of vulnerability based on the output of the
    fine-tuned Reasoner model. To evaluate TrustLLM, we collected a balanced dataset
    with 1,734 positive and 1,810 negative samples to fine-tune TrustLLM. We then
    compared it with traditional fine-tuned models (CodeBERT, GraphCodeBERT, CodeT5,
    and UnixCoder) as well as prompt learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b).
    On a dataset of 263 real smart contract vulnerabilities, TrustLLM achieves an
    F1 score of 91.21% and an accuracy of 91.11%. The causes generated by TrustLLM
    achieved a consistency of about 38% compared to the ground truth causes.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="p1.pic1" class="ltx_picture" height="78.97" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,78.97) matrix(1 0 0 -1 0 0) translate(0,2.77)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.75 11.81)"><foreignobject width="568.5" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">“One of the big skills in bug
    bounties that’s really difficult to teach is intuition. Everything I do I am following
    my intuition. It’s what looks interesting and what doesn’t look right.” — Katie
    Paxton-Fear One of the million-dollar-earning hackers [[1](#bib.bib1)].</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Smart contracts have emerged as a key application based on blockchain technology
    since the advent of Ethereum. Due to their openness, transparency, and irreversibility,
    smart contracts have become the foundation of decentralized financial applications
    (DeFi). However, since DeFi manages a significant amount of digital assets, identifying
    and fixing vulnerabilities in smart contracts is crucial. Currently, the real
    vulnerabilities exploited by hackers in smart contracts are mainly due to logical
    flaws [[2](#bib.bib2)], which render traditional pattern-based program analysis [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10)] less effective. According to Defillama Hacks [[11](#bib.bib11)],
    vulnerability attacks have caused losses of around $7.69 billion as of March 2024.
    Hence, there is an urgent need for innovative methods to combat these emerging
    threats.
  prefs: []
  type: TYPE_NORMAL
- en: Recent research [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]
    has shown that large language models (LLMs) have potential in auditing smart contracts,
    especially in demonstrating superior performance in detecting logic vulnerabilities [[2](#bib.bib2),
    [13](#bib.bib13)]. However, a recent systematic evaluation study [[15](#bib.bib15)]
    shows that even when equipping the LLM-based vulnerability detection paradigm
    with a state-of-the-art approach, namely enhancing GPT-4 with summarized vulnerability
    knowledge in a Retrieval Augmented Generation (RAG) [[16](#bib.bib16)] fashion,
    it still achieves only $\sim$30% precision when both the decision (i.e., whether
    the subject code is vulnerable) and justification (i.e., pinpointing the correct
    vulnerability type) are correct. This can be attributed to the fact that off-the-shelf
    LLMs (e.g., GPT-4), which were primarily pre-trained on a general text/code corpus,
    were not fine-tuned for the specific domain of Solidity¹¹1Solidity is a mainstream
    language for smart contract development. smart contract auditing.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning [[17](#bib.bib17), [18](#bib.bib18)] could be a promising approach
    to embed Solidity-specific vulnerability data into the model itself, compared
    to RAG [[19](#bib.bib19)], and thus address the problem mentioned above. In particular,
    by fine-tuning an LLM with vulnerable and non-vulnerable code, it could effectively
    perceive whether a new piece of code is vulnerable or not. According to insights
    from a million-dollar-earning hacker mentioned in the prologue, such intuition
    is quite important for vulnerability auditing. As such, instead of fine-tuning
    a single model to generate both vulnerability decisions (i.e., Yes or No) and
    the causes of vulnerabilities (i.e., the type or reason) simultaneously, we propose
    a novel two-stage fine-tuning approach. This approach first tunes a Detector model
    to make decisions only, and then tunes a Reasoner model to generate the causes
    of vulnerabilities. In this way, the fine-tuned LLMs could could mimic human hackers
    by first making intuitive judgments and then performing follow-up analysis of
    the code to identify the reasons for vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We implement this “perception-then-analysis” fune-tuning into a general framework
    called TrustLLM for intuitive smart contract auditing. In this implementation,
    TrustLLM allows Detector to make multiple intuitive judgments, each representing
    one perception. To achieve this, TrustLLM generates multiple variant prompts for
    the same vulnerability label to tune Detector and similarly employs multiple variant
    prompts for the same vulnerability reason to tune Reasoner. While it is possible
    to determine the optimal decision based on majority voting, fine-tuning alone
    cannot identify the optimal cause for a vulnerability during the inference phase.
    To address this new problem, we introduce the concept of LLM-based agents to the
    paradigm of fine-tuning in TrustLLM. Specifically, we introduce two dedicated
    LLM-based agents, the Ranker and Critic agents, to iteratively select and debate
    the most appropriate cause of vulnerability based on the output of the fine-tuned
    Reasoner model.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain high-quality data for training and testing TrustLLM, we propose leveraging
    reputable auditing reports to collect positive samples and employing our own data
    enhancement method to derive negative samples. Eventually, we collected a balanced
    dataset consisting of 1,734 positive samples, i.e., vulnerable functions with
    reasons from 263 smart contract auditing reports, and 1,810 negative samples,
    i.e., non-vulnerable benign code. We then compared TrustLLM with traditional full-model
    fine-tuning methods, including CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder,
    as well as with prompt learning-based LLMs, such as GPT-4/GPT-3.5 and CodeLlama-13b/34b.
    Our experimental results show that TrustLLM achieved an F1 score of 91.21%, significantly
    outperforming prompt learning-based LLMs (which are in the range of 60%+) and
    also notably beating other fine-tuned models (which are in the range of 80%+)
    that used the same training data as ours. Furthermore, in terms of alignment with
    ground-truth explanations, TrustLLM’s output is clearly superior to that of other
    models, reaching a consistency rate of 37.99%. In contrast, the second-ranked
    GPT-4 achieves only 24%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the evaluation results, we also conducted three ablation studies to
    further justify TrustLLM’s two-stage fine-tuning and majority voting strategies,
    as well as to measure the impact of additional call graph information on the model’s
    performance. We summarize the key findings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TrustLLM’s two-stage approach achieved better detection performance than the
    integration model, which outputs labels and reasons simultaneously. We also experimentally
    confirmed that the model struggles to focus on the labels when required to output
    both types of information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majority voting enhances the detection performance and stability. Using multiple
    prompts also allows the model to perform better than when using a single prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call graph information may enable the model to make better judgments in some
    cases, but we also observed situations where this additional information could
    potentially confuse the model, thereby reducing its performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Roadmap. The rest of this paper is organized as follows. We first introduce
    the relevant background in §[II](#S2 "II Background ‣ TrustLLM: Smelling and Reasoning
    Smart Contract Vulnerability"), followed by the design of TrustLLM in §[III](#S3
    "III Design of TrustLLM ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability").
    We then present our experimental setup and the results in §[IV](#S4 "IV Evaluation
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"). After that,
    we discuss related work and the limitations in §[V](#S5 "V Related Work ‣ TrustLLM:
    Smelling and Reasoning Smart Contract Vulnerability") and §[VI](#S6 "VI Threats
    to Validity ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"),
    respectively. Finally, §[VII](#S7 "VII Conclusion ‣ TrustLLM: Smelling and Reasoning
    Smart Contract Vulnerability") concludes this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Availability. To facilitate future research and comparison, we have made the
    inference code and dataset available at [[20](#bib.bib20)].
  prefs: []
  type: TYPE_NORMAL
- en: II Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Pre-trained Models and Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-trained models are models that have been initially trained on large datasets.
    These models can be quickly adapted to various specific tasks with minimal adjustments,
    avoiding the complex training process from scratch. Currently, most pre-trained
    models adopt an architecture based on transformers [[21](#bib.bib21)]. The innovation
    of this approach is that pre-trained models leverage large data and well-designed
    tasks for effective feature learning, which has been proven effective in multiple
    fields, such as text processing, image recognition, and software engineering.
    The standard transformer structure consists of one encoder and one decoder, which
    are structurally similar but function differently. Pre-trained models can be classified
    into encoder-based, decoder-based, or encoder-decoder combined types depending
    on the transformer structure used. For example, encoder-based models are represented
    by BERT [[22](#bib.bib22)] and CodeBERT [[23](#bib.bib23)], decoder models by
    the GPT series [[24](#bib.bib24), [25](#bib.bib25)], and encoder-decoder models
    by BART [[26](#bib.bib26)], T5 [[27](#bib.bib27)], and CodeT5 [[28](#bib.bib28)].
    Compared with general pre-trained models, Large Language Models (LLMs) [[29](#bib.bib29),
    [30](#bib.bib30)] differ significantly in their used larger data and model scales.
    These models are trained by learning world-wide knowledge bases, typically reaching
    billions in scale. As the model size, data volume, and computational capacity
    increase, performance also improves, as revealed by the Scaling Laws [[31](#bib.bib31)].
    Closed-source LLMs like GPT-3.5, GPT-4, and Gemini [[32](#bib.bib32)] offer their
    services externally through APIs, while open-source models like Llama2 [[33](#bib.bib33)]
    can achieve performance comparable or better to closed-source models after fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Parameter-Efficient Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs have extremely large parameters. Fully fine-tuning a large language model
    requires significant hardware resources and is very costly. Therefore, lightweight
    parameter fine-tuning [[34](#bib.bib34), [35](#bib.bib35)] is currently the main
    method of using LLMs compared to fully fine-tuning them. Although LLMs can be
    used without task-specific fine-tuning through in-context learning [[36](#bib.bib36)],
    this usually requires carefully prepared prompts. Furthermore, research has found
    that partial fine-tuning of LLMs with smaller parameters can achieve or even surpass
    the effects of huge models [[37](#bib.bib37), [38](#bib.bib38)]. These fine-tuning
    methods differ from full-model fine-tuning by focusing only on fine-tuning additional
    parameters while keeping the large model weights fixed, known collectively as
    parameter-efficient fine-tuning [[34](#bib.bib34), [35](#bib.bib35)]. They can
    be generally categorized into four types: Adapter [[37](#bib.bib37)], Low-Rank
    Adaptation (LoRA) [[39](#bib.bib39)], prefix tuning [[40](#bib.bib40)], and prompt
    tuning [[41](#bib.bib41)].'
  prefs: []
  type: TYPE_NORMAL
- en: Adapter [[37](#bib.bib37)] adds a lightweight additional module to each layer
    of the model to capture information specific to downstream tasks. During optimization,
    only the parameters of the additional module are optimized. Since the number of
    parameters in the Adapter is much smaller than that of the model itself, it significantly
    reduces the overall parameter count and computational complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Adaptation (LoRA) [[39](#bib.bib39)] is a parameter-efficient adaptation
    method for LLMs, which adjusts LLMs for downstream tasks at a lower parameter
    cost. The core idea of LoRA is to introduce additional, low-rank adaptation parameters
    into the self-attention mechanism, effectively adjusting the model to suit new
    tasks with minimal addition of extra parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix tuning [[40](#bib.bib40)] adds a “prefix” sequence to each layer of the
    model, serving as additional context input. This method allows the model to adapt
    to specific tasks while retaining most of the knowledge acquired during pre-training.
    Unlike prefix tuning, prompt tuning [[41](#bib.bib41)] adds prompt tokens to the
    input, which can be placed at any position.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, using adapters can increase inference latency [[39](#bib.bib39),
    [42](#bib.bib42)]. Prefix or prompt tuning is subject to structural constraints
    that inhibit the learning of new attention patterns [[43](#bib.bib43)]. LoRA is
    an efficient method with low cost and can have a performance close to the full
    fine-tuning approach [[39](#bib.bib39)].
  prefs: []
  type: TYPE_NORMAL
- en: II-C Smart Contracts and Their Vulnerabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Smart contracts are essential for realizing decentralized finance [[44](#bib.bib44)]
    as an application layer of blockchain technology. According to data from DeFiLlama [[45](#bib.bib45)],
    as of March 2024, the total value locked in the top three blockchain platforms
    (Ethereum, Tron, and BSC) has reached $73 billions. Given the close relationship
    between smart contracts and economic interests, their security has attracted widespread
    attention. Vulnerabilities in smart contracts can lead to significant losses,
    such reentrancy attacks and access-control attacks [[46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, hackers employ even more complex tactics. Currently, to address
    vulnerabilities in smart contracts, various static and dynamic detection tools [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10)] are used to test contract security. Unfortunately,
    some complex vulnerabilities are hard to be found by these detection tools. For
    example, in a sandwich attack [[47](#bib.bib47)], attackers monitor other pending
    transactions and execute their transactions first upon spotting a high-value yet
    uncompleted transaction. Due to this preemptive action, the attack transaction
    will be executed at a higher price, allowing the attacker to immediately sell
    the acquired excess profit for profit. Many well-funded project teams also invite
    third parties to audit their smart contracts before public release to ensure their
    safety.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e33450b9ba21d958b50e17e006caebc6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of TrustLLM, featuring its four roles: Detector, Reasoner,
    Ranker, and Critic.'
  prefs: []
  type: TYPE_NORMAL
- en: III Design of TrustLLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As motivated in §[I](#S1 "I Introduction ‣ TrustLLM: Smelling and Reasoning
    Smart Contract Vulnerability"), TrustLLM employs a novel two-stage fine-tuning
    approach and combines it with LLM-based agents for intuitive smart contract auditing
    with justifications. As shown in Fig. [1](#S2.F1 "Figure 1 ‣ II-C Smart Contracts
    and Their Vulnerabilities ‣ II Background ‣ TrustLLM: Smelling and Reasoning Smart
    Contract Vulnerability"), TrustLLM has the following four roles:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detector is the key component for achieving intuitive smart contract auditing.
    By fine-tuning an LLM with vulnerable and non-vulnerable code, Detector can discern
    whether a piece of code is vulnerable, much like how a human hacker perceives
    a potential vulnerability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasoner takes Detector’s initial vulnerability perception to further investigate
    problematic code locations. By connecting Detector’s output with Reasoner’s reasoning
    during both training and inference, TrustLLM achieves two-stage fine-tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To identify the optimal cause of a vulnerability during the inference phase,
    we further introduce the concept of LLM-based agents into the fine-tuning paradigm
    in TrustLLM. Specifically, Ranker evaluates the reasons for each potential vulnerability,
    selecting a top explanation, while Critic further assesses Ranker’s output to
    debate and determine the most appropriate cause of the vulnerability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenges. While TrustLLM’s four roles in Fig. [1](#S2.F1 "Figure 1 ‣ II-C
    Smart Contracts and Their Vulnerabilities ‣ II Background ‣ TrustLLM: Smelling
    and Reasoning Smart Contract Vulnerability") are intuitive, training and coordinating
    them well for effective smart contract auditing with reasonable justifications
    is difficult. More specifically, we encountered the following four challenges
    during the design and implementation of TrustLLM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'C1:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to collect and derive high-quality training data? For a fine-tuned model
    like TrustLLM, obtaining high-quality training data is always crucial. We propose
    leveraging reputable auditing reports to collect positive samples and employing
    our own data enhancement method to derive negative samples. Since this part is
    independent of TrustLLM’s design, we defer its presentation to the end of this
    section in §[III-D](#S3.SS4 "III-D High-quality Training Data Collection and Enhancement
    ‣ III Design of TrustLLM ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability").'
  prefs: []
  type: TYPE_NORMAL
- en: 'C2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to make effective vulnerability judgements? While fine-tuning a model with
    vulnerable and non-vulnerable code is straightforward, tuning it to be effective
    with limited data presents a challenge. We make an effort towards addressing this
    problem in §[III-A](#S3.SS1 "III-A Using Multi-prompt Tuning and Majority Voting
    for Effective Vulnerability Judgements in the Detector ‣ III Design of TrustLLM
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability") by opting to
    use multiple prompts for fine-tuning rather than a single prompt. The advantages
    of this approach are twofold: (i) it enriches the training dataset by increasing
    the volume of data, and (ii) it diminishes the bias associated with a single prompt,
    thereby enhancing the reliability of the results [[48](#bib.bib48)]. Optimal vulnerability
    perception could thus be achieved through majority voting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'C3:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to effectively connect Detector’s vulnerability sensing with Reasoner’s
    vulnerability reasoning? The fine-tuning of TrustLLM is unique because it employs
    a two-stage fine-tuning approach with the Detector and Reasoner models. Therefore,
    how to effectively connect these two models becomes a new issue not encountered
    in traditional fine-tuning. We present this aspect of TrustLLM’s design in §[III-B](#S3.SS2
    "III-B Connecting Detector for Reasoner’s Tuning & Inference ‣ III Design of TrustLLM
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability").'
  prefs: []
  type: TYPE_NORMAL
- en: 'C4:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to obtain the optimal vulnerability cause from Reasoner’s output? Since
    Reasoner also employs multi-prompt fine-tuning, it is necessary to identify the
    optimal cause of vulnerability among the multiple causes output by Reasoner. We
    introduce two LLM-based agents, namely the Ranker and Critic components, in §[III-C](#S3.SS3
    "III-C Ranking and Debating the Optimal Vulnerability Cause ‣ III Design of TrustLLM
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"), to iteratively
    select and debate the most appropriate cause of vulnerability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An Example of Workflow. To wrap up, Fig. [1](#S2.F1 "Figure 1 ‣ II-C Smart
    Contracts and Their Vulnerabilities ‣ II Background ‣ TrustLLM: Smelling and Reasoning
    Smart Contract Vulnerability") also illustrates an example of TrustLLM’s workflow.
    Initially, Detector perceives code vulnerabilities using five different inference
    paths (prompts). The perceived results are then subjected to majority voting to
    determine a consensus label. Based on the voting result, Reasoner interprets this
    outcome according to different inference paths, resulting in ten answers (each
    considering the context of the code location or not). Next, Ranker selects Reason
    1 with a confidence score 9/10 and explains this choice. Critic challenges this
    choice and advises Ranker to re-evaluate. Taking Critic’s feedback into account,
    Ranker re-ranks the ten reasons and selects Reason 3 with a confidence score of
    10/10. Critic reviews Ranker’s choice again and agrees with this decision. The
    loop is completed, and the final reason is returned to the user.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Using Multi-prompt Tuning and Majority Voting for Effective Vulnerability
    Judgements in the Detector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Detector is a fine-tuned expert model responsible for assessing whether code
    poses any risk. It mimics human intuitive judgment upon seeing a piece of code,
    assessing whether there are any issues. We employed LoRA [[39](#bib.bib39)] to
    fine-tune CodeLlama-13b [[49](#bib.bib49)] in the instruction manner [[50](#bib.bib50)]
    based on a high quality of dataset. During training, for the same input code,
    we wrap it with multiple prompts. These prompts, with different instruction formats,
    represent the different inference paths, as illustrated in Fig. [1](#S2.F1 "Figure
    1 ‣ II-C Smart Contracts and Their Vulnerabilities ‣ II Background ‣ TrustLLM:
    Smelling and Reasoning Smart Contract Vulnerability"). In the inference phase
    of Detector, based on the output results of each prompt, we adopt a majority voting
    approach to determine the input label and use the voting ratio as the confidence
    score. Based on Detector’s majority voting result, Reasoner in §[III-B](#S3.SS2
    "III-B Connecting Detector for Reasoner’s Tuning & Inference ‣ III Design of TrustLLM
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability") then generates
    different reasons according to different inference paths.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S3.F2.1.1.pic1" class="ltx_picture" height="229.68" overflow="visible"
    version="1.1" width="240"><g transform="translate(0,229.68) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 211.48)"><foreignobject width="196.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Detector’s
    Prompt Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 49.85)"><foreignobject width="196.69" height="143.9" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Below is an instruction that
    describes a classification task. [Task Description] ### Instruction: [Task Instruction]
    ### Input: [Input Description]:'
  prefs: []
  type: TYPE_NORMAL
- en: “‘Solidiy
  prefs: []
  type: TYPE_NORMAL
- en: '{code}'
  prefs: []
  type: TYPE_NORMAL
- en: ”’
  prefs: []
  type: TYPE_NORMAL
- en: Response:</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject width="196.69" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">The label is {Label Name}.</foreignobject></g></g></svg>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure 2: The Prompt Template Used by Detector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt template used by Detector is demonstrated in Fig. [2](#S3.F2 "Figure
    2 ‣ III-A Using Multi-prompt Tuning and Majority Voting for Effective Vulnerability
    Judgements in the Detector ‣ III Design of TrustLLM ‣ TrustLLM: Smelling and Reasoning
    Smart Contract Vulnerability"). Above the dashed line is the input $x$, with “{Label
    Name}” being the label placeholder, which can be either “safe” and “vulnerable.”
    The left table (Detector’s Multiple Prompts) in Fig. [4](#S3.F4 "Figure 4 ‣ III-B
    Connecting Detector for Reasoner’s Tuning & Inference ‣ III Design of TrustLLM
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability") details the
    [Task Description], [Task Instruction], and [Input Description], listed as notations
    a, b, and c, respectively. We fine-tune CodeLlama-13b using LoRA in a generative
    manner, as shown in Eq. [1](#S3.E1 "In III-A Using Multi-prompt Tuning and Majority
    Voting for Effective Vulnerability Judgements in the Detector ‣ III Design of
    TrustLLM ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"),'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L(\theta)=-\sum_{t=1}^{T}\log P_{\theta}(y_{t}&#124;x,y_{<t})$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\theta$ generating a token $y_{t}$ is conditioned only on the previous
    time steps ($<t$).
  prefs: []
  type: TYPE_NORMAL
- en: 'After fine-tuning, during inference, the proposed input follows the training
    format, and we need to extract labels from the output via keyword matching, using
    “safe” and “vulnerable.” Since we employ multiple prompts, we obtain multiple
    label predictions and use majority voting to decide the final predicted label.
    In majority voting, each inference path casts a vote for one of the available
    labels, “safe” or “vulnerable,” denoted as $l_{0}$ represent the set of the prompt
    voter. Each prompt voter $v_{i}$ for which the following condition holds: <math
    id="S3.SS1.p3.9.m9.2" class="ltx_Math" alttext="|\{v\in V:\text{vote}(v)=l_{i}\}|></math>.
    We also use the voting ratio of the winning label as the confidence score for
    the final decision.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Connecting Detector for Reasoner’s Tuning & Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reasoner is an expert model responsible for reasoning about and explaining
    code vulnerabilities. It interprets the majority voting result of the Detector,
    generating multiple alternative explanations. During the Reasoner’s LoRA fine-tuning
    process, our inputs include the code, its context, corresponding labels, and we
    construct zero-shot chain-of-thought (CoT) [[51](#bib.bib51)] prompts with different
    command formats for training. In the inference phase, Reasoner outputs multiple
    explanations based on the majority-voted label from Detector. We constructed two
    types of prompts: the first type includes the label, code information, and its
    function call relationships; the second type includes only the label and code
    information. In §[IV](#S4 "IV Evaluation ‣ TrustLLM: Smelling and Reasoning Smart
    Contract Vulnerability"), we will investigate the impact of including function
    call relationships or not. For each type, we designed five different instruction
    formats for the prompts, totaling ten inference paths, as illustrated in Fig.
    [1](#S2.F1 "Figure 1 ‣ II-C Smart Contracts and Their Vulnerabilities ‣ II Background
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability").'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S3.F3.1.1.pic1" class="ltx_picture" height="365.28" overflow="visible"
    version="1.1" width="240"><g transform="translate(0,365.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 347.08)"><foreignobject width="196.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Reasoner’s
    Prompt Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 51.24)"><foreignobject width="196.69" height="278.12" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Below is an instruction that
    describes a reasoning task. [Task Description] ### Instruction: [Task Instruction]
    ### Input: [Input Description]:'
  prefs: []
  type: TYPE_NORMAL
- en: “‘Solidiy
  prefs: []
  type: TYPE_NORMAL
- en: '{code}'
  prefs: []
  type: TYPE_NORMAL
- en: “‘
  prefs: []
  type: TYPE_NORMAL
- en: 'As a Caller: (Optional) [Caller Description] “‘'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '{caller info}'
  prefs: []
  type: TYPE_NORMAL
- en: “‘
  prefs: []
  type: TYPE_NORMAL
- en: 'As a Callee: (Optional) [Callee Description] “‘'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '{callee info}'
  prefs: []
  type: TYPE_NORMAL
- en: “‘
  prefs: []
  type: TYPE_NORMAL
- en: 'Response: [Label Information + Zero-shot-CoT Tip]</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="196.69"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">{Target
    Reason}</foreignobject></g></g></svg>'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure 3: The Prompt Template Used by Reasoner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt template used by the Reasoner is shown in Fig. [3](#S3.F3 "Figure
    3 ‣ III-B Connecting Detector for Reasoner’s Tuning & Inference ‣ III Design of
    TrustLLM ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"). “code”,
    “caller info”, “callee info”, and “Target Reason” are placeholders for the input
    code, caller context, callee context, and the target output, respectively. The
    right table (Reasoner’s Multiple Prompts) in Fig. [4](#S3.F4 "Figure 4 ‣ III-B
    Connecting Detector for Reasoner’s Tuning & Inference ‣ III Design of TrustLLM
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability") details the
    first prompt type with calling context, including [Task Description] denoted as
    a, [Task Instruction] denoted as b, [Input Description] denoted as c, [Caller
    Description] denoted as d, [Callee Description] denoted as e, and [Label Information
    + Zero-shot-CoT Tip] denoted as f. For the second prompt type, the calling context
    is omitted. Reasoner employed the same fine-tuning method as Detector, as shown
    by Eq. [1](#S3.E1 "In III-A Using Multi-prompt Tuning and Majority Voting for
    Effective Vulnerability Judgements in the Detector ‣ III Design of TrustLLM ‣
    TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"). During inference,
    the proposed input prompt follows the training format, and Reasoner generates
    ten answers to interpret Detector’s assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/daef5834d6b98e2e60c7e0e886060833.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Detailed Multiple Prompts for Detector and Reasoner.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Ranking and Debating the Optimal Vulnerability Cause
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ranker and Critic are two LLM-based agents collaborating to select the most
    appropriate cause of vulnerability from multiple explanations returned by Reasoner
    for a given code function. Ranker performs two actions: “rank” and “merge”. “Rank”
    involves selecting the best explanation from the ones provided, while “merge”
    involves integrating multiple selected explanations. We define 10 constraints
    for Ranker to select the top explanation. Critic evaluates Ranker’s answer in
    conjunction with the code function, providing three next-step action instructions:
    “agree”, “rerank”, and “merge”. “Agree” means the current answer is reasonable
    and can be returned to the user. “Rerank” indicates that Ranker needs to re-select,
    considering Critic’s feedback and previous answers. “Merge” suggests that the
    top reasons provided must be integrated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, Ranker employs the following 10 constraints in the prompt
    as its selection criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If one reason describes code that does not exist in the provided input, it is
    not valid.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If one reason is not related to the code, the reason is not valid.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If this reason violates the facts, the reason is unreasonable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If one reason is not related to the decision, the reason is not valid.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If one reason assume any information that is not provided, the reason is not
    valid.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the code is safe and one reason supports the decision, please check if the
    code has other potential vulnerabilities. If the code has other potential vulnerabilities,
    the reason is not valid.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The selected reason should be the most relevant to the decision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The selected reason must be the most reasonable and accurate one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The selected reason must be factual, logical and convincing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do not make any assumption out of the given code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both Ranker and Critic are LLMs agents implemented based on the Mixtral 8x7B-Instruct [[52](#bib.bib52)]
    model, the capability of which is close to that of larger LLMs [[52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54)]. Moreover, we have observed that the Mixture
    of Experts (MoE) [[52](#bib.bib52)] model can more effectively output data in
    the predetermined format than other models, making it easier for us to handle
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: III-D High-quality Training Data Collection and Enhancement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The quality of training data is crucial for fine-tuning LLMs. To collect positive
    samples, namely risky vulnerability code, we can employ auditing reports from
    reputable industry companies, such as Trail of Bits, Code4rena, and Immunefi.
    Specifically, we crawled and parsed 1,734 vulnerable functions with reasons from
    263 smart contract auditing reports, which were assembled by a popular auditing
    website called Solodit [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: However, to train our model, we also need non-vulnerable benign code (i.e.,
    negative samples), but this type of data is missing in the audit reports. Therefore,
    we propose our own data enhancement method to derive high-quality negative samples.
    Specifically, we adopt the GPT-4-based approach described in LLM4Vuln [[15](#bib.bib15)]
    to extract vulnerability knowledge from vulnerability reports on Code4rena. This
    includes the functionality descriptions of vulnerable functions and the code-level
    reasons why the vulnerabilities occur. We then cluster this raw vulnerability
    knowledge based on the functionality descriptions into groups and use GPT-4 to
    summarize a functionality description for each group. With the hierarchical information
    of group functionality, individual functionality, and vulnerability negligence,
    we employ the hierarchical GPT-based matching (i.e., matching the group first,
    then matching functionality and negligence) in GPTScan [[13](#bib.bib13)] to obtain
    the label information for tested code. A function is labeled as a negative sample
    if no vulnerability information matches. All prompts used are from LLM4Vuln and
    GPTScan.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we collected a balanced dataset with 1,734 positive samples and
    1,810 negative samples. This dataset was divided into training, validation, and
    test subsets, containing 2,268, 567, and 709 entries, respectively. During training,
    we use the training and validation sets. During testing, we use the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc30f39a05141308c8e9e9370bfa38d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Data Enhancement for Expanding Vulnerability Explanations based on
    GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After collecting the labeled and unlabeled data, we also obtained corresponding
    explanations for the vulnerabilities. However, the quality of these vulnerability
    justifications varies considerably. Furthermore, some data contain external links,
    which may cause the model to hallucinate and output non-existent links. To improve
    the interpretability of the reasons behind the vulnerabilities in the dataset,
    we used GPT-3.5 to enhance the existing explanations, expanding on the explanations
    of the vulnerabilities, proofs of concept (PoC), and recommended fixes. Fig. [5](#S3.F5
    "Figure 5 ‣ III-D High-quality Training Data Collection and Enhancement ‣ III
    Design of TrustLLM ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability")
    shows an example, where the left part presents the original reasons for the vulnerability,
    which are short and lack detail. We thus use them as prompts, along with the code
    context, to instruct GPT-3.5 to generate more detailed descriptions, including
    the PoC and the mitigation recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our study, we carefully selected a series of benchmark models, categorized
    into two groups: LLMs for zero-shot learning and pre-trained code models based
    on fine-tuning, to ensure a comprehensive and sound comparative analysis. For
    zero-shot learning LLMs, we chose CodeLlama-13b-Instruct, CodeLlama-34b-Instruct [[49](#bib.bib49)],
    GPT-3.5, and GPT-4 as benchmarks, representing the current state-of-the-art. Additionally,
    we selected CodeBERT [[23](#bib.bib23)], GraphCodeBERT [[56](#bib.bib56)], CodeT5 [[28](#bib.bib28)],
    UnixCoder [[57](#bib.bib57)], and CodeLlama-13b [[49](#bib.bib49)] to train classifiers.
    Among these, CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder underwent a complete
    model fine-tuning process to adapt to the specific code classification task. In
    particular, CodeLlama-13b employs LoRA for lightweight tuning and uses the last
    token representation for classification. Note that that our method is different;
    TrustLLM’s Detector achieves classification by generating label names as task
    outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Research Questions (RQs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since our proposed method comprises two core functions: vulnerability detection
    and reason explanation, we designed a series of experiments to evaluate and demonstrate
    the performance and effectiveness of both tasks. These experiments aim to answer
    the following research questions (RQs):'
  prefs: []
  type: TYPE_NORMAL
- en: RQ1 - Performance Comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How does the performance of TrustLLM in detecting vulnerabilities compare to
    other models? This question aims to understand how the effectiveness of Detector
    in detecting vulnerabilities compares to that of other existing models. The focus
    is on comparative analysis, involving metrics accuracy, precision, recall, and
    F1 score, to evaluate and contrast the performance.
  prefs: []
  type: TYPE_NORMAL
- en: RQ2 - Explanation Alignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To what extent do the explanations generated by TrustLLM’s Reasoner align with
    the real reasons? RQ2 concerns the quality of the explanations the Reasoner provided
    for the decision of the Detector. It questions whether the reasons given by TrustLLM
    correspond to the actual reasons behind the vulnerabilities, emphasizing the interpretability
    and trustworthiness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: RQ3 - Two-stage Approach vs. An Integration Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How does TrustLLM compare with an integration model that performs detection
    and reasoning simultaneously? Our method is based on a generative model, with
    two models trained on the generated labels and reasons, respectively. Another
    approach uses a single model to generate both reasons and labels. This question
    explores the effectiveness and impact of integrating the Detector and Reasoner
    components into one.
  prefs: []
  type: TYPE_NORMAL
- en: RQ4 - Effectiveness of Majority Voting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Can majority voting improve the effectiveness of the Detector? RQ4 investigates
    whether the effectiveness of the Detector can be improved by adopting a majority
    voting mechanism. Majority voting, a technique that makes the final decision based
    on the majority output of multiple models, may improve the robustness and accuracy
    of the method.
  prefs: []
  type: TYPE_NORMAL
- en: RQ5 - Impact of Additional Information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The call graph illustrates the interaction of code with other components within
    the project, which is expected to be advantageous for our task. We address two
    specific research sub-questions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ5.1\. Can the call graph enhance the Detector performance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ5.2\. In what way does the call graph influence our explanation generation
    process, specifically within the Reasoner-Ranker-Critic pipeline?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides the RQs above, we also used our model to audit two bounty projects (currently
    anonymous) on Code4rena. We invited audit experts to verify our findings. In the
    end, we found 6 critical vulnerabilities, which were recognized by the project
    team or audit experts. In particular, one vulnerability was not discovered by
    any tools, marked as a great finding. This demonstrates the real-world value of
    TrustLLM. Due to page limitations, we have included these case studies in the
    supplementary materials for interested readers.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C RQ1 - Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Firstly, we compared TrustLLM with LLMs based on zero-shot learning, as shown
    in Table [I](#S4.T1 "TABLE I ‣ IV-C RQ1 - Performance Comparison ‣ IV Evaluation
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"). Our method
    also uses a zero-shot approach during the inference phase. We considered two proprietary
    models (GPT-4 and GPT-3.5) and three open-source models (CodeLlama-13b and CodeLlama-34b).
    For the open-source models, we strictly adhered to their prompt formats. Huggingface
    Transformer [[58](#bib.bib58)] has integrated these prompt formats into its framework.
    The format conversion is completed by calling apply_chat_template. CodeLlama requires
    adding [INST] and [/INST] as well as special tags <<SYS>> and <</SYS>>. As shown
    in Table [I](#S4.T1 "TABLE I ‣ IV-C RQ1 - Performance Comparison ‣ IV Evaluation
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"), after fine-tuning,
    our proposed strategy significantly outperforms the baseline models in the zero-shot
    scenario in terms of F1, accuracy, and precision, achieving high scores of $0.9121$.
    We checked the confusing matrix and found that all test data are labelled by the
    vulnerability. For GPT-4 and GPT-3.5, we adopted the prompts which are provided
    by our industrial partner, a Web3 security company.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Performance comparison between TrustLLM’s Detector and zero-shot LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | F1 | Recall | Precision | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 0.6809 | 1 | 0.5162 | 0.5162 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 0.6809 | 1 | 0.5162 | 0.5162 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13b | 0.6767 | 0.9781 | 0.5173 | 0.5176 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-34b | 0.6725 | 0.9454 | 0.5219 | 0.5247 |'
  prefs: []
  type: TYPE_TB
- en: '| TrustLLM | 0.9121 | 0.8934 | 0.9316 | 0.9111 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: Performance comparison between TrustLLM’s Detector and other fine-tuned
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | F1 | Recall | Precision | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CodeBERT | 0.8221 | 0.7322 | 0.9371 | 0.8364 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphCodeBERT | 0.8841 | 0.8333 | 0.9414 | 0.8872 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5 | 0.8481 | 0.7705 | 0.9431 | 0.8575 |'
  prefs: []
  type: TYPE_TB
- en: '| UnixCoder | 0.8791 | 0.8443 | 0.9169 | 0.8801 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13b-class | 0.8936 | 0.8716 | 0.9167 | 0.8928 |'
  prefs: []
  type: TYPE_TB
- en: '| TrustLLM | 0.9121 | 0.8934 | 0.9316 | 0.9111 |'
  prefs: []
  type: TYPE_TB
- en: 'Secondly, we compared Detector with fine-tuned models in detecting vulnerabilities,
    using F1, recall, precision, and accuracy as evaluation metrics, as shown in Table [II](#S4.T2
    "TABLE II ‣ IV-C RQ1 - Performance Comparison ‣ IV Evaluation ‣ TrustLLM: Smelling
    and Reasoning Smart Contract Vulnerability"). We compared our method with CodeBERT,
    GraphCodeBERT, CodeT5, UnixCoder, and CodeLlama-13b-class. CodeBERT, GraphCodeBERT,
    CodeT5, and UnixCoder underwent full model fine-tuning. These traditional pre-trained
    models use the first token of the input sequence as the feature input for the
    classifier. CodeBERT is based on the transformer encoder. GraphCodeBERT has the
    same architecture as CodeBERT but includes additional pre-training on data dependency
    relations. CodeT5 utilizes the transformer encoder and decoder, adopting an architecture
    similar to T5 [[27](#bib.bib27)]. UnixCoder unifies the encoder and decoder architecture,
    controlling the model behaviour through a masked attention matrix with prefix
    adapters. CodeLlama-13b-class performs classification based on LoRA. We fine-tuned
    CodeLlama-13b-class for LoRA classification using the PEFT framework. CodeLlama-13b-class
    uses the representation of the last token of the input sequence as the feature
    input for the classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [II](#S4.T2 "TABLE II ‣ IV-C RQ1 - Performance Comparison
    ‣ IV Evaluation ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"),
    TrustLLM achieves the highest scores of F1, Recall, and Accuracy among all methods,
    $0.9121$. CodeLlama-13b-class is second only to our method regarding vulnerability
    detection rate, and the performance is relatively close. GraphCodeBERT and UnixCoder
    perform worse than CodeLlama-13b-class. Although CodeT5 achieves the highest precision,
    its other metrics are lower than GraphCodeBERT and UnixCoder. CodeBERT has the
    worst performance. Additionally, the accuracy scores of these models are relatively
    high (all are more than $0.91$), indicating that many of the predicted risky vulnerabilities
    are indeed risky.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS3.p4.pic1" class="ltx_picture" height="74.67" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,74.67) matrix(1 0 0 -1 0 0)
    translate(0,2.77)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.75 11.81)"><foreignobject width="568.5" height="45.51"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    for RQ1: The performance of TrustLLM’s Detector exceeds that of traditional full-model
    fine-tuning, LoRA fine-tuning in a classification manner, and LLMs based on in-context
    learning. The performance of fine-tuned models is also better than that of zero-shot
    learning.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D RQ2 - Explanation Alignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7868563ff5efd6f0889aae04867c714e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparing the alignment with ground-truth reasons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the effectiveness of Reasoner in explaining vulnerabilities, we
    compared the consistency between the explanations we generated and the root causes.
    Given the LLM’s outstanding performance in interpreting textual meaning, we used
    GPT-4 to verify whether our generated explanations align with the root causes.
    For this consistency assessment, we employed automated annotation prompts from
    Y. Sun et al. [[15](#bib.bib15)]. The results of our consistency test are depicted
    in Fig. [6](#S4.F6 "Figure 6 ‣ IV-D RQ2 - Explanation Alignment ‣ IV Evaluation
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"), where the y-axis
    represents the percentage of our explanations in the test set that match the root
    causes. Our method significantly outperformed the baseline methods, achieving
    a consistency rate of 37.99%, while no baseline method exceeded 25%. Among these
    baselines, GPT-4 performed the second best with 24.13% consistency. Additionally,
    the results also indicated that CodeLlama-13b had the weakest performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS4.p2.pic1" class="ltx_picture" height="71.97" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,71.97) matrix(1 0 0 -1 0 0)
    translate(0,2.77)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.75 11.81)"><foreignobject width="568.5" height="42.82"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    for RQ2: The rationality of Reasoner’s output is clearly superior to that of other
    models. On the test set, its consistency with real reasons reaches 37.99%, which
    is over 10% higher than the second-ranked GPT-4.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: IV-E RQ3 - Two-stage Approach vs. An Integration Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/96b86510b936bfde6c8679e8adb998dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparing TrustLLM with the integration models that make decisions
    and explain the vulnerabilities simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our research methodology involves vulnerability detection and explanation,
    executed in two stages. We trained two models, i.e., Detector and Reasoner, based
    on a generative approach to perform these tasks on their respective high-quality
    datasets. A question arises whether these two tasks can be merged and trained
    simultaneously in a single model. In response, we developed an integration model
    that generates labels and explanations for the vulnerabilities, comparing it to
    our two-stage approach. The integration model uses prompts similar to those of
    Reasoner, with additional requirements to output the label. We explored two integration
    training approaches: 1) generating labels first, then explaining the reasons;
    2) explaining the reasons first, then generating labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ IV-E RQ3 - Two-stage Approach
    vs. An Integration Model ‣ IV Evaluation ‣ TrustLLM: Smelling and Reasoning Smart
    Contract Vulnerability"), indicate that our step-wise approach outperforms the
    integration models across four key performance metrics. While the three methods
    are similar in precision, the differences in other metrics are notable. Analyzing
    these results, we found that the integration methods have higher accuracy for
    negative samples but a lower detection rate for positive samples (i.e., lower
    recall). This may be attributed to the generative loss optimization, where the
    output sequence is longer, making the label-related loss occupy a smaller proportion
    of the total loss, thus preventing the model from adequately focusing on the label.
    To test this hypothesis, we added data that includes only label generation to
    the dataset during the integration training process, guiding the model to focus
    more on the label. In the evaluation phase, we still required the model to output
    both labels and explanations simultaneously. Through this mixed training approach,
    we observed a significant improvement in the model’s vulnerability detection performance,
    with an F1 score of $0.8433$.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS5.p3.pic1" class="ltx_picture" height="91.27" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,91.27) matrix(1 0 0 -1 0 0)
    translate(0,2.77)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.75 11.81)"><foreignobject width="568.5" height="62.11"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    for RQ3: TrustLLM achieved better detection performance than the integration model
    that outputs labels and reasons simultaneously. We confirmed that the model struggles
    to focus on the labels when required to output both types of information, as evidenced
    by our inclusion of label-only data in the verification process.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Majority Voting vs. Single Prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | F1 | Recall | Precision | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Single-prompt | 0.8278 | 0.8005 | 0.8567 | 0.8279 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-1 | 0.8988 | 0.8852 | 0.9127 | 0.8970 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-2 | 0.9027 | 0.8743 | 0.9329 | 0.9027 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-3 | 0.9063 | 0.8852 | 0.9284 | 0.9055 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-4 | 0.9098 | 0.8962 | 0.9239 | 0.9083 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-5 | 0.9096 | 0.8934 | 0.9263 | 0.9083 |'
  prefs: []
  type: TYPE_TB
- en: '| TrustLLM | 0.9121 | 0.8934 | 0.9316 | 0.9111 |'
  prefs: []
  type: TYPE_TB
- en: IV-F RQ4 - Effectiveness of Majority Voting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our research explored a method using multiple prompts and a voting mechanism
    for Detector to determine the final label. This method aims to enhance the model’s
    precision and credibility. During the evaluation process, we continued to use
    metrics such as the F1 score, recall, precision, and accuracy. We calculated these
    metrics for each prompt individually for comparative analysis, as shown in Table [III](#S4.T3
    "TABLE III ‣ IV-E RQ3 - Two-stage Approach vs. An Integration Model ‣ IV Evaluation
    ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability"). It should be
    noted that the first row Single-prompt indicates that we used only one prompt
    format to train Detector. Prompt-1, Prompt-2, Prompt-3, Prompt-4, and Prompt-5
    represent the results for each prompt after multiple-prompt training. The last
    row shows the results after majority voting, indicating that majority voting can
    improve the overall performance of TrustLLM, with both the F1 score and accuracy
    being the highest. At the same time, except for Single-prompt, we noticed minimal
    performance differences among multiple prompts. Single-prompt performed much worse
    than the others. Training with multiple prompts can improve model performance
    compared to using only one prompt during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we divided the test set into two groups based on whether the
    predictions were correct or incorrect, named “correct prediction" and “incorrect
    prediction" groups, respectively, and analyzed the distribution of confidence
    scores within these two groups. We found that in the incorrect prediction group,
    the proportion of confidence scores within the range of $0.6$ is significantly
    higher than in the correct prediction group (11% vs 2%, 10% vs 3%, respectively),
    as shown in Fig. [8](#S4.F8 "Figure 8 ‣ IV-F RQ4 - Effectiveness of Majority Voting
    ‣ IV Evaluation ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability").
    The confidence score can reflect the reliability of the prediction results to
    a certain extent. When the confidence score is low, the prediction results are
    less credible.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS6.p3.pic1" class="ltx_picture" height="74.67" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,74.67) matrix(1 0 0 -1 0 0)
    translate(0,2.77)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.75 11.81)"><foreignobject width="568.5" height="45.51"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    for RQ4: Majority voting enhances the detection performance and stability. Additionally,
    using multiple prompts allows the model to perform better and be more reliable
    than when using a single prompt.</foreignobject></g></g></svg>![Refer to caption](img/7394abe3d47287314f25c68ffc90e5ce.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: The Distribution of Voting Scores for Correct Predictions and Wrong
    Predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-G RQ5 - Impact of Additional Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We explored whether introducing additional call graph information into the model
    could enhance its performance. We added function call relationships to the prompts
    as contextual information.
  prefs: []
  type: TYPE_NORMAL
- en: RQ5.1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Through comparative experiments as shown in Table [IV](#S4.T4 "TABLE IV ‣ RQ5.2
    ‣ IV-G RQ5 - Impact of Additional Information ‣ IV Evaluation ‣ TrustLLM: Smelling
    and Reasoning Smart Contract Vulnerability"), we found that this calling contextual
    information did not improve the model’s overall performance. In the second row,
    Call, we used the prompts with the calling information and then employed majority
    voting to decide the prediction result. For the third row, Call-OutCall, we used
    prompts both with and without calling information and also used majority voting.
    Compared with TrustLLM’s Detector, they exhibited lower precision, accuracy, and
    f1, with almost the same recall.'
  prefs: []
  type: TYPE_NORMAL
- en: RQ5.2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fig. [9](#S4.F9 "Figure 9 ‣ RQ5.2 ‣ IV-G RQ5 - Impact of Additional Information
    ‣ IV Evaluation ‣ TrustLLM: Smelling and Reasoning Smart Contract Vulnerability")
    demonstrates the selected reason distribution from Ranker-Critic. We can see that
    the majority (65%) of the selected reasons are from the prompts with calling information,
    while there is still a high ratio (35%) of selected reasons from the prompts without
    calling information.'
  prefs: []
  type: TYPE_NORMAL
- en: Although function call relationships provide more information, this information
    does not always help the model better complete the current task. In some cases,
    this information may cause interference, making it difficult for the model to
    identify critical information, thereby resulting in more false positives and affecting
    performance. Furthermore, not all function call relationships are practically
    valuable. If these additional pieces of information are not closely related to
    the problem the model is trying to solve, they may not help enhance the model’s
    performance. Our research indicates that merely adding function call information
    does not directly facilitate the model’s effectiveness in detecting vulnerabilities.
    In the field of vulnerability detection, exploring how to construct effective
    contextual information remains a challenging and worthy research question.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS7.SSS0.Px2.p3.pic1" class="ltx_picture" height="74.67" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,74.67) matrix(1 0 0 -1 0 0)
    translate(0,2.77)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.75 11.81)"><foreignobject width="568.5" height="45.51"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    for RQ5: Additional call graph information may enable the model to make better
    judgments in some cases. However, we also observed situations where this additional
    information could potentially confuse the model, thereby reducing its performance.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Impact with or without Additional Information.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | F1 | Recall | Precision | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Call | 0.9011 | 0.8962 | 0.9061 | 0.8984 |'
  prefs: []
  type: TYPE_TB
- en: '| Call-OutCall | 0.9083 | 0.8934 | 0.9237 | 0.9069 |'
  prefs: []
  type: TYPE_TB
- en: '| TrustLLM | 0.9121 | 0.8934 | 0.9316 | 0.9111 | ![Refer to caption](img/96e354c44aa85fd3c805f96096703961.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Final Reason Distribution of Ranker-Critic.'
  prefs: []
  type: TYPE_NORMAL
- en: V Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vulnerability detection has been critical problems for the healthy and sustainable
    development of the software ecosystem, especially on blockchain and smart contracts.
    Traditional vulnerability detection methods, such as those based on predefined
    static analysis rules [[3](#bib.bib3)], often lack good generalization capabilities
    and are difficult to extend to new types of vulnerabilities. In addition, some
    logic-related vulnerabilities [[2](#bib.bib2)] are also difficult to abstract
    into static analysis rules. Researchers have employed a deep learning-based approach
    to address this issue. For example, Zhuang et al. [[59](#bib.bib59)] used graph
    neural networks to detect vulnerabilities in smart contracts. Liu et al. [[60](#bib.bib60)]
    fused the interpretable graph features with expert patterns to achieve better
    results and interpretable weights. Wu et al. [[61](#bib.bib61)] utilized pre-training
    technique and crucial data flow graphs for the detection of smart contract vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of large language models (LLMs), researchers are utilizing not
    only traditional deep learning models but also LLMs for vulnerability detection.
    For example, Ullah et al. [[62](#bib.bib62)] evaluated LLMs on vulnerability detection
    tasks and found that they may not perform well. Fu et al. [[63](#bib.bib63)] further
    analyzed the gap for LLMs in detecting vulnerabilities. Thapa et al. [[64](#bib.bib64)]
    leveraged LLMs for software vulnerability detection, and David et al. [[12](#bib.bib12)]
    used LLMs for smart contract vulnerability tasks. Alqarni et al. [[65](#bib.bib65)]
    fine-tuned the BERT model [[66](#bib.bib66)] for source code vulnerability detection.
    Sun et al. [[15](#bib.bib15)] conducted an empirical study to find out how to
    enhance the ability of LLMs to detect vulnerabilities. In addition, Mathews et
    al. [[67](#bib.bib67)], Hu et al. [[14](#bib.bib14)], and Purba et al. [[68](#bib.bib68)]
    made efforts to utilize LLMs for detecting vulnerabilities. Some research also
    fused large language models with tradition program analysis methods. Sun et al. [[13](#bib.bib13)]
    proposed GPTScan for smart contracts, leveraging static program analysis to reduce
    the false positives of LLMs. Li et al. [[69](#bib.bib69)] proposed LLift for integrating
    LLMs with static analysis tools.
  prefs: []
  type: TYPE_NORMAL
- en: However, all these studies have not tuned domain-specific knowledge into the
    models themselves, focusing only on the knowledge from the pre-training dataset
    or the vulnerable code segment itself, which could not effectively detect logic
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: VI Threats to Validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the data collection process, there is a risk of data bias, which might prevent
    models trained and tested on these data from generalizing accurately. Moreover,
    the precision of data labelling significantly impacts model performance. To mitigate
    these issues, we collected verified data from real and public audit reports and
    utilized the latest tools, such as GPTScan [[13](#bib.bib13)] and LLM4Vuln [[15](#bib.bib15)],
    to assist in cleaning and annotating the data. It is important to note that external
    links in the data could induce LLM to produce incorrect information; therefore,
    we performed data cleaning to remove these links. Considering LLMs’ sensitivity
    to input data, we standardized the code data, including removing unnecessary spaces
    without changing code semantics, to enhance the model robustness and reliability.
    To maximize the performance of the zero-shot learning of GPT-3.5 and GPT-4, we
    adopted and optimized the prompts from our partner, a Web3 security company. These
    prompts have been integrated into their working pipeline. For open-source models,
    we collaborated with an auditing expert to adapt their prompts for these models.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a common issue during model training, which we addressed by implementing
    an early stopping strategy. The choice of different models might affect the ranker-critic
    architecture’s effectiveness. We tested multiple cutting-edge open-source models,
    including MoE [[52](#bib.bib52)], CodeLlama-70b [[49](#bib.bib49)], Llama2-70b [[33](#bib.bib33)],
    and the recently introduced Gemma [[70](#bib.bib70)], and compared their performance
    on inference benchmark tests. Based on factors like the strictness of the model
    output format and operational speed, we chose MoE [[52](#bib.bib52)]. Our research
    also showed that the consistency between the selected reasons from the MoE and
    the real reasons reached about 38%. To control costs, we limited the maximum iterations
    in the ranker-critic loop to five and adopted four-decimal precision handling.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we proposed TrustLLM, the first smart contract auditing framework
    that combines fine-tuning and LLM-based agents to detect vulnerabilities and explain
    the results. We adopted a multiple-prompt-based strategy and applied LoRA-based
    fine-tuning to train the Detector and Reasoner. The former generates results based
    on a majority voting mechanism, while the latter provides multiple alternative
    explanations based on different inference paths. Furthermore, we introduced two
    LLM agents, Ranker and Critic, to collaborate in selecting the most appropriate
    explanation. Our approach demonstrated superior performance in zero-shot scenarios
    compared to zero-shot LLM learning and traditional full-model fine-tuning methods.
    We studied the performance improvement brought by the majority voting strategy
    and compared different LoRA training methods, providing the rationality of our
    choice. We also explored how additional calling context affects our model’s performance.
    For future work, we will focus on enhancing the model’s stability and its alignment
    with human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] L. Whitney, “Google paid out $10 million in bug bounties to security researchers
    in 2023,” https://www.zdnet.com/article/google-paid-out-10-million-in-bug-bounties-to-security-researchers-in-2023/,
    Mar. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Z. Zhang, B. Zhang, W. Xu, and Z. Lin, “Demystifying Exploitable Bugs in
    Smart Contracts,” in *2023 IEEE/ACM 45th International Conference on Software
    Engineering (ICSE)*, May 2023, pp. 615–627.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Feist, G. Grieco, and A. Groce, “Slither: A static analysis framework
    for smart contracts,” in *2019 IEEE/ACM 2nd International Workshop on Emerging
    Trends in Software Engineering for Blockchain (WETSEB)*, May 2019, pp. 8–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Fang, D. Wu, X. Yi, S. Wang, Y. Chen, M. Chen, Y. Liu, and L. Jiang,
    “Beyond “protected” and “private”: An empirical security analysis of custom function
    modifiers in smart contracts,” in *Proceedings of the 32nd ACM SIGSOFT International
    Symposium on Software Testing and Analysis*, ser. ISSTA 2023.   New York, NY,
    USA: Association for Computing Machinery, 2023, p. 1157–1168\. [Online]. Available:
    [https://doi.org/10.1145/3597926.3598125](https://doi.org/10.1145/3597926.3598125)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. Brent, N. Grech, S. Lagouvardos, B. Scholz, and Y. Smaragdakis, “Ethainter:
    a smart contract security analyzer for composite vulnerabilities,” in *Proceedings
    of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation*,
    ser. PLDI 2020.   New York, NY, USA: Association for Computing Machinery, 2020,
    p. 454–469\. [Online]. Available: [https://doi.org/10.1145/3385412.3385990](https://doi.org/10.1145/3385412.3385990)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Chen, X. Xia, D. Lo, J. Grundy, X. Luo, and T. Chen, “Defectchecker:
    Automated smart contract defect detection by analyzing evm bytecode,” *IEEE Transactions
    on Software Engineering*, vol. 48, no. 7, p. 2189–2207, Jul. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] L. Brent, A. Jurisevic, M. Kong, E. Liu, F. Gauthier, V. Gramoli, R. Holz,
    and B. Scholz, “Vandal: A scalable security analysis framework for smart contracts,”
    no. arXiv:1809.03981, Sep. 2018, arXiv:1809.03981 [cs]. [Online]. Available: [http://arxiv.org/abs/1809.03981](http://arxiv.org/abs/1809.03981)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Kalra, S. Goel, M. Dhawan, and S. Sharma, “ZEUS: Analyzing safety of
    smart contracts,” in *Proc. ISOC NDSS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. Tsankov, A. Dan, D. Drachsler-Cohen, A. Gervais, F. Bünzli, and M. Vechev,
    “Securify: Practical security analysis of smart contracts,” in *Proc. ACM CCS*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Mossberg, F. Manzano, E. Hennenfent, A. Groce, G. Grieco, J. Feist,
    T. Brunson, and A. Dinaburg, “Manticore: A user-friendly symbolic execution framework
    for binaries and smart contracts,” in *2019 34th IEEE/ACM International Conference
    on Automated Software Engineering (ASE)*, Nov. 2019, p. 1186–1189\. [Online].
    Available: [https://ieeexplore.ieee.org/document/8952204](https://ieeexplore.ieee.org/document/8952204)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Defillama, “Defillama hacks,” 2024\. [Online]. Available: [https://defillama.com/hacks](https://defillama.com/hacks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] I. David, L. Zhou, K. Qin, D. Song, L. Cavallaro, and A. Gervais, “Do
    you still need a manual smart contract audit?” no. arXiv:2306.12338, Jun. 2023,
    arXiv:2306.12338 [cs]. [Online]. Available: [http://arxiv.org/abs/2306.12338](http://arxiv.org/abs/2306.12338)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Sun, D. Wu, Y. Xue, H. Liu, H. Wang, Z. Xu, X. Xie, and Y. Liu, “GPTScan:
    Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program
    Analysis,” in *Proceedings of the 46th IEEE/ACM International Conference on Software
    Engineering*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S. Hu, T. Huang, F. Ilhan, S. Tekin, and L. Liu, “Large language model-powered
    smart contract vulnerability detection: New perspectives,” in *2023 5th IEEE International
    Conference on Trust, Privacy and Security in Intelligent Systems and Applications
    (TPS-ISA)*.   Los Alamitos, CA, USA: IEEE Computer Society, nov 2023, pp. 297–306\.
    [Online]. Available: [https://doi.ieeecomputersociety.org/10.1109/TPS-ISA58951.2023.00044](https://doi.ieeecomputersociety.org/10.1109/TPS-ISA58951.2023.00044)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Sun, D. Wu, Y. Xue, H. Liu, W. Ma, L. Zhang, M. Shi, and Y. Liu, “LLM4Vuln:
    A Unified Evaluation Framework for Decoupling and Enhancing LLMs’ Vulnerability
    Reasoning,” no. arXiv:2401.16185, Jan. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
    M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented
    generation for knowledge-intensive NLP tasks,” in *Proceedings of the 34th International
    Conference on Neural Information Processing Systems*, ser. NIPS’20.   Red Hook,
    NY, USA: Curran Associates Inc., Dec. 2020, pp. 9459–9474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] K. Tian, E. Mitchell, H. Yao, C. D. Manning, and C. Finn, “Fine-tuning
    Language Models for Factuality,” no. arXiv:2311.08401, Nov. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] K. Lv, Y. Yang, T. Liu, Q. Gao, Q. Guo, and X. Qiu, “Full Parameter Fine-tuning
    for Large Language Models with Limited Resources,” no. arXiv:2306.09782, Jun.
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Balaguer, V. Benara, R. L. d. F. Cunha, R. d. M. E. Filho, T. Hendry,
    D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O. Nunes, R. Padilha, M. Sharp,
    B. Silva, S. Sharma, V. Aski, and R. Chandra, “RAG vs Fine-tuning: Pipelines,
    Tradeoffs, and a Case Study on Agriculture,” no. arXiv:2401.08406, Jan. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] TrustLLM, “Trustllm inference code and dataset,” 2024\. [Online]. Available:
    [https://sites.google.com/view/trustllm/home](https://sites.google.com/view/trustllm/home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, J. Burstein,
    C. Doran, and T. Solorio, Eds.   Minneapolis, Minnesota: Association for Computational
    Linguistics, Jun. 2019, p. 4171–4186\. [Online]. Available: [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
    T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained model for programming
    and natural languages,” in *Findings of the Association for Computational Linguistics:
    EMNLP 2020*, T. Cohn, Y. He, and Y. Liu, Eds.   Online: Association for Computational
    Linguistics, Nov. 2020, pp. 1536–1547\. [Online]. Available: [https://aclanthology.org/2020.findings-emnlp.139](https://aclanthology.org/2020.findings-emnlp.139)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
    models are unsupervised multitask learners,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” no. arXiv:1910.13461, Oct.
    2019, arXiv:1910.13461 [cs, stat]. [Online]. Available: [http://arxiv.org/abs/1910.13461](http://arxiv.org/abs/1910.13461)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified
    text-to-text transformer,” *J. Mach. Learn. Res.*, vol. 21, no. 1, jan 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware unified
    pre-trained encoder-decoder models for code understanding and generation,” in
    *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*,
    2021, pp. 8696–8708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang *et al.*, “A survey of large
    language models,” no. arXiv:2303.18223, Nov. 2023, arXiv:2303.18223 [cs]. [Online].
    Available: [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang,
    Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey on
    evaluation of large language models,” *ACM Trans. Intell. Syst. Technol.*, jan
    2024, just Accepted. [Online]. Available: [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
    S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language models,”
    no. arXiv:2001.08361, Jan. 2020, arXiv:2001.08361 [cs, stat]. [Online]. Available:
    [http://arxiv.org/abs/2001.08361](http://arxiv.org/abs/2001.08361)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Google, “Google gemini ai,” 2024\. [Online]. Available: [https://blog.google/technology/ai/google-gemini-ai](https://blog.google/technology/ai/google-gemini-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] H. Touvron, L. Martin, K. Stone *et al.*, “Llama 2: Open foundation and
    fine-tuned chat models,” no. arXiv:2307.09288, Jul. 2023, arXiv:2307.09288 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang, “Parameter-efficient
    fine-tuning methods for pretrained language models: A critical review and assessment,”
    no. arXiv:2312.12148, Dec. 2023, arXiv:2312.12148 [cs]. [Online]. Available: [http://arxiv.org/abs/2312.12148](http://arxiv.org/abs/2312.12148)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, J. Liu, Z. Qu, S. Yan, Y. Zhu,
    Q. Zhang, M. Chowdhury, and M. Zhang, “Efficient large language models: A survey,”
    no. arXiv:2312.03863, Jan. 2024, arXiv:2312.03863 [cs]. [Online]. Available: [http://arxiv.org/abs/2312.03863](http://arxiv.org/abs/2312.03863)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li,
    and Z. Sui, “A survey on in-context learning,” no. arXiv:2301.00234, Jun. 2023,
    arXiv:2301.00234 [cs]. [Online]. Available: [http://arxiv.org/abs/2301.00234](http://arxiv.org/abs/2301.00234)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Z. Hu, L. Wang, Y. Lan, W. Xu, E.-P. Lim, L. Bing, X. Xu, S. Poria, and
    R. Lee, “LLM-adapters: An adapter family for parameter-efficient fine-tuning of
    large language models,” in *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, H. Bouamor, J. Pino, and K. Bali, Eds.   Singapore:
    Association for Computational Linguistics, Dec. 2023, pp. 5254–5276\. [Online].
    Available: [https://aclanthology.org/2023.emnlp-main.319](https://aclanthology.org/2023.emnlp-main.319)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] W. Song, Z. Li, L. Zhang, H. Zhao, and B. Du, “Sparse is enough in fine-tuning
    pre-trained large language model,” no. arXiv:2312.11875, Dec. 2023, arXiv:2312.11875
    [cs]. [Online]. Available: [http://arxiv.org/abs/2312.11875](http://arxiv.org/abs/2312.11875)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen *et al.*,
    “Lora: Low-rank adaptation of large language models,” in *International Conference
    on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for
    generation,” in *Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, 2021, pp. 4582–4597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient
    prompt tuning,” in *Proceedings of the 2021 Conference on Empirical Methods in
    Natural Language Processing*, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih,
    Eds.   Online and Punta Cana, Dominican Republic: Association for Computational
    Linguistics, Nov. 2021, pp. 3045–3059\. [Online]. Available: [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] N. Mundra, S. Doddapaneni, R. Dabre, A. Kunchukuttan, R. Puduppully, and
    M. M. Khapra, “A comprehensive analysis of adapter efficiency,” in *Proceedings
    of the 7th Joint International Conference on Data Science & Management of Data
    (11th ACM IKDD CODS and 29th COMAD)*, 2024, pp. 136–154.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Petrov, P. Torr, and A. Bibi, “When do prompting and prefix-tuning
    work? a theory of capabilities and limitations,” in *The Twelfth International
    Conference on Learning Representations*, 2024\. [Online]. Available: [https://openreview.net/forum?id=JewzobRhay](https://openreview.net/forum?id=JewzobRhay)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] D. A. Zetzsche, D. W. Arner, and R. P. Buckley, “Decentralized finance
    (defi),” *Journal of Financial Regulation*, vol. 6, pp. 172–203, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Defillama, “Defillama chain,” 2024\. [Online]. Available: [https://defillama.com/chains](https://defillama.com/chains)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] P. Praitheeshan, L. Pan, J. Yu, J. Liu, and R. Doss, “Security analysis
    methods on ethereum smart contract vulnerabilities: A survey,” no. arXiv:1908.08605,
    Sep. 2020, arXiv:1908.08605 [cs]. [Online]. Available: [http://arxiv.org/abs/1908.08605](http://arxiv.org/abs/1908.08605)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] P. Züst, T. Nadahalli, and Y. W. R. Wattenhofer, “Analyzing and preventing
    sandwich attacks in ethereum,” *ETH Zürich*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. Zhou, J. He, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Prompt consistency
    for zero-shot task generalization,” in *Findings of the Association for Computational
    Linguistics: EMNLP 2022*, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.   Abu Dhabi,
    United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp.
    2613–2626\. [Online]. Available: [https://aclanthology.org/2022.findings-emnlp.192](https://aclanthology.org/2022.findings-emnlp.192)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla *et al.*, “Code llama:
    Open foundation models for code,” no. arXiv:2308.12950, Jan. 2024, arXiv:2308.12950
    [cs]. [Online]. Available: [http://arxiv.org/abs/2308.12950](http://arxiv.org/abs/2308.12950)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    and T. B. Hashimoto, “Stanford alpaca: An instruction-following llama model,”
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language
    models are zero-shot reasoners,” *Advances in neural information processing systems*,
    vol. 35, pp. 22 199–22 213, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, and et al., “Mixtral
    of experts,” no. arXiv:2401.04088, Jan. 2024, arXiv:2401.04088 [cs]. [Online].
    Available: [http://arxiv.org/abs/2401.04088](http://arxiv.org/abs/2401.04088)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Q. Jiang, A. Sablayrolles, A. Mensch, and et al., “Mistral 7b,” no.
    arXiv:2310.06825, Oct. 2023, arXiv:2310.06825 [cs]. [Online]. Available: [http://arxiv.org/abs/2310.06825](http://arxiv.org/abs/2310.06825)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] F. Xue, Z. Zheng, Y. Fu, J. Ni, Z. Zheng, W. Zhou, and Y. You, “Openmoe:
    An early effort on open mixture-of-experts language models,” no. arXiv:2402.01739,
    Jan. 2024, arXiv:2402.01739 [cs]. [Online]. Available: [http://arxiv.org/abs/2402.01739](http://arxiv.org/abs/2402.01739)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Solodit, “Solodit,” 2024\. [Online]. Available: [https://solodit.xyz/](https://solodit.xyz/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, L. Shujie, L. Zhou, N. Duan,
    A. Svyatkovskiy, S. Fu *et al.*, “Graphcodebert: Pre-training code representations
    with data flow,” in *International Conference on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, “Unixcoder: Unified
    cross-modal pre-training for code representation,” in *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 2022, pp. 7212–7225.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Huggingface, “Huggingface transformer,” 2024\. [Online]. Available: [https://huggingface.co/docs/transformers/](https://huggingface.co/docs/transformers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Zhuang, Z. Liu, P. Qian, Q. Liu, X. Wang, and Q. He, “Smart Contract
    Vulnerability Detection using Graph Neural Network,” in *Twenty-Ninth International
    Joint Conference on Artificial Intelligence*, vol. 3, Jul. 2020, pp. 3283–3290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Z. Liu, P. Qian, X. Wang, L. Zhu, Q. He, and S. Ji, “Smart Contract Vulnerability
    Detection: From Pure Neural Network to Interpretable Graph Feature and Expert
    Pattern Fusion,” in *Twenty-Ninth International Joint Conference on Artificial
    Intelligence*, vol. 3, Aug. 2021, pp. 2751–2759.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Wu, Z. Zhang, S. Wang, Y. Lei, B. Lin, Y. Qin, H. Zhang, and X. Mao,
    “Peculiar: Smart Contract Vulnerability Detection Based on Crucial Data Flow Graph
    and Pre-training Techniques,” in *2021 IEEE 32nd International Symposium on Software
    Reliability Engineering (ISSRE)*, Oct. 2021, pp. 378–389.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Ullah, M. Han, S. Pujar, H. Pearce, A. Coskun, and G. Stringhini, “Can
    Large Language Models Identify And Reason About Security Vulnerabilities? Not
    Yet,” no. arXiv:2312.12575, Dec. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Fu, C. Tantithamthavorn, V. Nguyen, and T. Le, “ChatGPT for Vulnerability
    Detection, Classification, and Repair: How Far Are We?” no. arXiv:2310.09810,
    Oct. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] C. Thapa, S. I. Jang, M. E. Ahmed, S. Camtepe, J. Pieprzyk, and S. Nepal,
    “Transformer-Based Language Models for Software Vulnerability Detection,” in *Proceedings
    of the 38th Annual Computer Security Applications Conference*, ser. ACSAC ’22.   New
    York, NY, USA: Association for Computing Machinery, Dec. 2022, pp. 481–496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. Alqarni and A. Azim, “Low Level Source Code Vulnerability Detection
    Using Advanced BERT Language Model,” in *Proceedings of the Canadian Conference
    on Artificial Intelligence*.   Canadian Artificial Intelligence Association (CAIAC),
    May 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding,” in *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, J. Burstein,
    C. Doran, and T. Solorio, Eds.   Minneapolis, Minnesota: Association for Computational
    Linguistics, Jun. 2019, pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] N. S. Mathews, Y. Brus, Y. Aafer, M. Nagappan, and S. McIntosh, “LLbezpeky:
    Leveraging Large Language Models for Vulnerability Detection,” no. arXiv:2401.01269,
    Feb. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] M. D. Purba, A. Ghosh, B. J. Radford, and B. Chu, “Software Vulnerability
    Detection using Large Language Models,” in *2023 IEEE 34th International Symposium
    on Software Reliability Engineering Workshops (ISSREW)*.   IEEE Computer Society,
    Oct. 2023, pp. 112–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H. Li, Y. Hao, Y. Zhai, and Z. Qian, “The Hitchhiker’s Guide to Program
    Analysis: A Journey with Large Language Models,” no. arXiv:2308.00245, Nov. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak
    *et al.*, “Gemma: Open models based on gemini research and technology,” no. arXiv:2403.08295,
    Mar. 2024, arXiv:2403.08295 [cs]. [Online]. Available: [http://arxiv.org/abs/2403.08295](http://arxiv.org/abs/2403.08295)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
