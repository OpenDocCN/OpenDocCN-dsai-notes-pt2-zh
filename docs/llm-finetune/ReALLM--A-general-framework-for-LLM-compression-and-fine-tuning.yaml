- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ReALLM: A general framework for LLM compression and fine-tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReALLM：一种通用的LLM压缩和微调框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
- en: \newaliascnt
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newaliascnt
- en: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
- en: Louis Leconte
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 路易斯·勒孔特
- en: Lisite, Isep, Sorbonne University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Lisite，Isep，索邦大学
- en: Math. and Algo. Sciences Lab, Huawei Tech
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与算法科学实验室，华为技术
- en: louis.leconte@ens-paris-saclay.fr
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: louis.leconte@ens-paris-saclay.fr
- en: '&Lisa Bedin^∗'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Lisa Bedin^∗'
- en: CMAP, Ecole Polytechnique, France
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CMAP，巴黎高科，法国
- en: lisa.bedin@polytechnique.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: lisa.bedin@polytechnique.edu
- en: Van Minh Nguyen
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 范敏阮
- en: Math. and Algo. Sciences Lab, Huawei Tech.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与算法科学实验室，华为技术。
- en: '&Eric Moulines'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '&Eric Moulines'
- en: CMAP, Ecole Polytechnique, France equal contribution
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CMAP，巴黎高科，法国，等贡献
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce ReALLM, a novel approach for compression and memory-efficient adaptation
    of pre-trained language models that encompasses most of the post-training quantization
    and fine-tuning methods for a budget of $<4$ bits. The decompression of a matrix
    requires only one embedding and a single forward pass with the decoder. Our weight-only
    quantization algorithm yields the best results on language generation tasks (C4
    and WikiText-2) for a budget of $3$ bits, ReALLM achieves state-of-the art performance
    after fine-tuning on a small calibration dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了ReALLM，一种新颖的压缩和内存高效适配预训练语言模型的方法，它涵盖了大多数后训练量化和微调方法，预算为$<4$位。矩阵的解压缩只需一个嵌入和一个解码器的前向传递。我们的仅权重量化算法在语言生成任务（C4和WikiText-2）中在$3$位预算下表现最佳，ReALLM在小型校准数据集上微调后实现了**最先进的性能**。
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) based on transformer architectures (Vaswani et al.,,
    [2017](#bib.bib48)) have attracted increasing interest, especially with the availability
    of high-quality, open-source LLMs such as LLaMA (Touvron et al.,, [2023](#bib.bib44)),
    Falcon (Almazrouei et al.,, [2023](#bib.bib1)) and Gemma (Team et al.,, [2024](#bib.bib43)).
    These open models offer the advantage that they can be used by end users for inference
    or local fine-tuning, provided their hardware has sufficient memory for the size
    of the models. However, “full fine-tuning” — a process that involves updating
    all previously trained parameters — is still prohibitively expensive for large
    models. For example, the standard 16-bits fine-tuning of the LLaMA-$65$ GB of
    GPU memory ([Dettmers et al., 2023a,](#bib.bib11) ). This high requirement is
    due to the need to store both the weights of the model and the states of the optimizer
    in GPU memory, a need that increases as the size of the LLMs increases.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变压器架构的**大型语言模型（LLMs）**（Vaswani et al., [2017](#bib.bib48)）引起了越来越多的关注，特别是高质量的开源LLMs如LLaMA（Touvron
    et al., [2023](#bib.bib44)）、Falcon（Almazrouei et al., [2023](#bib.bib1)）和Gemma（Team
    et al., [2024](#bib.bib43)）的出现。这些开放模型的优势在于，最终用户可以用其进行推理或本地微调，只要他们的硬件有足够的内存来容纳这些模型。然而，“完全微调”——一个涉及更新所有先前训练的参数的过程——对于大型模型仍然是极其昂贵的。例如，LLaMA-$65$
    GB GPU内存的标准16位微调（[Dettmers et al., 2023a,](#bib.bib11)）。这种高要求是由于需要在GPU内存中存储模型的权重和优化器的状态，随着LLMs规模的增加，这一需求也在增加。
- en: A common method to mitigate memory constraints is to quantize the model weights,
    activations, and gradients — to a lower bit precision. Quantization-Aware Training
    (QAT) is often used in computer vision; see Courbariaux et al., ([2015](#bib.bib8));
    Liu et al., ([2020](#bib.bib33)); Gholami et al., ([2022](#bib.bib16)). However,
    training large language models (LLMs) from scratch is impractical due to high
    computational cost. Post-training quantization (PTQ) is an efficient compromise
    (Dettmers et al.,, [2022](#bib.bib10); Frantar et al.,, [2022](#bib.bib14)), which
    has recently attracted much attention ([Kim et al., 2023b,](#bib.bib24) ; [Dettmers
    et al., 2023b,](#bib.bib12) ; [Kim et al., 2023a,](#bib.bib23) ; Shao et al.,,
    [2023](#bib.bib39)). Although most research focuses on scalar quantization (SQ),
    a few studies investigate LLM compression using vector quantization (VQ) (Tseng
    et al.,, [2024](#bib.bib46); Egiazarian et al.,, [2024](#bib.bib13)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解内存限制的一个常用方法是将模型权重、激活和梯度量化到较低的位精度。量化感知训练（QAT）通常用于计算机视觉；参见Courbariaux et al.,
    ([2015](#bib.bib8)); Liu et al., ([2020](#bib.bib33)); Gholami et al., ([2022](#bib.bib16))。然而，由于高计算成本，从头训练大型语言模型（LLMs）是不切实际的。后训练量化（PTQ）是一种高效的折中方案（Dettmers
    et al., [2022](#bib.bib10); Frantar et al., [2022](#bib.bib14)），最近受到广泛关注（[Kim
    et al., 2023b](#bib.bib24); [Dettmers et al., 2023b](#bib.bib12); [Kim et al.,
    2023a](#bib.bib23); Shao et al., [2023](#bib.bib39)）。尽管大多数研究集中在标量量化（SQ）上，但也有一些研究探讨了使用向量量化（VQ）进行LLM压缩（Tseng
    et al., [2024](#bib.bib46); Egiazarian et al., [2024](#bib.bib13)）。
- en: In [Dettmers et al., 2023a](#bib.bib11) , quantization is effectively combined
    with the Parameter Efficient Fine-Tuning (PEFT) method, LoRA (Hu et al.,, [2021](#bib.bib21)),
    to improve efficiency and practicality in memory-constrained environments. Post-Training
    Quantization (PTQ) has the potential to be further improved to achieve sub-$3$
    bit quantization (Li et al.,, [2023](#bib.bib27); Guo et al.,, [2023](#bib.bib18)).
    However, it was found that the weights of the LLM often contain outliers — weights
    with significantly higher values than others ([Kim et al., 2023b,](#bib.bib24)
    ; [Dettmers et al., 2023b,](#bib.bib12) ). These outliers pose a considerable
    challenge for model compression with PTQ and lead to significant quantization
    errors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Dettmers et al., 2023a](#bib.bib11)中，量化有效地与参数高效微调（PEFT）方法LoRA（Hu et al., [2021](#bib.bib21)）相结合，以提高在内存受限环境中的效率和实用性。后训练量化（PTQ）有可能进一步改进，实现小于$3$位的量化（Li
    et al., [2023](#bib.bib27); Guo et al., [2023](#bib.bib18)）。然而，研究发现LLM的权重往往包含异常值——权重值显著高于其他值（[Kim
    et al., 2023b](#bib.bib24); [Dettmers et al., 2023b](#bib.bib12)）。这些异常值对使用PTQ进行模型压缩构成了相当大的挑战，并导致了显著的量化误差。
- en: 'In this paper we present ReALLM - for Residual Autoencoder LLM - a general
    approach for LLM PTQ and fine-tuning. Pre-trained LLM matrices are decomposed
    into a $16$-bit remainder (low rank, sparse outliers, etc.) and a compressed part,
    which is fed into a VQ autoencoder (Van Den Oord et al.,, [2017](#bib.bib47)).
    In our experiments, we implement a low-rank and quantized decomposition of pre-trained
    LLM matrices. In this approach, only the low-rank components are fine-tuned (block-wise
    and end-to-end) while the quantized elements remain static. Our quantization strategy
    (i.e. the shape of the autoencoder) adapts to the matrix patterns: Our results
    suggest that some pre-trained LLM matrices exhibit “spatial” patterns (see [Figure 1](#S1.F1
    "In 1 Introduction ‣ ReALLM: A general framework for LLM compression and fine-tuning");
    left) that bear similarities to those in images/videos and allow for highly effective
    compression (see [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣
    ReALLM: A general framework for LLM compression and fine-tuning")).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们提出了ReALLM - 即Residual Autoencoder LLM - 一种通用的LLM PTQ（后训练量化）和微调方法。预训练的LLM矩阵被分解为$16$位余量（低秩、稀疏异常值等）和压缩部分，后者被送入VQ自编码器（Van
    Den Oord等，[2017](#bib.bib47)）。在我们的实验中，我们实现了对预训练LLM矩阵的低秩和量化分解。在这种方法中，只有低秩组件经过微调（按块和端到端），而量化元素保持静态。我们的量化策略（即自编码器的形状）会适应矩阵模式：我们的结果表明，一些预训练LLM矩阵展示了类似于图像/视频的“空间”模式（见[图1](#S1.F1
    "在1 引言 ‣ ReALLM: LLM压缩和微调的通用框架"); 左侧），这使得压缩效果极为有效（见[图3](#S3.F3 "在ReALLM: 新LLM格式.
    ‣ 3 方法 ‣ ReALLM: LLM压缩和微调的通用框架")）。'
- en: '![Refer to caption](img/69769f0dcf311395c643505d6b507101.png)![Refer to caption](img/a5078b6b39a9315e48bf083e47a52b0b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/69769f0dcf311395c643505d6b507101.png)![参见标题](img/a5078b6b39a9315e48bf083e47a52b0b.png)'
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mistral-7B (Jiang et al., [2023](#bib.bib22))
- en: '![Refer to caption](img/c322daf7100a0c194d410694fbf1811e.png)![Refer to caption](img/1a3fe2b2cd3f9062207e0f1903a5d246.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c322daf7100a0c194d410694fbf1811e.png)![参见说明](img/1a3fe2b2cd3f9062207e0f1903a5d246.png)'
- en: (b) Gemma-2B (Team et al.,, [2024](#bib.bib43))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Gemma-2B (Team等，[2024](#bib.bib43))
- en: 'Figure 1: Pre-trained matrix from the first block (left; with “structures”),
    and pre-trained matrix from the last block (right) for two different models. Stronger
    vertical patterns appear in the first blocks.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自第一个块的预训练矩阵（左；带有“结构”），以及来自最后一个块的预训练矩阵（右）用于两种不同的模型。第一个块中出现更强的垂直模式。
- en: 'Contributions:'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present ReALLM, a method that uses a novel autoencoder and a residual pipeline
    to efficiently compress pre-trained LLM matrices;
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了ReALLM，一种使用新型自编码器和残差管道来高效压缩预训练LLM矩阵的方法；
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that state-of-the-art PTQ approaches (Lin et al.,, [2023](#bib.bib29);
    Shao et al.,, [2023](#bib.bib39); Tseng et al.,, [2024](#bib.bib46); Egiazarian
    et al.,, [2024](#bib.bib13)) and fine-tuning methods (Hu et al.,, [2021](#bib.bib21);
    [Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18); Li et al.,,
    [2023](#bib.bib27); Liao and Monz,, [2024](#bib.bib28)) are all special cases
    of ReALLM;
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了最先进的PTQ方法（Lin等，[2023](#bib.bib29)；Shao等，[2023](#bib.bib39)；Tseng等，[2024](#bib.bib46)；Egiazarian等，[2024](#bib.bib13)）和微调方法（Hu等，[2021](#bib.bib21)；[Dettmers等，2023a,](#bib.bib11)；Guo等，[2023](#bib.bib18)；Li等，[2023](#bib.bib27)；Liao和Monz，[2024](#bib.bib28)）都是ReALLM的特殊情况；
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a preprocessing step that includes scaling and column permutations
    of matrices to mitigate the quantization errors associated with outliers; We also
    propose to adapt the general autoencoder scheme to the type of pre-trained matrix
    patterns.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种预处理步骤，包括矩阵的缩放和列排列，以减轻与异常值相关的量化误差；我们还建议将通用自编码器方案调整为预训练矩阵模式的类型。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our approach demonstrates that fine-tuning end-to-end with block-wise error
    reduction leads to the best results reported in the literature for 3 and 2-bit
    Post-Training Quantization (PTQ).
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法表明，通过块级误差减少进行端到端的微调可以获得文献中报告的最佳3位和2位后训练量化（PTQ）结果。
- en: 2 Related works
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs adapters.
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM适配器。
- en: After the introduction of high-performance open-source LLMs and due to the impracticality
    of “full fine-tuning”, several methods of parameter-efficient fine-tuning (PEFT)
    have emerged, including prefix tuning (Li and Liang,, [2021](#bib.bib26)), selective
    fine-tuning (Guo et al.,, [2021](#bib.bib17)) and Low Rank Adapter (LoRA). LoRA,
    introduced in Hu et al., ([2021](#bib.bib21)), is a simple but effective fine-tuning
    method that retains the pre-trained matrices but adds a low-rank component. For
    a typical pre-trained matrix $W$, where $r\ll 4096$ is the frozen pre-trained
    weight, $m$ denotes the Euclidean norm of a matrix over each column. DoRA with
    the trainable size vector requires little computational effort, but can lead to
    significant performance improvements (Liu et al.,, [2024](#bib.bib31)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在高性能开源LLM的引入以及“完全微调”不切实际的情况下，出现了几种参数高效微调（PEFT）的方法，包括前缀调优（Li和Liang，[2021](#bib.bib26)）、选择性微调（Guo等，[2021](#bib.bib17)）和低秩适配器（LoRA）。LoRA，由Hu等人（[2021](#bib.bib21)）提出，是一种简单但有效的微调方法，它保留了预训练矩阵但添加了低秩组件。对于一个典型的预训练矩阵
    $W$，其中 $r\ll 4096$ 是冻结的预训练权重，$m$ 表示矩阵在每一列上的欧几里得范数。带有可训练大小向量的DoRA需要很少的计算工作，但可以显著提高性能（Liu等，[2024](#bib.bib31)）。
- en: Quantization.
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化。
- en: Current methods for compressing LLMs predominantly use quantization techniques.
    Early strategies, such as ZeroQuant (Yao et al.,, [2022](#bib.bib52)) and nuQmm
    (Park et al.,, [2022](#bib.bib36)), relied primarily on direct rounding of weights
    to the nearest quantization level. Later developments improved this approach by
    handling outliers through quantization to higher bitwidths (Xiao et al.,, [2023](#bib.bib51);
    Dettmers et al.,, [2022](#bib.bib10); [Kim et al., 2023b,](#bib.bib24) ; [Dettmers
    et al., 2023b,](#bib.bib12) ). Methods similar to ReALLM include those that combine
    quantization with a low-rank decomposition; see e.g. [Dettmers et al., 2023a](#bib.bib11)
    ; Guo et al., ([2023](#bib.bib18)); Li et al., ([2023](#bib.bib27)); Liao and
    Monz, ([2024](#bib.bib28)). QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) combined
    Parameter Efficient Fine-Tuning (PEFT) and quantization, but added zero-initialised
    low-rank adapters after quantization. In contrast, Loftq (Li et al.,, [2023](#bib.bib27))
    and LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) propose to minimize quantization
    errors by initializing LoRA components with an SVD of the pre-trained weights.
    As part of this integration, ApiQ (Liao and Monz,, [2024](#bib.bib28)) uses gradient
    descent to optimize both the LoRA components and the quantization parameters for
    the entire model rather than for each individual layer. Quantization of pre-trained
    weights facilitates efficient inference on devices with limited memory. To achieve
    significant computational and energy efficiency, recent studies have combined
    quantization of weights with activation quantization (Liu et al.,, [2023](#bib.bib30);
    Nrusimha et al.,, [2024](#bib.bib35)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前用于压缩 LLM 的方法主要使用量化技术。早期的策略，如 ZeroQuant (Yao et al., [2022](#bib.bib52)) 和
    nuQmm (Park et al., [2022](#bib.bib36))，主要依赖于将权重直接舍入到最接近的量化级别。后来的发展通过对异常值进行更高位宽量化
    (Xiao et al., [2023](#bib.bib51); Dettmers et al., [2022](#bib.bib10); [Kim et
    al., 2023b](#bib.bib24); [Dettmers et al., 2023b](#bib.bib12)) 改进了这种方法。类似于 ReALLM
    的方法包括那些将量化与低秩分解相结合的技术；例如，[Dettmers et al., 2023a](#bib.bib11); Guo et al., ([2023](#bib.bib18));
    Li et al., ([2023](#bib.bib27)); Liao 和 Monz, ([2024](#bib.bib28))。QLoRA ([Dettmers
    et al., 2023a](#bib.bib11)) 将参数高效微调 (PEFT) 和量化结合起来，但在量化后添加了零初始化的低秩适配器。相比之下，Loftq
    (Li et al., [2023](#bib.bib27)) 和 LQ-LoRA (Guo et al., [2023](#bib.bib18)) 提出通过使用预训练权重的
    SVD 初始化 LoRA 组件来最小化量化误差。作为这种集成的一部分，ApiQ (Liao 和 Monz, [2024](#bib.bib28)) 使用梯度下降来优化整个模型的
    LoRA 组件和量化参数，而不是针对每一层单独优化。预训练权重的量化促进了在内存有限的设备上高效推理。为了实现显著的计算和能源效率，最近的研究将权重量化与激活量化结合起来
    (Liu et al., [2023](#bib.bib30); Nrusimha et al., [2024](#bib.bib35))。
- en: Block/Layer-Wise Tuning.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层/块级微调。
- en: GPTQ (Frantar et al.,, [2022](#bib.bib14)) introduced a higher accuracy strategy
    using an approximate large-scale solver to minimize the layer-wise quadratic error,
    which is crucial for low bit-width quantization, as highlighted in Tseng et al.,
    ([2024](#bib.bib46)); Egiazarian et al., ([2024](#bib.bib13)). Quip# (Tseng et al.,,
    [2024](#bib.bib46)) applies random rotations to the pre-trained matrices, segments
    the resulting matrix into vectors of dimension $d=8$ bits per parameter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ (Frantar et al., [2022](#bib.bib14)) 引入了一种更高精度的策略，通过使用近似的大规模求解器来最小化逐层的二次误差，这对于低位宽量化至关重要，正如
    Tseng et al., ([2024](#bib.bib46)); Egiazarian et al., ([2024](#bib.bib13)) 所强调的那样。Quip#
    (Tseng et al., [2024](#bib.bib46)) 对预训练矩阵应用随机旋转，将得到的矩阵划分为每个参数 $d=8$ 位的向量。
- en: 3 Method
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/1cdadff5c11726b871db407ed222c818.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1cdadff5c11726b871db407ed222c818.png)'
- en: 'Figure 2: ReALLM; during the fine-tuning step only low-rank and scales are
    updated'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: ReALLM；在微调步骤中，仅更新低秩和缩放因子'
- en: Low-rank/sparse decomposition.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩/稀疏分解。
- en: 'Starting from a pre-trained LLM matrix $W\in\mathbb{R}^{p\times q}$ (which
    is represented on average with $b$ and $L_{2}$ that (approximately) solve the
    following problem:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的 LLM 矩阵 $W\in\mathbb{R}^{p\times q}$ 开始（该矩阵平均表示为 $b$ 和 $L_{2}$，这些矩阵（近似）地解决以下问题：
- en: '|  | $\min_{Q,L_{1},L_{2}}\&#124;W-(Q+L_{1}(L_{2})^{t})\&#124;.$ |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{Q,L_{1},L_{2}}\&#124;W-(Q+L_{1}(L_{2})^{t})\&#124;.$ |  | (1) |'
- en: QLoRA [Dettmers et al., 2023a](#bib.bib11) provides a suboptimal solution for
    the previously described optimization problem by setting $L_{1}=0$. There is no
    guarantee that the initialization of the low-rank part to zero is optimal. It
    has been reported that QLoRA, Apiq and Loftq perform better than QLoRA in several
    language generation benchmarks (Guo et al.,, [2023](#bib.bib18); Liao and Monz,,
    [2024](#bib.bib28); Li et al.,, [2023](#bib.bib27)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA [Dettmers 等，2023a](#bib.bib11) 通过将 $L_{1}=0$ 提供了一个次优解。没有保证低秩部分初始化为零是最佳的。已报道
    QLoRA、Apiq 和 Loftq 在几个语言生成基准测试中表现优于 QLoRA (Guo 等，[2023](#bib.bib18); Liao 和 Monz，[2024](#bib.bib28);
    Li 等，[2023](#bib.bib27))。
- en: Mixed-autoencoder configuration.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合自编码器配置。
- en: 'An autoencoder is the composition of an encoding function $\mathcal{E}$ are
    parameterized by neural networks $\psi,\phi$. As far as we know, most previous
    works on quantization of LLMs have focused on applying the same quantization strategy
    *directly* to the (rotated) pre-trained matrix: i.e. take the embedding dimensions
    $e_{0}=p,e_{1}=q,e_{2}=1$. LQ-LoRA (Guo et al.,, [2023](#bib.bib18)), Loftq (Li
    et al.,, [2023](#bib.bib27)), and ApiQ (Liao and Monz,, [2024](#bib.bib28)) are
    special cases of ReALLM where the encoder and the decoder are defined as the identity
    matrix.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是编码函数 $\mathcal{E}$ 的组合，由神经网络 $\psi,\phi$ 参数化。根据我们所知，大多数关于 LLM 量化的先前工作都集中在将相同的量化策略*直接*应用于（旋转的）预训练矩阵上：即采用嵌入维度
    $e_{0}=p,e_{1}=q,e_{2}=1$。LQ-LoRA (Guo 等，[2023](#bib.bib18))，Loftq (Li 等，[2023](#bib.bib27))
    和 ApiQ (Liao 和 Monz，[2024](#bib.bib28)) 是 ReALLM 的特例，其中编码器和解码器被定义为单位矩阵。
- en: 'The approach may not be optimal as some matrices are more challenging to quantize
    than others (Guo et al.,, [2023](#bib.bib18)). Specifically, [Figure 1](#S1.F1
    "In 1 Introduction ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    shows that pre-trained LLM matrices can display very different “spatial” patterns.
    ReALLM adapts the autoencoder to the type and shape of the matrix. When quantizing
    pre-trained matrices with strong coefficient dependencies, ReALLM is akin to image
    and video compression techniques that use the implicit neural representation (Chen
    et al.,, [2023](#bib.bib6); Kwan et al.,, [2024](#bib.bib25)). ReALLM extracts
    latent representations $\mathcal{E}_{\psi}(W)$ are needed to reconstruct the original
    weight $W$ consisting of standard 2D convolutions, and a decoder combining 2D-convNeXt
    (Liu et al.,, [2022](#bib.bib32)) and PixelShuffle (Shi et al.,, [2016](#bib.bib40)).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '该方法可能不是最优的，因为某些矩阵比其他矩阵更难以量化 (Guo 等，[2023](#bib.bib18))。具体而言，[图 1](#S1.F1 "在
    1 介绍 ‣ ReALLM: 一种用于 LLM 压缩和微调的通用框架") 显示了预训练的 LLM 矩阵可以显示非常不同的“空间”模式。ReALLM 根据矩阵的类型和形状调整自编码器。量化具有强系数依赖关系的预训练矩阵时，ReALLM
    类似于使用隐式神经表示的图像和视频压缩技术 (Chen 等，[2023](#bib.bib6); Kwan 等，[2024](#bib.bib25))。ReALLM
    提取潜在表示 $\mathcal{E}_{\psi}(W)$，这些表示是重建由标准 2D 卷积组成的原始权重 $W$ 所需的，并且解码器结合了 2D-convNeXt
    (Liu 等，[2022](#bib.bib32)) 和 PixelShuffle (Shi 等，[2016](#bib.bib40))。'
- en: The decoding process is fast, as HNeRV requires only one network forward operation
    for decoding. ReALLM compression is a combination of a small (w.r.t. input signals)
    neural decoder model $\mathcal{D}_{\phi}$ bits during training with the straight-through
    estimator (Bengio,, [2013](#bib.bib5)). For a typical matrix of size $4096\times
    4096$. The total bit budget for the given matrix is therefore $\frac{6\cdot(7.2\cdot
    10^{6})+16\cdot(16\cdot 16\cdot 16\cdot\frac{4096^{2}}{512^{2}})}{(4096)^{2}}=2.82$
    bits per coordinate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 解码过程很快，因为 HNeRV 只需要一个网络前向操作进行解码。ReALLM 压缩结合了一个小的（相对于输入信号）神经解码器模型 $\mathcal{D}_{\phi}$
    位于训练期间与直通估计器 (Bengio，[2013](#bib.bib5))。对于一个典型的大小为 $4096\times 4096$ 的矩阵。因此，给定矩阵的总位预算为
    $\frac{6\cdot(7.2\cdot 10^{6})+16\cdot(16\cdot 16\cdot 16\cdot\frac{4096^{2}}{512^{2}})}{(4096)^{2}}=2.82$
    位每坐标。
- en: Vector Quantization (VQ).
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量量化 (VQ)。
- en: 'An efficient way to store the embedding $\mathcal{E}_{\psi}(W)$. First, we
    compute scales with NF-normalization ([Dettmers et al., 2023a,](#bib.bib11) ;
    Guo et al.,, [2023](#bib.bib18)). The scales are further quantized following the
    idea of LQ-LoRA, resulting in an additional memory cost of $0.1$ is quantized
    by the index of the closest element in the codebook (see [Figure 2](#S3.F2 "In
    3 Method ‣ ReALLM: A general framework for LLM compression and fine-tuning")).
    Consequently, the total number of bits required is $(bd)\frac{e_{0}e_{1}e_{2}}{d}$
    bits). It should be noted that no separate gradient is defined for the quantization
    operator with the closest element (Van Den Oord et al.,, [2017](#bib.bib47)).
    Therefore, during the backward pass, we approximate the gradient similarly to
    the straight-through estimator (Bengio,, [2013](#bib.bib5)) and simply copy the
    gradients from the decoder input to the encoder output.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '一种高效的存储嵌入$\mathcal{E}_{\psi}(W)$的方法。首先，我们使用NF-归一化计算尺度（[Dettmers et al., 2023a](#bib.bib11)；Guo
    et al., [2023](#bib.bib18)）。这些尺度随后按照LQ-LoRA的思路进行量化，从而导致额外的内存开销为$0.1$，通过代码本中最接近元素的索引进行量化（见[图2](#S3.F2
    "在 3 方法 ‣ ReALLM: 一般框架用于LLM压缩与微调")）。因此，总共需要的位数为$(bd)\frac{e_{0}e_{1}e_{2}}{d}$位)。需要注意的是，对于具有最接近元素的量化操作，没有定义单独的梯度（Van
    Den Oord et al., [2017](#bib.bib47)）。因此，在反向传播过程中，我们类似于直通估计器（Bengio, [2013](#bib.bib5)）来近似梯度，并仅仅将梯度从解码器输入复制到编码器输出。'
- en: Quantization pre-processing.
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化预处理。
- en: 'Before using a tensor quantization method, it is important to perform an appropriate
    scaling. Several parameters (number of blocks, quantile bins, etc.) are chosen
    to correspond to a given compression ratio. But the presence of outliers ([Kim
    et al., 2023b,](#bib.bib24) ; [Dettmers et al., 2023b,](#bib.bib12) ) forces the
    scaling and quantization methods to have a poor compression ratio (Lin et al.,,
    [2023](#bib.bib29); Tseng et al.,, [2024](#bib.bib46); Ashkboos et al.,, [2024](#bib.bib4)).
    Incoherence processing uses random rotations as a pre-processing step. Although
    the main purpose of incoherence processing is to reduce the effects of outliers
    (Tseng et al.,, [2024](#bib.bib46); Ashkboos et al.,, [2024](#bib.bib4)), this
    technique has a detrimental effect on the structure of the pre-trained matrices
    within the initial blocks of the LLM (see [Figures 1](#S1.F1 "In 1 Introduction
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [3](#S3.F3
    "Figure 3 ‣ ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")). This is a serious bottleneck as quantization
    errors in these initial blocks can propagate throughout the model. As shown in
    [Figure 1](#S1.F1 "In 1 Introduction ‣ ReALLM: A general framework for LLM compression
    and fine-tuning"), some matrices have no specific patterns and resemble random
    Gaussian noise interspersed with randomly positioned outliers. To deal with outliers
    in the latent representation, we suggest rearranging the columns to create some
    spatial regularity. This strategy aims to find the most effective permutations
    that cluster outliers. Trukhanov and Soloveychik, ([2024](#bib.bib45)) has recently
    elaborated a row/column permutation strategy that summarizes vectors (i.e. sets
    of rows or columns) with similar norms. In contrast, for ReALLM we propose to
    permute columns such that neighboring columns are “similar” and not just on the
    same hypersphere. We develop a basic, yet efficient method for this: first we
    select a block of size $128\times q$ scalar products and select the vector that
    minimizes it). Then, we permute the neighbor vector with the vector in the second
    position of the block. The process is then iterated; more details are given in
    [Algorithm 1](#algorithm1 "In Quantization pre-processing. ‣ 3 Method ‣ ReALLM:
    A general framework for LLM compression and fine-tuning") and [Section A.3](#A1.SS3
    "A.3 Permutations ‣ Appendix A Appendix / supplemental material ‣ ReALLM: A general
    framework for LLM compression and fine-tuning"). Note that the memory storage
    of the permutation is negligible: for a LLM matrix with $q=4096$ bits per coordinate.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在使用张量量化方法之前，进行适当的缩放是重要的。几个参数（块数、分位数箱等）被选择以对应于给定的压缩比。但是，异常值的存在（[Kim et al.,
    2023b,](#bib.bib24)；[Dettmers et al., 2023b,](#bib.bib12)）迫使缩放和量化方法的压缩比变差（Lin
    et al., [2023](#bib.bib29)；Tseng et al., [2024](#bib.bib46)；Ashkboos et al., [2024](#bib.bib4)）。不一致性处理使用随机旋转作为预处理步骤。尽管不一致性处理的主要目的是减少异常值的影响（Tseng
    et al., [2024](#bib.bib46)；Ashkboos et al., [2024](#bib.bib4)），但这种技术对 LLM 初始块中的预训练矩阵结构有不利影响（见
    [Figures 1](#S1.F1 "In 1 Introduction ‣ ReALLM: A general framework for LLM compression
    and fine-tuning") 和 [3](#S3.F3 "Figure 3 ‣ ReALLM: a new LLM format. ‣ 3 Method
    ‣ ReALLM: A general framework for LLM compression and fine-tuning")）。这是一个严重的瓶颈，因为这些初始块中的量化误差可能会在模型中传播。如
    [Figure 1](#S1.F1 "In 1 Introduction ‣ ReALLM: A general framework for LLM compression
    and fine-tuning") 所示，一些矩阵没有特定的模式，类似于随机高斯噪声夹杂着随机位置的异常值。为了处理潜在表示中的异常值，我们建议重新排列列以创建一些空间规律。这种策略旨在找到最有效的排列方式，以聚集异常值。Trukhanov
    和 Soloveychik（[2024](#bib.bib45)）最近详细阐述了一种行/列排列策略，该策略总结了具有相似范数的向量（即行或列的集合）。相比之下，对于
    ReALLM，我们建议排列列，使得相邻的列是“相似的”，而不仅仅是在同一超球面上。我们为此开发了一种基本但高效的方法：首先选择一个大小为 $128\times
    q$ 的块，并选择使其最小化的向量。然后，我们将邻近向量与块中第二位置的向量进行排列。该过程随后被迭代；更多细节见 [Algorithm 1](#algorithm1
    "In Quantization pre-processing. ‣ 3 Method ‣ ReALLM: A general framework for
    LLM compression and fine-tuning") 和 [Section A.3](#A1.SS3 "A.3 Permutations ‣
    Appendix A Appendix / supplemental material ‣ ReALLM: A general framework for
    LLM compression and fine-tuning")。请注意，排列的内存存储是微不足道的：对于一个 $q=4096$ 每坐标位的 LLM 矩阵。'
- en: Input : Matrix $w$ ;       $indx_{j}=get\_index\_nn(column_{j},w[:,j+1:q])$
    and $w[:,indx_{j}]$
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：矩阵 $w$； $indx_{j}=get\_index\_nn(column_{j},w[:,j+1:q])$ 和 $w[:,indx_{j}]$
- en: Algorithm 1 permutation function
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 排列函数
- en: 'ReALLM: a new LLM format.'
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReALLM：一种新的 LLM 格式。
- en: LLM standard formats represent LLM weights as a set of matrices encoded on $16$
    bits. Vector quantization (VQ) methods (Egiazarian et al.,, [2024](#bib.bib13);
    Tseng et al.,, [2024](#bib.bib46)) represent any matrix of size $p\times q$ bits
    and a vector dimension $d$ bits and a neural decoder model $\mathcal{D}_{\phi}$.
    This speeds up the decoding step compared to diffusion-based approaches (Wang
    et al.,, [2024](#bib.bib50); Soro et al.,, [2024](#bib.bib41)). Note that for
    ReALLM a decoder model has to be trained on LLM matrices, but this learning step
    is done once and for all. Additionally, the more we train and overfit, the better
    ReALLM becomes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 标准格式将 LLM 权重表示为一组 $16$ 位编码的矩阵。向量量化 (VQ) 方法 (Egiazarian et al., [2024](#bib.bib13);
    Tseng et al., [2024](#bib.bib46)) 将任何 $p\times q$ 位矩阵和 $d$ 位向量维度以及神经解码器模型 $\mathcal{D}_{\phi}$
    表示出来。这比基于扩散的方法 (Wang et al., [2024](#bib.bib50); Soro et al., [2024](#bib.bib41))
    加快了解码步骤。请注意，对于 ReALLM，需要在 LLM 矩阵上训练解码器模型，但这一学习步骤只需进行一次。此外，我们训练和过拟合的越多，ReALLM 的效果就越好。
- en: 'The set of hyper-parameters for ReALLM are: $r$ the number of parameters and
    the number of bits of the decoder. We have conducted extensive experiments to
    find suitable configurations; however, we were unable to test configurations with
    a large decoder size. For e.g., for small embeddings ($e_{0}e_{1}e_{2}<1024$ bits.
    Our GPU is unable to accommodate multiple LLM matrices in memory for ReALLM training,
    typically with size <math id="S3.SS0.SSS0.Px5.p2.9.m9.2" class="ltx_Math" alttext="n\times
    n;n></math> patches extracted from pre-trained LLM matrices, and we use the HNeRV
    Chen et al., ([2023](#bib.bib6)) autoencoder model. For more details on the practical
    aspect of decoder training, see [Section A.2](#A1.SS2 "A.2 Autoencoder computational
    limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM: A general
    framework for LLM compression and fine-tuning").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'ReALLM 的超参数集合为：$r$ 解码器的参数数量和位数。我们进行了广泛的实验以寻找合适的配置；然而，我们无法测试大解码器大小的配置。例如，对于小的嵌入
    ($e_{0}e_{1}e_{2}<1024$ 位)。我们的 GPU 无法在内存中容纳多个 LLM 矩阵用于 ReALLM 训练，通常大小 <math id="S3.SS0.SSS0.Px5.p2.9.m9.2"
    class="ltx_Math" alttext="n\times n;n></math> 从预训练的 LLM 矩阵中提取的补丁，我们使用 HNeRV Chen
    et al., ([2023](#bib.bib6)) 自编码器模型。有关解码器训练的实际细节，请参见[第 A.2 节](#A1.SS2 "A.2 Autoencoder
    computational limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM:
    A general framework for LLM compression and fine-tuning")。'
- en: We have experimentally discovered two sets of optimal combinations of hyperparameters
    that depend on the type and shape of the pre-trained matrix. Some pre-trained
    matrices, especially those closer to the input tokens, compress better with small
    latent representations ($e_{0}e_{1}e_{2}<1024$) with low bit precision ($b\ll
    8$).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验性地发现了两组最佳超参数组合，这些组合依赖于预训练矩阵的类型和形状。一些预训练矩阵，特别是那些更接近输入标记的矩阵，压缩效果更好，使用较小的潜在表示
    ($e_{0}e_{1}e_{2}<1024$) 和较低的比特精度 ($b\ll 8$)。
- en: '![Refer to caption](img/2a3b8c73103df94f28a4dac4b780901b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2a3b8c73103df94f28a4dac4b780901b.png)'
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mistral-7B (Jiang et al., [2023](#bib.bib22))
- en: '![Refer to caption](img/56a4889e163c5d185e777e5271277b5e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56a4889e163c5d185e777e5271277b5e.png)'
- en: (b) Llama2-7B (Touvron et al.,, [2023](#bib.bib44))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama2-7B (Touvron et al., [2023](#bib.bib44))
- en: 'Figure 3: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks. Quip# (Tseng et al.,, [2024](#bib.bib46)) does not take advantage of the
    structures in the first blocks.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：所有块中“Q”类型层的重建（Frobenius 范数）误差。Quip# (Tseng et al., [2024](#bib.bib46)) 未利用前几个块中的结构。
- en: 'In [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general
    framework for LLM compression and fine-tuning") ReALLM achieves the lowest Frobenius
    norm quantization error. We perform ablation experiments with this metric to decouple
    the effects of VQ and permutation preprocessing of ReALLM on the final performance.
    For example, in block $8$, while permutation alone (i.e. with SQ) leads to an
    error of $2.88$. Quip# rotates the matrices randomly, causing all patterns in
    the initial blocks to be lost.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在[图3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")中，ReALLM 实现了最低的 Frobenius 范数量化误差。我们用这个指标进行消融实验，以解耦
    VQ 和 ReALLM 的置换预处理对最终性能的影响。例如，在块 $8$ 中，仅置换（即带有 SQ）导致误差为 $2.88$。Quip# 随机旋转矩阵，导致初始块中的所有模式丢失。'
- en: Input : Number of end-to-end fine-tuning steps $T$), Number of weights in the
    decoder $c$, Rank $r$* do7       $B_{j}=\{W^{q},W^{k},W^{v},W^{o},W^{gate},W^{up},W^{down}\}[block=j]$
    ;11             $W^{l}_{j}=W^{l}_{j}-L1^{l}_{j}(L2^{l}_{j})^{t}$ /* with NF-normalization
    ([Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18)) */12            
    $codebook^{l}_{j}=Kmeans({\mathcal{E}_{\psi}}(W^{l}_{j}),b,d)$;14                        $dora^{l}_{j}=DoRA(W^{l}_{j},L1^{l}_{j},L2^{l}_{j})$* do19            
    Optimize $\{dora^{l}_{j},L1^{l}_{j},L2^{l}_{j}\}_{l\geq 0}$ with gradient descent
    ;26      27 end for
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '输入 : 端到端微调步骤数 $T$)，解码器中的权重数量 $c$，秩 $r$* do7 $B_{j}=\{W^{q},W^{k},W^{v},W^{o},W^{gate},W^{up},W^{down}\}[block=j]$
    ;11 $W^{l}_{j}=W^{l}_{j}-L1^{l}_{j}(L2^{l}_{j})^{t}$ /* 使用 NF-归一化 ([Dettmers et
    al., 2023a,](#bib.bib11) ; Guo et al., [2023](#bib.bib18)) */12 $codebook^{l}_{j}=Kmeans({\mathcal{E}_{\psi}}(W^{l}_{j}),b,d)$;14
    $dora^{l}_{j}=DoRA(W^{l}_{j},L1^{l}_{j},L2^{l}_{j})$* do19 优化 $\{dora^{l}_{j},L1^{l}_{j},L2^{l}_{j}\}_{l\geq
    0}$ 使用梯度下降 ;26 27 end for'
- en: Algorithm 2 Pseudo-code for ReALLM with block-wise and end-to-end fine-tuning
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 ReALLM 的伪代码，带有块级和端到端微调。
- en: 4 Experimental validation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验验证
- en: We test ReALLM on the LLaMA-2 (Touvron et al.,, [2023](#bib.bib44)) family models
    (with $7$ bits per coordinate. We partially reused code from the implementations
    of LQ-LoRA¹¹1https://github.com/HanGuo97/lq-lora/tree/main, AQLM ²²2https://github.com/Vahe1994/AQLM
    and HNeRV³³3https://github.com/haochen-rye/HNeRV. On an Nvidia A40 GPU (with $46$
    hours for a LLaMA2-7B model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LLaMA-2 (Touvron et al., [2023](#bib.bib44)) 家族模型上测试 ReALLM（每个坐标 $7$ 位）。我们部分重用了
    LQ-LoRA¹¹1https://github.com/HanGuo97/lq-lora/tree/main、AQLM ²²2https://github.com/Vahe1994/AQLM
    和 HNeRV³³3https://github.com/haochen-rye/HNeRV 的实现代码。在 Nvidia A40 GPU 上（LLaMA2-7B
    模型需要 $46$ 小时）。
- en: Language Generation Tasks.
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言生成任务。
- en: For continual language modeling, we train on a single partition of the C4 (Raffel
    et al.,, [2020](#bib.bib37)) dataset for half an epoch and use a sequence length
    of $4096$ (all except (Egiazarian et al.,, [2024](#bib.bib13)) follow this rule).
    Therefore, we use a sequence length of size $2048$ for both WikiText-2 (Merity
    et al.,, [2016](#bib.bib34)) and C4 evaluation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续语言建模，我们在 C4 (Raffel et al., [2020](#bib.bib37)) 数据集的单一分区上训练半个周期，并使用 $4096$
    的序列长度（除 (Egiazarian et al., [2024](#bib.bib13)) 外，所有遵循此规则）。因此，我们对 WikiText-2 (Merity
    et al., [2016](#bib.bib34)) 和 C4 评估使用 $2048$ 的序列长度。
- en: 'Our main baselines are LQ-LoRA (Guo et al.,, [2023](#bib.bib18)), Quip# (Tseng
    et al.,, [2024](#bib.bib46)), and AQLM (Egiazarian et al.,, [2024](#bib.bib13)).
    However, we also report the performance of popular quantization approaches GPTQ
    (Frantar et al.,, [2022](#bib.bib14)), AWQ (Lin et al.,, [2023](#bib.bib29)),
    Omniquant (Shao et al.,, [2023](#bib.bib39)), as well as the performance of recent
    work ApiQ (Liao and Monz,, [2024](#bib.bib28)) and QuaRot (Ashkboos et al.,, [2024](#bib.bib4)).
    In the results below, we present the target bits per parameter that takes into
    account quantized weights and include parameters kept in high precision (head
    layer, scales, codebooks, permutations in $16$ bits precision) similarly to the
    related work. The exact bit budget is detailed in [Table 5](#A1.T5 "In A.2 Autoencoder
    computational limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM:
    A general framework for LLM compression and fine-tuning") in the Appendix.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要基准是 LQ-LoRA (Guo et al., [2023](#bib.bib18))、Quip# (Tseng et al., [2024](#bib.bib46))
    和 AQLM (Egiazarian et al., [2024](#bib.bib13))。不过，我们还报告了流行的量化方法的性能，如 GPTQ (Frantar
    et al., [2022](#bib.bib14))、AWQ (Lin et al., [2023](#bib.bib29))、Omniquant (Shao
    et al., [2023](#bib.bib39))，以及最近的工作 ApiQ (Liao and Monz, [2024](#bib.bib28)) 和
    QuaRot (Ashkboos et al., [2024](#bib.bib4))。在下面的结果中，我们展示了每个参数的目标位数，考虑了量化权重，并包括保持高精度的参数（头层、缩放、代码本、$16$
    位精度的排列），类似于相关工作。确切的位预算详见附录中的[表 5](#A1.T5 "在 A.2 自编码器计算限制 ‣ 附录 A 附录 / 补充材料 ‣ ReALLM：LLM
    压缩和微调的一般框架")。
- en: In our experiments, following [Dettmers et al., 2023a](#bib.bib11) ; Guo et al.,
    ([2023](#bib.bib18)), we take a DoRA (Liu et al.,, [2024](#bib.bib31)) rank of
    $r=64$ to $(16,16,16)$ for $2000$, and a learning rate of $2\cdot e^{-5}$. As
    far as we know, we have also developed the first VQ code (available in the supplementary
    material) that makes efficient use of PyTorch’s “torch dispatch” functionality
    (Ansel et al.,, [2024](#bib.bib2)), which is known to be as fast as dedicated
    CUDA kernels (Guo et al.,, [2023](#bib.bib18)). This allows us to overload PyTorch
    operations to perform just-in-time dequantization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，按照[Dettmers et al., 2023a](#bib.bib11)；Guo et al., ([2023](#bib.bib18))的做法，我们采用DoRA
    (Liu et al.,, [2024](#bib.bib31))排名$r=64$，设置为$(16,16,16)$，训练$2000$步，学习率为$2\cdot
    e^{-5}$。据我们了解，我们还开发了第一个VQ代码（见补充材料），有效利用了PyTorch的“torch dispatch”功能（Ansel et al.,,
    [2024](#bib.bib2)），其速度与专用CUDA内核（Guo et al.,, [2023](#bib.bib18)）相当。这使我们能够重载PyTorch操作以进行即时去量化。
- en: 'In [Tables 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [2](#S4.T2
    "Table 2 ‣ Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A
    general framework for LLM compression and fine-tuning") we evaluate the perplexity
    of ReALLM on the respective validation datasets of C4 and WikiText-2 for a single
    run. During fine-tuning (on a single partition of the C4 dataset), we only update
    the DoRA components (scales and low-rank matrices). For each dataset, we provide
    three sets of results in [Table 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"):
    Perplexity without any fine-tuning (only low-rank and VQ autoencoder decomposition),
    perplexity with only block-wise fine-tuning, and perplexities with end-to-end
    fine-tuning (in addition to the block-wise fine-tuning process).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在[表1](#S4.T1 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")和[2](#S4.T2 "表2
    ‣ 语言生成任务。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")中，我们评估了ReALLM在C4和WikiText-2的验证数据集上的困惑度。微调过程中（在C4数据集的单一分区上），我们只更新DoRA组件（尺度和低秩矩阵）。对于每个数据集，我们在[表1](#S4.T1
    "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")中提供了三组结果：没有任何微调（仅低秩和VQ自编码器分解）的困惑度，仅块状微调的困惑度，以及端到端微调（加上块状微调过程）的困惑度。'
- en: 'Table 1: Perplexity $(\downarrow)$'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：困惑度 $(\downarrow)$
- en: '| Method | #bits | rank $r$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 排名 $r$ |'
- en: '| ReALLM (no fine-tuning) | $3$ | $6.21$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $3$ | $6.21$ |'
- en: '| ReALLM (block-wise) | $3$ | $6.01$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状训练) | $3$ | $6.01$ |'
- en: '| ReALLM (40% training) | $3$ | $5.80$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $3$ | $5.80$ |'
- en: '| ReALLM (full training) | $3$ | $5.79$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $3$ | $5.79$ |'
- en: '| ReALLM (no fine-tuning) | $3$ | $6.10$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $3$ | $6.10$ |'
- en: '| ReALLM (block-wise) | $3$ | $5.92$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状) | $3$ | $5.92$ |'
- en: '| ReALLM (40% training) | $3$ | $5.78$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $3$ | $5.78$ |'
- en: '| ReALLM (full training) | $3$ | $5.77$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $3$ | $5.77$ |'
- en: '| ReALLM (no fine-tuning) | $2$ | $51.74$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $2$ | $51.74$ |'
- en: '| ReALLM (block-wise 50 epochs) | $2$ | $16.95$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状50轮) | $2$ | $16.95$ |'
- en: '| ReALLM (block-wise 200 epochs) | $2$ | $8.31$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状200轮) | $2$ | $8.31$ |'
- en: '| ReALLM (40% training) | $2$ | $6.95$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $2$ | $6.95$ |'
- en: '| ReALLM (full training) | $2$ | $6.91$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $2$ | $6.91$ |'
- en: '| ReALLM (no fine-tuning) | $2$ | $40.85$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $2$ | $40.85$ |'
- en: '| ReALLM (block-wise 50 epochs) | $2$ | 15.74 | 12.08 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状50轮) | $2$ | 15.74 | 12.08 |'
- en: '| ReALLM (40% training) | $2$ | $6.74$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $2$ | $6.74$ |'
- en: '| ReALLM (full training) | $2$ | $6.69$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $2$ | $6.69$ |'
- en: 'Our *data-free* version of ReALLM (no fine-tuning; see [Table 1](#S4.T1 "In
    Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")) achieves state-of-the-art metrics for $3$
    has minimal effect on the final perplexity result, while halving the number of
    parameters that need to be tuned. Moreover, a larger VQ dimension $d=4$ bits are
    needed to store the codebook). Additional results for other models are available
    in the Appendix.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的*无数据*版本ReALLM（无微调；见[表1](#S4.T1 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")）在$3$的情况下达到了最先进的指标，对最终困惑度结果的影响极小，同时将需要调整的参数数量减少了一半。此外，存储代码本需要更大的VQ维度$d=4$位。其他模型的附加结果见附录。'
- en: 'Table 2: Perplexity $(\downarrow)$'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：困惑度 $(\downarrow)$
- en: '| Method | Number of bits | C4 $(\downarrow)$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | C4 $(\downarrow)$ |'
- en: '|  |  | 7B | 13B | 7B | 13B |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 7B | 13B | 7B | 13B |'
- en: '| LLaMA2 (Touvron et al.,, [2023](#bib.bib44)) | $16$ | $4.48$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 (Touvron et al., [2023](#bib.bib44)) | $16$ | $4.48$ |'
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $3$ | $5.42$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ (Frantar et al., [2022](#bib.bib14)) | $3$ | $5.42$ |'
- en: '| AWQ (Lin et al.,, [2023](#bib.bib29)) | $3$ | $5.32$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AWQ (Lin et al., [2023](#bib.bib29)) | $3$ | $5.32$ |'
- en: '| Omniquant (Shao et al.,, [2023](#bib.bib39)) | $3$ | $5.28$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant (Shao et al., [2023](#bib.bib39)) | $3$ | $5.28$ |'
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $3$ | $-$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LQ-LoRA (Guo et al., [2023](#bib.bib18)) | $3$ | $-$ |'
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $3$ | $5.13$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ (Li et al., [2023](#bib.bib27)) | $3$ | $5.13$ |'
- en: '| ApiQ[PTQ] (Liao and Monz,, [2024](#bib.bib28)) | $3$ | $5.18$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ApiQ[PTQ] (Liao and Monz, [2024](#bib.bib28)) | $3$ | $5.18$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $3$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | $3$ |'
- en: '| QuaRot[A16W3] (Ashkboos et al.,, [2024](#bib.bib4)) | $3$ | $5.37$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot[A16W3] (Ashkboos et al., [2024](#bib.bib4)) | $3$ | $5.37$ |'
- en: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $2$ | $7.69$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ (Li et al., [2023](#bib.bib27)) | $2$ | $7.69$ |'
- en: '| ApiQ (Liao and Monz,, [2024](#bib.bib28)) | $2$ | $6.29$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ApiQ (Liao and Monz, [2024](#bib.bib28)) | $2$ | $6.29$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $2$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | $2$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
- en: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
- en: 'In [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") we compare
    ReALLM with end-to-end fine-tuning, and the best performing PTQ approaches. All
    the methods cited in [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    also uses a calibration dataset. It is interesting to note that ReALLM with $2$
    bits precision) during the layer-wise fine-tuning. This does not only slow down
    the PTQ process (as gradients must be store for all weights in the given block),
    but it also means Quip# has to store learnable vectors and also quantized weights
    for *each* fine-tuning task.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [表 2](#S4.T2 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用的 LLM 压缩与微调框架") 中，我们将 ReALLM
    与端到端微调以及表现最佳的 PTQ 方法进行了比较。[表 2](#S4.T2 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用的 LLM
    压缩与微调框架") 中引用的所有方法也使用了校准数据集。有趣的是，ReALLM 在层级微调过程中使用 $2$ 位精度。这不仅会减慢 PTQ 过程（因为梯度必须存储在给定块中的所有权重中），还意味着
    Quip# 必须为*每个*微调任务存储可学习的向量和量化的权重。'
- en: 'Table 3: Accuracy $(\uparrow)$ in LM Eval (acc, not acc_norm).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LM Eval 中的准确率 $(\uparrow)$（acc，而非 acc_norm）。'
- en: '| Method | Size | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | Average
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 大小 | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | 平均值 |'
- en: '| LLaMA-2 | 7B | $16$ | $69.22$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 | 7B | $16$ | $69.22$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 7B | $2$ | $64.61$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | 7B | $2$ | $64.61$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 7B | $2$ | $64.89$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | 7B | $2$ | $64.89$ |'
- en: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
- en: '| LLaMA-2 | 13B | $16$ | $72.13$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 | 13B | $16$ | $72.13$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 13B | $3$ | $67.56$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | 13B | $3$ | $67.56$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 13B | $3$ | $69.13$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | 13B | $3$ | $69.13$ |'
- en: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
- en: Zero-Shot Tasks.
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Zero-Shot 任务。
- en: 'Following HuggingFace’s Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,
    and the literature (Frantar et al.,, [2022](#bib.bib14); Guo et al.,, [2023](#bib.bib18)),
    we also measure zero-shot accuracy on ARC (Clark et al.,, [2018](#bib.bib7)),
    PiQA (Tata and Patel,, [2003](#bib.bib42)), and Winogrande (Sakaguchi et al.,,
    [2021](#bib.bib38)), via the LM Evalaluation Harness (Gao et al.,, [2021](#bib.bib15)).
    We report results in [Table 3](#S4.T3 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"),
    and compute the average on the 4 mentioned tasks. For all LLM sizes, ReALLM provides
    a notable advantage (between $0.5$ bits) on the zero-shot tasks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '根据HuggingFace的Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard以及文献（Frantar等人，[2022](#bib.bib14)；Guo等人，[2023](#bib.bib18)），我们还测量了ARC（Clark等人，[2018](#bib.bib7)）、PiQA（Tata和Patel，[2003](#bib.bib42)）和Winogrande（Sakaguchi等人，[2021](#bib.bib38)）的零样本准确率，通过LM
    Evalaluation Harness（Gao等人，[2021](#bib.bib15)）。我们在[表3](#S4.T3 "In Language Generation
    Tasks. ‣ 4 Experimental validation ‣ ReALLM: A general framework for LLM compression
    and fine-tuning")中报告结果，并计算了4项任务的平均值。对于所有LLM尺寸，ReALLM在零样本任务上提供了显著优势（在$0.5$位之间）。'
- en: 5 Conclusion
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present ReALLM, a weight-only PTQ method that achieves state-of-the-art results
    on LLMs at $2$ GB of RAM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了ReALLM，这是一种仅限权重的PTQ方法，在$2$ GB RAM的LLMs上实现了最先进的结果。
- en: Large context sequence lengths result in large $KV$ matrices. We are currently
    studying how to adapt ReALLM to $KV$-cache quantization, and how to combine it
    with activation quantization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 大上下文序列长度会导致较大的$KV$矩阵。我们目前正在研究如何将ReALLM适应$KV$-缓存量化，并如何将其与激活量化结合。
- en: 6 Societal impact
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 社会影响
- en: This paper presents work whose goal is to advance the field of LLM compression
    and fine-tuning. There are many potential societal consequences of our work, in
    particular malicious usage of LLMs for spams or language generation on edge devices.
    However, this negative societal impact is not limited to ReALLM, but to the field
    of LLM in general.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了旨在推动LLM压缩和微调领域的工作。我们的工作有许多潜在的社会影响，特别是LLM在垃圾邮件或边缘设备语言生成中的恶意使用。然而，这种负面的社会影响不仅限于ReALLM，而是对LLM领域的普遍现象。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Almazrouei et al., (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic,
    Q., et al. (2023). The falcon series of open language models. arXiv preprint arXiv:2311.16867.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei等人，（2023）Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A.,
    Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic, Q.,
    等人。（2023）。Falcon系列开放语言模型。arXiv预印本arXiv:2311.16867。
- en: 'Ansel et al., (2024) Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A.,
    Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia,
    A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J.,
    Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano,
    M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso,
    M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang,
    X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G.,
    Wu, P., and Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic
    Python Bytecode Transformation and Graph Compilation. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (ASPLOS ’24). ACM.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansel等人，（2024）Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky,
    M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable,
    W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M.,
    Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang,
    Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim,
    M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W.,
    Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., 和Chintala,
    S.（2024）。PyTorch 2：通过动态Python字节码转换和图形编译加速机器学习。见第29届ACM国际编程语言和操作系统体系结构支持会议，第2卷（ASPLOS
    ’24）。ACM。
- en: 'Arthur et al., (2007) Arthur, D., Vassilvitskii, S., et al. (2007). k-means++:
    The advantages of careful seeding. In Soda, volume 7, pages 1027–1035.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arthur等人，（2007）Arthur, D., Vassilvitskii, S., 等人。（2007）。k-means++：精心初始化的优势。见Soda，第7卷，第1027–1035页。
- en: 'Ashkboos et al., (2024) Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B.,
    Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. (2024). Quarot: Outlier-free
    4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos等人，（2024）Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Jaggi,
    M., Alistarh, D., Hoefler, T., 和Hensman, J.（2024）。Quarot：在旋转LLMs中进行无异常4位推理。arXiv预印本arXiv:2404.00456。
- en: Bengio, (2013) Bengio, Y. (2013). Estimating or propagating gradients through
    stochastic neurons. arXiv preprint arXiv:1305.2982.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio (2013) Bengio, Y. (2013). 通过随机神经元估计或传播梯度。arXiv 预印本 arXiv:1305.2982。
- en: 'Chen et al., (2023) Chen, H., Gwilliam, M., Lim, S.-N., and Shrivastava, A.
    (2023). Hnerv: A hybrid neural representation for videos. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10270–10279.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2023) Chen, H., Gwilliam, M., Lim, S.-N., 和 Shrivastava, A. (2023).
    Hnerv: 一种用于视频的混合神经表示。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，第 10270–10279 页。'
- en: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
    Schoenick, C., 和 Tafjord, O. (2018). 觉得你已经解决了问答问题？试试 arc，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457。
- en: 'Courbariaux et al., (2015) Courbariaux, M., Bengio, Y., and David, J.-P. (2015).
    Binaryconnect: Training deep neural networks with binary weights during propagations.
    Advances in neural information processing systems, 28.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Courbariaux 等人 (2015) Courbariaux, M., Bengio, Y., 和 David, J.-P. (2015). Binaryconnect:
    在传播过程中训练具有二进制权重的深度神经网络。神经信息处理系统进展，28。'
- en: Dao et al., (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., and Ré, C. (2019).
    Learning fast algorithms for linear transforms using butterfly factorizations.
    In International conference on machine learning, pages 1517–1527\. PMLR.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等人 (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., 和 Ré, C. (2019). 使用蝶形分解学习快速算法进行线性变换。在国际机器学习会议上，第
    1517–1527 页。PMLR。
- en: 'Dettmers et al., (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. (2022). Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale.
    Advances in Neural Information Processing Systems, 35:30318–30332.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2022) Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L.
    (2022). Gpt3\. int8 (): 用于变换器的 8 位矩阵乘法。神经信息处理系统进展，35:30318–30332。'
- en: '(11) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023a).
    Qlora: Efficient finetuning of quantized llms. Advances in Neural Information
    Processing Systems, 36.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(11) Dettmers, T., Pagnoni, A., Holtzman, A., 和 Zettlemoyer, L. (2023a). Qlora:
    量化 LLM 的高效微调。神经信息处理系统进展，36。'
- en: '(12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. (2023b). Spqr: A
    sparse-quantized representation for near-lossless llm weight compression. In The
    Twelfth International Conference on Learning Representations.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., 和 Alistarh, D. (2023b). Spqr: 一种用于近乎无损的
    LLM 权重压缩的稀疏量化表示。在第十二届国际学习表示大会上。'
- en: Egiazarian et al., (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., and Alistarh, D. (2024). Extreme compression of large language
    models via additive quantization. arXiv preprint arXiv:2401.06118.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等人 (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., 和 Alistarh, D. (2024). 通过加法量化对大型语言模型进行极限压缩。arXiv 预印本 arXiv:2401.06118。
- en: 'Frantar et al., (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. (2022). Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 (2022) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. (2022).
    Gptq: 生成预训练变换器的准确后训练量化。arXiv 预印本 arXiv:2210.17323。'
- en: Gao et al., (2021) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for
    few-shot language model evaluation.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2021) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. (2021). 少样本语言模型评估框架。
- en: Gholami et al., (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    and Keutzer, K. (2022). A survey of quantization methods for efficient neural
    network inference. In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等人 (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., 和
    Keutzer, K. (2022). 高效神经网络推断的量化方法调查。在《低功耗计算机视觉》中，第 291–326 页。Chapman and Hall/CRC。
- en: 'Guo et al., (2021) Guo, D., Rush, A. M., and Kim, Y. (2021). Parameter-efficient
    transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等, (2021) Guo, D., Rush, A. M., 和 Kim, Y. (2021). 具有差异修剪的参数高效迁移学习。见于第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集，第4884–4896页。
- en: 'Guo et al., (2023) Guo, H., Greengard, P., Xing, E., and Kim, Y. (2023). Lq-lora:
    Low-rank plus quantized matrix decomposition for efficient language model finetuning.
    In The Twelfth International Conference on Learning Representations.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等, (2023) Guo, H., Greengard, P., Xing, E., 和 Kim, Y. (2023). Lq-lora:
    低秩加量化矩阵分解用于高效语言模型微调。第十二届国际学习表征会议。'
- en: Han et al., (2015) Han, S., Pool, J., Tran, J., and Dally, W. (2015). Learning
    both weights and connections for efficient neural network. Advances in neural
    information processing systems, 28.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等, (2015) Han, S., Pool, J., Tran, J., 和 Dally, W. (2015). 为高效神经网络学习权重和连接。神经信息处理系统进展,
    28。
- en: 'Hooper et al., (2024) Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W.,
    Shao, Y. S., Keutzer, K., and Gholami, A. (2024). Kvquant: Towards 10 million
    context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooper 等, (2024) Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao,
    Y. S., Keutzer, K., 和 Gholami, A. (2024). Kvquant: 通过KV缓存量化实现1000万上下文长度的LLM推断。arXiv
    预印本 arXiv:2401.18079。'
- en: 'Hu et al., (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., et al. (2021). Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等, (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., 等. (2021). Lora: 大语言模型的低秩适配。国际学习表征会议。'
- en: Jiang et al., (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. (2023). Mistral 7b. arXiv preprint arXiv:2310.06825.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等, (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等.
    (2023). Mistral 7b。arXiv 预印本 arXiv:2310.06825。
- en: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., and Lee,
    D. (2023a). Memory-efficient fine-tuning of compressed large language models via
    sub-4-bit integer quantization. arXiv preprint arXiv:2305.14152.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., 和 Lee,
    D. (2023a). 通过子4位整数量化的压缩大语言模型的内存高效微调。arXiv 预印本 arXiv:2305.14152。
- en: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., and Keutzer, K. (2023b). Squeezellm: Dense-and-sparse quantization. arXiv
    preprint arXiv:2306.07629.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., 和 Keutzer, K. (2023b). Squeezellm: 密集与稀疏量化。arXiv 预印本 arXiv:2306.07629。'
- en: 'Kwan et al., (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., and Bull, D.
    (2024). Hinerv: Video compression with hierarchical encoding-based neural representation.
    Advances in Neural Information Processing Systems, 36.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwan 等, (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., 和 Bull, D. (2024).
    Hinerv: 基于层次编码的神经表示视频压缩。神经信息处理系统进展, 36。'
- en: 'Li and Liang, (2021) Li, X. L. and Liang, P. (2021). Prefix-tuning: Optimizing
    continuous prompts for generation. In Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang, (2021) Li, X. L. 和 Liang, P. (2021). Prefix-tuning: 优化生成的连续提示。见于第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集，第4582–4597页。'
- en: 'Li et al., (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen,
    W., and Zhao, T. (2023). Loftq: Lora-fine-tuning-aware quantization for large
    language models. In The Twelfth International Conference on Learning Representations.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等, (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen, W.,
    和 Zhao, T. (2023). Loftq: 适用于大语言模型的Lora微调感知量化。第十二届国际学习表征会议。'
- en: 'Liao and Monz, (2024) Liao, B. and Monz, C. (2024). Apiq: Finetuning of 2-bit
    quantized large language model. arXiv preprint arXiv:2402.05147.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liao 和 Monz, (2024) Liao, B. 和 Monz, C. (2024). Apiq: 2位量化大语言模型的微调。arXiv 预印本
    arXiv:2402.05147。'
- en: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. (2023). Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等, (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han, S. (2023).
    Awq: 激活感知权重量化用于LLM压缩和加速。arXiv 预印本 arXiv:2306.00978。'
- en: 'Liu et al., (2023) Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang,
    B. (2023). Qllm: Accurate and efficient low-bitwidth quantization for large language
    models. In The Twelfth International Conference on Learning Representations.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., 和 Zhuang, B.（2023）。Qllm：针对大型语言模型的准确高效低位宽量化。在第十二届国际学习表示大会上。
- en: 'Liu et al., (2024) Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., and Chen, M.-H. (2024). Dora: Weight-decomposed low-rank adaptation.
    arXiv preprint arXiv:2402.09353.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024）Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., 和 Chen, M.-H.（2024）。Dora：权重分解的低秩适应。arXiv 预印本 arXiv:2402.09353。
- en: Liu et al., (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell,
    T., and Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 11976–11986.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., 和 Xie,
    S.（2022）。2020 年代的卷积网络。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页面 11976–11986。
- en: 'Liu et al., (2020) Liu, Z., Shen, Z., Savvides, M., and Cheng, K.-T. (2020).
    Reactnet: Towards precise binary neural network with generalized activation functions.
    In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XIV 16, pages 143–159\. Springer.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2020）Liu, Z., Shen, Z., Savvides, M., 和 Cheng, K.-T.（2020）。Reactnet：朝着具有广义激活函数的精确二进制神经网络前进。在计算机视觉–ECCV
    2020：第 16 届欧洲会议，英国格拉斯哥，2020 年 8 月 23–28 日，论文集，第 XIV 部分，第 16 页，页面 143–159。Springer。
- en: Merity et al., (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016).
    Pointer sentinel mixture models. In International Conference on Learning Representations.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等（2016）Merity, S., Xiong, C., Bradbury, J., 和 Socher, R.（2016）。指针哨兵混合模型。在国际学习表示大会上。
- en: Nrusimha et al., (2024) Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda,
    R., and Kim, Y. (2024). Mitigating the impact of outlier channels for language
    model quantization with activation regularization. arXiv preprint arXiv:2404.03605.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nrusimha 等（2024）Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda, R.,
    和 Kim, Y.（2024）。通过激活正则化缓解离群通道对语言模型量化的影响。arXiv 预印本 arXiv:2404.03605。
- en: 'Park et al., (2022) Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B.,
    Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). Lut-gemm: Quantized matrix
    multiplication based on luts for efficient inference in large-scale generative
    language models. arXiv preprint arXiv:2206.09557.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2022）Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B., Kwon, S.
    J., Kim, B., Lee, Y., 和 Lee, D.（2022）。Lut-gemm：基于 LUT 的量化矩阵乘法，用于大规模生成语言模型的高效推理。arXiv
    预印本 arXiv:2206.09557。
- en: Raffel et al., (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits
    of transfer learning with a unified text-to-text transformer. Journal of machine
    learning research, 21(140):1–67.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J.（2020）。使用统一的文本到文本变换器探索迁移学习的极限。《机器学习研究期刊》，21(140)：1–67。
- en: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等（2021）Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y.（2021）。Winogrande：大规模对抗性
    Winograd 模式挑战。《ACM 通讯》，64(9)：99–106。
- en: 'Shao et al., (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2023). Omniquant: Omnidirectionally
    calibrated quantization for large language models. In The Twelfth International
    Conference on Learning Representations.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等（2023）Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang,
    K., Gao, P., Qiao, Y., 和 Luo, P.（2023）。Omniquant：大型语言模型的全方位标定量化。在第十二届国际学习表示大会上。
- en: Shi et al., (2016) Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P.,
    Bishop, R., Rueckert, D., and Wang, Z. (2016). Real-time single image and video
    super-resolution using an efficient sub-pixel convolutional neural network. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1874–1883.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等（2016）Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P., Bishop,
    R., Rueckert, D., 和 Wang, Z.（2016）。使用高效的亚像素卷积神经网络进行实时单图像和视频超分辨率。在 IEEE 计算机视觉与模式识别会议论文集中，页面
    1874–1883。
- en: Soro et al., (2024) Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., and
    Hwang, S. J. (2024). Diffusion-based neural network weights generation. arXiv
    preprint arXiv:2402.18153.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soro 等（2024）Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., 和 Hwang,
    S. J.（2024）。基于扩散的神经网络权重生成。arXiv 预印本 arXiv:2402.18153。
- en: 'Tata and Patel, (2003) Tata, S. and Patel, J. M. (2003). Piqa: An algebra for
    querying protein data sets. In 15th International Conference on Scientific and
    Statistical Database Management, 2003., pages 141–150\. IEEE.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tata和Patel，（2003）Tata, S. 和 Patel, J. M.（2003）。Piqa: 查询蛋白质数据集的代数。第15届国际科学与统计数据库管理会议，2003，第141–150页。IEEE。'
- en: 'Team et al., (2024) Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju,
    S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024).
    Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team等人，（2024）Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S.,
    Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., 等。（2024）。Gemma: 基于双子座研究和技术的开放模型。arXiv预印本
    arXiv:2403.08295。'
- en: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人，（2023）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等。（2023）。Llama:
    开放而高效的基础语言模型。arXiv预印本 arXiv:2302.13971。'
- en: Trukhanov and Soloveychik, (2024) Trukhanov, N. and Soloveychik, I. (2024).
    Accurate block quantization in llms with outliers. arXiv preprint arXiv:2403.20137.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trukhanov和Soloveychik，（2024）Trukhanov, N. 和 Soloveychik, I.（2024）。带异常值的LLM中准确的块量化。arXiv预印本
    arXiv:2403.20137。
- en: 'Tseng et al., (2024) Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa,
    C. (2024). Quip#: Even better llm quantization with hadamard incoherence and lattice
    codebooks. arXiv preprint arXiv:2402.04396.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng等人，（2024）Tseng, A., Chee, J., Sun, Q., Kuleshov, V., 和 De Sa, C.（2024）。Quip#:
    通过Hadamard不相干性和晶格代码本改进的llm量化。arXiv预印本 arXiv:2402.04396。'
- en: Van Den Oord et al., (2017) Van Den Oord, A., Vinyals, O., et al. (2017). Neural
    discrete representation learning. Advances in neural information processing systems,
    30.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Den Oord等人，（2017）Van Den Oord, A., Vinyals, O., 等。（2017）。神经离散表示学习。神经信息处理系统进展，30。
- en: Vaswani et al., (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all
    you need. Advances in neural information processing systems, 30.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人，（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I.（2017）。注意力机制就是你所需的一切。神经信息处理系统进展，30。
- en: Viazovska, (2017) Viazovska, M. S. (2017). The sphere packing problem in dimension
    8. Annals of mathematics, pages 991–1015.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viazovska，（2017）Viazovska, M. S.（2017）。维度8的球体堆积问题。数学年刊，第991–1015页。
- en: Wang et al., (2024) Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu,
    Z., and You, Y. (2024). Neural network diffusion. arXiv preprint arXiv:2402.13144.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人，（2024）Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu, Z., 和 You,
    Y.（2024）。神经网络扩散。arXiv预印本 arXiv:2402.13144。
- en: 'Xiao et al., (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. (2023). Smoothquant: Accurate and efficient post-training quantization
    for large language models. In International Conference on Machine Learning, pages
    38087–38099\. PMLR.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人，（2023）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.（2023）。Smoothquant:
    大型语言模型的准确且高效的后训练量化。在国际机器学习会议上，第38087–38099页。PMLR。'
- en: 'Yao et al., (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C.,
    and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等人，（2022）Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., 和 He,
    Y.（2022）。Zeroquant: 大规模变换器的高效且经济的后训练量化。神经信息处理系统进展，35:27168–27183。'
- en: Appendix A Appendix / supplemental material
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录/补充材料
- en: A.1 Structures in pre-trained matrices
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 预训练矩阵中的结构
- en: 'Interestingly, the blocks that show some visual structures in LLaMA and Mistral
    models are not the same for Gemma LLMs. For instance in [Figure 4](#A1.F4 "In
    A.1 Structures in pre-trained matrices ‣ Appendix A Appendix / supplemental material
    ‣ ReALLM: A general framework for LLM compression and fine-tuning"), we can see
    that Gemma2b (Team et al.,, [2024](#bib.bib43))’s matrices keep some internal
    patterns in all blocks, not only at the very first blocks. Note this has no negative
    impact on ReALLM, as the shape of the encoder is experimentally adapted to each
    block.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，LLaMA和Mistral模型中显示一些视觉结构的块与Gemma LLMs不同。例如在[图4](#A1.F4 "在A.1 预训练矩阵中的结构
    ‣ 附录A附加材料 ‣ ReALLM: 一般的LLM压缩和微调框架")中，我们可以看到Gemma2b（Team等人，[2024](#bib.bib43)）的矩阵在所有块中保留了一些内部模式，而不仅仅是在最初的块中。请注意，这对ReALLM没有负面影响，因为编码器的形状会根据每个块进行实验性适配。'
- en: '![Refer to caption](img/bf9fe57040bc062c570f73b31d6925da.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bf9fe57040bc062c570f73b31d6925da.png)'
- en: 'Figure 4: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks of Gemma2b LLM.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Gemma2b LLM 所有块的“Q”类型层的重构（Frobenius 范数）误差。
- en: A.2 Autoencoder computational limitations
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 自编码器计算限制
- en: Our GPU can not directly work on LLM pre-trained matrices with large sizes (typically
    of shape $4096\times 4096$ bits using straight through estimator Bengio, ([2013](#bib.bib5)).
    We also tested a post training quantization method where the weight of the decoder
    are quantized with a round to nearest (RTN) approache, at the end of the decoder
    training steps.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 GPU 不能直接处理具有大尺寸的 LLM 预训练矩阵（通常为 $4096\times 4096$ 位，使用直通估计器 Bengio，[2013](#bib.bib5)）。我们还测试了一种后训练量化方法，其中解码器的权重在解码器训练步骤结束时采用四舍五入到最近（RTN）的方法进行量化。
- en: 'Table 4: Reconstruction (Frobenius norm) error for layer of type “Q” inside
    the first block of Mistral-7b model, for patches of size $512\times 512$, and
    a varying quantization strategy (during the decoder training, i.e. QAT, or after
    the training, i.e. PTQ).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：Mistral-7b 模型中第一块内的“Q”类型层的重构（Frobenius 范数）误差，适用于大小为 $512\times 512$ 的补丁，以及不同的量化策略（在解码器训练期间，即
    QAT，或在训练后，即 PTQ）。
- en: '| Error | # parameters c ($\times 10^{6}$ | bit budget | quantization |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 错误 | 参数数量 c ($\times 10^{6}$ | 位预算 | 量化 |'
- en: '| $0.84$ | NF3(Guo et al.,, [2023](#bib.bib18)) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $0.84$ | NF3（Guo 等，[2023](#bib.bib18)） |'
- en: '| $1.78$ | PTQ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| $1.78$ | PTQ |'
- en: '| $1.19$ | PTQ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| $1.19$ | PTQ |'
- en: '| $1.61$ | QAT |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| $1.61$ | QAT |'
- en: '| $1.24$ | QAT |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| $1.24$ | QAT |'
- en: '| $0.69$ | QAT |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| $0.69$ | QAT |'
- en: We vary the number of parameters $c$ and their respective bit precision $b_{\phi}$),
    ReALLM yields a smaller quantization error compared to the scalar quantization
    NF3 ([Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改变参数数量 $c$ 及其相应的位精度 $b_{\phi}$，与标量量化 NF3（[Dettmers 等，2023a,](#bib.bib11)；Guo
    等，[2023](#bib.bib18)）相比，ReALLM 产生了更小的量化误差。
- en: 'Table 5: Comparison of several LLM format for $m$ parameters trained on $b_{\phi}$.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：比较多种 LLM 格式在 $m$ 参数上训练的 $b_{\phi}$。
- en: '| Method | LoRA | VQ only (like AQLM) | ReALLM |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LoRA | 仅 VQ（如 AQLM） | ReALLM |'
- en: '| Matrix representation | $(p\times q)\cdot 16$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 矩阵表示 | $(p\times q)\cdot 16$ |'
- en: '| Codebook | $-$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 代码本 | $-$ |'
- en: '| Decoder | $-$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 解码器 | $-$ |'
- en: '| Low-rank | $(2\times r\times\min(p,q))\cdot 16$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 低秩 | $(2\times r\times\min(p,q))\cdot 16$ |'
- en: '| Total bit cost | $16(pq+2r\min(p,q))\cdot m$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 总位成本 | $16(pq+2r\min(p,q))\cdot m$ |'
- en: 'Table 6: Quantization and fine-tuning approaches as particular case of ReALLM (with
    a rank $r$.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：量化和微调方法作为 ReALLM 的特例（带有一个排名 $r$）。
- en: '| Method | rank $r$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 排名 $r$ |'
- en: '| LoRA (Hu et al.,, [2021](#bib.bib21)) | $64$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| LoRA（Hu 等，[2021](#bib.bib21)） | $64$ |'
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $0$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ（Frantar 等，[2022](#bib.bib14)） | $0$ |'
- en: '| QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) | $64$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA（[Dettmers 等，2023a,](#bib.bib11)） | $64$ |'
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $64$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| LQ-LoRA（Guo 等，[2023](#bib.bib18)） | $64$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $0$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Quip#（Tseng 等，[2024](#bib.bib46)） | $0$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $0$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| AQLM（Egiazarian 等，[2024](#bib.bib13)） | $0$ |'
- en: '| ReALLM | $64$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $64$ |'
- en: A.3 Permutations
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 排列
- en: In ReALLM, we compute permutations on sets of vectors in dimension $128$. We
    could work with smaller blocks, but it induces more memory dedicated to the permutation
    storage (one permutation for each block).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ReALLM 中，我们在维度为 $128$ 的向量集上计算排列。我们可以使用更小的块，但这会导致更多的内存用于存储排列（每个块一个排列）。
- en: 'We start from the first vector (i.e. the first column of the initial matrix
    shrunk to a dimension $d=128$ vectors. The process is then iterated. Details are
    given in [Algorithm 1](#algorithm1 "In Quantization pre-processing. ‣ 3 Method
    ‣ ReALLM: A general framework for LLM compression and fine-tuning").'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从第一个向量（即初始矩阵的第一列，缩小到维度 $d=128$ 向量）开始。然后迭代这个过程。详细信息见 [算法 1](#algorithm1 "在量化预处理。
    ‣ 3 方法 ‣ ReALLM：用于 LLM 压缩和微调的通用框架")。
- en: A.4 Broader impacts and Safeguards
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 更广泛的影响与保障
- en: Our computing unit seriously restricts the size of the decoder models we can
    train. We are not able to train one decoder model for a given LLM, but we work
    layer-wise and train a single decoder model for all patches extracted from the
    given layer. This layer-wise training forms the main limitation of ReALLM w.r.t. standard
    post-training quantization methods, such as round to nearest (RTN).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算单元严重限制了我们可以训练的解码器模型的大小。我们不能为特定的 LLM 训练一个解码器模型，而是逐层工作，并为从给定层提取的所有补丁训练一个解码器模型。这种逐层训练是
    ReALLM 相对于标准后训练量化方法（如四舍五入到最近（RTN））的主要限制。
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了旨在推动机器学习领域发展的工作。我们的工作具有许多潜在的社会影响，但我们认为没有必要在此特别突出。
- en: 'Table 7: Perplexity $(\downarrow)$'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：困惑度 $(\downarrow)$
- en: '| Method | #bits | rank $r$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #bits | 排名 $r$ |'
- en: '| ReALLM (no fine-tuning) | $3$ | $5.27$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（无微调） | $3$ | $5.27$ |'
- en: '| ReALLM (30% training) | $3$ | $5.14$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（30% 训练） | $3$ | $5.14$ |'
- en: '| ReALLM (no fine-tuning) | $2$ | $8.15$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（无微调） | $2$ | $8.15$ |'
- en: '| ReALLM (10% training) | $2$ | $5.99$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（10% 训练） | $2$ | $5.99$ |'
