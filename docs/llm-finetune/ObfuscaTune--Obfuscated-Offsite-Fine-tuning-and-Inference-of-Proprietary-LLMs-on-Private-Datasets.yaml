- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs
    on Private Datasets'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02960](https://ar5iv.labs.arxiv.org/html/2407.02960)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ahmed Frikha  Nassim Walha
  prefs: []
  type: TYPE_NORMAL
- en: Ricardo Mendes  Krishna Kanth Nakka  Xue Jiang Xuebing Zhou   Huawei Munich
    Research Center
  prefs: []
  type: TYPE_NORMAL
- en: ahmed.frikha1@huawei.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work addresses the timely yet underexplored problem of performing inference
    and finetuning of a proprietary LLM owned by a model provider entity on the confidential/private
    data of another data owner entity, in a way that ensures the confidentiality of
    both the model and the data. Hereby, the finetuning is conducted offsite, i.e.,
    on the computation infrastructure of a third-party cloud provider. We tackle this
    problem by proposing *ObfuscaTune*, a novel, efficient and fully utility-preserving
    approach that combines a simple yet effective obfuscation technique with an efficient
    usage of confidential computing (only $~{}5\%$ of the model parameters are placed
    on TEE). We empirically demonstrate the effectiveness of *ObfuscaTune* by validating
    it on GPT-2 models with different sizes on four NLP benchmark datasets. Finally,
    we compare to a naive version of our approach to highlight the necessity of using
    random matrices with low condition numbers in our approach to reduce errors induced
    by the obfuscation.
  prefs: []
  type: TYPE_NORMAL
- en: '*ObfuscaTune*: Obfuscated Offsite Fine-tuning and Inference of Proprietary
    LLMs on Private Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: Ahmed Frikha  Nassim Walha Ricardo Mendes  Krishna Kanth Nakka  Xue Jiang Xuebing
    Zhou   Huawei Munich Research Center ahmed.frikha1@huawei.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) such as GPT-4 Achiam et al. ([2023](#bib.bib1))
    are increasingly used due to their state-of-the-art performance in diverse tasks
    and productivity benefits Noy and Zhang ([2023](#bib.bib23)). While LLMs excel
    in zero-shot and few-shot predictions with in-context learning Mann et al. ([2020](#bib.bib20)),
    finetuning them on domain-specific data can significantly outperform foundation
    models in tasks like chip designThakur et al. ([2023](#bib.bib26)); Wu et al.
    ([2024](#bib.bib29)); Liu et al. ([2023](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: Model providers keep their proprietary models private due to the exorbitant
    costs of training them¹¹1Training GPT-4 costed more than $\$100$M Knight ([2023](#bib.bib12)).
    To enable their users to customize or apply the proprietary models to their data,
    model owners provide finetuning and inference services, e.g., OpenAI finetuning
    API²²2https://platform.openai.com/docs/guides/fine-tuning and GitHub Copilot³³3https://docs.github.com/en/copilot
    respectively. Hereby, the users have to share their data with the model owners
    to use these services. Due to concerns of privacy leakage and competitive disadvantage,
    several users and commercial entities are not willing to share their private or
    confidential data. For e.g., Samsung banned the usage of ChatGPT after sensitive
    code was leaked Ray ([2023](#bib.bib25)). Hence, approaches that enable the inference
    and finetuning of proprietary LLMs of one stakeholder on the confidential/private
    data of another stakeholder in a privacy-preserving way are crucially needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the following requirement that potential methods addressing this
    problem must fulfill: (a) Model confidentiality: prevent leakage of the proprietary
    model parameters, (b) Data confidentiality: prevent data leakage, (c) Utility:
    the performance and results of the inference and finetuning should be comparable
    with and without protection, (d) Efficiency: the computational time, memory footprint
    and communication should remain acceptable. To the best of our knowledge, no prior
    work fulfill all of these requirements simultaneously. In the following, we discuss
    different categories of prior works.'
  prefs: []
  type: TYPE_NORMAL
- en: Prior approaches based on differential privacy (DP) for inference Igamberdiev
    and Habernal ([2023](#bib.bib10)); Majmudar et al. ([2022](#bib.bib19)) and finetuning
    Yu et al. ([2021](#bib.bib31)) focus on protecting the data. However, they do
    not provide any protection for the model parameters and incur significant utility
    losses (Req. (a) and (c) are not fulfilled). Another line of work uses cryptographic
    techniques, e.g., multi-party computation (MPC) and homomorphic encryption (HE)
    Li et al. ([2022](#bib.bib14)); Liu and Liu ([2023](#bib.bib17)). While the confidentiality
    of both the model and the data can be ensured, their substantial slowdown and
    communication costs are not suitable for real-time applications (Req. (d) is not
    fulfilled). Another proposal Xiao et al. ([2023](#bib.bib30)) considers sending
    a distilled version of the model to the client where adapter layers are finetuned
    on the confidential data. At inference time, the finetuned adapter are used in
    combination with the proprietary model on the server side. This approach does
    not protect inference data and leads to utility losses of up to $6\%$ (Req. (b)
    and (c) are not fulfilled). The closest approach to the present work combines
    Trusted Execution Environments (TEE) with a lightweight encryption to address
    federated learning settings Huang et al. ([2024](#bib.bib9)). However, such proposal
    protects only the finetuned LoRA parameters by using the TEE and deploys the proprietary
    LLM on the client-side fully or partially (Req. (a) is not fulfilled).
  prefs: []
  type: TYPE_NORMAL
- en: Our contribution in the present work is threefold. First, we propose *ObfuscaTune*,
    a novel and efficient approach that combines TEE with a simple yet effective obfuscation
    technique. Our proposed approach enables finetuning and inference of LLMs in a
    way that preserves the confidentiality of the model and the data with no utility
    loss and acceptable efficiency loss, fulfilling all aforementioned requirements.
    Second, we empirically demonstrate the effectiveness of our method by validating
    it on GPT-2 models with different sizes on four NLP benchmark datasets. Hereby,
    only $5\%$ of the model parameters are placed on TEE. Finally, we highlight the
    necessity of our obfuscation technique by comparing it to a naive obfuscation
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We consider a problem setting involving three stakeholders: the model provider,
    the data owner and the cloud provider. The objective is to perform inference and
    finetuning of the proprietary LLM of the model provider on the confidential/private
    data of the data owner, in a way that ensures the confidentiality of both the
    model and the data. Due to the high computation and hardware costs required, we
    assume that the finetuning and/or inference is performed offsite, i.e., on the
    computational infrastructure of the cloud provider. We assume that the cloud provider
    is honest-but-curious, i.e., they will perform their task correctly but will try
    to find extra information about the other parties assets and data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this problem, we propose *ObfuscaTune*, an approach that addresses
    this problem by combining TEE and a simple yet effective obfuscation technique,
    ensuring model and data confidentiality while preserving utility. Following prior
    works, we consider the TEE as an isolated secure zone on a potentially adversary
    host where the data, code and computation processes used are inaccessible from
    outside Hou et al. ([2021](#bib.bib7)); Huang et al. ([2024](#bib.bib9)). Figure [1](#A1.F1
    "Figure 1 ‣ Appendix A Hyperparameters ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning
    and Inference of Proprietary LLMs on Private Datasets")⁴⁴4Will be part of the
    additional page in the camera ready version upon paper acceptance. presents an
    overview of the *ObfuscaTune* approach, which we detail next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model protection is ensured as follows: the model provider sends the proprietary
    model to the TEE on the cloud provider infrastructure. Within the TEE, the highly
    parameterized attention and MLP layers are protected using our obfuscation technique
    that we detail later and then sent outside the TEE. Since large models do not
    fit inside the TEE, the model layers can be sent there batchwise to be protected
    before leaving it. The remaining low-parameterized layers, e.g., the input, output,
    normalization and dropout layers, are kept on the TEE. After these steps, all
    model parameters are protected, either by TEE or by the obfuscation, and the majority
    of model parameters are outside of the TEE. We note that the TEE is controlled
    by authentication that ensures that only the data owner can query the model. This
    prevents the cloud provider from querying the model to perform model stealing
    Carlini et al. ([2024](#bib.bib5)) or embedding inversion attacks Li et al. ([2023](#bib.bib15));
    Morris et al. ([2023](#bib.bib22)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data protection in *ObfuscaTune*is conducted as follows: The data owner
    sends an encrypted batch of data directly to the TEE where it is first decrypted
    and then embedded using the model input layer. The resulting embedding is protected
    by our obfuscation method before leaving the TEE. The text tokenization can be
    conducted either before or after transmitting the data on the data owner side
    or in the TEE, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The obfuscated feedforward pass through one transformer block is executed as
    follows: Outside the TEE, the obfuscated data embedding is passed through the
    obfuscated model layers yielding an obfuscated intermediate embedding that is
    sent back to the TEE. The latter is then de-obfuscated and passed through the
    corresponding model layers on the TEE, depending on the model architecture. Subsequently,
    the resulting embedding is obfuscated again and leaves the TEE to be fed to the
    next transformer block. Finally, the output layer is applied in the TEE and the
    model output is sent back to the data owner (inference case) or used to computed
    the loss on the TEE and perform backpropagation and parameter updates (finetuning
    case).'
  prefs: []
  type: TYPE_NORMAL
- en: Our obfuscation method obfuscates the model parameters and data embeddings by
    multiplying them with random matrices that minimize numerical errors. We begin
    by introducing the obfuscation method and later explain how we limit the numerical
    errors. Let’s consider a multi-head attention layer and first focus on a single
    attention head with key, query, value layers parameterized by $W_{k}$ as its input.
    We obfuscate the embedding $X$, $W_{q}$, $W_{q}^{*}$, $W_{q}^{*}$ and $V$, of
    the original non-obfuscated operations (Eq. 1-3). All obfuscation operations are
    applied inside the TEE. The remaining aforementioned operations are performed
    outside of the TEE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output $H$ that are obfuscated by another randomly generated random matrix
    $R_{b}$. The bias term of this last projection layer has to be added after de-obfuscation
    and is therefore kept unobfuscated on the TEE. The obfuscation of the MLP layers
    of the proprietary LLM is conducted in an analogous manner to the obfuscation
    of the multi-head attention layers. Fig. [2](#A1.F2 "Figure 2 ‣ Appendix A Hyperparameters
    ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs
    on Private Datasets") shows an overview of all operations conducted in GPT-2 Radford
    et al. ([2019](#bib.bib24)) with annotations of which operations are performed
    inside or outside the TEE and on obfuscated or de-obfuscated variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that using the same or different random matrices to obfuscate different
    transformer blocks does not impact our method. Note that the layers that are kept
    on TEE involve non-linearities, e.g., layer-norm, and therefore cannot be applied
    to obfuscated variables since the subsequent de-obfuscation would not yield the
    same result. These layers have a low number of parameters compared to the attention
    and MLP layers placed outside of TEE, e.g., only ca. $5\%$ are obfuscated and
    placed outside of TEE, in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle K$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle V$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle O^{*}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle O$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Note that all data embeddings and parameters that are accessible to the adversary,
    i.e., the ones that are processed outside of the TEE, are obfuscated, except for
    the intermediate embeddings $Q$ and the model parameters $W_{k}$, while having
    access to only 4 equations involving them (Eq. 1-3 and Eq. 5). Hence, it is not
    possible to compute them analytically. For an additional layer of protection,
    model obfuscation with new randomly generated matrices can be conducted regularly,
    e.g., every day or every hour, although we believe this is not required. The model
    obfuscation can be performed very efficiently (ca. 10 seconds on a middle range
    GPU for a GPT2-XL model).
  prefs: []
  type: TYPE_NORMAL
- en: The minimal error property of our obfuscation method is designed to limit numerical
    errors resulting from the inverse computations of the random matrices as well
    as errors resulting from matrix multiplication between the random matrix and the
    data embeddings or model parameters. We use only orthogonal random matrices, as
    they have the minimum condition number of $1$ matrix computed by applying a $QR$
    is always orthogonal. In this case, the inverse computation is fully error-free
    since the inverse of an orthogonal matrix is its transposed version which is an
    error-free operation.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimental evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The conducted experiments aim to address the following key questions: (a) What
    is the impact of applying *ObfuscaTune* on utility, i.e., how do models finetuned
    with *ObfuscaTune* compare to the normally finetuned models? (b) How does our
    obfuscation method using orthogonal random matrices compare to naively using any
    random matrices?'
  prefs: []
  type: TYPE_NORMAL
- en: We apply our method to GPT2 Radford et al. ([2019](#bib.bib24)) models with
    different sizes, ranging from 117 million to 1.5 billion parameters. We implement
    *ObfuscaTune* on top of the nanoGPT implementation Karpathy ([2023](#bib.bib11)).
    All our experiments perform LoRA-finetuning Hu et al. ([2022](#bib.bib8)). Hereby,
    the LoRA parameters are randomly initialized and placed outside of the TEE. We
    apply LoRA to all linear and attention layers. Further hyperparameters are specified
    in the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: In each *ObfuscaTune* experiment, we use 2 GPU devices, one that is placed outside
    of TEE and another that simulates the TEE. We believe this is reasonable since
    high-end GPUs have TEE support Apsey et al. ([2023](#bib.bib2)). We evaluate the
    finetuning with *ObfuscaTune* and with a naive version that uses any random matrices
    on 4 question-answering benchmark datasets, including WebQuestions (WebQs) Berant
    et al. ([2013](#bib.bib3)), OpenBookQA (OBQA) Mihaylov et al. ([2018](#bib.bib21)),
    PIQA Bisk et al. ([2020](#bib.bib4)) and SciQ Welbl et al. ([2017](#bib.bib28)).
    We evaluate all models using lm-eval-harness⁵⁵5https://github.com/EleutherAI/lm-evaluation-harness.
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | WebQs | OBQA | PIQA | SciQ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-Small |'
  prefs: []
  type: TYPE_TB
- en: '| Unprotected | 16.0 | 23.0 | 64.1 | 91.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (random) | 0.0 | 15.4 | 53.1 | 19.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (ours) | 16.8 | 23.6 | 64.8 | 91.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Unprotected | 24.1 | 29.2 | 69.1 | 92.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (random) | 0.0 | 14.4 | 52.0 | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (ours) | 24.5 | 28.6 | 68.9 | 92.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-Large |'
  prefs: []
  type: TYPE_TB
- en: '| Unprotected | 30.0 | 35.0 | 72.1 | 93.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (random) | 0.0 | 14.4 | 52.0 | 19.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (ours) | 29.7 | 32.2 | 72.3 | 93.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-XL |'
  prefs: []
  type: TYPE_TB
- en: '| Unprotected | 32.4 | 34.2 | 74.1 | 93.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (random) | 0.0 | 14.8 | 52.5 | 20.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Protected (ours) | 32.6 | 33.2 | 73.9 | 93.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Test accuracy results (%) yielded by normally finetuned models (unprotected)
    and models which are protected by *ObfuscaTune* as well as a naive version of
    our method that uses an arbitrary random matrix with a non-optimized condition
    number (random).'
  prefs: []
  type: TYPE_NORMAL
- en: '| CN | 1 | 8 | 32 | 128 | 160 | random |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 16.8 | 15.5 | 15.2 | 14.7 | 0.3 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Test accuracy results (%) yielded by GPT2-small models finetuned on
    WebQs with *ObfuscaTune* using matrices with different condition numbers (CN).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3 Experimental evaluation ‣ ObfuscaTune: Obfuscated
    Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets") presents
    our main experimental results. We find that models finetuned with our method achieve
    a performance comparable to models finetuned without model and data protection.
    This observation is consistent across all model sizes and benchmark datasets.
    Besides, models that are finetuned with a naive method that uses arbitrary random
    matrices incur substantial utility loss due to the high accumulation of errors.
    Furthermore, we evaluate the impact of using random matrices with different condition
    numbers and empirically confirm that higher condition numbers deteriorate performance
    (Tab. [2](#S3.T2 "Table 2 ‣ 3 Experimental evaluation ‣ ObfuscaTune: Obfuscated
    Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets"), details
    in Appendix B).'
  prefs: []
  type: TYPE_NORMAL
- en: We also measure the percentage of model parameters present on TEE after model
    obfuscation to be $\boldsymbol{5.2\%}$ using MPC Knott et al. ([2021](#bib.bib13))
    and $10^{5}$ using HE Lou and Jiang ([2021](#bib.bib18)) with significantly smaller
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work tackled the timely but underexplored problem of performing offsite
    inference and finetuning of a proprietary LLM owned by a model provider entity
    on the confidential/private data of another data owner entity, in a way that ensures
    the confidentiality of both the model and the data. Our proposed approach, ObfuscaTune,
    achieves this by combining a simple yet effective obfuscation technique with an
    efficient usage of confidential computing (only $~{}5\%$ of the model parameters
    are placed on TEE). Our extensive empirical evaluation on four NLP benchmark datasets
    and different models highlights the effectiveness of our method and emphasizes
    the importance of using random matrices with low condition numbers for preserving
    high utility. In future work, we will investigate the effectiveness of our approach
    to RAG-systems.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One potential limitation of our work is that despite testing on different models
    and datasets, we focused on the same model architecture, i.e., GPT2\. However,
    most of the other LLMs are composed on the same building blocks, which makes the
    application of our method to them straightforward. Another limitation might be
    that while the slowdown incured by *ObfuscaTune* is substantially lower than other
    technologies, e.g., MPC and HE, it might still be unsuitable for some applications
    where efficiency has a higher importance than privacy
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apsey et al. (2023) Emily Apsey, Phil Rogers, Michael O’Connor, and Rob Nertney.
    2023. [Confidential computing on nvidia h100 gpus for secure and trustworthy ai,
    august 2023](https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
    2013. Semantic parsing on freebase from question-answer pairs. In *Proceedings
    of the 2013 conference on empirical methods in natural language processing*, pages
    1533–1544.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2024) Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham,
    Thomas Steinke, Jonathan Hayase, A Feder Cooper, Katherine Lee, Matthew Jagielski,
    Milad Nasr, Arthur Conmy, et al. 2024. Stealing part of a production language
    model. *arXiv preprint arXiv:2403.06634*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Golub and Van Loan (2013) Gene H Golub and Charles F Van Loan. 2013. *Matrix
    computations*. JHU press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2021) Jiahui Hou, Huiqi Liu, Yunxin Liu, Yu Wang, Peng-Jun Wan,
    and Xiang-Yang Li. 2021. Model protection: Real-time privacy-preserving inference
    service for model privacy at the edge. *IEEE Transactions on Dependable and Secure
    Computing*, 19(6):4270–4284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. [LoRA: Low-rank adaptation
    of large language models](https://openreview.net/forum?id=nZeVKeeFYf9). In *International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2024) Wei Huang, Yinggui Wang, Anda Cheng, Aihui Zhou, Chaofan
    Yu, and Lei Wang. 2024. A fast, performant, secure distributed training framework
    for large language model. *arXiv preprint arXiv:2401.09796*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Igamberdiev and Habernal (2023) Timour Igamberdiev and Ivan Habernal. 2023.
    Dp-bart for privatized text rewriting under local differential privacy. *arXiv
    preprint arXiv:2302.07636*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpathy (2023) Andrej Karpathy. 2023. [nanogpt](https://github.com/karpathy/nanoGPT).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knight (2023) Will Knight. 2023. [Openai’s ceo says the age of giant ai models
    is already over, april 2023](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knott et al. (2021) Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta,
    Mark Ibrahim, and Laurens van der Maaten. 2021. Crypten: Secure multi-party computation
    meets machine learning. *Advances in Neural Information Processing Systems*, 34:4961–4973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P Xing,
    and Hao Zhang. 2022. Mpcformer: fast, performant and private transformer inference
    with mpc. *arXiv preprint arXiv:2211.01452*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Haoran Li, Mingshi Xu, and Yangqiu Song. 2023. Sentence embedding
    leaks more information than you expect: Generative embedding inversion attack
    to recover the whole sentence. *arXiv preprint arXiv:2305.03010*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng,
    Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee,
    Ismet Bayraktaroglu, et al. 2023. Chipnemo: Domain-adapted llms for chip design.
    *arXiv preprint arXiv:2311.00176*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Liu (2023) Xuanqi Liu and Zhuotao Liu. 2023. Llms can understand encrypted
    prompt: Towards privacy-computing friendly transformers. *arXiv preprint arXiv:2305.18396*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lou and Jiang (2021) Qian Lou and Lei Jiang. 2021. Hemet: a homomorphic-encryption-friendly
    privacy-preserving mobile neural network architecture. In *International conference
    on machine learning*, pages 7102–7110\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majmudar et al. (2022) Jimit Majmudar, Christophe Dupuy, Charith Peris, Sami
    Smaili, Rahul Gupta, and Richard Zemel. 2022. Differentially private decoding
    in large language models. *arXiv preprint arXiv:2205.13621*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mann et al. (2020) Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan,
    P Shyam, G Sastry, A Askell, S Agarwal, et al. 2020. Language models are few-shot
    learners. *arXiv preprint arXiv:2005.14165*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open
    book question answering. *arXiv preprint arXiv:1809.02789*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morris et al. (2023) John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and
    Alexander M Rush. 2023. Text embeddings reveal (almost) as much as text. *arXiv
    preprint arXiv:2310.06816*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noy and Zhang (2023) Shakked Noy and Whitney Zhang. 2023. Experimental evidence
    on the productivity effects of generative artificial intelligence. *Science*,
    381(6654):187–192.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ray (2023) Siladitya Ray. 2023. [Samsung bans chatgpt among employees after
    sensitive code leak, may 2023](https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thakur et al. (2023) Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce,
    Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2023. Benchmarking
    large language models for automated verilog rtl code generation. In *2023 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*, pages 1–6\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welbl et al. (2017) Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing
    multiple choice science questions. *arXiv preprint arXiv:1707.06209*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2024) Haoyuan Wu, Zhuolun He, Xinyun Zhang, Xufeng Yao, Su Zheng,
    Haisheng Zheng, and Bei Yu. 2024. Chateda: A large language model powered autonomous
    agent for eda. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-tuning:
    Transfer learning without full model. *arXiv preprint arXiv:2302.04870*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A
    Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,
    et al. 2021. Differentially private fine-tuning of language models. *arXiv preprint
    arXiv:2110.06500*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We train all models for 10 epochs. We perform validation at the end of every
    epoch and use early stopping with a patience of 3\. We use a learning rate of
    ${3e-5}$. We did not perform hyperparameter tuning, which highlights the robustness
    of our method. We did all experiments on middle-range GPUs. Each experiment took
    between less than 1 and 8 GPU hours, depending on he model size and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b242ce36f31b38604d888a340fee2820.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of the proposed *ObfuscaTune*, composed by the three stakeholders:
    model provider, which seeks to keep the model confidential, data owner, which
    uses the model (finetuning or inference) while preserving privacy of their data,
    and cloud provider which provides the computation infrastructure, while potentially
    trying to eavesdrop on the data or steal the model. *ObfuscaTune* provides the
    necessary protection by keeping very few components of the model within a TEE,
    and obfuscating the remaining ones, effectively and efficiently preventing data
    or model stealing. This Figure will be part of the additional page in the camera
    ready version upon paper acceptance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f63f0c1be14a8237b441159ec9d06f9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Detailed architecture of the GPT-2 with M layers using *ObfuscaTune*.
    Diagram blocks in green are within the TEE, while the orange are outside the TEE.
    This diagram illustrates how the data is successfully sent from and to the TEE,
    while being obfuscated while outside the TEE. Note that both the input text and
    output text are always within the TEE to prevent inversion attacks. Note that
    the non-activation applied after the first MLP (bottom) is applied on the de-obfuscated
    embedding. The same applies for the softmax non-linear function.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Effect of the condition number
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The condition number $\kappa$ measures how much the mapping induced by that
    matrix can stretch vectors and $m=\min{\frac{\|Ax\|}{\|x\|}}$ measures how much
    it can shrink vectors. It determines how much a relative error in the input reflects
    on the output for solving linear systems, matrix inversion or matrix-vector multiplication
    Golub and Van Loan ([2013](#bib.bib6)). Such numerical errors get accumulated
    and increase with the number of sequential matrix multiplication operations, i.e.,
    the deeper the model the higher the accumulated error. We minimize the numerical
    errors by minimizing the condition number of the random matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we consider the condition number w.r.t the $\ell_{2}$ for every
    orthogonal matrix $A$. On the other side, from the definition we see that the
    lowest possible $\kappa$ is 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\sigma_{max}(A)$-induced operator norm norm the following holds :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;A\&#124;=\max{\frac{\&#124;Ax\&#124;}{\&#124;x\&#124;}}=\sigma_{max}(A).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: On the other hand, for A square and non-singular
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{x}\frac{\&#124;Ax\&#124;}{\&#124;x\&#124;}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{1}{\max_{y}\frac{\&#124;A^{-1}y\&#124;}{\&#124;y\&#124;}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{1}{\&#124;A^{-1}\&#124;}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{1}{\sigma_{max}(A^{-1})}=\sigma_{min}(A).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Finally we get for every square and non-singular matrix $A$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\kappa(A)=\frac{\sigma_{max}(A)}{\sigma_{min}(A)}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The last equation makes it possible to generate random matrices $R$ using the
    standard normal distribution. We then apply QR-decomposition on $A$. We then choose
    a random positive value for the largest singular value of the final matrix $R$.
    Then we construct the diagonal matrix $S$ to be having the following singular
    value decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R=Q_{A}SQ_{B}.$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'And can calculate $R^{-1}=Q_{B}^{T}S^{-1}Q_{A}^{T}$ with minimal rounding errors.
    We use this approach to generate random matrices of a given condition number and
    monitor the effect of the condition number on the test accuracy of the final model.
    The results are showcased in table [2](#S3.T2 "Table 2 ‣ 3 Experimental evaluation
    ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs
    on Private Datasets") show indeed that it is curcial to have a low condition number,
    otherwise the training degenerates.'
  prefs: []
  type: TYPE_NORMAL
