- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fine-Tuned ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot Generative
    AI Models in Text Classification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调后的“小型”LLMs（依然）在文本分类中显著优于零-shot生成性AI模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08660](https://ar5iv.labs.arxiv.org/html/2406.08660)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08660](https://ar5iv.labs.arxiv.org/html/2406.08660)
- en: Martin Juan José Bucher
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 马丁·胡安·何塞·布赫
- en: Stanford University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学
- en: mnbucher@stanford.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: mnbucher@stanford.edu
- en: \AndMarco Martini
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \AndMarco Martini
- en: University of Zurich
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世大学
- en: marco.martini@uzh.ch
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: marco.martini@uzh.ch
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Generative AI offers a simple, prompt-based alternative to fine-tuning smaller
    BERT-style LLMs for text classification tasks. This promises to eliminate the
    need for manually labeled training data and task-specific model training. However,
    it remains an open question whether tools like ChatGPT can deliver on this promise.
    In this paper, we show that smaller, fine-tuned LLMs (still) consistently and
    significantly outperform larger, zero-shot prompted models in text classification.
    We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude
    Opus) with several fine-tuned LLMs across a diverse set of classification tasks
    (sentiment, approval/disapproval, emotions, party positions) and text categories
    (news, tweets, speeches). We find that fine-tuning with application-specific training
    data achieves superior performance in all cases. To make this approach more accessible
    to a broader audience, we provide an easy-to-use toolkit alongside this paper.
    Our toolkit, accompanied by non-technical step-by-step guidance, enables users
    to select and fine-tune BERT-like LLMs for any classification task with minimal
    technical and computational effort.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性AI提供了一种简单的、基于提示的替代方案，来替代针对文本分类任务的小型BERT风格LLMs的微调。这有望消除对手动标注训练数据和任务特定模型训练的需求。然而，像ChatGPT这样的工具是否能兑现这一承诺仍然是一个悬而未决的问题。在本文中，我们展示了较小的、微调后的LLMs（依然）在文本分类中持续且显著地优于更大的零-shot提示模型。我们比较了三种主要的生成性AI模型（ChatGPT与GPT-3.5/GPT-4和Claude
    Opus）与几种微调后的LLMs，在各种分类任务（情感、批准/不批准、情绪、党派立场）和文本类别（新闻、推文、演讲）中。我们发现，使用特定应用的训练数据进行微调在所有情况下都能取得优越的性能。为了使这种方法更易于被更广泛的受众使用，我们在本文中提供了一个易于使用的工具包。我们的工具包配有非技术性的逐步指导，帮助用户选择和微调类似BERT的LLMs，针对任何分类任务，且仅需最少的技术和计算努力。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The last decade has seen a paradigm shift in Natural Language Processing (NLP)
    with the advent of pre-trained Large Language Models (LLMs). These models have
    not only achieved previously unseen performance on a wide range of benchmarks.
    They have also shifted expectations for the future of artificial intelligence
    (AI) more generally.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年，随着预训练大型语言模型（LLMs）的出现，自然语言处理（NLP）领域经历了一次范式转变。这些模型不仅在各种基准测试中取得了前所未有的性能，还改变了对人工智能（AI）未来的期望。
- en: 'Traditionally, leveraging LLMs involves two sequential steps: First, models
    are pre-trained on large text corpora to instill general language understanding.
    Pre-training is typically performed by specialized experts who then make the pre-trained
    models publicly available by providing all parameters and model states via checkpoints.
    Subsequently, these models can be further fine-tuned for specific tasks using
    smaller, dedicated training datasets, a task usually carried out by end-users
    who need to implement deep learning code and provide application-specific training
    data in the process. LLMs of this kind, like BERT or RoBERTa, have since been
    the state-of-the-art in numerous NLP tasks. Empirical evidence has shown that
    fine-tuned LLMs outperform traditional methods such as dictionaries, bag-of-words
    models, or approaches based on word embeddings [[13](#bib.bibx13), [24](#bib.bibx24),
    [45](#bib.bibx45), [11](#bib.bibx11)].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，利用LLMs涉及两个连续的步骤：首先，模型在大型文本语料库上进行预训练，以培养通用语言理解。预训练通常由专业专家进行，然后通过检查点提供所有参数和模型状态，使预训练模型公开可用。随后，这些模型可以使用较小的专用训练数据集进一步微调，这一任务通常由需要实现深度学习代码并提供应用特定训练数据的最终用户完成。这类LLMs，如BERT或RoBERTa，已成为众多NLP任务中的最先进技术。实证证据表明，微调后的LLMs优于传统方法，如词典、词袋模型或基于词嵌入的方法
    [[13](#bib.bibx13), [24](#bib.bibx24), [45](#bib.bibx45), [11](#bib.bibx11)]。
- en: 'More recently, with models continuously growing in size and complexity, instruction-tuned
    generative AI models such as ChatGPT and Claude Opus have emerged. These models
    are not only significantly larger, but also more versatile than BERT-style models:
    While generative AI models are also pre-trained, they can be directly prompted
    via text commands to perform tasks without the need for a further fine-tuning
    step. As a result, generative AI promises a straightforward and intuitive user
    interaction while challenging the traditional approach, which requires fitting
    a model to labeled data. Although the empirical picture is still far from decisive,
    first studies suggest that generative AI models are already outperforming fine-tuned
    LLMs in text classification tasks [[18](#bib.bibx18), [46](#bib.bibx46)].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着模型规模和复杂性的不断增长，像 ChatGPT 和 Claude Opus 这样的指令调优生成式 AI 模型已出现。这些模型不仅大得多，而且比
    BERT 风格的模型更为多样化：虽然生成式 AI 模型也经过预训练，但可以通过文本命令直接提示执行任务，无需进一步的微调步骤。因此，生成式 AI 提供了一种直接且直观的用户交互方式，同时挑战了传统的方法，该方法需要将模型拟合到标注数据上。尽管实证情况仍远未决定，但初步研究表明，生成式
    AI 模型在文本分类任务中已经超越了微调的 LLM [[18](#bib.bibx18), [46](#bib.bibx46)]。
- en: In this paper, we contribute to the ongoing debate by systematically comparing
    the text classification performance of three prompt-instructed generative AI models
    (ChatGPT-3.5, ChatGPT-4, and Claude Opus) as well as Facebook’s BART with that
    of several smaller, fine-tuned LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过系统比较三种提示指令生成式 AI 模型（ChatGPT-3.5、ChatGPT-4 和 Claude Opus）以及 Facebook
    的 BART 与若干较小、经过微调的 LLM 的文本分类性能，来为持续的辩论做出贡献。
- en: 'Our analysis spans a wide range of text classification tasks, including sentiment
    analysis, approval/disapproval recognition, emotion detection, and identification
    of political party positions. We also evaluate model performance across different
    text categories, such as news articles, tweets, and political speeches. Specifically,
    we conduct four case studies: (i) sentiment analysis of The New York Times coverage
    on the U.S. economy, (ii) stance classification of tweets about Brett Kavanaugh’s
    nomination to the U.S. Supreme Court, (iii) emotion detection in German political
    texts, and (iv) multi-class stance classification of nationalist party positions
    on European integration.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析涵盖了广泛的文本分类任务，包括情感分析、认可/不认可识别、情感检测和政治党派立场识别。我们还评估了模型在不同文本类别中的表现，如新闻文章、推文和政治演讲。具体来说，我们进行四个案例研究：（i）对《纽约时报》关于美国经济的报道进行情感分析，（ii）对有关布雷特·卡瓦诺提名至美国最高法院的推文进行立场分类，（iii）对德语政治文本中的情感进行检测，以及（iv）对民族主义政党在欧洲一体化问题上的多类立场分类。
- en: Our results demonstrate that fine-tuning smaller BERT-style models significantly
    outperforms generative AI models such as ChatGPT and Claude Opus (used in a "zero-shot"
    fashion) across all four applications when moderate amounts of training data for
    fine-tuning are provided. This tendency is especially pronounced for more specialized,
    non-standard classification tasks. Overall, these findings suggest that smaller,
    fine-tuned LLMs (still) constitute the state-of-the-art in text classification.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，在提供适量的微调训练数据时，微调较小的 BERT 风格模型在所有四个应用场景中的表现显著优于生成式 AI 模型（如 ChatGPT 和
    Claude Opus，以“零样本”方式使用）。这种倾向在更专业、非标准分类任务中尤为明显。总体而言，这些发现表明，较小、经过微调的 LLM（仍然）构成了文本分类领域的**最先进技术**。
- en: Our results also imply that model size and complexity are no sufficient substitute
    for application-specific training data. For text classification tasks, the lightweight,
    tailored approach of fine-tuning small LLMs via training data remains preferable
    to the heavy-duty, one-size-fits-all approach of zero-shot prompting generative
    AI models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果还表明，模型的规模和复杂性不足以替代特定应用的训练数据。在文本分类任务中，通过训练数据微调小型 LLM 的轻量化定制方法仍然优于零样本提示生成式
    AI 模型的重型“一刀切”方法。
- en: Given our clear-cut results in favor of fine-tuning, we make available an easy-to-use
    toolkit with this paper. Presented as a simple Jupyter Notebook built on top of
    Hugging Face, our toolkit simplifies the process of fine-tuning LLMs, which typically
    requires deep learning and programming experience. It allows users to select and
    fine-tune smaller pre-trained LLMs for any classification problem (e.g., sentiment)
    or text category (e.g., news) with minimal requirements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们明确支持微调的结果，我们随本文提供了一个易于使用的工具包。该工具包以Hugging Face为基础，呈现为一个简单的Jupyter Notebook，简化了微调LLMs的过程，这通常需要深度学习和编程经验。它允许用户选择并微调较小的预训练LLMs，以解决任何分类问题（例如情感分析）或文本类别（例如新闻），并且要求最低。
- en: Our toolkit supports different languages and handles both binary and non-binary
    classification problems. It includes pre-implemented methods to address class
    imbalance, common in many applications (e.g., news texts typically have more negative
    than positive sentiment). While the notebook allows for the configuration and
    optimization of deep-learning hyperparameters, it comes with default hyperparameters
    that deliver strong performance out-of-the-box. Computationally intensive hyperparameter
    tuning is therefore an option but not required. The modular design of the toolkit
    allows for the integration of additional/future model releases. We further provide
    a detailed documentation and step-by-step user guidance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工具包支持多种语言，并处理二分类和非二分类问题。它包括预实现的方法来解决类不平衡问题，这在许多应用中很常见（例如，新闻文本通常包含更多的负面情感而不是正面情感）。尽管笔记本允许配置和优化深度学习超参数，但它提供了默认的超参数，这些默认值开箱即用时能够提供强大的性能。因此，计算密集型的超参数调优是一个可选项，但不是必需的。工具包的模块化设计允许集成额外的或未来的模型发布。我们还提供了详细的文档和逐步的用户指导。
- en: To further facilitate use and adoption of fine-tuning, we provide a non-technical
    introduction to the functioning principles of LLMs, clarify how these models expand
    on earlier text-as-data methods, and explain why this results in significant performance
    gains (Section 3). We also discuss current research areas, potential future trends
    in NLP and AI, and the opportunities of few-shot learning (Section 6).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步促进微调的使用和采纳，我们提供了对LLMs（大型语言模型）工作原理的非技术性介绍，阐明这些模型如何扩展早期的文本数据方法，并解释为何这会带来显著的性能提升（第3节）。我们还讨论了当前的研究领域、NLP和AI中的潜在未来趋势，以及少量样本学习的机会（第6节）。
- en: 2 Related Work and Contribution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作与贡献
- en: A growing literature evaluates and compares the performance of fine-tuned and
    prompt-based LLMs [[5](#bib.bibx5), [6](#bib.bibx6), [8](#bib.bibx8), [10](#bib.bibx10),
    [41](#bib.bibx41), [15](#bib.bibx15), [18](#bib.bibx18), [47](#bib.bibx47), [46](#bib.bibx46)].
    However, some of this work does not specifically address text classification,
    which is the primary focus of our paper, but instead concentrates on various other
    text comprehension tasks [[6](#bib.bibx6), [41](#bib.bibx41)]. Other studies primarily
    compare the performance of different fine-tuning methods [[10](#bib.bibx10)],
    or evaluate various zero-shot approaches against each other [[47](#bib.bibx47)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不断增长的文献评估和比较了微调的LLMs和基于提示的LLMs的性能[[5](#bib.bibx5), [6](#bib.bibx6), [8](#bib.bibx8),
    [10](#bib.bibx10), [41](#bib.bibx41), [15](#bib.bibx15), [18](#bib.bibx18), [47](#bib.bibx47),
    [46](#bib.bibx46)]。然而，其中一些工作并未专门针对文本分类，这也是我们论文的主要关注点，而是集中于各种其他文本理解任务[[6](#bib.bibx6),
    [41](#bib.bibx41)]。其他研究主要比较了不同微调方法的性能[[10](#bib.bibx10)]，或评估了各种零样本方法的效果[[47](#bib.bibx47)]。
- en: The existing studies that come closest to our work, as they compare fine-tuned
    and prompt-based models for text classification tasks, are [[5](#bib.bibx5)] and
    [[15](#bib.bibx15)] as well as [[46](#bib.bibx46)] and [[18](#bib.bibx18)]. However,
    these papers provide conflicting evidence as to which approach is superior for
    text classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的工作最接近的现有研究是[[5](#bib.bibx5)]和[[15](#bib.bibx15)]，以及[[46](#bib.bibx46)]和[[18](#bib.bibx18)]，因为这些研究比较了针对文本分类任务的微调模型和基于提示的模型。然而，这些论文提供了相互矛盾的证据，无法确定哪种方法在文本分类中更优。
- en: For example, [[15](#bib.bibx15)] compare a fine-tuned RoBERTa model with prompt-based
    models like GPT-3.5 and Meta’s Llama models across different classification tasks.
    The authors find that fine-tuned models (trained on the entire dataset) generally
    outperform prompt-based models. Similarly, [[5](#bib.bibx5)] compare the performance
    of RoBERTa and GPT-3 in a study on an English-language dataset of parliamentary
    speeches, concluding that fine-tuning yields better results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[15](#bib.bibx15)]比较了微调的RoBERTa模型与基于提示的模型（如GPT-3.5和Meta的Llama模型）在不同分类任务中的表现。作者发现，微调模型（在整个数据集上训练）通常优于基于提示的模型。类似地，[[5](#bib.bibx5)]在对英语议会演讲数据集的研究中比较了RoBERTa和GPT-3的表现，得出微调能带来更好结果的结论。
- en: In contrast, recent work by [[46](#bib.bibx46)] and [[18](#bib.bibx18)] suggests
    that ChatGPT has caught up with or even surpassed fine-tuned models. [[46](#bib.bibx46)]
    find that ChatGPT is on par with BERT-style models for some text understanding
    tasks, though results are mixed overall. [[18](#bib.bibx18)] present results suggesting
    that ChatGPT even outperforms human annotators for text labeling tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，[[46](#bib.bibx46)]和[[18](#bib.bibx18)]的近期研究表明，ChatGPT已经赶上甚至超越了微调模型。[[46](#bib.bibx46)]发现ChatGPT在某些文本理解任务上与BERT风格的模型相当，尽管总体结果不一。[[18](#bib.bibx18)]则呈现了结果，表明ChatGPT在文本标注任务中甚至超越了人工标注员。
- en: Inspired by these works, we aim to provide an up-to-date, systematic empirical
    analysis that offers a comprehensive view on the performance of fine-tuned versus
    prompt-based models for text classification.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 受这些工作的启发，我们旨在提供一个最新的、系统的实证分析，提供关于微调与基于提示的模型在文本分类中的表现的全面视角。
- en: 'While our results clearly point in the same direction as the studies by [[5](#bib.bibx5)]
    and [[15](#bib.bibx15)], our paper expands on the existing literature in several
    ways:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的结果明显与[[5](#bib.bibx5)]和[[15](#bib.bibx15)]的研究方向一致，但我们的论文在几个方面扩展了现有文献：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensive Comparison: We systematically compare model performance across
    a diverse set of classification tasks and text categories. This allows us to identify
    performance variations across tasks and highlight areas where the differences
    between fine-tuned and prompt-based models are most (and least) pronounced.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合比较：我们系统地比较了模型在各种分类任务和文本类别上的表现。这使我们能够识别任务之间的表现差异，并突出微调模型与基于提示的模型之间差异最明显（和最不明显）的领域。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Latest Generative AI Models: We provide results for the latest generation of
    prompt-based generative AI models (GPT-4 and Claude Opus). This is crucial, as
    previous studies based on older models like GPT-3 offer limited insights into
    the current performance balance.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最新的生成性AI模型：我们提供了最新一代基于提示的生成AI模型（GPT-4和Claude Opus）的结果。这至关重要，因为基于旧模型（如GPT-3）的先前研究对当前性能平衡提供的见解有限。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Nuanced Understanding of Fine-Tuned LLMs: By investigating the performance
    of several fine-tuned LLMs, we offer a detailed understanding of how these models
    compare across classification tasks. This helps identify which models are better
    suited for specific tasks, thereby aiding user choice. Such a comparison provides
    a broader perspective on the current LLM landscape for text classification.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调LLMs的细致理解：通过研究多个微调LLMs的表现，我们提供了这些模型在分类任务中如何比较的详细理解。这有助于识别哪些模型更适合特定任务，从而帮助用户选择。这种比较提供了当前LLM在文本分类领域的更广泛视角。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ablation Studies and Training Data Impact: We conduct ablation studies and
    analyze the effect of training data size on model performance for fine-tuning.
    This offers valuable insights into the number of labeled samples required to achieve
    optimal performance, a crucial consideration given our consistent findings in
    favor of fine-tuned LLMs.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消融研究与训练数据影响：我们进行消融研究，并分析训练数据量对微调模型性能的影响。这为达到最佳性能所需的标注样本数量提供了宝贵的见解，这在我们对微调LLMs的一致发现中尤为重要。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accessible Toolkit for Fine-Tuning: We offer a one-stop-shop toolkit for text
    classification to enable a wider audience to use pre-trained LLMs for text classification
    tasks.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可访问的微调工具包：我们提供一个一站式文本分类工具包，使更广泛的受众能够使用预训练的LLM进行文本分类任务。
- en: With these contributions, we aim to provide a comprehensive and practical resource
    for understanding and applying LLMs in text classification.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些贡献，我们旨在提供一个全面且实用的资源，以便理解和应用LLMs进行文本分类。
- en: '3 Non-technical Background: From Keywords to Large Language Models'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 非技术背景：从关键词到大型语言模型
- en: This section provides a non-technical introduction to LLMs, which complements
    our empirical results and our provided fine-tuning toolkit. It explains the functioning
    principles of LLMs and the reasons for their superior performance compared to
    earlier methods. To provide a comprehensive overview, we first briefly cover traditional
    hand-coding and dictionary approaches before discussing machine learning methods
    and LLMs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对LLMs的非技术性介绍，这补充了我们的实证结果和提供的微调工具包。它解释了LLMs的工作原理及其优于早期方法的原因。为了提供全面的概述，我们首先简要介绍传统的手工编码和词典方法，然后讨论机器学习方法和LLMs。
- en: Hand-coding is among the earliest text-as-data approaches. When hand-coding,
    Human coders classify verbal information according to pre-specified codebooks
    and explicit coding rules [[12](#bib.bibx12)], [[33](#bib.bibx33)]. Because hand-coding
    leverages human text-understanding, it is relatively easy to perform and yields
    high-quality results. However, hand-coding is labor-intensive — both regarding
    codebook development and the coding process itself. This makes it expensive and
    slow (see Figure 1). Moreover, hand-coding can be inconsistent across coders and
    within coders over time, making it partially non-replicable. High costs, slow
    procedures, and limited reproducibility are thus clear drawbacks of hand-coding
    approaches.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 手工编码是最早的文本数据方法之一。在手工编码时，人工编码人员根据预先指定的编码手册和明确的编码规则对口头信息进行分类[[12](#bib.bibx12)],
    [[33](#bib.bibx33)]。由于手工编码依赖于人类的文本理解，它相对容易执行并且结果高质量。然而，手工编码劳动强度大——无论是在编码手册的开发还是编码过程本身。这使得手工编码既昂贵又缓慢（见图1）。此外，手工编码在编码者之间和编码者自身随时间的变化中可能存在不一致，导致部分不可重复。因此，高成本、缓慢的程序和有限的可重复性是手工编码方法的明显缺点。
- en: Dictionary approaches alleviate some of these concerns. These methods automatically
    extract information from text by mapping a larger set of keywords (or keyword
    combinations) to a small set of categories of interest. For example, sentiment
    dictionaries classify texts according to their content of positive and negative
    words or phrases [[21](#bib.bibx21), [37](#bib.bibx37)]. Dictionaries can be fully
    automated and therefore fast, replicable, and transparent. However, because dictionaries
    only take selected keywords into account, they ignore most nuanced textual information.
    Because words have different meanings in different settings, dictionaries can
    also be context-dependent and noisy when used off-the-shelf. Moreover, dictionary
    construction in itself is a difficult and time-consuming process and requires
    iterative testing to minimize false positives and false negatives. Thus, while
    dictionaries have advantages in terms of speed and replicability, they remain
    a relatively blunt tool.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 词典方法缓解了这些担忧。这些方法通过将较大的关键词集（或关键词组合）映射到一小组感兴趣的类别，从文本中自动提取信息。例如，情感词典根据文本中的积极和消极词汇或短语对文本进行分类[[21](#bib.bibx21),
    [37](#bib.bibx37)]。词典可以完全自动化，因此速度快、可重复且透明。然而，由于词典只考虑选定的关键词，它们忽略了大多数细微的文本信息。由于词汇在不同背景下有不同的含义，词典在即用时也可能具有上下文依赖性和噪声。此外，词典的构建本身是一个困难且耗时的过程，需要反复测试以最小化假阳性和假阴性。因此，尽管词典在速度和可重复性方面具有优势，但它们仍然是相对笨拙的工具。
- en: '![Refer to caption](img/3b7d5d64ae0d2d857f21d612add17929.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3b7d5d64ae0d2d857f21d612add17929.png)'
- en: 'Figure 1: Overview of existing text-as-data methods and their characteristics:
    Machine learning approaches for text classification have the *potential* to combine
    the advantages of hand-coding (high-quality) and dictionaries (speed), while avoiding
    their respective downsides. The degree to which this potential can be realized
    in practice depends on the underlying text representation (see Figure 2).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：现有文本数据方法及其特征概述：机器学习文本分类方法具有结合手工编码（高质量）和词典（速度）优点的*潜力*，同时避免各自的缺点。这个潜力在实际中能否实现，取决于基础文本表示（见图2）。
- en: Machine learning methods follow a different approach. Unlike hand-coding and
    dictionary approaches, which both rely on explicit human-crafted rules, machine
    learning techniques derive implicit rules from human-labeled data. In supervised
    learning, a model extracts classification-relevant patterns from a sample of manually-coded
    (labeled) training data. These patterns are then used to auto-classify the bulk
    of the remaining data [[28](#bib.bibx28), [16](#bib.bibx16), [9](#bib.bibx9)].
    Machine learning methods allow human coders to classify texts based on natural
    human language. This frees coders from having to create abstract rules and instead
    allows them to follow their intuitive language understanding. Humans can thus
    focus on the information content of texts, while the learning algorithm extracts
    predictive regularities from the relation between text features and labels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法采用不同的方法。与依赖于明确人工制定规则的手工编码和词典方法不同，机器学习技术从人类标记的数据中推导出隐含规则。在监督学习中，模型从一组手动编码（标记）训练数据中提取与分类相关的模式。这些模式随后用于自动分类大量剩余数据[[28](#bib.bibx28),
    [16](#bib.bibx16), [9](#bib.bibx9)]。机器学习方法使人工编码人员能够基于自然语言来分类文本。这使得编码人员不必创建抽象规则，而是能够遵循他们直观的语言理解。因此，人们可以专注于文本的信息内容，而学习算法则从文本特征与标签之间的关系中提取预测规律。
- en: The combination of human-labeled data and automated rule extraction is powerful
    because it can fuse the high quality of human coding with the speed of automated
    approaches (see Figure 1). This combination works best, however, with more sophisticated
    and information-dense numerical language representations. Deep-learning approaches
    based on LLMs are now making it possible to realize this potential. To see this,
    we next describe the evolution of language representations from bag-of-words models
    via word embeddings to LLMs and point out why each step has generated significant
    performance increases in NLP tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人工标记数据与自动规则提取的结合非常强大，因为它可以将人工编码的高质量与自动化方法的速度结合起来（见图1）。然而，这种结合在更复杂和信息密集的数值语言表示中效果最好。基于LLMs的深度学习方法现在使实现这种潜力成为可能。为了理解这一点，我们接下来描述从词袋模型到词嵌入再到LLMs的语言表示演变，并指出每一步为何在NLP任务中带来了显著的性能提升。
- en: 3.1 Bag-Of-Words
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 词袋模型
- en: Early machine learning approaches for automated text analysis rest on bag-of-words
    models [[42](#bib.bibx42)]. These models treat text documents as collections (or
    bags) of words without regard to word order and grammar. This allows representing
    complex language information in a simple spreadsheet format that is easy to work
    with in downstream applications. In this format, rows represent documents, columns
    represent words, and cells record how often a word occurs in a document. Thus,
    a corpus with $D$ matrix. Each row (corresponding to a document) then captures
    the word signature (i.e., word distribution) of the original input text.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的机器学习方法用于自动文本分析基于词袋模型[[42](#bib.bibx42)]。这些模型将文本文档视为单词的集合（或词袋），而不考虑单词的顺序和语法。这允许以简单的电子表格格式表示复杂的语言信息，这种格式易于在后续应用中使用。在这种格式中，行表示文档，列表示单词，单元格记录单词在文档中的出现频率。因此，一个包含$D$矩阵的语料库。每行（对应一个文档）然后捕捉原始输入文本的单词签名（即单词分布）。
- en: While bag-of-word approaches can be powerful, they have notable shortcomings.
    First, they tend to produce large sparse matrices - because most words typically
    do not occur in most documents. This also results in matrices with more columns
    (words = variables) than rows (documents = observations), leading to $$P></math>
    problems that are unwieldy in regression contexts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词袋模型方法可能很强大，但它们也有显著的不足。首先，这些方法往往会产生大型稀疏矩阵——因为大多数单词通常不会出现在大多数文档中。这也导致了矩阵的列数（单词
    = 变量）多于行数（文档 = 观测值），从而引发在回归环境中不便处理的$$P></math>问题。
- en: Second, the spreadsheet representation treats all words as equally different
    from each other. For example, bag-of-words representations cannot capture that
    the words ’cake’ and ’cookie’ have more in common than the words ‘cake’ and ‘court’.
    Bag-of-words approaches are oblivious to the meaning of words and cannot reflect
    relations between words.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，电子表格表示将所有单词视为彼此完全不同。例如，词袋表示不能捕捉到单词‘cake’和‘cookie’比单词‘cake’和‘court’更有共同点这一事实。词袋方法对单词的含义无动于衷，无法反映单词之间的关系。
- en: Third, bag-of-words approaches discard all information contained in word order,
    sentence structure, and grammar. This ignores the entire narrative content of
    a text and makes it difficult to retain information beyond the general topic of
    a document. This problem is further aggravated by pre-processing steps such as
    stopword removal, which typically discards negations, and thus discards further
    potentially important information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，词袋模型丢弃了所有关于词序、句子结构和语法的信息。这忽略了文本的整体叙事内容，使得保留超出文档一般主题的信息变得困难。这个问题被进一步加剧，因为预处理步骤如停用词移除通常会丢弃否定词，从而丢弃更多潜在的重要信息。
- en: 3.2 Word Embeddings
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 词嵌入
- en: In contrast to bag-of-words models, word embeddings capture core aspects of
    word meaning and similarity [[26](#bib.bibx26)]. Word embeddings represent words
    as vectors in a high-dimensional space, where one or multiple dimensions can be
    thought of as corresponding to a particular characteristic. For example, one dimension
    might represent the degree of positivity of a word, while another might represent
    its length or its frequency of use. Consequently, similar words are (ideally)
    mapped to vectors that point to a similar location in vector space.¹¹1Word embeddings
    are created by training neural networks to learn co-occurrence patterns of words.
    Well-known word embedding models are Word2Vec [[27](#bib.bibx27)], GloVe [[31](#bib.bibx31)],
    and FastText [[4](#bib.bibx4)].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与词袋模型相比，词嵌入捕捉了词义和相似性的核心方面[[26](#bib.bibx26)]。词嵌入将词表示为高维空间中的向量，其中一个或多个维度可以看作是对应于特定特征。例如，一个维度可能代表词的积极程度，而另一个维度可能代表词的长度或使用频率。因此，相似的词（理想情况下）会被映射到在向量空间中指向相似位置的向量上。¹¹词嵌入是通过训练神经网络来学习词的共现模式来创建的。著名的词嵌入模型有Word2Vec[[27](#bib.bibx27)]、GloVe[[31](#bib.bibx31)]和FastText[[4](#bib.bibx4)]。
- en: Word embeddings are a richer (and usually more compact) numerical representation
    of natural language than bags-of-words matrices. This has several advantages.
    First, word embeddings reduce the dimensionality of the language representation,
    which solves the <math id=$$ issue and facilitates the use of secondary models
    for downstream tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是自然语言比词袋矩阵更丰富（且通常更紧凑）的数值表示。这具有几个优点。首先，词嵌入降低了语言表示的维度，这解决了<math id=$$问题，并且促进了次级模型在下游任务中的使用。
- en: Second, word embeddings can extrapolate to words that do not occur in the training
    data for a downstream task. For example, when training a sentiment model where
    the training data contains positive references to ’apples’ and ’bananas,’ then
    the model will likely also attribute a positive sentiment to ’oranges’ - which
    will be located close to other fruits in the embedding space.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，词嵌入可以推断出训练数据中未出现的词汇在下游任务中的含义。例如，当训练一个情感模型时，训练数据包含对“苹果”和“香蕉”的积极评价，那么模型很可能也会对“橙子”赋予积极情感——它们将在嵌入空间中靠近其他水果的位置。
- en: Third, word embeddings can reflect further application-relevant intricacies
    of word meaning. For instance, embeddings can capture that ’man’ is related to
    ’woman’ and ’boy’ is related to ’girl’ on one dimension (gender), but also that
    ’man’ is related to ’boy’ and ’woman’ is related to ’girl’ on another dimension
    (age).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，词嵌入可以反映词义的进一步应用相关复杂性。例如，嵌入可以捕捉到“man”与“woman”以及“boy”与“girl”在一个维度（性别）上的关系，但也能捕捉到“man”与“boy”以及“woman”与“girl”在另一个维度（年龄）上的关系。
- en: However, word embeddings still focus on words and continue to abstract from
    sentence structure and word order. They will thus only be able to capture the
    information content of longer segments of text to a limited extent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，词嵌入仍然关注词汇本身，并继续抽象化句子结构和词序。因此，它们只能在有限的程度上捕捉较长文本片段的信息内容。
- en: '![Refer to caption](img/a2da4f6b38eacf0693f7c7a1c588bc63.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a2da4f6b38eacf0693f7c7a1c588bc63.png)'
- en: 'Figure 2: Text representation of different text-as-data approaches: Existing
    approaches differ starkly in the sophistication of their text representation.
    Pre-trained LLMs approximate a more holistic human text understanding that focuses
    on meaning (*concepts* rather than form (*wording*. This allows LLMs to effectively
    leverage the information contained in text. By contrast, earlier text-as-data
    approaches discard significant information due to their more rudimentary language
    representations.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同文本数据方法的文本表示：现有的方法在文本表示的复杂性上存在明显差异。预训练的LLM（大语言模型）近似于一种更全面的人类文本理解，重点关注意义（*概念*而非形式（*措辞*）。这使得LLM能够有效利用文本中包含的信息。相比之下，早期的文本数据方法由于其更为基础的语言表示，丢弃了大量重要信息。
- en: 3.3 Pre-trained Large Language Models
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 预训练的大型语言模型
- en: Pre-trained LLMs derive their name from the fact that, before being fine-tuned
    or prompted to perform a specific task, they are pre-trained on large amounts
    of text to ’learn’ the general structure and logic of language. The pre-training
    process encodes a language representation that focuses on sequences of text rather
    than individual words. For BERT-style (encoder-only) models, fine-tuning later
    enriches this language representation with task-specific information. For GPT-style
    (decoder-only) models, prompts induce an answer generation process that is, in
    essence, a text prediction exercise resting on the model’s pre-trained language
    understanding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的LLM得名于这样一个事实：在进行微调或提示以执行特定任务之前，它们会在大量文本上进行预训练，以“学习”语言的一般结构和逻辑。预训练过程编码了一种语言表示，重点关注文本序列而非单个词汇。对于BERT风格（仅编码器）模型，微调后会用任务特定的信息丰富这种语言表示。对于GPT风格（仅解码器）模型，提示引发的回答生成过程本质上是一种文本预测练习，依赖于模型预训练的语言理解。
- en: Pre-training relies on *unsupervised* learning. Unlike in supervised learning,
    the models are not given explicit instructions via ’labeled’ training data. Instead,
    they discover relationships by predicting (temporarily masked) words in their
    training texts based on the surrounding word sequences. Encoder-style models have
    typically been trained with a Masked Language Modeling (MLM) objective, where
    random words are ’masked out’ and the model must predict the missing words. In
    contrast, decoder-style models have typically been trained with a Causal Language
    Modeling (CLM) objective, where the goal is to predict the next token (or word)
    in the sequence.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练依赖于*无监督*学习。与监督学习不同，这些模型没有通过“标记”训练数据获得明确的指示。相反，它们通过预测（暂时屏蔽的）词汇来发现关系，这些词汇根据周围的词序列进行预测。编码器风格的模型通常使用掩蔽语言建模（MLM）目标进行训练，其中随机词汇被“掩蔽”，模型必须预测缺失的词汇。相比之下，解码器风格的模型通常使用因果语言建模（CLM）目标进行训练，目标是预测序列中的下一个标记（或词汇）。
- en: 'Pre-training is much more resource-intensive than fine-tuning: Learning the
    complexities of natural language requires long exposure to large volumes of text.
    Hence, although the pre-training phase is unsupervised and requires no manual
    input, it was long constrained by excessive computational costs. This changed
    with the development of the Transformer architecture [[38](#bib.bibx38)]. Transformers
    can be more effectively parallelized than earlier sequence-learning approaches
    such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory models (LSTMs).
    This paved the way for models such as BERT (Bidirectional Encoder Representations
    from Transformers; [[13](#bib.bibx13)] and much bigger models such as the GPT
    series (Generative Pre-trained Transformer; [[32](#bib.bibx32)].²²2While decoder-style
    models such as the GPT series are typically several orders of magnitude larger
    (in number of parameters) than encoder-style models, we refer to all of them as
    LLMs in this paper.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练比微调要资源密集得多：学习自然语言的复杂性需要长时间接触大量文本。因此，尽管预训练阶段是无监督的且不需要人工输入，但由于过高的计算成本，它曾经受到限制。这一情况随着Transformer架构的发展而改变[[38](#bib.bibx38)]。与早期的序列学习方法（如递归神经网络（RNNs）和长短期记忆模型（LSTMs））相比，Transformers可以更有效地进行并行处理。这为BERT（双向编码器表示来自Transformers；[[13](#bib.bibx13)]）等模型以及像GPT系列（生成预训练变换器；[[32](#bib.bibx32)]）这样更大的模型铺平了道路。²²2虽然像GPT系列这样的解码器风格模型通常比编码器风格模型（在参数数量上）大几个数量级，但在本文中我们将它们统称为LLM。
- en: Analogously to how word embeddings capture similarities between words, LLMs
    capture similarities between longer text sequences. For example, LLMs ’understand’
    that the sentences ’this is an article on natural language processing’ and ’you
    are reading a publication on automated text analysis’ are similar in meaning despite
    their different wording. LLMs also ’understand’ that ’this is an article on natural
    language processing’ and ’this is *not* an article on natural language processing’
    differ in meaning despite similar wording. LLMs take negations and related structures
    into account and are able to correctly interpret the associated shifts in meaning
    due to their attention mechanism in the underlying transformer architecture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于词嵌入如何捕捉单词之间的相似性，LLMs 捕捉更长文本序列之间的相似性。例如，LLMs “理解”尽管措辞不同，但句子“这是关于自然语言处理的文章”和“你正在阅读一篇关于自动文本分析的出版物”在意义上是相似的。LLMs
    也“理解”尽管措辞相似，“这是关于自然语言处理的文章”和“这*不是*关于自然语言处理的文章”在意义上有所不同。LLMs 考虑了否定及相关结构，并能够通过其底层变换器架构中的注意机制正确解释由此产生的意义转变。
- en: This explains the strong performance of pre-trained Transformer models on many
    downstream tasks. Equivalent to how ’apples’ and ’bananas’ are represented in
    a similar part of the vector space of word embeddings, word sequences with similar
    meanings are represented as being similar within the internal representation of
    an LLM. This allows the models to effectively extrapolate to unseen text sequences
    once they have been fine-tuned or prompted to perform a downstream task (also
    see [[22](#bib.bibx22)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了预训练 Transformer 模型在许多下游任务中表现强劲。类似于“苹果”和“香蕉”在词嵌入向量空间中的相似部分表示，具有相似意义的词序列在
    LLM 的内部表示中也被表示为相似。这使得模型能够在经过微调或提示执行下游任务后，有效地推断到未见过的文本序列（参见 [[22](#bib.bibx22)]）。
- en: For BERT-style encoder models, fine-tuning then constitutes a secondary training
    step that complements the unsupervised pre-training stage with a supervised task-orientation
    stage. It involves feeding the model with a small set of labeled data. During
    fine-tuning, the model thus learns the nuanced relationships between text sequences
    and the labels associated with them. This ’knowledge’ can then be used to classify
    unseen text sequences by predicting their most likely label.³³3Except for the
    more nuanced language representation contained within the pre-trained model, fine-tuning
    is similar to the use of supervised learning with bag-of-words or word embeddings
    representations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BERT 风格的编码器模型来说，微调构成了一个补充无监督预训练阶段的监督任务导向阶段的次级训练步骤。它包括用少量标记数据来训练模型。在微调过程中，模型从而学习文本序列与其关联标签之间的细微关系。这些“知识”可以用来通过预测最可能的标签来分类未见过的文本序列。³³3除了预训练模型中包含的更细微的语言表示外，微调类似于使用具有词袋或词嵌入表示的监督学习。
- en: For GPT-style decoder models, fine-tuning is not *per se* required. However,
    model performance following prompts will generate high-quality output only if
    the model has seen sufficient examples of the task at hand during its pre-training
    (& post-training) phase. As we discuss in more detail in Section 6, we argue that
    for most classification tasks this requirement appears to not be sufficiently
    satisfied — thereby explaining the better performance of fine-tuned models in
    our analysis.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT 风格的解码器模型，微调并不是*必须的*。然而，模型在提示后的性能将仅在模型在其预训练（& 后训练）阶段见过足够的任务示例时产生高质量输出。正如我们在第
    6 节中更详细地讨论的那样，我们认为对于大多数分类任务，这一要求似乎没有得到充分满足——这也解释了我们分析中微调模型表现更好的原因。
- en: '4 Method: Fine-tuned LLMs vs. Zero-Shot Generative AI Models'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法：微调 LLMs 与零样本生成性 AI 模型
- en: In the following, we compare the text classification performance of smaller,
    fine-tuned LLMs with that of three major generative AI models (ChatGPT with GPT-3.5
    / GPT-4, and Claude Opus) and BART. We evaluate all models across four heterogeneous
    case studies, which span a diverse set of classification tasks. These tasks vary
    by concept (sentiment, stance, and emotions), text form (news, tweets, and speeches),
    language (English and German), and number of classes (binary vs. multi-class).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对比较小且经过微调的语言模型（LLMs）与三大主要生成性人工智能模型（ChatGPT 的 GPT-3.5 / GPT-4 和 Claude
    Opus）以及 BART 的文本分类性能。我们在四个异质案例研究中评估所有模型，这些研究涵盖了各种分类任务。这些任务在概念（情感、立场和情绪）、文本形式（新闻、推文和演讲）、语言（英语和德语）以及类别数量（二分类与多分类）上有所不同。
- en: 'For smaller, fine-tuned LLMs, we assess five leading models of different sizes
    and architectures (RoBERTa Base, RoBERTa Large, DeBERTa V3, Electra Large, and
    XLNet). We fine-tune these models using our toolkit provided with this paper.⁴⁴4Our
    toolkit, including detailed step-by-step guidance, is available [here](https://github.com/mnbucher/text-cls-llms).
    We use our default hyperparameter setup throughout to showcase the capabilities
    of this setup, which we argue will be of greatest interest to most users, as no
    hyperparameter tuning is required (but possible; see Figure [3](#S4.F3 "Figure
    3 ‣ 4 Method: Fine-tuned LLMs vs. Zero-Shot Generative AI Models ‣ Fine-Tuned
    ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in
    Text Classification") for a suggested workflow summary).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的微调 LLMs，我们评估了五种不同大小和架构的领先模型（RoBERTa Base、RoBERTa Large、DeBERTa V3、Electra
    Large 和 XLNet）。我们使用本文提供的工具包对这些模型进行微调。⁴⁴4我们的工具包，包括详细的逐步指导，见 [这里](https://github.com/mnbucher/text-cls-llms)。我们在整个过程中使用默认的超参数设置，以展示该设置的能力，我们认为这将是大多数用户最感兴趣的，因为不需要（但可以）进行超参数调整（有关建议的工作流程总结，请参见图
    [3](#S4.F3 "图 3 ‣ 4 方法：微调 LLMs 与零-shot 生成 AI 模型 ‣ 微调的‘小型’ LLMs 在文本分类中仍显著优于零-shot
    生成 AI 模型")）。
- en: '![Refer to caption](img/fc546cfdb474b0f7863cfe5ebaf0ad47.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fc546cfdb474b0f7863cfe5ebaf0ad47.png)'
- en: 'Figure 3: Schematic representation of the workflow with our toolkit'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用我们的工具包的工作流程示意图
- en: 'For all fine-tuned models, we proceed in the same manner across all case studies.
    Each dataset is split into training and test sets, with the test set reserved
    for evaluating model predictions on unseen data. We then fine-tune the models
    for each case study and compare their classification performance on the test set
    using standard metrics: Accuracy, Precision, Recall, and F1-scores. To account
    for variability due to random seed initialization, we train each model three times
    with different seeds and report the mean and standard deviation for each metric
    across the three runs.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有微调的模型，我们在所有案例研究中采取相同的方法。每个数据集被拆分为训练集和测试集，测试集用于评估模型对未见数据的预测。然后，我们对每个案例研究微调模型，并使用标准指标：准确率、精确度、召回率和
    F1 分数，比较它们在测试集上的分类性能。为了考虑由于随机种子初始化带来的变异性，我们用不同的种子训练每个模型三次，并报告三次运行中每个指标的均值和标准差。
- en: For the zero-shot LLMs, we prompt all models to label the full datasets we used
    for evaluating the fine-tuned models (and not just the test sets). The prompts
    include a description of the dataset and the classification categories as well
    as the instruction to assign the individual observations to the most appropriate
    category (detailed prompts for all models are provided in the online appendix).
    Because the zero-shot approach requires no training data for fine-tuning, we do
    not split the data into training and test sets. Instead, we prompt the models
    to label all observations in the dataset. We then evaluate model performance by
    comparing the model-generated labels with the original human-annotated labels,
    using the same performance metrics as for the fine-tuned models. We provide the
    full prompts in Appendix A.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于零-shot LLMs，我们提示所有模型对我们用于评估微调模型的完整数据集进行标注（而不仅仅是测试集）。这些提示包括数据集的描述和分类类别，以及将各个观察结果分配到最合适类别的指令（所有模型的详细提示见在线附录）。由于零-shot
    方法不需要用于微调的训练数据，我们不会将数据拆分为训练集和测试集。相反，我们提示模型对数据集中的所有观察结果进行标注。然后，我们通过比较模型生成的标签与原始人工标注的标签来评估模型性能，使用与微调模型相同的性能指标。完整的提示见附录
    A。
- en: 'In summary, we evaluate the following models across our case studies:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们在我们的案例研究中评估了以下模型：
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MAJ-VOT — As a baseline, we use a failed classifier that assigns the majority
    class to all observations. For imbalanced datasets, this can suggest good performance
    on metrics sensitive to class-imbalance (e.g., Accuracy). However, metrics such
    as Recall or F1-Score reveal that this classifier performs poorly for the minority
    class(es).
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MAJ-VOT — 作为基准，我们使用一个失败的分类器，将所有观察结果分配给多数类。对于不平衡的数据集，这可能会在对类不平衡敏感的指标（例如，准确率）上表现良好。然而，像召回率或
    F1 分数这样的指标则表明该分类器在少数类的表现很差。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ROB-BASE (125M) — RoBERTa Base [[24](#bib.bibx24)], provided as ’roberta-base’
    via Hugging Face (HF) [[44](#bib.bibx44)], is pre-trained in a self-supervised
    manner using masked language modeling (MLM).
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ROB-BASE (125M) — RoBERTa Base [[24](#bib.bibx24)]，通过 Hugging Face (HF) [[44](#bib.bibx44)]
    提供为’roberta-base’，使用掩码语言建模 (MLM) 进行自监督预训练。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ROB-LRG (355M) — RoBERTa Large, provided as ’roberta-large’, is similar to RoBERTa
    Base but has more parameters. This boosts performance but increases computational
    costs.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ROB-LRG (355M) — RoBERTa Large，提供为 ’roberta-large’，与 RoBERTa Base 类似，但具有更多参数。这提高了性能，但增加了计算成本。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DEB-V3 (435M) — DeBERTa, provided as ’microsoft/deberta-v3-large’, builds on
    BERT/RoBERTa. Despite its similar size, it takes substantially longer to fine-tune
    because of differences in architecture.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DEB-V3 (435M) — DeBERTa，提供为 ’microsoft/deberta-v3-large’，基于 BERT/RoBERTa。尽管大小相似，但由于架构差异，微调所需时间显著较长。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ELE-LRG (335M) — ELECTRA Large, provided as ’google/electra-large-discriminator’
    and ’german-nlp-group/electra-base-german-uncased’, was introduced by [[11](#bib.bibx11)].
    ELECTRA claims to be more compute-efficient than existing alternatives.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ELE-LRG (335M) — ELECTRA Large，由 ’google/electra-large-discriminator’ 和 ’german-nlp-group/electra-base-german-uncased’
    提供，由 [[11](#bib.bibx11)] 介绍。ELECTRA 声称比现有替代方案更具计算效率。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: XLNET-LRG (340M) — XLNet [[45](#bib.bibx45)], provided as ’xlnet-large-cased’,
    was pre-trained with an autoregressive method to learn bidirectional contexts
    and has beens shown to perform comparable to RoBERTa.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XLNET-LRG (340M) — XLNet [[45](#bib.bibx45)]，提供为 ’xlnet-large-cased’，采用自回归方法预训练以学习双向上下文，表现出与
    RoBERTa 相当的性能。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BART-LRG (407M) — BART was introduced by Facebook [[23](#bib.bibx23)] and uses
    an encoder-decoder model. BART can generate text similar to ChatGPT and Claude,
    although relying on a different architecture. More details are provided in Appendix
    B.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BART-LRG (407M) — BART 由 Facebook [[23](#bib.bibx23)] 介绍，使用编码器-解码器模型。BART 可以生成类似于
    ChatGPT 和 Claude 的文本，尽管依赖于不同的架构。更多详细信息请参见附录 B。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-3.5 (175B+) / GPT-4 (n/a) — ChatGPT, developed by OpenAI [[30](#bib.bibx30)].
    It has substantially more parameters than any of the previsouly listed models
    (GPT-3.5 = 175B, GPT-4 $\approx$ 1.8T ⁵⁵5The exact number of parameters for GPT-4
    is unknown given OpenAI no longer reveals technical details of its models but
    many sources claim it might be around 1.8T. ChatGPT can process complex input
    texts and generate text of varying length as output. We use the ’gpt-3.5-turbo’
    and ’gpt-4-1106-preview’ checkpoints provided by their API. Details about prompting
    are provided in Appendix A.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-3.5 (175B+) / GPT-4 (n/a) — 由 OpenAI 开发的 ChatGPT [[30](#bib.bibx30)]。它的参数数量显著多于之前列出的任何模型（GPT-3.5
    = 175B，GPT-4 ≈ 1.8T ⁵⁵5GPT-4 的确切参数数量未知，因为 OpenAI 不再透露其模型的技术细节，但许多来源声称可能约为 1.8T。ChatGPT
    能处理复杂的输入文本并生成不同长度的输出文本。我们使用了其 API 提供的 ’gpt-3.5-turbo’ 和 ’gpt-4-1106-preview’ 检查点。关于提示的详细信息请参见附录
    A。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CLD-OPUS (n/a) — Claude 3 Opus, developed by Anthropic [[1](#bib.bibx1)]. The
    model size and technical details remain undisclosed. Opus can handle complex input
    prompts on par with GPT4, according to various benchmarks and empirical results
    published by Anthropic. We use the ’claude-3-opus-20240229’ checkpoint provided
    by their API. Details about prompting are provided in Appendix A.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CLD-OPUS (n/a) — Claude 3 Opus，由 Anthropic [[1](#bib.bibx1)] 开发。模型大小和技术细节仍未公开。根据
    Anthropic 发布的各种基准和实证结果，Opus 可以处理复杂的输入提示，与 GPT-4 相当。我们使用了其 API 提供的 ’claude-3-opus-20240229’
    检查点。关于提示的详细信息请参见附录 A。
- en: 5 Results
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: This section presents the results of our comparison. The first four subsections
    detail the findings for each case study for both fine-tuned and zero-shot prompted
    models. The fifth subsection reports the results of our ablation studies, which
    analyze the impact of training data size on the performance of our preferred fine-tuned
    model, RoBERTa Large.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了我们比较的结果。前四个子节详细介绍了每个案例研究的发现，包括经过微调和零-shot提示模型的情况。第五个子节报告了我们消融研究的结果，分析了训练数据量对我们首选的微调模型
    RoBERTa Large 性能的影响。
- en: 5.1 Sentiment Analysis on The New York Times Coverage of the US Economy
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 对《纽约时报》美国经济报道的情感分析
- en: In our first case study, we perform sentiment analysis — perhaps the most widely-studied
    problem in NLP. Sentiment analysis involves classifying statements into (i) positive
    and negative or (ii) positive, negative, and neutral categories.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个案例研究中，我们进行情感分析——这可能是自然语言处理（NLP）中研究最广泛的问题。情感分析涉及将陈述分类为（i）积极和消极，或（ii）积极、消极和中立类别。
- en: The dataset is taken from [[2](#bib.bibx2)]. It contains information on the
    sentiment of articles about the US economy from The New York Times. In the original
    study, the authors compare three dictionary-based approaches for sentiment analysis
    with a supervised machine learning approach (bag-of-words in combination with
    logistic regression). The machine learning model is reported to achieve an Accuracy
    of 0.71 and a Precision of 0.713, clearly surpassing the dictionaries (highest
    Accuracy = 60.5, highest Precision = 45.7).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集来自[[2](#bib.bibx2)]。它包含了《纽约时报》关于美国经济文章的情感信息。在原始研究中，作者比较了三种基于词典的情感分析方法与一种监督学习方法（词袋模型结合逻辑回归）。报告指出，机器学习模型的准确率为0.71，精确度为0.713，明显超过词典（最高准确率=60.5，最高精确度=45.7）。
- en: 'Table 1: Results for Sentiment Analysis (US Economy)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：情感分析（美国经济）结果
- en: '| Model Name | Accuracy | Prec. (wgt.) | Recall (wgt.) | F1 (macro) | F1 (wgt.)
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 准确率 | 精确度 (加权) | 召回率 (加权) | F1 (宏观) | F1 (加权) |'
- en: '| MAJ-VOT | $0.73\,\scriptstyle{(\pm 0.00)}$ | $0.61\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MAJ-VOT | $0.73\,\scriptstyle{(\pm 0.00)}$ | $0.61\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| ROB-BASE | $0.89\scriptstyle{\,(\pm 0.00)}$ | $0.89\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ROB-BASE | $0.89\scriptstyle{\,(\pm 0.00)}$ | $0.89\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| ROB-LRG | $\bm{0.92\,\scriptstyle{(\pm 0.01)}}$ | $\bm{0.92\,\scriptstyle{(\pm
    0.01)}}$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ROB-LRG | $\bm{0.92\,\scriptstyle{(\pm 0.01)}}$ | $\bm{0.92\,\scriptstyle{(\pm
    0.01)}}$ |'
- en: '| DEB-V3 | $0.92\,\scriptstyle{(\pm 0.02)}$ | $0.92\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| DEB-V3 | $0.92\,\scriptstyle{(\pm 0.02)}$ | $0.92\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| ELE-LRG | $0.90\,\scriptstyle{(\pm 0.01)}$ | $0.90\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ELE-LRG | $0.90\,\scriptstyle{(\pm 0.01)}$ | $0.90\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| XLNET-LRG | $0.81\,\scriptstyle{(\pm 0.01)}$ | $0.82\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| XLNET-LRG | $0.81\,\scriptstyle{(\pm 0.01)}$ | $0.82\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| BART-LRG | $0.85\,\scriptstyle{(\pm 0.00)}$ | $0.84\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| BART-LRG | $0.85\,\scriptstyle{(\pm 0.00)}$ | $0.84\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-3.5 | $0.82\,\scriptstyle{(\pm 0.00)}$ | $0.83\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | $0.82\,\scriptstyle{(\pm 0.00)}$ | $0.83\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-4 | $0.87\,\scriptstyle{(\pm 0.00)}$ | $0.87\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $0.87\,\scriptstyle{(\pm 0.00)}$ | $0.87\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| CLD-OPUS | $0.86\,\scriptstyle{(\pm 0.00)}$ | $0.87\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| CLD-OPUS | $0.86\,\scriptstyle{(\pm 0.00)}$ | $0.87\,\scriptstyle{(\pm 0.00)}$
    |'
- en: 'Note: Results for fine-tuned models on unseen test set with $N=200$. Results
    for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation
    with 8 steps and batch size 4, except DEB-V3 (batch size 2).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注：结果为在未见测试集上的微调模型，$N=200$。BART、GPT和Claude在全数据上的结果。微调模型使用8步梯度累积和批量大小4，除DEB-V3外（批量大小2）。
- en: For our analysis, we use the dataset ’3SU’. It contains $4195$ observations,
    where $N=200$ negative labels.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，我们使用数据集’3SU’。该数据集包含$4195$个观察值，其中$N=200$个负标签。
- en: The task is to predict whether a paragraph about the US economy is positive
    or negative. We fine-tuned all models as described above. Results are presented
    in Table [1](#S5.T1 "Table 1 ‣ 5.1 Sentiment Analysis on The New York Times Coverage
    of the US Economy ‣ 5 Results ‣ Fine-Tuned ‘Small’ LLMs (Still) Significantly
    Outperform Zero-Shot Generative AI Models in Text Classification"). The strongest
    results are achieved by ROB-LRG and DEB-V3.⁶⁶6Weigthed metrics are scaled by class
    size so that larger classes receive more weight. Macro F1 is computed as the arithmetic
    (i.e., unweighted) mean of each F1 score by class. Most other fine-tuned models
    except XLNET-LRG also achieve strong results with values around $0.90$ and small
    variances (indicating that the random seed has little effect on model performance).
    BART, ChatGPT, and Claude also perform well on the sentiment task but remain behind
    the fine-tuned models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是预测关于美国经济的段落是积极的还是消极的。我们如上所述微调了所有模型。结果呈现在表[1](#S5.T1 "表1 ‣ 5.1 对《纽约时报》关于美国经济报道的情感分析
    ‣ 5 结果 ‣ 微调‘小型’LLMs（仍然）显著优于零样本生成AI模型的文本分类")中。最强的结果由ROB-LRG和DEB-V3实现。加权指标按类别大小进行缩放，以便较大的类别获得更多权重。宏观F1作为每个类别F1分数的算术（即非加权）平均值计算。除XLNET-LRG外，大多数其他微调模型也获得了强劲的结果，值约为$0.90$且方差较小（表明随机种子对模型性能的影响很小）。BART、ChatGPT和Claude在情感任务上也表现良好，但仍落后于微调模型。
- en: 5.2 Stance Classification on Tweets about Kavanaugh Nomination
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 关于卡瓦诺提名的推文立场分类
- en: 'Our second case study investigates stance classification. Stance and sentiment
    are conceptually different: Whereas sentiment reflects the tone of a text from
    positive to negative, stance captures a positional attitude from supportive to
    opposing. For example, a statement such as ’I am happy that the government was
    voted out of office.’ has a positive sentiment but an opposing stance.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个案例研究调查了立场分类。立场和情感在概念上是不同的：情感反映了文本从积极到消极的语气，而立场则捕捉从支持到反对的态度。例如，像’我很高兴政府被投票罢免了。’这样的话语具有积极的情感，但立场是对立的。
- en: The dataset is taken from [[3](#bib.bibx3)]⁷⁷7kavanaugh_tweets_groundtruth.csv.
    It contains manually labeled tweets in which people express their view on the
    2018 nomination of Brett Kavanaugh to the U.S. Supreme Court. In the original
    study, the authors evaluate two dictionary methods, a bag-of-words approach combined
    with a support-vector machine (SVM), and a BERT language model. Both machine learning
    approaches are reported to outperform the dictionaries. BERT achieves an F1-score
    of $0.938\,(\pm 0.002)$, although the SVM performs almost identically.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集取自[[3](#bib.bibx3)]⁷⁷7kavanaugh_tweets_groundtruth.csv。它包含手动标注的推文，人们在这些推文中表达了他们对2018年布雷特·卡瓦诺提名至美国最高法院的看法。在原始研究中，作者评估了两种字典方法，一种是与支持向量机（SVM）结合的词袋方法，另一种是BERT语言模型。报告显示，这两种机器学习方法都优于字典方法。BERT的F1分数为$0.938\,(\pm
    0.002)$，虽然SVM的表现几乎相同。
- en: 'Table 2: Results for Stance Classification (Nomination Approval)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：立场分类结果（提名批准）
- en: '| Model Name | Accuracy | Prec. (wgt.) | Recall (wgt.) | F1 (macro) | F1 (wgt.)
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 准确率 | 精度（加权） | 召回率（加权） | F1（宏观） | F1（加权） |'
- en: '| MAJ-VOT | $0.50\,\scriptstyle{(\pm 0.00)}$ | $0.33\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MAJ-VOT | $0.50\,\scriptstyle{(\pm 0.00)}$ | $0.33\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| ROB-BASE | $0.86\,\scriptstyle{(\pm 0.01)}$ | $0.86\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ROB-BASE | $0.86\,\scriptstyle{(\pm 0.01)}$ | $0.86\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| ROB-LRG | $0.92\,\scriptstyle{(\pm 0.01)}$ | $0.92\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ROB-LRG | $0.92\,\scriptstyle{(\pm 0.01)}$ | $0.92\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| DEB-V3 | $\bm{0.94\,\scriptstyle{(\pm 0.01)}}$ | $\bm{0.94\,\scriptstyle{(\pm
    0.01)}}$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| DEB-V3 | $\bm{0.94\,\scriptstyle{(\pm 0.01)}}$ | $\bm{0.94\,\scriptstyle{(\pm
    0.01)}}$ |'
- en: '| ELE-LRG | $0.74\,\scriptstyle{(\pm 0.01)}$ | $0.69\,\scriptstyle{(\pm 0.02)}$
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ELE-LRG | $0.74\,\scriptstyle{(\pm 0.01)}$ | $0.69\,\scriptstyle{(\pm 0.02)}$
    |'
- en: '| XLNET-LRG | $0.83\,\scriptstyle{(\pm 0.01)}$ | $0.83\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| XLNET-LRG | $0.83\,\scriptstyle{(\pm 0.01)}$ | $0.83\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| BART-LRG | $0.53\,\scriptstyle{(\pm 0.00)}$ | $0.44\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| BART-LRG | $0.53\,\scriptstyle{(\pm 0.00)}$ | $0.44\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-3.5 | $0.53\,\scriptstyle{(\pm 0.00)}$ | $0.47\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | $0.53\,\scriptstyle{(\pm 0.00)}$ | $0.47\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-4 | $0.58\,\scriptstyle{(\pm 0.00)}$ | $0.51\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $0.58\,\scriptstyle{(\pm 0.00)}$ | $0.51\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| CLD-OPUS | $0.61\,\scriptstyle{(\pm 0.00)}$ | $0.57\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| CLD-OPUS | $0.61\,\scriptstyle{(\pm 0.00)}$ | $0.57\,\scriptstyle{(\pm 0.00)}$
    |'
- en: 'Note: Results for fine-tuned models on unseen test set with $N=200$. Results
    for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation
    with 8 steps and batch size 4, except DEB-V3 (batch size 2).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注：在未见过的测试集上微调模型的结果为$N=200$。BART、GPTs和Claude在完整数据上的结果。微调模型使用8步梯度累积和批量大小4，DEB-V3除外（批量大小2）。
- en: We use the ’text’ and ’stance’ columns from the dataset and apply some pre-processing
    by removing ’RT’ flags (Retweets), twitter handles, and URLs. Afterwards, several
    tweets have identical text content (original and cleaned re-tweets). We therefore
    aggregate identical tweets via majority voting on the class label. The final dataset
    has $N=1173$ observations for the $0$ observations for the test set.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用数据集中的’test’和’stance’列，并通过删除’RT’标记（转发）、Twitter用户名和URLs来进行一些预处理。之后，多个推文具有相同的文本内容（原始和清理后的转发）。因此，我们通过对类标签进行多数投票来聚合相同的推文。最终数据集包含$N=1173$个观测值和测试集中的$0$个观测值。
- en: 'The classification task is to predict whether a tweet supports or opposes Kavanaugh’s
    nomination. We train the same models as before. Results are reported in Table
    [2](#S5.T2 "Table 2 ‣ 5.2 Stance Classification on Tweets about Kavanaugh Nomination
    ‣ 5 Results ‣ Fine-Tuned ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot
    Generative AI Models in Text Classification"). We achieve the strongest results
    with DEB-V3 and ROB-LRG with a performance of $0.94$ respectively across metrics.
    The other three fine-tuned models show some variation. However, all fine-tuned
    models surpass the prompt-based approaches: BART, ChatGPT, and Claude perform
    rather weak on this more subtle task and score only marginally above the naive
    majority vote baseline.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务是预测推文是否支持或反对卡瓦诺的提名。我们训练了与之前相同的模型。结果见表[2](#S5.T2 "表2 ‣ 5.2 卡瓦诺提名推文的立场分类 ‣
    5 结果 ‣ 微调‘小型’LLM（仍然）显著优于零样本生成AI模型的文本分类")。我们在DEB-V3和ROB-LRG上获得了最强的结果，两者在各项指标上的表现均为$0.94$。其他三种微调模型显示出一些变化。然而，所有微调模型都超越了基于提示的方法：BART、ChatGPT和Claude在这个更微妙的任务上表现较弱，仅略高于简单的多数投票基线。
- en: 5.3 Emotion Detection on Political Texts in German
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 德语政治文本的情感检测
- en: Next, we turn to emotion classification, an application that has recently seen
    increased attention by social scientists [[17](#bib.bibx17), [39](#bib.bibx39),
    [29](#bib.bibx29)]. Emotions affect political discourse, behavior, and opinion
    formation. Yet emotional tone goes beyond positive vs. negative sentiment or supportive
    vs. opposing stance. For example, while anger and fear are negative in terms of
    sentiment, they can generate different political behavior.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们转向情感分类，这是一个最近受到社会科学家增加关注的应用[[17](#bib.bibx17), [39](#bib.bibx39), [29](#bib.bibx29)]。情感影响政治话语、行为和观点形成。然而，情感语调超越了积极与消极情感或支持与反对立场。例如，虽然愤怒和恐惧在情感上是负面的，但它们可以产生不同的政治行为。
- en: Our dataset comes from [[43](#bib.bibx43)]. The corpus contains crowd-coded
    snippets with political content in German from three countries (Germany, Austria,
    and Switzerland) and two different sources (parliamentary speeches and Facebook
    posts).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集来自[[43](#bib.bibx43)]。语料库包含来自三个国家（德国、奥地利和瑞士）和两个不同来源（议会演讲和Facebook帖子）的德语政治内容众包编码片段。
- en: 'In their study, the authors investigate eight emotions and compare several
    methods for emotion detection: An emotion dictionary, a classifier based on word
    embeddings, and a language model (ELECTRA). The strongest results are reported
    for ’Anger’ (the most frequent emotion in the data), for which ELECTRA achieves
    an F1-Score of 0.84 (word embeddings = 0.79, dictionary = 0.59).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究中，作者调查了八种情感，并比较了几种情感检测方法：情感词典、基于词嵌入的分类器和语言模型（ELECTRA）。报告了最强的结果是对’愤怒’（数据中最频繁的情感），ELECTRA的F1-Score为0.84（词嵌入
    = 0.79，词典 = 0.59）。
- en: In our case study, we focus on Anger detection. We perform some pre-processing
    to aggregate labels if multiple coders rated the same snippet. Our final dataset
    has $N=7969$ non-angry (0) cases.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究中，我们专注于愤怒检测。如果多个编码者对相同片段进行了评分，我们会进行一些预处理以汇总标签。我们的最终数据集有$N=7969$个非愤怒（0）案例。
- en: 'Since the corpus is in German, we had two modeling options: Either (i) use
    the German text with a German-language LLM, or (ii) translate the corpus into
    English before fine-tuning our English-language models. We decided to evaluate
    both options. For the translation we used the DeepL API ⁸⁸8https://www.deepl.com/en/pro-api.
    Both versions are identical in terms of labels and class distribution.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语料库是德语的，我们有两个建模选项：（i）使用德语文本和德语语言LLM，或（ii）在微调我们的英语模型之前将语料库翻译成英语。我们决定评估这两种选项。翻译使用了DeepL
    API ⁸⁸8https://www.deepl.com/en/pro-api。两个版本在标签和类别分布方面是相同的。
- en: 'Table 3: Results for Emotion Detection (Anger)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：情感检测结果（愤怒）
- en: '| Model Name | Accuracy | Prec. (wgt.) | Recall (wgt.) | F1 (macro) | F1 (wgt.)
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 准确率 | 精确率（加权） | 召回率（加权） | F1（宏观） | F1（加权） |'
- en: '| MAJ-VOT | $0.71\,\scriptstyle{(\pm 0.00)}$ | $0.59\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MAJ-VOT | $0.71\,\scriptstyle{(\pm 0.00)}$ | $0.59\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| ROB-BASE | $0.87\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ROB-BASE | $0.87\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| ROB-LRG | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ROB-LRG | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| DEB-V3 | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| DEB-V3 | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| ELE-LRG | $0.88\,\scriptstyle{(\pm 0.00)}$ | $0.88\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ELE-LRG | $0.88\,\scriptstyle{(\pm 0.00)}$ | $0.88\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| XLNET-LRG | $\bm{0.89\,\scriptstyle{(\pm 0.00)}}$ | $\bm{0.89\,\scriptstyle{(\pm
    0.00)}}$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| XLNET-LRG | $\bm{0.89\,\scriptstyle{(\pm 0.00)}}$ | $\bm{0.89\,\scriptstyle{(\pm
    0.00)}}$ |'
- en: '| ELE-BS-GER | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm
    0.01)}$ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ELE-BS-GER | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm
    0.01)}$ |'
- en: '| BART-LRG | $0.26\,\scriptstyle{(\pm 0.00)}$ | $0.29\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| BART-LRG | $0.26\,\scriptstyle{(\pm 0.00)}$ | $0.29\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-3.5 | $0.15\,\scriptstyle{(\pm 0.00)}$ | $0.16\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | $0.15\,\scriptstyle{(\pm 0.00)}$ | $0.16\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-4 | $0.20\,\scriptstyle{(\pm 0.00)}$ | $0.13\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $0.20\,\scriptstyle{(\pm 0.00)}$ | $0.13\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| CLD-OPUS | $0.15\,\scriptstyle{(\pm 0.00)}$ | $0.11\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CLD-OPUS | $0.15\,\scriptstyle{(\pm 0.00)}$ | $0.11\,\scriptstyle{(\pm 0.00)}$
    |'
- en: 'Note: Results for fine-tuned models on unseen test set with $N=200$. Results
    for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation
    with 8 steps and batch size 4, except DEB-V3 (batch size 2).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注：对于未见测试集的微调模型，$N=200$。BART、GPTs和Claude在全数据上的结果。微调模型使用8步梯度累积和批量大小4，DEB-V3除外（批量大小2）。
- en: The prediction task is to identify whether a snippet contains an expression
    of anger. The results are presented in Table [3](#S5.T3 "Table 3 ‣ 5.3 Emotion
    Detection on Political Texts in German ‣ 5 Results ‣ Fine-Tuned ‘Small’ LLMs (Still)
    Significantly Outperform Zero-Shot Generative AI Models in Text Classification").
    XLNET-LRG achieves the best results, closely followed by ELECTRA, ROB-LRG, and
    DEB-V3\. The ELECTRA model performs almost identically on the original German
    text and on the English translation. Translation can thus be a viable option when
    no language-specific pre-trained model exists.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 预测任务是识别片段是否包含愤怒的表达。结果见表[3](#S5.T3 "表3 ‣ 5.3 德语政治文本情感检测 ‣ 5 结果 ‣ 微调‘小型’LLM（仍然）显著优于零-shot生成AI模型在文本分类中的表现")。XLNET-LRG取得了最佳结果，其次是ELECTRA、ROB-LRG和DEB-V3。ELECTRA模型在原始德语文本和英文翻译上表现几乎相同。因此，当没有特定语言的预训练模型时，翻译可以是一个可行的选项。
- en: BART, ChatGPT, and Claude all have considerable difficulties with this task
    and do substantially worse than our naive classifier. These results suggest that
    zero-shot classification works best for standard tasks but does not travel well
    to more specialized use-cases and nuanced semantics (which the models will have
    encountered less frequently during training).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: BART、ChatGPT和Claude在这个任务上都有相当大的困难，表现明显不如我们的简单分类器。这些结果表明，零-shot分类在标准任务中效果最佳，但在更专业的应用场景和细致的语义方面（这些模型在训练中遇到的频率较低）表现不佳。
- en: 5.4 Multi-Class Stance Classification on Parties’ EU Positions
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 多类别立场分类：政党的欧盟立场
- en: In our final study, we investigate European nationalist party positions. The
    dataset comes from [[25](#bib.bibx25)]. It contains hand-coded evaluations of
    parties’ positions toward the EU and European integration as reported in major
    European newspapers between the run-up to the 2016 Brexit referendum and the UK’s
    2020 EU exit. Positions range from acceptance of the European status-quo to outright
    demands to leave the EU.⁹⁹9The corpus covers ten EU countries. For a list of countries
    and parties, see [[25](#bib.bibx25)], Table 1).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最终研究中，我们调查了欧洲民族主义政党的立场。数据集来自[[25](#bib.bibx25)]。它包含了对政党在欧盟及欧洲一体化问题上立场的人工编码评估，这些评估记录在主要的欧洲报纸中，涵盖了从2016年脱欧公投的准备阶段到英国2020年脱欧期间。立场从接受欧洲现状到明确要求离开欧盟不等。⁹⁹9该语料库涵盖了十个欧盟国家。有关国家和政党的列表，请参见[[25](#bib.bibx25)]，表1）。
- en: 'This setup implies a one-sided multi-class stance classification task: Stance
    because we consider party opposition (= opposing stance) toward European integration,
    one-sided because positions only range from neutral to strongly opposed (neglecting
    the supportive side of the spectrum), and multi-class because party positions
    are measured in three grades.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置意味着一个单方面的多类别立场分类任务：立场因为我们考虑政党的反对（= 反对立场）欧洲一体化，单方面因为立场仅从中立到强烈反对（忽略支持一方的范围），多类别因为政党的立场分为三个等级。
- en: 'Table 4: Results for Multi-Class Stance Classification (EU Positions)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：多类别立场分类（欧盟立场）结果
- en: '| Model Name | Accuracy | Prec. (wgt.) | Recall (wgt.) | F1 (macro) | F1 (wgt.)
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 准确率 | 精度（加权） | 召回率（加权） | F1（宏） | F1（加权） |'
- en: '| MAJ-VOT | $0.83\,\scriptstyle{(\pm 0.00)}$ | $0.75\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| MAJ-VOT | $0.83\,\scriptstyle{(\pm 0.00)}$ | $0.75\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| ROB-BASE | $0.84\,\scriptstyle{(\pm 0.00)}$ | $0.85\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| ROB-BASE | $0.84\,\scriptstyle{(\pm 0.00)}$ | $0.85\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| ROB-LRG | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.87\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ROB-LRG | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.87\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| DEB-V3 | $\bm{0.92\,\scriptstyle{(\pm 0.01)}}$ | $\bm{0.91\,\scriptstyle{(\pm
    0.01)}}$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DEB-V3 | $\bm{0.92\,\scriptstyle{(\pm 0.01)}}$ | $\bm{0.91\,\scriptstyle{(\pm
    0.01)}}$ |'
- en: '| ELE-LRG | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.87\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ELE-LRG | $0.88\,\scriptstyle{(\pm 0.01)}$ | $0.87\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| XLNET-LRG | $0.87\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.01)}$
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| XLNET-LRG | $0.87\,\scriptstyle{(\pm 0.01)}$ | $0.88\,\scriptstyle{(\pm 0.01)}$
    |'
- en: '| BART-LRG | $0.82\,\scriptstyle{(\pm 0.00)}$ | $0.75\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| BART-LRG | $0.82\,\scriptstyle{(\pm 0.00)}$ | $0.75\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-3.5 | $0.24\,\scriptstyle{(\pm 0.00)}$ | $0.27\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | $0.24\,\scriptstyle{(\pm 0.00)}$ | $0.27\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| GPT-4 | $0.38\,\scriptstyle{(\pm 0.00)}$ | $0.45\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $0.38\,\scriptstyle{(\pm 0.00)}$ | $0.45\,\scriptstyle{(\pm 0.00)}$
    |'
- en: '| CLD-OPUS | $0.26\,\scriptstyle{(\pm 0.00)}$ | $0.29\,\scriptstyle{(\pm 0.00)}$
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| CLD-OPUS | $0.26\,\scriptstyle{(\pm 0.00)}$ | $0.29\,\scriptstyle{(\pm 0.00)}$
    |'
- en: 'Note: Results for fine-tuned models on unseen test set with $N=200$. Results
    for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation
    with 8 steps and batch size 4, except DEB-V3 (batch size 2).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注：在未见测试集上，微调模型的结果为 $N=200$。BART、GPTs 和 Claude 在全数据上的结果。微调模型使用 8 步梯度累积和批量大小为
    4，除 DEB-V3 外（批量大小为 2）。
- en: 'We translate all news texts from the original languages to English using DeepL.
    We also aggregate the hand-coded euro-sceptic labels into three slightly broader
    categories: Status-quo acceptant or neutral statements are coded as 0 (least opposed),
    policy-level critique and reform demands are coded as 1, and institutional-level
    critique and outright leave-demands are coded as 2 (most opposed). We aggregate
    identical snippets by majority voting and ignore snippets coded as ’not relevant.’'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 DeepL 将所有新闻文本从原语言翻译成英文。我们还将手动编码的欧元怀疑主义标签汇总为三个稍微更广泛的类别：现状接受或中立陈述编码为 0（最少反对），政策级别的批评和改革要求编码为
    1，机构级别的批评和明确的离开要求编码为 2（最强烈反对）。我们通过多数投票汇总相同的片段，并忽略编码为“无关”的片段。
- en: The task is to predict neutral, moderately opposed, or strongly opposed positions
    toward the EU. The dataset has $N=3349$). Results are shown in Table [4](#S5.T4
    "Table 4 ‣ 5.4 Multi-Class Stance Classification on Parties’ EU Positions ‣ 5
    Results ‣ Fine-Tuned ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot Generative
    AI Models in Text Classification"). DEB-V3 outperforms all other models, scoring
    well above 0.90 for most metrics. Most other fine-tuned models perform at least
    decently. BART, ChatGPT, and Claude, however, again do not exceed the naive baseline
    classifier — suggesting once more that the fine-tuning approach is superior when
    tasks are non-standard or context-specific.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是预测对欧盟的中立、适度反对或强烈反对的立场。数据集有 $N=3349$。结果显示在表格 [4](#S5.T4 "表格 4 ‣ 5.4 多类别立场分类关于政党的欧盟立场
    ‣ 5 结果 ‣ 微调的‘小型’ LLM 在文本分类中显著优于零-shot生成AI模型") 中。DEB-V3 的表现优于所有其他模型，大多数指标的得分都高于
    0.90。其他大多数微调模型的表现至少也算不错。然而，BART、ChatGPT 和 Claude 的表现再次未能超越简单的基准分类器——这再次表明，当任务是非标准的或特定于上下文时，微调方法更为优越。
- en: '5.5 Fine-Tuning: The Effect of Training Set Size on Model Performance'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 微调：训练集大小对模型性能的影响
- en: In this section, we turn to a complementary analysis aimed at understanding
    the training data requirements for the fine-tuning approach and how they affect
    the final model performance on the unseen test set. Among the five fine-tuned
    LLMs analyzed above, no single model outperforms the others on all tasks, but
    ROB-LRG and DEB-V3 are slightly ahead of the rest. Since ROB-LRG is less computationally
    demanding than DEB-V3, we consider it to provide the best overall balance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们转向一种补充分析，旨在理解微调方法的训练数据需求及其如何影响在未见测试集上的最终模型性能。在上述分析的五种微调 LLM 中，没有单一模型在所有任务上都优于其他模型，但
    ROB-LRG 和 DEB-V3 稍微领先于其他模型。由于 ROB-LRG 的计算需求低于 DEB-V3，我们认为它提供了最佳的整体平衡。
- en: For this analysis, we therefore revisit our four case studies with ROB-LRG,
    but evaluate model performance repeatedly at different training set sizes. For
    each run, we sample $N$ across all runs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将对 ROB-LRG 进行四个案例研究的回顾，但在不同的训练集大小下重复评估模型性能。对于每次运行，我们在所有运行中抽样 $N$。
- en: '![Refer to caption](img/9744ce8e87dddc44f71153231111cab6.png)![Refer to caption](img/138832434e0b40183ba128d13f2d6de1.png)![Refer
    to caption](img/619253294f3bec5a2a8f42debc1fe5e5.png)![Refer to caption](img/9497b868fc4100e97da30831c3140421.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9744ce8e87dddc44f71153231111cab6.png)![参见说明](img/138832434e0b40183ba128d13f2d6de1.png)![参见说明](img/619253294f3bec5a2a8f42debc1fe5e5.png)![参见说明](img/9497b868fc4100e97da30831c3140421.png)'
- en: 'Figure 4: Effect of training set size on model performance: Results for ROB-LRG
    with varying number of training observations $N=\{50,100,200,500,1000\}$. The
    translucent markers above the 0-point denote the zero-shot results of BART. The
    rightmost points denote model performance if trained on the full dataset.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：训练集大小对模型性能的影响：ROB-LRG 在不同数量训练观测值 $N=\{50,100,200,500,1000\}$ 下的结果。0 点以上的半透明标记表示
    BART 的零-shot 结果。最右侧的点表示在全数据集上训练的模型性能。
- en: 'The results are depicted in Figure [4](#S5.F4 "Figure 4 ‣ 5.5 Fine-Tuning:
    The Effect of Training Set Size on Model Performance ‣ 5 Results ‣ Fine-Tuned
    ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in
    Text Classification") and show how model performance increases with training data
    size. For model performance, we report F1 Macro, F1 Weighted, and Accuracy on
    the y-axis, evaluated on the unseen test set. For dataset size, we report sample
    size $N$ on the x-axis.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如图 [4](#S5.F4 "图 4 ‣ 5.5 微调：训练集大小对模型性能的影响 ‣ 5 结果 ‣ 微调的‘小’ LLM (仍然) 显著优于零-shot
    生成 AI 模型在文本分类中的表现") 所示，展示了模型性能如何随着训练数据大小的增加而提高。对于模型性能，我们在 y 轴上报告 F1 Macro、F1 Weighted
    和准确率，这些是在未见测试集上评估的。对于数据集大小，我们在 x 轴上报告样本大小 $N$。
- en: Overall, we see that performance picks up quickly as the amount of training
    data grows from minimal to moderate levels and then saturates as the training
    set size increases further.^(10)^(10)10See [[22](#bib.bibx22)] for an insightful
    comparison of training efficiency between different machine learning approaches
    for text analysis. This effect is most pronounced for more balanced datasets (e.g.,
    the Kavanaugh Nomination Tweets). For imbalanced datasets (especially, EU positions
    data), we see high (but meaningless) accuracy scores at low sample sizes because
    poorly trained models always predict the majority class. Macro-F1 scores, however,
    which punish naive majority class prediction, consistently show the saturation
    pattern. Overall, the sweet-spot tends to lie between 200 and 500 training observations.
    Below 200 observations, model performance remains below its potential. Above 500
    observations, performance begins to level off.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们发现，随着训练数据量从最少到适中水平增长，性能迅速提高，然后随着训练集规模的进一步增加而饱和。^(10)^(10)10有关不同机器学习方法在文本分析中的训练效率的深入比较，请参见
    [[22](#bib.bibx22)]。这一效果在更平衡的数据集（例如 Kavanaugh 提名推文）中最为明显。对于不平衡的数据集（尤其是 EU 立场数据），我们在低样本量下看到高（但无意义的）准确率，因为训练不足的模型总是预测多数类别。然而，宏观
    F1 分数会惩罚简单的多数类预测，一致地显示出饱和模式。总体而言，最佳点往往在 200 到 500 个训练观测值之间。低于 200 个观测值时，模型性能仍未达到其潜力。超过
    500 个观测值时，性能开始趋于平稳。
- en: We also show the results of BART, arguably the most consistent of the zero-shot
    approaches overall (compared to GPT-3.5, GPT-4, and Claude Opus). Since the training
    set for zero-shot classification is essentially zero, we show the BART results
    on the left of each plot (x-axis value = $0$.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了 BART 的结果，这被认为是所有零-shot 方法中最一致的（与 GPT-3.5、GPT-4 和 Claude Opus 相比）。由于零-shot
    分类的训练集实际上为零，我们在每个图的左侧显示了 BART 的结果（x 轴值 = $0$）。
- en: 6 Discussion
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Our key finding is that fine-tuning smaller LLMs with dedicated training data
    is consistently superior to zero-shot prompting larger models such as BART, ChatGPT,
    and Claude Opus. While zero-shot results are decent for common tasks such as sentiment
    analysis, fine-tuned LLMs always surpass zero-shot performance as training set
    size increases. This tendency is more pronounced for less standard tasks such
    as stance classification or emotion detection.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键发现是，相较于对更大模型如 BART、ChatGPT 和 Claude Opus 进行零-shot 提示，用专门训练数据对较小的 LLM 进行微调的效果始终更好。尽管零-shot
    结果对于情感分析等常见任务表现尚可，但微调后的 LLM 总是能在训练集规模增加时超越零-shot 性能。这种趋势在如立场分类或情感检测等不太标准的任务中更为明显。
- en: 'An interesting question is whether next-generation generative AI models will
    catch up with, or even surpass, fine-tuned LLMs in performing specialized tasks.
    In the following, we point to some areas where we expect progress regarding both
    fine-tuning and prompt-based approaches, which may be helpful in structuring expectations
    about future trajectories: (i) data augmentation, (ii) secondary pre-training,
    (iii) model architecture, and (iv) prompt engineering and few-shot learning. Lastly,
    we discuss some general considerations when choosing between smaller BERT-like
    LLMs and larger generative AI models in production settings.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的问题是下一代生成式 AI 模型是否会赶上，甚至超越，在执行专门任务方面的细调 LLMs。以下，我们指出了一些我们期望在细调和基于提示的方法方面取得进展的领域，这可能有助于构建对未来发展轨迹的预期：（i）数据增强，（ii）二次预训练，（iii）模型架构，以及（iv）提示工程和少样本学习。最后，我们讨论了在生产环境中选择较小的类似
    BERT 的 LLMs 和较大的生成式 AI 模型时的一些一般性考虑因素。
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Augmentation: Data augmentation is an approach to synthetically increase
    labeled training data without human input. One approach in NLP is back translation,
    where the original text is translated into another language and then back-translated.
    Adding the back-translated sentence to the training data (with the same label
    as the original) can boost performance as shown by [[35](#bib.bibx35)]. Another
    option is token perturbation, where certain words are replaced with synonyms [[40](#bib.bibx40)].
    Going forward, these techniques can further reduce data requirements for fine-tuning
    LLMs.'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据增强：数据增强是一种合成增加标记训练数据的方法，无需人工干预。在 NLP 中的一种方法是回译，即将原始文本翻译成另一种语言，然后再回译。将回译后的句子添加到训练数据中（与原始句子具有相同标签）可以提升性能，如
    [[35](#bib.bibx35)] 所示。另一种选择是词汇扰动，即用同义词替换某些单词 [[40](#bib.bibx40)]。展望未来，这些技术可以进一步减少细调
    LLMs 所需的数据量。
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Secondary Pre-training: An additional pre-training phase on a smaller domain-specific
    corpus can improve both fine-tuned encoder-style and instruction-tuned decoder-style
    LLMs. For RoBERTa, [[20](#bib.bibx20)] demonstrated significant performance gains
    for downstream classification tasks when further pre-trained in the specific domain
    to gain more knowledge about a specific vocabulary and usage before performing
    downstream fine-tuning for the classification task.'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 二次预训练：在较小的领域特定语料库上进行额外的预训练可以改善细调的编码器风格和指令调优的解码器风格的语言模型（LLMs）。对于 RoBERTa，[[20](#bib.bibx20)]
    证明了在特定领域进一步预训练以获取有关特定词汇和用法的更多知识后，对下游分类任务的性能提升显著，这在进行下游细调以完成分类任务之前尤其有效。
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Architecture: While there has been a trend toward larger and larger models,
    recent work has shown that scaling from hundreds of billions to trillions of parameters
    yields saturating improvements and that scale alone may not be the single factor
    that determines performance [[34](#bib.bibx34), [7](#bib.bibx7)]. Instead, research
    has been devoted to investigate multi-modality — the integration of multiple modes
    of information such as text, image and audio [[14](#bib.bibx14)] —,’ensemble’
    (or mixture-of-experts) architectures [[36](#bib.bibx36)], and ways to improve
    training data quality while keeping model size in the billion parameter range
    [[19](#bib.bibx19)].'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型架构：尽管近年来模型规模越来越大，但最近的研究表明，从数百亿到数万亿参数的扩展仅能带来性能的饱和改善，且规模本身可能不是决定性能的唯一因素 [[34](#bib.bibx34),
    [7](#bib.bibx7)]。相反，研究已经转向多模态——即整合文本、图像和音频等多种信息模式 [[14](#bib.bibx14)]——‘集成’（或专家混合）架构
    [[36](#bib.bibx36)]，以及在保持模型规模在十亿参数范围内的同时改善训练数据质量 [[19](#bib.bibx19)]。
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt Engineering and Few-Shot Learning: Prompt engineering can boost the
    performance of instruction-tuned LLMs such as ChatGPT and Claude. One option is
    few-shot learning, which results in so-called in-context learning by providing
    labeled training data as part of the prompt. For GPT-3.5, few-shot prompting yielded
    some performance gains as seen in [[6](#bib.bibx6)]^(11)^(11)11In exploratory
    experiments on our datasets, few-shot prompting did not improve results for GPT-3.5
    and 4.0.. However, few-shot learning requires careful selection of examples to
    provide in the prompt and systematic evaluation of prompt designs. This significantly
    increases the required time investment and multiplies costs due to the increased
    length and number of prompts. Consequently, few-shot learning risks negating the
    simplicity and intuitive handling advantages of generative AI models. Nonetheless,
    with further developments in model architecture, in-context learning via few-shot
    prompting may eventually surpass fine-tuned encoder-style LLMs.'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程和少量学习：提示工程可以提升像ChatGPT和Claude这样的指令微调LLM的性能。一种选择是少量学习，通过将标记训练数据作为提示的一部分，来实现所谓的上下文学习。对于GPT-3.5，少量提示带来了一些性能提升，如[[6](#bib.bibx6)]^(11)^(11)11在我们数据集的探索性实验中，少量提示并未改善GPT-3.5和4.0的结果。然而，少量学习需要仔细选择提示中的示例，并对提示设计进行系统评估。这显著增加了所需的时间投资，并由于提示长度和数量的增加而增加了成本。因此，少量学习有可能会抵消生成AI模型的简单性和直观处理优势。尽管如此，随着模型架构的进一步发展，通过少量提示的上下文学习可能最终会超越微调的编码器风格LLM。
- en: Depending on how the above developments play out, the relative classification
    performance of smaller BERT-style LLMs and larger generative AI models may shift
    to one side or the other. However, at present, fine-tuning of smaller models remains
    the superior approach for text classification.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述发展情况，小型BERT风格LLM和较大生成AI模型的相对分类性能可能会有所变化。然而，目前，小型模型的微调仍然是文本分类的优越方法。
- en: 'Apart from performance, we see additional advantages of relying on smaller,
    fine-tuned LLMs in production settings:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能外，我们还看到在生产环境中依赖小型、微调LLM的额外优势：
- en: With an eye to trade secrets or privacy, smaller models provide an unmatched
    level of control. Because these models are relatively small, they can be stored,
    trained, and run on a local machine. Relatedly, all model parameters are visible
    and can be saved and backed up, providing programmatic stability and predictability.
    This is in stark contrast to proprietary, cloud-based generative AI models, which
    require information to be transferred to an external black-box model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于商业机密或隐私，小型模型提供了无与伦比的控制水平。由于这些模型相对较小，它们可以在本地计算机上存储、训练和运行。此外，所有模型参数都是可见的，并且可以保存和备份，提供了程序性的稳定性和可预测性。这与需要将信息传输到外部黑箱模型的专有云生成AI模型形成了鲜明对比。
- en: Additionally, it is important to remember that professional use of generative
    AI models requires labeled data for validation — even when using zero-shot prompting.
    Given the relatively better performance of smaller fine-tuned models with limited
    training data, along with their advantages regarding control and privacy, it seems
    likely that models like BERT and similar LLMs will remain valuable tools for years
    to come.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得记住的是，专业使用生成AI模型需要标记数据进行验证——即使在使用零-shot提示时也是如此。鉴于小型微调模型在有限训练数据下相对更好的性能，以及它们在控制和隐私方面的优势，像BERT及类似LLM的模型似乎在未来几年将继续是有价值的工具。
- en: 7 Conclusion
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we investigate whether zero-shot prompted generative AI models
    such as ChatGPT and Claude Opus can already outperform fitted models such as smaller
    fine-tuned LLMs for text classification tasks. To this end, we compare several
    fine-tuned LLMs such as RoBERTa with major generative AI models across text categories,
    tasks, languages, and imbalanced datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了像ChatGPT和Claude Opus这样的零-shot生成AI模型是否已经在文本分类任务中超越了诸如小型微调LLM这样的拟合模型。为此，我们比较了几个微调的LLM，如RoBERTa，和主要的生成AI模型在文本类别、任务、语言和不平衡数据集上的表现。
- en: Our results show that fine-tuning smaller LLMs still significantly outperforms
    zero-shot prompted generative AI models on all our case studies. In addition,
    we provide an ablation study to explore the relationship between dataset size
    (i.e., number of training examples) and model performance, showing that performance
    already begins to saturate after around 200 labels.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果显示，微调较小的LLM仍然在所有案例研究中显著优于零样本提示生成AI模型。此外，我们提供了一项消融研究，探索数据集规模（即训练样本数量）与模型性能之间的关系，表明性能在大约200个标签后就开始饱和。
- en: We further provide an easy-to-use text classification toolkit that requires
    no domain knowledge in NLP or deep learning. Our toolkit is modular and allows
    users to plug in additional models as desired or needed. This allows users to
    load smaller LLMs for other languages or upgrade to newly developed or improved
    language models and as they are published via Hugging Face. We hope that our tool
    will enable users to realize the potential of fine-tuned LLMs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个易于使用的文本分类工具包，不需要NLP或深度学习领域的知识。我们的工具包是模块化的，允许用户根据需要插入额外的模型。这使得用户能够加载其他语言的小型LLM，或升级到通过Hugging
    Face发布的新开发或改进的语言模型。我们希望我们的工具能够帮助用户实现微调LLM的潜力。
- en: Acknowledgments
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Daniel Grosshans, Sascha Langenbach, Janina
    Steinmetz, and Stefanie Walter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢 Daniel Grosshans, Sascha Langenbach, Janina Steinmetz 和 Stefanie Walter。
- en: Funding Statement
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资金声明
- en: This research was supported by the European Research Council under the Horizon
    2020 research and innovation programme grant agreement No 817582.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了欧洲研究委员会在“地平线2020”研究与创新计划资助协议编号817582的支持。
- en: Replication materials
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复现材料
- en: All code and replication materials are available [here](https://github.com/mnbucher/text-cls-llms).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码和复现材料可在 [这里](https://github.com/mnbucher/text-cls-llms) 获取。
- en: References
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Anthropic “The Claude 3 Model Family: Opus, Sonnet, Haiku”, 2024'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Anthropic “Claude 3 模型系列：Opus, Sonnet, Haiku”，2024年'
- en: '[2] Pablo Barberá et al. “Automated text classification of news articles: A
    practical guide” In *Political Analysis* 29.1 Cambridge University Press, 2021,
    pp. 19–42'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Pablo Barberá 等 “新闻文章的自动文本分类：实用指南” 发表于 *政治分析* 29.1 剑桥大学出版社，2021年，第19–42页'
- en: '[3] Samuel E Bestvater and Burt L Monroe “Sentiment is not stance: target-aware
    opinion classification for political text analysis” In *Political Analysis* Cambridge
    University Press, 2022, pp. 1–22'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Samuel E Bestvater 和 Burt L Monroe “情感不是立场：针对政治文本分析的目标感知意见分类” 发表于 *政治分析*
    剑桥大学出版社，2022年，第1–22页'
- en: '[4] Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov “Enriching
    word vectors with subword information” In *Transactions of the association for
    computational linguistics* 5 MIT Press, 2017, pp. 135–146'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Piotr Bojanowski, Edouard Grave, Armand Joulin 和 Tomas Mikolov “通过子词信息丰富词向量”
    发表于 *计算语言学协会会刊* 5 MIT Press，2017年，第135–146页'
- en: '[5] Mitchell Bosley, Musashi Jacobs-Harukawa, Hauke Licht and Alexander Hoyle
    “Do we still need BERT in the age of GPT? Comparing the benefits of domain-adaptation
    and in-context-learning approaches to using LLMs for Political Science Research”,
    2023'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Mitchell Bosley, Musashi Jacobs-Harukawa, Hauke Licht 和 Alexander Hoyle
    “在GPT时代，我们是否仍然需要BERT？比较领域适应和上下文学习方法在政治科学研究中使用LLM的好处”，2023年'
- en: '[6] Tom Brown et al. “Language models are few-shot learners” In *Advances in
    neural information processing systems* 33, 2020, pp. 1877–1901'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tom Brown 等 “语言模型是少样本学习者” 发表于 *神经信息处理系统进展* 33，2020年，第1877–1901页'
- en: '[7] Ethan Caballero, Kshitij Gupta, Irina Rish and David Krueger “Broken neural
    scaling laws” In *arXiv preprint arXiv:2210.14891*, 2022'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ethan Caballero, Kshitij Gupta, Irina Rish 和 David Krueger “破碎的神经缩放规律”
    发表于 *arXiv 预印本 arXiv:2210.14891*，2022年'
- en: '[8] Youngjin Chae and Thomas Davidson “Large language models for text classification:
    From zero-shot learning to fine-tuning” In *Open Science Foundation*, 2023'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Youngjin Chae 和 Thomas Davidson “大型语言模型在文本分类中的应用：从零样本学习到微调” 发表于 *开放科学基金会*，2023年'
- en: '[9] Charles Chang and Michael Masterson “Using Word Order in Political Text
    Classification with Long Short-term Memory Models” In *Political Analysis, 28(3):
    395-411*, 2020'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Charles Chang 和 Michael Masterson “在政治文本分类中使用词序与长短期记忆模型” 发表于 *政治分析, 28(3):
    395-411*，2020年'
- en: '[10] Zheng Chen and Yunchen Zhang “Better few-shot text classification with
    pre-trained language model” In *Artificial Neural Networks and Machine Learning–ICANN
    2021: 30th International Conference on Artificial Neural Networks, Bratislava,
    Slovakia, September 14–17, 2021, Proceedings, Part II 30*, 2021, pp. 537–548 Springer'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zheng Chen 和 Yunchen Zhang “利用预训练语言模型进行更好的少样本文本分类” 见于 *人工神经网络与机器学习–ICANN
    2021: 第30届国际人工神经网络会议，斯洛伐克布拉迪斯拉发，2021年9月14–17日，论文集，第II部分 30*，2021年，第537–548页 Springer'
- en: '[11] Kevin Clark, Minh-Thang Luong, Quoc V Le and Christopher D Manning “Electra:
    Pre-training text encoders as discriminators rather than generators” In *arXiv
    preprint arXiv:2003.10555*, 2020'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Kevin Clark、Minh-Thang Luong、Quoc V Le 和 Christopher D Manning “Electra：将文本编码器预训练为鉴别器而非生成器”
    见于 *arXiv 预印本 arXiv:2003.10555*，2020年'
- en: '[12] Gary W. Cox and Eric Magar “How much is majority status in the US Congress
    worth?” In *American Political Science Review 93(2): 299-309*, 1999'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Gary W. Cox 和 Eric Magar “在美国国会中，多数地位的价值是多少？” 见于 *美国政治学评论 93(2): 299-309*，1999年'
- en: '[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova “Bert:
    Pre-training of deep bidirectional transformers for language understanding” In
    *arXiv preprint arXiv:1810.04805*, 2018'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova “Bert：深度双向变换器的语言理解预训练”
    见于 *arXiv 预印本 arXiv:1810.04805*，2018年'
- en: '[14] Danny Driess et al. “Palm-e: An embodied multimodal language model” In
    *arXiv preprint arXiv:2303.03378*, 2023'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Danny Driess 等 “Palm-e：一种具身的多模态语言模型” 见于 *arXiv 预印本 arXiv:2303.03378*，2023年'
- en: '[15] Aleksandra Edwards and Jose Camacho-Collados “Language Models for Text
    Classification: Is In-Context Learning Enough?” In *arXiv preprint arXiv:2403.17661*,
    2024'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Aleksandra Edwards 和 Jose Camacho-Collados “文本分类的语言模型：上下文学习是否足够？” 见于 *arXiv
    预印本 arXiv:2403.17661*，2024年'
- en: '[16] Aaron Erlich et al. “Multi-label prediction for political text-as-data”
    In *Political Analysis 30(4): 463-480*, 2022'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Aaron Erlich 等 “政治文本数据的多标签预测” 见于 *政治分析 30(4): 463-480*，2022年'
- en: '[17] Gloria Gennaro and Elliott Ash “Emotion and reason in political language”
    In *The Economic Journal 132(643): 1037-1059*, 2022'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Gloria Gennaro 和 Elliott Ash “政治语言中的情感与理性” 见于 *经济学杂志 132(643): 1037-1059*，2022年'
- en: '[18] Fabrizio Gilardi, Meysam Alizadeh and Maël Kubli “ChatGPT outperforms
    crowd workers for text-annotation tasks” In *Proceedings of the National Academy
    of Sciences* 120.30 National Acad Sciences, 2023, pp. e2305016120'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Fabrizio Gilardi、Meysam Alizadeh 和 Maël Kubli “ChatGPT 在文本标注任务中优于众包工作者”
    见于 *美国国家科学院院刊* 120.30 国家科学院，2023年，第 e2305016120 页'
- en: '[19] Suriya Gunasekar et al. “Textbooks are all you need” In *arXiv preprint
    arXiv:2306.11644*, 2023'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Suriya Gunasekar 等 “教科书是你所需的一切” 见于 *arXiv 预印本 arXiv:2306.11644*，2023年'
- en: '[20] Suchin Gururangan et al. “Don’t stop pretraining: Adapt language models
    to domains and tasks” In *arXiv preprint arXiv:2004.10964*, 2020'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Suchin Gururangan 等 “不要停止预训练：将语言模型适应领域和任务” 见于 *arXiv 预印本 arXiv:2004.10964*，2020年'
- en: '[21] Alan M. Jacobs, J. Matthews, Timothy Hicks and Eric Merkley “Whose news?
    Class-biased economic reporting in the United States” In *American Political Science
    Review 115(3): 1016-1033*, 2021'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Alan M. Jacobs、J. Matthews、Timothy Hicks 和 Eric Merkley “谁的新闻？美国的阶级偏见经济报道”
    见于 *美国政治学评论 115(3): 1016-1033*，2021年'
- en: '[22] Moritz Laurer, Wouter Atteveldt, Andreu Casas and Kasper Welbers “Less
    Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised
    Machine Learning with Deep Transfer Learning and BERT-NLI” In *Political Analysis*
    Cambridge University Press, 2023, pp. 1–33'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Moritz Laurer、Wouter Atteveldt、Andreu Casas 和 Kasper Welbers “减少注释，更多分类：通过深度迁移学习和
    BERT-NLI 解决监督机器学习的数据稀缺问题” 见于 *政治分析* 剑桥大学出版社，2023年，第1–33页'
- en: '[23] Mike Lewis et al. “Bart: Denoising sequence-to-sequence pre-training for
    natural language generation, translation, and comprehension” In *arXiv preprint
    arXiv:1910.13461*, 2019'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Mike Lewis 等 “Bart: 用于自然语言生成、翻译和理解的序列到序列去噪预训练” 见于 *arXiv 预印本 arXiv:1910.13461*，2019年'
- en: '[24] Yinhan Liu et al. “Roberta: A robustly optimized bert pretraining approach”
    In *arXiv preprint arXiv:1907.11692*, 2019'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Yinhan Liu 等 “Roberta：一种稳健优化的 bert 预训练方法” 见于 *arXiv 预印本 arXiv:1907.11692*，2019年'
- en: '[25] Marco Martini and Stefanie Walter “Learning from precedent: how the British
    Brexit experience shapes nationalist rhetoric outside the UK” In *Journal of European
    Public Policy*, 2023, pp. 1–28'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Marco Martini 和 Stefanie Walter “从先例中学习：英国脱欧经历如何塑造英国以外的民族主义修辞” 见于 *欧洲公共政策杂志*，2023年，第1–28页'
- en: '[26] Tomas Mikolov et al. “Distributed representations of words and phrases
    and their compositionality” In *Advances in neural information processing systems
    26*, 2013'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Tomas Mikolov 等 “词语和短语的分布式表示及其组合性” 见 *神经信息处理系统进展 26*, 2013'
- en: '[27] Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean “Efficient estimation
    of word representations in vector space” In *arXiv preprint arXiv:1301.3781*,
    2013'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean “高效估计词向量空间中的词表示”
    见 *arXiv 预印本 arXiv:1301.3781*, 2013'
- en: '[28] Blake Miller, Fridolin Linder and Walter R. Mebane “Active learning approaches
    for labeling text: review and assessment of the performance of active learning
    approaches” In *Political Analysis 28(4): 532-551*, 2020'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Blake Miller, Fridolin Linder 和 Walter R. Mebane “文本标注的主动学习方法: 主动学习方法的回顾和评估”
    见 *政治分析 28(4): 532-551*, 2020'
- en: '[29] Moritz Osnabrügge, Sara B. Hobolt and Toni Rodon “Playing to the gallery:
    Emotive rhetoric in parliaments” In *American Political Science Review 115(3):
    885-899*, 2021'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Moritz Osnabrügge, Sara B. Hobolt 和 Toni Rodon “迎合观众: 议会中的情感修辞” 见 *美国政治学评论
    115(3): 885-899*, 2021'
- en: '[30] Long Ouyang et al. “Training language models to follow instructions with
    human feedback” In *Advances in Neural Information Processing Systems* 35, 2022,
    pp. 27730–27744'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Long Ouyang 等 “训练语言模型以遵循人类反馈的指示” 见 *神经信息处理系统进展* 35, 2022, 第27730–27744页'
- en: '[31] Jeffrey Pennington, Richard Socher and Christopher D Manning “Glove: Global
    vectors for word representation” In *Proceedings of the 2014 conference on empirical
    methods in natural language processing (EMNLP)*, 2014, pp. 1532–1543'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Jeffrey Pennington, Richard Socher 和 Christopher D Manning “Glove: 词向量的全局表示”
    见 *2014年自然语言处理经验方法会议论文集 (EMNLP)*, 2014, 第1532–1543页'
- en: '[32] Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever “Improving
    language understanding by generative pre-training” OpenAI, 2018'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Alec Radford, Karthik Narasimhan, Tim Salimans 和 Ilya Sutskever “通过生成预训练提高语言理解”
    OpenAI, 2018'
- en: '[33] Mark J. Richards and Herbert M. Kritzer “Jurisprudential regimes in Supreme
    Court decision making” In *American Political Science Review 96(2): 305-320*,
    2002'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Mark J. Richards 和 Herbert M. Kritzer “最高法院决策中的法理制度” 见 *美国政治学评论 96(2):
    305-320*, 2002'
- en: '[34] Rylan Schaeffer, Brando Miranda and Sanmi Koyejo “Are emergent abilities
    of Large Language Models a mirage?” In *arXiv preprint arXiv:2304.15004*, 2023'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Rylan Schaeffer, Brando Miranda 和 Sanmi Koyejo “大型语言模型的涌现能力是否是一种错觉？” 见
    *arXiv 预印本 arXiv:2304.15004*, 2023'
- en: '[35] Rico Sennrich, Barry Haddow and Alexandra Birch “Improving neural machine
    translation models with monolingual data” In *arXiv preprint arXiv:1511.06709*,
    2015'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Rico Sennrich, Barry Haddow 和 Alexandra Birch “通过单语数据改进神经机器翻译模型” 见 *arXiv
    预印本 arXiv:1511.06709*, 2015'
- en: '[36] Noam Shazeer et al. “Outrageously large neural networks: The sparsely-gated
    mixture-of-experts layer” In *arXiv preprint arXiv:1701.06538*, 2017'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Noam Shazeer 等 “极大的神经网络: 稀疏门控专家混合层” 见 *arXiv 预印本 arXiv:1701.06538*, 2017'
- en: '[37] Bruno Castanho Silva and Sven-Oliver Proksch “Fake it ‘til you make it:
    A natural experiment to identify European politicians’ benefit from Twitter bots”
    In *American Political Science Review 115(1): 316-322*, 2021'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Bruno Castanho Silva 和 Sven-Oliver Proksch “假装做到最好: 识别欧洲政治家从Twitter机器人中获益的自然实验”
    见 *美国政治学评论 115(1): 316-322*, 2021'
- en: '[38] Ashish Vaswani et al. “Attention is all you need” In *Advances in neural
    information processing systems* 30, 2017'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Ashish Vaswani 等 “注意力机制是你所需要的一切” 见 *神经信息处理系统进展* 30, 2017'
- en: '[39] Krishnapriya Vishnubhotla and Saif M. Mohammad “Tweet emotion dynamics:
    Emotion word usage in tweets from US and Canada” In *arXiv:2204.04862*, 2022'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Krishnapriya Vishnubhotla 和 Saif M. Mohammad “推文情感动态: 来自美国和加拿大的情感词使用”
    见 *arXiv:2204.04862*, 2022'
- en: '[40] Jason Wei and Kai Zou “Eda: Easy data augmentation techniques for boosting
    performance on text classification tasks” In *arXiv preprint arXiv:1901.11196*,
    2019'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Jason Wei 和 Kai Zou “Eda: 提升文本分类任务性能的简单数据增强技术” 见 *arXiv 预印本 arXiv:1901.11196*,
    2019'
- en: '[41] Jason Wei et al. “Finetuned language models are zero-shot learners” In
    *arXiv preprint arXiv:2109.01652*, 2021'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jason Wei 等 “微调语言模型是零样本学习者” 见 *arXiv 预印本 arXiv:2109.01652*, 2021'
- en: '[42] Kilian Weinberger et al. “Feature hashing for large scale multitask learning”
    In *Proceedings of the 26th annual international conference on machine learning:
    pp. 1113-1120*, 2009'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Kilian Weinberger 等 “大规模多任务学习的特征哈希” 见 *第26届国际机器学习年会论文集: 第1113-1120页*,
    2009'
- en: '[43] Tobias Widmann and Maximilian Wich “Creating and comparing dictionary,
    word embedding, and transformer-based models to measure discrete emotions in german
    political text” In *Political Analysis* Cambridge University Press, 2022, pp.
    1–16'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Tobias Widmann 和 Maximilian Wich “创建和比较字典、词嵌入和基于变换器的模型，以衡量德语政治文本中的离散情感”
    发表在 *Political Analysis* 剑桥大学出版社，2022年，第1–16页'
- en: '[44] Thomas Wolf et al. “Transformers: State-of-the-art natural language processing”
    In *Proceedings of the 2020 conference on empirical methods in natural language
    processing: system demonstrations*, 2020, pp. 38–45'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Thomas Wolf 等 “变换器：最先进的自然语言处理” 发表在 *2020年自然语言处理经验方法会议：系统演示*，2020年，第38–45页'
- en: '[45] Zhilin Yang et al. “Xlnet: Generalized autoregressive pretraining for
    language understanding” In *Advances in neural information processing systems*
    32, 2019'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Zhilin Yang 等 “Xlnet：用于语言理解的广义自回归预训练” 发表在 *Advances in neural information
    processing systems* 32，2019年'
- en: '[46] Qihuang Zhong et al. “Can chatgpt understand too? a comparative study
    on chatgpt and fine-tuned bert” In *arXiv preprint arXiv:2302.10198*, 2023'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Qihuang Zhong 等 “ChatGPT 也能理解吗？ChatGPT 和微调 BERT 的比较研究” 发表在 *arXiv preprint
    arXiv:2302.10198*，2023年'
- en: '[47] Ruiqi Zhong, Kristy Lee, Zheng Zhang and Dan Klein “Adapting language
    models for zero-shot learning by meta-tuning on dataset and prompt collections”
    In *arXiv preprint arXiv:2104.04670*, 2021'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Ruiqi Zhong, Kristy Lee, Zheng Zhang 和 Dan Klein “通过在数据集和提示集合上进行元调优来调整语言模型以实现零样本学习”
    发表在 *arXiv preprint arXiv:2104.04670*，2021年'
- en: Appendix
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'A: Zero-Shot Prompts for GPT-3.5, GPT-4, and Claude Opus'
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'A: GPT-3.5、GPT-4 和 Claude Opus 的零样本提示'
- en: After experimenting through the API with a few prompts, we fine-tuned our final
    prompt to maximize the likelihood that the model only returns one label from the
    given list of available class labels. As LLMs are discrete probabilistic models,
    it’s stochasticity can lead to a variance in its generated output. By controlling
    the temperature parameter and decreasing its value, we can decrease the variance
    of the generated output. However, the model still generates other content than
    just one of the given output labels. In the following, we present the prompts
    we provided to the generative AI APIs, where <Text> marks the text placeholder
    for the current text sample. We set temperature=0.1 for all runs to reduce variance
    and force the model to produce a single label output from the provided list of
    labels. We repeat the prompting if the output does not match the labels provided
    for the classification task until the model produces a correct label as output.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 API 尝试了几个提示后，我们对最终提示进行了微调，以最大限度地提高模型只返回给定类标签列表中的一个标签的可能性。由于大语言模型（LLMs）是离散的概率模型，其随机性可能导致生成的输出存在差异。通过控制温度参数并降低其值，我们可以减少生成输出的差异。然而，模型仍然会生成除了给定的输出标签之外的其他内容。在下文中，我们展示了我们提供给生成
    AI API 的提示，其中 <Text> 表示当前文本样本的文本占位符。我们将温度设置为0.1以减少差异，并迫使模型从提供的标签列表中生成单一标签输出。如果输出与分类任务中提供的标签不匹配，我们会重复提示，直到模型生成正确的标签作为输出。
- en: Sentiment Analysis on The New York Times Coverage of the US Economy
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于《纽约时报》对美国经济报道的情感分析
- en: 'Prompt:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for sentiment
    analysis. Your objective is to classify a given text snippet into one of several
    possible class labels, based on the sentiment expressed in the text. Your output
    should consist of a single class label that best matches the sentiment expressed
    in the text. Your output should consist of a single class label that best matches
    the given text. Choose ONLY from the given class labels below and ONLY output
    the label without any other characters. Text: <Text> Labels: ’Negative Sentiment’,
    ’Positive Sentiment’ Answer:'
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了情感分析的零样本文本分类任务。你的目标是将给定的文本片段分类到几个可能的类标签中的一个，基于文本中表达的情感。你的输出应由一个最符合文本中表达的情感的单一类标签组成。你的输出应由一个最符合给定文本的单一类标签组成。仅从以下给定的类标签中选择，并且仅输出标签而不添加其他字符。文本：<Text>
    标签：’负面情感’，’正面情感’ 答案：
- en: Stance Classification on Tweets about Kavanaugh Nomination
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于卡瓦诺提名的推文的立场分类
- en: 'Prompt:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for stance
    classification. Your objective is to classify a given text snippet into one of
    several possible class labels, based on the attitudinal stance towards the given
    text. Your output should consist of a single class label that best matches the
    stance expressed in the text. Your output should consist of a single class label
    that best matches the given text. Choose ONLY from the given class labels below
    and ONLY output the label without any other characters. Text: <Text> Labels: ’negative
    attitudinal stance towards’, ’positive attitudinal stance towards’ Answer:'
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了一个零样本文本分类的任务，目的是进行立场分类。你的目标是将给定的文本片段分类到几个可能的类别标签之一，基于对给定文本的态度立场。你的输出应包含一个最能匹配文本中表达的立场的单一类别标签。仅从以下给定的类别标签中选择，并且仅输出标签，不添加其他字符。文本：<Text>
    标签：’对离开要求的负面态度’，’对离开要求的积极态度’ 答案：
- en: Emotion Detection on Political Texts in German
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对德语政治文本中的情感检测
- en: 'Prompt:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for emotion
    classification. Your objective is to classify a given text snippet into one of
    several possible class labels, based on the anger level in the given text. Your
    output should consist of a single class label that best matches the anger expressed
    in the text. Choose ONLY from the given class labels below and ONLY output the
    label without any other characters. Text: <Text> Labels: ’Angry’, ’Non-Angry’
    Answer:'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了一个零样本文本分类的任务，目的是对情感进行分类。你的目标是将给定的文本片段分类到几个可能的类别标签之一，基于文本中的愤怒程度。你的输出应包含一个最能匹配文本中表达的愤怒情感的单一类别标签。仅从以下给定的类别标签中选择，并且仅输出标签，不添加其他字符。文本：<Text>
    标签：’愤怒’，’非愤怒’ 答案：
- en: Multi-Class Stance Classification on Parties’ EU Positions
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对政党的欧盟立场的多类别立场分类
- en: 'Prompt:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for political
    texts on attitudinal stance towards Brexit and leave demands related to the European
    Union (EU). Your objective is to classify a given text snippet into one of several
    possible class labels, based on the stance towards Brexit and general leave demands
    in the given text. Your output should consist of a single class label that best
    matches the content expressed in the text. Choose ONLY from the given class labels
    below and ONLY output the label without any other characters. Text: <Text> Labels:
    ’Neutral towards Leave demands’, ’Pro-Leave demands’, ’Very Pro-Leave demands’
    Answer:'
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了一个零样本文本分类的任务，目的是对有关脱欧和与欧盟（EU）相关的离开要求的政治文本进行分类。你的目标是将给定的文本片段分类到几个可能的类别标签之一，基于对脱欧和一般离开要求的态度。你的输出应包含一个最能匹配文本中表达的内容的单一类别标签。仅从以下给定的类别标签中选择，并且仅输出标签，不添加其他字符。文本：<Text>
    标签：’对离开要求的中立态度’，’支持离开要求’，’非常支持离开要求’ 答案：
- en: 'B: BART'
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'B: BART'
- en: For BART, we use the BART Large model facebook/bart-large-mnli introduced by
    [[23](#bib.bibx23)] together with the Zero-Shot Classification pipeline from Huggingface.
    We use a technique called Natural Language Inference (NLI), which prompts a model
    using two sentences, a Premise (in our case the text to be classified) and a Hypothesis
    (a possible class label for that text). The model then predicts if the hypothesis
    is consistent with the premise. NLI evaluates a hypothesis for each label and
    then selects the label with the highest confidence as output.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BART，我们使用了 BART Large 模型 facebook/bart-large-mnli，该模型由 [[23](#bib.bibx23)]
    介绍，并结合了来自 Huggingface 的零样本分类管道。我们使用了一种称为自然语言推理（NLI）的技术，它通过两个句子来提示模型，一个前提（在我们这种情况下是待分类的文本）和一个假设（该文本的可能类别标签）。然后模型预测假设是否与前提一致。NLI
    对每个标签进行评估，然后选择置信度最高的标签作为输出。
