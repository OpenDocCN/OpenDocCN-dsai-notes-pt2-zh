- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02303](https://ar5iv.labs.arxiv.org/html/2311.02303)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming
    Liang,
  prefs: []
  type: TYPE_NORMAL
- en: 'Dajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu, Jianguo Li¹¹footnotemark:
    1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ant Group, China Corresponding Author: {hyu.hugo, lijg.zero}@antgroup.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Code LLMs have emerged as a specialized research field, with remarkable studies
    dedicated to enhancing model’s coding capabilities through fine-tuning on pre-trained
    models. Previous fine-tuning approaches were typically tailored to specific downstream
    tasks or scenarios, which meant separate fine-tuning for each task, requiring
    extensive training resources and posing challenges in terms of deployment and
    maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness
    among different code-related tasks. To overcome these limitations, we present
    a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel
    fine-tuning on multiple tasks. By incorporating various loss functions, we effectively
    address common challenges in multi-task learning, such as data imbalance, varying
    difficulty levels, and inconsistent convergence speeds. Extensive experiments
    have conclusively demonstrated that our multi-task fine-tuning approach outperforms
    both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble
    of tasks. Moreover, MFTCoder offers efficient training capabilities, including
    efficient data tokenization modes and PEFT fine-tuning, resulting in significantly
    improved speed compared to traditional fine-tuning methods. MFTCoder seamlessly
    integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen.
    Leveraging the CodeLLama foundation, our MFTCoder fine-tuned model, CodeFuse-CodeLLama-34B,
    achieves an impressive pass@1 score of 74.4% on the HumaneEval benchmark, surpassing
    GPT-4 performance (67%, zero-shot). MFTCoder is open-sourced at [https://github.com/codefuse-ai/MFTCOder](https://github.com/codefuse-ai/MFTCOder)
  prefs: []
  type: TYPE_NORMAL
- en: '*Keywords* Large Language Model  $\cdot$ Multi-task Learning'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paradigm-shifting emergence of ChatGPT¹¹1https://openai.com/blog/chatgpt,
    powered by both GPT-3.5 and GPT-4 OpenAI ([2023](#bib.bib43)), has set ablaze
    the landscape of research and development in the realm of large language models
    (LLMs). This breakthrough has further sparked the interest in leveraging LLMs
    for code understanding and generation, commonly referred to as Code LLMs. By pretraining
    on extensive code data sources such as the Github public data, these Code LLMs
    can acquire comprehensive contextual representations that can be applied to various
    code-related tasks .
  prefs: []
  type: TYPE_NORMAL
- en: While the pretraining stage of (Code) LLMs seek to ensure their generalizability
    to different downstream tasks, the subsequent finetuning stage typically only
    adapt the (Code) LLMs to a specific task or a scenario. However, this approach
    overlooks two critical challenges. Firstly, it involves resource-intensive individual
    finetuning of large LLMs for each task, which hinders efficient deployment in
    production. Secondly, the interrelated nature of code domain tasks suggests that
    joint finetuning can enhance performance compared to separate finetuning. It is
    therefore imperative to conduct multitask finetuning, enabling simultaneous handling
    of all tasks while leveraging the strengths of related tasks to enhance performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an illuminating example, suppose we have two related tasks: code completion
    and code summarization. Code completion involves predicting the next line of code
    based on a partial code snippet, while code summarization aims to generate a concise
    human-readable summary of a given code snippet. Traditionally, separate models
    would be fine-tuned for each task, resulting in resource-intensive duplication.
    However, code completion and code summarization have inherent connections. Completion
    of a code snippet relies on understanding the overall functionality and purpose,
    while generating an accurate summary requires comprehending the structure, dependencies,
    and intended functionality. By employing multitask learning, a single model can
    be trained to jointly learn both tasks, leveraging shared knowledge and patterns,
    leading to improved performance on both tasks. The model understands the contextual
    dependencies between code elements, aiding in predicting the next snippet and
    generating informative summaries. Furthermore, multitask learning offers additional
    benefits beyond individual task performance: the shared representation between
    tasks helps mitigate overfitting, promote better generalization, and enhance the
    model’s ability to handle data scarcity for specific tasks. If code completion
    has a larger training dataset than code summarization, the model can leverage
    the abundance of completion data to enhance performance in summarization, effectively
    addressing data scarcity challenges. Multitask learning even enables the model
    to handle unseen but related tasks without specific training data. Overall, multitask
    learning allows models to jointly learn multiple related tasks, benefiting from
    shared knowledge, improving performance, enhancing generalization, and handling
    data scarcity.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the importance of multitask learning for finetuning, only a handful
    of existing studies have explored this approach in the domain of NLP Raffel et al.
    ([2023](#bib.bib46)); Aghajanyan et al. ([2021](#bib.bib2)); Aribandi et al. ([2022](#bib.bib6)).
    These studies incorporate multi-task data and merge it for large-scale model learning,
    without explicitly separating the tasks. Unfortunately, these studies tend to
    prioritize tasks with larger sample sizes, disregarding tasks with smaller sample
    sizes. Furthermore, they fail to ensure equal convergence speed among tasks, leading
    to over-optimization of some tasks and under-optimization of others.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we focus on multitask fine-tuing (MFT) of (Code) LLMs, in order
    to guarantee equitable attention to tasks with varying sample sizes and approximately
    similar optimization progress. In particular, our attention is on Code LLMs, as
    code domain tasks often exhibit correlations, and so we name our approach MFTCoder.
    We emphasize that MFTcoder can be extended to an arbitrary set of related-NLP
    tasks in a straighforward manner. To enhance the efficiency of MFTCoder, we incorporate
    parameter-efficient fine-tuning techniques, including LoRA Hu et al. ([2021](#bib.bib24))
    and QLoRA Dettmers et al. ([2023](#bib.bib18)). Experimental results demonstrate
    that multi-task models trained using the MFT approach outperform those fine-tuned
    individually for each task or by merging data from multiple tasks. We further
    validate the effectiveness of MFTCoder on various baseline pretrained LLMs, such
    as Qwen Bai et al. ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)),
    Llama Touvron et al. ([2023a](#bib.bib51)), Llama 2 Touvron et al. ([2023b](#bib.bib52)),
    StarCoder Li et al. ([2023a](#bib.bib32)), CodeLLama Rozière et al. ([2023](#bib.bib47)),
    and CodeGeex2 Zheng et al. ([2023](#bib.bib59)). Remarkably, when applying MFTCoder
    to the CodeLlama-34B-Python Rozière et al. ([2023](#bib.bib47)) base model, it
    achieves a pass@1 score of 74.4% on the humanEval evaluation dataset, even surpassing
    the performance of GPT-4 (67%, zero-shot) OpenAI ([2023](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose MFTCoder, a novel multitask finetuning approach for concurrently
    adapting LLMs to multiple code-related tasks. Our focus is on addressing the issues
    of data balance and convergence speed that commonly arise in previous multitask
    finetuning methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We validate MFTCoder on various baseline pretrained models, including Qwen Bai
    et al. ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)), Llama Touvron
    et al. ([2023a](#bib.bib51)), Llama 2 Touvron et al. ([2023b](#bib.bib52)), StarCoder Li
    et al. ([2023a](#bib.bib32)), CodeLLama Rozière et al. ([2023](#bib.bib47)), CodeFuse Di
    et al. ([2023](#bib.bib19)), and CodeGeex2 Zheng et al. ([2023](#bib.bib59)),
    demonstrating its compatibility with different baseline models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive experiments show that the MFT approach outperforms individual fine-tuning
    for each task or data merging from multiple tasks. Notably, when implementing
    MFTCoder with the CodeLlama-34B-Python Rozière et al. ([2023](#bib.bib47)) base
    model, it achieves an impressive pass@1 score of 74.4% on the humanEval evaluation
    dataset, surpassing the performance of GPT-4 (67%, zero-shot) OpenAI ([2023](#bib.bib43)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Code LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coding capability serves as a critical criterion for evaluating general large
    language models (LLMs) in code-related tasks. Notable performance on the widely-used
    HumanEval dataset Chen et al. ([2021](#bib.bib12)), a benchmark for code generation,
    has been observed across various models, including LaMDA Thoppilan et al. ([2022](#bib.bib50)),
    PaLM Chowdhery et al. ([2022](#bib.bib15)), PaLM 2 Anil et al. ([2023](#bib.bib4)),
    ChatGPT, and GPT-4 OpenAI ([2023](#bib.bib43)). In particular, GPT-4 has set a
    remarkable record of 67.0% pass@1 score. However, their closed-source nature limits
    their availability and hinders further collaborative advancements. In contrast,
    recent open-source LLMs, including LLaMA Touvron et al. ([2023a](#bib.bib51)),
    LLaMA 2 Touvron et al. ([2023b](#bib.bib52)), Qwen Bai et al. ([2023](#bib.bib8)),
    and Phi-1.5 Li et al. ([2023b](#bib.bib33)), have demonstrated notable progress
    in code-related tasks, with commentable scores of 23.7%, 29.9%, 32.3%, and 41.4%
    respectively. Despite this progress, their performance still lags behind the state-of-the-art
    closed-source models.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, LLMs specifically designed for code-related tasks, often
    referred to as code LLMs, have also undergone significant developments. Alongside
    closed-source Code LLMs such as Codex Chen et al. ([2021](#bib.bib12)), Code-Davinci Chen
    et al. ([2021](#bib.bib12)), AlphaCode Li et al. ([2022](#bib.bib34)), PaLM-Coder Chowdhery
    et al. ([2022](#bib.bib15)), and PanGu-Coder Christopoulou et al. ([2022](#bib.bib16)),
    open-source alternatives like including SantaCoder Allal et al. ([2023](#bib.bib3)),
    Phi-1.0 Gunasekar et al. ([2023](#bib.bib22)), CodeGeeX-2 Zheng et al. ([2023](#bib.bib59)),
    StarCoder Li et al. ([2023a](#bib.bib32)), Code LLaMA Rozière et al. ([2023](#bib.bib47))
    have showcased competitive performance with their closed-source counterparts.
    Notably, CodeLLama-34B-Python Rozière et al. ([2023](#bib.bib47)) obtains a score
    of 53.7% on HumanEval. Apart from pretraining, another intriguing approach to
    further enhancing Code LLMs is instruction fine-tuning, as showcased by CodeT5+ Wang
    et al. ([2023](#bib.bib54)), Phi-1.0 Gunasekar et al. ([2023](#bib.bib22)), OctoPack Muennighoff
    et al. ([2023](#bib.bib42)), and WizardCoder Luo et al. ([2023](#bib.bib40)).
    By leveraging carefully curated high-quality instruction datasets, these methods
    exhibit the potential of fine-tuning to enhance code generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Multitask Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multitask learning (MTL) Caruana ([1997](#bib.bib11)); Crawshaw ([2020](#bib.bib17))
    is a potent approach in machine learning that holds significant promise for enhancing
    model performance and addressing diverse challenges Crawshaw ([2020](#bib.bib17)).
    By training a single model on multiple related tasks, MTL enables the model to
    leverage shared knowledge and patterns, leading to enhanced generalization and
    improved accuracy. MTL methods can be categorized into two groups: hard parameter
    sharing Zhao et al. ([2018](#bib.bib58)); Liu et al. ([2019b](#bib.bib36), [a](#bib.bib38));
    Kendall et al. ([2018](#bib.bib26)); Liu et al. ([2019c](#bib.bib37)); Chen et al.
    ([2018](#bib.bib13)); Jean et al. ([2019](#bib.bib25)) and soft parameter sharing Duong
    et al. ([2015](#bib.bib21)); Yang and Hospedales ([2017](#bib.bib55)); Long et al.
    ([2017](#bib.bib39)); Lee et al. ([2018](#bib.bib29)); Sun et al. ([2020](#bib.bib49));
    Pascal et al. ([2021](#bib.bib44)). Hard parameter sharing involves sharing model
    weights between tasks, while soft parameter sharing incorporates task-specific
    models with separate weights. In the context of large language models (LLMs),
    hard parameter sharing is particularly relevant, since the large number of parameters
    in LLMs facilitates their ability to handle multiple related tasks with a common
    set of parameters. As a result, optimizing an LLM to effectively tackle multiple
    tasks lies at the heart of MTL for LLMs. In recent years, notable advancements
    have been made in MTL techniques. Google introduced T5 Raffel et al. ([2023](#bib.bib46))
    in 2020, where they explored the application of MTL techniques. Meta further introduced
    Mupper Aghajanyan et al. ([2021](#bib.bib2)) in 2021, which applies multi-task
    learning between pretraining and fine-tuning, termed as pre-fine-tuning (PFT).
    They discovered that incorporating this step enhances the performance of the pretrained
    model across various downstream tasks and significantly improves the speed of
    fine-tuning. However, if the number of tasks in PFT is too small, it can have
    a negative impact on the model’s performance. Therefore, it is recommended to
    have a minimum of 15 tasks for optimal results. Building upon T5, Google introduced
    ExT5 Aribandi et al. ([2022](#bib.bib6)), which increased the number of tasks
    to 107\. They found that as long as the number of tasks in pretraining is sufficiently
    large, even if there may be mutual interference among tasks, the ultimate results
    are still remarkably good. Ultimately, ExT5 outperformed T5 across multiple metrics.
    It is worth noting that these studies mainly focused on incorporating multi-task
    data and merging it for the large model to learn, without explicitly segregating
    the tasks. While these approaches have shown promising results, they tend to overlook
    data imbalance and convergence speed issues that often arise in MTL. In this paper,
    we address these challenges and propose MFTCoder, a multitask finetuning approach
    for LLMs that tackles these problems effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/637e7d5d79940ee0d8493042d8901bdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of MFTCoder framework.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will introduce our multi-task fine-tuning framework, MFTCoder ²²2https://github.com/codefuse-ai/MFTCoder,
    along with the design of its key components.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 MFT Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MFTCoder aims to seamlessly adapt LLMs to diverse new scenarios while maximizing
    their performance within a specific context. When applying MFTCoder to a new scenario,
    the initial step involves decomposing the scenario into smaller tasks that correspond
    to targeted abilities. For instance, in the domain of code LLMs, the overarching
    objective of enhancing models’ code capabilities can be further divided into specific
    tasks like code completion, text-to-code generation, unit test case generation,
    code repair, code debugging, and even cross-language translation. Our extensive
    practical experience has demonstrated that MFTCoder effectively handles multi-task
    scales ranging from single to dozens or even hundreds of tasks. Each task necessitates
    the collection and organization of fine-tuning datasets. However, data collection
    for certain tasks can pose challenges. To overcome this, MFTCoder leverages Self-Instruct Wang
    et al. ([2022](#bib.bib53)) techniques and Agents to generate instruction datasets.
    With the capability to concurrently fine-tune multiple downstream tasks, MFTCoder
    effectively handles substantial volumes of fine-tuning data, ensuring efficient
    training. It incorporates two efficient data tokenization modes and implements
    PEFT (Parameter-Efficient Fine-Tuning) techniques to enhance training efficiency.
    In the realm of multi-task learning, MFTCoder confronts the issue of task imbalances,
    encompassing imbalanced data distribution, varying task difficulties, and divergent
    convergence rates. To mitigate these challenges, MFTCoder introduces or adapts
    different loss functions to achieve task balance. Recognizing that different large-scale
    models possess distinct strengths and capabilities, MFTCoder facilitates the selection
    of suitable model architectures based on specific scenarios to achieve optimal
    performance. It has been adapted to popular LLMs, including LLama Touvron et al.
    ([2023a](#bib.bib51)), LLama 2 Touvron et al. ([2023b](#bib.bib52)), CodeLLama Rozière
    et al. ([2023](#bib.bib47)), Qwen Bai et al. ([2023](#bib.bib8)), Baichuan 1/2 Baichuan
    ([2023](#bib.bib9)), ChatGLM 2 Du et al. ([2022](#bib.bib20)), CodeGeeX 2 Zheng
    et al. ([2023](#bib.bib59)), GPT-NEOX Black et al. ([2022](#bib.bib10)), CodeFuse Di
    et al. ([2023](#bib.bib19)), StarCoder Li et al. ([2023a](#bib.bib32)), AntLLM,
    and more. We continuously update and expand the compatibility with additional
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall framework of MFTCoder is illustrated in Figure [1](#S3.F1 "Figure
    1 ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"). In
    the subsequent sections, we will provide a more detailed exploration of these
    components, including instruction datasets construction, efficient tokenization
    modes, PEFT fine-tuning and balanced loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Instruction Dataset Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a073458f1289833d4b15ce4da3eaa46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Data Generation Approach for Code Exercises Datasets using Single-turn
    Conversation Scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: For tasks with challenging data collection, We employ the Self-Instruct Wang
    et al. ([2022](#bib.bib53)) technique to generate fine-tuning data for downstream
    code-related tasks in MFTCoder. This involves providing customized prompts to
    GPT-3.5 or GPT-4 that clearly describe our instruction generation requirements,
    thereby generating instructional data. Furthermore, we drew inspiration from the
    Textbook approach employed in the PHI work Gunasekar et al. ([2023](#bib.bib22)),
    incorporating the self-instruct technique to generate Code Exercises datasets
    for downstream code-related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of specific implementation, we have two options. One is the Agents
    multi-turn conversation approach achieved through Camel Li et al. ([2023c](#bib.bib30)),
    and the other is the single-turn conversation method by directly invoking the
    ChatGPT API. In our multi-turn approach, we employ Camel to launch two agents,
    each assigned specific roles and task themes, facilitating a dialogue between
    them to generate instructional data aligned with the given theme. For instance,
    when generating Python exercise data, we designate the roles of ’teacher’ (simulating
    the user role of ChatGPT) and ’student’ (simulating the assistant role of ChatGPT)
    for the agents. The teacher’s responsibility is to provide exercise instructions
    to the student, while the student’s task is to offer corresponding solutions to
    those instructions. This iterative process continues, generating multiple exercise
    questions, until the task requirements are met or the maximum input length of
    ChatGPT is reached. To accommodate ChatGPT’s input length limitation, we cannot
    directly utilize a large question as the task theme. For instance, when creating
    Python exercise questions to assess students’ proficiency, we break down the main
    theme into smaller Python knowledge points (e.g. binary search tree) and initiate
    separate Camel sessions for each knowledge point. For a concrete example, please
    refer to Appendix [A](#A1 "Appendix A Code Exercises Generation with Camel ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The multi-turn approach provides high automation but can be costly due to the
    need for maintaining two agents, each making multi-turn calls to the ChatGPT API.
    To mitigate this, we propose a more cost-effective single-turn conversation generation
    approach, and the overall process is illustrated in Figure [2](#S3.F2 "Figure
    2 ‣ 3.2 Instruction Dataset Construction ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning"). We begin by creating an initial set of seeds,
    such as hundreds of Python knowledge points. These seeds are then combined with
    prepared fixed prompt templates to generate a set of patterned task prompts. To
    address the issue of reduced diversity caused by fixed templates and to ensure
    accurate prompt descriptions, we utilize Camel’s task prompt refinement feature
    to obtain precise and diverse task prompts. Each task prompt is used to generate
    a set of instructions related to the corresponding seed (e.g. exercise problems
    related to binary search trees). Using ChatGPT, we generate the corresponding
    solutions for the generated instructions. Finally, we assemble and deduplicate
    the instructions with their respective solutions to obtain an exercise dataset.
    We have open-sourced a Python Code Exercises dataset ³³3https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k
    constructed using this approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Efficient Tokenization Modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tokenization is an essential step in the pre-training and fine-tuning of LLM
    models, where input and output texts are split into smaller units to be processed.
    It, along with the loss function, effectively defines how the data is utilized
    during the training process, thus playing a crucial role in both the model’s effectiveness
    and training efficiency. In the typical SFT tokenization scheme, samples within
    the same batch are aligned to the maximum input length (seq-length) of the model
    with extra padding tokens, shown as Figure [3(a)](#S3.F3.sf1 "In Figure 3 ‣ 3.3
    Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning"). However, in practice, we have found that this approach
    results in a high proportion of padding tokens. For example, when using the CodeFuse-13B Di
    et al. ([2023](#bib.bib19)) tokenizer to process 35 downstream tasks, the average
    proportion of padding tokens is 92.22% (with seq-length set to 4096). This means
    a significant number of tokens are used solely for alignment purposes, providing
    no value to the training process. This results in lower training efficiency and
    wastage of offline tokenization storage space. To address this issue, we have
    adopted and optimized two tokenization modes, namely dynamic padding and pack
    modes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In dynamic padding mode, the micro batch window size of each GPU is determined
    by the maximum sample length in the micro batch. Shorter samples are padded with
    additional padding tokens to match this size, as shown in Figure [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). Although padding tokens do not affect
    the model’s training effectiveness, they add computational overhead during training,
    impacting the training speed. Dynamic padding mode effectively reduces the proportion
    of padding tokens used, leading to faster training. In our experience, this approach
    can yield approximately a twofold speed improvement compared to the traditional
    SFT tokenization mode (actual improvement depends on the dataset). It’s important
    to note that this mode is suitable for online tokenization scenarios only.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the dynamic padding mode reduces the micro batch window size, the pack
    mode, similar to Llama 2’s SFT tokenization mode Touvron et al. ([2023b](#bib.bib52)),
    maximizes the utilization of the model’s maximum input window length (seq-length).
    In the pack mode, multiple fine-tuning samples are sequentially packed into a
    window of seq-length, separated by eos tokens, as shown in Figure [3(c)](#S3.F3.sf3
    "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). In the figure, samples 1-4 of Figure [3(a)](#S3.F3.sf1
    "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning") are combined and placed in one window after
    one another. If a sample cannot fit in the current window, it is placed in the
    next window with padding tokens filling the remaining space. For instance, in
    Figure [3(c)](#S3.F3.sf3 "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"), sample 5 is placed
    in the second window with padding tokens, while sample 6 is accommodated in the
    third window. The pack mode, in comparison to the dynamic padding mode, offers
    even lower padding token ratio, resulting in improved training speed. Our practical
    experience demonstrates that this approach reduces the average proportion of padding
    tokens to less than 10% among the 35 tasks mentioned earlier, leading to a substantial
    boost in training speed while maintaining training effectiveness. It is important
    to highlight that MFTCoder supports both online and offline pack tokenization
    scenarios, serving not only the SFT phase but also the pre-training stages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19af03e73b12240f5aa403a2282710f2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Normal SFT Mode
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ef6612e6088bc2850692a6a4559a89e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Dynamic Padding Mode
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39ade741508de9f2aadeb8b1aad92369.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Pack SFT Mode
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Illustration of the differences in sample organization within a batch
    between normal SFT, dynmaic padding and Pack SFT tokenization modes. The light-colored
    squares in the figure represent the Prompt section of the samples, while the dark-colored
    squares represent the Label section (participating in loss calculation). The blank
    squares represent padding section.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 PEFT Efficient Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The prevalent large-scale models typically contain billions of parameters,
    while multi-task learning scenarios often involve numerous tasks, resulting in
    a substantial total number of fine-tuning samples. If we were to opt for full-fledged
    fine-tuning of these large models using a vast amount of data, two challenges
    would arise: firstly, the need for extensive storage and computational resources;
    secondly, the potential risk of catastrophic forgetting during training. To address
    these issues, MFTCoder incorporates the PEFT (Parameter-efficient fine-tuning)
    technique Houlsby et al. ([2019](#bib.bib23)), enabling efficient fine-tuning
    to be accomplished within a short timeframe and with minimal resource requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, MFTCoder supports two PEFT methods: Lora (Large-scale Language
    Model Low-Rank Adaptation) Hu et al. ([2021](#bib.bib24)) and QLora (Quantized
    Large-scale Language Model Low-Rank Adaptation) Dettmers et al. ([2023](#bib.bib18)).
    The fundamental concept of Lora is quite simple, as depicted in Figure [4](#S3.F4
    "Figure 4 ‣ 3.4 PEFT Efficient Fine-tuning ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning"). It involves adding an auxiliary branch to the
    original pretrained language model. During training, the parameters $W\in\mathbb{R}^{d\times
    d}$ is then added to the original model $W$, resulting in the newly trained model.
    Due to the significantly smaller magnitude of r compared to d, the number of trainable
    parameters can be dramatically reduced. Building upon LoRA, QLoRA incorporates
    a novel high-precision quantization technique called NF4 and dual quantization
    to quantize the pretrained model to 4 bits. It also introduces a small set of
    learnable low-rank adapter weights. These weights are fine-tuned by optimizing
    the gradients through back-propagation of the quantized weights. As a result,
    QLoRA enables the fine-tuning of larger models using fewer GPU resources. As an
    example, MFTCoder can fine-tune a 70B model on a single A100 with 80GB of VRAM.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e985fabe14ab6a8435c82d63d1f7984.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Visualizing the Essence of Lora’s Basic Idea.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Multitask Fine-Tuning with Balanced Losses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a multi-task learning framework, MFTCoder, as described in Section [2](#S2
    "2 Related Works ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"),
    faces a significant challenge of data imbalance, task heterogeneity, and varying
    convergence speeds. To address these challenges, MFTCoder incorporates a set of
    loss functions specifically designed to alleviate these imbalances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the issue of data imbalance, we first ensure that all samples from
    all tasks are utilized exactly once within a single epoch. To avoid the model
    favoring tasks with larger amounts of data, we introduce a weight assignment strategy
    during loss computation. Specifically, we support two weight calculation schemes:
    one based on the number of task samples and the other based on the number of valid
    tokens involved in the loss calculation. The former is more straightforward, but
    it may perform poorly when dealing with tasks that have extreme differences in
    the number of valid tokens, such as binary classification tasks like "yes" or
    "no" answering or single-choice exam tasks. On the other hand, the latter weight
    assignment scheme based on the actual number of valid tokens involved in the loss
    calculation can mitigate these issues. The specific formulation for weighted loss
    calculation is shown in Equation [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with
    Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning").
    In Equation [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3
    Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"), $N$ refers
    to the k-th valid token of the j-th sample for the i-th task.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'To address the issue of task heterogeneity, we drew inspiration from the focal
    loss approach and incorporated it into MFTCoder. We implemented two different
    levels of focal loss functions to cater to different granularities. One operates
    at the sample level, as shown in Equation [2](#S3.E2 "In 3.5 Multitask Fine-Tuning
    with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask
    Fine-Tuning"), while the other operates at the task level, as shown in Equation [3](#S3.E3
    "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathcal{L}_{2}(\theta)=\min\limits_{\theta}\frac{\sum^{N}_{i=1}\sum^{M_{i}}_{j=1}-\alpha_{i}*(1-P_{ij})^{\gamma}*Q_{ij}}{\sum^{N}_{i=1}M_{i}},\\
    P_{ij}=\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}P_{ijk},\\'
  prefs: []
  type: TYPE_NORMAL
- en: Q_{ij}=\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}\log(P_{ijk})$$ |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathcal{L}_{3}(\theta)=\min\limits_{\theta}\frac{1}{N}\sum^{N}_{i=1}-\alpha_{i}*(1-P_{i})^{\gamma}*Q_{i},\\
    P_{i}=\frac{1}{M_{i}}\sum^{M_{i}}_{j=1}\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}P_{ijk},\\'
  prefs: []
  type: TYPE_NORMAL
- en: Q_{i}=\frac{1}{M_{i}}\sum^{M_{i}}_{j=1}\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}\log(P_{ijk})$$
    |  | (3) |
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the issue of inconsistent convergence speeds, we drew inspiration
    from the FAMO Liu et al. ([2023](#bib.bib35)) approach and innovatively applied
    it to calculate the validation loss. Firstly, we assumed that each task, indexed
    by $i$ for the task with the slowest convergence speed, shown as Equation [4](#S3.E4
    "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). Here, $g_{t}$ denotes the parameters of
    the network in the $t$ is a small constant to prevent division by zero. Furthermore,
    we would like to provide further explanation on how we achieve balanced convergence.
    To ensure that tasks converge at a similar pace, we introduce a dynamic balancing
    mechanism. At each iteration, we update the task-specific weights based on the
    gradients of their validation losses. This approach aims to give more importance
    to tasks with slower convergence speeds, allowing them to have a larger influence
    on the overall optimization process. By dynamically adjusting the task weights,
    we create a balanced convergence scenario, where all tasks progress towards their
    optimal solutions at a similar rate. This mechanism effectively addresses the
    issue of disparate convergence speeds and enhances the overall stability and performance
    of the MFTCoder framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathcal{L}_{4}(\theta)=\max_{g_{t}}\min_{i}\frac{1}{\alpha}c^{i}(\alpha,g_{t})-\frac{1}{2}\&#124;g_{t}\&#124;^{2},\\
    g_{t}=\sum_{i}w_{t}^{i}\nabla\mathcal{L}^{i}(\theta_{t}),\\'
  prefs: []
  type: TYPE_NORMAL
- en: c^{i}(\alpha,g_{t})=\frac{\mathcal{L}^{i}(\theta_{t})-\mathcal{L}^{i}(\theta_{t}-\alpha
    d_{t})}{\mathcal{L}^{i}(\theta_{t})+\epsilon}$$ |  | (4) |
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating these different loss functions, MFTCoder effectively addresses
    the diverse requirements of various multitask scenarios and alleviates the challenges
    of data imbalance, task heterogeneity, and inconsistent convergence speeds typically
    encountered in existing large-scale MTL research. MFTCoder’s flexible framework
    provides a robust solution to these issues, empowering the development of more
    efficient and accurate multitask models.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will conduct multiple sets of experiments using MFTCoder
    to validate the effectiveness and superiority of the MFT method. Specifically,
    we aim to address the following three research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the MFT model, obtained by fine-tuning multiple tasks using MFT methodology,
    outperform the SFT-S(ingle) models, where each task is individually fine-tuned?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RQ2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the MFT model outperform the SFT-Mixed model, where multiple tasks are
    combined and fine-tuned as one?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RQ3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In terms of generalization to unseen tasks, does the MFT model outperform the
    SFT-Mixed model?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will commence by presenting the experimental setup. Subsequently, we
    will showcase and delve into the experimental results. Finally, we will culminate
    by summarizing and addressing the research questions raised in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Evaluation Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To address these three research questions, we selected 5 code-related downstream
    tasks and prepared the corresponding fine-tuning data, as shown in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Evaluation Setup ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs
    with Multitask Fine-Tuning"). Table [1](#S4.T1 "Table 1 ‣ 4.1 Evaluation Setup
    ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") presents
    the desired enhancements (Column III) and the number of samples (Column IV) for
    each task. For instance, the codecompletion-task aims to improve the model’s code
    completion ability and includes 192,547 fine-tuning samples. The codetrans-task
    aims to enhance the model’s code translation capability and consists of 307,585
    fine-tuning samples. Thus, we trained 7 models (Column I), including individual
    SFT-S-* models trained for each downstream task, a combined SFT-Mixed model for
    the 5 task data, and an MFT-5Tasks model trained using the MFT method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Various experimental models and their corresponding training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experimental Model | Task | Desired Ability | #Samples |'
  prefs: []
  type: TYPE_TB
- en: '&#124; #Samples &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; after packing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeCompletion | code-completion | Code Completion | 192,547 | 18,811
    |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-Text2Code | text2code | Text-to-code Generation | 94,086 | 14,399 |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeComment | code-comment | Code Comments Generation | 645,711 | 134,775
    |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeTrans | code-trans | Code Translation | 307,585 | 71,573 |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-UnitTest | unit-test | Unit test-case generation | 390,393 | 77,681
    |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | Mix of the above 5 tasks | All of the above | 1,630,322 | 317,239
    |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | The above 5 tasks | All of the above | 1,630,322 | 317,239 |'
  prefs: []
  type: TYPE_TB
- en: 'In the experiment, all models were configured identically except for the training
    data. The base model for all models was CodeLlama-13B-Python Rozière et al. ([2023](#bib.bib47)).
    Each model was trained using 16 A100 GPUs (with 80GB VRAM), a micro batch size
    of 8, and a global batch size of 128\. The Adam optimizer Kingma and Ba ([2017](#bib.bib27))
    was used with an initial learning rate of 2e-4, and a minimum learning rate of
    1e-5\. We employed the QLora-INT4 mode of MFTCoder for fine-tuning, with a consistent
    fine-tuning parameter proportion of 2.52%. The positions and initial values of
    the trainable parameters were also the same. All models incorporate the Data-Balance
    Loss (i.e., Equation [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with Balanced Losses
    ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")) and
    employ pack tokenization mode. Notably, when there is only one task, this loss
    function aligns with the conventional loss employed in standard GPT model pre-training.
    To determine the convergence point for each model, we trained them until the validation
    loss surpassed the loss from the current epoch for the next two consecutive epochs.
    This mechanism, known as early-stopping strategy, was employed to ensure optimal
    convergence for each model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evaluation Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this paper, we utilized publicly available and representative code assessment
    benchmarks for comparative evaluation. These benchmarks include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HumanEval Chen et al. ([2021](#bib.bib12)) is a widely used Python code completion
    evaluation dataset, meticulously curated by researchers at OpenAI.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HumanEval-X Zheng et al. ([2023](#bib.bib59)) is an extension of HumanEval,
    translated into multiple programming languages, enabling multi-language code completion
    evaluation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DS-1000 Lai et al. ([2022](#bib.bib28)) focuses on assessing a model’s ability
    to perform data science analysis using Python code, covering essential libraries
    such as Numpy, Pandas, TensorFlow, Pytorch, Scipy, Sklearn, and Matplotlib.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MBPP Austin et al. ([2021](#bib.bib7)) comprises 1000 Python programming problems,
    constructed through crowdsourcing, primarily targeting a model’s proficiency in
    basic Python. In this study, we selected 500 problems with ID 11-510 from MBPP
    to evaluate the text-to-code generation capability, specifically generating code
    based on problem descriptions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeFuseEval Di et al. ([2023](#bib.bib19)), building upon HumanEval and HumanEval-X,
    further extends the evaluation to include Chinese code completion (with Chinese
    docstrings), code translation, and unit test case generation capabilities, referred
    to as CodeFuseEval-CN, CodeFuseEval-CodeTrans, and CodeFuseEval-UnitTest, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout these evaluation datasets, we employed "pass@1" as the evaluation
    metric in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will showcase the evaluation results of seven trained models.
    For the SFT-S-* models, which were trained individually for each task, we will
    focus on testing their specific target capabilities. For instance, we will exclusively
    evaluate the performance of the SFT-S-CodeCompletion model in the code completion
    task. On the other hand, for the SFT-Mixed and MFT-5Tasks models, we will assess
    their performance on each task and compare it with the corresponding SFT-S-* models.
    Specifically, we will conduct tests to evaluate the capabilities of code completion,
    text-to-code generation, code comment generation, code translation, and unit test
    case generation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Code Completion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For code completion, we employed the HumanEval Chen et al. ([2021](#bib.bib12))
    and HumanEval-X  Zheng et al. ([2023](#bib.bib59)) evaluation datasets to assess
    the model’s performance. HumanEval is a widely-used benchmark dataset released
    by OpenAI specifically designed to evaluate the Python code completion ability
    of large language models. HumanEval-X, on the other hand, is an expansion of HumanEval
    that enables the evaluation of large models’ code completion performance across
    various programming languages. Consistent with other studies, we employed the
    pass@1 metric as the evaluation measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluated three models: SFT-S-CodeCompletion, SFT-Mixed, and MFT-5Tasks.
    The performance of these models on the HumanEval dataset is summarized in Table [2](#S4.T2
    "Table 2 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning") (Column III). Results indicate
    that the MFT-5Tasks model, trained using the MFT approach, outperforms the other
    two models. It achieves a 2.44% higher performance compared to the SFT-Mixed model,
    which was fine-tuned with mixed task data. It is worth noting that the SFT-Mixed
    model does not perform as well as the SFT-S-CodeCompletion model, which was trained
    individually for the code-completion task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we conducted a multilingual evaluation on the HumanEval-X dataset
    for the three models, as presented in Table [3](#S4.T3 "Table 3 ‣ 4.3.1 Code Completion
    ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask
    Fine-Tuning"). The MFT-5Tasks model demonstrates superior performance in Java
    and Golang, while the SFT-Mixed model excels in C++ and JavaScript. Overall, the
    evaluation affirms that the MFT-5Tasks model outperforms the others, with an average
    improvement of 1.22% over the SFT-Mixed model.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, in terms of code completion tasks, models trained using the MFT method
    outperform both individually fine-tuned models and models fine-tuned after combining
    multiple tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Pass@1 performance on HumanEval (Code Completion) and MBPP (Text-to-Code
    Generation). We utilized the greedy decoding strategy with zero-shot. The values
    of CodeLlama-Python-base are taken from Rozière et al. ([2023](#bib.bib47)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Humaneval &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MBPP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 13B | 43.3% |
    49.0% | 46.15% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeCompletion | 13B | 59.76% | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-Text2Code | 13B | NA | 54.2% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 13B | 57.93% | 53.6% | 55.765% |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 13B | 60.37% | 56.0% | 58.185% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of pass@1 Metric Performance on the multilingual HumanEval-X
    (zero-shot, greedy-decoding)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trained Model | Java | C++ | JavaScript | Golang | Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-Py-base | 43.3% | 41.46% | 34.76% | 38.41% | 29.27% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeCompletion | 50.0% | 39.02% | 47.56% | 40.23% | 44.20% |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 56.1% | 48.17% | 56.10% | 37.80% | 49.54% |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 57.32% | 46.34% | 54.27% | 45.12% | 50.76% |'
  prefs: []
  type: TYPE_TB
- en: 4.3.2 Text-to-Code Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To evaluate the models’ ability to generate code based on descriptions, we selected
    the MBPP Austin et al. ([2021](#bib.bib7)) evaluation dataset and used the pass@1
    metric. MBPP is specifically designed to assess models’ capacity to synthesize
    concise Python programs from natural language descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We tested three models, namely SFT-S-Text2Code, SFT-Mixed, and MFT-5Tasks,
    on the MBPP dataset, measuring their pass@1 performance as shown in Table [2](#S4.T2
    "Table 2 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning") (Column IV). Among these models,
    MFT-5Tasks exhibited the highest performance, surpassing the SFT-Mixed model by
    2.4%. Similarly, in terms of the text-to-code generation task, models fine-tuned
    after combining multiple tasks showed inferior performance compared to models
    fine-tuned specifically for this individual task.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, in terms of text-to-code generation tasks, models trained using the
    MFT method outperform both individually fine-tuned models and models fine-tuned
    after combining multiple tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Code Comment Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The objective of the code comment generation task is to have models add necessary
    comments to the code without modifying the input code itself. This includes both
    line comments and interface comments, making the code more readable and user-friendly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To assess this capability, we constructed an evaluation set based on 500 MBPP
    test questions (id xx-xx). For each question in the evaluation set, we had the
    SFT-S-CodeComment, SFT-Mixed and MFT-5Tasks models generate comments for it. Subsequently,
    we employed GPT-4 as the referee, which has been instructed with criteria for
    good comments, to determine which model performed the best. If it was not possible
    to determine, the output was labeled as UNKNOWN. Finally, we counted the number
    of questions where each model was determined to perform the best and calculated
    the corresponding proportions, shown in Table [4](#S4.T4 "Table 4 ‣ 4.3.3 Code
    Comment Generation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: It can be observed that 38.8% of the questions were determined to be best performed
    by the MFT-5Tasks model, surpassing the second-ranked SFT-Mixed by 7.4% and the
    third-ranked SFT-S-CodeComment by 10.8%. Additionally, 1.8% of the questions were
    marked as indeterminable by GPT-4. In summary, for this task, the models trained
    using the MFT method exhibit the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Performance Comparison of Three Models on Code Commenting Task. GPT-4
    Determines the Best Performing Model for Each Question. This Table Presents the
    Proportion of Questions Where Each Model Performs the Best. In particular, 1.8%
    of the evaluation cases were indeterminate for GPT-4 to determine the best-performing
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Model | Best identified by GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeComment | 28% |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 31.4% |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 38.8% |'
  prefs: []
  type: TYPE_TB
- en: 4.3.4 Code Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The objective of the code translation task is to accurately and precisely translate
    a given code snippet implemented in the source language into an equivalent code
    snippet implemented in the target language while ensuring that both implementations
    possess identical functionality. Here, we utilize the codefuseEval ⁴⁴4https://github.com/codefuse-ai/codefuse-evaluation Di
    et al. ([2023](#bib.bib19)) evaluation datasets’ code translation subset to support
    bidirectional translation between Java, Python, and C++. In order to evaluate
    the accuracy and functional equivalence of the translation results, we employ
    test cases that are semantically equivalent to ones of the source program for
    each task. These test cases are used to verify whether the result code can run
    and pass successfully, as indicated by the pass@1 criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test results of the three models are presented in Table [5](#S4.T5 "Table
    5 ‣ 4.3.4 Code Translation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning"). The MFT-5Tasks model performs
    the best in Python-to-Java, Python-to-C++, and C++-to-Java translations. The SFT-Mixed
    model excels in C++-to-Python translation, while the SFT-S-CodeTrans model performs
    the best in Java-to-Python and Java-to-C++ translations. Overall, the MFT-5Tasks
    model demonstrates superior performance, with an average improvement of 0.93%
    over SFT-Mixed and 10.9% over SFT-S-CodeTrans. This task also highlights the phenomenon
    that models trained using the MFT approach outperform the other two training methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison of pass@1 Metric Performance on the codefuseEVAL-CodeTranslation
     Di et al. ([2023](#bib.bib19)) (zero-shot, greedy-decoding)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Model | Py2Java | Py2C++ | Java2Py | C++2Py | Java2C++ | C++2Java
    | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-CodeTrans | 59.52% | 57.40% | 70.73% | 62.20% | 67.07% | 62.80% | 63.29%
    |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 80.16% | 71.20% | 67.68% | 72.56% | 65.85% | 82.31% | 73.29%
    |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 82.16% | 77.20% | 65.85% | 70.73% | 64.64% | 84.76% | 74.22%
    |'
  prefs: []
  type: TYPE_TB
- en: 4.3.5 Unit Test Case Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The task at hand is to generate unit test cases by training a model to produce
    a set of test cases for a given code snippet, such as a method or class, and verify
    if the provided code implementation is correct. We have opted to utilize the unittest
    subset from the codefuseEVAL Di et al. ([2023](#bib.bib19)) evaluation datasets
    as our test suite. We evaluate the test cases using the pass@1 metric, which means
    that if a model generates test cases for a sample program and the sample program
    passes all the test cases, the count of correctly generated samples increases
    by 1\. Similar to CodeLLama Rozière et al. ([2023](#bib.bib47)), we employ a greedy
    decoding strategy during the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compared three models across Python, Java, and JavaScript for their test
    generation capabilities. The results in Table [6](#S4.T6 "Table 6 ‣ 4.3.5 Unit
    Test Case Generation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning") indicate that the MFT-5Tasks model outperforms
    others in test generation for Python, with a 5.73% lead over the second-ranked
    SFT-Mixed model, and a significant 10.19% lead over the third-ranked SFT-S-UnitTest
    model. In JavaScript, the MFT-5Tasks model also excels, with a 7.93% advantage
    over the other models. However, in Java, the MFT-5Tasks model performs 5.37% better
    than SFT-S-UnitTest, but slightly lags behind SFT-Mixed by 5.44%. Overall, the
    MFT-5Tasks model consistently demonstrates the highest performance, with an average
    improvement of 2.74% over SFT-Mixed, and a remarkable 7.83% improvement over SFT-S-UnitTest.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, training models using the MFT approach yielded better performance
    compared to models fine-tuned with the same data mixed together. Furthermore,
    the performance of the MFT-trained models surpassed that of the model individually
    fine-tuned for the UNIT-TEST task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparison of pass@1 Metric Performance on the codefuseEVAL-TestcaseGeneration Di
    et al. ([2023](#bib.bib19)) (zero-shot, greedy-decoding)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trained Model | Python | Java | JavaScript | Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-S-UnitTest | 33.76% | 32.43% | 41.46% | 35.88% |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 38.22% | 43.24% | 41.46% | 40.97% |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 43.95% | 37.8% | 49.39% | 43.71% |'
  prefs: []
  type: TYPE_TB
- en: 4.3.6 Generalization on an Unseen Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to evaluating the performance of our models on tasks with training
    data to address RQ1 and RQ2, we also have RQ3 to answer, which is whether the
    models trained using the MFT approach exhibit better generalization on unseen
    tasks compared to models trained by combining multiple datasets into a single
    SFT approach. To investigate this, we selected the Text-to-SQL generation task
    as our testing target. The data for this task was not included in the training
    of the seven existing models. Furthermore, this task is code-related but distinctly
    different from the previous five downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We have selected two evaluation metrics, BLEU score and logical accuracy of
    SQL statements. The BLEU score assesses the textual similarity between the generated
    outputs and the reference answers. The logical accuracy metric, on the other hand,
    allows us to address the variations in SQL syntax that may occur. Specifically,
    Logical Accuracy measures the proportion of test samples in the dataset where
    the generated SQL statements are both syntactically correct and semantically equivalent
    to the reference answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Sample Text-to-SQL Test Cases from the Spider Yu et al. ([2019b](#bib.bib57))
    dataset and Example Predictions by the SFT-Mixed Model and MFT-5Tasks Model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| PROMPT: |'
  prefs: []
  type: TYPE_TB
- en: '&#124; <&#124;role_start&#124;>system<&#124;role_end&#124;>You are a professional
    SQL engineer and you are writing SQL queries for data query tasks. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; <&#124;role_start&#124;>human<&#124;role_end&#124;>What are the names
    of the heads who are born outside the California state? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; schema:head(head_ID&#124;name&#124;born_state&#124;age &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; <&#124;role_start&#124;>bot<&#124;role_end&#124;> &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LABEL: |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SELECT name FROM head WHERE born_state != ’California’ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed: |'
  prefs: []
  type: TYPE_TB
- en: '&#124; You can use the SQL SELECT statement to retrieve the names of the heads
    who are born outside the California state. Here is a sample query: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; “‘sql &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SELECT name &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FROM head &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WHERE born_state <> ’California’; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; “‘ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; This query selects the name column from the head table where the born_state
    column is not equal to ’California’. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MFT-5Tasks: |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SELECT name &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FROM head &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WHERE born_state <> ’California’; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'We selected five representative text-to-SQL datasets, including WikiSQL Zhong
    et al. ([2017](#bib.bib60)), Spider Yu et al. ([2019b](#bib.bib57)), CSpider Min
    et al. ([2019](#bib.bib41)), CoSQL Yu et al. ([2019a](#bib.bib56)), and BirdSQL Li
    et al. ([2023d](#bib.bib31)), and randomly sampled 200 examples from each dataset
    for evaluation. The test case examples are shown in Table [7](#S4.T7 "Table 7
    ‣ 4.3.6 Generalization on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"), where the first row
    demonstrates the fine-tuned data format similar to OpenAI ChatML format ⁵⁵5https://github.com/openai/openai-python/blob/main/chatml.md.
    Using each sampled dataset, we tested the logical accuracy and BLEU score of the
    SFT-Mixed and MFT-5Tasks models, as shown in Table [8](#S4.T8 "Table 8 ‣ 4.3.6
    Generalization on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Table [8](#S4.T8 "Table 8 ‣ 4.3.6 Generalization on an Unseen
    Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning"), MFT-5Tasks outperforms SFT-Mixed in terms of BLEU scores
    on each dataset, averaging 2.78 times higher. This indicates that the generated
    results of MFT-5Tasks exhibit higher similarity to the reference answer texts.
    This similarity can also be observed in Table [7](#S4.T7 "Table 7 ‣ 4.3.6 Generalization
    on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"), where MFT-5Tasks produces cleaner results,
    while SFT-Mixed provides more explanations, which may be preferred in certain
    scenarios. Moreover, MFT-5Tasks demonstrates better performance in terms of logical
    accuracy, achieving an overall accuracy that is 2.18 times higher than SFT-Mixed
    model, and up to 4.67 times higher on the WikiSQL dataset. Numerically, MFT-5Tasks
    exhibits superior performance compared to SFT-Mixed, indicating stronger generalization
    of MFT-trained models on the Text-to-SQL task, which is an unseen task during
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison of generalization capabilities between MFT-5Tasks and SFT-Mixed
    on the Text-to-SQL task. The evaluation metrics include SQL logical accuracy and
    BLEU score.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Trained Model | WIKISQL | SPIDER | CSPIDER | COSQL | BiRDSQL | Average |'
  prefs: []
  type: TYPE_TB
- en: '| Logical Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 1.5% | 2.0% | 7.0% | 6.5% | 5.5% | 4.5% |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 7.0% (4.67x) | 4.5% (2.25x) | 16.5% (2.36x) | 10.5%(1.62x) |
    10.5% (1.91x) | 9.8% (2.18x) |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| SFT-Mixed | 0.032 | 0.047 | 0.025 | 0.081 | 0.026 | 0.042 |'
  prefs: []
  type: TYPE_TB
- en: '| MFT-5Tasks | 0.138 | 0.138 | 0.116 | 0.119 | 0.074 | 0.117 |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Evaluation Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We selected five downstream tasks related to code and trained a total of seven
    models, including SFT-S-* models fine-tuned individually for each task, the SFT-Mixed
    model fine-tuned with a mixture of all task data, and the MFT-5Tasks model trained
    using the MFT method. We compared and tested the performance of each model in
    terms of their target capabilities. Additionally, we evaluated the generalization
    performance of the MFT method and the mixed SFT method on unseen tasks. The results
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: i
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Models trained with the MFT method outperformed those fine-tuned individually
    for each task, indicating a positive answer to RQ1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ii
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Models trained with the MFT method outperformed those fine-tuned with a mixture
    of multiple tasks, providing a positive answer to RQ2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: iii
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Models trained with the MFT method exhibit stronger generalization capabilities
    on new, unseen tasks compared to the SFT models fine-tuned with a mixture of multiple
    task data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Considering the outstanding performance of the MFT training method, we have
    leveraged our MFTCoder ^([2](#footnote2 "footnote 2 ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning")), developed based on this approach, to
    fine-tune the existing mainstream open-source LLM models. e.g. QWen Bai et al.
    ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)), CodeGeex2 Zheng et al.
    ([2023](#bib.bib59)), Llama Touvron et al. ([2023a](#bib.bib51)), LLama2 Touvron
    et al. ([2023b](#bib.bib52)), CodeLLama Rozière et al. ([2023](#bib.bib47)), StarCoder Li
    et al. ([2023a](#bib.bib32)).'
  prefs: []
  type: TYPE_NORMAL
- en: MFTCoder supports Lora and QLora, which significantly reduces the number of
    model training parameters. Coupled with dual quantization for model size compression,
    this ultimately leads to a substantial reduction in GPU memory requirements. As
    a result, it becomes possible to fine-tune a 70B model on a single A100 GPU with
    ease. When fine-tuning these models using MFTCoder, we set the trainable parameters
    to be within the range of 0.1% to 5% of the total parameters. Through empirical
    evidence, we have found that as the proportion of trainable parameters increases,
    performance improvement tends to plateau. In fact, we have observed that a trainable
    parameter proportion of less than 5% is often sufficient to achieve performance
    levels close to that of full-scale fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'When fine-tuning these models, we configure them for multitasking with a range
    of 3-7 tasks. Depending on the model size, we typically use Lora mode for models
    below 20B, and QLora mode for models larger than 20B. After fine-tuning, we evaluate
    their performance in code completion and text-to-code generation tasks, measuring
    their performance on HumanEval Chen et al. ([2021](#bib.bib12)) and MBPP Austin
    et al. ([2021](#bib.bib7)), as shown in Table [9](#S5.T9 "Table 9 ‣ 5 Application
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") Column III and IV.
    We have calculated the average improvement of MFT fine-tuning compared to the
    base models in terms of HumanEval and MBPP. As shown in column 5, the improvement
    ranges from 6.26% to 12.75%, with the improvements on HumanEval consistently surpassing
    those on MBPP. Additionally, we have also evaluated the code completion performance
    of the MFTCoder fine-tuned models on the multilingual benchmark, HumanEval-X Zheng
    et al. ([2023](#bib.bib59)). The results of this evaluation are presented in Table [10](#S5.T10
    "Table 10 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning").
    Notably, the fine-tuned CodeFuse-CodeLLama-Python-MFT (34B) achieved an average
    pass@1 of 56.88% across four languages: Java, C++, JavaScript, and Golang.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [9](#S5.T9 "Table 9 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning") also presents the performance of fine-tuned open-source
    models (e.g. OctoPack Muennighoff et al. ([2023](#bib.bib42)) and WizardCoder-Python Luo
    et al. ([2023](#bib.bib40))) and representative closed-source models (e.g., Claude2 Anthropic
    ([2023](#bib.bib5)), GPT-4 OpenAI ([2023](#bib.bib43))) on HumanEval and MBPP.
    It is worth noting that our fine-tuned model, CodeFuse-CodeLLama-34B ⁶⁶6https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B,
    based on CodeLlama-34B-Python achieves a remarkable performance of 74.4% on HumanEval,
    surpassing all the listed models in the table, including GPT-4 (67.00%, zero-shot) OpenAI
    ([2023](#bib.bib43)). We also evaluated the performance of the model on other
    benchmarks, including multilingual HumanEval-X Zheng et al. ([2023](#bib.bib59)),
    MBPP Austin et al. ([2021](#bib.bib7)), DS-1000 Lai et al. ([2022](#bib.bib28))and
    codefuseEval Di et al. ([2023](#bib.bib19)), and compared it against GPT-3.5 and
    GPT-4, as shown in Figure [5](#S5.F5 "Figure 5 ‣ 5 Application ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). CodeFuse-CodeLLama-34B outperforms GPT-4
    on CodeFuseEval-UnitTest and Humaneval, matches its performance in code translation
    ability, but falls behind in Chinese code completion (CodeFuseEval-CN), multi-language
    completion, data-science analysis (DS-1000), and text-to-code generation (MBPP)
    capabilities compared to GPT-4\. However, it surpasses or equals GPT-3.5 on all
    evaluation datasets. The input-output examples on each evaluation dataset can
    be found in Appendix [C](#A3 "Appendix C Examples of codefuse-codellama-34b across
    various tasks ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we conducted an evaluation to assess the impact of fine-tuning
    the models with MFTCoder and code-related data on their performance in NLP tasks,
    as illustrated in Figure [6](#S5.F6 "Figure 6 ‣ 5 Application ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). Taking CodeFuse-QWen-14B as a case study,
    we compared it against the base model QWen-14B and the official model QWen-14B-chat
    fine-tuned by Alibaba Cloud on top of it. It is evident that CodeFuse-QWen-14B
    maintains its proficiency in NLP. In fact, it exhibits a slight enhancement in
    language, reasoning, and understanding abilities compared to the other two models.
    However, there is a minor decline in its examination ability when compared to
    the base model QWen-14B and similar findings are observed for the fine-tuned QWen-14B-chat
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: pass@1 performance on HumanEval Chen et al. ([2021](#bib.bib12)) (Code
    Completion) and MBPP Austin et al. ([2021](#bib.bib7)) (Text-to-Code Generation)
    after fine-tuning with MFTCoder across multiple mainstream open-source models.
    The CodeFuse-*-MFT models are evaluated using a combination of greedy decoding
    and zero-shot testing strategy, while the metric values for the other models are
    taken from their respective papers, technical reports, or open-source project
    homepages.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Humaneval &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MBPP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average |'
  prefs: []
  type: TYPE_TB
- en: '| Open-source base models |  |'
  prefs: []
  type: TYPE_TB
- en: '| QWen-base Bai et al. ([2023](#bib.bib8)) | 14B | 32.3% | 40.8% | 36.55% |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-base Touvron et al. ([2023a](#bib.bib51)) | 65B | 23.7% | 37.7% | 30.7%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-base Touvron et al. ([2023b](#bib.bib52)) | 70B | 29.9% | 45.0% |
    37.45% |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder-base Li et al. ([2023a](#bib.bib32)) | 15B | 33.6% | 52.7% | 43.15%
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGeex2-base Zheng et al. ([2023](#bib.bib59)) | 6B | 35.9% | 42.4% | 39.15%
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 13B | 43.3% |
    49.0% | 46.15% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 34B | 53.7% |
    56.2% | 54.95% |'
  prefs: []
  type: TYPE_TB
- en: '| MFT fine-tuned models |  |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-QWen-MFT ⁷⁷7https://huggingface.co/codefuse-ai/CodeFuse-QWen-14B
    | 14B | 48.78% | 43.8% | 46.29% (+9.74%) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-Llama-MFT | 65B | 34.76% | 41.8% | 38.28% (+7.58) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-Llama2-MFT | 70B | 40.85% | 40.8% | 40.83% (+3.38%) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-StarCoder-MFT ⁸⁸8https://huggingface.co/codefuse-ai/CodeFuse-StarCoder-15B
    | 15B | 54.90% | 49.60% | 52.25% (+9.10%) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 46.2% | 45.66% (+6.51%) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 56.0% | 58.19% (+12.04%) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-CodeLLama-Python-MFT ^([6](#footnote6 "footnote 6 ‣ 5 Application
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")) | 34B | 74.4% | 61.0%
    | 67.70% (+12.75%) |'
  prefs: []
  type: TYPE_TB
- en: '| Open-source fine-tuned models |  |'
  prefs: []
  type: TYPE_TB
- en: '| QWen-chat Bai et al. ([2023](#bib.bib8)) | 14B | 43.9% | 46.4% | 45.15% |'
  prefs: []
  type: TYPE_TB
- en: '| PHI-1 Gunasekar et al. ([2023](#bib.bib22)) | 1.3B | 50.6% | 55.5% | 53.05%
    |'
  prefs: []
  type: TYPE_TB
- en: '| OctoCoder Muennighoff et al. ([2023](#bib.bib42)) | 15B | 46.2% | NA | NA
    |'
  prefs: []
  type: TYPE_TB
- en: '| WizardCoder Luo et al. ([2023](#bib.bib40)) | 15B | 57.3% | 51.8% | 54.55%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Phind-CodeLlama-v2 Phind ([2023](#bib.bib45)) | 34B | 71.95% | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: '| WizardCoder-Python Luo et al. ([2023](#bib.bib40)) | 34B | 73.2% | 61.2%
    | 67.2% |'
  prefs: []
  type: TYPE_TB
- en: '| Closed-source models |  |'
  prefs: []
  type: TYPE_TB
- en: '| PanGu-Coder2 Shen et al. ([2023](#bib.bib48)) | 15B | 61.2% | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Unnatural CodeLlama Rozière et al. ([2023](#bib.bib47)) | 34B | 62.2% | 61.2%
    | 61.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Claude2 Anthropic ([2023](#bib.bib5)) | NA | 71.2% | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 OpenAI ([2023](#bib.bib43)) | 175B | 48.1% | 52.2% | 50.15% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 (zero-shot) OpenAI ([2023](#bib.bib43)) | NA | 67.00% | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: pass@1 performance on Multi-lingual HumanEval-X (Zheng et al., [2023](#bib.bib59))
    after fine-tuning with MFTCoder across multiple mainstream open-source models.
    The metric values marked with an asterisk (*) were obtained from the models’ corresponding
    papers, technical reports, or open-source project homepages, while the remaining
    metric values were evaluated using a combination of greedy decoding and zero-shot
    testing strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Python | Java | C++ | JavaScript | Golang | Avgerage |'
  prefs: []
  type: TYPE_TB
- en: '| QWen-base | 14B | $32.3\%^{*}$ | 35.37% | 30.49% | 32.93% | 21.34% | 30.49%
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-QWen-MFT | 14B | 48.78% | 41.46% | 38.41% | 46.34% | 26.83% | 40.36%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-base | 65B | $23.7\%^{*}$ | 29.26% | 20.73% | 23.78% | 18.9% | 23.27%
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-Llama-MFT | 65B | 34.76% | 37.2% | 29.88% | 32.93% | 23.78% | 31.71%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-base | 70B | $29.9\%^{*}$ | 39.02% | 31.10% | 35.98% | 23.78% | 31.96%
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-Llama2-MFT | 70B | 40.85% | 35.98% | 32.32% | 38.41% | 27.44% |
    35.00% |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder-base | 15B | $33.6\%^{*}$ | 34.15% | 25.61% | 22.56% | 22.56% |
    29.48% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-StarCoder-MFT | 15B | 54.9% | 47.56 | 46.34% | 48.17% | 37.20% |
    46.83% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGeex2-base | 6B | $35.9\%^{*}$ | $22.5\%^{*}$ | 30.14% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 45.73% | 37.2% | 37.2% | 28.05% |
    38.66% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-Python-base | 13B | $43.3\%^{*}$ | 41.46% | 34.76% | 38.41% | 29.27%
    | 37.44% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 57.32% | 46.34% | 54.27% |
    45.12% | 52.68% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-34B-Python-base | 34B | $53.7\%^{*}$ | 45.73% | 42.68% | 45.73%
    | 31.71% | 43.91% |'
  prefs: []
  type: TYPE_TB
- en: '| CodeFuse-CodeLLama-Python-MFT | 34B | 74.4% | 61.6% | 54.3% | 61.0% | 50.6%
    | 60.38% |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/0681650bc66eb8a200ef95534b4da45c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Radar Chart of CodeFuse-CodeLlama-34B Model on HumanEval, HumanEval-X,
    MBPP, DS-1000, and codefuseEval benchmarks compared to GPT-3.5 and GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/634af086fd97c1ba7de354f05fd1e1b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Performance comparison of CodeFuse-QWen-14B fine-tuned with MFTCoder
    and code-related data, QWen-14B base model, and officially fine-tuned model QWen-14B-chat
    on NLP evaluation datasets. Detailed data can be found in Appendix [D](#A4 "Appendix
    D Evaluation results of CodeFuse-QWen-14B on NLP benchmark datasets ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the superior performance of the MFT training method compared to the
    task data mixing-based SFT training method in the aforementioned experiments,
    it should be noted that the effectiveness of the MFT approach is highly dependent
    on the task-splitting strategy. Not all scenarios are suitable for being split
    into multiple tasks. For instance, in our practical experience, we found that
    splitting a task based on difficulty levels and training it using the MFT method
    did not yield better results compared to the task-mixed SFT training method. Moreover,
    training code completion tasks as multiple tasks based on programming languages
    also did not outperform the mixed SFT method. Based on our practical experience,
    we have concluded that tasks with distinct main desired abilities are more suitable
    for task splitting and MFT training, whereas tasks with similar main training
    objectives are not well-suited for MFT training. We plan to further investigate
    and establish more precise criteria for task delineation in future research.
  prefs: []
  type: TYPE_NORMAL
- en: In our task generalization experiments, we observed that models trained using
    the MFT method produced inference results that were more similar to the reference
    answers and had more concise content. Conversely, inference results generated
    by the task-mixed SFT training method contained more Chain-of-Thought (CoT) information.
    In certain scenarios, the former approach was more preferred, such as in IDE plugins,
    while the latter approach was favored in other scenarios, such as web assistants.
    As a result, we cannot simply generalize that one method is better than the other.
    We are currently researching the reasons behind these performance differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a multi-task learning method, MFT also faces a major challenge during the
    training process: inconsistent convergence speeds among different tasks. For example,
    in the aforementioned experiments, the code completion task converged much faster
    than the unit test-case generation task (details can be found in Appendix [B](#A2
    "Appendix B Convergence Speed of MFT Training Process ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning")). This makes it difficult to find an optimal
    point that performs well on all tasks. The selected checkpoint either converges
    insufficiently on some tasks or overfits on others. To address this issue, we
    experimented with existing multi-task learning balancing optimization solutions
    such as FAMO Liu et al. ([2023](#bib.bib35)). However, FAMO requires dual back-propagation
    in each iteration, resulting in training time being approximately doubled. Furthermore,
    the required number of epochs for convergence also increases significantly, and
    the adjustability of the convergence speed is limited. Unfortunately, this exponentially
    increased cost does not yield equivalent benefits. In response, we are currently
    developing a more optimal and adaptive multi-task optimization balancing approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, even after balancing the convergence speeds of multiple tasks,
    where the same set of parameters is updated, it is still challenging to fundamentally
    eliminate the inherent conflicts in weight updates across different tasks. To
    address this issue, we are currently exploring the utilization of MoE (Mixture
    of Experts) Chen et al. ([2022](#bib.bib14)) to achieve MFT.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper introduces MFTCoder, a framework that supports multi-task fine-tuning,
    effectively addressing the challenges of data imbalance, varying difficulty levels,
    and inconsistent convergence speeds through the design of various loss functions.
    Experimental results demonstrate that this approach outperforms individual fine-tuning
    on each task or fine-tuning on a mixed ensemble of tasks. Additionally, MFTCoder
    facilitates efficient training, including efficient data utilization and PEFT
    training. It also provides a high-quality instruction dataset construction solution.
    Leveraging MFTCoder for fine-tuning on the CodeLLama base, the CodeFuse-CodeLLama-34B
    model achieves an impressive pass@1 score of 74.4% on the HumanEval dataset, surpassing
    the performance of GPT-4 (67%, zero-shot).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aghajanyan et al. (2021) Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
    Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive Multi-task
    Representations with Pre-Finetuning. arXiv:cs.CL/2101.11038'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi,
    Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel
    Romero, Michael Lappert, Francesco De Toni, Bernardo GarcÃa del RÃo, Qian Liu,
    Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco
    Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa,
    Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha,
    Harm de Vries, and Leandro von Werra. 2023. SantaCoder: don’t reach for the stars!
    arXiv:cs.SE/2301.03988'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, ClÃ©ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark DÃaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2
    Technical Report. arXiv:cs.CL/2305.10403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anthropic (2023) Anthropic. 2023. *Model Card and Evaluations for Claude Models*.
    [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aribandi et al. (2022) Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven
    Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo
    Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022. ExT5: Towards
    Extreme Multi-Task Scaling for Transfer Learning. arXiv:cs.CL/2111.10952'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program Synthesis with Large Language Models. *arXiv preprint
    arXiv:2108.07732* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. *arXiv preprint arXiv:2309.16609*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baichuan (2023) Baichuan. 2023. Baichuan 2: Open Large-scale Language Models.
    *arXiv preprint arXiv:2309.10305* (2023). [https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan
    Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive
    Language Model. arXiv:cs.CL/2204.06745'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caruana (1997) Rich Caruana. 1997. Multitask learning. *Machine learning* 28
    (1997), 41–75.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. (2021).
    arXiv:cs.LG/2107.03374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew
    Rabinovich. 2018. Gradnorm: Gradient normalization for adaptive loss balancing
    in deep multitask networks. In *International conference on machine learning*.
    PMLR, 794–803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022) Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi
    Li. 2022. Towards Understanding Mixture of Experts in Deep Learning. arXiv:cs.LG/2208.02813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language
    Modeling with Pathways. arXiv:cs.CL/2204.02311'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christopoulou et al. (2022) Fenia Christopoulou, Gerasimos Lampouras, Milan
    Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin
    Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng
    Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022.
    PanGu-Coder: Program Synthesis with Function-Level Language Modeling. arXiv:cs.LG/2207.11280'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crawshaw (2020) Michael Crawshaw. 2020. Multi-task learning with deep neural
    networks: A survey. *arXiv preprint arXiv:2009.09796* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:cs.LG/2305.14314'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Di et al. (2023) Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang
    Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi
    Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao,
    Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan
    Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin
    Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. 2023. CodeFuse-13B:
    A Pretrained Multi-lingual Code Large Language Model. arXiv:cs.SE/2310.06266'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with
    Autoregressive Blank Infilling. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*. 320–335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duong et al. (2015) Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015.
    Low resource dependency parsing: Cross-lingual parameter sharing in a neural network
    parser. In *Proceedings of the 53rd annual meeting of the Association for Computational
    Linguistics and the 7th international joint conference on natural language processing
    (volume 2: short papers)*. 845–850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. *arXiv preprint
    arXiv:2306.11644* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. arXiv:cs.LG/1902.00751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation
    of Large Language Models. arXiv:cs.CL/2106.09685'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jean et al. (2019) Sébastien Jean, Orhan Firat, and Melvin Johnson. 2019. Adaptive
    scheduling for multi-task learning. *arXiv preprint arXiv:1909.06434* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall et al. (2018) Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task
    learning using uncertainty to weigh losses for scene geometry and semantics. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    7482–7491.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method
    for Stochastic Optimization. arXiv:cs.LG/1412.6980'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2022) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
    2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
    *ArXiv* abs/2211.11501 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2018) Hae Beom Lee, Eunho Yang, and Sung Ju Hwang. 2018. Deep asymmetric
    multi-task feature learning. In *International Conference on Machine Learning*.
    PMLR, 2956–2964.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023c. CAMEL: Communicative Agents for "Mind"
    Exploration of Large Scale Language Model Society. arXiv:cs.AI/2303.17760'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023d) Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen
    Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao
    Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.
    2023d. Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale
    Database Grounded Text-to-SQLs. arXiv:cs.CL/2305.03111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. 2023a. StarCoder: may the source be with you! *arXiv preprint arXiv:2305.06161*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks Are All You Need II: phi-1.5
    technical report. *arXiv preprint arXiv:2309.05463* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal
    Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de
    Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation
    with AlphaCode. *Science* 378, 6624 (dec 2022), 1092–1097. [https://doi.org/10.1126/science.abq1158](https://doi.org/10.1126/science.abq1158)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Bo Liu, Yihao Feng, Peter Stone, and Qiang Liu. 2023. FAMO:
    Fast Adaptive Multitask Optimization. arXiv:cs.LG/2306.03792'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019b) Shikun Liu, Edward Johns, and Andrew J Davison. 2019b. End-to-end
    multi-task learning with attention. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 1871–1880.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019c) Shengchao Liu, Yingyu Liang, and Anthony Gitter. 2019c. Loss-balanced
    task weighting to reduce negative transfer in multi-task learning. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 33\. 9977–9978.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019a. Multi-Task Deep Neural Networks for Natural Language Understanding. In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.
    4487–4496.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2017) Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S
    Yu. 2017. Learning multiple tasks with multilinear relationship networks. *Advances
    in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering
    Code Large Language Models with Evol-Instruct. *arXiv preprint arXiv:2306.08568*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min et al. (2019) Qingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A Pilot Study
    for Chinese SQL Semantic Parsing. arXiv:cs.CL/1909.13293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Muennighoff et al. (2023) Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
    Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra,
    and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models.
    arXiv:cs.CL/2308.07124'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:cs.CL/2303.08774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pascal et al. (2021) Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet,
    and Maria A Zuluaga. 2021. Maximum roaming multi-task learning. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 35\. 9331–9341.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phind (2023) Phind. 2023. *Phind-CodeLlama-34B-v2*. [https://huggingface.co/Phind/Phind-CodeLlama-34B-v2](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:cs.LG/1910.10683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing
    Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, et al. 2023. Pangu-coder2:
    Boosting large language models for code with ranking feedback. *arXiv preprint
    arXiv:2307.14936* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko.
    2020. Adashare: Learning what to share for efficient deep multi-task learning.
    *Advances in Neural Information Processing Systems* 33 (2020), 8728–8740.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning
    language model with self generated instructions. *arXiv preprint arXiv:2212.10560*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui,
    Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code Large Language Models
    for Code Understanding and Generation. arXiv:cs.CL/2305.07922'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Hospedales (2017) Yongxin Yang and Timothy Hospedales. 2017. Trace
    Norm Regularised Deep Multi-Task Learning. In *5th International Conference on
    Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019a) Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang,
    Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro
    Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen
    Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S Lasecki,
    and Dragomir Radev. 2019a. CoSQL: A Conversational Text-to-SQL Challenge Towards
    Cross-Domain Natural Language Interfaces to Databases. arXiv:cs.CL/1909.05378'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019b) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir
    Radev. 2019b. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain
    Semantic Parsing and Text-to-SQL Task. arXiv:cs.CL/1809.08887'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang,
    and Ying Wu. 2018. A modulation module for multi-task learning with applications
    in image retrieval. In *Proceedings of the European Conference on Computer Vision
    (ECCV)*. 401–416.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and
    Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual
    Evaluations on HumanEval-X. In *KDD*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement
    Learning. *CoRR* abs/1709.00103 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Code Exercises Generation with Camel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 11: Examples of Prompts for Code Exercises Generation using Camel'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Original task prompt: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Create 50 coding exercises to test and improve students’ Python programming
    skills. The exercises must must must focus on Binary search. The difficulty of
    exercises must be from easy to medium to hard. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Specified task prompt: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A student will collaborate with a programming teacher to develop 50
    Python coding exercises centered specifically on Binary search algorithms. The
    exercises should progressively increase in difficulty, from easy to medium to
    hard, targeting various aspects of implementation and optimization techniques.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Final task prompt: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A student will collaborate with a programming teacher to develop 50
    Python coding exercises centered specifically on Binary search algorithms. The
    exercises should progressively increase in difficulty, from easy to medium to
    hard, targeting various aspects of implementation and optimization techniques.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AI Assistant sys message: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SystemMessage(role_name=’A student’, role_type = <RoleType.ASSISTANT:
    ’assistant’>, meta_dict={’task’: ’A student will assist a programming teacher
    in designing a series of 50 Python coding exercises that emphasize the application
    and optimization of Bubble Sort algorithm. These exercises will span varying levels
    of difficulty, from easy to medium to hard, allowing students to strengthen their
    sorting skills progressively.’, ’assistant_role’: ’A student’, ’user_role’: ’A
    programming teacher’}, role=’system’, content=’Never forget you are a A student
    and I am a A programming teacher. Never flip roles! Never instruct me! &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; We share a common interest in collaborating to successfully complete
    a task. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You must help me to complete the task. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Here is the task: A student will assist a programming teacher in designing
    a series of 50 Python coding exercises that emphasize the application and optimization
    of Bubble Sort algorithm. These exercises will span varying levels of difficulty,
    from easy to medium to hard, allowing students to strengthen their sorting skills
    progressively.. Never forget our task! &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I must instruct you based on your expertise and my needs to complete
    the task. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I must give you one instruction at a time. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You must write a specific solution that appropriately solves the requested
    instruction and explain your solutions. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You must decline my instruction honestly if you cannot perform the instruction
    due to physical, moral, legal reasons or your capability and explain the reasons.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unless I say the task is completed, you should always start with: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Solution: <YOUR_SOLUTION> &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; <YOUR_SOLUTION> should be very specific, include detailed explanations
    and provide preferable detailed implementations and examples and lists for task-solving.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Always end <YOUR_SOLUTION> with: Next request.’) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AI User sys message: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SystemMessage(role_name=’A programming teacher’, role_type = <RoleType.USER:
    ’user’>, meta_dict = {’task’: ’A student will assist a programming teacher in
    designing a series of 50 Python coding exercises that emphasize the application
    and optimization of Bubble Sort algorithm. These exercises will span varying levels
    of difficulty, from easy to medium to hard, allowing students to strengthen their
    sorting skills progressively.’, ’assistant_role’: ’A student’, ’user_role’: ’A
    programming teacher’}, role=’system’, content=’Never forget you are a A programming
    teacher and I am a A student. Never flip roles! You will always instruct me. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; We share a common interest in collaborating to successfully complete
    a task. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I must help you to complete the task. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Here is the task: A student will assist a programming teacher in designing
    a series of 50 Python coding exercises that emphasize the application and optimization
    of Bubble Sort algorithm. These exercises will span varying levels of difficulty,
    from easy to medium to hard, allowing students to strengthen their sorting skills
    progressively.. Never forget our task! &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You must instruct me based on my expertise and your needs to solve the
    task ONLY in the following two ways: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1\. Instruct with a necessary input: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Instruction: <YOUR_INSTRUCTION> &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input: <YOUR_INPUT> &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2\. Instruct without any input: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Instruction: <YOUR_INSTRUCTION> &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input: None &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The "Instruction" describes a task or question. The paired "Input" provides
    further context or information for the requested "Instruction". &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You must give me one instruction at a time. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I must write a response that appropriately solves the requested instruction.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I must decline your instruction honestly if I cannot perform the instruction
    due to physical, moral, legal reasons or my capability and explain the reasons.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You should instruct me not ask me questions. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Now you must start to instruct me using the two ways described above.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Do not add anything else other than your instruction and the optional
    corresponding input! &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Keep giving me instructions and necessary inputs until you think the
    task is completed. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; When the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Never say <CAMEL_TASK_DONE> unless my responses have solved your task.’)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Convergence Speed of MFT Training Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7886b1c3507c2b0fb5aa11df31c35bb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Code Comment Task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81b6cf2416231f62d039b89106b6edb6.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Code Completion Task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2dcbb01369c8f0b252e2660ae617f418.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Code Translation Task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8eed76d3e6fb88cce0ad431e74fea032.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Text-to-Code Task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/15dc9c6bfc3ad38da9fa1f2de84f913c.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Unit Test-case Generation Task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/828b915520980a1e0b676e65e52f57c7.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Overall of 5 tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Validation Loss Convergence Speeds: A Comparative Analysis of 5 Code-related
    Downstream Tasks and Overall Training Progress using the data-balanced loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Examples of codefuse-codellama-34b across various tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Code Completion on HumanEval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS1.p1.pic1" class="ltx_picture" height="928.27" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,928.27) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 910.22)"><foreignobject width="556.69"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt:</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="878.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000"><|role_start|>human<|role_end|># language: Python 
    from  typing  import  List def  below_zero(operations:  List[int])  ->  bool:
    """␣You’re␣given␣a␣list␣of␣deposit␣and␣withdrawal␣operations␣on␣a␣bank␣account␣that␣starts␣with
    ␣␣␣␣zero␣balance.␣Your␣task␣is␣to␣detect␣if␣at␣any␣point␣the␣balance␣of␣account␣fallls␣below␣zero,␣and
    ␣␣␣␣at␣that␣point␣function␣should␣return␣True.␣Otherwise␣it␣should␣return␣False.
    ␣␣␣␣>>>␣below_zero([1,␣2,␣3]) ␣␣␣␣False ␣␣␣␣>>>␣below_zero([1,␣2,␣-4,␣5]) ␣␣␣␣True
    ␣␣␣␣""" <|role_start|>bot<|role_end|></foreignobject></g></g></svg><svg id="A3.SS1.p2.pic1"
    class="ltx_picture" height="976.86" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,976.86) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g clip-path="url(#pgfcp13)"><g
    fill-opacity="1" stroke-opacity="1" transform="matrix(1.0 0.0 0.0 1.0 300 488.43)"><foreignobject
    width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned
    image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 961.34)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="929.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">   balance  =  0    for  op  in  operations:    balance  +=  op    if  balance  <  0:    return  True    return  False</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Multilingual Code Completion on HumanEval-X
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS2.p1.pic1" class="ltx_picture" height="1029.59" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,1029.59) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 1009.85)"><foreignobject width="556.69"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    (Java):</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0
    1.0 21.65 13.78)"><foreignobject width="556.69" height="978.35" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000"><|role_start|>human<|role_end|>//
    language: Java 
    import  java.util.*; import  java.lang.*; class  Solution  { /** Given  a  string  s,  count  the  number  of  uppercase  vowels  in  even  indices.
    For  example: countUpper("aBCdEf")  returns  1 countUpper("abcdefg")  returns  0
    countUpper("dBBE")  returns  0 */ public  int  countUpper(String  s)  { <|role_start|>bot<|role_end|></foreignobject></g></g></svg><svg
    id="A3.SS2.p2.pic1" class="ltx_picture" height="2753.53" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,2753.53) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g
    clip-path="url(#pgfcp14)"><g fill-opacity="1" stroke-opacity="1" transform="matrix(1.0
    0.0 0.0 1.0 300 1376.76)"><foreignobject width="0" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 2738.01)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="2706.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">   int  count  =  0;    for  (int  i  =  0;  i  <  s.length();  i  +=  2)  {    char  c  =  s.charAt(i);    if  (Character.isUpperCase(c)  &&  (c  ==  ’A’  ||  c  ==  ’E’  ||  c  ==  ’I’  ||  c  ==  ’O’  ||  c  ==  ’U’))  {    count++;    }    }    return  count;    }  }</foreignobject></g></g></svg>
    <svg id="A3.SS2.p3.pic1" class="ltx_picture" height="1180.18" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,1180.18) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 1160.44)"><foreignobject width="556.69"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    (Golang):</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="1128.95" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000"><|role_start|>human<|role_end|>//
    language: Go 
    import  ( "strings" ) //  Create  a  function  Encrypt  that  takes  a  string  as  an  argument  and
    //  returns  a  string  Encrypted  with  the  alphabet  being  rotated. //  The  alphabet  should  be  rotated  in  a  manner  such  that  the  letters
    //  shift  down  by  two  multiplied  to  two  places. //  For  example: //  Encrypt(’hi’)  returns  ’lm’
    //  Encrypt(’asdfghjkl’)  returns  ’ewhjklnop’ //  Encrypt(’gf’)  returns  ’kj’
    //  Encrypt(’et’)  returns  ’ix’ func  Encrypt(s  string)  string  { <|role_start|>bot<|role_end|></foreignobject></g></g></svg><svg
    id="A3.SS2.p4.pic1" class="ltx_picture" height="3266.88" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,3266.88) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g
    clip-path="url(#pgfcp15)"><g fill-opacity="1" stroke-opacity="1" transform="matrix(1.0
    0.0 0.0 1.0 300 1633.44)"><foreignobject width="0" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 3251.37)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="3219.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">   encrypted  :=  make([]byte,  len(s))    for  i,  c  :=  range  s  {    if  c  >=  ’a’  &&  c  <=  ’z’  {    encrypted[i]  =  byte((c-’a’+2)%26  +  ’a’)  //  shift  down  two  places    }  else  if  c  >=  ’A’  &&  c  <=  ’Z’  {    encrypted[i]  =  byte((c-’A’+2)%26  +  ’A’)    }  else  {    encrypted[i]  =  byte(c)  //  non-alphabetic  characters  remain  the  same    }    }    return  string(encrypted)  }</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Code Completion with Chinese prompts on CodeFuseEval-CN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS3.p1.pic1" class="ltx_picture" height="292.37" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,292.37) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 274.32)"><foreignobject width="556.69"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt:</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="242.83" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000"><|role_start|>human<|role_end|># language: Python![[Uncaptioned
    image]](img/86db69e57c0426a2d0beaae9ba122e98.png) <|role_start|>bot<|role_end|></foreignobject></g></g></svg>
    <svg id="A3.SS3.p2.pic1" class="ltx_picture" height="3367.89" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,3367.89) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g
    clip-path="url(#pgfcp16)"><g fill-opacity="1" stroke-opacity="1" transform="matrix(1.0
    0.0 0.0 1.0 300 1683.95)"><foreignobject width="0" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 3352.38)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="3320.88" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">   encoded_message  =  ’’    vowels  =  {’a’:  ’y’,  ’A’:  ’Y’,  ’e’:  ’c’,  ’E’:  ’C’,  ’i’:  ’g’,  ’I’:  ’G’,  ’o’:  ’m’,  ’O’:  ’M’,  ’u’:  ’s’,  ’U’:  ’S’}      for  letter  in  message:    if  letter.isalpha():    if  letter  in  vowels:    encoded_message  +=  vowels[letter]    elif  letter.islower():    encoded_message  +=  letter.upper()    elif  letter.isupper():    encoded_message  +=  letter.lower()    else:    encoded_message  +=  letter      return  encoded_message  }</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: C.4 Text-to-Code on MBPP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS4.p1.pic1" class="ltx_picture" height="1412.57" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,1412.57) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 1394.52)"><foreignobject width="556.69"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt:</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="1363.02" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000"><|role_start|>human<|role_end|> Write a function to find the number
    of subsequences having product smaller than k for the given non negative array.
    Your code should satisfy these tests: 
    assert  no_of_subsequences([1,2,3,4],  10)  ==  11 assert  no_of_subsequences([4,8,7,2],  50)  ==  9
    assert  no_of_subsequences([5,6,7,8],  15)  ==  4 <|role_start|>bot<|role_end|></foreignobject></g></g></svg><svg
    id="A3.SS4.p2.pic1" class="ltx_picture" height="5194.37" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,5194.37) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g
    clip-path="url(#pgfcp17)"><g fill-opacity="1" stroke-opacity="1" transform="matrix(1.0
    0.0 0.0 1.0 300 2597.19)"><foreignobject width="0" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 5178.86)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="5147.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
    def  no_of_subsequences(arr,  k):    n  =  len(arr)    dp  =  [0]  *  n    dp[0]  =  1    for  i  in  range(1,  n):    dp[i]  =  dp[i-1]    if  arr[i]  <  k:    dp[i]  +=  1    for  j  in  range(i):    if  arr[j]  <  k//arr[i]:    dp[i]  +=  dp[j]    return  dp[n-1]      import  unittest    class  TestNoOfSubsequences(unittest.TestCase):    def  test_no_of_subsequences(self):    self.assertEqual(no_of_subsequences([1,2,3,4],  10),  111)    self.assertEqual(no_of_subsequences([4,8,7,2],  50),  9)    self.assertEqual(no_of_subsequences([5,6,7,8],  15),  4)      if  __name__  ==  "__main__":    unittest.main()</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: C.5 Data Science Analysis on DS-1000
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS5.p1.pic1" class="ltx_picture" height="2488.09" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,2488.09) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 2470.04)"><foreignobject width="556.69"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt:</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="2438.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">I’m using tensorflow 2.10.0. I have a tensor of lengths in tensorflow,
    let’s say it looks like this:  [4,  3,  5,  2]
    I wish to create a mask of 1s and 0s whose number of 1s correspond to the entries
    to this tensor, padded in front by 0s to a total length of 8\. I.e. I want to
    create this tensor: 
    [[0.  0.  0.  0.  1.  1.  1.  1.] [0.  0.  0.  0.  0.  1.  1.  1.] [0.  0.  0.  1.  1.  1.  1.  1.]
    [0.  0.  0.  0.  0.  0.  1.  1.]] How might I do this? A: <code> import tensorflow
    as tf lengths = [4, 3, 5, 2] </code> result = … # put solution in this variable
    BEGIN SOLUTION <code></foreignobject></g></g></svg><svg id="A3.SS5.p2.pic1" class="ltx_picture"
    height="1749.58" overflow="visible" version="1.1" width="600"><g transform="translate(0,1749.58)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill="#BF6000" fill-opacity="1.0"><g clip-path="url(#pgfcp18)"><g fill-opacity="1"
    stroke-opacity="1" transform="matrix(1.0 0.0 0.0 1.0 300 874.79)"><foreignobject
    width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned
    image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 1734.06)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="1702.57" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
    max_length  =  tf.reduce_max(lengths)    #  Create  a  tensor  of  1s  and  0s  mask  =  tf.sequence_mask(lengths,  maxlen=max_length,  dtype=tf.float32)    #  Pad  the  mask  with  0s  to  a  total  length  of  8  padded_mask  =  tf.pad(mask,  [[0,  0],  [0,  8  -  max_length]],  constant_values=0)    result  =  padded_mask</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: C.6 Code Translation on CodeFuseEval-CodeTrans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS6.p1.pic1" class="ltx_picture" height="4931.32" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,4931.32) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 4913.27)"><foreignobject width="556.69"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt:</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="4881.77" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000"><|role_start|>human<|role_end|> 
    #  Translate  the  following  Java  program  to  Python #  Java import  java.util.*;
    import  java.lang.*; class  Solution  { public  List<List<Integer>>  getRow(List<List<Integer>>  lst,  int  x)  {
    List<List<Integer>>  coords  =  new  ArrayList<>(); for  (int  i  =  0;  i  <  lst.size();  i++)  {
    List<List<Integer>>  row  =  new  ArrayList<>(); for  (int  j  =  lst.get(i).size()  -  1;  j  >=  0;  j--)  {
    if  (lst.get(i).get(j)  ==  x)  { row.add(Arrays.asList(i,  j)); } } coords.addAll(row);
    } return  coords; } } #  Python <|role_start|>bot<|role_end|></foreignobject></g></g></svg><svg
    id="A3.SS6.p2.pic1" class="ltx_picture" height="1906.7" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,1906.7) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g
    clip-path="url(#pgfcp19)"><g fill-opacity="1" stroke-opacity="1" transform="matrix(1.0
    0.0 0.0 1.0 300 953.35)"><foreignobject width="0" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 1891.19)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="1859.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
    def  get_row(lst,  x):    coords  =  []    for  i  in  range(len(lst)):    for  j  in  range(len(lst[i])-1,  -1,  -1):    if  lst[i][j]  ==  x:    coords.append([i,  j])    return  coords</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: C.7 Unit test case Generation on CodeFuseEval-UnitTest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<svg id="A3.SS7.p1.pic1" class="ltx_picture" height="5399.01" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,5399.01) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 5380.96)"><foreignobject width="556.69"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt:</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="5349.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000"><|role_start|>human<|role_end|> 
    Write  unit  tests  for  the  following  code,  without  providing  explanations:
    ‘‘‘ public  class  ExceptionUtils  { private  static  final  String[]  CAUSE_METHOD_NAMES  =  {
    "getCause", "getNextException", "getTargetException", "getException", "getSourceException",
    "getRootCause", "getCausedByException", "getNested", "getLinkedException", "getNestedException",
    "getLinkedCause", "getThrowable", }; public  static  Throwable  getCause(final  Throwable  throwable,  String[]  methodNames)  {
    if  (throwable  ==  null)  { return  null; } if  (methodNames  ==  null)  { final  Throwable  cause  =  throwable.getCause();
    if  (cause  !=  null)  { return  cause; } methodNames  =  CAUSE_METHOD_NAMES;
    } return  Stream.of(methodNames).map(m  ->  getCauseUsingMethodName(throwable,  m)).filter(Objects::nonNull).findFirst().orElse(null);
    } } ‘‘‘ <|role_start|>bot<|role_end|></foreignobject></g></g></svg><svg id="A3.SS7.p2.pic1"
    class="ltx_picture" height="10590.8" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,10590.8) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill="#BF6000" fill-opacity="1.0"><g clip-path="url(#pgfcp20)"><g
    fill-opacity="1" stroke-opacity="1" transform="matrix(1.0 0.0 0.0 1.0 300 5295.4)"><foreignobject
    width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![[Uncaptioned
    image]](img/ad790c907c104b4df876d249d118ffe3.png)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 10575.29)"><foreignobject
    width="556.69" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFFFF">CodeFuse-CodeLLama-34B:</foreignobject></g><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="10543.79" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
    ‘‘‘  import  org.junit.Assert;  import  org.junit.Test;    import  java.util.Objects;  import  java.util.stream.Stream;    public  class  ExceptionUtilsTest  {    @Test    public  void  testGetCause_NullThrowable()  {    Throwable  throwable  =  null;    String[]  methodNames  =  {"getCause"};    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(null,  output);    }    @Test    public  void  testGetCause_NullMethodNames()  {    Throwable  throwable  =  new  RuntimeException("test");    String[]  methodNames  =  null;    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(null,  output);    }    @Test    public  void  testGetCause_FoundCause()  {    Throwable  throwable  =  new  RuntimeException("test",  new  NullPointerException());    String[]  methodNames  =  {"getCause"};    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(NullPointerException.class,  output.getClass());    }    @Test    public  void  testGetCause_NotFoundCause()  {    Throwable  throwable  =  new  RuntimeException("test");    String[]  methodNames  =  {"getCause"};    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(null,  output);    }    @Test    public  void  testGetCause_DefaultMethodNames_FoundCause()  {    Throwable  throwable  =  new  RuntimeException("test",  new  NullPointerException());    Throwable  output  =  ExceptionUtils.getCause(throwable);    Assert.assertEquals(NullPointerException.class,  output.getClass());    }    @Test    public  void  testGetCause_DefaultMethodNames_NotFoundCause()  {    Throwable  throwable  =  new  RuntimeException("test");    Throwable  output  =  ExceptionUtils.getCause(throwable);    Assert.assertEquals(null,  output);    }    private  Throwable  getCauseUsingMethodName(Throwable  throwable,  String  methodName)  {    try  {    return  (Throwable)  throwable.getClass().getMethod(methodName).invoke(throwable);    }  catch  (Exception  e)  {    return  null;    }    }  }  ‘‘‘</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Evaluation results of CodeFuse-QWen-14B on NLP benchmark datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 12: Comparisons of the performances of CodeFuse-QWen-14B, QWen-14B, and
    QWen-14b-chat on several NLP evaluation datasets. QWen-14B is a base model trained
    by Alibaba Cloud, QWen-14B-chat is a model fine-tuned by themselves on top of
    the QWen-14B base model, and CodeFuse-QWen-14B is a model fine-tuned by us using
    MFTCoder and code data.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | QWen-14B | QWen-14B-chat | CodeFuse-QWen-14B |'
  prefs: []
  type: TYPE_TB
- en: '| Language | AFQMC | 69.00% | 72.6% | 71.99% |'
  prefs: []
  type: TYPE_TB
- en: '| CHID | 84.7% | 72.3% | 84.42% |'
  prefs: []
  type: TYPE_TB
- en: '| Wic | 50.9% | 50.5% | 55.02% |'
  prefs: []
  type: TYPE_TB
- en: '| WSC | 66.3% | 66.3% | 68.27% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average | 67.73% | 65.43% | 69.93% |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning | COPA | 93% | 89% | 95.00% |'
  prefs: []
  type: TYPE_TB
- en: '| CMNLI | 62.1% | 60.3% | 55.33% |'
  prefs: []
  type: TYPE_TB
- en: '| OCNLI | 58.2% | 53.8% | 49.22% |'
  prefs: []
  type: TYPE_TB
- en: '| AX-b | 49.5% | 58.5% | 67.84% |'
  prefs: []
  type: TYPE_TB
- en: '| AX-g | 80.9% | 52.5% | 82.87% |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | 71.5% | 51.6% | 76.53% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average | 69.20% | 60.95% | 71.13% |'
  prefs: []
  type: TYPE_TB
- en: '| Understanding | CSL | 54.4% | 55.6% | 68.50% |'
  prefs: []
  type: TYPE_TB
- en: '| C3 | 90.8% | 91.7% | 91.01% |'
  prefs: []
  type: TYPE_TB
- en: '| EPRSTMT | 86.9% | 91.2% | 84.92% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average | 77.37% | 79.50% | 81.48% |'
  prefs: []
  type: TYPE_TB
- en: '| Examination | AX-b | 67.9% | 66.4% | 64.27% |'
  prefs: []
  type: TYPE_TB
- en: '| AX-g | 71.7% | 71.7% | 68.98% |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | 84.4% | 80.3% | 84.75% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average | 74.67% | 72.80% | 72.67% |'
  prefs: []
  type: TYPE_TB
