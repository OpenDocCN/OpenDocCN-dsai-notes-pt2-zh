- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01684](https://ar5iv.labs.arxiv.org/html/2402.01684)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chao Song
  prefs: []
  type: TYPE_NORMAL
- en: OPPO Research Institute
  prefs: []
  type: TYPE_NORMAL
- en: Shenzhen, Guangdong, China
  prefs: []
  type: TYPE_NORMAL
- en: songchao12@oppo.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Zhihao Ye'
  prefs: []
  type: TYPE_NORMAL
- en: OPPO Research Institute
  prefs: []
  type: TYPE_NORMAL
- en: Shenzhen, Guangdong, China
  prefs: []
  type: TYPE_NORMAL
- en: yezhihao3@oppo.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Qiqiang Lin'
  prefs: []
  type: TYPE_NORMAL
- en: OPPO Research Institute
  prefs: []
  type: TYPE_NORMAL
- en: Shenzhen, Guangdong, China
  prefs: []
  type: TYPE_NORMAL
- en: linqiqiang1@oppo.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Qiuying Peng'
  prefs: []
  type: TYPE_NORMAL
- en: OPPO Research Institute
  prefs: []
  type: TYPE_NORMAL
- en: Shenzhen, Guangdong, China
  prefs: []
  type: TYPE_NORMAL
- en: pengqiuying@oppo.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Jun Wang'
  prefs: []
  type: TYPE_NORMAL
- en: OPPO Research Institute
  prefs: []
  type: TYPE_NORMAL
- en: Shenzhen, Guangdong, China
  prefs: []
  type: TYPE_NORMAL
- en: wangjun7@oppo.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the productive evolution of large language models (LLMs) in the field
    of natural language processing (NLP), tons of effort has been made to effectively
    fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple
    specific domain (e.g., code interpreting, data analysis, medicine, and item recommendation).
    In practice, there are two prevailing ways, in which the adaptation can be achieved:
    (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently
    using the corresponding training samples from each task. (ii) An Integrated Model:
    Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. The
    first scheme, in most cases, leads to high computing cost since pre-trained LLMs
    need to undergo fine-tuning process numerous times separately. Although various
    parameter efficient fine-tuning (PEFT) methods are proposed and they indeed can
    alleviate the issue in a sense, high computing cost is still prohibitively and
    unacceptable. More seriously, the knowledge of each task stays isolated and it
    cannot be shared across different tasks efficiently, which may lead to the sub-optimal
    overall performance of fine-tuned LLMs. Regarding the second method, although
    different kinds of loss functions are ingeniously designed, which sufficiently
    consider especial situations of multi-task learning (MTL), a promotive consequence
    of one task frequently causes the degradation of other one or ones, called the
    seesawing issue. To address the high computing cost and seesawing issue simultaneously,
    we propose a unified framework that implements a $1+N$ mutli-task fine-tuning
    pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation
    (LoRA) algorithm. In detail, “1" stands for a central LLM offering common and
    extensive capabilities in general cases while “$N$" denotes N sets of CGC-LoRA
    modules that provides adaptive abilities to behave well on multiple clusters of
    unseen tasks, respectively. Our work aims to take an advantage of both MTL (i.e.,
    CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an
    innovative layer that contains two types of experts as additional trainable parameters
    to make LoRA be compatible with MTL. In specific, task-common experts intend to
    capture the message that should be shared across distinct tasks while service
    objects of task-specific experts are specific tasks. In addition, we also nominate
    a Task-motivated Gate (TMG) function that determines contributions of each expert
    to different tasks and this function desires to organize experts in an efficient
    way. To comprehensively evaluate the proposed framework, we conduct well-designed
    experiments on two public datasets including a wide range of multi-tasks: (i)
    Prompt Chinese Biomedical Language Understanding Evaluation (PromptCBLUE). (ii)
    Firefly. Both datasets involve over a thousand samples of each sub-task. The experimental
    results demonstrate that the unified framework with CGC-LoRA modules achieves
    higher evaluation scores than all benchmarks on both two datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords Multi-task Learning  $\cdot$ Parameter Efficient Fine-tuning  $\cdot$
    Large Language Model  $\cdot$ Customized Gate Control'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have made great progress recent two years [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)] and numerous versions of LLMs, namely, ChatGPT
    [[4](#bib.bib4), [5](#bib.bib5)], LlaMa [[6](#bib.bib6)], ERNIE [[7](#bib.bib7)],
    and ChatGLM [[8](#bib.bib8)], sprung up. Worldwide high-tech companies and non-profit
    research institutes are paying significant attention to the evolution of LLMs
    since they have already shown typically impressive performance on varied applications.
    For example, Wang and Thai in [[9](#bib.bib9), [10](#bib.bib10)] demonstrate that
    LLMs get outstanding achievements on document-level machine translation and meanwhile
    LLMs is proved by [[11](#bib.bib11), [12](#bib.bib12)] to be equipped with dramatic
    multilingual learning capability. Besides, LLMs present remarkable talent on multi-modal
    operation, such as precisely producing websites from handwritten text and determining
    humorous components within images [[13](#bib.bib13)]. As for recommendation system,
    LLMs also indicates a non-negligible potential on a variety of tasks, including
    sequential recommendation, rating prediction, explanation generation, review summary,
    direct recommendation [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)].
    Lastly but even more incredibly, Wei and Huang in [[17](#bib.bib17), [18](#bib.bib18)]
    announce that the reasoning ability of LLMs can even be elicited by a chain-of-thought
    (CoT) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of LLMs, especially for those extraodinarily authoritative ones
    like ChatGPT and LlaMa, are pre-trained on general purposes using universal data
    [[5](#bib.bib5), [4](#bib.bib4), [6](#bib.bib6)]. Although the few-shot learning
    and even zero-shot learning capability of LLMs can be stimulated via in-context
    learning [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)], the adaptation
    to a specific territory by fine-tuning is consistently favourable [[22](#bib.bib22),
    [23](#bib.bib23)]. For instance, Wang in [[22](#bib.bib22)] determines that fine-tuned
    large language model, named after Hua Tuo, outperforms the original LlaMa on Chinese
    medical knowledge. Additionally, CodeT5+ [[23](#bib.bib23)], a follow-on version
    of T5 fine-tuned on coding-related data, can achieve new SoTA results on the HumanEval,
    a specialized dataset on coding-related tasks [[24](#bib.bib24)], against other
    fundamental pre-trained LLMs including the basic version of T5 model. Therefore,
    in this work, we engage in the open-source LLMs and fine-tune them with task-specific
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'To a great extent, two inevitable dilemmas will occur when applying fine-tuning
    process to pre-trained LLMs. Firstly, a certain field, in most situations, involves
    a large numbers of heterogeneous tasks and consequently, a general fine-tuned
    LLMs cannot cover all tasks completely, called Various Task Impact [[2](#bib.bib2),
    [25](#bib.bib25)]. Considering the real-world advertisement recommendation area,
    it encompasses a wide range of assignments, like sequential recommendation, rating
    prediction, explanation generation, review summary, direct recommendation, etc.,
    and these diverse sub-tasks usually have objectives of similarity at different
    levels and even of conflict [[14](#bib.bib14), [16](#bib.bib16)]. To overcome
    this challenge, two feasible solutions are presented: (i) Multiple Independent
    Models: Fine-tuning an independent model for each particular task is practical,
    however, this strategy appeals considerable professional knowledge and a mass
    of labor. Also, information is precisely isolated and they cannot be shared across
    each task smoothly [[26](#bib.bib26), [27](#bib.bib27)]. What is more important,
    full parameters of a few fine-tuned LLMs for different tasks need to be saved
    that is unreasonable and even absurd, especially considering applications on mobile
    device due to the memory limitation. (ii) An Integrated Model: Fine-tuning an
    integrated model using samples from all tasks simultaneously can solve the information
    isolation and memory limitation problems mentioned above. As for this strategy,
    although knowledge is not segregated, at least in theory, the seesawing problem
    is frequently reported and it signifies that the enhancement of one task leads
    to the degradation of others [[26](#bib.bib26), [27](#bib.bib27)]. Secondly, High
    Computing Cost is another challenge. Since the LLMs having billions of parameters
    dominate nearly all areas, fine-tuning full parameters becomes extremely costly
    and utopian [[5](#bib.bib5), [7](#bib.bib7)]. Even if any companies or institutes
    can afford such astronomical computing cost, a limited amount of available data
    will ultimately cause over-fitting [[28](#bib.bib28)]. As a result, to have LLMs
    work excellently on applications of a wide variety, these two problems are urgently
    to be solved preferably.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the Various Task Impact, different kinds of multi-task learning
    (MTL) models have been proposed [[29](#bib.bib29), [30](#bib.bib30)], among which
    hard-parameter-sharing structure is designed primarily [[31](#bib.bib31)]. Although
    it, to some degree, promotes knowledge circulation among each task, negative transfer
    problem is exposed as parameters are straightforwardly and even wildly shared.
    To solve this issue, cross-stitch network [[32](#bib.bib32)] and sluice network
    [[33](#bib.bib33)] are announced by Misra and Ruder12, separately. Both networks
    can merge information from various tasks selectively as the combination is controlled
    linearly by a set of weights. Nonetheless, since weights stay the same for all
    samples, the seesawing phenomenon cannot be completely addressed. Next, the gate
    structure and both cross- and self-attention mechanisms are demonstrated to further
    solved the seesawing issue. Additionally, mixture-of-expert (MOE) related models
    [[34](#bib.bib34), [26](#bib.bib26)] first proposes to combine task-common experts
    by fusing the gate module and the attention module. In particular, both expert
    and attention modules are shared among all tasks while no task-specific module
    is scheduled. In contrast, Tang in [[27](#bib.bib27)] releases a customized gate
    control (CGC) module, which separates task-common and task-specific experts explicitly
    to keep clear of parameter disagreement caused by complicated correlation among
    tasks. Such an explicit design in virtue of professional prior knowledge is conducive
    to determine the convergence direction more accurately in practice [[35](#bib.bib35)].
    Besides the Various Task Impact problem, the High Computing Cost is another roadblock
    when MTL frameworks are applied to fine-tune LLMs since current frameworks are
    principally appropriate to full-parameter fine-tuning algorithms. Fortunately,
    a class of fine-tuning methods called parameter efficient fine-tuning (PEFT),
    which freeze parameters of pre-trained LLMs and instead only tune a delimited
    quantity of additional parameters, are explored to solve the high computing cost
    issue [[36](#bib.bib36), [37](#bib.bib37)]. Among proposed PEFT approaches, low-rank
    adaptation (LoRA) is a typical one and it suggests to just train a set of paired
    low-rank matrices to provide LLMs with adaptation capacity[[36](#bib.bib36)].
    However, making PEFT methods be compatible with MTL frameworks is still an open
    question [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the issues of Various Task Impact and High Computing Cost simultaneously
    by integrating MTL and PEFT, we proposed a universal framework to implement $1+N$
    multi-task fine-tuning pattern in LLMs. Specifically, a large range of tasks are
    divided into multiple clusters based on the professional prior knowledge or Inter-Task
    Affinity proposed by [[38](#bib.bib38)] and for a certain cluster, a central LLM
    is fine-tuned to adjust to these tasks by LoRA of a multi-task edition. This central
    LLM with sets of plug-in LoRA modules serves as the ultimate model that can handle
    diverse tasks from different fields with ease. As an essential component of the
    whole framework, we further propose an innovative unified LoRA and CGC structure,
    name after CGC-LoRA, inspired by MOE-LoRA [[25](#bib.bib25)] and it provides the
    framework with an effective and efficient fine-tuning performance in MTL situations.
    In detail, thanks to the explicit separation of task-common and task-specific
    experts, CGC is proved to have a practically preferred convergence behavior [[27](#bib.bib27)]
    and we demonstrate that such a valuable character can be smoothly transferred
    to the CGC-LoRA architecture. As the supplement to the basic edition of LoRA where
    a singular set of paired low-rank matrices are fine-tuned, CGC-LoRA incorporates
    multiple sets of paired low-rank matrices that stand for task-common and task-shared
    experts. Moreover, to keep the counts of trainable parameters the same, the summation
    of rank of task-common and task-specific matrices is equal to the rank of the
    original LoRA scheme. Furthermore, to advance the fusion capacity of experts,
    a task motivated gate function is proposed to adaptively determine the contribution
    of those two types of experts to each task only based on the task ID of the input
    sample. Eventually, the contributions of this work are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We announce a framework to implement $1+N$ multi-task fine-tuning pattern in
    LLMs, which can equip general pre-trained LLMs with adaptation to a large range
    of unseen tasks. It is worthy noting that this is a universal framework and it
    can be compatible with LoRA of any multi-task editions (e.g., MOE-LoRA [[25](#bib.bib25)],
    LoRAHub [[39](#bib.bib39)], and proposed CGC-LoRA).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We associate the power of both CGC structure and LoRA scheme by designing an
    ingenious unified MTL PEFT architecture. Besides, to lubricate the cooperation
    between task-common and task-specific experts, a task motivated gate function
    is proposed to regulate the weights of all experts and completely, determine the
    distinct contributions of experts to each task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We conduct comprehensive experiments on two public multi-task datasets: (i)
    the Prompt Chinese Biomedical Language Understanding Evaluation (PromptCBLUE)
    dataset [[40](#bib.bib40), [41](#bib.bib41)]; (ii) the Firefly dataset [[42](#bib.bib42)].
    The experimental results demonstrate the superiority of the proposed framework
    over other long-tested baselines. To the best of our knowledge, this paper represents
    the first effort to evaluate the dominance of the framework equipped with a MTL-PEFT-unified
    structure for LLM-driven models on vast of applications using multiple public
    datasets.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, a compact introduction to how LLMs can be applied to different
    kinds of applications in various domains is represented. First, an overview of
    the proposed framework that is to implement $1+N$ multi-task fine-tuning pattern
    in LLM is offered in Section [2.1](#S2.SS1 "2.1 An Overview of the Framework ‣
    2 Preliminary ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in
    LLMs Using The CGC-LoRA Algorithm") and next, we describe the details of input
    pre-processing along with those of output post-processing in Section [2.2](#S2.SS2
    "2.2 Input and Output Modification ‣ 2 Preliminary ‣ A Framework to Implement
    1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fd1cfdbb3828d3f14908fed3286bdea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of the proposed framework to implement $1+N$ multi-task
    fine-tuning pattern in LLMs. The first step is to divide a variety of tasks into
    $N$ clusters based on professional prior information or Inter-Task Affinity [[38](#bib.bib38)]
    and the second step is to adapt a central LLM to each cluster of tasks using the
    LoRA fine-tuning algorithm of multi-task editions (e.g., MOE-LoRA, LoRAHub, and
    CGC-LoRA).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 An Overview of the Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we present an overview of the proposed framework. It introduces
    how the framework implements $1+N$ multi-task fine-tuning pattern in LLMs to adapt
    a central LLM to clusters of tasks using the LoRA fine-tuning algorithm of multi-task
    editions (e.g., MOE-LoRA, LoRAHub, and CGC-LoRA). The entire framework can be
    divided into two primary parts: (i) Tasks clustering: As depicted in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Preliminary ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"), a LLM-driven model may face with
    a variety of tasks from different domains (e.g., code interpreting, data analysis,
    medicine, and item recommendation). Among them, tasks in certain fields highly
    have strong similarity, like code interpreting and data analysis while tasks in
    other fields, such as data analysis and item recommendation, almost have no relations
    and even conflicts. According to the assumptions in MTL, information sharing among
    related tasks can, to some degree, relieve the negative effect from lack of data
    and also benefit the effectiveness and efficiency of model training process and
    vice versa. As a consequence, tasks are grouped into $N$ clusters at the first
    stage. (ii) Adaptation to multiple clusters of tasks: Following tasks clustering,
    the LoRA fine-tuning algorithm of multi-task editions is applied to each cluster
    and finally a central LLM with $N$ sets of multi-task LoRA modules serves as an
    integrated model that can offer service to various tasks from all domains by switching
    to the corresponding module. Since the fine-tuning process for each cluster is
    repetitive, we set $N$ to be 1 in the remaining part for convenience and also
    without loss of generality.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0a6f5e961c73e7b6d93d2965a100c13a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A name entity recognition (NER) example to illustrate the whole pipeline
    of applications by large language models (LLMs). It includes the pre-processing
    to a conventional input, the textual answer of a LLM, the post-processing to extract
    final results.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Input and Output Modification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Except for multi-modal LLMs, the input and output of LLMs should be in text
    pattern [[14](#bib.bib14), [40](#bib.bib40), [41](#bib.bib41)], which leads to
    a typically different prototype from traditional models. As a consequence, there
    is a prerequisite to customize the input/output format to be compatible with LLMs.
    As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 An Overview of the Framework ‣ 2
    Preliminary ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs
    Using The CGC-LoRA Algorithm"), Name Entity Recognition (NER) task [[43](#bib.bib43)],
    as a concrete example, suggests the details of the reformulation of input and
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input Modification. Besides the initial textual input, auxiliary instruction
    templates that can guide LLMs in accomplishing the given task should be attached.
    In particular, as characterized in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 An Overview
    of the Framework ‣ 2 Preliminary ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"), the input is formatted as: “Please
    recognize the name entity in the following sentence: [Text]”, represented by $I_{modified}$,
    where [Text] acts as a placeholder for the original input, denoted as $I_{original}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output Modification. Outputs of traditional models in natural language processing
    (NLP) are commonly at word-level [[44](#bib.bib44), [45](#bib.bib45)] and as for
    LLMs, we integrate target entities $\{e_{1},...,e_{N_{e}}\}$ into linguistic texts.
    As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 An Overview of the Framework ‣ 2
    Preliminary ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs
    Using The CGC-LoRA Algorithm"), the output of NER task can be formatted as following:
    “The text has the following entities: $e_{1},...,e_{N_{e}}$”, represented by $O_{sentence}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the whole paradigm can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: With the transformation of input and output, the pre-trained LLMs, such as LlaMA
    [[6](#bib.bib6)], ChatGLM [[8](#bib.bib8)], etc., can be straightforwardly fed
    by purely lingual data. After fine-tuning, LLMs can routinely generate answers
    by following the pipeline formulated as Equation ([1](#S2.E1 "In 2.2 Input and
    Output Modification ‣ 2 Preliminary ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm")).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As described in Section [2.1](#S2.SS1 "2.1 An Overview of the Framework ‣ 2
    Preliminary ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs
    Using The CGC-LoRA Algorithm"), sorts of territories often envelop a variety of
    tasks in practice, for example, (i) Item Recommendation: Sequential recommendation,
    rating prediction, explanation generation, etc. (ii) Medical Application: Name
    entity recognition, attribute extraction, report generation, etc. (iii) Text Generation:
    Lyric generation, composition generation, poetry generation, etc. The purpose
    of this work is to fine-tune a pre-trained central LLM to meet the requirements
    from multiple domains, each of which contains numerous sub-tasks. Considering
    that the numbers of clusters has been set to $N=1$ for convenience, a cluster
    of tasks are denoted as $\mathbb{T}=\{\mathcal{T}_{1},...,\mathcal{T}_{i},...,\mathcal{T}_{N_{T}}\}$
    while the input and output data of $\mathcal{T}_{i}$ can be described as $\mathcal{I}_{i}=\{I^{\mathcal{T}_{i}}_{k}\}^{N_{\mathcal{T}_{i}}}_{k=1}$
    and $\mathcal{O}_{i}=\{O^{\mathcal{T}_{i}}_{k}\}^{N_{\mathcal{T}_{i}}}_{k=1}$,
    respectively. In specific, $N_{\mathcal{T}_{i}}$ stands for the numbers of paired
    data (i.e., linguistic input and output data) of $\mathcal{T}_{i}$. To be brief,
    the superscript $\mathcal{T}_{i}$ will be left out in the following essay. In
    detailed, the problem of multi-task fine-tuning can be formulated as: Given the
    paired input and output data of all tasks $\{I_{i},O_{i}\}^{N_{T}}_{i=1}$, the
    target is to optimize the parameters $\Theta$ of a pre-trained LLM to achieve
    the shining behavior among all tasks of distinguished similarity and it is noteworthy
    that $\Theta$ alternately stands for full parameters or selective ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the objective when fine-tuning the pre-trained LLMs, we straightforwardly
    apply a common one in the field of natural language processing (i.e., the conditional
    language modeling objective) after normalizing input and output data of all tasks
    into a steady linguistic format as detailed in Section [2.2](#S2.SS2 "2.2 Input
    and Output Modification ‣ 2 Preliminary ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"). Besides, in order
    to stimulate the information sharing across each task, all training samples are
    fed into the LLMs simultaneously and as a consequence, the objective function
    of LLMs fine-tuning in a multi-task case can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 4 CGC-LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, a comprehensive illustration of CGC-LoRA structure is represented.
    In Section [4.1](#S4.SS1 "4.1 Overview ‣ 4 CGC-LoRA ‣ A Framework to Implement
    1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"), a description
    over the whole fine-tuning process is primarily conducted and as follows, Section
    [4.2](#S4.SS2 "4.2 CGC-LoRA ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm") characterizes how CGC-LoRA
    takes advantage of both CGC and LoRA scheme in detail. Subsequently, a prominent
    component of CGC-LoRA network (i.e., a task-motivated gate function), which controls
    the contribution of both task-common and task-specific experts to each task is
    illustrated in Section [4.3](#S4.SS3 "4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm"). Finally, an elaborate definition of training and inference
    pipeline is given in Section [4.4](#S4.SS4 "4.4 Training and Inference ‣ 4 CGC-LoRA
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm").
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned, parameter efficient fine-tuning (PEFT) algorithm provides an
    innovative way, in which LLMs can be fine-tuned productively and efficiently.
    More concretely, only parameters of additional paired low-rank matrices are trainable
    while primordial parameters of pre-trained LLMs stay frozen. The extra parameters
    offer updated knowledge to dense layers. Inspired by MOE-LoRA [[25](#bib.bib25)],
    our approach blends CGC network with LoRA strategy and the combination is attached
    to Query module, Key module, Value module, and dense layers in the Transformer
    architecture as displayed in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Overview ‣ 4 CGC-LoRA
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm"). Furthermore, the proposed CGC-LoRA structure not only inherits
    the main advantage of PEFT algorithm (i.e., productiveness and efficiency) but
    also extends to solve the task variety problem and seesawing issue in multi-task
    learning (MTL) cases. Specifically, each CGC-LoRA layer incorporates two types
    of experts: (i) Task-common experts: They aim to capture conjunct knowledge across
    various tasks. (ii) Task-specific experts: Their responsibility is to extract
    specific information of each task. More details is depicted and elaborated in
    Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Overview ‣ 4 CGC-LoRA ‣ A Framework to Implement
    1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm") and
    Section [4.2](#S4.SS2 "4.2 CGC-LoRA ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N
    Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"), separately.
    Additionally, as an unimpressive but significant component of CGC-LoRA layer,
    a task-motivated gate module determines the contribution weights of two types
    of experts (i.e., task-common experts and task-specific experts) and more precisely,
    the outcome of each task-motivated gate only depends on task ID of each task rather
    than any other features of input samples. In Section [4.3](#S4.SS3 "4.3 Task-motivated
    Gate Function ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"), more details of how task-motivated
    gate modules are integrated into a CGC-LoRA layer can be found. Lastly, another
    implementation detail is worth noting that gate functions of all CGC-LoRA layers
    can be shared or independent, which, in a sense, influences the fitting ability
    of fine-tuned LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/761d710d426919abd5e347e5efd7a592.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The detailed architecture of the proposed CGC-LoRA scheme. Specifically,
    the parameters of modules in blue are frozen while those of components in green,
    yellow, red are trainable during the fine-tuning process.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 CGC-LoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thanks to the characteristic of both efficiency and effectiveness, Low-rank
    Adaptation (LoRA) gradually becomes a mainstream and prominent fine-tuning algorithm
    on Transformer-based networks [[36](#bib.bib36)]. Since the efficiency of our
    framework comes from the essential theorem of LoRA, we first briefly review its
    principle that can make the comprehension of the proposed CGC-LoRA be simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for LoRA, a low-rank decomposition is applied to the parameters fine-tuning
    procedure in LLMs and more precisely, full-rank matrices that traditionally stands
    for full parameters are decomposed into paired low-rank matrices as the equation
    $\mathbf{W}_{0}+\Delta\mathbf{W}=\mathbf{W}_{0}+\mathbf{BA}$. It is worthy noting
    that $\mathbf{W}_{0}\in\mathbb{R}^{d_{in}\times d_{out}}$ denotes the parameter
    matrices of a pre-trained central LLMs while $\Delta\mathbf{W}\in\mathbb{R}^{d_{in}\times
    d_{out}}$ represents the additional matrices, which offers adaptation capability
    to pre-trained LLMs. In addition, $\mathbf{B}\in\mathbb{R}^{d_{in}\times r}$ and
    $\mathbf{A}\in\mathbb{R}^{r\times d_{in}}$ stands for a pair of decomposed trainable
    low-rank matrices. Built upon these notations, an abstract expression of a dense
    layer paired with the LoRA scheme can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathbf{h}&amp;=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\Delta\mathbf{W}\mathbf{x}\\
    &amp;=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\mathbf{B}\mathbf{A}\mathbf{x}\end{split}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}$ and $\mathbf{h}$ stand for the input and output vector of
    dimension $d_{in}$ and $d_{out}$, respectively. Moreover, the numbers of additional
    trainable parameters is determined by $r$, the rank of decomposed low-rank matrices
    while $\alpha$ is designed to alleviate the impact caused by the variance of rank
    $r$. As emphasised, all parameters (e.g., $\mathbf{W}_{q},\mathbf{W}_{k},\mathbf{W}_{v}$
    and $\mathbf{W}$) of pre-trained LLMs stay frozen during the fine-tuning process
    as illustrated in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Overview ‣ 4 CGC-LoRA ‣ A
    Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA
    Algorithm"). Instead, the paired low-rank matrices, $\mathbf{A}$ and $\mathbf{B}$,
    are trained from scratch. As suggested by [[36](#bib.bib36)], $\mathbf{A}$ is
    initialized by a random Gaussian while $\mathbf{B}$ is set to be zero. Considering
    that $r\ll d_{in}$ and $r\ll d_{out}$, the total numbers of trainable parameters
    in $\mathbf{A}$ and $\mathbf{B}$ is typically fewer than that in $\mathbf{W}_{0}$,
    which is the principle mechanism stimulates the efficiency and effectiveness during
    the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although PEFT algorithms (i.e., LoRA) can solve the high computing cost issue
    because of the demand for full-parameter fine-tuning excellently, they are at
    a loss how to achieve a efficient knowledge circulation in MTL cases. As for LoRA,
    additional parameters are fine-tuned as a whole for all tasks by feeding all samples
    into the pre-trained LLMs simultaneously. To further optimize the fine-tuning
    algorithm, an ingenious plan, enlightened by the multi-expert structure, is to
    partition matrices $\mathbf{A}$ and $\mathbf{B}$ of rank $r$ into several segments
    $\{\mathbf{A}_{1},...,\mathbf{A}_{N}\}$ and $\{\mathbf{B}_{1},...,\mathbf{B}_{N}\}$,
    rank summation of which equals to $r$. In this work, we propose a innovative network
    layer, named by CGC-LoRA, which uses a Customized Gate Control (CGC) structure
    as reference. Particularly, CGC employs two types of expert networks (i.e., task-common
    and task-specific experts) to apprehend both shared and isolated knowledge of
    multi-task. Such a pattern can be integrated into a LoRA layer by a decomposition
    of matrices $\mathbf{A}$ and $\mathbf{B}$ and naturally, segments $\{\mathbf{A}_{1},...,\mathbf{A}_{N}\}$
    and $\{\mathbf{B}_{1},...,\mathbf{B}_{N}\}$ corresponds to each task-common and
    task-specific expert, denoted as $E^{C}\in\{E^{C}_{i}\}^{N_{C}}_{i=1}$ and $E^{S}\in\{E^{S}_{i}\}^{N_{S}}_{i=1}$,
    respectively. Here $N_{C}$ and $N_{S}$ stand for the numbers of task-common and
    task-specific expert, separately and the summation of $N_{C}$ and $N_{S}$ is equal
    to $N$, the total numbers of experts. Generally speaking, the numbers of task-specific
    experts is the same as that of tasks (i.e., $N_{C}=N_{T}$). During the CGC-LoRA
    fine-tuning process, additional parameters, serving as experts, are trained using
    data from all tasks and correspondingly, task-common and task-specific experts
    intrinsically capture shared and specific information. Given this architecture,
    a forward process of a CGC-LoRA layer for samples from task $\mathcal{T}_{j}$
    can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}\mathbf{h}_{j}&amp;=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\Delta\mathbf{W}_{j}\mathbf{x}_{j}\\
    &amp;=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot[w^{S}_{j}\cdot E^{S}_{j}(\mathbf{x}_{j})+\sum^{N_{C}}_{i=1}w^{C}_{ji}\cdot
    E^{C}_{i}(\mathbf{x}_{j})]\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot[w^{S}_{j}\cdot\mathbf{B}^{S}_{j}\mathbf{A}^{S}_{j}+\sum^{N_{C}}_{i=1}w^{C}_{ji}\cdot\mathbf{B}^{C}_{i}\mathbf{A}^{C}_{i}]\mathbf{x}_{j}\end{split}$$
    |  | (4) |'
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbf{h}_{j}$ and $\mathbf{x}_{j}$ stands for the input and output
    of intermediate LLM layers (e.g., a dense layer in a Transformer structure) for
    samples from $\mathcal{T}_{j}$, respectively. A pair of $\mathbf{B}^{S}_{j}\in\mathbb{R}^{d_{in}\times\frac{r}{N}}$
    and $\mathbf{A}^{S}_{j}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$ represents a
    task-specific expert for task $\mathcal{T}_{j}$ while $N_{C}$ pairs of $\mathbf{B}^{C}_{i}\in\mathbb{R}^{d_{in}\times\frac{r}{N}}$
    and $\mathbf{A}^{C}_{i}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$ denotes the
    task-common experts. For convenience, in Equation ([4](#S4.E4 "In 4.2 CGC-LoRA
    ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in
    LLMs Using The CGC-LoRA Algorithm")) the rank of $\mathbf{A}^{S}\in\{A^{S}\}^{N_{S}}_{j}$,
    $\mathbf{B}^{S}\in\{B^{S}\}^{N_{S}}_{j}$, $\mathbf{A}^{C}\in\{A^{C}\}^{N_{C}}_{i}$,
    and $\mathbf{B}^{C}\in\{B^{C}\}^{N_{C}}_{i}$ are all set to be $\frac{r}{N}$ equally.
    In a more general expression, the rank of $A^{S}_{j}$, $B^{S}_{j}$, $A^{C}_{i}$,
    and $B^{C}_{i}$ can be different as long as $rank(A^{S}_{j})=rank(B^{S}_{j})$
    and $rank(A^{C}_{i})=rank(B^{C}_{i})$ and also $\mathbf{h}_{j}$. Furthermore,
    the contribution weight of each expert are task-specific and in specific, weights
    are determined by our proposed task-motivated gate function, which will be specified
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: In this paragraph, we will compare the numbers of trainable parameters of basic
    LoRA and proposed CGC-LoRA. As for LoRA, trainable parameters are fully included
    in paired low-rank matrices $\mathbf{B}\in\mathbb{R}^{d_{in}\times r}$ and $\mathbf{A}\in\mathbb{R}^{r\times
    d_{out}}$ and consequently, the total counts of trainable parameters in LoRA can
    be calculated as $d_{in}\times r+r\times d_{out}=r\times(d_{in}+d_{out})$. Comparably,
    suppose the numbers of task-common and task-specific experts equals to $N_{C}$
    and $N_{S}$, respectively. Totally, the trainable parameters of all experts can
    be expressed as $N_{C}\times\frac{r}{N}\times(d_{in}+d_{out})+N_{S}\times\frac{r}{N}\times(d_{in}+d_{out})=\frac{N_{C}+N_{S}}{N}\times
    r\times(d_{in}+d_{out})=r\times(d_{in}+d_{out})$ considering $N_{C}+N_{S}=N$.
    As emphasized above, the equation illustrates that the limit of rank $r$ is divided
    into each expert evenly while the conclusion that CGC-LoRA has the same counts
    of trainable parameters as LoRA holds constant even if the division is uneven.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Task-motivated Gate Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we elaborate how weights of those two types of experts are
    determined by the proposed task-motivated gate function. As previously mentioned,
    the information learned by task-common experts should be shared across each task
    while task-specific experts need to focus on a given task. In practice, the contributions
    of those two kinds of experts to a certain task is determined by weights $w^{S}_{j}$
    and $w^{C}_{ji}$, which correspond to task-specific and task-common experts, separately.
    To achieve this, a task embedding matrix, denoted as $\mathbf{E}\in\mathbb{R}^{|\mathbb{T}|\times
    d_{T}}$, aims to represent task IDs in a hyper-dimensional space and here $d_{T}$
    stands for the dimension of the task embedding. For a certain task $\mathcal{T}_{j}$,
    the j-th column of $\mathbf{E}$ is extracted, which serves as the embedding vector
    for that task, denoted as $\mathbf{e}_{j}\in\mathbb{R}^{d_{T}}$. As for the weights
    of task-common experts (i.e., $\mathbf{w}^{C}_{j}=[w^{C}_{j1},...,w^{C}_{ji},...,w^{C}_{jN_{C}}]^{T}$),
    the formulation can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{w}^{C}_{j}=\mathbf{W}^{C}_{T}\mathbf{e}_{j}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{W}^{C}_{T}\in\mathbb{R}^{N_{C}\times d_{T}}$ stands for a transformation
    matrix that can complete a linear transformation of task embedding of a certain
    task-common expert. Similarly, considering the weights of task-specific experts,
    since each expert corresponds to its own task and has none contribution to the
    other tasks, the weight $w^{S}_{j}$ can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w^{S}_{j}=\mathbf{W}^{S}_{T}\mathbf{e}_{j}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{W}^{S}_{T}\in\mathbb{R}^{1\times d_{T}}$ denotes another transformation
    vector that applies a linear transformation of task embedding of a given task-specific
    expert. Finally, we apply a $\mathrm{Softmax}$ operation after the linear transformation
    to normalize the weights of both task-common and task-specific experts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{w}_{j}=\mathrm{softmax}([\mathbf{w}^{C}_{j},w^{S}_{j}])$ |  |
    (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{w}_{j}\in\mathbb{R}^{N_{C}+1}$ stands for the weights of both
    task-common and task-specific experts after normalization. In particular, the
    first $N_{C}$ elements denote weights of task-common experts while the last one
    represents weight of task-specific one. Besides, it is worthy noting that the
    linear transformation matrices (i.e., $\mathbf{W}^{C}_{T}$ and $\mathbf{W}^{S}_{T}$)
    can be various for different layers and it can increase the ability of representation
    and on the other hand, it is likely to cause over-parameterization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding a conventional gate function as in CGC [[27](#bib.bib27)], the input
    vector $x_{j}$ serves as the variable to the gate function and consequently, the
    output of the gate function (i.e., contribution weights of experts) varies among
    samples. Be contrast to that design, the proposed task-motivated one is fed by
    task IDs instead of the input vector $x_{j}$, which offers an admirable feature
    of being invariant to the input sample. Also thanks to another characteristic
    of linear combination among two types of experts, the corresponding parameters
    learned for a certain task $\mathcal{T}_{j}$ can be expressed as a linear process:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}\mathbf{W}_{j}&amp;=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\Delta\mathbf{W}_{j}\\
    &amp;=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot[w^{S}_{j}\cdot E^{S}_{j}+\sum^{N_{C}}_{i=1}w^{C}_{ji}\cdot
    E^{C}_{i}]\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot[w_{j}\cdot\mathbf{B}^{S}_{j}\mathbf{A}^{S}_{j}+\sum^{N_{C}}_{i=1}w^{C}_{ji}\cdot\mathbf{B}^{C}_{i}\mathbf{A}^{C}_{i}]\end{split}$$
    |  | (8) |'
  prefs: []
  type: TYPE_NORMAL
- en: According to Equation ([8](#S4.E8 "In 4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm")), the final parameters of a CGC-LoRA layer of a given task
    $\mathcal{T}_{j}$ (i.e., $\mathbf{W}_{j}$) is only determined by task ID $j$,
    which signifies that a unique set of parameters can be reclaimed for all samples
    from the task $\mathcal{T}_{j}$. In contrast, if the input vector $x_{j}$ also
    makes an effect on the gate function, the weight matrices $\mathbf{W}_{j}$ would
    alter across samples from even the same task. Such a tiny distinction definitely
    leads to high inference latency since distinct samples, even those from the same
    task $\mathcal{T}_{j}$, have their own unique parameters $\mathbf{W}_{j}$ and
    parameters need to be calculated repetitively. Considering our task-motivated
    gate function, the parameters of a given task $\mathcal{T}_{j}$ stay static for
    samples belonging to that task and during the inference process, all samples from
    a certain task can be conducted as a batch. Following such an operation, the latency
    caused by reasoning the parameters can be significantly decreased by $\frac{N_{T}}{N_{sample}}$
    in theory. Here, $N_{sample}$ denotes the total numbers of inference samples.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Training and Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we elaborate the details of the training and inference process
    of the proposed CGC-LoRA structure. Algorithm [1](#alg1 "Algorithm 1 ‣ 4.4 Training
    and Inference ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm") summarizes an entire pipeline and
    anyone can replicate the training and inference process by following those steps.
  prefs: []
  type: TYPE_NORMAL
- en: Training. Firstly, the pre-trained central LLM and the layers that demand fine-tuning
    using CGC-LoRA algotithm should be defined (line 1). Next, multiple hyper-parameters,
    such as rank value $r$, scale value $\alpha$, the numbers of task-common experts,
    etc., should be determined in advance (line 2-5). During the fine-tuning procedure,
    full parameters of the pre-trained LLM stay frozen (line 6) and as suggested in
    [[25](#bib.bib25)], a batch of training samples are randomly sampled without replacement
    from various tasks iteratively (line 7). Moreover, we behave the forward process
    and calculate the loss function based on batches of training samples (line 8-9).
    Finally, the parameters of CGC-LoRA module and task-motivated gate function (i.e.,
    the parameters of CGC-LoRA $\{\mathbf{A}^{S}_{i},\mathbf{B}^{S}_{i}\}^{N_{S}}_{i=1}$
    and $\{\mathbf{A}^{C}_{i},\mathbf{B}^{C}_{i}\}^{N_{C}}_{i=1}$ and the parameters
    of task motivated gate function $\mathbf{E},\mathbf{W}^{C}_{T}$, and $\mathbf{W}^{S}_{T}$)
    are updated according to the backpropagation mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Inference. After being fine-tuned, the updated parameters corresponding to each
    task $\mathcal{T}_{j}$ can be retrieved by following Equation ([8](#S4.E8 "In
    4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm")). Once the fine-tuned
    parameter matrices of each task are recovered by steps described in line 12-15,
    we can apply the corresponding parameters for inference, given a specific task
    $\mathcal{T}_{j}$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 The training and inference process of CGC-LoRA
  prefs: []
  type: TYPE_NORMAL
- en: '1:Specify the pre-trained central LLM and the layers that demand CGC-LoRA fine-tuning.2:Specify
    the rank value $r$ and scale value $\alpha$.3:Specify the numbers of task-common
    experts $N_{C}$ of CGC-LoRA. (Note: The numbers of task-specific experts $N_{S}$
    is set to $N_{S}=N_{T}$)4:Specify the rank value $r_{i}$ of each expert. Note:
    (i) By default, $r_{i}=\frac{r}{N}$. (ii) $\sum^{N}_{i=1}r_{i}=r$.5:Specify if
    the transformation matrices $\mathbf{W^{C}_{T}}$ and $\mathbf{W^{S}_{T}}$ stay
    the same for different layers: Yes or No.6:7:Freeze all parameters in the pre-trained
    LLM, e.g., $\mathbf{W}_{q}$, $\mathbf{W}_{k}$, $\mathbf{W}_{v}$, and $\mathbf{W}$.8:for a
    batch of samples $\mathcal{S}$ in $\mathcal{D}$ do9:     Execute forward process
    for LLM guided by CGC-LoRA scheme following Equation ([4](#S4.E4 "In 4.2 CGC-LoRA
    ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in
    LLMs Using The CGC-LoRA Algorithm")).10:     Calculate the loss function by Equation
    ([2](#S3.E2 "In 3 Problem Formulation ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm")).11:     Update the
    parameters of CGC-LoRA $\{\mathbf{A}^{S}_{i},\mathbf{B}^{S}_{i}\}^{N_{S}}_{i=1}$
    and $\{\mathbf{A}^{C}_{i},\mathbf{B}^{C}_{i}\}^{N_{C}}_{i=1}$ and the parameters
    of task-motivated gate function $\mathbf{E},\mathbf{W}^{C}_{T}$, and $\mathbf{W}^{S}_{T}$12:end for13:14:for $\mathcal{T}_{j}$
    in $\mathbb{T}$ do15:     Calculate the contribution weights $\mathbf{w}^{C}_{j}$
    for task-common experts and $w^{S}_{j}$ for task-common experts following Equation
    ([5](#S4.E5 "In 4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA ‣ A Framework to
    Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"))
    and Equation ([6](#S4.E6 "In 4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA ‣ A
    Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA
    Algorithm")), respectively and compute the normalized weights $\mathbf{w}_{j}$
    by Equation ([7](#S4.E7 "In 4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA ‣ A
    Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA
    Algorithm")).16:     Retrieve the parameters of the fine-tuned LLM with CGC-LoRA
    scheme by Equation ([8](#S4.E8 "In 4.3 Task-motivated Gate Function ‣ 4 CGC-LoRA
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm")) for each task $j$.17:end for18:Given a specific task $\mathcal{T}_{j}$,
    apply the corresponding parameters of the fine-tuned LLM for inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To comprehensively prove the effectiveness of the proposed CGC-LoRA structure,
    we conduct well-designed experiments on two public multi-task datasets: (i) Prompt
    Chinese Biomedical Language Understanding Evaluation (PromptCBLUE) dataset [[40](#bib.bib40),
    [41](#bib.bib41)]. (ii) Firefly dataset [[42](#bib.bib42)]. In Section [5.1](#S5.SS1
    "5.1 Dataset ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"), an elaborate introduction to PromptCBLUE
    and Firefly Dataset is presented and a detailed description of baselines is given
    in Section [5.2](#S5.SS2 "5.2 Baselines ‣ 5 Experiments ‣ A Framework to Implement
    1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"). Next,
    the implementation details, such as the settings of software, hardware and hyper-parameters,
    are illustrated in Section [5.3](#S5.SS3 "5.3 Implementation Details ‣ 5 Experiments
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm"). Moreover, Section [5.4](#S5.SS4 "5.4 Evaluation Metrics
    ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern
    in LLMs Using The CGC-LoRA Algorithm") and Section [5.5](#S5.SS5 "5.5 Overall
    Performance ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm") amplify the evaluation indexes
    and the corresponding results of different methods on each task of those two public
    datasets, respectively. In addition, the conclusion of an ablation study on each
    component of CGC-LoRA is made in Section [5.6](#S5.SS6 "5.6 Ablation Study ‣ 5
    Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs
    Using The CGC-LoRA Algorithm") while the effects of hyper-parameter selection
    are debated in Section [5.7](#S5.SS7 "5.7 Hyper-parameter Analysis ‣ 5 Experiments
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To the best of our knowledge, Prompt Chinese Biomedical Language Understanding
    Evaluation dataset [[40](#bib.bib40), [41](#bib.bib41)] and Firefly dataset [[42](#bib.bib42)]
    are two most thorough multi-task datasets in Chinese, both of which include numerous
    tasks of noticeable diversity, such as entity recognition, text generation, text
    classification, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'PromptCBLUE dataset: It is derived from Chinese Biomedical Languange Understanding
    Evaluation (CBLUE) dataset by transforming all samples into a pure text format
    as described in Section [2.2](#S2.SS2 "2.2 Input and Output Modification ‣ 2 Preliminary
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm"). PromptCLUE is released by East China Normal University on
    Tianchi Competition Platform [[46](#bib.bib46)] and totally, it contains 16 medicine-related
    tasks, such as medical named entity recognition, diagnosis report generation,
    etc. In order to compare with MOE-LoRA [[25](#bib.bib25)] fairly and rationally,
    we reuse exactly the same training/validation/test datasets as MOE-LoRA and also
    follow its pre-processing procedure. The detailed statistics of datasets corresponding
    to 8 selected tasks are summarized in Table [I](#S5.T1 "Table I ‣ 5.1 Dataset
    ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern
    in LLMs Using The CGC-LoRA Algorithm").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table I: The brief task description and statistics of 8 selected sub-datasets
    from PromptCBLUE.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Description | # Train | # Validation | # Test |'
  prefs: []
  type: TYPE_TB
- en: '| CMeIE | Name Entity Recognition | 2828 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-CDN | Normalization | 2381 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-CDEE | Attribute Extraction | 1562 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-MDCFNPC | Clinic Entity Discovery | 4935 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-CTC | Medical Text Classification | 3622 | 1100 | 1100 |'
  prefs: []
  type: TYPE_TB
- en: '| KUAKE-QIC | Query Intention | 3279 | 660 | 660 |'
  prefs: []
  type: TYPE_TB
- en: '| IMCS-V2-MRG | Report Generation | 1799 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| MedDG | Doctor Dialogue | 4964 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: 'Firefly dataset: It is a comprehensive, public, multi-task dataset in Chinese,
    totally including over one million samples [[42](#bib.bib42)]. Specifically, it
    contains 23 general tasks, such as lyric generation, sentiment analysis, natural
    language inference, etc., from which we randomly select 8 tasks (Text Correction
    (TC), Summary, Keyword Recognition (KR), Text Matching (TM), Sentiment Analyze
    (SA), MRC, NER, and NLI) and considering the computing complexity, we fix the
    number of training, evaluation, and test samples of those 8 tasks at 1800, 300,
    and 300, respectively.To comprehensively evaluate the proposed framework in various
    situations, we do not transform samples into pure linguistic text format as PromptCBLUE
    and instead samples stay in an input-target format as displayed in Table [II](#S5.T2
    "Table II ‣ 5.1 Dataset ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table II: Examples from Firefly dataset on four tasks: Natural Language Inference,
    Name Entity Recognition, Sentiment Analysis, and Couplet.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Input | Target |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Language Inference | Text:soccer game with multiple males playing.
    Hypothesis:Some men are playing a sport. | Entailment |'
  prefs: []
  type: TYPE_TB
- en: '| Named Entity Recognition | Beijing is the capital of China. Please recognize
    all city entities in the sentence. | Beijing |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment Analysis | This book is really exciting. What is the sentiment
    of this comment, positive or negative? | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| Couplet | Monk. Please give the second line of a couplet: | Wukong |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the proposed framework powered by the CGC-LoRA module, we conduct
    a well-designed experiment to compare it with three types of baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM without fine-tuning: Regarding the baseline without fine-tuning, ChatGPT
    [[4](#bib.bib4), [5](#bib.bib5)] acts as the fundamental pre-trained LLM. In specific,
    to pilot ChatGPT to generate the output in a desired format, we apply In-Context
    Learning algorithm [[47](#bib.bib47)] by providing 3 to 10 randomly sampled examples
    as the task description.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM with LoRA: As a well-tested fine-tuning algorithm, LoRA is naturally compatible
    with LLMs due to its effectiveness and efficiency [[36](#bib.bib36)]. In multi-task
    learning (MTL) cases, two straightforward scenarios can be implemented with LoRA:
    (i) LoRA Full: All samples from disparate tasks are utilized simultaneously to
    fine-tune a pre-trained central LLM using LoRA. (ii) LoRA Single: a LLM with a
    unique set of LoRA modules is fine-tuned for each task using its corresponding
    samples.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM with multi-task LoRA: For this type of baselines, the basic pre-trained
    LLMs are fine-tuned by two recently released LoRA algorithms having MTL capability,
    separately. One is LoRAHub [[39](#bib.bib39)] that fine-tunes a distinct set of
    LoRA for each task respectively and combines all fine-tuned LLMs using the weight
    optimization via gradient-free methods. The other one is MOE-LoRA, which integrates
    mixture-of-expert (MOE) structure [[34](#bib.bib34)] into LoRA algorithm.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, it is worthy noting that ChatGLM-6B [[48](#bib.bib48)] is utilized
    in both LLM with LoRA and LLM with multi-task LoRA because of its dominance in
    Chinese text related competitions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The configuration details are presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Softare and Hardware: Software configuration is PyTorch 1.12.0 and Python 3.6.5
    while hardware configuration is Tesla A100 GPU.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic LLMs: As for approaches with fine-tuning (i.e., LLM with LoRA, LLM with
    multi-task LoRA, and proposed CGC-LoRA), ChatGLM-6B [[48](#bib.bib48)] serves
    as the basic LLM.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input/Output Length: The maximum length of input and output are set to 1024
    and 196, separately.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyper-parameters: Batch size and maximum training steps are configured to 64
    and 8000, respectively. Besides, the $\alpha$ value is fixed at 0.1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trainable Layers: Regarding all fine-tuned LLM, the trainable layers are limited
    within the self-attention (i.e., Query, Key, Value head) and linear layers of
    ChatGLM-6B. More details are shown as Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Overview
    ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in
    LLMs Using The CGC-LoRA Algorithm").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.4 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since both PromptCBLUE and Firefly contain a variety of tasks, different metrics
    are implemented to evaluate the performance of baselines and proposed CGC-LoRA
    on each task. As for PromptCBLUE dataset, CMeIE, CHIPCDN, CHIP-CDEE and CHIP-MDCFNPC
    evaluate the performance using Micro-F1 score and Macro-F1 score is calculated
    for CHIP-CTC and KUAKE-QIC. In addition, Rouge-L [[49](#bib.bib49)] is applied
    in text generation tasks, such as report generation and doctor dialogue. To assess
    the overall performance, the average score over all tasks is implemented. As for
    Firefly dataset, identical evaluation metrics are applied. In particular, Micro-F1
    score is used for Keyword Recognition, MRC, and NER while Macro-F1 score is calculated
    for Text Matching, NLI, and Sentiment Analyze. Also, for text generation tasks
    (e.g., Text Correction and Summary), Rouge-L serves as the evaluation metric.
    Similarly, the average score is used to evaluate the overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Overall Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [III](#S5.T3 "Table III ‣ 5.5 Overall Performance ‣ 5 Experiments ‣ A
    Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA
    Algorithm") and Table [IV](#S5.T4 "Table IV ‣ 5.5 Overall Performance ‣ 5 Experiments
    ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The
    CGC-LoRA Algorithm") declare the thorough experimental results of the proposed
    CGC-LoRA and baselines on PromptCBLUE dataset and Firefly dataset, respectively.
    According to the average scores across all tasks of two datasets, a substantial
    conclusion can be made that the proposed CGC-LoRA persistently surpasses all baselines
    and also achieves the most robust performance across a variety of tasks on two
    totally different datasets. More specific investigation to the experimental results
    is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLMs without fine-tuning: As a baseline without any fine-tuning, ChatGPT falls
    behind other approaches on both PromptCBLUE and Firefly datasets, which, in a
    large sense, underlines the value of fine-tuning procedure. More specifically,
    task-specific information can be, to a certain extent, injected into the pre-train
    LLMs through further fine-tuning, which finally has a positive effect on the comprehensive
    performance on specific tasks. In spite of the global deficiency, we still can
    observe that ChatGPT outperforms all other methods on Doctor Dialogue thanks to
    its excellent competence on dialogue comprehension and generation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM with LoRA: For LoRA strategy, we make an attempt to implement two straightforward
    schemes (i.e., LLM Full and LLM Single) to adapt it to multi-task learning (MTL).
    Between them, LLM Full is in the lead on nearly all tasks and also the overall
    performance on PromptCBLUE dataset while a completely opposite achievement is
    observed on Firefly dataset. We suspect that such a paradoxical conclusion is
    caused by variable similarity across distinctive tasks on those two datasets and
    this investigation typically demonstrates the significance and necessity of sharing
    knowledge across diverse tasks in a reasonable way that is also the inspiration
    to the proposed CGC-LoRA.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM with multi-task LoRA: As two types of LoRA strategies that have been proven
    to be compatible with MTL, LoRAHub and MOE-LoRA serves as two competitive benchmarks.
    According to the results displayed in Table [III](#S5.T3 "Table III ‣ 5.5 Overall
    Performance ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm") and Table [IV](#S5.T4 "Table IV
    ‣ 5.5 Overall Performance ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"), MOE-LoRA leads the
    other baselines by a large margin on PromptCBLUE dataset while LoRAHub is ahead
    of others except for LoRA Single on Firefly dataset. Similarly, the contradictory
    consequence that indicates the sensitivity of algorithms to a wide range of tasks
    can still be disclosed. What is more important, it further signifies the significance
    of the network architecture that can promote effective and efficient information
    sharing across distinct tasks in a robust way.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CGC-LoRA: Thanks to the task-specific and task-common experts, CGC-LoRA offers
    an outstanding and also stable achievement on 16 tasks across two datasets and
    these 16 tasks cover widespread applications in natural language processing. Specifically,
    it widely leads the other approaches on 10 out of 16 tasks and also stays in a
    pioneering position on overall performance, which exposes its effectiveness, efficiency
    and also robustness.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In summary, according to the experimental results displayed in Table [III](#S5.T3
    "Table III ‣ 5.5 Overall Performance ‣ 5 Experiments ‣ A Framework to Implement
    1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm") and
    Table [IV](#S5.T4 "Table IV ‣ 5.5 Overall Performance ‣ 5 Experiments ‣ A Framework
    to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"),
    the proposed CGC-LoRA demonstrates its preferable achievement over basic LoRA
    strategies (i.e., LoRA Full and LoRA Single) and multi-task LoRA schemes (i.e.,
    LoRAHub and MOE-LoRA) on these two public datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table III: The overall results of three kinds of baselines (i.e., LLM without
    fine-tuning, LLM with LoRA, and LLM with multi-task LoRA) and CGC-LoRA on PromptCBLUE
    dataset. The boldface stands for the highest score and the underline represents
    the best result of the baselines. “$\star$” marks the statistically significant
    improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 0.3058 | 0.6069 | 0.2838 | 0.5854 | 0.5253 | 0.4851 | 0.3253 |
    0.1361 | 0.4067 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA Full | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678
    | 0.1113 | 0.6155 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA Single | 0.4984 | 0.8882 | 0.5528 | 0.7765 | 0.8694 | 0.8524 | 0.3583
    | 0.1143 | 0.6138 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRAHub | 0.4411 | 0.8442 | 0.5041 | 0.7177 | 0.8564 | 0.8502 | 0.3061 |
    0.1192 | 0.5799 |'
  prefs: []
  type: TYPE_TB
- en: '| MOE-LoRA | 0.5193 | 0.8928 | 0.5697 | $\mathbf{0.7933}^{\star}$ | 0.8691
    | 0.8675 | 0.3681 | 0.1089 | 0.6236 |'
  prefs: []
  type: TYPE_TB
- en: '| CGC-LoRA | $\mathbf{0.5207}$ | $\mathbf{0.8948}$ | $\mathbf{0.5720}$ | 0.7822
    | 0.8509 | $\mathbf{0.8727}$ | $\mathbf{0.3808}^{\star}$ | 0.1184 | $\mathbf{0.6240}$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table IV: The overall results of three kinds of baselines (i.e., LLM without
    fine-tuning, LLM with LoRA, and LLM with multi-task LoRA) and CGC-LoRA on PromptCBLUE
    dataset. The boldface stands for the highest score and the underline represents
    the best result of the baselines. “$\star$” marks the statistically significant
    improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | TC | Summary | KR | TM | MRC | NER | NLI | SA | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 0.8799 | 0.2320 | 0.2275 | 0.4700 | 0.6142 | 0.2379 | 0.5408 |
    0.9593 | 0.5202 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA Full | 0.9057 | 0.2605 | 0.2740 | 0.7766 | 0.6584 | 0.5500 | 0.7233
    | 0.9866 | 0.6418 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA Single | 0.9174 | $\mathbf{0.2745}$ | $\mathbf{0.2744}$ | $\mathbf{0.7966}$
    | 0.7222 | 0.5807 | 0.7027 | 0.9866 | 0.6569 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRAHub | 0.9168 | 0.2681 | 0.2447 | 0.7733 | 0.7310 | 0.5889 | 0.7433 |
    0.9833 | 0.6562 |'
  prefs: []
  type: TYPE_TB
- en: '| MOE-LoRA | 0.9093 | 0.2599 | 0.2471 | 0.7833 | 0.7230 | 0.5683 | 0.7333 |
    0.9766 | 0.6501 |'
  prefs: []
  type: TYPE_TB
- en: '| CGC-LoRA | $\mathbf{0.9178}$ | 0.2681 | 0.2720 | 0.7866 | $\mathbf{0.7311}$
    | $\mathbf{0.6085}^{\star}$ | $\mathbf{0.7533}^{\star}$ | $\mathbf{0.9900}$ |
    $\mathbf{0.6659}^{\star}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table V: The experimental results of ablation study for CGC-LoRA. The boldface
    stands for the highest score. “$\star$” marks the statistically significant improvements
    (i.e., two-sided t-test with p < 0.05) over the other approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| w/o CGC | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678 |
    0.1113 | 0.6155 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o gate | 0.5015 | 0.8840 | 0.5378 | 0.7789 | 0.8818 | 0.8699 | 0.3709 |
    0.1140 | 0.6174 |'
  prefs: []
  type: TYPE_TB
- en: '| w multiple gates | 0.5246 | 0.8874 | 0.5523 | 0.7818 | 0.8300 | 0.8716 |
    0.3645 | 0.1205 | 0.6166 |'
  prefs: []
  type: TYPE_TB
- en: '| CGC-LoRA | 0.5207 | $\mathbf{0.8948}^{\star}$ | $\mathbf{0.5720}^{\star}$
    | 0.7822 | 0.8509 | $\mathbf{0.8727}$ | $\mathbf{0.3808}^{\star}$ | 0.1184 | $\mathbf{0.6240}^{\star}$
    |'
  prefs: []
  type: TYPE_TB
- en: 5.6 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To extensively investigate the impact of each component in proposed CGC-LoRA
    structure, we conduct a comprehensive ablation study on PromptCBLUE dataset and
    the results are exhibited in Table [V](#S5.T5 "Table V ‣ 5.5 Overall Performance
    ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern
    in LLMs Using The CGC-LoRA Algorithm").
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'w/o CGC: As a primary module of CGC-LoRA structure, it plays a compelling role
    on capturing and processing both task-common and task-specific knowledge. CGC-LoRA
    will naturally reverts to LoRA Full when excluding the CGC architecture. Comparing
    with CGC-LoRA, w/o CGC demonstrates an inferior performance on both task-specific
    evaluation and overall assessment, highlighting that CGC module critically contributes
    to the outstanding achievement of proposed CGC-LoRA.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'w/o gate: Task motivated gate function is responsible for calculating the contribution
    of task-common and task-specific experts to each task only based on task IDs.
    For the w/o gate variant, uniform expert weights, bypassing the gate function,
    are employed instead of expert-specific ones. As shown in Table [V](#S5.T5 "Table
    V ‣ 5.5 Overall Performance ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task
    Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"), CGC-LoRA exceeds the
    w/o gate variant on 7 out of 8 tasks, which proves the validness of task motivated
    gate module.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'w multiple gates: As mentioned in Section [4.3](#S4.SS3 "4.3 Task-motivated
    Gate Function ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"), motivated gate function can be
    unique for each CGC-LoRA layer and it is denoted as the w multiple gates variant.
    From Table [V](#S5.T5 "Table V ‣ 5.5 Overall Performance ‣ 5 Experiments ‣ A Framework
    to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using The CGC-LoRA Algorithm"),
    we can inspect that CGC-LoRA with multiple gates cannot attain comparable results
    to the one with single gate on different tasks, in a large sense, due to its over-parameterization
    and also considering a higher count of trainable parameters brought by multiple
    gate functions, the proposed CGC-LoRA is designed as a single gate setup.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The ablation study demonstrate the effectiveness and necessity of fundamental
    modules (i.e., CGC and task motivated gate function) of the proposed CGC-LoRA
    structure, as well as the significance of specific optimization pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Hyper-parameter Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To explain the primary hyper-parameter selection in our study, we investigate
    the influence of the task-common expert number $N_{C}$ while the impact of the
    task-specific expert number is not considered since it is usually equal to the
    counts of tasks in MTL. Besides, we also explore the effect of rank of each expert
    using both task specific evaluation metrics and the average evaluation score on
    PromptCBLUE dataset. As illustrated in Table [VI](#S5.T6 "Table VI ‣ 5.7 Hyper-parameter
    Analysis ‣ 5 Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"), we set the number of task-specific
    expert and the rank of each expert to 8 and 2, separately. As for the impact of
    numbers of the task-common expert on the average score, the curve is in a parabola
    shape and the peak occurs at numbers equal to 8 and this result clarifies that
    adequate task-common experts are prerequisite to completely capture and efficiently
    process the task-common knowledge across different tasks. In addition, when fixing
    the counts of both task-specific and task-common expert at 8, we probe the influence
    of rank of each expert. Similarly, the curve is also in a parabola shape the the
    optimal overall achievement happens at rank equal to 2 and detailed results are
    displayed in Table [VII](#S5.T7 "Table VII ‣ 5.7 Hyper-parameter Analysis ‣ 5
    Experiments ‣ A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs
    Using The CGC-LoRA Algorithm").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table VI: The experimental results of Hyper-parameter analysis for CGC-LoRA
    on PromptCBLUE dataset. The number of task-specific experts is fixed at 8, which
    is equal to the counts of tasks and the rank of each expert is set to 2\. The
    boldface stands for the highest score. “$\star$” marks the statistically significant
    improvements (i.e., two-sided t-test with p < 0.05) over the other hyper-parameter
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| # task-common experts | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC
    | KUAKE-QIC | IMCS-V2-MRG | MedDG | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.4958 | 0.8751 | 0.5532 | 0.7775 | 0.8400 | 0.8564 | 0.3608 | 0.1167
    | 0.6094 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.5137 | 0.8922 | 0.5599 | 0.7776 | 0.8309 | 0.8694 | 0.3593 | 0.1159
    | 0.6149 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | $\mathbf{0.5206}^{\star}$ | 0.8947 | $\mathbf{0.5720}^{\star}$ | 0.7822
    | 0.8509 | 0.8727 | $\mathbf{0.3807}^{\star}$ | 0.1184 | $\mathbf{0.6240}^{\star}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 0.4984 | 0.8882 | 0.5528 | 0.7765 | $\mathbf{0.8694}^{\star}$ | 0.8761
    | 0.3583 | 0.1143 | 0.6167 |'
  prefs: []
  type: TYPE_TB
- en: 'Table VII: The experimental results of Hyper-parameter analysis for CGC-LoRA
    on PromptCBLUE dataset. The number of task-specific and task-common experts are
    both fixed at 8 and the rank of each expert stay the same. The boldface stands
    for the highest score. “$\star$” marks the statistically significant improvements
    (i.e., two-sided t-test with p < 0.05) over the other hyper-parameter settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| rank | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.5136 | 0.8886 | 0.5625 | 0.7878 | 0.8463 | 0.7960 | 0.3609 | 0.1147
    | 0.6088 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | $\mathbf{0.5206}^{\star}$ | 0.8947 | 0.5720 | 0.7822 | 0.8509 | 0.8727
    | $\mathbf{0.3807}^{\star}$ | 0.1184 | 0.6240 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.5054 | 0.8826 | 0.5801 | 0.7772 | 0.8481 | 0.8669 | 0.3708 | 0.1146
    | 0.6182 |'
  prefs: []
  type: TYPE_TB
- en: 6 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will review two research areas that are directly related
    to our work: (i) Multi-task learning (MTL) and (ii) Parameter efficient fine-tuning
    (PEFT). In a large sense, the proposed CGC-LoRA structure intents to unify these
    two strategies in an innovative way.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Multi-Task Learning (MTL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hard parameter sharing [[31](#bib.bib31)] gives the first and most basic attempt
    to achieve MTL framework while the unfavorable information transfer caused by
    straightforward parameters sharing across tasks frequently appears. To solve such
    a negative transfer issue, cross-stitch network [[32](#bib.bib32)] and sluice
    network [[33](#bib.bib33)] are proposed successively, both of which consist of
    weights of linear combinations and aim to merge knowledge from distinct tasks
    discriminatorily. Unfortunately, the seesawing problem emerges since knowledge
    from various tasks is coarsely blended with static weights for all samples. To
    further enhance the learning efficiency in multi-task cases, several studies with
    the gate structure and attention mechanism are introduced and such a design intends
    to make message fusion from individual tasks more flexible and effective. For
    example, Mixture-of-Experts (MOE) model first presents an innovative architecture
    that task-common experts are shared at the bottom layer while information from
    these experts is fused through a gate function at the top layer [[34](#bib.bib34)].
    Next, Multi-Gate Mixture-of-Experts (MMOE) network extends the single-gate structure
    in MOE to the multiple-gate one, which can assign personalized fusing weights
    to different tasks [[26](#bib.bib26)]. Furthermore, Progressive Layered Extraction
    (PLE) architecture incorporates both task-common and task-specific experts into
    the model. More specifically, in order to alleviate task conflict and seesawing
    problems caused by complicated task correlations, parameters of task-common and
    task-specific experts are isolated in a explicit way, by which can make the convergence
    during optimization procedure more valid in practice. In this work, we aim to
    broaden the application scenarios of these impressive schemes to fine-tuning process
    of large language models (LLMs) to solve similar issues (e.g., the seesawing issue
    and the task conflict problem).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Parameter Efficient Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With a rapid development of large language models (LLMs), a trend of enlarging
    the counts of parameters to further ameliorate LLMs becomes unambiguous gradually
    and as a result, adapting the pre-trained LLMs to tasks in a brand-new field through
    a full-parameter fine-tuning procedure turns to be prohibitive and impractical.
    Parameter efficient fine-tuning (PEFT), as an alternative algorithm, aims to degrade
    the computing cost level by updating partial parameters instead of entire ones.
    As in [[23](#bib.bib23), [50](#bib.bib50)], a straightforward idea is to only
    update parameters of selected layers (e.g., the output embedding layer) and meanwhile
    freeze parameters of the other layers. Alternatively, we can also append one or
    multiple additional layers to the base model and only parameters of these additional
    layers are trainable. As a modified version of the latter one, Adapter Tuning
    [[51](#bib.bib51)] incorporates a lightweight adapter network with only a few
    trainable parameters into LLMs and it demonstrates competitive performance to
    fine-tuning the top layers. Considering Prefix-tuning [[52](#bib.bib52)] and P-tuning
    [[37](#bib.bib37)], one or several task-specific virtual token, serving as an
    additional, trainable, continuous prompt or embedding, is appended to the original
    input and compared with the discrete prompt, the continuous one is more feasible
    and is not restricted to discrete real tokens [[53](#bib.bib53)]. To further break
    the limit of sequence length of LLMs, LoRA propose another train of thought, which
    incorporates a pair of trainable low-rank matrices into each dense layer and it
    also demonstrates comparable performance to full-parameter fine-tuning [[36](#bib.bib36)].
    Two outstanding advantages of LoRA are: (i) Low computing cost during fine-tuning.
    (ii) High efficiency during inference. However, LoRA can only learn paired integral
    matrices for all tasks that is not an effective and efficient way to capture the
    task-common and task-specific knowledge across distinct tasks in MTL. In this
    work, we proposed a framework to implement $1+N$ multi-task fine-tuning pattern
    in LLMs powered by CGC-LoRA network and it takes advantage of both MTL (e.g.,
    MMOE and PLE) and PEFT (e.g., P-tunig and LoRA) and reveals its parameter efficient
    fine-tuning capability in MTL cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, we primarily take advantage of both parameter efficient fine-tuning
    (PEFT) approaches (i.e., LoRA algorithm) and multi-task learning (MTL) strategies
    (i.e., CGC network). In specific, we propose a general framework that implements
    $1+N$ multi-task fine-tuning pattern in LLMs. Firstly, a variety of tasks are
    grouped into $N$ clusters based on professional prior information or Inter-Task
    Affinity. Next, we release a novel multi-task PEFT method, named by CGC-LoRA,
    and a pre-trained central LLM is fine-tuned with $N$ one-to-one sets of CGC-LoRA
    modules on those clusters of tasks. Furthermore, a CGC-LoRA module consists of
    task-common and task-specific experts that can extract and process the common
    and specific knowledge across various tasks. Moreover, we also design a task-motivated
    gate function to decide the contributions of two types of experts to a given task.
    Since the gate function is only fed by task IDs, parameters of all experts can
    be represented in a unified way as illustrated in Section [4.3](#S4.SS3 "4.3 Task-motivated
    Gate Function ‣ 4 CGC-LoRA ‣ A Framework to Implement 1+N Multi-task Fine-tuning
    Pattern in LLMs Using The CGC-LoRA Algorithm"). Furthermore, inheriting from PEFT
    methods, the proposed CGC-LoRA offers an efficient way, in which a pre-trained
    LLM can be fine-tuned using a small counts of additional trainable parameters.
    To demonstrate the proposed framework powered by CGC-LoRA module, we conduct comprehensive
    experiments on two public datasets: (i) PromptCBLUE dataset: It is a medical specific
    dataset in Chinese; (ii) Firefly dataset: It is a general dataset including a
    variety of tasks also in Chinese. The experimental results definitely prove the
    effectiveness and efficiency of CGC-LoRA module. In the future, we plan to further
    explore the feasibility of implementing the proposed framework in MTL case of
    even higher diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Wayne Xin Zhao, Kun Zhou, et al. A survey of large language models. arXiv
    preprint arXiv:2303.18223, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Yupeng Chang, Xu Wang, et al. A survey on evaluation of large language
    models. arXiv preprint arXiv:2307.03109, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Humza Naveed, Asad Ullah Khan, et al. A comprehensive overview of large
    language models. arXiv preprint arXiv:2307.06435, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Yiheng Liu, Tianle Han, et al. Summary of chatgpt-related research and
    perspective towards the future of large language models. Meta-Radiology, page
    100017, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ouyang Long, Jeffrey, et al. Training language models to follow instructions
    with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Hugo Touvron, Thibaut Lavril, et al. Llmama: Open and efficient foundation
    language models. arXiv preprint arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yu Sun, Shuohuan Wang, et al. Ernie 3.0: Large-scale knowledge enhanced
    pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Aohan Zeng, Xiao Liu, et al. Glm-130b: An open bilingual pre-trained model.
    arXiv preprint arXiv:2210.02414, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Longyue Wang, Chenyang Lyu, et al. Document-level machine translation with
    large language models. arXiv preprint arXiv:2304.02210, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Katherine Thai, Marzena Karpinska, et al. Exploring document-level literary
    machine translation with parallel paragraphs from world literature. arXiv preprint
    arXiv:2210.14250, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Yue Deng, Wenxuan Zhang, et al. Multilingual jailbreak challenges in large
    language models. arXiv preprint arXiv:2310.06474, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Viet Dac Lai, Nghia Trung Ngo, et al. Chatgpt beyond english: Towards
    a comprehensive evaluation of large language models in multilingual learning.
    arXiv preprint arXiv:2304.05613, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Deyao Zhu, Jun Chen, et al. Minigpt-4: Enhancing vision-language understanding
    with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Shijie Geng, Shuchang Liu, et al. Recommendation as language processing
    (rlp): A unified pretrain, personalized prompt and predict paradigm (p5). In Proceedings
    of the 16th ACM Conference on Recommender Systems, pages 299–315, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Likang Wu, Zhi Zheng, et al. A survey on large language models for recommendation.
    arXiv preprint arXiv:2305.19860, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Junling Liu, Chao Liu, et al. Is chatgpt a good recommender? a preliminary
    study. arXiv preprint arXiv:2304.10149, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Jason Wei, Xuezhi Wang, et al. Chain-of-thought prompting elicits reasoning
    in large language models. Advances in Neural Information Processing Systems, 35:24824–24837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language
    models: A survey. arXiv preprint arXiv:2212.10403, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Monica Agrawal, Stefan Hegselmann, et al. Large language models are few-shot
    clinical information extractors. In Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 1998–2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Stefan Hegselmann, Alejandro Buendia, et al. Tabllm: Few-shot classification
    of tabular data with large language models. In International Conference on Artificial
    Intelligence and Statistics, pages 5549–5581, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Gautier Izacard, Patrick Lewis, et al. Atlas: Few-shot learning with retrieval
    augmented language models. arXiv preprint arXiv:2208.03299, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Haochun Wang, Chi Liu, et al. Huatuo: Tuning llama model with chinese
    medical knowledge. arXiv preprint arXiv:2304.06975, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yue Wang, Hung Le, et al. Codet5+: Open code large language models for
    code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Mark Chen, Jerry Tworek, et al. Evaluating large language models trained
    on code. arXiv preprint arXiv:2107.03374, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Qidong Liu, Xian Wu, et al. Moelora: An moe-based parameter efficient
    fine-tuning method for multi-task medical applications. arXiv preprint arXiv:2310.18339,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jiaqi Ma, Zhe Zhao, et al. Modeling task relationships in multi-task learning
    with multi-gate mixture-of-experts. In Proceedings of The 24th ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, pages 1930–1939, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Hongyan Tang, Junning Liu, et al. Progressive layered extraction (ple):
    A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings
    of the 14th ACM Conference on Recommender Systems, pages 269–278, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Swaroop Mishra, Daniel Khashabi, et al. Cross-task generalization via
    natural language crowdsourcing instructions. ACL, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Yu Zhang and Qiang Yang. An overview of multi-task learning. National
    Science Review, 5(1):30–43, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Kim-Han Thung and Chong-Yaw Wee. A brief review on multi-task learning.
    Multimedia Tools and Applications, 77:29705–29725, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Ishan Misra, Abhinav Shrivastava, et al. Cross-stitch networks for multi-task
    learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 3994–4003, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Sebastian Ruder12, Joachim Bingel, et al. Sluice networks: Learning what
    to share between loosely related tasks. stat, 1050:23, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Robert A. Jacobs, Michael I. Jordan, et al. Adaptive mixtures of local
    experts. Neural computation, 3(1):79–87, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Shikun Liu, Edward Johns, et al. End-to-end multi-task learning with attention.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1871–1880, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Edward J. Hu, Yelong Shen, et al. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Xiao Liu, Kaixuan Ji, et al. P-tuning: Prompt tuning can be comparable
    to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics, 2:61–68, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Christopher Fifty, Ehsan Amid, et al. Efficiently identifying task groupings
    for multi-task learning. arXiv preprint arXiv:2109.04617, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Chengsong Huang, Qian Liu, et al. Lorahub: Efficient cross-task generalization
    via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Hongying Zan, Wenxin Li, et al. Building a Pediatric Medical Corpus: Word
    Segmentation and Named Entity Annotation. Chinese Lexical Semantics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Wei Zhu, Xiaoling Wang, et al. Promptcblue: A chinese prompt tuning benchmark
    for the medical domain. arXiv preprint arXiv:2310.14151, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jianxin Yang. Firefly(流萤): 中文对话式大语言模型. [https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Vikas Yadav and Steven Bethard. A survey on recent advances in named entity
    recognition from deep learning models. arXiv preprint arXiv:1910.11470, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Jacob Devlin, Ming-Wei Chang, et al. Bert: Pre-training of deep bidirectional
    transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yifan Peng, Shankai Yan, et al. Transfer learning in biomedical natural
    language processing: An evaluation of bert and elmo on ten benchmarking datasets.
    arXiv preprint arXiv:1906.05474, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] AlibabaCloud. Tianchi competition platform, 2023. [https://tianchi.aliyun.com/?spm=a2c22.27124976.J_3941670930.7.71de132aU2uCjD](https://tianchi.aliyun.com/?spm=a2c22.27124976.J_3941670930.7.71de132aU2uCjD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Qingxiu Dong, Lei Li, et al. A survey on in-context learning. arXiv preprint
    arXiv:2301.00234, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Zhengxiao Du, Yujie Qian, et al. Glm: General language model pretraining
    with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Chin-Yew Lin and Franz J. Och. Automatic evaluation of machine translation
    quality using longest common subsequence and skip-bigram statistics. In Proceedings
    of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04),
    pages 605–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Robert Tinn, Hao Cheng, et al. Fine-tuning large neural language models
    for biomedical natural language processing. Patterns, 4(4), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Neil Houlsby, Andrei Giugiu, et al. Parameter-efficient transfer learning
    for nlp. in International Conference on Machine Learning, pages 2790–2799, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Xiang Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation. in Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing, 1:4582–4597, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Pengfei Liu, Weizhe Yuan, et al. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. ACM Computing Surveys,
    55(9):1–35, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
