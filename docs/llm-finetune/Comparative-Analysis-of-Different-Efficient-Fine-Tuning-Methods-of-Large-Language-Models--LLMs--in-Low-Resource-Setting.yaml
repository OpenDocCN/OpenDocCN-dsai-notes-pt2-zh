- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:36:43'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13181](https://ar5iv.labs.arxiv.org/html/2405.13181)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Krishna Prasad Varadarajan Srinivasan
  prefs: []
  type: TYPE_NORMAL
- en: Georgia Institute of Technology
  prefs: []
  type: TYPE_NORMAL
- en: kvarada@gatech.edu
  prefs: []
  type: TYPE_NORMAL
- en: '& Prasanth Gumpena'
  prefs: []
  type: TYPE_NORMAL
- en: Georgia Institute of Technology
  prefs: []
  type: TYPE_NORMAL
- en: pgumpena3@gatech.edu
  prefs: []
  type: TYPE_NORMAL
- en: '& Madhusudhana Yattapu'
  prefs: []
  type: TYPE_NORMAL
- en: Georgia Institute of Technology
  prefs: []
  type: TYPE_NORMAL
- en: myattapu3@gatech.edu
  prefs: []
  type: TYPE_NORMAL
- en: '& Vishal H. Brahmbhatt'
  prefs: []
  type: TYPE_NORMAL
- en: Georgia Institute of Technology
  prefs: []
  type: TYPE_NORMAL
- en: vbrahmbhatt3@gatech.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the domain of large language models (LLMs), Mosbach et al. ([2023](#bib.bib1))
    showed that few-shot full-model fine-tuning – namely Vanilla Fine Tuning (FT)
    and Pattern-Based Fine Tuning (PBFT) –, and In-Context Learning (ICL) generalize
    similarly on Out-Of-Domain (OOD) datasets, but vary in terms of task adaptation.
    However, they both pose challenges, especially in term of memory requirements.
    In this paper, we further try to push the understanding of different fine-tuning
    strategies for LLM and aim to bring a myriad of these on the same pedestal for
    an elaborate comparison with full-model fine-tuning on two diverse datasets. To
    that end, we conducted a series of experiments, beginning with state-of-the-art
    methods like vanilla fine-tuning and Pattern-Based Fine-Tuning (PBFT) on pre-trained
    models across two datasets, COLA and MNLI. We then investigate adaptive fine-tuning
    and the efficiency of LoRA adapters in a few-shot setting. Finally, we also compare
    an alternative approach that has gained recent popularity – context distillation
    – with the vanilla FT and PBFT with and without few-shot setup.
  prefs: []
  type: TYPE_NORMAL
- en: Our findings suggest that these alternative strategies that we explored can
    exhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.
    PBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the need
    for effective prompts. Further, our adaptive-fine tuning and LoRA experiments
    perform comparable or slightly worse than the standard fine-tunings as anticipated,
    since standard fine-tunings involve tuning the entire model. Finally, our context
    distillation experiments out-perform the standard fine-tuning methods. These findings
    underscore that eventually the choice of an appropriate fine-tuning method depends
    on the available resources (memory, compute, data) and task adaptability.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of our work is to evaluate and compare the performance of a pre-trained
    large language model on sequence classification tasks. We aimed to do this by
    employing – 1) different fine tuning (FT) methods, 2) applying Low-Rank Adaptation
    - LoRA (Hu et al. ([2021](#bib.bib2))) adaptors with few-shot learning, and 3)
    performing context-distillation both with and without few-shot learning setting.
    We aim to understand the efficacy of alternative fine-tuning methods on a pre-trained
    large language model’s performance in sequence classification tasks using 2 datasets,
    namely [MNLI](https://paperswithcode.com/dataset/multinli) (Williams et al. ([2018](#bib.bib3)))
    and [COLA](https://nyu-mll.github.io/CoLA/) (Warstadt et al. ([2018](#bib.bib4))),
    which are further explained in Section [1.1](#S1.SS1 "1.1 Datasets ‣ 1 Introduction
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting"). We explored alternate ways of efficiently
    fine-tuning the model and compared them with the baseline methods (vanilla and
    pattern-based fine-tuning) for Open Pre-trained Transformer (OPT) (Zhang et al.
    ([2022](#bib.bib5))) model’s performance on the text sequence classification task
    using both in-domain and out of domain accuracies. We try to keep the training
    process, experiments, and hyper-parameters similar across various experiments,
    wherever possible, for a fair comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Currently large language models (LLMs) are pre-dominantly used by leveraging
    In-Context Learning - ICL (Brown et al. ([2020](#bib.bib6))), whereby during the
    inference time, the model learns to answer follow-up questions from a series of
    prompts. This approach requires significant inference time memory and compute.
    In recent times, bunch of alternate methods have been proposed and explored to
    augment the issues faced with ICL. We explore and analyze a number of these methods
    in our work presented here.
  prefs: []
  type: TYPE_NORMAL
- en: Our work is relevant to anyone leveraging large language models for tasks like
    chat bots and code completion. If successful, our methods could improve the efficiency
    and performance of these models, particularly in tasks requiring the ingestion
    of large sequences of dialogue, code, or text.
  prefs: []
  type: TYPE_NORMAL
- en: We expand on all of these methods in section [2](#S2 "2 Approach ‣ Comparative
    Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs)
    in Low-Resource Setting") of this paper. We also further expand on the 2 datasets
    that we have performed our experiments on, in the section [1.1](#S1.SS1 "1.1 Datasets
    ‣ 1 Introduction ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting").
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1.1.1 MNLI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-Genre Natural Language Inference - MNLI (Williams et al. ([2018](#bib.bib3)))
    is a crowd-sourced collection of sentence pairs with textual entailment annotations.
    This is one of the largest corpora available for natural language inference (NLI)
    with ten distinct genres of written and spoken English. This paper used a subset
    of 261,802 samples accessed via GLUE (Wang et al. ([2019](#bib.bib7))) for training
    and entire dataset for in-domain-performance. Note that the labels were binarized
    to only include entailment and contradiction. The most important aspects of the
    dataset are suitability for NLI task and variety of samples belonging to different
    genres. An example of premise, hypothesis, and label are -
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise: How do you know? All this is their information again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis: This information belongs to them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: 0 (entailment)'
  prefs: []
  type: TYPE_NORMAL
- en: HANS dataset (McCoy et al. ([2019](#bib.bib8))) with 30,000 samples, was used
    for out-of-domain performance. It was chosen to evaluate performance when training
    on the MNLI dataset using different methods. This dataset includes examples that
    challenge conventional MNLI patterns by failing three syntactic heuristics - lexical
    overlap, subsequence, and constituent. These examples test how well models handle
    misleading or incorrect heuristic cues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise: The senators supported the actor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis: The actor supported the senators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Label: 1 (positive)'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 COLA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Corpus of Linguistic Acceptability (COLA) (Warstadt et al. ([2018](#bib.bib4))),
    developed by researchers at New York University (NYU) and Meta’s AI research lab,
    encompasses 10,657 English sentences from 23 linguistic publications. It provides
    annotations on sentence grammaticality, with 9,594 sentences for training and
    1,063 for testing. These annotations are provided by the authors of the sources.
    COLA aims to uniquely explore neural networks’ capacity to understand grammatical
    nuances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset uses labels for grammatical acceptability: 0 - unacceptable, 1
    - acceptable. For instance:  Sentence: This paper was written in Overleaf.  Label:
    1 (acceptable)'
  prefs: []
  type: TYPE_NORMAL
- en: Among the 23 sources, 17 are deemed in-domain, while the remaining 6 are out-of-domain
    (OOD). Access to the in-domain data was via the GLUE framework (Wang et al. ([2019](#bib.bib7))),
    and the OOD data from (Inc. ([2023](#bib.bib9))) GitHub repository, allowing for
    consistent analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We aimed to enhance sequence classification performance of large language models.
    Using fine-tuning methods like vanilla and Pattern-Based Fine Tuning (PBFT), we
    established baselines on Facebook’s OPT 125M and OPT 350M models in a few-shot
    setting. Limited by time and resources, we focused solely on these models. Our
    baseline experiments, which replicated (Mosbach et al. ([2023](#bib.bib1))), results
    for Vanilla and PBFT methods, used both datasets mentioned in Section [1.1](#S1.SS1
    "1.1 Datasets ‣ 1 Introduction ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting").
  prefs: []
  type: TYPE_NORMAL
- en: We broadened our experiments to include Context Distillation, as per Anthropic’s
    discussion in Askell et al. ([2021](#bib.bib10)). We experimented this fine-tuning
    in both few-shot and standard scenarios, with performance being comparable. We
    also tested Parameter-Efficient Fine-Tuning (PEFT) enhanced with Low-Rank Adaptation
    - LoRA (Hu et al. ([2021](#bib.bib2))) and Adaptive Fine-Tuning. Our findings
    indicate that while all methods perform similarly, they can be unstable and under-perform
    due to training instability. Performance generally improves with model size, and
    context distillation often excels in generalization. A comprehensive analysis
    is provided in Section [6](#S6 "6 Experiments and Results ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting").
  prefs: []
  type: TYPE_NORMAL
- en: Our approach, a unique mix of traditional and new methods, shows potential.
    While similar methods may have been individually explored, we focus on their benefits
    when applied separately and the synergy when few-shot learning is combined with
    LoRA adapter or context distillation. We aim to thoroughly analyze their performance
    in sequence classification tasks, hoping our exploration offers valuable insights
    to the field.
  prefs: []
  type: TYPE_NORMAL
- en: We foresaw challenges with computational resources and time, leading us to focus
    on the OPT 125M and OPT 350M models. We also expected performance differences
    between in-domain and out-of-domain accuracies. A main challenge was ensuring
    fair comparison across methods, especially as context distillation had significantly
    more training examples on the full set than in a few-shot setting.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Few-Shot Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our experiments, we used few-shot learning to design models that learn from
    a small number of examples, using knowledge from related tasks. We conducted experiments
    with varying numbers of examples (N = 2, 16, 32, 64, and 128), fine-tuning and
    evaluating models in each setting. This process, repeated across all settings,
    allowed us to observe the impact of example quantity on model performance. To
    ensure robust results, we performed 10 runs per model size per N, accounting for
    data seed variability.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Infrastructure and Replicating the Codebase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The experiments were conducted using Google Colab Pro, equipped with Nvidia
    Tesla T4 and L4 GPUs. The entire codebase for our work is hosted on GitHub and
    can be accessed at [https://github.com/iamvarada/llm_finetuning](https://github.com/iamvarada/llm_finetuning)
    ¹¹1Please contact the authors for any access issues. Researchers interested in
    replicating or extending our work are encouraged to clone or fork this repository.
    To ensure consistency and ease of setup, all necessary Python dependencies are
    listed in the requirements.txt file within the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Fine-Tuning Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Vanilla Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vanilla fine-tuning was performed on two models, OPT 125M and OPT 350M, instantiated
    as SequenceClassification with dual classification heads for binary labels. The
    models were directly fed with the ‘premise’ and ‘hypothesis’, and respective labels
    for MNLI and COLA datasets, without any prompts. The training loop, focused on
    few-shot learning as explained in Section [2.1](#S2.SS1 "2.1 Few-Shot Learning
    ‣ 2 Approach ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting"),
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pattern-Based Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pattern-Based Fine-Tuning (PBFT) is an advanced approach that uses language
    patterns to guide fine-tuning. Unlike vanilla fine-tuning, PBFT uses specific
    prompts or patterns to guide model predictions. We use GPT-3 derived patterns
    to transform the ‘premise’ and ‘hypothesis’ into a more learnable format. The
    model is then fine-tuned on these transformed inputs. PBFT, particularly with
    GPT-3 patterns, leverages inductive biases in prompts for effective learning and
    better task performance, especially useful in few-shot learning scenarios with
    limited data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Adaptive Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adaptive fine-tuning enables nuanced training using two techniques: Freezing
    Layers and Dynamic Learning Rate. The initial layers of the OPT 125M model, which
    hold general-purpose knowledge, are frozen to avoid overwriting during fine-tuning.
    A dynamic learning rate scheduler is used, applying a higher rate to the final
    task-specific layers and a lower rate to the frozen layers, aiding model adaptation
    while retaining pre-trained knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Parameter-Efficient Modeling using Low-Rank Adaptation (LoRA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We further explored Parameter Efficient Fine-Tuning (Houlsby et al. ([2019](#bib.bib11)))
    for fine-tuning our pre-trained OPT 125M model on COLA dataset by employing the
    Low-Rank Adaptation (LoRA) adapter (Hu et al. ([2021](#bib.bib2))). In this method
    of fine-tuning a model, we break down the weight matrix that is learned during
    the gradient descent step of backpropagation into two, smaller-rank matrices,
    thereby significantly reducing the number of parameters that we optimize for during
    our training process. This is depicted in Figure [1](#S4.F1 "Figure 1 ‣ 4 Parameter-Efficient
    Modeling using Low-Rank Adaptation (LoRA) ‣ Comparative Analysis of Different
    Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting"). If say, in a regular training process, we learn a matrix $\Delta W$
    x $m$, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W\xleftarrow{}W+\Delta W$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: ', with LoRA, we decompose the $\Delta W$ and $B$ x $r$ x $m$, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\Delta W=A.B,\\ \ni r\ll m\end{split}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: ', where $r$, rank of the matrices, is a hyper-parameter of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: In the domain of LLMs, decomposing non-trivial weight matrices in each layer
    with r = 2, 4, 6, …, significantly reduces the parameters to optimize, depending
    on the chosen rank. This decomposition is an approximation that aims for a balance
    between model accuracy and resource utilization during fine-tuning. In LoRA, the
    adapter layer output is multiplied by a factor $\alpha$ (another hyper-paramter),
    which determines the impact of the LoRA decomposition on the model layer to which
    the adapter is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/74595c7f4336990707b8ef6062bb111c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Weight update step (right) with and (left) without with LoRA adapter,
    Figure courtesy: Raschka ([2024](#bib.bib12))'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Context Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our approach, training uses two primary loss functions: distillation and
    classification loss. Distillation loss, computed using Kullback-Leibler divergence
    (Csiszar ([1975](#bib.bib13))), is a measure that quantifies the difference between
    any two probability distributions. More specifically, in our case, the KL divergence
    is calculated between the log-softmax of the student model’s outputs and the probability
    distribution of the teacher model, with the results averaged across the batch.
    Classification loss, determined using cross-entropy loss function, assesses the
    discrepancy between the student model’s predictions and the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We integrate these losses into training by formulating the overall loss as
    a weighted sum of both, with each loss component assigned a task-dependent weight.
    This balance is represented by the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Loss}=0.5\times\text{Distillation Loss}+0.5\times\text{Classification
    Loss}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: This approach allows the student model to learn from hard labels and mimic the
    teacher model’s probabilistic output, fostering nuanced understanding and potential
    improved generalization on unseen data (Askell et al. ([2021](#bib.bib10))).
  prefs: []
  type: TYPE_NORMAL
- en: We also assessed context distillation with few-shot learning setup from section
    [2.1](#S2.SS1 "2.1 Few-Shot Learning ‣ 2 Approach ‣ Comparative Analysis of Different
    Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting"), where the student model was fine-tuned using limited data. Subsets
    of representative samples were used to fine-tune the student model, and its performance
    was evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experiments and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section examines the effectiveness of fine-tuning techniques on two OPT
    models: OPT 125M and OPT 350M. We cover Vanilla FT, PBFT, PEFT with LoRA adapters,
    Adaptive FT, and Context Distillation (with and without few-shot learning). These
    experiments helped us understand the model’s adaptability to sequence classification
    and methods to improve its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Hyper-Parameters and Experiment Set-Up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We initiated this empirical study by evaluating the impact of various hyper-parameters
    in our experiments. To compare the myriad techniques we developed against the
    benchmark fairly, we aligned our fine-tuning parameters with those in Mosbach
    et al. ([2023](#bib.bib1)). We used few-shot sample sizes of 2, 16, 32, 64, and
    128 examples per class to reflect different levels of information availability.
    Our training spanned 40 epochs with a batch size of 32 to balance data exposure
    and computational efficiency. We conducted 10 trials per run to remove bias, selecting
    samples randomly from the training set. The learning rate was set at $1e^{-5}$
    and employed the AdamW optimizer (Loshchilov and Hutter ([2019](#bib.bib14))),
    a typical choice for LLM optimization. A consolidated table of the hyper-parameters
    used in our experiments can be found in Appendix [A](#A1 "Appendix A Appendix:
    Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Vanilla FT, PBFT and Adaptive FT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.2.1 Vanilla FT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our first experiment, we assessed the impact of vanilla fine-tuning on the
    CoLA and MNLI datasets using OPT 125M and OPT 350M models. The left plot in Figure
    [2](#S6.F2 "Figure 2 ‣ 6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT
    ‣ 6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting") shows
    that for the CoLA dataset, both models exhibited increased accuracy with larger
    sample sizes, with the OPT 125M model peaking at N=128\. OPT 350M model having
    nearly 3 times the parameters of the OPT 125M showed less pronounced effect of
    data size on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9c89fa4f407e08b106d35c7bb7e5d93.png)![Refer to caption](img/7ff127b0601457140c4185c9ad405677.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Vanilla FT accuracy for OPT 125M and OPT 350M on (left) COLA and
    (right) MNLI for different samples'
  prefs: []
  type: TYPE_NORMAL
- en: The right plot in Figure [2](#S6.F2 "Figure 2 ‣ 6.2.1 Vanilla FT ‣ 6.2 Vanilla
    FT, PBFT and Adaptive FT ‣ 6 Experiments and Results ‣ Comparative Analysis of
    Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting") presents a similar case for the MNLI data set. The only caveat was that
    models’ out-of-domain accuracy remained relatively stagnant across varying sample
    sizes. The larger OPT 350M model showed moderate enhancements in in-domain accuracy
    as the sample size grew, with minimal fluctuations in out-of-domain accuracy.
    This was expected because as detailed in Section [1.1](#S1.SS1 "1.1 Datasets ‣
    1 Introduction ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting"), MNLI dataset uses the
    HANS dataset as OOD data while the COLA dataset uses a subset of its own sources.
    The OPT 125M model consistently outperformed the OPT 350M model in in-domain accuracy
    across all few-shot learning sizes. This is due to the higher complexity of OPT
    350M which did not allow better generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 PBFT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the COLA dataset, as we can see in the left plot of Figure [3](#S6.F3 "Figure
    3 ‣ 6.2.2 PBFT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting"), OPT 125M model displayed better out-of-domain
    accuracy at the smallest sample size (N=2), while the OPT 350M model showed better
    in-domain performance at mid-range sample sizes (N=16, 64). At N=128, the OPT
    350M model slightly surpassed the 125M model in out-of-domain accuracy, demonstrating
    its potential for higher performance gains at larger few-shot sizes. These findings
    highlight the complex effects of PBFT on model accuracy, suggesting larger few-shot
    sizes may be preferable for optimal performance in linguistic acceptability tasks.
    We used GPT-3 style prompts for our patterns, suggesting that improved prompting
    could further enhance our models’ performance. However, the study of prompts’
    effect on model accuracy is beyond the scope of this work.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ec0558405ad979fda2122793d86949e.png)![Refer to caption](img/0e1b36e34f3321927d06489328c83eef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: PBFT accuracy for OPT 125M and OPT 350M on (left) COLA and (right)
    MNLI for different samples'
  prefs: []
  type: TYPE_NORMAL
- en: For the MNLI dataset, OPT 125M model consistently outperformed the OPT 350M
    model in in-domain accuracy at higher sample sizes, while out-of-domain accuracies
    remained relatively stable across both models, with slight improvements noted
    at the largest sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Adaptive FT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We performed adaptive tuning on the OPT 125M model by freezing the entire model
    and tuning only the last two decoder layers (11th and 12th). We used a dynamic
    learning rate strategy. This approach aimed to assess the impact of targeted layer
    freezing and responsive learning rate adjustments on model performance. Our results
    showed progressive improvement in both in-domain and out-of-domain accuracies
    as sample size increased, peaking at N=128 as seen in Figure [4](#S6.F4 "Figure
    4 ‣ 6.2.3 Adaptive FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and
    Results ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large
    Language Models (LLMs) in Low-Resource Setting"). These findings underscore the
    effectiveness of this approach in boosting performance and enhancing generalization
    capabilities. However, compared to full-model fine-tuning, [6.2.1](#S6.SS2.SSS1
    "6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting") and [6.2.2](#S6.SS2.SSS2 "6.2.2 PBFT ‣
    6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results ‣ Comparative
    Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs)
    in Low-Resource Setting"), the accuracies obtained in adaptive fine-tuning were
    lower, as expected, since we were only tuning the last few layers of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88629c3ba898dc3565096cc9c670680e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Adaptive FT accuracy for OPT 125M on COLA for different few-shot
    samples'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Effect of using parameter-efficient modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We anticipated similar performance between parameter-efficient fine-tuning (PEFT),
    vanilla, and pattern-based fine-tuning. Experiments were conducted for different
    matrix ranks, adapting all model layers with LoRA adapters. The LoRA configuration
    was applied to training but not inference. After experimenting with different
    $\alpha$ and LoRA dropout values, we settled on 8 and 0.1 respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [5](#S6.F5 "Figure 5 ‣ 6.3 Effect of using parameter-efficient modeling
    ‣ 6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting") – The
    plot compares in-domain and out-of-domain accuracies on the COLA dataset for different
    example sizes, showing an upward trend for N=2 to N=64, highlighting the benefits
    of more training data. However, accuracy decreases at N=128, a trend differing
    from other tuning methods used in this study. This suggests that the matrix decomposition
    approximation might impact the adaptability of the original model for the inferred
    task. Given the OPT model’s design for sequence generation and its use for classification
    here, techniques like LoRA and few-shot, which add approximation to backpropagation
    and reduce the training set, might not be as effective. We refer user to Appendix
    [B](#A2 "Appendix B Appendix: Plots for LoRA for different ranks ‣ Comparative
    Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs)
    in Low-Resource Setting") for the graphs for individual ranks.'
  prefs: []
  type: TYPE_NORMAL
- en: These results illuminate the nuanced balance required in parameter-efficient
    fine-tuning strategies to enhance model performance while maintaining robust generalization
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76bd4d9e8018b857bd6b163d18e5c61d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Accuracy on COLA for various few shot samples averaged across ranks
    of LoRA layers'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Effect of using context distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We expected two outcomes from context distillation. Firstly, the distilled models
    would likely show better performance due to acquired knowledge. Secondly, the
    few-shot distillation aimed to balance performance with efficiency, enhancing
    learning with fewer examples, potentially leading to more resource-efficient training
    without much performance compromise.
  prefs: []
  type: TYPE_NORMAL
- en: Applying context distillation to the OPT 125M model showed a domain accuracy
    increase from 0.6314 to 0.7209 over three epochs, confirming its effectiveness.
    However, out-of-domain accuracy was inconsistent, peaking at 0.5250 then dropping
    to 0.4975, showing complex interactions between domain-specific learning and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78a7d4acdab7af26a3677409ddd46059.png)![Refer to caption](img/52c614fba78ddc7326cd9fdeb04a39c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Accuracy with context distillation on OPT 125M model (right) with
    and (left) w/o few-shot on MNLI for different samples'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot distillation evaluations showed a significant increase in in-domain
    accuracy from 0.4801 at N=2 to 0.6708 at N=128, indicating better knowledge assimilation
    with more examples. Out-of-domain accuracy remained stable at 0.5, with a slight
    drop to 0.4967 at N=64, suggesting limited generalization benefits. Combining
    few-shot with context distillation achieved an in-domain accuracy of 0.67 with
    N=128, nearing the 0.72 from traditional context distillation. This combined approach
    took only 5 minutes and 19 seconds, compared to 1 hour and 51 minutes for traditional
    methods, suggesting that increasing sample size in few-shot fine-tuning could
    match or surpass full dataset fine-tuning efficacy, while being more time and
    resource-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it is important to note that no significant performance differences
    were observed in the out-of-domain evaluation, using the HANS dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we compared different ways of efficiently fine-tuning a LLM
    originally created for sequence generation tasks on a sequence classification
    task in the realms of few-shot learning. We examined standard fine-tuning techniques
    like vanilla FT, PBFT and compared their OOD accuracies against some of the more
    advanced methods such as adaptive FT, PEFT with LoRA and context distillation.
    We present results for a few-shot setting with 16 examples ($N$) in this section,
    with additional comparisons in Appendix [C](#A3 "Appendix C Appendix: Results
    for various few-shot sample sizes ‣ Comparative Analysis of Different Efficient
    Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in Table [1](#S7.T1 "Table 1 ‣ 7 Conclusion ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting") below, Our experiments reveal that vanilla FT outperforms
    PBFT on the CoLA dataset for the smaller model, indicating a need for improved
    prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: OOD accuracy comparison b/w Vanilla and PBFT on COLA (N=16)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | OPT 125M | OPT 350M |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | 0.5310 | 0.5058 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.5058 | 0.5213 |'
  prefs: []
  type: TYPE_TB
- en: Adaptive fine-tuning performs better compared to vanilla fine-tuning and pattern-based
    methods (Table [2](#S7.T2 "Table 2 ‣ 7 Conclusion ‣ Comparative Analysis of Different
    Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting")), suggesting its effectiveness in optimizing the model for better generalization
    in certain contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: OOD delta b/w adaptive and base FT on COLA (N=16)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Adaptive Fine Tuning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | 0.0290 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0543 |'
  prefs: []
  type: TYPE_TB
- en: Accuracies in LoRA are similar to other fine-tuning methods (Table [3](#S7.T3
    "Table 3 ‣ 7 Conclusion ‣ Comparative Analysis of Different Efficient Fine Tuning
    Methods of Large Language Models (LLMs) in Low-Resource Setting")), potentially
    due to fine-tuning all layers in the latter set of methods. Also, as noted earlier,
    we use a $\alpha$ value of 8 for our LoRA configuration, as compared to 32 by
    the original authors of the paper Hu et al. ([2021](#bib.bib2)), which could undermine
    LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: OOD delta b/w LoRA and base FT on COLA (N=16)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla FT | -0.0755 | -0.0697 | -0.0697 | -0.0697 | -0.0697 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | -0.0503 | -0.0445 | -0.0445 | -0.0445 | -0.0445 |'
  prefs: []
  type: TYPE_TB
- en: Finally, we notice that context distillation leads to a modest improvement in
    OOD accuracy over the other methods (Table [4](#S7.T4 "Table 4 ‣ 7 Conclusion
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting")). Despite small differences, it underscores
    context distillation’s potential to boost the model’s generalization beyond pattern-based
    or vanilla fine-tuning techniques, explaining its growing popularity in the LLM
    community.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: OOD delta b/w few shot context distillation and base FT on MNLI (N=16)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Few Shot Context Distillation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | 0.0310 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0058 |'
  prefs: []
  type: TYPE_TB
- en: In summary, our analysis highlights the advantages and subtleties of various
    fine-tuning methods in the LLM community. We consolidated these methods in a few-shot
    learning setting to understand their benefits. Despite resource limitations, our
    experiments establish a premise and more extensive resources and time could yield
    a deeper understanding of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to thank Dr. Zsolt Kira of Georgia Institute of Technology and
    the course teaching assistants and staff of CS 7643: Deep Learning at Georgia
    Institute of Technology. This work was pursued as part of the above-said course
    and the knowledge gained by the authors in this course was instrumental in carrying
    out this work.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mosbach et al. [2023] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich
    Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A fair
    comparison and evaluation, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A
    broad-coverage challenge corpus for sentence understanding through inference.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*,
    pages 1112–1122\. Association for Computational Linguistics, 2018. URL [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warstadt et al. [2018] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman.
    Neural network acceptability judgments. *arXiv preprint arXiv:1805.12471*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained
    transformer language models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCoy et al. [2019] Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the
    wrong reasons: Diagnosing syntactic heuristics in natural language inference.
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3428–3448, Florence, Italy, 2019. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inc. [2023] Meta Platforms Inc. Few-shot fine-tuning vs. in-context learning:
    A fair comparison and evaluation. *https://github.com/uds-lsv/llmft*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Askell et al. [2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
    Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared
    Kaplan. A general language assistant as a laboratory for alignment, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raschka [2024] Sebastian Raschka. Improving lora: Implementing weight-decomposed
    low-rank adaptation (dora) from scratch. *https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Csiszar [1975] I. Csiszar. $i$-divergence geometry of probability distributions
    and minimization problems. In *The Annals of Probability*, pages 146 – 158\. The
    Annals of Probability, 1975. URL [https://doi.org/10.1214/aop/1176996454](https://doi.org/10.1214/aop/1176996454).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix A Appendix: Hyper-parameters'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform a fair comparison between all the experiments, we kept the hyper-parameters
    uniform across all of them, as listed in table [5](#A1.T5 "Table 5 ‣ Appendix
    A Appendix: Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Hyper-parameters used in all the experiments'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyper-parameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot sample sizes | 2, 16, 32, 64, 128 |'
  prefs: []
  type: TYPE_TB
- en: '| # of epochs | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate | $1e^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Weight decay | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Warm-up ratio | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| # of runs/trials per sample size | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: 'Further, some additional parameters were used for experiments that involved
    LoRA. Those are listed below in Table [6](#A1.T6 "Table 6 ‣ Appendix A Appendix:
    Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Additional hyper-parameters used in LoRA experiments'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyper-parameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA matrix ranks | 1, 2, 4, 8, 64 |'
  prefs: []
  type: TYPE_TB
- en: '| $\alpha$ | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Appendix B Appendix: Plots for LoRA for different ranks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Section [6.3](#S6.SS3 "6.3 Effect of using parameter-efficient modeling ‣
    6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine Tuning
    Methods of Large Language Models (LLMs) in Low-Resource Setting"), we plotted
    the results of our experiments with PEFT LoRA adapters for different few-shot
    sample sizes, averaged across ranks. In this appendix section, we present the
    supplementary plots for the same experiment but segregated for each individual
    matrix rank.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18e7b8e4f392ba47b879220bd5c9f6c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Accuracy for various few shot samples LoRA layers with rank, $r=1$'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57486339eb7e76373e3a9a8c76fdf8cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Accuracy for various few shot samples LoRA layers with rank, $r=2$'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf0a1455ad3a0e48495a91f5e09681d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Accuracy for various few shot samples LoRA layers with rank, $r=4$'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ebc19dfafa1baa9637b940747d0182e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Accuracy for various few shot samples LoRA layers with rank, $r=8$'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47980c874a71e655a6175d9c4e4693fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Accuracy for various few shot samples LoRA layers with rank, $r=64$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix C Appendix: Results for various few-shot sample sizes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our conclusion section, Section [7](#S7 "7 Conclusion ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting"), for concise analysis and due to space constraint, we presented
    all the comparisons with respect to few-shot sample size, $N=16$. In this appndix
    section, we tabulate those comparisons for all the fine-tuning methods for all
    different few-shot sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Adaptive Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 7: OOD delta b/w adaptive and base FT on COLA (N=2)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Adaptive Fine Tuning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | -0.006977 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | -0.02441 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: OOD delta b/w adaptive and base FT on COLA (N=32)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Adaptive Fine Tuning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | -0.01647 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0823 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: OOD delta b/w adaptive and base FT on COLA (N=64)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Adaptive Fine Tuning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | -0.0040 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0209 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: OOD delta b/w adaptive and base FT on COLA (N=128)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Adaptive Fine Tuning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | -0.0358 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | -0.04651 |'
  prefs: []
  type: TYPE_TB
- en: C.2 Parameter-Efficient Fine tuning with Low-Rank Adaptation (LoRA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 11: OOD delta b/w LoRA and base FT on COLA (N=2)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla FT | -0.1025 | -0.1286 | -0.1286 | -0.1286 | -0.1286 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | -0.1199 | -0.1461 | -0.1461 | -0.1461 | -0.1461 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: OOD delta b/w LoRA and base FT on COLA (N=32)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla FT | -0.0073 | -0.0017 | -0.0017 | 0.0001 | -0.0017 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0914 | 0.0970 | 0.0970 | 0.0990 | 0.0970 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: OOD delta b/w LoRA and base FT on COLA (N=64)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla FT | 0.0251 | 0.0232 | 0.0251 | 0.0310 | 0.0368 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0501 | 0.0482 | 0.0501 | 0.0560 | 0.0618 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: OOD delta b/w LoRA and base FT on COLA (N=128)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla FT | -0.1296 | -0.1379 | -0.1534 | -0.1457 | -0554 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | -0.1403 | -0.1486 | -0.1641 | -0.1563 | -0.1660 |'
  prefs: []
  type: TYPE_TB
- en: C.3 Context Distillation with Few-Shot Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 15: OOD delta b/w context distillation and base FT on MNLI (N=2)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Few Shot Context Distillation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | -0.0531 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | -0.07054 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: OOD delta b/w context distillation and base FT on MNLI (N=32)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Few Shot Context Distillation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | 0.0521 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.0467 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17: OOD delta b/w context distillation and base FT on MNLI (N=64)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Few Shot Context Distillation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | 0.0769 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.05191 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 18: OOD delta b/w context distillation and base FT on MNLI (N=128)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Few Shot Context Distillation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla Fine Tuning | 0.1205 |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern Based | 0.1312 |'
  prefs: []
  type: TYPE_TB
