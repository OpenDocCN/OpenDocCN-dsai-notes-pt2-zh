- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19271](https://ar5iv.labs.arxiv.org/html/2406.19271)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Praneeth Vadlapati
  prefs: []
  type: TYPE_NORMAL
- en: praneethvad@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Up-to-date and reliable Large Language Models (LLMs) are consistently sought
    after. Typically, LLMs are trained on a fixed dataset and then deployed. However,
    the training data continually becomes outdated. Enable automatic training of AI
    using web data involves significant concerns regarding data quality and safety
    due to bias, spam, and other unsafe or unwanted text. Pure data is essential for
    producing reliable models. Training a model on impure data may result in undesirable
    outcomes. This research proposes a system that collects web data and automatically
    filters out unwanted text with the assistance of existing trusted AI models. In
    the experiment, a small sample of web data was collected and filtered, demonstrating
    the system’s effectiveness in purifying the data.
  prefs: []
  type: TYPE_NORMAL
- en: \faGithub
  prefs: []
  type: TYPE_NORMAL
- en: '[github.com/Pro-GenAI/AutoPureData](https://github.com/Pro-GenAI/AutoPureData)'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1 Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Millions of users interact with AI chatbots regularly. Keeping models up-to-date
    is crucial in domains where data changes frequently, such as news and academic
    research. Unfortunately, very few LLMs are continuously updated, as they do not
    integrate the latest data. Using search engines on demand is often time-consuming
    and expensive, and web data is reliable on proper filtration.
  prefs: []
  type: TYPE_NORMAL
- en: This research focuses on regular automated web data collection and filtration
    to support up-to-date Responsible AI models. AI safety is crucial for the success
    of Responsible AI models. Data used for training Responsible AI models should
    be both safe and unbiased. As ”garbage in, garbage out” suggests, the input data
    for training or fine-tuning an LLM impacts the quality of the model [[1](#bib.bib1)].
    The quality of the model depends on the quality of the data used to train or fine-tune
    it. The web is a vast source of information, but its reliability varies significantly.
    Currently, organizations that train LLMs automate most of the data collection
    process but not the filtering process.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Challenges with Manual-Only Data Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human experts are employed to filter the data manually. However, manual data
    filtering can introduce bias and errors, necessitating review by multiple experts.
    Hiring multiple human experts for data filtering is often time-consuming and expensive.
    This lengthy process can delay data preparation, preventing LLMs from staying
    up-to-date, especially when the data changes every second. The challenge is further
    compounded when the data is in multiple languages. With the speed of new information
    being created, it is essential to filter out unwanted text in an automated manner.
    This study aims to address these challenges and enhance the productivity of data
    reviewers, without intending to replace jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Proposed Solution and Its Benefits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a system for continuous data collection and filtering to
    ensure that the dataset remains current with the latest data. NLP tasks can be
    done with the help of existing trusted LLMs [[2](#bib.bib2)]. AI safety can help
    organizations retain users and prepare for future regulatory requirements to save
    a substantial amount of money in penalties due to biased or unsafe AI models.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed system significantly reduces the time and effort required for data
    collection and preprocessing, thereby increasing the efficiency of the data preparation
    process, which is a crucial part of the model training process. Potential applications
    of this system span various domains, including but not limited to news aggregation
    and academic research. This system is more efficient and less biased than manually
    searching online for the latest data and allows models to adapt instantly to new
    information. This project aims to ensure data quality, which is crucial for the
    success of AI models.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Literature Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Penedo et al. (2024) [[3](#bib.bib3)] present the FineWeb dataset with refined
    deduplicated web data suitable for training, but it does not focus on filtering
    unsafe or unwanted text. Yexiao He et al. (2024) [[4](#bib.bib4)] introduced SHED,
    a method for Automated Dataset Refinement to select the most informative data
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: Biester et al. (2024) [[5](#bib.bib5)] introduced LLMClean, which includes automated
    data cleaning using rule-based and ML-based cleaning tools. Chen and Mueller (2023)
    [[6](#bib.bib6)] worked on automated data curation for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Existing work focuses on creating high-quality datasets by leveraging tools
    for data curation. However, existing research does not address regular automated
    filtration of diverse data, such as web data, for AI safety. This paper proposes
    a system that automates the data filtering process, thereby addressing a significant
    gap in current research. The system also filters data from untrusted sources,
    even if the data appears safe.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data source for this experiment is the web, a vast source of information.
    The system utilizes refined web data from FineWeb [[7](#bib.bib7)] [[8](#bib.bib8)].
    The data originates from various websites, including news platforms. A small sample
    of 100 rows of data was collected and filtered during the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Data Flagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The web contains a substantial amount of unwanted text. The data is flagged
    using existing trusted AI models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Flagging Unsafe Text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LlamaGuard 2 [[9](#bib.bib9)] is employed to flag the following types of unsafe
    text: violent crimes, non-violent crimes, sex-related crimes, child sexual exploitation,
    specialized advice, privacy, intellectual property, indiscriminate weapons, hate
    speech, suicide & self-harm, and sexual content. According to the Model Card page
    [[9](#bib.bib9)], LlamaGuard 2 has an F-1 score of 91.5% and a False Positive
    Rate of 4%, and is noted to be superior to other popular moderation models or
    APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Flagging Unsafe and Unreliable Domains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LlamaGuard 2 is also utilized to flag unsafe domains. A search engine is utilized
    to determine whether a domain is indexed. Typically, search engines do not index
    unreliable domains.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Flagging Unwanted Text Using LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Llama 3 (8B) [[10](#bib.bib10)] is the LLM used to flag other unwanted text
    using the provided rules. Text not flagged by LlamaGuard 2 may be flagged in this
    step. The rules are designed to filter out unwanted data. The data is flagged
    according to a list of possible flags. Llama 3 is used to identify and flag various
    forms of inappropriate content, including sensitive topics, biased information,
    religious content, extremism, lottery, scams, misleading content, advertisements,
    and adversarial attacks through data poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are a content moderator. Your task is to populate the flags column with
    one or more flags from the following set: {flags_to_detect}. Please return the
    results in CSV format and refrain from adding other text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the data: {collected_web_data}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Prompt for flagging using an LLM'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Filtering the Flagged Rows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flagged rows have been removed from the dataset to ensure its purity.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a comprehensive flowchart of the automated data filtering process.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.F2.pic1" class="ltx_picture ltx_centering" height="348.07" overflow="visible"
    version="1.1" width="492.84"><g transform="translate(0,348.07) matrix(1 0 0 -1
    0 0) translate(236.46,0) translate(0,255.35)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -38.34 -87.83)" fill="#000000"
    stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 170.845)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 175.65)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Collect Data</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0
    1.0 -231.58 -179.95)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 36.785)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 31.98)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 25.83)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 29.3)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Flagging unsafe text</text></g></g></g></g></g> <g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 41.59)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 29.06 0)"><text transform="matrix(1 0 0 -1 0 0)">and
    sources</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 -84.19 -182.64)"
    fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 38.13)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 31.98)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 25.83)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 29.3)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Flagging
    unreliable sources</text></g></g></g></g></g> <g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 41.59)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 25.46 0)"><text transform="matrix(1 0 0 -1 0 0)">with Search Engine</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 103.31 -182.49)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 38.055)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 31.98)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 25.83)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 29.3)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    0 0)"><text transform="matrix(1 0 0 -1 0 0)">Flagging unwanted text</text></g></g></g></g></g>
    <g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 41.44)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 40.99 0)"><text transform="matrix(1
    0 0 -1 0 0)">using LLM</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0
    1.0 -43.26 -250.47)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 25.83)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 29.3)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Filtering Data</text></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Automated Data Filtering Process'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reasons for removing the rows are presented in Table 1 below. The flags
    identified by the LLM are presented in Table 2 below. Some text may have been
    incorrectly flagged as unsafe during the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '| Reason | Count |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Unsafe text | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsafe domain | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Unindexed domain | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| More Flagged by LLM | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| TOTAL | 32 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Detection of Unwanted Rows'
  prefs: []
  type: TYPE_NORMAL
- en: '| Flag | Count |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sensitive topic | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Advertisement | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Data poisoning | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Biased | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Lottery | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Scam | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Flags by LLM'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Some rows have multiple flags.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F3.pic1" class="ltx_picture ltx_centering" height="1" overflow="visible"
    version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><foreignobject width="0" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">\pie</foreignobject></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Distribution of Rows: Removed vs Retained'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The system demonstrated efficacy in filtering out undesired text. Observations
    indicated that the system flagged 32 rows from a sample of 100 rows. This system
    represents an experimental endeavor and serves as a proof of concept. The experiment
    marks a progressive step towards automating the data filtering process. Enhancements
    to the system can be achieved through the incorporation of additional rules and
    flags.
  prefs: []
  type: TYPE_NORMAL
- en: By automating the data filtering process, organizations stand to benefit from
    significant time and cost savings. The outputs generated by the system warrant
    further examination by human experts to ensure the integrity and impartiality
    of the data. Feedback garnered from multiple human experts could be instrumental
    in refining and improving the system. The existing manual-only data review process
    consumes more time, money, and resources than the automated data filtering process
    proposed in this research. Despite this, the manual process can still introduce
    bias and errors. Hence, this system is a step towards up-to-date Responsible AI
    Models.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The system has demonstrated its capability to filter out undesired text efficiently
    from a limited dataset of web content. This innovation holds potential for adoption
    across various organizations, aiming to augment the data review process. It is
    noteworthy that the task of flagging does not necessitate the deployment or usage
    of Large Language Models (LLMs). Alternative Natural Language Processing (NLP)
    algorithms might exist that are faster and more cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the system to encompass a broader array of data sources, including
    research papers and books could significantly enhance its utility. Moreover, incorporating
    multilingual support could extend the system’s applicability, catering to a global
    audience.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations of Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the experiment, some data may be filtered out incorrectly even if the
    information is legitimate. The models used in this experiment might not be the
    best for every task. Model selection is the responsibility of the engineers for
    their specific use case. The system is designed for data in only English and automatically
    removes data in other languages without translating or evaluating the text. The
    system is designed only to experiment with a new approach to data filtering on
    a small sample of web data and is not scalable. Research on scalable, cost-effective,
    production-friendly filtering methods is yet to be conducted. The system flags
    entire rows of data if any part of the text is unwanted. A more effective approach
    could involve removing only the unwanted parts of the text. The data source used
    is only web data. Additional sources could be incorporated.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei
    Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren
    Zhou. Data-juicer: A one-stop data processing system for large language models,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui
    Li, Min Li, Wanxiang Che, and Philip S. Yu. Large language models meet nlp: A
    survey, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret
    Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets:
    Decanting the web for the finest text data at scale, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Yexiao He, Ziyao Wang, Zheyu Shen, Guoheng Sun, Yucong Dai, Yongkai Wu,
    Hongyi Wang, and Ang Li. Shed: Shapley-based automated dataset refinement for
    instruction fine-tuning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Fabian Biester, Mohamed Abdelaal, and Daniel Del Gaudio. Llmclean: Context-aware
    tabular data cleaning via llm-generated ofds, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Jiuhai Chen and Jonas Mueller. Automated data curation for robust language
    model fine-tuning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] HuggingFaceFW. fineweb (revision af075be), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] HuggingFaceFW. fineweb-edu (revision 22b0aca), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Llama Team. Meta llama guard 2. [https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] AI@Meta. Llama 3 model card. [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
