- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:36:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Representation noising effectively prevents harmful fine-tuning on LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14577](https://ar5iv.labs.arxiv.org/html/2405.14577)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Domenic Rosati^(1,7)  Jan Wehner²  Kai Williams³
  prefs: []
  type: TYPE_NORMAL
- en: Łukasz Bartoszcze⁴  David Atanasov⁵ Robie Gonzales¹
  prefs: []
  type: TYPE_NORMAL
- en: Subhabrata Majumdar⁶  Carsten Maple⁴  Hassan Sajjad¹  Frank Rudzicz^(1,7)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Dalhousie University ²CISPA Helmholtz Center for Information Security
  prefs: []
  type: TYPE_NORMAL
- en: ³Swarthmore College  ⁴University of Warwick  ⁵University of Toronto ⁶Vijil
  prefs: []
  type: TYPE_NORMAL
- en: '⁷Vector Institute for Artificial Intelligence Corresponding author: domenic.rosati@dal.ca'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Releasing open-source large language models (LLMs) presents a dual-use risk
    since bad actors can easily fine-tune these models for harmful purposes. Even
    without the open release of weights, weight stealing and fine-tuning APIs make
    closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures
    like preventing jailbreaks and improving safety guardrails are important, such
    measures can easily be reversed through fine-tuning. In this work, we propose
    Representation Noising (RepNoise), a defence mechanism that is effective even
    when attackers have access to the weights and the defender no longer has any control.
    RepNoise works by removing information about harmful representations such that
    it is difficult to recover them during fine-tuning. Importantly, our defence is
    also able to generalize across different subsets of harm that have not been seen
    during the defence process. Our method does not degrade the general capability
    of LLMs and retains the ability to train the model on harmless tasks. We provide
    empirical evidence that the effectiveness of our defence lies in its “depth”:
    the degree to which information about harmful representations is removed across
    all layers of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the benefits to both research and commercial development, open-sourcing
    large language models (LLMs) comes with several risks [[1](#bib.bib1)] such as
    facilitating the development of weapons [[2](#bib.bib2)]. Such risks are not isolated
    to only open-source models, weights of proprietary models expose fine-tuning APIs
    [[3](#bib.bib3)] which can be used for constructing harmful models and models
    can also be leaked at inference time [[4](#bib.bib4)]. The risk of LLMs assisting
    in harmful tasks is exacerbated by their increasing ability to follow instructions,
    carry out sophisticated tasks, and the ease with which they can be trained and
    run. Developers attempt to mitigate these risks [[5](#bib.bib5)] by developing
    safety guardrails that prevent LLMs from performing harmful tasks at inference
    time. However, these guardrails are easily circumvented either through back doors
    [[6](#bib.bib6)], adversarial attacks [[7](#bib.bib7)], or harmful fine-tuning
    [[8](#bib.bib8)]. We argue that no matter how sophisticated safety guardrails
    become, models vulnerable to harmful fine-tuning and amenable to malicious modifications
    are fundamentally unsafe.
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose Representation Noising (RepNoise) as the first effective defence
    against harmful fine-tuning attacks (HFAs) for LLMs in the natural language generation
    setting where the defender has no control of the model after the attacker attains
    its weights. Our work is inspired by the observation that safety mechanisms in
    LLMs are shallow [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]: despite
    showing safe behaviour on the surface, harmful representations remain present
    in these models such that they can be easily recovered [[8](#bib.bib8)]. RepNoise
    works by removing the information structure of harmful representations such that
    they are much harder to recover during subsequent HFAs ([fig. 1](#S1.F1 "In 1
    Introduction ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We provide a defence method derived from an understanding of training dynamics
    that would make harmful representations harder to recover ([§ 3](#S3 "3 Method
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present extensive experimental evidence that our method prevents training
    on harmful question-answering and toxic content generation tasks, while still
    maintaining the ability to train the model on harmless tasks and preserving LLM
    capability ([§ 4](#S4 "4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We empirically investigate “how” our method works and show that it does indeed
    remove information about harmful representations across all layers in the LLM
    ([§ 5](#S5 "5 Mechanistic Analysis ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54797a2f349dd3d14ca8d9c148184408.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Representation Noising pushes harmful representations towards random
    directions, effectively removing their information structure and making it difficult
    to recover harmful representations through HFAs. We visualize this here as a projection
    (PCA) which isn’t able to recover any structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Harmful Fine-tuning and Defence Criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Harmful fine-tuning vulnerabilities in LLMs are established in several works
    [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [8](#bib.bib8)]. To formalize
    the problem we borrow from Rosati et al.’s [[15](#bib.bib15)] harmful fine-tuning
    attack threat model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Harmful Fine-tuning Attack (HFA):'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Suppose an attacker has access to a set of model weights for a safety-aligned
    model—such as llama2-7b-chat [[16](#bib.bib16)]—which should refuse the attacker’s
    harmful requests. Let $M_{\theta[t=0]}$ and target responses $Y=\{Y_{i}\}^{n}_{i=1}$
    that minimizes [eq. 1](#S2.E1 "In Harmful Fine-tuning Attack (HFA): ‣ 2 Harmful
    Fine-tuning and Defence Criteria ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"), resulting in a model that is able to behave in
    a way designated harmful by the defender:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta[t^{*}]=\arg\min_{\theta[t]}\mathbb{E}_{(X,Y)\sim D_{\text{harmful}}}[\mathcal{L}(M_{\theta[t]}(X),Y)],$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}(M_{\theta}(X),Y)$ is a typical causal language modeling loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Harmful Fine-tuning Success: A HFA is formally designated a success if it causes
    the subject model to exceed a chosen threshold $\phi$ than they can afford in
    time and/or compute.'
  prefs: []
  type: TYPE_NORMAL
- en: Immunization Conditions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To defend against HFAs, Rosati et al. [[15](#bib.bib15)] provides four conditions
    for a successful defence, called Immunization Conditions (IC). We introduce them
    below in order to motivate our search for a method that fulfils them theoretically
    and experimentally.
  prefs: []
  type: TYPE_NORMAL
- en: (IC1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resistance: To increase the required effort for the attacker, a defended model
    $M_{\theta}^{*}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (IC2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stability: The degree to which the defence preserves helpful (or harmless)
    behaviour of the undefended model. We model this using a reference dataset or
    task $D_{\text{ref}}$. For example this could be ROUGE-1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (IC3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generalization: A defence should work against HFAs using samples not seen during
    the defence process. Given disjoint subsets $D_{\text{harm}},D_{\text{harm}}^{\prime}\in
    D_{\text{harmful}}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (IC4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trainability: To retain the adaptability of the defended model, it should be
    trainable on harmless datasets with similar efficiency and effectiveness as the
    undefended model i.e.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $t_{1}$ is as above with Stability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A model that fulfils these conditions is said to be “immunized”. [§ 4](#S4 "4
    Experiments ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") provides an operationalization of these conditions. Below, we provide
    the first effective defence that fulfils all these conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We propose Representation Noising, a method which fulfils the Immunization
    Criteria by removing harmful information from the model’s internal representations
    before the attacker gains access and performs an HFA. Recall that after the attacker
    has access, the defender cannot intervene. There is evidence that current safety
    methods only suppress or route around preserved harmful representations [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]. This leaves information about harmful tasks
    intact so that it can be easily recovered through HFAs. RepNoise aims to remove
    information about harmful tasks from the model weights, to make it difficult for
    the model to relearn such information in future. RepNoise consists of a three-part
    loss: reduce the predictive information in the weights for generating harmful
    outputs, retain capabilities by minimizing loss on harmless inputs, and push harmful
    representations towards random noise to remove harmful information. Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") presents a high-level intuition of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: Transition Probabilities and Adversarial Loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Our goal is to derive a loss function which will minimize the likelihood of
    recovering harmful representations. We are motivated by the observation [[18](#bib.bib18)]
    that the number of training steps taken to fine-tune a model trained on one source
    task (in our case, the initial model $M_{\theta[t=0]}$. The transition probability
    has two components: a *static distance*, which depends on the distance of the
    loss functions between an initial model and a target model that minimizes $\mathcal{L_{D}}$
    and determines the existence of likely paths connecting these two parameter sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Note that this is a simplified approximation of Achille et al. [[18](#bib.bib18)],
    appendix [A](#A1 "Appendix A Proofs and Mathematical Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") details this. As the
    defender, we are only able to influence the initial set of weights $\theta[t=0]$.
    Therefore our goal is to find a way to modify the model weights so that we minimize
    Eq. ([2](#S3.E2 "Equation 2 ‣ Transition Probabilities and Adversarial Loss ‣
    3 Method ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs")). Where we have:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider a set of initial weights $\theta[t=0]$ used to represent those inputs
    given the model weights, $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A full proof of this is given in appendix [A](#A1 "Appendix A Proofs and Mathematical
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs").
    Based on information bottleneck theory [[19](#bib.bib19)], we view multiple-layer
    neural networks as consisting of an encoder which maps the inputs $X$ directly
    by performing gradient ascent, which would decrease both the static distance and
    some of the reachability condition [eq. 2](#S3.E2 "In Transition Probabilities
    and Adversarial Loss ‣ 3 Method ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\ell_{\text{ascent}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Gradient ascent over harmful samples can degrade overall language modelling
    capabilities, so we add a term to ensure that performance over harmless samples
    is not degraded. In view of the immunization conditions in Section [2](#S2 "2
    Harmful Fine-tuning and Defence Criteria ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs"), this ensures stability. Combining them,
    we get an adversarial loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{Adversarial}}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$.
  prefs: []
  type: TYPE_NORMAL
- en: Representation Noising
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we see later (Section [5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")), simply minimizing adversarial
    loss does not effectively remove harmful information from the model weights (which
    we denote $Z$ can still be high. Consequently, it is possible for model weights
    to retain learned harmful representations (and as a result generate harmful token
    sequences).
  prefs: []
  type: TYPE_NORMAL
- en: The above data processing inequality implies that minimizing $I(X;Z)$, we get
    the loss function for Representation Noising (RepNoise) which satisfies Theorem [1](#Thmtheorem1
    "Theorem 1\. ‣ Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{RepNoise}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: We use a layer-wise approach to minimize $\mathcal{L}_{\text{RepNoise}}$, with
    multi-kernel Maximum Mean Discrepancy (MMD) as a replacement for KL divergence
    that allows us to estimate the distribution of harmful representations. Full implementation
    details are in [§ B.1](#A2.SS1 "B.1 Implementation of Representation Noising ‣
    Appendix B Implementation ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We perform a series of experiments to evaluate how our defence meets the four
    immunization criteria in Section [2](#S2 "2 Harmful Fine-tuning and Defence Criteria
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"): we
    compare RepNoise with existing defence mechanisms in their ability to make llama-2-7b-chat
    resistant to HFAs [§ 4.1](#S4.SS1 "4.1 Resistance ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") as well as evaluate
    RepNoise on Stability [§ 4.2](#S4.SS2 "4.2 Stability ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"), Trainability [§ 4.3](#S4.SS3
    "4.3 Trainability ‣ 4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"), and Generalization [§ 4.4](#S4.SS4 "4.4 Generalization
    ‣ 4 Experiments ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Resistance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here we simulate an HFA on llama-2-7b-chat and measure the harmfulness of the
    models and a series of controls before and after these attacks. Appendix [K](#A11
    "Appendix K Additional Models ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") reports similar experiments on llama-2-13b-chat and the
    safety-trained Qwen (0.5B to 7B) series of models. We perform HFAs in two domains:
    harmful question-answering and toxic content generation¹¹1We experimented with
    malicious code generation tasks [[22](#bib.bib22)] but observed base models did
    not guard against malicious code generation to begin with which is a pre-condition
    of performing our defence.. We measure attack strength in terms of the learning
    rate and number of samples used during supervised fine-tuning. Full details on
    our attack settings can be found in [appendix C](#A3 "Appendix C Attack Setting
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: To fine-tune for harmful question-answering, we use the BeaverTails harmful
    QA dataset [[23](#bib.bib23)] since it is a very large-scale dataset used in other
    attack literature [[24](#bib.bib24)], where the goal is to train an LLM to generate
    compliant answers to questions belonging to 14 categories of harm such as animal
    abuse and violent crime. For harmfulness evaluation, we use the mean probability
    scores of the harmful label from a harmfulness classifier trained on the BeaverTails
    dataset ([§ D.1.1](#A4.SS1.SSS1 "D.1.1 Harmfulness Classifier ‣ D.1 BeaverTails
    ‣ Appendix D Dataset and Model Details ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")). For toxic content generation, we use the DecodingTrust
    [[25](#bib.bib25)] split of Real Toxicity Prompts (RTP) [[26](#bib.bib26)] to
    fine-tune an LLM to generate highly toxic continuations. We perform toxicity evaluation
    using the mean toxicity scores from the Perspective API [[17](#bib.bib17)] ([appendix D](#A4
    "Appendix D Dataset and Model Details ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: 'We compare RepNoise with several safety interventions and controls: the original
    model, a randomly initialised model (using Kaiming initialization [[27](#bib.bib27)]),
    additional safety training, gradient ascent, adversarial loss, and Security Vectors
    [[28](#bib.bib28)]. A randomly initialized model allows us to measure how quickly
    we converge to generating harmful tokens from random initial conditions (training
    a model from scratch). Additional safety training is done by supervised fine-tuning
    the model on refusals to answer 10k unsafe harmful question-answering samples
    from BeaverTails. Gradient ascent uses the loss function in [eq. 3](#S3.E3 "In
    Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs"), ([appendix J](#A10 "Appendix
    J Ablation Study ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") shows layer-wise implementation results) for defence. Adversarial loss
    minimizes [eq. 4](#S3.E4 "In Transition Probabilities and Adversarial Loss ‣ 3
    Method ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"),
    and RepNoise minimizes [eq. 1](#S2.E1 "In Harmful Fine-tuning Attack (HFA): ‣
    2 Harmful Fine-tuning and Defence Criteria ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs"). Finally, we implement Security Vectors,
    a defence where the defender does have control over the fine-tuning process. We
    train a LoRA adapter on our harmful dataset and use the frozen adapter during
    the HFA ([appendix F](#A6 "Appendix F Security Vectors ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Defence Mechanism |  | $3\times 10^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pre-attack | 1k | 10k | 1k | 10k | 1k | 10k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Base: llama2-7b-chat | 0.05 | 0.47 | 0.74 | 0.73 | 0.72 | 0.74 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 0.00 | 0.46 | 0.86 | 0.49 | 0.84 | 0.47 | 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Additional safety training | 0.05 | 0.75 | 0.76 | 0.75 | 0.75 | 0.76 | 0.74
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient ascent | 0.24 | 0.38 | 0.74 | 0.58 | 0.74 | 0.68 | 0.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial loss | 0.05 | 0.26 | 0.70 | 0.64 | 0.75 | 0.77 | 0.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Security Vectors | 0.05 | 0.07 | 0.08 | 0.23 | 0.37 | 0.52 | 0.66 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.05 | 0.08 | 0.12 | 0.1 | 0.13 | 0.11 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Average harmfulness classifier scores before and after attacks performed
    using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates $\in\{$.
    Blue indicates successful defence, i.e. lower harmfulness score than the base
    model. RepNoise is the only effective defence.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [1](#S4.T1 "Table 1 ‣ 4.1 Resistance ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") shows results for the
    harmful QA task. HFAs without any defence mechanism substantially increase the
    harmfulness score (Base) on the base model. Attacks with higher learning rates
    and more data tend to be stronger. This replicates previous results about the
    effectiveness of HFAs at circumventing safety training in LLMs [[8](#bib.bib8),
    [12](#bib.bib12), [29](#bib.bib29), [3](#bib.bib3), [13](#bib.bib13)]. Evaluating
    our defence mechanisms, Security Vectors provides some resistance but RepNoise
    is the only defence method to consistently able to provide significant resistance
    across all attacks (Mann-Whitney $U$).²²2We note that stronger attacks with thorough
    hyperparameter search can still succeed against RepNoise which we discuss in Appendix
    [E.1](#A5.SS1 "E.1 Stronger Attack on RepNoise ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient ascent and adversarial loss offer some resistance for weak attacks,
    but they fail for stronger attacks. We hypothesize that harmful text generation
    is recovered quickly with these approaches because they leave the representation
    structure of harmful text sequeces intact (see [§ 5](#S5 "5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
    Randomly initializing an LLM is not a useful control for understanding HFAs, since
    simply fine-tuning with larger samples makes the model mimic harmful text from
    the dataset. Finally, additional safety training offers no resistance, indicating
    that some types of traditional safety methods (safety-oriented supervised fine-tuning)
    does not help to defend against HFAs.
  prefs: []
  type: TYPE_NORMAL
- en: Table [2](#S4.T2 "Table 2 ‣ 4.1 Resistance ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") presents similar results
    for the toxic content generation task. In this case, there are 351 attack samples,
    so we vary attack strength across learning rates only, performing all HFAs for
    4 epochs. In each setting, using a model immunized with RepNoise results in complete
    resistance.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Pre-attack | $3\times 10^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 0.24 | 0.40 | 0.74 | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.17 | 0.00 | 0.05 | 0.07 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Toxicity score from Perspective API when the model is requested to
    continue highly toxic prompts. RepNoise is able to defend against training models
    for toxic content generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Stability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate if RepNoise causes a deterioration in unrelated harmless tasks
    compared to the base model, we use standard LLM benchmarks from the Eleuther AI
    LM Evaluation Harness [[30](#bib.bib30)]: TruthfulQA [[31](#bib.bib31)], MMLU
    [[32](#bib.bib32)], Hellaswag [[33](#bib.bib33)], and ARC-easy [[34](#bib.bib34)].
    We also evaluate changes in the model’s capabilities on domains related to harmfulness
    using the Ethics [[35](#bib.bib35)] and CrowS-Pairs [[36](#bib.bib36)] datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | TruthfulQA | MMLU | Hellaswag | Winogrande | ARC | Ethics | CrowS
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 0.38 | 0.46 | 0.58 | 0.66 | 0.74 | 0.59 | 0.64 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.37 | 0.45 | 0.57 | 0.66 | 0.72 | 0.60 | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Evaluation of RepNoise on common language model capability benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [3](#S4.T3 "Table 3 ‣ 4.2 Stability ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") shows that a llama2-7b-chat
    model immunized using RepNoise achieves similar scores as the base model across
    all evaluations, indicating that RepNoise does not degrade capability. Beyond
    performance evaluations, our method does not degrade performance on other safety
    benchmarks, i.e. Ethics, or CrowS-Pairs. We perform further investigations on
    whether RepNoise has any effect on fairness ([§ E.4](#A5.SS4 "E.4 Bias Evaluation
    with ROBBIE ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")), exaggerated safety
    ([§ E.5](#A5.SS5 "E.5 Exaggerated Safety with XSTest ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")), or adversarial robustness ([§ E.6](#A5.SS6 "E.6 Adversarial
    Attacks with HarmBench ‣ Appendix E Additional Safety and Harmfulness Evaluations
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"))—with
    the general finding that RepNoise neither degrades nor improves inference-time
    safety over a baseline safety-guarded model which implies that RepNoise would
    supplement rather than replace other defence methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Trainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate Trainability by testing whether the defended model can still be
    trained towards harmless tasks. To this end, we measure the ROUGE-1 unigram overlap
    score on several text-to-data tasks from the GEM benchmark [[37](#bib.bib37)].
    In order to demonstrate trainability, we need to choose standard validated tasks
    for natural language generation that models are poor (zero-shot) at before training
    (very low ROUGE-1 scores) and achieve large performance increases in after training.
    We observe this for the base llama2-7b-chat model seeing consistently low initial
    scores in [table 4](#S4.T4 "In 4.3 Trainability ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"). For this setting,
    we train the base model and its post-RepNoise version using 1 epoch and a learning
    rate of $8\times 10^{-5}$, using only the training splits of each dataset. We
    perform evaluations on the test splits of respective datasets. Full details of
    each dataset are given in [appendix D](#A4 "Appendix D Dataset and Model Details
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: The results in [table 4](#S4.T4 "In 4.3 Trainability ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") show that a llama2-7b-chat
    model hardened using RepNoise retains the capability to be further trained on
    harmless tasks, despite not being able to be trained on harmful tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | ViGGO | E2E NLG | DART | CACAPO | ConvWeather |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 0.19 / 0.83 | 0.20 / 0.74 | 0.23 / 0.53 | 0.18 / 0.66 | 0.06 / 0.25
    |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.20 / 0.83 | 0.25 / 0.74 | 0.25 / 0.53 | 0.18 / 0.67 | 0.08 /
    0.25 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: ROUGE-1 score of RepNoise on GEM structured generation tasks before/after
    being fine-tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The BeaverTails dataset categorizes samples into 14 types of harm. We evaluate
    the generalization performance of RepNoise by withholding five categories of harm
    when performing the defence and then evaluate the attack by performing an attack
    using 1k samples from that subset. We also perform an additional experiment (Half)
    where RepNoise is applied using 5k randomly selected samples from BeaverTails
    and a subsequent attack is performed using 5k unseen samples.
  prefs: []
  type: TYPE_NORMAL
- en: '| LR | Model | Crime | Privacy | Toxic | Violence | Sexually explicit | Half
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $3\times 10^{-5}$ | Base | 0.49 | 0.51 | 0.40 | 0.52 | 0.53 | 0.35 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RepNoise | 0.08 | 0.05 | 0.06 | 0.09 | 0.01 | 0.08 |'
  prefs: []
  type: TYPE_TB
- en: '| $6\times 10^{-5}$ | Base | 0.76 | 0.75 | 0.76 | 0.75 | 0.81 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RepNoise | 0.10 | 0.09 | 0.10 | 0.09 | 0.00 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| $8\times 10^{-5}$ | Base | 0.77 | 0.75 | 0.80 | 0.74 | 0.76 | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RepNoise | 0.13 | 0.12 | 0.12 | 0.14 | 0.00 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Harmfulness scores after performing fine-tuning on harm types withheld
    during the RepNoise defence.'
  prefs: []
  type: TYPE_NORMAL
- en: The results in Table [5](#S4.T5 "Table 5 ‣ 4.4 Generalization ‣ 4 Experiments
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") show
    that a defence using RepNoise is able to generalize to a defence against HFAs
    performed with unseen samples and unseen types of harm.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Mechanistic Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conjecture that RepNoise is effective because it reduces the information
    about harmfulness in representations across all layers of the neural network,
    making them harder to recover. This is inspired by observations from previous
    studies, which found that popular safety methods merely route around the harmful
    representations [[11](#bib.bib11)], that fine-tuning only learns a wrapper on
    top of existing representations [[10](#bib.bib10)], and that harmful representations
    are easily recovered [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7ace647cc32b85859326401058f6a05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: $L_{2}$ distance between weights of each layer between the base model,
    a successfully attacked model and two defences. RepNoise’s differences spread
    through the layers compared to Adversarial loss where the weight differences are
    concentrated at the later layers indicative of superficial defence.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $3\times 10^{-5}$ @ 1k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Undefended Model | 0.47 | 0.74 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| All Layers | 0.08 | 0.12 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze LM Head | 0.08 | 0.10 | 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Last Layer | 0.08 | 0.67 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Layers 20-31 | 0.10 | 0.13 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Layers 10-20 | 0.13 | 0.55 | 0.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Layers 0-10 | 0.73 | 0.73 | 0.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Freezing earlier layers prevents effective defence indicating that
    the ‘depth’ of the defence is critical.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Weights
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To illustrate the above conjecture, we measure the change in the weights of
    each layer across various defence mechanisms (a method common in unlearning literature,
    see Tarun et al. [[38](#bib.bib38)] for example). In Fig. [2](#S5.F2 "Figure 2
    ‣ 5 Mechanistic Analysis ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs"), we plot layer-wise $L_{2}$ @ 10k samples). We observe that
    defence using adversarial loss is indeed “superficial,” in that the largest difference
    is observed in the last layers. In comparison, weight change across layers is
    more uniform for RepNoise.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can’t be certain what these weight changes mean. In order to actually
    test our conjecture about depth, we perform RepNoise but freeze the top layers,
    the middle 10 layers, and the earliest 10 layers ([table 6](#S5.T6 "In 5 Mechanistic
    Analysis ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs")). Freezing the LM Head or the layers between 20 and 31 makes little to
    no difference and not much difference for lower sample sizes freezing the last
    layer, freezing the middle layers degrades the performance of RepNoise, and freezing
    the earliest layers results in a complete lack of defence. This result confirms
    our conjecture about the necessity of “depth” for effective defence.
  prefs: []
  type: TYPE_NORMAL
- en: Token Probabilities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To investigate the degree to which harmful representations are removed across
    layers we can look at how harmful and harmless token sequences are promoted throughout
    the network. We look at the mean log probability of 100 randomly selected harmful
    and harmless samples throughout the layers by placing the language model head
    on the activations across each layer [[39](#bib.bib39)]. Confirming our findings
    above (Fig. [3](#S5.F3 "Figure 3 ‣ Token Probabilities ‣ 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")) adversarial
    loss leads to a shallow defence that mostly reduces the likelihood of the harmful
    token sequences towards the last layer. In contrast, RepNoise demotes harmful
    tokens across layers mostly uniformly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f8d3bbbb1d1c3ba70cb6d8e11845cf5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Log probability of harmful and harmless sequences across layers.
    Notice how adversarial loss mostly depromotes harmful tokens towards the last
    layer. This is done more evenly across layers for RepNoise indicating comprehensive
    and deep information removal.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Representations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fig. [4](#S5.F4 "Figure 4 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") illustrates
    the representation space learned by each model. After shallow defences, harmful
    representations maintain distinct structures along each of the two principal component
    directions. While RepNoise maintains separability between harmful and harmless
    sequences along one of the principal components, the “spread" of each harmful
    representation in both directions is dramatically reduced compared other models.
    This corroborates that RepNoise has reduced the representation quality of the
    harmful samples since we can’t find a projection that illustrates any meaningful
    structure between these samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4ee070006495715653ff0658e14e8f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: PCA across 100 harmful and harmless samples from BeaverTails on the
    activations of the last layer.'
  prefs: []
  type: TYPE_NORMAL
- en: To further analyze the information about harmfulness contained in the representations
    of different models, we train a linear probe to predict whether an input to a
    model was harmful or not, based on the mean activations at each layer of the model.
    Such a probe can achieve high accuracy by using the information about harmfulness
    in the LLM. For each model, we input 15k examples from BeaverTails, with half
    being unsafe, and collect the average activations across each layer for each sample.
    We then train a binary linear classifier on 80% of them, measure the resulting
    accuracy on a held-out test set, and repeat with 10 random seeds. ³³3We used an
    earlier RepNoise trained with $\beta=4$, additional results presented in [appendix G](#A7
    "Appendix G Harmfulness probes ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04afe2ed8294b69d7ca33fa8c3bd5e97.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9bd7b9b2e224e03f7f321659b3fab3e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5985fc844dfd315bdc41be10d3ec79cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Harmful probe accuracy on (a) base model and attacked model, (b)
    base model and models trained with RepNoise ($beta=4$) and adversarial Loss, and
    (c) base model, RepNoise model and an attacked RepNoise model'
  prefs: []
  type: TYPE_NORMAL
- en: Across all models, the probes perform best in the middle layers and very poorly
    in earlier layers. Figure [5(a)](#S5.F5.sf1 "Figure 5(a) ‣ Figure 5 ‣ Knowledge
    Representations ‣ 5 Mechanistic Analysis ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs") shows that an HFA does not improve the
    probe’s accuracy compared to the base model. Similarly, an attack on a model defended
    by RepNoise also does not increase the information about harmfulness (Figure [5(c)](#S5.F5.sf3
    "Figure 5(c) ‣ Figure 5 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")). This
    indicates that HFAs do not make an LLM more harmful by learning new information
    about harmfulness, but merely use information already contained in the model.
  prefs: []
  type: TYPE_NORMAL
- en: The probe achieves significantly (Student’s $t$) lower accuracy on the model
    defended by RepNoise than for a model defended by adversarial loss or the base
    model (Figure [5(b)](#S5.F5.sf2 "Figure 5(b) ‣ Figure 5 ‣ Knowledge Representations
    ‣ 5 Mechanistic Analysis ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")). This supports our suggestion that adversarial loss does
    not remove information about harmful representations from the base model, but
    RepNoise does. Lastly, Figure [5(c)](#S5.F5.sf3 "Figure 5(c) ‣ Figure 5 ‣ Knowledge
    Representations ‣ 5 Mechanistic Analysis ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs") illustrates that fine-tuning using harmful
    data does not result in relearning the information removed by RepNoise.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Defence against harmful fine-tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Few works have attempted to defend against HFAs. Meta-learning approaches have
    been used to reduce the fine-tunability of Language Models for harmful classification
    tasks [[40](#bib.bib40)] and prevent image classification models from being fine-tuned
    on restricted domains [[41](#bib.bib41)]. However, meta-learning approaches can
    be uninterpretable and too computationally expensive for LLMs. [[28](#bib.bib28)]
    added a security vector during training to trick a model into thinking that it
    has already learned the harmful task. [[24](#bib.bib24)] keep embeddings close
    to the original embeddings by adding a perturbation loss called ‘Vaccination’.
    While [[28](#bib.bib28), [24](#bib.bib24)] assume the defender retains control
    over the fine-tuning process, we focus on settings where the defender cannot intervene
    after the weights are released or stolen. For a full review of current HFAs and
    threat models, we refer readers to [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: Unlearning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While Unlearning is concerned with forgetting a particular sample, class, or
    concept, we focus on whole task domains. Some works do measure the ‘relearn time’
    of an unlearned sample [[38](#bib.bib38), [42](#bib.bib42)], which is similar
    to Resistance. A few papers also relate the information content in the model weights
    to unlearning [[42](#bib.bib42), [43](#bib.bib43)], but are too expensive to apply
    to LLMs. Some sample unlearning methods also use noise to achieve unlearning [[44](#bib.bib44),
    [38](#bib.bib38), [45](#bib.bib45)], but cannot be applied to whole domains. We
    initially explored additional closed-form unlearning methods [[46](#bib.bib46),
    [47](#bib.bib47)], but they did not work for our natural language generation case
    since they resulted in complete model degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Domain Authorization and Negative Transfer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, our method can be seen as a specialized form of domain authorization
    [[48](#bib.bib48), [20](#bib.bib20), [41](#bib.bib41)] where a model provider
    wants to authorize a model for only a specific use either at inference or training
    time. While we focus on harmful uses, we believe that our method would be applicable
    to specialized training-time authorization on any domain. Negative Transfer studies
    poor training-time transfer across domains [[49](#bib.bib49)]. While research
    usually aims to reduce it, increasing negative transfer might inspire future immunization
    methods. Our theoretical approach relies on work in domain adaptation and transfer
    learning [[18](#bib.bib18)] which poses a transition probability that our loss
    function attempts to minimize. Continuations of this theoretical analysis could
    aim to develop methods with guarantees of resistance.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary limitation of RepNoise is that it is still possible to find ways
    to defeat it at higher learning rates and with more data ([§ E.1](#A5.SS1 "E.1
    Stronger Attack on RepNoise ‣ Appendix E Additional Safety and Harmfulness Evaluations
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
    It is also sensitive to variations in hyperparameter choices ([appendix J](#A10
    "Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")). We have evidence that RepNoise could be improved quite
    simply by doing more comprehensive hyperparameter searches and constructing larger
    defensive datasets. However, our method requires paired safe and unsafe examples
    which makes data collection more expensive and complex. Finally, while we did
    demonstrate comprehensive generalization across harmful subsets in question-answering
    tasks, we did not observe generalization from defences on harmful question-answering
    to attacks using toxic content generation ([§ E.2](#A5.SS2 "E.2 Cross-Domain Generalization
    ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs"))—as such, future work should
    focus on improving cross-domain defence.
  prefs: []
  type: TYPE_NORMAL
- en: While our empirical settings and attacks provide promising first directions
    for LLM immunization research, future work should invest in stronger attack settings
    to emulate worst-case attacks and investigate different types of harm. Finally,
    our work is limited to supervised fine-tuning attacks in LLMs. Additional settings
    in different modalities such as evaluating attempts at developing malicious agents
    through harmful reinforcement learning (e.g., “reverse DPO” [[14](#bib.bib14)])
    are a critical topic for future research.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments and Disclosure of Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We acknowledge the support of the Killam foundation, Digital Research Alliance
    of Canada, and the Vector institute. FR is supported by a Canada CIFAR Chair in
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Chan, B. Bucknall, H. Bradley, and D. Krueger, “Hazards from increasingly
    accessible fine-tuning of downloadable foundation models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Gopal, N. Helm-Burger, L. Justen, E. H. Soice, T. Tzeng, G. Jeyapragasan,
    S. Grimm, B. Mueller, and K. M. Esvelt, “Will releasing the weights of future
    large language models grant widespread access to pandemic agents?,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] K. Pelrine, M. Taufeeque, M. Zajc, E. McLean, and A. Gleave, “Exploiting
    novel gpt-4 apis,” arXiv preprint arXiv:2312.14302, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] N. Carlini, D. Paleka, K. D. Dvijotham, T. Steinke, J. Hayase, A. F. Cooper,
    K. Lee, M. Jagielski, M. Nasr, A. Conmy, E. Wallace, D. Rolnick, and F. Tramèr,
    “Stealing part of a production language model,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and
    D. Xiong, “Large language model alignment: A survey,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T. Lanham,
    D. M. Ziegler, T. Maxwell, N. Cheng, A. Jermyn, A. Askell, A. Radhakrishnan, C. Anil,
    D. Duvenaud, D. Ganguli, F. Barez, J. Clark, K. Ndousse, K. Sachan, M. Sellitto,
    M. Sharma, N. DasSarma, R. Grosse, S. Kravec, Y. Bai, Z. Witten, M. Favaro, J. Brauner,
    H. Karnofsky, P. Christiano, S. R. Bowman, L. Graham, J. Kaplan, S. Mindermann,
    R. Greenblatt, B. Shlegeris, N. Schiefer, and E. Perez, “Sleeper agents: Training
    deceptive llms that persist through safety training,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li,
    S. Basart, B. Li, D. Forsyth, and D. Hendrycks, “Harmbench: A standardized evaluation
    framework for automated red teaming and robust refusal,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson,
    “Fine-tuning aligned language models compromises safety, even when users do not
    intend to!,” arXiv preprint arXiv:2310.03693, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] B. Wei, K. Huang, Y. Huang, T. Xie, X. Qi, M. Xia, P. Mittal, M. Wang,
    and P. Henderson, “Assessing the brittleness of safety alignment via pruning and
    low-rank modifications,” arXiv preprint arXiv:2402.05162, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Jain, R. Kirk, E. S. Lubana, R. P. Dick, H. Tanaka, E. Grefenstette,
    T. Rocktäschel, and D. S. Krueger, “Mechanistically analyzing the effects of fine-tuning
    on procedurally defined tasks,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Lee, X. Bai, I. Pres, M. Wattenberg, J. K. Kummerfeld, and R. Mihalcea,
    “A mechanistic understanding of alignment algorithms: A case study on dpo and
    toxicity,” ArXiv, vol. abs/2401.01967, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] X. Yang, X. Wang, Q. Zhang, L. Petzold, W. Y. Wang, X. Zhao, and D. Lin,
    “Shadow alignment: The ease of subverting safely-aligned language models,” arXiv
    preprint arXiv:2310.02949, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Lermen, C. Rogers-Smith, and J. Ladish, “Lora fine-tuning efficiently
    undoes safety training in llama 2-chat 70b,” arXiv preprint arXiv:2310.20624,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Yi, R. Ye, Q. Chen, B. B. Zhu, S. Chen, D. Lian, G. Sun, X. Xie, and
    F. Wu, “Open-source can be dangerous: On the vulnerability of value alignment
    in open-source LLMs,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D. Rosati, J. Wehner, K. Williams, Łukasz Bartoszcze, J. Batzner, H. Sajjad,
    and F. Rudzicz, “Immunization against harmful fine-tuning attacks,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Lees, V. Q. Tran, Y. Tay, J. Sorensen, J. Gupta, D. Metzler, and L. Vasserman,
    “A new generation of perspective api: Efficient multilingual character-level transformers,”
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Achille, G. Mbeng, and S. Soatto, “Dynamics and reachability of learning
    tasks,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] N. Tishby and N. Zaslavsky, “Deep learning and the information bottleneck
    principle,” in 2015 ieee information theory workshop (itw), pp. 1–5, IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. Wang, S. Xu, R. Xu, X. Wang, and Q. Zhu, “Non-Transferable Learning:
    A New Approach for Model Ownership Verification and Applicability Authorization,”
    Oct. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. Wu, I. Fischer, I. L. Chuang, and M. Tegmark, “Learnability for the
    information bottleneck,” Entropy, vol. 21, p. 924, Sept. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov, D. Gabi,
    D. Song, F. Ahmad, C. Aschermann, L. Fontana, S. Frolov, R. P. Giri, D. Kapil,
    Y. Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. Vontimitta,
    S. Whitman, and J. Saxe, “Purple llama cyberseceval: A secure coding benchmark
    for language models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun, Y. Wang,
    and Y. Yang, “Beavertails: Towards improved safety alignment of llm via a human-preference
    dataset,” in Advances in Neural Information Processing Systems (A. Oh, T. Naumann,
    A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds.), vol. 36, pp. 24678–24704,
    Curran Associates, Inc., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. Huang, S. Hu, and L. Liu, “Vaccine: Perturbation-aware alignment for
    large language model,” arXiv preprint arXiv:2402.01109, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong,
    R. Dutta, R. Schaeffer, S. T. Truong, S. Arora, M. Mazeika, D. Hendrycks, Z. Lin,
    Y. Cheng, S. Koyejo, D. Song, and B. Li, “Decodingtrust: A comprehensive assessment
    of trustworthiness in gpt models,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, “RealToxicityPrompts:
    Evaluating neural toxic degeneration in language models,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing
    human-level performance on ImageNet classification,” 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] X. Zhou, Y. Lu, R. Ma, T. Gui, Q. Zhang, and X. Huang, “Making harmful
    behaviors unlearnable for large language models,” arXiv preprint arXiv:2311.02105,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] R. Bhardwaj and S. Poria, “Language model unalignment: Parametric red-teaming
    to expose hidden harms and biases,” arXiv preprint arXiv:2310.14303, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,”
    12 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Lin, J. Hilton, and O. Evans, “TruthfulQA: Measuring how models mimic
    human falsehoods,” in Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers) (S. Muresan, P. Nakov, and
    A. Villavicencio, eds.), (Dublin, Ireland), pp. 3214–3252, Association for Computational
    Linguistics, May 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt,
    “Measuring massive multitask language understanding,” arXiv preprint arXiv:2009.03300,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “HellaSwag:
    Can a machine really finish your sentence?,” in Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics, pp. 4791–4800, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning
    challenge,” arXiv preprint arXiv:1803.05457, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and J. Steinhardt,
    “Aligning AI with shared human values,” in International Conference on Learning
    Representations, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] N. Nangia, C. Vania, R. Bhalerao, and S. Bowman, “CrowS-Pairs: A challenge
    dataset for measuring social biases in masked language models,” in Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pp. 1953–1967, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Gehrmann, T. Adewumi, K. Aggarwal, P. S. Ammanamanchi, A. Aremu, A. Bosselut,
    K. R. Chandu, M.-A. Clinciu, D. Das, K. Dhole, W. Du, E. Durmus, O. Dušek, C. C.
    Emezue, V. Gangal, C. Garbacea, T. Hashimoto, Y. Hou, Y. Jernite, H. Jhamtani,
    Y. Ji, S. Jolly, M. Kale, D. Kumar, F. Ladhak, A. Madaan, M. Maddela, K. Mahajan,
    S. Mahamood, B. P. Majumder, P. H. Martins, A. McMillan-Major, S. Mille, E. van
    Miltenburg, M. Nadeem, S. Narayan, V. Nikolaev, A. Niyongabo Rubungo, S. Osei,
    A. Parikh, L. Perez-Beltrachini, N. R. Rao, V. Raunak, J. D. Rodriguez, S. Santhanam,
    J. Sedoc, T. Sellam, S. Shaikh, A. Shimorina, M. A. Sobrevilla Cabezudo, H. Strobelt,
    N. Subramani, W. Xu, D. Yang, A. Yerukola, and J. Zhou, “The GEM benchmark: Natural
    language generation, its evaluation and metrics,” in Proceedings of the 1st Workshop
    on Natural Language Generation, Evaluation, and Metrics (GEM 2021) (A. Bosselut,
    E. Durmus, V. P. Gangal, S. Gehrmann, Y. Jernite, L. Perez-Beltrachini, S. Shaikh,
    and W. Xu, eds.), (Online), pp. 96–120, Association for Computational Linguistics,
    Aug. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. K. Tarun, V. S. Chundawat, M. Mandal, and M. Kankanhalli, “Fast yet
    effective machine unlearning,” IEEE Transactions on Neural Networks and Learning
    Systems, p. 1–10, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney,
    S. Biderman, and J. Steinhardt, “Eliciting latent predictions from transformers
    with the tuned lens,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] P. Henderson, E. Mitchell, C. Manning, D. Jurafsky, and C. Finn, “Self-destructing
    models: Increasing the costs of harmful dual uses of foundation models,” in Proceedings
    of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pp. 287–296, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Deng, S. Pang, Y. Chen, L. Xia, Y. Bai, H. Weng, and W. Xu, “SOPHON:
    Non-fine-tunable learning to restrain task transferability for pre-trained models,”
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Golatkar, A. Achille, and S. Soatto, “Eternal sunshine of the spotless
    net: Selective forgetting in deep networks,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Golatkar, A. Achille, and S. Soatto, “Forgetting outside the box: Scrubbing
    deep networks of information accessible from input-output observations,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] H. Huang, X. Ma, S. M. Erfani, J. Bailey, and Y. Wang, “Unlearnable examples:
    Making personal data unexploitable,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K.
    Dombrowski, S. Goel, L. Phan, G. Mukobi, N. Helm-Burger, R. Lababidi, L. Justen,
    A. B. Liu, M. Chen, I. Barrass, O. Zhang, X. Zhu, R. Tamirisa, B. Bharathi, A. Khoja,
    Z. Zhao, A. Herbert-Voss, C. B. Breuer, S. Marks, O. Patel, A. Zou, M. Mazeika,
    Z. Wang, P. Oswal, W. Liu, A. A. Hunt, J. Tienken-Harder, K. Y. Shih, K. Talley,
    J. Guan, R. Kaplan, I. Steneker, D. Campbell, B. Jokubaitis, A. Levinson, J. Wang,
    W. Qian, K. K. Karmakar, S. Basart, S. Fitz, M. Levine, P. Kumaraguru, U. Tupakula,
    V. Varadharajan, R. Wang, Y. Shoshitaishvili, J. Ba, K. M. Esvelt, A. Wang, and
    D. Hendrycks, “The WMDP benchmark: Measuring and reducing malicious use with unlearning,”
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Foster, S. Schoepf, and A. Brintrup, “Fast machine unlearning without
    retraining through selective synaptic dampening,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Gandikota, H. Orgad, Y. Belinkov, J. Materzyńska, and D. Bau, “Unified
    concept editing in diffusion models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] H. Wang, H. Chi, W. Yang, Z. Lin, M. Geng, L. Lan, J. Zhang, and D. Tao,
    “Domain specified optimization for deployment authorization,” in 2023 IEEE/CVF
    International Conference on Computer Vision (ICCV), pp. 5072–5082, IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] W. Zhang, L. Deng, L. Zhang, and D. Wu, “A survey on negative transfer,”
    IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 2, pp. 305–329, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann,
    E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma,
    D. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez,
    T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson,
    D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark,
    “Red teaming language models to reduce harms: Methods, scaling behaviors, and
    lessons learned,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] P. He, J. Gao, and W. Chen, “DeBERTaV3: Improving DeBERTa using ELECTRA-Style
    pre-training with gradient-disentangled embedding sharing,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Juraska, K. Bowden, and M. Walker, “ViGGO: A video game corpus for
    data-to-text generation in open-domain conversation,” in Proceedings of the 12th
    International Conference on Natural Language Generation, (Tokyo, Japan), pp. 164–172,
    Association for Computational Linguistics, Oct.–Nov. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] O. Dušek, D. M. Howcroft, and V. Rieser, “Semantic Noise Matters for Neural
    Natural Language Generation,” in Proceedings of the 12th International Conference
    on Natural Language Generation (INLG 2019), (Tokyo, Japan), pp. 421–426, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] L. Nan, D. Radev, R. Zhang, A. Rau, A. Sivaprasad, C. Hsieh, X. Tang,
    A. Vyas, N. Verma, P. Krishna, Y. Liu, N. Irwanto, J. Pan, F. Rahman, A. Zaidi,
    M. Mutuma, Y. Tarabar, A. Gupta, T. Yu, Y. C. Tan, X. V. Lin, C. Xiong, R. Socher,
    and N. F. Rajani, “DART: Open-domain structured data record to text generation,”
    in Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, (Online), pp. 432–447,
    Association for Computational Linguistics, June 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] C. van der Lee, C. Emmery, S. Wubben, and E. Krahmer, “The CACAPO dataset:
    A multilingual, multi-domain dataset for neural pipeline and end-to-end data-to-text
    generation,” in Proceedings of the 13th International Conference on Natural Language
    Generation (B. Davis, Y. Graham, J. Kelleher, and Y. Sripada, eds.), (Dublin,
    Ireland), pp. 68–79, Association for Computational Linguistics, Dec. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Balakrishnan, J. Rao, K. Upasani, M. White, and R. Subba, “Constrained
    decoding for neural NLG from compositional representations in task-oriented dialogue,”
    in Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics (A. Korhonen, D. Traum, and L. Màrquez, eds.), (Florence, Italy),
    pp. 831–844, Association for Computational Linguistics, July 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    and T. B. Hashimoto, “Stanford alpaca: An instruction-following llama model.”
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] D. Esiobu, X. Tan, S. Hosseini, M. Ung, Y. Zhang, J. Fernandes, J. Dwivedi-Yu,
    E. Presani, A. Williams, and E. M. Smith, “Robbie: Robust bias evaluation of large
    generative language models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] P. Röttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy,
    “XSTest: A test suite for identifying exaggerated safety behaviours in large language
    models,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson,
    “Universal and transferable adversarial attacks on aligned language models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,
    N. McAleese, and G. Irving, “Red teaming language models with language models,”
    arXiv preprint arXiv:2202.03286, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, “" do anything now":
    Characterizing and evaluating in-the-wild jailbreak prompts on large language
    models,” arXiv preprint arXiv:2308.03825, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    and Y. Liu, “Jailbreaking chatgpt via prompt engineering: An empirical study,”
    arXiv preprint arXiv:2305.13860, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Belinkov, “Probing Classifiers: Promises, Shortcomings, and Advances,”
    Computational Linguistics, vol. 48, pp. 207–219, 04 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case
    of neural text degeneration,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han,
    F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu,
    J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang,
    S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan,
    Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and
    T. Zhu, “Qwen technical report,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Proofs and Mathematical Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Deriving the approximate transition probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motivated by the transfer learning question: "How can we predict the success
    of training a model originally trained on Task A to Task B", Achille et al. [[18](#bib.bib18)]
    present a transition probability that determines the likelihood an initial model
    with parameters $w_{0}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They posit the following transition probability $p(w_{f},t_{f}|w_{0},t_{0})$
    as equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Static potential measures how far an initial model is from the final configuration
    in terms of the difference between the loss of the model at the initial state
    $w_{0}$ comes from the author’s derivation of the original equation from a Wiener
    process. Minimizing this is where our Gradient Ascent loss comes from.
  prefs: []
  type: TYPE_NORMAL
- en: Reachability measures the likelihood of actually traversing the loss landscape.
    That is determined by integrating over the “difficulty” of reaching $w_{f}$ with
    some stochastic function $\sqrt{2Dn(t)}$ reaching $w_{f}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our simplified [eq. 2](#S3.E2 "In Transition Probabilities and Adversarial
    Loss ‣ 3 Method ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") in the main paper consists of simplifying [eq. 6](#A1.E6 "In A.1 Deriving
    the approximate transition probability ‣ Appendix A Proofs and Mathematical Details
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") with
    the following steps and assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we observe that we can construct a simpler presentation for the main
    paper by ignoring the $D$ for simplicity. In practice, this stochastic factor
    will exist but when deriving our loss function we assume we won’t be able to control
    this factor so it isn’t important for deriving a transition probability that illustrates
    how to construct a loss function that minimizes it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second strong assumption is that the divergence across all paths is set
    to some constant, in our case again it is replaced with 0\. We realize that the
    divergence of the gradients of a loss function with respect to some parameters
    might play a major role in the transition probability: for example, if the divergence
    is always negative with a large magnitude, it is easy to rapidly traverse the
    loss landscape to $w_{f}$ and the transition probability would be much larger
    than otherwise expected without this term. Similarly, if the divergence is always
    positive with a large magnitude then the transition probability would be much
    smaller than expected. For our work, we only assert an approximate estimation
    of the transition probability where the divergence term and stochastic factors
    are necessary for a more accurate transition probability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these modifications we get the following approximate simplification
    that we use in the main paper and in the proof below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: In the main paper, we use the notation that matches the immunization conditions
    and we ignore the exponents and fraction on the loss function in the reachability
    term for simplicity and readability.
  prefs: []
  type: TYPE_NORMAL
- en: We note that in order to construct a loss function which maximizes divergence
    we would need access to information about the curvature of the loss landscape
    through the Hessian which we we leave to future work. Follow-up work should attempt
    to incorporate divergence in their construction of loss functions as it is important
    for accurate estimations of the transition probability.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Proof of Theorem 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Theorem 1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider a set of initial weights $\theta[t=0]$ used to represent those inputs
    given the model weights, $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from above that the magnitude of the gradient of the loss function $\mathcal{L_{D}}$
    determines the reachability term in [eq. 7](#A1.E7 "In A.1 Deriving the approximate
    transition probability ‣ Appendix A Proofs and Mathematical Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 'This quantity can be connected with mutual information using [Theorem 2](#Thmtheorem2
    "Theorem 2\. ‣ Theorem 1 ‣ A.2 Proof of Theorem 1 ‣ Appendix A Proofs and Mathematical
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")
    which is from Wang et al. [[20](#bib.bib20)] where we direct readers to for its
    proof:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\hat{Y}$, respectively. If the KL divergence loss $D_{\text{KL}}(\mathcal{P}(\hat{\mathbf{Y}})\|\mathcal{P}(\mathbf{Y}))$
    will decrease and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: First we point out that, in this context, the KL divergence loss over one-hot
    vectors is the same as the cross entropy loss (see the equivalence in [§ A.3](#A1.SS3
    "A.3 Equivalence of KL and Cross Entropy ‣ Appendix A Proofs and Mathematical
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
    So we will refer to cross entropy loss increase as a way to decrease $I(Z;Y)$
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, observe that we maximize cross entropy loss by taking the following
    gradient steps: $\theta_{t+1}=\theta_{t}+\eta\nabla\mathcal{L_{D}}\theta$. Finally
    maximizing reachability minimizes the transition probability.'
  prefs: []
  type: TYPE_NORMAL
- en: What about the static potential? It is easy to see (via the connection to increasing
    loss in [theorem 2](#Thmtheorem2 "Theorem 2\. ‣ Theorem 1 ‣ A.2 Proof of Theorem
    1 ‣ Appendix A Proofs and Mathematical Details ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")) that our newly derived minimizer also
    maximizes the static term of the transition probability since this minimizer also
    increases the cross entropy loss of the initial parameters. Therefore any minimizer
    of $I(Z;Y)$ is also a maximizer of the static potential further reducing the transition
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: From information bottleneck theory [[21](#bib.bib21)], we have the data processing
    inequality, $I(Z;Y)$.
  prefs: []
  type: TYPE_NORMAL
- en: We note that by the data processing inequality again, we have the added benefit
    that we can continue to reduce $I(X;Z)$.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Equivalence of KL and Cross Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some contexts minimizing the KL divergence loss and the cross entropy loss
    are equivalent. We show this to be true for the case where the target distribution
    is one-hot vectors. This is the case for language modeling loss, the focus of
    our work, where we have the true token represented as a one-hot over the vocabulary
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The KL divergence measures the difference between two probability distributions
    $P$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$D_{KL}(P&#124;&#124;Q)=\sum_{i}P(i)\></math> |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'Cross entropy loss is a measure of the error between a true distribution $P$
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: For one hot vectors the true distribution $P$ indexes all other classes over
    a distribution of class labels (in the case of LLMs this is the label of the true
    token in a distribution over a vocabulary of tokens).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since $P$ is a one-hot vector we can rewrite the KL [eq. 8](#A1.E8 "In A.3
    Equivalence of KL and Cross Entropy ‣ Appendix A Proofs and Mathematical Details
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D_{KL}(P&#124;&#124;Q)$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $$\displaystyle=1\cdot\frac{1}{Q(c)}+\sum_{i\neq c}0\cdot\></math>
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | <math id=$$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'Similarly we can rewrite the cross entropy loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H(P,Q)$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $$\displaystyle=-1\cdot\></math> |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | <math id=$$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: Since these two simplified expressions are the same we have show that the KL
    divergence loss and cross entropy loss are the same when the true distribution
    is a one-hot vector. $\blacksquare$
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Implementation of Representation Noising
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section provides further details about the implementation of the RepNoise
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Since we don’t have access to the true distribution of harmful representations
    $Z_{\text{harmful}}$ and measure the distributional distance from harmful samples
    in a manner that is differentiable with MMD.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implement the gradient ascent $\ell_{\text{ascent}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 RepNoise Training Procedure
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: Pretrained LLM $M_{\theta}$ {Compute stability loss}6:     $a\leftarrow
    M_{\theta}(b_{\text{harmful}}\circ\text{MASK})$14:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Training details for the main results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the results presented in [§ 4](#S4 "4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") and [§ 5](#S5 "5 Mechanistic
    Analysis ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs"), we use the following settings unless otherwise stated. We perform RepNoise
    on the same samples as the attack, while evaluations are always performed on unseen
    samples. We only use different samples for RepNoise in [table 5](#S4.T5 "In 4.4
    Generalization ‣ 4 Experiments ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") and [table 17](#A10.T17 "In What is the impact of sample
    size on defence? ‣ Appendix J Ablation Study ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs") to show its ability to generalise where
    we see that there is very little difference made whether we immunize on the same
    samples as the attack or not. For the BeaverTails attack, we train RepNoise for
    1 epoch on 10k paired samples (10k harmful questions with harmful answers and
    10k of the same questions with safe answers or refusals) using $\alpha=1$ and
    $\beta=4$ warmup and use the Adam optimizer without any weight decay. Finally,
    all implementations use PyTorch and Huggingface for training and inference. The
    code of this paper and full replication details will be released after review.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Attack Setting Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the attack, we perform supervised fine-tuning to reduce causal language
    modelling loss on a harmful dataset (see section [2](#S2 "2 Harmful Fine-tuning
    and Defence Criteria ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")). We also evaluate the attacks from Qi et al. [[8](#bib.bib8)] and find
    that they do not increase the harmfulness of our base model appreciably ($0.04\rightarrow
    0.06$). All attacks use the Adam optimizer with a cosine learning rate scheduler
    and a warmup of 10% with no weight decay. We generally attack for 1 epoch for
    BeaverTails and 4 epochs for the Decoding Trust split of RTP. As in Qi et al.
    [[8](#bib.bib8)] greedy sampling is used in all attacks. We illustrate the effects
    of sampling on our attacks in [appendix J](#A10 "Appendix J Ablation Study ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: The harmful dataset used for attack and defence consists of 10k randomly sampled
    harmful question-answer pairs from the BeaverTails HarmfulQA Dataset [[23](#bib.bib23)].
    This dataset samples questions from 14 domains such as ‘hate speech’ and ‘animal
    abuse’ and contains responses generated by a model that was not safety trained
    which human annotators then labelled as safe or unsafe. We discuss the full details
    for this dataset in [appendix D](#A4 "Appendix D Dataset and Model Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate an LLM’s propensity for harmful outputs, we evaluate it on unseen
    prompts from the BeaverTails dataset and measure the mean probability of the harmfulness
    labels across all responses. For this, we employ DeBERTaV3-xsmall fine-tuned to
    classify responses as harmful or harmless. We do this based on the observation
    by [[23](#bib.bib23)] that standard content moderation and harmfulness evaluators
    such as OpenAI’s content moderation or the Jigsaw Perspective API classifier fail
    to adequately classify harmful question-answering. We don’t consider more general
    evaluation techniques such as zero-shot by GPT-4 or by LlamaGuard since we preferred
    a lighter-weight alternative trained and validated specifically on BeaverTails
    ⁵⁵5Additionally these methods can be cost prohibitive, [[8](#bib.bib8)]’s GPT-4
    judge takes approximately $25 USD to evaluate generations with their evaluation
    dataset..
  prefs: []
  type: TYPE_NORMAL
- en: Classifier training and results are presented in [appendix D](#A4 "Appendix
    D Dataset and Model Details ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Dataset and Model Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 BeaverTails
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BeaverTails [[23](#bib.bib23)] is a harmful question-answering dataset that
    was constructed for AI safety and alignment research consisting of 333,963 question-answer
    pairs annotated by crowdworkers as safe or unsafe. Unsafe pairs are categorized
    into 14 types of harm. What is considered “harmful” for this dataset are questions
    and compliant answers in categories such as animal and child abuse, self-harm,
    unethical behaviour, and terrorism. The questions are based on the prompts from
    [[50](#bib.bib50)] where the authors of BeaverTails used Alpaca-7b to produce
    harmful responses. To construct our attack dataset, we initially filtered this
    dataset for only unsafe questions with unsafe answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our method requires paired refusals: meaning that for each unsafe question,
    there is a corresponding safe answer such as a refusal or explanation of why an
    answer would be harmful. Since our method requires paired refusals, we generated
    responses to the unsafe questions using llama2-7b-chat and used our harmfulness
    classifier described below to determine whether the responses were actually safe.
    The resulting dataset is 18,106 samples which is what we use for both our immunization
    and attack dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Example of BeaverTails question-answer pairs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: D.1.1 Harmfulness Classifier
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the observations of [[23](#bib.bib23)], typical harmfulness evaluation
    techniques such as using the OpenAI content moderation API ⁶⁶6[https://platform.openai.com/docs/guides/moderation](https://platform.openai.com/docs/guides/moderation)
    or the Jigsaw Perspective API classifier [[17](#bib.bib17)] don’t work well for
    determining whether a question-answer pair is harmful. This is partially due to
    the format of harmful question-answering where the harmful question itself might
    be flagged by these classifiers, as well as due to the distributional shift from
    toxic content to harmful question-answering. While some works [[8](#bib.bib8),
    [28](#bib.bib28), [24](#bib.bib24)] have relied on GPT-4 for harmfulness evaluation,
    we feel this approach is not well validated empirically and can be very expensive
    for extensive evaluations. Instead, we use the same approach as [[23](#bib.bib23)]
    and train our own harmfulness classifier on the 330k samples of safe and unsafe
    question-answer pairs from BeaverTails which allows us to compare our classifier
    performance with human annotations. We train a deberta-v3-xsmall [[51](#bib.bib51)]
    model for 4 epochs on this dataset using a batch size of 128, Adam optimizer,
    and learning rate of $6\times 10^{-6}$. Our classifier achieves an F1 score of
    0.87.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Decoding Trust Toxicity Split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the “toxicity” fine-tuning attacks and defences we used the RealToxicityPrompts
    (RTP) [[26](#bib.bib26)] split from the DecodingTrust benchmark [[25](#bib.bib25)]
    since it filters RTP for toxic and non-toxic continuations and provides a framework
    for evaluating toxicity generation in LLMs by using a prompt asking the LLM to
    continue a toxic snippet. As above, since we need paired refusals for our method,
    we filter the content for toxic continuations only and generate continuations
    from llama2-7b-chat resulting in 351 samples. To evaluate the toxicity of continuations,
    we use the same approach as [[25](#bib.bib25)] and leverage the Jigsaw Perspective
    API [[17](#bib.bib17)]. This toxicity evaluator was originally designed for content
    moderation purposes for text very similar to RTP and is widely used for toxicity
    evaluation in similar contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Example of DecodingTrust Toxic Continuation
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning: example contains swear words.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: D.3 Eleuther LM Harness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For stability evaluations (Table [3](#S4.T3 "Table 3 ‣ 4.2 Stability ‣ 4 Experiments
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")),
    we use tasks drawn from the Eleuther LM Harness⁷⁷7[https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    which is a common tool for the evaluation of LLMs general zero and few-shot capability
    post-training. We selected the top tasks used for evaluating language models on
    the Huggingface open LLM leaderboard ⁸⁸8[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    These were TruthfulQA [[31](#bib.bib31)], MMLU [[32](#bib.bib32)], Hellaswag [[33](#bib.bib33)],
    and ARC (easy) [[34](#bib.bib34)]. We also evaluate changes in the model’s capabilities
    on domains related to harmfulness on the Ethics [[35](#bib.bib35)] and Crows S
    pairs [[36](#bib.bib36)] that are found in the LM harness (additional safety evaluations
    are performed in [appendix E](#A5 "Appendix E Additional Safety and Harmfulness
    Evaluations ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: D.4 GEM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For trainability (whether the defended model can be continued to be trained
    on harmless tasks), we select two datasets from the GEM benchmark [[37](#bib.bib37)]
    which is designed to evaluate natural language generation. The datasets we selected
    were ViGGO (video game review [[52](#bib.bib52)] 5.1k train/1.08k test), E2E NLG
    (restaurant dialogue [[53](#bib.bib53)] 33.5k train/1.85k test), DART (Multiple
    [[54](#bib.bib54)] 62.7k train/5.1k test), CACAPO (Bilingual Dutch/English News
    [[55](#bib.bib55)] 15.k train/3.03k test), Conversational Weather (Weather Reports
    [[56](#bib.bib56)] 25.4k train/3.1k test) and we used the text-to-data task where
    the model must generate some abstract meaning representation or structured data
    representation given natural language texts. We chose this because it wasn’t something
    llama2-7b-chat was good at doing before training and training produced a large
    increase in the ROUGE-1 scores (see the low initial scores in [table 4](#S4.T4
    "In 4.3 Trainability ‣ 4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")). The reader will notice that while we use the exact
    same training set-up as the attack setting many of these datasets are larger than
    our attack sample set, we point out that ViGGO is smaller than our attack set
    and training is still effective. For some examples of what these tasks look like,
    see below:'
  prefs: []
  type: TYPE_NORMAL
- en: Example of ViGGO text-to-data task
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Example of E2E NLG text-to-data task
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Appendix E Additional Safety and Harmfulness Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to understand the impact of our method on more general LLM Safety
    we performed three additional evaluations: benign and identity-shifting fine-tuning
    attacks ([§ E.3](#A5.SS3 "E.3 Benign and Identity Shifting fine-tuning Attacks
    ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")), language model bias evaluation
    ([§ E.4](#A5.SS4 "E.4 Bias Evaluation with ROBBIE ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")), exaggerated safety evaluations ([§ E.5](#A5.SS5 "E.5 Exaggerated
    Safety with XSTest ‣ Appendix E Additional Safety and Harmfulness Evaluations
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")),
    and adversarial attacks ([§ E.6](#A5.SS6 "E.6 Adversarial Attacks with HarmBench
    ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")). We also present empirical
    results on a few successful attacks on RepNoise-hardened models ([§ E.1](#A5.SS1
    "E.1 Stronger Attack on RepNoise ‣ Appendix E Additional Safety and Harmfulness
    Evaluations ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")), and cross-domain generalization ([§ E.2](#A5.SS2 "E.2 Cross-Domain
    Generalization ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"))'
  prefs: []
  type: TYPE_NORMAL
- en: E.1 Stronger Attack on RepNoise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond the results in [table 1](#S4.T1 "In 4.1 Resistance ‣ 4 Experiments ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs") we were
    able to find an attack that defeats our method starting at $3\times 10^{-4}@10k$
    for 1 epoch but not at learning rates before. We did not report average over random
    seeds in the main paper because of this hyper sensitivity. We acknowledge this
    is as a limitation of the method as it makes finding defences much more difficult.
    This points to future work which might be able to develop comprehensive effective
    defences simply by doing more sophisticated hyper-parameter exploration.
  prefs: []
  type: TYPE_NORMAL
- en: E.2 Cross-Domain Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we demonstrated that our method can generalize across different subsets
    [table 5](#S4.T5 "In 4.4 Generalization ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") and number of samples used
    [table 17](#A10.T17 "In What is the impact of sample size on defence? ‣ Appendix
    J Ablation Study ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs"), we were additionally curious whether our method provides a ‘cross-domain’
    defence, meaning that performing the RepNoise defence using samples from one task
    could provide defence against samples drawn from an unrelated task.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate how well our method generalizes between the BeaverTails dataset
    and the RealToxicitiyPrompts (RTP) split from the DecodingTrust benchmark [[25](#bib.bib25)].
    While BeaverTails contains potentially unsafe question-answer pairs, RTP consists
    of prompts likely to be followed by toxic completions which is a very distinct
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: '| immunization set | attack set | pre-attack | $3\times 10^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| None | Decoding Trust | 0.24 | 0.40 | 0.74 | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Decoding Trust | Decoding Trust | 0.17 | 0.00 | 0.05 | 0.07 |'
  prefs: []
  type: TYPE_TB
- en: '| BeaverTails | Decoding Trust | 0.15 | 0.63 | 0.65 | 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| None | BeaverTails | 0.05 | 0.47 | 0.73 | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: '| BeaverTails | BeaverTails | 0.08 | 0.13 | 0.10 | 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Decoding Trust | BeaverTails | 0.04 | 0.05 | 0.43 | 0.64 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Cross-domain generalization: Harmfulness scores after attacks with
    learning rates $\in\{$ and immunization using RepNoise on different datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [7](#A5.T7 "Table 7 ‣ E.2 Cross-Domain Generalization ‣ Appendix E Additional
    Safety and Harmfulness Evaluations ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs") illustrates that defence trained on DecodingTrust
    improves upon the base model’s resistance against weak attacks using BeaverTails.
    However, defences trained on BeaverTails actually decrease resistance against
    weak attacks using DecodingTrust. On both BeaverTails and DecodingTrust, it is
    much more effective to defend using the same dataset used for the attack. This
    means that while RepNoise does provide generalization where the defender doesn’t
    have access to the same samples as the attacker, RepNoise does not appear to provide
    resistance to cross-distribution attacks where we have no samples at all from
    the domain of the attack. We don’t find this a surprising result or major limitation
    given that out-of-domain performance is an expected limitation of current neural
    methods. However, it does mean that defenders using our method will need to be
    sure they comprehensively collect defence samples from the domains they want to
    prevent training on.
  prefs: []
  type: TYPE_NORMAL
- en: E.3 Benign and Identity Shifting fine-tuning Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Qi et al. [[8](#bib.bib8)] observed that even benign fine-tuning could accidentally
    make models more harmful by increasing the likelihood of following harmful instructions.
    Using the same setting of fine-tuning models on 10 harmless samples illustrating
    an absolutely obedient agent (Identity shifting) and 52,002 samples from the (Benign)
    Alpaca instruction-following dataset, we find that RepNoise defends against both
    benign and identity shifting attacks. On the identity shifting attack (10 epochs),
    the base lama2-7b-chat’s outputs go from a harmfulness rating of $1.02$, RepNoise
    was slightly more susceptible to the benign fine-tuning attack where the model
    went from $1.16$ which is still far from a large increase in harmfulness.
  prefs: []
  type: TYPE_NORMAL
- en: E.4 Bias Evaluation with ROBBIE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ROBBIE suite of robust bias evaluation [[58](#bib.bib58)] allows us to ask
    whether our defence has any impact on the overall demographic bias of the model.
    We used 100 random samples drawn from each ROBBIE benchmark and the Jigsaw Perspective
    API evaluator [[17](#bib.bib17)] to evaluate bias (in addition to the other evaluation
    procedures presented in [[58](#bib.bib58)]). Using the RepNoise defence with the
    settings presented in the main paper, we find that our method doesn’t have any
    significant impact on the bias of the original model (Mann-Whitney $U$). Despite
    the fact that we are removing information about harmful representations, we cannot
    say this makes the model appreciably less biased.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | bold | holisticbiasr | realtoxicityprompts | regard | safetyscore |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| llama2-7b-chat | 0.07 | 0.05 | 0.04 | 0.19 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.08 | 0.05 | 0.05 | 0.18 | 0.07 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: No significant differences are observed for the impact of our defence
    on bias.'
  prefs: []
  type: TYPE_NORMAL
- en: E.5 Exaggerated Safety with XSTest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Röttger et al. [[59](#bib.bib59)] provides a set of evaluations for understanding
    "exaggerated safety" where models might refuse to answer harmless questions which
    only superficially seem unsafe for example asking about the definition of a bomb
    rather than how to construct a bomb. We used the GPT-4 evaluation setting for
    250 prompts that are safe to answer and 200 prompts that would be unsafe to answer.
    In [table 9](#A5.T9 "In E.5 Exaggerated Safety with XSTest ‣ Appendix E Additional
    Safety and Harmfulness Evaluations ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"), we observed that applying RepNoise does result
    in a small but significant increase in safe refusals ($\chi^{2}$). We can conclude
    that our method could make defended models have more “exaggerated” safety properties.
  prefs: []
  type: TYPE_NORMAL
- en: '| Safe Prompts | Refusal Rate (%) | Partial Refusal Rate (%) | Compliance Rate
    (%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| llama2-7b-chat | 7.95 | 3.97 | 88.08 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 17.71 | 9.38 | 72.92. |'
  prefs: []
  type: TYPE_TB
- en: '| Unsafe Prompts | Refusal Rate (%) | Partial Refusal Rate (%) | Compliance
    Rate (%) |'
  prefs: []
  type: TYPE_TB
- en: '| llama2-7b-chat | 86.49 | 5.41 | 8.11 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 79.14 | 6.47 | 14.39 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: RepNoise increases the number of refusals to safe answers.'
  prefs: []
  type: TYPE_NORMAL
- en: E.6 Adversarial Attacks with HarmBench
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How does our method impact inference-time adversarial attacks like jailbreaks?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We examine this question using the HarmBench [[7](#bib.bib7)] benchmark with
    the following methods GCG [[60](#bib.bib60)], ZeroShot [[61](#bib.bib61)], HumanJailbreak
    [[62](#bib.bib62), [63](#bib.bib63)], and DirectRequest [[61](#bib.bib61)]. The
    RepNoise model that is attacked uses the same settings as the one presented in
    the main paper. For demonstration purposes, we use the “harmful” and “illegal”
    subsets of HarmBench which consists of 64 test cases that a language model should
    normally refuse to answer since the answer would be harmful. Table [10](#A5.T10
    "Table 10 ‣ How does our method impact inference-time adversarial attacks like
    jailbreaks? ‣ E.6 Adversarial Attacks with HarmBench ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") illustrates that our method does provide a small decrease
    in susceptibility to GCG-based attacks but it is not statistically significant
    ($\chi^{2}$). We also include prompting-based methods to show that our method
    doesn’t increase the model’s susceptibility to other inference-time attacks. Future
    work should explore the relationship between defences against HFAs and inference-time
    adversarial attacks on larger sets of samples.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | GCG | ZeroShot | HumanJailbreak | DirectRequest |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| llama2-7b-chat | 11% | 0% | 0% | % |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 5% | 0% | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: A noticeable drop is observed in the attack success rate of GCG when
    attempted after performing our RepNoise defence. No increase in attack success
    is on prompting-based attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Security Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security Vectors [[28](#bib.bib28)] is a defence where the defender has access
    to and complete control over the fine-tuning process. The defence consists of
    training a LoRA adapter [[64](#bib.bib64)] represented by the parameters $\theta_{s}$
    from the base model are being trained. The authors base their method on the observations
    from [[44](#bib.bib44)] that if the training loss is already very small to begin
    with, then little learning will take place.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our experiments, we trained the LoRA adapter with a $1\times 10^{-3}$ epoch
    on [eq. 1](#S2.E1 "In Harmful Fine-tuning Attack (HFA): ‣ 2 Harmful Fine-tuning
    and Defence Criteria ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") to minimize causal language modelling loss on our 10k harmful samples
    from the BeaverTails dataset. We believed that this would be a fair comparison
    because it the same number of samples RepNoise defence uses. We did not perform
    the min-min bi-level optimisation procedure as details for this process were missing
    from the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Harmfulness probes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeat the probing experiments from Section [5](#S5 "5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") for
    RepNoise when we use $\beta=0.001$. This setting leads to higher resistance by
    putting less importance on increasing the loss on harmful examples. Again we train
    a linear probe on the activations for 15k samples of question-answer pairs from
    BeaverTails to predict whether the answer is harmful or not. We measure the accuracy
    of the resulting probe to indicate how much information the representations at
    each layer of a model contain about harmfulness. However, it should be noted that
    probes have been criticised as a interpretability method [[65](#bib.bib65)] due
    to their susceptibility to spurious correlations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7bd2dc0ec01053d75aa045e2df6253b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9dd7cdddefdfeba6de77aa5f1d65a9a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Harmful probe accuracy on (a) base model and models trained with
    RepNoise and adversarial Loss, and (b) base model, RepNoise model and an attacked
    RepNoise model'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6(a)](#A7.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ Appendix G Harmfulness probes
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") shows
    that RepNoise slightly reduces the information about harmfulness compared to the
    base model and the model defended by the Adversarial Loss. The average accuracy
    across layers of RepNoise was 0.003 lower than for the base model (Students $t$
    term plays an important role in removing information about harmfulness.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6(b)](#A7.F6.sf2 "Figure 6(b) ‣ Figure 6 ‣ Appendix G Harmfulness probes
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") underlines
    the finding from Section [5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") that HFAs do not increase the
    amount of information about harmfulness. However, in the setting with $\beta=0.001$
    is able to navigate the weights to an advantageous position in the loss landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Causal Analysis of Layer Effect on Defence Effectiveness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | $3\times 10^{-5}$ @ 1k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Undefended Model | 0.47 | 0.74 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| All Layers | 0.08 | 0.12 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze LM Head | 0.08 | 0.10 | 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Layers 20-31 | 0.10 | 0.13 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Last Layer | 0.08 | 0.67 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Layers 10-20 | 0.13 | 0.55 | 0.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Freeze Layers 0-10 | 0.73 | 0.73 | 0.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Freezing earlier layers prevents effective defence indicating that
    the “depth” of the defence is critical.'
  prefs: []
  type: TYPE_NORMAL
- en: In the main paper, we hypothesize that the effectiveness of our defence is due
    to its depth. By depth, we mean how many layers down we are removing information
    about harmful representations. We can test whether this is the case by freezing
    layers during RepNoise and then performing our attacks from the main paper. Using
    a 32-layer LLM (llama2-7b-chat we freeze the LM Head, the last layers (32, 20-31),
    the middle layers (10-20) and the last layers (0-10). [table 6](#S5.T6 "In 5 Mechanistic
    Analysis ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs") shows that freezing the LM head or “unembed” layer makes little difference.
    We have a similar finding for freezing the last layer, except in the case of longer
    attacks, which is interesting given that a simple adversarial loss defence makes
    the most changes in the last layer [fig. 2](#S5.F2 "In 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"). Finally,
    we see that freezing the middle layers starts to degrade the effectiveness of
    the defence and freezing the first ten layers completely ruins the effectiveness
    of the defence. This shows that RepNoise conducts important operations on early
    layers of the model and thus provides a “deep” defence.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Analysis of Attacked Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We extend our analysis from [§ 5](#S5 "5 Mechanistic Analysis ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") by presenting an illustration
    of what the token probability distributions and PCA characterization look like
    on successfully attacked models after performing a defence. These results use
    the same setting from [§ 5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") using 100 harmful and harmless
    samples from the BeaverTails harmfulQA task. The defence performed with adversarial
    loss is successfully attacked at $8\times 10^{-5}$ @ 10k. [Figure 7](#A9.F7 "In
    Appendix I Analysis of Attacked Models ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs") reveals that the probability distribution of drawing
    harmful token sequences after a successful attack is largely the same as the distribution
    from the original attacked model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65de9248fa3f257007aefd3fbd86b501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Log probability of harmful and harmless sequences across layers.
    Notice how adversarial loss mostly depromotes harmful tokens towards the last
    layer, while this is done more evenly across layers for RepNoise.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [8](#A9.F8 "Figure 8 ‣ Appendix I Analysis of Attacked Models ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") shows a PCA of 100
    harmful and harmless samples. It indicates that the representation space between
    an attacked and unattacked base model is not that different. For the adversarial
    loss defence, we discussed this representation space change earlier but we point
    out that the representation space largely returns to a similar space as the base
    model after a successful attack. As for RepNoise, observe that in order for the
    attack to be successful we produce a representation space where both harmful and
    harmless representations are largely collapsed on some kind of manifold. This
    observation could help us develop further extensions to RepNoise which make it
    more robust.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dfd0843b21e9d6063e623dc1a7e99cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: PCA across 100 harmful and harmless samples from BeaverTails on the
    activations of the last layer of both attacked and unattacked models.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix J Ablation Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | $3\times 10^{-5}$ @ 10k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Our Method | 0.08 | 0.11 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| w/ noise | 0.08 | 0.32 | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| w/ ascent | 0.77 | 0.76 | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Ablation study showing the effect of removing the noise and ascent
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: Is noise loss necessary?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We performed an ablation study ([table 12](#A10.T12 "In Appendix J Ablation
    Study ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"))
    removing the noise term and the gradient ascent terms. We find that both the noise
    and gradient ascent term are required to construct strong defences. Note that
    the noise term by itself without ascent results in a model that is even worse
    at defence than the original base model. We believe that this is the case because
    simply noising harmful representations without explicitly trying to remove their
    predictive power could be contributing to improving the minimality properties
    of the representations themselves (the ideal property that representations contain
    as little information about the input as possible are more effective), see the
    relationship between learnability and minimality for representation learning in
    [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: RepNoise is very sensitive
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: During our investigations we found that our method is unfortunately very sensitive
    to hyperparameter variations, requiring extensive hyperparameter search to be
    effective. We illustrate these shifts in Table [13](#A10.T13 "Table 13 ‣ RepNoise
    is very sensitive ‣ Appendix J Ablation Study ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs"),[14](#A10.T14 "Table 14 ‣ RepNoise is very
    sensitive ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"),[15](#A10.T15 "Table 15 ‣ RepNoise is very sensitive
    ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs"), and [table 16](#A10.T16 "In RepNoise is very sensitive
    ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") while searching for the hyperparameters for the model whose
    results are in the main body of the paper. Generally, the $\alpha$ value for ascent
    made little impact so it is not recorded here.
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning Rate | $2\times 10^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.12 | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Learning rate study, even slightly larger learning rates result in
    ineffective defences. Results are reported on an $8\times 10^{-5}$ @ 10k sample
    attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Epochs | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.12 | 0.78 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Increasing the number of epochs of the defence results in a model
    that is easily attacked. Results are reported on an $8\times 10^{-5}$ @ 10k sample
    attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\beta$ | 0.001 | 0.01 | 0.0001 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.12 | 0.38 | 0.75 | 0.65 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Similar to the above results, the $beta$ @ 10k sample attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Random Seed | $6\times 10^{-5}$ @ 10k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 0.12 | 0.13 | 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.12 | 0.11 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 0.09 | 0.79 | 0.77 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: Our method is even quite sensitive to the random seed used'
  prefs: []
  type: TYPE_NORMAL
- en: What is the impact of sample size on defence?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While performing our defence on multiple epochs appears to have a negative effect,
    possibly again due to working against minimality, we did notice the number of
    samples has a major impact on the quality of defence. We show this in the sample
    ablation in [table 17](#A10.T17 "In What is the impact of sample size on defence?
    ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs"). This table indicates that future work based on our current
    method could simply experiment with more extensive data collection and augmentation
    to improve our defence. Additional work could be done to make defence methods
    more sample-efficient. Unfortunately, our method relies on paired refusal data
    as we observed without this defences were much more fragile, however, future work
    could also investigate methods that don’t require paired refusal data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack Strength | 1k | 2.5k | 5k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $3\times 10^{-5}$ @ 1k | 0.28 | 0.10 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| $3\times 10^{-5}$ @ 10k | 0.68 | 0.10 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| $6\times 10^{-5}$ @ 1k | 0.60 | 0.10 | 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| $6\times 10^{-5}$ @ 10k | 0.72 | 0.12 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $8\times 10^{-5}$ @ 1k | 0.70 | 0.10 | 0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| $8\times 10^{-5}$ @ 10k | 0.73 | 0.68 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17: Sample ablation using only 1k, 2.5k, or 5k samples for training our
    defence (our method in the main paper uses 10k samples unless specified otherwise).
    The effectiveness of a defence has a strong relationship with the number of samples
    used to train the defence.'
  prefs: []
  type: TYPE_NORMAL
- en: What if we use nucleus sampling?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, we investigate the effect of sampling on our defence. In [table 18](#A10.T18
    "In What if we use nucleus sampling? ‣ Appendix J Ablation Study ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") we compare the effect
    of using greedy or nucleus [[66](#bib.bib66)] sampling on attack effectiveness.
    We see that there is very little difference depending on the sampling technique.
  prefs: []
  type: TYPE_NORMAL
- en: '| Sampling Method | Greedy | Nucleus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $6\times 10^{-5}$ @ 10k | 0.13 | 0.13 |'
  prefs: []
  type: TYPE_TB
- en: '| $8\times 10^{-5}$ @ 1k | 0.11 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| $8\times 10^{-5}$ @ 10k | 0.12 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 18: Sampling study comparing the use of greedy or nucleus sampling, there
    is very little difference of the attack effectiveness and the sampling method
    used.'
  prefs: []
  type: TYPE_NORMAL
- en: We corroborate this finding by illustrating the mean log probabilities of 100
    harmful and harmless sequences at the last layer of our base llama model, a superficial
    defence like adversarial loss and our method ([fig. 9](#A10.F9 "In What if we
    use nucleus sampling? ‣ Appendix J Ablation Study ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")). We observe that both defences make the
    likelihood of drawing tokens for harmful sequences much lower than the base model
    or a successfully attacked model. Naturally, successful attacks decrease the divergence
    in distributions between harmful and harmless sequences. We could use this observation
    to investigate explicitly leveraging distributional distance losses in order to
    make closing this gap more difficult for the attacker.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3a5a7a05e0acca90d3535919b9d14829.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Log probability of harmful and harmless sequences for the last layer
    of each model. The probability density for harmful sequences in RepNoise is much
    lower than other methods. Please note the axes are different so that we are able
    to illustrate these differences in scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix K Additional Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We validated our approach for additional models by performing RepNoise on the
    larger llama2-13b-chat model and a series of smaller Qwen 1.5 models (0.5B to
    7B) [[67](#bib.bib67)]. We evaluate these models using the same attack settings
    as [table 1](#S4.T1 "In 4.1 Resistance ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs"). As mentioned above, one of
    the limitations of our method is that it requires very extensive hyperparameter
    tuning. For each model, we performed a grid search across the following learning
    rates ($1\times 10^{-3}$ to be the most effective. For llama-13b-chat, we similarly
    found a higher learning rate of $1\times 10^{-3}$. While our results are effective
    in these settings, we highlight the need for stronger more comprehensive attacks
    as well as future work that makes RepNoise require less extensive hyperparameter
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '| LR |  | $3\times 10^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | pre-attack | 1k | 10k | 1k | 10k | 1k | 10k |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| llama2-7b-chat | 0.05 | 0.47 | 0.74 | 0.73 | 0.72 | 0.74 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.05 | 0.08 | 0.12 | 0.1 | 0.13 | 0.11 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| llama2-13b-chat | 0.07 | 0.79 | 0.76 | 0.73 | 0.73 | 0.75 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-0.5B-chat | 0.00 | 0.50 | 0.66 | 0.68 | 0.71 | 0.70 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.06 | 0.07 | 0.10 | 0.09 | 0.00 | 0.12 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-1.5B-chat | 0.00 | 0.72 | 0.74 | 0.76 | 0.76 | 0.75 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.02 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-4B-chat | 0.0 | 0.72 | 0.74 | 0.76 | 0.75 | 0.77 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.09 | 0.02 | 0.00 | 0.02 | 0.04 | 0.00 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-7B-chat | 0.08 | 0.70 | 0.72 | 0.74 | 0.76 | 0.76 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| RepNoise | 0.01 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 19: Results from various models with and without RepNoise applied. RepNoise
    is effective across a variety of types of models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the cases where the harmfulness score is very low this is often the case
    of a “self-destructing” model [[40](#bib.bib40)] where the modes outputs completely
    degrade as a result of training which is in contrast to a model that preserves
    fluency across the attack. These are illustrated below selected from random examples
    from the series of models above:'
  prefs: []
  type: TYPE_NORMAL
- en: Example of model outputs that preserve fluency during an attack
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Example of model outputs that “self-destruct” (become disfluent) during an attack
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Appendix L Statement on Compute Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We primarily used a single node with 4XA100 (80GB VRAM) GPUs for our results.
    Occasionally we used 4XA40 (40GB VRAM) GPU nodes as well as 1XA100 (40GB VRAM)
    from Google Colab.
  prefs: []
  type: TYPE_NORMAL
