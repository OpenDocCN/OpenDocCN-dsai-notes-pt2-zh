- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12264](https://ar5iv.labs.arxiv.org/html/2402.12264)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Oleksandr Balabanov    Hampus Linander
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fine-tuning large language models can improve task specific performance, although
    a general understanding of what the fine-tuned model has learned, forgotten and
    how to trust its predictions is still missing. We derive principled uncertainty
    quantification for fine-tuned LLMs with posterior approximations using computationally
    efficient low-rank adaptation ensembles. We analyze three common multiple-choice
    datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative
    and qualitative conclusions on their perceived complexity and model efficacy on
    the different target domains during and after fine-tuning. In particular, backed
    by the numerical experiments, we hypothesise about signals from entropic uncertainty
    measures for data domains that are inherently difficult for a given architecture
    to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) learns conditional distributions of vocabularies
    useful for generative tasks, text classification and code completion. Trained
    on a corpus of sequences produced by humans, such as natural language text and
    program source code, these models have shown remarkable capabilities (Bubeck et al.,
    [2023](#bib.bib7); Touvron et al., [2023a](#bib.bib65), [b](#bib.bib66); Jiang
    et al., [2023](#bib.bib36)). As the applications of LLMs continue to explode,
    informed use of these models hinge on a proper understanding of the uncertainty
    of their output (Huang et al., [2023](#bib.bib35); Kuhn et al., [2023](#bib.bib39);
    Malinin & Gales, [2021](#bib.bib49); Ren et al., [2023](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: To align an LLM towards particular needs, such as answering factual questions
    and instructions, a common approach is to fine-tune the model using a specialised
    dataset targetting human interaction (Ouyang et al., [2022](#bib.bib55)). Fine-tuning
    involves additional training of a pre-trained LLM on a smaller dataset generated
    with humans (Zhong et al., [2021](#bib.bib82)) or by other LLMs (Peng et al.,
    [2023](#bib.bib58); Zhang et al., [2023](#bib.bib80)). By fine-tuning, the model
    adapts its parameters to better capture the nuances, vocabulary, and style of
    the target domain(Peng et al., [2023](#bib.bib58)).
  prefs: []
  type: TYPE_NORMAL
- en: Even full-model fine-tuning often requires orders of magnitude less training
    time, but still incur the same computational complexity and memory usage. Prior
    studies have shown that pre-trained language models can effectively adapt to specific
    tasks within smaller parameter subspaces (Li et al., [2018](#bib.bib40); Aghajanyan
    et al., [2020](#bib.bib1)), indicating an inherently low rank during the fine-tuning
    process. To further reduce the training time and computational complexity of fine-tuning,
    a common method used is Low-Rank Adaptation (LoRA) (Hu et al., [2021](#bib.bib34)).
    As one member of the family of methods typically referred to as parameter efficient
    fine-tuning (PEFT), LoRA effectively reduces the number of parameters requiring
    training by keeping the pre-trained weights unchanged and integrating low-rank
    trainable matrices into each layer of the LLM transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key questions arise after fine-tuning: What areas of knowledge remain outside
    the model’s expertise? What knowledge is retained from the pre-trained model?
    What knowledge is gained during the fine-tuning process on the target dataset?
    These questions are fundamental in guiding us towards a more reliable, interpretable
    and trustworthy application of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: We contribute to addressing these questions by analyzing the uncertainty in
    LLMs that have been fine-tuned using LoRA for answering multiple-choice question
    answers (QAs). These tasks involve definite single-token target labels, which
    significantly simplifies the analysis while still requiring the model to have
    a thorough understanding of context. We explore how uncertainty estimates, divided
    into epistemic model uncertainty and aleatoric data uncertainty, can be utilized
    to observe changes in the models knowledge regarding the presented data. We use
    predictive entropy and mutual information to quantify uncertainty, the former
    containing both aleatoric and epistemic contributions whereas the latter is solely
    epistemic. These entropic uncertainty measures are calculated for a Bayesian posterior
    derived from an ensemble of LLMs fine-tuned with LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: We use the pre-trained Mistral-7b (Jiang et al., [2023](#bib.bib36)) model as
    a prior for the weight distribution, fine-tuning on the CommonsenseQA (CQA) (Talmor
    et al., [2019](#bib.bib62)), and the Social Sciences as well as STEM components
    from MMLU (Hendrycks et al., [2021](#bib.bib29)) multiple-choice QA datasets.
    By quantifying the evolution of knowledge using predictive entropy and mutual
    information, we show how these measures can be used to reason about complexity
    of the dataset, and expected model efficacy on the target domain. Code is available
    (Balabanov & Linander, [2024](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We derive posterior approximations for LLMs fine-tuned on target datasets using
    ensembles of LoRA members. On the way, we provide a Bayesian interpretation of
    fine-tuning, early-stopping and conditional generative tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We show how uncertainty quantification with entropic measures using the ensemble
    posteriors can be used to reason about dataset complexity and model efficacy on
    the target domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyze three common multi-choice QA datasets, CQA, MMLU STEM and MMLU Social
    Sciences, using LoRA ensemble posteriors derived from a pre-trained Mistral-7b
    model. The analyzis of the evolution of the entropic uncertainty measures during
    fine-tuning lets us draw quantitative conclusions on the relative complexity,
    out-of-distribution behaviour, and model efficacy on the different target domains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Uncertainty quantification in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There has been significant interest in uncertainty quantification across various
    tasks and domains in neural networks (Gal & Ghahramani, [2015](#bib.bib20), [2016](#bib.bib21);
    Malinin & Gales, [2018](#bib.bib48); Ovadia et al., [2019b](#bib.bib57); Malinin
    et al., [2021](#bib.bib50); Lin et al., [2022](#bib.bib41); Kuhn et al., [2023](#bib.bib39);
    Lin et al., [2023](#bib.bib42)).
  prefs: []
  type: TYPE_NORMAL
- en: This interest extends to the realm of Large Language Models (LLMs), where accurately
    quantifying prediction uncertainty is a key focus (Xiao et al., [2022a](#bib.bib73);
    Lin et al., [2022](#bib.bib41); Mielke et al., [2022](#bib.bib52); Chen & Mueller,
    [2023](#bib.bib9); Duan et al., [2023](#bib.bib15); Huang et al., [2023](#bib.bib35)).
    The application of LLMs in generative tasks introduces unique challenges, notably
    in measuring uncertainty of the generative outputs. (Liu et al., [2019](#bib.bib45);
    Malinin & Gales, [2021](#bib.bib49); Kuhn et al., [2023](#bib.bib39); Lin et al.,
    [2023](#bib.bib42)). The disentanglement of uncertainty into aleatoric and epistemic
    was recently discussed in the context of LLMs (Hou et al., [2023](#bib.bib31)).
    However, it was done via ensembling of the model inputs rather than the model
    instances, and not in the context of fine-tuning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Fine-tuning in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning has recently become an integral part of the LLM ecosystem where
    it is used to target specific tasks such as general instruction-following model,
    through methods like Reinforcement Learning from Human Feedback (Houlsby et al.,
    [2019](#bib.bib33); Hu et al., [2021](#bib.bib34); Liu et al., [2019](#bib.bib45);
    Ding et al., [2022](#bib.bib13), [2023](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: The large computational demands of training and fine-tuning LLMs have resulted
    in development of more efficient techniques, known as parameter-efficient fine-tuning
    (PEFT)(Liu et al., [2022](#bib.bib44); Ding et al., [2022](#bib.bib13), [2023](#bib.bib14);
    Shi & Lipani, [2024](#bib.bib60)). These methods typically involve training a
    small number of additional parameters on top of a fixed, pre-trained LLM, with
    a prominent approach being the use of low-rank adapters (LoRA) for each weight
    matrix (Hu et al., [2021](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 LLMs within Bayesian methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayesian inference techniques in neural networks have a long-standing history
    (Denker et al., [1987](#bib.bib12); Tishby et al., [1989](#bib.bib64); Buntine
    & Weigend, [1991](#bib.bib8); MacKay, [1991](#bib.bib46)). These methods establish
    a systematic approach to derive reliable and interpretable uncertainty estimates.
    Previous research on Bayesian language models has been primarily concentrated
    on the pretraining phase of language models, rather than on their fine-tuning
    (Tran et al., [2019](#bib.bib67); Xue et al., [2021](#bib.bib75); Cinquin et al.,
    [2021](#bib.bib11); Zhang et al., [2018](#bib.bib78); Chen & Li, [2024](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies have begun exploring the fine-tuning of language models using
    a Bayesian approach. For instance, in (Fan et al., [2020](#bib.bib17)) and (Zhang
    et al., [2021](#bib.bib81)), the attention modules are sampled either from simplex-constrained
    attention distributions or using Bayesian Belief Networks. Neither of these studies
    address entropic uncertainty measures, nor do they utilize ensembling or PEFT
    methods for posterior approximation.
  prefs: []
  type: TYPE_NORMAL
- en: (Yang et al., [2024](#bib.bib76)) employs a post-hoc Laplace approximation (Mackay,
    [1992](#bib.bib47)) to model LoRA parameters for fine-tuning. While this study
    does use LoRA for fine-tuning, it does not explore entropic uncertainty measures
    and focuses on the posterior over LoRA parameters rather than the model as a whole,
    which could limit straightforward Bayesian interpretability. There are also strong
    indications that deep ensembles provide more accurate posteriors compared to single-model
    stochastic methods like Laplace and Monte Carlo dropout (Gustafsson et al., [2019](#bib.bib25);
    Ovadia et al., [2019a](#bib.bib56); Fort et al., [2019](#bib.bib18); Wilson &
    Izmailov, [2020](#bib.bib71); Dwaracherla et al., [2022](#bib.bib16); Balabanov
    et al., [2023](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Ensembling LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two recent studies have explored the use of ensembling in fine-tuning LLMs,
    specifically focusing on full model fine-tuning method where all weights are optimized
    (Gleave & Irving, [2022](#bib.bib23); Sun et al., [2022](#bib.bib61)). This approach
    has large memory overhead by construction. While uncertainty quantification using
    variance across the ensembles is considered, their methods lack a Bayesian formalism
    that could provide a more grounded interpretation and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative method, BatchEnsemble (Wen et al., [2020](#bib.bib70)), employs
    a base model with modifications through multiplicative, component-specific rank-1
    matrices. This method has been applied to LLMs, but in the context of pre-training
    rather than fine-tuning (Tran et al., [2022](#bib.bib68)).
  prefs: []
  type: TYPE_NORMAL
- en: There are also recent endeavors using LoRA ensembles for fine-tuning LLMs (Wang
    et al., [2023](#bib.bib69); Zhai et al., [2023](#bib.bib77)). Although these studies
    consider uncertainty quantification, they do so without employing a Bayesian framework
    and do not clearly separate epistemic and aleatoric components, critial for data
    interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Bayesian Deep Learning for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When formulated as a conditional generative model, an LLM can be viewed as a
    function taking a sequence of tokens $s^{*}$ from the vocabulary as output. Such
    a model is typically trained to predict the next token in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The prediction should be consistent with our observed data. This is encapsulated
    in the predictive probability distribution $p(t^{*}|s^{*},\mathcal{D})$.
  prefs: []
  type: TYPE_NORMAL
- en: Given a model with parameters $\theta$ can be expressed as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: The posterior parameter distribution $p(\theta|\mathcal{D})$ reflects aleatoric
    uncertainty inherent in the data. Thus, as any data driven model, the output of
    LLMs has an associated uncertainty stemming from an uncertainty about model parameters,
    as well as an uncertainty regarding the correct output given the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: An example of aleatoric uncertainty is given by the input sequence “The color
    of the sky is ”. One would expect a model trained on a large corpus of text to
    produce a predictive distribution with support on the colors. This posterior distribution
    tells us that the model is fairly certain that the next token is a color, but
    that there is an uncertainty about which color. The uncertainty about the color
    cannot be decreased by improving the training data, it is inherent in the input
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, aleatoric uncertainty depends on the architecture of the model. For
    example, when a current generation LLM is asked a question like “The square root
    of 123456789 equals ”, we expect the predictive distribution to exhibit high aleatoric
    uncertainty. Although the answer is definitive, and the model understands the
    context, it might still be incapable of providing the correct answer. This limitation
    might be inherent in the models architecture, and cannot be resolved through further
    training.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the input sequence “Large language models are ” presents
    a simple conditional completion task, but we would not trust the output predictions
    from a model trained on a text corpus compiled before the concept was introduced.
    This uncertainty is not captured in the output distribution, but rather in the
    uncertainty of the model parameters themselves. This epistemic uncertainty can
    be systematically calculated within a Bayesian formalism, where it corresponds
    to shape of the posterior distribution of model parameters conditioned on the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Separating the origin of uncertainty is important to make informed decisions
    about how to interpret the preditions of LLMs. Should we trust the output for
    a given sequence at all, and given such trust, what is the inherent uncertainty
    of the output?
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The objective of fine-tuning is to use a specialized dataset to further improve
    the knowledge of a generally pre-trained model for a given task. This process
    can be conceptualized as a conditional generative task on a target domain, denoted
    as $\mathcal{D}_{\text{fine-tuning}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning assumes prior knowledge about the models parameters, expressed as
    $p(\theta)$. By adjusting the prior variance $\lambda^{-1}$ implies that the pre-trained
    models knowledge is not utilized. As with any posterior considerations in a Bayesian
    formalism, fine-tuning requires careful adjustment of this parameter to target
    optimal performance for the target tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this formulation, there is no emphasis on the origin of the pre-trained
    parameters. Typically, for large language models, the training procedure consists
    of maximum likelihood optimization combined with supervised reinforcement learning
    (Touvron et al., [2023b](#bib.bib66)) and the specific details of the training
    procedure are often not publicly disclosed. In our formulation, we do not need
    to delve into these origins and instead formally take it as prior knowledge upon
    which we base our fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Posterior approximation methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Variational inference (Barber & Bishop, [1998](#bib.bib5); Graves, [2011](#bib.bib24);
    Blundell et al., [2015](#bib.bib6)) offers a computationally efficient approximation
    method for the Bayesian posterior. It involves exploring a family of distributions
    $q_{\omega}(\theta)$ in this set involves minimizing the Kullback-Leibler (KL)
    divergence $\text{KL}(q_{\omega}(\theta)\,||\,p(\theta|\mathcal{D}))$, can be
    expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\begin{split}&amp;\text{KL}(q_{\omega}(\theta)\,&#124;&#124;\,p(\theta&#124;\mathcal{D}))=\int\,d\theta\,q_{\omega}(\theta)\,\log\,\frac{q_{\omega}(\theta)}{p(\theta&#124;\mathcal{D})}\\
    &amp;=\text{KL}(q_{\omega}(\theta)\,&#124;&#124;\,p(\theta))-\mathbb{E}_{q_{\omega}(\theta)}[\log\,p(t&#124;\theta,s)]+C,\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}$$ |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{D}=\{(t,s)\}$. Minimizing the loss function given in Eq. ([2](#S4.E2
    "Equation 2 ‣ 4.2 Posterior approximation methods ‣ 4 Bayesian Deep Learning for
    LLMs ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")) results
    in an approximation of the posterior $p(\theta|\mathcal{D})$ and (2) the expected
    negative log likelihood (ENLL), a common loss metric for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization problem described in Eq. ([2](#S4.E2 "Equation 2 ‣ 4.2 Posterior
    approximation methods ‣ 4 Bayesian Deep Learning for LLMs ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")) does not specify the origin of the
    in-domain (observed) data $\mathcal{D}$, whereas for a typical training procedure,
    the prior is usually considered to be centered around the origin in parameter
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Early Stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early stopping can be used to enhance generalization to unseen in-domain data,
    thereby reducing the effect of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of Bayesian deep learning, the objective is to approximate the
    posterior distribution $p(\theta|\mathcal{D})$. Generally, $p(\theta|\mathcal{D}^{\text{train}})$
    only acknowledges the training data and struggles to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping improves the approximation of $p(\theta|\mathcal{D})$ using $\text{KL}(q_{\omega}(\theta)\,||\,p(\theta|\mathcal{D}))$,
    assuming it represents the entire task domain effectively and was not part of
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Deep Ensembles with Low-Rank Adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Low-Rank Adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Low-Rank Adaptation (LoRA) provides a computationally efficient alternative
    to full-model fine-tuning by keeping the pre-trained weight matrices static and
    adding trainable low-rank matrices into each transformer layer, thereby decreasing
    the total number of parameters requiring training (Hu et al., [2021](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: Given a pre-trained weight matrix $W_{\text{pretrained}}$ resides in $\mathbb{R}^{d\times
    r}$ is substantially smaller than both $d$ are tuned, while $W_{\text{pretrained}}$
    is kept unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 LoRA Deep Ensembles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use LoRA ensembles for approximating the posterior $p(\theta|\mathcal{D}_{\text{fine-tune}})$.
    Ensembles of LoRA members have been introduced recently (Wang et al., [2023](#bib.bib69)),
    here we provide a Bayesian treatment.
  prefs: []
  type: TYPE_NORMAL
- en: Deep ensembles can be explicitly interpreted within the Bayesian variational
    framework by assuming an ansatz in the form of a sum of sharply peaked distributions
    around parameter realizations $\omega_{k}$ indexing ensemble members (Hoffmann
    & Elster, [2021](#bib.bib30); Balabanov et al., [2023](#bib.bib4)). In this context,
    the variational KL loss simplifies to a sum of conventional single-member log
    likelihood loss terms, augmented with L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: We adopt the deep ensemble variational inference formulation to include LoRA
    reparametrization. The trainable low-rank LoRA matrices $A_{k}$ and $B_{k}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Prior Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The choice of an optimal prior is crucial for obtaining a posterior distribution
    that exhibits optimal performance. Our approach is to assume a normally distributed
    prior around the pre-trained model, $p(\theta)=N(\theta;\omega_{\text{pretrained}},\lambda^{-1}I_{\text{dim}[\theta]})$.
    This variance is a hyperparameter that must be carefully chosen based on specific
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: An inappropriate selection of the prior can lead to a posterior that performs
    poorly on the task at hand. Consequently, we adjust the variance of the prior
    with the aim of optimizing the posterior quality, as measured by the log likelihood
    on the validation data. Different choice for the priors variance results in adjusting
    L2 regularization loss according to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $A^{(i)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Uncertainty quantification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We quantify total predictive uncertainty, with both aleatoric and epistemic
    contributions, using predictive entropy
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $s^{*}$ are computed by averaging softmax outputs from network parameters
    $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: We quantify epistemic uncertainty by the average mutual Shannon information
    between the model parameters and a test data sample, given the training dataset
    (Mackay, [1992](#bib.bib47))
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\begin{split}&amp;\text{MI}(\theta,t^{*}&#124;s^{*},\mathcal{D})\\
    &amp;=H(\theta&#124;\mathcal{D})-\mathbb{E}_{t^{*}\sim p(t^{*}&#124;s^{*},\mathcal{D})}\left[H(\theta&#124;t^{*},s^{*},\mathcal{D})\right]\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=H(t^{*}&#124;s^{*},\mathcal{D})-\mathbb{E}_{\theta\sim p(\theta&#124;\mathcal{D})}\left[H(t^{*}&#124;s^{*},\theta)\right].\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}$$ |  | (5) |
  prefs: []
  type: TYPE_NORMAL
- en: The mutual information MI in Eq. ([5](#S6.E5 "Equation 5 ‣ 6 Uncertainty quantification
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")) is the
    expected change in the posterior parameter distribution when a new data point
    is added to the training set (first row of RHS) (Houlsby et al., [2011](#bib.bib32)),
    and can be conveniently calculated in terms of the entropy of the predictive distributions
    (second row of RHS).
  prefs: []
  type: TYPE_NORMAL
- en: Even though entropic uncertainty measures have been shown to contain valuable
    information for neural network uncertainty quantification (Linander et al., [2023](#bib.bib43)),
    one should be aware of unintuitive properties shown in recent studies (Wimmer
    et al., [2023](#bib.bib72)).
  prefs: []
  type: TYPE_NORMAL
- en: 7 Numerical Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate our methods on three multiple-choice QA datasets: CommonsenseQA
    (CQA) (Talmor et al., [2019](#bib.bib62)), and the Social Sciences (SS) as well
    as STEM components from MMLU (Hendrycks et al., [2021](#bib.bib29)). Detailed
    information about the training and validation (test) splits is available in Appendix
    [A](#A1 "Appendix A Datasets ‣ Uncertainty quantification in fine-tuned LLMs using
    LoRA ensembles").'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 LoRA Ensembles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We train LoRA ensembles using the pre-trained 7 billion parameter Mistral-7b
    model (Jiang et al., [2023](#bib.bib36)). The LoRA parametrization was implemented
    using PEFT (Mangrulkar et al., [2022](#bib.bib51)). Our ensembles contain 5 members
    $(M=5)$, which gives $3,407,872$ are initialized using a Kaiming-uniform distribution
    (He et al., [2015](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble members are trained using the prescribed regularisation from Section [5.3](#S5.SS3
    "5.3 Prior Selection ‣ 5 Deep Ensembles with Low-Rank Adaptation ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles"), corresponding to using
    the pretrained Mistral-7b model as a prior. We use the Adam optimizer, training
    for 10 epochs with learning step sizes of $5\cdot 10^{-6}$ and $10$ for the CQA
    and MMLU datasets, respectively. The training focuses solely on the single token
    output representing the answer, with all other token outputs being masked.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate performance metrics and uncertainty estimates, we reduced the output
    dimension from 32,000 to 6\. Five of these dimensions represent QA choice tokens
    (a, b, c, d, e), and the sixth aggregates the softmax prediction scores associated
    with all other tokens. We found that, even before fine-tuning, the pretrained
    Mistral-7b checkpoint assigns a near-zero probability to the sixths class with
    very few exceptions. This suggests that the model reliably comprehends the format
    of multiple-choice questions and answers, consistently producing appropriate tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75f79b6a85f697362b0bc5aa98af8b72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Performance of the LoRA ensembles trained and evaluated on either
    CQA, MMLU STEM, or MMLU SS dataset. The metrics include accuracy, log likelihood
    loss, and expected calibration error (ECE). The number of ensemble members is
    either $M=5$ the average results over 5 distinct realizations are shown.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Performance metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Fig. [1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles ‣ 7 Numerical Results ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles"), we illustrate the performance
    metrics for LoRA ensembles with both $M=1$ members. The ensembles are trained
    on multiple-choice QAs from CQA (first column), MMLU STEM (second column), and
    MMLU Social Studies datasets (third column).
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble members start over-fitting after a couple of epochs as can be seen
    by the validation losses in the second row of panels, but interestingly without
    considerable drop in accuracy in the first row of panels. As will be seen in more
    detail in Section [7.5](#S7.SS5 "7.5 Dynamics of uncertainty metrics ‣ 7 Numerical
    Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"),
    this signals that the ensemble members become more overconfident on predictions
    that are wrong, corroborated by the increasing expected calibration error with
    epoch seen in the bottom row of Fig. [1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles
    ‣ 7 Numerical Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA
    ensembles"). This is consistent with the recent reports that fine-tuned LLMs often
    exhibit overconfidence (Jiang et al., [2021](#bib.bib37); Lin et al., [2022](#bib.bib41);
    Xiao et al., [2022b](#bib.bib74); He et al., [2023](#bib.bib26); Tian et al.,
    [2023](#bib.bib63); OpenAI et al., [2023](#bib.bib54)).
  prefs: []
  type: TYPE_NORMAL
- en: For fine-tuning on CQA seen in the first column, the overfitting is significantly
    reduced for ensembles compared to individual members, as can be seen by the gap
    between the loss curves for $M=1$ for later epochs. This means that ensemble members
    output more confident, but different predictions. This is a common feature of
    Bayesian posteriors (Blundell et al., [2015](#bib.bib6); Zhang et al., [2020](#bib.bib79);
    Kristiadi et al., [2020](#bib.bib38); Ober & Aitchison, [2021](#bib.bib53); Fortuin
    et al., [2022](#bib.bib19); Aitchison et al., [2021](#bib.bib2); Yang et al.,
    [2024](#bib.bib76)). This signals high epistemic uncertainty in this regime, indicating
    that the validation set is perceived as out-of-domain. There is no significant
    difference between single member and ensemble metrics for models trained on the
    MMLU datasets. We attribute this to the fact that the MMLU training datasets are
    small, see Appendix [A](#A1 "Appendix A Datasets ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles"), facilitating fast overfitting before
    any significant generalization is attained.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8187d201d4997b5826477f7facb2b76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: AUROC computed for the CQA, MMLU STEM, and MMLU Social Studies multiple-choice
    QA datasets. In each subpanel, one dataset is used for fine-tuning (T) on the
    training subset and two datasets for the AUROC evaluation (V) on the validation
    subset. The LoRA ensemble size is $M=5$.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Uncertainty metrics visualized by AUROC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Area under receiver operator curve (AUROC) is commonly used as a metric for
    out-of-domain detection (Hendrycks & Gimpel, [2016](#bib.bib28)), but can also
    be used more generally to measure how well a model can separate two datasets.
    By evaluating AUROC using uncertainty measures, we can gain understanding of which
    of the datasets is more consistently perceived by the model as more confusing
    (using entropy) or as more out-of-domain (using mutual information).
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [2](#S7.F2 "Figure 2 ‣ 7.3 Performance metrics ‣ 7 Numerical Results
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"), we show
    the AUROC results comparing in- and out-of-domain validation datasets. The ensembles
    were fine-tuned on the training subset of the CQA, MMLU STEM, and MMLU Social
    Studies datasets. For the AUROC evaluation, we use the validation subset of these
    datasets as in-domain samples, and the validation subset of other datasets as
    out-of-domain.
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the AUROC curves for the ensemble trained on CQA in the first column
    of Fig. [2](#S7.F2 "Figure 2 ‣ 7.3 Performance metrics ‣ 7 Numerical Results ‣
    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"), it is evident
    that the model consistently perceives samples from CQA as more in-domain compared
    to those from MMLU datasets, as indicated by an AUROC greater than 0.5\. This
    aligns with expectations. However, the AUROC is not close to 1, suggesting that
    a significant portion of the MMLU dataset is perceived similarly to CQA in terms
    of entropy and mutual information, with MMLU Social Sciences appearing closer
    than MMLU STEM as can be seen by the lower AUROC values. A somewhat similar pattern
    is observed for the ensemble trained on MMLU Social Sciences.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the ensemble trained on MMLU STEM in the second column, a peculiar behavior
    is observed, where the AUROC is below 0.5\. We interpret this as follows: MMLU
    STEM poses greater complexity for the LLM architecture, hence, even with training,
    it struggles to identify the right features for correct answers. Conversely, the
    ensemble retains robust knowledge about features pertinent to MMLU SS and CQA.
    A mutual information score below 0.5 indicates that the ensemble perceives the
    validation set of MMLU STEM as more out-of-domain compared to the other datasets,
    even though the model was fine-tuned on the training subset of MMLU STEM.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Dynamics of uncertainty metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To analyze the trends observed through the AUROC curves in more detail, we also
    produce data-resolved dynamics of uncertainty measures by looking at two-dimensional
    histogram plots of mutual information and entropy(Linander et al., [2023](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [3](#S7.F3 "Figure 3 ‣ 7.5 Dynamics of uncertainty metrics ‣ 7 Numerical
    Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"),
    we show the epoch evolution of uncertainty measures for a LoRA ensemble that was
    fine-tuned and evaluated on the CQA dataset. The mutual information (MI) on the
    vertical axis, and the predictive entropy (Entropy) on the horisontal axis, are
    binned over every sample of the validation set of CQA and the number of samples
    in each bin is indicated by the colorbar. We distinguish between incorrect and
    correct predictions to enhance interpretability. The ensemble attains minimal
    validation loss at epoch 3, as shown in Fig. [1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles
    ‣ 7 Numerical Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA
    ensembles"). Epochs 1 and 6 exhibit underfitting and overfitting, respectively.
    The true posterior $p(\theta|D)$ should yield nearly zero mutual information when
    evaluated on the validation dataset, which should be in-domain by design. Samples
    with high mutual information indicate that parts of the target domain are perceived
    as out-of-domain by the model, showing the most important regions to consider
    for improving the training dataset targetting a better posterior approximation.
    We observe a moderate spread in entropy on the horizontal axis, indicating that
    the model predicts a broad distribution even though the sample is considered in-domain
    through their small mutual information. This spread can be attributed to the questions
    where the model does not have enough context or expressive power for a distinct
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [4](#S7.F4 "Figure 4 ‣ 7.5 Dynamics of uncertainty metrics ‣ 7 Numerical
    Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")
    shows the uncertainty histograms for MMLU STEM and MMLU SS, calculated using an
    ensemble trained on CQA. Starting with MMLU STEM in the first and second row of
    panels, we draw two conclusions. First, out of the wrongly classified samples
    for the optimal epoch 3 shown in the second row, second column, there is a large
    fraction with high predictive entropy and low mutual information compared to the
    more moderate spread of predictive entropy in the corresponding panel for MMLU
    SS in the last row, second column. This affirms conclusions from Section [7.4](#S7.SS4
    "7.4 Uncertainty metrics visualized by AUROC ‣ 7 Numerical Results ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles") regarding the relatively
    higher complexity of STEM compared to SS. The ensemble trained on CQA perceives
    the STEM samples equally in-domain as the SS samples, as evident by the similar
    mutual information, but predict a broader distribution indicating a lack of knowledge.
    Secondly, in the over-fitting regime corresponding to the last column, the mutual
    information of the wrongly classified samples of the second row increase more
    than the correctly classified samples in the first row. This indicates that the
    ensemble forgets the samples with high predictive entropy quicker, i.e. it forgets
    the confusing samples faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b0456062bf78f6fd4ee5d03997c52de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Histograms of predictive entropy and mutual information for a LoRA
    ensemble trained and evaluated on the CQA dataset. The ensemble consists of $M=5$,
    and an L2 LoRA loss of 1\. This figure illustrates the evolution of uncertainty
    measures for in-domain data across training epochs, differentiated by correct
    and incorrect predictions. We also depict the corresponding mean (red) and median
    (green) entropy and mutual information values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03c72e60eacf4b09cfce16238d8bb19e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Histograms of predictive entropy (Entropy) and mutual information
    (MI) for a LoRA ensemble trained on CQA and evaluated on MMLU STEM and MMLU SS
    datasets. The ensemble consists of $M=5$, and an L2 LoRA loss of 1\. This panels
    illustrates the evolution (left to right) of uncertainty measures for out-of-training-domain
    data across training epochs, differentiated by correct and incorrect predictions.
    We also depict the corresponding mean (red) and median (green) entropy and mutual
    information values.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with MMLU SS in the third and fourth row, we observe a decrease in
    predictive entropy in the over-fitting regime, indicating that the ensemble does
    find features giving confident but wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, the evolution of entropic quantities shows a quantitative difference:
    For MMLU STEM, there is a significant density of high-entropy, low-mutual-information
    samples at epoch 6, whereas for MMLU SS, the data with high entropy is more dispersed
    in terms of mutual information. This suggests that the fine-tuned model perceives
    confusing samples from MMLU STEM and MMLU SS differently, forgetting the confusing
    samples from MMLU SS more rapidly.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We derived a principled, and practically efficient method for posterior approximation
    of fine-tuned LLMs using ensembles of low-rank adaptation models. By analysing
    the evolution of entropic uncertainty measures during fine-tuning, we identified
    signals of dataset complexity and signs of architecture limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The methods presented are generally applicable to different fine-tuning scenarios,
    and provide a systematic treatment of prediction uncertainty for fine-tuned LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HL was supported by a grant from the KAW Foundation. The computations were enabled
    by resources provided by the National Academic Infrastructure for Supercomputing
    in Sweden (NAISS) and the Swedish National Infrastructure for Computing (SNIC)
    at C3SE partially funded by the Swedish Research Council through grant agreements
    no. 2022-06725 and no. 2018-05973. OB would like to thank Ihor Balabanov for valuable
    discussions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aghajanyan et al. (2020) Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrinsic
    dimensionality explains the effectiveness of language model fine-tuning, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aitchison et al. (2021) Aitchison, L., Yang, A., and Ober, S. W. Deep kernel
    processes. In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  130–140\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/aitchison21a.html](https://proceedings.mlr.press/v139/aitchison21a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balabanov & Linander (2024) Balabanov, O. and Linander, H. [github.com/oleksandr-balabanov/equivariant-posteriors/commit/38d5fb2817e43fa79cc8e4ddd3782fb4d7fb3ff2](github.com/oleksandr-balabanov/equivariant-posteriors/commit/38d5fb2817e43fa79cc8e4ddd3782fb4d7fb3ff2),
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balabanov et al. (2023) Balabanov, O., Mehlig, B., and Linander, H. Bayesian
    posterior approximation with stochastic ensembles. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  13701–13711,
    June 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barber & Bishop (1998) Barber, D. and Bishop, C. M. Ensemble learning in bayesian
    neural networks. 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
    D. Weight uncertainty in neural networks. In *Proceedings of the 32nd International
    Conference on International Conference on Machine Learning - Volume 37*, ICML’15,
    pp. 1613–1622\. JMLR.org, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
    Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks
    of artificial general intelligence: Early experiments with gpt-4. *arXiv preprint
    arXiv:2303.12712*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buntine & Weigend (1991) Buntine, W. L. and Weigend, A. S. Bayesian back-propagation.
    *Complex Syst.*, 5, 1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen & Mueller (2023) Chen, J. and Mueller, J. Quantifying uncertainty in answers
    from any language model and enhancing their trustworthiness, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen & Li (2024) Chen, W. and Li, Y. Calibrating transformers via sparse gaussian
    processes, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cinquin et al. (2021) Cinquin, T., Immer, A., Horn, M., and Fortuin, V. Pathologies
    in priors and inference for bayesian transformers, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denker et al. (1987) Denker, J. S., Schwartz, D. B., Wittner, B. S., Solla,
    S. A., Howard, R. E., Jackel, L. D., and Hopfield, J. J. Large automatic learning,
    rule extraction, and generalization. *Complex Syst.*, 1, 1987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2022) Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y.,
    Hu, S., Chen, Y., Chan, C.-M., Chen, W., Yi, J., Zhao, W., Wang, X., Liu, Z.,
    Zheng, H.-T., Chen, J., Liu, Y., Tang, J., Li, J., and Sun, M. Delta tuning: A
    comprehensive study of parameter efficient methods for pre-trained language models,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023) Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y.,
    Hu, S., Chen, Y., Chan, C.-M., Chen, W., Yi, J., Zhao, W., Wang, X., Liu, Z.,
    Zheng, H.-T., Chen, J., Liu, Y., Tang, J., Li, J., and Sun, M. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*,
    5(3):220–235, Mar 2023. ISSN 2522-5839. doi: 10.1038/s42256-023-00626-4. URL [https://doi.org/10.1038/s42256-023-00626-4](https://doi.org/10.1038/s42256-023-00626-4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2023) Duan, J., Cheng, H., Wang, S., Zavalny, A., Wang, C., Xu,
    R., Kailkhura, B., and Xu, K. Shifting attention to relevance: Towards the uncertainty
    estimation of large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dwaracherla et al. (2022) Dwaracherla, V., Wen, Z., Osband, I., Lu, X., Asghari,
    S. M., and Van Roy, B. Ensembles for Uncertainty Estimation: Benefits of Prior
    Functions and Bootstrapping. *arXiv e-prints*, art. arXiv:2206.03633, June 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Fan, X., Zhang, S., Chen, B., and Zhou, M. Bayesian attention
    modules. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
    (eds.), *Advances in Neural Information Processing Systems*, volume 33, pp.  16362–16376\.
    Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fort et al. (2019) Fort, S., Hu, H., and Lakshminarayanan, B. Deep Ensembles:
    A Loss Landscape Perspective. *arXiv e-prints*, art. arXiv:1912.02757, December
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortuin et al. (2022) Fortuin, V., Garriga-Alonso, A., Ober, S. W., Wenzel,
    F., Rätsch, G., Turner, R. E., van der Wilk, M., and Aitchison, L. Bayesian neural
    network priors revisited, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal & Ghahramani (2015) Gal, Y. and Ghahramani, Z. Bayesian Convolutional Neural
    Networks with Bernoulli Approximate Variational Inference. *arXiv e-prints*, art.
    arXiv:1506.02158, June 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal & Ghahramani (2016) Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation:
    Representing model uncertainty in deep learning. In Balcan, M. F. and Weinberger,
    K. Q. (eds.), *Proceedings of The 33rd International Conference on Machine Learning*,
    volume 48 of *Proceedings of Machine Learning Research*, pp.  1050–1059, New York,
    New York, USA, 20–22 Jun 2016\. PMLR. URL [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gawlikowski et al. (2021) Gawlikowski, J., Rovile Njieutcheu Tassi, C., Ali,
    M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R.,
    Shahzad, M., Yang, W., Bamler, R., and Zhu, X. X. A Survey of Uncertainty in Deep
    Neural Networks. *arXiv e-prints*, art. arXiv:2107.03342, July 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gleave & Irving (2022) Gleave, A. and Irving, G. Uncertainty estimation for
    language reward models, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves (2011) Graves, A. Practical variational inference for neural networks.
    In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K.
    (eds.), *Advances in Neural Information Processing Systems*, volume 24\. Curran
    Associates, Inc., 2011. URL [https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gustafsson et al. (2019) Gustafsson, F. K., Danelljan, M., and Schön, T. B.
    Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision.
    *arXiv e-prints*, art. arXiv:1906.01620, June 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2023) He, G., Chen, J., and Zhu, J. Preserving pre-trained features
    helps calibrate fine-tuned language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into
    rectifiers: Surpassing human-level performance on imagenet classification, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks & Gimpel (2016) Hendrycks, D. and Gimpel, K. A baseline for detecting
    misclassified and out-of-distribution examples in neural networks. *arXiv preprint
    arXiv:1610.02136*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    *Proceedings of the International Conference on Learning Representations (ICLR)*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann & Elster (2021) Hoffmann, L. and Elster, C. Deep Ensembles from a Bayesian
    Perspective. *arXiv e-prints*, art. arXiv:2105.13283, May 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. (2023) Hou, B., Liu, Y., Qian, K., Andreas, J., Chang, S., and Zhang,
    Y. Decomposing uncertainty for large language models through input clarification
    ensembling, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2011) Houlsby, N., Huszar, F., Ghahramani, Z., and Lengyel,
    M. Bayesian active learning for classification and preference learning. *CoRR*,
    abs/1112.5745, 2011. URL [http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745](http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), *Proceedings
    of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings
    of Machine Learning Research*, pp.  2790–2799\. PMLR, 09–15 Jun 2019. URL [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *ICL2022*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Huang, Y., Song, J., Wang, Z., Zhao, S., Chen, H., Juefei-Xu,
    F., and Ma, L. Look Before You Leap: An Exploratory Study of Uncertainty Measurement
    for Large Language Models, October 2023. URL [http://arxiv.org/abs/2307.10236](http://arxiv.org/abs/2307.10236).
    arXiv:2307.10236 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021) Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can
    we know when language models know? on the calibration of language models for question
    answering. *Transactions of the Association for Computational Linguistics*, 9:962–977,
    2021. doi: 10.1162/tacl˙a˙00407. URL [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kristiadi et al. (2020) Kristiadi, A., Hein, M., and Hennig, P. Being bayesian,
    even just a bit, fixes overconfidence in relu networks, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuhn et al. (2023) Kuhn, L., Gal, Y., and Farquhar, S. Semantic Uncertainty:
    Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,
    April 2023. URL [http://arxiv.org/abs/2302.09664](http://arxiv.org/abs/2302.09664).
    arXiv:2302.09664 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018) Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the
    intrinsic dimension of objective landscapes, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2022) Lin, S., Hilton, J., and Evans, O. Teaching Models to Express
    Their Uncertainty in Words. *arXiv e-prints*, art. arXiv:2205.14334, May 2022.
    doi: 10.48550/arXiv.2205.14334.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Lin, Z., Trivedi, S., and Sun, J. Generating with Confidence:
    Uncertainty Quantification for Black-box Large Language Models. *arXiv e-prints*,
    art. arXiv:2305.19187, May 2023. doi: 10.48550/arXiv.2305.19187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linander et al. (2023) Linander, H., Balabanov, O., Yang, H., and Mehlig, B.
    Looking at the posterior: accuracy and uncertainty of neural-network predictions.
    *Machine Learning: Science and Technology*, 4(4):045032, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal,
    M., and Raffel, C. Few-shot parameter-efficient fine-tuning is better and cheaper
    than in-context learning, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized
    bert pretraining approach, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacKay (1991) MacKay, D. Bayesian model comparison and backprop nets. In Moody,
    J., Hanson, S., and Lippmann, R. (eds.), *Advances in Neural Information Processing
    Systems*, volume 4\. Morgan-Kaufmann, 1991. URL [https://proceedings.neurips.cc/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf](https://proceedings.neurips.cc/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mackay (1992) Mackay, D. J. C. Information-based objective functions for active
    data selection. *Neural Computation*, 4(2):550–604, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinin & Gales (2018) Malinin, A. and Gales, M. Predictive uncertainty estimation
    via prior networks, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinin & Gales (2021) Malinin, A. and Gales, M. Uncertainty Estimation in Autoregressive
    Structured Prediction, February 2021. URL [http://arxiv.org/abs/2002.07650](http://arxiv.org/abs/2002.07650).
    arXiv:2002.07650 [cs, stat].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinin et al. (2021) Malinin, A., Prokhorenkova, L., and Ustimenko, A. Uncertainty
    in gradient boosting via ensembles, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mangrulkar et al. (2022) Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y.,
    Paul, S., and Bossan, B. Peft: State-of-the-art parameter-efficient fine-tuning
    methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mielke et al. (2022) Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y.-L.
    Reducing conversational agents’ overconfidence through linguistic calibration.
    *Transactions of the Association for Computational Linguistics*, 10:857–872, 2022.
    doi: 10.1162/tacl˙a˙00494. URL [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ober & Aitchison (2021) Ober, S. W. and Aitchison, L. Global inducing point
    variational posteriors for bayesian neural networks and deep gaussian processes.
    In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International Conference
    on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*,
    pp.  8248–8259\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/ober21a.html](https://proceedings.mlr.press/v139/ober21a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2023) OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
    Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat,
    S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H.,
    Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner,
    C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks,
    T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson,
    C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen,
    R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D.,
    Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar,
    A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T.,
    Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao,
    L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R.,
    Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y.,
    Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C.,
    Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X.,
    Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin,
    D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider,
    I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner,
    H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis,
    A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung,
    J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe,
    R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y.,
    Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan,
    P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin,
    P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély,
    D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang,
    L., O’Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,
    G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A.,
    de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny,
    Pokrass, M., Pong, V., Powell, T., Power, A., Power, B., Proehl, E., Puri, R.,
    Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C.,
    Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S.,
    Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K.,
    Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens,
    M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N.,
    Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M., Tillet,
    P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.
    F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang,
    A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng,
    J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman,
    L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba,
    W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk,
    W., and Zoph, B. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language
    models to follow instructions with human feedback. *Advances in Neural Information
    Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ovadia et al. (2019a) Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D.,
    Nowozin, S., Dillon, J., Lakshminarayanan, B., and Snoek, J. Can you trust your
    model's uncertainty? evaluating predictive uncertainty under dataset shift. In
    Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett,
    R. (eds.), *Advances in Neural Information Processing Systems*, volume 32\. Curran
    Associates, Inc., 2019a. URL [https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ovadia et al. (2019b) Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D.,
    Nowozin, S., Dillon, J. V., Lakshminarayanan, B., and Snoek, J. Can you trust
    your model’s uncertainty? evaluating predictive uncertainty under dataset shift,
    2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2023) Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruction
    tuning with gpt-4. *arXiv preprint arXiv:2304.03277*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2023) Ren, J., Luo, J., Zhao, Y., Krishna, K., Saleh, M., Lakshminarayanan,
    B., and Liu, P. J. Out-of-Distribution Detection and Selective Generation for
    Conditional Language Models, March 2023. URL [http://arxiv.org/abs/2209.15558](http://arxiv.org/abs/2209.15558).
    arXiv:2209.15558 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi & Lipani (2024) Shi, Z. and Lipani, A. Dept: Decomposed prompt tuning for
    parameter-efficient fine-tuning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Sun, M., Yan, W., Abbeel, P., and Mordatch, I. Quantifying
    uncertainty in foundation models via ensembles. In *NeurIPS 2022 Workshop on Robustness
    in Sequence Modeling*, 2022. URL [https://openreview.net/forum?id=LpBlkATV24M](https://openreview.net/forum?id=LpBlkATV24M).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talmor et al. (2019) Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA:
    A question answering challenge targeting commonsense knowledge. In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 
    4149–4158, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1421. URL [https://aclanthology.org/N19-1421](https://aclanthology.org/N19-1421).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2023) Tian, K., Mitchell, E., Zhou, A., Sharma, A., Rafailov,
    R., Yao, H., Finn, C., and Manning, C. D. Just ask for calibration: Strategies
    for eliciting calibrated confidence scores from language models fine-tuned with
    human feedback, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tishby et al. (1989) Tishby, Levin, and Solla. Consistent inference of probabilities
    in layered networks: predictions and generalizations. In *International 1989 Joint
    Conference on Neural Networks*, pp.  403–409 vol.2, 1989. doi: 10.1109/IJCNN.1989.118274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2019) Tran, D., Dusenberry, M., van der Wilk, M., and Hafner,
    D. Bayesian layers: A module for neural network uncertainty. In Wallach, H., Larochelle,
    H., Beygelzimer, A., d''Alché-Buc, F., Fox, E., and Garnett, R. (eds.), *Advances
    in Neural Information Processing Systems*, volume 32\. Curran Associates, Inc.,
    2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2022) Tran, D., Liu, J., Dusenberry, M. W., Phan, D., Collier,
    M., Ren, J., Han, K., Wang, Z., Mariet, Z., Hu, H., Band, N., Rudner, T. G. J.,
    Singhal, K., Nado, Z., van Amersfoort, J., Kirsch, A., Jenatton, R., Thain, N.,
    Yuan, H., Buchanan, K., Murphy, K., Sculley, D., Gal, Y., Ghahramani, Z., Snoek,
    J., and Lakshminarayanan, B. Plex: Towards reliability using pretrained large
    model extensions, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Wang, X., Aitchison, L., and Rudolph, M. Lora ensembles for
    large language model fine-tuning, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2020) Wen, Y., Tran, D., and Ba, J. Batchensemble: An alternative
    approach to efficient ensemble and lifelong learning, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilson & Izmailov (2020) Wilson, A. G. and Izmailov, P. Bayesian deep learning
    and a probabilistic perspective of generalization. *Advances in neural information
    processing systems*, 33:4697–4708, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wimmer et al. (2023) Wimmer, L., Sale, Y., Hofman, P., Bischl, B., and Hüllermeier,
    E. Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional
    entropy and mutual information appropriate measures? In *Uncertainty in Artificial
    Intelligence*, pp.  2282–2292. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2022a) Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov,
    R., and Morency, L.-P. Uncertainty quantification with pre-trained language models:
    A large-scale empirical analysis. In Goldberg, Y., Kozareva, Z., and Zhang, Y.
    (eds.), *Findings of the Association for Computational Linguistics: EMNLP 2022*,
    pp. 7273–7284, Abu Dhabi, United Arab Emirates, December 2022a. Association for
    Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.538. URL [https://aclanthology.org/2022.findings-emnlp.538](https://aclanthology.org/2022.findings-emnlp.538).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2022b) Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov,
    R., and Morency, L.-P. Uncertainty quantification with pre-trained language models:
    A large-scale empirical analysis, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2021) Xue, B., Yu, J., Xu, J., Liu, S., Hu, S., Ye, Z., Geng, M.,
    Liu, X., and Meng, H. Bayesian transformer language models for speech recognition.
    pp.  7378–7382, 06 2021. doi: 10.1109/ICASSP39728.2021.9414046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2024) Yang, A. X., Robeyns, M., Wang, X., and Aitchison, L. Bayesian
    low-rank adaptation for large language models, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhai et al. (2023) Zhai, Y., Zhang, H., Lei, Y., Yu, Y., Xu, K., Feng, D., Ding,
    B., and Wang, H. Uncertainty-penalized reinforcement learning from human feedback
    with diverse reward lora ensembles, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.
    mixup: Beyond empirical risk minimization, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A. G.
    Cyclical stochastic gradient mcmc for bayesian deep learning, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P.,
    Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language
    models with zero-init attention. *arXiv preprint arXiv:2303.16199*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Zhang, S., Fan, X., Chen, B., and Zhou, M. Bayesian attention
    belief networks, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2021) Zhong, R., Lee, K., Zhang, Z., and Klein, D. Adapting language
    models for zero-shot learning by meta-tuning on dataset and prompt collections.
    *arXiv preprint arXiv:2104.04670*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For MMLU STEM and Social Studies datasets, we merge the development and validation
    sets to create the training set and use the original test split as the validation
    set, as in (Wang et al., [2023](#bib.bib69)). For CQA dataset we use the standard
    training and validation splits. The topics in the MMLU dataset are categorized
    into STEM, Social Studies, Humanities, and Other, following the classification
    defined in (Hendrycks et al., [2021](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Train Size | Validation Size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CQA | 9741 | 1221 |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU SS | 397 | 3077 |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU STEM | 430 | 3153 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Summary of the CQA and MMLU dataset sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The datasets have been preprocessed into a single, consistently formatted prompt
    question-answer. Below, we present examples of these questions for each of the
    considered dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: A revolving door is convenient for two direction travel, but it also serves
    as a security measure at a what? Answer Choices: (a) bank (b) library (c) department
    store (d) mall (e) new york A: (a). CommonsenseQAQ: Which one of the following
    is the most appropriate definition of a 99% confidence interval? Answer Choices:
    (a) 99% of the time in repeated samples, the interval would contain the true value
    of the parameter (b) 99% of the time in repeated samples, the interval would contain
    the estimated value of the parameter (c) 99% of the time in repeated samples,
    the null hypothesis will be rejected (d) 99% of the time in repeated samples,
    the null hypothesis will not be rejected when it was false A: (a). MMLU SSQ: Find
    all c in Z_3 such that Z_3[x]/(x^2 + c) is a field. Answer Choices: (a) 0 (b)
    1 (c) 2 (d) 3 A: (b). MMLU STEM'
  prefs: []
  type: TYPE_NORMAL
