- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15751](https://ar5iv.labs.arxiv.org/html/2402.15751)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yong Liu    Zirui Zhu    Chaoyu Gong    Minhao Cheng    Cho-Jui Hsieh    Yang
    You
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While fine-tuning large language models (LLMs) for specific tasks often yields
    impressive results, it comes at the cost of memory inefficiency due to back-propagation
    in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently
    proposed to address this issue, only require forward passes during training, making
    them more memory-friendly. However, the quality of gradient estimates in zeroth
    order optimization often depends on the data dimensionality, potentially explaining
    why MeZO still exhibits significant performance drops compared to standard fine-tuning
    across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning
    (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order
    optimization approach that applies ZO only to a carefully chosen subset of parameters.
    We propose a simple yet effective parameter selection scheme that yields significant
    performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized
    implementation for sparse masking, ensuring the algorithm requires only inference-level
    memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100
    GPU. Experimental results illustrate that Sparse-MeZO consistently improves both
    performance and convergence speed over MeZO without any overhead. For example,
    it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO on the
    RTE task.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning large language models for specific tasks or datasets has become
    a prevalent practice in machine learning. However, a major obstacle in fine-tuning
    is the substantial memory requirements, which escalate as models increase in size
    and complexity, thereby limiting the scalability and accessibility for those with
    limited computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f406b4b5dd32ad9fef2285700eadccf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Performance of MeZO and Sparse-MeZO (S-MeZO) on RTE task. S-MeZO
    can achieve $3.5$x speedup compared with MeZO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate the memory constraints, Parameter Efficient Fine-Tuning (PEFT)
    has been developed, allowing for the modification of only a subset of parameters
    and achieving comparable results to full model tuning (Zaken et al., [2021](#bib.bib53);
    Li & Liang, [2021](#bib.bib27); Lester et al., [2021](#bib.bib26); Hu et al.,
    [2021](#bib.bib22); Zhang et al., [2023](#bib.bib54)). However, PEFT methods still
    necessitate the calculation of gradients for backpropagation and caching of numerous
    activations during training, which introduces additional memory overhead. For
    instance, [Malladi et al.](#bib.bib34) demonstrates that, even with PEFT, training
    still requires approximately 6 times more memory than the memory cost for inference.
    This discrepancy raises a critical question: Can large language models be fine-tuned
    solely with the cost of inference?'
  prefs: []
  type: TYPE_NORMAL
- en: In response to these challenges, zeroth-order (ZO) optimization presents a promising
    solution (Spall, [1992](#bib.bib41)). ZO optimization is a gradient-free method
    that estimates gradients using only the forward pass of the model, eliminating
    the need for backpropagation and, consequently, reducing memory usage. MeZO (Malladi
    et al., [2023](#bib.bib34)) is a recently proposed zeroth-order method for fine-tuning
    LLMs that has demonstrated impressive performance. However, the performance and
    convergence rate of ZO methods are usually highly dependent on the dimensionality
    of the parameters, with higher dimensionality leading to slower convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building on the insights from PEFT, we propose a novel sparse memory efficient
    zeroth-order method (Sparse-MeZO) to solve the issues from the high dimensionality
    of LLMs. In traditional parameter-efficient fine-tuning, there’s a tradeoff between
    memory efficiency and performance. But surprisingly, when we only tune a subset
    of parameters in MeZO, it significantly improves the performance while accelerating
    the convergence rate. This renders our method an improvement over MeZO without
    any overhead. Our contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this paper, we propose a sparse Memory-Efficient Zeroth-Order optimization
    method Sparse-MeZO (S-MeZO) for large language model fine-tuning. We also provide
    theoretical analysis to show the convergence of Sparse-MeZO.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To determine the sparse parameters that need to be updated, we investigate how
    the importance of each parameter in fine-tuning correlates with its magnitude.
    Although existing sparsification methods usually prune out parameters with smaller
    magnitudes, our evaluations show that smaller parameters are more important than
    larger weights for Zeroth-order fine-tuning methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different from the efficient implementation with random seed in MeZO, we propose
    a novel memory-efficient implementation of Sparse-MeZO, which can compute the
    sparse mask and perturb parameters in the forward pass. The technique enables
    fine-tuning LLaMA-30b with Sparse-MeZO on a single A100 GPU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We conduct empirical studies on LLaMA and OPT. The experimental results demonstrate
    that Sparse-MeZO can improve the fine-tuning performance and yield a faster convergence
    rate compared with vanilla MeZO across a wide range of natural language processing
    tasks. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup
    over MeZO on the RTE task, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Parameter-Efficient Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning (PEFT) is designed to facilitate efficient adaptation
    by updating only a subset of the model’s parameters, rather than fine-tuning the
    entire model (Hu et al., [2021](#bib.bib22); Zaken et al., [2021](#bib.bib53)).
    These PEFT approaches can be categorized in various ways. We mainly focus on the
    selective methods and additive methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Selective Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Selective Methods try to selectively fine-tune a portion of a model and these
    methods have been explored in various studies. For example, [Zaken et al.](#bib.bib53);
    [Cai et al.](#bib.bib5) focused on the model’s bias terms, finding that fine-tuning
    these terms alone could rival the results of fine-tuning the entire model. However,
    the effectiveness of this approach diminishes with larger datasets, as shown in
    further analysis by [Zaken et al.](#bib.bib53). Beyond static parameter adjustments,
    there has been an exploration into dynamically modifying parts of the model. For
    instance, FreezeOut, introduced by [Brock et al.](#bib.bib3), suggests a gradual
    freezing of layers, effectively removing early layers from the backward pass to
    quicken training. This concept was later applied to language models, with AutoFreeze
    (Liu et al., [2021](#bib.bib33)) confirming its viability. Nevertheless, these
    techniques still demand considerable computational resources and tend to yield
    less optimal final outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Additive Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Additive methods, as an alternative to updating existing parameters, involve
    incorporating new layers into models, with the fine-tuning process focusing solely
    on these added layers (Houlsby et al., [2019](#bib.bib21); Rebuffi et al., [2017](#bib.bib37);
    Lin et al., [2020](#bib.bib28); Hu et al., [2021](#bib.bib22)). Traditional techniques
    in this category, such as adapters (Houlsby et al., [2019](#bib.bib21)), implemented
    layer additions in a sequential manner, which unfortunately led to increased inference
    latency. LoRA (Hu et al., [2021](#bib.bib22)) has been proposed to mitigate this
    issue, which freezes the weights of the pre-trained model and introduces trainable
    matrices based on rank decomposition into each layer. Then, it can directly integrate
    the newly learned weights into the main model. Following this, IA3 (Liu et al.,
    [2022](#bib.bib29)) introduced innovative methods for adding parameters, balancing
    parameter count with accuracy, while LST (Sung et al., [2022](#bib.bib45)) introduced
    a highway structure that learns only small, auxiliary channels, aiming to decrease
    memory demands. Despite these advancements, additive methods generally require
    meticulous design, and many fail to reduce the computational load during the backward
    pass, as seen in IA3, LoRA, and Adapter approaches. Additionally, some of these
    methods, including Adapter and LST, can incur additional computational overhead
    in the forward pass, making them less practical for certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Zeroth-Order Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike traditional gradient-based optimization methods that rely on derivatives
    to guide the search for optimal solutions, zeroth-order (ZO) optimization techniques
    do not require derivatives for optimization (Spall, [1992](#bib.bib41); Liu et al.,
    [2018](#bib.bib30), [2019](#bib.bib31)). These methods utilize only the value
    of the objective function, denoted as $f(\boldsymbol{x})$ and $f(\boldsymbol{x}-\epsilon\boldsymbol{z})$
    being a minimal value. Following this, conventional optimization algorithms, such
    as gradient descent or coordinate descent, are implemented using these approximated
    gradient values. Currently, ZO methods have been widely used in various applications,
    such as adversarial attack and defense (Chen et al., [2017](#bib.bib9); Tu et al.,
    [2019](#bib.bib47); Ye et al., [2018](#bib.bib52); Ilyas et al., [2018](#bib.bib23)),
    Auto-ML (Ruan et al., [2019](#bib.bib39); Wang et al., [2022](#bib.bib50)), natural
    language processing (Sun et al., [2022a](#bib.bib43), [b](#bib.bib44)), reinforcement
    learning (Vemula et al., [2019](#bib.bib48)), Signal Processing (Liu et al., [2020](#bib.bib32)),
    and on-chip training (Gu et al., [2021](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 SPSA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, [1992](#bib.bib41))
    is a descent method that optimizes systems with multiple unknown parameters to
    identify global minima without relying on backpropagation. SPSA estimates gradients
    based on two measurements of the objective function, thus eliminating the need
    for automatic differentiation and backpropagation to obtain precise gradients.
    This approach notably avoids memory overhead associated with storing intermediate
    results for backpropagation. Specially, it employs a random vector $\boldsymbol{z}\in\mathbb{R}^{d}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{g_{z}(\theta)}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\approx\boldsymbol{z}\boldsymbol{z}^{\top}g(\boldsymbol{\theta}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 2.2.2 MeZO
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ZO-SGD employs SPSA to estimate the gradient. In general, conventional ZO-SGD
    algorithms utilizing SPSA consume twice the inference memory. MeZO (Malladi et al.,
    [2023](#bib.bib34)) is a memory-efficient variant of ZO-SGD. It circumvents the
    storage of gradients by saving the random seed and resampling the same random
    noise $\boldsymbol{z}$ with the seed during forward process. Therefore, it can
    reduce the training costs close to inference memory cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'We try to further explain how MeZO can be memory-efficient in Figure [2](#S2.F2
    "Figure 2 ‣ 2.2.2 MeZO ‣ 2.2 Zeroth-Order Optimization ‣ 2 Preliminaries ‣ Sparse
    MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning").
    At step 1, it will sample a noise $\boldsymbol{z}$. At step 2, MeZO can resample
    the same noise $\boldsymbol{z}$. Then, it can use $\boldsymbol{\theta_{t}^{\prime}}-2\epsilon\boldsymbol{z}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77f014fa9a19face1aeccabffff629c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Memory Efficient Implementation in MeZO. At step 1, we will sample
    a random noise $\boldsymbol{z}$ with same $s_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Sparsity for Zeroth-order Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The hypothesis proposed by [Frankle & Carbin](#bib.bib13), known as the lottery
    ticket hypothesis, showed that within a densely connected neural network that
    is randomly initialized, there exists a subnetwork of sparse yet high-quality
    connections. Several related works have tried to apply the sparsity to zeroth-order
    optimization (Wang et al., [2018](#bib.bib51); Cai et al., [2022](#bib.bib7),
    [2021](#bib.bib6); Balasubramanian & Ghadimi, [2018](#bib.bib1); Ohta et al.,
    [2020](#bib.bib35); Gu et al., [2021](#bib.bib15); Chen et al., [2023](#bib.bib8)).
    For example, DeepZero proposes a novel ZO training protocol with model pruning
    guided sparsity. However, these methods mainly focus on the neural network training
    from scratch with random initialization, while the application of sparse zeroth-order
    optimization in fine-tuning tasks remains an area of ongoing exploration. In this
    paper, we will analyze the effects of sparse ZO methods on large language model
    fine-tuning tasks and propose a novel sparse zeroth-order method to obtain a better
    performance and faster convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Proposed Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning (PEFT) has been widely used in various areas,
    which only updates a fraction of full parameters and can yield comparable performance.
    In addition, it’s widely recognized that both the performance and the convergence
    rate are significantly influenced by the dimensionality of the weights to ZO methods.
    For example, [Duchi et al.](#bib.bib12) illustrates that an increase in dimensionality
    tends to proportionally slow down the convergence rate. These motivate us to propose
    a sparse zeroth-order optimization method for large language model fine-tuning,
    which only updates a subset of full parameters and yields a comparable performance
    and faster convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Sparse-MeZO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider a labelled dataset $\mathcal{D}=\{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\}_{i\in\mathcal{[|D|]}}$
    to selectively sample the random noise $\boldsymbol{z}\in\mathbb{R}^{d}\text{
    with }\boldsymbol{z}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{d})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{\hat{z}}=\boldsymbol{m}\odot\boldsymbol{z}.$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Based on this sparse perturbation $\boldsymbol{\hat{z}}$, which can be defined
    as :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{g_{\hat{z}}(\boldsymbol{\theta})}$ |  | (3)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{m}\odot\boldsymbol{z};\mathcal{B})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{m}\odot\boldsymbol{z};\mathcal{B})}{2\epsilon},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Convergence Analysis of Sparse-MeZO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will explain why Sparse-MeZO can accelerate the convergence.
    We can define a sub-network in pre-trained large language models, which is determined
    by the sparse mask $\boldsymbol{m}$ is the number of parameters in the sub-network.
    Therefore, ZO can use fewer steps to converge when we only focus on a sub-network.
    Some related work has illustrated that only tuning the sub-network can achieve
    comparable performance, which will be empirically verified in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we assume the loss function $\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})$
    is Lipschitz Continuous:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 3.1  (Lipschitz Continuous).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '|  | $\&#124;\nabla\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})-\nabla\mathcal{L}(\boldsymbol{\theta^{\prime}},\boldsymbol{x})\&#124;\leq\frac{L(l)}{2}\&#124;\boldsymbol{\theta}-\boldsymbol{\theta^{\prime}}\&#124;^{2},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\nabla\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})$ represents the
    Lipschitz constant of $\mathcal{L}(\cdot)$:'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 3.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'ZO gradient $\boldsymbol{g_{\hat{z}}(\boldsymbol{\theta})}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widehat{\nabla}_{\boldsymbol{\theta}}\mathcal{L}_{\hat{z}}(\boldsymbol{\boldsymbol{\boldsymbol{\theta}}})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\boldsymbol{m}\odot\nabla_{\boldsymbol{\boldsymbol{\theta}}}\mathbb{E}_{\boldsymbol{\hat{z}}}[\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\boldsymbol{\hat{z}}}[\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{\hat{z}})}{2\epsilon}\boldsymbol{\hat{z}}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\boldsymbol{\hat{z}}}[\boldsymbol{g_{\hat{z}}(\theta)}],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{g_{\hat{z}}(\theta)}=\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{\hat{z}})}{2\epsilon}\boldsymbol{\hat{z}}$
    in Lemma [5](#S3.E5 "Equation 5 ‣ Lemma 3.2\. ‣ 3.2 Convergence Analysis of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"), we can use the distance $\|\widehat{\nabla}_{\boldsymbol{\theta}}\mathcal{L}_{\hat{z}}(\boldsymbol{\theta})-\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\boldsymbol{\theta}})\|$:'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 3.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\mathcal{L}$ be Lipschitz Continuous, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\theta})=\boldsymbol{m}\odot\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})$
    represents the Lipschitz constant. Finally, we can obtain the convergence rate
    of Sparse-MeZO.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Assuming a sequence of generated parameters $\{\boldsymbol{\theta_{t}}\}_{t\geq
    0}$ in Sparse-MeZO. We can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{\hat{z},x}[\&#124;\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\theta_{T}})\&#124;^{2}]\leq\sigma^{2}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: for any $T=\mathcal{O}(\frac{\hat{d}L}{\sigma^{2}})$
  prefs: []
  type: TYPE_NORMAL
- en: where $L(l)\leq L$. This theorem illustrates that the presence of pronounced
    sparsity patterns, along with the smoothness of the objective function, can significantly
    enhance the rate of convergence, potentially achieving a linear acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Less Parameters for Better Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the analysis above, we find that Sparse-MeZO can speed up the convergence
    of zeroth-order optimization. After that, the main problem is how to determine
    the sparse mask $\boldsymbol{m}$. Network pruning is a popular method to prune
    the unimportant parameters and only use these selected important parameters to
    make the inference process fast and efficient (LeCun et al., [1989](#bib.bib25);
    Hanson & Pratt, [1988](#bib.bib19); Hassibi & Stork, [1992](#bib.bib20); Ström,
    [1997](#bib.bib42); Han et al., [2015a](#bib.bib17), [b](#bib.bib18)). Specifically,
    it typically removes connections with small parameters and retains those with
    large values. Drawing from these recent studies in network pruning, we first explore
    how the value of parameters affects zeroth-order optimization. This leads us to
    examine if perturbations to larger parameters are more critical.
  prefs: []
  type: TYPE_NORMAL
- en: Is the perturbation on larger parameters more important than smaller parameters
    ?
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct experiments to examine how parameter values of varying magnitudes
    impact performance. Specifically, we set the median value of each layer as a threshold
    to separate parameters into two groups. Then, we only perturb and update the parameters
    in one of these groups. For instance, we experiment with updating either just
    the smaller or larger parameters, using the same settings as the standard MeZO,
    on SuperGLUE. Then, we compared their performance to that of vanilla MeZO. The
    results in Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Less Parameters for Better Performance
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning") show that updating only the parameters smaller than the median
    value leads to better performance on BoolQ, RTE, WIC, and comparable results on
    SST-2\. However, updating only the larger parameters results in significantly
    worse performance compared to the standard MeZO. This suggests that the key to
    MeZO’s performance gain lies in the smaller weights.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason updating only smaller parameters leads to better performance might
    be due to the larger weights being well-trained already. Moving them in a random
    direction is unlikely to enhance performance. On the other hand, since smaller
    weights are less trained, there’s a higher chance of improvement regardless of
    the direction of the update.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd1e955e98442e24c513f8e976907036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparing performance across various parameter adjustments. MeZO:
    the vanilla MeZO approach, MeZO-S: Fine-Tuning smaller parameters with MeZO, MeZO-L:
    Fine-Tuning larger parameters with MeZO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the observations from Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Less Parameters
    for Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for
    Better Performance in Zeroth-Order LLM Fine-Tuning"), we can create a sparse mask,
    $\boldsymbol{m}$. It’s important to note that we still preserve the complete set
    of parameters, but we apply sparse perturbations and gradient estimations only
    to the selected ones. This approach allows us to integrate the sparse mask into
    the standard MeZO method as a straightforward, adaptable tool. Then, we will introduce
    when and where to calculate the mask.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Constant Mask: Setting the Mask Before Training. We compare the parameter values
    to a threshold for each layer to set the mask before training begins. However,
    a significant downside of this approach is the extra memory required to store
    a sparse mask, which is as large as the pre-trained model itself. Our goal is
    for our method to enhance performance without using more GPU memory or causing
    extra overhead.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic Mask: Determining Mask at Each Iteration. We can establish a threshold
    for each layer before training and then generate the mask by comparing parameter
    values to this threshold during each iteration. This method avoids the necessity
    of storing a large mask, $\boldsymbol{m}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Algorithm 1 Sparse-MeZO (S-MeZO)
  prefs: []
  type: TYPE_NORMAL
- en: 'Require: $\boldsymbol{\theta}$ represents sparsification interval.  Initialize
    random seed $s$ do     Sample Minibatch $\mathcal{B}$     $\boldsymbol{\theta_{t}}\leftarrow\text{PerturbParameters}(\boldsymbol{\theta_{t}},\epsilon,s,\boldsymbol{m})$     $\boldsymbol{\theta_{t}}\leftarrow\text{PerturbParameters}(\boldsymbol{\theta},\epsilon,s,\boldsymbol{m})$ do        $z_{i}\sim\mathcal{N}(0,1)$     end for  end for'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we’ll employ a dynamic mask to choose which parameters to perturb
    and update, addressing the issue of memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo-code is provided in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Less
    Parameters for Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters
    for Better Performance in Zeroth-Order LLM Fine-Tuning"). This algorithm outlines
    that we first establish the threshold $h_{i}$ and apply the mask $\boldsymbol{m}$
    to get new parameters $\boldsymbol{\theta_{t}+\epsilon\hat{z}}$. From these losses,
    we calculate the estimated sparse gradient $\boldsymbol{g_{m}}(\boldsymbol{\theta_{t}})=\text{proj\_grad}*\boldsymbol{\hat{z}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 PerturbParameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\boldsymbol{\theta}$.  Reset random seed $s$  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 GetMask
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\boldsymbol{\theta}$  for $i\leftarrow$ then           $\theta_{i,j}=1$        end if     end for  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Memory-Efficient Implementation of Sparse-MeZO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this paper, our primary aim is to introduce an efficient method for fine-tuning
    language models using zeroth-order optimization, enhancing performance on downstream
    tasks. As outlined in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Less Parameters for
    Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better
    Performance in Zeroth-Order LLM Fine-Tuning"), our approach involves perturbing
    the parameters $\boldsymbol{\theta_{t}}$. This step typically requires storing
    two separate sets of parameters, leading to increased memory usage during fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2.2 MeZO ‣ 2.2 Zeroth-Order
    Optimization ‣ 2 Preliminaries ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"), recently proposed MeZO, conserves memory by
    saving random seeds $s$, and reconstructing $\boldsymbol{\theta_{t}}$ by saving
    the random seed because the sparse mask, determined by parameter magnitudes, changes
    when parameters are altered by the perturbation. To address this, we propose potential
    solutions for the memory issue.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5835ba868e987379b23bc7f9e3208630.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Efficient Implementation of Sparse-MeZO. (a) represents vanilla implementation
    of S-MeZO, (b) represents our proposed efficient implementation of S-MeZO.'
  prefs: []
  type: TYPE_NORMAL
- en: '1-bit Quantization: We can apply 1-bit quantization to store the mask $\boldsymbol{m}$
    on the fly during the forward pass.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | BoolQ | RTE | WIC | MultiRC | SST-2 | COPA | Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | Zero-Shot | $65.1$ | $79.7$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | ICL | $67.4$ | $81.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | FT | $84.5\pm 0.0$ | $95.7\pm 0.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | MeZO | $75.9\pm 1.1$ | $94.6\pm 0.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | R-MeZO | $76.9\pm 0.7$ | $94.6\pm 0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | S-MeZO | $\bf{80.9\pm 1.6}$ | $\bf{95.0\pm 0.3}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Accuracy of Fine-Tuning LLaMA-7b on SuperGLUE (1,000 examples). ICL:
    In-Context Learning, FT: full-parameter fine-tuning with Adam, R-MeZO: MeZO with
    Random Mask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the Mask During the Forward Pass: By computing the mask and perturb
    parameters in the forward pass, we eliminate the need to store perturbed parameters
    $\boldsymbol{\theta_{t}^{\prime}}$: $\boldsymbol{\theta_{t}^{\prime}}=\boldsymbol{\theta_{t}}+\epsilon\boldsymbol{m}\odot\boldsymbol{z}$
    can be defined as $\boldsymbol{y^{(i)}}=\boldsymbol{\theta_{t}^{\prime(i)}}\boldsymbol{x^{(i)}}+\boldsymbol{b^{(i)}}$.
    More specially, we can calculate the mask $\boldsymbol{m^{(i)}}$ represents the
    function GetMask to calculate mask $\boldsymbol{m^{(i)}}$ and calculate the output
    and mask of next layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following a similar setting to MeZO, we evaluate the performance of our proposed
    method on SuperGLUE (Wang et al., [2019](#bib.bib49)). The experimental results
    show that our proposed method can achieve better performance while also attaining
    faster convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets. To verify the performance gain of our proposed method, we conduct
    experiments on various fine-tuning tasks include SST-2 (Socher et al., [2013](#bib.bib40)),
    RTE (Dagan et al., [2005](#bib.bib11); Haim et al., [2006](#bib.bib16); Giampiccolo
    et al., [2007](#bib.bib14); Bentivogli et al., [2009](#bib.bib2)), BoolQ (Clark
    et al., [2019](#bib.bib10)), WIC (Pilehvar & Camacho-Collados, [2018](#bib.bib36)),
    MultiRC (Khashabi et al., [2018](#bib.bib24))) and multi-class task COPA (Roemmele
    et al., [2011](#bib.bib38)).
  prefs: []
  type: TYPE_NORMAL
- en: Models. We primarily use pre-trained LLaMA-7b (Touvron et al., [2023](#bib.bib46))
    to evaluate the performance of our proposed method on downstream tasks. To further
    demonstrate our method’s versatility, we also test it with OPT-13b (Zhang et al.,
    [2022](#bib.bib55)). Additionally, to examine our method’s scalability, we evaluate
    it on larger models, such as LLaMA-30b.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines. First, we compare our method to the vanilla MeZO to demonstrate how
    sparsification enhances MeZO’s convergence speed and overall performance. Additionally,
    to show that our proposed S-MeZO effectively identifies and modifies crucial parameters,
    we contrast it with R-MeZO (a version of MeZO applying a random mask to select
    parameters for optimization). In addition, we also explore the impact of zero-shot
    optimization on improving a pre-trained language model’s capabilities through
    experiments with zeroth-shot learning and in-context learning techniques (Brown
    et al., [2020](#bib.bib4)). Lastly, to understand the performance gap between
    zeroth-order and first-order optimization in fine-tuning large language models,
    we present results from conventional full-parameter fine-tuning (FT) using the
    Adam optimizer, the most widely used method for such tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Training Procedure. We adopt most of the training hyperparameters from the standard
    MeZO, including dataset configuration, batch size, training epochs, epsilon value,
    and task prompts, with the key difference being a higher learning rate for S-MeZO
    due to updating only a subset of the parameters. The primary goal of our training
    is the next token prediction. For the dataset, we use MeZO’s approach, randomly
    selecting 1,000 examples for training and testing the model on another set of
    1,000 examples (Zhou et al., [2023](#bib.bib56)). We perform the experiments using
    three different seeds and report the average of the outcomes. In addition, the
    total training steps for LLaMA and OPT is 20,000 and we evaluate its performance
    on the test dataset every 100 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the performance of our proposed method S-MeZO, we initially tested
    it on the SuperGLUE benchmark using the LLaMA-7b model. The fine-tuning results,
    presented in Table [1](#S3.T1 "Table 1 ‣ 3.4 Memory-Efficient Implementation of
    Sparse-MeZO ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"), reveal that our S-MeZO method outperforms other
    zero-order (ZO) techniques like MeZO and R-MeZO. For instance, S-MeZO boosts MeZO’s
    accuracy from $71.7\%$ to $80.9\%$). Furthermore, all zeroth-order-based methods
    surpassed the performance of Zero-shot learning and in-context learning, demonstrating
    that zeroth-order optimization significantly enhances the pre-trained model’s
    effectiveness on downstream tasks. Finally, we can find that S-MeZO significantly
    bridges the performance gap between zero-order and first-order optimization methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/790d4e738f15a0605465027cd3fceda8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RTE
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a7a4230136d0ea03d9500f5737a5b96.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) BoolQ
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0396baeff7933ad33a9f75bc1cc3cea4.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) WIC
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Convergence Curves of Fine-Tuning LLaMA-7b with MeZO and Sparse-MeZO
    (S-MeZO) on (a) RTE, (b) BoolQ, (c) WIC tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Convergence Rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To verify that S-MeZO converges faster than MeZO, we carried out multiple experiments
    for comparison. The accuracy over steps is plotted in Figure [5](#S4.F5 "Figure
    5 ‣ 4.2 Performance ‣ 4 Experiments ‣ Sparse MeZO: Less Parameters for Better
    Performance in Zeroth-Order LLM Fine-Tuning"), which shows that S-MeZO can use
    fewer steps to achieve a better performance than vanilla MeZO. For example, S-MeZO
    only needs about 5,000 steps to achieve $70\%$x speedup on BoolQ.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Memory Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.4 Memory Usage ‣ 4 Experiments ‣ Sparse MeZO:
    Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning") shows
    the memory consumption for MeZO, S-MeZO, and traditional full-parameter fine-tuning
    of LLaMA-7b. The data reveal that S-MeZO does not require more memory than MeZO
    and offers a substantial saving of roughly $12$ GB of vanilla S-MeZO to $14.6$
    GB across all five tasks, which also illustrates the efficiency of our proposed
    implementation method: Calculating the Mask During the Forward Pass. Finally,
    we can only use inference memory cost to fine-tune large language models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | SST-2 | RTE | BoolQ | WIC | MultiRC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FT | $114.7$ | $158.6$ |'
  prefs: []
  type: TYPE_TB
- en: '| MeZO | $14.6$ | $14.6$ |'
  prefs: []
  type: TYPE_TB
- en: '| S-MeZO | $28.3$ | $28.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| S-MeZO-EI | $14.6$ | $14.6$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Memory Usage (batch size = 1) of Fine-Tuning LLaMA-7b on SuperGLUE
    (1,000 examples). EI represents the Efficient Implementation in section [3.4](#S3.SS4
    "3.4 Memory-Efficient Implementation of Sparse-MeZO ‣ 3 Proposed Method ‣ Sparse
    MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Sparse Rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For S-MeZO, we need to define the sparsity of the pre-trained model before starting
    to fine-tune it. To analyze the effects of sparsity value on the performance,
    we conduct experiments with various sparsity values (from $0.0$. In addition,
    for most tasks, a sparsity value of $0.8$) to $82.3\%$ to $82.5\%$).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5654a25197cba533f50cabfe9bca37de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The effects of Sparsity for Fine-tuning LLaMA-7b with S-MeZO.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | BoolQ | RTE | WIC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | MeZO | $75.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7b | S-MeZO | $80.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-30b | MeZO | $83.8$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-30b | S-MeZO | 85.7 | 82.1 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Accuracy of Fine-Tuning LLaMA-7b and LLaMA-30b on SuperGLUE (1,000
    examples).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [1](#S3.T1 "Table 1 ‣ 3.4 Memory-Efficient Implementation of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"), we mainly introduce the performance of our methods on LLaMA-7b.
    A direct question is whether our proposed method can scale to larger language
    models. Therefore, in this section, we further explore our proposed method S-MeZO
    on LLaMA-30b. As shown in Table [3](#S4.T3 "Table 3 ‣ 4.5 Sparse Rate ‣ 4 Experiments
    ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning"),
    we can see that the a larger model usually can obtain a better fine-tuned performance.
    For example, the accuracy on RTE with MeZO can be improved from $71.1\%$ on LLaMA-30b.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | BoolQ | RTE | WIC |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13b | Zero Shot | $59.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13b | ICL | $66.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13b | MeZO | $72.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13b | R-MeZO | $72.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13b | S-MeZO | $\bf{73.8}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Accuracy of Fine-Tuning OPT on SuperGLUE (1,000 examples). ICL: In-Context
    Learning, R-MeZO: MeZO with Random Mask.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 The Analysis about Efficient Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In section [3.4](#S3.SS4 "3.4 Memory-Efficient Implementation of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"),we present the efficient implementation of S-MeZO, which enables
    our proposed method to require only the inference memory cost for fine-tuning
    large language models. To analyze the actual GPU memory usage during the training
    process, we provide these results in Table [2](#S4.T2 "Table 2 ‣ 4.4 Memory Usage
    ‣ 4 Experiments ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"). We can find that S-MeZO needs the same GPU memory for all five
    tasks, which can also save about $50\%$ memory compared to sparse-mezo. That also
    illustrates the efficiency of our proposed efficient implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose a novel memory-efficient zeroth-order fine-tuning
    method Sparse-MeZO, which can use a similar memory cost to the inference process.
    We evaluate the performance of fine-tuning LLaMA and OPT with Sparse-MeZO on SuperGULE
    benchmark and the experimental results illustrate that Sparse-MeZO can achieve
    a higher accuracy and faster convergence. Finally, we can fine-tune LLaMA-30b
    in a single A100 GPU. However, there is still a performance gap between our proposed
    method Sparse-MeZO and first-order fine-tuning methods. We plan to address these
    limitations and enhance Sparse-MeZO’s capabilities in our future research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Balasubramanian & Ghadimi (2018) Balasubramanian, K. and Ghadimi, S. Zeroth-order
    (non)-convex stochastic optimization via conditional gradient and gradient updates.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bentivogli et al. (2009) Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo,
    D. The fifth pascal recognizing textual entailment challenge. *TAC*, 7(8):1, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brock et al. (2017) Brock, A., Lim, T., Ritchie, J. M., and Weston, N. Freezeout:
    Accelerate training by progressively freezing layers. *arXiv preprint arXiv:1706.04983*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2020) Cai, H., Gan, C., Zhu, L., and Han, S. Tinytl: Reduce activations,
    not trainable parameters for efficient on-device learning. *arXiv preprint arXiv:2007.11622*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2021) Cai, H., Lou, Y., McKenzie, D., and Yin, W. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. In *International
    Conference on Machine Learning*, pp.  1193–1203\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2022) Cai, H., Mckenzie, D., Yin, W., and Zhang, Z. Zeroth-order
    regularized optimization (zoro): Approximately sparse gradients and adaptive sampling.
    *SIAM Journal on Optimization*, 32(2):687–714, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J.,
    Parasyris, K., Zhang, Y., Zhang, Z., Kailkhura, B., and Liu, S. Deepzero: Scaling
    up zeroth-order optimization for deep model training. *arXiv preprint arXiv:2310.02025*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017) Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
    Zoo: Zeroth order optimization based black-box attacks to deep neural networks
    without training substitute models. In *Proceedings of the 10th ACM workshop on
    artificial intelligence and security*, pp.  15–26, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dagan et al. (2005) Dagan, I., Glickman, O., and Magnini, B. The pascal recognising
    textual entailment challenge. In *Machine learning challenges workshop*, pp. 
    177–190\. Springer, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duchi et al. (2015) Duchi, J. C., Jordan, M. I., Wainwright, M. J., and Wibisono,
    A. Optimal rates for zero-order convex optimization: The power of two function
    evaluations. *IEEE Transactions on Information Theory*, 61(5):2788–2806, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giampiccolo et al. (2007) Giampiccolo, D., Magnini, B., Dagan, I., and Dolan,
    W. B. The third pascal recognizing textual entailment challenge. In *Proceedings
    of the ACL-PASCAL workshop on textual entailment and paraphrasing*, pp.  1–9,
    2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2021) Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., and Pan,
    D. Z. Efficient on-chip learning for optical neural networks through power-aware
    sparse zeroth-order optimization. In *Proceedings of the AAAI conference on artificial
    intelligence*, volume 35, pp.  7583–7591, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haim et al. (2006) Haim, R. B., Dagan, I., Dolan, B., Ferro, L., Giampiccolo,
    D., Magnini, B., and Szpektor, I. The second pascal recognising textual entailment
    challenge. In *Proceedings of the Second PASCAL Challenges Workshop on Recognising
    Textual Entailment*, volume 7, pp.  785–794, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015a) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015b) Han, S., Pool, J., Tran, J., and Dally, W. Learning both
    weights and connections for efficient neural network. *Advances in neural information
    processing systems*, 28, 2015b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanson & Pratt (1988) Hanson, S. and Pratt, L. Comparing biases for minimal
    network construction with back-propagation. *Advances in neural information processing
    systems*, 1, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassibi & Stork (1992) Hassibi, B. and Stork, D. Second order derivatives for
    network pruning: Optimal brain surgeon. *Advances in neural information processing
    systems*, 5, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for nlp. In *International Conference on Machine Learning*,
    pp.  2790–2799\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilyas et al. (2018) Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box
    adversarial attacks with limited queries and information. In *International conference
    on machine learning*, pp.  2137–2146\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khashabi et al. (2018) Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S.,
    and Roth, D. Looking beyond the surface: A challenge set for reading comprehension
    over multiple sentences. In *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)*, pp.  252–262, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of
    scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
    prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020) Lin, Z., Madotto, A., and Fung, P. Exploring versatile generative
    language model via parameter-efficient transfer learning. *arXiv preprint arXiv:2004.03829*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal,
    M., and Raffel, C. A. Few-shot parameter-efficient fine-tuning is better and cheaper
    than in-context learning. *Advances in Neural Information Processing Systems*,
    35:1950–1965, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S.,
    and Amini, L. Zeroth-order stochastic variance reduction for nonconvex optimization.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Liu, S., Chen, P.-Y., Chen, X., and Hong, M. signsgd via zeroth-order
    oracle. In *International conference on learning representations*. International
    Conference on Learning Representations, ICLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III,
    A. O., and Varshney, P. K. A primer on zeroth-order optimization in signal processing
    and machine learning: Principals, recent advances, and applications. *IEEE Signal
    Processing Magazine*, 37(5):43–54, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Liu, Y., Agarwal, S., and Venkataraman, S. Autofreeze: Automatically
    freezing model blocks to accelerate fine-tuning. *arXiv preprint arXiv:2102.01386*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malladi et al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D.,
    Chen, D., and Arora, S. Fine-tuning language models with just forward passes.
    *arXiv preprint arXiv:2305.17333*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ohta et al. (2020) Ohta, M., Berger, N., Sokolov, A., and Riezler, S. Sparse
    perturbations for improved convergence in stochastic zeroth-order optimization.
    In *Machine Learning, Optimization, and Data Science: 6th International Conference,
    LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II 6*,
    pp.  39–64\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pilehvar & Camacho-Collados (2018) Pilehvar, M. T. and Camacho-Collados, J.
    Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.
    *arXiv preprint arXiv:1808.09121*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rebuffi et al. (2017) Rebuffi, S.-A., Bilen, H., and Vedaldi, A. Learning multiple
    visual domains with residual adapters. *Advances in neural information processing
    systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roemmele et al. (2011) Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice
    of plausible alternatives: An evaluation of commonsense causal reasoning. In *2011
    AAAI Spring Symposium Series*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruan et al. (2019) Ruan, Y., Xiong, Y., Reddi, S., Kumar, S., and Hsieh, C.-J.
    Learning to learn by zeroth-order oracle. *arXiv preprint arXiv:1910.09464*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
    C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality
    over a sentiment treebank. In *Proceedings of the 2013 conference on empirical
    methods in natural language processing*, pp.  1631–1642, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spall (1992) Spall, J. C. Multivariate stochastic approximation using a simultaneous
    perturbation gradient approximation. *IEEE transactions on automatic control*,
    37(3):332–341, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ström (1997) Ström, N. Phoneme probability estimation with dynamic sparsely
    connected artificial neural networks. *The Free Speech Journal*, 5(1-41):2, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2022a) Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X.-J., and Qiu,
    X. Bbtv2: towards a gradient-free future with large language models. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pp. 
    3916–3930, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022b) Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box
    tuning for language-model-as-a-service. In *International Conference on Machine
    Learning*, pp.  20841–20855\. PMLR, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sung et al. (2022) Sung, Y.-L., Cho, J., and Bansal, M. Lst: Ladder side-tuning
    for parameter and memory efficient transfer learning. *Advances in Neural Information
    Processing Systems*, 35:12991–13005, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2019) Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi,
    J., Hsieh, C.-J., and Cheng, S.-M. Autozoom: Autoencoder-based zeroth order optimization
    method for attacking black-box neural networks. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 33, pp.  742–749, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vemula et al. (2019) Vemula, A., Sun, W., and Bagnell, J. Contrasting exploration
    in parameter and action space: A zeroth-order optimization perspective. In *The
    22nd International Conference on Artificial Intelligence and Statistics*, pp. 
    2926–2935\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael,
    J., Hill, F., Levy, O., and Bowman, S. Superglue: A stickier benchmark for general-purpose
    language understanding systems. *Advances in neural information processing systems*,
    32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Wang, X., Guo, W., Su, J., Yang, X., and Yan, J. Zarts:
    On zero-order optimization for neural architecture search. *Advances in Neural
    Information Processing Systems*, 35:12868–12880, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Wang, Y., Du, S., Balakrishnan, S., and Singh, A. Stochastic
    zeroth-order optimization in high dimensions. In *International conference on
    artificial intelligence and statistics*, pp.  1356–1365\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2018) Ye, H., Huang, Z., Fang, C., Li, C. J., and Zhang, T. Hessian-aware
    zeroth-order optimization for black-box adversarial attack. *arXiv preprint arXiv:1812.11377*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaken et al. (2021) Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple
    parameter-efficient fine-tuning for transformer-based masked language-models.
    *arXiv preprint arXiv:2106.10199*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen,
    W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning.
    *arXiv preprint arXiv:2303.10512*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma,
    X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. *arXiv
    preprint arXiv:2305.11206*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A The Prompts in LLaMA and OPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Dataset | Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | cls. | {premise} |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Does this mean that “{hypothesis}” is true? Yes or No? |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Yes/No |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | cls. | Suppose “{premise}” Can we infer that “{hypothesis}”? Yes, No,
    or Maybe? |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Yes/No/Maybe |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | cls. | {passage} {question} ? |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Yes/No |'
  prefs: []
  type: TYPE_TB
- en: '| WIC | cls. | Does the word “{word}” have the same meaning in these two sentences?
    Yes, No? |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | {sent1} |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | {sent2} |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Yes/No |'
  prefs: []
  type: TYPE_TB
- en: '| MultiRC | cls. | {paragraph} |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Question: {question} |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | I found this answer “{answer}”. Is that correct? Yes or No? |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Yes/No |'
  prefs: []
  type: TYPE_TB
- en: '| COPA | mch. | {premise} so/because {candidate} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The prompts of the datasets we used in our LLaMA experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will introduce the hyperparameters searching grids in Table [6](#A2.T6 "Table
    6 ‣ Appendix B Hyperparameters ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"), which can help people to reproduce our results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Hyperparameters | Values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MeZO | Batch size | $16$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning rate | $\{5\mathrm{e}{-7},1\mathrm{e}{-6},2\mathrm{e}{-6}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\epsilon$ |'
  prefs: []
  type: TYPE_TB
- en: '| MeZO-Random | Batch size | $16$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning rate | $\{1\mathrm{e}{-6},2\mathrm{e}{-6},3\mathrm{e}{-6},4\mathrm{e}{-6},5\mathrm{e}{-6}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\epsilon$ |'
  prefs: []
  type: TYPE_TB
- en: '| S-MeZO | Batch size | $16$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning rate | $\{1\mathrm{e}{-6},2\mathrm{e}{-6},3\mathrm{e}{-6},4\mathrm{e}{-6},5\mathrm{e}{-6}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\epsilon$ |'
  prefs: []
  type: TYPE_TB
- en: '| FT with Adam | Batch size | $8$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rates | $\{1\mathrm{e}{-5},5\mathrm{e}{-5},8\mathrm{e}{-5}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The hyperparameter searching grids for LLaMA-7b experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: We then introduce the sparsity of each task in SuperGULU when we fine-tune LLaMA-7b.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | SST-2 | RTE | BoolQ | WIC | MultiRC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse MeZO | $0.70$ | $0.80$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Sparsity in SuperGULU when we fine-tune LLaMA-7b.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C The Proof of Lemma 3.2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let $\mathcal{L}_{z}(\theta)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta):$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\mathcal{L}(\theta+\epsilon\hat{z})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We can obtain the Lemma:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=m\odot\mathbb{E}_{z}[\nabla_{\theta}\mathcal{L}(\theta+\epsilon
    m\odot z)]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{z}[\frac{\mathcal{L}(\theta+\epsilon m\odot
    z)-\mathcal{L}(\theta-\epsilon m\odot z)}{2\epsilon}m\odot z]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\widehat{\nabla}_{\theta}\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=m\odot\nabla_{\theta}\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=m\odot\int_{\hat{z}}\nabla_{\theta}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{1}{k}m\odot\int_{\hat{z}}\frac{z}{\epsilon}e^{-\frac{1}{2}\&#124;z\&#124;^{2}}\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=m\odot\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)\frac{z}{\epsilon}dz$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[m\odot\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where we can define $y=\theta+\epsilon z$ is the number of 1 in $\boldsymbol{m}$.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can obtain the gradient $\nabla_{\theta}\mathcal{L}_{m}(\theta)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we will prove $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)}{\epsilon}\hat{z}]$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'After that, we can get the relationship between $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta)-\mathcal{L}(\theta-\epsilon\hat{z})}{\epsilon}\hat{z}]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta)-\mathcal{L}(\theta-\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z}-\mathcal{L}(\theta))}{\epsilon}\hat{z}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Based on the Equation [11](#A3.E11 "Equation 11 ‣ Appendix C The Proof of Lemma
    3.2 ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM
    Fine-Tuning") and Equation [12](#A3.E12 "Equation 12 ‣ Appendix C The Proof of
    Lemma 3.2 ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"), we can obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Finally, we can obtain the relationship between $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    and finish the proof.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D The Proof of Lemma 3.3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can first define the distance between $\widehat{\nabla}_{\boldsymbol{\boldsymbol{\boldsymbol{\theta}}}}\mathcal{L}_{\hat{z}}(\boldsymbol{\boldsymbol{\theta}})=\mathbb{E}_{\hat{z}}[\boldsymbol{g_{\hat{z}}(\theta)}]$
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\&#124;\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)-\nabla_{\theta}\mathcal{L}_{m}(\theta)\&#124;$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{\epsilon L(l)}{2}\mathbb{E}_{\hat{z}}[\&#124;\hat{z}\&#124;^{3}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\frac{\epsilon L(l)}{2}(\hat{d}+3)^{\frac{3}{2}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\hat{d}$ and obtain that $\|\boldsymbol{a}\|^{2}\leq 2\|\boldsymbol{a}-\boldsymbol{b}\|^{2}+2\|\boldsymbol{b}\|^{2}$,
    we can obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\nabla_{\theta}\mathcal{L}_{m}(\theta)\&#124;^{2}$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix E The Proof of Theorem 3.4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta)-\mathcal{L}(\theta)$ |  |
    (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)-\epsilon\langle\nabla\mathcal{L}(\theta),\hat{z}\rangle]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{\epsilon^{2}L(l)}{2}\mathbb{E}_{\hat{z}}[\&#124;\hat{z}\&#124;^{2}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\frac{\epsilon^{2}L(l)}{2}\hat{d}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The first inequality holds because Lipschitz Continuous: $|\mathcal{L}(\theta^{\prime})-\mathcal{L}(\theta)-\langle\nabla\mathcal{L}(\theta),\theta^{\prime}-\theta\rangle|\leq\frac{L(l)}{2}\|\theta^{\prime}-\theta\|^{2}$
    is the number of $1$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\frac{\epsilon^{4}L^{2}(l)}{2}\hat{d}^{2}+\frac{\epsilon^{4}L^{2}(l)}{2}\hat{d}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\epsilon^{4}L^{2}(l)\hat{d}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The first inequality is due to $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$.
    The second inequality is due to the Equation [17](#A5.E17 "Equation 17 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ² | $1$2 |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The first inequality is due to $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle[\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)]^{2}$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'The first inequality is due to $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$.
    The last inequality holds because Equation [18](#A5.E18 "Equation 18 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning") and Equation [19](#A5.E19 "Equation 19 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{z,x}[\&#124;g_{\hat{z}}(\theta)\&#124;^{2}]$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The first inequality holds because $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq
    2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$, $\mathbb{E}_{\hat{z}}[\|\hat{z}\|^{p}]\leq(\hat{d}+p)^{\frac{p}{2}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the assumption about Lipschitz Continuous, we can obtain: $1$2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'Based on the equation, we can follow the update rule: $\theta_{t+1}=\theta_{t}-\eta_{t}g_{\hat{z}}(\theta_{t})$
    and we can find:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta_{t+1})$ |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\eta_{t}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{E}_{\hat{z},x}[\mathcal{L}_{\hat{z}}(\theta_{t+1})]$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'The first inequality is due to the Equation [9](#A3.E9 "Equation 9 ‣ Appendix
    C The Proof of Lemma 3.2 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning") and Equation [23](#A5.E23 "Equation 23 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"). The second inequality holds because Equation
    [21](#A5.E21 "Equation 21 ‣ Appendix E The Proof of Theorem 3.4 ‣ Sparse MeZO:
    Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning") provides
    the result about $\mathbb{E}_{\hat{z},x}[\|g_{z}(\theta_{t})\|^{2}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can select learning rate $\eta_{t}=\frac{1}{4(\hat{d}_{t}+4)L(l)}$
    and obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'Then, taking the sum of Equation [25](#A5.E25 "Equation 25 ‣ Appendix E The
    Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance in
    Zeroth-Order LLM Fine-Tuning") over the index from $T+1$, we can have that :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'where $L(l)\leq L$. Thus, based on Lemma [6](#S3.E6 "Equation 6 ‣ Lemma 3.3\.
    ‣ 3.2 Convergence Analysis of Sparse-MeZO ‣ 3 Proposed Method ‣ Sparse MeZO: Less
    Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning"), we can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{\hat{z},x}[\&#124;\nabla\mathcal{L}_{m}(\theta_{T})\&#124;^{2}]$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'The second inequality is due to the Equation [26](#A5.E26 "Equation 26 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"). To obtain $\sigma$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 16(\hat{d}+4)L\frac{\mathcal{L}_{\hat{z}}(\theta_{0})-\mathcal{L}_{\hat{z}}^{*}}{T+1}+\mathcal{O}(\epsilon^{2}L^{2}\hat{d}^{3})$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle T$ |  |'
  prefs: []
  type: TYPE_TB
- en: Finally, we can finish the proof of the theorem. This theorem illustrates that
    the presence of pronounced sparsity patterns, along with the smoothness of the
    objective function, can significantly enhance the rate of convergence, potentially
    achieving a linear acceleration.
  prefs: []
  type: TYPE_NORMAL
