- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02969](https://ar5iv.labs.arxiv.org/html/2403.02969)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Junwen He Work done during internship at DAMO Academy. Dalian University of
    Technology DAMO Academy, Alibaba Group Yifan Wang Dalian University of Technology
    Lijun Wang Corresponding author Dalian University of Technology Huchuan Lu Dalian
    University of Technology Jun-Yan He DAMO Academy, Alibaba Group Jin-Peng Lan DAMO
    Academy, Alibaba Group Bin Luo DAMO Academy, Alibaba Group Xuansong Xie DAMO Academy,
    Alibaba Group
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multimodal Large Language Model (MLLMs) leverages Large Language Models as a
    cognitive framework for diverse visual-language tasks. Recent efforts have been
    made to equip MLLMs with visual perceiving and grounding capabilities. However,
    there still remains a gap in providing fine-grained pixel-level perceptions and
    extending interactions beyond text-specific inputs. In this work, we propose AnyRef,
    a general MLLM model that can generate pixel-wise object perceptions and natural
    language descriptions from multi-modality references, such as texts, boxes, images,
    or audio. This innovation empowers users with greater flexibility to engage with
    the model beyond textual and regional prompts, without modality-specific designs.
    Through our proposed refocusing mechanism, the generated grounding output is guided
    to better focus on the referenced object, implicitly incorporating additional
    pixel-level supervision. This simple modification utilizes attention scores generated
    during the inference of LLM, eliminating the need for extra computations while
    exhibiting performance enhancements in both grounding masks and referring expressions.
    With only publicly available training data, our model achieves state-of-the-art
    results across multiple benchmarks, including diverse modality referring segmentation
    and region-level referring expression generation. Code and models are available
    at [https://github.com/jwh97nn/AnyRef](https://github.com/jwh97nn/AnyRef)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have garnered widespread influence across various
    domains, and advancements have been achieved by augmenting LLMs with visual perception
    modules to bridge the gap between vision and language tasks [[6](#bib.bib6), [23](#bib.bib23),
    [18](#bib.bib18), [61](#bib.bib61)], thereby transforming them into Multimodal
    Large Language Models (MLLMs). Most recent research aims to further endow MLLMs
    with finer-grained visual understanding abilities, like visual grounding and referring
    expression generation, through user-defined formats (*e.g.*, coordinates, bounding
    boxes, etc.) [[31](#bib.bib31), [4](#bib.bib4), [57](#bib.bib57)], surpassing
    the confines of textual responses alone.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/190fdbdb5f4685d51e3fe657e2459075.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Multi-modality Referring Segmentation and Expression Generation with
    AnyRef. Our model possesses the capacity to generate natural language descriptions
    as well as pixel-wise grounding masks for the referred object. It accommodates
    various referring modalities such as text, bounding boxes, images and audio, enabling
    more flexible user interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the encouraging results demonstrated by existing MLLMs in grounding
    linguistic expressions to visual scenes, their capacity for precise localization
    remains restricted to coarse-grained levels (bounding boxes), falling short of
    pixel-level perceptions (As illustrated in [Tab. 1](#S1.T1 "In 1 Introduction
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception")). The
    most recent work, as exemplified by [[16](#bib.bib16)], has focused on enhancing
    MLLMs by integrating segmentation models that generate binary segmentation masks
    based on textual descriptions. However, this approach is constrained by its reliance
    solely on textual referring instructions, thereby limiting the versatility of
    MLLMs in various multimodal interaction scenarios, such as region-based referring
    or audio comprehension tasks. The interactive segmentation model SEEM [[63](#bib.bib63)]
    attempts to receive audio inputs, but it turns audio into textural prompts with
    the off-the-shelf speech recognition model Whisper [[34](#bib.bib34)], so essentially
    it is still the textual references.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Image | Referring Format | Pixel-level Grounding | End-to-End Model
    |'
  prefs: []
  type: TYPE_TB
- en: '| Region | Image* | Audio |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA (NeurIPS-23) [[23](#bib.bib23)] | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| BuboGPT (arXiv-23) [[58](#bib.bib58)] | ✓ | ✗ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Vision-LLM (arXiv-23) [[41](#bib.bib41)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DetGPT (arXiv-23) [[41](#bib.bib41)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| KOSMOS-2 (arXiv-23) [[31](#bib.bib31)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Shikra (arXiv-23) [[4](#bib.bib4)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4RoI (arXiv-23) [[57](#bib.bib57)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| NExT-GPT (arXiv-23) [[44](#bib.bib44)] | ✓ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ASM (arXiv-23) [[42](#bib.bib42)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| LISA (arXiv-23) [[16](#bib.bib16)] | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef (Ours) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparisons of recent Multi-modal Large Language Models. The term
    *Referring Format* emphasizes the acceptable modalities used for referencing,
    whereas *Image** indicates visual references derived from another image.'
  prefs: []
  type: TYPE_NORMAL
- en: In light of the above observation, we propose AnyRef, a novel multi-modal instruction-tuned
    LLM with fine-grained visual perception. As shown in [Tab. 1](#S1.T1 "In 1 Introduction
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception"), AnyRef
    advances existing MLLMs with the strong capability to perform pixel-level object
    grounding and generate region-aware expressions derived from references of diverse
    modalities, including text, bounding boxes, images, and audio inputs, (See [Fig. 1](#S1.F1
    "In 1 Introduction ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual
    Perception") as an example). To this end, we first propose a unified representation
    for referring across different modalities and map them to the token space of LLMs.
    We extract features from all the modalities mentioned above to form the *Unified
    Referring Representation*, which can be processed uniformly by the LLM, utilizing
    its ability of understanding and reasoning in generating the grounded output.
    This enables flexible referring beyond textual descriptions, without requiring
    modality-specific designs or changes to the existing model.
  prefs: []
  type: TYPE_NORMAL
- en: To perform pixel-level grounding with LLMs, a possible solution [[16](#bib.bib16)]
    is to trigger the segmentation action by generating a special token <obj>, whose
    embedding will be subsequently employed as the input to the segmentation model.
    As opposed to using coordinates sequence of polygons [[41](#bib.bib41), [5](#bib.bib5)]
    to represent segmentation results, the introduction of the <obj> token effectively
    simplifies pixel-level visual grounding. Nevertheless, the embedding of the <obj>
    token is confined in a fixed feature space, due to the nature of next token prediction,
    leading to limited representational capacity and thus inaccurate segmentation
    results. To address this constraint, we propose a simple yet effective *refocusing
    mechanism*, which takes into account the correlation between the grounded expression
    and the <obj> token. This mechanism utilizes attention scores to weight such correlation,
    enhancing the mask embedding with additional grounded embeddings, and since the
    attention scores are intermediate outputs of the self-attention layers, the additional
    computation introduced by the refocusing mechanism is minimal. Furthermore, the
    refocusing mechanism also provides a short-cut connection between the generated
    grounded expression and the segmentation results, allowing pixel-level labels
    to implicitly supervise the learning process of language expression generation,
    thereby enhancing the model’s regional understanding capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, our contributions are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce AnyRef, the first general MLLM capable of producing pixel-level
    object perceptions as well as region-aware referring descriptions. It adeptly
    accommodates multi-modality references including texts, bounding boxes, images
    or audio in a general manner, fostering more flexible interactions for users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a simple yet effective *refocusing mechanism* to enhance the grounded
    mask predictions, leveraging the correlations of generated tokens without incurring
    additional computational overhead, and concurrently yields improvements in regional
    expression referring.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorough experiments conducted on multiple datasets demonstrate the efficacy
    of the proposed method, resulting in state-of-the-art performance across a diverse
    range of multi-modality tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our model is built upon LLaVA-7B [[23](#bib.bib23)], which can be efficiently
    fine-tuned with 8 NVIDIA 32G V100 GPUs, making our method easily reproducible
    at a reasonable computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Multi-modal Large Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-modal Large Language Models (MLLMs), built upon large language models
    (LLMs) as their foundations, extend their capabilities beyond traditional textual
    understanding to incorporate various modalities such as images, videos, and audio.
    Building upon the concept of instruction tuning, Flamingo [[1](#bib.bib1)] utilizes
    visual feature inputs as prompts, resulting in impressive performance across diverse
    visual-language tasks such as image captioning and visual question answering (VQA).
    Subsequent models, includin BLIP-2 [[19](#bib.bib19)], LLaVA [[23](#bib.bib23)],
    InstructBLIP [[6](#bib.bib6)], Otter [[18](#bib.bib18)] and LLaMa-Adapter [[56](#bib.bib56)],
    utilize additional generated visual instruction-following data for better visual-language
    alignment, and demonstrate impressive multi-modal chat abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies expand the capabilities of MLLMs to address localization tasks
    with region-aware functionalities. KOSMOS-2 [[31](#bib.bib31)] and VisionLLM [[41](#bib.bib41)]
    introduce additional location tokens to the vocabulary, enabling the conversion
    of coordinates into textual representations. These representations are then inputted
    into LLMs to enhance region understanding. On the other hand, Shikra [[4](#bib.bib4)]
    represents coordinates directly in natural language form. In contrast, GPT4RoI
    [[57](#bib.bib57)] streamlines the process by employing RoI-aligned visual features
    without incorporating explicit positional information.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, these models lack the capacity to produce fine-grained perceptions
    (*e.g.*, pixel-level masks), and restrict their referring expressions to textural
    descriptions and regions within the image. Our model, leveraging the best of both
    worlds, not only generates pixel-level grounding masks, but also accommodates
    a broader range of referring formats (*e.g.*, visual reference from other images
    or audio) in a unified manner.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Referring Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Referring Expression Segmentation translates explicit textual descriptions into
    corresponding pixel-level segmentations, requiring a comprehensive understanding
    of both visual content and linguistic expression. Recent methods including SAM
    [[15](#bib.bib15)], X-Decoder [[62](#bib.bib62)] and SEEM [[63](#bib.bib63)] unify
    multiple segmentation tasks within a single model, supporting various human interaction
    methods. While LISA [[16](#bib.bib16)] utilizes the powerful reasoning and comprehension
    abilities of LLMs to process textural instructions and generate masks through
    the SAM [[15](#bib.bib15)] decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Referring Segmentation can be related to one/few-shot segmentation, where
    an example of a certain object with its corresponding mask is provided to segment
    the same object in the query image [[30](#bib.bib30), [12](#bib.bib12), [55](#bib.bib55),
    [43](#bib.bib43), [44](#bib.bib44)]. Recently, CLIPSeg [[28](#bib.bib28)] builds
    upon the CLIP model to treat the example image as a visual prompt, which can generalize
    to novel forms of prompts. Painter [[43](#bib.bib43)] and SegGPT [[44](#bib.bib44)]
    utilize in-context learning to perform general vision tasks using input task prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Audio-Visual Segmentation aims to generate pixel-level masks for object(s) emitting
    sound, initially introduced in [[60](#bib.bib60)]. AVSegFormer [[8](#bib.bib8)]
    innovatively incorporates learnable audio queries, enabling selective attention
    to relevant visual features. Additionally, AUSS [[21](#bib.bib21)] proposes unmixing
    self-supervised losses to bridge the gap between audio signals and visual semantics.
  prefs: []
  type: TYPE_NORMAL
- en: While these models have achieved satisfactory results in their respective domains,
    there is currently a gap in addressing all referring tasks within a single model.
    Most of the aforementioned methods rely on modality-specific or task-specific
    designs, which may not generalize well beyond their intended tasks. Our approach
    leverages the robust comprehension ability of LLMs to concurrently tackle all
    these tasks while preserving the region-level reasoning capacity. Additionally,
    the *refocusing mechanism* aids in enhancing region-level referring expression
    through implicit pixel-level supervisions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The overall framework of AnyRef comprises a vision encoder, multi-modal feature
    projection layers, a LLM, and a mask decoder, as illustrated in [Fig. 2](#S3.F2
    "In 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception").
    These initial three components together form a multi-modality LLM, enabling support
    for various reference formats and generating region-aware grounded textual responses.
    Additionally, a distinctive <obj> token is introduced to the vocabulary, which
    provides the input for the mask decoder through a refocusing mechanism, facilitating
    the generation of pixel-level perceptions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5542b0b33d1e5090525be307985c957.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overall pipeline of AnyRef. Vision-language, audio-language projection
    and MLP layers are omitted for simplicity and clarity. The Unified Referring Representation
    ([Sec. 3.1.1](#S3.SS1.SSS1 "3.1.1 Unified Referring Representation ‣ 3.1 Model
    Architecture ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception")) receives references from diverse types of modalities and
    transforms them into embeddings aligned with the LLM. The Refocusing Mechanism
    ([Sec. 3.1.2](#S3.SS1.SSS2 "3.1.2 Refocusing Mechanism ‣ 3.1 Model Architecture
    ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception"))
    enhances the embedding from the single <obj> token with grounded textural embeddings,
    thus providing a broader representational capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We adopt the pretrained ViT-L/14 from CLIP [[33](#bib.bib33)] as the vision
    encoder, and LLaMA-7B [[39](#bib.bib39)] as our LLM. For audio inputs, we choose
    the pretrained audio encoder from ImageBind [[9](#bib.bib9)] to extract audio
    features. To connect multi-modality information beyond texts to the existing LLM,
    such as images and audio, we adopt vision-language and audio-language projection
    layers to project image and audio features to the language space. The input image
    is converted into a fixed number of $16\times 16$ patch embeddings. Both the image
    and audio embeddings are then projected to the same dimension as word embeddings.
    The LLM takes the interleaved embeddings in the same way as language tokens to
    generate outputs via an auto-regressive manner.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Unified Referring Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To receive multi-modality referring prompts beyond texts, we convert them into
    fixed-sized tokens and *quote* them between newly introduced special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'For visual prompts including regional bounding boxes or visual examples from
    another image, we introduce <img_ref> and </img_ref>, where visual features will
    be inserted in between. Drawing inspiration from [[57](#bib.bib57)], we represent
    bounding boxes using extracted region-level features from RoIAlign [[11](#bib.bib11)]
    with a fixed size of $4\times 4$ as well. To refer to them in the same way as
    textual descriptions, we build prompts such as: “Can you provide a description
    of <img_ref><img_feat></img_ref> in this image?”, where <img_feat> will be replaced
    by the extracted visual features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For audio prompts, we introduce <aud_ref> and </aud_ref> for LLM to be aware
    of audio referring inputs, and the extracted audio features will be projected
    through audio-language projection layer and then inserted in between. And the
    audio prompted instruction will be built like: “Can you segment the object that
    makes sound of <aud_ref><aud_feat></aud_ref> in this image?”. In this way, the
    referring representation from different modalities is unified, which can be treated
    the same way as language instructions and easily handled by the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03d25812dd92c4d1aebbbc1f142fb0a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Qualitative results of AnyRef’s applicable capabilities on multiple
    tasks, including (a) referring expression segmentation, (b) region-level captioning
    and grounding, (c) image-level referring segmentation and (d) audio-visual segmentation.
    AnyRef demonstrates proficiency in generating both textual responses and pixel-level
    perceptions across diverse modality instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Refocusing Mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by [[16](#bib.bib16)], we employ another special token <obj> to succinctly
    represent the instance segmentation mask as an embedding. This embedding $\boldsymbol{h}_{obj}$
    can be expressed mathematically as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M=\mathcal{S}\Big{(}\gamma(\boldsymbol{h}_{obj}),\mathcal{V}_{seg}(\boldsymbol{x}_{img})\Big{)},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{x}_{img}$ denotes the vision encoder of the segmentation
    model.
  prefs: []
  type: TYPE_NORMAL
- en: However, since <obj> is a token in the LLM vocabulary, its representation will
    be limited in a fixed feature range, which will potentially limit its representational
    capacity and influence the decoded mask quality. Therefore, we propose a *refocusing
    mechanism* which augments the original mask embedding with grounded text embeddings.
    The motivation behind is to explicitly force the final mask embedding to focus
    more on the referring or grounded object with its textural expression. The updated
    mask embedding can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\boldsymbol{h}}_{obj}=\boldsymbol{h}_{obj}+\lambda_{f}\sum_{i}^{i<obj}\bar{\boldsymbol{a}}_{i}\cdot\boldsymbol{h}_{i},\vspace{-0.3cm}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $i\text{\textless}obj$ controls the focusing weight of augmentation embeddings.
    This approach enhances the mask embedding, providing a more adaptable feature
    range compared to the original, thereby expanding its representational capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Training Objectives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The model is trained in the end-to-end manner with a combination of text loss
    and mask loss. The text loss follows the next word prediction loss [[23](#bib.bib23)],
    and the mask loss includes binary cross-entropy loss and dice loss [[29](#bib.bib29)],
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where we choose $\lambda_{text}=1.0$ Due to the *refocusing mechanism*, tokens
    generated before the <obj> token can receive additional supervisory signals from
    pixel-level ground truth. This mutual interaction can further benefit the vision-language
    understanding ability of AnyRef, given the interrelated nature of referring expressions
    and grounding masks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Implementation Details.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1 Training Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unless otherwise specified, we employ the pre-trained CLIP ViT-L/14 as the vision
    encoder, ImageBind-H [[9](#bib.bib9)] as the audio encoder, and LLaMa-7B as the
    LLM. The vision-language projection layer is initialized from LLaVa [[23](#bib.bib23)],
    while the audio-language projection layer is randomly initialized. The word embeddings
    of newly introduced special tokens are initialized randomly. Furthermore, the
    segmentation model utilizes the pre-trained SAM-H [[15](#bib.bib15)]. The image
    resolution is $224\times 224$ by rescaling and padding for the segmentation model.
    For audio inputs, we follow settings in [[60](#bib.bib60)] to use the 5-second
    audio clips and convert to 3 fixed-sized embeddings after padding, since the ImageBind
    [[9](#bib.bib9)] audio encoder samples 2-second audio each time.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure training efficiency and preserve generalization ability, we freeze
    the vision encoders and audio encoder. Fine-tuning of the LLM is conducted using
    LoRA [[13](#bib.bib13)], and the trainable parameters comprise the mask decoder
    and projection layers, accounting for approximately 7% of the total parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We conduct training using 8 NVIDIA V100 GPUs, each with a batch size of 6, and
    employ a gradient accumulation step set to 8. The training utilizes mixed precision,
    converting both the vision and audio encoder to float16 precision. AdamW [[26](#bib.bib26)]
    optimizer with a learning rate of 5e-5 and weight decay of 0.01 is employed, alongside
    a cosine annealing scheduler incorporating 200 warmup steps. LoRA operates with
    the rank of 8 and alpha of 16, exclusively applied to query and value projections
    within the LLM. We employ ZeRO stage-2 [[35](#bib.bib35)] with DeepSpeed [[37](#bib.bib37)]
    which completes network training in 10K steps.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method |  | RefCOCO |  | RefCOCO+ |  | RefCOCOg |'
  prefs: []
  type: TYPE_TB
- en: '|  | val | testA | testB |  | val | testA | testB |  | val(U) | test(U) |'
  prefs: []
  type: TYPE_TB
- en: '| *Specialist Segmentation Models* |'
  prefs: []
  type: TYPE_TB
- en: '| CRIS [[45](#bib.bib45)] |  | 70.5 | 73.2 | 66.1 |  | 65.3 | 68.1 | 53.7 |  |
    59.9 | 60.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LAVT [[49](#bib.bib49)] |  | 72.7 | 75.8 | 68.8 |  | 62.1 | 68.4 | 55.1 |  |
    61.2 | 62.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GRES [[22](#bib.bib22)] |  | 73.8 | 76.5 | 70.2 |  | 66.0 | 71.0 | 57.7 |  |
    65.0 | 66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| PolyFormer [[25](#bib.bib25)] |  | 76.0 | 78.3 | 73.3 |  | 69.3 | 74.6 |
    61.9 |  | 69.2 | 70.2 |'
  prefs: []
  type: TYPE_TB
- en: '| UNINEXT [[48](#bib.bib48)] |  | 82.2 | 83.4 | 81.3 |  | 72.5 | 76.4 | 66.2
    |  | 74.7 | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SEEM [[63](#bib.bib63)] |  | - | - | - |  | - | - | - |  | 65.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| *Generalist MLLMs* |'
  prefs: []
  type: TYPE_TB
- en: '| X-Decoder [[62](#bib.bib62)] |  | - | - | - |  | - | - | - |  | 64.6 | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| LISA-7B [[16](#bib.bib16)] |  | 74.1 | 76.5 | 71.1 |  | 62.4 | 67.4 | 56.5
    |  | 66.4 | 68.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LISA-7B (ft) [[16](#bib.bib16)] |  | 74.9 | 79.1 | 72.3 |  | 65.1 | 70.8
    | 58.1 |  | 67.9 | 70.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef |  | 74.1 | 75.5 | 70.8 |  | 64.1 | 68.7 | 57.5 |  | 68.1 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef (ft) |  | 76.9 | 79.9 | 74.2 |  | 70.3 | 73.5 | 61.8 |  | 70.0 | 70.7
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Referring expression segmentation results (cIOU) on RefCOCO(+/g) datasets.
    (ft) denotes finetuning the model on RefCOCO(+/g) datasets. Our model surpasses
    all generalist models and most specialist (segmentation-oriented) models.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training process involves a diverse range of datasets. For general semantic
    and instance segmentation, COCO-Stuff [[3](#bib.bib3)], ADE20K [[59](#bib.bib59)],
    and PACO-LVIS [[36](#bib.bib36)] are utilized, with one category chosen per batch.
    Referring expression segmentation incorporates RefClef, RefCOCO, RefCOCO+ [[14](#bib.bib14)],
    RefCOCOg [[52](#bib.bib52)], and PhraseCut [[46](#bib.bib46)]. Image-level referring
    segmentation adopts the method outlined in [[27](#bib.bib27)], where samples are
    chosen from COCO [[20](#bib.bib20)], PascalVOC [[7](#bib.bib7)], and PhraseCut
    [[46](#bib.bib46)] datasets. Random cropped samples are drawn from images that
    contain the same category as their corresponding linguistic expressions. Region-level
    captioning involves RefCOCO(+/g) and Flickr30K Entities [[32](#bib.bib32)]. Audio-visual
    segmentation employs AVSBench [[60](#bib.bib60)] with both single and multiple
    sound sources. To prevent data leakage, samples with images in the validation
    or test splits are excluded.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We assess the capabilities of our model through evaluations on various benchmarks,
    including different modality referring segmentation (text/image/audio) for pixel-level
    perception and referring expression generation for regional understanding. Models
    are categorized as *specialists* or *generalists*, with the former designed exclusively
    for specific tasks. We provide examples for each task in [Fig. 3](#S3.F3 "In 3.1.1
    Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods ‣ Multi-modal
    Instruction Tuned LLMs with Fine-grained Visual Perception"), and more illustrations
    can be found in the supplementary material.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Multi-modality Referring Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Referring Expression Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The task involves labeling pixels within an image corresponding to an object
    instance referred to by a linguistic expression. We instruct our model as: “Can
    you segment {exp} in this image?”, where {exp} is the given explicit description.
    Evaluation is conducted using Cumulative-IoU (cIoU) as the metric. We make comparisons
    with state-of-the-art models on validation and test sets of RefCOCO, RefCOCO+
    and RefCOCOg [[14](#bib.bib14), [52](#bib.bib52)]. As shown in [Tab. 2](#S3.T2
    "In 3.2.1 Training Setup ‣ 3.2 Implementation Details. ‣ 3 Methods ‣ Multi-modal
    Instruction Tuned LLMs with Fine-grained Visual Perception"), our performance
    surpasses all generalist models and most specialist models except UNINEXT-H [[48](#bib.bib48)],
    which is trained using a considerably larger dataset that includes video samples.
    Specialist models excel solely at segmentation-related tasks, while generalist
    models possess additional capabilities for generating textural descriptions and
    are capable of handling more complex references.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Image Referring Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Predicting masks using image examples is akin to one- or few-shot segmentation,
    where regions corresponding to the highlighted object in the example image must
    be located in a query image. We prompt our model with queries like “Can you find
    similar object of <img_ref><img_feat></img_ref> in this image?”, where <img_feat>
    denotes pooled features from example images as detailed in [Sec. 3.1.1](#S3.SS1.SSS1
    "3.1.1 Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods ‣
    Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception"). The
    evaluation takes place under the in-domain setting on $\text{COCO-20}^{i}$ [[7](#bib.bib7)]
    for a fair comparison, as most classes are encountered during the training stages.
    In the few-shot evaluation, the model inferences multiple times using different
    example images, with the averaged mask serving as the final prediction. In our
    referring examples, we do not have corresponding mask examples, which is different
    from the standard setting. we follow [[28](#bib.bib28)] to crop out the target
    object for highlighting, using their segmentation masks. As demonstrated in [Tab. 3](#S4.T3
    "In 4.1.2 Image Referring Segmentation ‣ 4.1 Multi-modality Referring Segmentation
    ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual
    Perception"), our model achieves competitive performance compared to state-of-the-art
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | $\text{COCO-20}^{i}$ |'
  prefs: []
  type: TYPE_TB
- en: '| one-shot | few-shot | one-shot | few-shot |'
  prefs: []
  type: TYPE_TB
- en: '| *Specialist Segmentation Models* |'
  prefs: []
  type: TYPE_TB
- en: '| HSNet* [[30](#bib.bib30)] | 41.7 | 50.7 | 68.7 | 73.8 |'
  prefs: []
  type: TYPE_TB
- en: '| VAT* [[12](#bib.bib12)] | 42.9 | 49.4 | 72.4 | 76.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CLIPSeg [[28](#bib.bib28)] | 33.2 | - | 59.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SegGPT [[44](#bib.bib44)] | 56.1 | 67.9 | 83.2 | 89.8 |'
  prefs: []
  type: TYPE_TB
- en: '| *Generalist Multi-task Models* |'
  prefs: []
  type: TYPE_TB
- en: '| Painter [[43](#bib.bib43)] | 32.8 | 32.6 | 64.5 | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef | 43.5 | 51.3 | 74.8 | 78.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef$\dagger$ | 46.3 | 55.2 | 76.5 | 80.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Quantitative results of example-based few-shot segmentation. * indicates
    that the categories in training cover that in testing as in [[44](#bib.bib44)],
    and $\dagger$ denotes using mask cropping setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Audio-Visual Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The AVS benchmark comprises single- and multi-sources subsets based on the number
    of sounding objects. We utilize prompts like, “Can you segment the object(s) that
    produce sound of <aud_ref><aud_feat></aud_ref> in this image?”, to instruct the
    model for mask predictions. Following [[60](#bib.bib60)], evaluation metrics include
    mean IoU (mIoU) for region similarity and F-score¹¹1$F_{\beta}=\frac{(1+\beta^{2})\times\text{precision}\times\text{recall}}{\beta^{2}\times\text{precision}+\text{recall}}$
    following [[60](#bib.bib60)] for contour accuracy. The quantitative results in
    [Tab. 4](#S4.T4 "In 4.1.3 Audio-Visual Segmentation ‣ 4.1 Multi-modality Referring
    Segmentation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception") demonstrate that our model consistently outperforms most methods
    on single-source split, indicating successful alignment of audio features with
    the LLM during fine-tuning. However, when confronted with audios containing multiple
    sound sources, our model encounters challenges in producing masks that cover more
    than one object. Moreover, owing to the ability of LLM, our model can determine
    the textural category of the sounding objects, as depicted in [Fig. 3](#S3.F3
    "In 3.1.1 Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception") (d).
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Single-source |  | Multi-source |'
  prefs: []
  type: TYPE_TB
- en: '| mIOU | F-score |  | mIOU | F-score |'
  prefs: []
  type: TYPE_TB
- en: '| AVS [[60](#bib.bib60)] | 78.7 | 0.879 |  | 54.0 | 0.645 |'
  prefs: []
  type: TYPE_TB
- en: '| BG [[10](#bib.bib10)] | 81.7 | 0.904 |  | 55.1 | 0.668 |'
  prefs: []
  type: TYPE_TB
- en: '| AVSegformer [[8](#bib.bib8)] | 82.1 | 0.899 |  | 58.4 | 0.693 |'
  prefs: []
  type: TYPE_TB
- en: '| AUSS [[21](#bib.bib21)] | 89.4 | 0.942 |  | 63.5 | 0.752 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef | 82.8 | 0.908 |  | 55.6 | 0.663 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Quantitative results of audio-visual segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Referring Expression Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This task involves generating a textual description associated with an object
    based on its location (bounding box). We evaluate our generated expressions using
    automatic caption generation metrics, including CIDEr [[40](#bib.bib40)] and Meteor
    [[17](#bib.bib17)], on RefCOCO, RefCOCO+ and RefCOCOg. Our model achieves remarkable
    performance among generalist LLM-based models and demonstrates competitive result
    to specialist models, as shown in [Tab. 5](#S4.T5 "In 4.2 Referring Expression
    Generation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/409c1bfbf46a6b85572de2a398fafdff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of generated expressions between ground-truth and LLM-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, as stated in [[54](#bib.bib54), [24](#bib.bib24), [2](#bib.bib2)],
    standard automated evaluation metrics do not authentically capture generation
    quality due to the constraints of ground-truth expressions. This scenario is particularly
    pronounced in open-text generation, especially for LLM-based models. These models
    have the ability to generate rich, natural sentences, while the provided ground-truth
    expressions often tend to be concise, as indicated in [Fig. 4](#S4.F4 "In 4.2
    Referring Expression Generation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned
    LLMs with Fine-grained Visual Perception").
  prefs: []
  type: TYPE_NORMAL
- en: To further evaluate the quality of the generated expressions, we conduct human
    evaluations following [[51](#bib.bib51), [54](#bib.bib54), [2](#bib.bib2)]. We
    randomly select 100 images from the validation datasets and ask five human raters
    to choose the bounding box that best matches the generated expression, and the
    averaged score is considered the final result. In [Tab. 6](#S4.T6 "In 4.2 Referring
    Expression Generation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with
    Fine-grained Visual Perception"), we present the results of the human evaluations,
    including both traditional methods and LLM-based methods. The LLM-based methods
    produce more detailed descriptions, closely resembling human behavior, which are
    preferred by the human raters. We provide more examples in supplementary material.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method |  | RefCOCO |  | RefCOCO+ |  | RefCOCOg |'
  prefs: []
  type: TYPE_TB
- en: '|  | testA | testB |  | testA | testB |  | val |'
  prefs: []
  type: TYPE_TB
- en: '|  | Meteor | CIDEr | Meteor | CIDEr |  | Meteor | CIDEr | Meteor | CIDEr |  |
    Meteor | CIDEr |'
  prefs: []
  type: TYPE_TB
- en: '| *Specialist Models* |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Visdif [[53](#bib.bib53)] |  | 18.5 | - | 24.7 | - |  | 14.2 | - | 13.5 |
    - |  | 14.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SLR [[54](#bib.bib54)] |  | 29.6 | 77.5 | 34.0 | 132.0 |  | 21.3 | 52.0 |
    21.5 | 73.5 |  | 15.9 | 66.2 |'
  prefs: []
  type: TYPE_TB
- en: '| easyREG [[38](#bib.bib38)] |  | 31.3 | 83.7 | 34.1 | 132.9 |  | 24.2 | 66.4
    | 22.8 | 78.7 |  | 17.0 | 77.7 |'
  prefs: []
  type: TYPE_TB
- en: '| IREG [[50](#bib.bib50)] |  | 34.9 | 105.4 | 37.3 | 154.1 |  | 30.8 | 89.8
    | 26.4 | 97.0 |  | 19.4 | 101.2 |'
  prefs: []
  type: TYPE_TB
- en: '| *Generalist MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GRIT [[47](#bib.bib47)] |  | - | - | - | - |  | - | - | - | - |  | 15.2 |
    71.6 |'
  prefs: []
  type: TYPE_TB
- en: '| KOSMOS-2 [[31](#bib.bib31)] |  | - | - | - | - |  | - | - | - | - |  | 14.1
    | 62.3 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef |  | 23.9 | 74.8 | 26.7 | 118.6 |  | 16.4 | 59.4 | 14.3 | 62.9 |  |
    16.2 | 69.0 |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef (ft) |  | 30.4 | 79.5 | 32.7 | 138.6 |  | 23.2 | 67.7 | 20.1 | 80.1
    |  | 17.1 | 79.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Quantitative results on region-level referring expression generation.
    *Generalist models* (LLM-based) perform poorly on automated evaluation metrics
    due to the limitation of constrained ground-truth expressions, as stated in [Sec. 4.2](#S4.SS2
    "4.2 Referring Expression Generation ‣ 4 Experiments ‣ Multi-modal Instruction
    Tuned LLMs with Fine-grained Visual Perception").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method |  | RefCOCO |  | RefCOCO+ |'
  prefs: []
  type: TYPE_TB
- en: '|  | testA | testB |  | testA | testB |'
  prefs: []
  type: TYPE_TB
- en: '| SLR [[54](#bib.bib54)] |  | 66% | 62% |  | 43% | 38% |'
  prefs: []
  type: TYPE_TB
- en: '| SLR+Rerank [[54](#bib.bib54)] |  | 73% | 77% |  | 49% | 46% |'
  prefs: []
  type: TYPE_TB
- en: '| KOSMOS-2 [[31](#bib.bib31)] |  | 88% | 84% |  | 63% | 65% |'
  prefs: []
  type: TYPE_TB
- en: '| Shikra [[4](#bib.bib4)] |  | 91% | 81% |  | 59% | 62% |'
  prefs: []
  type: TYPE_TB
- en: '| AnyRef |  | 87% | 80% |  | 67% | 66% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Human evaluation on referring expression generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1d85971c1fd210961eeb9d2f04d7172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visualization of mask embeddings before and after the *refocusing
    mechanism*. original denotes original mask embeddings, while vehicle, person,
    and animal represent the updated mask embeddings corresponding to their respective
    referring objects contained in the textural expression.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct extensive ablation studies to reveal the contribution of each component.
  prefs: []
  type: TYPE_NORMAL
- en: '| $\lambda_{f}$ | RefCOCOg |  | AVSBench |  | RefCOCOg |'
  prefs: []
  type: TYPE_TB
- en: '| cIOU |  | mIOU |  | Meteor | CIDEr |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0 | 68.7 |  | 81.4 |  | 16.8 | 71.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 67.1 |  | 80.6 |  | 14.3 | 68.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 70.0 |  | 82.8 |  | 17.1 | 73.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0$\dagger$ | 68.0 |  | 81.1 |  | 15.7 | 70.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1$\dagger$ | 69.3 |  | 82.0 |  | 17.0 | 73.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Ablation study on refocusing weight $\lambda_{f}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Refocusing Mechanism. We first investigate the effectiveness of enhancing the
    <obj> token through *refocusing mechanism*, and explore the impact of different
    refocusing weights $\lambda_{f}$ for our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: We further employ PCA to visualize the mask embeddings before and after implementing
    the *refocusing mechanism* in [Fig. 5](#S4.F5 "In 4.3 Ablation Study ‣ 4 Experiments
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception") We
    choose three subsets representing different referring objects including vehicles,
    persons and animals (*e.g.*, the person subset comprises output expressions containing
    “person,” “man,” “woman,” etc.). The visualization illustrates that the *refocusing
    mechanism* results in a wider representation range of the mask embedding. Moreover,
    the updated embeddings demonstrate a clustering pattern aligned with the associated
    textual expressions, contributing to a more precise decoding of masks.
  prefs: []
  type: TYPE_NORMAL
- en: Training Datasets. The impact of different types of datasets is validated in
    [Tab. 8](#S4.T8 "In 4.3 Ablation Study ‣ 4 Experiments ‣ Multi-modal Instruction
    Tuned LLMs with Fine-grained Visual Perception"), and evaluation is carried out
    on RefCOCOg validation split. Region/Image Ref. refers to region-level and image-level
    referring data, as explained in [Sec. 3.2.2](#S3.SS2.SSS2 "3.2.2 Datasets ‣ 3.2
    Implementation Details. ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with
    Fine-grained Visual Perception"). It becomes apparent that the model’s generalization
    improves as the type of datasets increases.
  prefs: []
  type: TYPE_NORMAL
- en: '| Exp. | Referring | General | Region Ref. | Image Ref. | cIOU |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | ✓ |  |  |  | 66.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | ✓ | ✓ |  |  | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | ✓ | ✓ | ✓ |  | 67.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | ✓ | ✓ |  | ✓ | 67.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | ✓ | ✓ | ✓ | ✓ | 68.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Ablation study on training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present AnyRef, a pioneering MLLM model capable of generating pixel-level
    object perceptions and language descriptions from various modality references,
    including texts, regions, images, and audio. This is made possible by the unified
    referring representation, which connects different types of inputs to the LLM.
    We further propose a refocusing mechanism that uses attention scores to improve
    the segmentation embedding and enhance pixel-level vision perception. Across various
    downstream tasks, our model exhibits remarkable performance while providing users
    with enhanced interacting flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements. This work is supported by the National Natural Science Foundation
    of China (U23A20386, 62276045, 62293540, 62293542), Dalian Science and Technology
    Talent Innovation Support Plan (2022RY17).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.
    *Advances in Neural Information Processing Systems*, 35:23716–23736, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bracha et al. [2023] Lior Bracha, Eitan Shaar, Aviv Shamsian, Ethan Fetaya,
    and Gal Chechik. Disclip: Open-vocabulary referring expression generation. *arXiv
    preprint arXiv:2305.19108*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caesar et al. [2018] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
    Coco-stuff: Thing and stuff classes in context. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 1209–1218, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
    Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s referential dialogue magic.
    *arXiv preprint arXiv:2306.15195*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2022] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J
    Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks. *Advances
    in Neural Information Processing Systems*, 35:31333–31346, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. [2010] Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge.
    *International journal of computer vision*, 88:303–338, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2023] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu.
    Avsegformer: Audio-visual segmentation with transformer. *arXiv preprint arXiv:2307.01146*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girdhar et al. [2023] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
    Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding
    space to bind them all. In *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. [2023] Dawei Hao, Yuxin Mao, Bowen He, Xiaodong Han, Yuchao Dai,
    and Yiran Zhong. Improving audio-visual segmentation with bidirectional generation.
    *arXiv preprint arXiv:2308.08288*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    Mask r-cnn. In *Proceedings of the IEEE international conference on computer vision*,
    pages 2961–2969, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. [2022] Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong
    Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation.
    In *European Conference on Computer Vision*, pages 108–126\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazemzadeh et al. [2014] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
    Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes.
    In *Proceedings of the 2014 conference on empirical methods in natural language
    processing (EMNLP)*, pages 787–798, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
    Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C.
    Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. *arXiv:2304.02643*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. [2023] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,
    Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model.
    *arXiv preprint arXiv:2308.00692*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lavie and Agarwal [2007] Alon Lavie and Abhaya Agarwal. METEOR: An automatic
    metric for MT evaluation with high levels of correlation with human judgments.
    In *Proceedings of the Second Workshop on Statistical Machine Translation*, pages
    228–231, Prague, Czech Republic, 2007\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023a] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang
    Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning.
    *arXiv preprint arXiv:2305.03726*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023b] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2:
    bootstrapping language-image pre-training with frozen image encoders and large
    language models. In *ICML*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *Computer Vision–ECCV 2014: 13th European Conference,
    Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13*, pages 740–755\.
    Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. [2023] Yuhang Ling, Yuxi Li, Zhenye Gan, Jiangning Zhang, Mingmin
    Chi, and Yabiao Wang. Hear to segment: Unmixing the audio to guide the semantic
    segmentation. *arXiv preprint arXiv:2305.07223*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023a] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized
    referring expression segmentation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 23592–23601, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023b] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    Visual instruction tuning. In *NeurIPS*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2017] Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring expression
    generation and comprehension via attributes. In *Proceedings of the IEEE International
    Conference on Computer Vision*, pages 4856–4864, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023c] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar
    Satzoda, Vijay Mahadevan, and R Manmatha. Polyformer: Referring image segmentation
    as sequential polygon generation. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pages 18653–18663, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lüddecke and Ecker [2022] Timo Lüddecke and Alexander Ecker. Image segmentation
    using text and image prompts. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 7086–7096, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lüddecke and Ecker [2022] Timo Lüddecke and Alexander Ecker. Image segmentation
    using text and image prompts. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pages 7086–7096, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Milletari et al. [2016] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
    V-net: Fully convolutional neural networks for volumetric medical image segmentation.
    In *2016 fourth international conference on 3D vision (3DV)*, pages 565–571\.
    IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min et al. [2021] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze
    for few-shot segmentation. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang,
    Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models
    to the world. *arXiv preprint arXiv:2306.14824*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plummer et al. [2017] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes,
    Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities:
    Collecting region-to-phrase correspondences for richer image-to-sentence models.
    *IJCV*, 123(1):74–93, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    In *International Conference on Machine Learning*, pages 28492–28518\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pages 1–16\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramanathan et al. [2023] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic,
    Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek
    Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, and Dhruv Mahajan. PACO: Parts
    and attributes of common objects. In *arXiv preprint arXiv:2301.01795*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasley et al. [2020] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. Deepspeed: System optimizations enable training deep learning models
    with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, pages 3505–3506, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanaka et al. [2019] Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro
    Sato, Yoshitaka Ushiku, and Tatsuya Harada. Generating easy-to-understand referring
    expressions for target identifications. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 5794–5803, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4566–4575, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large
    language model is also an open-ended decoder for vision-centric tasks. *arXiv
    preprint arXiv:2305.11175*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023b] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang
    Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing
    project: Towards panoptic visual recognition and understanding of the open world.
    *arXiv preprint arXiv:2308.01907*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023c] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun
    Huang. Images speak in images: A generalist painter for in-context visual learning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 6830–6839, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023d] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua
    Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. *arXiv preprint
    arXiv:2304.03284*, 2023d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo,
    Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 11686–11695, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2020] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu
    Maji. Phrasecut: Language-based image segmentation in the wild. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    10216–10225, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2022] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng
    Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer
    for object understanding. *arXiv preprint arXiv:2212.00280*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2023] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan, Ping
    Luo, and Huchuan Lu. Universal instance perception as object discovery and retrieval.
    In *CVPR*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2022] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang
    Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring
    image segmentation. In *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2023] Fulong Ye, Yuxing Long, Fangxiang Feng, and Xiaojie Wang. Whether
    you can locate or not? interactive referring expression generation. In *Proceedings
    of the 31st ACM International Conference on Multimedia*, pages 4697–4706, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2016a] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
    and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part II 14*, pages 69–85\. Springer, 2016a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2016b] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
    and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part II 14*, pages 69–85\. Springer, 2016b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2016c] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
    and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part II 14*, pages 69–85\. Springer, 2016c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. [2017] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint
    speaker-listener-reinforcer model for referring expressions. In *Proceedings of
    the IEEE conference on computer vision and pattern recognition*, pages 7282–7290,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022] Jian-Wei Zhang, Yifan Sun, Yi Yang, and Wei Chen. Feature-proxy
    transformer for few-shot segmentation. *Advances in Neural Information Processing
    Systems*, 35:6575–6588, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023a] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun
    Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter:
    Efficient fine-tuning of language models with zero-init attention. *arXiv preprint
    arXiv:2303.16199*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023b] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
    Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large
    language model on region-of-interest, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi
    Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms.
    *arXiv preprint arXiv:2307.08581*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2019] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,
    Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through
    the ade20k dataset. *International Journal of Computer Vision*, 127:302–321, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2022] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing
    Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio–visual
    segmentation. In *European Conference on Computer Vision*, pages 386–403\. Springer,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. [2023a] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,
    Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized
    decoding for pixel, image, and language. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 15116–15127, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. [2023b] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
    Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. *arXiv
    preprint arXiv:2304.06718*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
