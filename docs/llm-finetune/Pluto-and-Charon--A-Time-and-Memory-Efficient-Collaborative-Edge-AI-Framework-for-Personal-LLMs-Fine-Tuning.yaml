- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:34:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.10746](https://ar5iv.labs.arxiv.org/html/2408.10746)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bei Ouyang^★¹, Shengyuan Ye^★¹, Liekang Zeng², Tianyi Qian¹, Jingyi Li¹, Xu
    Chen^†¹ ¹School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou,
    China ²IoT Thrust and Research Center for Digital World with Intelligent Things,
    HKUST (GZ), Guangzhou, China [ouyb9, yeshy8,qianty,lijy573@mail2.sysu.edu.cn,
    liekangzeng@hkust-gz.edu.cn, chenxu35@mail.sysu.edu.cn](mailto:ouyb9,%20yeshy8,qianty,lijy573@mail2.sysu.edu.cn,%20liekangzeng@hkust-gz.edu.cn,%20chenxu35@mail.sysu.edu.cn)(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have unlocked a plethora of powerful applications
    at the network edge, such as intelligent personal assistants. Data privacy and
    security concerns have prompted a shift towards edge-based fine-tuning of personal
    LLMs, away from cloud reliance. However, this raises issues of computational intensity
    and resource scarcity, hindering training efficiency and feasibility. While current
    studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate
    resource constraints, our analysis indicates that these techniques are not sufficiently
    resource-efficient for edge devices. Other studies focus on exploiting the potential
    of edge devices through resource management optimization, yet are ultimately bottlenecked
    by the resource wall of individual devices.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory
    efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks
    the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system
    co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique
    that is efficient in terms of parameters, time, and memory. It utilizes Parallel
    Adapters to circumvent the need for a full backward pass through the LLM backbone.
    Additionally, an activation cache mechanism further streamlining the process by
    negating the necessity for repeated forward passes across multiple epochs. (2)
    Systematically, PAC leverages edge devices in close proximity, pooling them as
    a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid
    data and pipeline parallelism to orchestrate distributed training. The use of
    the activation cache eliminates the need for forward pass through the LLM backbone,
    enabling exclusive fine-tuning of the Parallel Adapters using data parallelism.
    Extensive evaluation based on prototype implementation demonstrates that PAC remarkably
    outperforms state-of-the-art approaches, achieving up to $8.64\times$ reduction
    in memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edge intelligence, large language model, parameter-efficient fine-tuning, pipeline
    parallelism, data parallelism, parallel processing$\bigstar$: Corresponding author.^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: The 53rd International Conference
    on Parallel Processing; August 12–15, 2024; Gotland, Sweden^†^†booktitle: The
    53rd International Conference on Parallel Processing (ICPP ’24), August 12–15,
    2024, Gotland, Sweden^†^†doi: 10.1145/3673038.3673043^†^†isbn: 979-8-4007-1793-2/24/08'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) (Vaswani et al., [2017](#bib.bib23); Raffel et al.,
    [2020](#bib.bib21); Lewis et al., [2019](#bib.bib14)) have ushered in a revolution
    in machine intelligence, owing to their exceptional capabilities in a wide range
    of machine learning tasks. While born on datacenter warehouse, LLMs have quickly
    sunk to edge devices and facilitated a range of intelligent applications at the
    network edge, such as intelligent personal assistants (IPAs) which are software
    agents that can augment individuals’ abilities, complete complicated tasks, and
    even satisfy emotional needs. A recent survey (Li et al., [2024](#bib.bib15))
    targeting LLM-based IPAs has revealed that over $80\%$ of industry experts believe
    that, owing to the sensitive and privacy-critical nature of user data, personal
    LLMs should be fully (or primarily) hosted at the edge in order to enable privacy-preserving
    model personalization and serving. Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning") illustrates the scenario of hosting a personal
    LLM-based intelligent agent within a smart home. A personal LLM agent provides
    users with high-performance, privacy-preserving intelligent services. Meanwhile,
    the agent also tracks user interactions, learns from experiences, and extracts
    knowledge to fine-tune the personal LLMs and further enhance the service quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9512761aaf7918d70324b7c94b854462.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An illustration of hosting personal LLM-based intelligent agents
    within a smart home.
  prefs: []
  type: TYPE_NORMAL
- en: While the serving of LLMs on edge devices has been made feasible through careful
    engineering (Guo et al., [2023](#bib.bib7); Xu et al., [2023](#bib.bib27); Ye
    et al., [2024a](#bib.bib29)), fine-tuning these models remains significantly challenging
    due to the resource-intensive nature of LLM training. Towards alleviating the
    resource challenges, some research works (Cai et al., [2023](#bib.bib5); Miao
    et al., [2024](#bib.bib18)) have explored parameter-efficient fine-tuning (PEFT)
    techniques, such as Adapters (Houlsby et al., [2019](#bib.bib10)) and LoRA (Hu
    et al., [2021](#bib.bib11)), which modify less than $2\%$ GB with LoRA and $6.8$
    GB with Adapters, is often unaffordable as typical mobile devices only possess
    4-12GB DRAMs in total to run both system software and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Other leading researchers have explored designing sophisticated resource management
    mechanisms (e.g., CPU-DSP co-execution (Xu et al., [2022](#bib.bib26)), memory
    budget adapting (Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)))
    to leverage native resources, but are still bottlenecked by the intrinsic resource
    shortage of single device. To break the resource wall of a single device, we alternatively
    observe that prevalent edge environments like smart homes usually comprise a group
    of trusted idle devices beyond a single terminal (e.g., phones and smart-home
    devices). These accompanying devices are typically in physical proximity and can
    be associated as a resource augmentation for in-situ personal LLMs fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'As motivated, in this paper, we introduce Pluto and Charon (PAC), a time and
    memory efficient collaborative edge AI framework for personal LLMs fine-tuning.
    PAC’s contribution goes beyond merely leveraging distributed edge devices, instead
    it breaks the resource wall of in-situ personal LLMs fine-tuning with a sophisticated
    algorithm-system co-design:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ (Algorithm) We evaluate two predominant PEFT techniques, Adapters
    and LoRA, and reveal that although parameter efficient, these techniques do not
    achieve sufficient resource efficiency. In light of the side-tuning (Zhang et al.,
    [2020](#bib.bib35)) techniques, we employ not only parameter but also time and
    memory-efficient personal LLMs fine-tuning techniques with Parallel Adapters,
    which provides a dedicated gradient "highway" for the trainable parameters. Additionally,
    our Parallel Adapters stand out from other PEFT techniques by preserving the invariant
    intermediate activations from the LLM backbone for any given input sequence. By
    reusing these cached activations across multiple epochs, PAC increases resource
    efficiency and reduces fine-tuning latency by eliminating repetitive forward propagation
    through the LLM backbone.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ (System) We leverage edge devices in physical proximity and associate
    them as an edge resource pool for in-situ personal LLMs fine-tuning. Our fine-tuning
    process can be divided into two phases: (1) For the first epoch, the LLMs backbone,
    augmented with Parallel Adapters, is fine-tuned across multiple edge devices.
    To enhance scalability and training throughput, a hybrid parallelism approach
    that combines the merits of both data and pipeline parallelism is employed by
    PAC as a principle to manage collaborative training across multiple edge devices.
    (2) In subsequent fine-tuning epochs, the activation cache obviates the need for
    forward propagation through the LLM backbone, allowing for the exclusive fine-tuning
    of our Parallel Adapters using data parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: We implement PAC in realistic testbeds with a cluster of edge devices. Extensive
    evaluations across three LLMs demonstrate that PAC not only accelerates fine-tuning
    up to $8.64\times$, without sacrificing model performance. The main contributions
    are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We carry out extensive measurement studies on predominant PEFT techniques on
    resource-constrained edge devices and demonstrate that they are not sufficiently
    resource-efficient.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We design a not only parameter but also resource efficient LLM fine-tuning technique
    for resource-limited edge environments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a time and memory efficient collaborative edge AI framework PAC for
    the in-situ fine-tuning of personal LLMs, which combines sophisticated algorithm-system
    co-design.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement PAC and evaluate it in realistic edge testbeds. Experimental results
    show up to $8.64\times$ memory reduction without sacrificing performance compared
    to state-of-the-art methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0a5cedce86b76470a6fc2bb57edf2ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Illustration of the model structures with two PEFT.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. MOTIVATION AND PRELIMINARIES
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Transformer-Based LLMs and Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transformer-Based LLMs. Transformer-based LLMs have gained prominence in various
    language-related applications due to their impressive performance. These models
    consist of multiple Transformer layers, each comprising two main components: the
    Multi-head Attention and the Feed Forward block. The Multi-head Attention block
    utilizes linear layers to generate query (Q), key (K), and value (V) matrices
    for each attention head, allowing for independent self-attention computations.
    The outputs of these attention heads are then concatenated and processed through
    a final linear layer. The Feed Forward block involves two linear operations that
    increase the hidden size from $h$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Personal LLMs Fine-Tuning. The training of LLMs typically consists of two stages:
    pre-training and fine-tuning. Before being deployed for specific tasks, language
    models are often pre-trained on extensive text datasets containing vast linguistic
    data. The pre-training process enables the model to acquire a general understanding
    of linguistic structure and patterns that are widely applicable. The fine-tuning
    adapts the pre-trained model to various, concrete downstream language tasks such
    as intelligent personal assistants. During actual deployment, the data required
    for fine-tuning is often generated at the user end, which can carry significant
    concerns regarding data security and privacy. In recent years, in-situ learning
    on edge devices (Patil et al., [2022](#bib.bib20); Lin et al., [2022](#bib.bib16);
    Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)) has emerged as
    a promising approach for customizing LLMs while preserving user data fully in-situ.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11faa94b183b15703eb9a2abccb91113.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. The comparison of floating point of operations (FLOPs). Mini-batch
    size: 16; sequence length: 128.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Techniques | Trainable Parameters | Memory Footprint (GB) |'
  prefs: []
  type: TYPE_TB
- en: '| Weights | Activations | Gradients | Total |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 737M (100%) | 2.75 | 5.33 | 2.75 | 10.83 |'
  prefs: []
  type: TYPE_TB
- en: '| Adapters | 12M (1.70 %) | 2.80 | 4.04 | 0.05 | 6.89 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 9M (1.26%) | 2.78 | 4.31 | 0.04 | 7.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Inference | / | 2.75 | / | / | 2.75 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1\. The breakdown of memory footprint. "Activations" contain the intermediate
    results and optimizer states. Model: T5-Large; mini-batch size: 16; sequence length:
    128.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Full model fine-tuning updates all parameters of an LLM for a specific downstream
    task. However, it is impractical for adapting an LLM to multiple distinct downstream
    tasks, as each target task would require maintaining a separate LLM with whole
    parameters. Some leading researchers have proposed parameter-efficient fine-tuning
    (PEFT) techniques (Lester et al., [2021](#bib.bib13); Houlsby et al., [2019](#bib.bib10);
    Hu et al., [2021](#bib.bib11); Liu et al., [2024](#bib.bib17)) which adapt a small
    subset of the LLM parameters or a set of newly added parameters for each new task.
    Adapters (Houlsby et al., [2019](#bib.bib10)) and LoRA (Hu et al., [2021](#bib.bib11))
    are two of the most widely used PEFT techniques. Figure [2](#S1.F2 "Figure 2 ‣
    1\. Introduction ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") illustrates how the transformer
    layer structure incorporates these two techniques. Specifically, adapters are
    compact bottleneck modules inserted at the end of each transformer layer. Similarly,
    LoRA injects trainable low-rank matrices into a frozen pre-trained model. These
    decompose the weight matrix parameter updates into two learnable low-rank matrices.
    Extensive experiments have demonstrated that these PEFT techniques can achieve
    performance comparable to full fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Although these PEFT techniques can greatly reduce the number of trainable parameters
    (around $98\%$. The reason is that both Adapters and LoRA introduce trainable
    structures within the LLM backbone, such as at the end of each transformer block
    or as bypasses to linear layers. Computing gradients for trainable parameters
    via backpropagation involves traversing the LLM backbone, compromising the efficiency
    of PEFT techniques due to the additional computational overhead and memory required
    to maintain considerable intermediate activations in LLM backbone.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Personal LLMs Fine-Tuning with Resource-Constrained Edge Devices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On-device fine-tuning enables leveraging idle resources at the edge while fully
    preserving user data privacy (Patil et al., [2022](#bib.bib20); Lin et al., [2022](#bib.bib16);
    Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)). This paradigm
    is widely adopted in privacy-sensitive edge computing applications. However, the
    resource-intensive nature of LLMs fine-tuning presents two significant challenges
    for resource-limited edge devices: (1) The computational capabilities of edge
    devices are constrained. Edge devices often face stark computational constraints
    compared to the powerful accelerators available in cloud datacenters. The Jetson
    Nano (jet, [2019](#bib.bib2)), a specialized platform for edge AI, peaks at a
    mere $0.47$ of the parameters, they still require substantial memory $6.89$, often
    insufficient for typical mobile devices with 4-12 GB DRAM to run system software
    and applications.'
  prefs: []
  type: TYPE_NORMAL
- en: To break the resource wall of a single edge device, in our work, we alternatively
    observe that prevalent edge scenarios usually comprise a group of trusted idle
    devices beyond a single terminal. These accompanying devices are typically located
    in close physical proximity, such as being connected to the same local area network
    (LAN), and can be utilized as a resource augmentation for in-situ LLMs fine-tuning
    acceleration. While several pioneering research works (Ye et al., [2024a](#bib.bib29);
    Wei et al., [2024](#bib.bib25)) have delved into collaborative edge computing
    to overcome resource limitations faced by edge devices, the majority of these
    works primarily focus on LLMs inference. Other studies (Cai et al., [2023](#bib.bib5);
    Xu et al., [2024](#bib.bib28)) employing federated learning for fine-tuning LLMs
    with collaborative edge devices primarily address the dissolution of data silos,
    rather than resource augmentation within LANs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57879c4b56dc1f4c7e6aa5b609b738ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. PAC workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. System Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PAC is a time, memory and parameter efficient collaborative framework for personal
    LLMs fine-tuning across multiple edge devices. PAC first equips the target LLM
    with our Parallel Adapters module (Step <svg id="S3.p1.1.pic1" class="ltx_picture"
    height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74)
    matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF"
    stroke="#FFFFFF" color="#FFFFFF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></svg>). PAC profiler
    fine-tunes the LLM using a calibration dataset on edge devices to record the runtime
    profile required for parallelism planning (Step <svg id="S3.p1.2.pic2" class="ltx_picture"
    height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74)
    matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF"
    stroke="#FFFFFF" color="#FFFFFF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>). PAC planner
    then takes the profiling results as input and generates planning configurations,
    including LLM partitioning points and device grouping strategies (Step <svg id="S3.p1.3.pic3"
    class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g
    transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>).
    We configure the Parallel Adapters as trainable while freezing the LLM backbone
    parameters (Step <svg id="S3.p1.4.pic4" class="ltx_picture" height="15.74" overflow="visible"
    version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0
    0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>). The parallel
    configurations generated by the PAC planner are then applied to the edge devices,
    enabling time, memory, and parameter-efficient hybrid data and pipeline parallelism
    fine-tuning of the target LLM (Step <svg id="S3.p1.5.pic5" class="ltx_picture"
    height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74)
    matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF"
    stroke="#FFFFFF" color="#FFFFFF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>). Since the
    LLM backbone parameters remain fixed, the intermediate activations generated by
    the backbone model are invariant for a given input sequence. The PAC maintains
    a cache of these invariant activations. Through leveraging the cached activations,
    the efficiency of the fine-tuning process can be accelerated (Step <svg id="S3.p1.6.pic6"
    class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g
    transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Time, Memory and Parameter Efficient Fine-Tuning Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1\. Fine-Tuning LLMs with Parallel Adapters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Observation and Key Insight. As discussed in §[2](#S2 "2\. MOTIVATION AND PRELIMINARIES
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning"), while techniques such as LoRA (Hu et al., [2021](#bib.bib11))
    and Adapters (Houlsby et al., [2019](#bib.bib10)) reduce the number of parameters
    that need to be updated during fine-tuning, they do not significantly reduce the
    computational and memory requirements during the training on edge devices. This
    is because the parameters being updated are still inside the LLM backbone. To
    calculate the gradients for backpropagation, the full backward passes through
    the entire pre-trained model are still necessary, as illustrated in Figure [5](#S4.F5
    "Figure 5 ‣ 4.1\. Fine-Tuning LLMs with Parallel Adapters ‣ 4\. Time, Memory and
    Parameter Efficient Fine-Tuning Algorithm ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")(a) and
    (b). In the research field of AI, side-tuning (Zhang et al., [2020](#bib.bib35))
    is a specialized fine-tuning technique. It adds a trainable side network that
    runs in parallel to the backbone model, with the side network’s representation
    summed with the backbone’s output in the final layer. Crucially, side-tuning only
    updates the side network, without backpropagating through the backbone model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c00b2abea6e9315a36e2cb3f5b137344.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Comparison between LLMs fine-tuning with LoRA, Adapters, and our
    Parallel Adapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel Adapters Architecture. In light of side-tuning, we employ a time and
    memory efficient personal LLMs fine-tuning technique with Parallel Adapters. The
    overall structure is illustrated in Figure [5](#S4.F5 "Figure 5 ‣ 4.1\. Fine-Tuning
    LLMs with Parallel Adapters ‣ 4\. Time, Memory and Parameter Efficient Fine-Tuning
    Algorithm ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI
    Framework for Personal LLMs Fine-Tuning")(c). Specifically, we decouple conventional
    Adapters (Houlsby et al., [2019](#bib.bib10)) from the LLM backbone, avoiding
    their integration at the end of each transformer layer. Instead, we provide a
    dedicated parallel highway for our trainable adapters network, which takes intermediate
    activations from the backbone transformer as input and generates the final predictions.
    In this way, backpropagation through the LLM backbone is free, reducing memory
    demands for massive activations and computational burdens, thereby enhancing time
    and memory efficiency over techniques like Adapters and LoRA. Our adapters module
    demonstrates comprehensive compatibility with established LLM fine-tuning adapters
    architectures, including the use of linear layers for upward and downward projections
    as well as trimmed lightweight versions of the backbone transformer (Houlsby et al.,
    [2019](#bib.bib10); Han et al., [2024](#bib.bib8); Sung et al., [2022](#bib.bib22)).
    To ensure the lightweight and resource-efficient nature of our parallel network,
    the hidden dimension of our Parallel Adapters will be $r$ intermediate outputs
    $\mathrm{b}_{1},\mathrm{b}_{2},\ldots\mathrm{b}_{L}$. We denote the embedding
    input sequence as $\mathrm{b}_{0}\in\mathbb{R}^{n\times d}$, $\mathbf{a}_{i}\in\mathbb{R}^{n\times
    r}$ for $i$-th adapter of our Parallel Adapters, which operate on these intermediate
    outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\displaystyle\mathrm{a}_{i}=f_{i}(\mathrm{b}_{i},\mathrm{a}_{i-1}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Our evaluation in §[6](#S6 "6\. Evaluation ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning") reveals
    that parallel adapters can achieve comparable model performance to mainstream
    fine-tuning techniques while being more resource-efficient and better suited for
    resource-constrained edge environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. PAC Activation Cache for Parallel Adapters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Observation and Opportunities. Leveraging Parallel Adapters substantially diminishes
    the computational and memory demands by circumventing backward propagation through
    the LLM backbone. However, for edge environments with limited resources, forward
    propagation calculations on the backbone of LLMs also require substantial computational
    resources. Figure [3](#S2.F3 "Figure 3 ‣ 2.1\. Transformer-Based LLMs and Fine-Tuning
    ‣ 2\. MOTIVATION AND PRELIMINARIES ‣ Pluto and Charon: A Time and Memory Efficient
    Collaborative Edge AI Framework for Personal LLMs Fine-Tuning") demonstrates that
    the computational overhead for forward propagation constitutes $54\%$ of the total
    overhead when fine-tuning the T5-Large with Adapters and LoRA, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize the computational demand, we identify two distinct opportunities
    for utilizing Parallel Adapters in in-situ fine-tuning of LLMs: (1) During the
    pre-training phase of LLMs, due to the vast volumes of data involved, researchers
    typically train for only one epoch, meaning each sequence input is processed by
    the model a single time. However, in typical in-situ LLM fine-tuning scenarios,
    users often utilize small datasets collected from their specific context, repeatedly
    training the models with these inputs until achieving model convergence. (2) When
    employing parallel adapters to fine-tune LLMs, the parameters of the LLM backbone
    remain fixed. Unlike other PEFT techniques, the LLM backbone operates independently
    of the intermediate outputs generated by Parallel Adapters. Consequently, for
    a given input sequence, the activations generated by the LLM backbone are always
    invariant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-Tuning Parallel Adapters with PAC Activation Cache. Our key idea leverages
    the frozen parameters of the backbone model, enabling the caching of activations
    produced during the forward propagation of the same input sequence, thereby facilitating
    their reuse across multiple epochs (Cai et al., [2023](#bib.bib5)). As discussed
    in §[4.1](#S4.SS1 "4.1\. Fine-Tuning LLMs with Parallel Adapters ‣ 4\. Time, Memory
    and Parameter Efficient Fine-Tuning Algorithm ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning"), the
    parallel adapters are a lightweight, separate network that takes the intermediate
    activations from the backbone transformer as input and generates predictions.
    During the first epoch, when processing a new input sequence, we cache all the
    input activations required by the Parallel Adapters that are obtained from the
    LLM backbone, as illustrated in Figure [5](#S4.F5 "Figure 5 ‣ 4.1\. Fine-Tuning
    LLMs with Parallel Adapters ‣ 4\. Time, Memory and Parameter Efficient Fine-Tuning
    Algorithm ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI
    Framework for Personal LLMs Fine-Tuning")(c), highlighted by the red circle. In
    subsequent fine-tuning epochs using the same input sequence, we can skip the forward
    propagation through the LLM backbone entirely, since the required activations
    have already been cached. The combination of Parallel Adapters and activation
    caching allows efficient fine-tuning of the LLMs without the need for both forward
    and backward propagation through the backbone network, thereby (1) significantly
    accelerating the fine-tuning process and (2) reducing the memory footprint by
    allowing the release of the memory space occupied by the LLM parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In PAC, we leverage edge devices in physical proximity and associate them as
    a resource pool to boost in-situ fine-tuning. Specifically, the fine-tuning procedure
    comprises two phases: (1) In the initial epoch, the backbone of LLMs, enhanced
    with Parallel Adapters, undergoes fine-tuning across multiple edge devices through
    a blend of data and pipeline parallelism (§[5.1](#S5.SS1 "5.1\. Resource-Efficient
    Collaborative Orchestration for LLMs Fine-Tuning ‣ 5\. Collaborative Edge AI System
    for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")); (2)
    In subsequent epochs, the activation cache eliminates the necessity for forward
    propagation within the backbone, thereby enabling the exclusive fine-tuning of
    our Parallel Adapters utilizing data parallelism (§[5.2](#S5.SS2 "5.2\. Cache-Enabled
    Collaborative Edge Fine-Tuning of Parallel Adapters ‣ 5\. Collaborative Edge AI
    System for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and
    Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Observation of Data and Pipeline Parallelism at the Edge. When collaborating
    on LLM fine-tuning among edge devices, the principle question is which type of
    parallelism should be used. The most common way to train models in parallel is
    data parallelism (DP) (Hao and Zhang, [2021](#bib.bib9)). However, DP necessitates
    that each device maintains a replica of the entire model, a requirement difficult
    to meet for LLMs with extensive parameter sizes, often surpassing the capacity
    of a single device. Pipeline parallelism (PP) (Ye et al., [2022](#bib.bib31))
    is further proposed to address this problem. In PP, the model is partitioned into
    multiple consecutive stages and each stage is mapped to a separate device. Consequently,
    PP enables the training of increasingly large models by deploying more devices.
    Nonetheless, PP encounters scalability constraints as the addition of edge devices
    results in more stages. This not only results in a significant presence of pipeline
    bubbles but also amplifies the impact of inter-stage communication latency, thereby
    hindering efficiency. The above observation motivates us to employ a hybrid parallelism
    (HP) architecture that incorporates the best of both DP and PP, so as to achieve
    superior performance and scalability in resource-constrained edge environments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/697e1a15b3bb48cbbe030fd569cc3624.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The LLM transformer layers is partitioned into two stages, where both Stage
    0 and 1 are replicated on a device group with two devices for intra-stage data
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/687119dfa74008fd91d63c12d5bfb638.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Fine-tuning pipeline of 6 micro-batches. The numbers in the cells represent
    micro-batch ids. AllReduce (AR) is performed in both Stage 0 and 1 for model synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. An instance of hybrid parallelism in PAC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hybrid Parallelism Architecture in PAC. As illustrated in Figure [6(a)](#S5.F6.sf1
    "In Figure 6 ‣ 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
    ‣ 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning"), PAC first divides an LLM into multiple stages where each contains
    a stage model composed of a set of consecutive transformer layer. Edge devices
    are allocated into several device groups, each comprising one or more devices.
    PAC maps each stage to a group, with the stage model replicated across all devices
    within that group. Throughout the fine-tuning process, a mini-batch is divided
    into several micro-batches for concurrent processing to enhance parallelism. If
    a device cluster hosts multiple devices, micro-batches are further subdivided.
    Each device is responsible for executing the forward (FP) and backward passes
    (BP) for its assigned stage model and aggregates gradients across all micro-batches
    for every mini-batch. Upon completing a mini-batch, gradient synchronization within
    each device group is achieved through AllReduce. Since the majority of parameters
    in LLMs are frozen, AllReduce synchronizes only the lightweight parallel adapters,
    ensuring a swift process. We adopt the one-forward-one-backward ($1$B) micro-batch
    scheduling (Narayanan et al., [2019](#bib.bib19)) which schedules the BP early
    to release the activation memory produced by FP for reuse. Figure [6(b)](#S5.F6.sf2
    "In Figure 6 ‣ 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
    ‣ 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning") depicts a well-structured hybrid parallelism, encompassing
    FP, BP, and inter-stage communication.'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling. To enable parallelism planning, PAC profiler first fine-tunes the
    target LLM using calibration datasets to record the runtime profile required for
    planning. We define $t_{f}^{d,l}(\beta)$ with batch size of $\beta$. The size
    of output activations, input gradients, and weight parameters in bytes will also
    be collected to calculate memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Planning Algorithm for Hybrid Parallelism. The global throughput of a pipeline
    is determined by the execution time of the slowest stage. Consequently, our algorithm
    endeavors to partition the model into balanced stages. We consider an LLM consisting
    of $L$ devices in $\mathcal{D}$ with $\mathcal{D}_{n}$ stages. To solve this partitioning
    problem, we break the pipeline into sub-pipelines and leverage the idea of dynamic
    programming. The formula of the dynamic programming algorithm can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle W(0\rightarrow y,\mathcal{D}_{n},s)=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle T(q+1\rightarrow y,\{d_{n-m}\ldots,d_{n-1}\})\},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first term inside the max is the time of the optimally balanced sub-pipeline
    between layers $0$ to $y$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $M$ is the sum of the memory usage of the LLM parameters, parameter gradients,
    and activations. Without out-of-memory (OOM) exceptions, total data-parallel execution
    time is determined by the slowest device If OOM occurs, the time will be set to
    positive infinity. During the dynamic programming, we will record pipeline planning
    configurations, including LLM segmentation points and device groupings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon the completion of dynamic programming process, we obtain a set of balanced
    partition configurations for various number of pipeline stages: $\{W_{s}|\text{
    config. of }W(0\rightarrow L,\mathcal{D},s),s\in\{1,2,...,|\mathcal{D}|\}\}$ and
    $e_{b}^{s}(i)$ and $c_{b}^{s}(i)$. As shown in Figure [6(b)](#S5.F6.sf2 "In Figure
    6 ‣ 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
    ‣ 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning"), we can divide per mini-batch training of $W_{s}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| (5) |  | $\displaystyle L_{n}^{s}=\max_{i\in\{1...,s\}}(\text{AR}^{s}(i)+\displaystyle\sum_{j=i}^{s-1}(e_{b}^{s}(j)+c_{b}^{s}(j)),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (6) |  | $\displaystyle\min_{s}{(L_{b}^{s}+L_{e}^{s}+L_{n}^{s})}.\vspace{-15pt}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Our algorithm aims to minimize this total latency by optimally determining the
    number of stages $s$. In our experiment, the whole planning time is within three
    seconds on an edge device.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning of Parallel Adapters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data-Parallel Fine-Tuning for Parallel Adapters The computationally lightweight
    nature of the Parallel Adapters precludes the use of pipeline parallelism to fine-tuning
    with activation cache, as it would result in unoverlapable inter-stage communication
    latency. Therefore, we employ data parallelism to exclusively fine-tune our Parallel
    Adapters. Specifically, after the first training epoch, the activation cache for
    all samples is already collected. We then perform collective communication to
    redistribute the Parallel Adapters parameters and locally cached activations across
    all devices, ensuring each device receives the complete set of adapter parameters
    and corresponding activations. The devices then utilize this shared information
    to fine-tune the parallel adapters in a data-parallel manner. In our experiments,
    fine-tuning the BART-Large model on the MRPC dataset for three epochs, the redistribution
    of parameters and activations only contributed to approximately $8\%$ of the total
    training time. Notably, the overhead of this process can be further amortized
    over additional training epochs. An instance of personal LLMs fine-tuning with
    activation cache is depicted in Figure [7](#S5.F7 "Figure 7 ‣ 5.2\. Cache-Enabled
    Collaborative Edge Fine-Tuning of Parallel Adapters ‣ 5\. Collaborative Edge AI
    System for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and
    Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa9539bf9ee3dbb2b34b67703179a5b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. An instance of fine-tuning with activation cache.
  prefs: []
  type: TYPE_NORMAL
- en: Storage Cost Analysis. Employing activation caching can reduce the computational
    requirements of forward propagation; however, it incurs additional storage overhead
    for activations. Specifically, the storage overhead is $s\times h\times l$ corresponds
    to the number of transformer layers. For T5-Base model, the activation caching
    requires less than $1$ of the storage of a modern mobile device, e.g., hundreds
    of GB. During fine-tuning, the activation cache is reloaded from disk per micro-batch,
    a process that takes no more than tens of milliseconds on embedded flash storage.
    The cache will be cleared once the fine-tuning process finishes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2\. Training durations (in hours) for different methods: 3 epochs for
    MRPC and STS-B, and 1 epoch for SST-2 and QNLI.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fine-tuning Techniques | Baseline Methods | T5-Base |  | BART-Large |  |
    T5-Large |'
  prefs: []
  type: TYPE_TB
- en: '| MRPC | STS-B | SST-2 | QNLI |  | MRPC | STS-B | SST-2 | QNLI |  | MRPC |
    STS-B | SST-2 | QNLI |'
  prefs: []
  type: TYPE_TB
- en: '| Full Model | Standalone | OOM | OOM | OOM | OOM |  | OOM | OOM | OOM | OOM
    |  | OOM | OOM | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Eco-FL | 0.45 | 0.71 | 2.74 | 4.32 |  | 2.41 | 3.78 | 14.56 | 22.98 |  |
    OOM | OOM | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| EDDL | OOM | OOM | OOM | OOM |  | OOM | OOM | OOM | OOM |  | OOM | OOM |
    OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Adapters | Standalone | 1.21 | 1.9 | 7.29 | 11.51 |  | OOM | OOM | OOM |
    OOM |  | OOM | OOM | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Eco-FL | 0.39 | 0.61 | 2.35 | 3.71 |  | 0.54 | 0.85 | 3.27 | 5.16 |  | 2.75
    | 4.31 | 16.59 | 26.19 |'
  prefs: []
  type: TYPE_TB
- en: '| EDDL | 0.34 | 0.53 | 2.06 | 3.25 |  | OOM | OOM | OOM | OOM |  | OOM | OOM
    | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | Standalone | 1.21 | 1.89 | 7.28 | 11.49 |  | OOM | OOM | OOM | OOM
    |  | OOM | OOM | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Eco-FL | 0.41 | 0.64 | 2.45 | 3.87 |  | 0.55 | 0.87 | 3.33 | 5.26 |  | 2.73
    | 4.28 | 16.48 | 26.02 |'
  prefs: []
  type: TYPE_TB
- en: '| EDDL | 0.31 | 0.48 | 1.86 | 2.94 |  | OOM | OOM | OOM | OOM |  | OOM | OOM
    | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel Adapters | PAC (Ours) | 0.14 | 0.22 | 1.34 | 2.12 |  | 0.29 | 0.45
    | 2.69 | 4.25 |  | 0.69 | 1.09 | 8.88 | 14.02 |'
  prefs: []
  type: TYPE_TB
- en: Table 3\. Comparison of final performance between different fine-tuning techniques
    across four datasets. We report the average of F1 score and accuracy for MRPC.
    We use Pearson-Spearman Correlation as the metric for STS-B. For SST-2 and QNLI,
    we report accuracy. The mean value is the average performance of Full Model, Adapters
    and LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: '| Fine-tuning Techniques | T5-Base |  | BART-Large |  | T5-Large |'
  prefs: []
  type: TYPE_TB
- en: '| MRPC | STS-B | SST-2 | QNLI |  | MRPC | STS-B | SST-2 | QNLI |  | MRPC |
    STS-B | SST-2 | QNLI |'
  prefs: []
  type: TYPE_TB
- en: '| Full Model | 89.71 | 90.94 | 94.03 | 93.08 |  | 88.16 | 91.10 | 95.64 | 94.40
    |  | 92.78 | 91.08 | 95.30 | 93.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Adapters | 88.73 | 90.51 | 93.58 | 93.04 |  | 86.63 | 90.24 | 94.93 | 93.27
    |  | 91.86 | 90.58 | 96.10 | 94.07 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 86.27 | 90.73 | 93.69 | 93.30 |  | 87.46 | 90.36 | 95.23 | 94.48 |  |
    90.27 | 92.08 | 95.53 | 94.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Value | 88.24 | 90.73 | 93.77 | 93.14 |  | 87.42 | 90.57 | 95.27 | 94.05
    |  | 91.64 | 91.25 | 95.64 | 93.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel Adapters (Ours) | 88.24 | 90.43 | 93.46 | 93.25 |  | 87.71 | 90.54
    | 95.25 | 93.68 |  | 91.7 | 91.57 | 95.76 | 93.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Difference from Mean | +0.00 | -0.30 | -0.31 | +0.11 |  | +0.29 | -0.03 |
    -0.02 | -0.37 |  | +0.06 | +0.32 | +0.12 | -0.15 |'
  prefs: []
  type: TYPE_TB
- en: 6\. Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1\. Implementation and Setups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementation of PAC. We have fully implemented the prototype framework of
    PAC and baselines with $\sim$ times the corresponding weights and hidden states
    of the backbone model. In our experiments, the reduction factor $k$ is set to
    8\. The weights of the Parallel Adapters are initialized based on structural pruning,
    using the weights of the backbone model. We insert Parallel Adapters at the end
    of each transformer layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Models and Datasets. We evaluate PAC with three typical transformer based LLM
    with parameters ranging from $0.25$B, as detailed in Table [4](#S6.T4 "Table 4
    ‣ 6.1\. Implementation and Setups ‣ 6\. Evaluation ‣ Pluto and Charon: A Time
    and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning"),
    which are widely considered for IPA and edge deployments (Li et al., [2024](#bib.bib15);
    Yuan et al., [2023](#bib.bib33)). All experiments were performed under conditions
    using Float32 precision to ensure fine-tuning performance. We employ two variants
    of the T5 model (Raffel et al., [2020](#bib.bib21)), specifically T5-Base and
    T5-Large with differing parameter sizes. We also compare PAC with baseline methods
    with BART-Large (Lewis et al., [2019](#bib.bib14)) as the backbone for our parallel
    adapters. We evaluate our fine-tuned LLMs with four tasks from GLUE benchmark.
    The four tasks evaluate models on multiple diverse tasks over sentiment analysis
    (SST2), similarity and paraphrase (MRPC, STS-B) and natural language inference
    (QNLI).'
  prefs: []
  type: TYPE_NORMAL
- en: Edge Environment Setup. We evaluate PAC across a realistic edge platform consisting
    of multiple NVIDIA Jetson Nano (jet, [2019](#bib.bib2)), widely recognized as
    prevalent off-the-shelf edge devices. Each device is equipped with a $128$Mbps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baseline Methods. We compare PAC with both single-device method and the state-of-the-art
    collaborative edge training methods: (1) Standalone means fine-tuning LLMs on
    a single edge device. We compare with it to analyze the scalability performance
    of PAC. (2) Eco-FL (Ye et al., [2022](#bib.bib31)) is a collaborative edge system
    that facilitates pipeline parallelism training across an edge device cluster within
    the same local area network, segmenting LLMs into sequential stages for processing
    in a pipeline fashion. (3) EDDL (Hao and Zhang, [2021](#bib.bib9)) employs conventional
    data parallel training across edge devices, distributing batch data among cluster
    devices for simultaneous processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. LLM model specifications used for experiments. "en-de" indicates encoder-decoder
    LLM structure.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Structure | Layers | Heads |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Hidden &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Size &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Param. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Count &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| T5-Base (Raffel et al., [2020](#bib.bib21)) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; en-de &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 12 | 12 | 768 | 0.25B |'
  prefs: []
  type: TYPE_TB
- en: '| BART-Large (Lewis et al., [2019](#bib.bib14)) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; en-de &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 12 | 16 | 1024 | 0.41B |'
  prefs: []
  type: TYPE_TB
- en: '| T5-Large (Raffel et al., [2020](#bib.bib21)) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; en-de &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 24 | 16 | 1024 | 0.74B |'
  prefs: []
  type: TYPE_TB
- en: Considering that the aforementioned baseline systems were not specifically designed
    for the fine-tuning of LLMs, we ensure a fair comparison by equipping these edge
    systems with various LLM fine-tuning techniques. These include full model fine-tuning
    and popular PEFT techniques. (1) In Full model fine-tuning, all the LLM parameters
    are updated for a downstream task. (2) LoRA (Hu et al., [2021](#bib.bib11)) is
    a widely-used PEFT technique that decomposes the parameter update for a weight
    matrix into two trainable low-rank matrices. (3) Adapters (Houlsby et al., [2019](#bib.bib10))
    is another widely-used PEFT technique that injects small trainable modules at
    the end of each transformer layer.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. End-to-end Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning
    of Parallel Adapters ‣ 5\. Collaborative Edge AI System for Efficient Personal
    LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") and Table [3](#S5.T3 "Table
    3 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning of Parallel Adapters ‣
    5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning") summarize the end-to-end performance comparisons between PAC,
    the single-device method, and state-of-the-art collaborative edge training methods.
    To ensure fair comparisons, these baseline methods are enhanced with prevalent
    PEFT techniques, including Adapters and LoRA. Fine-tuning the smaller datasets,
    MRPC and STS-B, is conducted over three epochs, with the latter two epochs benefiting
    from the PAC activation cache. Conversely, for larger datasets such as STS-2 and
    QNLI, a single epoch of fine-tuning is sufficient to achieve satisfactory performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24aeae266747101e9878751076864f11.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The comparison of average sample training time of different fine-tuning
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c384cb3683587f5efbeeae7aa4a4c236.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Maximum total memory consumption per device across the edge cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8\. Comparison of different fine-tuning techniques. P.A. indicates our
    Parallel Adapters technique. Mini-batch size 16; sequence length: 128.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PAC significantly speeds up the training process while preserving convergence
    performance. PAC achieves an acceleration ranging from $1.21\times$ on SST-2 and
    QNLI, without utilizing activation cache. Parallel Adapters not only alleviate
    the memory footprint of LLM parameters but also intermediate activations. Eco-FL’s
    pipeline parallel strategies allow each edge device to host only a portion of
    the model parameters. However, these devices still bear a substantial memory footprint
    from intermediate activations, even when employing PEFT technologies such as LoRA
    and Adapters. Therefore, the Eco-FL approach necessitates the use of smaller micro-batch
    sizes or a reduction in the number of micro-batches simultaneously input into
    the pipeline. This results in decreased concurrency in pipeline parallelism and
    lowers the training throughput. Moreover, our hybrid parallelism merges the benefits
    of both data and pipeline parallelism, providing an expanded search space for
    parallel architectures to accommodate complex edge environments. Our method enables
    the identification of the most efficient parallel configuration with maximum throughput
    within the constraints of available resources. With the integration of our activation
    cache mechanism, PAC achieves speedups of up to $8.64\times$ on the MRPC and STS-B
    datasets. As discussed in §[5](#S4.F5 "Figure 5 ‣ 4.1\. Fine-Tuning LLMs with
    Parallel Adapters ‣ 4\. Time, Memory and Parameter Efficient Fine-Tuning Algorithm
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning"), our Parallel Adapters constitute a lightweight,
    independent network. We can skip both the forward and backward passes through
    the LLM backbone, since the required activations have already been calculated
    and stored. Consequently, training overhead can be markedly reduced in the second
    and third fine-tuning epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning
    of Parallel Adapters ‣ 5\. Collaborative Edge AI System for Efficient Personal
    LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") displays the performance of
    various full model and PEFT fine-tuning methods on four datasets after training.
    Fine-tuning involves 3 epochs for the smaller MRPC and STS-B datasets, and 1 epoch
    for the larger SST-2 and QNLI datasets. We can observe that PAC achieves comparable
    or superior performance to full model fine-tuning and PEFT techniques across various
    models and datasets. The largest discrepancy in mean performance metrics between
    PAC and these methods is only -0.37, a negligible difference. Notably, PAC frequently
    outperforms these methods and achieves the highest performance on the SST-2 dataset
    with the T5-Large model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/013b9fbc2160b5581df29a2b839d8b46.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Throughput results with a varying number of Jetson Nano.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c349dc5477596a45ffe48f2e0699d8d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Peak memory consumption of LLM weights per device across the edge cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9\. Comparison of different fine-tuning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b70a05fa19233a2036f0e0e1acbb0478.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10\. Device grouping results of PAC’s hybrid parallelism for experiments
    in Figure [9](#S6.F9 "Figure 9 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning"). "N" indicates Jetson Nano.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Significance of Parallel Adapters at the Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conducted experiments to assess the time and memory efficiency of Parallel
    Adapters at the edge. In this section, we perform data parallelism for Parallel
    Adapters with activation cache across 8 devices and hybrid parallelism for other
    fine-tuning techniques without 1F1B micro-batch scheduling. "Activations" contain
    the intermediate results and optimizer states. Figure [8](#S6.F8 "Figure 8 ‣ 6.2\.
    End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning") illustrates
    that Parallel Adapters outperform other fine-tuning techniques regarding both
    time and memory efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel Adapters markedly reduce per-sample training time. Figure [8(a)](#S6.F8.sf1
    "In Figure 8 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon:
    A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs
    Fine-Tuning") presents the average sample training time across different fine-tuning
    techniques. Without activation cache, Parallel Adapters can reduce the average
    sample training time by $31.94\%$ compared to full fine-tuning. Moreover, Parallel
    Adapters With activation cache mechanism can further decrease the average sample
    training time up to $96.39\%$. These results demonstrate the substantial reduction
    in training time achieved by Parallel Adapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel Adapters yield a substantial reduction in memory usage. Figure [8(b)](#S6.F8.sf2
    "In Figure 8 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon:
    A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs
    Fine-Tuning") depicts the breakdown of the memory footprint for different fine-tuning
    techniques. We report the peak memory consumption per device across edge clusters.
    Without activation cache, Parallel Adapters can reduce memory usage by $25.27\%$.
    With activation cache, Parallel Adapters can decrease the peak memory footprint
    from $74.57\%$ compared to baselines. This is because it’s sufficient to store
    only the lightweight Parallel Adapters, eliminating the need to host the entire
    LLM backbone in memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. Analysis of Collaborative Edge Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We perform an ablation study to understand the contribution of hybrid parallelism
    and activation cache in our system design.
  prefs: []
  type: TYPE_NORMAL
- en: Comparasion PAC with EDDL and Eco-FL. To explore the scalability advantages
    of PAC’s hybrid parallelism over Eco-FL’s pipeline parallelism and EDDL’s data
    parallelism, we compared the throughput of these methods when training collaboratively
    across 2 to 8 edge devices. The batch size was consistent with the number of devices,
    and the sequence length of each sample was fixed at 128\. We implement Eco-FL
    and EEDL using the Parallel Adapters technique to ensure a fair comparison. Note
    that none of the three methods utilizes activation cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [9(b)](#S6.F9.sf2 "In Figure 9 ‣ 6.2\. End-to-end Performance ‣ 6\.
    Evaluation ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge
    AI Framework for Personal LLMs Fine-Tuning") illustrates the maximum per device
    memory footprint of model weights across edge cluster. For EDDL, each device must
    host a complete LLM, preventing the reduction of the parameters’ memory footprint
    through scaling up the number of devices. Therefore, as shown in Figure [9(a)](#S6.F9.sf1
    "In Figure 9 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon:
    A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs
    Fine-Tuning"), the EDDL method exhibits OOM errors with both the BART-Large and
    T5-Large models. Conversely, PAC and Eco-FL utilize pipeline parallelism, partitioning
    the model into multiple stages with each handled by different devices. This approach
    allows for scaling the number of devices to reduce the peak memory footprint.
    PAC’s hybrid parallelism offers a broader search space for parallel strategies
    compared to Eco-FL’s pipeline parallelism. Our planning algorithm for PAC is capable
    of identifying more efficient hybrid parallel configurations within memory constraints,
    enhancing resource utilization. Although PAC may incur higher memory overhead
    in some instances, it achieves greater system throughput. Specifically, when compared
    to Eco-FL, PAC exhibits an increase in throughput from $39.50\%$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f41edd815dee66ff3d3d1183ece01571.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11\. Fine-tuning time with PAC. Time without activation cache is represented
    by bars. The corresponding reduction in time achieved utilizing activation cache
    is represented by shaded areas. Dataset: MRPC.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To more clearly illustrate the parallel strategies adopted by PAC, we present
    the device grouping configurations for PAC across various LLMs and numbers of
    devices in Figure [10](#S6.F10 "Figure 10 ‣ 6.2\. End-to-end Performance ‣ 6\.
    Evaluation ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge
    AI Framework for Personal LLMs Fine-Tuning"). On the left, a table displays all
    the grouping results across three models. On the right, an instance is shown where
    a model is divided into three stages, with two devices handling the first stage
    to perform data parallelism. Specifically, when fine-tuning BART-Large with eight
    devices, EDDL encounters OOM issues because a single Jetson Nano cannot accommodate
    a complete BART-Large. Eco-FL addresses this problem by dividing the model into
    eight stages and employing straight pipeline parallelism for training. On the
    contrary, our PAC approach divides BART-Large into two stage models, with each
    stage replicated across four devices. This configuration significantly reduces
    the number of stages in the pipeline, thereby minimizing inter-stage data dependencies
    and communication latency, which in turn enhances the pipeline’s concurrent efficiency.
    These results demonstrate that our hybrid parallel approach offers a larger search
    space for parallel configurations, providing enhanced scalability and robustness
    across varying numbers of devices and workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of PAC with and without activation cache. We further investigated
    how our activation cache mechanism benefits the required fine-tuning latency.
    By leveraging activation cache, the fine-tuning latency per epoch can decrease
    up to $79.51\%$, whereas training for ten epochs increases the reduction to $71\%$.
    This reduction can be attributed to the fact that the Parallel Adapters constitute
    a lightweight, independent network, resulting in a significant decrease in training
    cost compared to the LLM backbone. We can bypass both the forward and backward
    passes through the LLM backbone since the necessary activations are already cached.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning for LLM. Prompt tuning (Lester et al., [2021](#bib.bib13))
    proposes to prepend the model input embeddings with a trainable tensor. Adapters
    tuning (Houlsby et al., [2019](#bib.bib10)) adds domain-specific layers after
    attention and FFN layers in transformer. LoRA (Hu et al., [2021](#bib.bib11))
    decomposes the parameter update for a weight matrix into two trainable low-rank
    matrices. To further reduce the memory overhead, pioneering studies explore fine-tuning
    techniques that obviate the need for backpropagation through the backbone model.
    Y-tuning (Liu et al., [2024](#bib.bib17)) learns additional task-specific label
    representations, which are integrated with the output of the backbone model to
    circumvent backpropagation. LST (Sung et al., [2022](#bib.bib22)) involves the
    use of pruned lightweight transformer structures from the backbone as a side network.
    E³VA (Yin et al., [2023](#bib.bib32)) extends the concept of the side network
    into the realm of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: On-device DNN Fine-Tuning. POET (Patil et al., [2022](#bib.bib20)) achieves
    the finetuning of a BERT model on embedded devices, optimizing for both training
    speed and energy consumption. Lin et al. (Lin et al., [2022](#bib.bib16)) enable
    training directly on devices with a minimal memory requirement of only 256KB.
    Sage and Melon (Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24))
    implement hybrid memory management and conservation strategies, including operator
    fusion and the use of a dedicated memory pool, to mitigate memory limitations.
    Additionally, Mandheling (Xu et al., [2022](#bib.bib26)) incorporates mixed-precision
    training along with DSP offloading to enhance the speed of learning.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Edge Computing for DNN Fine-Tuning. Federated Learning (FL) has
    been a promising paradigm in distributed machine learning that enables in-situ
    model fine-tuning. FwdLLM (Xu et al., [2024](#bib.bib28)) designs a backpropagation-free
    fine-tuning FL protocol to enhance efficiency. AdaFL (Cai et al., [2023](#bib.bib5))
    proposes an FL framework for fine-tuning LLMs that features adaptable depth and
    width in its adapters modules. Breaking through the conventional paradigm of FL,
    Ye et al. (Ye et al., [2022](#bib.bib31); Zeng et al., [2024](#bib.bib34)) devise
    a pipeline parallel architecture that facilitates the collaborative fine-tuning
    of DNNs across multiple edge devices. EDDL (Hao and Zhang, [2021](#bib.bib9))
    adopts data parallelism training across embedded devices in a local area network.
    Asteroid (Ye et al., [2024b](#bib.bib30)) also employs HPP across multiple edge
    devices for DNN training, but it does not specifically address the parameter-efficient
    fine-tuning of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper proposes PAC, a time and memory efficient collaborative edge AI framework
    for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning
    with a sophisticated algorithm-system co-design, achieving a acceleration of $8.64\times$
    memory reduction compared to state-of-the-art methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: jet (2019) 2019. Jetson-Nano. [https://developer.nvidia.com/embedded/jetson-nano-developer-kit](https://developer.nvidia.com/embedded/jetson-nano-developer-kit).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pyt (2019) 2019. PyTorch. [https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tfl (2021) 2021. On-device training with tensorflow lite. [https://www.tensorflow.org/lite/examples/on_device_training/overview](https://www.tensorflow.org/lite/examples/on_device_training/overview).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2023) Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix Xiaozhu Lin,
    and Mengwei Xu. 2023. Efficient federated learning for modern nlp. In *MobiCom*.
    1–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gim and Ko (2022) In Gim and JeongGil Ko. 2022. Memory-efficient dnn training
    on mobile devices. In *MobiSys*. 464–476.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Liwei Guo, Wonkyo Choe, and Felix Xiaozhu Lin. 2023. Sti:
    Turbocharge nlp inference at the edge via elastic pipelining. In *ASPLOS, Volume
    2*. 791–803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2024) Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024.
    Parameter-efficient fine-tuning for large models: A comprehensive survey. *arXiv
    preprint arXiv:2403.14608* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao and Zhang (2021) Pengzhan Hao and Yifan Zhang. 2021. Eddl: A distributed
    deep learning system for resource-limited edge computing environment. In *2021
    IEEE/ACM Symposium on Edge Computing (SEC)*. IEEE, 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. Parameter-efficient transfer learning for NLP. In *ICML*. PMLR, 2790–2799.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020) Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan
    Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu Cai, Tianhang Yu, et al. 2020. Mnn:
    A universal and efficient inference engine. *Proceedings of MLSys* 2 (2020), 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 2024. Personal
    llm agents: Insights and survey about the capability, efficiency and security.
    *arXiv preprint arXiv:2401.05459* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022) Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan,
    and Song Han. 2022. On-device training under 256kb memory. *NeurIPS* 35 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Yitao Liu, Chenxin An, and Xipeng Qiu. 2024. Y-tuning: An
    efficient tuning paradigm for large-scale pre-trained models via label representation
    learning. *Frontiers of Computer Science* 18, 4 (2024), 184320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miao et al. (2024) Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin
    Unger, and Zhihao Jia. 2024. FlexLLM: A System for Co-Serving Large Language Model
    Inference and Parameter-Efficient Finetuning. *arXiv:2402.18789* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    2019. PipeDream: generalized pipeline parallelism for DNN training. In *SOSP*.
    1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patil et al. (2022) Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica,
    and Joseph Gonzalez. 2022. POET: Training neural networks on tiny devices with
    integrated rematerialization and paging. In *International Conference on Machine
    Learning*. PMLR, 17573–17583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *JMLR*
    21, 140 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sung et al. (2022) Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Lst: Ladder
    side-tuning for parameter and memory efficient transfer learning. *NeurIPS* 35
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *NeurIPS* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang
    Yuan, Xin Jin, Gang Huang, Yunxin Liu, and Xuanzhe Liu. 2022. Melon: Breaking
    the memory wall for resource-efficient on-device machine learning. In *MobiSys*.
    450–463.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2024) Yuanxin Wei, Shengyuan Ye, Jiazhi Jiang, Xu Chen, Dan Huang,
    Jiangsu Du, and Yutong Lu. 2024. Communication-Efficient Model Parallelism for
    Distributed In-situ Transformer Inference. In *DATE*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022) Daliang Xu, Mengwei Xu, Qipeng Wang, Shangguang Wang, Yun
    Ma, Kang Huang, Gang Huang, Xin Jin, and Xuanzhe Liu. 2022. Mandheling: Mixed-precision
    on-device dnn training with dsp offloading. In *MobiCom*. 214–227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei,
    Mengwei Xu, and Xuanzhe Liu. 2023. Llmcad: Fast and scalable on-device large language
    model inference. *arXiv preprint arXiv:2309.04255* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2024) M Xu, D Cai, Y Wu, X Li, and S Wang. 2024. Fwdllm: Efficient
    fedllm using forward gradient. (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2024a) Shengyuan Ye, Jiangsu Du, Liekang Zeng, Wenzhong Ou, Xiaowen
    Chu, Yutong Lu, and Xu Chen. 2024a. Galaxy: A Resource-Efficient Collaborative
    Edge AI System for In-situ Transformer Inference. *arXiv preprint arXiv:2405.17245*
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2024b) Shengyuan Ye, Liekang Zeng, Xiaowen Chu, Guoliang Xing, and
    Xu Chen. 2024b. Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative
    DNN Training on Heterogeneous Edge Devices. In *Proceedings of the 30th Annual
    International Conference on Mobile Computing and Networking*. 312–326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2022) Shengyuan Ye, Liekang Zeng, Qiong Wu, Ke Luo, Qingze Fang,
    and Xu Chen. 2022. Eco-FL: Adaptive federated learning with efficient edge collaborative
    pipeline training. In *Proceedings of the 51st International Conference on Parallel
    Processing*. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023) Dongshuo Yin, Xueting Han, Bin Li, Hao Feng, and Jing Bai.
    2023. Parameter-efficient is not sufficient: Exploring parameter, memory, and
    time efficient adapter tuning for dense predictions. *arXiv preprint arXiv:2306.09729*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2023) Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan,
    Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, et al. 2023. Rethinking
    mobile AI ecosystem in the LLM era. *arXiv preprint arXiv:2308.14363* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2024) Liekang Zeng, Shengyuan Ye, Xu Chen, and Yang Yang. 2024.
    Implementation of Big AI Models for Wireless Networks with Collaborative Edge
    Computing. *IEEE Wireless Communications* 31, 3 (2024), 50–58.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas,
    and Jitendra Malik. 2020. Side-tuning: a baseline for network adaptation via additive
    side networks. In *ECCV 2020, Proceedings, Part III 16*. Springer, 698–714.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
