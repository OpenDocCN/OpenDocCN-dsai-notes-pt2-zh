- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.20999](https://ar5iv.labs.arxiv.org/html/2407.20999)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Yupeng Chen Equal contribution. Email: {senmiaowang1, yushunzhang}@link.cuhk.edu.cn,  zeyu.qin@connect.ust.hk,
    {yupengchen1224, linzhihang, sunruoyu}@cuhk.edu.cn,   dingtian@sribd.cn The Chinese
    University of Hong Kong, Shenzhen, China Senmiao Wang^∗ The Chinese University
    of Hong Kong, Shenzhen, China Zhihang Lin The Chinese University of Hong Kong,
    Shenzhen, China Zeyu Qin Hong Kong University of Science and Technology Yushun
    Zhang The Chinese University of Hong Kong, Shenzhen, China Shenzhen Research Institute
    of Big Data Tian Ding Shenzhen Research Institute of Big Data Ruoyu Sun Corresponding
    author. The Chinese University of Hong Kong, Shenzhen, China Shenzhen Research
    Institute of Big Data'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, large language models (LLMs) have demonstrated remarkable capabilities
    in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and
    subsequently fine-tuned on task-specific datasets. However, during fine-tuning,
    LLMs may forget the knowledge acquired in the pre-training stage, leading to a
    decline in general capabilities. To address this issue, we propose a new fine-tuning
    algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to
    iteratively select and update the model parameters with the largest momentum magnitudes.
    Compared to full-parameter training, MoFO achieves similar fine-tuning performance
    while keeping parameters closer to the pre-trained model, thereby mitigating knowledge
    forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines
    the following two advantages. First, MoFO does not require access to pre-training
    data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training
    data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second,
    MoFO does not alter the original loss function. This could avoid impairing the
    model performance on the fine-tuning tasks. We validate MoFO through rigorous
    convergence analysis and extensive experiments, demonstrating its superiority
    over existing methods in mitigating forgetting and enhancing fine-tuning performance.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The success of large language models (LLMs) lies in their strong capabilities
    in language understanding and generation. Typically, LLMs are initially pre-trained
    on extensive corpora to acquire general capabilities, and subsequently, they are
    fine-tuned on smaller, task-specific datasets to adapt to particular tasks or
    domains (Dai and Le, [2015](#bib.bib17); Kenton and Toutanova, [2019](#bib.bib32);
    Radford et al., [2018](#bib.bib53)). However, it has been observed that during
    the fine-tuning process, LLMs may forget the knowledge acquired in pre-training,
    leading to a decline in general capabilities (Lin et al., [2023](#bib.bib40);
    Chen et al., [2020](#bib.bib13); Dong et al., [2021](#bib.bib18); Korbak et al.,
    [2022](#bib.bib35); Luo et al., [2023](#bib.bib45)). Therefore, addressing the
    issue of forgetting during fine-tuning has become an important research direction
    for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the literature, two classes of methods are commonly adopted to mitigate
    the forgetting: replay-based methods, and regularization-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replay-based methods leverage pre-training data during the fine-tuning process
    (Rolnick et al., [2019](#bib.bib61); Wang et al., [2020](#bib.bib71); Ouyang et al.,
    [2022](#bib.bib51)). However, most open-source LLMs, such as the Llama series
    (Touvron et al., [2023](#bib.bib66)), have not fully disclosed their pre-training
    datasets. Moreover, even with access to pre-training data, incorporating it into
    the fine-tuning process may significantly increase computational and memory costs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization-based methods add penalty terms to the loss function, encouraging
    the fine-tuned model to remain close to the pre-trained model, thereby reducing
    the risk of forgetting pre-training knowledge (Li et al., [2018](#bib.bib38);
    Kirkpatrick et al., [2017](#bib.bib34); Miceli-Barone et al., [2017](#bib.bib47);
    Panigrahi et al., [2023](#bib.bib52)). However, as we will present later, modifying
    the original loss function during fine-tuning may impair the model’s performance
    on the fine-tuning task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this paper, we design a replay-free and regularization-free method to mitigate
    forgetting during the fine-tuning process. We propose the Momentum-Filtered Optimizer
    (MoFO). At each iteration, MoFO selects and updates only the parameters with the
    top $\alpha\%$ is the filtering hyperparameter. Here, the blocks refer to the
    parameters of different parts of the network (e.g., weight matrices and bias terms).
  prefs: []
  type: TYPE_NORMAL
- en: Our method is motivated by the following observation. The fine-tuning loss of
    LLMs has many minima¹¹1In this paper, we use the term ”minimum” (or ”minima” in
    the plural) to refer to a parameter configuration whose fine-tuning loss is near
    its lowest value in a small neighborhood, while acknowledging that this terminology
    may not strictly represent a local minimum of the fine-tuning loss function. and
    these minima can vary significantly in their distances to the pretrained-model.
    We notice that the minima closer to the pre-trained model are less likely to experience
    forgetting. Inspired by this observation, MoFO manages to approach closer minima
    by dynamically updating a portion of parameters that are most effective for reducing
    fine-tuning loss at each iteration. Therefore, MoFO can reduce the risk of forgetting
    without sacrificing the fine-tuning performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose the MoFO algorithm, a new optimization method designed to mitigate
    the forgetting of pre-training knowledge during the fine-tuning process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present an initial convergence analysis of the MoFO algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct experiments on various tasks, demonstrating that MoFO outperforms
    existing methods both in fine-tuning performance and mitigating forgetting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Momentum Filtered Optimizer (MoFO)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During fine-tuning, different training methods usually converge to different
    minima. We observe that these minima share similar fine-tuning loss but can vary
    significantly in their distances to the pre-trained model. Furthermore, minima
    closer are less likely to forget pre-training knowledge. Below, we provide an
    illustrative example of this phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct an experiment using the Pythia-160m model to illustrate this observation.
    We fine-tune this model on a subset of the FLAN dataset²²2The subset we use is
    ‘definite_pronoun_resolution_10templates,’ available at [https://huggingface.co/datasets/Muennighoff/flan](https://huggingface.co/datasets/Muennighoff/flan).
    This is a preprocessed version of the FLAN dataset that incorporates updates made
    to the FLAN datasets since the release of the original FLAN. using two different
    optimizers: the Adam optimizer (Kingma and Ba, [2014](#bib.bib33)) and the Lion
    optimizer (Chen et al., [2024](#bib.bib14)). Figure LABEL:pythia_landscape_lion
    demonstrates that Adam and Lion converges to two different minima of the fine-tuning
    loss. Figure LABEL:pythia_landscape_lion(a) reveals that the two minima share
    similar fine-tuning loss. The minimum reached by Adam is significantly closer
    to the pre-trained model. Figure LABEL:pythia_landscape_lion(b) indicates that
    the minimum reached by Adam (the closer minimum) has a lower pre-training loss
    than that reached by Lion (the farther minimum). Furthermore, we evaluate the
    forgetting of pre-training knowledge in some common sense reasoning tasks, which
    include HellaSwag (Zellers et al., [2019](#bib.bib78)), ARC-Challenge, and ARC-Easy
    (Clark et al., [2018](#bib.bib15)). Table [1](#S2.T1 "Table 1 ‣ 2.1 Motivation
    ‣ 2 Momentum Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered Optimizer for
    Mitigating Forgetting in LLM Fine-Tuning") shows that Adam suffers from less accuracy
    degradation on average, indicating better preservation of the pre-training knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we conjecture that the severity of forgetting during fine-tuning
    correlates with the distance between the fine-tuned model and the pre-trained
    model: in general, models that remain closer to the pre-trained model are less
    likely to experience forgetting. Consequently, we consider whether it is possible
    to find a method superior to the Adam optimizer that not only reaches the minimum
    during fine-tuning but also converges closer to the pre-trained model, thereby
    mitigating forgetting more effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Pythia-160m’s accuracies on common sense tasks, after being fine-tuned
    with the Adam optimizer and Lion optimizer. Adam achieves less forgetting than
    Lion on average.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | HellaSwag | ARC-easy | ARC-challenge | Average |'
  prefs: []
  type: TYPE_TB
- en: '| Pythia-160m | 30.1 | 39.6 | 23.8 | 31.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Adam | 28.3 | 37.4 | 22.1 | 29.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Lion | 26.5 | 29.0 | 24.1 | 26.5 |'
  prefs: []
  type: TYPE_TB
- en: We first discuss how to keep the model closer to the pre-trained model. To achieve
    this goal, we recall the classical block coordinate descent (BCD) method (Tseng,
    [2001](#bib.bib67)). We believe that the BCD algorithm may converge to a minimum
    that is closer to the pre-trained model than the default full parameter fine-tuning
    with Adam. This is because BCD only updates a subset of parameters at each iteration.
    Compared to full parameter updates, this method usually involves smaller adjustments
    to the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining issue is how to design BCD that reaches a similar performance
    to the default methods. An effective strategy is to prioritize updating parameters
    that have the greatest influence on reducing fine-tuning loss. A straightforward
    approach is to measure the parameter’s influence by the magnitude of its gradient.
    However, in the widely used Adam optimizer, momentum directly affects parameter
    updates, while gradients influence parameter updates indirectly by affecting the
    momentum.
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by these discussions, to mitigate forgetting and achieve comparable
    performance in fine-tuning tasks, we will modify the Adam optimizer by updating
    the subset of parameters with the largest momentum magnitude. We will discuss
    more details in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/50554b39f8ff378562b02c4daf41e6e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of MoFO. Compared with Adam optimizer, MoFO updates
    only the parameters with the largest $\alpha\%$ momentum in each partition.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Algorithm Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We formally introduce the Momentum-Filtered Optimizer (MoFO) in Algorithm [1](#alg1
    "Algorithm 1 ‣ 2.2 Algorithm Formulation ‣ 2 Momentum Filtered Optimizer (MoFO)
    ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning").
    MoFO partitions all the parameters into $B$ is the pre-determined hyperparameter.
    The momentum filtering mechanism is illustrated in Figure [2](#S2.F2 "Figure 2
    ‣ 2.1 Motivation ‣ 2 Momentum Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: We note that the NN parameters are naturally composed of different parts (e.g.,
    weight matrices, bias terms) in the network architecture, and PyTorch’s backward
    propagation mechanism automatically returns the gradients of the loss with respect
    to each part of the parameters. To reduce computational complexity, we partition
    all parameters according to these fixed parts and select $\alpha\%$ of the parameter
    entries for BCD update within each part.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [3.4](#S3.SS4 "3.4 Further Analysis ‣ 3 Experiments ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning"), we will empirically
    demonstrate that MoFO’s momentum-based selection rule outperforms its gradient-based
    variant in fine-tuning tasks. It indicates that the momentum-based selection rule
    allows for better incorporation with Adam during the optimization process in fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: MoFO efficiently selects and updates the most influential parameters, as dictated
    by the momentum’s magnitude, thus enhancing the fine-tuning process while alleviating
    the catastrophic forgetting of pre-training knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Momentum Filtered Optimizer (MoFO)
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: Filtering threshold $\alpha\%$, hyperparameters $\beta_{1},\beta_{2},\epsilon$
    from $1,2,\dots$ do5:        $g^{(k)}_{t}=\nabla_{(k)}\mathcal{L}_{finetune}(\Theta_{t-1})$9:        $\hat{v}^{(k)}_{t}=v^{(k)}_{t}/(1-\beta_{2}^{t})$ do11:           $(\texttt{FILTER}_{t}^{(k)})_{i}=1$’s
    entries else 012:        end for13:        $\alpha\%$ # Momentum Filtering14:     end for15:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Initial Exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we empirically examine whether an LLM fine-tuned with MoFO
    converges to a minimum closer to the pre-trained model compared to the one fine-tuned
    with the Adam optimizer, and whether MoFO mitigates catastrophic forgetting in
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fine-tune the Pythia-160m on the same dataset used in the experiment described
    at the beginning of Section [2](#S2 "2 Momentum Filtered Optimizer (MoFO) ‣ MoFO:
    Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning"), using
    both the Adam optimizer and MoFO. First, as shown in Figure LABEL:pythia_landscape(a),
    both MoFO and Adam optimizer achieve minimal fine-tuning loss, so switching Adam
    to MoFO does not lead to performance degeneracy. Second, as shown in Figure LABEL:pythia_landscape(b),
    the distance from the pre-trained model to the minimum reached by MoFO is approximately
    20% of the distance to that reached by the Adam optimizer. This shows that MoFO
    significantly reduces the amount of parameter movement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Pythia-160m’s performance on common sense tasks, after being fine-tuned
    with the Adam optimizer and MoFO. The results indicate that MoFO significantly
    mitigates catastrophic forgetting. Bold values denote the best results among these
    optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | HellaSwag | ARC-easy | ARC-challenge | Average |'
  prefs: []
  type: TYPE_TB
- en: '| Pythia-160m | 30.1 | 39.6 | 23.8 | 31.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Adam | 28.3 | 37.4 | 22.1 | 29.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MoFO | 29.9 | 42.0 | 22.9 | 31.6 |'
  prefs: []
  type: TYPE_TB
- en: 'We further verify whether the reduced parameter movement (by MoFO) effectively
    mitigates the forgetting of general capabilities acquired by pre-training. We
    evaluate the degree of forgetting from two perspectives: pre-training loss and
    evaluation benchmarks. First, in Figure LABEL:pythia_landscape(b), we find MoFO
    has a lower pre-training loss compared to the Adam optimizer, indicating that
    MoFO memorizes pre-training data better. Second, in Table [2](#S2.T2 "Table 2
    ‣ 2.3 Initial Exploration ‣ 2 Momentum Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning"), we evaluate the accuracies
    of Pythia-160m, fine-tuned using both the Adam optimizer and MoFO, on some widely-used
    common sense tasks, which measures the commonsense reasoning capabilities of LLMs.
    The results show that MoFO achieves less accuracy degradations on all three tasks
    than the Adam optimizer. This indicates that MoFO not only helps maintain a smaller
    pre-training loss but also more effectively mitigates the forgetting of general
    capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [4](#S4 "4 Why MoFO Converges to a Closer Point ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning"), we will provide further
    insights on why MoFO converges to a point closer to the pre-trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Convergence Result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we conduct a convergence analysis of MoFO. We study a simplified
    version of our MoFO algorithm as a variant of gradient descent (GD), which is
    described by Algorithm [2](#alg2 "Algorithm 2 ‣ 2.4 Convergence Result ‣ 2 Momentum
    Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting
    in LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 GD version of MoFO
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: Filtering threshold $\alpha\%$ learning rate schedule $\{\eta_{t}\}$
    from $1$ from $1$ is among the top-$\alpha\%$11:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1  (Convergence of MoFO).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose that the minimum value of the loss function is ${\mathcal{L}}^{*}$,
    it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{0\leq t<T}\&#124;g_{t}\&#124;_{\infty}=O(T^{-\frac{1}{2}}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In summary, we demonstrate the convergence of a GD version of MoFO, providing
    theoretical support for the strong performance of MoFO in fine-tuning tasks. We
    note that it seems rather non-trivial to prove the convergence of the original
    version of MoFO: MoFO contains 1st and 2nd-order momentum in a fractional form,
    and such structure is known to be challenging to handle (Zhang et al., [2022](#bib.bib81)).
    We leave the convergence of the original MoFO method as an interesting future
    direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we verify the effectiveness of MoFO on instruction fine-tuning and continual
    fine-tuning. We use Llama-2-7B (Touvron et al., [2023](#bib.bib66)) and TinyLlama-1.1B
    (Zhang et al., [2024](#bib.bib80)) as the base models for our experiments. We
    also provide studies on the choice of update fraction and update strategies of
    MoFO.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets for instruction fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This group of datasets covers question-answer pairs from different domains
    like mathematical reasoning and code generation. Specifically, the datasets include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetaMathQA (Yu et al., [2024a](#bib.bib76)). This dataset comprises 395K math
    question-answer pairs. Numerous studies indicate that LLMs significantly enhance
    performance metrics on mathematical benchmarks such as GSM8K after fine-tuning
    on this dataset. In this paper, we randomly select 10% of this dataset for training
    LLMs, which includes 33,000 question-answer pairs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code-Alpaca (Chaudhary, [2023](#bib.bib7)). This dataset contains 20,022 question-answer
    pairs. We fine-tune LLMs on this dataset to enhance their coding capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Datasets for continual fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We investigate the performance of MoFO in the continual fine-tuning scenario
    by implementing our approach on the TRACE benchmark dataset (Wang et al., [2023b](#bib.bib69)).
    TRACE benchmark is designed with a comprehensive set of 8 distinct tasks across
    various domains, including domain-specific knowledge, multilingual proficiency,
    code generation, and mathematical reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for instruction fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We introduce a set of widely used benchmarks to assess the performance and
    catastrophic forgetting effects on the general capabilities of LLMs after instruction
    fine-tuning. These benchmarks include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MMLU (Massive Multitask Language Understanding) (Hendrycks et al., [2021](#bib.bib25)).
    It is a popular benchmark to evaluate factual knowledge of LLMs. This benchmark
    spans 57 diverse subjects, ranging from STEM fields and the humanities to social
    sciences. We report the 0-shot accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonsense. We employ the widely recognized benchmarks ARC-Challenge, ARC-Easy
    (Clark et al., [2018](#bib.bib15)), and HellaSwag (Zellers et al., [2019](#bib.bib78))
    to measure the commonsense reasoning capabilities of LLMs. In this paper, we refer
    to these benchmarks collectively as the Commonsense benchmark, and we use the
    average of these three metrics as the evaluation for this Commonsense benchmark.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GSM8K (Cobbe et al., [2021](#bib.bib16)). This benchmark consists of 8.5K high-quality
    grade school math problems. We evaluate the math capability of LLM on the test
    set of GSM8K through the LM Eval Harness framework (Gao et al., [2023](#bib.bib21)).
    We follow the default implementation setting of LM Eval Harness and set the temperature
    hyperparameter as 0 and report 5-shot accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HumanEval (Chen et al., [2021](#bib.bib12)). We adopt the widely used HumanEval
    to assess the coding capabilities of LLMs. It comprises 164 unique programming
    problems. We report the pass@10 performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Metrics for continual fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To evaluate the LLM’s performance in continual learning, we consider two key
    metrics in this scenario: Overall Performance (OP) (Chaudhry et al., [2018](#bib.bib8))
    and BackWard Transfer (BWT) (Lopez-Paz and Ranzato, [2017](#bib.bib42)). Let $R_{t,i}$).
    The OP score measures the average accuracy on all the $T$ tasks after the continual
    fine-tuning process, which is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $OP:=\frac{1}{T}\sum_{i=1}^{T}R_{T,i}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: The BWT score measures the average accuracy change of each task after learning
    new tasks, which is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $BWT:=\frac{1}{T-1}\sum_{i=1}^{T-1}(R_{T,i}-R_{i,i}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: These metrics provide a comprehensive assessment of the model’s ability to learn
    incrementally while retaining knowledge from past experiences. In this paper,
    we will utilize these scores to evaluate the effectiveness of our method in continual
    learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Instruction Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we aim to investigate the effectiveness of the MoFO approach
    in preserving general capabilities while learning fine-tuning tasks. We fine-tune
    Llama-2-7B on MetaMathQA and Code-Alpaca datasets, and compare the performance
    of MoFO against several baseline methods. The evaluation includes changes in performance
    on fine-tuning tasks and general capability metrics, with the pre-trained model’s
    performance serving as the reference point for comparison. The implementation
    details can be seen in [B](#A2 "Appendix B Implementation Details ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare the proposed MoFO algorithm with some of the most widely used optimization
    techniques, which aim to alleviate forgetting, as our baselines. These methods
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full FT refers to default full-parameter fine-tuning approach with the loss
    function $\mathcal{L}_{finetune}(\theta)$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $L_{2}$, with the regularization hyperparameter $\lambda_{2}$ set to 1e-3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $L_{1}$, with the regularization hyperparameter $\lambda_{1}$ set to 1e-6.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Half Fine-tuning (HFT) (Hui et al., [2024](#bib.bib29)) randomly updates half
    of the parameter blocks within each transformer layer at each iteration while
    the other half are frozen. HFT can be considered a specific case of the BCD algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 3: The performance on the math task (GSM8K) and the score changes in
    general capabilities of Llama-2-7B after fine-tuning on the MetaMathQA dataset.
    The results show that MoFO achieves comparable performance in the math task, while
    significantly mitigating catastrophic forgetting of general capabilities. Bold
    values denote the best results among these methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Performance |  | General Capability |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K |  | Commensense | MMLU | HumanEval | Average |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7B | 13.7 |  | 65.6 | 42.0 | 24.2 | 43.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Full FT | 49.4 |  | 62.3 | 36.6 | 16.1 | 38.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -3.3 | -5.4 | -8.1 | -5.6 |'
  prefs: []
  type: TYPE_TB
- en: '| HFT | 47.5 |  | 65.5 | 42.3 | 23.6 | 43.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -0.1 | +0.3 | -0.6 | -0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $L_{1}$-regularization | 39.0 |  | 65.1 | 38.3 | 27.4 | 43.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -0.5 | -3.7 | +3.2 | -0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| $L_{2}$-regularization | 44.5 |  | 65.5 | 39.2 | 25.9 | 43.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -0.1 | -2.8 | +1.7 | -0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MoFO ($\alpha\%=15\%$) | 47.7 |  | 65.7 | 42.7 | 24.6 | 44.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +0.1 | +0.7 | +0.4 | +0.4 |'
  prefs: []
  type: TYPE_TB
- en: Results of fine-tuning on MetaMathQA.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We fine-tune Llama-2-7B on MetaMathQA using various baseline methods and present
    the experimental results on mathematical reasoning (GSM8K) and general capabilities
    in Table [3](#S3.T3 "Table 3 ‣ Baselines ‣ 3.2 Instruction Fine-Tuning ‣ 3 Experiments
    ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning").
    These results demonstrate the effectiveness of our proposed MoFO algorithm in
    both optimization and mitigating catastrophic forgetting.'
  prefs: []
  type: TYPE_NORMAL
- en: MoFO is compatible to the performance of Full FT and HFT on the math task, yet
    significantly outperforms these methods in preserving general capability. Specifically,
    Full FT shows a decline of $5\%$.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to $L_{1}$, respectively. Additionally, while $L_{1}$ regularizations
    suffer from significant catastrophic forgetting on MMLU, MoFO shows no decline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The performance on the coding generation task (HumanEval) and accuracy
    changes in general capabilities of Llama-2-7B after fine-tuning on the Code-Alpaca
    dataset. The results show that MoFO achieves comparable performance in the HumanEval
    while significantly mitigating catastrophic forgetting of general capabilities.
    Bold values denote the best results among these methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Performance |  | General Capability |'
  prefs: []
  type: TYPE_TB
- en: '| HumanEval |  | Commensense | MMLU | GSM8K | Average |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7B | 24.2 |  | 65.6 | 42.0 | 13.7 | 40.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Full FT | 27.7 |  | 65.2 | 39.6 | 5.5 | 36.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -1.4 | -2.4 | -8.2 | -3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| HFT | 30.9 |  | 68.3 | 40.6 | 6.8 | 38.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +2.7 | -1.4 | -6.9 | -1.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $L_{1}$-regularization | 33.8 |  | 67.6 | 41.3 | 5.1 | 38.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +2.0 | -0.7 | -8.6 | -2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| $L_{2}$-regularization | 30.8 |  | 66.1 | 42.5 | 7.6 | 38.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +0.5 | +0.5 | -6.1 | -1.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MoFO ($\alpha\%=10\%$) | 33.7 |  | 67.3 | 42.7 | 8.0 | 39.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +1.7 | +0.7 | -5.7 | -1.1 |'
  prefs: []
  type: TYPE_TB
- en: Results of fine-tuning on Code-Alpaca.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We fine-tune Llama-2-7B on Code-Alpaca using various baseline methods and present
    the experimental results on coding generation and general capabilities in Table
    [4](#S3.T4 "Table 4 ‣ Results of fine-tuning on MetaMathQA. ‣ 3.2 Instruction
    Fine-Tuning ‣ 3 Experiments ‣ MoFO: Momentum-Filtered Optimizer for Mitigating
    Forgetting in LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: MoFO performs well on the fine-tuning task of code generation. It significantly
    outperforms Full FT, HFT, and $L_{2}$ difference.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of general capability, MoFO demonstrates the least degradation compared
    to other baselines, with an average accuracy reduction of only $1.1\%$, while
    our method not only preserves but slightly improves the MMLU accuracy. On the
    math benchmark GSM8K, although all methods experience forgetting, our proposed
    MoFO algorithm exhibits a smaller decline of $5.7\%$.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our MoFO algorithm shows competitive performance in instruction
    fine-tuning while preserving the general capabilities of pre-trained LLMs, effectively
    alleviating catastrophic forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Continual Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we explore the performance of our proposed MoFO in continual
    fine-tuning on the TRACE benchmark (Wang et al., [2023b](#bib.bib69)). We sequentially
    train TinyLlama-1.1B on the TRACE dataset, which includes the eight tasks from
    different domains. The implementation details can be seen in [B](#A2 "Appendix
    B Implementation Details ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting
    in LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: Performance upper bound.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-task Learning (MTL) mixes samples from all eight distinct tasks together
    during training and typically achieves the highest OP score. We will use MTL as
    an upper bound for performance comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Some orthogonal methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We consider several traditional methods from the field of continual learning.
    These methods can be orthogonal combined with MoFO to further enhance performance.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replay involves optimizing the model using current data along with a memory
    buffer containing old samples from previous tasks to mitigate catastrophic forgetting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient of Episodic Memory (GEM) mitigates catastrophic forgetting by using
    gradients from old tasks to adjust the parameter updates during the training of
    new tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 5: The OP and BWT scores of TinyLlama-1.1B after fine-tuning on TRACE
    benchmark. The results show that MoFO outperforms Full FT and HFT in continual
    learning and can combine well with traditional continual learning methods. Bold
    values denote the best results among these methods in each group.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | OP | BWT |'
  prefs: []
  type: TYPE_TB
- en: '| Full FT | 38.4 | -10.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HFT | 39.9 | -10.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MoFO $(\alpha\%=5\%)$ | 41.3 | -5.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GEM | 40.8 | -8.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GEM + MoFO $(\alpha\%=5\%)$ | 41.7 | -6.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Replay | 45.5 | 4.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Replay + MoFO $(\alpha\%=5\%)$ | 47.0 | 4.8 |'
  prefs: []
  type: TYPE_TB
- en: '| MTL | 50.8 |  |'
  prefs: []
  type: TYPE_TB
- en: Results of continual fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present the experimental results of sequentially fine-tuning TinyLlama-1.1B
    on the TRACE benchmark with various methods in Table [5](#S3.T5 "Table 5 ‣ Some
    orthogonal methods. ‣ 3.3 Continual Fine-Tuning ‣ 3 Experiments ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning"). The results indicate
    that in continual fine-tuning, MoFO outperforms Full FT and HFT by at least $1.4\%$
    improvement on the OP metric compared to using GEM alone.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, these results underscore the superior performance of MoFO in continual
    fine-tuning and its effectiveness in alleviating catastrophic forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Further Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we first investigate the impact of the update fraction of parameters
    in the MoFO algorithm at each iteration, and then explore the effects of different
    update strategies within MoFO.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of update fraction of parameters in MoFO.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following the setting in Section 4.2, we fine-tune Llama-2-7B on the MetaMathQA
    dataset using MoFO with varying update fractions of parameters at each iteration
    for 2 epochs. The experimental results of math reasoning (GSM8K) and average general
    capability performance changes are presented in Figure LABEL:ablation_fraction.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter update fraction affects the fine-tuning performance. Figure LABEL:ablation_fraction(a)
    shows that larger update fractions can improve MoFO’s optimization effectiveness.
    Furthurmore, MoFO with a $5\%$, MoFO’s performance matches that of Full FT.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter update fraction also affects the preservation of general capabilities.
    Figure LABEL:ablation_fraction(b) indicates that MoFO avoids forgetting in general
    capabilities when the parameter update fraction is below $20\%$, further increases
    in the parameter update fraction lead to a decline in general capabilities. Despite
    this, MoFO still forgets significantly less than Full FT.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, MoFO can preserve pre-training knowledge and significantly enhance
    fine-tuning performance by choosing a moderate update fraction, avoiding the extremes
    of too small or too large fractions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The performance on the math reasoning task (GSM8K) and accuracy changes
    of general capabilities of Llama-2-7B after fine-tuning on MetaMathQA using different
    updating strategies in MoFO.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Performance |  | General Capability |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K |  | Commensense | MMLU | HumanEval | Average |'
  prefs: []
  type: TYPE_TB
- en: '| Pre-trained model and Full FT |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7B | 13.7 |  | 65.6 | 42.0 | 24.2 | 43.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Full FT | 49.4 |  | 62.3 | 36.6 | 16.1 | 38.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | -3.3 | -5.4 | -8.1 | -5.6 |'
  prefs: []
  type: TYPE_TB
- en: '| BCD based Methods ($\alpha\%=10\%$) |'
  prefs: []
  type: TYPE_TB
- en: '| Randomized BCD | 35.0 |  | 65.8 | 41.1 | 25.1 | 44.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +0.2 | -0.9 | +0.9 | +0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient-filtered BCD | 40.2 |  | 66.0 | 41.6 | 28.0 | 45.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +0.4 | -0.4 | +3.8 | +1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MoFO | 45.4 |  | 65.7 | 43.5 | 27.4 | 45.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | +0.1 | +1.5 | +3.2 | +1.6 |'
  prefs: []
  type: TYPE_TB
- en: Impact of update strategy in MoFO.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition to MoFO, we consider two other BCD methods, randomized BCD and
    gradient-filtered BCD. Randomized BCD updates a random subset of parameters at
    each iteration. Gradient-filtered BCD selects the filter based on gradient magnitudes
    rather than the momentum magnitudes used in MoFO. Specifically, line 11 in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 2.2 Algorithm Formulation ‣ 2 Momentum Filtered Optimizer
    (MoFO) ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning")
    is replaced by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(\texttt{FILTER}_{t}^{(k)})_{i}=1\text{ if }&#124;(g^{(k)}_{t})_{i}&#124;\text{
    is among the top-}\alpha\%\text{ of all }&#124;g^{(k)}_{t}&#124;\text{''s entries
    else }0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We fine-tune Llama-2-7B on MetaMathQA using these three methods with $10\%$
    parameter update fraction and present the results in Table [6](#S3.T6 "Table 6
    ‣ Impact of update fraction of parameters in MoFO. ‣ 3.4 Further Analysis ‣ 3
    Experiments ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM
    Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental results show that all three BCD methods exhibit significantly less
    forgetting compared to Full FT, demonstrating the effectiveness of BCD algorithms
    in mitigating catastrophic forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of GSM8K performance, our proposed MoFO method significantly surpasses
    both Gradient-filtered BCD and randomized BCD, indicating that updating parameters
    with the largest momentum leads to strong optimization power.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Why MoFO Converges to a Closer Point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Section 3, we discuss how the distance of parameter movement during fine-tuning
    impacts the retention of pre-training knowledge. Drawing on these insights and
    the BCD method, we introduce MoFO. Here, we propose the following question as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Why does MoFO converge closer to the pre-trained LLMs than those
    of Adam?'
  prefs: []
  type: TYPE_NORMAL
- en: 'We attempt to answer this question by the following toy example. We denote
    $\Theta=(\theta_{1},\theta_{2})\in\mathbb{R}^{2}$ to be the trainable parameters
    of our model and make the following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pre-training loss is $\mathcal{L}_{pretrain}(\Theta)=\theta_{1}^{2}+\theta_{2}^{2}$
    during the pre-trained phase.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuning loss is $\mathcal{L}_{finetune}(\Theta)=(\theta_{1}-1)^{2}(\theta_{2}-1)^{2}$,
    which is a union of two straight lines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For full-parameter fine-tuning with Adam, starting from $\Theta_{0}=(0,0)$ during
    the fine-tuning phase along the green arrow in Figure LABEL:example_landscape,
    resulting in a pre-training loss of 1\. This demonstrates that MoFO can converge
    to a minimum that is closer to the pre-training model, thereby mitigating forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Insights: In this example, we find that when a loss function has multiple distinct
    minima, they can be considered as different attractors. These attractors can influence
    the gradient direction of a pre-trained model, possibly drawing the model’s weights
    away from the nearest minimum. Specifically, full-parameter gradient descent based
    methods may converge to the balanced point of these attractors’ influences, which
    is the orange point in Figure LABEL:example_landscape(a). On the contrary, MoFO
    addresses this issue by updating only a subset of parameters during each iteration.
    This selective updating rule reduces interference among attractors, allowing the
    model to converge to a closer minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Catastrophic forgetting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Catastrophic forgetting, a significant issue where models forget previously
    learned information upon learning new data, has received considerable attention
    in machine learning (McCloskey and Cohen, [1989](#bib.bib46); Goodfellow et al.,
    [2013](#bib.bib22); Kemker et al., [2018](#bib.bib31); Ramasesh et al., [2021](#bib.bib54);
    Liu et al., [2024](#bib.bib41)). In the realm of LLMs, there has been a growing
    body of recent works (Luo et al., [2023](#bib.bib45); Kotha et al., [2024](#bib.bib36);
    Shi et al., [2024](#bib.bib63); Wu et al., [2024](#bib.bib72)) focusing on the
    forgetting of models’ knowledge during the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers have proposed numerous methods to alleviate forgetting in continual
    learning, which involves learning a sequence of tasks. These methods are not limited
    to learning in a sequential manner and can be applied to broader paradigms. Generally,
    three primary approaches are used: replay-based methods, regularization-based
    methods, and architecture-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replay-based methods leverage past experiences to facilitate the learning of
    new tasks. The most straightforward implementation of this approach is experience
    replay, which involves maintaining old samples in a buffer and replaying them
    during incremental training (Rolnick et al., [2019](#bib.bib61)) with some variants
    enhancing performance or optimizing memory usage (Aljundi et al., [2019a](#bib.bib2);
    Hayes et al., [2019](#bib.bib24); Cha et al., [2021](#bib.bib6); Chaudhry et al.,
    [2019b](#bib.bib10); Riemer et al., [2019b](#bib.bib59)). Several other variants
    utilize gradient information from past tasks (Lopez-Paz and Ranzato, [2017](#bib.bib42);
    Riemer et al., [2019a](#bib.bib58); Chaudhry et al., [2019a](#bib.bib9); Farajtabar
    et al., [2020](#bib.bib19); Aljundi et al., [2019b](#bib.bib3); Chaudhry et al.,
    [2021](#bib.bib11); Tiwari et al., [2022](#bib.bib65)). In LLMs, several works
    (Yin et al., [2023](#bib.bib75); Wang et al., [2024](#bib.bib70); Ouyang et al.,
    [2022](#bib.bib51)) propose replay-based methods to mitigate forgetting. We emphasize
    that our MoFO method is orthogonal to replay-based methods. MoFO can be integrated
    into these replay strategies to further enhance their effectiveness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularization-based methods introduce constraints to preserve old knowledge.
    Several studies add regularization terms to the loss functions to penalize parameter
    changes and mitigate forgetting (Kirkpatrick et al., [2017](#bib.bib34); Aljundi
    et al., [2018](#bib.bib1); Zenke et al., [2017](#bib.bib79); Li et al., [2018](#bib.bib38);
    Ritter et al., [2018](#bib.bib60); Kumar et al., [2023](#bib.bib37)). Some other
    works apply regularization to the embedding or output changes (Li and Hoiem, [2017](#bib.bib39);
    Rannen et al., [2017](#bib.bib55); Buzzega et al., [2020](#bib.bib5); Huang et al.,
    [2021](#bib.bib28)). Unlike these approaches, MoFO does not modify the loss function
    and as a result, MoFO reached better fine-tuning performance (see Table [3](#S3.T3
    "Table 3 ‣ Baselines ‣ 3.2 Instruction Fine-Tuning ‣ 3 Experiments ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning") and [4](#S3.T4 "Table
    4 ‣ Results of fine-tuning on MetaMathQA. ‣ 3.2 Instruction Fine-Tuning ‣ 3 Experiments
    ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture-based methods balance the goals of learning new knowledge and keeping
    the old one through model architectural modifications. LoRA (Hu et al., [2022](#bib.bib27)),
    as the most popular parameter-efficient fine-tuning (PEFT) methods, modifies the
    model architecture by freezing the pre-training weights and introducing low-rank
    trainable matrices. Empirical works shows that LoRA forgets less but learns less
    during fine-tuning (Biderman et al., [2024](#bib.bib4)). Some variants of LoRA
    find applications in continual learning of LLMs (Ren et al., [2024](#bib.bib56);
    Wang et al., [2023a](#bib.bib68)). In comparison, our MoFO still allows for high-rank
    updates to achieve better fine-tuning performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Another line of architecture-based methods focuses on model merging. The idea
    stem from the understanding that the task-specific knowledge is located at a small
    subspace of the weight space (Panigrahi et al., [2023](#bib.bib52); Gueta et al.,
    [2023](#bib.bib23); Zhu et al., [2024](#bib.bib82)). Consequently, various model
    merging methods have been proposed to simultaneously retain pre-training knowledge
    and improve fine-tuning task performance (Panigrahi et al., [2023](#bib.bib52);
    Yadav et al., [2024](#bib.bib74); Yu et al., [2024b](#bib.bib77)). However, these
    methods require an additional post-fine-tuning process before model merging. In
    contrast, our method only requires only one fine-tuning stage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Block coordinate descent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Block Coordinate Descent (BCD) involves iteratively optimizing over a block
    of coordinates while holding the others constant. The foundational work of Tseng
    ([2001](#bib.bib67)) provides a comprehensive analysis of the convergence properties
    of BCD under certain conditions. Subsequent research has explored various BCD
    variants (Hong et al., [2017](#bib.bib26)), including randomized BCD (Nesterov,
    [2012](#bib.bib48); Richtárik and Takáč, [2014](#bib.bib57); Lu and Xiao, [2015](#bib.bib43)),
    cyclic BCD (Sun and Hong, [2015](#bib.bib64)), and greedy BCD (Nutini et al.,
    [2015](#bib.bib49)). Among these, the greedy variant, also known as Gauss-Southwell
    BCD method, has drawn attention due to its ability to prioritize coordinates that
    yield the most substantial improvement in each iteration, thereby potentially
    accelerating convergence.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of machine learning, BCD has also found applications (Nutini et al.,
    [2022](#bib.bib50)). For example, Luo et al. ([2024](#bib.bib44)) leverages BCD
    to perform memory-efficient fine-tuning of LLM and Xu and Zhang ([2024](#bib.bib73))
    use random masking to perform this. In federated learning, Rothchild et al. ([2020](#bib.bib62))
    adopts top-$k$ momentum filtering to tackle communication bottleneck and convergence
    issues. A recent work, Hui et al. ([2024](#bib.bib29)) addresses catastrophic
    forgetting during fine-tuning of LLMs by selectively freezing half of the parameters
    during training. Our approach is akin to a more efficient greedy BCD, achieving
    superior performance in fine-tuning tasks and alleviating forgetting better.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents the Momentum-Filtered Optimizer (MoFO), a new approach designed
    to mitigate the crucial issue of pre-training knowledge forgetting in LLMs during
    fine-tuning. By selectively updating the parameters with the largest momentum
    magnitudes in each parameter block, MoFO converges to a point closer to the pre-trained
    model compared to full-parameter fine-tuning and effectively preserves pre-trained
    knowledge. Our experimental results demonstrate that MoFO not only significantly
    alleviates catastrophic forgetting but also surpasses the performance of traditional
    fine-tuning methods. Future work will explore further optimizations and potential
    applications of MoFO in multimodal LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aljundi et al. [2018] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and
    T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In *Proceedings
    of the European conference on computer vision (ECCV)*, pages 139–154, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aljundi et al. [2019a] R. Aljundi, E. Belilovsky, T. Tuytelaars, L. Charlin,
    M. Caccia, M. Lin, and L. Page-Caccia. Online continual learning with maximal
    interfered retrieval. *Advances in neural information processing systems*, 32,
    2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aljundi et al. [2019b] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio. Gradient
    based sample selection for online continual learning. *Advances in neural information
    processing systems*, 32, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biderman et al. [2024] D. Biderman, J. G. Ortiz, J. Portes, M. Paul, P. Greengard,
    C. Jennings, D. King, S. Havens, V. Chiley, J. Frankle, et al. LoRA learns less
    and forgets less. *arXiv preprint arXiv:2405.09673*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buzzega et al. [2020] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara.
    Dark experience for general continual learning: a strong, simple baseline. *Advances
    in neural information processing systems*, 33:15920–15930, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cha et al. [2021] H. Cha, J. Lee, and J. Shin. Co2l: Contrastive continual
    learning. In *Proceedings of the IEEE/CVF International conference on computer
    vision*, pages 9516–9525, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhary [2023] S. Chaudhary. Code alpaca: An instruction-following llama
    model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhry et al. [2018] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr.
    Riemannian walk for incremental learning: Understanding forgetting and intransigence.
    In *European Conference on Computer Vision*, pages 556–572, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhry et al. [2019a] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny.
    Efficient lifelong learning with A-GEM. In *International Conference on Learning
    Representations*, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhry et al. [2019b] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan,
    P. K. Dokania, P. H. Torr, and M. Ranzato. On tiny episodic memories in continual
    learning. *arXiv preprint arXiv:1902.10486*, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhry et al. [2021] A. Chaudhry, A. Gordo, P. Dokania, P. Torr, and D. Lopez-Paz.
    Using hindsight to anchor past knowledge in continual learning. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 35, pages 6993–7001,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language
    models trained on code. *arXiv preprint arXiv:2107.03374*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2020] S. Chen, Y. Hou, Y. Cui, W. Che, T. Liu, and X. Yu. Recall
    and learn: Fine-tuning deep pretrained language models with less forgetting. In
    *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP)*, pages 7870–7881, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2024] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, H. Pham, X. Dong,
    T. Luong, C.-J. Hsieh, Y. Lu, et al. Symbolic discovery of optimization algorithms.
    *Advances in neural information processing systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2018] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal,
    C. Schoenick, and O. Tafjord. Think you have solved question answering? Try ARC,
    the AI2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. [2021] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
    M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai and Le [2015] A. M. Dai and Q. V. Le. Semi-supervised sequence learning.
    *Advances in neural information processing systems*, 28, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. [2021] X. Dong, A. T. Luu, M. Lin, S. Yan, and H. Zhang. How should
    pre-trained language models be fine-tuned towards adversarial robustness? *Advances
    in Neural Information Processing Systems*, 34:4356–4369, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farajtabar et al. [2020] M. Farajtabar, N. Azizan, A. Mott, and A. Li. Orthogonal
    gradient descent for continual learning. In *International Conference on Artificial
    Intelligence and Statistics*, pages 3762–3773\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2020] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,
    J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of
    diverse text for language modeling. *arXiv preprint arXiv:2101.00027*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2023] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi,
    C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff,
    C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang,
    A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model
    evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2013] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville,
    and Y. Bengio. An empirical investigation of catastrophic forgetting in gradient-based
    neural networks. *arXiv preprint arXiv:1312.6211*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gueta et al. [2023] A. Gueta, E. Venezian, C. Raffel, N. Slonim, Y. Katz, and
    L. Choshen. Knowledge is a region in weight space for fine-tuned language models.
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    1350–1370, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hayes et al. [2019] T. L. Hayes, N. D. Cahill, and C. Kanan. Memory efficient
    experience replay for streaming learning. In *2019 International Conference on
    Robotics and Automation (ICRA)*, pages 9769–9776\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2021] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,
    D. Song, and J. Steinhardt. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. [2017] M. Hong, X. Wang, M. Razaviyayn, and Z.-Q. Luo. Iteration
    complexity analysis of block coordinate descent methods. *Mathematical Programming*,
    163:85–114, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2022] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
    W. Chen, et al. LoRA: Low-rank adaptation of large language models. In *International
    Conference on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2021] Y. Huang, Y. Zhang, J. Chen, X. Wang, and D. Yang. Continual
    learning for text classification with information disentanglement based regularization.
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2736–2746,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hui et al. [2024] T. Hui, Z. Zhang, S. Wang, W. Xu, Y. Sun, and H. Wu. Hft:
    Half fine-tuning for large language models. *arXiv preprint arXiv:2404.18466*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ivison et al. [2023] H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters,
    P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, et al. Camels in a changing
    climate: Enhancing lm adaptation with tulu 2. *arXiv preprint arXiv:2311.10702*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kemker et al. [2018] R. Kemker, M. McClure, A. Abitino, T. Hayes, and C. Kanan.
    Measuring catastrophic forgetting in neural networks. In *Proceedings of the AAAI
    conference on artificial intelligence*, volume 32, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kenton and Toutanova [2019] J. D. M.-W. C. Kenton and L. K. Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. In
    *Proceedings of NAACL-HLT*, pages 4171–4186, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba [2014] D. P. Kingma and J. Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirkpatrick et al. [2017] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness,
    G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska,
    et al. Overcoming catastrophic forgetting in neural networks. *Proceedings of
    the national academy of sciences*, 114(13):3521–3526, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korbak et al. [2022] T. Korbak, H. Elsahar, G. Kruszewski, and M. Dymetman.
    Controlling conditional language models without catastrophic forgetting. In *International
    Conference on Machine Learning*, pages 11499–11528\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kotha et al. [2024] S. Kotha, J. M. Springer, and A. Raghunathan. Understanding
    catastrophic forgetting in language models via implicit inference. In *The Twelfth
    International Conference on Learning Representations*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. [2023] S. Kumar, H. Marklund, and B. Van Roy. Maintaining plasticity
    via regenerative regularization. *arXiv preprint arXiv:2308.11958*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2018] X. Li, Y. Grandvalet, and F. Davoine. Explicit inductive bias
    for transfer learning with convolutional networks. In *International Conference
    on Machine Learning*, pages 2825–2834\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Hoiem [2017] Z. Li and D. Hoiem. Learning without forgetting. *IEEE transactions
    on pattern analysis and machine intelligence*, 40(12):2935–2947, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Y. Lin, L. Tan, H. Lin, Z. Zheng, R. Pi, J. Zhang, S. Diao,
    H. Wang, H. Zhao, Y. Yao, et al. Speciality vs generality: An empirical study
    on catastrophic forgetting in fine-tuning foundation models. *arXiv preprint arXiv:2309.06256*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2024] C. Liu, S. Wang, Y. Kang, L. Qing, F. Zhao, C. Sun, K. Kuang,
    and F. Wu. More than catastrophic forgetting: Integrating general capabilities
    for domain-specific llms. *arXiv preprint arXiv:2405.17830*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lopez-Paz and Ranzato [2017] D. Lopez-Paz and M. Ranzato. Gradient episodic
    memory for continual learning. *Advances in neural information processing systems*,
    30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu and Xiao [2015] Z. Lu and L. Xiao. On the complexity analysis of randomized
    block-coordinate descent methods. *Mathematical Programming*, 152:615–642, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2024] Q. Luo, H. Yu, and X. Li. Badam: A memory efficient full
    parameter training method for large language models. *arXiv preprint arXiv:2404.02827*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2023] Y. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang. An
    empirical study of catastrophic forgetting in large language models during continual
    fine-tuning. *arXiv preprint arXiv:2308.08747*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCloskey and Cohen [1989] M. McCloskey and N. J. Cohen. Catastrophic interference
    in connectionist networks: The sequential learning problem. In *Psychology of
    learning and motivation*, volume 24, pages 109–165\. Elsevier, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miceli-Barone et al. [2017] A. V. Miceli-Barone, B. Haddow, U. Germann, and
    R. Sennrich. Regularization techniques for fine-tuning in neural machine translation.
    In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing*, pages 1489–1494, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nesterov [2012] Y. Nesterov. Efficiency of coordinate descent methods on huge-scale
    optimization problems. *SIAM Journal on Optimization*, 22(2):341–362, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nutini et al. [2015] J. Nutini, M. Schmidt, I. Laradji, M. Friedlander, and
    H. Koepke. Coordinate descent converges faster with the gauss-southwell rule than
    random selection. In *International Conference on Machine Learning*, pages 1632–1641\.
    PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nutini et al. [2022] J. Nutini, I. Laradji, and M. Schmidt. Let’s make block
    coordinate descent converge faster: faster greedy rules, message-passing, active-set
    complexity, and superlinear convergence. *Journal of Machine Learning Research*,
    23(131):1–74, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,
    P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models
    to follow instructions with human feedback. *Advances in neural information processing
    systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panigrahi et al. [2023] A. Panigrahi, N. Saunshi, H. Zhao, and S. Arora. Task-specific
    skill localization in fine-tuned language models. In *International Conference
    on Machine Learning*, pages 27011–27033\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.
    Improving language understanding with unsupervised learning. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramasesh et al. [2021] V. V. Ramasesh, A. Lewkowycz, and E. Dyer. Effect of
    scale on catastrophic forgetting in neural networks. In *International Conference
    on Learning Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rannen et al. [2017] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars.
    Encoder based lifelong learning. In *Proceedings of the IEEE international conference
    on computer vision*, pages 1320–1328, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. [2024] W. Ren, X. Li, L. Wang, T. Zhao, and W. Qin. Analyzing and
    reducing catastrophic forgetting in parameter efficient tuning. *arXiv preprint
    arXiv:2402.18865*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Richtárik and Takáč [2014] P. Richtárik and M. Takáč. Iteration complexity of
    randomized block-coordinate descent methods for minimizing a composite function.
    *Mathematical Programming*, 144(1):1–38, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riemer et al. [2019a] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu,
    and G. Tesauro. Learning to learn without forgetting by maximizing transfer and
    minimizing interference. In *International Conference on Learning Representations*,
    2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riemer et al. [2019b] M. Riemer, T. Klinger, D. Bouneffouf, and M. Franceschini.
    Scalable recollections for continual lifelong learning. In *Proceedings of the
    AAAI conference on artificial intelligence*, volume 33, pages 1352–1359, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ritter et al. [2018] H. Ritter, A. Botev, and D. Barber. Online structured laplace
    approximations for overcoming catastrophic forgetting. *Advances in Neural Information
    Processing Systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolnick et al. [2019] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne.
    Experience replay for continual learning. *Advances in neural information processing
    systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rothchild et al. [2020] D. Rothchild, A. Panda, E. Ullah, N. Ivkin, I. Stoica,
    V. Braverman, J. Gonzalez, and R. Arora. Fetchsgd: Communication-efficient federated
    learning with sketching. In *International Conference on Machine Learning*, pages
    8253–8265\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2024] H. Shi, Z. Xu, H. Wang, W. Qin, W. Wang, Y. Wang, and H. Wang.
    Continual learning of large language models: A comprehensive survey. *arXiv preprint
    arXiv:2404.16789*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun and Hong [2015] R. Sun and M. Hong. Improved iteration complexity bounds
    of cyclic block coordinate descent for convex problems. *Advances in Neural Information
    Processing Systems*, 28, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tiwari et al. [2022] R. Tiwari, K. Killamsetty, R. Iyer, and P. Shenoy. Gcr:
    Gradient coreset based replay buffer selection for continual learning. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    99–108, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open
    foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tseng [2001] P. Tseng. Convergence of a block coordinate descent method for
    nondifferentiable minimization. *Journal of optimization theory and applications*,
    109:475–494, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023a] X. Wang, T. Chen, Q. Ge, H. Xia, R. Bao, R. Zheng, Q. Zhang,
    T. Gui, and X. Huang. Orthogonal subspace learning for language model continual
    learning. In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023b] X. Wang, Y. Zhang, T. Chen, S. Gao, S. Jin, X. Yang, Z. Xi,
    R. Zheng, Y. Zou, T. Gui, et al. Trace: A comprehensive benchmark for continual
    learning in large language models. *arXiv preprint arXiv:2310.06762*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2024] Y. Wang, Y. Liu, C. Shi, H. Li, C. Chen, H. Lu, and Y. Yang.
    Inscl: A data-efficient continual learning paradigm for fine-tuning large language
    models with instructions. In *Proceedings of the 2024 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies (Volume 1: Long Papers)*, pages 663–677, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] Z. Wang, S. V. Mehta, B. Poczós, and J. G. Carbonell. Efficient
    meta lifelong-learning with limited memory. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 535–548, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2024] T. Wu, L. Luo, Y.-F. Li, S. Pan, T.-T. Vu, and G. Haffari.
    Continual learning for large language models: A survey. *arXiv preprint arXiv:2402.01364*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu and Zhang [2024] J. Xu and J. Zhang. Random masking finds winning tickets
    for parameter efficient fine-tuning. *arXiv preprint arXiv:2405.02596*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yadav et al. [2024] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal.
    Ties-merging: Resolving interference when merging models. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. [2023] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, and
    K.-W. Chang. Dynosaur: A dynamic growth paradigm for instruction-tuning data curation.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 4031–4047, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2024a] L. Yu, W. Jiang, H. Shi, Y. Jincheng, Z. Liu, Y. Zhang, J. Kwok,
    Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions
    for large language models. In *The Twelfth International Conference on Learning
    Representations*, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2024b] L. Yu, B. Yu, H. Yu, F. Huang, and Y. Li. Language models
    are super mario: Absorbing abilities from homologous models as a free lunch. In
    *Forty-first International Conference on Machine Learning*, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi.
    Hellaswag: Can a machine really finish your sentence? In *Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics*, pages 4791–4800,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zenke et al. [2017] F. Zenke, B. Poole, and S. Ganguli. Continual learning through
    synaptic intelligence. In *International conference on machine learning*, pages
    3987–3995\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2024] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source
    small language model. *arXiv preprint arXiv:2401.02385*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022] Y. Zhang, C. Chen, N. Shi, R. Sun, and Z.-Q. Luo. Adam can
    converge without any modification on update rules. *Advances in neural information
    processing systems*, 35:28386–28399, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2024] D. Zhu, Z. Sun, Z. Li, T. Shen, K. Yan, S. Ding, K. Kuang,
    and C. Wu. Model tailor: Mitigating catastrophic forgetting in multi-modal large
    language models. *arXiv preprint arXiv:2402.12048*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix A Proof of Theorem [1](#Thmthm1 "Theorem 1 (Convergence of MoFO).
    ‣ 2.4 Convergence Result ‣ 2 Momentum Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before providing the proof, we will give some preliminary information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume the parameter space $\mathbb{R}^{d}$ denotes the product of the
    dimensions (i.e., the number of rows multiplied by the number of columns) of the
    $k$, we denote
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z={\rm Concat}(z^{(1)};z^{(2)};\dots;z^{(B)}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $z^{(k)}\in\mathbb{R}^{d_{k}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $z\in\mathbb{R}^{d}$, we define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;{z}\&#124;_{\text{top-}\alpha\%}:=\&#124;z\odot{\mathbf{e}}_{S}\&#124;_{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where ${\mathbf{e}}_{S}={\rm Concat}({\mathbf{e}}^{(1)}_{S_{1}};{\mathbf{e}}^{(2)}_{S_{2}};\dots;{\mathbf{e}}^{(B)}_{S_{B}})$.
    Here,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and ${\mathbf{e}}_{S_{k}}^{(k)}$, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: $\|{\cdot}\|_{\text{top-}\alpha\%}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By Definition [1](#Thmdefn1 "Definition 1\. ‣ Appendix A Proof of Theorem 1
    ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning"),
    we get'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: First, if $\|{z}\|_{\text{top-}\alpha\%}=0$. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: So $z^{(k)}$ is a zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: Second, for any given $c\in\mathbb{R}_{+}$ and $cz$ and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Third, for the vectors $x,y\in\mathbb{R}^{d}$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;{x+y}\&#124;_{\text{top-}\alpha\%}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\&#124;x\odot{\mathbf{e}}_{S^{\prime\prime}}+y\odot{\mathbf{e}}_{S^{\prime\prime}}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\&#124;x\odot{\mathbf{e}}_{S^{\prime\prime}}\&#124;_{2}+\&#124;y\odot{\mathbf{e}}_{S^{\prime\prime}}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\&#124;x\odot{\mathbf{e}}_{S}\&#124;_{2}+\&#124;y\odot{\mathbf{e}}_{S^{\prime}}\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\&#124;{x}\&#124;_{\text{top-}\alpha\%}+\&#124;{y}\&#124;_{\text{top-}\alpha\%}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof of Theorem [1](#Thmthm1 "Theorem 1 (Convergence of MoFO). ‣ 2.4 Convergence
    Result ‣ 2 Momentum Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered Optimizer
    for Mitigating Forgetting in LLM Fine-Tuning").'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By the definition of the simple version of MOFO in Algorithm [2](#alg2 "Algorithm
    2 ‣ 2.4 Convergence Result ‣ 2 Momentum Filtered Optimizer (MoFO) ‣ MoFO: Momentum-Filtered
    Optimizer for Mitigating Forgetting in LLM Fine-Tuning"), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Theta_{t+1}-\Theta_{t}=-\eta_{t}\hat{m}_{t}\odot\texttt{FILTER}_{t}=-\eta_{t}g_{t}\odot\texttt{FILTER}_{t},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\texttt{FILTER}_{t}$ in Definition [1](#Thmdefn1 "Definition 1\. ‣ Appendix
    A Proof of Theorem 1 ‣ MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting
    in LLM Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: By the Lipschitz condition of $\nabla\cal{L}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathcal{L}}(\Theta_{t+1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq{\mathcal{L}}(\Theta_{t})-\frac{1}{2L}\&#124;{g_{t}}\&#124;_{\text{top-}\alpha\%}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the second inequality becomes an equality if the learning rate $\alpha=1/L$.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instruction fine-tuning. In our instruction fine-tuning experiments, we follow
    the implementation of Ivison et al. [[2023](#bib.bib30)]. The learning rate is
    set to 2e-5 with a cosine decay scheduler. For fine-tuning on the MetaMathQA dataset,
    we set the maximum sequence length to 1024, the batch size to 128, and we train
    the Llama-2-7B for 2 epochs. The parameter update fraction for MoFO is set to
    $15\%$, while keeping other settings the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continual fine-tuning. In our continual fine-tuning experiments, we follow
    the default settings of the TRACE benchmark. We sequentially train TinyLlama-1.1B
    on the TRACE benchmark datasets: C-STANCE, FOMC, MeetingBank, Py150, ScienceQA,
    NumGLUE-cm, NumGLUE-ds, and 20Minuten for 5, 3, 7, 5, 3, 5, 5, and 7 epochs, respectively.
    We use a learning rate of 1e-5 with a cosine decay schedule and a batch size of
    64\. The parameter update fraction for MoFO is set to $5\%$.'
  prefs: []
  type: TYPE_NORMAL
- en: All experiments are conducted on four A800 (80GB) GPUs.
  prefs: []
  type: TYPE_NORMAL
