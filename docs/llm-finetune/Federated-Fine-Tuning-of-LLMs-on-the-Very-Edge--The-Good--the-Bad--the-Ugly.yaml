- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03150](https://ar5iv.labs.arxiv.org/html/2310.03150)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Herbert Woisetschläger
  prefs: []
  type: TYPE_NORMAL
- en: Technical University of Munich
  prefs: []
  type: TYPE_NORMAL
- en: herbert.woisetschlaeger@tum.de Alexander Isenko
  prefs: []
  type: TYPE_NORMAL
- en: Technical University of Munich
  prefs: []
  type: TYPE_NORMAL
- en: alex.isenko@tum.de Shiqiang Wang
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: wangshiq@us.ibm.com Ruben Mayer
  prefs: []
  type: TYPE_NORMAL
- en: University of Bayreuth
  prefs: []
  type: TYPE_NORMAL
- en: ruben.mayer@uni-bayreuth.de Hans-Arno Jacobsen
  prefs: []
  type: TYPE_NORMAL
- en: University of Toronto
  prefs: []
  type: TYPE_NORMAL
- en: jacobsen@eecg.toronto.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLM) and foundation models are popular as they offer
    new opportunities for individuals and businesses to improve natural language processing,
    interact with data, and retrieve information faster. However, training or fine-tuning
    LLMs requires a vast amount of data, which can be challenging to access due to
    legal or technical restrictions and may require private computing resources. Federated
    Learning (FL) is a solution designed to overcome these challenges and expand data
    access for deep learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper takes a hardware-centric approach to explore how LLMs can be brought
    to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family,
    ranging from 80M to 3B parameters, using FL for a text summarization task. We
    provide a micro-level hardware benchmark, compare the model FLOP utilization to
    a state-of-the-art data center GPU, and study the network utilization in realistic
    conditions. Our contribution is twofold: First, we evaluate the current capabilities
    of edge computing systems and their potential for LLM FL workloads. Second, by
    comparing these systems with a data-center GPU, we demonstrate the potential for
    improvement and the next steps toward achieving greater computational efficiency
    at the edge.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Foundation models are omnipresent in academia and practice and fuel new innovations [[1](#bib.bib1)].
    These models have grown significantly w.r.t. to parameter size, as more parameters
    generally improve the performance to a certain degree [[2](#bib.bib2)]. In line
    with the growing computational need for these models, deep learning (DL) hardware
    accelerators have become increasingly more capable. Recent developments indicate
    a generational leap in computational power for data center applications, with
    the NVIDIA H100 NVL delivering 7.8 TB/s memory bandwidth compared to the previous
    state-of-the-art A100 80GB GPU that only has 2 TB/s ([Figure 1](#S1.F1 "In 1 Introduction
    ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly")).
    Due to memory-bandwidth bottlenecked operations taking up to 40% of the training
    time [[3](#bib.bib3)], this improvement may lead to much faster training times
    for both small and large models. At the same time, computational capabilities
    on embedded devices for mobile edge computing are significantly growing, with
    the NVIDIA Jetson AGX Orin 64GB being the first-of-a-kind DL-accelerated embedded
    device that provides capabilities for training foundation models [[4](#bib.bib4)].
    This has never been possible before and enables us to build FL workloads with
    large transformer models, benefit from scattered data, and bring generative AI
    closer to users. At the same time, computational capabilities on embedded devices
    for mobile edge computing are significantly growing, with the NVIDIA Jetson AGX
    Orin 64GB being the first-of-a-kind DL accelerated embedded device that provides
    capabilities for training foundation models [[4](#bib.bib4)]. This has never been
    possible before and enables us to build FL workloads with large transformer models,
    benefit from scattered data, and bring generative AI closer to users.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18d8e2e725088dceca6ba3c775f44417.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Development of computational power and resource availability of DL
    accelerators 2017 - 2023 for data centers and embedded systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As this type of device is oftentimes scattered across geographies and entities,
    federated DL (FL) imposes itself as a major technique for fine-tuning foundation
    models in a distributed and private fashion. To our knowledge, the largest models
    discussed in FL to this point entail FedBert and GPT2 [[5](#bib.bib5), [6](#bib.bib6)].
    Both models were trained with FL methods on multi-GPU data center nodes. However,
    if we want to gain access to a broader data basis, we need to foster FL on the
    edge and bring foundation models to embedded devices. However, there are several
    challenges that we have to overcome:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limited memory bandwidth on embedded devices. We currently see a generation
    leap in data center DL accelerators regarding memory bandwidth, which has increased
    significantly (up to 7.8 TB/s). Even though the memory size on embedded devices
    has increased, the memory bandwidth remains comparatively low (up to 0.2 TB/s).
    This affects key memory-bandwidth bottlenecked operations for the training process,
    which could lead to severe training time penalties.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Foundation models have a large number of parameters. Foundation models can have
    several billions of parameters, which, even in data-center environments, requires
    efficient communication and optimal memory utilization so that distributed training
    creates a speedup. Techniques like gradient checkpointing [[7](#bib.bib7)], quantization [[8](#bib.bib8)],
    and delayed parameter updates [[9](#bib.bib9)] are common to improve hardware
    utilization and training throughput. These have not been sufficiently evaluated
    on edge environments, which work with different communication and hardware trade-offs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Communication on the edge is significantly more expensive than in data centers.
    While network bandwidth in data centers is available at 100 Gbit [[10](#bib.bib10)],
    mobile or remote communication over wide area networks is still a difficult challenge
    to achieve, especially when handling multi-billion parameter DL models. Techniques
    like ZeRo-offloading [[9](#bib.bib9)] and FSDP [[11](#bib.bib11)] utilize a high-bandwidth
    interconnect for all-reduce communication between nodes to not materialize the
    full model, optimizer, and gradient state due to limited memory sizes. This is
    typically not a viable option on the edge due to a much more limited interconnect,
    as they rely on high communication to optimize computational load.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Power is a key control variable in edge deployments. Contrary to data center
    deployments, workloads on the edge are subject to energy limitations and often
    require adjustments to frequently changing operating environments. This has direct
    implications for the training speed of FL clients and creates strict constraints
    on what is feasible to run on the edge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Based on the open challenges to create efficient edge computing systems that
    are capable of training foundation models and LLMs, we formulate our research
    questions: How is fine-tuning LLMs on the edge different from data center environments?
    What are the control levers to optimize LLM fine-tuning on the edge?'
  prefs: []
  type: TYPE_NORMAL
- en: 'By exploring this research question, we make two contributions and discover
    two main findings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contribution 1: In-depth comparison of state-of-the-art DL accelerators for
    FL workloads. Nowadays, most papers in the FL space use data center hardware for
    their experiments, while large amounts of data are scattered on the edge and must
    not be neglected as a field of application. We, therefore, conduct an in-depth
    micro-benchmark of various transformer models on the latest data center and embedded
    DL accelerators.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contribution 2: Thorough study of optimization levers for FL workloads at the
    edge on embedded hardware. Our hardware-centric study outlines the effects of
    energy efficiency and the model FLOP utilization on overall training speeds. Further,
    we outline the suitability of granularity (see [Section 3](#S3 "3 Methodology
    ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly"))
    to estimate the practicality of FL workloads over varying network conditions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finding 1: Power draw is a lever for DL performance estimation the edge. As
    we draw out the direct correlation of the model FLOP utilization, known for its
    broad applicability in the HPC domain, and energy efficiency, a crucial driver
    in the edge computing field, we identify an optimization lever for FL client selection
    and model aggregation strategies that enables applications, which focus on optimal
    computational resource utilization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finding 2: Training foundation models on the edge is all about memory bandwidth
    and network utilization. With our study, we identify the two major limiting factors
    required for efficiently training FMs on embedded hardware: memory bandwidth and
    network communication. We further point out strategies that can reduce memory
    bandwidth utilization and network communication.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This paper is structured as follows. Section 2 will outline relevant background.
    In Section 3, we present our methodology, and in Section 4, we present our benchmark
    design, including datasets, DL models, and FL strategies. Section 5 contains experimental
    evaluations of our benchmark. In Section 6, we conclude our work.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance objectives in data center environments. One of the most important
    issues when training in a data center is to maximize throughput by trying to use
    the hardware to its limit without being blocked by communication. Communication
    concerns both local communication, i.e., memory movement, and communication between
    GPUs and nodes, typically with a high bandwidth interconnect such as NVLink (7.8 TB/s)
    and Ethernet (100 Gbit) [[10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring the effectiveness of each GPU in training is possible via Model FLOP
    Utilization (MFU) [[12](#bib.bib12)], which is the ratio of throughput achieved
    compared to the theoretical throughput of a model and a set of hardware. Common
    values for the MFU are between 5 – 20% ([Figure 2(b)](#S5.F2.sf2 "In Figure 2
    ‣ 5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad,
    the Ugly")) because DL models are not defined as a single matrix multiplication
    that can be perfectly parallelized between tensor cores but as many operations
    with memory bandwidth bottlenecks such as softmax, residual additions, and activations [[3](#bib.bib3)].
    These operations result in below-average FLOP usage, and each model architecture
    has its own set of operations that slow down throughput. However, the MFU can
    be used as a benchmark for how well a model is suited to work on a particular
    piece of hardware, as it fits the trade-offs between memory bandwidth, memory
    capacity, and FLOP. This way, we can compare the MFU for the same model on different
    hardware and contrast their results.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance objectives on the Edge. In edge computing systems that involve embedded
    devices, performance considerations differ from those in data center environments
    as use cases typically vary [[13](#bib.bib13)]. Yet, to run FL workloads on embedded
    devices on the edge, we need to unite performance characteristics from data centers
    and edge computing.
  prefs: []
  type: TYPE_NORMAL
- en: Running FL workloads on the edge is all about minimizing the time we use a client’s
    hardware and subsequently maximizing the throughput. Yet, the hardware is often
    located in remote areas with limited access to power or even a mobile device with
    very restrictive battery management [[14](#bib.bib14)]. Also, in remote and mobile
    environments, network bandwidth utilization and total network traffic are critical.
    Both have a significant impact on communication latency, i.e., how fast we can
    move model weights between clients and a server. For foundation models and LLMs,
    both can become a hurdle, as this kind of model tends to grow beyond several hundreds
    of millions of parameters in size or, in other words, beyond 1 GB in parameters
    to transfer over the network. Putting that into perspective with the average available
    wireless network bandwidth of 50 Mbit on mobile devices [[15](#bib.bib15)] yields
    communication times substantially longer than the actual computation time on clients
    [[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our hardware-centric study entails a combination of metrics from the data center
    / HPC and edge computing domains. Generally, when transferring LLM fine-tuning
    to the edge, we also transfer the challenges we currently have in data center
    environments into systems that suffer from more significant resource limitations.
    While energy efficiency is a specific challenge to edge computing systems, network
    bandwidth and computational efficiency are frequently discussed topics for DL
    applications in data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Energy efficiency. In edge computing systems, energy efficiency is a core system
    design variable. Also, on the edge, we often employ embedded devices that are
    based on system-on-a-chip design. This allows us to capture the total energy consumption
    of a client and each component on the chip (e.g., CPU, GPU). This is an easy-to-capture
    system metric that can help identify computational limits and find trade-offs
    for the highest energy efficiency. We define energy efficiency as the tokens per
    second ($\mathrm{TPS}$.
  prefs: []
  type: TYPE_NORMAL
- en: In data center settings, we can only capture the GPU power consumption as the
    system is often virtualized and does not have direct access to the actual hardware
    [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: Computational efficiency. Maximization of resource utilization is the superior
    objective for DL and FL applications in data center environments, as this is usually
    equivalent to a cost-optimal solution [[18](#bib.bib18)]. In the HPC domain, MFU
    is used to calculate the hardware resource utilization based on the number of
    theoretical hardware FLOP/s. By varying the minibatch size, the MFU can also be
    used to identify computational bottlenecks, i.e., whether we are computationally
    bound or memory bandwidth limited. In our experiments, the theoretical capacity
    of the NVIDIA A100 is 312 TFLOP, while the Jetson AGX Orin 64 GB provides 42.5 TFLOP
    or 13% of the A100.
  prefs: []
  type: TYPE_NORMAL
- en: Communication efficiency. Communication is equally important for federated LLM
    fine-tuning as computational efficiency. Typically, full models or partial model
    weights are being communicated between client and server [[19](#bib.bib19)]. Yet,
    communication on data center settings is built on top of high-performance networking
    infrastructure that enables bandwidths of 100 Gbit, and more [[10](#bib.bib10)].
    On the edge, we often find significantly slower network links with 1 Gbit and
    below. For instance, the global average for communication over 4G LTE wireless
    is 40 Mbit download and 15 Mbit upload [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, communication over LTE is much more expensive than in a data
    center via fiber. So, we need a reliable metric to quantify communication efficiency
    that, at the same time, tells us whether it is useful to further scale a FL workload
    over more clients or not. Borrowing from the HPC domain, Granularity ($G$) and
    to communicate the model gradients or weights ($T_{\mathrm{comm}}$.
  prefs: []
  type: TYPE_NORMAL
- en: In our FL scenario, the computation time is the maximum fine-tuning time on
    a client in each round, and the communication time is the time spent sending the
    model state, waiting, and receiving it from the server. In general, $G\gg 1$ adding
    more clients. Values of $G\ll 1$ indicate we spend too much time on communication
    rather than on computation, i.e., we do not achieve net speedup of the whole system
    by adding another client.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our hardware-centric study for FL on the edge focuses on evaluating state-of-the-art
    DL workloads on embedded devices. As such, we focus on current transformer models
    for NLP problems (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation hardware. In our hardware-centered study, we focus on the state of
    the art of deep learning accelerators for data centers and embedded computing.
    We employ a cloud VM with a single NVIDIA A100 80 GB (SXM4) as a data center node
    (A100) to perform our local baseline experiments. Further, we use a dedicated
    cluster consisting of ten NVIDIA Jetson AGX Orin 64 GB nodes (Orin) as the state-of-the-art
    embedded computing platform. The Orins are connected with a 10 Gbit synchronous
    network link and are monitored with 5 Hz for their power metrics. For our FL experiments,
    we use a GPU-accelerated VM that is co-located with the Orins to handle the model
    aggregation and testing of the global model.
  prefs: []
  type: TYPE_NORMAL
- en: DL models. For our experiments, we adopt the FLAN-T5 transformer model family
    [[4](#bib.bib4)] for conditional text generation. Specifically, we evaluate the
    computational training performance of the FLAN-T5-Small model with 80M parameters
    or 308 MB in size, the FLAN-T5-Base model with 250M parameters (990 MB), the FLAN-T5-Large
    model with 783M parameters (3.1 GB), and the FLAN-T5-XL model with 3B parameters
    (11.4 GB). For all models, we use the pre-trained FLAN-T5-Base tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset. All the FLAN-T5 models are fine-tuned on the Samsum dataset with the
    objective of summarizing texts with a maximum token length of 512 elements [[21](#bib.bib21)].
    The maximum model output length is 95 tokens, which can be translated into the
    summaries of the respective inputs. For our FL experiments, we choose a latent
    Dirichlet allocation as it is frequently used in related work [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)]. We use $\alpha=1$ to randomly split the input
    data into ten subsets that we distribute on the Orin compute cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate the feasibility of fine-tuning large transformer models on embedded
    devices and in potentially remote locations, we conduct extensive experiments.
    Specifically, we evaluate the hardware DL fine-tuning efficiency and the network
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f30e74394b70ce3b0104cb659a8b40e8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) $\eta_{e}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ccdb61f6ea97a5d60b0f39f8f016df0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) MFU in %
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: We study the energy efficiency ($\eta_{e}$, which is useful to evaluate
    root causes for poor training speeds. Additional metrics are available in Appendix
    [Appendix B](#A2 "Appendix B Results ‣ Federated Fine-Tuning of LLMs on the Very
    Edge: The Good, the Bad, the Ugly").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Orin platform is severely bottlenecked by memory bandwidth compared to
    the A100. We study $\eta_{e}$ until a minibatch size of 8 ([Figure 2(a)](#S5.F2.sf1
    "In Figure 2 ‣ 5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The
    Good, the Bad, the Ugly")). Afterwards, $\eta_{e}$ unveils an identical trend
    ([Figure 2(b)](#S5.F2.sf2 "In Figure 2 ‣ 5 Results ‣ Federated Fine-Tuning of
    LLMs on the Very Edge: The Good, the Bad, the Ugly")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A stagnating MFU as minibatch sizes increase means that increased parallel
    computation potential does not result in additional used FLOP. This can only happen
    if we encounter a memory bandwidth bottleneck. We see from [Figure 3](#S5.F3 "In
    5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad,
    the Ugly") that the Orin opt.step() function updating model weights and biases
    is taking up a significant amount of time in comparison to A100, which suggests
    that its performance is highly dependent on memory bandwidth. This should be prioritized
    for targeted optimization and profiled in more detail to find out which kernels
    are responsible for the slow performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Network performance vs. computational capabilities across different
    transport technologies measured by $G$. The computation times are normalized to
    100K tokens on each FLAN-T5 model and hardware type. LTE speeds are 40 Mbit down
    and 15 Mbit upload. 10 G is synonymous with a 10 Gbit synchronous network link.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Communication &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; time per FL round &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Computation per FL round &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (norm. to 100K tokens) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $G$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | LTE | 10 G |  |  | LTE | 10 G |'
  prefs: []
  type: TYPE_TB
- en: '| Model | Size |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Up &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Down &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Total | Total | A100 | Orin | A100 | Orin | A100 | Orin |'
  prefs: []
  type: TYPE_TB
- en: '| Small | 308 MB | 157s | 61s | 218s | 0.2s | 1s | 14s | 0.005 | 0.064 | 5.00
    | 70.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 990 MB | 504s | 197s | 701s | 0.8s | 4s | 39s | 0.006 | 0.056 | 5.00
    | 48.75 |'
  prefs: []
  type: TYPE_TB
- en: '| Large | 3,100 MB | 620s | 1,653s | 2,273s | 2.5s | 15s | 118s | 0.007 | 0.052
    | 6.00 | 47.20 |'
  prefs: []
  type: TYPE_TB
- en: 'Effective foundation model training on the edge requires sufficient communication
    bandwidth or significant compression. While communication plays a subordinate
    role in environments with high bandwidth availability (100 Gbit+), even for multi-million
    parameter models, it is accountable by a significant margin for the total time
    of an FL round ([Table 1](#S5.T1 "In 5 Results ‣ Federated Fine-Tuning of LLMs
    on the Very Edge: The Good, the Bad, the Ugly")) in a mobile environment. This
    is especially evident when looking at the granularity. While $G\gg 1$ for all
    FLAN-T5 transformers in FL systems with LTE connectivity. $G\approx 0$ is a strong
    indication of the inefficiency of distributed training of DL models on a given
    hardware type and under given network conditions. As such, when bringing transformer
    models to the edge onto embedded devices in wide-area networks, communication
    optimization is a critical variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75e9ac5ce2e62d443a97dc70c8dde944.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: DL training step times across FLAN-T5 transformer models with varying
    minibatch sizes on the Samsum dataset running on the NVIDIA A100 and Jetson AGX
    Orin platform. Detailed metrics are available in Appendix [Appendix B](#A2 "Appendix
    B Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad,
    the Ugly").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Increasing the minibatch size on embedded devices does not scale well. From
    our preceding insights, we know that we run into a memory bottleneck when fine-tuning
    FLAN-T5 on the edge. To further explore the source, we find linearly growing opt.step()
    times for all models as we scale the minibatch size on the Orins, while the step
    times on the A100 platform scale logarithmically with increasing batch size ([Figure 3](#S5.F3
    "In 5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the
    Bad, the Ugly")). From that, we derive the open research question: The root causes
    for the slowdown and what can be done to optimize it are interesting directions
    for future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our work, we have studied hardware performance optimization for fine-tuning
    LLMs and foundation models on the very edge with state-of-the-art embedded devices.
    We have shown the computational bottlenecks on embedded FL clients with an in-depth
    micro-benchmark and identified a strong correlation between $\eta_{e}$, a key
    system design variable in edge computing systems, and the MFU, a core performance
    indicator in the HPC domain. Furthermore, we have quantified the trade-off between
    communication and computation in FL systems with granularity, which shows the
    stark need to improve communication efficiency in FL systems to render FM training
    practical. With our work, we hope to raise awareness of the substantial challenges
    that need to be overcome to enable FM training on a broad basis for systems suffering
    from limited computational and network resources.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments and Disclosure of Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work is partially funded by the Bavarian Ministry of Economic Affairs,
    Regional Development and Energy (Grant: DIK0446/01) and the German Research Foundation
    (Grant: 392214008).'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
    Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
    et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv
    preprint arXiv:2203.15556, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.
    Data movement is all you need: A case study on optimizing transformers. Proceedings
    of Machine Learning and Systems, 3:711–732, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane
    Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros,
    Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
    Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi,
    Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.
    Scaling instruction-finetuned language models. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao
    Sun. FedBERT: When federated learning meets pre-training. ACM Transactions on
    Intelligent Systems and Technology, 13(4):1–26, August 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin
    Wang, and Yiran Chen. Towards building the federated gpt: Federated instruction
    tuning. arXiv preprint arXiv:2305.05644, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep
    nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
    Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale
    model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages
    551–564, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Amazon AWS p3 instance types. [https://aws.amazon.com/ec2/instance-types/p3/](https://aws.amazon.com/ec2/instance-types/p3/).
    Accessed: 2023-09-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu,
    Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences
    on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Blesson Varghese, Nan Wang, David Bermbach, Cheol-Ho Hong, Eyal de Lara,
    Weisong Shi, and Christopher Stewart. A survey on edge performance benchmarking.
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Samuel S. Ogden and Tian Guo. MODI: Mobile deep inference made efficient
    by edge computing. In USENIX Workshop on Hot Topics in Edge Computing (HotEdge
    18), Boston, MA, July 2018\. USENIX Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Martino Trevisan, Ali Safari Khatouni, and Danilo Giordano. Errant: Realistic
    emulation of radio access networks. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques,
    Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão,
    and Nicholas D. Lane. Flower: A friendly federated learning research framework,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Guillaume Fieni, Romain Rouvoy, and Lionel Seiturier. SelfWatts: On-the-fly
    selection of performance events to optimize software-defined power meters. In
    2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing
    (CCGrid). IEEE, May 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Nathan C. Frey, Baolin Li, Joseph McDonald, Dan Zhao, Michael Jones, David
    Bestor, Devesh Tiwari, Vijay Gadepally, and Siddharth Samsi. Benchmarking resource
    usage for efficient distributed deep learning, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
    Agüera y Arcas. Communication-efficient learning of deep networks from decentralized
    data. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Kai Hwang. Advanced Computer Architecture: Parallelism,Scalability,Programmability.
    McGraw-Hill Higher Education, 1st edition, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum
    corpus: A human-annotated dialogue dataset for abstractive summarization. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu,
    Harsha V. Madhyastha, and Mosharaf Chowdhury. FedScale: Benchmarking model and
    system performance of federated learning at scale. In International Conference
    on Machine Learning (ICML), 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
    Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang,
    Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr.
    Fedml: A research library and benchmark for federated machine learning. Advances
    in Neural Information Processing Systems, Best Paper Award at Federate Learning
    Workshop, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný,
    H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for
    federated settings, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 FL aggregation strategy & configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our experiments, we use Federated Averaging [[19](#bib.bib19)] as it provides
    a strong baseline for FL workloads. For every FL training round, we randomly sample
    three clients out of a total of ten clients and train one local epoch on each
    client with a minibatch size of 32\. After each round, we communicate the locally
    trained models, aggregate them on the server, and test the global model performance.
    This procedure is repeated for 20 FL training rounds. FL training is implemented
    with Flower [[16](#bib.bib16)]. However, the FL aggregation process is not the
    focus of our work.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This appendix section contains additional results on the energy efficiency measurements
    and micro-benchmark timings.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Energy efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Energy efficiency is derived from the average power draw of each device during
    our experiments. The experiments are fixed to 100 steps per epoch for each experiment.
    [Table 2](#A2.T2 "In B.1 Energy efficiency ‣ Appendix B Results ‣ Federated Fine-Tuning
    of LLMs on the Very Edge: The Good, the Bad, the Ugly") contains details on our
    energy efficiency calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Energy efficiency is measured in $\frac{\mathrm{TPS}}{W}$.'
  prefs: []
  type: TYPE_NORMAL
- en: FLAN-T5 Model Minib. Size Device Avg. Power Draw (W) $\eta_{e}$ TPS Small 1
    A100 75.8 21.16 1603.62 AGX Orin 16.7 55.26 923.04 8 A100 103.17 121.63 12548.15
    AGX Orin 28.1 200.75 5641.15 16 A100 120.96 215.03 26010.34 AGX Orin 35.8 198.32
    7099.35 32 A100 171.15 258.52 44246.31 AGX Orin 38.84 196.53 7633.56 64 A100 212.53
    304.82 64782.11 AGX Orin 38.82 203.57 7903.25 128 A100 247.72 327.2 81055.16 AGX
    Orin 41.08 195.73 8040.93 Base 1 A100 85.63 13.0 1113.37 AGX Orin 24.86 25.23
    627.45 8 A100 135.57 63.77 8645.35 AGX Orin 31.3 74.21 2322.81 16 A100 159.09
    94.55 15041.23 AGX Orin 38.59 66.51 2566.63 32 A100 223.55 99.28 22194.39 AGX
    Orin 38.54 69.84 2691.45 64 A100 260.68 100.08 26088.77 AGX Orin Out of memory
    Large 1 A100 91.61 6.06 554.97 AGX Orin 26.1 11.24 293.38 8 A100 173.43 24.24
    4204.11 AGX Orin 41.46 20.36 844.18 16 A100 196.9 33.76 6647.37 AGX Orin Out of
    memory XL 1 A100 128.21 4.31 552.0 AGX Orin Out of memory
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Model FLOP Utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MFU helps to identify computational or memory bottlenecks. [Table 3](#A2.T3
    "In B.2 Model FLOP Utilization ‣ Appendix B Results ‣ Federated Fine-Tuning of
    LLMs on the Very Edge: The Good, the Bad, the Ugly") depicts all details required
    to calculate the MFU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Details on MFU calculation for the NVIDIA A100 and Jetson AGX Orin
    platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'FLAN-T5 Model Minib. Size Device TPS Params # Layers $d_{\mathrm{model}}$ $n_{\mathrm{att_{h}eads}}$
    Seq. Len. MFU Small 1 A100 1657.0 0.0 8.0 512.0 1024.0 8.0 512.0 0.3 Orin AGX
    927.0 0.0 8.0 512.0 1024.0 8.0 512.0 1.1 8 A100 13051.0 0.0 8.0 512.0 1024.0 8.0
    512.0 2.0 Orin AGX 5665.0 0.0 8.0 512.0 1024.0 8.0 512.0 6.5 16 A100 26741.0 0.0
    8.0 512.0 1024.0 8.0 512.0 4.2 Orin AGX 7112.0 0.0 8.0 512.0 1024.0 8.0 512.0
    8.1 32 A100 45428.0 0.0 8.0 512.0 1024.0 8.0 512.0 7.1 Orin AGX 7713.0 0.0 8.0
    512.0 1024.0 8.0 512.0 8.8 64 A100 65944.0 0.0 8.0 512.0 1024.0 8.0 512.0 10.3
    Orin AGX 8040.0 0.0 8.0 512.0 1024.0 8.0 512.0 9.2 128 A100 82045.0 0.0 8.0 512.0
    1024.0 8.0 512.0 12.8 Orin AGX 8094.0 0.0 8.0 512.0 1024.0 8.0 512.0 9.3 Base
    1 A100 1134.0 0.0 12.0 768.0 2048.0 12.0 512.0 0.6 Orin AGX 631.0 0.0 12.0 768.0
    2048.0 12.0 512.0 2.3 8 A100 8805.0 0.0 12.0 768.0 2048.0 12.0 512.0 4.4 Orin
    AGX 2339.0 0.0 12.0 768.0 2048.0 12.0 512.0 8.5 16 A100 15380.0 0.0 12.0 768.0
    2048.0 12.0 512.0 7.6 Orin AGX 2591.0 0.0 12.0 768.0 2048.0 12.0 512.0 9.4 32
    A100 22427.0 0.0 12.0 768.0 2048.0 12.0 512.0 11.1 Orin AGX 2692.0 0.0 12.0 768.0
    2048.0 12.0 512.0 9.8 64 A100 26250.0 0.0 12.0 768.0 2048.0 12.0 512.0 13.0 AGX
    Orin Out of memory Large 1 A100 562.0 0.0 24.0 1024.0 2816.0 16.0 512.0 0.9 Orin
    AGX 298.0 0.0 24.0 1024.0 2816.0 16.0 512.0 3.4 8 A100 4260.0 0.0 24.0 1024.0
    2816.0 16.0 512.0 6.6 Orin AGX 853.0 0.0 24.0 1024.0 2816.0 16.0 512.0 9.7 16
    A100 6728.0 0.0 24.0 1024.0 2816.0 16.0 512.0 10.5 AGX Orin Out of memory XL 1
    A100 560.0 0.0 24.0 2048.0 5120.0 32.0 512.0 3.1 AGX Orin Out of memory'
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Micro-benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 4](#A2.T4 "In B.3 Micro-benchmark ‣ Appendix B Results ‣ Federated Fine-Tuning
    of LLMs on the Very Edge: The Good, the Bad, the Ugly") describes the step timings
    in detail and provides a perspective on the speed differences between data center
    and embedded hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Results of the micro-benchmark of the FLAN-T5 transformer model family
    on the NVIDIA A100 and Jetson AGX Orin platforms. The sequence length per batch
    item is 512.'
  prefs: []
  type: TYPE_NORMAL
- en: FLAN-T5 Model Batch Size Device Backward Opt. Step Loss Calc. Forward Batch
    Loading TPS Total Time Small 1 A100 0.09 0.16 0.0 0.06 0.01 1657.87 0.32 AGX Orin
    0.15 0.3 0.0 0.09 0.01 927.51 0.55 8 A100 0.09 0.16 0.0 0.06 0.01 13051.3 0.33
    AGX Orin 0.22 0.4 0.0 0.1 0.01 5665.54 0.73 16 A100 0.09 0.16 0.0 0.06 0.01 26741.79
    0.31 AGX Orin 0.36 0.63 0.0 0.15 0.01 7112.45 1.15 32 A100 0.12 0.18 0.0 0.06
    0.01 45428.27 0.37 AGX Orin 0.66 1.16 0.0 0.32 0.01 7713.02 2.15 64 A100 0.17
    0.26 0.0 0.06 0.01 65944.32 0.51 AGX Orin 1.28 2.22 0.0 0.64 0.01 7992.08 4.15
    128 A100 0.28 0.46 0.0 0.06 0.02 82045.0 0.81 AGX Orin 2.6 4.34 0.0 1.21 0.01
    8046.96 8.15 Base 1 A100 0.14 0.23 0.0 0.08 0.01 1134.51 0.46 AGX Orin 0.22 0.45
    0.0 0.14 0.01 631.08 0.82 8 A100 0.14 0.23 0.0 0.09 0.01 8805.38 0.47 AGX Orin
    0.51 0.95 0.0 0.3 0.01 2339.49 1.76 16 A100 0.17 0.27 0.0 0.09 0.02 15380.06 0.54
    AGX Orin 0.93 1.69 0.0 0.57 0.01 2590.44 3.19 32 A100 0.24 0.38 0.0 0.1 0.01 22427.21
    0.74 AGX Orin 1.81 3.19 0.0 1.09 0.01 2692.26 6.09 64 A100 0.4 0.65 0.0 0.19 0.01
    26250.25 1.26 AGX Orin Out of memory Large 1 A100 0.27 0.46 0.0 0.18 0.02 562.2
    0.92 AGX Orin 0.43 1.03 0.0 0.27 0.01 298.91 1.75 8 A100 0.29 0.49 0.0 0.18 0.02
    4260.38 0.97 AGX Orin 1.31 2.62 0.0 0.92 0.01 850.43 4.85 16 A100 0.4 0.62 0.0
    0.2 0.02 6728.79 1.23 AGX Orin Out of memory XL 1 A100 0.27 0.48 0.0 0.17 0.02
    560.07 0.93 AGX Orin Out of memory
  prefs: []
  type: TYPE_NORMAL
