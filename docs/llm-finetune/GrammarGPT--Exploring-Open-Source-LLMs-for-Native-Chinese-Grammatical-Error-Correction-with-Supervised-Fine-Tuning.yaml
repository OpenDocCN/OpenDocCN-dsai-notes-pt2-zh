- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.13923](https://ar5iv.labs.arxiv.org/html/2307.13923)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: School of Data Science, The Chinese University of Hong Kong,
    Shenzhen, China ²²institutetext: School of Computer Science and Technology, Soochow
    University, China ³³institutetext: Shenzhen Research Institute of Big Data, Shenzhen,
    Guangdong, China ⁴⁴institutetext: School of Information Science and Technology,
    University of Science and Technology of China, China'
  prefs: []
  type: TYPE_NORMAL
- en: '⁴⁴email: yxfansuda@stu.suda.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: pfli@suda.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '{jeffreyjiang,haizhouli}@cuhk.edu.cnYaxin Fan 112233    Feng Jiang Corresponding
    Author113344    Peifeng Li 22    Haizhou Li 1133'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Grammatical error correction aims to correct ungrammatical sentences automatically.
    Recently, some work has demonstrated the excellent capabilities of closed-source
    Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction. However,
    the potential of open-source LLMs remains unexplored. In this paper, we introduced
    GrammarGPT, an open-source LLM, to preliminary explore its potential for native
    Chinese grammatical error correction. The core recipe of GrammarGPT is to leverage
    the hybrid dataset of ChatGPT-generated and human-annotated. For grammatical errors
    with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical
    sentences by providing those clues. For grammatical errors without clues, we collected
    ungrammatical sentences from publicly available websites and manually corrected
    them. In addition, we employed an error-invariant augmentation method to enhance
    the ability of the model to correct native Chinese grammatical errors. We ultimately
    constructed about 1k parallel data and utilized these data to fine-tune open-source
    LLMs (e.g., Phoenix, released by The Chinese University of Hong Kong, Shenzhen)
    with instruction tuning. The experimental results show that GrammarGPT outperforms
    the existing SOTA system significantly. Although model parameters are 20x larger
    than the SOTA baseline, the required amount of data for instruction tuning is
    1200x smaller, illustrating the potential of open-source LLMs on native CGEC.
    Our GrammarGPT ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach’s
    effectiveness. The code and data are available at [https://github.com/FreedomIntelligence/GrammarGPT](https://github.com/FreedomIntelligence/GrammarGPT).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Native Chinese grammatical error correction Large language models ChatGPT Instruction
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Grammatical Error Correction (GEC) aims to automatically correct ungrammatical
    sentences without changing their meaning [[26](#bib.bib26), [10](#bib.bib10),
    [27](#bib.bib27)]. Previous works [[28](#bib.bib28), [13](#bib.bib13), [14](#bib.bib14),
    [26](#bib.bib26)] in Chinese Grammatical Error Correction (CGEC) mainly study
    the errors from foreign Chinese learners, which are very obvious and naive. Therefore,
    recent works  [[27](#bib.bib27), [10](#bib.bib10)] shift to the grammatical errors
    made by native speakers, which are more subtle and challenging. Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ GrammarGPT: Exploring Open-Source LLMs for Native
    Chinese Grammatical Error Correction with Supervised Fine-Tuning") shows the six
    main types of grammatical errors made by native speakers, which can be divided
    into two types, e.g., with (w/) and without (w/o) clues. We can find that the
    incorrect sentences are fluent and in line with the habits of native Chinese.
    However, they do not conform to Chinese grammar, which is more difficult to correct.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies in GEC mainly adopted both Seq2edit [[5](#bib.bib5), [26](#bib.bib26),
    [9](#bib.bib9), [10](#bib.bib10)] and Seq2seq [[7](#bib.bib7), [29](#bib.bib29),
    [15](#bib.bib15)] paradigms and have achieved impressive performance on various
    GEC benchmarks. With the emergence of LLMs, Fang et al. [[4](#bib.bib4)] evaluated
    the performance of closed-source LLMs (e.g., ChatGPT ¹¹1https://chat.openai.com/)
    on GEC and revealed its excellent capabilities for error detection and correction.
    However, the potential of open-source LLMs remains unexplored.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we introduce GrammarGPT, a novel model for studying the potential
    of open-source LLMs architectures in addressing Native Chinese Grammatical Error
    Correction (CGEC) through supervised fine-tuning. The key challenge in fine-tuning
    LLMs for CGEC is obtaining high-quality parallel data comprising grammatical errors
    made by native speakers. However, manually annotating such data is not only time-consuming
    but also expensive, necessitating the exploration of automatic data annotation
    methods. Recent works [[25](#bib.bib25), [22](#bib.bib22)] have successfully leveraged
    distilled data from ChatGPT and real-world datasets to fine-tune LLMs for specific
    domains, effectively reducing costs while achieving superior performance. Inspired
    by this line of research, we propose a hybrid dataset that incorporates different
    types of native Chinese grammatical errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we first proposed a heuristic method for the grammatical errors
    with clues as shown in Fig. [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT:
    Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with
    Supervised Fine-Tuning") that guides ChatGPT to generate ungrammatical sentences
    by providing those clues. Then, for those errors without clues, we collected the
    ungrammatical sentences from the public website and corrected them manually. In
    addition, we proposed an error-invariant data augmentation method to enhance the
    diversity of the data by substituting the named entities in parallel data with
    similar ones, which can improve the ability of the model to correct native Chinese
    grammatical errors. We ultimately constructed 1k parallel data and utilized these
    data to fine-tune LLMs with instruction tuning. The experimental results show
    that GrammarGPT can significantly outperform state-of-the-art (SOTA) systems.
    Although the size of model parameters is 20x larger than the SOTA baseline, the
    data for fine-tuning is 1200x smaller, which demonstrated the potential of open-source
    LLMs on Chinese grammatical error correction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Examples of sentences with various types of grammatical errors. For
    those errors with clues, we can easily detect and correct them. For example, the
    co-occurrence of *超过*(*more than*) and *左右* (*about*) lead to redundant component
    error and we can remove one of them to make the sentence conform to Chinese grammar.
    However, for those errors without clues, a deeper understanding of Chinese grammar
    is required to detect and correct.'
  prefs: []
  type: TYPE_NORMAL
- en: '| w/ Clues |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Redundant &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Component &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (RC) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect:这座卫星城的人口估计 超过一百万左右。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The population of this satellite city is estimated to be &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; more than  about one million. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct:这座卫星城的人口估计超过一百万。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The population of this satellite city is estimated to be &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; over one million. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Structural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Confusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (SC) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect:这次网络故障的原因是由服务器故障引起的。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The cause of this network failure is caused by the server failure. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct:这次网络故障的原因是服务器故障。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The cause of the network failure is the server failure. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Improper &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Collocation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (IC) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect:西湖区正全面 提升区域产城融合发展的 步伐。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Xihu District is promoting the pace of integration of regional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; industry and city development. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct: 西湖区正全面加快区域产城融合发展的步伐。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Xihu District is accelerating the pace of integration of regional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; industry and city development. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| w/o Clues |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Improper &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Word Order &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (IWO) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect:: 学校三个月内要求每名学生完成20个小时的义工服务。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The school in three months requires each student to complete &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 20 hours of volunteer service. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct:学校要求每名学生三个月内完成20个小时的义工服务。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The school requires each student to complete 20 hours of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; volunteer service in three months. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Improper &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Logicality &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (IL) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect:集团向社会各界人士、沿途村庄百姓表示歉意。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The group apologizes to people from all walks of life and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; villagers along the way. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct:集团向社会各界人士表示歉意。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The group apologizes to people from all walks of life. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Missing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Component &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (MC) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorrect:这篇报告控诉了人类破坏大自然(…)。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The report accused man of destroying nature. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct:这篇报告控诉了人类破坏大自然的罪行。 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The report accused man the crime of destroying nature. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, we are the first to explore the potential of open-source
    LLMs with instruction tuning for native Chinese grammatical error correction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have constructed a hybrid dataset generated by ChatGPT and manual annotation,
    which can effectively cover native Chinese grammatical errors for taming the LLMs
    into an excellent grammar detector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We designed an error-invariant data augmentation method to substitute the named
    entities in parallel data with similar ones, making the model more accurate in
    correcting grammatical errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The experimental results show that GrammarGPT can outperform the SOTA system
    significantly, and the data size for instruction tuning is only 1/1200 of the
    SOTA system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Grammatical Error Correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The works in grammatical error correction can be divided into two paradigms:
    the Seq2edit paradigm and the Seq2seq paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: Seq2edit paradigm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Seq2edit paradigm aims to predict the modification label, including insertion,
    deletion, and substitution, for each position of the sentence iteratively. Hinson
    et al. [[5](#bib.bib5)] proposed a heterogeneous approach to CGEC, composed of
    a NMT-based model, a sequence editing model, and a spell checker. Liang et al.
    [[9](#bib.bib9)] introduced and transferred the BERT-fused NMT model and sequence
    tagging model into the CGEC field. Zhang et al. [[26](#bib.bib26)] proposed a
    multi-reference multi-source evaluation dataset for CGEC and adopted the seq2edit
    method that enhanced with large pre-trained language models. Ma et al. [[10](#bib.bib10)]
    propose a linguistic rules-based approach to construct large-scale CGEC training
    corpora with automatically generated grammatical errors and adopt the seq2edit
    method for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq paradigm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This paradigm treats CGEC as a monolingual translation task. Katsumata and Komachi
    [[7](#bib.bib7)] explored the utility of bidirectional and auto-regressive transformers
    (BART) as a generic pre-trained encoder-decoder model for GEC. Zhao and Wang [[29](#bib.bib29)]
    proposed a simple yet effective method to improve the NMT-based GEC models by
    dynamic masking, which can generate more diverse instances to enhance model generalization.
    Rothe et al. [[15](#bib.bib15)] proposed a language-agnostic method to generate
    a large number of synthetic examples, and then fine-tune large-scale multilingual
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, several works [[9](#bib.bib9), [5](#bib.bib5), [8](#bib.bib8),
    [26](#bib.bib26)] observe the complementary power of the above two paradigms,
    thus promoting the performance through the model ensemble. In this paper, we adopt
    the Se2seq paradigm to fine-tune LLMs with instruction tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Instruction Tuning for LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instruction tuning [[21](#bib.bib21), [16](#bib.bib16)] can improve the ability
    of model generalization by learning from a large number of tasks guided by instruction,
    which has been successfully applied to fine-tune LLMs on some specific tasks.
    The work on task-specific instruction tuning can be categorized into three types
    by data sources: ChatGPT-generated, human-annotated, and hybrid dataset of ChatGPT
    and human.'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT-generated data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Several works adopted the data generated by ChatGPT to fine-tune LLMs in the
    form of instructions. Ho et al. [[6](#bib.bib6)] proposed Fine-tune-CoT, a method
    that generates reasoning samples from LLMS to fine-tune smaller models, which
    enables substantial reasoning capability of small models. Wang et al. [[19](#bib.bib19)]
    proposed SCOTT, a faithful knowledge distillation method to learn a small, self-consistent
    CoT model from a teacher model that is orders of magnitude. Chen et al. [[1](#bib.bib1)]
    explored distilling the reasoning ability of LLMs into a more compact student
    model for multimodal named entity and multimodal relation extraction. Chen et
    al. [[1](#bib.bib1)] proposed a data synthesis framework built upon the data generation
    functions parameterized by LLMs and prompts and used synthesized data to fine-tune
    LLaMA.
  prefs: []
  type: TYPE_NORMAL
- en: Human-annotated data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some works directly convert the supervised data into the format of instructions
    to fine-tune LLMs. Zhang et al. [[24](#bib.bib24)] proposed to fine-tune LLaMA
    [[18](#bib.bib18)] on financial sentiment analysis with a small portion of supervised
    financial sentiment analysis data. Wang et al. [[20](#bib.bib20)] proposed a unified
    information extraction framework based on instruction tuning to model various
    information extraction tasks and capture the inter-task dependency.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid dataset of ChatGPT and human
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recently, some works utilized the hybrid data of humans and ChatGPT/GPT-4 to
    fine-tune LLMs. Zhang et al. [[25](#bib.bib25)] proposed to leverage both distilled
    data from ChatGPT and real-world data from doctors to fine-tune Bloom [[17](#bib.bib17)].
    Yu et al. [[22](#bib.bib22)] adopted a hybrid data of Chinese education and general-domain
    instructions [[12](#bib.bib12)] generated by GPT-4 to fine-tune LLaMA [[18](#bib.bib18)].
    In this paper, we follow this line and fine-tune LLMs on native CGEC with the
    hybrid dataset of ChatGPT-generated and human-annotated with instruction tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/511c4823d6895e17ef1f1d1f57551d5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The framework of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [1](#S2.F1 "Figure 1 ‣ Hybrid dataset of ChatGPT and human ‣ 2.2 Instruction
    Tuning for LLMs ‣ 2 Related Work ‣ GrammarGPT: Exploring Open-Source LLMs for
    Native Chinese Grammatical Error Correction with Supervised Fine-Tuning") illustrates
    the framework of our method, which involves the construction of parallel data
    comprising six types of native Chinese grammatical errors to facilitate the fine-tuning
    of open-source Language Model (LLMs). While human-annotated data offer high-quality
    samples, the associated high cost remains a significant concern. To address this,
    we adopt a compromise approach. We first guide ChatGPT to generate ungrammatical
    sentences with clues by providing those clues collected from the Internet. Then,
    we annotate the ungrammatical sentences without clues collected from the Internet.
    Additionally, we propose an error-invariant augmentation technique to substitute
    named entities in the parallel data with similar ones, further enhancing the model’s
    capability to correct native Chinese grammatical errors. Finally, we convert the
    parallel data into instructions, which are then utilized for fine-tuning LLMs.
    Detailed explanations of these steps are provided in the following subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Hybrid Dataset Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 ChatGPT-generated Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in the first three lines of Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning"), the grammatical errors with clues are
    easy to detect and correct by recognizing the specific clues. For example, *”more
    than”* and *”about”* are used together leading to redundant component, *”The cause”*
    and *”caused by”* are used together leading to structural confusion, and *”prompting”*
    and *”pace”* are used together leading to improper collocation. Conversely, we
    can construct the ungrammatical sentences by inserting these cues into grammatical
    sentences. Thanks to the strong capabilities of ChatGPT, we can instruct ChatGPT
    to generate the ungrammatical sentences that meet our requirements by providing
    these clues collected from public websites ²²2https://wenku.baidu.com. An example
    is as shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1.2 Human-annotated Data ‣ 3.1 Hybrid
    Dataset Construction ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source LLMs for
    Native Chinese Grammatical Error Correction with Supervised Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Human-annotated Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some types of native ungrammatical errors are hard to recognize, as shown in
    the last three lines of Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT:
    Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with
    Supervised Fine-Tuning"). We can find that those ungrammatical sentences are fluent
    and with no obvious clues of grammatical errors can help us to recognize them.
    For these types of grammatical errors, we mainly collected ungrammatical sentences
    from publicly available websites³³3https://tiku.baidu.com/ and then manually annotated
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/906f19d8db823dba2f721b356e21dd08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Process of ungrammatical sentences generated by ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed4a5a7c9bfc9ae718d85140563510f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An example of error-invariant augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Error-invariant Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prioritize the model’s focus on native grammar errors and improve its robustness,
    we have devised an error-invariant augmentation method, as shown in Fig. [3](#S3.F3
    "Figure 3 ‣ 3.1.2 Human-annotated Data ‣ 3.1 Hybrid Dataset Construction ‣ 3 Methods
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning"). Native Chinese grammatical errors are
    often subtle and infrequently found in the position of named entities. To address
    this, we adopt a strategy of substituting the named entities in the parallel data
    with similar ones⁴⁴4https://github.com/chatopera/Synonyms. By employing this augmentation
    method, the model can concentrate on identifying unchanged errors rather than
    specific nouns, thereby improving its performance in correcting subtle and imperceptible
    grammar errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Instruction Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instruction tuning[[21](#bib.bib21), [16](#bib.bib16)] has emerged as the mainstream
    approach for fine-tuning LLMs by providing explicit instructions to enhance model
    comprehension. In this paper, we followed this mainstream trend and fine-tuned
    LLMs with instruction tuning. Instruction details are as shown in Table [2](#S3.T2
    "Table 2 ‣ 3.3 Instruction Tuning ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source
    LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning"),
    which mainly consists of four components.'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Task prefix: This component guides LLMs to assume the role of an AI assistant.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Task description: Here, the specific task that LLMs are required to accomplish
    is outlined.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Input: This corresponds to ungrammatical sentences that are used as input
    during the fine-tuning process.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Output: This represents grammatical sentences, which serve as the expected
    output during fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Components of an instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Instruction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; {Task Prefix} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Human:{Task Description} {Input} Assistant :{Output} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Prefix |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A chat between a curious human and an artificial &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; intelligence assistant. The assistant gives helpful, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detailed, and polite answers to the human’s questions. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Description | Evaluate this sentence for grammar mistake |'
  prefs: []
  type: TYPE_TB
- en: '| Input | *Ungrammatical sentence* |'
  prefs: []
  type: TYPE_TB
- en: '| Output | *Grammatical sentence* |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Statistic of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Number | Percentage of Different Grammatical Errors (%) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-generated | Human-annotated |'
  prefs: []
  type: TYPE_TB
- en: '|  RC |  SC |  IC | IWO |  IL | MC |'
  prefs: []
  type: TYPE_TB
- en: '| training set | 1061 |  23.54 |  28.25 |  13.70 | 6.50 |  13.18 | 15.07 |'
  prefs: []
  type: TYPE_TB
- en: '| validating set | 500 |  - |  - |  - |  - |  - |  - |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We constructed a total of 1061 parallel data samples for training, and the
    data statistics are provided in Table [3](#S3.T3 "Table 3 ‣ 3.3 Instruction Tuning
    ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical
    Error Correction with Supervised Fine-Tuning"). Roughly 35% of the data were manually
    annotated, while the remaining 65% were generated using ChatGPT. To evaluate the
    performance of our model, we utilized the validating set available on the NLPCC2023
    SharedTask1 website⁵⁵5https://github.com/masr2000/NaCGEC, which consists of 500
    parallel data samples. We report the model’s performance on this validating set
    for all the experiments conducted.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evaluation of a grammatical error correction system relies on the extent
    to which its proposed corrections or edits align with the gold-standard edits
    [[11](#bib.bib11)]. In line with previous research [[10](#bib.bib10), [26](#bib.bib26)],
    we adopt the word-level and char-level MaxMatch (M2) Scorer [[3](#bib.bib3)] for
    evaluation⁶⁶6https://github.com/HillZhang1999/MuCGEC/tree/main/scorers/ChERRANT.
    This scorer computes Precision, Recall, and F[0.5] scores, comparing the gold
    edit set with the system edit set.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Hyper-parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The models are implemented in PyTorch using the Huggingface Transformers⁷⁷7https://huggingface.co/.
    We used phoenix-inst-chat-7b ⁸⁸8https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b
    [[2](#bib.bib2)] as the backbone. We set the max sequence length to 256\. The
    model is trained with the AdamW optimizer, where the batch size and epoch are
    set to 64 and 3, respectively. We set the learning rate and the schedule type
    of learning rate to 2e-5 and ’linear’, respectively. The warmup step is set to
    5\. The hyper-parameters are shown in Table [4](#S4.T4 "Table 4 ‣ 4.3 Hyper-parameters
    ‣ 4 Experiments ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical
    Error Correction with Supervised Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Details of hyper-parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Backbone | phoenix-inst-chat-7b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Max length | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| Epoch | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning rate | 2e-5 |'
  prefs: []
  type: TYPE_TB
- en: '| Lr schedule type | Linear |'
  prefs: []
  type: TYPE_TB
- en: '| Warmup steps | 5 |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To validate the effectiveness of our method, we conducted a comparison between
    our GrammarGPT and the state-of-the-art (SOTA) baseline, S2S_BART [[26](#bib.bib26)].
    S2S_BART utilizes Chinese BART as the pre-trained model and fine-tunes it on the
    Lang8 [[28](#bib.bib28)] and HSK [[23](#bib.bib23)] datasets, which consist of
    approximately 1.2 million parallel data samples. We also fine-tuned S2S_BART on
    the hybrid dataset that we constructed, and the results are presented in Table [5](#S4.T5
    "Table 5 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ GrammarGPT: Exploring Open-Source
    LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: Remarkably, we observed that S2S_BART trained on our 1k hybrid dataset achieved
    17.57 and 18.16 $F_{0.5}$ on Word-level and Char-level separately, which is comparable
    to that baseline model using the 1.2M data from foreign language speakers. We
    attribute this to the significant discrepancy between the grammatical errors made
    by foreign language speakers and native Chinese speakers, making it challenging
    to effectively improve the performance of native CGEC by relying solely on data
    from foreign language speakers. These results further highlight the effectiveness
    of our method in constructing a hybrid dataset that contains native Chinese grammatical
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, our GrammarGPT exhibited substantial improvement with only about
    1k data samples for fine-tuning, achieving 32.56 and 35.84 $F_{0.5}$ ⁹⁹9https://github.com/masr2000/NaCGEC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Performance comparison between GrammarGPT and the SOTA baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | #Param. | Data | Data size | Word-level | Char-level |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prec | Rec | F[0.5] | Prec | Rec | F[0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S2S_BART | 375M |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Lang8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1.2M | 22.31 | 10.14 | 17.99 | 22.13 | 9.66 | 17.59 |'
  prefs: []
  type: TYPE_TB
- en: '| S2S_BART | 375M | Ours | 1061 | 21.08 | 10.54 | 17.57 | 22.09 | 10.62 | 18.16
    |'
  prefs: []
  type: TYPE_TB
- en: '| GrammarGPT | 7B | Ours | 1061 | 42.42 | 16.87 | 32.56 | 46.67 | 18.58 | 35.84
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Ablation study of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data | Word-level | Char-level |'
  prefs: []
  type: TYPE_TB
- en: '| Prec | Rec | F[0.5] | Prec | Rec | F[0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Augmentation | Human-annotated | 12.20 | 1.51 | 5.04 | 13.89 | 1.48 |
    5.19 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-generated | 30.38 | 7.21 | 18.49 | 30.86 | 7.35 | 18.83 |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid dataset | 41.76 | 11.45 | 27.30 | 44.32 | 11.50 | 28.22 |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Augmentation | Human-annotated | 15.46 | 4.52 | 10.42 | 16.48 | 4.44 |
    10.68 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-generated | 43.75 | 6.33 | 20.04 | 44.90 | 6.49 | 20.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid dataset | 42.42 | 16.87 | 32.56 | 46.87 | 18.58 | 35.84 |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our analysis of the impact of our contributions, namely the construction
    of a hybrid dataset and the error-invariant augmentation method, we present the
    results in Table [6](#S4.T6 "Table 6 ‣ 4.4 Experimental Results ‣ 4 Experiments
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: Notably, the model trained on ChatGPT-generated data consistently outperforms
    that trained the human-annotated data, irrespective of whether data augmentation
    is applied. We attribute this observation to two primary reasons. First, the quantity
    of human-annotated data is smaller than the data generated by ChatGPT due to the
    high cost of human annotation. Second, grammatical errors without clues are more
    challenging to correct.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, our hybrid dataset demonstrates the potential for enhancing the
    performance of native CGEC. This finding substantiates the effectiveness of our
    approach in constructing the hybrid dataset consisting of native Chinese grammatical
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by employing the error-invariant augmentation method, we observe our
    model trained on hybrid dataset has significant improvements in Recall and F[0.5]
    metrics but only minor improvements in Precision. It indicates that our augmentation
    technique enhances the model’s ability to detect grammatical errors by forcing
    the model to pay more attention to grammar errors in the augmentation data.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce GrammarGPT, an open-source Large Language Model
    (LLM) specifically designed for native Chinese grammatical error correction. We
    first construct a hybrid dataset containing approximately 1k parallel data samples.
    It comprises both ChatGPT-generated data and human-annotated data for dealing
    with grammatical errors with and without clues. Additionally, we introduced an
    error-invariant augmentation method to improve the model’s capabilities in native
    Chinese grammatical error correction by forcing the model to pay more attention
    to grammar errors in the augmentation data. We further fine-tune the open-source
    large-scale language model on the constructed dataset. Experimental results and
    in-depth analysis demonstrate the effectiveness of our GrammarGPT in native Chinese
    grammatical error correction.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by the National Natural Science Foundation of China (Grant
    No. 62271432) and the Guangdong Provincial Key Laboratory of Big Data Computing,
    The Chinese University of Hong Kong, Shenzhen (Grant No. B10120210117).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Chen, F., Feng, Y.: Chain-of-Thought Prompt Distillation for Multimodal
    Named Entity and Multimodal Relation Extraction. ArXiv preprint arXiv:2306.14122
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Chen, Z., Jiang, F., Chen, J., Wang, T., Yu, F., Chen, G., Zhang, H., Liang,
    J., Zhang, C., Zhang, Z., et al.: Phoenix: Democratizing ChatGPT across languages.
    arXiv preprint arXiv:2304.10453 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Dahlmeier, D., Ng, H.T.: Better Evaluation for Grammatical Error Correction.
    In: Proceedings of the 2012 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies. pp. 568–572\. Association
    for Computational Linguistics, Montréal, Canada (Jun 2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Fang, T., Yang, S., Lan, K., Wong, D.F., Hu, J., Chao, L.S., Zhang, Y.:
    Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive
    Evaluation. arXiv preprint arXiv:2304.01746 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Hinson, C., Huang, H.H., Chen, H.H.: Heterogeneous Recycle Generation for
    Chinese Grammatical Error Correction. In: Proceedings of the 28th International
    Conference on Computational Linguistics. pp. 2191–2201 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Ho, N., Schmid, L., Yun, S.Y.: Large Language Models Are Reasoning Teachers.
    arXiv preprint arXiv:2212.10071 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Katsumata, S., Komachi, M.: Stronger Baselines for Grammatical Error Correction
    Using a Pretrained Encoder-Decoder Model. In: Proceedings of the 1st Conference
    of the Asia-Pacific Chapter of the Association for Computational Linguistics and
    the 10th International Joint Conference on Natural Language Processing. pp. 827–832
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Li, J., Guo, J., Zhu, Y., Sheng, X., Jiang, D., Ren, B., Xu, L.: Sequence-to-Action:
    Grammatical Error Correction with Action Guided Sequence Generation. Proceedings
    of the AAAI Conference on Artificial Intelligence 36(10), 10974–10982 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Liang, D., Zheng, C., Guo, L., Cui, X., Xiong, X., Rong, H., Dong, J.:
    BERT Enhanced Neural Machine Translation and Sequence Tagging Model for Chinese
    Grammatical Error Diagnosis. In: Proceedings of the 6th Workshop on Natural Language
    Processing Techniques for Educational Applications. pp. 57–66. Association for
    Computational Linguistics (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Ma, S., Li, Y., Sun, R., Zhou, Q., Huang, S., Zhang, D., Yangning, L.,
    Liu, R., Li, Z., Cao, Y., Zheng, H., Shen, Y.: Linguistic Rules-Based Corpus Generation
    for Native Chinese Grammatical Error Correction. In: Findings of the Association
    for Computational Linguistics: EMNLP 2022\. pp. 576–589 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ng, H.T., Wu, S.M., Briscoe, T., Hadiwinoto, C., Susanto, R.H., Bryant,
    C.: The CoNLL-2014 Shared Task on Grammatical Error Correction. In: Proceedings
    of the Eighteenth Conference on Computational Natural Language Learning: Shared
    Task. pp. 1–14 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction Tuning with
    GPT-4. arXiv preprint arXiv:2304.03277 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Rao, G., Gong, Q., Zhang, B., Xun, E.: Overview of NLPTEA-2018 Share Task
    Chinese Grammatical Error Diagnosis. In: Proceedings of the 5th Workshop on Natural
    Language Processing Techniques for Educational Applications. pp. 42–51 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Rao, G., Yang, E., Zhang, B.: Overview of NLPTEA-2020 Shared Task for
    Chinese grammatical error diagnosis. In: Proceedings of the 6th Workshop on Natural
    Language Processing Techniques for Educational Applications. pp. 25–35 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Rothe, S., Mallinson, J., Malmi, E., Krause, S., Severyn, A.: A Simple
    Recipe for Multilingual Grammatical Error Correction. In: Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 2: Short Papers). pp.
    702–707 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A., et al.: Multitask Prompted
    Training Enables Zero-shot Task Generalization. arXiv preprint arXiv:2110.08207
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné,
    R., Luccioni, A.S., Yvon, F., Gallé, M., et al.: Bloom: A 176B-parameter Open-access
    Multilingual Language Model. arXiv preprint arXiv:2211.05100 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., Ren, X.: SCOTT: Self-Consistent
    Chain-of-Thought Distillation. arXiv preprint arXiv:2305.01879 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Wang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., Zheng, R., Ye,
    J., Zhang, Q., Gui, T., et al.: InstructUIE: Multi-task Instruction Tuning for
    Unified Information Extraction. arXiv preprint arXiv:2304.08085 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N.,
    Dai, A.M., Le, Q.V.: Finetuned Language Models Are Zero-shot Learners. arXiv preprint
    arXiv:2109.01652 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Yu, J., Zhu, J., Wang, Y., Liu, Y., Chang, H., Nie, J., Kong, C., Cong,
    R., XinLiu, An, J., Lu, L., Fang, M., Zhu, L.: Taoli LLaMA. [https://github.com/blcuicall/taoli](https://github.com/blcuicall/taoli)
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Zhang, B.: Features and Functions of the HSK Dynamic Composition Corpus.
    International Chinese Language Education 4, 71–79 (2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Zhang, B., Yang, H., Liu, X.Y.: Instruct-FinGPT: Financial Sentiment Analysis
    by Instruction Tuning of General-Purpose Large Language Models. arXiv preprint
    arXiv:2306.12659 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Zhang, H., Chen, J., Jiang, F., Yu, F., Chen, Z., Li, J., Chen, G., Wu,
    X., Zhang, Z., Xiao, Q., et al.: HuatuoGPT, towards Taming Language Model to Be
    a Doctor. arXiv preprint arXiv:2305.15075 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Zhang, Y., Li, Z., Bao, Z., Li, J., Zhang, B., Li, C., Huang, F., Zhang,
    M.: MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical
    Error Correction. In: Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies.
    pp. 3118–3130 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Zhang, Y., Zhang, B., Jiang, H., Li, Z., Li, C., Huang, F., Zhang, M.:
    NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native
    Speaker Texts. arXiv preprint arXiv:2305.16023 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Zhao, Y., Jiang, N., Sun, W., Wan, X.: Overview of the NLPCC 2018 Shared
    Task: Grammatical Error Correction. In: Natural Language Processing and Chinese
    Computing: 7th CCF International Conference, NLPCC 2018, Hohhot, China, August
    26–30, 2018, Proceedings, Part II 7\. pp. 439–445\. Springer (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Zhao, Z., Wang, H.: MaskGEC: Improving Neural Grammatical Error Correction
    via Dynamic Masking. Proceedings of the AAAI Conference on Artificial Intelligence
    34(01), 1226–1233 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
