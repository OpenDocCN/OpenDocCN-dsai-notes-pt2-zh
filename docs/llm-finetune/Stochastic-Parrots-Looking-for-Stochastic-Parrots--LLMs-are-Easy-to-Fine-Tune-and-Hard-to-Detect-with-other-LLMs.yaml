- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.08968](https://ar5iv.labs.arxiv.org/html/2304.08968)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: IC School EPFL, Switzerland'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: {firstname.name}@epfl.ch ²²institutetext: Now at HES-SO Valais-Wallis,
    Switzerland'
  prefs: []
  type: TYPE_NORMAL
- en: '²²email: {firstname.name}@hevs.ch'
  prefs: []
  type: TYPE_NORMAL
- en: Da Silva Gameiro Henrique 11    Andrei Kucharavy Contact author11 2 2    Rachid
    Guerraoui 11
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The self-attention revolution allowed generative language models to scale and
    achieve increasingly impressive abilities. Such models - commonly referred to
    as Large Language Models (LLMs) - have recently gained prominence with the general
    public, thanks to conversational fine-tuning, putting their behavior in line with
    public expectations regarding AI. This prominence amplified prior concerns regarding
    the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in
    the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, most such tools are critically flawed. While major publications
    in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned
    autoencoders, the limitations of their results are easy to overlook. Specifically,
    they assumed publicly available generative models without fine-tunes or non-trivial
    prompts. While the importance of these assumptions has been demonstrated, until
    now, it remained unclear how well such detection could be countered.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we show that an attacker with access to such detectors’ reference human
    texts and output not only evades detection but can fully frustrate the detector
    training - with a reasonable budget and all its outputs labeled as such. Achieving
    it required combining common "reinforcement from critic" loss function modification
    and AdamW optimizer, which led to surprisingly good fine-tuning generalization.
    Finally, we warn against the temptation to transpose the conclusions obtained
    in RNN-driven text GANs to LLMs due to their better representative ability.
  prefs: []
  type: TYPE_NORMAL
- en: These results have critical implications for the detection and prevention of
    malicious use of generative language models, and we hope they will aid the designers
    of generative models and detectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models NLP Generative ML Generative ML Text GANs
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ever since the introduction of the Transformer modular self-attention architecture
    [[62](#bib.bib62)], the training parallelization this architecture allowed led
    to a continuous scaling of models - be it in the number of parameters, computational
    resources invested into training them, and training datasets beyond what was previously
    imaginable [[53](#bib.bib53)]. Earning them the name of Large Language Models
    - LLMs, such architectures are ubiquitous in modern NLP ML. Be it GPT family optimized
    for the text generation [[50](#bib.bib50), [51](#bib.bib51), [11](#bib.bib11),
    [44](#bib.bib44)], BERT/RoBERTa optimized for gap-filling and classification [[16](#bib.bib16),
    [36](#bib.bib36)], or T5 [[52](#bib.bib52)] optimized for translation-like generation,
    LLMs successfully scaled across more than four orders of magnitude in parameter
    and training dataset size, all while continuously improving in their primary task.
  prefs: []
  type: TYPE_NORMAL
- en: Unexpectedly, in addition to continuously improving in the task they were trained
    for, LLMs also underwent a qualitative transition in their capabilities, unlocking
    unexpected and seemingly unrelated capabilities. Learning basic arithmetic [[11](#bib.bib11)];
    generating valid Python programs from natural language requirements [[2](#bib.bib2)];
    predicting recidivism better than specialized models and generating unprompted
    hate speech [[20](#bib.bib20)]; - all were achieved without any further model
    modification, through prompt choice alone.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most impressive achievement of LLMs was their ability to generate
    natural language so well that some LLMs could not be distinguished from humans
    in most settings. While GPT-2 was already becoming good at this [[30](#bib.bib30)],
    it was GPT-3’s ability to run popular wellness advice blogs [[25](#bib.bib25)]
    and write newspaper articles about itself [[24](#bib.bib24)] that led to wide-reaching
    concerns about the detectability of LLM outputs. The situation got only worse
    with the release into open access of conversationally fine-tuned LLMs [[45](#bib.bib45)],
    making LLMs accessible and easy to use to the general public, and with the augmentation
    of LLMs with auxiliary capabilities, such as information retrieval, calculator
    and image analysis [[57](#bib.bib57), [61](#bib.bib61), [44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: Such performance is impressive, but it poses a serious threat. Generative models
    have no inherent moral code, and safety features added by LLM designers to imitate
    it are easily bypassed. LLMs can trivially be used to run blogs and social media
    accounts and write articles pushing disinformation, supporting harassment campaigns,
    or promoting self-harm. Undercover social influence and harassment with economical
    [[38](#bib.bib38)], political [[27](#bib.bib27)], and military goals [[15](#bib.bib15)]
    is a well-developed and highly active industry [[9](#bib.bib9), [3](#bib.bib3)].
    Everything points towards generative language models being able drive an automation
    revolution in it [[12](#bib.bib12), [60](#bib.bib60)]. Even relatively small,
    widely accessible models that can be run on commodity hardware can be extremely
    effective in generating misleading news articles [[66](#bib.bib66), [33](#bib.bib33)],
    writing fake reviews [[1](#bib.bib1)], phishing [[43](#bib.bib43)], or disrupting
    of democratic processes [[63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the best intentions of their original authors, generative language models
    pose a major threat in case of misuse. It is essential to be able to detect them
    in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Generated Text Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior research into generative language model detection is extensive and predates
    LLMs [[28](#bib.bib28)]. Model detection concern was so great for the creators
    of GPT-2 model that they provided a baseline detection tool for their model [[58](#bib.bib58)].
    Creators of GROVER - an LLM specializing in news-like text generation - claimed
    that it also could detect generative models at large [[66](#bib.bib66)]. Such
    detection tools were deemed highly necessary, given that even for relatively small
    LLMs, humans were shown to be inadequate at detecting them [[30](#bib.bib30)].
    Several high-profile benchmarking studies found that fine-tuned classifier LLMs
    performed best[[58](#bib.bib58), [66](#bib.bib66), [30](#bib.bib30), [60](#bib.bib60),
    [41](#bib.bib41), [1](#bib.bib1), [17](#bib.bib17), [18](#bib.bib18)]. Such detectors
    were trained by feeding a pretrained LLM, often a BERT/RoBERTa, with examples
    of machine-generated and human-generated texts, hoping that the language representation
    learned during the pretraining will allow the LLM to learn robust features that
    are specific to either of classes.
  prefs: []
  type: TYPE_NORMAL
- en: However, most work on generative model detection did not consider the possibility
    that attackers would fine-tune their generative models to evade detection or prompt
    them in a non-trivial manner. A flurry of research papers moderating the initial
    optimistic results followed. A model fine-tuned for unrelated purposes was shown
    to evade detectors trained on the base model [[1](#bib.bib1)]. Similar results
    could be achieved by using longer prompts [[4](#bib.bib4)], changing the sampling
    strategy [[30](#bib.bib30), [60](#bib.bib60)], or adversarially perturbing characters
    or words in the prompt [[19](#bib.bib19), [64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: Even in the white-box setting - when the generative model is identified, and
    its outputs are correctly labeled - SotA methods struggle to detect them. [[1](#bib.bib1)]
    showed that in cases where fine-tuned models were known in advance, a BERT-based
    detector could be trained to detect them, although with low precision and recall
    ( 40% for both). [[18](#bib.bib18)] found that even for Twitter bots based on
    generative models that do not attempt evasion, even fine-tuned detectors struggled
    to differentiate GPT2-based ones from humans ( 70% accuracy). Perhaps more concerning,
    the memorization capabilities of generative LLMs mean that well-designed prompts
    can trigger perfect recall of training data - which is human-generated text [[14](#bib.bib14)].
    A detector would need access to the whole training dataset of the generative model
    to succeed in that setting, which is an unrealistic assumption, especially in
    an adversarial detection setting.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Detection and Evasion in Text GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common framework to approach an arms race of detection and detection evasion
    in ML is Generative Adversarial Networks (GANs) [[23](#bib.bib23)]. They are notorious
    for excellent capabilities in image generation, with photorealism and style transfer
    often cited as examples of performance [[10](#bib.bib10), [31](#bib.bib31)]. For
    natural language generation, GANs have been investigated as ways to improve pretrained
    models, to further improve models trained through maximum likelihood methods -
    specifically mitigate the exposure bias. Exposure bias is a tendency of language
    models to generate a succession of tokens they never encountered in training and
    find themselves without a statistical model to continue generation, leading to
    repetitive, degenerate output [[29](#bib.bib29), [26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, despite a wealth of proposed architectures, none of the text-generating
    GANs imitating image-generating ones offered any improvement over MLE [[13](#bib.bib13)].
    While several modern GANs have been proposed to mitigate that, such as ScratchGAN
    [[42](#bib.bib42)] or ColdGAN [[34](#bib.bib34)], they depart significantly from
    the traditional adversarial setting and require a collaboration between the generator
    and discriminator, which is an unrealistic assumption for in-the-wild detection.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, all text-generating GANs proposed until now that are suitable
    for the adversarial setting are based on RNNs. Prior research suggests that proposed
    architectures stop working upon a transition to self-attention-based LLMs [[7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this issue, we start with a Diversity-Promoting GAN (DPGAN) [[65](#bib.bib65)],
    that combines rewards for words and the whole sentence and corresponds to the
    scenario where the attacker would have access to both overall human/machine score
    and single word influence on the score. This is a realistic scenario for an attacker
    seeking to bypass a standard, widely accessible generative models detector, such
    as the HuggingFace OpenAI detector, a common tool that gained popularity thanks
    to its public accessibility, or GLTR - an early generative text detection tool
    [[22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: '2.3 Training Stochastic Parrots: Reinforcement from Critic'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adjusting LLMs to improve a specific aspect of their behavior is a common practice
    and is often done through further model training - fine-tuning. Reinforcement
    Learning from Human Feedback is an example of such fine-tuning that uses an external
    critic model trained from human feedback [[45](#bib.bib45)], but the general approach
    is significantly older [[68](#bib.bib68)].
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement from a critic model has been particularly prominent in the generative
    model normativity alignment field [[55](#bib.bib55)]. LLMs are trained on large
    datasets of texts collected on the internet and are representative of those texts.
    Because of that, they are prone to unexpectedly biased and toxic text generation
    [[8](#bib.bib8), [20](#bib.bib20)]. A term of Stochastic Parrots has emerged to
    designate this tendency [[5](#bib.bib5)], and one of the approaches to counter
    it is normative fine-tuning. The principle of normative fine-tuning is to use
    a critic LLM trained to predict when non-normative text is being generated and
    fine-tune the generator LLM from the feedback of the critic LLM, using rewards
    as a custom loss [[59](#bib.bib59), [46](#bib.bib46), [68](#bib.bib68)].
  prefs: []
  type: TYPE_NORMAL
- en: Here, we provide a setting that combines the reinforcement from a critic model
    with a more modern AdamW optimizer[[40](#bib.bib40)], rather than the traditional
    Adam one [[32](#bib.bib32)] to lead to a robustly generalizing fine-tune. We start
    by performing a normativity fine-tune using a sentiment classifier - an imperfect,
    although valid approach [[46](#bib.bib46)]. After confirming our approach, we
    swap the normativity critic model for a generative model detector.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We show that generative LLM detection with a discriminator LLM is impossible
    if the attacker has access to the reference "human" dataset used to train the
    discriminator LLM. Simply fine-tuning the dataset and using prompts from it leads
    to a complete failure of the discriminator to learn the difference between machine
    and human-generated texts, even in a setting where all LLM outputs are correctly
    labeled.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We show that reinforcement from critic generalizes significantly better than
    previously described when paired with the AdamW optimizer rather than the commonly
    used Adam one and allows well-generalizing model fine-tunes from limited data,
    matching prior SotA in normativity fine-tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We demonstrate a critical weakness on a previously proposed text-generating
    GAN architecture - DPGAN, and show the connection of this weakness to the difference
    in representative power of LLMs and RNNs used in text GANs compatible with the
    in-the-wild detection setting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our code is based on a pre-existing Pytorch implementation of common text-generating
    GANs, including DPGAN, available from [https://github.com/williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch).
  prefs: []
  type: TYPE_NORMAL
- en: We chose GPT-2 small (117M parameters) as a generator due to its wide availability
    and extensive research record concerning its usage. In addition to similar considerations
    for the choice of BERT base (110M parameters) as a discriminator, fine-tuned BERT
    is close to SotA among methods for generative model detection in the wild [[17](#bib.bib17),
    [18](#bib.bib18)]. Overall, the configuration represents a likely attacker/defender
    language model configuration when both are limited to commodity hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'We rely on two datasets provided by the base text-GAN generating library, the
    10 000 entries MS COCO scene description dataset and the 280 000 entries 2017
    EMNLP news sample corpus. The first five tokens are used as prompts whenever applicable,
    and we use an 80/20% train/validation set. No hyperparameter search was performed,
    with default parameters and parameters from prior literature being used. Adam
    [[32](#bib.bib32)] and AdamW optimizers [[37](#bib.bib37), [40](#bib.bib40)] were
    used, according to subsections of [5](#S5 "5 Results and Discussion ‣ Stochastic
    Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to
    Detect with other LLMs"). Per prior research on the importance of model training
    re-starts for methodology evaluation [[39](#bib.bib39)], we report all model training
    runs and evaluate the model performance on the best-out-of-five.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A more detailed overview of the methodology is available in Appendix [0.A](#Pt0.A1
    "Appendix 0.A Detailed Methodology ‣ Stochastic Parrots Looking for Stochastic
    Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs"). The
    code and detailed instructions to reproduce all experiments can be found at [https://github.com/8a3539f168fd077097ea473cc8a9c093/gpt_bert_gan](https://github.com/8a3539f168fd077097ea473cc8a9c093/gpt_bert_gan).'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 GPT-2 Collapses Rapidly Due to DP-GAN’s Discriminator Misspecified Reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DPGAN attributes to generated samples a sentence-level reward and a word-level
    reward for each of the generated tokens to favor/discourage the generation of
    tokens at some position. By replacing the initial generator with a GPT-2 implemented
    based on prior code (more details in Appendix [0.A](#Pt0.A1 "Appendix 0.A Detailed
    Methodology ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy
    to Fine-Tune and Hard to Detect with other LLMs")), we train GPT-2 from scratch
    with the MS COCO dataset, while keeping the same discriminator. We expect full
    memorization from GPT-2 trained from scratch and expect the discriminator to fail
    and provide overall rewards favoring rare words. Surprisingly, we noticed consistent
    output degeneration after only two epochs, ie. GPT-2 only generating ’a’ tokens.
    This contradicts the original paper suggesting that DPGAN was specifically designed
    to avoid repetitive common tokens in its output [[65](#bib.bib65)]. As we can
    see in table [1](#S5.T1 "Table 1 ‣ 5.1 GPT-2 Collapses Rapidly Due to DP-GAN’s
    Discriminator Misspecified Reward ‣ 5 Results and Discussion ‣ Stochastic Parrots
    Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect
    with other LLMs"), the generator quickly starts to prioritize the generation of
    the token ’a’ at position two, even though otherwise rewards indeed favor more
    rare words, as authors originally suggested. While almost all samples in MS COCO
    start with an ’a’, it should have a probability of almost one only at the first
    position and certainly not the second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability of token at position 2 epoch 0 epoch 1 epoch 2 epoch 3 woman: 0.31
    woman: 0.46 a: 0.66 a: 0.99 view: 0.14 is: 0.19 is: 0.31 is: 0.0001 corner: 0.05
    kitchen: 0.05 woman: 0.009 woman: 0.000002 kitchen: 0.04 white: 0.04 kitchen:
    0.009 kitchen: 0.000001 bathroom: 0.04 cat: 0.02 white: 0.003 white: 0.00000008'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Tokens with the highest probability of being generated by GPT-2 at
    position two during adversarial training'
  prefs: []
  type: TYPE_NORMAL
- en: 'After noticing the repetition of the token ’a’, we inspected the rewards the
    discriminator gives to generated samples. We noticed that token ’a’ was favored
    only in the first position, while the rest of the text favored less frequent tokens
    (see table [2](#S5.T2 "Table 2 ‣ 5.1 GPT-2 Collapses Rapidly Due to DP-GAN’s Discriminator
    Misspecified Reward ‣ 5 Results and Discussion ‣ Stochastic Parrots Looking for
    Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs")).
    In particular, the discriminator outputs positive and negative rewards, but these
    rewards seem to be too high for ’a’, which likely overwhelms the word-based reward
    of the DPGAN discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: epoch 0 BOS a woman is walking across the street 33 868 0.4 0.01 1.2 0.4 0.4
    1.8
  prefs: []
  type: TYPE_NORMAL
- en: epoch 1 BOS a large gray airplane flying through the sky 33 869 5.6 6.6 11 10
    1.7 2.8 7.5
  prefs: []
  type: TYPE_NORMAL
- en: epoch 2 BOS a a a a a a a 33 870 0.6 0.07 0.03 0.04 0.05 0.07
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Rewards given to the first generated tokens by the discriminator'
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing the same training with different variations of the MS COCO dataset
    (see appendix [0.B.1](#Pt0.A2.SS1 "0.B.1 variation of MS COCO dataset ‣ Appendix
    0.B DP-GAN with GPT-2 Debugging ‣ Stochastic Parrots Looking for Stochastic Parrots:
    LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs")), we arrived at
    the conclusion that, in our setup, the discriminator favored the generation of
    tokens that are frequently present in the dataset due to unexpected behavior of
    the discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This, however, brought forwards the question of why the problem has not been
    discovered and reported in the original paper. Given that the problem arose when
    we replaced an LSTM generator with a larger and more powerful GPT-2, we investigated
    whether the size/representative power of GPT-2 relative to the training dataset
    was an issue. Experiments with different sizes of GPT-2 and a larger dataset (details
    about different sizes can be found in appendix [0.B.2](#Pt0.A2.SS2 "0.B.2 lighter
    GPT-2 and bigger dataset ‣ Appendix 0.B DP-GAN with GPT-2 Debugging ‣ Stochastic
    Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to
    Detect with other LLMs")). Even though we observed a small drop in quality(repetition
    of tokens), we didn’t observe the same degeneration as with the larger model and
    no evolution of the model at all in the case of a larger dataset (as can be seen
    in Supplementary Fig. [7](#Pt0.A2.F7 "Figure 7 ‣ 0.B.2 lighter GPT-2 and bigger
    dataset ‣ Appendix 0.B DP-GAN with GPT-2 Debugging ‣ Stochastic Parrots Looking
    for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other
    LLMs")). We believe that this confirms our hypothesis and is a more general problem
    to which all RNN-based text GANs would be susceptible when converted to self-attention-based
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Making Sure a GAN Setting Allows the Generator to Train
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given that we did not see any model evolution and a discriminator failure in
    DPGAN, we decided to completely change the architecture and first validate it
    with a fixed discriminator to show that a critic-guided fine-tuning of GPT-2 worked
    as expected. Given that a common setting for such critic-guided fine-tuning is
    normativity alignment, we used a previously proposed approach to it - using a
    sentiment analysis classifier. The rationale behind this approach is that non-normative
    text would be associated with a negative reaction to it, leading the sentiment
    classifier to detect and suppress it [[46](#bib.bib46)]. This would allow us to
    not only validate our approach for evasion fine-tuning but also provide results
    for the normativity alignment field. The details about how GPT-2 is fine-tuned
    can be found in Appendix [0.D.1](#Pt0.A4.SS1 "0.D.1 generator training in negativity
    reduction ‣ Appendix 0.D Architectures for Sentiment and Fake Detection Training
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments described in this part, the "BERT model" is a pretrained
    model fine-tuned for sentiment analysis. In this section, we will call nice GPT-2
    a base GPT-2 model fine-tuned using the sentiment classification BERT model, with
    the generation being prompted by the first five tokens in the MS COCO dataset.
    We call base GPT-2 a default pretrained GPT-2-small model without any fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: We experimented with different BERTs trained for sentiment analysis. The difference
    between them was in the labels they gave to generated samples. We found that having
    a BERT model that could attribute a third "neutral" label (mapped to a loss reward
    of 0.5 in our case) to the generated samples was helpful for the training. Most
    likely, this helped because most sentences in the MS COCO dataset were neutral,
    given that MS COCO entries are generally neutral image descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used Adam optimizer with $betas=(0.9,0.999)$. For the loss function, we
    used MAE (mean absolute error). We already observed some meaningful but unstable
    results while training with the MS COCO dataset. Out of 5 runs, we observed one
    where the negativity didn’t change, three that had about 70% less negativity,
    and one that fluctuated between more than double negativity and 50% less negativity(see
    Fig. [1](#S5.F1 "Figure 1 ‣ 5.2 Making Sure a GAN Setting Allows the Generator
    to Train ‣ 5 Results and Discussion ‣ Stochastic Parrots Looking for Stochastic
    Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e93d79a8c3ff1977c92e41087f195e7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Number of samples classified as negative by BERT during training
    at each epoch with 10,000 prefixes for three different runs'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then tried to see if our result would generalize to another dataset, so
    we compared pre-trained GPT-2 with our nice GPT-2 on the EMNLP news dataset. We
    observed that the result generalized to about 35% less negativity with GPT-2 nice
    compared to base GPT-2 (see Fig. [2](#S5.F2 "Figure 2 ‣ 5.2 Making Sure a GAN
    Setting Allows the Generator to Train ‣ 5 Results and Discussion ‣ Stochastic
    Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to
    Detect with other LLMs")), which is less than the 70% less negativity observed
    for the training set(MS COCO). This is comparable to the prior SotA from [[46](#bib.bib46)].
    We also provided samples generated by base GPT-2 and nice GPT-2 in appendix [0.C](#Pt0.A3
    "Appendix 0.C Examples of Samples Generated by Nice GPT-2 Compared to Base GPT-2
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs"). We believe that GPT-2 sometimes learns during
    training to avoid generating types of sentences considered negative by the BERT,
    and this procedure generalizes to a validation dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1781f7118d7d05aa13c93edf09d99dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison of the sentiment of text generated by base GPT-2 and finetuned
    version on a reduced version of EMNLP dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We noticed that, on average, the base GPT-2 generates more negative samples
    than positive ones, and some of them are very negative (we provide a selection
    of them in appendix [0.C](#Pt0.A3 "Appendix 0.C Examples of Samples Generated
    by Nice GPT-2 Compared to Base GPT-2 ‣ Stochastic Parrots Looking for Stochastic
    Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs")). This
    considerable bias should be accounted for when using GPT-2\. It is now well known
    that some prefixes lead to extreme toxicity by the GPT family [[21](#bib.bib21)],
    but it seems that sometimes prefixes that we would not suspect also lead to very
    toxic generated text. This is not surprising and has been previously reported
    by [[47](#bib.bib47)], although the problem is known to be significantly worse
    for larger models of the GPT family (see [[20](#bib.bib20)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Ensuring Generalizable Generator Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with the prior approach was that in about 2/5 cases, the training
    did not lead to improvement at all. We observed on some runs an increase in negativity
    up to double or no changes in negativity during training. Also, the negativity
    reduction on the validation set(EMNLP news) was 35% compared to the 70% reduction
    for the training set. The instability of large language model training is well-known,
    especially on smaller datasets, such as MS COCO (multiple restarts are often used
    to obtain state-of-the-art models). However, we were unsatisfied with this performance.
    We tried to use tricks known to stabilize training and increase the generalization
    capabilities of the model, namely switching to use *AdamW* for the optimizer,
    learning rate scheduling(with warm-up), gradient clipping, and weight decay, all
    of which were used for GPT-2 pretraining. We also tried to use SGD, which is known
    to generate results that generalize better and allow model "grokking" [[49](#bib.bib49)].
    Unfortunately, SGD is known to fare poorly on language models and failed outright.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/15dfb9fe5f66bd846e54603090890fde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Number of samples classified as negative by BERT during training
    at each epoch with 10,000 prompts for five different runs'
  prefs: []
  type: TYPE_NORMAL
- en: We ran our model with 14 epochs(we stopped early when we saw no changes for
    five epochs). We tested learning rate scheduling with linear scheduling and a
    starting learning rate at 5e-5(the learning rate is reduced linearly ten times
    per epoch until reaching 0 at epoch 20)). The reason we use a learning rate scheduling
    is to improve stability because it allows the model to reach a more robust and
    deeper local minimum. For the other parameters(in particular for the parameters
    of AdamW, we looked at default parameters from:[https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the above tricks, we observe only marginal improvement in stability.
    We still observed the same rate of runs that did not lead to improvements. However,
    we saw better convergence towards about 5% less negativity for the training dataset(for
    runs that did reduce the negativity). We also observed better generalization for
    the validation set with 60% reduced negativity. In particular, one of the training
    produced a particularly good model that generalized well with the validation EMLP
    dataset to an approximate 60% decrease in negativity and a 60% increase in positivity(see
    Fig. [4](#S5.F4 "Figure 4 ‣ 5.3 Ensuring Generalizable Generator Training ‣ 5
    Results and Discussion ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs
    are Easy to Fine-Tune and Hard to Detect with other LLMs")). We thus think that
    the model can get similar levels of reduction of negativity for the validation
    set when the GPT-2 generator is able to train. However, even with different optimization
    tricks, we observed the same rate of about 2/5 runs that did not lead to less
    negativity.'
  prefs: []
  type: TYPE_NORMAL
- en: Relevant literature suggests that transformer NLP models training and fine-tuning
    are hard tasks that require extensive hyper-parameter optimization and additional
    tricks to stabilize the training(see eg. [[35](#bib.bib35)]).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/14ee11ef4999ed796f03b1cced88a3ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of the sentiment of text generated by base GPT-2 and nice
    GPT-2 with a generalizing improvement in positivity tested on the EMNLP news dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '5.4 Running GAN: Hide-and-Seek Between GTP-2 and BERT'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of these experiments is to see if it is possible to train a BERT model
    to detect text generated by GPT-2 and use it to then train GPT-2 to escape training,
    aka perform GAN iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture we designed is functionally divided into two parts. First,
    BERT fine-tuning, which we will call *discriminator training* (see Fig [5](#S5.F5
    "Figure 5 ‣ 5.4 Running GAN: Hide-and-Seek Between GTP-2 and BERT ‣ 5 Results
    and Discussion ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy
    to Fine-Tune and Hard to Detect with other LLMs")) and generator training with
    scores from BERT, which we will call *generator training*(see Supplementary Fig.
    [9](#Pt0.A4.F9 "Figure 9 ‣ 0.D.2 generator training in fake detection ‣ Appendix
    0.D Architectures for Sentiment and Fake Detection Training ‣ Stochastic Parrots
    Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect
    with other LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: The BERT fine-tuning (discriminator training) consists of fine-tuning a pre-trained
    BERT classifier to detect fake samples. We feed BERT with samples generated by
    a base pre-trained GPT-2 that uses prefixes from a dataset to generate samples.
    We give the label 0 to fake samples generated by GPT-2 and label 1 for the true
    samples coming from the same dataset as the prefixes that GPT-2 uses. Then BERT
    outputs a probability of the output being fake or true given an input sample.
    We then compute a loss based on the labels and perform back-propagation. The process
    is repeated for 1-3 epochs, where each epoch corresponds to the full dataset traversal.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c931a5e9143fc100e4efce9613fb2c30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: BERT training phase of the GAN for fake detection'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part of the training(generator training) consists of training GPT-2
    with scores exactly the same way as we did for sentiment analysis(cf. part [5.2](#S5.SS2
    "5.2 Making Sure a GAN Setting Allows the Generator to Train ‣ 5 Results and Discussion
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs")). The details about how it differs from the
    previous part can be found in appendix [0.D.2](#Pt0.A4.SS2 "0.D.2 generator training
    in fake detection ‣ Appendix 0.D Architectures for Sentiment and Fake Detection
    Training ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to
    Fine-Tune and Hard to Detect with other LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: The GAN architecture we designed starts with an optional fine-tuning of the
    GPT-2 generator then we repeat parts 1 and 2 during the training.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Training with Fine-Tuned Generator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We first tried to fine-tune GPT-2 for the MS COCO dataset before the two repeated
    phases mentioned above, then built a dataset from the true MS COCO dataset and
    fake MS COCO generated by fine-tuned GPT-2\. This dataset is used to fine-tune
    BERT to detect fake samples. Finally, use BERT fine-tuned for fake detection to
    train GPT-2 as we did in the previous experiment with sentiment analysis(the goal
    being to build a GAN from that).
  prefs: []
  type: TYPE_NORMAL
- en: 'The result was that BERT could not distinguish samples generated by GPT-2 from
    true samples(see Fig. [6](#S5.F6 "Figure 6 ‣ 5.4.1 Training with Fine-Tuned Generator
    ‣ 5.4 Running GAN: Hide-and-Seek Between GTP-2 and BERT ‣ 5 Results and Discussion
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs")). Also, we discovered here that BERT achieved
    almost 100% accuracy when the samples generated by GPT-2 have a different length
    than those in the true dataset (more details about this training setup can be
    found in appendix [0.E.1](#Pt0.A5.SS1 "0.E.1 training with fine-tuned GPT-2 on
    MS COCO dataset ‣ Appendix 0.E Training Plots from GPT-2 Fake Detection Training
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e246c7104ca0b9931525cb90bab2699.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: BERT accuracy on the validation set when GPT-2 is fine-tuned'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Training Without a Fine-Tuned Generator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since we found that BERT could not train well when the difference between fake
    and true datasets is small(one or two tokens) or too big (samples generated from
    GPT-2 have ten more words than those of the dataset), we decided to perform the
    same procedure, but without fine-tuning GPT-2, and with correction of the lengths
    of the generated samples(so that all samples true/fake have same lengths). In
    that scenario, BERT had a high accuracy of about 90%. However, GPT-2 was not able
    to learn to evade BERT fine-tuned for fake detection (see appendix [0.E.2](#Pt0.A5.SS2
    "0.E.2 training without fine-tuned GPT-2 on MS COCO dataset ‣ Appendix 0.E Training
    Plots from GPT-2 Fake Detection Training ‣ Stochastic Parrots Looking for Stochastic
    Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs") for more
    details about this part).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our hypothesis as to causes is two-fold. First, we saw in part 5.2 that training
    GPT-2 with scores, as we did, is unstable (this could probably be improved). Also,
    we inspected the scores that BERT gave to the generated samples(see appendix [0.F](#Pt0.A6
    "Appendix 0.F Examples of Samples by GPT-2 during the Fake GAN Training and Score
    Given ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs") for examples of samples and scores), and
    we didn’t observe a general pattern in the samples that received a good score.
    GPT-2 is, therefore, probably not able to find types of samples to avoid or to
    generate more of to fool BERT (as it maybe did in part [5.2](#S5.SS2 "5.2 Making
    Sure a GAN Setting Allows the Generator to Train ‣ 5 Results and Discussion ‣
    Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs") because the scores were more meaningful).
    BERT and GPT-2 might need more data to generalize for fake detection. In particular,
    transformers require a lot of data to be trained (see [[48](#bib.bib48)]), and
    GPT-2 might need to be further fine-tuned by using prompts derived from the training
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Training with a Partially Fine-Tuned Generator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the reasons above, we combined EMNLP news and MS COCO as a single dataset,
    and we fine-tuned the generator using MLE training with 30% of that dataset before
    adversarial training. During the discriminator training, BERT achieved 88% accuracy
    in fake detection on the validation dataset. The result for the adversarial training
    in terms of the number of samples generated by GPT-2 classified as fake or true
    is shown in Supplementary Fig. [13](#Pt0.A5.F13 "Figure 13 ‣ 0.E.3 training with
    partially fine-tuned GPT-2 on EMNLP news + MS COCO dataset ‣ Appendix 0.E Training
    Plots from GPT-2 Fake Detection Training ‣ Stochastic Parrots Looking for Stochastic
    Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs") in Appendix.
    Although GPT-2 is able to learn to evade fake detection, it degenerates during
    training (as can be seen with the samples in appendix [0.G](#Pt0.A7 "Appendix
    0.G Example of Samples Generated by GPT-2 during Fake GAN Training with EMNLP
    News + MS COCO Dataset after one Epoch of Adversarial Training ‣ Stochastic Parrots
    Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect
    with other LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our overarching result is the demonstration of a highly potent attack against
    a common type of generative LLM detectors, which are considered SotA and are highly
    used in the wake of ChatGPT public release in late 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we show that an attacker with access to the human reference texts
    used to train the detector and access to the detector rating of generative model
    outputs is able not only to evade the model detection but even in the white-box
    setting, where all the generative mode outputs are correctly labeled, they can
    fully stop the detector training against it. While the detection of generative
    LLMs with other LLMs has been previously shown not to work well, we go further,
    showing that in the setting where common datasets are used to train the discriminator,
    a minimally competent attacker can fully defeat the detector.
  prefs: []
  type: TYPE_NORMAL
- en: On the way to this result, we showed that reinforcement from a critic model
    could be used to fine-tune a generative model on a relatively small dataset, as
    long as AdamW rather than the common Adam optimizer was used, with multiple restarts
    yielding the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we showed that existing literature on text-generating GANs, with both
    generator and discriminators built on RNNs, cannot be trusted to translate to
    LLMs as-is. While we demonstrate a failure in the DP-GAN architecture specifically,
    the reasons we believe led that failure to have gone undetected are the fundamental
    difference in the representative power of GANs and RNNs. As such, careful re-validation
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we believe that our results strongly argue against the continued use
    of LLMs fine-tuned for classification to detect generative LLMs in the wild. We
    hope that this prompts research into novel methods of LLM detection, even if it
    is just fingerprinting of texts from major generative LLM access providers.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that our auxiliary results will also be of interest to the NLP ML community,
    helping fine-tune generative models more efficiently and will prompt the re-examination
    of results obtained on RNN-based text GAN before their extrapolation to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the results presented here are primarily experimental. We do not provide
    theoretical guarantees, and it is unclear how results obtained on GPT-2 small
    and BERT will scale up to larger models, given their tendency to unlock new capabilities
    with the increase in size [[20](#bib.bib20)]. Similarly, while prior research
    suggests that results obtained on those two specific architectures generalize
    to other Transformer-based architectures, it remains an open question. GPT-2 small
    and BERT are architectures of comparable sizes (117M parameters vs. 110M parameters).
    An adversarial training involving models of significantly different representative
    power would likely result in different dynamics. A notable example of such a setting
    is the case where the discrimination model is substantially smaller than the generative
    one - which is a likely setting, given the attacker can invest considerable computational
    resources, whereas due to the sheer volume of content generated and consumed by
    humans online, a defensive discriminator will likely need to run on end-user devices.
  prefs: []
  type: TYPE_NORMAL
- en: While large language models fine-tuned for detection - such as the ones we used
    here - have been criticized as stylometry unsuited for generative models detection
    [[6](#bib.bib6), [54](#bib.bib54)], there has so far been no alternative detection
    method shown to perform better. Notably, methods relying on the factual structure
    of the text have shown similar vulnerability to prompt selection and fine-tuning
    [[67](#bib.bib67)], even before the arrival of factual database augmented generative
    models [[56](#bib.bib56), [57](#bib.bib57)].
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, evasion detection through prompt selection - that we use to some
    extent - has been almost entirely neglected until now. However, results from the
    generative model normativity and privacy fields suggest that well-chosen prompts
    can lead to highly unexpected and uncharacteristic texts [[47](#bib.bib47), [14](#bib.bib14)],
    likely leading to a degradation of detection capabilities presented here as a
    previous SotA.
  prefs: []
  type: TYPE_NORMAL
- en: If anything, both of those avenues for evasion detection reinforce our conclusion
    that a competent attacker can easily evade detection even with relatively small
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We essentially provide a blueprint for a competent attacker to create a generative
    model that would effectively evade detection by most if not all, means available
    today.
  prefs: []
  type: TYPE_NORMAL
- en: While the first version of this paper was prepared in mid-2022, following the
    release of ChatGPT, we delayed its submission for publication by four months following
    the public release of ChatGPT to leave time for model developers to improve detection
    mechanisms. We similarly have contacted some of the entities operating common
    generative LLM detection endpoints to report our findings, to no avail.
  prefs: []
  type: TYPE_NORMAL
- en: Given that prior knowledge of reference "human" texts used to train the detector,
    LLM is a critical component of full evasion described here, and given that it
    is currently unlikely for most deployed detectors using classification fine-tuned
    LLMs, we decided in favor of releasing our results publicly.
  prefs: []
  type: TYPE_NORMAL
- en: Another factor that contributed to our decision to release these results is
    an increasing reliance on fundamentally flawed detection tools to make prejudicial
    decisions. We observed LLM-based detectors for generative LLMs used in the educational
    setting, meaning that their high false-positive rate led to students being unjustly
    accused and disciplined, potentially impacting their long-term perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: We hope the results presented here will lead to a more rigorous study of alternative
    ways to detect LLMs, starting with the generated text fingerprinting by major
    generative LLM providers.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank the armasuisse - Cyber-Defence (CYD) Campus for the Distinguished
    Post Doctoral Fellowship supporting AK, as well as Fabien Salvi (EPFL) for the
    technical support regarding the computational infrastructure organization, and
    France Faille (EPFL) for the administrative support.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Adelani, D.I., Mai, H.T., Fang, F., Nguyen, H.H., Yamagishi, J., Echizen,
    I.: Generating sentiment-preserving fake online reviews using neural language
    models and their human- and machine-based detection. In: AINA (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Austin, J., Odena, A., Nye, M.I., Bosma, M., Michalewski, H., Dohan, D.,
    Jiang, E., Cai, C.J., Terry, M., Le, Q.V., Sutton, C.: Program synthesis with
    large language models. CoRR abs/2108.07732 (2021), [https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Bagdasaryan, E., Shmatikov, V.: Spinning language models: Risks of propaganda-as-a-service
    and countermeasures. In: 43rd IEEE Symposium on Security and Privacy, SP 2022,
    San Francisco, CA, USA, May 22-26, 2022\. pp. 769–786\. IEEE (2022). https://doi.org/10.1109/SP46214.2022.9833572,
    [https://doi.org/10.1109/SP46214.2022.9833572](https://doi.org/10.1109/SP46214.2022.9833572)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Bakhtin, A., Gross, S., Ott, M., Deng, Y., Ranzato, M., Szlam, A.: Real
    or fake? learning to discriminate machine from human generated text. CoRR abs/1906.03351
    (2019), [http://arxiv.org/abs/1906.03351](http://arxiv.org/abs/1906.03351)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers
    of stochastic parrots: Can language models be too big? In: Elish, M.C., Isaac,
    W., Zemel, R.S. (eds.) FAccT ’21: 2021 ACM Conference on Fairness, Accountability,
    and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021\. pp. 610–623\.
    ACM (2021). https://doi.org/10.1145/3442188.3445922, [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Bhat, M.M., Parthasarathy, S.: How effectively can machines defend against
    machine-generated fake news? an empirical study. In: INSIGHTS (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Blin, K., Kucharavy, A.: Can the transformer be used as a drop-in replacement
    for rnns in text-generating gans? In: Angelova, G., Kunilovskaya, M., Mitkov,
    R., Nikolova-Koleva, I. (eds.) Proceedings of the International Conference on
    Recent Advances in Natural Language Processing (RANLP 2021), Held Online, 1-3September,
    2021\. pp. 173–181\. INCOMA Ltd. (2021), [https://aclanthology.org/2021.ranlp-1.21](https://aclanthology.org/2021.ranlp-1.21)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Bolukbasi, T., Chang, K., Zou, J.Y., Saligrama, V., Kalai, A.T.: Man is
    to computer programmer as woman is to homemaker? debiasing word embeddings. In:
    Lee, D.D., Sugiyama, M., von Luxburg, U., Guyon, I., Garnett, R. (eds.) Advances
    in Neural Information Processing Systems 29: Annual Conference on Neural Information
    Processing Systems 2016, December 5-10, 2016, Barcelona, Spain. pp. 4349–4357
    (2016), [https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html](https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Bradshaw, S., Bailey, H., Howard, P.N.: Industrialized disinformation:
    2020 global inventory of organized social media manipulation. computational propaganda
    research project (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high
    fidelity natural image synthesis. In: 7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net
    (2019), [https://openreview.net/forum?id=B1xsqj09Fm](https://openreview.net/forum?id=B1xsqj09Fm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models
    are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
    Lin, H. (eds.) Advances in Neural Information Processing Systems 33: Annual Conference
    on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual (2020), [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Buchanan, B., Lohn, A., Musser, M., Sedova, K.: Truth, lies, and automation.
    Center for Security and Emerging Technology (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Caccia, M., Caccia, L., Fedus, W., Larochelle, H., Pineau, J., Charlin,
    L.: Language gans falling short. In: 8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net
    (2020), [https://openreview.net/forum?id=BJgza6VtPB](https://openreview.net/forum?id=BJgza6VtPB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A.,
    Lee, K., Roberts, A., Brown, T.B., Song, D., Erlingsson, Ú., Oprea, A., Raffel,
    C.: Extracting training data from large language models. In: Bailey, M., Greenstadt,
    R. (eds.) 30th USENIX Security Symposium, USENIX Security 2021, August 11-13,
    2021\. pp. 2633–2650\. USENIX Association (2021), [https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Dawson, A., Innes, M.: How russia’s internet research agency built its
    disinformation campaign. The Political Quarterly (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep
    bidirectional transformers for language understanding. In: Burstein, J., Doran,
    C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers). pp. 4171–4186\. Association for Computational Linguistics (2019). https://doi.org/10.18653/v1/n19-1423,
    [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Diwan, N., Chakraborty, T., Shafiq, Z.: Fingerprinting fine-tuned language
    models in the wild. ArXiv abs/2106.01703 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Fagni, T., Falchi, F., Gambini, M., Martella, A., Tesconi, M.: Tweepfake:
    About detecting deepfake tweets. PLoS ONE 16 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Gagiano, R., Kim, M., Zhang, X., Biggs, J.: Robustness analysis of grover
    for machine-generated news detection. In: ALTA (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Ganguli, D., Hernandez, D., Lovitt, L., DasSarma, N., Henighan, T., Jones,
    A., Joseph, N., Kernion, J., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly,
    T., Drain, D., Elhage, N., Showk, S.E., Fort, S., Hatfield-Dodds, Z., Johnston,
    S., Kravec, S., Nanda, N., Ndousse, K., Olsson, C., Amodei, D., Amodei, D., Brown,
    T.B., Kaplan, J., McCandlish, S., Olah, C., Clark, J.: Predictability and surprise
    in large generative models. CoRR abs/2202.07785 (2022), [https://arxiv.org/abs/2202.07785](https://arxiv.org/abs/2202.07785)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Gehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: RealToxicityPrompts:
    Evaluating neural toxic degeneration in language models. In: Findings of the Association
    for Computational Linguistics: EMNLP 2020\. pp. 3356–3369\. Association for Computational
    Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.findings-emnlp.301,
    [https://aclanthology.org/2020.findings-emnlp.301](https://aclanthology.org/2020.findings-emnlp.301)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Gehrmann, S., Strobelt, H., Rush, A.M.: GLTR: statistical detection and
    visualization of generated text. In: Costa-jussà, M.R., Alfonseca, E. (eds.) Proceedings
    of the 57th Conference of the Association for Computational Linguistics, ACL 2019,
    Florence, Italy, July 28 - August 2, 2019, Volume 3: System Demonstrations. pp.
    111–116\. Association for Computational Linguistics (2019). https://doi.org/10.18653/v1/p19-3019,
    [https://doi.org/10.18653/v1/p19-3019](https://doi.org/10.18653/v1/p19-3019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
    D., Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial nets. In: Ghahramani,
    Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (eds.) Advances
    in Neural Information Processing Systems 27: Annual Conference on Neural Information
    Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada. pp. 2672–2680
    (2014), [https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] GPT-3: A robot wrote this entire article. are you scared yet, human? The
    Guardian (2020), [https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Holmes, A.: A fake blog written by AI shot to the top of hacker news after
    people thought it was real — here’s how a college student made it. Business Insider
    (2020), [https://www.businessinsider.fr/us/fake-ai-generated-gpt3-blog-hacker-news-2020-8](https://www.businessinsider.fr/us/fake-ai-generated-gpt3-blog-hacker-news-2020-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Holtzman, A., Buys, J., Du, L., Forbes, M., Choi, Y.: The curious case
    of neural text degeneration. In: 8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), [https://openreview.net/forum?id=rygGQyrFvH](https://openreview.net/forum?id=rygGQyrFvH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Hotez, P.J.: Anti-science kills: From soviet embrace of pseudoscience
    to accelerated attacks on US biomedicine. PLOS Biology 19(1), e3001068 (2021).
    https://doi.org/10.1371/journal.pbio.3001068, [https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001068](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001068),
    publisher: Public Library of Science'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Hovy, D.: The enemy in your own camp: How well can we detect statistically-generated
    fake reviews – an adversarial study. In: ACL (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Huszar, F.: How (not) to train your generative model: Scheduled sampling,
    likelihood, adversary? CoRR abs/1511.05101 (2015), [http://arxiv.org/abs/1511.05101](http://arxiv.org/abs/1511.05101)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ippolito, D., Duckworth, D., Callison-Burch, C., Eck, D.: Automatic detection
    of generated text is easiest when humans are fooled. In: ACL (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Isola, P., Zhu, J., Zhou, T., Efros, A.A.: Image-to-image translation
    with conditional adversarial networks. In: 2017 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017\. pp.
    5967–5976\. IEEE Computer Society (2017). https://doi.org/10.1109/CVPR.2017.632,
    [https://doi.org/10.1109/CVPR.2017.632](https://doi.org/10.1109/CVPR.2017.632)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In:
    Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015),
    [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Kreps, S., McCain, M., Brundage, M.: All the news that’s fit to fabricate:
    Ai-generated text as a tool of media misinformation. Journal of Experimental Political
    Science 9, 104 – 117 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Lai, P., Chen, C., Lo, L., Chen, C.C.: Coldgan: Resolving cold start user
    recommendation by using generative adversarial networks. CoRR abs/2011.12566 (2020),
    [https://arxiv.org/abs/2011.12566](https://arxiv.org/abs/2011.12566)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Liu, L., Liu, X., Gao, J., Chen, W., Han, J.: Understanding the difficulty
    of training transformers. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2020, Online, November 16-20, 2020\. pp. 5747–5763. Association for Computational
    Linguistics (2020). https://doi.org/10.18653/v1/2020.emnlp-main.463, [https://doi.org/10.18653/v1/2020.emnlp-main.463](https://doi.org/10.18653/v1/2020.emnlp-main.463)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized BERT pretraining
    approach. CoRR abs/1907.11692 (2019), [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In:
    7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019\. OpenReview.net (2019), [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Luca, M., Zervas, G.: Fake it till you make it: Reputation, competition,
    and yelp review fraud. Consumer Financial Fraud eJournal (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Lucic, M., Kurach, K., Michalski, M., Gelly, S., Bousquet, O.: Are gans
    created equal? A large-scale study. In: Bengio, S., Wallach, H.M., Larochelle,
    H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information
    Processing Systems 31: Annual Conference on Neural Information Processing Systems
    2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada. pp. 698–707 (2018),
    [https://proceedings.neurips.cc/paper/2018/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Ma, J., Yarats, D.: Quasi-hyperbolic momentum and adam for deep learning.
    In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019\. OpenReview.net (2019), [https://openreview.net/forum?id=S1fUpoR5FQ](https://openreview.net/forum?id=S1fUpoR5FQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Maronikolakis, A., Schütze, H., Stevenson, M.: Identifying automatically
    generated headlines using transformers. In: NLP4IF (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] de Masson d’Autume, C., Mohamed, S., Rosca, M., Rae, J.W.: Training language
    gans from scratch. In: Wallach, H.M., Larochelle, H., Beygelzimer, A., d’Alché-Buc,
    F., Fox, E.B., Garnett, R. (eds.) Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada. pp. 4302–4313 (2019), [https://proceedings.neurips.cc/paper/2019/hash/a6ea8471c120fe8cc35a2954c9b9c595-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/a6ea8471c120fe8cc35a2954c9b9c595-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Mink, J., Luo, L., Barbosa, N.M., Figueira, O., Wang, Y., Wang, G.: Deepphish:
    Understanding user trust towards artificially generated profiles in online social
    networks. In: Butler, K.R.B., Thomas, K. (eds.) 31st USENIX Security Symposium,
    USENIX Security 2022, Boston, MA, USA, August 10-12, 2022\. pp. 1669–1686\. USENIX
    Association (2022), [https://www.usenix.org/conference/usenixsecurity22/presentation/mink](https://www.usenix.org/conference/usenixsecurity22/presentation/mink)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] OpenAI: Gpt-4 technical report. CoRR abs/2303.08774 (2023), [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin,
    P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
    F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P.F., Leike,
    J., Lowe, R.: Training language models to follow instructions with human feedback.
    CoRR abs/2203.02155 (2022). https://doi.org/10.48550/arXiv.2203.02155, [https://doi.org/10.48550/arXiv.2203.02155](https://doi.org/10.48550/arXiv.2203.02155)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Peng, X., Li, S., Frazier, S., Riedl, M.O.: Reducing non-normative text
    generation from language models. In: Davis, B., Graham, Y., Kelleher, J.D., Sripada,
    Y. (eds.) Proceedings of the 13th International Conference on Natural Language
    Generation, INLG 2020, Dublin, Ireland, December 15-18, 2020\. pp. 374–383\. Association
    for Computational Linguistics (2020), [https://aclanthology.org/2020.inlg-1.43/](https://aclanthology.org/2020.inlg-1.43/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Perez, E., Huang, S., Song, H.F., Cai, T., Ring, R., Aslanides, J., Glaese,
    A., McAleese, N., Irving, G.: Red teaming language models with language models.
    CoRR abs/2202.03286 (2022), [https://arxiv.org/abs/2202.03286](https://arxiv.org/abs/2202.03286)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Popel, M., Bojar, O.: Training tips for the transformer model. Prague
    Bull. Math. Linguistics 110, 43–70 (2018), [http://ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf](http://ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Power, A., Burda, Y., Edwards, H., Babuschkin, I., Misra, V.: Grokking:
    Generalization beyond overfitting on small algorithmic datasets. CoRR abs/2201.02177
    (2022), [https://arxiv.org/abs/2201.02177](https://arxiv.org/abs/2201.02177)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language
    understanding by generative pre-training. OpenAI blog (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language
    models are unsupervised multitask learners. OpenAI blog 1(8),  9 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,
    Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
    unified text-to-text transformer. J. Mach. Learn. Res. 21, 140:1–140:67 (2020),
    [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version
    of BERT: smaller, faster, cheaper and lighter. CoRR abs/1910.01108 (2019), [http://arxiv.org/abs/1910.01108](http://arxiv.org/abs/1910.01108)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Schuster, T., Schuster, R., Shah, D.J., Barzilay, R.: The limitations
    of stylometry for detecting machine-generated fake news. Computational Linguistics
    Just Accepted, 1–18 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Sheng, E., Chang, K., Natarajan, P., Peng, N.: Societal biases in language
    generation: Progress and challenges. In: Zong, C., Xia, F., Li, W., Navigli, R.
    (eds.) Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing,
    ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021\. pp.
    4275–4293\. Association for Computational Linguistics (2021). https://doi.org/10.18653/v1/2021.acl-long.330,
    [https://doi.org/10.18653/v1/2021.acl-long.330](https://doi.org/10.18653/v1/2021.acl-long.330)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Shu, K., Li, Y., Ding, K., Liu, H.: Fact-enhanced synthetic news generation.
    In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third
    Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The
    Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,
    Virtual Event, February 2-9, 2021\. pp. 13825–13833\. AAAI Press (2021), [https://ojs.aaai.org/index.php/AAAI/article/view/17629](https://ojs.aaai.org/index.php/AAAI/article/view/17629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Shuster, K., Komeili, M., Adolphs, L., Roller, S., Szlam, A., Weston,
    J.: Language models that seek for knowledge: Modular search & generation for dialogue
    and prompt completion. CoRR abs/2203.13224 (2022). https://doi.org/10.48550/arXiv.2203.13224,
    [https://doi.org/10.48550/arXiv.2203.13224](https://doi.org/10.48550/arXiv.2203.13224)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu,
    J., Radford, A., Wang, J.: Release strategies and the social impacts of language
    models. ArXiv abs/1908.09203 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Solaiman, I., Dennison, C.: Process for adapting language models to society
    (PALMS) with values-targeted datasets. In: Ranzato, M., Beygelzimer, A., Dauphin,
    Y.N., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing
    Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS
    2021, December 6-14, 2021, virtual. pp. 5861–5873 (2021), [https://proceedings.neurips.cc/paper/2021/hash/2e855f9489df0712b4bd8ea9e2848c5a-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/2e855f9489df0712b4bd8ea9e2848c5a-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Stiff, H., Johansson, F.: Detecting computer-generated disinformation.
    International Journal of Data Science and Analytics 13, 363–383 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Thoppilan, R., Freitas, D.D., Hall, J., Shazeer, N., Kulshreshtha, A.,
    Cheng, H., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H.S.,
    Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen,
    D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhou, Y., Chang, C., Krivokon, I.,
    Rusch, W., Pickett, M., Meier-Hellstern, K.S., Morris, M.R., Doshi, T., Santos,
    R.D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson,
    B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R.,
    Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil,
    R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E.H., Le, Q.: Lamda: Language models
    for dialog applications. CoRR abs/2201.08239 (2022), [https://arxiv.org/abs/2201.08239](https://arxiv.org/abs/2201.08239)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Guyon, I., von
    Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett,
    R. (eds.) Advances in Neural Information Processing Systems 30: Annual Conference
    on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
    CA, USA. pp. 5998–6008 (2017), [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Weiss, M.: Deepfake bot submissions to federal public comment websites
    cannot be distinguished from human submissions. In: Technology Science. vol. 2019121801
    (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Wolff, M.: Attacking neural text detectors. In: Towards Trustworthy ML:
    Rethinking Security and Privacy for ML (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Xu, J., Ren, X., Lin, J., Sun, X.: Diversity-promoting GAN: A cross-entropy
    based generative adversarial network for diversified text generation. In: Riloff,
    E., Chiang, D., Hockenmaier, J., Tsujii, J. (eds.) Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, Brussels, Belgium, October
    31 - November 4, 2018\. pp. 3940–3949\. Association for Computational Linguistics
    (2018). https://doi.org/10.18653/v1/d18-1428, [https://doi.org/10.18653/v1/d18-1428](https://doi.org/10.18653/v1/d18-1428)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Zellers, R., Holtzman, A., Rashkin, H., Bisk, Y., Farhadi, A., Roesner,
    F., Choi, Y.: Defending against neural fake news. In: Wallach, H.M., Larochelle,
    H., Beygelzimer, A., d’Alché-Buc, F., Fox, E.B., Garnett, R. (eds.) Advances in
    Neural Information Processing Systems 32: Annual Conference on Neural Information
    Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada.
    pp. 9051–9062 (2019), [https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Zhong, W., Tang, D., Xu, Z., Wang, R., Duan, N., Zhou, M., Wang, J., Yin,
    J.: Neural deepfake detection with factual structure of text. In: Webber, B.,
    Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020\.
    pp. 2461–2470\. Association for Computational Linguistics (2020). https://doi.org/10.18653/v1/2020.emnlp-main.193,
    [https://doi.org/10.18653/v1/2020.emnlp-main.193](https://doi.org/10.18653/v1/2020.emnlp-main.193)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Ziegler, D.M., Stiennon, N., Wu, J., Brown, T.B., Radford, A., Amodei,
    D., Christiano, P.F., Irving, G.: Fine-tuning language models from human preferences.
    CoRR abs/1909.08593 (2019), [http://arxiv.org/abs/1909.08593](http://arxiv.org/abs/1909.08593)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix 0.A Detailed Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 0.A.1 GAN configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We based our code on the Pytorch implementation of a set of text-generating
    GANs by [https://github.com/williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch),
    including the DPGAN [[65](#bib.bib65)].
  prefs: []
  type: TYPE_NORMAL
- en: 0.A.2 Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We chose GPT-2 model as a generator due to its wide availability and extensive
    record of usage in natural text generation in different contexts. Besides, GPT
    architecture scaling across 4 orders of magnitude in parameters with minimal modifications,
    we can hope for a generalization of our results to larger models. For GPT-2 implementation,
    we used [https://github.com/graykode/gpt-2-Pytorch](https://github.com/graykode/gpt-2-Pytorch)
    as a basis. Unless otherwise specified, we used the GPT-2 small architecture (117M
    parameters). For part [5.2](#S5.SS2 "5.2 Making Sure a GAN Setting Allows the
    Generator to Train ‣ 5 Results and Discussion ‣ Stochastic Parrots Looking for
    Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs")
    (and also the following parts), we used the weights for GPT-2 from the default
    HuggingFace GPT-2 repository ([https://huggingface.co/gpt2](https://huggingface.co/gpt2)),
    but rather than using the `transformers` library provided by HuggingFace, loaded
    the weights into our implementation of GPT-2 architecture as in [5.1](#S5.SS1
    "5.1 GPT-2 Collapses Rapidly Due to DP-GAN’s Discriminator Misspecified Reward
    ‣ 5 Results and Discussion ‣ Stochastic Parrots Looking for Stochastic Parrots:
    LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs"). GPT-2 was sampled
    using a top-40 sampling strategy, starting with the first five tokens in a sentence
    from a dataset used.'
  prefs: []
  type: TYPE_NORMAL
- en: We chose BERT as a base for our defensive discriminator due to its wide availability,
    extensive usage in creating classifiers for natural languages, and excellent performance
    in generative models detection in the past [[17](#bib.bib17), [18](#bib.bib18)].
    Overall, the configuration represents a likely attacker/defender language model
    configuration for both being limited by commodity hardware
  prefs: []
  type: TYPE_NORMAL
- en: 0.A.3 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For fine-tuning and experiments, we used subsets of *MS COCO* and *2017 EMNLP
    news excerpts datasets*, provided by the text-generating GAN repository [https://github.com/williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch).
    In addition to that, as described in appendix [0.B.1](#Pt0.A2.SS1 "0.B.1 variation
    of MS COCO dataset ‣ Appendix 0.B DP-GAN with GPT-2 Debugging ‣ Stochastic Parrots
    Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect
    with other LLMs"), we removed the first token from sentences in COCO and rotated
    tokens to remove the first tokens'
  prefs: []
  type: TYPE_NORMAL
- en: 0.A.4 Optimizers and Training Parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For all experiments, we used batch sizes varying from 8 to 32 (with accumulation)
    depending on whether the GPU could support it for training at 14 epochs prior
    to section [5.4](#S5.SS4 "5.4 Running GAN: Hide-and-Seek Between GTP-2 and BERT
    ‣ 5 Results and Discussion ‣ Stochastic Parrots Looking for Stochastic Parrots:
    LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs") and a 1 to 5 epoch
    after that. Prior to section [5.3](#S5.SS3 "5.3 Ensuring Generalizable Generator
    Training ‣ 5 Results and Discussion ‣ Stochastic Parrots Looking for Stochastic
    Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs"), Adam
    optimizer [[32](#bib.bib32)] was used with default parameters of its PyTorch implementation,
    with section [5.3](#S5.SS3 "5.3 Ensuring Generalizable Generator Training ‣ 5
    Results and Discussion ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs
    are Easy to Fine-Tune and Hard to Detect with other LLMs") using AdamW optimizer
    [[40](#bib.bib40)] with default parameters of the PyTorch implementation, and
    a linear learning rate scheduling from 1e-5 to 1e-6 over 10 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: 0.A.5 Reproducibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the experiments were run on a workstation equipped with Intel Core i9-9900K
    (8 cores/16 threads CPU), 64 Gb of RAM clocked at 2666 MHz, 2 TB NVME M.2 SSD,
    and an RTX 3080 graphics cards (10GM VRAM; 29.77 TFLOPS@F16/F32), running an Ubuntu
    20.04 LTS distribution. The evaluations were performed within a Docker container,
    Docker Community Edition, version 20.10.12\. The code used Miniconda version 4.12.0,
    Python 3.9 with CUDA version 11.3.1; PyTorch version 1.10.2, and HuggingFace-hub
    0.5\. The code and instructions to reproduce all experiments can be found at [https://github.com/8a3539f168fd077097ea473cc8a9c093/gpt_bert_gan](https://github.com/8a3539f168fd077097ea473cc8a9c093/gpt_bert_gan).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.B DP-GAN with GPT-2 Debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 0.B.1 variation of MS COCO dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After observing that rewards for ’a’ were very high, we thought that it was
    perhaps the fact that almost all sentences start with the ’a’ token in the MS
    COCO dataset that caused the problem. We tested what would happen if the ’a’ was
    moved to the end of the sample of the dataset to analyze the impact of the position
    of this token (maybe having the same token at the first position very frequently
    on the dataset could lead to dysfunction while training). However, we found the
    same result as above. We also tried to remove the ’a’s entirely (only the ones
    at the beginning of the samples from MS COCO). We observed the same outcome as
    before, ie. a very frequent token in the dataset was being repeated after a few
    epochs (not ’a’ this time but other tokens that were also very frequent in the
    dataset, such as ’the’ or ’woman’).
  prefs: []
  type: TYPE_NORMAL
- en: 0.B.2 lighter GPT-2 and bigger dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We tried to use 12 layers, 6 layers, and 3 layers(and also 12, 6, and 4 heads,
    respectively). The total amount of parameters for each configuration is (approximately)
    therefore 117M, 23M, and 7M, respectively. We have noticed a different behavior
    with 3 layers compared to 12 layers: while the repetitions of some tokens still
    happen after a few epochs with 3 layers, we observed no repetitions of a single
    token compared to the model with 12 layers. We thus think that the size of the
    model is an important factor in the problems that we observed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally tried to see how our model would behave with a bigger dataset. We
    chose to use the EMNLP news dataset, which is 15 times bigger than MS COCO. The
    reason to test on another dataset was not only the size of the dataset but also
    the fact that EMNLP news is more diverse (MS COCO samples are all short descriptions
    and begin with the ’a’ token). We observed an interesting result here. With GPT-2
    with only 6 layers, 30 epochs of adversarial training, and the EMNLP dataset as
    a training dataset: the BLEU score stays the same(see Fig. [7](#Pt0.A2.F7 "Figure
    7 ‣ 0.B.2 lighter GPT-2 and bigger dataset ‣ Appendix 0.B DP-GAN with GPT-2 Debugging
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs")). However, we notice no improvement either.
    One big difference here is that, while GPT-2 (even with 3 layers) is able to memorize(ie.
    reproduce perfectly samples from the training dataset given a prefix from the
    dataset), the MS COCO dataset in not more than 30 epochs, GPT-2 with 6 or 3 layers
    is not able to memorize completely EMNLP news with the same 30 epochs of MLE pre-training
    and without weights initialization from a pre-trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d05f76aa81c73453202d60187bf0b6c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: evolution of 2-gram BLEU score with different datasets and sizes
    of GPT-2'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.C Examples of Samples Generated by Nice GPT-2 Compared to Base GPT-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Warning, some sentences may contain offensive language*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: These samples were obtained by giving the same prefixes from the EMNLP
    news dataset to base GPT-2 and nice GPT-2\. These tables contain the output of
    the models given the prefixes. Also, *"improved"* means here that BERT assigned
    the label "positive" to the generated sample from nice GPT-2, whereas it received
    the label "negative" when generated by base GPT-2 given the same prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: 0.C.1 improved samples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: samples that were improved when generating them using nice GPT-2 compared
    to base GPT-2'
  prefs: []
  type: TYPE_NORMAL
- en: '| base GPT-2 | nice GPT-2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *"sometimes i think about leaving sex work to my children. I think about
    having them and my children and being able to do whatever I want to them. And
    thatś what I think about when Iḿ working with them. I want them to be safe and
    happy"* | *"sometimes i think about leaving sex work at home in the morning, but
    i have found it’s the right way to deal with this. When i first started i had
    no idea what to expect from the sex work i had been working on for the past few
    months. I was"* |'
  prefs: []
  type: TYPE_TB
- en: '| *"I was then stuck in the house with my brother and my sister. It was my
    first time in the house. My sister told me that my brother’s name was Luli and
    he was going to kill me. My sister did not want to hear this because she knew
    I would never"* | *"I was then stuck in the house, in the room of a man whose
    name was Burt and who was the son of a man called J."* |'
  prefs: []
  type: TYPE_TB
- en: '| *"we would open our main presents after lunch, and I’d go back to my room
    to pick up my own thing. I don’t know if I would have done it that way, but I
    can’t think of anything worse than a big pile of things of my own."* | *"we would
    open our main presents after lunch. The menu is a simple yet effective combination
    of delicious sandwiches, salads, and even delicious sandwiches. The menu is filled
    with fresh, locally sourced ingredients, and an extensive menu of seasonal, hand-selected
    selections. Our selection of’ "* |'
  prefs: []
  type: TYPE_TB
- en: '| *"an estimated 80 million people across 20 states and the District of Columbia
    had been affected by Hurricane Sandy, and as the storm continued to hit the Midwest,
    the region was especially affected."Weŕe seeing more and more of this," said Robert
    L. Stryker,"* | *"an estimated 80 million people across 20 states will have access
    to the internet within the next five years. The National Press Club is excited
    about this announcement that the National Press Club is bringing to all of our
    readers," said Chris Johnson, Senior Director of Public Affairs for the"* |'
  prefs: []
  type: TYPE_TB
- en: '| *" the 32 - year - old reality star. The story of a young woman who was living
    in an abusive life in a low-rent, low-temperature home in the foothills of California.
    She had been a woman of good character, and had been at peace"* | *"the 32 - year
    - old reality star who has been a fixture on the big screen since he was 19\.
    However, it is his decision to make his last outing as a celebrity, in the 2008
    thriller, The Big Sick, which he directed, that has received the most buzz"* |'
  prefs: []
  type: TYPE_TB
- en: 0.C.2 not improved samples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: samples that were not improved when generating them using nice GPT-2
    compared to base GPT-2'
  prefs: []
  type: TYPE_NORMAL
- en: '| base GPT-2 | nice GPT-2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *"my sources have suggested that so far the majority of the American population
    has been the product of a combination of a political system that has been largely
    corrupted by the corruption of the people. The fact that this system has succeeded
    in creating an ever-growing majority of the population has been the reason that"*
    | *"my sources have suggested that so far the Syrian Observatory for Human Rights
    has confirmed it has been involved in fighting in the past. The Syrian Observatory
    for Human Rights said that a group of people attacked a house in the town of Al-Wabah,
    near the Turkish border"* |'
  prefs: []
  type: TYPE_TB
- en: '| *"in only one state , utah , was a commoner than the other states . No God
    existed in this world . He was created from the Father of all that was in Him
    and is the same as the Father of all that was in Him and . The Father is the Father
    of all"* | *"in only one state , utah , which is the Hebrew word for "place of
    refuge." But there is no place of refuge in our God. We are not our own land or
    our country, and in the land of my God we are born. But the place of refuge which
    is"* |'
  prefs: []
  type: TYPE_TB
- en: '| *"there was no immediate claim of responsibility for the attack. In May 2017,
    a group of people in Syria, including members of the YPG, were massacred by the
    Syrian army while trying to cross the Euphrates River, a major crossing point
    for refugees. In July, a group"* | *"there was no immediate claim of responsibility
    for this, but many have been killed or wounded by the enemy. There have been two
    other casualties who have been killed by the enemy. The first, a man of the order
    of the Emperor, who had been wounded in a great fire, is still"* |'
  prefs: []
  type: TYPE_TB
- en: Appendix 0.D Architectures for Sentiment and Fake Detection Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 0.D.1 generator training in negativity reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BERT outputs a sentiment, which is a probability of the input sentiment having
    the label positive, negative, or, optionally, neutral. For each sample, we transform
    this sentiment into a score between 0 and 1, by applying the following formula
    $1$2 (where the label is the label that BERT outputs given a sample as input).
    However, due to how GPT-2 works, we transform this score for the whole sample
    into a score for the tokens of the samples by giving the whole sentence-level
    score to each generated token. We then compute a loss between the target of having
    a sample given the label 1 by BERT(perfect score) for each generated token and
    comparing it with the score given to the sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e492340748b3c198c20b330b616dd077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: GPT-2 with BERT trained for sentiment analysis training'
  prefs: []
  type: TYPE_NORMAL
- en: 0.D.2 generator training in fake detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The GPT-2 generator creates samples using prefixes from the dataset, this sample
    is then fed to BERT. BERT outputs a probability of the sample being fake or true,
    we then apply a softmax function to the probability output by BERT to produce
    a value between 0 and 1\. This value is what we call the score here(note the difference
    with the score that we computed in [5.2](#S5.SS2 "5.2 Making Sure a GAN Setting
    Allows the Generator to Train ‣ 5 Results and Discussion ‣ Stochastic Parrots
    Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect
    with other LLMs"). We finally attribute this score for each token generated by
    GPT-2 and compute a loss between the score and the target score consisting of
    only 1s(perfect score) that we use to train GPT-2.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1330df25b6ae4848fdc43edd16662ddc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: GPT-2 training phase of the GAN for fake detection'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.E Training Plots from GPT-2 Fake Detection Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 0.E.1 training with fine-tuned GPT-2 on MS COCO dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we can see in Supplementary Fig. [6](#S5.F6 "Figure 6 ‣ 5.4.1 Training with
    Fine-Tuned Generator ‣ 5.4 Running GAN: Hide-and-Seek Between GTP-2 and BERT ‣
    5 Results and Discussion ‣ Stochastic Parrots Looking for Stochastic Parrots:
    LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs"), the BERT model
    was not able to distinguish generated text from true text, even though there are
    small variations of one or two tokens for each sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Our hypothesis is that, while the loss decreases during training, BERT overfits
    the train set, and the features it uses to distinguish fake from true text do
    not generalize to the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also found while doing this experiment that, when the difference between
    the length of the fake samples and the true samples are different on average,
    BERT is able to pick up on that and have almost 100% accuracy(cf. Supplementary
    Fig. [10](#Pt0.A5.F10 "Figure 10 ‣ 0.E.1 training with fine-tuned GPT-2 on MS
    COCO dataset ‣ Appendix 0.E Training Plots from GPT-2 Fake Detection Training
    ‣ Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune
    and Hard to Detect with other LLMs")). The setup with GPT-2 and BERT seems tricky
    to train. Indeed, when GPT-2 outputs sampling resembling too much that of the
    dataset(variation of one of two tokens for example as for the above experiment),
    BERT is not able to distinguish true and fake data at all. When GPT-2 outputs
    samples that are too different from that of the dataset, BERT achieves almost
    100% accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c7f68543fa00cddc6f47ce463244d6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: BERT accuracy on the validation set when the samples from GPT-2
    are not of the same length as the dataset on which BERT has been trained'
  prefs: []
  type: TYPE_NORMAL
- en: 0.E.2 training without fine-tuned GPT-2 on MS COCO dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In that scenario, BERT was able to distinguish with relatively high accuracy(about
    90% on average) fake from true samples(see Supplementary Fig. [11](#Pt0.A5.F11
    "Figure 11 ‣ 0.E.2 training without fine-tuned GPT-2 on MS COCO dataset ‣ Appendix
    0.E Training Plots from GPT-2 Fake Detection Training ‣ Stochastic Parrots Looking
    for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other
    LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/446521bee0e0fc26ebfa985d267a672a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: BERT accuracy on the validation set when GPT-2 is not fine-tuned
    and length of GPT-2’s output are adjusted to match the true dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, GPT-2 was not able to train well. As we can see in Supplementary Fig.
    [12](#Pt0.A5.F12 "Figure 12 ‣ 0.E.2 training without fine-tuned GPT-2 on MS COCO
    dataset ‣ Appendix 0.E Training Plots from GPT-2 Fake Detection Training ‣ Stochastic
    Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to
    Detect with other LLMs"), although in some epochs GPT-2 is able to produce more
    samples that fool BERT, in general, it is not able to and sometimes it gets worse(other
    runs gave similar results if not worse). Thus, in this setup, GPT-2 is not able
    to learn how to evade BERT fine-tuned for fake detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0945769e89ef350b53889d471f3d912b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: GPT-2 training with scores from BERT fine-tuned for fake detection
    with MS COCO dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 0.E.3 training with partially fine-tuned GPT-2 on EMNLP news + MS COCO dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the last epoch of training, about 40% of samples generated by GPT-2, using
    prompts from the dataset, were classified as true (which we can compare with the
    88% accuracy of BERT for the validation dataset before adversarial training).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed47627406fc923639fca249b87037ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: GPT-2 training with scores from BERT fine-tuned for fake detection
    with EMNLP news dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.F Examples of Samples by GPT-2 during the Fake GAN Training and Score
    Given
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note: a score is a number between 0 and 1, which is computed by applying the
    softmax function to the probability of giving a positive score that BERT outputs
    given a sample as an input. This means that a score close to 1 means that BERT
    attributes a good probability that the sample is a true sample from the dataset,
    whereas a score close to 0 means that it assigns a low probability of being a
    true sample.'
  prefs: []
  type: TYPE_NORMAL
- en: We also provided the corresponding sample in the dataset(created in the discriminator
    training phase) that has been given the label 1(true samples from a dataset) and
    the one that has been given the label 0(generated sample).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: samples generated by GPT-2 during fake GAN training and respective
    score'
  prefs: []
  type: TYPE_NORMAL
- en: '| bad score($<0.5$) |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| *"a bicycle replica with a clock as the base. The replica will be sold"*  score:
    0.0057 |'
  prefs: []
  type: TYPE_TB
- en: '| label 0: *"a bicycle replica with a clock as the baseplate. Another"* |'
  prefs: []
  type: TYPE_TB
- en: '| label 1: *"a bicycle replica with a clock as the front wheel."* |'
  prefs: []
  type: TYPE_TB
- en: '| *"a car that seems to be parked illegally in a Houston suburb. A car"*  score:
    0.0057 |'
  prefs: []
  type: TYPE_TB
- en: '| label 0: *"a car that seems to be parked illegally. A young"* |'
  prefs: []
  type: TYPE_TB
- en: '| label 1: *"a car that seems to be parked illegally behind a legally parked
    car"* |'
  prefs: []
  type: TYPE_TB
- en: '| *"a black honda motorcycle parked in front of a hotel in Seoul, South Korea"*  score:
    0.0633 |'
  prefs: []
  type: TYPE_TB
- en: '| label 0: *"a black honda motorcycle parked in front of the garage"* |'
  prefs: []
  type: TYPE_TB
- en: '| label 1: *"a black honda motorcycle parked in front of a garage."* |'
  prefs: []
  type: TYPE_TB
- en: '| good score($\geq 0.5$) |'
  prefs: []
  type: TYPE_TB
- en: '| *"a car that seems to be parked illegally near the Trump Tower in New York"*  score:
    0.794 |'
  prefs: []
  type: TYPE_TB
- en: '| label 0: *"a car that seems to be parked illegally. A young"* |'
  prefs: []
  type: TYPE_TB
- en: '| label 1: *"a car that seems to be parked illegally behind a legally parked
    car"* |'
  prefs: []
  type: TYPE_TB
- en: '| *"a honda motorcycle parked in a grassy area in this file photo in Oakland"*  score:
    0.976 |'
  prefs: []
  type: TYPE_TB
- en: '| lable 0: *"a honda motorcycle parked in a grassy area near the"* |'
  prefs: []
  type: TYPE_TB
- en: '| label 1: *"true: a honda motorcycle parked in a grass driveway"* |'
  prefs: []
  type: TYPE_TB
- en: Appendix 0.G Example of Samples Generated by GPT-2 during Fake GAN Training
    with EMNLP News + MS COCO Dataset after one Epoch of Adversarial Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| a bicycle replica with a clock as the:: the at:: into:::, in as a -: for:
    around \xad: it at of said more said: ’ on the: the the:: reportedly said,:- police,
    \xad: at: the said, |'
  prefs: []
  type: TYPE_TB
- en: '| a black honda motorcycle parked in front for on an said: the said: \n, said
    said,,,:, but: as -, a in said the not a. said:: have had, in had in.: and: at,
    -: " in -,’ |'
  prefs: []
  type: TYPE_TB
- en: '| a room with blue walls and a white::::: said \xad: on the, have it: said,::
    after \n: -,. more::.: in in, in: not:, \n to said at \xad:: of.:. said: |'
  prefs: []
  type: TYPE_TB
