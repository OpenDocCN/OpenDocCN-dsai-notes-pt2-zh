- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17706](https://ar5iv.labs.arxiv.org/html/2406.17706)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feijie Wu Purdue University [wu1977@purdue.edu](mailto:wu1977@purdue.edu) , 
    Zitao Li Alibaba Group [zitao.l@alibaba-inc.com](mailto:zitao.l@alibaba-inc.com)
    ,  Yaliang Li Alibaba Group [yaliang.li@alibaba-inc.com](mailto:yaliang.li@alibaba-inc.com)
    ,  Bolin Ding Alibaba Group [bolin.ding@alibaba-inc.com](mailto:bolin.ding@alibaba-inc.com)
     and  Jing Gao Purdue University [jinggao@purdue.edu](mailto:jinggao@purdue.edu)(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) show amazing performance on many domain-specific
    tasks after fine-tuning with some appropriate data. However, many domain-specific
    data are privately distributed across multiple owners. Thus, this dilemma raises
    the interest in how to perform LLM fine-tuning in federated learning (FL). However,
    confronted with limited computation and communication capacities, FL clients struggle
    to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient
    LLM fine-tuning approach to FL. Specifically, our method involves the server generating
    a compressed LLM and aligning its performance with the full model. Subsequently,
    the clients fine-tune a lightweight yet important part of the compressed model,
    referred to as an adapter. Notice that as the server has no access to the private
    data owned by the clients, the data used for alignment by the server has a different
    distribution from the one used for fine-tuning by clients. We formulate the problem
    into a bi-level optimization problem to minimize the negative effect of data discrepancy
    and derive the updating rules for the server and clients. We conduct extensive
    experiments on LLaMA-2, empirically showing that the adapter has exceptional performance
    when reintegrated into the global LLM. The results also indicate that the proposed
    FedBiOT significantly reduces resource consumption compared to existing benchmarks,
    all while achieving comparable performance levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Federated Learning; Large Language Models^†^†journalyear: 2024^†^†copyright:
    rightsretained^†^†conference: Proceedings of the 30th ACM SIGKDD Conference on
    Knowledge Discovery and Data Mining; August 25–29, 2024; Barcelona, Spain^†^†booktitle:
    Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain^†^†doi: 10.1145/3637528.3671897^†^†isbn:
    979-8-4007-0490-1/24/08^†^†ccs: Computing methodologies Distributed algorithms^†^†ccs:
    Computing methodologies Natural language generation^†^†ccs: Information systems Language
    models'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recent advancements in large language models (LLMs) have demonstrated incredible
    performance in various tasks, such as question-answering and problem-solving.
    This success owes to the pretraining on large datasets, covering a wide range
    of linguistic patterns and general knowledge. However, in specific domains such
    as legal advice (Nay et al., [2024](#bib.bib24); Cui et al., [2023](#bib.bib8))
    and medical diagnosis (Thirunavukarasu et al., [2023](#bib.bib33); Singhal et al.,
    [2023](#bib.bib28); Wang et al., [2023c](#bib.bib40)), LLMs may not provide professional
    responses because the terminology and context significantly differ from general
    language use. To address this limitation and enable the generation of domain-specific
    content, it becomes imperative to fine-tune LLMs with relevant data. This fine-tuning
    process allows the models to learn from the specific instances and nuances of
    the target application, ensuring their capability within specialized fields. The
    quality and quantity of the task-specific data are directly related to the performance
    of the fine-tuned model on downstream tasks: large and well-labeled data can significantly
    improve the model, while small and irrelevant data can only benefit the model
    marginally. However, there are many cases where task-specific data are possessed
    by multiple data parties, while each of them may have a limited number of samples
    that can be used to fine-tune LLMs. For example, a hospital in a rural area may
    only have a limited number of lung cancer cases recorded in its own system; if
    an LLM is only fine-tuned on one set of those cases, it may not obtain comprehensive
    knowledge and easily be overfitted.'
  prefs: []
  type: TYPE_NORMAL
- en: To incorporate all the distributed data in the fine-tuning of LLMs, one may
    consider the batch fine-tuning as follows. If we demand all the data owners to
    share their data with the LLM server, then LLM fine-tuning could be conducted
    at the server side. For example, some LLM owners offer fine-tuning APIs as services,
    but the users must pack their data as files and upload them to use a black-box
    fine-tuning (OpenAI, [2023](#bib.bib25)). Apparently, this setup is not applicable
    to users who have privacy concerns. Especially, some businesses are subject to
    data privacy regulations (GDPR, [2016](#bib.bib9); CCPA, [2023](#bib.bib3)), which
    makes it challenging to share local data with LLM server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a more practical setting is to let individual data owners keep their
    data locally, run fine-tuning locally and aggregate the fine-tuning results at
    the LLM server. This fits well the *federated learning* (FL) framework, which
    is a distributed paradigm that places a paramount emphasis on preserving privacy.
    Its conventional algorithms, such as FedAvg (McMahan et al., [2017](#bib.bib23)),
    are considered practical solutions to overcome data barriers across different
    data owners. In this paradigm, data owners are treated as clients, and an LLM
    server coordinates the computation. The standard FL workflow involves three steps
    repeatedly: (i) The server distributes the global model to all clients; (ii) Each
    client trains the model locally for multiple iterations and sends the updated
    model to the server; (iii) The server aggregates the models from the clients and
    updates the global model accordingly. Despite the potential of this method to
    facilitate collaborative fine-tuning of an LLM without sharing local data, its
    feasibility is hindered by two main limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Access to full model of state-of-the-art LLMs: There exist some open-source
    LLMs whose model the public can download and have full access to their parameters.
    However, the most recent and powerful versions of LLMs are usually closed-sourced,
    i.e, the architecture and parameters are not available to the public. The best
    closed-source LLMs still have leading performance on a wide range of language
    tasks, and its leading edge can be maintained or even enhanced after fine-tuning,
    making it a better choice. As aforementioned, using the blackbox fine-tuning service
    provided by these closed-source LLMs often violates data users’ privacy requirements.
    Therefore, a federated learning framework that conducts collaborative fine-tuning
    with the assumption of no access to the full model of LLMs at the client side
    is more desirable.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computation and communication costs: Existing federated learning framework
    could also suffer from the computation and communication challenges when conducting
    collaborative fine-tuning on LLMs. The fine-tuning process for LLMs entails substantial
    computational demands and communication costs due to the vast number of trainable
    model parameters. Clients with limited computational power may struggle to perform
    complex model updates, leading to prolonged training times or potential disruptions.
    The transfer of expensive models between the server and the clients also incurs
    substantial communication costs, leading to substantial bandwidth consumption
    and increased communication latency. At the server side, there could be network
    congestion when clients send back their updated huge amount of parameters concurrently.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this paper, we aim to tackle these two challenges and propose to design an
    effective and practical collaborative LLM fine-tuning framework. To address the
    first challenge, We follow the setting proposed in offsite-tuning (Xiao et al.,
    [2023](#bib.bib44)) and its federated version FedOT (Kuang et al., [2024](#bib.bib14)).
    We assume that the LLM owner does not collect data directly from clients but serves
    as the server in FL, who can use a public dataset to distill her LLM and aggregate
    some local updates on part of the model from clients; multiple clients want to
    collaborate on fine-tuning for similar downstream tasks. Different from the classic
    FL setting (Zhang et al., [2021](#bib.bib48); Wang et al., [2023a](#bib.bib36);
    Lin et al., [2020](#bib.bib19)), we do not assume the data distribution on clients
    or the public data owned by the server to be the same. Our goal, in general, is
    to provide a framework for collaborative clients to fine-tune without access to
    full LLM or sharing local data directly. More importantly, the fine-tuned model
    can still achieve better performance than fine-tuning LLM locally with their local
    data exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although FedOT (Kuang et al., [2024](#bib.bib14)) was developed for this objective,
    it could incur significant computational and communication costs, thereby suffering
    from the second challenge. In light of this challenge, we propose to integrate
    various parameter-efficient fine-tuning (PEFT) techniques into the proposed FL
    framework. Specifically, the server employs linear dropout to compress the LLM,
    integrates LoRA (Hu et al., [2021](#bib.bib12)) to reduce the trainable parameters,
    and divides it into two components: an emulator and an adapter. The emulator retains
    a consistent representation of the raw model on the server’s dataset, while the
    adapter assimilates domain-specific linguistic patterns from the clients’ local
    datasets. Considering the significant distribution shift between the clients’
    datasets and the server’s dataset, we separate the fine-tuning of these two components
    into two processes during FL training, i.e., the clients perform multiple local
    updates to fine-tune the adapter, and the server distill the emulator from the
    original LLM while aggregating the updated adapters from the clients. To this
    end, a bi-level optimization is formulated.'
  prefs: []
  type: TYPE_NORMAL
- en: This design, named Federated Bi-level Offsite Tuning (FedBiOT), offers twofold
    advantages from the clients’ perspectives. Firstly, instead of loading the complete
    model, clients load a compressed version with fewer layers, considerably reducing
    computation costs. Secondly, clients exclusively fine-tune the adapter, affecting
    only the last few layers of the LLM and thereby minimizing computation and communication
    expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Throughout the paper, our contributions are highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose an algorithm FedBiOT that avoids full model fine-tuning and significantly
    reduces the communication and computation overhead. To the best of our knowledge,
    this is the first work that addresses both the aforementioned two challenges in
    the federated LLM fine-tuning framework. With our proposed framework, clients’
    data are ensured to be kept locally and computation and communication burden is
    significantly reduced.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We formulate a bi-level optimization problem that enables the LLM fine-tuning
    without access to the full model. By partitioning the compressed model into the
    adapter and the emulator, the emulator acts as a simulator of the original raw
    model, while the adapter adeptly learns domain-specific linguistic patterns with
    clients’ local datasets. To this end, we realize that fine-tuning the compressed
    model is equivalent to the refinement of the counterpart of the complete LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive experiments on LLaMA-2 for fine-tuning with three tasks,
    i.e., code generating, math problem solving, and question answering. The empirical
    studies also demonstrate that the proposed approach has significant improvement
    over all these tasks compared with the baseline approaches in terms of computation
    and communication overheads and final accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Traditional FL Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider there is an FL system with a total of $M$. Each client $m\in[M]$. A
    client’s local loss is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $F_{m}(\boldsymbol{w}):=\frac{1}{&#124;\mathcal{D}_{m}&#124;}\sum_{(\boldsymbol{x},\boldsymbol{y})\in\mathcal{D}_{m}}f\left(\mathcal{M}(\boldsymbol{x};\boldsymbol{w});\boldsymbol{y}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{M}(\boldsymbol{x};\boldsymbol{w})$ and an input $\boldsymbol{x}$
    is defined on the model output and the ground truth $\boldsymbol{y}$ is part of
    the input $\boldsymbol{x}$, where a sequence of tokens in the input is used to
    predict the next token, and the ground truth is used to identify the part needing
    to be predicted by the model. Such a dataset is commonly adopted in previous works
    to fine-tune an LLM (Wei et al., [2021](#bib.bib41); Ouyang et al., [2022](#bib.bib26)).
    Then, based on the definition, a conventional FL system aims to find an optimal
    model across all clients, which is formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\min_{\boldsymbol{w}\in\mathbb{R}^{d}}F(\boldsymbol{w})=\sum_{m\in[M]}p_{m}F_{m}(\boldsymbol{w}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $p_{m}=|\mathcal{D}_{m}|/|\mathcal{D}|$, where $\mathcal{D}$. Generally,
    this problem can be optimized by different FL algorithms (Li et al., [2019](#bib.bib16);
    Karimireddy et al., [2020](#bib.bib13); Wang et al., [2020](#bib.bib39)) repeating
    the following paradigm until convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 1: At the beginning of each round $t$;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: After receiving the model $\boldsymbol{w}^{(t)}$ performs multi-step
    local updates on $\boldsymbol{w}^{(t)}$;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: The server collects the locally updated model parameters $\boldsymbol{w}^{(t)}_{m}$
    for next round.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Applying PEFT to federated LLM fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The existing FL algorithms (Acar et al., [2020](#bib.bib2); Wu et al., [2023](#bib.bib43);
    He et al., [2023](#bib.bib10); Wang et al., [2023b](#bib.bib37), [2022](#bib.bib38))
    are confronted with computation and communication bottlenecks when fine-tuning
    an LLM. To mitigate the limitations, researchers have extended existing parameter-efficient
    fine-tuning (PEFT) approaches to FL, named FedPEFT (Yi et al., [2023](#bib.bib46);
    Zhang et al., [2023](#bib.bib50); Sun et al., [2024](#bib.bib31)). These methods
    minimize the number of trainable parameters by introducing a PEFT module and keeping
    the original LLM parameters unchanged. By focusing local updates exclusively on
    the PEFT module rather than the entire model, these methods effectively reduce
    computational load and support larger batch sizes on a single GPU. Additionally,
    the FL server merely aggregates the updated parameters of a given model, thus
    obviating the need to transmit unchanged parameters and minimizing communication
    overheads.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, FedPEFT is still confronted with the intrinsic challenge wherein
    clients face obstacles in loading an LLM due to its substantial computation prerequisites.
    For instance, the loading of a full-precision LLaMA-2-7B necessitates a memory
    capacity of no less than 28 GB.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The era of LLM poses the necessity of model privacy protection, where the details
    of LLM cannot be visible to the clients. To this end, Xiao et al. ([2023](#bib.bib44))
    proposes a method named Offsite-tuning under the scenario where there is a server
    (a.k.a. LLM owner) and a client, while Kuang et al. ([2024](#bib.bib14)) extends
    this work to an FL version and names it as FedOT. They achieve model privacy protection
    by compressing the model, where only some layers are visible to the clients. However,
    these works require the preservation of a large number of layers to guarantee
    the performance, hindering the effectiveness of model privacy protection. In contrast,
    our work only discloses a few model parameters of the original LLM to the clients,
    i.e., the clients only know the adapter parameters that come from the original
    LLM, while the emulator parameters have been updated and different from the original
    LLM. Besides, neither offsite-tuning (Xiao et al., [2023](#bib.bib44)) nor FedOT
    (Kuang et al., [2024](#bib.bib14)) consider the difference between alignment data
    on the server and the fine-tuning data on clients. In contrast, the bi-level optimization
    problem proposed in our work naturally considers this factor and we design updating
    rules based on it.
  prefs: []
  type: TYPE_NORMAL
- en: Black-box is also a practical way to protect model privacy, where the clients
    access the LLM via an API, and they cannot fine-tune the LLM. Therefore, the optimization
    solely relies on prompt-based learning (Sordoni et al., [2023](#bib.bib29); Li
    and Liang, [2021](#bib.bib17); Lester et al., [2021](#bib.bib15); Liu et al.,
    [2023](#bib.bib21)). In the context of FL, there are two typical works, namely,
    Fed-BBPT (Lin et al., [2023](#bib.bib20)) and FedBPT (Sun et al., [2023](#bib.bib30)).
    These two works guarantee the model privacy in FL, but they should transmit the
    prompt together with the input to the LLM owner, leading to concerns about data
    privacy when the input contains sensitive information, violating the requirement
    of FL. In contrast, the proposed FedBiOT will not lead to this concern because
    its training is fully on the clients such that the data are never shared with
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. FedBiOT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that some clients may be unable to load a complete LLM, this section introduces
    an algorithm designed to enable these clients to fine-tune the LLM without requiring
    access to its full version. In other words, our goal is to refine the part of
    a compressed model that should yield performance comparable to fine-tuning its
    counterpart within a full model. To accomplish this, the server initially compresses
    the LLM and divides it into two distinct components, each serving specific functions.
    The first component, termed an emulator, is tasked with replicating the behavior
    of the uncompressed LLM. The second component, referred to as an adapter, focuses
    on adeptly acquiring domain-specific linguistic patterns from clients. Upon reintegrating
    the adapter into the uncompressed emulator, its performance should demonstrate
    significant improvement compared to the original LLM.
  prefs: []
  type: TYPE_NORMAL
- en: However, direct fine-tuning of the adapter on its models presents two significant
    limitations. Firstly, given that a single layer of a large language model (LLM)
    comprises millions of parameters, such as the decoder layer of LLaMA-2 with 202
    million parameters, the adapter’s parameter count is immense. This necessitates
    clients to possess powerful computational equipment to handle the fine-tuning
    of the layer. Additionally, transmitting the layer updates to the server poses
    another bottleneck, particularly in scenarios with unreliable network connections
    or limited bandwidth, hindering the smooth transmission of updates to the server.
  prefs: []
  type: TYPE_NORMAL
- en: To address these constraints, we integrate LoRA (Hu et al., [2021](#bib.bib12)),
    a PEFT module, into our proposed method. LoRA significantly reduces the number
    of tunable parameters, with a LoRA module for LLaMA-2 comprising 0.13 million
    trainable parameters, which is merely 0.06% of the original layer’s size. Consequently,
    the communication cost experiences a remarkable reduction of 99.94% compared to
    transmitting a full layer.
  prefs: []
  type: TYPE_NORMAL
- en: Organization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the subsequent sections, we will delve into the concrete details of the
    algorithm design. Specifically, Section [3.1](#S3.SS1 "3.1\. Compressed Model
    Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in
    Federated Learning without Full Model") illustrates how the compressed model is
    prepared. Following that, Section [3.2](#S3.SS2 "3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") discusses the problem formulation for the aforementioned
    objectives. On top of this, Section [3.3](#S3.SS3 "3.3\. Client Updates ‣ 3\.
    FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model")
    and Section [3.4](#S3.SS4 "3.4\. Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM
    Local Fine-tuning in Federated Learning without Full Model") outline the detailed
    steps of the proposed algorithm, namely local updates and server aggregation,
    showcasing the seamless integration of LoRA modules. Full implementation of the
    pseudocode is given in Algorithm [2](#alg2 "Algorithm 2 ‣ Discussion. ‣ 3.2\.
    Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model").'
  prefs: []
  type: TYPE_NORMAL
- en: '3.1\. Compressed Model Preparation: Linear Dropout'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose a pre-trained LLM has a total of $n$ layers of transformers to form
    a submodel. We denoted this by a function $\textsf{LayerExtract}(\mathcal{M},L)$
    from the model $\mathcal{M}$. The function consists of the following three steps,
    and its pseudocode implementation of the first two steps is presented in Algorithm
    [1](#alg1 "Algorithm 1 ‣ Step 2: Layer dropout to form emulator. ‣ 3.1\. Compressed
    Model Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Identify the adapters in the original model.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We choose the bottom few layers¹¹1The bottom/last layers refer to the transformer
    decoders near the output, while the top/first layers refer to the part close to
    the input. of the original LLM as the adapter. To be more specific, suppose the
    size of the adapter is $a$. Therefore, $\mathcal{A}\leftarrow\textsf{LayerExtract}(\mathcal{M},L_{\mathcal{A}})$.
    We denote $\boldsymbol{w}_{\mathcal{A}}$. The $\boldsymbol{w}_{\mathcal{A}}$.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of adapters brings two advantages. First, regarding the computation
    constraints of the clients, this proposed adapter is computation-efficient because
    it only needs to store the activations of transformers in the last few layers,
    leading to a lower memory cost. Second, as the adapter focuses more on domain-specific
    features, it is eco-friendly to spend the effort fine-tuning the last few layers.
    The conclusion is drawn from a well-known finding (Yosinski et al., [2014](#bib.bib47))
    in neural networks that the first few layers tend to learn general features while
    the last layers encode specific ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Layer dropout to form emulator.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by the experimental results presented by Xiao et al. ([2023](#bib.bib44)),
    we form an emulator by means of a uniform layer dropout (Sajjad et al., [2023](#bib.bib27))
    from the remaining part $\mathcal{E}^{*}$. Denote there are $n_{\mathcal{E}^{*}}$.
    The dropout rate of the emulator is denoted as $\beta=\frac{|L_{\mathcal{E}}|}{n_{\mathcal{E}^{*}}}$
    as emulator and $\mathcal{E}^{*}$ and $\boldsymbol{w}_{\mathcal{E}^{*}}$ and $\mathcal{E}^{*}$,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, we can attain two combined models, namely, Adaptor + Emulator
    (AdapEmu, i.e., $\mathcal{E}\circ\mathcal{A}$). As Xiao et al. ([2023](#bib.bib44))
    describes, AdapFu performs better than AdapEmu. These two models have different
    functionalities in real-world scenarios: AdapEmu is adopted if the input contains
    sensitive information that cannot be shared with the LLM owner, e.g., drafting
    a petition letter, while AdapFu is adopted when the users aim to have better generation
    results, e.g., solving a math problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 LayerExtract
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: pre-trained LLM $\mathcal{M}$, dropout rate $\beta$.'
  prefs: []
  type: TYPE_NORMAL
- en: '1:Get the size of model: $n\leftarrow|\mathcal{M}|$2:Compute the number of
    layers in the compressed model, i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle n^{\prime}\leftarrow\lfloor\beta(n-s)\rfloor$ |  |'
  prefs: []
  type: TYPE_TB
- en: 3:Initialize non-compressed emulator $\mathcal{E}^{*}\leftarrow\{\mathcal{M}_{0},\dots\mathcal{M}_{n-s-1}\}$4:Initialize
    emulator $\mathcal{E}\leftarrow\{\}$6:for $j=0,\dots,n^{\prime}-1$ to emulator,
    i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{E}\leftarrow\mathcal{E}\cup\{\mathcal{M}_{\lfloor
    j\times stride\rfloor}\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 8:end for9:return $\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}},\boldsymbol{w}_{\mathcal{E}^{*}}$![Refer
    to caption](img/fb78219d4e0cfd52dbf62ff7cf6f95f9.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. The Workflow of FedBiOT during the FL training
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Pre-alignment.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before the FL training stage, we pre-align the emulator with the non-compressed
    one such that it can mimic the performance of the raw model. Assume there is a
    public dataset $\mathcal{D}_{public}$, representing the input and the ground truth,
    respectively. Therefore, in the rest of the section, we assume the input $\boldsymbol{x}$
    contains an attention mask that can identify the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of training the compressed model with the ground truth, we utilize
    knowledge distillation (Hinton et al., [2015](#bib.bib11)) to transfer the general
    linguistic patterns from the original LLM to the compressed one by tuning the
    emulator $\mathcal{E}$-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\displaystyle\mathcal{L}_{repr}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Additionally, we ensure the compressed model has consistent final outputs of
    the original LLM on the ground truth by minimizing the following KL divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle\mathcal{L}_{kd}$ |  |'
  prefs: []
  type: TYPE_TB
- en: In a nutshell, we optimize the emulator $\mathcal{E}$ by finding the optimal
    parameters for the following equation on the public dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Let the optimal emulator $\mathcal{E}$ with the parameter of $\boldsymbol{w}_{\mathcal{E}_{init}}$
    with the parameter of $\boldsymbol{w}_{\mathcal{A}_{init}}$. To reduce the computation
    and communication costs, we incorporate LoRA (Hu et al., [2021](#bib.bib12)) for
    the adapter $\mathcal{A}$, denoted as $\mathcal{A}_{lora}$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into the details of the proposed FedBiOT, we briefly go through
    the workflow as described in Figure [1](#S3.F1 "Figure 1 ‣ Step 2: Layer dropout
    to form emulator. ‣ 3.1\. Compressed Model Preparation: Linear Dropout ‣ 3\. FedBiOT
    ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model"). The
    figure visually presents the workflow of the federated learning process of our
    proposed FedBiOT, including the local updates on clients (Section [3.3](#S3.SS3
    "3.3\. Client Updates ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")) and the aggregation on the server (Section [3.4](#S3.SS4
    "3.4\. Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")). At the beginning of clients’ fine-tuning, the
    server broadcasts the adapter $\mathcal{A}_{lora}$ to the clients. Subsequently,
    the clients perform multiple local updates to fine-tune the adapter $\mathcal{A}_{lora}$
    to the server, and the server thereby aggregates the adapters. To ensure that
    the emulator is still able to reproduce the behavior of the uncompressed LLM,
    the server fine-tunes the emulator $\mathcal{E}_{lora}$ with the public dataset.
    Finally, the server distributes the updated parameters to the clients and launches
    a new round of training.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Formulation of Bi-level Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Section [3.1](#S3.SS1 "3.1\. Compressed Model Preparation:
    Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"), we compress and divide the LLM into two parts, namely, an
    adapter and an emulator. These two components are designated to satisfy the following
    objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emulator should be tuned towards perfectly imitating the non-compressed part
    in the full model, especially in extracting and encoding information on the server’s
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapter should be able to digest the output of the emulator efficiently and
    should be encoded with the knowledge from clients’ datasets effectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define $\boldsymbol{w}_{\mathcal{A}}=\{\boldsymbol{w}_{\mathcal{A}_{init}},\boldsymbol{w}_{\mathcal{A}_{lora}}\}$
    to integrate the LoRA parameters while the initial parameters for the adapter
    and the emulator remain unchanged during the training. Toward the goal, we formulate
    the objectives as a bi-level optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.\quad$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathcal{L}(\boldsymbol{x})\overset{\triangle}{=}\left\&#124;\mathcal{E}\left(\boldsymbol{x};\boldsymbol{w}_{\mathcal{E}}\right)-\mathcal{E}^{*}\left(\boldsymbol{x};\boldsymbol{w}_{\mathcal{E}^{*}}\right)\right\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (7) |  |  | $\displaystyle\qquad\quad+\lambda\cdot D_{KL}\left(\mathcal{M}(\boldsymbol{x};\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}^{*}}\})\&#124;\mathcal{M}(\boldsymbol{x};\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}}\})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{w}_{\mathcal{A}_{lora}}^{(t)}$ reconstructs for the same
    size of the adapter $\boldsymbol{w}_{\mathcal{A}}$ represents the public dataset
    on the server, which can be unlabeled. $D_{KL}(\cdot\|\cdot)$ and $\lambda$ are
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The upper-level objective (Equation ([6](#S3.E6 "In 3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"))).'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The upper-level objective function consists of two terms. The first term represents
    the loss of the model on local clients’ data, with the current emulator and adapter.
    It follows a classic weighted average loss in FL to balance the loss of different
    clients’ heterogeneous local data. The goal of introducing this term is straightforward:
    by minimizing the loss of the first term, we expect the emulator-adapter combination
    to be improved on the local training set. The second term is a regularization
    of the adapter component to ensure it will be within a reasonable distance from
    the synchronized and broadcast adapter at the beginning of each communication
    round. Enforcing a restriction on the adapter’s change can reduce the difference
    of losses for the emulator distillation after locally adapter are tuned locally
    on clients, so it can help the convergence of emulator distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower-level objective (Equation ([7](#S3.E7 "In 3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"))).'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first term in the constraint is the $\ell_{2}$-norm difference between the
    activation output by the emulator and the full model. The second term is the KL
    divergence between the output of output distribution of the full model-adapter
    combination and the emulator-adapter. Although only the emulator is trainable
    to minimize the loss of these two terms, these two terms provide different optimization
    meaning for the emulator. The first term encourages the emulator to provide activations
    as close as possible to the full model, *excluding* the effect of the adapter.
    The second term ensures the emulator can provide output distributions close to
    the one when the full model with adapters is added on.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The introduced algorithm can optimize the bi-level problems (i.e., Equation
    ([6](#S3.E6 "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model")) and ([7](#S3.E7
    "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model"))) to an equilibrium point
    for both adapter and emulator. This is because when we optimize the adapter, the
    fixed emulator constrains its updates, and vice versa, and thereby, the emulator
    and adapter are distilled or trained interchangeably. At this equilibrium, the
    emulator can more faithfully extract and encode the information for the clients’
    dataset and benefit from the training of the adapter in reverse.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, FedBiOT does not require the design of an emulator to follow
    linear dropout. Instead, this is a general framework that compresses an LLM and
    divides it into two components: an emulator and an adapter. There are numerous
    designs for the emulator, but they share the same objective where the emulator
    simulates the non-compressed part of an LLM. For simplicity, we follow offsite-tuning
    (Xiao et al., [2023](#bib.bib44)) and prepare the emulator by means of uniform
    layer dropout (Sajjad et al., [2023](#bib.bib27)) to demonstrate the effectiveness
    of FedBiOT.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 FedBiOT
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: learning rate $\eta$, global model alignment steps $E$, local update
    regularization $\epsilon$, pre-trained LLM $\mathcal{M}$, adapter size $s$, number
    of clients $M$.'
  prefs: []
  type: TYPE_NORMAL
- en: '1:$\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}},\boldsymbol{w}_{\mathcal{E}^{*}}\leftarrow\textsf{LayerExtract}(\mathcal{M},s,\beta)$
    See Algo. [1](#alg1 "Algorithm 1 ‣ Step 2: Layer dropout to form emulator. ‣ 3.1\.
    Compressed Model Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model") for details2:for $t=0,\dots,R-1$ do4:         Randomly
    sample $(\boldsymbol{x},\boldsymbol{y})$5:         Optimize $\boldsymbol{w}_{\mathcal{E}_{lora}}$
    with clients $m\in[M]$ in parallel do9:         Initialize $\boldsymbol{w}_{\mathcal{A}_{lora}}^{(t)}$,
    and $\boldsymbol{w}_{\mathcal{E}}$ do11:              Compute a gradient $g$ using
    Equation ([10](#S3.E10 "In 3.3\. Client Updates ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model"))13:         end for14:         Communicate
    $\boldsymbol{w}_{\mathcal{A}_{lora},m}$ using Equation ([11](#S3.E11 "In 3.4\.
    Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model"))17:end for18:return AdapEmu $\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}}\}$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Client Updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the local updates, the clients barely fine-tune the parameters of the
    adapter $\mathcal{A}$. By enabling LoRA, the LoRA of the adapter will get updated,
    and therefore, the clients should upload the updated $\boldsymbol{w}_{\mathcal{A}_{lora}}$
    to the server after the local fine-tuning ends.
  prefs: []
  type: TYPE_NORMAL
- en: Consider client $i\in[M]$-th round. Before optimizing the adapter locally, the
    client receives the updated emulator $\boldsymbol{w}_{\mathcal{E}_{lora}}$ from
    the client, and we denote them by
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Suppose the client performs the local update for $K$, a LoRA module of the
    adapter. Therefore, based on Equation ([6](#S3.E6 "In 3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model")), the gradient w.r.t. $\boldsymbol{w}_{\mathcal{A}_{lora},m}$
    should be'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{w}_{\mathcal{A}}=\{\boldsymbol{w}_{\mathcal{A}_{init}},\boldsymbol{w}_{\mathcal{A}_{lora},m}\}$
    be the optimizer (e.g., SGD and AdamW (Loshchilov and Hutter, [2018](#bib.bib22)))
    that updates the model parameters, and $\eta$ be the learning rate. Therefore,
    in each local update, the local model is updated for
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\displaystyle\boldsymbol{w}_{\mathcal{A}_{lora},m}\leftarrow\textsc{Optim}(\boldsymbol{w}_{\mathcal{A}_{lora},m},g,\eta)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: After finishing the local update, the client $i$ to the server.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Model Aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the server aggregation, the server performs the weighted average to update
    the adapters $\mathcal{A}$. By enabling the LoRA, only the parameters $\boldsymbol{w}_{\mathcal{A}_{lora}}$
    in the emulator are updated, while the rest (i.e., $\boldsymbol{w}_{\mathcal{A}_{init}}$)
    remain unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the server collects a set of updated LoRAs of the adapter, i.e., $\left\{\boldsymbol{w}_{\mathcal{A}_{lora},m}\right\}_{m\in[M]}$
    from the clients. Based on the definition of Equation ([6](#S3.E6 "In 3.2\. Formulation
    of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")), the server performs weighted aggregation via'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $\displaystyle\boldsymbol{w}_{\mathcal{A}_{lora}}\leftarrow\sum_{m\in[M]}p_{m}\boldsymbol{w}_{\mathcal{A}_{lora},m}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: After the weighted averaging, the server distills the emulator $\mathcal{E}$
    and the updated adapter $\mathcal{A}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1\. Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section discusses the implementation of our experiments, covering details
    such as the model utilized and evaluation metrics. The code is now available at
    [https://github.com/HarliWu/FedBiOT](https://github.com/HarliWu/FedBiOT).
  prefs: []
  type: TYPE_NORMAL
- en: Model and computation environment.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The experiments utilize LLaMA-2-7B, an open-source pre-trained LLM maintained
    by Meta and released in July 2023 (Touvron et al., [2023b](#bib.bib35)). Preceding
    this, the model’s first generation was introduced in February 2023 (Touvron et al.,
    [2023a](#bib.bib34)). This model supports a maximum of 4096 input tokens and consists
    of 32 hidden layers with a total of 6.7 billion parameters. The experimental setup
    involves machines equipped with Nvidia A100 GPU cards, Intel Xeon Platinum 8369B
    CPUs, and a 512GB RAM configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Dataset details for LLM training and evaluation
  prefs: []
  type: TYPE_NORMAL
- en: '|   Task | Training Dataset | # training samples | # clients | Partition Rules
    | Max. | Min. | Std. | Test Dataset | # test samples |'
  prefs: []
  type: TYPE_TB
- en: '| Math Problem Solving | GSM-8K | 7473 | 3 | i.i.d. | 2491 | 2491 | 0 | GSM-8K
    | 1319 |'
  prefs: []
  type: TYPE_TB
- en: '| Code Generation | Rosetta | 7954 | 9 | Prog. Lang. | 1172 | 439 | 236.94
    | HumanEvalX | 656 |'
  prefs: []
  type: TYPE_TB
- en: '| Question Answering | Dolly | 15015 | 8 | Category | 3611 | 711 | 795.06 |
    Helm | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Public Dataset | Alpaca | 52002 | —————————————————————————————— |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Datasets and Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the experiments, we use the benchmark datasets and tasks in (Kuang et al.,
    [2024](#bib.bib14)) to train and evaluate the LLM on three different NLP tasks,
    covering math problem-solving, code generation, and question-answering:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For math problem-solving, we split the GSM-8K training dataset (Cobbe et al.,
    [2021](#bib.bib6)) ensuring i.i.d. across three clients, and we assess the updated
    model using the GSM-8K test dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For code generation, we fine-tune the model with the Rosetta dataset (Chaudhary,
    [2023](#bib.bib4)), which is partitioned across the programming languages, and
    a total of nine clients separately hold the data from nine different programming
    languages. Regarding its evaluation, we utilize HumanEvalX (Zheng et al., [2023](#bib.bib51)),
    an extension of a coding evaluation dataset (Chen et al., [2021](#bib.bib5)) that
    requires the model to fill in the code for a given problem in the required programming
    language (i.e., C++, GO, Java, Python).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For question answering, the model is trained on dolly-15K (Conover et al., [2023](#bib.bib7)),
    which is partitioned into 8 clients based on the categories of the questions,
    and we evaluate the new model with the selected tasks on HELM (Liang et al., [2022](#bib.bib18)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ Model and computation environment. ‣ 4.1\. Experimental
    Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") gives a detailed description of these three tasks. As Section
    [3](#S3 "3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model") mentions, the server will perform the emulator alignment during the
    model aggregation. Then, we use the Alpaca dataset (Taori et al., [2023](#bib.bib32))
    as the public dataset for the server to do the emulator alignment for all three
    NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This work is built upon an open-source federated learning platform named FederatedScope
    (Xie et al., [2023](#bib.bib45)). The training data are reformatted following
    the predesigned instructions (Chaudhary, [2023](#bib.bib4); Zhang et al., [2024](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from (Xiao et al., [2023](#bib.bib44); Kuang et al., [2024](#bib.bib14)),
    we regard the last two and the last four decoders as the adapter. The experiments
    consider two dropout rates, i.e., $\beta\in\{0.2,0.5\}$ towards minimizing the
    loss of Equation ([7](#S3.E7 "In 3.2\. Formulation of Bi-level Optimization ‣
    3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full
    Model")). During the FL training, the server takes 10 iterations to align the
    emulator $\mathcal{E}$ between two successive communication rounds after aggregating
    local adapters with FedAvg (Li et al., [2019](#bib.bib16)). These experiments
    run for 500 communication rounds, and we report the results based on the fine-tuned
    LLM obtained at the 500th round. During the training, we only fine-tune the adapter
    in the clients’ local update procedures, and we update the emulator on the server
    side. In other words, other parts of the pre-trained model, such as word embeddings,
    are frozen during the training.'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA, Optimizers and Hyperparameters.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We add the LoRA to all decoder layers in the adapter and the emulator by setting
    the rank to 8 and the alpha to 16\. We use AdamW as an optimizer to solve Equation
    ([6](#S3.E6 "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model")) and ([7](#S3.E7
    "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model")) on the clients (for the
    adapters) and the server (for the emulators), respectively. We search for the
    best learning rate in $\{1\times 10^{-5},3\times 10^{-5},5\times 10^{-5},8\times
    10^{-5},1\times 10^{-4}\}$. As for other hyperparameters related to the optimizer,
    we use the default setting. Furthermore, we also conduct grid search for FedBiOT-specific
    hyperparameters, i.e., $\epsilon$. Throughout the experiments, we demonstrate
    the result of the best hyperparameter combination. To avoid randomness, we utilize
    three different random seeds and report the averaged results.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Offsite-tuning is the only method that satisfies the constraints that fine-tuning
    without access to full model. Xiao et al. ([2023](#bib.bib44)) introduces a single-client
    offsite-tuning, while Kuang et al. ([2024](#bib.bib14)) extends it to an FL version
    (i.e., FedOT). We apply offsite-tuning with one single client, where all data
    are loaded to the client. As FedOT supports FL, we reproduce the algorithm to
    work on the FL tasks. In terms of the setting of the adapters and the emulators,
    both Offsite-tuning and FedOT treat the first two and the last two decoders as
    the adapter. To enable the parameter-efficient fine-tuning for both baselines,
    we add LoRA to both baselines, the same as the setting adopted by FedBiOT.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metric.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the experiments, we report the results on two models, i.e., AdapEmu and
    AdapFu, as defined in Section [3.1](#S3.SS1 "3.1\. Compressed Model Preparation:
    Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"). The evaluation metrics for each task follow  Kuang et al.
    ([2024](#bib.bib14)), and the detailed description is given in Appendix [A](#A1
    "Appendix A Testing Dataset and Evaluation ‣ FedBiOT: LLM Local Fine-tuning in
    Federated Learning without Full Model").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Test accuracy on math problem-solving task under different dropout
    rates
  prefs: []
  type: TYPE_NORMAL
- en: '|   Dropout Rate ($\beta$) | Methods | AdapEmu | AdapFu |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta=0.0$ | Few-shot CoT | NA | 13.42% (177/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline$\beta=0.2$ | Offsite-tuning | 3.03% (40/1319) | 9.93% (131/1319)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedOT | 2.43% (32/1319) | 10.16% (134/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 2) | 3.71% (49/1319) | 15.16% (200/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 4) | 3.41% (45/1319) | 15.23% (201/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta=0.5$ | Offsite-tuning | 2.27% (30/1319) | 7.58% (100/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| FedOT | 1.90% (25/1319) | 7.51% (99/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 2) | 2.05% (27/1319) | 11.83% (156/1319) |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 4) | 1.82% (24/1319) | 14.03% (185/1319) |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Table 3\. Pass@1 (%) and Pass@10 (%) in code generation task at various rounds
    when dropout rate is 0.2
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | Model | C++ |  | GO |  | Java |  | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1
    | Pass@10 |'
  prefs: []
  type: TYPE_TB
- en: '| Offsite-tuning | AdapEmu | 3.99 | 6.45 |  | 1.80 | 2.44 |  | 5.64 | 6.09
    |  | 5.01 | 6.38 |'
  prefs: []
  type: TYPE_TB
- en: '| AdapFu | 8.78 | 10.82 |  | 4.94 | 6.63 |  | 9.57 | 12.81 |  | 13.19 | 17.32
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedOT | AdapEmu | 2.50 | 4.89 |  | 1.86 | 3.05 |  | 5.00 | 5.49 |  | 4.91
    | 6.83 |'
  prefs: []
  type: TYPE_TB
- en: '| AdapFu | 8.60 | 11.36 |  | 5.95 | 7.11 |  | 6.30 | 9.42 |  | 12.23 | 13.58
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 2) | AdapEmu | 4.82 | 6.43 |  | 3.57 | 4.85 |  | 5.92 |
    6.36 |  | 4.97 | 6.95 |'
  prefs: []
  type: TYPE_TB
- en: '| AdapFu | 9.76 | 14.18 |  | 9.97 | 13.29 |  | 12.93 | 16.28 |  | 14.91 | 19.77
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 4) | AdapEmu | 3.20 | 4.57 |  | 2.20 | 2.44 |  | 4.91 |
    5.73 |  | 5.43 | 6.10 |'
  prefs: []
  type: TYPE_TB
- en: '| AdapFu | 9.12 | 13.41 |  | 8.02 | 11.08 |  | 11.28 | 13.10 |  | 14.57 | 18.41
    |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Pass@1 (%) and Pass@10 (%) in code generation task at various rounds
    when dropout rate is 0.5\. We do not show AdapEmu’s performance because it struggles
    to generate meaningful codes, accounting for its small size.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | Model | C++ |  | GO |  | Java |  | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1
    | Pass@10 |'
  prefs: []
  type: TYPE_TB
- en: '| Offsite-tuning | AdapFu | 5.30 | 7.26 |  | 3.32 | 7.55 |  | 4.61 | 5.33 |  |
    8.75 | 10.26 |'
  prefs: []
  type: TYPE_TB
- en: '| FedOT | AdapFu | 4.92 | 7.33 |  | 5.00 | 8.33 |  | 3.86 | 4.37 |  | 7.33
    | 8.91 |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 2) | AdapFu | 7.71 | 11.84 |  | 7.68 | 10.01 |  | 9.51 |
    14.34 |  | 13.29 | 16.87 |'
  prefs: []
  type: TYPE_TB
- en: '| FedBiOT (Adapter 4) | AdapFu | 5.03 | 11.09 |  | 6.25 | 8.47 |  | 7.41 |
    13.32 |  | 13.54 | 16.74 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 4.2\. Quantitative Evaluation on i.i.d. Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We demonstrate the experimental results of GSM-8K provided in Table [2](#S4.T2
    "Table 2 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model") and highlight
    the worth-noted phenomenon when the data are i.i.d. across the clients.'
  prefs: []
  type: TYPE_NORMAL
- en: A notable phenomenon observed in the table is that AdapEmu significantly falls
    behind AdapFu, particularly at a low dropout rate (i.e., $\beta=0.2$). To explain
    this, we examine the accuracy of the LLaMA-2 model with a dropout rate of 0.2,
    which is 2.12% without fine-tuning and increases to 2.43% after fine-tuning the
    emulator with a public dataset. The performance gap between AdapEmu and AdapFu
    can be attributed to layer dropout, which reduces the size of the LLM and subsequently
    impacts its performance. Additionally, this result highlights the difficulty of
    accurately reproducing the non-compressed parts with the emulator. Fortunately,
    all methods improve AdapEmu’s performance compared to the version without fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: When we take a look at the proposed FedBiOT at different adapters’ sizes, we
    notice that FedBiOT with adapter 4 achieves better performance than that with
    adapter 2 under the AdapFu setting. As we know, a larger adapter has more trainable
    parameters, and therefore, it can easily absorb the knowledge from the downstream
    tasks. Note that the performances of these two adapter settings have subtle differences
    under AdapEmu, meaning that their emulator achieves very similar effects to the
    non-compressed emulator. When we plug the adaptor back into the non-compressed
    emulator, the adapter with more trainable parameters obviously can achieve a better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'When comparing our proposed model with the baselines, we can notice a significant
    dominance in performance, especially in the AdapFu setting. More specifically,
    when the dropout rate becomes larger, the performance of AdapFu with FedBiOT decreases
    more mildly in contrast to other baselines. This is thanks to two factors: 1)
    the regularization term ensures the adapters will not change dramatically; 2)
    the on-the-fly distillation of the emulator with mixed losses can work better
    with clients’ data. Although the other two baselines use a public dataset to achieve
    similar functionality, the deterioration may still occur due to the data domain
    shift and the significant information loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Quantitative Evaluation on non-i.i.d. Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to Table [1](#S4.T1 "Table 1 ‣ Model and computation environment.
    ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model"), code generation and question answering
    are two tasks split in non-i.i.d. styles. In this section, we evaluate our proposed
    FedBiOT when it trains an LLM with a non-i.i.d. dataset. It is worth noting that
    the evaluation task could be either in-distribution or out-of-distribution to
    the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Code generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣
    4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model") and [4](#S4.T4 "Table 4 ‣ Evaluation Metric. ‣ 4.1\. Experimental
    Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") illustrate the best results in different programming languages
    based on different hyperparameter settings. Let us take a look at the results
    of the FedBiOT at different adapter sizes. Apparently, FedBiOT with two layers
    of adapter constantly outperforms FedBiOT with four under both AdapEmu and AdapFu.
    This conclusion is different from the one when an LLM is trained with an i.i.d.
    dataset. The discrepancy can be attributed to the clients’ objectives: under i.i.d.
    datasets, a larger adapter size benefits training by absorbing downstream linguistic
    patterns uniformly. Conversely, with non-i.i.d. datasets, clients are biased towards
    their local optima, where the emulator’s effect becomes crucial.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When comparing our proposed algorithm with the baselines, we notice a distinct
    dominance in AdapFu across all programming languages. In particular, when the
    dropout rate is 0.5, we can achieve up to 6% improvement over other baselines
    in terms of Pass@1, and up to 10% improvement of Pass@10\. Notably, the most distinct
    dominance can be witnessed under the “column” of Java in Table [4](#S4.T4 "Table
    4 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f8e90d32ab1b558e499567bf1fc4225c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e14a458cff45d9840f1817616abe2e26.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) AdapEmu (Dropout rate 0.2)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/138fb4f8266319a94df4c8b2caeb4db2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) AdapFu (Dropout rate 0.2)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ea1d629551ab31e09ceebc1d4e01252.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) AdapFu (Dropout rate 0.5)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2\. Test accuracy in eight types of question-answering tasks (Left to
    right: Natural Questions (closed-book), QuAC, MMLU, OpenbookQA, NarrativeQA, Natural
    Questions (open-book), HellaSwag, BoolQ) and the average accuracy under different
    baselines (bars from left to right: Offsite-tuning, FedOT, FedBiOT (Adapter 2),
    FedBiOT (Adapter 4)) and different dropout rates.'
  prefs: []
  type: TYPE_NORMAL
- en: Question Answering.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ Code generation. ‣ 4.3\. Quantitative Evaluation
    on non-i.i.d. Data ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model") shows the evaluation results using the HELM benchmark
    while we train the LLM with Dolly-15K. Generally speaking, FedBiOT (Adapter 2)
    performs significantly better than Adapter 4 in some tasks in terms of AdapEmu.
    As both AdapEmu have the same number of layers, this result exhibits the importance
    of the emulator, i.e., the model with a larger emulator can achieve leading performance.
    To some extent, this result supports our previous conclusion that an emulator
    plays a more important role than an adapter in a non-i.i.d. task. As for AdapFu,
    the performance difference is trivial between the two adapter sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed algorithm outperforms offsite-tuning and FedOT in most datasets,
    which is consistent with the findings in other training tasks. The dominance of
    AdapFu becomes more pronounced as the dropout rate increases from 0.2 to 0.5\.
    For instance, FedBiOT is approximately 10% better than the baselines at a 0.5
    dropout rate in Natural Questions (closed-book), compared to a 2% improvement
    at a 0.2 dropout rate. Notably, comparing Figure [2b](#S4.F2.sf2 "In Figure 2
    ‣ Code generation. ‣ 4.3\. Quantitative Evaluation on non-i.i.d. Data ‣ 4\. Experiments
    ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model") and
    [2c](#S4.F2.sf3 "In Figure 2 ‣ Code generation. ‣ 4.3\. Quantitative Evaluation
    on non-i.i.d. Data ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model"), we notice that FedBiOT is mildly affected by changes
    in the dropout rate, while the baselines suffer significant degradation as the
    dropout rate increases. This stability can be attributed to round-by-round emulator
    alignment, where the non-compressed part of the full model is set as an anchor,
    regardless of the dropout rate. Consequently, this approach stabilizes the adapter
    training process, ensuring that adapters of the same size achieve similar performance
    across varying dropout rates.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Discussion on Computation and Communication Overhead
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.4\. Discussion on Computation and Communication
    Overhead ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") presents the computation and communication overhead of different
    methods under different dropout rates. As mentioned in the experimental setting,
    all algorithms have been applied with LoRA, and therefore, the number of trainable
    parameters dramatically reduces. From the clients’ perspectives, the number of
    trainable parameters is determined by the number of decoder layers in the adapter.
    Apparently, FedBiOT (Adapter 2) should be with the minimum number of trainable
    parameters among other methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The computation costs in Table [5](#S4.T5 "Table 5 ‣ 4.4\. Discussion on Computation
    and Communication Overhead ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model") are measured by per-token floating
    point operation (FLOP/token). As we can see, the proposed FedBiOT costs less overhead
    than offsite-tuning and FedOT. The difference arises on account of the position
    of the trainable parameters. The adapter of the proposed FedBiOT is near the output
    layer. As for offsite-tuning and FedOT, the adapters are located separately at
    the top and the bottom two layers, thereby consuming more computation costs in
    the backward propagation for transmitting the derivative from the bottom to the
    top.'
  prefs: []
  type: TYPE_NORMAL
- en: However, our proposed method may require more communication overhead than the
    baselines. This is because the server should transmit the LoRA parameters of both
    the adapter and the emulator to the clients in our proposed method, while in offsite-tuning
    and FedOT, the server merely transmits the aggregated LoRA of the adapter to the
    clients. However, the overall cost is trivial, compared to the full LLM transmission
    at a cost of 28GB.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Computation and communication costs of different methods under different
    dropout rates at client side.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Dropout Rate ($\beta$) | Methods | #Layers in Adapter | #Layers in Emulator
    | Trainable Param. (M) | Comp. Costs (GFLOP/token) | Comm. Costs (MB/round) |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta=0.2$ | Offsite-tuning /FedOT | 4 | 22 | 0.524 | 10.33 | 4.19 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-7 | FedBiOT (Adapter 2) | 2 | 24 | 0.262 | 5.47 | 14.68 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-7 | FedBiOT (Adapter 4) | 4 | 22 | 0.524 | 5.87 | 15.73 |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta=0.5$ | Offsite-tuning /FedOT | 4 | 14 | 0.524 | 7.09 | 4.19 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-7 | FedBiOT (Adapter 2) | 2 | 15 | 0.262 | 3.65 | 9.96 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-7 | FedBiOT (Adapter 4) | 4 | 14 | 0.524 | 4.25 | 11.53 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce FedBiOT, a federated learning algorithm that avoids
    full model fine-tuning while substantially reducing computation overhead. Specifically,
    we compress the LLM and divide it into two components, namely, an emulator and
    an adapter. By formulating a bi-level optimization problem, our proposed FedBiOT
    ensures that the emulator partially simulates the original LLM, while the adapter
    focuses on learning domain-specific linguistic patterns. Extensive experiments
    show the superiority of the proposed FedBiOT working with LLaMA-2, where it can
    achieve significant accuracy improvement than the existing baselines (i.e., Offsite-tuning
    and FedOT) in all tasks (i.e., math problem-solving, code generation, and question
    answering).
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The authors would like to thank the anonymous reviewers for their constructive
    comments. This work is supported in part by the US National Science Foundation
    under grants NSF-IIS 1747614 and NSF-IIS 2141037\. Any opinions, findings, and
    conclusions or recommendations expressed in this material are those of the author(s)
    and do not necessarily reflect the views of the National Science Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acar et al. (2020) Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina,
    Paul Whatmough, and Venkatesh Saligrama. 2020. Federated Learning Based on Dynamic
    Regularization. In *Proc. of International Conference on Learning Representations
    (ICLR’20)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CCPA (2023) CCPA. 2023. *California Consumer Privacy Act (CCPA)*. [https://oag.ca.gov/privacy/ccpa](https://oag.ca.gov/privacy/ccpa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following
    LLaMA model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conover et al. (2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
    Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
    2023. *Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned
    LLM*. [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2023) Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan.
    2023. Chatlaw: Open-source legal large language model with integrated external
    knowledge bases. *arXiv preprint arXiv:2306.16092* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GDPR (2016) GDPR. 2016. *Regulation (EU) 2016/679 of the European Parliament
    and of the Council*. [https://data.europa.eu/eli/reg/2016/679/oj](https://data.europa.eu/eli/reg/2016/679/oj)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2023) Shiqi He, Qifan Yan, Feijie Wu, Lanjun Wang, Mathias Lécuyer,
    and Ivan Beschastnikh. 2023. GlueFL: Reconciling Client Sampling and Model Masking
    for Bandwidth Efficient Federated Learning. *Proc. of Machine Learning and Systems
    (MLSys’23)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large
    Language Models. In *Proc. of International Conference on Learning Representations
    (ICLR’21)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karimireddy et al. (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
    Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020. Scaffold: Stochastic
    controlled averaging for federated learning. In *Proc. of International conference
    on machine learning (ICML’20)*. 5132–5143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuang et al. (2024) Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei
    Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024.
    FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning. In *Proc. of the ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining (KDD’24)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    Power of Scale for Parameter-Efficient Prompt Tuning. In *Proc. of the Conference
    on Empirical Methods in Natural Language Processing (EMNLP’21)*. 3045–3059.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua
    Zhang. 2019. On the Convergence of FedAvg on Non-IID Data. In *Proc. of International
    Conference on Learning Representations (ICLR’19)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing
    Continuous Prompts for Generation. In *Proc. of the Annual Meeting of the Association
    for Computational Linguistics and the International Joint Conference on Natural
    Language Processing (ACL/IJNLP’21)*. 4582–4597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020) Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi.
    2020. Ensemble distillation for robust model fusion in federated learning. In
    *Proc. of Advances in Neural Information Processing Systems (NeurIPS’20)*. 2351–2363.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2023) Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li
    Shen, and Dacheng Tao. 2023. Efficient federated prompt tuning for black-box large
    pre-trained models. *arXiv preprint arXiv:2310.03123* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. 2023. GPT understands, too. *AI Open* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter (2018) Ilya Loshchilov and Frank Hutter. 2018. Decoupled
    Weight Decay Regularization. In *Proc. of International Conference on Learning
    Representations (ICLR’18)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks
    from decentralized data. In *Proc. of Artificial intelligence and statistics (AISTAT’17)*.
    1273–1282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nay et al. (2024) John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao,
    Meghana Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H Choi, and Jungo Kasai.
    2024. Large language models as tax attorneys: a case study in legal capabilities
    emergence. *Philosophical Transactions of the Royal Society A* 382, 2270 (2024),
    20230159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2023) OpenAI. 2023. Fine-tuning - OpenAI API. [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
    Accessed: 2023-09-29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    In *Proc. of Advances in Neural Information Processing Systems (NeurIPS’22)*.
    27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sajjad et al. (2023) Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav
    Nakov. 2023. On the effect of dropping layers of pre-trained transformer models.
    *Computer Speech & Language* 77 (2023), 101429.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singhal et al. (2023) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,
    Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen
    Pfohl, et al. 2023. Large language models encode clinical knowledge. *Nature*
    620, 7972 (2023), 172–180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sordoni et al. (2023) Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté,
    Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner,
    and Nicolas Le Roux. 2023. Joint prompt optimization of stacked llms using variational
    inference. In *Proc. of Advances in Neural Information Processing Systems (NeurIPS’23)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu,
    Yiran Chen, and Holger R Roth. 2023. FedBPT: Efficient Federated Black-box Prompt
    Tuning for Large Language Models. *arXiv preprint arXiv:2310.01467* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2024) Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. 2024. Improving
    LoRA in Privacy-preserving Federated Learning. In *Proc. of The International
    Conference on Learning Representations (ICLR’24)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    Alpaca: An Instruction-following LLaMA model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    Large language models in medicine. *Nature medicine* 29, 8 (2023), 1930–1940.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng
    Zhan, and Zhigang Zeng. 2023a. Dafkd: Domain-aware federated knowledge distillation.
    In *Proc. of the IEEE/CVF conference on Computer Vision and Pattern Recognition
    (CVPR’23)*. 20412–20421.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Haozhao Wang, Haoran Xu, Yichen Li, Yuan Xu, Ruixuan Li,
    and Tianwei Zhang. 2023b. FedCDA: Federated Learning with Cross-rounds Divergence-aware
    Aggregation. In *Proc. of The International Conference on Learning Representations
    (ICLR’23)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang
    Gu, and Jing Gao. 2022. FedKC: Federated knowledge composition for multilingual
    natural language understanding. In *Proc. of the ACM Web Conference 2022 (WWW’22)*.
    1839–1850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent
    Poor. 2020. Tackling the objective inconsistency problem in heterogeneous federated
    optimization. In *Proc. of Advances in neural information processing systems (NeurIPS’20)*.
    7611–7623.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang
    Shen. 2023c. Chatcad: Interactive computer-aided diagnosis on medical image using
    large language models. *arXiv preprint arXiv:2302.07257* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned Language
    Models are Zero-Shot Learners. In *Proc. of International Conference on Learning
    Representations (ICLR’21)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. In *Proc. of Advances in Neural Information
    Processing Systems (NeurIPS’22)*. 24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023) Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, and Jing
    Gao. 2023. Anchor sampling for federated learning with partial client participation.
    In *Proc. of International Conference on Machine Learning (ICML’23)*. 37379–37416.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-tuning:
    Transfer learning without full model. *arXiv preprint arXiv:2302.04870* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2023) Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope:
    A Flexible Federated Learning Platform for Heterogeneity. In *Proc. of the VLDB
    Endowment (VLDB’23)*. 1059–1072.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2023) Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. 2023. Fedlora:
    Model-heterogeneous personalized federated learning with lora tuning. *arXiv preprint
    arXiv:2310.13283* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    2014. How transferable are features in deep neural networks?. In *Proc. of Advances
    in neural information processing systems (NeurIPS’14)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao
    Xu, and Feijie Wu. 2021. Parameterized knowledge transfer for personalized federated
    learning. In *Proc. of Advances in Neural Information Processing Systems (NeurIPS’21)*.
    10092–10104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024) Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li,
    Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. 2024. Towards building the
    federatedGPT: Federated instruction tuning. In *Proc. of IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP’24)*. 6915–6919.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu,
    Lizhen Qu, and Zenglin Xu. 2023. FedPETuning: When federated learning meets the
    parameter-efficient tuning methods of pre-trained language models. In *Proc. of
    Annual Meeting of the Association of Computational Linguistics (ACL’23)*. 9963–9977.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: A
    pre-trained model for code generation with multilingual benchmarking on humaneval-x.
    In *Proc. of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining
    (KDD’23)*. 5673–5684.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Testing Dataset and Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As described in Table [1](#S4.T1 "Table 1 ‣ Model and computation environment.
    ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model"), we utilize three datasets to assess
    the fine-tuning performance. In this section, we briefly introduce all these datasets
    and provide the details about how they evaluate a given LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: GSM-8K.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use the GSM-8K test set (Cobbe et al., [2021](#bib.bib6)) to evaluate the
    ability of a large language model (LLM) to solve math problems. This dataset includes
    ”questions” and ”ground truth” answers. We assess correctness by determining how
    often the LLM answers a given question correctly. Following chain of thought (CoT)
    (Wei et al., [2022](#bib.bib42)), we prepare a set of sample questions (a.k.a.
    few-shot prompting) and prompt the LLM to generate step-by-step solutions, ensuring
    the answers are formatted correctly. Finally, we extract the answers from these
    solutions and compare them with the ground truth to calculate the correctness
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: HumanevalX.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a task for code autofill, which consists of 164 test samples for five
    programming languages (Zheng et al., [2023](#bib.bib51)). It is worth noting that
    we use four of them (i.e., C++, GO, Java, and Python) because there are no JavaScript
    codes in the training dataset. Each test sample is constituted with “task id”,
    “prompt” (i.e., Task description with partial codes), “entry point” (i.e., the
    function to be achieved), “canonical solution” (i.e., a sampled solution), and
    “test” (i.e., evaluate if the generated code can obtain the correct answer based
    on the given input). In this task, we use “prompt” as the input and generate five
    versions of codes using a given model. We compile the code and check if it can
    pass the given “test”. Let $c$ be the number of correct codes generated by LLM
    and passed unit tests, and therefore, Pass@k can be computed via
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Pass@k}=\mathbb{E}_{\text{problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: HELM.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HELM (Liang et al., [2022](#bib.bib18)) is a benchmark that contains a wide
    range of NLP tasks. We upload the well-trained models to the benchmark and evaluate
    them on question-answering tasks, which includes eight datasets, i.e., MMLU, BoolQ,
    NarrativeQA, Natural Questions (closed-book), Natural Questions (open-book), QuAC,
    HellaSwag, OpenbookQA. For different tasks, the results come from different metrics,
    i.e., exact match for HellaSwag, OpenbookQA, and MMLU; quasi-exact match for BoolQ;
    F1 for the rest.
  prefs: []
  type: TYPE_NORMAL
