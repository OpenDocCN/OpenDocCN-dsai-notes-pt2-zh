- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:37:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.05904](https://ar5iv.labs.arxiv.org/html/2405.05904)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zorik Gekhman^T  Gal Yona^G  Roee Aharoni^G Matan Eyal^G  Amir Feder^G
  prefs: []
  type: TYPE_NORMAL
- en: Roi Reichart^T Jonathan Herzig^G
  prefs: []
  type: TYPE_NORMAL
- en: ^TTechnion - Israel Institute of Technology  ^GGoogle Research
  prefs: []
  type: TYPE_NORMAL
- en: zorikgekhman@gmail.com, jherzig@google.com   Work done during an internship
    at Google Research.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When large language models are aligned via supervised fine-tuning, they may
    encounter new factual information that was not acquired through pre-training.
    It is often conjectured that this can teach the model the behavior of *hallucinating*
    factually incorrect responses, as the model is trained to generate facts that
    are not grounded in its pre-existing knowledge. In this work, we study the impact
    of such exposure to new knowledge on the capability of the fine-tuned model to
    utilize its pre-existing knowledge. To this end, we design a controlled setup,
    focused on closed-book QA, where we vary the proportion of the fine-tuning examples
    that introduce new knowledge. We demonstrate that large language models struggle
    to acquire new factual knowledge through fine-tuning, as fine-tuning examples
    that introduce new knowledge are learned significantly slower than those consistent
    with the model’s knowledge. However, we also find that as the examples with new
    knowledge are eventually learned, they linearly increase the model’s tendency
    to hallucinate. Taken together, our results highlight the risk in introducing
    new factual knowledge through fine-tuning, and support the view that large language
    models mostly acquire factual knowledge through pre-training, whereas fine-tuning
    teaches them to use it more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d35a58459c3bc75e537229b382b4b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Train and development accuracies as a function of the fine-tuning
    duration, when fine-tuning on $50\%$ and $50\%$ examples. $\mathtt{Unknown}$.
    The best development performance is obtained when the LLM fits the majority of
    the $\mathtt{Known}$ ones. From this point, fitting $\mathtt{Unknown}$ examples
    reduces the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training Large Language Models (LLMs) on textual corpora embeds substantial
    factual knowledge in their parameters Petroni et al. ([2019](#bib.bib20)); AlKhamissi
    et al. ([2022](#bib.bib1)); Cohen et al. ([2023](#bib.bib5)), which is essential
    for excelling in various downstream applications. These models often require further
    alignment to desired behaviors, typically achieved through supervised fine-tuning
    on instruction-following tasks Wei et al. ([2022](#bib.bib30)); Mishra et al.
    ([2022](#bib.bib18)) and preference learning from human feedback Ouyang et al.
    ([2022](#bib.bib19)); Rafailov et al. ([2024](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: In the fine-tuning phase, the model is usually trained on outputs created by
    human annotators or other LLMs. As a result, the model may encounter new factual
    information, extending beyond the knowledge it acquired during pre-training. This
    raises the question of how LLMs integrate new facts outside of their pre-existing
    knowledge. One possibility is that the model simply adapts by learning this new
    factual information. However, a common conjecture posits that such exposure to
    new knowledge may encourage the model to *hallucinate* factually incorrect responses,
    as the model is essentially trained to generate facts that are not grounded in
    its pre-existing knowledge Schulman ([2023](#bib.bib24)); Huang et al. ([2023](#bib.bib9));
    Gao ([2021](#bib.bib6)); Goldberg ([2023](#bib.bib7)); Gudibande et al. ([2023](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we study how learning new factual knowledge through fine-tuning
    impacts the model’s tendency to hallucinate w.r.t. its pre-existing knowledge,
    exploring the above conjecture.¹¹1While we focus on supervised fine-tuning, our
    findings are relevant to offline preference optimization methods such as DPO Rafailov
    et al. ([2024](#bib.bib21)) that may add new knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: To study the impact of new knowledge, we must be able to assess whether a single
    fine-tuning example is consistent with the model’s knowledge. We propose SliCK,
    a hierarchy of four *knowledge categories*, derived from a continuous measure
    that quantifies the agreement between model-generated answers and the ground-truth
    labels. In SliCK, examples are first categorized into $\mathtt{Known}$ types,
    where the latter corresponds to examples with facts that are most likely unknown
    to the model. The $\mathtt{Known}$, $\mathtt{MaybeKnown}$ ([Figure 2](#S2.F2 "In
    2 Study Setup ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).
  prefs: []
  type: TYPE_NORMAL
- en: Equipped with the above method, we carefully design a controlled study, focused
    on closed-book question answering (QA), where we vary the proportion of the fine-tuning
    examples categorized as $\mathtt{Unknown}$, while controlling for other factors.
  prefs: []
  type: TYPE_NORMAL
- en: Our study empirically demonstrates that learning from $\mathtt{Unknown}$ examples
    is correlated with better utilization of pre-existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Through an analysis of the training dynamics, we discover that the LLM fits
    $\mathtt{Unknown}$ examples (top plot in [Figure 1](#S1.F1 "In 1 Introduction
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")). This indicates
    that during fine-tuning, LLMs struggle to integrate new factual knowledge (present
    in the $\mathtt{Unknown}$ fine-tuning examples).
  prefs: []
  type: TYPE_NORMAL
- en: From a practical perspective, mitigating overfitting using *early-stopping*
    (vertical dotted line in [Figure 1](#S1.F1 "In 1 Introduction ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?")) can minimize the risk of the
    hallucinations caused by fitting the $\mathtt{Unknown}$ fine-tuning examples substantially
    reduces the risk of overfitting, without sacrificing performance.
  prefs: []
  type: TYPE_NORMAL
- en: We further evaluate the impact of fine-tuning examples from each of our three
    $\mathtt{Known}$, does not yield the best results. Our analysis reveals that incorporating
    $\mathtt{MaybeKnown}$ fine-tuning examples, representing facts with lower degrees
    of certainty, plays an important part in properly handling such examples in test
    time. This indicates that the composition of fine-tuning examples significantly
    influences the extent to which LLMs effectively utilize their pre-existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we study the effect of new factual knowledge in the fine-tuning
    data by designing a controlled setup that isolates this factor. We find that fine-tuning
    examples that introduce new knowledge are learned slowly, which suggests that
    LLMs struggle to integrate new knowledge through fine-tuning and supports the
    view that LLMs mostly acquire knowledge through pre-training Zhou et al. ([2023](#bib.bib34));
    Lin et al. ([2023](#bib.bib15)). However, we also find that as the model eventually
    learns new knowledge through fine-tuning, it becomes more prone to hallucinations
    w.r.t. its pre-existing knowledge. Collectively, our findings highlight the potential
    for unintended consequences when introducing new knowledge through fine-tuning,
    and imply that fine-tuning may be more useful as a mechanism to enhance the utilization
    of pre-existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Study Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Type | Category | Definition | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Known}$ | $P_{\mathtt{Correct}}(q,a;M,T=0)=1$ | Greedy decoding
    *always* predicts the correct answer. |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{MaybeKnown}$ | Greedy decoding *sometimes* (but not always) predicts
    the correct answer. |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{WeaklyKnown}$ | Greedy decoding *never* predicts the correct answer,
    whereas temperature sampling with <math id="S2.T1.st1.8.8.8.3.1.1.m1.1" class="ltx_Math"
    alttext="T></math> *sometimes* predicts the correct answer. |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Unknown}$ | $P_{\mathtt{Correct}}(q,a;M,T\geq 0)=0$ | The model
    *never* predicts the correct answer, thus it seem to lack the knowledge of the
    correct answer. |'
  prefs: []
  type: TYPE_TB
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Question | Gold Answer | Greedy Answers | Sampled Answers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{HighlyKnown}$ | Who founded Science of Mind? | Ernest Holmes | [Ernest
    Holmes, .. Ernest Holmes, ..] | […, …] |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{MaybeKnown}$ | What is the capital of Toledo District? | Punta Gorda
    | [Belmopan, .., Punta Gorda, ..] | […, …] |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{WeaklyKnown}$ | What kind of work does Scott McGrew do? | Journalist
    | [Film director, .. Actor, ..] | [Musician, .. Journalist, ..] |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Unknown}$ | Where is Benedict located? | Hubbard County | [Louisiana,
    .. New Mexico, ..] | [Washington, .. Texas, ..] |'
  prefs: []
  type: TYPE_TB
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Formal definitions of the SliCK knowledge categories, based on the
    $P_{\mathtt{Correct}}$ measure as defined in §[3](#S3 "3 Quantifying Knowledge
    in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") (a),
    accompanied with real examples from the annotated EntityQuestions dataset used
    in our study (b).'
  prefs: []
  type: TYPE_NORMAL
- en: Given a fine-tuning dataset $D$, we denote by $M_{D}$ on $D$ affects $M_{D}$
    with varying proportions of examples that are unknown to $M$.
  prefs: []
  type: TYPE_NORMAL
- en: When constructing $D$, where $q$ is the ground-truth answer (e.g., “France”).
    To this end, we use EntityQuestions Sciavolino et al. ([2021](#bib.bib25)), where
    triplets from a diverse set of relations from Wikidata Vrandečić and Krötzsch
    ([2014](#bib.bib28)) are converted to QA pairs. These relations encompass a broad
    spectrum of factual knowledge, including biographical information, geographical
    data, ownership and authorship details, history and more. We use the original
    development and test splits, and we sub-sample the train split to create different
    variants of $D$. We focus on 12 diverse relations and reserve 7 additional relations
    for an *out-of-distribution* test set, used (only) in §[4.5](#S4.SS5 "4.5 Generalization
    to New Relations ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: As $M$, we use the PaLM 2-M base model Anil et al. ([2023](#bib.bib2)). We focus
    on exact match (EM) as our evaluation metric.²²2We validated that in our setting
    EM strongly correlates with word-level F1 Rajpurkar et al. ([2016](#bib.bib22)),
    and we choose EM as it is more intuitive for the purposes of our analysis. Full
    technical details are in §[A](#A1 "Appendix A Data Preprocessing ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: 3 Quantifying Knowledge in LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To assess the effect of new knowledge in $D$, we have to annotate each $(q,a)$
    w.r.t. whether $M$ is $a$ measure based on samples from $M$ pairs into four *knowledge
    categories*. We name this approach SliCK (Sampling-based Categorization of Knowledge).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b3ae11ed58b850538a0c65864c005e8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e7c479730dcb1b5c36867e65ca4b5e0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Test performance as a function of the $\%$ examples in the fine-tuning
    dataset $D$ and are identical to (a). Dotted lines correspond to fine-tuning on
    the ablated variants $D_{\mathtt{Known}}$ examples are filtered-out. For $0\%$
    $D=$ and for $100\%$ there is no $D_{\mathtt{Known}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining $\bm{P_{\bm{\mathtt{Correct}}}}$.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adopt the perspective that $M$ is $a$ when prompted to answer $q$ is a base
    model that has not been specifically fine-tuned to follow instructions, we prompt
    $M$.³³3In our study we achieve this by using exemplars from the same relation.
    E.g., if $q=$“Where is Paris located?”, the exemplars would follow the pattern
    “Where is {X} located?”.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, $M$ as an estimate of how likely is $M$ to $q$.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of our study we approximate the value of $P_{\mathtt{Correct}}$
    different random 4-shot prompts.⁴⁴4We use 4-shot simply since we found it enough
    for $M$ and $16$. $P_{\mathtt{Correct}}(q,a;M,T=0)$ by the fraction of correct
    sampled answers. Full details are in §[C](#A3 "Appendix C 𝑷_𝙲𝚘𝚛𝚛𝚎𝚌𝚝 Approximation
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: Deriving knowledge categories from $\bm{P_{\bm{\mathtt{Correct}}}}$.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We define the $\mathtt{Unknown}$ pairs for which $M$. In our notations this
    means that $P_{\mathtt{Correct}}(q,a;M,T\geq 0)=0$, i.e. $M$, we consider $(q,a)$.
    In this choice, we posit that if prompting $M$ can *sometimes* result with the
    correct answer $a$ must have some association with the relevant fact.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing that knowledge can vary in degrees of certainty and extent, we divide
    the $\mathtt{Known}$ pairs into three distinct categories (top three rows in Tables
    [1(a)](#S2.T1.st1 "Table 1(a) ‣ Figure 2 ‣ 2 Study Setup ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?") and [1(b)](#S2.T1.st2 "Table 1(b)
    ‣ Figure 2 ‣ 2 Study Setup ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?")). Motivated by the principle that $M$ if $(q,a)$, we put emphasis
    on *greedy decoding* outcomes, represented with $P_{\mathtt{Correct}}(q,a;M,T=0)$
    represents $(q,a)$ *always* greedily predicts $a$ *sometimes* (but not always)
    greedily predicts $a$ as $\mathtt{MaybeKnown}$ *never* greedily predicts $a$ as
    $\mathtt{WeaklyKnown}$.
  prefs: []
  type: TYPE_NORMAL
- en: We apply SliCK to annotate each $(q,a)$.⁵⁵5In EntityQuestions we have $24\%$,
    $23\%$, $17\%$, and $36\%$. Full per-relation statistics are in §[D](#A4 "Appendix
    D Data Annotation ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
    We analyze the quality of our categories in §[6](#S6 "6 SliCK Knowledge Categories
    Analysis ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: 4 How Harmful are $\mathtt{Unknown}$ Examples?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we study the effect of new knowledge in the fine-tuning dataset
    $D$ examples in $D$ and create variants of $D$ of $\mathtt{Unknown}$ $\mathtt{Known}$
    categories collectively (see [Table 1(a)](#S2.T1.st1 "In Figure 2 ‣ 2 Study Setup
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")), and provide
    a per-category analysis in §[5](#S5 "5 Understanding Knowledge Types: Their Value
    and Impact ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
    We denote early-stopping based on the development set as early_stop (happens after
    5-10 epochs) and 50 fine-tuning epochs as Convergence, as at this point $M$ (i.e.
    $100\%$ training accuracy). We measure test performance as a proxy for hallucinations
    since we are in a closed-book QA setup with disjoint train/test splits, where
    the model has to use its per-existing knowledge to answer test questions (see
    §[B](#A2 "Appendix B Test performance as Proxy for Hallucinations ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?") for further discussion).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Higher $\mathtt{Unknown}$ Ratio is Proportional to Performance Degradation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 3(a)](#S3.F3.sf1 "In Figure 3 ‣ 3 Quantifying Knowledge in LLMs ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") presents the performance
    as a function of the % of $\mathtt{Unknown}$, for different fine-tuning durations.
    Higher %$\mathtt{Unknown}$ examples are less useful than $\mathtt{Known}$. Interestingly,
    this effect increases with larger $\%$ (the inter-line spacing from early_stop
    exhibits a monotonic increase along the positive x-axis), suggesting that a higher
    %$\mathtt{Unknown}$ increases the risk of overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 $\mathtt{Unknown}$ Examples: Harmful or Neutral?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since $|D|$ could stem from simply the lower number of the $\mathtt{Known}$
    examples are *harmful* or *neutral*. To address this, we measure the effect of
    filtering-out all the $\mathtt{Unknown}$. For each $D$, consisting only from the
    $\mathtt{Known}$. E.g., if $D$ $\mathtt{Unknown}$ $\mathtt{Known}$$D_{\mathtt{Known}}$.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3(b)](#S3.F3.sf2 "In Figure 3 ‣ 3 Quantifying Knowledge in LLMs ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") presents the results.
    Perhaps surprisingly, for early_stop the results for $D$, indicating that the
    $\mathtt{Unknown}$ examples are actually very *harmful*. In this case $D$, and
    the gap between them is proportional to the $\mathtt{Unknown}$ ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, for $D_{\mathtt{Known}}$ (full lines). This indicates that the
    presence of $\mathtt{Unknown}$ ratios more prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 $\mathtt{Unknown}$ Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We showed that $\mathtt{Unknown}$ were fitted by $M$ and $\mathtt{Unknown}$
    as a function of the fine-tuning duration. The development accuracy is presented
    in a zoomed-in plot at the bottom, as it falls within a narrower range. We include
    a breakdown of the train accuracy per $\mathtt{Known}$ category in §[F](#A6 "Appendix
    F Train Accuracy on Different 𝙺𝚗𝚘𝚠𝚗 Categories ‣ Does Fine-Tuning LLMs on New
    Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f66df0e142a9a0850f73422f885e127.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The state of the examples in the fine-tuning dataset $D$ (y-axis),
    we illustrate which portion of the examples in $D$).'
  prefs: []
  type: TYPE_NORMAL
- en: '$M$ fine-tuning examples substantially slower than $\mathtt{Known}$ reaches
    peak performance on the development set, while fitting the majority of the $\mathtt{Known}$.
    In [Figure 4](#S4.F4 "In 4.3 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples are Fitted Slower than 𝙺𝚗𝚘𝚠𝚗 Examples
    ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?"), we show that this behavior is consistent across all
    our variants of $D$ examples had a *neutral* effect on performance (§[4.2](#S4.SS2
    "4.2 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples: Harmful or Neutral? ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples?
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")), as at this
    point $M$ examples are the ones that are likely to introduce new factual knowledge,
    their significantly slow fitting rate suggests that LLMs struggle to acquire new
    factual knowledge through fine-tuning, instead they learn to expose their pre-existing
    knowledge using the $\mathtt{Known}$ examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.4 The Influence of $\mathtt{Unknown}$ on Accuracy: A Linear Model Perspective'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | $\beta_{0}$ | $\beta_{\text{unk}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| In-distribution (§[4.4](#S4.SS4 "4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on
    Accuracy: A Linear Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")) | $36.9$ | $-8.3$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Out-of-distribution (§[4.5](#S4.SS5 "4.5 Generalization to New Relations
    ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?")) | $36.2$ | $-3.0$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results of our linear model for predicting the test accuracy as defined
    by [Equation 1](#S4.E1 "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy:
    A Linear Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | early_stop |  | Convergence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathtt{Full}$ | $\mathtt{Mkn}$ | $\mathtt{Unk}$ |  | $\mathtt{Hkn}$
    | $\mathtt{Wkn}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{HighlyKnown}}$ | 40.5 |  | 98.7 | 60.1 | 9.0 | 0.6 |  | 40.0
    |  | 98.4 | 58.8 | 8.5 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{MaybeKnown}}$ | 43.6 |  | 98.4 | 69.9 | 12.1 | 1.0 |  | 43.2
    |  | 97.5 | 68.2 | 12.9 | 1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{WeaklyKnown}}$ | 39.2 |  | 95.0 | 59.2 | 8.6 | 0.4 |  | 35.4
    |  | 73.5 | 55.8 | 17.2 | 2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{Unknown}}$ | 37.5 |  | 95.6 | 52.9 | 6.5 | 0.6 |  | 25.8 |  |
    55.8 | 36.6 | 12.2 | 3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{Natural}}$ | 43.5 |  | 98.0 | 67.6 | 14.1 | 1.8 |  | 41.8 |  |
    95.5 | 61.7 | 14.8 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Accuracies for the single-category variants from §[5](#S5 "5 Understanding
    Knowledge Types: Their Value and Impact ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?"), across per-category subsets of the test set. $\mathtt{Full}$=$\mathtt{HighlyKnown}$=$\mathtt{MaybeKnown}$=$\mathtt{WeaklyKnown}$=$\mathtt{Unknown}$
    (significance test details are in §[I](#A9 "Appendix I Statistic Significance
    Tests ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1](#S1.F1 "In 1 Introduction ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?") demonstrates that after the development performance
    peaks at early_stop (vertical dotted line), it deteriorates as $M$ examples. In
    this section, we aim to characterize this relationship more accurately by assessing
    whether a simple linear dependency can tie the impact of fitting $\mathtt{Known}$
    training examples on test accuracy. To this end we use the following linear regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Accuracy$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $N_{\text{Kn}}$ are the number of the $\mathtt{Known}$ examples in $D$
    fits.
  prefs: []
  type: TYPE_NORMAL
- en: 'We estimate the coefficients⁶⁶6Full details in §[G](#A7 "Appendix G Linear
    Model ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"). We
    note that this linear model is only valid in bounded region of $N_{\text{kn}}\leq|D|$.
    by collecting ($Accuracy$, $N_{\text{Unk}}$ variants. [Table 1](#S2.T1 "In 4.4
    The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear Model Perspective ‣ 4
    How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?") presents the results (top row). The high $R^{2}$ examples hurts
    performance ($\beta_{unk}<0$ examples improves it ($\beta_{\text{kn}}></math>
    roughly matches the positive impact from <math id=$).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Generalization to New Relations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the above setup, the $(q,a)$. We now investigate whether our observed dynamics
    has a broader effect on the model’s knowledge, and transfers to relations not
    represented in $D$. To test this, we reserve a subset of the relations for an
    *out-of-distribution* (OOD) test set, excluding them from the train and development
    splits. See §[A](#A1 "Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs on
    New Knowledge Encourage Hallucinations?") for details and Tables [4](#A1.T4 "Table
    4 ‣ Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?") and [5](#A1.T5 "Table 5 ‣ Appendix A Data Preprocessing ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") for in-distribution
    vs OOD relations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results on the OOD test set reveal similar key insights: (1) Higher $\mathtt{Unknown}$
    examples are harmful for OOD performance, but mostly when $M$, $\beta_{\text{kn}}></math>
    and <math id=$ (see [Table 1](#S2.T1 "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗
    on Accuracy: A Linear Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples?
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")). More details
    are in §[H](#A8 "Appendix H Out-of-distribution (OOD) Evaluation ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?").'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, *our insights transfer across relations*. This essentially shows that
    fine-tuning on $\mathtt{Unknown}$ examples such as *"Where is [E1] located?"*,
    can encourage hallucinations on seemingly unrelated questions, such as *"Who founded
    [E2]?"*. This further supports the conclusion that the observed effects likely
    stem from the model learning the *behavior* of generating answers that are not
    grounded in its pre-existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '5 Understanding Knowledge Types: Their Value and Impact'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When addressing our main research question on the effect of $\mathtt{Unknown}$
    categories collectively for simplicity (see [Table 1(a)](#S2.T1.st1 "In Figure
    2 ‣ 2 Study Setup ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).
    We now examine the effect of each category, exploring the following questions:
    Q1: How *training examples* from each category impact the test performance? Q2:
    What is the model’s performance across *test examples* from each category? To
    address Q1 we created single-category variants of the fine-tuning dataset $D$
    consisting solely of examples from the category $\mathtt{CAT}$. For reference,
    we include a variant with the *natural* categories distribution in EntityQuestions,
    denoted $D_{\mathtt{Natural}}$ is fixed and identical to our experiments in §[4](#S4
    "4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?"). To address Q2, we further break down the test set
    performance by category. [Table 2](#S4.T2 "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs
    𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples?
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") presents
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: MaybeKnown Examples are Essential.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since $\mathtt{Unknown}$ examples. Surprisingly, $D_{\mathtt{HighlyKnown}}$
    test examples, yet its performance on the remaining categories is inferior. $D_{\mathtt{MaybeKnown}}$,
    $D_{\mathtt{MaybeKnown}}$’s performance on $\mathtt{MaybeKnown}$), without compromising
    performance on $\mathtt{HighlyKnown}$). This suggests that $\mathtt{MaybeKnown}$
    to correctly handle such examples during inference. It also demonstrates that
    with the right fine-tuning examples, $M_{D}$ becomes more capable of utilizing
    its pre-existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Limited Knowledge Enhances Overfitting.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In §[4.2](#S4.SS2 "4.2 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples: Harmful or Neutral? ‣ 4 How Harmful
    are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"),
    we demonstrated that $\mathtt{Unknown}$, though to a lesser degree. Specifically,
    at Convergence, $D_{\mathtt{WeaklyKnown}}$ experience significant performance
    drops compared to early_stop ($39.2{\mkern-3.0mu}\rightarrow{\mkern-3.0mu}35.4$).
    With training to Convergence, they show a modest improvement on $\mathtt{WeaklyKnown}$
    but substantially degrade on $\mathtt{HighlyKnown}$. This highlights that the
    decrease in performance is strongly attributed to an increased rate of hallucinations
    w.r.t. facts that were already known to $M$ after pre-training.'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, $D_{\mathtt{Natural}}$ in early_stop, suggesting that the mere
    presence of $\mathtt{MaybeKnown}$ suffices for high performance on $\mathtt{MaybeKnown}$
    has additional examples from other categories. Yet, $D_{\mathtt{Natural}}$– indicating
    that it still suffers from overfitting, most-likely due to the presence of $\mathtt{WeaklyKnown}$
    examples. Taken together these results demonstrate that $D_{\mathtt{MaybeKnown}}$
    stands out both in terms of top performance and reduced risk to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 6 SliCK Knowledge Categories Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assessing a model’s knowledge remains an open problem, particularly since evaluating
    the quality of such methods is challenging due to the lack of ground truth about
    what the model truly knows. In this work we proposed SliCK (§[3](#S3 "3 Quantifying
    Knowledge in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")):
    a four-category classification of facts w.r.t. the model’s knowledge. We now further
    analyze and discuss our design choices, hoping that SliCK can serve as a useful
    taxonomy to guide future research on this subject.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained Known Categories
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first reflect on whether our choice of splitting $\mathtt{Known}$ indeed
    captures facts with high degree of knowledge, as it consistently exceeds $95\%$
    and $\mathtt{WeaklyKnown}$ is worse that on $\mathtt{MaybeKnown}$. Additionally,
    the *exact* categories distinction we made was proven useful since it revealed
    important insights on the importance of the $\mathtt{MaybeKnown}$ fine-tuning
    examples, as discussed in detail in §[5](#S5 "5 Understanding Knowledge Types:
    Their Value and Impact ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/207718f4a13afc5dc87f1cb456cc87f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: SliCK $\mathtt{Unknown}$ as $\mathtt{Unknown}$ of test examples classified
    as $\mathtt{Unknown}$. Our $\mathtt{Unknown}$ with less than $10$ random 4-shot
    exemplars (see §[3](#S3 "3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?") and §[C](#A3 "Appendix C 𝑷_𝙲𝚘𝚛𝚛𝚎𝚌𝚝
    Approximation ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Unknown Test Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A desired property for $(q,a)$ that appear in the test set, is that $M$ post
    fine-tuning (otherwise they are not truly $\mathtt{Unknown}$ is extremely low
    ($3.2\%$ examples are actually unknown to $M$.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a case study for comparison, we analyze the P(True) approach by Kadavath
    et al. ([2022](#bib.bib10)): a continuous score that estimates the probability
    a model assigns to the correctness of a specific answer. P(True) was originally
    used for *self-evaluating* model-generated answers, while we use it to assess
    whether $M$ and compare this methodology to SliCK. Our results indicate that,
    at least in our setting, our approach categorizes $\mathtt{Unknown}$ using both
    methods, the accuracy on the P(True)-based $\mathtt{Unknown}$ is crucial, as using
    $N_{\text{ex}}<10$ examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Practical Implications.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This work highlights the risk in using supervised fine-tuning to update LLMs’
    knowledge, as we present empirical evidence that acquiring new knowledge through
    fine-tuning is correlated with hallucinations w.r.t pre-existing knowledge. Additionally,
    this work raises important questions for future exploration regarding fine-tuning
    practices. We saw that $\mathtt{Unknown}$ ones, thus their negative effect manifests
    as a form of *overfitting*, which emphasizes the importance of using *early-stopping*
    instead of a fixed number of fine-tuning steps. However, early-stopping may be
    less effective when fine-tuning on numerous tasks with distinct optimal stopping
    points. An alternative solution can be to align the fine-tuning data with the
    model’s knowledge by filtering-out $\mathtt{Unknown}$ fine-tuning examples can
    still be useful to teach LLMs to express uncertainty on $\mathtt{Unknown}$ fine-tuning
    examples with uncertainty expressions* (e.g., *“I don’t know”*) *reduce their
    negative effect?* Our preliminary experiment (described in §[K](#A11 "Appendix
    K Re-labeling 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Fine-tuning Example with an Uncertainty Expression: Initial
    Experiment ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"))
    suggests that the answer is *yes*, which indicates that such approaches could
    be the most promising. Exploring this is an interesting direction for future work.'
  prefs: []
  type: TYPE_NORMAL
- en: Superficial Alignment Hypothesis.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhou et al. ([2023](#bib.bib34)) hypothesized that the knowledge and capabilities
    of LLMs are mostly learned during pre-training, while alignment is a simple process
    where the model learns the style or format for interacting with users. They substantiate
    this hypothesis by showing that fine-tuning on just $\mathtt{1k}$ examples and
    mostly learn to utilize their pre-existing knowledge. We also showed that fine-tuning
    on $\mathtt{HighlyKnown}$ examples led to sub-optimal utilization of pre-existing
    knowledge, despite our task format being simpler than LIMA’s and our dataset being
    six times larger. Taken together, our findings suggest that even though most of
    the LLM’s knowledge is indeed acquired through pre-training, the model learns
    more than just style or format through fine-tuning, as the selection of fine-tuning
    examples significantly influences the model’s capability to utilize its pre-existing
    knowledge post fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New knowledge and hallucinations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Schulman ([2023](#bib.bib24)), Goldberg ([2023](#bib.bib7)) and Gudibande et al.
    ([2023](#bib.bib8)) mention the conjecture that fine-tuning on new factual knowledge
    may encourage hallucinations. Huang et al. ([2023](#bib.bib9)) categorized hallucination
    causes and formally defined this scenario as *capability misalignment*. They highlight
    that limited research addresses capability misalignment due to the challenge of
    defining the knowledge boundary of LLMs. Kang et al. ([2024](#bib.bib12)) showed
    that when a fine-tuned LLM encounters unknown queries at test time, its responses
    mimic the responses associated with the unknown examples in the fine-tuning data.
    Yin et al. ([2023](#bib.bib31)) showed that LLMs’ performance is not satisfactory
    when they face new knowledge in their input contexts and Lee et al. ([2023](#bib.bib14))
    analyzed the impact of unknown *in-context* learning examples. To the best of
    our knowledge, our work is the first to empirically assess the impact of exposure
    to new knowledge through fine-tuning on tendency of the fine-tuned model to hallucinate.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying knowledge in LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SliCK can be seen as a confidence elicitation method for the ground truth label
    ($M$ if it is confident that $a$). Existing work derive calibrated confidence
    from LLMs by examining agreement across multiple samples Kuhn et al. ([2023](#bib.bib13));
    Manakul et al. ([2023](#bib.bib17)); Tian et al. ([2023a](#bib.bib26)); Lyu et al.
    ([2024](#bib.bib16)), probing internal representations Azaria and Mitchell ([2023](#bib.bib3));
    Burns et al. ([2022](#bib.bib4)), eliciting verbalized probability Tian et al.
    ([2023b](#bib.bib27)) or direct prompting Kadavath et al. ([2022](#bib.bib10)).
    [Kadavath et al.](#bib.bib10) also trained a separate P(IK) model to predict if
    the LLM knows the answer to $q$ (§[3](#S3 "3 Quantifying Knowledge in LLMs ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")). A key difference
    is that we also define the SliCK categories, and provide evidence that we capture
    meaningful and useful categories.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We study the impact of integrating new factual knowledge through fine-tuning
    on the model’s tendency to hallucinate. We first propose SliCK, a categorization
    of facts w.r.t. LLM’s knowledge. We then design a controlled study where we isolate
    the impact of new knowledge and rigorously evaluate its effects. We provide multiple
    insights on the fine-tuning dynamics, with the following key findings: (1) Acquiring
    new knowledge via supervised fine-tuning is correlated with hallucinations w.r.t.
    pre-existing knowledge. (2) LLMs struggle to integrate new knowledge through fine-tuning
    and mostly learn to use their pre-existing knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 10 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our experiments were conducted using a single LLM, and thus it is unclear whether
    results will vary with different LLMs. Having said that, our study is extremely
    compute-heavy and thus challenging to replicate on multiple LLMs: First, our fine-tuning
    is compute-heavy as its runs are very long as we wanted to analyze the behavior
    during different stages of fine-tuning (including the overfitting stages). Second,
    and most importantly, to facilitate our study we needed to annotate a large scale
    dataset w.r.t the SliCK categories. To derive reliable conclusions, it was crucial
    to accurately assess the model’s knowledge w.r.t. a single fine-tuning example.
    In our case we run 170 inference steps per example, i.e., more than $15M$ inference
    steps to categorize our full dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, since we focus on closed-book QA, the practical implications from
    our study such as filtering-out $\mathtt{Unknown}$. We leave this for future work.
    Long-form generation tasks introduce evaluation challenges, leading to a wide
    adoption of LLM-based evaluations. Our choice to focus explicitly on closed book
    QA facilitates more accurate evaluation that enhances the reliability of our findings.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we did not test the effect of adding additional fine-tuning examples
    from diverse tasks into the fine-tuning mixture. While this could more closely
    approximate a typical instruction fine-tuning scenario, such dataset extension
    may introduce new factual knowledge in an uncontrollable way, which will limit
    our findings.
  prefs: []
  type: TYPE_NORMAL
- en: 11 Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Ori Ram, Uri Shaham, Alon Jacovi, Mor Ventura, Yochai
    Blau, Eyal Ben-David, Avi Caciularu, Avinatan Hassidim and the members of Roi
    Reichart’s NLP group for reviewing the paper draft and providing valuable feedback.
    Special thanks to Uri Shaham for assisting in setting up the fine-tuning pipeline
    during the early stages of our research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AlKhamissi et al. (2022) Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona
    Diab, and Marjan Ghazvininejad. 2022. A review on language models as knowledge
    bases. *arXiv preprint arXiv:2204.06031*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, et al. 2023. Palm 2 technical report. *arXiv preprint arXiv:2305.10403*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azaria and Mitchell (2023) Amos Azaria and Tom Mitchell. 2023. The internal
    state of an llm knows when its lying. *arXiv preprint arXiv:2304.13734*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burns et al. (2022) Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
    2022. Discovering latent knowledge in language models without supervision. *arXiv
    preprint arXiv:2212.03827*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cohen et al. (2023) Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson.
    2023. [Crawling the internal knowledge-base of language models](https://doi.org/10.18653/v1/2023.findings-eacl.139).
    In *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    1856–1869, Dubrovnik, Croatia. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao (2021) Leo Gao. 2021. [Behavior cloning is miscalibrated](https://www.alignmentforum.org/posts/BgoKdAzogxmgkuuAt/behavior-cloning-is-miscalibrated).
    *AI Alignment Forum*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goldberg (2023) Yoav Goldberg. 2023. [Reinforcement learning for language models](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang
    Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise
    of imitating proprietary llms. *arXiv preprint arXiv:2305.15717*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.
    2023. A survey on hallucination in large language models: Principles, taxonomy,
    challenges, and open questions. *arXiv preprint arXiv:2311.05232*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. *arXiv
    preprint arXiv:2207.05221*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamalloo et al. (2023) Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and
    Davood Rafiei. 2023. [Evaluating open-domain question answering in the era of
    large language models](https://doi.org/10.18653/V1/2023.ACL-LONG.307). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 5591–5606\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2024) Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and
    Sergey Levine. 2024. Unfamiliar finetuning examples control how language models
    hallucinate. *arXiv preprint arXiv:2403.05612*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuhn et al. (2023) Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic
    uncertainty: Linguistic invariances for uncertainty estimation in natural language
    generation. *arXiv preprint arXiv:2302.09664*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2023) Yoonsang Lee, Pranav Atreya, Xi Ye, and Eunsol Choi. 2023.
    Crafting in-context examples according to lms’ parametric knowledge. *arXiv preprint
    arXiv:2311.09579*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha
    Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023.
    [The unlocking spell on base llms: Rethinking alignment via in-context learning](http://arxiv.org/abs/2312.01552).
    *ArXiv preprint*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2024) Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai
    Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, and Chris Callison-Burch.
    2024. Calibrating large language models with sample consistency. *arXiv preprint
    arXiv:2402.13904*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023.
    Selfcheckgpt: Zero-resource black-box hallucination detection for generative large
    language models. *arXiv preprint arXiv:2303.08896*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishra et al. (2022) Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh
    Hajishirzi. 2022. [Cross-task generalization via natural language crowdsourcing
    instructions](https://doi.org/10.18653/v1/2022.acl-long.244). In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 3470–3487, Dublin, Ireland. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 35: Annual Conference on
    Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,
    November 28 - December 9, 2022*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. [Language models
    as knowledge bases?](https://doi.org/10.18653/v1/D19-1250) In *Proceedings of
    the 2019 Conference on Empirical Methods in Natural Language Processing and the
    9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [SQuAD: 100,000+ questions for machine comprehension of text](https://doi.org/10.18653/v1/D16-1264).
    In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    [Learning to retrieve prompts for in-context learning](https://doi.org/10.18653/v1/2022.naacl-main.191).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2655–2671,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schulman (2023) John Schulman. 2023. [Reinforcement learning from human feedback:
    Progress and challenges](https://www.youtube.com/watch?v=hhiLw5Q_UFg&ab_channel=BerkeleyEECS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sciavolino et al. (2021) Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,
    and Danqi Chen. 2021. [Simple entity-centric questions challenge dense retrievers](https://doi.org/10.18653/V1/2021.EMNLP-MAIN.496).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
    2021*, pages 6138–6148\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2023a) Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D
    Manning, and Chelsea Finn. 2023a. Fine-tuning language models for factuality.
    *arXiv preprint arXiv:2311.08401*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2023b) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma,
    Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023b. Just
    ask for calibration: Strategies for eliciting calibrated confidence scores from
    language models fine-tuned with human feedback. *arXiv preprint arXiv:2305.14975*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vrandečić and Krötzsch (2014) Denny Vrandečić and Markus Krötzsch. 2014. [Wikidata:
    a free collaborative knowledgebase](https://doi.org/10.1145/2629489). *Commun.
    ACM*, 57(10):78–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen
    Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang. 2023. [Evaluating
    open-qa evaluation](http://papers.nips.cc/paper_files/paper/2023/hash/f323d594aa5d2c68154433a131c07959-Abstract-Datasets_and_Benchmarks.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. [Finetuned language
    models are zero-shot learners](https://openreview.net/forum?id=gEZrGCozdqR). In
    *The Tenth International Conference on Learning Representations, ICLR 2022, Virtual
    Event, April 25-29, 2022*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023) Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. [ALCUNA:
    Large language models meet new knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.87).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 1397–1414, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yona et al. (2024) Gal Yona, Roee Aharoni, and Mor Geva. 2024. Narrowing the
    knowledge evaluation gap: Open-domain question answering with multi-granularity
    answers. *arXiv preprint arXiv:2401.04695*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung, Qing Lian,
    Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2023. R-tuning: Teaching large
    language models to refuse unknown questions. *arXiv preprint arXiv:2311.09677*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao
    Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. [LIMA: less is more for alignment](http://papers.nips.cc/paper_files/paper/2023/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Data Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| relation | question template | $\mathtt{HighlyKnown}$ | $\mathtt{WeaklyKnown}$
    | Total | Min |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| P131 | Where is [E] located? | 553 | 2529 | 1493 | 3071 | 7646 | 553 |'
  prefs: []
  type: TYPE_TB
- en: '| P136 | What type of music does [E] play? | 236 | 3410 | 1892 | 1978 | 7516
    | 236 |'
  prefs: []
  type: TYPE_TB
- en: '| P17 | Which country is [E] located in? | 4387 | 2628 | 511 | 364 | 7890 |
    364 |'
  prefs: []
  type: TYPE_TB
- en: '| P19 | Where was [E] born? | 369 | 1884 | 1498 | 4170 | 7921 | 369 |'
  prefs: []
  type: TYPE_TB
- en: '| P26 | Who is [E] married to? | 1609 | 1503 | 1087 | 3257 | 7456 | 1087 |'
  prefs: []
  type: TYPE_TB
- en: '| P264 | What music label is [E] represented by? | 206 | 1444 | 1854 | 3820
    | 7324 | 206 |'
  prefs: []
  type: TYPE_TB
- en: '| P36 | What is the capital of [E]? | 4160 | 1634 | 449 | 572 | 6815 | 449
    |'
  prefs: []
  type: TYPE_TB
- en: '| P40 | Who is [E]’s child? | 692 | 1467 | 1271 | 2680 | 6110 | 692 |'
  prefs: []
  type: TYPE_TB
- en: '| P495 | Which country was [E] created in? | 5459 | 1101 | 408 | 706 | 7674
    | 408 |'
  prefs: []
  type: TYPE_TB
- en: '| P69 | Where was [E] educated? | 233 | 1126 | 1712 | 3650 | 6721 | 233 |'
  prefs: []
  type: TYPE_TB
- en: '| P740 | Where was [E] founded? | 1323 | 1618 | 1428 | 2902 | 7271 | 1323 |'
  prefs: []
  type: TYPE_TB
- en: '| P800 | What is [E] famous for? | 301 | 330 | 222 | 503 | 1356 | 222 |'
  prefs: []
  type: TYPE_TB
- en: '| TOTAL | - | 19528 | 20674 | 13825 | 27673 | 81700 | 6142 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Statistics of the EntityQuestions train split annotated with SliCK
    categories. We annotate the entire train split but always fine-tune on exactly
    6142 examples (see the Min column). Refer to §[E](#A5 "Appendix E Fine-tuning
    Details ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") for
    more details.'
  prefs: []
  type: TYPE_NORMAL
- en: '| relation | question template | $\mathtt{HighlyKnown}$ | $\mathtt{WeaklyKnown}$
    | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| P131 | Where is [E] located? | 57 | 362 | 158 | 388 | 965 |'
  prefs: []
  type: TYPE_TB
- en: '| P136 | What type of music does [E] play? | 6 | 432 | 248 | 281 | 967 |'
  prefs: []
  type: TYPE_TB
- en: '| P17 | Which country is [E] located in? | 448 | 432 | 65 | 51 | 996 |'
  prefs: []
  type: TYPE_TB
- en: '| P19 | Where was [E] born? | 107 | 148 | 243 | 501 | 999 |'
  prefs: []
  type: TYPE_TB
- en: '| P26 | Who is [E] married to? | 177 | 238 | 158 | 378 | 951 |'
  prefs: []
  type: TYPE_TB
- en: '| P264 | What music label is [E] represented by? | 47 | 157 | 268 | 486 | 958
    |'
  prefs: []
  type: TYPE_TB
- en: '| P36 | What is the capital of [E]? | 580 | 152 | 62 | 86 | 880 |'
  prefs: []
  type: TYPE_TB
- en: '| P40 | Who is [E]’s child? | 99 | 191 | 167 | 344 | 801 |'
  prefs: []
  type: TYPE_TB
- en: '| P495 | Which country was [E] created in? | 699 | 147 | 51 | 96 | 993 |'
  prefs: []
  type: TYPE_TB
- en: '| P69 | Where was [E] educated? | 27 | 145 | 227 | 441 | 840 |'
  prefs: []
  type: TYPE_TB
- en: '| P740 | Where was [E] founded? | 182 | 245 | 181 | 334 | 942 |'
  prefs: []
  type: TYPE_TB
- en: '| P800 | What is [E] famous for? | 35 | 50 | 28 | 76 | 189 |'
  prefs: []
  type: TYPE_TB
- en: '| TOTAL | - | 2464 | 2699 | 1856 | 3462 | 10481 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: In-distribution test set statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '| relation | question template | $\mathtt{HighlyKnown}$ | $\mathtt{WeaklyKnown}$
    | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| P127 | Who owns [E]? | 125 | 383 | 168 | 314 | 990 |'
  prefs: []
  type: TYPE_TB
- en: '| P50 | Who is the author of [E]? | 287 | 193 | 115 | 372 | 967 |'
  prefs: []
  type: TYPE_TB
- en: '| P407 | Which language was [E] written in? | 366 | 153 | 59 | 45 | 623 |'
  prefs: []
  type: TYPE_TB
- en: '| P176 | Which company is [E] produced by? | 289 | 277 | 181 | 225 | 972 |'
  prefs: []
  type: TYPE_TB
- en: '| P170 | Who was [E] created by? | 142 | 284 | 120 | 304 | 850 |'
  prefs: []
  type: TYPE_TB
- en: '| P175 | Who performed [E]? | 94 | 120 | 103 | 663 | 980 |'
  prefs: []
  type: TYPE_TB
- en: '| P112 | Who founded [E]? | 134 | 116 | 76 | 140 | 466 |'
  prefs: []
  type: TYPE_TB
- en: '| TOTAL | - | 1437 | 1526 | 822 | 2063 | 5848 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Out-of-distribution test set statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: This section expands §[2](#S2 "2 Study Setup ‣ Does Fine-Tuning LLMs on New
    Knowledge Encourage Hallucinations?") with additional details about our data preprocessing
    steps. The EntityQuestions dataset Sciavolino et al. ([2021](#bib.bib25)) consists
    of train, development and test splits and spans 24 relations. Our train, development
    and test sets are curated based on the original splits from EntityQuestions. However,
    we use only 12 relations, since we wanted to reserve some relations for out-of-distribution
    test set. To avoid cherry-picking, the 12 relations used in our train, development
    and test sets are randomly sampled. The resulting relations are presented in Tables
    [3](#A1.T3 "Table 3 ‣ Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs on
    New Knowledge Encourage Hallucinations?") and [4](#A1.T4 "Table 4 ‣ Appendix A
    Data Preprocessing ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: 'We reserved the remaining 12 relations for out-of-distribution test set. However,
    we found that in those 12 reserved relations, 5 were too similar to some of the
    relations that we train on (Table [3](#A1.T3 "Table 3 ‣ Appendix A Data Preprocessing
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")), thus we
    suspected that this could lead to a test set that is not truly out-of-distribution.
    To address that, we filtered out those relations and were left with 7 relations
    for our-of-distribution. Specifically we filtered-out the following relations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P276 was filtered out since it directly overlaps with P131 since for both relations
    the question in EntityQuestions is of the form “Where is [E] located?”. P276 stands
    for “location” ([https://www.wikidata.org/wiki/Property:P276](https://www.wikidata.org/wiki/Property:P276))
    and P131 stands for “located in the administrative territorial entity” ([https://www.wikidata.org/wiki/Property:P131](https://www.wikidata.org/wiki/Property:P131)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P20, for which the question template is *“Where did [E] die?”*, was filtered
    out since it may require knowledge that relates to P19, for which the question
    template is *“Where was [E] born?”*. P20 stands for “place of death” ([https://www.wikidata.org/wiki/Property:P20](https://www.wikidata.org/wiki/Property:P20))
    and P19 stands for “place of birth” ([https://www.wikidata.org/wiki/Property:P19](https://www.wikidata.org/wiki/Property:P19)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P106, for which the question template is *“What kind of work does [E] do?”*,
    was filtered out since it may require knowledge that relates to P800, for which
    the question template is *“What is [E] famous for?”*. P106 stands for “occupation”
    ([https://www.wikidata.org/wiki/Property:P106](https://www.wikidata.org/wiki/Property:P106))
    and P800 stands for “notable work” ([https://www.wikidata.org/wiki/Property:P800](https://www.wikidata.org/wiki/Property:P800)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P413, for which the question template is *“What position does [E] play?”*, was
    filtered out since it may require knowledge that relates to P800, for which the
    question template is *“What is [E] famous for?”*. P413 stands for “position played
    on team / speciality” ([https://www.wikidata.org/wiki/Property:P413](https://www.wikidata.org/wiki/Property:P413))
    and P800 stands for “notable work” ([https://www.wikidata.org/wiki/Property:P800](https://www.wikidata.org/wiki/Property:P800)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P159, for which the question template is *“Where is the headquarters of [E]?”*,
    was filtered out since it may require knowledge that relates to P36, for which
    the question template is *“What is the capital of [E]?”*. P159 stands for “headquarters
    location” ([https://www.wikidata.org/wiki/Property:P159](https://www.wikidata.org/wiki/Property:P159))
    and P36 stands for “capital” ([https://www.wikidata.org/wiki/Property:P36](https://www.wikidata.org/wiki/Property:P36)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The 7 relations used for out-of-distribution test set are presented in [Table 5](#A1.T5
    "In Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we perform two additional filtering steps: (1) To simplify the process
    of categorizing the examples w.r.t. $M$ and $3.9\%$ and $P413$ of the EntityQuestions
    train set.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Test performance as Proxy for Hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now detail the relation between the test performance in our setting and hallucinations.
    In our study, poorer performance of a fine-tuned model $M_{D1}$ on the test set,
    can be attributed to a higher rate of hallucinations in $M_{D1}$, relative to
    its pre-existing knowledge, due to the following explanation.
  prefs: []
  type: TYPE_NORMAL
- en: The test set can be conceptually divided into two types of questions. First,
    there are questions with answers that are unknown to $M$ and $M_{D2}$ and $M_{D2}$,
    i.e. $M$ and $M_{D2}$ must rely on their pre-existing knowledge to answer such
    questions, and a lower performance on such question can be only categorized as
    an hallucination w.r.t. pre-existing knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C $\bm{P_{\bm{\mathtt{Correct}}}}$ Approximation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section expands §[3](#S3 "3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?") with additional details about
    our $P_{\mathtt{Correct}}$ based on the fraction of correct answers to $q$. We
    begin with randomly sampling $N_{\text{ex}}$-shot exemplars for each relation
    in our dataset (§[A](#A1 "Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?")). Then, to approximate $P_{\mathtt{Correct}}(q,a;M,T)$
    to generate answers to $q$ exemplars from the relation corresponding to $q$ to
    sample $N_{\text{sample}}$ exemplars. $P_{\mathtt{Correct}}(q,a;M,T></math> predictions.
    We also generate the greedy decoding prediction (<math id=$ exemplars. $P_{\mathtt{Correct}}(q,a;M,T=0)$
    predictions.^(13)^(13)13Since we can only have one greedy prediction for every
    k-shot exemplars.
  prefs: []
  type: TYPE_NORMAL
- en: We use $k=4$ to output answers in the correct format. We use $N_{\text{ex}}=10$.
    The $N_{\text{sample}}=16$ are sampled from Top 40.
  prefs: []
  type: TYPE_NORMAL
- en: The $k$ different samples since we found that even when the few-shot exemplars
    are sampled per-relation, their exact choice still affects the prediction. In
    §[6](#S6 "6 SliCK Knowledge Categories Analysis ‣ Does Fine-Tuning LLMs on New
    Knowledge Encourage Hallucinations?") and [Figure 5](#S6.F5 "In Fine-grained Known
    Categories ‣ 6 SliCK Knowledge Categories Analysis ‣ Does Fine-Tuning LLMs on
    New Knowledge Encourage Hallucinations?") we show evidence that this also improves
    the quality of our categories.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of our 4-shot prompt format, from real example from EntityQuestions
    with the relation $P106$ representing occupation.^(14)^(14)14[https://www.wikidata.org/wiki/Property:P106](https://www.wikidata.org/wiki/Property:P106)
    The question in this case is *“What kind of work does Ron Konopka do?”* and the
    ground truth asnwer is *“geneticist”*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: What kind of work does Nicolas Roeg do? A: film director Q: What kind of
    work does Crystal Geoffré do? A: actor Q: What kind of work does Maurice Blondel
    do? A: philosopher Q: What kind of work does Javier de Burgos do? A: politician
    Q: What kind of work does Ron Konopka do? A: | Wrong Answer | Paraphrase | Higher
    Granularity | Lower Granularity |'
  prefs: []
  type: TYPE_NORMAL
- en: '| $90\%$ | $2\%$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Error Analysis of 100 Predictions of the Pre-trained Model, for Which
    Exact Match is False.'
  prefs: []
  type: TYPE_NORMAL
- en: To decide whether a sampled answer is correct, we use the Exact Match (EM) metric
    to compare it with the ground truth answer. The main advantage in this choice
    is that when EM is True, we know that the answer is correct for $100\%$) and 50
    samples with $T=0.5$ of the cases where EM is False, the predicted answer is indeed
    incorrect. Which is a reasonable performance for our purpose, especially considering
    that when EM is True the answer is $100\%$ correct.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Data Annotation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: we first calculate $P_{\mathtt{Correct}}(q,a;M,T=0)$ for each $(q,a)$ approximation
    (§[3](#S3 "3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?") and §[C](#A3 "Appendix C 𝑷_𝙲𝚘𝚛𝚛𝚎𝚌𝚝 Approximation ‣
    Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")). We then use
    these values to categorize each $(q,a)$ pair into one of our four categories (§[3](#S3
    "3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?") and [Figure 2](#S2.F2 "In 2 Study Setup ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?")). We provide the full statistics
    of the categories on the train and test set, as well as the out-of-distribution
    test set in Tables [3](#A1.T3 "Table 3 ‣ Appendix A Data Preprocessing ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"), [4](#A1.T4 "Table
    4 ‣ Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?") and [5](#A1.T5 "Table 5 ‣ Appendix A Data Preprocessing ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Fine-tuning Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning Data.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In §[4](#S4 "4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on
    New Knowledge Encourage Hallucinations?") we examine the effect of new knowledge
    in the fine-tuning dataset $D$, by varying the proportion of $\mathtt{Unknown}$.
    When we create variants of $D$ of $\mathtt{Unknown}$ $\mathtt{Known}$ of $\mathtt{Unknown}$
    *from each relation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In §[5](#S5 "5 Understanding Knowledge Types: Their Value and Impact ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") we create single-category
    variants of $D$ across all variants, we want to make sure that we have $|D|$ as
    their sum. In other words, for each relation we calculate the size of the smallest
    category and sum these values. This leads to $|D|=6142$ to be the examples from
    category CAT and relation r. Consequently $\text{size}(\text{CAT}_{\text{r}})$.
    For example $\text{size}($ ${}_{\text{P131}})=553$ (see [Table 3](#A1.T3 "In Appendix
    A Data Preprocessing ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).
    We then define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$&#124;D&#124;=\sum_{r\in R_{\text{Train}}}\min\left\{\text{size}(CAT_{r})&#124;\
    \left.\begin{array}[]{l}\text{CAT}\in\{\\ \text{$\mathtt{HighlyKnown}$},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \text{$\mathtt{MaybeKnown}$},\\
  prefs: []
  type: TYPE_NORMAL
- en: \text{$\mathtt{WeaklyKnown}$},\\
  prefs: []
  type: TYPE_NORMAL
- en: \text{$\mathtt{Unknown}$}\}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{array}\right\}\right.$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $\text{R}_{\text{Train}}$ are the 12 relations from the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of our data format in the train, development and test sets,
    from real example from EntityQuestions with the relation $P106$ representing occupation.^(15)^(15)15[https://www.wikidata.org/wiki/Property:P106](https://www.wikidata.org/wiki/Property:P106)
    The question in this case is *“What kind of work does Ron Konopka do?”* and the
    ground truth asnwer is *“geneticist”*. Answer the following question. What kind
    of work does Ron Konopka do?
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning hypeparameters.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We fine-tune every model for 50 epochs for all our model variants to completely
    fit the training set, so we can examine all stages of fine-tuning. We use learning
    rate of 1e-5, a batch size of 128, and a dropout rate of 0.05. We evaluate the
    models every epoch on the development set. The early_stop stopping criteria is
    defined to be the epoch with the maximum accuracy on the development set.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Train Accuracy on Different $\mathtt{Known}$ Categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f1bebe64fac6944111fa1f3da2001f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Training accuracy as a function of fine-tuning duration, evaluated
    on the variant with $50\%$ fine-tuning examples. For reference, we also include
    the accuracy on the development set, accompanied by a zoom-in plot within a narrower
    range, to provide a more visible and clear view.'
  prefs: []
  type: TYPE_NORMAL
- en: In §[4.3](#S4.SS3 "4.3 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples are Fitted Slower than 𝙺𝚗𝚘𝚠𝚗 Examples
    ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?") we analyze the fine-tuning dynamic and present the
    training accuracy as function of the fine-tuning duration in [Figure 1](#S1.F1
    "In 1 Introduction ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
    For simplicity we treated the $\mathtt{Known}$ categories collectively. For reference
    we also include the plot with the full per-category breakdown in [Figure 6](#A6.F6
    "In Appendix F Train Accuracy on Different 𝙺𝚗𝚘𝚠𝚗 Categories ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?").
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Linear Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In §[4.4](#S4.SS4 "4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear
    Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?") and §[4.5](#S4.SS5 "4.5 Generalization
    to New Relations ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?") we use a linear model ([Equation 1](#S4.E1
    "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear Model Perspective
    ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?")) that predicts that test accuracy and the out-of-distribution
    test accuracy. We estimate the parameters of this linear model based on results
    from all our variants of $D$ and $\mathtt{Unknown}$ fits during different fine-tuning
    stages. This way we collect a dataset with examples of the form $(Accuracy,N_{\text{Kn}},N_{\text{Unk}})$,
    which we use to fit a linear regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Out-of-distribution (OOD) Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/094837f6c2114bc180b4e31f77fb3f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d374814505ee99c74a3063838f0169a7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Performance on the *out-of-distribution (OOD)* test set as a function
    of the $\%$ examples in the fine-tuning dataset $D$. This plot is the OOD version
    of [Figure 3](#S3.F3 "In 3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?"). Everything is similar to [Figure 3](#S3.F3
    "In 3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?"), except that y-axis is the accuracy on the OOD test set. We
    note that *the development set did not change (not OOD)*, thus it does not necessarily
    reflects the optimal stopping point for OOD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In §[4.5](#S4.SS5 "4.5 Generalization to New Relations ‣ 4 How Harmful are
    𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")
    we discuss *out-of-distribution (OOD)* results. In these experiments we simply
    used our OOD test set consisting of 7 relations unseen during fine-tuning (see
    §[A](#A1 "Appendix A Data Preprocessing ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?")). When we perform the analysis discussed in §[4.1](#S4.SS1
    "4.1 Higher 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Ratio is Proportional to Performance Degradation ‣ 4 How Harmful
    are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")
    and §[4.2](#S4.SS2 "4.2 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples: Harmful or Neutral? ‣ 4 How Harmful
    are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"),
    we additionally evaluated the models on the OOD test set. For completeness, we
    add here [Figure 7](#A8.F7 "In Appendix H Out-of-distribution (OOD) Evaluation
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"), which is
    the out-of-distribution version of [Figure 3](#S3.F3 "In 3 Quantifying Knowledge
    in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?").
    [Figure 7(a)](#A8.F7.sf1 "In Figure 7 ‣ Appendix H Out-of-distribution (OOD) Evaluation
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") presents
    the OOD test performance as a function of $\%$ examples in $D$ fine-tuning examples.
    The corresponding *in-distribution* results ([Figure 3(b)](#S3.F3.sf2 "In Figure
    3 ‣ 3 Quantifying Knowledge in LLMs ‣ Does Fine-Tuning LLMs on New Knowledge Encourage
    Hallucinations?")) were discussed in §[4.2](#S4.SS2 "4.2 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples: Harmful
    or Neutral? ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New
    Knowledge Encourage Hallucinations?"). We notice that similar trends, just with
    a smaller overall magnitude of the performance drop, up to 6 points drop compared
    to up to 14 for in-distribution. This smaller drop magnitude is also reflected
    in smaller values of $|\beta_{\text{ukn}}|$ ([Table 1](#S2.T1 "In 4.4 The Influence
    of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear Model Perspective ‣ 4 How Harmful are
    𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | early_stop |  | Convergence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathtt{Full}$ | $\mathtt{Mkn}$ | $\mathtt{Unk}$ |  | $\mathtt{Hkn}$
    | $\mathtt{Wkn}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{HighlyKnown}}$ | 40.5^(∗∗) |  | 98.7 | 60.1^(∗∗) | 9.0^(∗∗) |
    0.6^(∗∗) |  | 40.0^(∗∗) |  | 98.4 | 58.8^(∗∗) | 8.5^(∗∗) | 0.7^(∗∗) |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{MaybeKnown}}$ | 43.6 |  | 98.4 | 69.9 | 12.1^(∗∗) | 1.0^(∗∗)
    |  | 43.2 |  | 97.5^∗ | 68.2 | 12.9^(∗∗) | 1.3^(∗∗) |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{WeaklyKnown}}$ | 39.2^(∗∗) |  | 95.0^(∗∗) | 59.2^(∗∗) | 8.6^(∗∗)
    | 0.4^(∗∗) |  | 35.4^(∗∗) |  | 73.5^(∗∗) | 55.8^(∗∗) | 17.2 | 2.2^(∗∗) |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{Unknown}}$ | 37.5^(∗∗) |  | 95.6^(∗∗) | 52.9^(∗∗) | 6.5^(∗∗)
    | 0.6^(∗∗) |  | 25.8^(∗∗) |  | 55.8^(∗∗) | 36.6^(∗∗) | 12.2^(∗∗) | 3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{Natural}}$ | 43.5 |  | 98.0^∗ | 67.6^(∗∗) | 14.1 | 1.8 |  | 41.8^(∗∗)
    |  | 95.5^(∗∗) | 61.7^(∗∗) | 14.8^(∗∗) | 2.5^∗ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: A copy of [Table 2](#S4.T2 "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗
    on Accuracy: A Linear Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples?
    ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") with detailed
    notation of the statistic significant test results. In each column, statistically
    significant differences from the best result are indicated using ^∗ and ^(∗∗)
    for $p<0.05$ respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Statistic Significance Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In §[5](#S5 "5 Understanding Knowledge Types: Their Value and Impact ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") we present [Table 2](#S4.T2
    "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear Model Perspective
    ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?"). As mentioned in the caption, we perform statistic
    significance tests for each column. To this end we compare all the values to the
    maximal value in this column.'
  prefs: []
  type: TYPE_NORMAL
- en: For each subset of the test set, we randomly shuffle all the examples in it,
    split them up into 100 approximately equally sized subsets, and compute accuracy
    for each of them for all the models of interest. We then apply paired-sample t-test
    with $p<0.05$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Table 2](#S4.T2 "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy:
    A Linear Model Perspective ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning
    LLMs on New Knowledge Encourage Hallucinations?"), the best result is in bold,
    as well as all the results with statistically non-significant difference from
    the best with $p<0.05$, except two cases where it is only with $p<0.05$ $\mathtt{Unk}$
    $\mathtt{Mkn}$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we also discuss “horizontal” comparisons, where we compare early_stop
    to Convergence, we additionally run significance tests (not annotated in [Table 2](#S4.T2
    "In 4.4 The Influence of 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 vs 𝙺𝚗𝚘𝚠𝚗 on Accuracy: A Linear Model Perspective
    ‣ 4 How Harmful are 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Examples? ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?")) for $All$ was not statistically significant while
    for all others (including $D_{\mathtt{Natural}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix J The P(True) Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In §[6](#S6 "6 SliCK Knowledge Categories Analysis ‣ Does Fine-Tuning LLMs
    on New Knowledge Encourage Hallucinations?") we used the P(True) metric from Kadavath
    et al. ([2022](#bib.bib10)) as a case study for comparison. In [Figure 5](#S6.F5
    "In Fine-grained Known Categories ‣ 6 SliCK Knowledge Categories Analysis ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?") we compare our $\mathtt{Unknown}$
    based on a threshold of P(True). We calculated P(True) for every $(q,a)$ pair
    in the test set using Kadavath et al. ([2022](#bib.bib10))’s prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Where is Paris located? Proposed Answer: France Is the proposed answer:
    (A) True (B) False The proposed answer is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then treated $(q,a)$. We experimented with each possible threshold $T$,
    according to our test set. For each threshold $T$ out of the test set, (2) what
    was the accuracy on these examples after fine-tuning. We plot the results in [Figure 5](#S6.F5
    "In Fine-grained Known Categories ‣ 6 SliCK Knowledge Categories Analysis ‣ Does
    Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"), where P(True) is
    represented with the yellow line and our $\mathtt{Unknown}$). We also check smaller
    values of $N_{\text{ex}}$ (§[5](#S5 "5 Understanding Knowledge Types: Their Value
    and Impact ‣ Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix K Re-labeling $\mathtt{Unknown}$ Fine-tuning Example with an Uncertainty
    Expression: Initial Experiment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | early_stop |  | Convergence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Accuracy | % Answered |  | Accuracy | % Answered |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $D$ | 43.0 | 100.0 |  | 38.8 | 100.0 |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{\mathtt{IDK}}$ | 61.8 | 58.7 |  | 61.8 | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Results of our initial experiment where the label of the $\mathtt{Unknown}$
    in this case is the variant with $50\%$ and $50\%$. $D_{\mathtt{IDK}}$ $\mathtt{Unknown}$
    did not respond with *“I don’t know”*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work we showed that fitting $\mathtt{Unknown}$ examples from the fine-tuning
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We now perform a preliminary experiment where check whether fine-tuning the
    model to abstain from $\mathtt{Unknown}$ fine-tuning examples with the expression
    *“I don’t know”* and test whether this mitigates the observed overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8](#A11.T8 "In Appendix K Re-labeling 𝚄𝚗𝚔𝚗𝚘𝚠𝚗 Fine-tuning Example with
    an Uncertainty Expression: Initial Experiment ‣ Does Fine-Tuning LLMs on New Knowledge
    Encourage Hallucinations?") presents the $\%$ did not respond with *“I don’t know”*)
    and the accuracy on those questions. This experiment was conducted on the $D$
    $\mathtt{Unknown}$ as a reference and the second row is for the results with $D_{\mathtt{IDK}}$
    of the $\mathtt{Unknown}$ was replaced with *“I don’t know”*'
  prefs: []
  type: TYPE_NORMAL
- en: Consistent with the findings from previous work Zhang et al. ([2023](#bib.bib33)),
    we observe an improved accuracy on willingly answered test examples (when comparing
    $D$). When we compare early_stop vs Convergence for $D$) which illustrates the
    overfitting effect. However, we observe that re-labeling the $\mathtt{Unknown}$
    remains $61.8$)
  prefs: []
  type: TYPE_NORMAL
